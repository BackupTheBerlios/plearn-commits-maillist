From breuleux at mail.berlios.de  Fri Jun  1 00:16:19 2007
From: breuleux at mail.berlios.de (breuleux at BerliOS)
Date: Fri, 1 Jun 2007 00:16:19 +0200
Subject: [Plearn-commits] r7477 - trunk/plearn/vmat
Message-ID: <200705312216.l4VMGJOn021493@sheep.berlios.de>

Author: breuleux
Date: 2007-06-01 00:16:18 +0200 (Fri, 01 Jun 2007)
New Revision: 7477

Modified:
   trunk/plearn/vmat/CompactFileVMatrix.cc
Log:
minor corrections

Modified: trunk/plearn/vmat/CompactFileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.cc	2007-05-31 21:36:09 UTC (rev 7476)
+++ trunk/plearn/vmat/CompactFileVMatrix.cc	2007-05-31 22:16:18 UTC (rev 7477)
@@ -130,6 +130,8 @@
         i++;
     i++;
 
+    info = TVec<GroupInfo>();
+
     // Here we painfully read the header.
     while (1) {
         char type = header[i];
@@ -186,7 +188,6 @@
 
     header_length = i;
 
-        
 
     // If there is a non-empty list of active groups, activate only those.
     if (active_list_.length()) {
@@ -240,7 +241,10 @@
 /////////////////////
 void CompactFileVMatrix::openCurrentFile()
 {
-    if (!isfile(filename_)) {
+    if (f) {
+        PLERROR("In CompactFileVMatrix::openCurrentFile - File already open (%s)", filename_.c_str());
+    }
+    else if (!isfile(filename_)) {
         PLERROR("In CompactFileVMatrix::openCurrentFile - File does not exist (%s)", filename_.c_str());
     }
     else {
@@ -291,11 +295,16 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(info, copies);
+    TVec<GroupInfo> temp = info;
+    info = TVec<GroupInfo>();
+    for (int i=0; i<temp.length(); i++) {
+        info.append(temp[i]);
+    }
+
     deepCopyField(cache_index, copies);
 
-    // We don't want to share the file descriptor, so we close/reopen the file.
-    closeCurrentFile();
+    // We don't want to share the file descriptor
+    f = 0;
     openCurrentFile();
 }
 
@@ -313,6 +322,8 @@
 
 void CompactFileVMatrix::cleanup() {
     CompactFileVMatrix::closeCurrentFile();
+    cache = string();
+    cache_index = TVec<unsigned char>();
 }
 
 



From larocheh at mail.berlios.de  Fri Jun  1 02:40:07 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Jun 2007 02:40:07 +0200
Subject: [Plearn-commits] r7478 - trunk/plearn/vmat
Message-ID: <200706010040.l510e71w014161@sheep.berlios.de>

Author: larocheh
Date: 2007-06-01 02:40:04 +0200 (Fri, 01 Jun 2007)
New Revision: 7478

Modified:
   trunk/plearn/vmat/MultiTaskSeparationSplitter.cc
Log:
Changed how the training set was appended, because the original code seemed to create memory problems (according to valgrind)...


Modified: trunk/plearn/vmat/MultiTaskSeparationSplitter.cc
===================================================================
--- trunk/plearn/vmat/MultiTaskSeparationSplitter.cc	2007-05-31 22:16:18 UTC (rev 7477)
+++ trunk/plearn/vmat/MultiTaskSeparationSplitter.cc	2007-06-01 00:40:04 UTC (rev 7478)
@@ -152,7 +152,13 @@
     TVec<VMat> split(2);
     split[0] = get_input(multi_target_one_hot(add_missing(dataset,miss_cols_train),MISSING_VALUE,MISSING_VALUE),dataset->inputsize(),dataset->targetsize());
     split[1] = get_input(multi_target_one_hot(add_missing(dataset,miss_cols_test),MISSING_VALUE,MISSING_VALUE),dataset->inputsize(),dataset->targetsize());
-    if(append_train) split.append(split[0]);
+    if(append_train)
+    {
+        split.resize(3);
+        split[2] = split[0];
+    }
+    //split.append(split[0]);
+    //if(append_train) split.append(get_input(multi_target_one_hot(add_missing(dataset,miss_cols_train),MISSING_VALUE,MISSING_VALUE),dataset->inputsize(),dataset->targetsize()));
     return split;
 }
 



From larocheh at mail.berlios.de  Fri Jun  1 02:43:51 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Jun 2007 02:43:51 +0200
Subject: [Plearn-commits] r7479 - trunk/plearn/vmat
Message-ID: <200706010043.l510hpFh014589@sheep.berlios.de>

Author: larocheh
Date: 2007-06-01 02:43:49 +0200 (Fri, 01 Jun 2007)
New Revision: 7479

Modified:
   trunk/plearn/vmat/GeneralizedOneHotVMatrix.cc
   trunk/plearn/vmat/GeneralizedOneHotVMatrix.h
Log:
Added some comments and made compatibal with nan targets...


Modified: trunk/plearn/vmat/GeneralizedOneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GeneralizedOneHotVMatrix.cc	2007-06-01 00:40:04 UTC (rev 7478)
+++ trunk/plearn/vmat/GeneralizedOneHotVMatrix.cc	2007-06-01 00:43:49 UTC (rev 7479)
@@ -47,7 +47,9 @@
 /** GeneralizedOneHotVMatrix **/
 
 PLEARN_IMPLEMENT_OBJECT(GeneralizedOneHotVMatrix,
-                        "ONE LINE DESC", "ONE LINE HELP");
+                        "VMatrix that maps many columns to a one-hot vector", 
+                        "This VMat is a generalization of OneHotVMatrix where many columns (given\n"
+                        "by the Vec index) are mapped, instead of just the last one.");
 
 GeneralizedOneHotVMatrix::GeneralizedOneHotVMatrix(bool call_build_)
     : inherited(call_build_)
@@ -123,13 +125,13 @@
                   (OptionBase::learntoption | OptionBase::nosave),
                   "DEPRECATED - use 'source' instead.");
     declareOption(ol, "index", &GeneralizedOneHotVMatrix::index,
-                  OptionBase::buildoption, "");
+                  OptionBase::buildoption, "Columns to map to one-hot vector representation.");
     declareOption(ol, "nclasses", &GeneralizedOneHotVMatrix::nclasses,
-                  OptionBase::buildoption, "");
+                  OptionBase::buildoption, "Size of the one-hot vector for each columns to map.");
     declareOption(ol, "cold_value", &GeneralizedOneHotVMatrix::cold_value,
-                  OptionBase::buildoption, "");
+                  OptionBase::buildoption, "Cold values for all columns to map.");
     declareOption(ol, "hot_value", &GeneralizedOneHotVMatrix::hot_value,
-                  OptionBase::buildoption, "");
+                  OptionBase::buildoption, "Hot values for all columns to map.");
     inherited::declareOptions(ol);
 }
 
@@ -155,8 +157,9 @@
             Vec target = v.subVec(v_pos, nb_class);
             const real cold = cold_value[index_pos];
             const real hot = hot_value[index_pos];
-            const int classnum = int(source->get(i,j));
-            fill_one_hot(target, classnum, cold, hot);
+            const real classnum = source->get(i,j); // Maybe not a desired behavior
+            if(is_missing(classnum)) target.fill(cold);
+            else fill_one_hot(target, (int) classnum, cold, hot);
             v_pos += nb_class;
         }
     }

Modified: trunk/plearn/vmat/GeneralizedOneHotVMatrix.h
===================================================================
--- trunk/plearn/vmat/GeneralizedOneHotVMatrix.h	2007-06-01 00:40:04 UTC (rev 7478)
+++ trunk/plearn/vmat/GeneralizedOneHotVMatrix.h	2007-06-01 00:43:49 UTC (rev 7479)
@@ -51,7 +51,7 @@
 using namespace std;
 
 
-//!  This VMat is a generalization of OneHotVMatrix where all columns (given
+//!  This VMat is a generalization of OneHotVMatrix where many columns (given
 //!  by the Vec index) are mapped, instead of just the last one.
 class GeneralizedOneHotVMatrix: public SourceVMatrix
 {
@@ -59,9 +59,13 @@
 
 protected:
 
+    //! Columns to map to one-hot vector representation
     Vec index;
+    //! Size of the one-hot vector for each columns to map
     Vec nclasses;
+    //! Cold values for all columns to map
     Vec cold_value;
+    //! Hot values for all columns to map
     Vec hot_value;
 
 public:



From larocheh at mail.berlios.de  Fri Jun  1 02:46:49 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Jun 2007 02:46:49 +0200
Subject: [Plearn-commits] r7480 - trunk/plearn/vmat
Message-ID: <200706010046.l510kneb014810@sheep.berlios.de>

Author: larocheh
Date: 2007-06-01 02:46:49 +0200 (Fri, 01 Jun 2007)
New Revision: 7480

Modified:
   trunk/plearn/vmat/AddMissingVMatrix.h
Log:
Copied the parameters of inline building method add_missing to the built object instead of sharing same TVec<int>...


Modified: trunk/plearn/vmat/AddMissingVMatrix.h
===================================================================
--- trunk/plearn/vmat/AddMissingVMatrix.h	2007-06-01 00:43:49 UTC (rev 7479)
+++ trunk/plearn/vmat/AddMissingVMatrix.h	2007-06-01 00:46:49 UTC (rev 7480)
@@ -134,7 +134,8 @@
   {
     AddMissingVMatrix* ret = new AddMissingVMatrix();
     ret->source = source;
-    ret->missing_values_columns = missing_values_columns;
+    ret->missing_values_columns.resize(missing_values_columns.length());
+    ret->missing_values_columns << missing_values_columns;
     ret->build();
     return ret;
   }



From larocheh at mail.berlios.de  Fri Jun  1 02:48:12 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Jun 2007 02:48:12 +0200
Subject: [Plearn-commits] r7481 - trunk/plearn/vmat
Message-ID: <200706010048.l510mCoO014878@sheep.berlios.de>

Author: larocheh
Date: 2007-06-01 02:48:11 +0200 (Fri, 01 Jun 2007)
New Revision: 7481

Modified:
   trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
Log:
Added some deepcopy calls...


Modified: trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2007-06-01 00:46:49 UTC (rev 7480)
+++ trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2007-06-01 00:48:11 UTC (rev 7481)
@@ -429,6 +429,9 @@
 {
   inherited::makeDeepCopyFromShallowCopy(copies);
   deepCopyField(data, copies);
+  deepCopyField(source_target, copies);
+  deepCopyField(target_descriptor, copies);
+  deepCopyField(source_and_target, copies);
 }
 
 } // end of namespace PLearn



From larocheh at mail.berlios.de  Fri Jun  1 02:53:13 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Jun 2007 02:53:13 +0200
Subject: [Plearn-commits] r7482 - trunk/plearn_learners_experimental
Message-ID: <200706010053.l510rDI0015178@sheep.berlios.de>

Author: larocheh
Date: 2007-06-01 02:53:10 +0200 (Fri, 01 Jun 2007)
New Revision: 7482

Modified:
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h
Log:
Working towards NIPS!


Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-01 00:48:11 UTC (rev 7481)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-01 00:53:10 UTC (rev 7482)
@@ -304,9 +304,6 @@
         else
             PLERROR("In LinearInductiveTransferClassifier::build_(): model_type %s is not valid", model_type.c_str());
 
-        Var sup_output;
-        Var new_output;
-
         TVec<bool> class_tags(noutputs);
         if(targetsize() == 1)
         {

Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h	2007-06-01 00:48:11 UTC (rev 7481)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h	2007-06-01 00:53:10 UTC (rev 7482)
@@ -215,16 +215,16 @@
     mutable Func sup_output_and_target_to_cost; 
 
     // Neural networks variables
-    //! Parameters for hidden to output layer weights prediction
-    VarArray As;
-    //! Parameters for input to hidden layer weights prediction
-    VarArray Ws;
     //! Input to hidden layer weights 
     Var W;
-    //! Scale parameter for input to hidden layer weights prediction
-    VarArray s_hids;
-    //! Hidden layer neurons
-    VarArray hidden_neurons;
+//    //! Parameters for hidden to output layer weights prediction
+//    VarArray As;
+//    //! Parameters for input to hidden layer weights prediction
+//    VarArray Ws;
+//    //! Scale parameter for input to hidden layer weights prediction
+//    VarArray s_hids;
+//    //! Hidden layer neurons
+//    VarArray hidden_neurons;
     
 protected:
     //#####  Protected Member Functions  ######################################



From yoshua at mail.berlios.de  Fri Jun  1 15:01:18 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 1 Jun 2007 15:01:18 +0200
Subject: [Plearn-commits] r7483 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706011301.l51D1IbM026237@sheep.berlios.de>

Author: yoshua
Date: 2007-06-01 15:01:17 +0200 (Fri, 01 Jun 2007)
New Revision: 7483

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Generalized autolr to multiple schedules in parallel


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-01 00:53:10 UTC (rev 7482)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-01 13:01:17 UTC (rev 7483)
@@ -18,36 +18,96 @@
     return o
 
 
+def merge_schedules(schedules):
+    """Merge several learning rate schedules into a kind of multi-schedule
+    with one column per schedule but a unified sequence of stages.
+    Each schedule is a Nx2 array, with the first column containing number of
+    stages (examples) and the second column containing corresponding learning
+    rates. N may vary across schedules. If we have a schedule first row with [10000 0.01]
+    and a second row with [30000 0.001], it means that from the learner's current stage
+    to stage 10000 (excluded) we should use a learning rate of 0.01, and from stage 10000
+    to 30000 (excluded) we should use a learning rate of 0.001. The different
+    schedules do not have to have the same N nor the same maximum stage. They
+    will be merged in one big schedule ranging from the mininum to the maximum
+    of the stages found in all the schedules.
+    The result is an array with the stages in the first column and one
+    additional column (with a sequence of learning rates) for each input schedule.
+    """
+    n_schedules = len(schedules)
+    stages = []
+    learning_rates = []
+    row_indices = zeros([n_schedules],Int)
+    lrates = array([schedule[0,1] for schedule in schedules],Float)
+    schedule_lengths = array([schedule.shape[0] for schedule in schedules],Int)
+    maxstage = max([max(schedule[:,0]) for schedule in schedules])
+    while sum(row_indices<schedule_lengths)>0:
+        changed = []
+        stage = maxstage+1
+        for i in range(n_schedules):
+            if row_indices[i]<schedule_lengths[i]:
+                s=schedules[i][row_indices[i],0]
+                if s<stage:
+                    changed=[i]
+                    stage=s
+                elif s==stage:
+                    changed.append(i)
+        for i in changed:
+            lrates[i]=schedules[i][row_indices[i],1]
+            row_indices[i]+=1
+        stages.append(stage)
+        learning_rates.append(lrates.copy())
+    res=zeros([len(stages),1+n_schedules],Float)
+    res[:,0]=stages
+    res[:,1:]=learning_rates
+    return res
+    
+
 def train_with_schedule(learner,
-                        lr_options, # e.g. ["module.modules[0].cd_learning_rate" "module.modules[1].cd_learning_rate"]
-                        learning_rates, # list of learning rates to try
-                        stages,         # corresponding list of stages associated with each above learning rate
+                        lr_options,
+                        schedules,
                         trainset,testsets,expdir,
                         cost_to_select_best=0,
                         selected_costnames = False,
-                        logfile=False):
+                        logfile=None):
+    """Train a learner with one or more schedules of learning rates.
+lr_options is a list of list of option strings. Each list in lr_options
+is associated with a group of options (e.g. associated with a group
+of modules within the learner) that has its own learning rates schedule.
+Exemple of lr_options with only one schedule applied to two modules:
+  [["module.modules[0].cd_learning_rate" "module.modules[1].cd_learning_rate"]]
+The schedules argument is an array with the stages sequence in its first
+column and sequences of learning rates (one sequence per group) in
+each of the other columns (just like the result of the call to merge_schedules).
+"""
     costnames = learner.getTestCostNames()
     if not selected_costnames:
         # use all cost names if not user-provided
         selected_costnames=costnames
     cost_indices = [costnames.index(name) for name in selected_costnames]
     learner.setTrainingSet(trainset,False)
-    n_train = len(learning_rates)
+    stages = schedules[:,0]
+    learning_rates = schedules[:,1:]
+    n_train = len(stages)
+    n_schedules = len(lr_options)
     n_tests = len(testsets)
     n_costs = len(costnames)
-    results = zeros([n_train,2+n_tests*n_costs],Float32)
+    results = zeros([n_train,2+n_tests*n_costs],Float)
     best_err = 1e10
     if plearn.bridgemode.interactive:
         clf()
     colors="bgrcmyk"
     styles=['-', '--', '-.', ':', '.', ',', 'o', '^', 'v', '<', '>', 's', '+', 'x', 'D']
-    for i in range(n_train):
+    for i in range(n_train-1):
         learner.nstages = int(stages[i])
-        for lr_option in lr_options:
-            learner.changeOptions({lr_option:str(learning_rates[i])})
+        options = {}
+        for s in range(n_schedules):
+            for lr_option in lr_options[s]:
+                options[lr_option]=str(learning_rates[i,s])
+        learner.changeOptions(options)
         learner.train()
         results[i,0] = learner.stage
-        results[i,1] = learning_rates[i]
+        for s in range(n_schedules):
+            results[i,1+s] = learning_rates[i][s]
         for j in range(0,n_tests):
             ts = pl.VecStatsCollector()
             learner.test(testsets[j],ts,0,0)
@@ -55,7 +115,7 @@
                 print >>logfile, "At stage ",learner.stage," test" + str(j+1),": ",
             for k in range(0,n_costs):
                 err = ts.getStat("E["+str(k)+"]")
-                results[i,j*n_costs+k+2]=err
+                results[i,j*n_costs+k+1+n_schedules]=err
                 costname = costnames[cost_indices[k]]
                 if logfile:
                     print >>logfile, costname, "=", err,
@@ -64,7 +124,7 @@
                     learner.save(expdir+"/"+"best_learner.psave","plearn_ascii")
                 if plearn.bridgemode.interactive:
                     plot(results[0:i+1,0],results[0:i+1,
-                         j*n_costs+k+2],colors[k%7]+styles[j%15],
+                         j*n_costs+k+1+n_schedules],colors[k%7]+styles[j%15],
                          label='test'+str(j+1)+':'+costname)
             if logfile:
                 print >>logfile
@@ -79,7 +139,8 @@
                       cost_to_select_best=0,
                       initial_lr=0.01,
                       call_forget=True,
-                      lr_steps=exp(log(10)/2)):
+                      lr_steps=exp(log(10)/2),
+                      logfile=None):
     """
 Optimize initial learning rate by exploring greedily from a given initial learning rate
 If call_forget then the provided initial_learner is changed (and not necessarily the
@@ -116,6 +177,9 @@
         ts = pl.VecStatsCollector()
         learner.test(testset,ts,0,0)
         err=ts.getStat("E["+str(cost_to_select_best)+"]")
+        if logfile:
+            print >>logfile, "*trying* initial learning rate ",lr(i), \
+                  " and obtained err=",err," on cost ",cost_to_select_best
         if err<best_err:
             best_learner=learner
             best_err=err
@@ -145,13 +209,17 @@
     return (lr(best),perfs,best_learner)
 
 def train_adapting_lr(learner,
-                      epoch,nstages,
                       trainset,testsets,expdir,
                       lr_options,
+                      optimized_group=0,
+                      schedules=None,
+                      nstages=None,
+                      epoch=None,
                       initial_lr=0.1,
                       nskip=2,
                       cost_to_select_best=0,
-                      save_best=False,
+                      return_best_model=False, # o/w return final model
+                      save_best=False, # for paranoids: save best model each time an improvement is found
                       selected_costnames = False,
                       min_epochs_to_delete = 2,
                       lr_steps=exp(log(10)/2),
@@ -159,17 +227,19 @@
                       keep_lr=2):
 
     min_epochs_to_delete = max(1,min_epochs_to_delete) # although 1 is probably too small
+
+    def error_curve(active,start_t,current_t):
+        delta_t = current_t+1-start_t
+        return all_results[active][start_t:current_t+1,2+cost_to_select_best]
         
-    # used within the dominates function to predict future value of the error
-    def dominates(c1,c2,current_t):
-        """c1 has a lower last error than c2, but
-           will c2 eventually cross c1? if yes return False o/w return True"""
+    def error_curve_dominates(c1,c2,t):
+        """curve1 has a lower last error than curve2, but
+           will curve2 eventually cross curve1? if yes return False o/w return True"""
+
         start_t = max(all_start[c1],all_start[c2])
-        delta_t = current_t+1-start_t
-        curve1 = all_results[c1][start_t:current_t+1,2+cost_to_select_best]
-        curve2 = all_results[c2][start_t:current_t+1,2+cost_to_select_best]
-        # wait to have at least 3 points in curve =2 epochs since split between the candidates
-        if delta_t-1<min_epochs_to_delete or curve1[-1]>=curve2[-1]:
+        curve1 = error_curve(c1,start_t,t),
+        curve2 = error_curve(c2,start_t,t),
+        if len(curve1)-1<min_epochs_to_delete or curve1[-1]>=curve2[-1]:
             return False
         slope1=curve1[-1]-curve1[-2]
         slope2=curve2[-1]-curve2[-2]
@@ -189,7 +259,6 @@
             # keep if alone and a larger learning rate and improving
             return False
         return True
-        #return all_last_err[j]>all_last_err[i] and all_slope[j]>=all_slope[i]
     
     costnames = learner.getTestCostNames()
     if not selected_costnames:
@@ -198,33 +267,54 @@
     cost_indices = [costnames.index(name) for name in selected_costnames]
     n_tests = len(testsets)
     n_costs = len(costnames)
+    if schedules:
+        stages = schedules[:,0]
+        learning_rates = schedules[:,1:]
+    else:
+        if not nstages or not epoch:
+            raise ValueError("if schedules is not specified, then nstages and epoch must be")
+        stages = arange(learner.stage+epoch,learner.stage+nstages+1,epoch,Float)
+        learning_rates = initial_lr*ones([len(stages),1],Float)
+        schedules = zeros([len(stages),2],Float)
+        schedules[:,0]=stages
+        schedules[:,1]=learning_rates[:,0]
+        optimized_group=0 # no choice, there is only one group
+    n_train = len(stages)
+    n_schedules = len(lr_options)
+    assert n_schedules==learning_rates.shape[1]
     best_err = 1e10
     previous_best_err = best_err
     best_active = -1
-    n_epochs=nstages/epoch
-    all_results = [1e10*ones([n_epochs,2+n_tests*n_costs],Float32)]
+    all_results = [1e10*ones([n_train,2+n_tests*n_costs],Float)]
     all_candidates = [learner]
     all_last_err = [best_err]
-    #all_slope = [0]
     all_lr = [initial_lr]
     all_start = [0]
     actives = [0]
+    best_candidate = learner
+    best_early_stop = stages[0]
     if plearn.bridgemode.interactive:
         clf()
         colors="bgrcmyk"
         styles=['-', '--', '-.', ':', '.', ',', 'o', '^', 'v', '<', '>', 's', '+', 'x', 'D']
-    initial_stage=learner.stage
-    for (s,t) in zip(range(epoch,nstages+epoch,epoch),range(n_epochs)):
+    for t in range(n_train):
+        stage=stages[t]
         if logfile:
-            print >>logfile, "At stage ", initial_stage+s
+            print >>logfile, "At stage ", stage
         print "actives now: ",actives, " with lr=", array(all_lr)[actives]
         print >>logfile, "actives now: ",actives, " with lr=", array(all_lr)[actives]
         for active in actives:
             candidate = all_candidates[active]
             results = all_results[active]
-            candidate.nstages = int(initial_stage+s)
-            for lr_option in lr_options:
-                candidate.changeOptions({lr_option:str(all_lr[active])})
+            candidate.nstages = int(stage)
+            options = {}
+            for s in range(n_schedules):
+                for lr_option in lr_options[s]:
+                    if s==optimized_group:
+                        options[lr_option]=str(all_lr[active])
+                    else:
+                        options[lr_option]=str(learning_rates[t,s])
+            candidate.changeOptions(options)
             candidate.setTrainingSet(trainset,False)
             candidate.train()
             results[t,0] = candidate.stage
@@ -254,13 +344,14 @@
                                 plot(results[start:t+1,0],
                                      results[start:t+1,j*n_costs+k+2],colors[active%7]+styles[j%15])
 
-                        #if s>epoch:
-                        #        all_slope[active]=0.5*all_slope[active]+0.5*(err-all_last_err[active])
                         all_last_err[active]=err
                         if err < best_err:
                             best_err = err
                             best_active = active
-                            if save_best:
+                            best_early_stop = stage
+                            if return_best_model:
+                                best_candidate = deepcopy(candidate)
+                            if save_best: 
                                 candidate.save(expdir+"/"+"best_learner.psave","plearn_binary")
             if logfile:
                 print >>logfile
@@ -278,7 +369,7 @@
             best_last = all_last_err[best_active]
             ndeleted = 0
             for (a,j) in zip(actives,range(len(actives))):
-                if a!=best_active and dominates(best_active,a,t):
+                if a!=best_active and error_curve_dominates(best_active,a,t):
                     if logfile:
                         print >>logfile,"REMOVE candidate ",a
                     all_candidates[a]=None # hopefully this destroys the candidate
@@ -297,9 +388,13 @@
             if logfile:
                 print >>logfile,"CREATE candidate ", new_a, " from ",best_active,"at epoch ",s," with lr=",all_lr[new_a]
                 logfile.flush()
-    if save_best:
-        final_model = loadObject(expdir+"/"+"best_learner.psave")
+    if return_best_model:
+        final_model = best_candidate
     else:
         final_model = all_candidates[best_active]
-    return (final_model,all_results[best_active],all_results,all_last_err,all_start)
+    schedules[:,1+optimized_group]=all_results[best_active][:,1]
+    if logfile and best_err < all_last_err[best_active]:
+        print >>logfile, "WARNING: best performing model would have stopped early at stage ",best_early_stop
+    return (final_model,schedules,all_results[best_active], 
+            all_results,all_last_err,all_start,best_early_stop)
 



From tihocan at mail.berlios.de  Fri Jun  1 15:51:59 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 15:51:59 +0200
Subject: [Plearn-commits] r7484 - trunk/plearn_learners/online
Message-ID: <200706011351.l51DpxZx029129@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 15:51:58 +0200 (Fri, 01 Jun 2007)
New Revision: 7484

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
- Fixed computation of contrastive divergence gradient w.r.t. bias port to be the actual gradient of what is being computed (note: this is not the same gradient as the one usually used in contrastive divergence updates)
- Added safety warnings/checks to ensure we are not trying to back-propagate the CD gradient when 'cd_learning_rate' is set to 0 (because the current code does not support this)
- Added missing initialization of 'hidden_bias' matrix in bpropAccUpdate
- Removed debug pouts


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-01 13:01:17 UTC (rev 7483)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-01 13:51:58 UTC (rev 7484)
@@ -254,6 +254,7 @@
         addPortName("contrastive_divergence");
         addPortName("negative_phase_visible_samples.state");
         addPortName("negative_phase_hidden_expectations.state");
+        addPortName("negative_phase_hidden_activations.state");
     }
 
     port_sizes.resize(nPorts(), 2);
@@ -289,6 +290,11 @@
             port_sizes(getPortIndex("negative_phase_visible_samples.state"),1) = visible_layer->size; 
         if (hidden_layer)
             port_sizes(getPortIndex("negative_phase_hidden_expectations.state"),1) = hidden_layer->size; 
+        if (fast_exact_is_equal(cd_learning_rate, 0))
+            PLWARNING("In RBMModule::build_ - Contrastive divergence is "
+                    "computed but 'cd_learning_rate' is set to 0: no internal "
+                    "update will be performed AND no contrastive divergence "
+                    "gradient will be propagated.");
     }
 }
 
@@ -545,6 +551,7 @@
     Mat* contrastive_divergence = 0;
     Mat* negative_phase_visible_samples = 0;
     Mat* negative_phase_hidden_expectations = 0;
+    Mat* negative_phase_hidden_activations = NULL;
     if (compute_contrastive_divergence)
     {
         contrastive_divergence = ports_value[getPortIndex("contrastive_divergence")]; 
@@ -557,6 +564,8 @@
             ports_value[getPortIndex("negative_phase_visible_samples.state")];
         negative_phase_hidden_expectations = 
             ports_value[getPortIndex("negative_phase_hidden_expectations.state")];
+        negative_phase_hidden_activations =
+            ports_value[getPortIndex("negative_phase_hidden_activations.state")];
     }
 
     bool hidden_expectations_are_computed = false;
@@ -799,8 +808,6 @@
             for( int i=0; i<n_Gibbs_steps_CD; i++)
             {
                 hidden_layer->generateSamples();
-                //pout << "Negative phase hidden sample: " <<
-                    //hidden_layer->samples << endl;
                 // (Negative phase) Generate visible samples.
                 sampleVisibleGivenHidden(hidden_layer->samples);
                 // compute corresponding hidden expectations.
@@ -808,12 +815,19 @@
                 hidden_layer->computeExpectations();
             }
             PLASSERT(negative_phase_visible_samples);
-            PLASSERT(negative_phase_hidden_expectations);
+            PLASSERT(negative_phase_hidden_expectations &&
+                     negative_phase_hidden_expectations->isEmpty());
+            PLASSERT(negative_phase_hidden_activations &&
+                     negative_phase_hidden_activations->isEmpty());
             negative_phase_visible_samples->resize(mbs,visible_layer->size);
             *negative_phase_visible_samples << visible_layer->samples;
             negative_phase_hidden_expectations->resize(hidden_expectations.length(),
                                                        hidden_expectations.width());
             *negative_phase_hidden_expectations << hidden_expectations;
+            const Mat& neg_hidden_act = hidden_layer->activations;
+            negative_phase_hidden_activations->resize(neg_hidden_act.length(),
+                                                      neg_hidden_act.width());
+            *negative_phase_hidden_activations << neg_hidden_act;
 
             // compute the energy (again for now only in the binomial case)
             PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
@@ -920,11 +934,12 @@
     Mat* hidden_bias_grad = ports_gradient[getPortIndex("hidden_bias")];
     weights = ports_value[getPortIndex("weights")]; 
     Mat* weights_grad = ports_gradient[getPortIndex("weights")];    
+    hidden_bias = ports_value[getPortIndex("hidden_bias")];
+    Mat* contrastive_divergence_grad = NULL;
 
     // Ensure the gradient w.r.t. contrastive divergence is 1 (if provided).
-#ifdef BOUNDCHECK
     if (compute_contrastive_divergence) {
-        Mat* contrastive_divergence_grad =
+        contrastive_divergence_grad =
             ports_gradient[getPortIndex("contrastive_divergence")];
         if (contrastive_divergence_grad) {
             PLASSERT( !contrastive_divergence_grad->isEmpty() );
@@ -932,7 +947,6 @@
             PLASSERT( max(*contrastive_divergence_grad) <= 1 );
         }
     }
-#endif
 
     if(reconstruction_connection)
         reconstruction_error_grad = 
@@ -1028,6 +1042,11 @@
             compute_contrastive_divergence ?
                 ports_value[getPortIndex("negative_phase_hidden_expectations.state")]
                 : NULL;
+        Mat* negative_phase_hidden_activations =
+            compute_contrastive_divergence ?
+                ports_value[getPortIndex("negative_phase_hidden_activations.state")]
+                : NULL;
+        
         PLASSERT( visible && hidden );
         PLASSERT( !negative_phase_visible_samples ||
                   !negative_phase_visible_samples->isEmpty() );
@@ -1046,12 +1065,15 @@
             }
             PLASSERT( !compute_contrastive_divergence );
             PLASSERT( !negative_phase_hidden_expectations );
+            PLASSERT( !negative_phase_hidden_activations );
             negative_phase_hidden_expectations = &(hidden_layer->getExpectations());
-
             negative_phase_visible_samples = &(visible_layer->samples);
+            negative_phase_hidden_activations = &(hidden_layer->activations);
         }
         PLASSERT( negative_phase_hidden_expectations &&
                   !negative_phase_hidden_expectations->isEmpty() );
+        PLASSERT( negative_phase_hidden_activations &&
+                  !negative_phase_hidden_activations->isEmpty() );
         // Perform update.
         visible_layer->update(*visible, *negative_phase_visible_samples);
         if (weights_grad)
@@ -1093,12 +1115,27 @@
                 PLASSERT(hidden_bias_grad->width() == hidden_layer->size);
                 hidden_bias_grad->resize(mbs,hidden_layer->size);
             }
-            // d(contrastive_divergence)/dhidden_bias =
-            //     hidden - negative_phase_hidden_expectations
-            *hidden_bias_grad += *hidden;
-            *hidden_bias_grad -= *negative_phase_hidden_expectations;
+            // d(contrastive_divergence)/dhidden_bias
+            for (int k = 0; k < hidden_bias_grad->length(); k++) {
+                for (int i = 0; i < hidden_bias_grad->width(); i++) {
+                    real p_i_p = (*hidden)(k, i);
+                    real a_i_p = (*hidden_act)(k, i);
+                    real p_i_n = (*negative_phase_hidden_expectations)(k, i);
+                    real a_i_n = (*negative_phase_hidden_activations)(k, i);
+                    (*hidden_bias_grad)(k, i) +=
+                        - p_i_p * (1 - p_i_p) * a_i_p + p_i_p    // Pos. phase
+                     -( - p_i_n * (1 - p_i_n) * a_i_n + p_i_n ); // Neg. phase
+
+                }
+            }
         }
         partition_function_is_stale = true;
+    } else {
+        PLCHECK_MSG( !contrastive_divergence_grad ||
+                     (!hidden_bias_grad && !weights_grad),
+                "You currently cannot compute the "
+                "gradient of contrastive divergence w.r.t. external ports "
+                "when 'cd_learning_rate' is set to 0" );
     }
 
     if (reconstruction_error_grad && !reconstruction_error_grad->isEmpty()) {
@@ -1184,6 +1221,7 @@
     // Reset pointers to ensure we do not reuse them by mistake.
     hidden_act = NULL;
     weights = NULL;
+    hidden_bias = NULL;
 }
 
 ////////////
@@ -1257,7 +1295,6 @@
     computeHiddenActivations(visible);
     hidden_layer->computeExpectations();
     hidden_layer->generateSamples();
-    //pout << "sampleHiddenGivenVisible: " << hidden_layer->samples << endl;
 }
 
 //////////////////////////////
@@ -1268,7 +1305,6 @@
     computeVisibleActivations(hidden);
     visible_layer->computeExpectations();
     visible_layer->generateSamples();
-    //pout << "RBMModule::sampleVisibleGivenHidden: " << visible_layer->samples << endl;
 }
 
 /////////////////////



From tihocan at mail.berlios.de  Fri Jun  1 16:47:44 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 16:47:44 +0200
Subject: [Plearn-commits] r7485 - trunk/plearn/vmat
Message-ID: <200706011447.l51EliVb000675@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 16:47:44 +0200 (Fri, 01 Jun 2007)
New Revision: 7485

Modified:
   trunk/plearn/vmat/AddMissingVMatrix.h
Log:
Since we are doing a copy we could as well give the 'missing_values_columns' as a reference

Modified: trunk/plearn/vmat/AddMissingVMatrix.h
===================================================================
--- trunk/plearn/vmat/AddMissingVMatrix.h	2007-06-01 13:51:58 UTC (rev 7484)
+++ trunk/plearn/vmat/AddMissingVMatrix.h	2007-06-01 14:47:44 UTC (rev 7485)
@@ -130,7 +130,7 @@
 
 DECLARE_OBJECT_PTR(AddMissingVMatrix);
 
-inline VMat add_missing(VMat source, TVec<int> missing_values_columns)
+inline VMat add_missing(VMat source, const TVec<int>& missing_values_columns)
   {
     AddMissingVMatrix* ret = new AddMissingVMatrix();
     ret->source = source;



From tihocan at mail.berlios.de  Fri Jun  1 16:51:15 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 16:51:15 +0200
Subject: [Plearn-commits] r7486 - trunk/plearn_learners/hyper
Message-ID: <200706011451.l51EpF0v001255@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 16:51:15 +0200 (Fri, 01 Jun 2007)
New Revision: 7486

Modified:
   trunk/plearn_learners/hyper/HyperLearner.h
Log:
Made run() virtual

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2007-06-01 14:47:44 UTC (rev 7485)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2007-06-01 14:51:15 UTC (rev 7486)
@@ -110,7 +110,7 @@
 
     void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
-    void run();
+    virtual void run();
 
 }; // class HyperLearner
 



From plearner at mail.berlios.de  Fri Jun  1 16:52:47 2007
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 1 Jun 2007 16:52:47 +0200
Subject: [Plearn-commits] r7487 - in trunk/python_modules/plearn: . plotting
Message-ID: <200706011452.l51Eql2K001467@sheep.berlios.de>

Author: plearner
Date: 2007-06-01 16:52:47 +0200 (Fri, 01 Jun 2007)
New Revision: 7487

Added:
   trunk/python_modules/plearn/plotting/
   trunk/python_modules/plearn/plotting/__init__.py
Removed:
   trunk/python_modules/plearn/plotting.py
Log:
Changed plotting to a directory


Copied: trunk/python_modules/plearn/plotting/__init__.py (from rev 7485, trunk/python_modules/plearn/plotting.py)

Deleted: trunk/python_modules/plearn/plotting.py
===================================================================
--- trunk/python_modules/plearn/plotting.py	2007-06-01 14:51:15 UTC (rev 7486)
+++ trunk/python_modules/plearn/plotting.py	2007-06-01 14:52:47 UTC (rev 7487)
@@ -1,289 +0,0 @@
-# plotting.py
-# Copyright (C) 2005 Pascal Vincent
-#
-#  Redistribution and use in source and binary forms, with or without
-#  modification, are permitted provided that the following conditions are met:
-#
-#   1. Redistributions of source code must retain the above copyright
-#      notice, this list of conditions and the following disclaimer.
-#
-#   2. Redistributions in binary form must reproduce the above copyright
-#      notice, this list of conditions and the following disclaimer in the
-#      documentation and/or other materials provided with the distribution.
-#
-#   3. The name of the authors may not be used to endorse or promote
-#      products derived from this software without specific prior written
-#      permission.
-#
-#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-#  This file is part of the PLearn library. For more information on the PLearn
-#  library, go to the PLearn Web site at www.plearn.org
-
-
-# Author: Pascal Vincent
-
-# from array import *
-import numarray
-
-import string
-import matplotlib
-# matplotlib.interactive(True)
-#matplotlib.use('TkAgg')
-#matplotlib.use('GTK')
-  
-from pylab import *
-from numarray import *
-from mayavi.tools import imv
-
-from plearn.vmat.PMat import *
-
-
-threshold = 0
-
-def margin(scorevec):
-    if len(scorevec)==1:
-        return abs(scorevec[0]-threshold)
-    else:
-        sscores = sort(scorevec)    
-        return sscores[-1]-sscores[-2]
-
-def winner(scorevec):
-    if len(scorevec)==1:
-        if scorevec[0]>threshold:
-            return 1
-        else:
-            return 0
-    else:
-        return argmax(scorevec)
-
-def xyscores_to_winner_and_magnitude(xyscores):
-    return array([ (v[0], v[1], winner(v[2:]),max(v[2:])) for v in xyscores ])
-
-def xyscores_to_winner_and_margin(xyscores):
-    return array([ (v[0], v[1], winner(v[2:]),margin(v[2:])) for v in xyscores ])
-
-def regular_xyval_to_2d_grid_values(xyval):
-    """Returns (grid_values, x0, y0, deltax, deltay)"""
-    xyval = numarray.array(xyval)
-    n = len(xyval)
-    x = xyval[:,0]
-    y = xyval[:,1]
-    values = xyval[:,2:].copy()
-    # print "type(values)",type(values)
-    valsize = numarray.size(values,1)
-    x0 = x[0]
-    y0 = y[0]
-
-    k = 1
-    if x[1]==x0:
-        deltay = y[1]-y[0]
-        while x[k]==x0:
-            k = k+1
-        deltax = x[k]-x0
-        ny = k
-        nx = n // ny
-        # print 'A) nx,ny:',nx,ny
-        values.shape = (nx,ny,valsize)
-        # print "A type(values)",type(values)
-        values = numarray.transpose(values,(1,0,2))
-        # print "B type(values)",type(values)
-    elif y[1]==y0:
-        deltax = x[1]-x[0]
-        while y[k]==y0:
-            k = k+1
-        deltay = y[k]-y0
-        nx = k
-        ny = n // nx
-        # print 'B) nx,ny:',nx,ny
-        values.shape = (ny,nx,valsize)
-        # print "C type(values)",type(values)
-        values = numarray.transpose(values,(1,0,2))
-        # print "D type(values)",type(values)
-    else:
-        raise ValueError("Strange: x[1]!=x0 and y[1]!=y0 this doesn't look like a regular grid...")
-
-    print 'In regular_xyval_to_2d_grid_values: ', type(xyval), type(values)
-    return values, x0, y0, deltax, deltay
-
-
-def divide_by_mean_magnitude(xymagnitude):
-    mag = xymagnitude[:,2]
-    meanval = mag.mean()
-    mag *= 1./meanval
-    return meanval
-
-def divide_by_max_magnitude(xymagnitude):
-    mag = xymagnitude[:,2]
-    maxval = mag.max()
-    mag *= 1./maxval
-    return maxval
-
-def transform_magnitude_into_covered_percentage(xymagnitude):
-    magnitudes = []
-    l = len(xymagnitude)
-    for i in xrange(l):
-        row = xymagnitude[i]
-        magnitudes.append([row[-1],i])
-    magnitudes.sort()
-    # magnitude.reverse()
-    cum = 0
-    for row in magnitudes:
-        mag, i = row
-        cum += mag
-        row[0] = cum
-    for mag,i in magnitudes:
-        xymagnitude[i][-1] = mag/cum
-    return cum
-        
-def imshow_xymagnitude(regular_xymagnitude, interpolation='nearest', cmap = cm.jet):
-    grid_values, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xymagnitude)
-    # print 'In imshow_xymagnitude: ', type(regular_xymagnitude), type(grid_values)
-    imshow_2d_grid_values(grid_values, x0, y0, deltax, deltay, interpolation, cm.jet)
-    
-def imshow_xyrgb(regular_xyrgb, interpolation='nearest'):
-    grid_rgb, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xyrgb)
-    # print 'grid_rgb shape=',grid_rgb.shape
-    imshow_2d_grid_rgb(grid_rgb, x0, y0, deltax, deltay, interpolation, cm.jet)
-
-def classcolor(winner,margin=0):
-    colors = { 0: [0.5, 0.5, 1.0],
-               1: [1.0, 0.5, 0.5],
-               2: [0.5, 1.0, 0.5],
-             }
-    return colors[winner]
-    
-def xy_winner_magnitude_to_xyrgb(xy_winner_margin):
-    res = []
-    for x,y,w,m in xy_winner_margin:
-        res.append([x,y]+classcolor(w,m))
-    return res
-
-def xymagnitude_to_x_y_grid(regular_xymagnitude):
-    gridvalues, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xymagnitude)
-    nx = numarray.size(gridvalues,0)
-    ny = numarray.size(gridvalues,1)
-    gridvalues = numarray.reshape(gridvalues,(nx,ny))
-    x = numarray.arange(x0,x0+nx*deltax-1e-6,deltax)
-    y = numarray.arange(y0,y0+ny*deltay-1e-6,deltay)
-    # print "x = ",x
-    # print "y = ",y
-    # print "z = ",gridvalues
-    # print "type(x) = ",type(x)
-    # print "type(y) = ",type(y)
-    # print "type(z) = ",type(gridvalues)
-    # imv.view(gridvalues)
-    return x, y, gridvalues
-    
-def surfplot_xymagnitude(regular_xymagnitude):
-    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
-    imv.surf(x, y, gridvalues)
-
-def contour_xymagnitude(regular_xymagnitude):
-    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
-    clabel(contour(x, y, gridvalues))
-    
-def contourf_xymagnitude(regular_xymagnitude):
-    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
-    contourf(x, y, gridvalues)
-    colorbar()
-
-def imshow_2d_grid_rgb(gridrgb, x0, y0, deltax, deltay, interpolation='nearest', cmap = cm.jet):
-    nx = numarray.size(gridrgb,0)
-    ny = numarray.size(gridrgb,1)
-    extent = (x0-.5*deltax, x0+nx*deltax, y0-.5*deltay, y0+ny*deltay)
-    # gridrgb = numarray.reshape(gridrgb,(nx,ny))
-    # print 'SHAPE:',gridrgb.shape
-    # print 'gridrgb:', gridrgb
-    imshow(gridrgb, cmap=cmap, origin='lower', extent=extent, interpolation=interpolation)
-    colorbar()
-
-
-def imshow_2d_grid_values(gridvalues, x0, y0, deltax, deltay, interpolation='nearest', cmap = cm.jet):
-    nx = numarray.size(gridvalues,0)
-    ny = numarray.size(gridvalues,1)
-    extent = (x0-.5*deltax, x0+nx*deltax, y0-.5*deltay, y0+ny*deltay)
-    print 'gridval type', type(gridvalues)
-    gridvalues = numarray.reshape(gridvalues,(nx,ny))
-    # print 'SHAPE:',gridvalues.shape
-    # print 'gridvalues:', gridvalues
-    imshow(gridvalues, cmap=cmap, origin='lower', extent=extent, interpolation=interpolation)
-    colorbar()
-
-def plot_2d_points(pointlist, style='bo'):
-    x, y = zip(*pointlist)
-    plot(x, y, style)
-
-def plot_2d_class_points(pointlist, styles):
-    classnum = 0
-    for style in styles:
-        points_c = [ [x,y] for x,y,c in pointlist if c==classnum]
-        if len(points_c)==0:
-            break
-        # print 'points',classnum,':',points_c
-        plot_2d_points(points_c, style)
-        classnum += 1
-
-
-# def generate_2D_color_plot(x_y_color):
-
-# def plot_2D_decision_surface(training_points
-
-def main():
-    print "Still under development. Do not use!!!"
-    extent = (1, 25, -5, 25)
-
-    x = arange(7)
-    y = arange(5)
-    X, Y = meshgrid(x,y)
-    Z = rand( len(x), len(y))
-    # pcolor_classic(X, Y, transpose(Z))
-    #show()
-    #print 'pcolor'
-
-    for interpol in ['bicubic',
-                     'bilinear',
-                     'blackman100',
-                     'blackman256',
-                     'blackman64',
-                     'nearest',
-                     'sinc144',
-                     'sinc256',
-                     'sinc64',
-                     'spline16',
-                     'spline36']:
-
-        raw_input()
-        print interpol
-        clf()
-        imshow(Z, cmap=cm.jet, origin='upper', extent=extent, interpolation=interpol)
-        markers = [(15.9, 14.5), (16.8, 15), (20,20)]
-        x,y = zip(*markers)
-        plot(x, y, 'o')
-        plot(rand(20)*25, rand(20)*25, 'o') 
-        show()
-        # draw()
-
-    show()
-
-
-if __name__ == "__main__":
-    main()
-
-
-
-#t = arange(0.0, 5.2, 0.2)
-
-# red dashes, blue squares and green triangles
-#plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
-#show()
-



From tihocan at mail.berlios.de  Fri Jun  1 17:01:54 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 17:01:54 +0200
Subject: [Plearn-commits] r7488 - in
	trunk/plearn_learners/online/test/ModuleTester: . .pytest
	.pytest/PL_ModuleTester_RBM
	.pytest/PL_ModuleTester_RBM/expected_results
Message-ID: <200706011501.l51F1sDt002303@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 17:01:54 +0200 (Fri, 01 Jun 2007)
New Revision: 7488

Added:
   trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/
   trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/
   trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log
   trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
Modified:
   trunk/plearn_learners/online/test/ModuleTester/pytest.config
Log:
Added test 'PL_ModuleTester_RBM'


Property changes on: trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results


Added: trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log	2007-06-01 14:52:47 UTC (rev 7487)
+++ trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log	2007-06-01 15:01:54 UTC (rev 7488)
@@ -0,0 +1,14 @@
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+All tests passed successfully on module RBMModule
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+All tests passed successfully on module RBMModule
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+All tests passed successfully on module RBMModule
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+All tests passed successfully on module RBMModule
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+All tests passed successfully on module RBMModule
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+All tests passed successfully on module RBMModule
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+All tests passed successfully on module RBMModule

Added: trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-06-01 14:52:47 UTC (rev 7487)
+++ trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-06-01 15:01:54 UTC (rev 7488)
@@ -0,0 +1,94 @@
+# Run safety checks on RBMModule.
+
+from plearn.pyplearn import pl
+
+def rbm(cd_learning_rate, grad_learning_rate, visible_size, hidden_size, name,
+        compute_cd = False):
+    # Return a standard binomial RBM.
+    return pl.RBMModule(
+            name = name,
+            compute_contrastive_divergence = compute_cd,
+            cd_learning_rate = cd_learning_rate,
+            grad_learning_rate = grad_learning_rate,
+            visible_layer = pl.RBMBinomialLayer(size = visible_size),
+            hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
+            connection = pl.RBMMatrixConnection(
+                down_size = visible_size,
+                up_size = hidden_size))
+
+
+# All port configurations used in tests.
+conf_basic = \
+        {"in_grad":[ "visible" ],
+         "out_grad":[ "hidden.state" ],
+         "out_nograd":[ "hidden_activations.state" ]}
+
+conf_bias_cd = \
+        {"in_grad":[ "hidden_bias" ],
+         "in_nograd":[ "visible" ],
+         "out_grad":[ "contrastive_divergence" ],
+         "out_nograd":[ "hidden.state", "hidden_activations.state", "negative_phase_visible_samples.state",
+                        "negative_phase_hidden_expectations.state", "negative_phase_hidden_activations.state" ]}
+
+conf_bias_grad = \
+        {"in_grad":[ "hidden_bias" ],
+         "in_nograd":[ "visible" ],
+         "out_grad":[ "hidden.state" ],
+         "out_nograd":[ "hidden_activations.state" ]}
+
+conf_bias_both = \
+        {"in_grad":[ "hidden_bias" ],
+         "in_nograd":[ "visible" ],
+         "out_grad":[ "hidden.state", "contrastive_divergence" ],
+         "out_nograd":[ "hidden_activations.state", "negative_phase_visible_samples.state",
+                        "negative_phase_hidden_expectations.state", "negative_phase_hidden_activations.state" ]}
+
+
+testers = [
+
+    # Test a simple RBM that does not update itself (learning rates are set
+    # to 0).
+    pl.ModuleTester(
+            module = rbm(0, 0, 10, 20, 'rbm'),
+            configurations = [ conf_basic ]),
+
+    # Test with a positive gradient learning rate.
+    pl.ModuleTester(
+            module = rbm(0, 1e-3, 10, 20, 'rbm'),
+            configurations = [ conf_basic ]),
+
+    # Test with a positive contrastive divergence learning rate.
+    pl.ModuleTester(
+            module = rbm(1e-3, 0, 10, 20, 'rbm'),
+            configurations = [ conf_basic ]),
+
+    # Test with both contrastive divergence and gradient learning rate.
+    pl.ModuleTester(
+            module = rbm(1e-3, 1e-2, 10, 20, 'rbm'),
+            configurations = [ conf_basic ]),
+
+    # Test of gradient of contrastive divergence w.r.t. bias.
+    pl.ModuleTester(
+            module = rbm(1e-3, 0, 10, 20, 'rbm', True),
+            configurations = [ conf_bias_cd ],
+            min_out_grad = 1,
+            max_out_grad = 1),
+
+    # Test of backpropagation gradient w.r.t. bias.
+    pl.ModuleTester(
+            module = rbm(0, 1e-3, 10, 20, 'rbm'),
+            configurations = [ conf_bias_grad ]),
+
+    # Test of both backpropagation and contrastive divergence gradients w.r.t. bias.
+    pl.ModuleTester(
+            module = rbm(1e-2, 1e-3, 10, 20, 'rbm', True),
+            configurations = [ conf_bias_both ],
+            min_out_grad = 1,
+            max_out_grad = 1)
+
+    ]
+
+def main():
+    return pl.RunObject( objects = testers )
+
+

Modified: trunk/plearn_learners/online/test/ModuleTester/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/pytest.config	2007-06-01 14:52:47 UTC (rev 7487)
+++ trunk/plearn_learners/online/test/ModuleTester/pytest.config	2007-06-01 15:01:54 UTC (rev 7488)
@@ -137,4 +137,18 @@
     disabled = False
     )
 
+Test(
+    name = "PL_ModuleTester_RBM",
+    description = "Test RBMModule",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "PL_ModuleTester_RBM.pyplearn",
+    resources = [ "PL_ModuleTester_RBM.pyplearn" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False
+    )
 



From yoshua at mail.berlios.de  Fri Jun  1 17:12:59 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 1 Jun 2007 17:12:59 +0200
Subject: [Plearn-commits] r7489 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706011512.l51FCxOm003614@sheep.berlios.de>

Author: yoshua
Date: 2007-06-01 17:12:59 +0200 (Fri, 01 Jun 2007)
New Revision: 7489

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Fixed bug in autolr due to change from lists to arrays


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-01 15:01:54 UTC (rev 7488)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-01 15:12:59 UTC (rev 7489)
@@ -239,7 +239,7 @@
         start_t = max(all_start[c1],all_start[c2])
         curve1 = error_curve(c1,start_t,t),
         curve2 = error_curve(c2,start_t,t),
-        if len(curve1)-1<min_epochs_to_delete or curve1[-1]>=curve2[-1]:
+        if curve1.shape[0]-1<min_epochs_to_delete or curve1[-1]>=curve2[-1]:
             return False
         slope1=curve1[-1]-curve1[-2]
         slope2=curve2[-1]-curve2[-2]



From tihocan at mail.berlios.de  Fri Jun  1 18:29:31 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 18:29:31 +0200
Subject: [Plearn-commits] r7490 - trunk/plearn_learners/online
Message-ID: <200706011629.l51GTVUB011542@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 18:29:31 +0200 (Fri, 01 Jun 2007)
New Revision: 7490

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Added option 'standard_weights_grad' to choose between the usual weights update formula and the 'true' contrastive divergence gradient (only when using an external weights matrix)


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-01 15:12:59 UTC (rev 7489)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-01 16:29:31 UTC (rev 7490)
@@ -100,6 +100,7 @@
     Gibbs_step(0),
     log_partition_function(0),
     partition_function_is_stale(true),
+    standard_weights_grad(true),
     hidden_bias(NULL),
     weights(NULL),
     hidden_act(NULL),
@@ -135,12 +136,23 @@
 
     declareOption(ol, "cd_learning_rate", &RBMModule::cd_learning_rate,
                   OptionBase::buildoption,
-        "Learning rate for the constrastive divergence step.");
+        "Learning rate for the constrastive divergence step. Note that when\n"
+        "set to 0, the gradient of the contrastive divergence will not be\n"
+        "computed at all.");
 
     declareOption(ol, "compute_contrastive_divergence", &RBMModule::compute_contrastive_divergence,
                   OptionBase::buildoption,
         "Compute the constrastive divergence in an output port.");
 
+    declareOption(ol, "standard_weights_grad",
+                  &RBMModule::standard_weights_grad,
+                  OptionBase::buildoption,
+        "This option is only used when weights of the connection are given\n"
+        "through the 'weights' port. When this is the case, the gradient of\n"
+        "contrastive divergence w.r.t. weights is either computed:\n"
+        "- by the usual formula if 'standard_weights_grad' is true\n"
+        "- by the true gradient if 'standard_weights_grad' is false.");
+
     declareOption(ol, "n_Gibbs_steps_CD", 
                   &RBMModule::n_Gibbs_steps_CD,
                   OptionBase::buildoption,
@@ -958,11 +970,16 @@
             "an input gradient w.r.t. visible units" );
 
     bool compute_visible_grad = visible_grad && visible_grad->isEmpty();
+    bool compute_weights_grad = weights_grad && weights_grad->isEmpty();
     
     int mbs = (visible && !visible->isEmpty()) ? visible->length() : -1;
-    if (visible && !visible->isEmpty() && 
-        (hidden_grad && !hidden_grad->isEmpty()))
+
+    if (hidden_grad && !hidden_grad->isEmpty())
     {
+        // Note: the assert below is for behavior compatibility with previous
+        // code. It might not be necessary, or might need to be modified.
+        PLASSERT( visible && !visible->isEmpty() );
+
         // Note: we need to perform the following steps even if the gradient
         // learning rate is equal to 0. This is because we must propagate the
         // gradient to the visible layer, even though no update is required.
@@ -1076,45 +1093,84 @@
                   !negative_phase_hidden_activations->isEmpty() );
         // Perform update.
         visible_layer->update(*visible, *negative_phase_visible_samples);
-        if (weights_grad)
-        {
+
+        bool connection_update_is_done = false;
+        if (compute_weights_grad) {
+            // First resize the 'weights_grad' matrix.
             int up = connection->up_size;
             int down = connection->down_size;
             PLASSERT( weights && !weights->isEmpty() &&
-                    weights_grad->isEmpty() &&
-                    weights_grad->width() == up * down );
+                      weights_grad->width() == up * down );
             weights_grad->resize(mbs, up * down);
 
-            Mat wg;
-            Vec vp, hp, vn, hn;
-            for(int i=0; i<mbs; i++)
+            if (standard_weights_grad)
             {
-                vp = (*visible)(i);
-                hp = (*hidden)(i);
-                vn = (*negative_phase_visible_samples)(i);
-                hn = (*negative_phase_hidden_expectations)(i);
-                wg = Mat(up, down,(*weights_grad)(i));
-                connection->petiteCulotteOlivierCD(
-                        vp, hp,
-                        vn,
-                        hn,
-                        wg,
-                        true);
+                // Perform both computation of weights gradient and do update
+                // at the same time.
+                Mat wg;
+                Vec vp, hp, vn, hn;
+                for(int i=0; i<mbs; i++)
+                {
+                    vp = (*visible)(i);
+                    hp = (*hidden)(i);
+                    vn = (*negative_phase_visible_samples)(i);
+                    hn = (*negative_phase_hidden_expectations)(i);
+                    wg = Mat(up, down,(*weights_grad)(i));
+                    connection->petiteCulotteOlivierCD(
+                            vp, hp,
+                            vn,
+                            hn,
+                            wg,
+                            true);
+                    connection_update_is_done = true;
+                }
+            } else {
+                // Only do computation of gradient here.
+                PLASSERT( connection->classname() == "RBMMatrixConnection" &&
+                          visible_layer->classname() == "RBMBinomialLayer" &&
+                          hidden_layer->classname() == "RBMBinomialLayer" );
+
+                for (int k = 0; k < mbs; k++) {
+                    int idx = 0;
+                    for (int i = 0; i < up; i++) {
+                        real p_i_p = (*hidden)(k, i);
+                        real a_i_p = (*hidden_act)(k, i);
+                        real p_i_n =
+                            (*negative_phase_hidden_expectations)(k, i);
+                        real a_i_n =
+                            (*negative_phase_hidden_activations)(k, i);
+                        real scale_p = 1 - (1 - p_i_p) * a_i_p;
+                        real scale_n = 1 - (1 - p_i_n) * a_i_n;
+                        for (int j = 0; j < down; j++, idx++) {
+                            // Weight 'idx' is the (i,j)-th element in the
+                            // 'weights' matrix.
+                            real v_j_p = (*visible)(k, j);
+                            real v_j_n =
+                                (*negative_phase_visible_samples)(k, j);
+                            (*weights_grad)(k, idx) +=
+                                p_i_p * v_j_p * scale_p   // Positive phase.
+                             - (p_i_n * v_j_n * scale_n); // Negative phase.
+                        }
+                    }
+                }
             }
         }
-        else
-        {
+        if (!connection_update_is_done)
             connection->update(*visible, *hidden,
                     *negative_phase_visible_samples,
                     *negative_phase_hidden_expectations);
-        }
+
         hidden_layer->update(*hidden, *negative_phase_hidden_expectations);
+
         if (hidden_bias_grad)
         {
             if (hidden_bias_grad->isEmpty()) {
                 PLASSERT(hidden_bias_grad->width() == hidden_layer->size);
                 hidden_bias_grad->resize(mbs,hidden_layer->size);
             }
+            PLASSERT_MSG( hidden_layer->classname() == "RBMBinomialLayer" &&
+                          visible_layer->classname() == "RBMBinomialLayer",
+                          "Only implemented for binomial layers" );
             // d(contrastive_divergence)/dhidden_bias
             for (int k = 0; k < hidden_bias_grad->length(); k++) {
                 for (int i = 0; i < hidden_bias_grad->width(); i++) {

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-06-01 15:12:59 UTC (rev 7489)
+++ trunk/plearn_learners/online/RBMModule.h	2007-06-01 16:29:31 UTC (rev 7490)
@@ -90,6 +90,8 @@
     real log_partition_function;
     bool partition_function_is_stale;
 
+    bool standard_weights_grad;
+
 public:
     //#####  Public Member Functions  #########################################
 



From yoshua at mail.berlios.de  Fri Jun  1 19:30:02 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 1 Jun 2007 19:30:02 +0200
Subject: [Plearn-commits] r7491 - trunk/python_modules/plearn/learners/online
Message-ID: <200706011730.l51HU2NO028273@sheep.berlios.de>

Author: yoshua
Date: 2007-06-01 19:30:02 +0200 (Fri, 01 Jun 2007)
New Revision: 7491

Modified:
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Added generic code to build a supervised classification mlp in online package.


Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-06-01 16:29:31 UTC (rev 7490)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-06-01 17:30:02 UTC (rev 7491)
@@ -10,6 +10,37 @@
     else:
         return condp
 
+def supervised_classification_mlp(name,input_size,n_hidden,n_classes,
+                                  L1wd=0,L2wd=0):
+    return pl.NetworkModule(name=name,
+                            modules=[ pl.GradNNetLayerModule(name='a1',input_size=input_size,
+                                                             output_size=nh,L2_penalty_factor=L2wd,
+                                                             L1_penalty_factor=L1wd),
+                                      pl.TanhModule(name='tanh',input_size=nh,output_size=nh),
+                                      pl.GradNNetLayerModule(name='a2',input_size=nh,
+                                                             output_size=n_classes,
+                                                             L2_penalty_factor=L2wd,
+                                                             L1_penalty_factor=L1wd),
+                                      pl.SoftmaxModule(name='softmax',input_size=n_classes,
+                                                       output_size=n_classes),
+                                      pl.IdentityModule(name='target'),
+                                      pl.NLLCostModule(name='nll',input_size=n_classes),
+                                      pl.ClassErrorCostModule(name='clerr',input_size=n_classes)],
+                            connections=[  connection('a1.output','tanh.input'),
+                                           connection('tanh.output','a2.input'),
+                                           connection('a2.output','softmax.input'),
+                                           connection('softmax.output','nll.prediction'),
+                                           connection('softmax.output','clerr.prediction'),
+                                           connection('target.output','nll.target'),
+                                           connection('target.output','clerr.target')],
+                            ports = [ ('in', 'a1.input'),
+                                      ('target', 'target.input'),
+                                      ('out', 'softmax.output'),
+                                      ('nll', 'nll.cost')
+                                      ('class_err','clerr.cost') ] )
+                            
+
+
 def rbm(name,
         visible_size,
         hidden_size,



From lamblin at mail.berlios.de  Fri Jun  1 20:17:04 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 1 Jun 2007 20:17:04 +0200
Subject: [Plearn-commits] r7492 - trunk/plearn_learners/online
Message-ID: <200706011817.l51IH4iW031374@sheep.berlios.de>

Author: lamblin
Date: 2007-06-01 20:17:03 +0200 (Fri, 01 Jun 2007)
New Revision: 7492

Modified:
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
Log:
Minor changes


Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2007-06-01 17:30:02 UTC (rev 7491)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2007-06-01 18:17:03 UTC (rev 7492)
@@ -105,9 +105,10 @@
             cost += target_i * softplus(activation_i);
         if(!fast_exact_is_equal(target_i,1.0))
             // ret -= (1-target_i) * pl_log(1-expectation_i);
-            // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x))) =
-            //                         = -x -log(1+exp(-x)) = -x-softplus(-x)
-            cost += (1-target_i) * (softplus(activation_i) - activation_i);
+            // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+            //                         = log(1/(1+exp(x)))
+            //                         = -log(1+exp(x)) = -softplus(x)
+            cost += (1-target_i) * softplus(-activation_i);
     }
 
 }

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-01 17:30:02 UTC (rev 7491)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-01 18:17:03 UTC (rev 7492)
@@ -319,16 +319,17 @@
             ret += target_i * softplus(activation_i);
         if(!fast_exact_is_equal(target_i,1.0))
             // ret -= (1-target_i) * pl_log(1-expectation_i);
-            // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x))) = 
-            //                         = -x -log(1+exp(-x)) = -x-softplus(-x)
-            ret += (1-target_i) * (softplus(activation_i) - activation_i);
+            // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+            //                         = log(1/(1+exp(x)))
+            //                         = -log(1+exp(x)) = -softplus(x)
+            ret += (1-target_i) * softplus(-activation_i);
     }
     return ret;
 }
 
 void RBMBinomialLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
 {
-    computeExpectations();
+    // computeExpectations(); // why?
 
     PLASSERT( targets.width() == input_size );
     PLASSERT( targets.length() == batch_size );
@@ -350,9 +351,11 @@
                 nll += target[i] * softplus(activation[i]);
             if(!fast_exact_is_equal(target[i],1.0))
                 // nll -= (1-target[i]) * pl_log(1-output[i]);
-                // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x))) = 
-                //                         = -x -log(1+exp(-x)) = -x-softplus(-x)
-                nll += (1-target[i]) * (softplus(activation[i])-activation[i]);
+                // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                //                         = log(1/(1+exp(x)))
+                //                         = -log(1+exp(x))
+                //                         = -softplus(x)
+                nll += (1-target[i]) * softplus(-activation[i]);
 
         }
         costs_column(k,0) = nll;



From yoshua at mail.berlios.de  Fri Jun  1 20:17:22 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 1 Jun 2007 20:17:22 +0200
Subject: [Plearn-commits] r7493 - in trunk/plearn_learners/online: .
	EXPERIMENTAL
Message-ID: <200706011817.l51IHMdg031422@sheep.berlios.de>

Author: yoshua
Date: 2007-06-01 20:17:22 +0200 (Fri, 01 Jun 2007)
New Revision: 7493

Added:
   trunk/plearn_learners/online/IdentityModule.cc
   trunk/plearn_learners/online/IdentityModule.h
Removed:
   trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.h
Modified:
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/ModuleLearner.h
   trunk/plearn_learners/online/RBMModule.cc
Log:
Moved IdentityModule to mainstream, check that
negative likelihood is actually computable in RBMModule,
and added reset_seed_upon_train option in ModuleLearner.


Deleted: trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.cc	2007-06-01 18:17:03 UTC (rev 7492)
+++ trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.cc	2007-06-01 18:17:22 UTC (rev 7493)
@@ -1,236 +0,0 @@
-// -*- C++ -*-
-
-// IdentityModule.cc
-//
-// Copyright (C) 2007 Olivier Delalleau
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Olivier Delalleau
-
-/*! \file IdentityModule.cc */
-
-
-
-#include "IdentityModule.h"
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    IdentityModule,
-    "Module that simply replicates its input.",
-    ""
-);
-
-IdentityModule::IdentityModule()
-{}
-
-////////////////////
-// declareOptions //
-////////////////////
-void IdentityModule::declareOptions(OptionList& ol)
-{
-    // ### Declare all of this object's options here.
-    // ### For the "flags" of each option, you should typically specify
-    // ### one of OptionBase::buildoption, OptionBase::learntoption or
-    // ### OptionBase::tuningoption. If you don't provide one of these three,
-    // ### this option will be ignored when loading values from a script.
-    // ### You can also combine flags, for example with OptionBase::nosave:
-    // ### (OptionBase::buildoption | OptionBase::nosave)
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-////////////
-// build_ //
-////////////
-void IdentityModule::build_()
-{
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a "reloaded" object: i.e. from the complete set of
-    // ###    all serialised options.
-    // ###  - Updating or "re-building" of an object after a few "tuning"
-    // ###    options have been modified.
-    // ### You should assume that the parent class' build_() has already been
-    // ### called.
-}
-
-// ### Nothing to add here, simply calls build_
-void IdentityModule::build()
-{
-    inherited::build();
-    build_();
-}
-
-
-/////////////////////////////////
-// makeDeepCopyFromShallowCopy //
-/////////////////////////////////
-void IdentityModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("IdentityModule::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
-}
-
-///////////
-// fprop //
-///////////
-void IdentityModule::fprop(const Vec& input, Vec& output) const
-{
-    output.resize(input.length());
-    output << input;
-}
-
-void IdentityModule::fprop(const Mat& inputs, Mat& outputs) {
-    outputs.resize(inputs.length(), inputs.width());
-    outputs << inputs;
-}
-
-////////////////////
-// bpropAccUpdate //
-////////////////////
-void IdentityModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
-                                    const TVec<Mat*>& ports_gradient)
-{
-    // Deal with 'standard case' only.
-    PLASSERT( ports_gradient.length() == 2 );
-    Mat* input_grad = ports_gradient[0];
-    Mat* output_grad = ports_gradient[1];
-    if (!input_grad)
-        return;
-    PLASSERT( output_grad && !output_grad->isEmpty() &&
-              input_grad->isEmpty() );
-    PLASSERT( input_grad->width() == output_grad->width() );
-    input_grad->resize(output_grad->length(), input_grad->width());
-    *input_grad += *output_grad;
-}
-
-/////////////////
-// bpropUpdate //
-/////////////////
-/* THIS METHOD IS OPTIONAL
-void IdentityModule::bpropUpdate(const Vec& input, const Vec& output,
-                               Vec& input_gradient,
-                               const Vec& output_gradient,
-                               bool accumulate)
-{
-}
-*/
-
-/* THIS METHOD IS OPTIONAL
-void IdentityModule::bpropUpdate(const Vec& input, const Vec& output,
-                               const Vec& output_gradient)
-{
-}
-*/
-
-//////////////////
-// bbpropUpdate //
-//////////////////
-/* THIS METHOD IS OPTIONAL
-void IdentityModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                Vec& input_gradient,
-                                const Vec& output_gradient,
-                                Vec& input_diag_hessian,
-                                const Vec& output_diag_hessian,
-                                bool accumulate)
-{
-}
-*/
-
-/* THIS METHOD IS OPTIONAL
-void IdentityModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                const Vec& output_gradient,
-                                const Vec& output_diag_hessian)
-{
-}
-*/
-
-////////////
-// forget //
-////////////
-void IdentityModule::forget()
-{
-    // Nothing to forget.
-}
-
-//////////////
-// finalize //
-//////////////
-/* THIS METHOD IS OPTIONAL
-void IdentityModule::finalize()
-{
-}
-*/
-
-//////////////////////
-// bpropDoesNothing //
-//////////////////////
-/* THIS METHOD IS OPTIONAL
-bool IdentityModule::bpropDoesNothing()
-{
-}
-*/
-
-/////////////////////
-// setLearningRate //
-/////////////////////
-/* OPTIONAL
-void IdentityModule::setLearningRate(real dynamic_learning_rate)
-{
-}
-*/
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.h	2007-06-01 18:17:03 UTC (rev 7492)
+++ trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.h	2007-06-01 18:17:22 UTC (rev 7493)
@@ -1,207 +0,0 @@
-// -*- C++ -*-
-
-// IdentityModule.h
-//
-// Copyright (C) 2007 Olivier Delalleau
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Olivier Delalleau
-
-/*! \file IdentityModule.h */
-
-
-#ifndef IdentityModule_INC
-#define IdentityModule_INC
-
-#include <plearn_learners/online/OnlineLearningModule.h>
-
-namespace PLearn {
-
-class IdentityModule : public OnlineLearningModule
-{
-    typedef OnlineLearningModule inherited;
-
-public:
-    //#####  Public Build Options  ############################################
-
-public:
-    //#####  Public Member Functions  #########################################
-
-    //! Default constructor
-    // ### Make sure the implementation in the .cc
-    // ### initializes all fields to reasonable default values.
-    IdentityModule();
-
-    // Your other public member functions go here
-
-    //! given the input, compute the output (possibly resize it  appropriately)
-    virtual void fprop(const Vec& input, Vec& output) const;
-
-    //! Mini-batch fprop.
-    //! Default implementation raises an error.
-    virtual void fprop(const Mat& inputs, Mat& outputs);
-
-    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
-                                const TVec<Mat*>& ports_gradient);
-
-
-    /* Optional
-       THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
-    //! Adapt based on the output gradient, and obtain the input gradient.
-    //! The flag indicates wether the input_gradient is accumulated or set.
-    //! This method should only be called just after a corresponding
-    //! fprop; it should be called with the same arguments as fprop
-    //! for the first two arguments (and output should not have been
-    //! modified since then).
-    //! Since sub-classes are supposed to learn ONLINE, the object
-    //! is 'ready-to-be-used' just after any bpropUpdate.
-    virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient,
-                             const Vec& output_gradient,
-                             bool accumulate=false);
-    */
-
-    /* Optional
-       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-       JUST CALLS
-            bpropUpdate(input, output, input_gradient, output_gradient)
-       AND IGNORES INPUT GRADIENT.
-    //! This version does not obtain the input gradient.
-    virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             const Vec& output_gradient);
-    */
-
-    /* Optional
-       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-       RAISES A PLERROR.
-    //! Similar to bpropUpdate, but adapt based also on the estimation
-    //! of the diagonal of the Hessian matrix, and propagates this
-    //! back. If these methods are defined, you can use them INSTEAD of
-    //! bpropUpdate(...)
-    virtual void bbpropUpdate(const Vec& input, const Vec& output,
-                              Vec& input_gradient,
-                              const Vec& output_gradient,
-                              Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian,
-                              bool accumulate=false);
-    */
-
-    /* Optional
-       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
-       WHICH JUST CALLS
-            bbpropUpdate(input, output, input_gradient, output_gradient,
-                         out_hess, in_hess)
-       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
-    //! This version does not obtain the input gradient and diag_hessian.
-    virtual void bbpropUpdate(const Vec& input, const Vec& output,
-                              const Vec& output_gradient,
-                              const Vec& output_diag_hessian);
-    */
-
-
-    //! Reset the parameters to the state they would be BEFORE starting
-    //! training.  Note that this method is necessarily called from
-    //! build().
-    virtual void forget();
-
-
-    /* Optional
-       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
-       DO ANYTHING.
-    //! Perform some processing after training, or after a series of
-    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
-    //! operation.
-    virtual void finalize();
-    */
-
-    /* Optional
-       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
-    //! In case bpropUpdate does not do anything, make it known
-    virtual bool bpropDoesNothing();
-    */
-
-    /* Optional
-       Default implementation prints a warning and does nothing
-    //! If this class has a learning rate (or something close to it), set it.
-    //! If not, you can redefine this method to get rid of the warning.
-    virtual void setLearningRate(real dynamic_learning_rate);
-    */
-
-    //#####  PLearn::Object Protocol  #########################################
-
-    // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
-    PLEARN_DECLARE_OBJECT(IdentityModule);
-
-    // Simply calls inherited::build() then build_()
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
-
-
-protected:
-    //#####  Protected Member Functions  ######################################
-
-    //! Declares the class options.
-    static void declareOptions(OptionList& ol);
-
-private:
-    //#####  Private Member Functions  ########################################
-
-    //! This does the actual building.
-    void build_();
-
-private:
-    //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(IdentityModule);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: trunk/plearn_learners/online/IdentityModule.cc (from rev 7468, trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.cc)

Copied: trunk/plearn_learners/online/IdentityModule.h (from rev 7468, trunk/plearn_learners/online/EXPERIMENTAL/IdentityModule.h)

Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2007-06-01 18:17:03 UTC (rev 7492)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2007-06-01 18:17:22 UTC (rev 7493)
@@ -79,6 +79,7 @@
     target_ports(TVec<string>(1, "target")),
     // Note: many learners do not use weights, thus the default behavior is not
     // to have a 'weight' port in 'weight_ports'.
+    reset_seed_upon_train(0),
     mbatch_size(-1)
 {
     random_gen = new PRandom();
@@ -99,6 +100,12 @@
        "User-specified number of samples fed to the network at each iteration of learning.\n"
        "Use '0' for full batch learning.");
 
+    declareOption(ol, "reset_seed_upon_train", &ModuleLearner::reset_seed_upon_train,
+                  OptionBase::buildoption,
+                  "Whether to reset the random generator seed upon starting the train\n"
+                  "method. If positive this is the seed. If -1 use the value of the\n"
+                  "option 'use_a_separate_random_generator_for_testing'.\n");
+
     declareOption(ol, "cost_ports", &ModuleLearner::cost_ports,
                   OptionBase::buildoption,
        "List of ports that contain costs being computed (the first cost is\n"
@@ -275,6 +282,14 @@
     if (!initTrain())
         return;
 
+    if (reset_seed_upon_train)
+    {
+        if (reset_seed_upon_train>0)
+            random_gen->manual_seed(reset_seed_upon_train);
+        else if (reset_seed_upon_train==-1)
+            random_gen->manual_seed(use_a_separate_random_generator_for_testing);
+        else PLERROR("ModuleLearner::reset_seed_upon_train should be >=-1");
+    }
     OnlineLearningModule::during_training=true;
     if (stage == 0) {
         // Perform training set-dependent initialization here.

Modified: trunk/plearn_learners/online/ModuleLearner.h
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.h	2007-06-01 18:17:03 UTC (rev 7492)
+++ trunk/plearn_learners/online/ModuleLearner.h	2007-06-01 18:17:22 UTC (rev 7493)
@@ -63,6 +63,8 @@
     TVec<string> target_ports;
     TVec<string> weight_ports;
 
+    int reset_seed_upon_train;
+
 public:
     //#####  Public Member Functions  #########################################
 

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-01 18:17:03 UTC (rev 7492)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-01 18:17:22 UTC (rev 7493)
@@ -630,6 +630,9 @@
     {
         if (partition_function_is_stale && !during_training)
         {
+            PLASSERT_MSG(hidden_layer->size<32 || visible_layer->size<32,
+                         "To compute exact log-likelihood of an RBM, hidden_layer->size "
+                         "or visible_layer->size must be <32");
             // recompute partition function
             if (hidden_layer->size > visible_layer->size)
                 // do it by log-summing minus-free-energy of visible configurations



From yoshua at mail.berlios.de  Fri Jun  1 20:18:07 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 1 Jun 2007 20:18:07 +0200
Subject: [Plearn-commits] r7494 - in trunk/python_modules/plearn/learners:
	autolr online
Message-ID: <200706011818.l51II7BN031467@sheep.berlios.de>

Author: yoshua
Date: 2007-06-01 20:18:07 +0200 (Fri, 01 Jun 2007)
New Revision: 7494

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Fixed minor bugs in online and autolr


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-01 18:17:22 UTC (rev 7493)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-01 18:18:07 UTC (rev 7494)
@@ -237,8 +237,8 @@
            will curve2 eventually cross curve1? if yes return False o/w return True"""
 
         start_t = max(all_start[c1],all_start[c2])
-        curve1 = error_curve(c1,start_t,t),
-        curve2 = error_curve(c2,start_t,t),
+        curve1 = error_curve(c1,start_t,t)
+        curve2 = error_curve(c2,start_t,t)
         if curve1.shape[0]-1<min_epochs_to_delete or curve1[-1]>=curve2[-1]:
             return False
         slope1=curve1[-1]-curve1[-2]

Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-06-01 18:17:22 UTC (rev 7493)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-06-01 18:18:07 UTC (rev 7494)
@@ -14,10 +14,12 @@
                                   L1wd=0,L2wd=0):
     return pl.NetworkModule(name=name,
                             modules=[ pl.GradNNetLayerModule(name='a1',input_size=input_size,
-                                                             output_size=nh,L2_penalty_factor=L2wd,
+                                                             output_size=n_hidden,
+                                                             L2_penalty_factor=L2wd,
                                                              L1_penalty_factor=L1wd),
-                                      pl.TanhModule(name='tanh',input_size=nh,output_size=nh),
-                                      pl.GradNNetLayerModule(name='a2',input_size=nh,
+                                      pl.TanhModule(name='tanh',input_size=n_hidden,
+                                                    output_size=n_hidden),
+                                      pl.GradNNetLayerModule(name='a2',input_size=n_hidden,
                                                              output_size=n_classes,
                                                              L2_penalty_factor=L2wd,
                                                              L1_penalty_factor=L1wd),
@@ -36,7 +38,7 @@
                             ports = [ ('in', 'a1.input'),
                                       ('target', 'target.input'),
                                       ('out', 'softmax.output'),
-                                      ('nll', 'nll.cost')
+                                      ('nll', 'nll.cost'),
                                       ('class_err','clerr.cost') ] )
                             
 



From tihocan at mail.berlios.de  Fri Jun  1 20:23:05 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 20:23:05 +0200
Subject: [Plearn-commits] r7495 - trunk/plearn_learners/online
Message-ID: <200706011823.l51IN5AK031739@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 20:23:04 +0200 (Fri, 01 Jun 2007)
New Revision: 7495

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
- Renamed option 'standard_weights_grad' to 'standard_cd_weights_grad'
- Added option 'standard_cd_grad' to control whether to use standard CD gradient or 'true' gradient


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-01 18:18:07 UTC (rev 7494)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-01 18:23:04 UTC (rev 7495)
@@ -40,6 +40,7 @@
 
 #include "RBMModule.h"
 #include <plearn/vmat/VMat.h>
+#include <plearn_learners/online/RBMMatrixConnection.h>
 
 #define PL_LOG_MODULE_NAME "RBMModule"
 #include <plearn/io/pl_log.h>
@@ -100,7 +101,8 @@
     Gibbs_step(0),
     log_partition_function(0),
     partition_function_is_stale(true),
-    standard_weights_grad(true),
+    standard_cd_weights_grad(true),
+    standard_cd_grad(true),
     hidden_bias(NULL),
     weights(NULL),
     hidden_act(NULL),
@@ -144,14 +146,23 @@
                   OptionBase::buildoption,
         "Compute the constrastive divergence in an output port.");
 
-    declareOption(ol, "standard_weights_grad",
-                  &RBMModule::standard_weights_grad,
+    declareOption(ol, "standard_cd_grad",
+                  &RBMModule::standard_cd_grad,
                   OptionBase::buildoption,
+        "Whether to use the standard contrastive divergence gradient for\n"
+        "updates, or the true gradient of the contrastive divergence. This\n"
+        "affects only the gradient w.r.t. internal parameters of the layers\n"
+        "and connections. Currently, this option works only with layers of\n"
+        "the type 'RBMBinomialLayer', connected by a 'RBMMatrixConnection'.");
+
+    declareOption(ol, "standard_cd_weights_grad",
+                  &RBMModule::standard_cd_weights_grad,
+                  OptionBase::buildoption,
         "This option is only used when weights of the connection are given\n"
         "through the 'weights' port. When this is the case, the gradient of\n"
         "contrastive divergence w.r.t. weights is either computed:\n"
-        "- by the usual formula if 'standard_weights_grad' is true\n"
-        "- by the true gradient if 'standard_weights_grad' is false.");
+        "- by the usual formula if 'standard_cd_weights_grad' is true\n"
+        "- by the true gradient if 'standard_cd_weights_grad' is false.");
 
     declareOption(ol, "n_Gibbs_steps_CD", 
                   &RBMModule::n_Gibbs_steps_CD,
@@ -515,6 +526,8 @@
 
     deepCopyField(hidden_exp_grad, copies);
     deepCopyField(hidden_act_grad, copies);
+    deepCopyField(store_weights_grad, copies);
+    deepCopyField(store_hidden_bias_grad, copies);
     deepCopyField(visible_exp_grad, copies);
     deepCopyField(visible_act_grad, copies);
     deepCopyField(visible_bias_grad, copies);
@@ -1094,6 +1107,7 @@
                   !negative_phase_hidden_expectations->isEmpty() );
         PLASSERT( negative_phase_hidden_activations &&
                   !negative_phase_hidden_activations->isEmpty() );
+
         // Perform update.
         visible_layer->update(*visible, *negative_phase_visible_samples);
 
@@ -1106,7 +1120,7 @@
                       weights_grad->width() == up * down );
             weights_grad->resize(mbs, up * down);
 
-            if (standard_weights_grad)
+            if (standard_cd_weights_grad)
             {
                 // Perform both computation of weights gradient and do update
                 // at the same time.
@@ -1127,8 +1141,20 @@
                             true);
                     connection_update_is_done = true;
                 }
-            } else {
-                // Only do computation of gradient here.
+            }
+        }
+        if (!standard_cd_weights_grad || !standard_cd_grad) {
+            // Compute 'true' gradient of contrastive divergence w.r.t.
+            // the weights matrix.
+            int up = connection->up_size;
+            int down = connection->down_size;
+            Mat* weights_g = weights_grad;
+            if (!weights_g) {
+                // We need to store the gradient in another matrix.
+                store_weights_grad.resize(mbs, up * down);
+                store_weights_grad.clear();
+                weights_g = & store_weights_grad;
+            }
                 PLASSERT( connection->classname() == "RBMMatrixConnection" &&
                           visible_layer->classname() == "RBMBinomialLayer" &&
                           hidden_layer->classname() == "RBMBinomialLayer" );
@@ -1150,44 +1176,78 @@
                             real v_j_p = (*visible)(k, j);
                             real v_j_n =
                                 (*negative_phase_visible_samples)(k, j);
-                            (*weights_grad)(k, idx) +=
+                            (*weights_g)(k, idx) +=
                                 p_i_p * v_j_p * scale_p   // Positive phase.
                              - (p_i_n * v_j_n * scale_n); // Negative phase.
                         }
                     }
                 }
-            }
+                if (!standard_cd_grad) {
+                    // Update connection manually.
+                    Mat& weights = ((RBMMatrixConnection*)
+                            get_pointer(connection))->weights;
+                    real lr = cd_learning_rate / mbs;
+                    for (int k = 0; k < mbs; k++) {
+                        int idx = 0;
+                        for (int i = 0; i < up; i++)
+                            for (int j = 0; j < down; j++, idx++)
+                                weights(i, j) -= lr * (*weights_g)(k, idx);
+                    }
+                    connection_update_is_done = true;
+                }
         }
         if (!connection_update_is_done)
+            // Perform standard update of the connection.
             connection->update(*visible, *hidden,
                     *negative_phase_visible_samples,
                     *negative_phase_hidden_expectations);
 
-        hidden_layer->update(*hidden, *negative_phase_hidden_expectations);
+        Mat* hidden_bias_g = hidden_bias_grad;
+        if (!standard_cd_grad && !hidden_bias_grad) {
+            // We need to compute the CD gradient w.r.t. bias of hidden layer,
+            // but there is no bias coming from the outside. Thus we need
+            // another matrix to store this gradient.
+            store_hidden_bias_grad.resize(mbs, hidden_layer->size);
+            store_hidden_bias_grad.clear();
+            hidden_bias_g = & store_hidden_bias_grad;
+        }
 
-        if (hidden_bias_grad)
+        if (hidden_bias_g)
         {
-            if (hidden_bias_grad->isEmpty()) {
-                PLASSERT(hidden_bias_grad->width() == hidden_layer->size);
-                hidden_bias_grad->resize(mbs,hidden_layer->size);
+            if (hidden_bias_g->isEmpty()) {
+                PLASSERT(hidden_bias_g->width() == hidden_layer->size);
+                hidden_bias_g->resize(mbs,hidden_layer->size);
             }
             PLASSERT_MSG( hidden_layer->classname() == "RBMBinomialLayer" &&
                           visible_layer->classname() == "RBMBinomialLayer",
                           "Only implemented for binomial layers" );
             // d(contrastive_divergence)/dhidden_bias
-            for (int k = 0; k < hidden_bias_grad->length(); k++) {
-                for (int i = 0; i < hidden_bias_grad->width(); i++) {
+            for (int k = 0; k < hidden_bias_g->length(); k++) {
+                for (int i = 0; i < hidden_bias_g->width(); i++) {
                     real p_i_p = (*hidden)(k, i);
                     real a_i_p = (*hidden_act)(k, i);
                     real p_i_n = (*negative_phase_hidden_expectations)(k, i);
                     real a_i_n = (*negative_phase_hidden_activations)(k, i);
-                    (*hidden_bias_grad)(k, i) +=
+                    (*hidden_bias_g)(k, i) +=
                         - p_i_p * (1 - p_i_p) * a_i_p + p_i_p    // Pos. phase
                      -( - p_i_n * (1 - p_i_n) * a_i_n + p_i_n ); // Neg. phase
 
                 }
             }
         }
+
+        if (standard_cd_grad) {
+            hidden_layer->update(*hidden, *negative_phase_hidden_expectations);
+        } else {
+            PLASSERT( hidden_layer->classname() == "RBMBinomialLayer" );
+            // Update hidden layer by hand.
+            Vec& bias = hidden_layer->bias;
+            real lr = cd_learning_rate / mbs;
+            for (int i = 0; i < mbs; i++)
+                bias -= lr * (*hidden_bias_g)(i);
+        }
+
+
         partition_function_is_stale = true;
     } else {
         PLCHECK_MSG( !contrastive_divergence_grad ||

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-06-01 18:18:07 UTC (rev 7494)
+++ trunk/plearn_learners/online/RBMModule.h	2007-06-01 18:23:04 UTC (rev 7495)
@@ -90,7 +90,9 @@
     real log_partition_function;
     bool partition_function_is_stale;
 
-    bool standard_weights_grad;
+    bool standard_cd_weights_grad;
+    
+    bool standard_cd_grad;
 
 public:
     //#####  Public Member Functions  #########################################
@@ -240,6 +242,12 @@
     Mat* hidden_act;
     bool hidden_activations_are_computed;    
 
+    //! Used to store the contrastive divergence gradient w.r.t. weights.
+    Mat store_weights_grad;
+
+    //! Used to store the contrastive divergence gradient w.r.t. hidden bias.
+    Mat store_hidden_bias_grad;
+
     //! List of port names.
     TVec<string> ports;
 



From tihocan at mail.berlios.de  Fri Jun  1 20:36:26 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 20:36:26 +0200
Subject: [Plearn-commits] r7496 - trunk/plearn_learners/online
Message-ID: <200706011836.l51IaQPd032636@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 20:36:26 +0200 (Fri, 01 Jun 2007)
New Revision: 7496

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
More explicit error message when checkProp() fails

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-06-01 18:23:04 UTC (rev 7495)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-06-01 18:36:26 UTC (rev 7496)
@@ -332,9 +332,15 @@
 ///////////////
 void OnlineLearningModule::checkProp(const TVec<Mat*>& ports_data)
 {
+#ifdef BOUNDCHECK
     for (int i = 0; i < ports_data.length(); i++) {
-        PLCHECK( !ports_data[i] || !ports_data[i]->isEmpty() );
+        if (ports_data[i] && ports_data[i]->isEmpty())
+            PLERROR("In OnlineLearningModule::checkProp - Data for port '%s' "
+                    "of module '%s' (of class '%s') was not properly computed "
+                    "(this may have happened at the end of a fprop or a "
+                    "bpropAccUpdate");
     }
+#endif
 }
 
 //////////////////



From tihocan at mail.berlios.de  Fri Jun  1 20:37:25 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 20:37:25 +0200
Subject: [Plearn-commits] r7497 - trunk/plearn_learners/online
Message-ID: <200706011837.l51IbPYE032716@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 20:37:25 +0200 (Fri, 01 Jun 2007)
New Revision: 7497

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
Fixed mistake in previous commit

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-06-01 18:36:26 UTC (rev 7496)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-06-01 18:37:25 UTC (rev 7497)
@@ -338,7 +338,8 @@
             PLERROR("In OnlineLearningModule::checkProp - Data for port '%s' "
                     "of module '%s' (of class '%s') was not properly computed "
                     "(this may have happened at the end of a fprop or a "
-                    "bpropAccUpdate");
+                    "bpropAccUpdate", getPortName(i).c_str(), name.c_str(),
+                    classname().c_str());
     }
 #endif
 }



From yoshua at mail.berlios.de  Fri Jun  1 20:54:23 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 1 Jun 2007 20:54:23 +0200
Subject: [Plearn-commits] r7498 - trunk/plearn_learners/online
Message-ID: <200706011854.l51IsN8X001138@sheep.berlios.de>

Author: yoshua
Date: 2007-06-01 20:54:23 +0200 (Fri, 01 Jun 2007)
New Revision: 7498

Modified:
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/TanhModule.cc
   trunk/plearn_learners/online/TanhModule.h
Log:
Fixed a PLERROR in CostModule and a PLASSERT in TanhModule


Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2007-06-01 18:37:25 UTC (rev 7497)
+++ trunk/plearn_learners/online/CostModule.cc	2007-06-01 18:54:23 UTC (rev 7498)
@@ -163,6 +163,7 @@
                     "set to false, or outgoing connections from '%s.cost' have"
                     " their 'propagate_gradient' flag set to true).",
                     OnlineLearningModule::name.c_str(), classname().c_str(),
+                    OnlineLearningModule::name.c_str(),
                     OnlineLearningModule::name.c_str());
         }
     }

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2007-06-01 18:37:25 UTC (rev 7497)
+++ trunk/plearn_learners/online/TanhModule.cc	2007-06-01 18:54:23 UTC (rev 7498)
@@ -79,6 +79,37 @@
     }
 }
 
+void TanhModule::fprop(const Mat& inputs, Mat& outputs)
+{
+    int mbs=inputs.length();
+    outputs.resize(mbs,output_size);
+    for (int i=0;i<mbs;i++)
+    {
+        Vec in_i = inputs(i);
+        Vec out_i = outputs(i);
+        fprop(in_i,out_i);
+    }
+}
+
+void TanhModule::bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients, const Mat& output_gradients,
+                             bool accumulate)
+{
+    int mbs=inputs.length();
+    PLASSERT(mbs==outputs.length() && 
+             mbs==output_gradients.length());
+    input_gradients.resize(mbs,input_size);
+    for (int i=0;i<mbs;i++)
+    {
+        Vec in_i = inputs(i);
+        Vec out_i = outputs(i);
+        Vec gin_i = input_gradients(i);
+        Vec gout_i = output_gradients(i);
+        bpropUpdate(in_i,out_i,gin_i,gout_i,
+                    accumulate);
+    }
+}
+
 // Nothing to update
 void TanhModule::bpropUpdate(const Vec& input, const Vec& output,
                              const Vec& output_gradient)

Modified: trunk/plearn_learners/online/TanhModule.h
===================================================================
--- trunk/plearn_learners/online/TanhModule.h	2007-06-01 18:37:25 UTC (rev 7497)
+++ trunk/plearn_learners/online/TanhModule.h	2007-06-01 18:54:23 UTC (rev 7498)
@@ -74,10 +74,15 @@
     // Your other public member functions go here
 
     virtual void fprop(const Vec& input, Vec& output) const;
+    virtual void fprop(const Mat& inputs, Mat& outputs);
 
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              const Vec& output_gradient);
 
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients, const Mat& output_gradients,
+                             bool accumulate=false);
+
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient, const Vec& output_gradient,
                              bool accumulate=false);



From yoshua at mail.berlios.de  Fri Jun  1 21:17:50 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 1 Jun 2007 21:17:50 +0200
Subject: [Plearn-commits] r7499 - trunk/python_modules/plearn/learners/online
Message-ID: <200706011917.l51JHofR002732@sheep.berlios.de>

Author: yoshua
Date: 2007-06-01 21:17:49 +0200 (Fri, 01 Jun 2007)
New Revision: 7499

Modified:
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Fixed mlp construction in online package


Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-06-01 18:54:23 UTC (rev 7498)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-06-01 19:17:49 UTC (rev 7499)
@@ -32,9 +32,9 @@
                                            connection('tanh.output','a2.input'),
                                            connection('a2.output','softmax.input'),
                                            connection('softmax.output','nll.prediction'),
-                                           connection('softmax.output','clerr.prediction'),
-                                           connection('target.output','nll.target'),
-                                           connection('target.output','clerr.target')],
+                                           connection('softmax.output','clerr.prediction',False),
+                                           connection('target.output','nll.target',False),
+                                           connection('target.output','clerr.target',False)],
                             ports = [ ('in', 'a1.input'),
                                       ('target', 'target.input'),
                                       ('out', 'softmax.output'),



From lamblin at mail.berlios.de  Fri Jun  1 22:07:46 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 1 Jun 2007 22:07:46 +0200
Subject: [Plearn-commits] r7500 - trunk/python_modules/plearn/pyext
Message-ID: <200706012007.l51K7kqV006785@sheep.berlios.de>

Author: lamblin
Date: 2007-06-01 22:07:45 +0200 (Fri, 01 Jun 2007)
New Revision: 7500

Modified:
   trunk/python_modules/plearn/pyext/__init__.py
Log:
Add emulation of pyplearn's TMat mechanism


Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2007-06-01 19:17:49 UTC (rev 7499)
+++ trunk/python_modules/plearn/pyext/__init__.py	2007-06-01 20:07:45 UTC (rev 7500)
@@ -31,7 +31,7 @@
 #  library, go to the PLearn Web site at www.plearn.org
 
 from plearn.pyext.plext import *
-   
+
 from plearn.pyplearn.plargs import *
 import cgitb
 cgitb.enable(format='PLearn')
@@ -54,6 +54,30 @@
                 return obj
             return newObj
 
+# Redefines function TMat to emulate pyplearn behaviour
+def TMat( *args ):
+    """Returns a list of lists, each inner list being a row of the TMat"""
+    nargs = len(args)
+    assert nargs in (0, 1, 3)
+
+    # Empty TMat
+    if nargs == 0:
+        return []
+
+    # Argument is already a list of lists
+    elif nargs == 1:
+        return args[0]
+
+    # Argument is a (length, width, content) tuple, content being a list
+    elif nargs == 3:
+        nrows = args[0]
+        ncols = args[1]
+        content = args[2]
+        assert nrows*ncols == len(content)
+
+        return [ content[i*ncols:(i+1)*ncols] for i in range(nrows) ]
+
+
 # Enact the use of plargs: the current behavior is to consider as a plargs
 # any command-line argument that contains a '=' char and to neglect all
 # others
@@ -69,7 +93,7 @@
     print sys.argv[1:]
     print A.T
     print B.T
-        
+
     # python __init__.py T=10 B.T=25
     # ['T=10', 'B.T=25']
     # 10



From tihocan at mail.berlios.de  Fri Jun  1 22:40:47 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Jun 2007 22:40:47 +0200
Subject: [Plearn-commits] r7501 - trunk/plearn_learners/online
Message-ID: <200706012040.l51Kelqv009109@sheep.berlios.de>

Author: tihocan
Date: 2007-06-01 22:40:46 +0200 (Fri, 01 Jun 2007)
New Revision: 7501

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
Fixed typo in error message

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-06-01 20:07:45 UTC (rev 7500)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-06-01 20:40:46 UTC (rev 7501)
@@ -338,7 +338,7 @@
             PLERROR("In OnlineLearningModule::checkProp - Data for port '%s' "
                     "of module '%s' (of class '%s') was not properly computed "
                     "(this may have happened at the end of a fprop or a "
-                    "bpropAccUpdate", getPortName(i).c_str(), name.c_str(),
+                    "bpropAccUpdate)", getPortName(i).c_str(), name.c_str(),
                     classname().c_str());
     }
 #endif



From plearner at mail.berlios.de  Fri Jun  1 22:59:37 2007
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 1 Jun 2007 22:59:37 +0200
Subject: [Plearn-commits] r7502 - trunk/python_modules/plearn/plotting
Message-ID: <200706012059.l51KxbJN011684@sheep.berlios.de>

Author: plearner
Date: 2007-06-01 22:59:37 +0200 (Fri, 01 Jun 2007)
New Revision: 7502

Added:
   trunk/python_modules/plearn/plotting/vtk_utils.py
Log:
Further additions to the plotting utilities (functions to easily create vtk files for common uses)


Added: trunk/python_modules/plearn/plotting/vtk_utils.py
===================================================================
--- trunk/python_modules/plearn/plotting/vtk_utils.py	2007-06-01 20:40:46 UTC (rev 7501)
+++ trunk/python_modules/plearn/plotting/vtk_utils.py	2007-06-01 20:59:37 UTC (rev 7502)
@@ -0,0 +1,89 @@
+# Module containing utility functions to generate .vtk files
+
+
+def create_unstructured_vector_file(vtkfilename, vec_origin, vec_end, vecname="vec", datatitle="unstructured vector"):
+    """Creates a VTK unstructured grid populated with n 3d vectors going
+    from vec_origin to vec_end (both must be of dimension n times 3).
+    The vec_origin will serve as the unstructured grid points,
+    while the the VTK VECTORS point_data (named vecname) will be populated
+    by the difference between vec_end and vec_origin."""
+
+    l = len(vec_origin)
+
+    f = file(vtkfilename,"w")
+    f.write("# vtk DataFile Version 3.1\n")
+    f.write(datatitle+"\n")
+
+    f.write("ASCII\n")
+    f.write("DATASET UNSTRUCTURED_GRID\n")
+    f.write("POINTS %d FLOAT\n"%l)
+    
+    for x,y,z in vec_origin:
+        f.write("%f %f %f\n"%(x,y,z))
+
+    f.write("\n")
+    f.write("POINT_DATA %d \n"%l)
+    f.write("VECTORS "+vecname+" FLOAT\n")
+    for i,(x2,y2,z2) in enumerate(vec_end):
+        x1,y1,z1 = vec_origin[i]
+        dx = float(x2-x1)
+        dy = float(y2-y1)
+        dz = float(z2-z1)
+        f.write("%f %f %f\n"%(dx,dy,dz))
+
+    f.close()
+
+
+def create_vector_file_from_2d_weighted_samples(vtkfilename, x_y_weight, vecname="weighted_samples"):
+    vec_origin = []
+    for x,y,w in x_y_weight:
+        vec_origin.append((x,y,0))
+    vec_end = x_y_weight
+    create_unstructured_vector_file(vtkfilename, vec_origin, vec_end, vecname=vecname, datatitle=vecname)
+
+
+def create_2d_grid_values_file(vtkfilename, x0, y0, dx, dy, zmat, valuename="Scalars", datatitle="2d grid values"):
+    """ zmat -- A 2D array for the x and y points with x varying fastest (column indexes)
+        and y next (row indexes). """
+
+    nx = len(zmat[0])
+    ny = len(zmat)
+
+    l = nx*ny
+
+    f = file(vtkfilename,"w")
+    f.write("# vtk DataFile Version 3.1\n")
+    f.write(datatitle+"\n")
+
+    f.write("ASCII\n")
+    f.write("DATASET STRUCTURED_POINTS\n")
+    f.write("DIMENSIONS %d %d %d\n"%(nx,ny,1))
+    f.write("ORIGIN %f %f %f\n"%(x0,y0,0))
+    f.write("SPACING %f %f %f\n"%(dx,dy,1))
+
+    f.write("\n")
+    f.write("POINT_DATA %d \n"%l)
+
+    f.write("SCALARS "+valuename+" FLOAT\n")
+    f.write("LOOKUP_TABLE default\n")
+
+    for row in zmat:
+        for elem in row:
+            f.write("%f\n"%elem)
+
+    f.close()
+
+def create_2d_grid_values_file_from_regular_x_y_z(vtkfilename, x, y, z):
+
+    if callable(z):
+        zmat = [ [z(xv, yv) for yv in y] for xv in x ]
+    else:
+        zmat = z
+        
+    create_2d_grid_values_file(vtkfilename, x[0], y[0],
+                               x[1]-x[0], y[1]-y[0], zmat)    
+
+def create_2d_grid_values_file_from_regular_xymagnitude(vtkfilename, regular_xymagnitude):
+    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
+    create_2d_grid_values_file_from_regular_x_y_z(x, y, gridvalues)
+



From simonl at mail.berlios.de  Sat Jun  2 00:04:49 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Sat, 2 Jun 2007 00:04:49 +0200
Subject: [Plearn-commits] r7503 - trunk/plearn/var
Message-ID: <200706012204.l51M4nCC016108@sheep.berlios.de>

Author: simonl
Date: 2007-06-02 00:04:49 +0200 (Sat, 02 Jun 2007)
New Revision: 7503

Modified:
   trunk/plearn/var/Variable.cc
Log:
Made build call resize systematically to correctly initialize value and valuedata from matValue
(when reloading saved Var Graph) -- Pascal & Simon



Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2007-06-01 20:59:37 UTC (rev 7502)
+++ trunk/plearn/var/Variable.cc	2007-06-01 22:04:49 UTC (rev 7503)
@@ -256,8 +256,18 @@
     int w_previous = width();
     int l, w;
     recomputeSize(l, w);
-    if (l && w && (l != l_previous || w != w_previous))
-        resize(l, w);
+    if(l==0 || w==0)
+    {
+        l = l_previous;
+        w = w_previous;
+    }
+    // we call resize in all cases, even if we already had matValue correctly sized
+    // the call to resize makes sure that value, valuedata, matGradient, gradient, gradientdata 
+    // are correctly sized and initialized.
+    resize(l, w);
+
+    //if (l && w && (l != l_previous || w != w_previous))
+    //    resize(l, w);
 }
 
 ///////////



From simonl at mail.berlios.de  Sat Jun  2 00:05:35 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Sat, 2 Jun 2007 00:05:35 +0200
Subject: [Plearn-commits] r7504 - in trunk: plearn/var/EXPERIMENTAL
	plearn_learners/generic/EXPERIMENTAL
	python_modules/plearn/plotting python_modules/plearn/var
Message-ID: <200706012205.l51M5Zvm016209@sheep.berlios.de>

Author: simonl
Date: 2007-06-02 00:05:34 +0200 (Sat, 02 Jun 2007)
New Revision: 7504

Added:
   trunk/python_modules/plearn/plotting/netplot.py
Modified:
   trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/var/Var.py
Log:
Cool new stuff for plotting networks and DeepReconstructorNet


Modified: trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-01 22:05:34 UTC (rev 7504)
@@ -56,7 +56,7 @@
 
 ProbabilityPairsVariable::ProbabilityPairsVariable(Variable* input, real min, real max)
     : inherited(input, input->length(), input->width()*2),
-      max(max),min(min)
+      min(min),max(max)
 
 {
     build_();

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-01 22:05:34 UTC (rev 7504)
@@ -42,6 +42,7 @@
 #include <plearn/var/Var_operators.h>
 #include <plearn/vmat/ConcatColumnsVMatrix.h>
 #include <plearn/var/ConcatColumnsVariable.h>
+#include <plearn/io/load_and_save.h>
 
 namespace PLearn {
 using namespace std;
@@ -52,7 +53,8 @@
    "MULTI-LINE \nHELP");
 
 DeepReconstructorNet::DeepReconstructorNet()
-    :good_improvement_rate(-1e10),
+    :supervised_nepochs(0),
+     good_improvement_rate(-1e10),
      fine_tuning_improvement_rate(-1e10),
      minibatch_size(1)
 {
@@ -81,6 +83,10 @@
                   OptionBase::buildoption,
                   "recontruction_costs[k] is the reconstruction cost for layer[k]");
 
+    declareOption(ol, "reconstructed_layers", &DeepReconstructorNet::reconstructed_layers,
+                  OptionBase::buildoption,
+                  "reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]");
+
     declareOption(ol, "reconstruction_optimizer", &DeepReconstructorNet::reconstruction_optimizer,
                   OptionBase::buildoption,
                   "");
@@ -121,6 +127,10 @@
                   OptionBase::buildoption,
                   "");
 
+    declareOption(ol, "supervised_nepochs", &DeepReconstructorNet::supervised_nepochs,
+                  OptionBase::buildoption,
+                  "");
+
     
     
     // Now call the parent class' declareOptions
@@ -150,6 +160,20 @@
                   &DeepReconstructorNet::listParameter,
                   (BodyDoc("Returns a list of the parameters"),
                    RetDoc("Returns a list of the names")));
+
+    declareMethod(rmm,
+                  "computeRepresentations",
+                  &DeepReconstructorNet::computeRepresentations,
+                  (BodyDoc("Compute the representation of each hidden layer"),
+                   ArgDoc("input", "the input"),
+                   RetDoc("The representations")));
+
+    declareMethod(rmm,
+                  "computeReconstructions",
+                  &DeepReconstructorNet::computeReconstructions,
+                  (BodyDoc("Compute the reconstructions of the input of each hidden layer"),
+                   ArgDoc("input", "the input"),
+                   RetDoc("The reconstructions")));
 }
 
 void DeepReconstructorNet::build_()
@@ -222,11 +246,13 @@
     deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
     deepCopyField(reconstruction_costs, copies);
+    deepCopyField(reconstructed_layers, copies);
     deepCopyField(reconstruction_optimizer, copies);
-    deepCopyField(target, copies);
+    varDeepCopyField(target, copies);
     deepCopyField(supervised_costs, copies);
-    deepCopyField(supervised_costvec, copies);
-    deepCopyField(fullcost, copies);    
+    varDeepCopyField(supervised_costvec, copies);
+    deepCopyField(supervised_costs_names, copies);
+    varDeepCopyField(fullcost, copies);    
     deepCopyField(parameters,copies);
     deepCopyField(supervised_optimizer, copies);
     deepCopyField(fine_tuning_optimizer, copies);
@@ -274,34 +300,48 @@
     if (!initTrain())
         return;
 
-    PPath outmatfname = expdir/"outmat";
+    while(stage<nstages)
+    {
+        if(stage<1)
+        {
+            PPath outmatfname = expdir/"outmat";
 
-    int nreconstructions = reconstruction_costs.length();
-    int insize = train_set->inputsize();
-    VMat inputs = train_set.subMatColumns(0,insize);
-    VMat targets = train_set.subMatColumns(insize, train_set->targetsize());
-    VMat dset = inputs;
+            int nreconstructions = reconstruction_costs.length();
+            int insize = train_set->inputsize();
+            VMat inputs = train_set.subMatColumns(0,insize);
+            VMat targets = train_set.subMatColumns(insize, train_set->targetsize());
+            VMat dset = inputs;
 
-    bool must_train_supervised_layer = (training_schedule[training_schedule.length()-2]>0);
+            bool must_train_supervised_layer = (training_schedule[training_schedule.length()-2]>0);
+            
+            PLearn::save(expdir/"learner_0.psave", this);
+            for(int k=0; k<nreconstructions; k++)
+            {
+                trainHiddenLayer(k, dset);
+                PLearn::save(expdir/"learner_"+tostring(k+1)+".psave", this);
+                // 'if' is a hack to avoid precomputing last hidden layer if not needed
+                if(k<nreconstructions-1 ||  must_train_supervised_layer) 
+                { 
+                    int width = layers[k+1].width();
+                    outmat[k] = new FileVMatrix(outmatfname+tostring(k+1)+".pmat",0,width);
+                    outmat[k]->defineSizes(width,0);
+                    buildHiddenLayerOutputs(k, dset, outmat[k]);
+                    dset = outmat[k];
+                }
+            }
 
-    for(int k=0; k<nreconstructions; k++)
-    {
-        trainHiddenLayer(k, dset);
-        // 'if' is a hack to avoid precomputing last hidden layer if not needed
-        if(k<nreconstructions-1 ||  must_train_supervised_layer) 
-        { 
-            int width = layers[k+1].width();
-            outmat[k] = new FileVMatrix(outmatfname+tostring(k+1)+".pmat",0,width);
-            outmat[k]->defineSizes(width,0);
-            buildHiddenLayerOutputs(k, dset, outmat[k]);
-            dset = outmat[k];
+            if(must_train_supervised_layer)
+                trainSupervisedLayer(dset, targets);
         }
+        else
+        {
+            pout << "Fine tuning stage " << stage+1 << endl;
+            prepareForFineTuning();            
+            fineTuningFor1Epoch();
+        }
+        ++stage;
+        train_stats->finalize(); // finalize statistics for this epoch
     }
-
-    if(must_train_supervised_layer)
-        trainSupervisedLayer(dset, targets);
-
-    fineTuning();
     /*
     while(stage<nstages)
     {        
@@ -334,14 +374,8 @@
     }
 }
 
-void DeepReconstructorNet::fineTuning()
+void DeepReconstructorNet::prepareForFineTuning()
 {
-    int l = train_set->length();
-    int nepochs = training_schedule[training_schedule.length()-1];
-    perr << "\n\n*********************************************" << endl;
-    perr << "*** Performing fine tuning for " << nepochs << " epochs " << endl;
-    perr << "*** each epoch has " << l << " examples and " << l/minibatch_size << " optimizer stages (updates)" << endl;
-
     Func f(layers[0]&target, supervised_costvec);
     Var totalcost = sumOf(train_set, f, minibatch_size);
     // displayVarGraph(supervised_costvec);
@@ -349,7 +383,69 @@
 
     VarArray params = totalcost->parents();
     supervised_optimizer->setToOptimize(params, totalcost);
+}
+
+
+TVec<Mat> DeepReconstructorNet::computeRepresentations(Mat input)
+{
+    int nlayers = layers.length();
+    TVec<Mat> representations(nlayers);
+    VarArray proppath = propagationPath(layers[0],layers[nlayers-1]);
+    layers[0]->matValue << input;
+    proppath.fprop();
+    for(int k=0; k<nlayers; k++)
+        representations[k] = layers[k]->matValue.copy();
+    return representations;
+}
+
+void DeepReconstructorNet::reconstructInputFromLayer(int layer)
+{
+    for(int k=layer; k>0; k--)
+    {
+        VarArray proppath = propagationPath(layers[k],reconstructed_layers[k-1]);
+        proppath.fprop();
+        reconstructed_layers[k-1]->matValue >> layers[k-1]->matValue;
+    }
+}
+
+TVec<Mat> DeepReconstructorNet::computeReconstructions(Mat input)
+{
+    int nlayers = layers.length();
+    VarArray proppath = propagationPath(layers[0],layers[nlayers-1]);
+    layers[0]->matValue << input;
+    proppath.fprop();
+
+    TVec<Mat> reconstructions(nlayers-2);
+    for(int k=1; k<nlayers-1; k++)
+    {
+        reconstructInputFromLayer(k);
+        reconstructions[k-1] = layers[0]->matValue.copy();
+    }
+    return reconstructions;
+}
+
+
+void DeepReconstructorNet::fineTuningFor1Epoch()
+{
+    if(train_stats.isNull())
+        train_stats = new VecStatsCollector();
+
+    int l = train_set->length();
     supervised_optimizer->reset();
+    supervised_optimizer->nstages = l/minibatch_size;
+    supervised_optimizer->optimizeN(*train_stats);
+}
+
+void DeepReconstructorNet::fineTuningFullOld()
+{
+    prepareForFineTuning();
+
+    int l = train_set->length();
+    int nepochs = nstages;
+    perr << "\n\n*********************************************" << endl;
+    perr << "*** Performing fine tuning for " << nepochs << " epochs " << endl;
+    perr << "*** each epoch has " << l << " examples and " << l/minibatch_size << " optimizer stages (updates)" << endl;
+
     VecStatsCollector st;
     real prev_mean = -1;
     real relative_improvement = fine_tuning_improvement_rate;
@@ -375,7 +471,7 @@
 {
     int l = inputs->length();
     int last_hidden_layer = layers.length()-2;
-    int nepochs = training_schedule[training_schedule.length()-2];
+    int nepochs = supervised_nepochs;
     perr << "\n\n*********************************************" << endl;
     perr << "*** Training only supervised layer for " << nepochs << " epochs " << endl;
     perr << "*** each epoch has " << l << " examples and " << l/minibatch_size << " optimizer stages (updates)" << endl;

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-01 22:05:34 UTC (rev 7504)
@@ -75,17 +75,21 @@
     //! input layer).
 
     TVec<int> training_schedule;
+    int supervised_nepochs;    
 
     real good_improvement_rate;
     real fine_tuning_improvement_rate;
 
     // layers[0] is the input variable
     // last layer is final output layer
-    TVec<Var> layers;
+    VarArray layers;
 
     // reconstruction_costs[k] is the reconstruction cost for layers[k]
-    TVec<Var> reconstruction_costs;
+    VarArray reconstruction_costs;
 
+    // reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]
+    VarArray reconstructed_layers;
+
     PP<Optimizer> reconstruction_optimizer;
 
 
@@ -175,9 +179,16 @@
     //! Returns a list of the parameters
     TVec<Mat> listParameter();
 
-    void fineTuning();
+    void prepareForFineTuning();
+    void fineTuningFor1Epoch();
+    void fineTuningFullOld();
 
     void trainSupervisedLayer(VMat inputs, VMat targets);
+
+    TVec<Mat> computeRepresentations(Mat input);
+    void reconstructInputFromLayer(int layer);
+    TVec<Mat> computeReconstructions(Mat input);
+
     
 
     // *** SUBCLASS WRITING: ***

Added: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-06-01 22:05:34 UTC (rev 7504)
@@ -0,0 +1,445 @@
+from pylab import *
+
+
+#################
+### constants ###
+#################
+
+defaultColorMap = cm.jet
+defaultInterpolation = 'nearest'
+
+########################
+### plotting methods ###
+########################
+
+def setPlotParams(titre='', color_bar=True, disable_ticks=False):
+    title(titre)
+    if color_bar:
+        colorbar()
+    if disable_ticks:
+        xticks([],[])
+        yticks([],[])
+
+def findMinMax(matrix):
+    M = matrix
+    mi = M[0][0]
+    ma = M[0][0]
+    for row in M:
+        for el in row:
+            if el > ma:
+                ma = el
+            if el < mi:
+                mi = el
+    return mi,ma
+
+def customColorBar(min, max, (x,y,width,height) = (0.9,0.1,0.1,0.8), nb_ticks = 50., color_map = defaultColorMap):
+    axes((x, y, width,height))
+    cbarh = arange(min, max,  (max-min)/50.)
+    cbar = vecToVerticalMatrix(cbarh)
+    cbarh_str = []
+    for el in cbarh:
+        cbarh_str.append(str(el)[0:5])
+    yticks(arange(len(cbarh)),cbarh_str)
+    xticks([],[])                              
+    imshow(cbar, cmap = color_map, vmin=min, vmax=max)
+
+
+
+
+def plotLayer1Old(W, M, width):
+    '''plots each row of W and M as if they where matrix (width is the width of one of these matrix)'''
+
+    #some calculations for plotting
+
+    nbPlotStyles = 3 # (normal and two times 1 sur 2)
+    subPlotHeight = nbPlotStyles*2
+    subPlotWidth = max(len(W), len(M))
+    axesHeight = 1./float(subPlotHeight)
+    axesWidth = 1./float(subPlotWidth)
+
+    colorMap = defaultColorMap
+    
+    matrices = [W,M]
+    names = ["W", "M"]
+
+    #THE plotting
+
+    for i, matrix in enumerate(matrices):
+        for j, row in enumerate(matrix):
+            
+            #normal
+        
+            #subplot(subPlotHeight, subPlotWidth,(i%2)*subPlotWidth + j + 1)
+            axes((j*axesWidth, i*axesHeight, axesWidth, axesHeight))
+            imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap)
+            setPlotParams(names[i%2] + "_" + str(i) + "_" + str(j), False, True)
+            
+            #1 sur 2 -
+        
+            #subplot(subPlotHeight, subPlotWidth, (2+i%2)*subPlotWidth + j + 1)
+            axes((j*axesWidth, (i+2)*axesHeight, axesWidth, axesHeight))
+            imshow(rowToMatrix(toMinusRow(row),width), interpolation="nearest", cmap = colorMap)
+            setPlotParams(names[i%2] + "_" + str(i) + "_" + str(j), False, True)
+            
+            #1 sur 2 +
+
+            #subplot(subPlotHeight, subPlotWidth, (4+i%2)*subPlotWidth + j + 1)
+            axes((j*axesWidth, (i+4)*axesHeight, axesWidth, axesHeight))
+            imshow(rowToMatrix(toPlusRow(row),width), interpolation="nearest", cmap = colorMap)
+            setPlotParams(names[i%2] + "_" + str(i) + "_" + str(j), False, True)
+
+def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows_before = None):
+
+    
+    #some calculations for plotting
+
+    #hack
+    if length == -1:
+        length = len(M)
+
+    mWidth = float(len(M[0]))
+    mHeight = float(len(M))
+    sbi = space_between_images
+    plotHeight = mHeight/mWidth*plotWidth
+    cbw = .01 # color bar width
+
+    colorMap = defaultColorMap
+
+    mi,ma = findMinMax(M)
+    
+    ma = max(abs(mi),abs(ma))
+    mi = -ma
+            
+
+    #THE plotting
+    
+    x,y = sbi,sbi
+    
+    for i in arange(start,length):
+    
+        #normal
+        row = M[i]
+        
+        axes((x, y, plotWidth, plotHeight))
+        imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap, vmin = mi, vmax = ma)
+        setPlotParams('row_' + str(i), False, True)
+
+        x = x + plotWidth + sbi
+        if x + plotWidth +cbw > 1:
+            x = sbi
+            y = y + plotHeight + sbi
+        if y + plotHeight > 1:
+            # images that follows would be out of the figure...
+            break
+
+    #custom color bar
+    customColorBar(mi,ma,(1.-cbw-sbi, sbi, sbi, 1.-2.*cbw))    
+        
+
+        #1 sur 2 -
+        
+        #subplot(subPlotHeight, subPlotWidth, (2+i%2)*subPlotWidth + j + 1)
+        #axes((i*axesWidth, 2*axesHeight, axesWidth, axesHeight))
+        #imshow(rowToMatrix(toMinusRow(row),width), interpolation="nearest", cmap = colorMap)
+        #setPlotParams(str(i), False, True)
+        
+        #1 sur 2 +
+        
+        #subplot(subPlotHeight, subPlotWidth, (4+i%2)*subPlotWidth + i + 1)
+        #axes((i*axesWidth, 4*axesHeight, axesWidth, axesHeight))
+        #imshow(rowToMatrix(toPlusRow(row),width), interpolation="nearest", cmap = colorMap)
+        # setPlotParams(str(i), False, True)
+            
+
+def plotCharOLD(char, layers=[], space_between_layers = 5):
+    '''plots a caracter and hidden layers
+    '''  
+    #some 'plotting' consts
+    nbLayers = len(layers)
+    print 'nbLayers', nbLayers
+    totalLayersWidth = 0
+    for layer in layers:
+        totalLayersWidth += len(layer[0]) + space_between_layers
+    totalLayersWidth += space_between_layers
+    print 'totalLayersWidth', totalLayersWidth
+    
+    unit = .9/totalLayersWidth
+    print 'unit', unit
+    sbl = space_between_layers*unit
+    print 'sbl', sbl
+    plotCharWidth = .099
+    plotCharHeight = plotCharWidth
+    #plotLayerWidth = unit
+    plotCharBottom = (1.-plotCharHeight)/2.
+    
+    #plot of the char
+    axes((sbl,          
+          plotCharBottom,
+          plotCharWidth,
+          plotCharHeight))
+    imshow(char, interpolation="nearest", cmap = defaultColorMap)
+    setPlotParams("", True, True)
+    print 'char ok'
+    #plots of the layers
+
+
+    x,y=sbl,1.-2*len(layers[0])
+    axes((x,y,len(layers[0][0]), len(layers[0])))
+    imshow(layer, interpolation = "nearest", cmap = defaultColorMap)
+    print 'layer[0] ok'
+    
+    k=1
+    x+=len(layers[0][0])
+    
+    while k < nbLayers:
+
+       
+        
+        plotLayerHeight = unit*len(layers[k])
+        plotLayerWidth = unit*len(layers[k][0])
+        if plotLayerHeight > 1:
+            plotLayerHeight = 1.-2.*sbl
+            
+        plotLayerBottom = (1.-plotLayerHeight)/2.
+
+        print 'k',k
+        print 'x',x
+        print 'plotLayerBottom', plotLayerBottom
+        print 'plotLayerWidth', plotLayer
+        axes((x,plotLayerBottom, plotLayerWidth, plotLayerHeight))
+        imshow(layers[k], interpolation="nearest", cmap = defaultColorMap)
+        setPlotParams("",True,True)
+        x += plotLayerWidth
+        k += 1
+
+def plotMatrices(matrices, same_color_bar = False, space_between_matrices = 5):
+    '''plot matrices from left to right
+    '''
+    colorMap = cm.gray
+    nbMatrices = len(matrices)
+    print 'plotting ' + str(nbMatrices) + ' matrices'
+
+    totalWidth = 0
+    maxHeight = 0
+    
+    for matrix in matrices:
+        if len(matrix) > maxHeight:
+            maxHeight = len(matrix)
+        totalWidth += len(matrix[0])
+
+
+    unit = min(1./((nbMatrices+1)*space_between_matrices + totalWidth), 1./(maxHeight-2.*space_between_matrices))
+    sbm = space_between_matrices*unit
+
+    x=sbm
+    for matrix in matrices:
+        
+        h = len(matrix)*unit
+        w = len(matrix[0])*unit
+        if h>1 :
+            h = 1.-2*sbm
+        bottom = (1.-h)/2.
+
+        axes(( x,bottom, w,h))
+        imshow(matrix, interpolation = 'nearest', cmap = colorMap)
+        colorbar()
+        x += w+sbm
+
+
+def truncate_imshow(mat, max_height_or_width = 200, width_height_ratio = 1, space_between_submatrices=5):
+
+    matWidth = float(len(mat[0]))
+    matHeight = float(len(mat))
+    
+    mhow = max_height_or_width
+
+    s = float(space_between_submatrices)
+    r = float(width_height_ratio)
+        
+    if (matWidth > mhow and matHeight < mhow) or (matWidth < mhow and matHeight > mhow):
+        
+        if matWidth > matHeight:
+            n = (s + sqrt(s*s + 4.*(matHeight+s)*matWidth/r))/(2.*(matHeight+s))
+        else :
+            n = (s + sqrt(s*s + 4.*(matWidth+s)*matHeight/r))/(2.*(matWidth_s))        
+        newMat = truncateMatrix(mat, n)
+    else:
+        n=1.
+        newMat = [mat]
+    
+    
+    height = len(newMat[0])
+    width = len(newMat[0][0])
+    
+    #on met les submatrices de bas en haut
+    if width>height :
+        
+        #somes plotting constants
+        unit = min(1./(n*height + (n+1)*s), 1./(width+2*s))
+        plotHeight = height*unit
+        plotWidth = width*unit
+        sbs = s*unit        
+        x,y=sbs,sbs
+    
+        for i,matrix in enumerate(newMat):
+            axes((x,y,plotWidth, plotHeight))
+            imshow(matrix, interpolation = defaultInterpolation, cmap = defaultColorMap)
+            setPlotParams("",False,True)
+            y = y + plotHeight + sbs
+    
+    #on met les matrices de gauche a droite     
+    else :
+
+        
+        #somes plotting constants
+        unit = min(1./(n*width + (n+1)*s), 1./(height+2*s))
+        plotHeight = height*unit
+        plotWidth = width*unit
+        sbs = s*unit
+        x,y=sbs,sbs
+    
+        for i,matrix in enumerate(newMat):
+            axes((x,y,plotWidth, plotHeight))            
+            imshow(matrix, interpolation = defaultInterpolation, cmap = defaultColorMap)
+            setPlotParams("",False,True)
+            x = x + plotWidth + sbs
+
+    #custom colorBar
+    mi,ma = findMinMax(mat)
+    
+    ma = max(abs(mi),abs(ma))
+    mi = -ma
+    customColorBar(mi,ma)
+    
+
+
+################################################
+### somes methods for matrix transformations ###
+################################################
+
+def toPlusRow(row):
+    '''[1,2,3,4,5,6]->[2,4,6]
+    '''
+    row2 = []
+    for i in arange(1,len(row),2):
+        row2.append(row[i])
+    return row2
+
+def toMinusRow(row):
+    '''[1,2,3,4,5,6]->[1,3,5]
+    '''
+    row2 = []
+        
+    for i in arange(0,len(row),2):
+        row2.append(row[i])
+    return row2
+
+def rowToMatrix(row, width, validate_size = True, fill_value = 0.):
+    '''change a row [1,2,3,4,5,6] into a matrix [[1,2],[3,4],[5,6]] if width = 2
+       or [1,2,3,4,5,6] -> [[1,2,3,4],[5,6,fill_value,fill_value]] if width = 4 and validate_size = False
+    '''
+    if len(row)%width != 0 and validate_size:
+        raise Exception, "dimensions does not fit ( " + str(width) + " does not divide " + str(len(row)) + ")"
+            
+    m = []
+    k = 0
+    a=-1
+    for i,e in enumerate(row):        
+        if(i%width == 0):        
+            a=a+1
+            m.append([])
+        m[a].append(e)
+
+    # we finish by filling fill the last elements of the last row
+    if len(m)>=2:
+        for i in arange(len(m[-1]),len(m[-2])):
+            m[-1].append(fill_value)
+    
+    return m
+
+def vecToVerticalMatrix(vec):
+    '''ex : [1,2,3,4,5] --> [[1],[2],[3],[4],[5]]
+    '''
+    mat = []
+    for elem in vec:
+        mat.append([elem])
+    return mat
+
+def truncateMatrixOld(mat, maxHeight=10, maxWidth=10):
+    
+    width = len(mat[0])
+    height = len(mat)
+
+    #si notre matrice est trop haute (seulement), on va la couper en morceaux
+    if width > maxWidth and height <= maxHeight:        
+        truncMat = []
+        a = -1
+        for indCol in arange(len(mat[0])):
+            if indCol % maxWidth == 0:               
+                truncMat.append([])
+                a=a+1
+                for l in arange(height):#on ajoute des lignes
+                    truncMat[a].append([])                
+            for l in arange(height):#on remplie les lignes
+                truncMat[a][l].append(mat[l][indCol])            
+            
+        return truncMat
+    #si notre matrice est trop large   (seulement)    
+    elif height > maxHeight and width <= maxWidth:
+        truncMat = [[]]
+        a,l=0,0
+        for ligne in mat:
+            if(l >= maxHeight):
+                truncMat.append([])
+                a=a+1
+                l=0
+            truncMat[a].append(ligne)
+            l=l+1
+            
+        return truncMat
+    elif height > maxHeight and width > maxWidth:
+        print 'matrice trop grosse... rien a faire..'
+        truncMat = [mat]
+    else:
+        truncMat = [mat]
+
+    return truncMat
+
+def truncateMatrix(mat, n=10.):
+        
+    width = len(mat[0])
+    height = len(mat)
+
+    #si notre matrice est trop haute (seulement), on va la couper en morceaux
+    if width > height:
+        maxWidth = int(width/n)
+        truncMat = []
+        a = -1
+        for indCol in arange(len(mat[0])):
+            if indCol % maxWidth == 0:               
+                truncMat.append([])
+                a=a+1
+                for l in arange(height):#on ajoute des lignes
+                    truncMat[a].append([])                
+            for l in arange(height):#on remplie les lignes
+                truncMat[a][l].append(mat[l][indCol])            
+            
+        return truncMat
+    #si notre matrice est trop large   (seulement)    
+    else: # height >= width
+        maxHeight = int(height/n)
+        truncMat = [[]]
+        a,l=0,0
+        for ligne in mat:
+            if(l >= maxHeight):
+                truncMat.append([])
+                a=a+1
+                l=0
+            truncMat[a].append(ligne)
+            l=l+1
+            
+    return truncMat
+
+      
+        
+    

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/python_modules/plearn/var/Var.py	2007-06-01 22:05:34 UTC (rev 7504)
@@ -54,6 +54,9 @@
     def _serial_number(self):
         return self.v._serial_number()
 
+    def exp(self):
+        return Var(pl.ExpVariable(input=self.v))
+
     def sigmoid(self):
         return Var(pl.SigmoidVariable(input=self.v))
 
@@ -157,19 +160,22 @@
     and an optional parameter b(1,ow)
     Then output = sigmoid(input.W^T + b)
     
-    Returns a triple (hidden, reconstruciton_cost)"""
+    Returns a triple (hidden, reconstruciton_cost, reconstructed_input)"""
     W = Var(ow,iw,"uniform", -1./sqrt(iw), 1./sqrt(iw), varname=basename+'_W')
     
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')        
         hidden = input.matrixProduct_A_Bt(W).add(b).sigmoid()        
         br = Var(1,iw,"fill",0, varname=basename+'_br')
-        cost = hidden.matrixProduct(W).add(br).negCrossEntropySigmoid(input)
+        reconstr_activation = hidden.matrixProduct(W).add(br)
     else:
         hidden = input.matrixProduct_A_Bt(W).sigmoid()
-        cost = hidden.matrixProduct(W).negCrossEntropySigmoid(input)
-    return hidden, cost
+        reconstr_activation = hidden.matrixProduct(W)
 
+    reconstructed_input = reconstr_activation.sigmoid()
+    cost = reconstr_activation.negCrossEntropySigmoid(input)
+    return hidden, cost, reconstructed_input
+
 def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, basename=""):
     """iw is the input's width
     igs is the input's group size
@@ -179,8 +185,10 @@
     M = Var(ow/ogs, iw, "uniform", -1./sqrt(iw), 1./sqrt(iw), False, varname=basename+"_M")
     W = Var(ogs, iw, "uniform", -1./sqrt(iw), 1./sqrt(iw), False, varname=basename+"_W")
     hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
-    cost = -hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs).dot(input)            
-    return hidden, cost
+    log_reconstructed = hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs)
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxDoubleProductRLayer(input, iw, igs, ow, ogs, basename=""):
     """iw is the input's width
@@ -191,21 +199,28 @@
     hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
     Mr = Var(iw/igs, ow, "uniform", -1./ow, 1./ow, False, varname=basename+"_Mr")
     Wr = Var(igs, ow, "uniform", -1./ow, 1./ow, False, varname=basename+"_Wr")
-    cost = -hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs).dot(input)       
-    return hidden, cost
+    # TODO: a repenser s'il faut un transpose ou non
+    log_reconstructed = hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs)
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, basename=""):
     W = Var(ow, iw, "uniform", -1./iw, 1./iw, varname=basename+'_W')
     hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
-    cost = -hidden.matrixProduct(W).multiLogSoftMax(igs).dot(input)
-    return hidden, cost
+    log_reconstructed = hidden.matrixProduct(W).multiLogSoftMax(igs)
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs):
     W = Var(ow, iw, "uniform", -1./iw, 1./iw, varname=basename+'_W')
     hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
     Wr = Var(ow, iw, "uniform", -1./ow, 1./ow, varname=basename+'_Wr')
-    cost = -hidden.matrixProduct(Wr).multiLogSoftMax(igs).dot(input)
-    return hidden, cost
+    log_reconstructed = hidden.matrixProduct(Wr).multiLogSoftMax(igs)
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
 
 #################################
 # These build supervised layers



From lamblin at mail.berlios.de  Sat Jun  2 00:48:54 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 2 Jun 2007 00:48:54 +0200
Subject: [Plearn-commits] r7505 - trunk/python_modules/plearn/pyext
Message-ID: <200706012248.l51Mms5W020430@sheep.berlios.de>

Author: lamblin
Date: 2007-06-02 00:48:53 +0200 (Sat, 02 Jun 2007)
New Revision: 7505

Modified:
   trunk/python_modules/plearn/pyext/
Log:
Ignore binary files



Property changes on: trunk/python_modules/plearn/pyext
___________________________________________________________________
Name: svn:ignore
   + *.pyc
*.so




From lamblin at mail.berlios.de  Sat Jun  2 00:50:33 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 2 Jun 2007 00:50:33 +0200
Subject: [Plearn-commits] r7506 - trunk/plearn/python
Message-ID: <200706012250.l51MoXqg022197@sheep.berlios.de>

Author: lamblin
Date: 2007-06-02 00:50:32 +0200 (Sat, 02 Jun 2007)
New Revision: 7506

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Fix bug when pyext.so was compiled in -float mode


Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-06-01 22:48:53 UTC (rev 7505)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-06-01 22:50:32 UTC (rev 7506)
@@ -695,6 +695,11 @@
     return PyFloat_FromDouble(x);
 }
 
+PyObject* ConvertToPyObject<float>::newPyObject(const float& x)
+{
+    return PyFloat_FromDouble(double(x));
+}
+
 PyObject* ConvertToPyObject<char*>::newPyObject(const char* x)
 {
     return PyString_FromString(x);

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-06-01 22:48:53 UTC (rev 7505)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-06-01 22:50:32 UTC (rev 7506)
@@ -282,6 +282,9 @@
 template<> struct ConvertToPyObject<double>
 { static PyObject* newPyObject(const double& x); };
 
+template<> struct ConvertToPyObject<float>
+{ static PyObject* newPyObject(const float& x); };
+
 template<> struct ConvertToPyObject<char*>
 { static PyObject* newPyObject(const char* x); };
 



From saintmlx at mail.berlios.de  Sat Jun  2 01:30:58 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Sat, 2 Jun 2007 01:30:58 +0200
Subject: [Plearn-commits] r7507 - in trunk: plearn/base plearn/python
	python_modules/plearn/pyplearn python_modules/plearn/utilities
Message-ID: <200706012330.l51NUwOq011095@sheep.berlios.de>

Author: saintmlx
Date: 2007-06-02 01:30:57 +0200 (Sat, 02 Jun 2007)
New Revision: 7507

Modified:
   trunk/plearn/base/Option.h
   trunk/plearn/python/PythonCodeSnippet.cc
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/python_modules/plearn/pyplearn/plargs.py
   trunk/python_modules/plearn/utilities/toolkit.py
Log:
- several fixes to plearn/python bridge, a few additions



Modified: trunk/plearn/base/Option.h
===================================================================
--- trunk/plearn/base/Option.h	2007-06-01 22:50:32 UTC (rev 7506)
+++ trunk/plearn/base/Option.h	2007-06-01 23:30:57 UTC (rev 7507)
@@ -126,13 +126,16 @@
 #ifdef PL_PYTHON_VERSION 
     virtual PythonObjectWrapper getAsPythonObject(Object* o) const 
     { 
-        return PythonObjectWrapper(*(OptionType*)getAsVoidPtr(o),
+        return PythonObjectWrapper(ConvertToPyObject<OptionType>::
+                                   newPyObject(*(OptionType*)getAsVoidPtr(o)),
                                    PythonObjectWrapper::transfer_ownership); 
+        
     }
 
     virtual PythonObjectWrapper getAsPythonObject(const Object* o) const 
     { 
-        return PythonObjectWrapper(*(OptionType*)getAsVoidPtr(o),
+        return PythonObjectWrapper(ConvertToPyObject<OptionType>::
+                                   newPyObject(*(OptionType*)getAsVoidPtr(o)),
                                    PythonObjectWrapper::transfer_ownership); 
 
     }

Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-06-01 22:50:32 UTC (rev 7506)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-06-01 23:30:57 UTC (rev 7507)
@@ -250,12 +250,14 @@
 
     PyObject* pFunc= 0;
     bool instance_method= false;
-    char* fn= new char[strlen(function_name)+1];
-    strcpy(fn, function_name);
     if(!m_instance.isNull())
-        pFunc= PyObject_GetAttrString(m_instance.getPyObject(),
-                                      fn);
-    delete[] fn;
+    {
+        char* fn= new char[strlen(function_name)+1];
+        strcpy(fn, function_name);
+        if(PyObject_HasAttrString(m_instance.getPyObject(), fn))
+            pFunc= PyObject_GetAttrString(m_instance.getPyObject(), fn);
+        delete[] fn;
+    }
     if(pFunc) 
         instance_method= true;
     else
@@ -276,12 +278,14 @@
 
     PyObject* pFunc= 0;
     bool instance_method= false;
-    char* fn= new char[strlen(function_name)+1];
-    strcpy(fn, function_name);
     if(!m_instance.isNull())
-        pFunc= PyObject_GetAttrString(m_instance.getPyObject(),
-                                      fn);
-    delete[] fn;
+    {
+        char* fn= new char[strlen(function_name)+1];
+        strcpy(fn, function_name);
+        if(PyObject_HasAttrString(m_instance.getPyObject(), fn))
+            pFunc= PyObject_GetAttrString(m_instance.getPyObject(), fn);
+        delete[] fn;
+    }
     if(pFunc) 
         instance_method= true;
     else
@@ -327,12 +331,14 @@
 
     PyObject* pFunc= 0;
     bool instance_method= false;
-    char* fn= new char[strlen(function_name)+1];
-    strcpy(fn, function_name);
     if(!m_instance.isNull())
-        pFunc= PyObject_GetAttrString(m_instance.getPyObject(),
-                                      fn);
-    delete[] fn;
+    {
+        char* fn= new char[strlen(function_name)+1];
+        strcpy(fn, function_name);
+        if(PyObject_HasAttrString(m_instance.getPyObject(), fn))
+            pFunc= PyObject_GetAttrString(m_instance.getPyObject(), fn);
+        delete[] fn;
+    }
     if(pFunc) 
         instance_method= true;
     else
@@ -723,6 +729,7 @@
         }
         else {
             PyErr_Print();
+            PyErr_Clear();
             PLERROR("PythonCodeSnippet: encountered Python exception.\n%s", 
                     extramsg.c_str());
         }

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-06-01 22:50:32 UTC (rev 7506)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-06-01 23:30:57 UTC (rev 7507)
@@ -173,7 +173,15 @@
 Object* ConvertFromPyObject<Object*>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT(pyobj);
+    if(pyobj == Py_None)
+        return 0;
 
+    if(!PyObject_HasAttrString(pyobj, "_cptr"))
+    {
+        PLERROR("in ConvertFromPyObject<Object*>::convert : "
+                "python object has no attribute '_cptr'");
+        return 0;
+    }
     PyObject* cptr= PyObject_GetAttrString(pyobj, "_cptr");
 
     if (! PyCObject_Check(cptr))
@@ -710,7 +718,12 @@
     return PyString_FromString(x.c_str());
 }
 
+PyObject* ConvertToPyObject<PPath>::newPyObject(const PPath& x)
+{
+    return PyString_FromString(x.c_str());
+}
 
+
 PyObject* ConvertToPyObject<Vec>::newPyObject(const Vec& data)
 {
     PyArrayObject* pyarr = 0;

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-06-01 22:50:32 UTC (rev 7506)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-06-01 23:30:57 UTC (rev 7507)
@@ -293,6 +293,9 @@
     
 template<> struct ConvertToPyObject<string>
 { static PyObject* newPyObject(const string& x); };
+
+template<> struct ConvertToPyObject<PPath>
+{ static PyObject* newPyObject(const PPath& x); };
   
 //! PLearn Vec: use numarray
 template<> struct ConvertToPyObject<Vec>
@@ -603,6 +606,10 @@
 template <class T>
 PP<T> ConvertFromPyObject<PP<T> >::convert(PyObject* pyobj, bool print_traceback)
 {
+    PLASSERT( pyobj );
+    if(pyobj == Py_None)
+        return 0;
+
     PPointable* o= 0;
     if(PyCObject_Check(pyobj))
         o= ConvertFromPyObject<PPointable*>::convert(pyobj, print_traceback);
@@ -758,6 +765,8 @@
 template <class T>
 PyObject* ConvertToPyObject<PP<T> >::newPyObject(const PP<T>& data)
 {
+    if(data == 0)
+        return PythonObjectWrapper::newPyObject();
     Object* o= dynamic_cast<Object*>(static_cast<T*>(data));
     if(o)
         return ConvertToPyObject<Object*>::newPyObject(o);

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-01 22:50:32 UTC (rev 7506)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-01 23:30:57 UTC (rev 7507)
@@ -890,7 +890,7 @@
     getHolder = staticmethod(getHolder)
 
     def getPlopt(cls, optname):
-        return object.__getattribute__(cls, optname)
+        return super(cls, cls).__getattribute__(cls, optname)
     getPlopt = classmethod(getPlopt)
 
     def inherit(namespace):
@@ -921,9 +921,10 @@
                 # Do not use plopt.optdict: the documentation, choices and other
                 # property would be lost...
                 for opt in plopt.iterator(namespace):
-                    inh_opt = copy.deepcopy(opt)
-                    inh_opt.set( opt.get() )
-                    dic[inh_opt.getName()] = inh_opt
+                    if not opt.getName() in dic:
+                        inh_opt = copy.deepcopy(opt)
+                        inh_opt.set( opt.get() )
+                        dic[inh_opt.getName()] = inh_opt
                     
                 #OLD: optdict = dict([ (
                 #OLD:     opt.getName(), opt) for opt in plopt.iterator(namespace) ])

Modified: trunk/python_modules/plearn/utilities/toolkit.py
===================================================================
--- trunk/python_modules/plearn/utilities/toolkit.py	2007-06-01 22:50:32 UTC (rev 7506)
+++ trunk/python_modules/plearn/utilities/toolkit.py	2007-06-01 23:30:57 UTC (rev 7507)
@@ -192,9 +192,12 @@
     elif docform==1:
         ## Determine a logical starting point that skips blank lines after
         ## the first line of documentation
-        for i in range(1,len(lines)):
+        #for i in range(1,len(lines)):
+        i= 1
+        while i < len(lines):
             if lines[i].strip() != "":
                 break
+            i+= 1
         lines = lines[i:]
     elif docform==2:
         pass



From plearner at mail.berlios.de  Sat Jun  2 21:10:40 2007
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 2 Jun 2007 21:10:40 +0200
Subject: [Plearn-commits] r7508 - trunk/python_modules/plearn/var
Message-ID: <200706021910.l52JAeZM026772@sheep.berlios.de>

Author: plearner
Date: 2007-06-02 21:10:40 +0200 (Sat, 02 Jun 2007)
New Revision: 7508

Added:
   trunk/python_modules/plearn/var/__init__.py
Log:
Added forgotten __init__.py in plearn/var


Added: trunk/python_modules/plearn/var/__init__.py
===================================================================



From larocheh at mail.berlios.de  Sun Jun  3 03:17:39 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sun, 3 Jun 2007 03:17:39 +0200
Subject: [Plearn-commits] r7509 - trunk/plearn_learners_experimental
Message-ID: <200706030117.l531HdC2002360@sheep.berlios.de>

Author: larocheh
Date: 2007-06-03 03:17:38 +0200 (Sun, 03 Jun 2007)
New Revision: 7509

Modified:
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h
Log:
Added option to initialize hidden weight matrix using RBM training.


Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-02 19:10:40 UTC (rev 7508)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-03 01:17:38 UTC (rev 7509)
@@ -81,6 +81,7 @@
 #include <plearn/vmat/ConcatColumnsVMatrix.h>
 #include <plearn/math/random.h>
 #include <plearn/math/plapack.h>
+#include <plearn_learners/online/RBMMatrixConnection.h>
 
 namespace PLearn {
 using namespace std;
@@ -111,6 +112,18 @@
     declareOption(ol, "optimizer", &LinearInductiveTransferClassifier::optimizer, 
                   OptionBase::buildoption,
                   "Optimizer of the discriminative classifier");
+    declareOption(ol, "rbm_nstages", 
+                  &LinearInductiveTransferClassifier::rbm_nstages, 
+                  OptionBase::buildoption,
+                  "Number of RBM training to initialize hidden layer weights");
+    declareOption(ol, "visible_layer",
+                  &LinearInductiveTransferClassifier::visible_layer, 
+                  OptionBase::buildoption,
+                  "Visible layer of the RBM");
+    declareOption(ol, "hidden_layer",
+                  &LinearInductiveTransferClassifier::hidden_layer, 
+                  OptionBase::buildoption,
+                  "Hidden layer of the RBM");
     declareOption(ol, "batch_size", &LinearInductiveTransferClassifier::batch_size,
                   OptionBase::buildoption, 
                   "How many samples to use to estimate the avergage gradient before updating the weights\n"
@@ -301,6 +314,7 @@
             weights =vconcat(-product(exp(s),square(weights)) & weights); // Making sure that the scaling factor is going to be positive
             output = affine_transform(tanh(affine_transform(input,W)), weights);
         }
+
         else
             PLERROR("In LinearInductiveTransferClassifier::build_(): model_type %s is not valid", model_type.c_str());
 
@@ -539,6 +553,8 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(class_reps, copies);
     deepCopyField(optimizer, copies);
+    deepCopyField(visible_layer, copies);
+    deepCopyField(hidden_layer, copies);
     deepCopyField(params, copies);
     deepCopyField(paramsvalues, copies);
     deepCopyField(invars, copies);
@@ -630,6 +646,76 @@
     if(f.isNull()) // Net has not been properly built yet (because build was called before the learner had a proper training set)
         build();
     
+    if(stage == 0 && nstages > 0 && model_type == "nnet_discriminative_1_vs_all")
+    {
+        Vec input, target;
+        real example_weight;
+        real recons = 0;
+        RBMMatrixConnection* c = new RBMMatrixConnection();
+        PP<RBMMatrixConnection> layer_matrix_connections = c;
+        PP<RBMConnection> layer_connections = c;
+        hidden_layer->size = nhidden;
+        visible_layer->size = inputsize_;
+        layer_connections->up_size = inputsize_;
+        layer_connections->down_size = nhidden;
+        
+        hidden_layer->random_gen = random_gen;
+        visible_layer->random_gen = random_gen;
+        layer_connections->random_gen = random_gen;
+        
+        hidden_layer->build();
+        visible_layer->build();
+        layer_connections->build();
+        
+        Vec pos_visible,pos_hidden,neg_visible,neg_hidden;
+        pos_visible.resize(inputsize_);
+        pos_hidden.resize(nhidden);
+        neg_visible.resize(inputsize_);
+        neg_hidden.resize(nhidden);
+
+        for(int i = 0; i < rbm_nstages; i++)
+        {
+            for(int i=0; i<train_set->length(); i++)
+            {
+                train_set->getExample(i,input,target,example_weight);
+
+                pos_visible = input;
+                layer_connections->setAsUpInput( input );
+                hidden_layer->getAllActivations( layer_connections );
+                hidden_layer->computeExpectation();
+                hidden_layer->generateSample();
+                pos_hidden << hidden_layer->expectation;            
+
+                layer_connections->setAsDownInput( hidden_layer->sample );
+                visible_layer->getAllActivations( layer_connections );
+                visible_layer->computeExpectation();
+                visible_layer->generateSample();
+                neg_visible = visible_layer->sample;
+
+                layer_connections->setAsUpInput( visible_layer->sample );
+                hidden_layer->getAllActivations( layer_connections );
+                hidden_layer->computeExpectation();
+                neg_hidden = hidden_layer->expectation;
+
+                // Compute reconstruction error
+                layer_connections->setAsDownInput( pos_hidden );
+                visible_layer->getAllActivations( layer_connections );
+                visible_layer->computeExpectation();
+                recons += visible_layer->fpropNLL(input);
+                
+                // Update
+                visible_layer->update(pos_visible, neg_visible);
+                hidden_layer->update(pos_hidden, neg_hidden);
+                layer_connections->update(pos_hidden, pos_visible,
+                                          neg_hidden, neg_visible);
+            }
+            if(verbosity > 2)
+                cout << "Reconstruction error = " << recons << endl;
+            recons = 0;
+        }
+        W->matValue.subMat(1,0,inputsize_,nhidden) << layer_matrix_connections->weights;
+    }
+
     if(model_type == "discriminative" || model_type == "discriminative_1_vs_all" || model_type == "generative_0-1" || model_type == "nnet_discriminative_1_vs_all")
     {
         // number of samples seen by optimizer before each optimizer update
@@ -955,7 +1041,6 @@
         random_gen->fill_random_normal(weights->value, 0, delta);
     else
         random_gen->fill_random_uniform(weights->value, -delta, delta);
-
     if(zero_first_row)
         weights->matValue(0).clear();
 }

Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h	2007-06-02 19:10:40 UTC (rev 7508)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h	2007-06-03 01:17:38 UTC (rev 7509)
@@ -47,6 +47,7 @@
 #include <plearn_learners/generic/PLearner.h>
 #include <plearn/opt/Optimizer.h>
 #include <plearn/var/VarArray.h>
+#include <plearn_learners/online/RBMLayer.h>
 
 namespace PLearn {
 
@@ -91,6 +92,12 @@
     real sigma_min;
     //! Number of hidden units for neural network
     int nhidden;
+    //! Number of RBM training to initialize hidden layer weights
+    int rbm_nstages;
+    //! Visible layer of the RBM
+    PP<RBMLayer> visible_layer;
+    //! Hidden layer of the RBM
+    PP<RBMLayer> hidden_layer;
 
 public:
     //#####  Public Member Functions  #########################################



From larocheh at mail.berlios.de  Sun Jun  3 03:36:31 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sun, 3 Jun 2007 03:36:31 +0200
Subject: [Plearn-commits] r7510 - trunk/plearn_learners_experimental
Message-ID: <200706030136.l531aVgH003421@sheep.berlios.de>

Author: larocheh
Date: 2007-06-03 03:36:30 +0200 (Sun, 03 Jun 2007)
New Revision: 7510

Modified:
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
Log:
Changed index of for loop...


Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-03 01:17:38 UTC (rev 7509)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-03 01:36:30 UTC (rev 7510)
@@ -675,9 +675,9 @@
 
         for(int i = 0; i < rbm_nstages; i++)
         {
-            for(int i=0; i<train_set->length(); i++)
+            for(int j=0; j<train_set->length(); j++)
             {
-                train_set->getExample(i,input,target,example_weight);
+                train_set->getExample(j,input,target,example_weight);
 
                 pos_visible = input;
                 layer_connections->setAsUpInput( input );



From larocheh at mail.berlios.de  Sun Jun  3 03:45:56 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sun, 3 Jun 2007 03:45:56 +0200
Subject: [Plearn-commits] r7511 - trunk/plearn_learners_experimental
Message-ID: <200706030145.l531juP6003940@sheep.berlios.de>

Author: larocheh
Date: 2007-06-03 03:45:55 +0200 (Sun, 03 Jun 2007)
New Revision: 7511

Modified:
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
Log:
Forgot to initialize hidden layer bias...


Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-03 01:36:30 UTC (rev 7510)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-03 01:45:55 UTC (rev 7511)
@@ -714,6 +714,7 @@
             recons = 0;
         }
         W->matValue.subMat(1,0,inputsize_,nhidden) << layer_matrix_connections->weights;
+        W->matValue(0) << hidden_layer->bias;
     }
 
     if(model_type == "discriminative" || model_type == "discriminative_1_vs_all" || model_type == "generative_0-1" || model_type == "nnet_discriminative_1_vs_all")



From larocheh at mail.berlios.de  Mon Jun  4 00:49:11 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 4 Jun 2007 00:49:11 +0200
Subject: [Plearn-commits] r7512 - trunk/plearn_learners_experimental
Message-ID: <200706032249.l53MnBb8022804@sheep.berlios.de>

Author: larocheh
Date: 2007-06-04 00:49:07 +0200 (Mon, 04 Jun 2007)
New Revision: 7512

Modified:
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
Log:
Rearranged the deep copy items in the order they appear in the .h file. 


Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-03 01:45:55 UTC (rev 7511)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-03 22:49:07 UTC (rev 7512)
@@ -555,12 +555,29 @@
     deepCopyField(optimizer, copies);
     deepCopyField(visible_layer, copies);
     deepCopyField(hidden_layer, copies);
+
+    varDeepCopyField(input, copies);
+    varDeepCopyField(output, copies);
+    varDeepCopyField(sup_output, copies);
+    varDeepCopyField(new_output, copies);
+    varDeepCopyField(target, copies);
+    varDeepCopyField(sup_target, copies);
+    varDeepCopyField(new_target, copies);
+    varDeepCopyField(sampleweight, copies);
+    varDeepCopyField(A, copies);
+    varDeepCopyField(s, copies);
+    varDeepCopyField(class_reps_var, copies);
+
+    deepCopyField(costs, copies);
+    deepCopyField(new_costs, copies);
     deepCopyField(params, copies);
     deepCopyField(paramsvalues, copies);
+    deepCopyField(penalties, copies);
+
+    varDeepCopyField(training_cost, copies);
+    varDeepCopyField(test_costs, copies);
+
     deepCopyField(invars, copies);
-    deepCopyField(costs, copies);
-    deepCopyField(new_costs, copies);
-    deepCopyField(penalties, copies);
     deepCopyField(seen_targets, copies);
     deepCopyField(unseen_targets, copies);
 
@@ -570,27 +587,12 @@
     deepCopyField(sup_test_costf, copies);
     deepCopyField(sup_output_and_target_to_cost, copies);
 
-    varDeepCopyField(A, copies);
-    varDeepCopyField(s, copies);
-    varDeepCopyField(class_reps_var, copies);
-
     varDeepCopyField(W, copies);
     //deepCopyField(As, copies);
     //deepCopyField(Ws, copies);
     //deepCopyField(s_hids, copies);
     //deepCopyField(hidden_neurons, copies);
 
-    varDeepCopyField(input, copies);
-    varDeepCopyField(output, copies);
-    varDeepCopyField(sup_output, copies);
-    varDeepCopyField(new_output, copies);
-    varDeepCopyField(target, copies);
-    varDeepCopyField(sup_target, copies);
-    varDeepCopyField(new_target, copies);
-    varDeepCopyField(sampleweight, copies);
-    varDeepCopyField(training_cost, copies);
-    varDeepCopyField(test_costs, copies);
-
     //PLERROR("LinearInductiveTransferClassifier::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 



From tihocan at mail.berlios.de  Mon Jun  4 15:55:39 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 4 Jun 2007 15:55:39 +0200
Subject: [Plearn-commits] r7513 - in
	trunk/plearn_learners/online/test/ModuleTester: .
	.pytest/PL_ModuleTester_RBM/expected_results
Message-ID: <200706041355.l54Dtd5E018240@sheep.berlios.de>

Author: tihocan
Date: 2007-06-04 15:55:39 +0200 (Mon, 04 Jun 2007)
New Revision: 7513

Modified:
   trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log
   trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
Log:
Added test of gradient w.r.t. external weights

Modified: trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log	2007-06-03 22:49:07 UTC (rev 7512)
+++ trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log	2007-06-04 13:55:39 UTC (rev 7513)
@@ -12,3 +12,5 @@
 All tests passed successfully on module RBMModule
  WARNING: RBMMatrixConnection: cannot forget() without random_gen
 All tests passed successfully on module RBMModule
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+All tests passed successfully on module RBMModule

Modified: trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-06-03 22:49:07 UTC (rev 7512)
+++ trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-06-04 13:55:39 UTC (rev 7513)
@@ -3,7 +3,7 @@
 from plearn.pyplearn import pl
 
 def rbm(cd_learning_rate, grad_learning_rate, visible_size, hidden_size, name,
-        compute_cd = False):
+        compute_cd = False, standard_cd_weights_grad = True):
     # Return a standard binomial RBM.
     return pl.RBMModule(
             name = name,
@@ -12,6 +12,7 @@
             grad_learning_rate = grad_learning_rate,
             visible_layer = pl.RBMBinomialLayer(size = visible_size),
             hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
+            standard_cd_weights_grad = standard_cd_weights_grad,
             connection = pl.RBMMatrixConnection(
                 down_size = visible_size,
                 up_size = hidden_size))
@@ -43,9 +44,15 @@
          "out_nograd":[ "hidden_activations.state", "negative_phase_visible_samples.state",
                         "negative_phase_hidden_expectations.state", "negative_phase_hidden_activations.state" ]}
 
+conf_weights_cd = \
+        {"in_grad":[ "weights" ],
+         "in_nograd":[ "visible" ],
+         "out_grad":[ "contrastive_divergence",  ],
+         "out_nograd":[ "hidden.state", "hidden_activations.state", "negative_phase_visible_samples.state",
+                        "negative_phase_hidden_expectations.state", "negative_phase_hidden_activations.state" ]}
 
+
 testers = [
-
     # Test a simple RBM that does not update itself (learning rates are set
     # to 0).
     pl.ModuleTester(
@@ -84,8 +91,16 @@
             module = rbm(1e-2, 1e-3, 10, 20, 'rbm', True),
             configurations = [ conf_bias_both ],
             min_out_grad = 1,
-            max_out_grad = 1)
+            max_out_grad = 1),
 
+    # Test of gradient of contrastive divergence w.r.t. weights.
+    pl.ModuleTester(
+            default_length = 5,
+            module = rbm(1e-3, 0, 10, 20, 'rbm', True, False),
+            configurations = [ conf_weights_cd ],
+            min_out_grad = 1,
+            max_out_grad = 1),
+
     ]
 
 def main():



From tihocan at mail.berlios.de  Mon Jun  4 16:03:34 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 4 Jun 2007 16:03:34 +0200
Subject: [Plearn-commits] r7514 - in trunk/plearn_learners/online: .
	test/ModuleTester
Message-ID: <200706041403.l54E3YXV019006@sheep.berlios.de>

Author: tihocan
Date: 2007-06-04 16:03:33 +0200 (Mon, 04 Jun 2007)
New Revision: 7514

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
   trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
Log:
Changed default computation of CD gradient w.r.t. external hidden biases: the default behavior is now to use the standard CD gradient instead of the 'true' one. This is to ensure consistent behavior between external and hidden biases.

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-04 13:55:39 UTC (rev 7513)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-04 14:03:33 UTC (rev 7514)
@@ -101,8 +101,9 @@
     Gibbs_step(0),
     log_partition_function(0),
     partition_function_is_stale(true),
+    standard_cd_grad(true),
+    standard_cd_bias_grad(true),
     standard_cd_weights_grad(true),
-    standard_cd_grad(true),
     hidden_bias(NULL),
     weights(NULL),
     hidden_act(NULL),
@@ -155,6 +156,15 @@
         "and connections. Currently, this option works only with layers of\n"
         "the type 'RBMBinomialLayer', connected by a 'RBMMatrixConnection'.");
 
+    declareOption(ol, "standard_cd_bias_grad",
+                  &RBMModule::standard_cd_bias_grad,
+                  OptionBase::buildoption,
+        "This option is only used when biases of the hidden layer are given\n"
+        "through the 'hidden_bias' port. When this is the case, the gradient\n"
+        "of contrastive divergence w.r.t. these biases is either computed:\n"
+        "- by the usual formula if 'standard_cd_bias_grad' is true\n"
+        "- by the true gradient if 'standard_cd_bias_grad' is false.");
+
     declareOption(ol, "standard_cd_weights_grad",
                   &RBMModule::standard_cd_weights_grad,
                   OptionBase::buildoption,
@@ -319,6 +329,10 @@
                     "update will be performed AND no contrastive divergence "
                     "gradient will be propagated.");
     }
+
+    PLCHECK_MSG(!(!standard_cd_grad && standard_cd_bias_grad), "You cannot "
+            "compute the standard CD gradient w.r.t. external hidden bias and "
+            "use the 'true' CD gradient w.r.t. internal hidden bias");
 }
 
 ///////////
@@ -1229,6 +1243,7 @@
                     real p_i_n = (*negative_phase_hidden_expectations)(k, i);
                     real a_i_n = (*negative_phase_hidden_activations)(k, i);
                     (*hidden_bias_g)(k, i) +=
+                        standard_cd_bias_grad ? p_i_p - p_i_n :
                         - p_i_p * (1 - p_i_p) * a_i_p + p_i_p    // Pos. phase
                      -( - p_i_n * (1 - p_i_n) * a_i_n + p_i_n ); // Neg. phase
 

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-06-04 13:55:39 UTC (rev 7513)
+++ trunk/plearn_learners/online/RBMModule.h	2007-06-04 14:03:33 UTC (rev 7514)
@@ -90,9 +90,10 @@
     real log_partition_function;
     bool partition_function_is_stale;
 
+    bool standard_cd_grad;
+    bool standard_cd_bias_grad;
     bool standard_cd_weights_grad;
     
-    bool standard_cd_grad;
 
 public:
     //#####  Public Member Functions  #########################################

Modified: trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-06-04 13:55:39 UTC (rev 7513)
+++ trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-06-04 14:03:33 UTC (rev 7514)
@@ -3,7 +3,7 @@
 from plearn.pyplearn import pl
 
 def rbm(cd_learning_rate, grad_learning_rate, visible_size, hidden_size, name,
-        compute_cd = False, standard_cd_weights_grad = True):
+        compute_cd = False, standard_cd_weights_grad = True, standard_cd_bias_grad = True):
     # Return a standard binomial RBM.
     return pl.RBMModule(
             name = name,
@@ -13,6 +13,7 @@
             visible_layer = pl.RBMBinomialLayer(size = visible_size),
             hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
             standard_cd_weights_grad = standard_cd_weights_grad,
+            standard_cd_bias_grad = standard_cd_bias_grad,
             connection = pl.RBMMatrixConnection(
                 down_size = visible_size,
                 up_size = hidden_size))
@@ -76,19 +77,19 @@
 
     # Test of gradient of contrastive divergence w.r.t. bias.
     pl.ModuleTester(
-            module = rbm(1e-3, 0, 10, 20, 'rbm', True),
+            module = rbm(1e-3, 0, 10, 20, 'rbm', True, True, False),
             configurations = [ conf_bias_cd ],
             min_out_grad = 1,
             max_out_grad = 1),
 
     # Test of backpropagation gradient w.r.t. bias.
     pl.ModuleTester(
-            module = rbm(0, 1e-3, 10, 20, 'rbm'),
+            module = rbm(0, 1e-3, 10, 20, 'rbm', False, True, False),
             configurations = [ conf_bias_grad ]),
 
     # Test of both backpropagation and contrastive divergence gradients w.r.t. bias.
     pl.ModuleTester(
-            module = rbm(1e-2, 1e-3, 10, 20, 'rbm', True),
+            module = rbm(1e-2, 1e-3, 10, 20, 'rbm', True, True, False),
             configurations = [ conf_bias_both ],
             min_out_grad = 1,
             max_out_grad = 1),



From larocheh at mail.berlios.de  Mon Jun  4 16:37:19 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 4 Jun 2007 16:37:19 +0200
Subject: [Plearn-commits] r7515 - trunk/plearn_learners_experimental
Message-ID: <200706041437.l54EbJI6022293@sheep.berlios.de>

Author: larocheh
Date: 2007-06-04 16:37:18 +0200 (Mon, 04 Jun 2007)
New Revision: 7515

Modified:
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h
Log:
Debugged the rbm learning initialization option...


Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-04 14:03:33 UTC (rev 7514)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-06-04 14:37:18 UTC (rev 7515)
@@ -102,7 +102,9 @@
       use_bias_in_weights_prediction(false),
       multi_target_classifier(false),
       sigma_min(1e-5),
-      nhidden(-1)
+      nhidden(-1),
+      rbm_nstages(0),
+      rbm_learning_rate(0.01)
 {
     random_gen = new PRandom();
 }
@@ -116,6 +118,10 @@
                   &LinearInductiveTransferClassifier::rbm_nstages, 
                   OptionBase::buildoption,
                   "Number of RBM training to initialize hidden layer weights");
+    declareOption(ol, "rbm_learning_rate", 
+                  &LinearInductiveTransferClassifier::rbm_learning_rate, 
+                  OptionBase::buildoption,
+                  "Learning rate for the RBM");
     declareOption(ol, "visible_layer",
                   &LinearInductiveTransferClassifier::visible_layer, 
                   OptionBase::buildoption,
@@ -312,7 +318,10 @@
             //                   hidden_neurons[i]);
             //}
             weights =vconcat(-product(exp(s),square(weights)) & weights); // Making sure that the scaling factor is going to be positive
-            output = affine_transform(tanh(affine_transform(input,W)), weights);
+            if(rbm_nstages>0)
+                output = affine_transform(tanh(affine_transform(input,W)), weights);
+            else
+                output = affine_transform(sigmoid(affine_transform(input,W)), weights);
         }
 
         else
@@ -648,8 +657,15 @@
     if(f.isNull()) // Net has not been properly built yet (because build was called before the learner had a proper training set)
         build();
     
-    if(stage == 0 && nstages > 0 && model_type == "nnet_discriminative_1_vs_all")
+    if(rbm_nstages>0 && stage == 0 && nstages > 0 && model_type == "nnet_discriminative_1_vs_all")
     {
+        if(!visible_layer)
+            PLERROR("In LinearInductiveTransferClassifier::train(): "
+                    "visible_layer must be provided.");
+        if(!hidden_layer)
+            PLERROR("In LinearInductiveTransferClassifier::train(): "
+                    "hidden_layer must be provided.");
+
         Vec input, target;
         real example_weight;
         real recons = 0;
@@ -664,6 +680,11 @@
         hidden_layer->random_gen = random_gen;
         visible_layer->random_gen = random_gen;
         layer_connections->random_gen = random_gen;
+
+        visible_layer->setLearningRate(rbm_learning_rate);
+        hidden_layer->setLearningRate(rbm_learning_rate);
+        layer_connections->setLearningRate(rbm_learning_rate);
+
         
         hidden_layer->build();
         visible_layer->build();
@@ -712,7 +733,7 @@
                                           neg_hidden, neg_visible);
             }
             if(verbosity > 2)
-                cout << "Reconstruction error = " << recons << endl;
+                cout << "Reconstruction error = " << recons/train_set->length() << endl;
             recons = 0;
         }
         W->matValue.subMat(1,0,inputsize_,nhidden) << layer_matrix_connections->weights;

Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h	2007-06-04 14:03:33 UTC (rev 7514)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h	2007-06-04 14:37:18 UTC (rev 7515)
@@ -94,6 +94,8 @@
     int nhidden;
     //! Number of RBM training to initialize hidden layer weights
     int rbm_nstages;
+    //! Learning rate for the RBM
+    real rbm_learning_rate;
     //! Visible layer of the RBM
     PP<RBMLayer> visible_layer;
     //! Hidden layer of the RBM



From yoshua at mail.berlios.de  Mon Jun  4 16:38:29 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 4 Jun 2007 16:38:29 +0200
Subject: [Plearn-commits] r7516 - in trunk/python_modules/plearn/learners:
	autolr online
Message-ID: <200706041438.l54EcTp5022427@sheep.berlios.de>

Author: yoshua
Date: 2007-06-04 16:38:29 +0200 (Mon, 04 Jun 2007)
New Revision: 7516

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Added lrate option in rbm (online package)


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 14:37:18 UTC (rev 7515)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 14:38:29 UTC (rev 7516)
@@ -266,7 +266,7 @@
         selected_costnames=costnames
     cost_indices = [costnames.index(name) for name in selected_costnames]
     n_tests = len(testsets)
-    n_costs = len(costnames)
+    n_costs = len(cost_indices)
     if schedules:
         stages = schedules[:,0]
         learning_rates = schedules[:,1:]
@@ -360,6 +360,17 @@
             previous_best_err = best_err
             if logfile:
                 print >>logfile,"BEST to now is candidate ",best_active," with err=",best_err
+                print >>logfile, "stage\tl.rate\t",
+                for cost_index in cost_indices:
+                    costname = costnames[cost_index]
+                    print >>logfile, costname+"\t",
+                print >>logfile
+                for row in all_results[best_active][0:t+1,:]:
+                    for val in row:
+                        print >>logfile,val,"\t",
+                    print >>logfile
+                print >>logfile
+                
         else:
             if logfile:
                 print >>logfile, "THE BEST ACTIVE HAS GOTTEN WORSE!!!!"

Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-06-04 14:37:18 UTC (rev 7515)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-06-04 14:38:29 UTC (rev 7516)
@@ -11,16 +11,18 @@
         return condp
 
 def supervised_classification_mlp(name,input_size,n_hidden,n_classes,
-                                  L1wd=0,L2wd=0):
+                                  L1wd=0,L2wd=0,lrate=0.01):
     return pl.NetworkModule(name=name,
                             modules=[ pl.GradNNetLayerModule(name='a1',input_size=input_size,
                                                              output_size=n_hidden,
+                                                             start_learning_rate=lrate,
                                                              L2_penalty_factor=L2wd,
                                                              L1_penalty_factor=L1wd),
                                       pl.TanhModule(name='tanh',input_size=n_hidden,
                                                     output_size=n_hidden),
                                       pl.GradNNetLayerModule(name='a2',input_size=n_hidden,
                                                              output_size=n_classes,
+                                                             start_learning_rate=lrate,
                                                              L2_penalty_factor=L2wd,
                                                              L1_penalty_factor=L1wd),
                                       pl.SoftmaxModule(name='softmax',input_size=n_classes,



From tihocan at mail.berlios.de  Mon Jun  4 16:42:15 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 4 Jun 2007 16:42:15 +0200
Subject: [Plearn-commits] r7517 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706041442.l54EgFp3022914@sheep.berlios.de>

Author: tihocan
Date: 2007-06-04 16:42:15 +0200 (Mon, 04 Jun 2007)
New Revision: 7517

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Added comments

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 14:38:29 UTC (rev 7516)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 14:42:15 UTC (rev 7517)
@@ -210,21 +210,21 @@
 
 def train_adapting_lr(learner,
                       trainset,testsets,expdir,
-                      lr_options,
-                      optimized_group=0,
-                      schedules=None,
-                      nstages=None,
-                      epoch=None,
-                      initial_lr=0.1,
-                      nskip=2,
-                      cost_to_select_best=0,
+                      lr_options, # List of lists of options (one list/group of options with a given schedule)
+                      optimized_group=0, # Group of options that is actually optimized
+                      schedules=None, # Matrix of schedules (number of columns = 1 (stage) + number of groups)
+                      nstages=None, # Used to construct default schedule if 'schedules' is None
+                      epoch=None,   # ""
+                      initial_lr=0.1, # Starting value for group being optimized
+                      nskip=2,   # Number of epochs after which we add/remove candidates
+                      cost_to_select_best=0, # Index of cost being optimized
                       return_best_model=False, # o/w return final model
                       save_best=False, # for paranoids: save best model each time an improvement is found
                       selected_costnames = False,
                       min_epochs_to_delete = 2,
-                      lr_steps=exp(log(10)/2),
+                      lr_steps=exp(log(10)/2), # Scaling coefficient when modifying learning rates
                       logfile=False,
-                      keep_lr=2):
+                      keep_lr=2): # Learning rate interval for heuristic
 
     min_epochs_to_delete = max(1,min_epochs_to_delete) # although 1 is probably too small
 
@@ -406,6 +406,11 @@
     schedules[:,1+optimized_group]=all_results[best_active][:,1]
     if logfile and best_err < all_last_err[best_active]:
         print >>logfile, "WARNING: best performing model would have stopped early at stage ",best_early_stop
-    return (final_model,schedules,all_results[best_active], 
-            all_results,all_last_err,all_start,best_early_stop)
+    return (final_model,   # Learner
+            schedules,     # Matrix of schedules (including the one that was optimized)
+            all_results[best_active], # Error curve (matrix) for best model
+            all_results, # List of all error curve matrices
+            all_last_err, # List of all last errors recorded for each candidate (not necessarily at last epoch)
+            all_start,  # Epoch index where each candidate was created (not necessarily the first epoch)
+            best_early_stop) # Timestep (in stages) at which early-stopping should have happened (i.e. stage of best error found)
 



From tihocan at mail.berlios.de  Mon Jun  4 17:09:41 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 4 Jun 2007 17:09:41 +0200
Subject: [Plearn-commits] r7518 - in trunk/python_modules/plearn/learners: .
	autolr online
Message-ID: <200706041509.l54F9fmt025772@sheep.berlios.de>

Author: tihocan
Date: 2007-06-04 17:09:41 +0200 (Mon, 04 Jun 2007)
New Revision: 7518

Modified:
   trunk/python_modules/plearn/learners/
   trunk/python_modules/plearn/learners/autolr/
   trunk/python_modules/plearn/learners/online/
Log:
Ignoring .pyc files


Property changes on: trunk/python_modules/plearn/learners
___________________________________________________________________
Name: svn:ignore
   + *.pyc



Property changes on: trunk/python_modules/plearn/learners/autolr
___________________________________________________________________
Name: svn:ignore
   + *.pyc



Property changes on: trunk/python_modules/plearn/learners/online
___________________________________________________________________
Name: svn:ignore
   + *.pyc




From plearner at mail.berlios.de  Mon Jun  4 18:33:04 2007
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Mon, 4 Jun 2007 18:33:04 +0200
Subject: [Plearn-commits] r7519 - in trunk: plearn/var/EXPERIMENTAL
	plearn_learners/generic/EXPERIMENTAL python_modules/plearn/var
Message-ID: <200706041633.l54GX4VQ009110@sheep.berlios.de>

Author: plearner
Date: 2007-06-04 18:33:02 +0200 (Mon, 04 Jun 2007)
New Revision: 7519

Modified:
   trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc
   trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
   trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/var/Var.py
Log:
Fixed deepCopy in new variables and modified options in DeepReconstructorNet to allow specifying a minimum number of stages.


Modified: trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc	2007-06-04 16:33:02 UTC (rev 7519)
@@ -142,9 +142,6 @@
 
     // ### If you want to deepCopy a Var field:
     // varDeepCopyField(somevariable, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("DoubleProductVariable::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
 void DoubleProductVariable::declareOptions(OptionList& ol)

Modified: trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-04 16:33:02 UTC (rev 7519)
@@ -133,9 +133,6 @@
 
     // ### If you want to deepCopy a Var field:
     // varDeepCopyField(somevariable, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("ProbabilityPairsVariable::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
 void ProbabilityPairsVariable::declareOptions(OptionList& ol)

Modified: trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc	2007-06-04 16:33:02 UTC (rev 7519)
@@ -145,9 +145,6 @@
 
     // ### If you want to deepCopy a Var field:
     // varDeepCopyField(somevariable, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("TransposedDoubleProductVariable::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
 void TransposedDoubleProductVariable::declareOptions(OptionList& ol)

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-04 16:33:02 UTC (rev 7519)
@@ -53,9 +53,8 @@
    "MULTI-LINE \nHELP");
 
 DeepReconstructorNet::DeepReconstructorNet()
-    :supervised_nepochs(0),
-     good_improvement_rate(-1e10),
-     fine_tuning_improvement_rate(-1e10),
+    :supervised_nepochs(pair<int,int>(0,0)),
+     supervised_min_improvement_rate(-10000),
      minibatch_size(1)
 {
 }
@@ -71,10 +70,26 @@
     // ### (OptionBase::buildoption | OptionBase::nosave)
 
     
-    declareOption(ol, "training_schedule", &DeepReconstructorNet::training_schedule,
+    declareOption(ol, "unsupervised_nepochs", &DeepReconstructorNet::unsupervised_nepochs,
                   OptionBase::buildoption,
-                  "training_schedule[k] conatins the number of epochs for the training of the hidden layer taking layer k as input (k=0 corresponds to input layer).");
+                  "unsupervised_nepochs[k] contains a pair of integers giving the minimum and\n"
+                  "maximum number of epochs for the training of layer k+1 (taking layer k"
+                  "as input). Thus k=0 corresponds to the training of the first hidden layer.");
 
+    declareOption(ol, "unsupervised_min_improvement_rate", &DeepReconstructorNet::unsupervised_min_improvement_rate,
+                  OptionBase::buildoption,
+                  "unsupervised_min_improvement_rate[k] should contain the minimum required relative improvement rate\n"
+                  "for the training of layer k+1 (taking input from layer k.)");
+
+    declareOption(ol, "supervised_nepochs", &DeepReconstructorNet::supervised_nepochs,
+                  OptionBase::buildoption,
+                  "");
+
+    declareOption(ol, "supervised_min_improvement_rate", &DeepReconstructorNet::supervised_min_improvement_rate,
+                  OptionBase::buildoption,
+                  "supervised_min_improvement_rate contains the minimum required relative improvement rate\n"
+                  "for the training of the supervised layer.");
+
     declareOption(ol, "layers", &DeepReconstructorNet::layers,
                   OptionBase::buildoption,
                   "layers[0] is the input variable ; last layer is final output layer");
@@ -119,20 +134,7 @@
                   OptionBase::buildoption,
                   "");
 
-    declareOption(ol, "good_improvement_rate", &DeepReconstructorNet::good_improvement_rate,
-                  OptionBase::buildoption,
-                  "");
-
-    declareOption(ol, "fine_tuning_improvement_rate", &DeepReconstructorNet::fine_tuning_improvement_rate,
-                  OptionBase::buildoption,
-                  "");
-
-    declareOption(ol, "supervised_nepochs", &DeepReconstructorNet::supervised_nepochs,
-                  OptionBase::buildoption,
-                  "");
-
     
-    
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -243,7 +245,11 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(training_schedule, copies);
+    deepCopyField(unsupervised_nepochs, copies);
+    deepCopyField(unsupervised_min_improvement_rate, copies);
+    deepCopyField(supervised_nepochs, copies);
+    deepCopyField(supervised_min_improvement_rate, copies);
+
     deepCopyField(layers, copies);
     deepCopyField(reconstruction_costs, copies);
     deepCopyField(reconstructed_layers, copies);
@@ -312,13 +318,13 @@
             VMat targets = train_set.subMatColumns(insize, train_set->targetsize());
             VMat dset = inputs;
 
-            bool must_train_supervised_layer = (training_schedule[training_schedule.length()-2]>0);
+            bool must_train_supervised_layer = supervised_nepochs.second>0;
             
-            PLearn::save(expdir/"learner_0.psave", this);
+            PLearn::save(expdir/"learner.psave", this);
             for(int k=0; k<nreconstructions; k++)
             {
                 trainHiddenLayer(k, dset);
-                PLearn::save(expdir/"learner_"+tostring(k+1)+".psave", this);
+                PLearn::save(expdir/"learner.psave", this);
                 // 'if' is a hack to avoid precomputing last hidden layer if not needed
                 if(k<nreconstructions-1 ||  must_train_supervised_layer) 
                 { 
@@ -331,12 +337,19 @@
             }
 
             if(must_train_supervised_layer)
+            {
                 trainSupervisedLayer(dset, targets);
+                PLearn::save(expdir/"learner.psave", this);
+            }
+            perr << "\n\n*********************************************" << endl;
+            perr << "****      Now performing fine tuning     ****" << endl;
+            perr << "********************************************* \n" << endl;
+
         }
         else
         {
-            pout << "Fine tuning stage " << stage+1 << endl;
-            prepareForFineTuning();            
+            perr << "+++ Fine tuning stage " << stage+1 << " **" << endl;
+            prepareForFineTuning();
             fineTuningFor1Epoch();
         }
         ++stage;
@@ -436,6 +449,7 @@
     supervised_optimizer->optimizeN(*train_stats);
 }
 
+/*
 void DeepReconstructorNet::fineTuningFullOld()
 {
     prepareForFineTuning();
@@ -443,7 +457,7 @@
     int l = train_set->length();
     int nepochs = nstages;
     perr << "\n\n*********************************************" << endl;
-    perr << "*** Performing fine tuning for " << nepochs << " epochs " << endl;
+    perr << "*** Performing fine tuning for max. " << nepochs << " epochs " << endl;
     perr << "*** each epoch has " << l << " examples and " << l/minibatch_size << " optimizer stages (updates)" << endl;
 
     VecStatsCollector st;
@@ -465,15 +479,17 @@
         prev_mean = m;
     }
 }
+*/
 
-
 void DeepReconstructorNet::trainSupervisedLayer(VMat inputs, VMat targets)
 {
     int l = inputs->length();
+    pair<int,int> nepochs = supervised_nepochs;
+    real min_improvement = supervised_min_improvement_rate;
+
     int last_hidden_layer = layers.length()-2;
-    int nepochs = supervised_nepochs;
     perr << "\n\n*********************************************" << endl;
-    perr << "*** Training only supervised layer for " << nepochs << " epochs " << endl;
+    perr << "*** Training only supervised layer for max. " << nepochs.second << " epochs " << endl;
     perr << "*** each epoch has " << l << " examples and " << l/minibatch_size << " optimizer stages (updates)" << endl;
 
     Func f(layers[last_hidden_layer]&target, supervised_costvec);
@@ -489,8 +505,8 @@
     supervised_optimizer->reset();
     VecStatsCollector st;
     real prev_mean = -1;
-    real relative_improvement = good_improvement_rate;
-    for(int n=0; n<nepochs && relative_improvement >= good_improvement_rate; n++)
+    real relative_improvement = 1000;
+    for(int n=0; n<nepochs.first || (n<nepochs.second && relative_improvement >= min_improvement); n++)
     {
         st.forget();
         supervised_optimizer->nstages = l/minibatch_size;
@@ -501,8 +517,8 @@
         perr << "mean error: " << m << " +- " << s.stderror() << endl;
         if(prev_mean>0)
         {
-            relative_improvement = ((prev_mean-m)/prev_mean)*100;
-            perr << "Relative improvement: " << relative_improvement << " %"<< endl;
+            relative_improvement = (prev_mean-m)/prev_mean;
+            perr << "Relative improvement: " << relative_improvement*100 << " %"<< endl;
         }
         prev_mean = m;
         //displayVarGraph(supervised_costvec, true);
@@ -514,9 +530,12 @@
 void DeepReconstructorNet::trainHiddenLayer(int which_input_layer, VMat inputs)
 {
     int l = inputs->length();
-    int nepochs = training_schedule[which_input_layer];
+    pair<int,int> nepochs = unsupervised_nepochs[which_input_layer];
+    real min_improvement = -10000;
+    if(unsupervised_min_improvement_rate.length()!=0)
+        min_improvement = unsupervised_min_improvement_rate[which_input_layer];
     perr << "\n\n*********************************************" << endl;
-    perr << "*** Training layer " << which_input_layer+1 << " for " << nepochs << " epochs " << endl;
+    perr << "*** Training (unsupervised) layer " << which_input_layer+1 << " for max. " << nepochs.second << " epochs " << endl;
     perr << "*** each epoch has " << l << " examples and " << l/minibatch_size << " optimizer stages (updates)" << endl;
     Func f(layers[which_input_layer], reconstruction_costs[which_input_layer]);
     //displayVarGraph(reconstruction_costs[which_input_layer]);
@@ -525,22 +544,39 @@
     VarArray params = totalcost->parents();
     reconstruction_optimizer->setToOptimize(params, totalcost);
     reconstruction_optimizer->reset();
+
+    TVec<string> colnames(4);
+    colnames[0] = "nepochs";
+    colnames[1] = "reconstr_cost";
+    colnames[2] = "stderror";
+    colnames[3] = "relative_improvement";
+    VMat training_curve = new FileVMatrix(expdir/"training_costs_layer_"+tostring(which_input_layer+1)+".pmat",0,colnames);
+    Vec costrow(4);
+
     VecStatsCollector st;
     real prev_mean = -1;
-    real relative_improvement = good_improvement_rate;
-    for(int n=0; n<nepochs && relative_improvement >= good_improvement_rate; n++)
+    real relative_improvement = 1000;
+    for(int n=0; n<nepochs.first || (n<nepochs.second && relative_improvement >= min_improvement); n++)
     {
         st.forget();
         reconstruction_optimizer->nstages = l/minibatch_size;
         reconstruction_optimizer->optimizeN(st);
         const StatsCollector& s = st.getStats(0);
         real m = s.mean();
+        real er = s.stderror();
         perr << "Epoch " << n+1 << " mean error: " << m << " +- " << s.stderror() << endl;
         if(prev_mean>0)
         {
-            relative_improvement = ((prev_mean-m)/prev_mean)*100;
-            perr << "Relative improvement: " << relative_improvement << " %"<< endl;
+            relative_improvement = (prev_mean-m)/prev_mean;
+            perr << "Relative improvement: " << relative_improvement*100 << " %"<< endl;
         }
+        costrow[0] = n+1;
+        costrow[1] = m;
+        costrow[2] = er;
+        costrow[3] = relative_improvement*100;
+        training_curve->appendRow(costrow);
+        training_curve->flush();
+
         prev_mean = m;
     }
 }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-04 16:33:02 UTC (rev 7519)
@@ -70,16 +70,12 @@
     //! Start your comments with Doxygen-compatible comments such as //!
     
 
-    //! training_schedule[k] conatins the number of nstages to run the optimizer for the
-    //! training of the hidden layer taking layer k as input (k=0 corresponds to
-    //! input layer).
+    TVec< pair<int,int> > unsupervised_nepochs;
+    Vec unsupervised_min_improvement_rate;
 
-    TVec<int> training_schedule;
-    int supervised_nepochs;    
+    pair<int,int> supervised_nepochs;    
+    real supervised_min_improvement_rate;
 
-    real good_improvement_rate;
-    real fine_tuning_improvement_rate;
-
     // layers[0] is the input variable
     // last layer is final output layer
     VarArray layers;
@@ -181,7 +177,7 @@
 
     void prepareForFineTuning();
     void fineTuningFor1Epoch();
-    void fineTuningFullOld();
+    // void fineTuningFullOld();
 
     void trainSupervisedLayer(VMat inputs, VMat targets);
 

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/python_modules/plearn/var/Var.py	2007-06-04 16:33:02 UTC (rev 7519)
@@ -69,8 +69,8 @@
         # So we make sure data is flushed to disk.
         return self.v.plearn_repr(indent_level, inner_repr)
 
-    def probabilityPairs(self, min, max):
-        return Var(pl.ProbabilityPairsVariable(input=self.v, min=min, max=max))
+    def probabilityPairs(self, min, max, varname=""):
+        return Var(pl.ProbabilityPairsVariable(input=self.v, min=min, max=max, varname=varname))
 
     def transpose(self):
         return Var(pl.TransposeVariable(input=self.v))
@@ -176,16 +176,22 @@
     cost = reconstr_activation.negCrossEntropySigmoid(input)
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, basename=""):
+def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, constrain_mask=False, basename=""):
     """iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output"""
-    #M = Var(ow/ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_M")
-    #W = Var(ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_W")
-    M = Var(ow/ogs, iw, "uniform", -1./sqrt(iw), 1./sqrt(iw), False, varname=basename+"_M")
-    W = Var(ogs, iw, "uniform", -1./sqrt(iw), 1./sqrt(iw), False, varname=basename+"_W")
-    hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
-    log_reconstructed = hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs)
+    M = Var(ow/ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_M")
+    if constrain_mask:
+        M = M.sigmoid()
+    W = Var(ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_W")
+    if add_bias:
+        b = Var(1,ow,"fill",0, varname=basename+'_b')
+        hidden = input.doubleProduct(W,M).add(b).multiSoftMax(ogs)
+        br = Var(1,iw,"fill",0, varname=basename+'_br')
+        log_reconstructed = hidden.transposeDoubleProduct(W,M).add(br).multiLogSoftMax(igs)
+    else:
+        hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
+        log_reconstructed = hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input



From yoshua at mail.berlios.de  Mon Jun  4 19:56:27 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 4 Jun 2007 19:56:27 +0200
Subject: [Plearn-commits] r7520 - in trunk/plearn: io math
Message-ID: <200706041756.l54HuRmZ024416@sheep.berlios.de>

Author: yoshua
Date: 2007-06-04 19:56:26 +0200 (Mon, 04 Jun 2007)
New Revision: 7520

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/math/PRandom.h
Log:
Proper handling of 64-bit ints in PStream
seed should be a 32-bit int in PRandom


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-06-04 16:33:02 UTC (rev 7519)
+++ trunk/plearn/io/PStream.cc	2007-06-04 17:56:26 UTC (rev 7520)
@@ -1185,6 +1185,13 @@
                 || (c==0x08 && byte_order()==LITTLE_ENDIAN_ORDER) )
                 endianswap(&x);
         }
+        else if(c==0x16 || c==0x17)  // plearn_binary
+        {
+            read(reinterpret_cast<char*>(&x),sizeof(long));
+            if( (c==0x16 && byte_order()==BIG_ENDIAN_ORDER) 
+                || (c==0x17 && byte_order()==LITTLE_ENDIAN_ORDER) )
+                endianswap(&x);
+        }
         else  // plearn_ascii
         {
             unget();
@@ -1753,11 +1760,22 @@
         put(' ');
         break;
     case plearn_binary:
+        if(sizeof(long)==4)
+        {
 #ifdef BIGENDIAN
         put((char)0x08);
 #else
         put((char)0x07);
 #endif
+        }
+        else if(sizeof(long)==8)
+        {
+#ifdef BIGENDIAN
+        put((char)0x17);
+#else
+        put((char)0x16);
+#endif
+        }
         write((char*)&x,sizeof(long));
         break;
     default:

Modified: trunk/plearn/math/PRandom.h
===================================================================
--- trunk/plearn/math/PRandom.h	2007-06-04 16:33:02 UTC (rev 7519)
+++ trunk/plearn/math/PRandom.h	2007-06-04 17:56:26 UTC (rev 7520)
@@ -89,7 +89,8 @@
     // * protected options *
     // *********************
 
-    long fixed_seed;
+    //long fixed_seed;
+    int fixed_seed;
 
 public:
 
@@ -97,7 +98,8 @@
     // * public build options *
     // ************************
 
-    long seed_;
+    //long seed_; // CAUSES PROBLEMS WITH PYTHON SERVER INTERFACE
+    int seed_;
 
     // ****************
     // * Constructors *



From yoshua at mail.berlios.de  Mon Jun  4 19:58:50 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 4 Jun 2007 19:58:50 +0200
Subject: [Plearn-commits] r7521 - in trunk/python_modules/plearn: io
	learners/autolr
Message-ID: <200706041758.l54HwoCK024648@sheep.berlios.de>

Author: yoshua
Date: 2007-06-04 19:58:49 +0200 (Mon, 04 Jun 2007)
New Revision: 7521

Modified:
   trunk/python_modules/plearn/io/serialize.py
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Proper handling of 64-bit ints (long)


Modified: trunk/python_modules/plearn/io/serialize.py
===================================================================
--- trunk/python_modules/plearn/io/serialize.py	2007-06-04 17:56:26 UTC (rev 7520)
+++ trunk/python_modules/plearn/io/serialize.py	2007-06-04 17:58:49 UTC (rev 7521)
@@ -232,9 +232,9 @@
         elif c=="\x06": # unsigned short big endian
             return struct.unpack('>H',self.read(2))[0]
         
-        elif c=="\x07": # int little endian
+        elif c=="\x07": # 32 bit int little endian
             return struct.unpack('<i',self.read(4))[0]
-        elif c=="\x08": # int big endian
+        elif c=="\x08": # 32 bit int big endian
             return struct.unpack('>i',self.read(4))[0]
         elif c=="\x0B": # unsigned int little endian
             return struct.unpack('<I',self.read(4))[0]
@@ -255,10 +255,15 @@
             self.unread(c)
             return self.binread_sequence()
 
-        elif c=="\x16": # DEPRECATED binary pair format (for backward compatibility)
-            self.unread(c)
-            return self.binread_pair()
+        elif c=="\x16": # 64 bit int little endian
+            return struct.unpack('<q',self.read(8))[0]
+        elif c=="\x17": # 64 bit int big endian
+            return struct.unpack('>q',self.read(8))[0]
 
+        #elif c=="\x16": # DEPRECATED binary pair format (for backward compatibility)
+        #    self.unread(c)
+        #    return self.binread_pair()
+
         elif c=='"': # string
             self.unread(c)
             return self.read_string()

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 17:56:26 UTC (rev 7520)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 17:58:49 UTC (rev 7521)
@@ -9,7 +9,7 @@
 def deepcopy(plearnobject):
     # actually not a deep-copy, only copy options
     if plearn.bridgemode.useserver:
-        o = newObject(plearnobject.getObject())
+        o = serv.new(plearnobject.getObject())
     else:
         o = newObject(str(plearnobject))
     if o==None:
@@ -110,6 +110,8 @@
             results[i,1+s] = learning_rates[i][s]
         for j in range(0,n_tests):
             ts = pl.VecStatsCollector()
+            if plearn.bridgemode.useserver:
+                ts=serv.new(ts)
             learner.test(testsets[j],ts,0,0)
             if logfile:
                 print >>logfile, "At stage ",learner.stage," test" + str(j+1),": ",
@@ -175,6 +177,8 @@
         learner.nstages = int(nstages+initial_stage)
         learner.train()
         ts = pl.VecStatsCollector()
+        if plearn.bridgemode.useserver:
+            ts=serv.new(ts)
         learner.test(testset,ts,0,0)
         err=ts.getStat("E["+str(cost_to_select_best)+"]")
         if logfile:
@@ -279,6 +283,8 @@
         schedules[:,0]=stages
         schedules[:,1]=learning_rates[:,0]
         optimized_group=0 # no choice, there is only one group
+        if len(lr_options)!=1:
+            lr_options=[lr_options[0]]
     n_train = len(stages)
     n_schedules = len(lr_options)
     assert n_schedules==learning_rates.shape[1]
@@ -323,6 +329,8 @@
                 print >>logfile, "candidate ",active,":",
             for j in range(0,n_tests):
                 ts = pl.VecStatsCollector()
+                if plearn.bridgemode.useserver:
+                    ts=serv.new(ts)
                 candidate.test(testsets[j],ts,0,0)
                 if logfile:
                     print >>logfile, " test" + str(j+1),": ",



From manzagop at mail.berlios.de  Mon Jun  4 21:17:18 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 4 Jun 2007 21:17:18 +0200
Subject: [Plearn-commits] r7522 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200706041917.l54JHI8w000067@sheep.berlios.de>

Author: manzagop
Date: 2007-06-04 21:17:17 +0200 (Mon, 04 Jun 2007)
New Revision: 7522

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
Added an option for applying L1 regularization to the output layer's parameters.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-06-04 17:58:49 UTC (rev 7521)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-06-04 19:17:17 UTC (rev 7522)
@@ -61,6 +61,7 @@
       params_averaging_freq(5),
       init_lrate(0.01),
       lrate_decay(0),
+      output_layer_L1_penalty_factor(0.0),
       output_layer_lrate_scale(1),
       minibatch_size(1),
       output_type("NLL"),
@@ -154,6 +155,13 @@
                   OptionBase::buildoption,
                   "Learning rate decay factor\n");
 
+    declareOption(ol, "output_layer_L1_penalty_factor",
+                  &NatGradNNet::output_layer_L1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L1 regularization term, i.e.\n"
+                  "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| during training.\n"
+                  "Gets multiplied by the learning rate. Only on output layer!!");
+
     declareOption(ol, "output_layer_lrate_scale", &NatGradNNet::output_layer_lrate_scale,
                   OptionBase::buildoption,
                   "Scaling factor of the learning rate for the output layer. Values less than 1"
@@ -334,6 +342,8 @@
     }
     else PLERROR("NatGradNNet: output_type should be NLL or MSE\n");
 
+    if( output_layer_L1_penalty_factor < 0. )
+        PLWARNING("NatGradNNet::build_ - output_layer_L1_penalty_factor is negative!\n");
 
     if(use_pvgrad && minibatch_size!=1)
         PLERROR("PV's gradient technique (triggered by use_pvgrad): support for minibatch not yet implemented (must have minibatch_size=1)");
@@ -790,6 +800,23 @@
     //    (*ng_corrprof)(all_params_delta);
     //}
 
+    // Output layer L1 regularization
+    if( output_layer_L1_penalty_factor != 0. )    {
+        real L1_delta = lrate * output_layer_L1_penalty_factor;
+        real* m_i = layer_params[n_layers-2].data();
+
+        for(int i=0; i<layer_params[n_layers-2].length(); i++,m_i+=layer_params[n_layers-2].mod())  {
+            for(int j=0; j<layer_params[n_layers-2].width(); j++)   {
+                if( m_i[j] > L1_delta )
+                    m_i[j] -= L1_delta;
+                else if( m_i[j] < -L1_delta )
+                    m_i[j] += L1_delta;
+                else
+                    m_i[j] = 0.;
+            }
+        }
+    }
+
 }
 
 void NatGradNNet::pvGradUpdate()

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-06-04 17:58:49 UTC (rev 7521)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-06-04 19:17:17 UTC (rev 7522)
@@ -79,6 +79,9 @@
     //! learning rate decay factor
     real lrate_decay;
 
+    //! L1 penalty applied to the output layer's parameters
+    real output_layer_L1_penalty_factor;
+
     //! scaling factor of the learning rate for the output layer
     real output_layer_lrate_scale;
 



From yoshua at mail.berlios.de  Mon Jun  4 21:21:51 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 4 Jun 2007 21:21:51 +0200
Subject: [Plearn-commits] r7523 - in trunk/python_modules/plearn: . pyext
Message-ID: <200706041921.l54JLpMn000687@sheep.berlios.de>

Author: yoshua
Date: 2007-06-04 21:21:50 +0200 (Mon, 04 Jun 2007)
New Revision: 7523

Modified:
   trunk/python_modules/plearn/bridge.py
   trunk/python_modules/plearn/bridgemode.py
   trunk/python_modules/plearn/pyext/plext.cc
Log:
executable for PLearn server command name can be user-provided, default=plearn.


Modified: trunk/python_modules/plearn/bridge.py
===================================================================
--- trunk/python_modules/plearn/bridge.py	2007-06-04 19:17:17 UTC (rev 7522)
+++ trunk/python_modules/plearn/bridge.py	2007-06-04 19:21:50 UTC (rev 7523)
@@ -8,7 +8,7 @@
     from plearn.pyplearn import *
     from plearn.io.server import *
     import time
-    server_command = 'plearn_curses server'
+    server_command = plearn.bridgemode.server_exe + ' server'
     serv = launch_plearn_server(command = server_command)
     time.sleep(0.5) # give some time to the server to get born and well alive
 else:

Modified: trunk/python_modules/plearn/bridgemode.py
===================================================================
--- trunk/python_modules/plearn/bridgemode.py	2007-06-04 19:17:17 UTC (rev 7522)
+++ trunk/python_modules/plearn/bridgemode.py	2007-06-04 19:21:50 UTC (rev 7523)
@@ -4,6 +4,9 @@
 
 useserver = False
 
+# which plearn executable do we want?
+server_exe = 'plearn'
+
 # whether we plot error curve graphs interactively (pylab)
 #
 

Modified: trunk/python_modules/plearn/pyext/plext.cc
===================================================================
--- trunk/python_modules/plearn/pyext/plext.cc	2007-06-04 19:17:17 UTC (rev 7522)
+++ trunk/python_modules/plearn/pyext/plext.cc	2007-06-04 19:21:50 UTC (rev 7523)
@@ -32,7 +32,8 @@
 // library, go to the PLearn Web site at www.plearn.org
 
 #include <plearn/python/PythonExtension.h>
-#include <commands/plearn_full_inc.h>
+//#include <commands/plearn_full_inc.h>
+#include <commands/myplearn_light_inc.h>
 #include <commands/PLearnCommands/plearn_main.h>
 
 using namespace PLearn;



From louradou at mail.berlios.de  Mon Jun  4 21:29:21 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 4 Jun 2007 21:29:21 +0200
Subject: [Plearn-commits] r7524 - trunk/python_modules/plearn/learners
Message-ID: <200706041929.l54JTL3N001220@sheep.berlios.de>

Author: louradou
Date: 2007-06-04 21:29:20 +0200 (Mon, 04 Jun 2007)
New Revision: 7524

Added:
   trunk/python_modules/plearn/learners/discr_power_SVM.py
Log:
To evaluate the discriminative power of a representation using SVM


Added: trunk/python_modules/plearn/learners/discr_power_SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-04 19:21:50 UTC (rev 7523)
+++ trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-04 19:29:20 UTC (rev 7524)
@@ -0,0 +1,405 @@
+import sys, os, time
+#from numarray import *
+from math import *
+from libsvm import *
+
+	     
+class SVM_expert(object):
+      __attributes__ = ['kernel_type',
+			'parameters_names',
+			'tried_parameters',
+			'best_parameters',
+			'accuracy'
+			]
+			
+      def __init__( self ):
+	  self.parameters_names = ['C']
+	  self.tried_parameters = {}
+	  self.best_parameters  = None
+	  self.accuracy         = 0
+
+      def reset(self):
+          self.tried_parameters = {}
+	  #if self.best_parameters != None:
+	  #   self.add_parameter_to_tried_list(self.getBestValue('C'), self.best_parameters[1:])
+	  self.accuracy         = 0
+	  
+      def get_svm_parameter( self, parameters ):
+          s= ', '.join([ self.parameters_names[i]+' = '+str(parameters[i]) for i in range(len(self.parameters_names)) ])
+	  return eval('svm_parameter( svm_type = C_SVC, kernel_type = '+self.kernel_type+', '+s+')')
+
+      def add_parameter_to_tried_list(self, C, kernel_parameters):
+          if self.tried_parameters.has_key(kernel_parameters):
+	     self.tried_parameters[kernel_parameters]+=[C]
+	  else:
+	     self.tried_parameters[kernel_parameters] =[C] 
+	     
+      def init_C(self, C_value):
+          return [C_value/10., C_value, C_value*10.]
+
+      def init_parameters(self, kernel_parameters_list):
+	  C_base = self.init_C(10.)
+          table  = []
+          for C in C_base:
+	      for prm in kernel_parameters_list:
+	          table.append( parameters2list(C, prm) )
+		  self.add_parameter_to_tried_list(C, prm)
+          return table
+
+      def choose_new_parameters(self, kernel_parameters):
+          if self.tried_parameters.has_key(kernel_parameters):
+             C_table = choose_new_parameters_geom( self.tried_parameters[kernel_parameters], self.getBestValue('C') )
+	  else:
+	     C_table = self.init_C( self.best_parameters[0] )
+          table  = []
+          for C in C_table:
+	          table.append( parameters2list(C, kernel_parameters) )
+		  self.add_parameter_to_tried_list(C, kernel_parameters)
+          return table
+	  
+      def getBestValue(self, name_prm):
+          return self.best_parameters[ self.parameters_names.index(name_prm) ]
+	  
+
+class RBF_expert(SVM_expert):
+      def __init__( self ):
+          SVM_expert.__init__( self )
+          self.kernel_type       = 'RBF'
+	  self.parameters_names += ['gamma']
+
+      def init_gamma(self, gamma):
+          return [gamma/9., gamma, gamma*9.]
+	  
+      def init_parameters( self, samples ):
+          dim = len(samples[0])
+	  std = mean_std(samples)
+	  rho=sqrt(dim)*std
+          gamma0 = 1/(2*rho**2)
+	  gamma_base = self.init_gamma(gamma0)
+	  return SVM_expert.init_parameters( self, gamma_base )
+	  
+      def choose_new_parameters( self ):
+	  best_gamma = self.getBestValue('gamma') 
+	  tried_gamma = self.tried_parameters
+	  if tried_gamma.has_key(best_gamma):
+	     if self.getBestValue('C') == self.tried_parameters[best_gamma][0] or self.getBestValue('C') == self.tried_parameters[best_gamma][len(self.tried_parameters[best_gamma])-1]:
+	        return SVM_expert.choose_new_parameters(self, best_gamma)
+	     else:
+	        proposed_gammas = choose_new_parameters_geom( tried_gamma, best_gamma)
+	  else:
+	     proposed_gammas = self.init_gamma(best_gamma)
+          return SVM_expert.init_parameters(self, proposed_gammas)
+
+class POLY_expert(SVM_expert):
+      def __init__( self ):
+          SVM_expert.__init__( self )
+          self.kernel_type       = 'POLY'
+	  self.parameters_names += ['degree','coef0']
+
+      def init_degree(self, degree):
+          return [  (degree-1,1), (degree,1), (degree+1,1) ]
+
+      def init_parameters( self, samples ):
+          #return SVM_expert.init_parameters(self, self.init_degree(3) )
+          return SVM_expert.init_parameters(self, [ (2,1), (3,1), (4,1) ] )
+	  
+      def choose_new_parameters( self ):
+          best_degree = self.getBestValue('degree')
+          tried_degrees = [prms[0] for prms in self.tried_parameters]
+	  if self.tried_parameters.has_key(best_degree):
+             if best_degree == max(tried_degrees):
+	        return SVM_expert.init_parameters(self, [(best_degree+1,1)])
+	     else:
+                return SVM_expert.choose_new_parameters(self, (self.best_parameters[1], self.best_parameters[2])  )
+          else:
+	     return SVM_expert.init_parameters(self, self.init_degree(best_degree) )
+	     
+class LINEAR_expert(SVM_expert):
+      def __init__( self ):
+          SVM_expert.__init__( self )
+          self.kernel_type = 'LINEAR'
+
+      def init_parameters( self, samples ):
+          return SVM_expert.init_parameters(self, [None])
+
+      def choose_new_parameters( self ):
+          return SVM_expert.choose_new_parameters(self, None)
+
+
+class discr_power_SVM_eval(object):
+
+      __attributes__ = ['accuracy',
+                        'valid_accuracy',
+			'best_parameters',
+			'tried_parameters'
+			]
+       
+      def __init__( self ):
+      
+	  self.accuracy       = 0
+	  self.valid_accuracy = 0
+	  
+	  self.LINEAR_expert  = LINEAR_expert()
+	  self.RBF_expert     = RBF_expert()
+	  self.POLY_expert    = POLY_expert()
+	  
+	  self.best_parameters      = None  
+	  self.tried_parameters     = {}
+	  
+          # For cross-validation
+	  self.nr_fold        = 5
+
+      def reset( self ):
+          self.LINEAR_expert.reset()
+	  self.RBF_expert.reset()
+	  self.POLY_expert.reset()
+	  
+          self.tried_parameters = {}
+	  #if self.best_parameters != None:
+	  #   self.add_parameter_to_tried_list(self.best_parameters[0], self.best_parameters[1:])
+	  self.accuracy       = 0
+	  self.valid_accuracy = 0
+
+      def add_parameter_to_tried_list(self, kernel, kernel_parameters):
+          if self.tried_parameters.has_key(kernel):
+	     self.tried_parameters[kernel]+=[kernel_parameters]
+	  else:
+	     self.tried_parameters[kernel] =[kernel_parameters] 
+
+
+      def valid_and_compute_accuracy(self, kernel_type, samples_target_list):
+	  check_samples_target_list(samples_target_list)
+	  
+	  expert = eval( 'self.'+kernel_type+'_expert' )
+	  
+	  if len(expert.tried_parameters) == 0:
+	     recompute_best = True
+	  else:
+	     recompute_best = False
+
+          if expert.best_parameters  == None:
+	     parameters_to_try = expert.init_parameters( samples_target_list[0][0] )
+	  else:
+	     parameters_to_try = expert.choose_new_parameters()
+	  
+	  best_parameters = expert.best_parameters
+	  best_accuracy   = expert.accuracy
+
+          for parameters in parameters_to_try:
+	      if parameters != expert.best_parameters or recompute_best:
+	      
+		  self.add_parameter_to_tried_list(kernel_type, parameters)
+	          param = expert.get_svm_parameter( parameters )
+		  
+	          if len(samples_target_list) == 1: # cross-validation
+		     accuracy = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
+		  else:
+		     train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
+		     model = svm_model(train_problem, param)
+		     accuracy = do_simple_validation(model, samples_target_list[1][0], samples_target_list[1][1], param)
+		     	     
+		  if accuracy > best_accuracy:
+		         best_parameters = parameters
+		         best_accuracy = accuracy
+			 if len(samples_target_list) == 3:
+			    best_model = model
+
+          if best_accuracy > expert.accuracy:
+	     expert.best_parameters = best_parameters
+	     expert.accuracy = best_accuracy
+	     
+	     if best_accuracy > self.valid_accuracy:
+		self.best_parameters = [kernel_type, best_parameters]
+		self.valid_accuracy = best_accuracy
+	        if len(samples_target_list) == 3: # train-valid-test
+	           best_param = expert.get_svm_parameter( best_parameters )  
+                   self.accuracy = do_simple_validation(best_model, samples_target_list[2][0], samples_target_list[2][1], best_param)
+	        else:
+		   self.accuracy = self.valid_accuracy
+	  
+	  return self.accuracy
+	  
+	  #self.clerror  = 100 - self.accuracy
+
+      def compute_accuracy(self, samples_target_list):
+	  best_expert = eval( 'self.'+self.best_parameters[0]+'_expert' )
+	  best_parameters = best_expert.best_parameters[1:]
+	  param = best_expert.get_svm_parameter( best_parameters )
+	  if len(samples_target_list) == 1: # cross-validation
+	     accuracy = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
+	  else:
+	     train_problem = svm_problem( samples_target_list[0][1], samples_target_list[0][0] )
+	     model = svm_model(train_problem, param)
+	     accuracy = do_simple_validation(model, samples_target_list[1][0], samples_target_list[1][1], param)
+	  return accuracy
+    
+def mean_std(data):
+    stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
+    return sum(stds)/len(stds)
+def get_std_cmp(data,i):
+    values=[vec[i] for vec in data]
+    tot = sum(values)
+    avg = tot*1.0/len(values)
+    sdsq = sum([(i-avg)**2 for i in values])
+    return (sdsq*1.0/(len(values)-1 or 1))**.5
+
+def arithm_mean(data):
+    if type(data[0]) == list:
+       return [sum( [data[i][coor] for i in range(len(data))] )*1.0/len(data) for coor in range(len(data[0]))]
+    else:
+       return sum(data)*1.0/len(data)
+def geom_mean(data):
+    if type(data[0]) == list:
+       res=[]
+       for coor in range(len(data[0])):
+           res.append( geom_mean( [data[i][coor] for i in range(len(data))] ) )
+       return res
+    else:
+       prod = 1.0
+       for value in data:
+           prod *= value
+       return prod**(1.0/len(data))
+
+def choose_new_parameters_geom( table, best_value ):
+    sorted_table = sorted(table)
+    if best_value == sorted_table[0]:
+       # smallest value
+       proposed_table = [ best_value*1.1*best_value/sorted_table[1],
+                          #best_value,
+		          geom_mean([sorted_table[1],best_value]) ]
+    elif best_value == sorted_table[len(table)-1]:
+       # largest value
+       proposed_table = [ geom_mean([sorted_table[len(table)-2],best_value]), 
+                          #best_value,
+		          best_value*0.9*best_value/sorted_table[len(table)-2] ]
+    else:
+       # middle value (best case: dichotomie)
+       if best_value not in sorted_table:
+          raise TypeError, "in RBF.choose_new_parameters: "+str(best_value)+" not found in tried_parameters"
+       index = sorted_table.index(best_value)
+       proposed_table = [ geom_mean([sorted_table[index-1],best_value]),
+                          #best_value,
+		          geom_mean([sorted_table[index+1],best_value]) ]
+    return proposed_table
+
+def do_cross_validation(samples, targets, param, nr_fold):
+	prob_l = len(targets)
+	total_correct = 0
+	prob = svm_problem(targets, samples)
+    	outputs = cross_validation(prob, param, nr_fold)
+	for i in range(prob_l):
+	    if outputs[i] == targets[i]:
+	       total_correct = total_correct + 1 
+	return (100.0 * total_correct / prob_l)
+
+def do_cross_validation(samples, targets, param, nr_fold):
+        n_train = int( len(samples)/nr_fold )
+        train_samples = samples[0:n_train]
+	train_targets = targets[0:n_train]
+	test_samples = samples[n_train:]
+	test_targets = targets[n_train:]
+        train_problem = svm_problem( train_targets, train_samples )
+	model = svm_model(train_problem, param)
+	return do_simple_validation(model, test_samples, test_targets, param)	     
+	
+def do_simple_validation(model, samples, targets, param):
+	prob_l = len(targets)
+	total_correct = 0
+	for i in range(prob_l):
+	    if model.predict(samples[i]) == targets[i]:
+	       total_correct = total_correct + 1
+	return (100.0 * total_correct / prob_l)
+
+def check_samples_target_list(samples_target_list):
+          if type(samples_target_list) != list or len(samples_target_list) == 0 or type(samples_target_list[0]) != list:
+	     raise TypeError, "ERROR: samples_target_list must be a list of list (of arrays)"
+          else:
+	     for samples_target in samples_target_list:
+	         if len(samples_target) != 2:
+	            raise TypeError, "ERROR: samples_target_list has an element with length "+str(len(samples_target))+" (instead of 2)"
+	  if len(samples_target_list) == 1:
+	     print "cross-validation"
+	     return
+	  elif len(samples_target_list) == 2:
+	     print "simple validation"
+	     return
+	  elif len(samples_target_list) == 3:
+	     print "validation + test"
+	     return
+	  else:
+	      raise TypeError, "ERROR: samples34_target_list have length "+str(len(samples_target_list))+" (not in [1,2,3])"
+	  #print "samples_target_list has to be a list of [sample, target] arrays"
+	  #print "for example :\n\t[[TrainSet, TrainLabels]]"
+	  #print "\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels]]"
+	  #print "\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels], [TestSamples, TestLabels]]"
+
+def parameters2list(C, kernel_parameters):
+          if kernel_parameters == None:
+	     return [C]
+	  elif type(kernel_parameters) == list:
+	     return [C]+kernel_parameters
+	  elif type(kernel_parameters) == tuple:
+	     return [C]+[prm for prm in kernel_parameters]
+	  else:
+             return [C, kernel_parameters ]
+
+
+if __name__ == '__main__':
+   from random import *
+   
+   raise('hihi')
+   
+   nsamples=10
+   dim=25
+   x=[]
+   t=[]
+   for i in range(nsamples):
+       x.append([])
+       for j in range(dim):
+           x[i].append(random())
+       t.append(randint(0,6))
+   #print x
+   #print t
+   x=zip([-2,-1, 0, 2, 0, 1,-2,-1, 1,-1,0,1,2,3,-3,-3,-3,-2,-2,   -1, 2, 0, 1, 2,-1, 0, 2, 0, 2,-3,-3,-3,-2,-1, 0, 1],
+         [ 2, 2, 2, 2, 1, 1, 0, 0,-1, 3,3,3,3,1, 2, 1, 0,-1,-2,    1, 1, 0, 0, 0,-1,-1,-1,-2,-2,-1,-2,-3,-3,-3,-3,-3])
+   t=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,   1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
+   x2=zip([3,3,3 , 1],
+          [0,2,-1, -0.5])+x
+   t2=[0,0,1,1]+t
+
+   learner = loadObject("/u/bengioy/LisaPLearn/UserExp/bengioy/babyAI/exp/minimize1_baby_image_rbm_trainsize=1000000_nh=100_mbs=20_nstages=1000000_nskip=2_mincost=rec.err._mre=0_2007-06-04#08:51:14.600291/final_learner.psave")
+
+   expdir = "exp/toto"
+
+   execfile("/u/bengioy/LisaPLearn/UserExp/bengioy/babyAI/baby_data.py")
+
+   valid1000 = pl.SubVMatrix(source=validSet,length=1000)
+
+   (ts,out,c) = learner.test(valid1000,pl.VecStatsCollector(),1,0)
+
+
+   data=valid1000.getMat()
+
+   y = [ vec[data.shape[1]-1]  for vec in data]
+   text = [ vec[1024:data.shape[1]-1]  for vec in data]
+   x = [ [out[isample][coor] for coor in range(len(out[0]))]+[text[isample][coor] for coor in range(len(text[0]))]  for isample in range(len(out)) ]
+
+   E=discr_power_SVM_eval()
+   E.valid_and_compute_accuracy( 'LINEAR' ,  [[x,y]])
+   E.valid_and_compute_accuracy( 'RBF' ,    [[x,y]])
+   
+
+   E.reset()
+   
+   
+   
+   E.valid_and_compute_accuracy( 'RBF' ,  [[x,t],[x2,t2]])
+   E.valid_and_compute_accuracy( 'RBF' ,  [[x,t],[x2,t2]])
+   E.valid_and_compute_accuracy( 'POLY' ,  [[x,t],[x2,t2]])
+
+   E.accuracy
+   
+   E.reset
+
+   print E.compute_accuracy( [[x,t],[x2,t2]])
+



From yoshua at mail.berlios.de  Mon Jun  4 21:33:37 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 4 Jun 2007 21:33:37 +0200
Subject: [Plearn-commits] r7525 - trunk/python_modules/plearn/pyext
Message-ID: <200706041933.l54JXbb0001592@sheep.berlios.de>

Author: yoshua
Date: 2007-06-04 21:33:37 +0200 (Mon, 04 Jun 2007)
New Revision: 7525

Modified:
   trunk/python_modules/plearn/pyext/plext.cc
Log:
oops corrected something which should not have been committed


Modified: trunk/python_modules/plearn/pyext/plext.cc
===================================================================
--- trunk/python_modules/plearn/pyext/plext.cc	2007-06-04 19:29:20 UTC (rev 7524)
+++ trunk/python_modules/plearn/pyext/plext.cc	2007-06-04 19:33:37 UTC (rev 7525)
@@ -32,8 +32,8 @@
 // library, go to the PLearn Web site at www.plearn.org
 
 #include <plearn/python/PythonExtension.h>
-//#include <commands/plearn_full_inc.h>
-#include <commands/myplearn_light_inc.h>
+#include <commands/plearn_full_inc.h>
+//#include <commands/myplearn_light_inc.h>
 #include <commands/PLearnCommands/plearn_main.h>
 
 using namespace PLearn;



From louradou at mail.berlios.de  Mon Jun  4 21:39:38 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 4 Jun 2007 21:39:38 +0200
Subject: [Plearn-commits] r7526 - trunk/python_modules/plearn/learners
Message-ID: <200706041939.l54JdceL002063@sheep.berlios.de>

Author: louradou
Date: 2007-06-04 21:39:37 +0200 (Mon, 04 Jun 2007)
New Revision: 7526

Modified:
   trunk/python_modules/plearn/learners/discr_power_SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/discr_power_SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-04 19:33:37 UTC (rev 7525)
+++ trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-04 19:39:37 UTC (rev 7526)
@@ -346,60 +346,48 @@
 
 if __name__ == '__main__':
    from random import *
-   
-   raise('hihi')
-   
-   nsamples=10
-   dim=25
-   x=[]
-   t=[]
-   for i in range(nsamples):
-       x.append([])
-       for j in range(dim):
-           x[i].append(random())
-       t.append(randint(0,6))
-   #print x
-   #print t
-   x=zip([-2,-1, 0, 2, 0, 1,-2,-1, 1,-1,0,1,2,3,-3,-3,-3,-2,-2,   -1, 2, 0, 1, 2,-1, 0, 2, 0, 2,-3,-3,-3,-2,-1, 0, 1],
-         [ 2, 2, 2, 2, 1, 1, 0, 0,-1, 3,3,3,3,1, 2, 1, 0,-1,-2,    1, 1, 0, 0, 0,-1,-1,-1,-2,-2,-1,-2,-3,-3,-3,-3,-3])
-   t=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,   1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
-   x2=zip([3,3,3 , 1],
-          [0,2,-1, -0.5])+x
-   t2=[0,0,1,1]+t
 
    learner = loadObject("/u/bengioy/LisaPLearn/UserExp/bengioy/babyAI/exp/minimize1_baby_image_rbm_trainsize=1000000_nh=100_mbs=20_nstages=1000000_nskip=2_mincost=rec.err._mre=0_2007-06-04#08:51:14.600291/final_learner.psave")
 
-   expdir = "exp/toto"
-
    execfile("/u/bengioy/LisaPLearn/UserExp/bengioy/babyAI/baby_data.py")
 
    valid1000 = pl.SubVMatrix(source=validSet,length=1000)
 
-   (ts,out,c) = learner.test(valid1000,pl.VecStatsCollector(),1,0)
+   (ts,image_repr,c) = learner.test(valid1000,pl.VecStatsCollector(),1,0)
 
-
    data=valid1000.getMat()
 
    y = [ vec[data.shape[1]-1]  for vec in data]
    text = [ vec[1024:data.shape[1]-1]  for vec in data]
-   x = [ [out[isample][coor] for coor in range(len(out[0]))]+[text[isample][coor] for coor in range(len(text[0]))]  for isample in range(len(out)) ]
+   x = [ [image_repr[isample][coor] for coor in range(len(image_repr[0]))]+[text[isample][coor] for coor in range(len(text[0]))]  for isample in range(len(image_repr)) ]
 
-   E=discr_power_SVM_eval()
-   E.valid_and_compute_accuracy( 'LINEAR' ,  [[x,y]])
-   E.valid_and_compute_accuracy( 'RBF' ,    [[x,y]])
-   
 
-   E.reset()
+   # an EXAMPLE to use the class...
+
+   # Initialisation 
    
+   E=discr_power_SVM_eval()
    
+   # Compute the accuracies (exploring a bit, each time, the space of parameters)
+   # -> Type E.accuracy to have the accuracy value.
    
-   E.valid_and_compute_accuracy( 'RBF' ,  [[x,t],[x2,t2]])
-   E.valid_and_compute_accuracy( 'RBF' ,  [[x,t],[x2,t2]])
-   E.valid_and_compute_accuracy( 'POLY' ,  [[x,t],[x2,t2]])
+   E.valid_and_compute_accuracy( 'LINEAR' ,  [[x,y]])
+   E.valid_and_compute_accuracy( 'LINEAR' ,  [[x,y]])
+   E.valid_and_compute_accuracy( 'RBF' ,    [[x,y]])
+   E.valid_and_compute_accuracy( 'RBF' ,    [[x,y]])
 
-   E.accuracy
-   
-   E.reset
+   print E.accuracy
+   print E.best_parameters
+   print E.tried_parameters
 
-   print E.compute_accuracy( [[x,t],[x2,t2]])
+   # To reset the explored tables of parameters and accuracies
+   # (only keep the information about the last best parameters sets)
+   # -> This has an interest when you use a new representation/dataset closed to the previous one.
 
+   E.reset()
+   E.valid_and_compute_accuracy( 'LINEAR' ,  [[x,y]])
+   E.valid_and_compute_accuracy( 'RBF' ,  [[x,t],[x2,t2]])
+
+   print E.accuracy
+   print E.best_parameters
+   print E.tried_parameters



From yoshua at mail.berlios.de  Mon Jun  4 21:51:34 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 4 Jun 2007 21:51:34 +0200
Subject: [Plearn-commits] r7527 - trunk/commands
Message-ID: <200706041951.l54JpYW4002933@sheep.berlios.de>

Author: yoshua
Date: 2007-06-04 21:51:33 +0200 (Mon, 04 Jun 2007)
New Revision: 7527

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
added IdentityModule in plearn_noblas_inc.h


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-06-04 19:39:37 UTC (rev 7526)
+++ trunk/commands/plearn_noblas_inc.h	2007-06-04 19:51:33 UTC (rev 7527)
@@ -216,6 +216,7 @@
 #include <plearn_learners/online/ProcessInputCostModule.h>
 #include <plearn_learners/online/RBMBinomialLayer.h>
 #include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/IdentityModule.h>
 #include <plearn_learners/online/RBMConnection.h>
 #include <plearn_learners/online/RBMConv2DConnection.h>
 #include <plearn_learners/online/RBMGaussianLayer.h>



From tihocan at mail.berlios.de  Mon Jun  4 22:35:34 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 4 Jun 2007 22:35:34 +0200
Subject: [Plearn-commits] r7528 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706042035.l54KZYUL006774@sheep.berlios.de>

Author: tihocan
Date: 2007-06-04 22:35:34 +0200 (Mon, 04 Jun 2007)
New Revision: 7528

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Added ability to actually do not do optimization by setting optimized_group=-1

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 19:51:33 UTC (rev 7527)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 20:35:34 UTC (rev 7528)
@@ -215,7 +215,7 @@
 def train_adapting_lr(learner,
                       trainset,testsets,expdir,
                       lr_options, # List of lists of options (one list/group of options with a given schedule)
-                      optimized_group=0, # Group of options that is actually optimized
+                      optimized_group=0, # Group of options that is actually optimized (if -1, then no optimization is performed)
                       schedules=None, # Matrix of schedules (number of columns = 1 (stage) + number of groups)
                       nstages=None, # Used to construct default schedule if 'schedules' is None
                       epoch=None,   # ""
@@ -254,7 +254,7 @@
         c2lr=all_lr[c2]
         c2err=all_last_err[c2]
         for a in actives:
-            if all_lr[a]==c2lr and all_last_err[a]<c2err:
+            if all_lr[a]==c2lr and all_last_err[a]<=c2err:
                 return True # throw it away if worse than other actives of same lr
             if a!=c2 and abs(log(all_lr[a]/c2lr))<keep_lr*log(lr_steps):
                 alone=False
@@ -282,7 +282,9 @@
         schedules = zeros([len(stages),2],Float)
         schedules[:,0]=stages
         schedules[:,1]=learning_rates[:,0]
-        optimized_group=0 # no choice, there is only one group
+        if optimized_group != 0 and optimized_group != -1:
+            print "Incorrect value for 'optimized_group'"
+            raise Error
         if len(lr_options)!=1:
             lr_options=[lr_options[0]]
     n_train = len(stages)
@@ -411,7 +413,8 @@
         final_model = best_candidate
     else:
         final_model = all_candidates[best_active]
-    schedules[:,1+optimized_group]=all_results[best_active][:,1]
+    if optimized_group >= 0:
+        schedules[:,1+optimized_group]=all_results[best_active][:,1]
     if logfile and best_err < all_last_err[best_active]:
         print >>logfile, "WARNING: best performing model would have stopped early at stage ",best_early_stop
     return (final_model,   # Learner



From dorionc at mail.berlios.de  Mon Jun  4 23:22:43 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Mon, 4 Jun 2007 23:22:43 +0200
Subject: [Plearn-commits] r7529 - trunk/python_modules/plearn/pyplearn
Message-ID: <200706042122.l54LMhU9010487@sheep.berlios.de>

Author: dorionc
Date: 2007-06-04 23:22:42 +0200 (Mon, 04 Jun 2007)
New Revision: 7529

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
Bug fix for multilevel namespace inheritance...


Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-04 20:35:34 UTC (rev 7528)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-04 21:22:42 UTC (rev 7529)
@@ -915,7 +915,7 @@
         Note that if 'Existing.some_option=VALUE' is overriden through the command-line,
         the value of 'New.some_option' will be VALUE unless explicitely overrode. 
         """
-        META = plnamespace.__metaclass__
+        META = namespace.__metaclass__
         class __metaclass__(META):
             def __new__(metacls, clsname, bases, dic):
                 # Do not use plopt.optdict: the documentation, choices and other



From yoshua at mail.berlios.de  Tue Jun  5 00:10:13 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 5 Jun 2007 00:10:13 +0200
Subject: [Plearn-commits] r7530 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706042210.l54MADGQ013912@sheep.berlios.de>

Author: yoshua
Date: 2007-06-05 00:10:13 +0200 (Tue, 05 Jun 2007)
New Revision: 7530

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
added save every x epoch option in autolr


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 21:22:42 UTC (rev 7529)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 22:10:13 UTC (rev 7530)
@@ -223,11 +223,12 @@
                       nskip=2,   # Number of epochs after which we add/remove candidates
                       cost_to_select_best=0, # Index of cost being optimized
                       return_best_model=False, # o/w return final model
-                      save_best=False, # for paranoids: save best model each time an improvement is found
+                      save_best=False, # for paranoids: save best model every save_best epochs, or not at all (if = False)
                       selected_costnames = False,
                       min_epochs_to_delete = 2,
                       lr_steps=exp(log(10)/2), # Scaling coefficient when modifying learning rates
                       logfile=False,
+                      min_lr=1e-6, # do not try to go below this learning rate
                       keep_lr=2): # Learning rate interval for heuristic
 
     min_epochs_to_delete = max(1,min_epochs_to_delete) # although 1 is probably too small
@@ -256,7 +257,8 @@
         for a in actives:
             if all_lr[a]==c2lr and all_last_err[a]<=c2err:
                 return True # throw it away if worse than other actives of same lr
-            if a!=c2 and abs(log(all_lr[a]/c2lr))<keep_lr*log(lr_steps):
+            # say that it is alone if there are no other actives with nearby and greater lr
+            if a!=c2 and all_lr[a]>c2lr and abs(log(all_lr[a]/c2lr))<keep_lr*log(lr_steps):
                 alone=False
         c1lr=all_lr[c1]
         if alone and c2lr>c1lr: # and slope2<0: 
@@ -361,11 +363,11 @@
                             best_early_stop = stage
                             if return_best_model:
                                 best_candidate = deepcopy(candidate)
-                            if save_best: 
-                                candidate.save(expdir+"/"+"best_learner.psave","plearn_binary")
             if logfile:
                 print >>logfile
                 logfile.flush()
+        if save_best and t%save_best==0:
+            all_candidates[best_active].save(expdir+"/"+"best_learner.psave","plearn_binary")
         if previous_best_err > best_err:
             previous_best_err = best_err
             if logfile:
@@ -397,18 +399,19 @@
                     del actives[j-ndeleted]
                     ndeleted+=1
             # add a candidate with slightly lower learning rate than best_active, starting from it
-            new_candidate = deepcopy(all_candidates[best_active])
-            new_a = len(all_candidates)
-            actives.append(new_a)
-            all_candidates.append(new_candidate)
-            all_results.append(all_results[best_active].copy())
-            all_last_err.append(best_last)
-            #all_slope.append(best_slope)
-            all_lr.append(all_lr[best_active]/lr_steps) # always try a smaller learning rate
-            all_start.append(t)
-            if logfile:
-                print >>logfile,"CREATE candidate ", new_a, " from ",best_active,"at epoch ",s," with lr=",all_lr[new_a]
-                logfile.flush()
+            new_lr=all_lr[best_active]/lr_steps # only try a smaller learning rate
+            if new_lr>=min_lr:
+                all_lr.append(new_lr)
+                new_candidate = deepcopy(all_candidates[best_active])
+                new_a = len(all_candidates)
+                actives.append(new_a)
+                all_candidates.append(new_candidate)
+                all_results.append(all_results[best_active].copy())
+                all_last_err.append(best_last)
+                all_start.append(t)
+                if logfile:
+                    print >>logfile,"CREATE candidate ", new_a, " from ",best_active,"at epoch ",s," with lr=",all_lr[new_a]
+                    logfile.flush()
     if return_best_model:
         final_model = best_candidate
     else:



From breuleux at mail.berlios.de  Tue Jun  5 00:38:53 2007
From: breuleux at mail.berlios.de (breuleux at BerliOS)
Date: Tue, 5 Jun 2007 00:38:53 +0200
Subject: [Plearn-commits] r7531 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706042238.l54McrcC032137@sheep.berlios.de>

Author: breuleux
Date: 2007-06-05 00:38:47 +0200 (Tue, 05 Jun 2007)
New Revision: 7531

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Added parallelism for multiple cores.

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 22:10:13 UTC (rev 7530)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 22:38:47 UTC (rev 7531)
@@ -1,15 +1,79 @@
 from math import *
 from numarray import *
 from plearn.bridge import *
+from threading import *
 
 # YET TO BE DOCUMENTED AND CLEANED UP CODE
 
 
+servers_lock = Lock() # Lock on the servers list so we don't have race conditions
+servers = [[serv, 0]] # List of [server, amount_of_jobs_server_is_running] lists
+servers_max = 1e10 # Maximal amount of servers we are willing to run
+
+
+def execute(object, tasks, use_threads = False):
+    def job(object):
+        for method, args in tasks: # do each task sequentially
+            getattr(object, method)(*args)
+        
+    if plearn.bridgemode.useserver and use_threads:
+        t = Thread(target = job, args = (object,))
+        t.start()
+        return t
+    else:
+        for method, args in tasks:
+            getattr(object, method)(*args)
+        return True
+
+def acquire_server(use_threads = False):
+    if not use_threads:
+        return serv
+    servers_lock.acquire()
+    min_load = 1e10
+    least_loaded = 0
+    nservers = len(servers)
+    for servinfo, i in zip(servers, xrange(nservers)):
+        server, njobs = servinfo
+        if not njobs:
+            servinfo[1] = 1
+            servers_lock.release()
+            return server
+        if njobs < min_load:
+            min_load = njobs
+            least_loaded = i
+    if nservers < servers_max:
+        command = 'plearn_curses server'
+        server = launch_plearn_server(command)
+        servers.append([server, 1])
+    else:
+        servinfo = servers[least_loaded]
+        server = servinfo[0]
+        servinfo[1] += 1
+    servers_lock.release()
+    return server
+
+def release_server(object, use_threads = False):
+    if plearn.bridgemode.useserver and use_threads:
+        server = object.server
+        servers_lock.acquire()
+        for servinfo in servers:
+            server_, njobs = servinfo
+            if server_ is server: # == doesn't work
+                servinfo[1] -= 1 # this server is done so we reduce its job count
+                break
+        servers_lock.release()
+
+def assign(object, use_threads = False):
+    server = acquire_server(use_threads)
+    o = server.new(object)
+    return o
+
+
 # ugly way to copy until done properly with PLearn's deepcopy
-def deepcopy(plearnobject):
+def deepcopy(plearnobject, use_threads = False):
     # actually not a deep-copy, only copy options
     if plearn.bridgemode.useserver:
-        o = serv.new(plearnobject.getObject())
+        o = assign(plearnobject.getObject(), use_threads)
     else:
         o = newObject(str(plearnobject))
     if o==None:
@@ -229,8 +293,12 @@
                       lr_steps=exp(log(10)/2), # Scaling coefficient when modifying learning rates
                       logfile=False,
                       min_lr=1e-6, # do not try to go below this learning rate
-                      keep_lr=2): # Learning rate interval for heuristic
+                      keep_lr=2, # Learning rate interval for heuristic
+                      use_threads=False):
 
+    if plearn.bridgemode.useserver:
+        servers[0][1] = 1 # learner
+
     min_epochs_to_delete = max(1,min_epochs_to_delete) # although 1 is probably too small
 
     def error_curve(active,start_t,current_t):
@@ -313,6 +381,9 @@
             print >>logfile, "At stage ", stage
         print "actives now: ",actives, " with lr=", array(all_lr)[actives]
         print >>logfile, "actives now: ",actives, " with lr=", array(all_lr)[actives]
+
+        threads = []
+        active_stats = []
         for active in actives:
             candidate = all_candidates[active]
             results = all_results[active]
@@ -326,16 +397,30 @@
                         options[lr_option]=str(learning_rates[t,s])
             candidate.changeOptions(options)
             candidate.setTrainingSet(trainset,False)
-            candidate.train()
+            tasks = [('train', ())]
+            stats = []
+            for j in range(0,n_tests):
+                ts = pl.VecStatsCollector()
+                if plearn.bridgemode.useserver:
+                    ts = candidate.server.new(ts)
+                stats.append(ts)
+                tasks.append(('test', (testsets[j], ts, 0, 0)))
+            active_stats.append(stats)
+            threads.append(execute(candidate, tasks, use_threads))
+
+        for thread in threads:
+            if isinstance(thread, Thread):
+                thread.join() # wait for all experiments to finish
+
+        for active, stats in zip(actives, active_stats):
+            candidate = all_candidates[active]
+            results = all_results[active]
+        
             results[t,0] = candidate.stage
             results[t,1] = all_lr[active]
             if logfile:
                 print >>logfile, "candidate ",active,":",
-            for j in range(0,n_tests):
-                ts = pl.VecStatsCollector()
-                if plearn.bridgemode.useserver:
-                    ts=serv.new(ts)
-                candidate.test(testsets[j],ts,0,0)
+            for j, ts in zip(range(0,n_tests), stats):
                 if logfile:
                     print >>logfile, " test" + str(j+1),": ",
                 for k in range(0,n_costs):
@@ -395,6 +480,7 @@
                 if a!=best_active and error_curve_dominates(best_active,a,t):
                     if logfile:
                         print >>logfile,"REMOVE candidate ",a
+                    release_server(all_candidates[a], use_threads)
                     all_candidates[a]=None # hopefully this destroys the candidate
                     del actives[j-ndeleted]
                     ndeleted+=1
@@ -402,7 +488,7 @@
             new_lr=all_lr[best_active]/lr_steps # only try a smaller learning rate
             if new_lr>=min_lr:
                 all_lr.append(new_lr)
-                new_candidate = deepcopy(all_candidates[best_active])
+                new_candidate = deepcopy(all_candidates[best_active], use_threads)
                 new_a = len(all_candidates)
                 actives.append(new_a)
                 all_candidates.append(new_candidate)



From yoshua at mail.berlios.de  Tue Jun  5 00:51:59 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 5 Jun 2007 00:51:59 +0200
Subject: [Plearn-commits] r7532 - in trunk/plearn_learners/online: .
	test/ModuleLearner
Message-ID: <200706042251.l54MpxHT012414@sheep.berlios.de>

Author: yoshua
Date: 2007-06-05 00:51:45 +0200 (Tue, 05 Jun 2007)
New Revision: 7532

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/TanhModule.cc
   trunk/plearn_learners/online/test/ModuleLearner/pytest.config
Log:
Changed sigmoid, softplus, tanh to fast versions, expect tests failing slightly 
because of small numerical differences


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-04 22:38:47 UTC (rev 7531)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-04 22:51:45 UTC (rev 7532)
@@ -114,7 +114,7 @@
         return;
 
     for( int i=0 ; i<size ; i++ )
-        expectation[i] = sigmoid( -activation[i] );
+        expectation[i] = fastsigmoid( -activation[i] );
 
     expectation_is_up_to_date = true;
 }
@@ -131,7 +131,7 @@
               && expectations.length() == batch_size );
     for (int k = 0; k < batch_size; k++)
         for (int i = 0 ; i < size ; i++)
-            expectations(k, i) = sigmoid(-activations(k, i));
+            expectations(k, i) = fastsigmoid(-activations(k, i));
 
     expectations_are_up_to_date = true;
 }
@@ -145,7 +145,7 @@
     output.resize( output_size );
 
     for( int i=0 ; i<size ; i++ )
-        output[i] = sigmoid( -input[i] - bias[i] );
+        output[i] = fastsigmoid( -input[i] - bias[i] );
 }
 
 void RBMBinomialLayer::fprop( const Mat& inputs, Mat& outputs ) const
@@ -156,7 +156,7 @@
 
     for( int k = 0; k < mbatch_size; k++ )
         for( int i = 0; i < size; i++ )
-            outputs(k,i) = sigmoid( -inputs(k,i) - bias[i] );
+            outputs(k,i) = fastsigmoid( -inputs(k,i) - bias[i] );
 }
 
 void RBMBinomialLayer::fprop( const Vec& input, const Vec& rbm_bias,
@@ -167,7 +167,7 @@
     output.resize( output_size );
 
     for( int i=0 ; i<size ; i++ )
-        output[i] = sigmoid( -input[i] - rbm_bias[i]);
+        output[i] = fastsigmoid( -input[i] - rbm_bias[i]);
 }
 
 /////////////////

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-04 22:38:47 UTC (rev 7531)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-04 22:51:45 UTC (rev 7532)
@@ -393,7 +393,7 @@
     {
         energy(i,0) = hidden_layer->energy(hidden(i));
         for (int j=0;j<visible_layer->size;j++)
-            energy(i,0) -= softplus(-visible_layer->activations(i,j));
+            energy(i,0) -= tabulated_softplus(-visible_layer->activations(i,j));
     }
 }
 
@@ -425,7 +425,7 @@
     {
         energy(i,0) = visible_layer->energy(visible(i));
         for (int j=0;j<hidden_layer->size;j++)
-            energy(i,0) -= softplus(-(*hidden_activations)(i,j));
+            energy(i,0) -= tabulated_softplus(-(*hidden_activations)(i,j));
     }
 }
 

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2007-06-04 22:38:47 UTC (rev 7531)
+++ trunk/plearn_learners/online/TanhModule.cc	2007-06-04 22:51:45 UTC (rev 7532)
@@ -75,7 +75,7 @@
     output.resize( output_size );
     for( int i=0 ; i<output_size ; i++ )
     {
-        output[i] = ex_scale * tanh( in_scale * input[i] );
+        output[i] = ex_scale * fasttanh( in_scale * input[i] );
     }
 }
 

Modified: trunk/plearn_learners/online/test/ModuleLearner/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/pytest.config	2007-06-04 22:38:47 UTC (rev 7531)
+++ trunk/plearn_learners/online/test/ModuleLearner/pytest.config	2007-06-04 22:51:45 UTC (rev 7532)
@@ -117,7 +117,7 @@
         ),
     arguments = "PL_ModuleLearner_TwoRBMs.plearn grad_lr=1e-2 cd_lr=1e-3 batch_size=11 nstages=1001",
     resources = [ "PL_ModuleLearner_TwoRBMs.plearn" ],
-    precision = 1e-06,
+    precision = 1e-03,
     pfileprg = "__program__",
     disabled = False
     )
@@ -132,7 +132,7 @@
         ),
     arguments = "PL_ModuleLearner_Greedy.pyplearn",
     resources = [ "PL_ModuleLearner_Greedy.pyplearn" ],
-    precision = 1e-06,
+    precision = 1e-03,
     pfileprg = "__program__",
     disabled = False
     )



From tihocan at mail.berlios.de  Tue Jun  5 02:43:36 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 5 Jun 2007 02:43:36 +0200
Subject: [Plearn-commits] r7533 - trunk/commands
Message-ID: <200706050043.l550haSx012542@sheep.berlios.de>

Author: tihocan
Date: 2007-06-05 02:43:34 +0200 (Tue, 05 Jun 2007)
New Revision: 7533

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Reorder by alphabetical order

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-06-04 22:51:45 UTC (rev 7532)
+++ trunk/commands/plearn_noblas_inc.h	2007-06-05 00:43:34 UTC (rev 7533)
@@ -203,6 +203,7 @@
 #include <plearn_learners/online/DeepBeliefNet.h>
 #include <plearn_learners/online/ForwardModule.h>
 #include <plearn_learners/online/GradNNetLayerModule.h>
+#include <plearn_learners/online/IdentityModule.h>
 #include <plearn_learners/online/LinearCombinationModule.h>
 #include <plearn_learners/online/MatrixModule.h>
 #include <plearn_learners/online/MaxSubsampling2DModule.h>
@@ -216,7 +217,6 @@
 #include <plearn_learners/online/ProcessInputCostModule.h>
 #include <plearn_learners/online/RBMBinomialLayer.h>
 #include <plearn_learners/online/RBMClassificationModule.h>
-#include <plearn_learners/online/IdentityModule.h>
 #include <plearn_learners/online/RBMConnection.h>
 #include <plearn_learners/online/RBMConv2DConnection.h>
 #include <plearn_learners/online/RBMGaussianLayer.h>



From yoshua at mail.berlios.de  Tue Jun  5 04:46:59 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 5 Jun 2007 04:46:59 +0200
Subject: [Plearn-commits] r7534 -
	trunk/plearn_learners/online/test/DeepBeliefNet
Message-ID: <200706050246.l552kx2u030080@sheep.berlios.de>

Author: yoshua
Date: 2007-06-05 04:46:59 +0200 (Tue, 05 Jun 2007)
New Revision: 7534

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
Log:
reduced precision to allow fast tanh


Modified: trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-06-05 00:43:34 UTC (rev 7533)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-06-05 02:46:59 UTC (rev 7534)
@@ -102,7 +102,7 @@
         ),
     arguments = "PL_DBN_Mini-batch.pyplearn",
     resources = [ "PL_DBN_Mini-batch.pyplearn" ],
-    precision = 1e-06,
+    precision = 1e-03,
     pfileprg = "__program__",
     disabled = False
     )
@@ -117,7 +117,7 @@
         ),
     arguments = "--enable-logging DeepBeliefNet,RBMClassificationModule SimpleRBM_test.pyplearn",
     resources = [ "SimpleRBM_test.pyplearn" ],
-    precision = 1e-06,
+    precision = 1e-03,
     pfileprg = "__program__",
     disabled = False
     )



From yoshua at mail.berlios.de  Tue Jun  5 15:44:33 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 5 Jun 2007 15:44:33 +0200
Subject: [Plearn-commits] r7535 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706051344.l55DiX5n000325@sheep.berlios.de>

Author: yoshua
Date: 2007-06-05 15:44:32 +0200 (Tue, 05 Jun 2007)
New Revision: 7535

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Fixed definition of server-mode-only variables to be done
only in server mode


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-05 02:46:59 UTC (rev 7534)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-05 13:44:32 UTC (rev 7535)
@@ -6,11 +6,11 @@
 # YET TO BE DOCUMENTED AND CLEANED UP CODE
 
 
-servers_lock = Lock() # Lock on the servers list so we don't have race conditions
-servers = [[serv, 0]] # List of [server, amount_of_jobs_server_is_running] lists
-servers_max = 1e10 # Maximal amount of servers we are willing to run
+if plearn.bridgemode.useserver:
+    servers_lock = Lock() # Lock on the servers list so we don't have race conditions
+    servers = [[serv, 0]] # List of [server, amount_of_jobs_server_is_running] lists
+    servers_max = 1e10 # Maximal amount of servers we are willing to run
 
-
 def execute(object, tasks, use_threads = False):
     def job(object):
         for method, args in tasks: # do each task sequentially



From yoshua at mail.berlios.de  Tue Jun  5 16:40:53 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 5 Jun 2007 16:40:53 +0200
Subject: [Plearn-commits] r7536 - in trunk/plearn_learners/online: .
	test/ModuleTester
Message-ID: <200706051440.l55Eerhp010265@sheep.berlios.de>

Author: yoshua
Date: 2007-06-05 16:40:52 +0200 (Tue, 05 Jun 2007)
New Revision: 7536

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/TanhModule.cc
   trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
Log:
added use_fast_approximations option


Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-06-05 13:44:32 UTC (rev 7535)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-06-05 14:40:52 UTC (rev 7536)
@@ -72,7 +72,8 @@
     input_size(-1),
     output_size(-1),
     name(the_name),
-    estimate_simpler_diag_hessian(false)
+    estimate_simpler_diag_hessian(false),
+    use_fast_approximations(true)
 {
     if (call_build_) {
         if (the_name.empty())
@@ -287,8 +288,12 @@
 
     declareOption(ol, "name", &OnlineLearningModule::name,
                   OptionBase::buildoption,
-        "Name of the module (if not provided, the class name is used).");
+                  "Name of the module (if not provided, the class name is used).");
 
+    declareOption(ol, "use_fast_approximations", &OnlineLearningModule::use_fast_approximations,
+                  OptionBase::buildoption,
+                  "Use tables to approximate nonlinearities such as sigmoid, tanh, and softplus\n");
+
     declareOption(ol, "estimate_simpler_diag_hessian",
                   &OnlineLearningModule::estimate_simpler_diag_hessian,
                   OptionBase::buildoption,

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-06-05 13:44:32 UTC (rev 7535)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-06-05 14:40:52 UTC (rev 7536)
@@ -94,6 +94,9 @@
     //! optional random generator, possibly shared among several modules
     PP<PRandom> random_gen;
 
+    //! use tables to approximate nonlinearities such as sigmoid, tanh, and softplus
+    bool use_fast_approximations;
+
 public:
     //#####  Public Member Fun ctions  #########################################
 

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-05 13:44:32 UTC (rev 7535)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-05 14:40:52 UTC (rev 7536)
@@ -113,8 +113,12 @@
     if( expectation_is_up_to_date )
         return;
 
-    for( int i=0 ; i<size ; i++ )
-        expectation[i] = fastsigmoid( -activation[i] );
+    if (use_fast_approximations)
+        for( int i=0 ; i<size ; i++ )
+            expectation[i] = fastsigmoid( -activation[i] );
+    else
+        for( int i=0 ; i<size ; i++ )
+            expectation[i] = sigmoid( -activation[i] );
 
     expectation_is_up_to_date = true;
 }
@@ -129,9 +133,14 @@
 
     PLASSERT( expectations.width() == size
               && expectations.length() == batch_size );
-    for (int k = 0; k < batch_size; k++)
-        for (int i = 0 ; i < size ; i++)
-            expectations(k, i) = fastsigmoid(-activations(k, i));
+    if (use_fast_approximations)
+        for (int k = 0; k < batch_size; k++)
+            for (int i = 0 ; i < size ; i++)
+                expectations(k, i) = fastsigmoid(-activations(k, i));
+    else
+        for (int k = 0; k < batch_size; k++)
+            for (int i = 0 ; i < size ; i++)
+                expectations(k, i) = sigmoid(-activations(k, i));
 
     expectations_are_up_to_date = true;
 }
@@ -144,8 +153,12 @@
     PLASSERT( input.size() == input_size );
     output.resize( output_size );
 
-    for( int i=0 ; i<size ; i++ )
-        output[i] = fastsigmoid( -input[i] - bias[i] );
+    if (use_fast_approximations)
+        for( int i=0 ; i<size ; i++ )
+            output[i] = fastsigmoid( -input[i] - bias[i] );
+    else
+        for( int i=0 ; i<size ; i++ )
+            output[i] = sigmoid( -input[i] - bias[i] );
 }
 
 void RBMBinomialLayer::fprop( const Mat& inputs, Mat& outputs ) const
@@ -154,9 +167,14 @@
     PLASSERT( inputs.width() == size );
     outputs.resize( mbatch_size, size );
 
-    for( int k = 0; k < mbatch_size; k++ )
-        for( int i = 0; i < size; i++ )
-            outputs(k,i) = fastsigmoid( -inputs(k,i) - bias[i] );
+    if (use_fast_approximations)
+        for( int k = 0; k < mbatch_size; k++ )
+            for( int i = 0; i < size; i++ )
+                outputs(k,i) = fastsigmoid( -inputs(k,i) - bias[i] );
+    else
+        for( int k = 0; k < mbatch_size; k++ )
+            for( int i = 0; i < size; i++ )
+                outputs(k,i) = sigmoid( -inputs(k,i) - bias[i] );
 }
 
 void RBMBinomialLayer::fprop( const Vec& input, const Vec& rbm_bias,
@@ -166,8 +184,12 @@
     PLASSERT( rbm_bias.size() == input_size );
     output.resize( output_size );
 
-    for( int i=0 ; i<size ; i++ )
-        output[i] = fastsigmoid( -input[i] - rbm_bias[i]);
+    if (use_fast_approximations)
+        for( int i=0 ; i<size ; i++ )
+            output[i] = fastsigmoid( -input[i] - rbm_bias[i]);
+    else
+        for( int i=0 ; i<size ; i++ )
+            output[i] = sigmoid( -input[i] - rbm_bias[i]);
 }
 
 /////////////////

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-05 13:44:32 UTC (rev 7535)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-05 14:40:52 UTC (rev 7536)
@@ -392,8 +392,12 @@
     for (int i=0;i<mbs;i++)
     {
         energy(i,0) = hidden_layer->energy(hidden(i));
-        for (int j=0;j<visible_layer->size;j++)
-            energy(i,0) -= tabulated_softplus(-visible_layer->activations(i,j));
+        if (use_fast_approximations)
+            for (int j=0;j<visible_layer->size;j++)
+                energy(i,0) -= tabulated_softplus(-visible_layer->activations(i,j));
+        else
+            for (int j=0;j<visible_layer->size;j++)
+                energy(i,0) -= softplus(-visible_layer->activations(i,j));
     }
 }
 
@@ -424,8 +428,12 @@
     for (int i=0;i<mbs;i++)
     {
         energy(i,0) = visible_layer->energy(visible(i));
-        for (int j=0;j<hidden_layer->size;j++)
-            energy(i,0) -= tabulated_softplus(-(*hidden_activations)(i,j));
+        if (use_fast_approximations)
+            for (int j=0;j<hidden_layer->size;j++)
+                energy(i,0) -= tabulated_softplus(-(*hidden_activations)(i,j));
+        else
+            for (int j=0;j<hidden_layer->size;j++)
+                energy(i,0) -= softplus(-(*hidden_activations)(i,j));
     }
 }
 

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2007-06-05 13:44:32 UTC (rev 7535)
+++ trunk/plearn_learners/online/TanhModule.cc	2007-06-05 14:40:52 UTC (rev 7536)
@@ -73,10 +73,12 @@
     }
 
     output.resize( output_size );
-    for( int i=0 ; i<output_size ; i++ )
-    {
-        output[i] = ex_scale * fasttanh( in_scale * input[i] );
-    }
+    if (use_fast_approximations)
+        for( int i=0 ; i<output_size ; i++ )
+            output[i] = ex_scale * fasttanh( in_scale * input[i] );
+    else
+        for( int i=0 ; i<output_size ; i++ )
+            output[i] = ex_scale * tanh( in_scale * input[i] );
 }
 
 void TanhModule::fprop(const Mat& inputs, Mat& outputs)

Modified: trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-06-05 13:44:32 UTC (rev 7535)
+++ trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-06-05 14:40:52 UTC (rev 7536)
@@ -8,10 +8,11 @@
     return pl.RBMModule(
             name = name,
             compute_contrastive_divergence = compute_cd,
-            cd_learning_rate = cd_learning_rate,
+            cd_learning_rate = cd_learning_rate, 
             grad_learning_rate = grad_learning_rate,
-            visible_layer = pl.RBMBinomialLayer(size = visible_size),
-            hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
+            use_fast_approximations=False,
+            visible_layer = pl.RBMBinomialLayer(size = visible_size, use_fast_approximations=False),
+            hidden_layer = pl.RBMBinomialLayer(size = hidden_size, use_fast_approximations=False),
             standard_cd_weights_grad = standard_cd_weights_grad,
             standard_cd_bias_grad = standard_cd_bias_grad,
             connection = pl.RBMMatrixConnection(



From nouiz at mail.berlios.de  Tue Jun  5 16:55:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 Jun 2007 16:55:53 +0200
Subject: [Plearn-commits] r7537 - trunk/plearn_learners/online
Message-ID: <200706051455.l55EtrWc011719@sheep.berlios.de>

Author: nouiz
Date: 2007-06-05 16:55:53 +0200 (Tue, 05 Jun 2007)
New Revision: 7537

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
Log:
use tabulated_softplus if use_fast_approximations is set to true


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-05 14:40:52 UTC (rev 7536)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-05 14:55:53 UTC (rev 7537)
@@ -329,22 +329,42 @@
 
     real ret = 0;
     real target_i, activation_i;
-    for( int i=0 ; i<size ; i++ )
-    {
-        target_i = target[i];
-        activation_i = activation[i];
-        if(!fast_exact_is_equal(target_i,0.0))
-            // nll -= target[i] * pl_log(expectations[i]); 
-            // but it is numerically unstable, so use instead
-            // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-            // but note that expectation = sigmoid(-activation)
-            ret += target_i * softplus(activation_i);
-        if(!fast_exact_is_equal(target_i,1.0))
-            // ret -= (1-target_i) * pl_log(1-expectation_i);
-            // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
-            //                         = log(1/(1+exp(x)))
-            //                         = -log(1+exp(x)) = -softplus(x)
-            ret += (1-target_i) * softplus(-activation_i);
+    if(use_fast_approximations){
+        for( int i=0 ; i<size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            if(!fast_exact_is_equal(target_i,0.0))
+                // nll -= target[i] * pl_log(expectations[i]); 
+                // but it is numerically unstable, so use instead
+                // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
+                // but note that expectation = sigmoid(-activation)
+                ret += target_i * tabulated_softplus(activation_i);
+            if(!fast_exact_is_equal(target_i,1.0))
+                // ret -= (1-target_i) * pl_log(1-expectation_i);
+                // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                //                         = log(1/(1+exp(x)))
+                //                         = -log(1+exp(x)) = -softplus(x)
+                ret += (1-target_i) * tabulated_softplus(-activation_i);
+        }
+    }else{
+        for( int i=0 ; i<size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            if(!fast_exact_is_equal(target_i,0.0))
+                // nll -= target[i] * pl_log(expectations[i]); 
+                // but it is numerically unstable, so use instead
+                // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
+                // but note that expectation = sigmoid(-activation)
+                ret += target_i * softplus(activation_i);
+            if(!fast_exact_is_equal(target_i,1.0))
+                // ret -= (1-target_i) * pl_log(1-expectation_i);
+                // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                //                         = log(1/(1+exp(x)))
+                //                         = -log(1+exp(x)) = -softplus(x)
+                ret += (1-target_i) * softplus(-activation_i);
+        }
     }
     return ret;
 }
@@ -363,22 +383,41 @@
         real nll = 0;
         real* activation = activations[k];
         real* target = targets[k];
-        for( int i=0 ; i<size ; i++ ) // loop over outputs
-        {
-            if(!fast_exact_is_equal(target[i],0.0))
-                // nll -= target[i] * pl_log(expectations[i]); 
-                // but it is numerically unstable, so use instead
-                // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                // but note that expectation = sigmoid(-activation)
-                nll += target[i] * softplus(activation[i]);
-            if(!fast_exact_is_equal(target[i],1.0))
-                // nll -= (1-target[i]) * pl_log(1-output[i]);
-                // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
-                //                         = log(1/(1+exp(x)))
-                //                         = -log(1+exp(x))
-                //                         = -softplus(x)
-                nll += (1-target[i]) * softplus(-activation[i]);
-
+        if(use_fast_approximations){
+            for( int i=0 ; i<size ; i++ ) // loop over outputs
+            {
+                if(!fast_exact_is_equal(target[i],0.0))
+                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // but it is numerically unstable, so use instead
+                    // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
+                    // but note that expectation = sigmoid(-activation)
+                    nll += target[i] * tabulated_softplus(activation[i]);
+                if(!fast_exact_is_equal(target[i],1.0))
+                    // nll -= (1-target[i]) * pl_log(1-output[i]);
+                    // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                    //                         = log(1/(1+exp(x)))
+                    //                         = -log(1+exp(x))
+                    //                         = -softplus(x)
+                    nll += (1-target[i]) * tabulated_softplus(-activation[i]);
+            }
+        }else{
+            for( int i=0 ; i<size ; i++ ) // loop over outputs
+            {
+                if(!fast_exact_is_equal(target[i],0.0))
+                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // but it is numerically unstable, so use instead
+                    // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
+                    // but note that expectation = sigmoid(-activation)
+                    nll += target[i] * softplus(activation[i]);
+                if(!fast_exact_is_equal(target[i],1.0))
+                    // nll -= (1-target[i]) * pl_log(1-output[i]);
+                    // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                    //                         = log(1/(1+exp(x)))
+                    //                         = -log(1+exp(x))
+                    //                         = -softplus(x)
+                    nll += (1-target[i]) * softplus(-activation[i]);
+                
+            }
         }
         costs_column(k,0) = nll;
     }



From simonl at mail.berlios.de  Tue Jun  5 18:04:16 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Tue, 5 Jun 2007 18:04:16 +0200
Subject: [Plearn-commits] r7538 - in trunk: plearn/display
	plearn_learners/generic/EXPERIMENTAL
	python_modules/plearn/plotting python_modules/plearn/var
Message-ID: <200706051604.l55G4GRc019463@sheep.berlios.de>

Author: simonl
Date: 2007-06-05 18:04:16 +0200 (Tue, 05 Jun 2007)
New Revision: 7538

Modified:
   trunk/plearn/display/DisplayUtils.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/var/Var.py
Log:
Added some options for displaying values of graphs in DisplayUtils.cc
improved deepReconstructorNet dans his friends netplot.py and Var.py


Modified: trunk/plearn/display/DisplayUtils.cc
===================================================================
--- trunk/plearn/display/DisplayUtils.cc	2007-06-05 14:55:53 UTC (rev 7537)
+++ trunk/plearn/display/DisplayUtils.cc	2007-06-05 16:04:16 UTC (rev 7538)
@@ -303,6 +303,14 @@
 }
 
 
+  //! returns a subvector made of the (max) n "central" values of v
+  Vec centerSubVec(Vec v, int n=16)
+  {
+    int l = v.length();
+    if(l<=n)
+      return v;    
+    return v.subVec((l-n)/2,n);
+  }
 
 /** VarGraph **/
 
@@ -478,7 +486,7 @@
       PStream str_descr = openString(descr, PStream::raw_ascii, "w");
       str_descr << v;
 
-      if(display_values && v->size() <= 16)
+      if(display_values)
         {
           gs.usefont("Times-Bold", 11.0);
           gs.centerShow(my_x, my_y+boxheight/4, descr.c_str());
@@ -487,14 +495,14 @@
           gs.usefont("Courrier", 6.0);
           if (v->rValue.length()>0) // print rvalue if there are some...
           {
-            gs.centerShow(my_x, my_y-boxheight/5, v->value);
-            gs.centerShow(my_x, my_y-boxheight/3, v->gradient);
-            gs.centerShow(my_x, my_y-boxheight/1, v->rValue);
+            gs.centerShow(my_x, my_y-boxheight/5, centerSubVec(v->value));
+            gs.centerShow(my_x, my_y-boxheight/3, centerSubVec(v->gradient));
+            gs.centerShow(my_x, my_y-boxheight/1, centerSubVec(v->rValue));
           }
           else
           {
-            gs.centerShow(my_x, my_y-boxheight/5, v->value);
-            gs.centerShow(my_x, my_y-boxheight/2.5, v->gradient);
+            gs.centerShow(my_x, my_y-boxheight/5, centerSubVec(v->value));
+            gs.centerShow(my_x, my_y-boxheight/2.5, centerSubVec(v->gradient));
           }
           /*
           cout << descr << " " << nameline << " (" << v->value.length() << ")" << endl;

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-05 14:55:53 UTC (rev 7537)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-05 16:04:16 UTC (rev 7538)
@@ -406,6 +406,8 @@
     VarArray proppath = propagationPath(layers[0],layers[nlayers-1]);
     layers[0]->matValue << input;
     proppath.fprop();
+    perr << "Graph for computing representations" << endl;
+    // displayVarGraph(proppath,true, 333, "repr");
     for(int k=0; k<nlayers; k++)
         representations[k] = layers[k]->matValue.copy();
     return representations;
@@ -417,7 +419,21 @@
     {
         VarArray proppath = propagationPath(layers[k],reconstructed_layers[k-1]);
         proppath.fprop();
-        reconstructed_layers[k-1]->matValue >> layers[k-1]->matValue;
+        perr << "Graph for reconstructing layer " << k-1 << " from layer " << k << endl;
+        //displayVarGraph(proppath,true, 333, "reconstr");
+
+        //WARNING MEGA-HACK
+        if (reconstructed_layers[k-1].width() == 2*layers[k-1].width())
+        {
+            Mat temp(layers[k-1].length(), layers[k-1].width());
+            for (int n=0; n < layers[k-1].length(); n++)
+                for (int i=0; i < layers[k-1].width(); i++)
+                    temp(n,i) = reconstructed_layers[k-1]->matValue(n,i*2);
+            temp >> layers[k-1]->matValue;
+        }        
+        //END OF MEGA-HACK
+        else
+            reconstructed_layers[k-1]->matValue >> layers[k-1]->matValue;
     }
 }
 
@@ -539,7 +555,7 @@
     perr << "*** each epoch has " << l << " examples and " << l/minibatch_size << " optimizer stages (updates)" << endl;
     Func f(layers[which_input_layer], reconstruction_costs[which_input_layer]);
     //displayVarGraph(reconstruction_costs[which_input_layer]);
-    // displayVarGraph(fproppath,true, 333, "ffpp", false);
+    //displayFunction(f,false,false, 333, "train_func");
     Var totalcost = sumOf(inputs, f, minibatch_size);
     VarArray params = totalcost->parents();
     reconstruction_optimizer->setToOptimize(params, totalcost);

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-06-05 14:55:53 UTC (rev 7537)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-06-05 16:04:16 UTC (rev 7538)
@@ -34,6 +34,8 @@
 
 def customColorBar(min, max, (x,y,width,height) = (0.9,0.1,0.1,0.8), nb_ticks = 50., color_map = defaultColorMap):
     axes((x, y, width,height))
+    if(min == max):
+        max=min + 1e-6
     cbarh = arange(min, max,  (max-min)/50.)
     cbar = vecToVerticalMatrix(cbarh)
     cbarh_str = []
@@ -88,7 +90,7 @@
             imshow(rowToMatrix(toPlusRow(row),width), interpolation="nearest", cmap = colorMap)
             setPlotParams(names[i%2] + "_" + str(i) + "_" + str(j), False, True)
 
-def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows_before = None):
+def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows = None):
 
     
     #some calculations for plotting
@@ -100,12 +102,15 @@
     mWidth = float(len(M[0]))
     mHeight = float(len(M))
     sbi = space_between_images
-    plotHeight = mHeight/mWidth*plotWidth
+    #plotHeight = mHeight/mWidth*plotWidth
+    plotHeight = mWidth/width/width*plotWidth
     cbw = .01 # color bar width
 
     colorMap = defaultColorMap
 
     mi,ma = findMinMax(M)
+    print 'min', mi
+    print 'max', ma
     
     ma = max(abs(mi),abs(ma))
     mi = -ma
@@ -119,11 +124,18 @@
     
         #normal
         row = M[i]
+        if apply_to_rows != None:
+            row = apply_to_rows(row)
         
+
+        print x,y,plotWidth, plotHeight
+        
         axes((x, y, plotWidth, plotHeight))
         imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap, vmin = mi, vmax = ma)
         setPlotParams('row_' + str(i), False, True)
 
+        
+
         x = x + plotWidth + sbi
         if x + plotWidth +cbw > 1:
             x = sbi
@@ -217,7 +229,7 @@
     '''
     colorMap = cm.gray
     nbMatrices = len(matrices)
-    print 'plotting ' + str(nbMatrices) + ' matrices'
+    #print 'plotting ' + str(nbMatrices) + ' matrices'
 
     totalWidth = 0
     maxHeight = 0
@@ -306,6 +318,8 @@
 
     #custom colorBar
     mi,ma = findMinMax(mat)
+    print 'min', mi
+    print 'max', ma
     
     ma = max(abs(mi),abs(ma))
     mi = -ma

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-06-05 14:55:53 UTC (rev 7537)
+++ trunk/python_modules/plearn/var/Var.py	2007-06-05 16:04:16 UTC (rev 7538)
@@ -219,11 +219,17 @@
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs):
+def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=""):
     W = Var(ow, iw, "uniform", -1./iw, 1./iw, varname=basename+'_W')
-    hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
     Wr = Var(ow, iw, "uniform", -1./ow, 1./ow, varname=basename+'_Wr')
-    log_reconstructed = hidden.matrixProduct(Wr).multiLogSoftMax(igs)
+    if add_bias:
+        b = Var(1,ow,"fill",0, varname=basename+'_b')
+        hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)
+        br = Var(1,iw,"fill",0, varname=basename+'_br')
+        log_reconstructed = hidden.matrixProduct(Wr).add(br).multiLogSoftMax(igs)
+    else:
+        hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
+        log_reconstructed = hidden.matrixProduct(Wr).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input



From tihocan at mail.berlios.de  Tue Jun  5 18:31:27 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 5 Jun 2007 18:31:27 +0200
Subject: [Plearn-commits] r7539 - trunk/plearn_learners/online
Message-ID: <200706051631.l55GVRC9023508@sheep.berlios.de>

Author: tihocan
Date: 2007-06-05 18:31:27 +0200 (Tue, 05 Jun 2007)
New Revision: 7539

Modified:
   trunk/plearn_learners/online/NetworkModule.cc
Log:
Added detection of infinite loops

Modified: trunk/plearn_learners/online/NetworkModule.cc
===================================================================
--- trunk/plearn_learners/online/NetworkModule.cc	2007-06-05 16:04:16 UTC (rev 7538)
+++ trunk/plearn_learners/online/NetworkModule.cc	2007-06-05 16:31:27 UTC (rev 7539)
@@ -484,6 +484,7 @@
     // A vector that stores the index of a module in the fprop path.
     TVec<int> module_index_to_path_index(all_modules.length(), -1);
     while (is_done.find(false) >= 0) {
+        bool at_least_one_done = false;
         for (int i = 0; i < all_modules.length(); i++) {
             if (!is_done[i] && inputs_needed[i] == 0) {
                 for (int j = 0; j < compute_input_of[i].length(); j++)
@@ -614,8 +615,21 @@
                 bprop_path[bprop_idx] = i;
 
                 is_done[i] = true;
+                at_least_one_done = true;
             }
         }
+        if (!at_least_one_done) {
+            // Infinite loop: the algorithm cannot find a module that can be
+            // propagated at this point.
+            string err;
+            for (int i = 0; i < all_modules.length(); i++)
+                if (!is_done[i])
+                    err += all_modules[i]->name + ", ";
+            PLERROR("In NetworkModule::build_ - Cannot build propagation path "
+                    "due to the following modules waiting for incoming "
+                    "connections (there may be an infinite loop): %s",
+                    err.c_str());
+        }
     }
     PLASSERT( module_index_to_path_index.find(-1) == -1 );
 }



From tihocan at mail.berlios.de  Tue Jun  5 19:07:05 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 5 Jun 2007 19:07:05 +0200
Subject: [Plearn-commits] r7540 - trunk/plearn/math
Message-ID: <200706051707.l55H75Kq009107@sheep.berlios.de>

Author: tihocan
Date: 2007-06-05 19:07:04 +0200 (Tue, 05 Jun 2007)
New Revision: 7540

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
Added 'setFieldNames' in declareMethod

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-06-05 16:31:27 UTC (rev 7539)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-06-05 17:07:04 UTC (rev 7540)
@@ -185,6 +185,13 @@
                  "registered with setFieldNames.\n"),
          RetDoc ("Requested statistic (a real number).")));
 
+    declareMethod(
+        rmm, "setFieldNames", &VecStatsCollector::setFieldNames,
+        (BodyDoc("Set field names.\n"),
+         ArgDoc ("fieldnames", 
+                 "A vector of strings corresponding to the names of each field"
+                 " in the VecStatsCollector.\n")));
+
 }
 
 int VecStatsCollector::length() const



From breuleux at mail.berlios.de  Tue Jun  5 21:51:00 2007
From: breuleux at mail.berlios.de (breuleux at BerliOS)
Date: Tue, 5 Jun 2007 21:51:00 +0200
Subject: [Plearn-commits] r7541 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706051951.l55Jp0Xw023877@sheep.berlios.de>

Author: breuleux
Date: 2007-06-05 21:51:00 +0200 (Tue, 05 Jun 2007)
New Revision: 7541

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Now uses plearn.bridgemode.server_exe

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-05 17:07:04 UTC (rev 7540)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-05 19:51:00 UTC (rev 7541)
@@ -42,7 +42,7 @@
             min_load = njobs
             least_loaded = i
     if nservers < servers_max:
-        command = 'plearn_curses server'
+        command = plearn.bridgemode.server_exe + ' server'
         server = launch_plearn_server(command)
         servers.append([server, 1])
     else:



From saintmlx at mail.berlios.de  Tue Jun  5 22:11:04 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 5 Jun 2007 22:11:04 +0200
Subject: [Plearn-commits] r7542 - in trunk: commands/PLearnCommands
	plearn/io python_modules/plearn python_modules/plearn/plide
	python_modules/plearn/pybridge python_modules/plearn/pyext
	python_modules/plearn/pyplearn python_modules/plearn/utilities
	python_modules/plearn/utilities/resources scripts
Message-ID: <200706052011.l55KB42H025280@sheep.berlios.de>

Author: saintmlx
Date: 2007-06-05 22:11:02 +0200 (Tue, 05 Jun 2007)
New Revision: 7542

Added:
   trunk/python_modules/plearn/utilities/options_dialog.py
   trunk/python_modules/plearn/utilities/resources/
   trunk/python_modules/plearn/utilities/resources/options_dialog.glade
Modified:
   trunk/commands/PLearnCommands/RunCommand.cc
   trunk/plearn/io/PyPLearnScript.cc
   trunk/plearn/io/PyPLearnScript.h
   trunk/python_modules/plearn/pl_pygtk.py
   trunk/python_modules/plearn/plide/plide_options.py
   trunk/python_modules/plearn/pybridge/test_embedded_code_snippet.py
   trunk/python_modules/plearn/pyext/__init__.py
   trunk/python_modules/plearn/pyplearn/plargs.py
   trunk/scripts/pyplearn_driver.py
Log:
- added '+gui' option to pyplearn and python scripts: graphically show plopts in all plnamespaces



Modified: trunk/commands/PLearnCommands/RunCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/RunCommand.cc	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/commands/PLearnCommands/RunCommand.cc	2007-06-05 20:11:02 UTC (rev 7542)
@@ -104,6 +104,9 @@
         if ( script == "" )
             return;    
 
+        PL_Log::instance().enableNamedLogging(pyplearn_script->module_names);
+        PL_Log::instance().verbosity(pyplearn_script->verbosity);
+
         in = openString( script, PStream::plearn_ascii );
     }
     else if(extension=="plearn")  // perform plearn macro expansion

Modified: trunk/plearn/io/PyPLearnScript.cc
===================================================================
--- trunk/plearn/io/PyPLearnScript.cc	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/plearn/io/PyPLearnScript.cc	2007-06-05 20:11:02 UTC (rev 7542)
@@ -139,6 +139,11 @@
             }
         }
     }
+
+    final_args.push_back(string("--named-logging=")
+                         + join(PL_Log::instance().namedLogging(), ","));
+    final_args.push_back(string("--verbosity=")
+                         + tostring(PL_Log::instance().verbosity()))    ;
 #ifdef WIN32
     Popen popen("python.exe", final_args);
 #else
@@ -198,7 +203,9 @@
     do_help(false),
     do_dump(false),
     metainfos(""),
-    expdir("")
+    expdir(""),
+    verbosity(PL_Log::instance().verbosity()),
+    module_names(PL_Log::instance().namedLogging())
 {}
   
 PLEARN_IMPLEMENT_OBJECT( PyPLearnScript,
@@ -211,6 +218,9 @@
 
 void PyPLearnScript::run()
 {
+    PL_Log::instance().enableNamedLogging(module_names);
+    PL_Log::instance().verbosity(verbosity);
+
     PStream in = openString( plearn_script, PStream::plearn_ascii );
 
     while ( in )
@@ -250,6 +260,14 @@
                    OptionBase::buildoption,
                    "The expdir of the experiment described by the script" );
 
+    declareOption( ol, "verbosity", &PyPLearnScript::verbosity,
+                   OptionBase::buildoption,
+                   "The verbosity level to set for this experiment" );
+
+    declareOption( ol, "module_names", &PyPLearnScript::module_names,
+                   OptionBase::buildoption,
+                   "Modules for which logging should be activated" );
+
   
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);

Modified: trunk/plearn/io/PyPLearnScript.h
===================================================================
--- trunk/plearn/io/PyPLearnScript.h	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/plearn/io/PyPLearnScript.h	2007-06-05 20:11:02 UTC (rev 7542)
@@ -47,6 +47,8 @@
 #include <plearn/base/Object.h>
 #include <plearn/io/PPath.h>
 #include <plearn/io/openString.h>
+#include <plearn/io/pl_log.h>
+#include <vector>
 
 namespace PLearn {
 
@@ -145,7 +147,13 @@
 
     //! The expdir of the experiment described by the script
     PPath expdir;
-  
+
+    //! The verbosity level to set for this experiment
+    int verbosity;
+
+    //! Modules for which logging should be activated
+    vector<string> module_names;
+
     // ****************
     // * Constructors *
     // ****************

Modified: trunk/python_modules/plearn/pl_pygtk.py
===================================================================
--- trunk/python_modules/plearn/pl_pygtk.py	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/python_modules/plearn/pl_pygtk.py	2007-06-05 20:11:02 UTC (rev 7542)
@@ -175,6 +175,8 @@
     def destroy(self):
         self.w_root.destroy()
 
+    def set_title(self, title):
+        self.w_root.set_title(title)
 
 #####  MessageBox (a la Windows)  ###########################################
 

Modified: trunk/python_modules/plearn/plide/plide_options.py
===================================================================
--- trunk/python_modules/plearn/plide/plide_options.py	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/python_modules/plearn/plide/plide_options.py	2007-06-05 20:11:02 UTC (rev 7542)
@@ -33,7 +33,8 @@
 
 from plearn.utilities.metaprog import public_members
 #from plearn.pyplearn.pyplearn  import *
-from plearn.pyplearn           import *
+#from plearn.pyplearn           import *
+from plearn.pyplearn           import plargs
 from plearn.utilities.toolkit  import doc as toolkit_doc
 
 from plearn.pl_pygtk import GladeAppWindow, GladeDialog, MessageBox
@@ -67,13 +68,13 @@
         return self.group_object.is_plopt(option_name)
 
     def get_plopt( self, option_name ):
-        return self.group_object.get_plopt(option_name)
+        return self.group_object.getPlopt(option_name)
 
     def get( self, option_name ):
         return getattr(self.group_object, option_name)
 
     def set( self, option_name, option_value ):
-        if self.group_object.is_plopt(option_name):
+        if True or self.group_object.is_plopt(option_name):
             setattr(self.group_object, option_name, option_value)
         else:
             setattr(self.group_object, option_name,
@@ -88,7 +89,10 @@
     to initialize the PyPLearnOptionsDialog, which provides the view and
     controller for this object.
     """
-    def __init__( self, script_name, script_code, script_directory ):
+    verbosity_map = { 0:0, 1:1, 2:5, 3:10, 4:500 }
+    inv_verb_map= dict([(y,x) for x,y in verbosity_map.iteritems()])
+    
+    def __init__( self, script_name, script_code, script_directory, namespaces= ['__ALL__'] ):
         self.script_name      = script_name
         self.script_code      = script_code
         self.launch_directory = script_directory
@@ -97,8 +101,8 @@
         self.log_enable       = [ "__NONE__" ]     # List of named logs to enable
         self.option_overrides = [ ]                # Manual overrides
         self.option_groups    = [ ]                # List of groups of script options
+        self.namespaces= namespaces
         
-
         ## Assume that the script has already been executed and is
         ## syntactically valid.  (More formal execution context passing for
         ## subclasses of plargs_namespace will come later)
@@ -108,14 +112,15 @@
         #     'GlobalOptions', public_members(plarg_defaults),
         #     plarg_defaults, 'Global Configuration Options'))
 
-        ## Look at all options in plargs_namespace
-        #for clsname,cls in plargs_namespace._subclasses.iteritems():
-        for clsname,cls in plnamespace._subclasses.iteritems():
-            short_doc = toolkit_doc(cls, 0).rstrip('.')
-            full_doc  = toolkit_doc(cls, 1, "\n")
-            group = PyPLearnOptionsGroup(clsname, public_members(cls).keys(),
-                                         cls, short_doc, full_doc)
-            self.option_groups.append(group)
+        ## Look at all options in plnamespace
+        for cls in plargs.getNamespaces():
+            clsname= cls.__name__
+            if clsname in namespaces or '__ALL__' in namespaces:
+                short_doc = toolkit_doc(cls, 0).rstrip('.')
+                full_doc  = toolkit_doc(cls, 1, "\n")
+                group = PyPLearnOptionsGroup(clsname, public_members(cls).keys(),
+                                             cls, short_doc, full_doc)
+                self.option_groups.append(group)
 
     def pyplearn_actualize( self ):
         """Since the values of the fields in the options dialog box have
@@ -130,8 +135,7 @@
         if self.option_overrides:
             plargs.parse(self.option_overrides)
 
-        verbosity_map = { 0:0, 1:1, 2:5, 3:10, 4:500 }
-        verbosity = verbosity_map.get(self.log_verbosity, 5)
+        verbosity = self.verbosity_map.get(self.log_verbosity, 5)
         injected.loggingControl(verbosity, self.log_enable)
 
 
@@ -144,7 +148,8 @@
     def __init__( self, options_holder ):
         GladeDialog.__init__(self, gladeFile())
         self.options_holder = options_holder
-
+        self.set_title('Script Options ['+options_holder.script_name+']')
+        
         ## Fill out the first page of notebook
         self.w_launch_directory.set_text(options_holder.launch_directory)
         for v in options_holder.log_verbosities:
@@ -166,7 +171,7 @@
 
             ## The body of the notebook page is contained in a scrolled window
             scrolled = gtk.ScrolledWindow()
-            scrolled.set_policy(gtk.POLICY_NEVER, gtk.POLICY_AUTOMATIC)
+            scrolled.set_policy(gtk.POLICY_AUTOMATIC, gtk.POLICY_AUTOMATIC)
             scrolled.set_shadow_type(gtk.SHADOW_NONE)
             align = gtk.Alignment(xalign=0.0, yalign=0.0, xscale=1.0, yscale=1.0)
             align.set_padding(padding_top=8, padding_bottom=6,
@@ -199,6 +204,10 @@
             option_names.sort()
             table = gtk.Table(rows=len(option_names), columns=3, homogeneous=False)
             for i in xrange(len(option_names)):
+                option_object     = group.get_plopt(option_names[i])
+                if not option_object.getGui():
+                    continue
+
                 ## Option name
                 option_label = gtk.Label(option_names[i])
                 option_label.set_alignment(1.0, 0.5)   # Right align and middle-align
@@ -215,14 +224,14 @@
                 ## str      Combobox if multiple choices
                 ## str      Text entry field for unconstrained string
                 ## list     Text entry field
-                if group.is_plopt(option_names[i]):
-                    option_object     = group.get_plopt(option_names[i])
+                if True or group.is_plopt(option_names[i]):
+                    #option_object     = group.get_plopt(option_names[i])
                     option_value      = group.get(option_names[i])
-                    option_type       = option_object.get_type()
-                    option_docstr     = option_object.__doc__
-                    option_bounds     = option_object.get_bounds()
-                    option_choices    = option_object.get_choices()
-                    option_fr_choices = option_object.get_free_choices()
+                    option_type       = option_object.getType()
+                    option_docstr     = option_object._doc
+                    option_bounds     = option_object.getBounds()
+                    option_choices    = option_object.getChoices()
+                    option_fr_choices = option_object.getFreeChoices()
                 else:
                     option_value      = group.get(option_names[i])
                     option_type       = type(option_value)
@@ -304,7 +313,7 @@
                 option_input.show()
                 table.attach(option_input, left_attach=1, right_attach=2,
                              top_attach=i, bottom_attach=i+1,
-                             xoptions=gtk.EXPAND | gtk.FILL, yoptions=gtk.EXPAND | gtk.FILL,
+                             xoptions=gtk.EXPAND | gtk.FILL, yoptions=gtk.EXPAND ,
                              xpadding=4, ypadding=0)
 
                 ## Remember how to map the widget's contents into options
@@ -318,7 +327,7 @@
                 option_doc.show()
                 table.attach(option_doc, left_attach=2, right_attach=3,
                              top_attach=i, bottom_attach=i+1,
-                             xoptions=gtk.EXPAND | gtk.FILL, yoptions=gtk.EXPAND | gtk.FILL,
+                             xoptions=gtk.EXPAND | gtk.FILL , yoptions=gtk.EXPAND | gtk.FILL,
                              xpadding=4, ypadding=0)
 
             table.set_row_spacings(8)
@@ -344,7 +353,8 @@
         """
         for (widget_getter, group, option_name) in self.widget_map:
             value = widget_getter()
-            group.set(option_name, value)
+            if value!='None':
+                group.set(option_name, value)
 
         ## Updates from the first page of the dialog
         self.options_holder.launch_directory = self.w_launch_directory.get_text()

Modified: trunk/python_modules/plearn/pybridge/test_embedded_code_snippet.py
===================================================================
--- trunk/python_modules/plearn/pybridge/test_embedded_code_snippet.py	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/python_modules/plearn/pybridge/test_embedded_code_snippet.py	2007-06-05 20:11:02 UTC (rev 7542)
@@ -87,4 +87,8 @@
         gc.collect()
         return gc.get_referrers(self.recTest)
 
-pl_embedded_code_snippet_type= Machin
+class Truc(Machin, Bidule):
+    pass
+    
+
+pl_embedded_code_snippet_type= Truc

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/python_modules/plearn/pyext/__init__.py	2007-06-05 20:11:02 UTC (rev 7542)
@@ -78,11 +78,22 @@
         return [ content[i*ncols:(i+1)*ncols] for i in range(nrows) ]
 
 
+from plearn.utilities.options_dialog import *
+verb, logs, namespaces, use_gui= getGuiInfo(sys.argv)
+
 # Enact the use of plargs: the current behavior is to consider as a plargs
 # any command-line argument that contains a '=' char and to neglect all
 # others
 plargs.parse([ arg for arg in sys.argv if arg.find('=') != -1 ])
 
+if use_gui:
+    runit, verb, logs= optionsDialog(sys.argv[0], plargs.expdir,
+                                     verb, logs, namespaces)
+    if not runit:
+        sys.exit()
+    loggingControl(verb, logs)
+
+
 if __name__ == "__main__":
     class A(plargs):
         T = plopt(0)

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-05 20:11:02 UTC (rev 7542)
@@ -299,6 +299,7 @@
     def __init__(self, value, **kwargs):
         self._name  = kwargs.pop("name", self.unnamed)
         self._doc   = kwargs.pop("doc", '')
+        self._gui   = kwargs.pop("gui", True)
 
         # type: This keyword can and must only be used when the default
         # value is None. Otherwise, it is infered using 'type(value)'.
@@ -416,6 +417,9 @@
     def getType(self):
         return self._type
 
+    def getGui(self):
+        return self._gui
+
     def reset(self):
         """Simply deletes any override for this plopt in the current context.
         

Added: trunk/python_modules/plearn/utilities/options_dialog.py
===================================================================
--- trunk/python_modules/plearn/utilities/options_dialog.py	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/python_modules/plearn/utilities/options_dialog.py	2007-06-05 20:11:02 UTC (rev 7542)
@@ -0,0 +1,113 @@
+#
+# options_dialog
+# Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+from plearn.plide.plide_options import *
+import sys, os
+
+def getVerbosity(args):
+    """
+    get verbosity from PLearn
+    """
+    verb_pos= -1
+    for i in range(len(args)):
+        if args[i].startswith('--verbosity='):
+            verb_pos= i
+    verb= 5
+    if verb_pos >= 0:
+        verb= int(args[verb_pos].split('=')[1])
+    return verb, verb_pos
+
+def getModulesToLog(args):
+    """
+    get modules to log
+    """
+    logs_pos= -1
+    for i in range(len(args)):
+        if args[i].startswith('--named-logging='):
+            logs_pos= i
+    logs= ['__NONE__']
+    if logs_pos >= 0:
+        logs= args[logs_pos].split('=')[1].split(',')
+    return logs, logs_pos
+
+def getGuiNamespaces(args):
+    gui_pos= -1
+    # get +gui option
+    for i in range(len(args)):
+        if args[i].startswith('+gui'):
+            gui_pos= i
+    gui_namespaces= ['__ALL__']
+    if gui_pos >= 0:
+        x= args[gui_pos].split(':')
+        if len(x)>1:
+            gui_namespaces= x[1].split(',')
+    return gui_namespaces, gui_pos
+
+def getGuiInfo(args):
+    """
+    get verbosity, modules to log and
+    namespaces from command line
+    """
+    verb, pos= getVerbosity(args)
+    if pos >= 0: del args[pos]
+    logs, pos= getModulesToLog(args)
+    if pos >= 0: del args[pos]
+    namespaces, pos= getGuiNamespaces(args)
+    if pos >= 0: del args[pos]
+    return verb, logs, namespaces, pos>=0
+
+def gladeFile():
+    import plearn.plide.plide
+    return os.path.join(os.path.dirname(plearn.utilities.options_dialog.__file__),
+                        "resources", "options_dialog.glade")
+
+def optionsDialog(name, expdir, verbosity, named_logging, namespaces):
+    """
+    pop a dialog showing all plnamespaces
+    """
+    PyPLearnOptionsDialog.define_injected(None, gladeFile)
+    options_holder= PyPLearnOptionsHolder(name, None, expdir, namespaces)
+    options_holder.log_enable= named_logging
+    ks= options_holder.inv_verb_map.keys()
+    ks.sort()
+    for k in ks:
+        if k <= verbosity:
+            options_holder.log_verbosity= options_holder.inv_verb_map[k]
+    options_dialog= PyPLearnOptionsDialog(options_holder)
+    result= options_dialog.run()
+    if result == gtk.RESPONSE_OK:
+        options_dialog.update_options_holder()
+    options_dialog.destroy()
+    #plargs.expdir= options_holder.launch_directory
+    return result == gtk.RESPONSE_OK, \
+           options_holder.verbosity_map.get(options_holder.log_verbosity, 5), \
+           options_holder.log_enable

Added: trunk/python_modules/plearn/utilities/resources/options_dialog.glade
===================================================================
--- trunk/python_modules/plearn/utilities/resources/options_dialog.glade	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/python_modules/plearn/utilities/resources/options_dialog.glade	2007-06-05 20:11:02 UTC (rev 7542)
@@ -0,0 +1,480 @@
+<?xml version="1.0" standalone="no"?> <!--*- mode: xml -*-->
+<!DOCTYPE glade-interface SYSTEM "http://glade.gnome.org/glade-2.0.dtd">
+
+<glade-interface>
+
+<widget class="GtkDialog" id="PyPLearnOptionsDialog">
+  <property name="visible">True</property>
+  <property name="title" translatable="yes">Script Options</property>
+  <property name="type">GTK_WINDOW_TOPLEVEL</property>
+  <property name="window_position">GTK_WIN_POS_NONE</property>
+  <property name="modal">True</property>
+  <property name="default_width">700</property>
+  <property name="default_height">480</property>
+  <property name="resizable">True</property>
+  <property name="destroy_with_parent">False</property>
+  <property name="decorated">True</property>
+  <property name="skip_taskbar_hint">False</property>
+  <property name="skip_pager_hint">False</property>
+  <property name="type_hint">GDK_WINDOW_TYPE_HINT_DIALOG</property>
+  <property name="gravity">GDK_GRAVITY_NORTH_WEST</property>
+  <property name="focus_on_map">True</property>
+  <property name="urgency_hint">False</property>
+  <property name="has_separator">True</property>
+
+  <child internal-child="vbox">
+    <widget class="GtkVBox" id="dialog_vbox1">
+      <property name="visible">True</property>
+      <property name="homogeneous">False</property>
+      <property name="spacing">0</property>
+
+      <child internal-child="action_area">
+	<widget class="GtkHButtonBox" id="dialog_action_area1">
+	  <property name="visible">True</property>
+	  <property name="layout_style">GTK_BUTTONBOX_END</property>
+
+	  <child>
+	    <widget class="GtkButton" id="button3">
+	      <property name="visible">True</property>
+	      <property name="can_default">True</property>
+	      <property name="can_focus">True</property>
+	      <property name="label">gtk-cancel</property>
+	      <property name="use_stock">True</property>
+	      <property name="relief">GTK_RELIEF_NORMAL</property>
+	      <property name="focus_on_click">True</property>
+	      <property name="response_id">-6</property>
+	    </widget>
+	  </child>
+
+	  <child>
+	    <widget class="GtkButton" id="button4">
+	      <property name="visible">True</property>
+	      <property name="can_default">True</property>
+	      <property name="can_focus">True</property>
+	      <property name="label">gtk-ok</property>
+	      <property name="use_stock">True</property>
+	      <property name="relief">GTK_RELIEF_NORMAL</property>
+	      <property name="focus_on_click">True</property>
+	      <property name="response_id">-5</property>
+	    </widget>
+	  </child>
+	</widget>
+	<packing>
+	  <property name="padding">0</property>
+	  <property name="expand">False</property>
+	  <property name="fill">True</property>
+	  <property name="pack_type">GTK_PACK_END</property>
+	</packing>
+      </child>
+
+      <child>
+	<widget class="GtkVBox" id="options_vbox">
+	  <property name="visible">True</property>
+	  <property name="homogeneous">False</property>
+	  <property name="spacing">8</property>
+
+	  <child>
+	    <widget class="GtkAlignment" id="alignment1">
+	      <property name="visible">True</property>
+	      <property name="xalign">0.5</property>
+	      <property name="yalign">0.5</property>
+	      <property name="xscale">1</property>
+	      <property name="yscale">1</property>
+	      <property name="top_padding">4</property>
+	      <property name="bottom_padding">4</property>
+	      <property name="left_padding">4</property>
+	      <property name="right_padding">4</property>
+
+	      <child>
+		<widget class="GtkNotebook" id="option_groups">
+		  <property name="visible">True</property>
+		  <property name="can_focus">True</property>
+		  <property name="show_tabs">True</property>
+		  <property name="show_border">True</property>
+		  <property name="tab_pos">GTK_POS_TOP</property>
+		  <property name="scrollable">True</property>
+		  <property name="enable_popup">False</property>
+
+		  <child>
+		    <widget class="GtkScrolledWindow" id="scrolledwindow1">
+		      <property name="visible">True</property>
+		      <property name="can_focus">True</property>
+		      <property name="hscrollbar_policy">GTK_POLICY_AUTOMATIC</property>
+		      <property name="vscrollbar_policy">GTK_POLICY_AUTOMATIC</property>
+		      <property name="shadow_type">GTK_SHADOW_NONE</property>
+		      <property name="window_placement">GTK_CORNER_TOP_LEFT</property>
+
+		      <child>
+			<widget class="GtkViewport" id="viewport5">
+			  <property name="visible">True</property>
+			  <property name="shadow_type">GTK_SHADOW_NONE</property>
+
+			  <child>
+			    <widget class="GtkAlignment" id="alignment4">
+			      <property name="visible">True</property>
+			      <property name="xalign">0.5</property>
+			      <property name="yalign">0.5</property>
+			      <property name="xscale">1</property>
+			      <property name="yscale">1</property>
+			      <property name="top_padding">6</property>
+			      <property name="bottom_padding">4</property>
+			      <property name="left_padding">6</property>
+			      <property name="right_padding">6</property>
+
+			      <child>
+				<widget class="GtkTable" id="table1">
+				  <property name="visible">True</property>
+				  <property name="n_rows">3</property>
+				  <property name="n_columns">2</property>
+				  <property name="homogeneous">False</property>
+				  <property name="row_spacing">5</property>
+				  <property name="column_spacing">5</property>
+
+				  <child>
+				    <widget class="GtkLabel" id="label5">
+				      <property name="visible">True</property>
+				      <property name="label" translatable="yes">PLearn logs verbosity</property>
+				      <property name="use_underline">False</property>
+				      <property name="use_markup">True</property>
+				      <property name="justify">GTK_JUSTIFY_RIGHT</property>
+				      <property name="wrap">False</property>
+				      <property name="selectable">False</property>
+				      <property name="xalign">1</property>
+				      <property name="yalign">0.5</property>
+				      <property name="xpad">0</property>
+				      <property name="ypad">0</property>
+				      <property name="ellipsize">PANGO_ELLIPSIZE_NONE</property>
+				      <property name="width_chars">-1</property>
+				      <property name="single_line_mode">False</property>
+				      <property name="angle">0</property>
+				    </widget>
+				    <packing>
+				      <property name="left_attach">0</property>
+				      <property name="right_attach">1</property>
+				      <property name="top_attach">1</property>
+				      <property name="bottom_attach">2</property>
+				      <property name="x_options">fill</property>
+				      <property name="y_options"></property>
+				    </packing>
+				  </child>
+
+				  <child>
+				    <widget class="GtkLabel" id="label6">
+				      <property name="visible">True</property>
+				      <property name="label" translatable="yes">Named logs to activate</property>
+				      <property name="use_underline">False</property>
+				      <property name="use_markup">True</property>
+				      <property name="justify">GTK_JUSTIFY_RIGHT</property>
+				      <property name="wrap">False</property>
+				      <property name="selectable">False</property>
+				      <property name="xalign">1</property>
+				      <property name="yalign">0.5</property>
+				      <property name="xpad">0</property>
+				      <property name="ypad">0</property>
+				      <property name="ellipsize">PANGO_ELLIPSIZE_NONE</property>
+				      <property name="width_chars">-1</property>
+				      <property name="single_line_mode">False</property>
+				      <property name="angle">0</property>
+				    </widget>
+				    <packing>
+				      <property name="left_attach">0</property>
+				      <property name="right_attach">1</property>
+				      <property name="top_attach">2</property>
+				      <property name="bottom_attach">3</property>
+				      <property name="x_options">fill</property>
+				      <property name="y_options"></property>
+				    </packing>
+				  </child>
+
+				  <child>
+				    <widget class="GtkComboBox" id="plearn_log_verbosity">
+				      <property name="visible">True</property>
+				      <property name="items" translatable="yes"></property>
+				      <property name="add_tearoffs">False</property>
+				      <property name="focus_on_click">True</property>
+				    </widget>
+				    <packing>
+				      <property name="left_attach">1</property>
+				      <property name="right_attach">2</property>
+				      <property name="top_attach">1</property>
+				      <property name="bottom_attach">2</property>
+				      <property name="x_options">fill</property>
+				      <property name="y_options">fill</property>
+				    </packing>
+				  </child>
+
+				  <child>
+				    <widget class="GtkEntry" id="named_logs_activate">
+				      <property name="visible">True</property>
+				      <property name="can_focus">True</property>
+				      <property name="editable">True</property>
+				      <property name="visibility">True</property>
+				      <property name="max_length">0</property>
+				      <property name="text" translatable="yes"></property>
+				      <property name="has_frame">True</property>
+				      <property name="invisible_char">*</property>
+				      <property name="activates_default">False</property>
+				    </widget>
+				    <packing>
+				      <property name="left_attach">1</property>
+				      <property name="right_attach">2</property>
+				      <property name="top_attach">2</property>
+				      <property name="bottom_attach">3</property>
+				      <property name="y_options"></property>
+				    </packing>
+				  </child>
+
+				  <child>
+				    <widget class="GtkHBox" id="hbox2">
+				      <property name="visible">True</property>
+				      <property name="homogeneous">False</property>
+				      <property name="spacing">2</property>
+
+				      <child>
+					<widget class="GtkEntry" id="launch_directory">
+					  <property name="visible">True</property>
+					  <property name="can_focus">True</property>
+					  <property name="editable">False</property>
+					  <property name="visibility">True</property>
+					  <property name="max_length">0</property>
+					  <property name="text" translatable="yes"></property>
+					  <property name="has_frame">False</property>
+					  <property name="invisible_char">*</property>
+					  <property name="activates_default">False</property>
+					</widget>
+					<packing>
+					  <property name="padding">0</property>
+					  <property name="expand">True</property>
+					  <property name="fill">True</property>
+					</packing>
+				      </child>
+
+				      <child>
+					<widget class="GtkButton" id="pick_directory">
+					  <property name="sensitive">False</property>
+					  <property name="relief">GTK_RELIEF_NORMAL</property>
+					  <property name="focus_on_click">True</property>
+					  <signal name="clicked" handler="on_pick_directory_clicked" last_modification_time="Thu, 27 Apr 2006 17:54:45 GMT"/>
+
+					  <child>
+					    <widget class="GtkImage" id="image36">
+					      <property name="visible">True</property>
+					      <property name="stock">gtk-directory</property>
+					      <property name="icon_size">4</property>
+					      <property name="xalign">0.5</property>
+					      <property name="yalign">0.5</property>
+					      <property name="xpad">0</property>
+					      <property name="ypad">0</property>
+					    </widget>
+					  </child>
+					</widget>
+					<packing>
+					  <property name="padding">0</property>
+					  <property name="expand">False</property>
+					  <property name="fill">False</property>
+					</packing>
+				      </child>
+				    </widget>
+				    <packing>
+				      <property name="left_attach">1</property>
+				      <property name="right_attach">2</property>
+				      <property name="top_attach">0</property>
+				      <property name="bottom_attach">1</property>
+				      <property name="x_options">fill</property>
+				      <property name="y_options">fill</property>
+				    </packing>
+				  </child>
+
+				  <child>
+				    <widget class="GtkLabel" id="label8">
+				      <property name="visible">True</property>
+				      <property name="label" translatable="yes">Experiment directory [read-only]</property>
+				      <property name="use_underline">False</property>
+				      <property name="use_markup">False</property>
+				      <property name="justify">GTK_JUSTIFY_RIGHT</property>
+				      <property name="wrap">False</property>
+				      <property name="selectable">False</property>
+				      <property name="xalign">0</property>
+				      <property name="yalign">0.5</property>
+				      <property name="xpad">0</property>
+				      <property name="ypad">0</property>
+				      <property name="ellipsize">PANGO_ELLIPSIZE_NONE</property>
+				      <property name="width_chars">-1</property>
+				      <property name="single_line_mode">False</property>
+				      <property name="angle">0</property>
+				    </widget>
+				    <packing>
+				      <property name="left_attach">0</property>
+				      <property name="right_attach">1</property>
+				      <property name="top_attach">0</property>
+				      <property name="bottom_attach">1</property>
+				      <property name="x_options">fill</property>
+				      <property name="y_options"></property>
+				    </packing>
+				  </child>
+				</widget>
+			      </child>
+			    </widget>
+			  </child>
+			</widget>
+		      </child>
+		    </widget>
+		    <packing>
+		      <property name="tab_expand">False</property>
+		      <property name="tab_fill">True</property>
+		    </packing>
+		  </child>
+
+		  <child>
+		    <widget class="GtkLabel" id="label3">
+		      <property name="visible">True</property>
+		      <property name="label" translatable="yes">&lt;b&gt;General Settings&lt;/b&gt;</property>
+		      <property name="use_underline">False</property>
+		      <property name="use_markup">True</property>
+		      <property name="justify">GTK_JUSTIFY_LEFT</property>
+		      <property name="wrap">False</property>
+		      <property name="selectable">False</property>
+		      <property name="xalign">0</property>
+		      <property name="yalign">0.5</property>
+		      <property name="xpad">2</property>
+		      <property name="ypad">2</property>
+		      <property name="ellipsize">PANGO_ELLIPSIZE_NONE</property>
+		      <property name="width_chars">-1</property>
+		      <property name="single_line_mode">False</property>
+		      <property name="angle">0</property>
+		    </widget>
+		    <packing>
+		      <property name="type">tab</property>
+		    </packing>
+		  </child>
+
+		  <child>
+		    <widget class="GtkAlignment" id="alignment2">
+		      <property name="visible">True</property>
+		      <property name="xalign">0.5</property>
+		      <property name="yalign">0.5</property>
+		      <property name="xscale">1</property>
+		      <property name="yscale">1</property>
+		      <property name="top_padding">8</property>
+		      <property name="bottom_padding">6</property>
+		      <property name="left_padding">8</property>
+		      <property name="right_padding">6</property>
+
+		      <child>
+			<widget class="GtkVBox" id="manual_script_options_vbox">
+			  <property name="visible">True</property>
+			  <property name="homogeneous">False</property>
+			  <property name="spacing">4</property>
+
+			  <child>
+			    <widget class="GtkLabel" id="manual_script_options_doc">
+			      <property name="visible">True</property>
+			      <property name="label" translatable="yes">Enter manual script options of the form &lt;i&gt;&lt;b&gt;Option1=Value1 Option2=Value2 ...&lt;/b&gt;&lt;/i&gt;</property>
+			      <property name="use_underline">False</property>
+			      <property name="use_markup">True</property>
+			      <property name="justify">GTK_JUSTIFY_LEFT</property>
+			      <property name="wrap">False</property>
+			      <property name="selectable">False</property>
+			      <property name="xalign">0</property>
+			      <property name="yalign">0.5</property>
+			      <property name="xpad">2</property>
+			      <property name="ypad">2</property>
+			      <property name="ellipsize">PANGO_ELLIPSIZE_NONE</property>
+			      <property name="width_chars">-1</property>
+			      <property name="single_line_mode">False</property>
+			      <property name="angle">0</property>
+			    </widget>
+			    <packing>
+			      <property name="padding">0</property>
+			      <property name="expand">False</property>
+			      <property name="fill">False</property>
+			    </packing>
+			  </child>
+
+			  <child>
+			    <widget class="GtkScrolledWindow" id="scrolledwindow2">
+			      <property name="visible">True</property>
+			      <property name="can_focus">True</property>
+			      <property name="hscrollbar_policy">GTK_POLICY_NEVER</property>
+			      <property name="vscrollbar_policy">GTK_POLICY_AUTOMATIC</property>
+			      <property name="shadow_type">GTK_SHADOW_IN</property>
+			      <property name="window_placement">GTK_CORNER_TOP_LEFT</property>
+
+			      <child>
+				<widget class="GtkTextView" id="manual_script_options">
+				  <property name="visible">True</property>
+				  <property name="can_focus">True</property>
+				  <property name="editable">True</property>
+				  <property name="overwrite">False</property>
+				  <property name="accepts_tab">True</property>
+				  <property name="justification">GTK_JUSTIFY_LEFT</property>
+				  <property name="wrap_mode">GTK_WRAP_WORD</property>
+				  <property name="cursor_visible">True</property>
+				  <property name="pixels_above_lines">0</property>
+				  <property name="pixels_below_lines">0</property>
+				  <property name="pixels_inside_wrap">0</property>
+				  <property name="left_margin">0</property>
+				  <property name="right_margin">0</property>
+				  <property name="indent">0</property>
+				  <property name="text" translatable="yes"></property>
+				</widget>
+			      </child>
+			    </widget>
+			    <packing>
+			      <property name="padding">0</property>
+			      <property name="expand">True</property>
+			      <property name="fill">True</property>
+			    </packing>
+			  </child>
+			</widget>
+		      </child>
+		    </widget>
+		    <packing>
+		      <property name="tab_expand">False</property>
+		      <property name="tab_fill">True</property>
+		    </packing>
+		  </child>
+
+		  <child>
+		    <widget class="GtkLabel" id="label7">
+		      <property name="visible">True</property>
+		      <property name="label" translatable="yes">&lt;b&gt;Manual Options&lt;/b&gt;</property>
+		      <property name="use_underline">False</property>
+		      <property name="use_markup">True</property>
+		      <property name="justify">GTK_JUSTIFY_LEFT</property>
+		      <property name="wrap">False</property>
+		      <property name="selectable">False</property>
+		      <property name="xalign">0</property>
+		      <property name="yalign">0.5</property>
+		      <property name="xpad">2</property>
+		      <property name="ypad">2</property>
+		      <property name="ellipsize">PANGO_ELLIPSIZE_NONE</property>
+		      <property name="width_chars">-1</property>
+		      <property name="single_line_mode">False</property>
+		      <property name="angle">0</property>
+		    </widget>
+		    <packing>
+		      <property name="type">tab</property>
+		    </packing>
+		  </child>
+		</widget>
+	      </child>
+	    </widget>
+	    <packing>
+	      <property name="padding">0</property>
+	      <property name="expand">True</property>
+	      <property name="fill">True</property>
+	    </packing>
+	  </child>
+	</widget>
+	<packing>
+	  <property name="padding">0</property>
+	  <property name="expand">True</property>
+	  <property name="fill">True</property>
+	</packing>
+      </child>
+    </widget>
+  </child>
+</widget>
+
+</glade-interface>

Modified: trunk/scripts/pyplearn_driver.py
===================================================================
--- trunk/scripts/pyplearn_driver.py	2007-06-05 19:51:00 UTC (rev 7541)
+++ trunk/scripts/pyplearn_driver.py	2007-06-05 20:11:02 UTC (rev 7542)
@@ -13,6 +13,19 @@
 lines = pyplearn_file.read()
 pyplearn_file.close()
 
+from plearn.utilities import options_dialog
+orig_verb, orig_logs, gui_namespaces, use_gui= options_dialog.getGuiInfo(sys.argv)
+
+gui_code= """
+from plearn.utilities import options_dialog
+runit, verb, logs= options_dialog.optionsDialog(%s,plargs.expdir,%d,%s,%s)
+"""%(repr(sys.argv[1]),orig_verb,repr(orig_logs),repr(gui_namespaces))
+
+if use_gui:
+    lines+= gui_code
+else:
+    lines+= "\nrunit, verb, logs= True, %d, %s\n"%(orig_verb,repr(orig_logs))
+
 if len(sys.argv) == 3 and sys.argv[2] == '--help':
     # Simply print the docstring of the pyplearn script
     lines += 'print __doc__\n'
@@ -21,12 +34,13 @@
     # expdirs.
     sys.argv.remove('--PyPLearnScript')
     plargs.parse(sys.argv[2:])
-    lines += 'print PyPLearnScript( main() )\n'
+    lines += 'if runit: print PyPLearnScript( main(), verbosity= verb, module_names= logs)\nelse: print PyPLearnScript("")\n'
 else:
     # Default mode: simply dump the plearn_representation of this
     plargs.parse(sys.argv[2:])
-    lines += 'print main()\n'
+    lines += 'if runit: print main()\nelse: print PyPLearnScript("")\n'
 
+
 # Closing the current context so that, for instance, mispelled plargs can
 # be identified.
 lines += "import plearn; plearn.pyplearn.context.closeCurrentContext(); del plearn\n"



From saintmlx at mail.berlios.de  Tue Jun  5 22:47:03 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 5 Jun 2007 22:47:03 +0200
Subject: [Plearn-commits] r7543 - trunk/python_modules/plearn/utilities
Message-ID: <200706052047.l55Kl3Vg027939@sheep.berlios.de>

Author: saintmlx
Date: 2007-06-05 22:47:03 +0200 (Tue, 05 Jun 2007)
New Revision: 7543

Modified:
   trunk/python_modules/plearn/utilities/options_dialog.py
Log:
- fix prev. commit: don't import gtk when gui not requested



Modified: trunk/python_modules/plearn/utilities/options_dialog.py
===================================================================
--- trunk/python_modules/plearn/utilities/options_dialog.py	2007-06-05 20:11:02 UTC (rev 7542)
+++ trunk/python_modules/plearn/utilities/options_dialog.py	2007-06-05 20:47:03 UTC (rev 7543)
@@ -30,7 +30,6 @@
 #  This file is part of the PLearn library. For more information on the PLearn
 #  library, go to the PLearn Web site at www.plearn.org
 
-from plearn.plide.plide_options import *
 import sys, os
 
 def getVerbosity(args):
@@ -94,15 +93,18 @@
     """
     pop a dialog showing all plnamespaces
     """
-    PyPLearnOptionsDialog.define_injected(None, gladeFile)
-    options_holder= PyPLearnOptionsHolder(name, None, expdir, namespaces)
+    from plearn.plide import plide_options
+    import gtk
+    plide_options.PyPLearnOptionsDialog.define_injected(None, gladeFile)
+    options_holder= plide_options.PyPLearnOptionsHolder(name, None,
+                                                        expdir, namespaces)
     options_holder.log_enable= named_logging
     ks= options_holder.inv_verb_map.keys()
     ks.sort()
     for k in ks:
         if k <= verbosity:
             options_holder.log_verbosity= options_holder.inv_verb_map[k]
-    options_dialog= PyPLearnOptionsDialog(options_holder)
+    options_dialog= plide_options.PyPLearnOptionsDialog(options_holder)
     result= options_dialog.run()
     if result == gtk.RESPONSE_OK:
         options_dialog.update_options_holder()



From tihocan at mail.berlios.de  Wed Jun  6 17:00:07 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 6 Jun 2007 17:00:07 +0200
Subject: [Plearn-commits] r7544 - trunk/python_modules/plearn
Message-ID: <200706061500.l56F073b025306@sheep.berlios.de>

Author: tihocan
Date: 2007-06-06 17:00:07 +0200 (Wed, 06 Jun 2007)
New Revision: 7544

Modified:
   trunk/python_modules/plearn/bridge.py
Log:
Fixed parsing of command line arguments when in server mode, and gave .5 more seconds to the server to launch for safety

Modified: trunk/python_modules/plearn/bridge.py
===================================================================
--- trunk/python_modules/plearn/bridge.py	2007-06-05 20:47:03 UTC (rev 7543)
+++ trunk/python_modules/plearn/bridge.py	2007-06-06 15:00:07 UTC (rev 7544)
@@ -8,9 +8,10 @@
     from plearn.pyplearn import *
     from plearn.io.server import *
     import time
+    plargs.parse([ arg for arg in sys.argv if arg.find('=') != -1 ]) # Parse command-line.
     server_command = plearn.bridgemode.server_exe + ' server'
     serv = launch_plearn_server(command = server_command)
-    time.sleep(0.5) # give some time to the server to get born and well alive
+    time.sleep(1) # give some time to the server to get born and well alive
 else:
     from plearn.pyext import *
 # from gdb do the following to see PLearn symbols AFTER pyext has been loaded



From tihocan at mail.berlios.de  Wed Jun  6 17:51:30 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 6 Jun 2007 17:51:30 +0200
Subject: [Plearn-commits] r7545 - trunk/plearn_learners/online
Message-ID: <200706061551.l56FpUv1030540@sheep.berlios.de>

Author: tihocan
Date: 2007-06-06 17:51:29 +0200 (Wed, 06 Jun 2007)
New Revision: 7545

Modified:
   trunk/plearn_learners/online/ForwardModule.cc
Log:
Added 'current' as an option so it can modified externally

Modified: trunk/plearn_learners/online/ForwardModule.cc
===================================================================
--- trunk/plearn_learners/online/ForwardModule.cc	2007-06-06 15:00:07 UTC (rev 7544)
+++ trunk/plearn_learners/online/ForwardModule.cc	2007-06-06 15:51:29 UTC (rev 7545)
@@ -69,6 +69,8 @@
 void ForwardModule::declareOptions(OptionList& ol)
 {
 
+    // 'build' options.
+
     declareOption(ol, "modules", &ForwardModule::modules,
                   OptionBase::buildoption,
         "The list of modules that can be used to forward calls.");
@@ -84,6 +86,14 @@
         "modules. Otherwise, only the current module pointed by 'forward_to'\n"
         "will be forgotten.");
 
+    // 'nosave' options.
+    
+    // 'current' is an option only so it can be modified in server mode for
+    // instance, in order to bypass a call to build (yes, this is a hack!).
+    declareOption(ol, "current", &ForwardModule::current,
+                  OptionBase::nosave,
+        "Index in 'modules' of the module given by 'forward_to'.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 
@@ -198,7 +208,7 @@
 // getPorts //
 //////////////
 const TVec<string>& ForwardModule::getPorts() {
-      return modules[current]->getPorts();
+    return modules[current]->getPorts();
 }
 
 //////////////////



From breuleux at mail.berlios.de  Wed Jun  6 19:08:47 2007
From: breuleux at mail.berlios.de (breuleux at BerliOS)
Date: Wed, 6 Jun 2007 19:08:47 +0200
Subject: [Plearn-commits] r7546 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706061708.l56H8lLu024457@sheep.berlios.de>

Author: breuleux
Date: 2007-06-06 19:08:47 +0200 (Wed, 06 Jun 2007)
New Revision: 7546

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Thread safety.

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-06 15:51:29 UTC (rev 7545)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-06 17:08:47 UTC (rev 7546)
@@ -3,22 +3,25 @@
 from plearn.bridge import *
 from threading import *
 
+
 # YET TO BE DOCUMENTED AND CLEANED UP CODE
 
 
 if plearn.bridgemode.useserver:
     servers_lock = Lock() # Lock on the servers list so we don't have race conditions
-    servers = [[serv, 0]] # List of [server, amount_of_jobs_server_is_running] lists
+    servers = [[serv, 0, Lock()]] # List of [server, amount_of_jobs_server_is_running, lock_on_the_server] lists
     servers_max = 1e10 # Maximal amount of servers we are willing to run
 
 def execute(object, tasks, use_threads = False):
     def job(object):
+        lock = [servinfo[2] for servinfo in servers if servinfo[0] is object.server][0]
+        lock.acquire() # we will only send our job to the server if nothing is already running
         for method, args in tasks: # do each task sequentially
             getattr(object, method)(*args)
+        lock.release()
         
     if plearn.bridgemode.useserver and use_threads:
         t = Thread(target = job, args = (object,))
-        t.start()
         return t
     else:
         for method, args in tasks:
@@ -33,7 +36,7 @@
     least_loaded = 0
     nservers = len(servers)
     for servinfo, i in zip(servers, xrange(nservers)):
-        server, njobs = servinfo
+        server, njobs, lock = servinfo
         if not njobs:
             servinfo[1] = 1
             servers_lock.release()
@@ -43,8 +46,10 @@
             least_loaded = i
     if nservers < servers_max:
         command = plearn.bridgemode.server_exe + ' server'
-        server = launch_plearn_server(command)
-        servers.append([server, 1])
+        server = launch_plearn_server(command = command)
+        time.sleep(0.5) # give some time to the server to get born and well alive (taken from bridge.py)
+        lock = Lock()
+        servers.append([server, 1, lock])
     else:
         servinfo = servers[least_loaded]
         server = servinfo[0]
@@ -57,7 +62,7 @@
         server = object.server
         servers_lock.acquire()
         for servinfo in servers:
-            server_, njobs = servinfo
+            server_, njobs, lock = servinfo
             if server_ is server: # == doesn't work
                 servinfo[1] -= 1 # this server is done so we reduce its job count
                 break
@@ -65,7 +70,12 @@
 
 def assign(object, use_threads = False):
     server = acquire_server(use_threads)
+    if use_threads:
+        lock = [servinfo[2] for servinfo in servers if servinfo[0] is server][0]
+        lock.acquire() # plearn servers are not thread safe
     o = server.new(object)
+    if use_threads:
+        lock.release()
     return o
 
 
@@ -402,16 +412,24 @@
             for j in range(0,n_tests):
                 ts = pl.VecStatsCollector()
                 if plearn.bridgemode.useserver:
-                    ts = candidate.server.new(ts)
+                    ts = candidate.server.new(ts) # no threads are running so we don't need to lock here
                 stats.append(ts)
                 tasks.append(('test', (testsets[j], ts, 0, 0)))
             active_stats.append(stats)
             threads.append(execute(candidate, tasks, use_threads))
 
+        # All threads must be started here. The servers are not thread-safe,
+        # but the threads we start get a lock on their respective servers.
         for thread in threads:
             if isinstance(thread, Thread):
-                thread.join() # wait for all experiments to finish
+                thread.start()
 
+        # Similarly, we must wait for all experiments to finish before doing
+        # anything else.
+        for thread in threads:
+            if isinstance(thread, Thread):
+                thread.join()
+
         for active, stats in zip(actives, active_stats):
             candidate = all_candidates[active]
             results = all_results[active]
@@ -447,7 +465,7 @@
                             best_active = active
                             best_early_stop = stage
                             if return_best_model:
-                                best_candidate = deepcopy(candidate)
+                                best_candidate = deepcopy(candidate, use_threads)
             if logfile:
                 print >>logfile
                 logfile.flush()



From nouiz at mail.berlios.de  Wed Jun  6 23:03:52 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 Jun 2007 23:03:52 +0200
Subject: [Plearn-commits] r7547 - trunk/plearn/io
Message-ID: <200706062103.l56L3qtr001839@sheep.berlios.de>

Author: nouiz
Date: 2007-06-06 23:03:51 +0200 (Wed, 06 Jun 2007)
New Revision: 7547

Modified:
   trunk/plearn/io/pl_io.cc
Log:
BUGFIX:If a compresed file like a dmat was truncated, it was generating a seg fault. Now return a PLERROR.


Modified: trunk/plearn/io/pl_io.cc
===================================================================
--- trunk/plearn/io/pl_io.cc	2007-06-06 17:08:47 UTC (rev 7546)
+++ trunk/plearn/io/pl_io.cc	2007-06-06 21:03:51 UTC (rev 7547)
@@ -646,7 +646,11 @@
     {
         if(l<0)
             PLERROR("Big problem in new_read_compressed: l=%d", l);
-        mode = (unsigned char)(getc(in)); 
+        int i = getc(in);
+        if(i == EOF)
+            PLERROR("Got EOF while expecting more data in the file!");
+        mode = (unsigned char)(i); 
+            
         ++nbytes;
         unsigned char N1 = (mode & 0x1F);
         switch(N1)



From nouiz at mail.berlios.de  Thu Jun  7 16:02:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Jun 2007 16:02:53 +0200
Subject: [Plearn-commits] r7548 - trunk/plearn/sys
Message-ID: <200706071402.l57E2rlb025204@sheep.berlios.de>

Author: nouiz
Date: 2007-06-07 16:02:52 +0200 (Thu, 07 Jun 2007)
New Revision: 7548

Modified:
   trunk/plearn/sys/Profiler.cc
   trunk/plearn/sys/Profiler.h
Log:
Added the possibility to call start on a profiler key upto a given maximum number of time. Usefull for recursive function or for profiler key that are the same on multiple object


Modified: trunk/plearn/sys/Profiler.cc
===================================================================
--- trunk/plearn/sys/Profiler.cc	2007-06-06 21:03:51 UTC (rev 7547)
+++ trunk/plearn/sys/Profiler.cc	2007-06-07 14:02:52 UTC (rev 7548)
@@ -51,7 +51,7 @@
 
 #ifdef PROFILE
 // start recording time for named piece of code
-void Profiler::start(const string& name_of_piece_of_code)
+void Profiler::start(const string& name_of_piece_of_code, const int max_nb_going)
 {
     if (active)
     {
@@ -60,7 +60,7 @@
         if (it == codes_statistics.end())
         {
             Stats stats;
-            stats.on_going = true;
+            stats.nb_going = 1;
             stats.wall_last_start   = times(&t);
             stats.user_last_start   = t.tms_utime;
             stats.system_last_start = t.tms_stime;
@@ -69,13 +69,15 @@
         else
         {
             Profiler::Stats& stats = it->second;
-            if (stats.on_going)
-                PLERROR("Profiler::start(%s) called while previous start had not ended",
-                        name_of_piece_of_code.c_str());
-            stats.on_going = true;
-            stats.wall_last_start   = times(&t);
-            stats.user_last_start   = t.tms_utime;
-            stats.system_last_start = t.tms_stime;
+            if (stats.nb_going >= max_nb_going)
+                PLERROR("Profiler::start(%s) called while previous %d starts had not ended and we allowed only %d starts",
+                        name_of_piece_of_code.c_str(),stats.nb_going, max_nb_going);
+            if (stats.nb_going==0){
+                stats.wall_last_start   = times(&t);
+                stats.user_last_start   = t.tms_utime;
+                stats.system_last_start = t.tms_stime;
+            }
+            stats.nb_going++;
         }
     }
 }
@@ -94,36 +96,43 @@
             PLERROR("Profiler::end(%s) called before any call to start(%s)",
                     name_of_piece_of_code.c_str(),name_of_piece_of_code.c_str());
         Profiler::Stats& stats = it->second;
-        if (!stats.on_going)
+        if (stats.nb_going == 0)
             PLERROR("Profiler::end(%s) called before previous start was called",
                     name_of_piece_of_code.c_str());
 
-        stats.on_going = false;
+        stats.nb_going--;
         stats.frequency_of_occurence++;
-        long wall_duration   = end_time    - stats.wall_last_start;
-        long user_duration   = t.tms_utime - stats.user_last_start;
-        long system_duration = t.tms_stime - stats.system_last_start;
-        if (wall_duration < 0) {
-            wall_duration = user_duration = system_duration = 1;
-            PLWARNING("Profiler: negative duration measured with times!");
+        if (stats.nb_going==0){
+            long wall_duration   = end_time    - stats.wall_last_start;
+            long user_duration   = t.tms_utime - stats.user_last_start;
+            long system_duration = t.tms_stime - stats.system_last_start;
+            stats.wall_duration   += wall_duration;
+            stats.user_duration   += user_duration;
+            stats.system_duration += system_duration;
+            if (wall_duration < 0) {
+                wall_duration = user_duration = system_duration = 1;
+                PLWARNING("Profiler: negative duration measured with times!");
+            }
         }
-        stats.wall_duration   += wall_duration;
-        stats.user_duration   += user_duration;
-        stats.system_duration += system_duration;
     }
 }
 
-// start recording time for named piece of code if PL_PROFILE is set
 #ifdef PL_PROFILE
-void Profiler::pl_profile_start(const string& name_of_piece_of_code){
-        Profiler::start(name_of_piece_of_code);}
-#endif
-// end recording time for named piece of code, and increment
-// frequency of occurence and total duration of this piece of code.
-// if PL_PROFILE is set
-#ifdef PL_PROFILE
+// call Profiler::start if PL_PROFILE is set
+void Profiler::pl_profile_start(const string& name_of_piece_of_code, const int max_nb_going){
+    Profiler::start(name_of_piece_of_code, max_nb_going);}
+// call Profiler::end if PL_PROFILE is set
 void Profiler::pl_profile_end(const string& name_of_piece_of_code){
-        Profiler::end(name_of_piece_of_code);}
+    Profiler::end(name_of_piece_of_code);}
+// call Profiler::activate if PL_PROFILE is set
+void Profiler::pl_profile_activate(){
+    Profiler::activate();}
+// call Profiler::report if PL_PROFILE is set
+void Profiler::pl_profile_report(ostream& out){
+    Profiler::report(out);}
+// call Profiler::reportwall if PL_PROFILE is set
+void Profiler::pl_profile_reportwall(ostream& out){
+    Profiler::reportwall(out);}
 #endif
 
 #endif

Modified: trunk/plearn/sys/Profiler.h
===================================================================
--- trunk/plearn/sys/Profiler.h	2007-06-06 21:03:51 UTC (rev 7547)
+++ trunk/plearn/sys/Profiler.h	2007-06-07 14:02:52 UTC (rev 7548)
@@ -107,7 +107,7 @@
         clock_t wall_last_start;             //!< Wall when last started
         clock_t user_last_start;             //!< User when last started
         clock_t system_last_start;           //!< System when last started
-        bool on_going;                       //!< Whether we have started this stat
+        int nb_going;                       //!< Whether we have started this stat
       
         Stats()
             : frequency_of_occurence(0),
@@ -117,7 +117,7 @@
               wall_last_start(0),
               user_last_start(0),
               system_last_start(0),
-              on_going(false)
+              nb_going(0)
         { }
     };
 
@@ -141,9 +141,9 @@
     
     //!  Start recording time for named piece of code
 #ifdef PROFILE
-    static void start(const string& name_of_piece_of_code);
+    static void start(const string& name_of_piece_of_code, const int max_nb_going=1);
 #else
-    static inline void start(const string& name_of_piece_of_code) { }
+    static inline void start(const string& name_of_piece_of_code, const int max_nb_going=1) { }
 #endif
 
     //!  End recording time for named piece of code, and increment
@@ -154,22 +154,41 @@
     static inline void end(const string& name_of_piece_of_code) { } 
 #endif
 
-    //!  Start recording time for named piece of code if PL_PROFILE is set
+    //!  call start if if PL_PROFILE is set
 #if defined(PROFILE) && defined(PL_PROFILE)
-    static void pl_profile_start(const string& name_of_piece_of_code);
+    static void pl_profile_start(const string& name_of_piece_of_code, const int max_nb_going=1);
 #else
-    static inline void pl_profile_start(const string& name_of_piece_of_code) {}
+    static inline void pl_profile_start(const string& name_of_piece_of_code, const int max_nb_going=1) {}
 #endif
 
-    //!  End recording time for named piece of code, and increment
-    //!  frequency of occurence and total duration of this piece of code.
-    //!  if PL_PROFILE is set
+    //!  call end() if if PL_PROFILE is set
 #if defined(PROFILE) && defined(PL_PROFILE)
     static void pl_profile_end(const string& name_of_piece_of_code);
 #else
     static inline void pl_profile_end(const string& name_of_piece_of_code) { } 
 #endif
 
+    //!  call activate() if if PL_PROFILE is set
+#if defined(PROFILE) && defined(PL_PROFILE)
+    static void pl_profile_activate();
+#else
+    static inline void pl_profile_activate() {}
+#endif
+
+    //!  call report() if if PL_PROFILE is set
+#if defined(PROFILE) && defined(PL_PROFILE)
+    static void pl_profile_report(ostream& out);
+#else
+    static inline void pl_profile_report(ostream& out) {}
+#endif
+
+    //!  call reportwall() if if PL_PROFILE is set
+#if defined(PROFILE) && defined(PL_PROFILE)
+    static void pl_profile_reportwall(ostream& out);
+#else
+    static inline void pl_profile_reportwall(ostream& out) {}
+#endif
+
     //! Return the number of clock ticks per second on this computer.
 #ifdef PROFILE
     static long ticksPerSecond() { return sysconf(_SC_CLK_TCK); }



From nouiz at mail.berlios.de  Thu Jun  7 16:43:23 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Jun 2007 16:43:23 +0200
Subject: [Plearn-commits] r7549 - branches
Message-ID: <200706071443.l57EhNQq029255@sheep.berlios.de>

Author: nouiz
Date: 2007-06-07 16:43:22 +0200 (Thu, 07 Jun 2007)
New Revision: 7549

Added:
   branches/cgi-desjardin/
Log:
create a branch at the time Gilles made its last update of plearn.


Copied: branches/cgi-desjardin (from rev 5640, trunk)



From tihocan at mail.berlios.de  Thu Jun  7 17:43:00 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 7 Jun 2007 17:43:00 +0200
Subject: [Plearn-commits] r7550 - in
	trunk/plearn_learners/online/test/DeepBeliefNet: .
	.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0
	.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0
	.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1
	.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0
	.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0
	.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0
Message-ID: <200706071543.l57Fh02R001867@sheep.berlios.de>

Author: tihocan
Date: 2007-06-07 17:42:58 +0200 (Thu, 07 Jun 2007)
New Revision: 7550

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/PL_DBN_Mini-batch.pyplearn
   trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
Log:
Regenerated test 'PL_DBN_Mini-batch' without fast approximations, so the test does not require to lower precision

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave	2007-06-07 14:43:22 UTC (rev 7549)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave	2007-06-07 15:42:58 UTC (rev 7550)
@@ -19,6 +19,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3 ->PRandom(
@@ -33,116 +34,117 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -3.21580575368705368 -3.22364753521804781 0.0657827866426371755 0.0696030904799730954 0.0622029618057324957 0.0657378684765310972 0.0656101265818516827 0.0638544637987858155 0.066834155056612693 0.06851399357392432 0.0638885967214564421 0.0682193222523920367 0.0614189353126122703 0.0625664766511173087 0.0644591111231603703 0.0672080258882709475 0.0681189224163839119 0.0620144498236062935 0.0658168511287077068 0.0614584698111661498 0.0685006396586318317 0.0670479847182284955 0.0641700406260315298 0.0661248069558832696 0.0671701269075536539 0.0619280966934446855 0.0631002357645635142 0.0616579996278061107 0.0672834623592647452 0.0648275672466712555 0.0651764494044027171 0.0700494806732006453 0.0633482123265595304 0.0705131377405389115 0.0622909603789631242 0.0628167867929062956 0.0663347352041287752 0.0649449035714201317 0.0699396681514389273 0.0655818667632351859 0.0685066157524031466 0.0638341419302661112 0.0675948606542311481 0.066125477314180614 0.06856318275!
 81079101 0.0618955421308295811 0.0659183468910491677 0.0612163654801688162 0.0654024349135764044 0.0621983451516724406 0.0622920880548175557 0.061245812941509839 0.0618062259590423102 0.0630664460752235617 0.0669921649854180873 0.0666066772938697232 0.0718850881647959683 0.0635225507551099161 0.0682316342600248621 0.0678732444344292557 0.0660568040568560999 0.0640291125303126818 0.0636386992597940543 0.0679831833666539737 0.0699530259034038676 0.0660419388372846805 0.0688321552553833454 0.0649094861976335091 0.065869184195756797 0.0614068151019241601 0.0690146718658728037 0.0673971388239837654 0.0672023840821927426 0.0671655330874224299 0.0664861857760034314 0.0693207953501458057 0.0637773254435998888 0.0642392433002831748 0.0668207490601321341 0.0679989486904433588 0.0652705848529419042 0.0645560824978384101 0.0664835106333893366 0.0702736876926789017 0.0636389201503628904 0.0714316391373439191 0.0665688382090242059 0.0651440873146577892 0.0660134349623082289 0.06810149087!
 29601215 0.0664832647847770769 0.0662017489390426123 0.0627373!
 53259297
7873 0.064759889243550986 0.0639710264357981478 0.0659921336077071846 0.0622046577630927522 0.0658226101336753244 0.0660693261083197952 0.0686926071107428898 ] ;
+bias = 100 [ -3.21580575368705368 -3.22364753521804737 0.0657827866426371755 0.0696030904799730815 0.0622029618057324957 0.0657378684765310972 0.0656101265818516688 0.0638544637987858155 0.0668341550566126791 0.0685139935739243339 0.0638885967214564421 0.0682193222523920229 0.0614189353126122703 0.0625664766511173087 0.0644591111231603425 0.0672080258882709475 0.068118922416383898 0.0620144498236062866 0.0658168511287076929 0.0614584698111661498 0.0685006396586318178 0.0670479847182284955 0.0641700406260315298 0.0661248069558832557 0.0671701269075536539 0.0619280966934446925 0.0631002357645635142 0.0616579996278061038 0.067283462359264759 0.0648275672466712555 0.0651764494044027171 0.0700494806732006176 0.0633482123265595443 0.0705131377405389115 0.0622909603789631242 0.062816786792906254 0.0663347352041287891 0.0649449035714201317 0.0699396681514389412 0.0655818667632351998 0.0685066157524031605 0.0638341419302660973 0.0675948606542311481 0.066125477314180614 0.06856318275!
 81078962 0.0618955421308295742 0.0659183468910491538 0.0612163654801688092 0.0654024349135763905 0.0621983451516724475 0.0622920880548175557 0.0612458129415098251 0.0618062259590422894 0.0630664460752235617 0.0669921649854180873 0.0666066772938697094 0.0718850881647959405 0.0635225507551099022 0.0682316342600248621 0.0678732444344292418 0.066056804056856086 0.0640291125303126818 0.0636386992597940543 0.0679831833666539737 0.0699530259034038676 0.0660419388372846944 0.0688321552553833454 0.0649094861976334953 0.0658691841957568108 0.0614068151019241601 0.0690146718658728037 0.0673971388239837654 0.0672023840821927287 0.067165533087422416 0.0664861857760034314 0.0693207953501458057 0.0637773254435999026 0.0642392433002831748 0.0668207490601321202 0.0679989486904433588 0.0652705848529419319 0.0645560824978383824 0.0664835106333893228 0.0702736876926789017 0.0636389201503628765 0.0714316391373439191 0.0665688382090241643 0.0651440873146577892 0.0660134349623082289 0.06810149087!
 29601215 0.066483264784777063 0.0662017489390426123 0.06273735!
 32592977
873 0.064759889243550986 0.0639710264357981478 0.0659921336077071846 0.0622046577630927452 0.0658226101336753244 0.0660693261083197814 0.068692607110742876 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
--1.21415711267087545 	-1.29354388830904199 	
--1.24418997827235556 	-1.27717900530481843 	
-0.0690673896090493733 	0.00106171994505328718 	
--0.0516350068945717125 	0.00282024601463587909 	
-0.11576289542965755 	0.0779696628865056002 	
-0.0937738356356867114 	-0.0213411066976067106 	
-0.0972995083962690521 	-0.020993689739654587 	
-0.125729266175554244 	0.0102265283424613773 	
-0.0763061532337257903 	-0.0398982061240517327 	
--0.0404402347009997568 	0.0256125208841107184 	
-0.0915328264772420214 	0.0429378683671718595 	
--0.0570336912737006846 	0.0526007297112317876 	
-0.102969530366209094 	0.119573904600153644 	
-0.0772888280315299769 	0.104255387702343258 	
-0.116394649377435544 	-0.00151413154731652386 	
-0.0426918411965738348 	-0.0187388372631490142 	
-0.00309655765104493259 	-0.00765739105447751315 	
-0.122622375403801082 	0.0780030163934584825 	
-0.0483846049070615694 	0.0210722938700518524 	
-0.125103439244352965 	0.0962171519640014489 	
-0.00709479590682871725 	-0.0237722892465301559 	
--0.0227695901334794282 	0.0543427620501140685 	
-0.0687163096186778483 	0.0558421452465020091 	
-0.0873560612567313016 	-0.0280358619475561018 	
--0.0464106966487873446 	0.0764830303301708425 	
-0.104489235992809307 	0.100178142145328428 	
+-1.21415711267087589 	-1.29354388830904177 	
+-1.24418997827235622 	-1.27717900530481865 	
+0.0690673896090493872 	0.00106171994505329303 	
+-0.0516350068945717194 	0.00282024601463587519 	
+0.11576289542965755 	0.0779696628865055724 	
+0.0937738356356867114 	-0.021341106697606714 	
+0.0972995083962690244 	-0.0209936897396545835 	
+0.125729266175554216 	0.0102265283424613773 	
+0.0763061532337257764 	-0.0398982061240517327 	
+-0.0404402347009997706 	0.0256125208841107149 	
+0.0915328264772420214 	0.0429378683671718664 	
+-0.0570336912737006985 	0.0526007297112317876 	
+0.102969530366209081 	0.119573904600153644 	
+0.0772888280315299631 	0.104255387702343244 	
+0.116394649377435544 	-0.00151413154731653296 	
+0.042691841196573814 	-0.0187388372631490177 	
+0.00309655765104493563 	-0.00765739105447752182 	
+0.122622375403801068 	0.0780030163934584547 	
+0.0483846049070615694 	0.0210722938700518593 	
+0.125103439244352993 	0.0962171519640014627 	
+0.00709479590682871551 	-0.0237722892465301594 	
+-0.0227695901334794421 	0.0543427620501140754 	
+0.0687163096186778344 	0.0558421452465020021 	
+0.0873560612567313155 	-0.0280358619475560913 	
+-0.0464106966487873446 	0.0764830303301708148 	
+0.104489235992809293 	0.100178142145328428 	
 0.0776732347849002358 	0.0845290052909978551 	
 0.103692129703154029 	0.111410227121787497 	
-0.0430717292444947747 	-0.0217089627191627867 	
-0.0813883488134935357 	0.0210933819137022818 	
-0.129102379174570153 	-0.036355898240278843 	
--0.0286792898569442622 	-0.034418242614459478 	
-0.0835488383450222472 	0.0702781455282296547 	
--0.0580004135006431804 	-0.018560401134722463 	
+0.0430717292444947816 	-0.0217089627191627901 	
+0.0813883488134935357 	0.0210933819137022749 	
+0.129102379174570181 	-0.036355898240278843 	
+-0.0286792898569442761 	-0.034418242614459485 	
+0.0835488383450222333 	0.0702781455282296547 	
+-0.0580004135006431942 	-0.01856040113472247 	
 0.0855508878825002456 	0.106579346680886644 	
-0.0488784468778965461 	0.125566752615730415 	
--0.0231226793861455185 	0.0787930530316436339 	
+0.0488784468778965461 	0.125566752615730387 	
+-0.023122679386145515 	0.0787930530316436201 	
 0.114978759718911597 	-0.0154694984082932637 	
--0.0386528393637206868 	-0.0209924608991451864 	
-0.0160783249246924942 	0.0627623210970354867 	
+-0.0386528393637206938 	-0.0209924608991451864 	
+0.0160783249246924942 	0.0627623210970354728 	
 0.0331228024081457323 	-0.0498308212634323566 	
-0.0582388085345983963 	0.0783366438529952969 	
-0.0406519677826411505 	-0.0288869980516132593 	
+0.0582388085345984102 	0.0783366438529952969 	
+0.0406519677826411435 	-0.0288869980516132489 	
 -0.0597141338098050589 	0.126602072017401146 	
-0.0220616100720658487 	-0.0404334267520467042 	
-0.0907453618703057824 	0.115803257937635892 	
-0.015050429772469601 	0.0522433262471396487 	
-0.113094839239612374 	0.117470889171381862 	
+0.0220616100720658521 	-0.0404334267520467042 	
+0.0907453618703058101 	0.115803257937635892 	
+0.0150504297724695993 	0.0522433262471396417 	
+0.113094839239612333 	0.117470889171381862 	
 -0.00690135564924855734 	0.0928396049715910776 	
-0.109649827478102743 	0.0849798591361091427 	
-0.125891761693266185 	0.0650552724959696654 	
-0.117040829207718577 	0.112558749468915059 	
+0.109649827478102729 	0.0849798591361091565 	
+0.125891761693266213 	0.0650552724959696516 	
+0.11704082920771855 	0.112558749468915059 	
 0.127449941476074358 	0.0808065072573193877 	
-0.0919423334473952747 	0.0707898574066043729 	
+0.0919423334473952608 	0.0707898574066043729 	
 0.0858278120887288193 	-0.0534235543680405545 	
-0.0602800406259822799 	-0.0170601239435519865 	
--0.053810352097996561 	-0.0635244256416778175 	
-0.0499377763793946508 	0.0982902763280064928 	
-0.00693050343376365886 	-0.0151916842537587263 	
-0.0331727775095305988 	-0.0296617801682230176 	
-0.00833280114998716931 	0.0539681702117705275 	
-0.031461760898633194 	0.100064949873964068 	
-0.0855326426254968963 	0.0573150170515133658 	
-0.0232074937816082634 	-0.0233352076821683845 	
--0.0583191191212871565 	-0.000979764064585391878 	
--0.0479906282582027513 	0.116284453036889127 	
-0.0210591810317722582 	-0.0477397138282999595 	
+0.060280040625982266 	-0.0170601239435519865 	
+-0.0538103520979965749 	-0.0635244256416778175 	
+0.0499377763793946439 	0.0982902763280064928 	
+0.00693050343376365452 	-0.015191684253758728 	
+0.0331727775095305918 	-0.0296617801682230349 	
+0.0083328011499871589 	0.0539681702117705206 	
+0.0314617608986332009 	0.100064949873964096 	
+0.0855326426254968963 	0.057315017051513345 	
+0.023207493781608253 	-0.0233352076821683845 	
+-0.0583191191212871773 	-0.000979764064585384288 	
+-0.0479906282582027444 	0.116284453036889113 	
+0.0210591810317722547 	-0.0477397138282999317 	
 0.0434376062540039742 	0.0561817288505665896 	
-0.0452950663181666685 	0.0224140022605995755 	
-0.125304222726260267 	0.0973538514926476223 	
-0.0273905390733705234 	-0.0593483422458846235 	
+0.0452950663181666685 	0.022414002260599572 	
+0.125304222726260239 	0.0973538514926476223 	
+0.0273905390733705199 	-0.0593483422458846235 	
 -0.0482520815783283041 	0.0706049670949486785 	
 -0.0230148144711252271 	0.0495507483466000181 	
-0.0282514195131074453 	-0.00321472486741666928 	
-0.058562138849964078 	-0.0111312665818238624 	
--0.00555360888280790905 	-0.0355896183418070028 	
+0.0282514195131074383 	-0.00321472486741668142 	
+0.058562138849964071 	-0.0111312665818238642 	
+-0.00555360888280791425 	-0.0355896183418070028 	
 0.0834121315444082828 	0.0554918556198370494 	
-0.103621345570545725 	0.018758547809826151 	
-0.10042262676283964 	-0.0625169307095753413 	
--0.0307622944804058357 	0.0317687524264961343 	
-0.0373219821352471087 	0.051103099269427181 	
-0.0747649366073993987 	0.0365997076737267432 	
+0.103621345570545739 	0.0187585478098261545 	
+0.100422626762839626 	-0.0625169307095753274 	
+-0.0307622944804058426 	0.0317687524264961274 	
+0.0373219821352471226 	0.0511030992694271741 	
+0.074764936607399371 	0.0365997076737267432 	
 -0.0396251857033068436 	0.0922725594729125048 	
--0.0415625864709825293 	-0.028102500526772628 	
-0.0938474228099761137 	0.0492258097438833461 	
+-0.0415625864709825432 	-0.0281025005267726037 	
+0.0938474228099761276 	0.0492258097438833461 	
 -0.0465543952870261582 	-0.0573943961338660089 	
-0.0642999004945939284 	-0.0190406742732633638 	
+0.0642999004945939145 	-0.0190406742732633638 	
 0.0192566912094867988 	0.0737614039978816 	
-0.0456318762720610976 	0.0172826898145707966 	
+0.0456318762720610838 	0.0172826898145707897 	
 -0.0449514150475808227 	0.0440256246457320088 	
-0.00338020415539047082 	0.0453054325727234586 	
-0.0505121972965595842 	0.0063691522094812443 	
-0.0637718544343126165 	0.112154363518923827 	
-0.0496769642818689003 	0.0555319134645490958 	
-0.0129640742770456077 	0.121565282319786139 	
-0.0825645251590735596 	-0.018662570808792963 	
-0.0912876914719137483 	0.10330232917136499 	
--0.0207879822675519077 	0.0935891931065968158 	
+0.00338020415539046389 	0.0453054325727234586 	
+0.0505121972965595842 	0.00636915220948123736 	
+0.0637718544343126165 	0.112154363518923855 	
+0.0496769642818689003 	0.0555319134645490819 	
+0.0129640742770456147 	0.121565282319786111 	
+0.0825645251590735457 	-0.0186625708087929665 	
+0.0912876914719137483 	0.103302329171365018 	
+-0.0207879822675519077 	0.0935891931065968435 	
 0.0864228173229119323 	-0.0252615256322490621 	
-0.0219937469394293449 	-0.0441223905892084572 	
+0.0219937469394293449 	-0.0441223905892084711 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -157,6 +159,7 @@
 input_size = 2 ;
 output_size = 100 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *6 ->PRandom(
@@ -178,14 +181,15 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0569056865162901743 	0.0506708704397472176 	-0.00281005636301307577 	-0.00883585699550448571 	0.000968317251690560001 	-0.00934023592686317183 	-0.00511697102021179719 	0.00254076500836063809 	-0.00654591324086042751 	-0.00364155904253430198 	0.00817896291237938874 	-0.0107260314649677869 	-0.00859582448416375787 	-0.00872110586451122979 	-0.00976548188772112871 	-0.00033687045127778274 	-0.00688041685678250983 	-0.00350102615490139246 	0.00751567670499836849 	-0.00318769815895596437 	-0.00901948279713810086 	-0.00105238899124976588 	-0.0110583305687190249 	-0.00333929250749425864 	0.00606232293495695927 	-0.00317578753170242103 	-0.00827366343925372769 	0.00617556755864134225 	-0.00795541585758342655 	0.00698929690863082757 	0.0065417829948051457 	0.00169294246983757034 	-0.00144916085077765526 	-0.00849923651446167841 	0.00130887417565956113 	-0.000892816117790639924 	-0.00850108920620314644 	0.00801754174355075351 	-0.000575280972634687263 	0.000958759987538854185 	-0.!
 00981588084176744299 	-0.00608657171273093746 	-0.00418528552938027872 	0.00562735261577514568 	-0.000518050642663959635 	0.00384599591075757401 	0.00226942270388939913 	0.00182406737587419068 	-0.00745302084548695939 	-0.00275203624784231202 	0.0028967032650019371 	0.00571329210131102903 	-0.0103166826500750639 	-0.0106067278271120852 	0.0034960808896459957 	-0.0103525295822042684 	-0.00969075522674989545 	-0.00199977291888563807 	-0.00244342369869020607 	0.00242178409774834215 	-0.00559882884989377615 	0.00311628278103911376 	0.000164874834633516618 	0.00718502450411628076 	-0.00681132342906103257 	0.000672392566058983279 	0.006353803411059223 	-0.0105935035307277298 	-0.00137094737061349494 	-0.0100798145466800323 	-0.00410415532879708153 	0.00529336414233273073 	0.00643952878798065634 	-0.00912602857564504336 	0.00860528804414868211 	0.00461363049429684442 	0.00539813907327492751 	-0.00692511721474696734 	-0.0107749454556943496 	-0.0011298693980604567 	0.007461578090293!
 6189 	-0.00855606835453529349 	0.00444295613423474883 	-0.0043!
 67958872
62757861 	0.00603238991047171016 	-0.00251719714595639247 	0.00758185921101216503 	0.000155711332240578094 	-0.000431246983178083744 	0.00890166994818770464 	0.00461909965740889619 	-0.00113556307955706127 	-0.000295616485385759753 	0.00070233125797974276 	-0.0109428013806251619 	0.0053839440736755716 	-0.00903117295401473924 	0.00450712775405930251 	-0.00359892962931220289 	0.00308890717513798062 	
--0.0557374783241875241 	-0.0420545575624722279 	0.00813500400145023526 	0.00229379993579922892 	0.00906162971906279702 	-0.00743593205632607068 	0.00762647643368445302 	0.00869621363055657348 	0.00397056380565017752 	-0.00579797794713223392 	0.00955106062258910884 	0.00215716664630538277 	0.00727422984820777323 	-0.000500762515461652285 	0.00459815118867967255 	0.00915734535344571443 	0.00344464560102859026 	0.00282240875012756401 	0.00817036200424728007 	-0.00208257539611419887 	0.00419592447125884897 	0.00214751070833023739 	0.00025531758112026467 	0.00943034204455691152 	-0.00699278020379851591 	-0.00729840657960003063 	-0.00026052884997202928 	-0.00460316891749462404 	0.00777252327054752876 	0.00560798717555513124 	-0.00274809084372059807 	-0.00372002594684294266 	-0.00651644438953755296 	0.0103209839040837199 	-0.00611001780234204147 	0.000184097771358786262 	-0.00780916915251383091 	0.00192688876696273255 	0.00323210381052388245 	-0.0070290469823388246 	-0.00072044654!
 3576197204 	0.00699558581945906952 	0.0032477701625577963 	0.01089182316609324 	0.0016064462503216899 	-0.00329273878532777712 	-0.00116410462046155259 	0.00455768810001197944 	0.00318682572519701952 	-0.00676669811785936097 	0.00236205422100616881 	0.00399784141973917508 	-0.00508214037276732457 	0.00348082825425537245 	-0.00478393313130366302 	0.00227700635475103479 	0.00605499926312637934 	0.00809352000850258567 	0.01050136892500937 	-0.00838322001458193834 	0.00928850191789566508 	0.00801900879720902557 	0.00977389443603241675 	0.0026095707746832497 	0.00646325893474177394 	0.00487004162247831052 	0.0099304085933341283 	0.00514526110946576277 	0.000832536835531273549 	0.00100537648604621423 	-0.00857486094001503295 	0.001418336490835488 	0.00779028400638433465 	0.0107715494732272214 	0.0110326988617241387 	-0.00901334626340150047 	-0.00387799214547862332 	-0.00230720040427873432 	0.000256380375565747564 	0.000419927959661202995 	0.000527364086741302185 	-0.0015952439128!
 724663 	-0.0076566933826778304 	-0.00201338956872734514 	0.008!
 91610717
349773998 	-0.00492588019476571277 	-0.00240272388192210766 	0.00899383373568070354 	0.0020209477909953655 	-0.00233203482051662705 	0.00845518840813633266 	-0.00456789830484366326 	0.00927236344592883947 	0.000431382254701681849 	0.00692583736414298379 	0.00586433371064961143 	-0.00744023043615439812 	0.0111956952692525424 	0.00678182668483380286 	-0.00339477021191207391 	
+0.056905686516290202 	0.0506708704397472037 	-0.00281005636301307664 	-0.00883585699550448571 	0.000968317251690557616 	-0.0093402359268631753 	-0.00511697102021179805 	0.00254076500836063982 	-0.00654591324086042751 	-0.00364155904253430198 	0.00817896291237939568 	-0.0107260314649677869 	-0.00859582448416375787 	-0.00872110586451123153 	-0.00976548188772113045 	-0.000336870451277782902 	-0.00688041685678251157 	-0.00350102615490139463 	0.00751567670499836502 	-0.00318769815895596524 	-0.00901948279713809913 	-0.00105238899124976762 	-0.0110583305687190266 	-0.00333929250749426037 	0.0060623229349569584 	-0.00317578753170242017 	-0.00827366343925372769 	0.00617556755864134051 	-0.00795541585758343002 	0.00698929690863082324 	0.00654178299480514744 	0.00169294246983757121 	-0.00144916085077765396 	-0.00849923651446167841 	0.00130887417565956178 	-0.00089281611779064025 	-0.00850108920620314297 	0.00801754174355075178 	-0.000575280972634687588 	0.000958759987538856137 	-0.00!
 981588084176744473 	-0.00608657171273093572 	-0.00418528552938027959 	0.00562735261577514308 	-0.000518050642663960611 	0.00384599591075757358 	0.00226942270388939653 	0.00182406737587418894 	-0.00745302084548695766 	-0.00275203624784230985 	0.00289670326500193623 	0.00571329210131102903 	-0.0103166826500750639 	-0.0106067278271120887 	0.00349608088964599092 	-0.0103525295822042718 	-0.00969075522674989025 	-0.00199977291888563851 	-0.00244342369869020694 	0.00242178409774834041 	-0.00559882884989377876 	0.00311628278103911289 	0.000164874834633516076 	0.00718502450411627643 	-0.0068113234290610343 	0.000672392566058982845 	0.0063538034110592204 	-0.0105935035307277263 	-0.00137094737061349472 	-0.010079814546680034 	-0.0041041553287970798 	0.00529336414233272726 	0.00643952878798065547 	-0.00912602857564504683 	0.00860528804414868211 	0.00461363049429684268 	0.00539813907327492664 	-0.00692511721474696647 	-0.0107749454556943548 	-0.00112986939806045779 	0.0074615780902936!
 1716 	-0.00855606835453529696 	0.00444295613423474709 	-0.0043!
 67958872
62757948 	0.00603238991047170842 	-0.0025171971459563942 	0.00758185921101216329 	0.000155711332240578772 	-0.000431246983178085804 	0.00890166994818770117 	0.00461909965740889532 	-0.00113556307955706365 	-0.000295616485385759048 	0.000702331257979741893 	-0.0109428013806251619 	0.00538394407367557247 	-0.00903117295401474444 	0.00450712775405929817 	-0.00359892962931220245 	0.00308890717513798105 	
+-0.0557374783241876143 	-0.0420545575624722071 	0.00813500400145022833 	0.00229379993579923065 	0.00906162971906280222 	-0.00743593205632606721 	0.00762647643368444868 	0.00869621363055657695 	0.00397056380565017839 	-0.00579797794713223652 	0.00955106062258910364 	0.00215716664630538537 	0.00727422984820777236 	-0.000500762515461652936 	0.00459815118867967342 	0.00915734535344571443 	0.00344464560102859417 	0.00282240875012756531 	0.00817036200424728354 	-0.00208257539611419844 	0.00419592447125884897 	0.00214751070833023739 	0.000255317581120266405 	0.00943034204455690979 	-0.00699278020379851418 	-0.00729840657960003237 	-0.000260528849972028196 	-0.00460316891749462144 	0.00777252327054753483 	0.0056079871755551321 	-0.00274809084372059894 	-0.00372002594684294049 	-0.00651644438953754689 	0.0103209839040837165 	-0.00611001780234204234 	0.000184097771358787942 	-0.00780916915251383265 	0.00192688876696273451 	0.00323210381052388331 	-0.00702904698233882633 	-0.000720446!
 543576197204 	0.00699558581945907473 	0.00324777016255779717 	0.0108918231660932469 	0.00160644625032169033 	-0.00329273878532777625 	-0.0011641046204615515 	0.00455768810001198118 	0.00318682572519702039 	-0.0067666981178593601 	0.00236205422100616838 	0.00399784141973917768 	-0.0050821403727673237 	0.00348082825425537461 	-0.00478393313130366128 	0.00227700635475103566 	0.00605499926312638454 	0.0080935200085025874 	0.01050136892500937 	-0.00838322001458193487 	0.00928850191789566682 	0.0080190087972090273 	0.00977389443603242022 	0.00260957077468325057 	0.00646325893474177654 	0.00487004162247831052 	0.00993040859333413004 	0.00514526110946576711 	0.000832536835531275934 	0.00100537648604621553 	-0.00857486094001503295 	0.0014183364908354893 	0.00779028400638433292 	0.0107715494732272214 	0.0110326988617241439 	-0.00901334626340149873 	-0.00387799214547862245 	-0.00230720040427873562 	0.000256380375565748214 	0.000419927959661204188 	0.000527364086741303703 	-0.001595243!
 91287246608 	-0.0076566933826778278 	-0.00201338956872734688 	!
 0.008916
10717349773998 	-0.00492588019476571104 	-0.00240272388192210636 	0.00899383373568070354 	0.00202094779099536506 	-0.00233203482051662749 	0.00845518840813633092 	-0.00456789830484366499 	0.00927236344592883947 	0.000431382254701682337 	0.00692583736414298292 	0.00586433371064961143 	-0.00744023043615439552 	0.0111956952692525424 	0.00678182668483380286 	-0.00339477021191207651 	
 ]
 ;
 bias = 2 [ -0.0323078363819910008 0.032307836381991098 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *9 ->PRandom(
@@ -195,12 +199,14 @@
 *10 ->SoftmaxModule(
 input_size = 2 ;
 name = "SoftmaxModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
 ] ;
 n_modules = 2 ;
 name = "ModuleStackModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -211,6 +217,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "NLLCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -219,6 +226,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "ClassErrorCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -229,6 +237,7 @@
 input_size = 2 ;
 output_size = 3 ;
 name = "CombiningCostsModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -238,6 +247,7 @@
 input_size = 100 ;
 output_size = 1 ;
 name = "NLLCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave	2007-06-07 14:43:22 UTC (rev 7549)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave	2007-06-07 15:42:58 UTC (rev 7550)
@@ -19,6 +19,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3 ->PRandom(
@@ -33,116 +34,117 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -4.32711526727717022 -4.3372701645486238 0.0884972513670912592 0.093104715836575605 0.0841804577845747803 0.0884423692492627839 0.0882859838041619716 0.0861706449956606002 0.0897606441010612016 0.0917997185462621212 0.0862172150073365617 0.0914492689853110274 0.0832344574341433024 0.0846255461316875041 0.0868979334356979743 0.0902140465230860805 0.0913138521627375882 0.0839526766363259125 0.0885437376916390906 0.0832835086330831614 0.0917704045377616684 0.0900370148671758574 0.0865570415437719454 0.0889061535637738626 0.0901933883949007226 0.0838551264647667549 0.0852683942355133145 0.0835320783360215119 0.0903033950528041307 0.0873495022725012532 0.0877663854611010485 0.0936361074968803386 0.0855698088882395069 0.09419432614006161 0.0842970734238651653 0.0849357771997706762 0.0891822142770180987 0.0874879941894524393 0.0935054566357249317 0.0882701258129414273 0.0917743293773878865 0.086154371844172295 0.0906792393721227302 0.0889428130004461526 0.091844875169!
 0769255 0.0838195887654502175 0.0886733457025641741 0.0829929657926603837 0.0880565159446428936 0.0841799095124265567 0.0842893022098406275 0.0830294042875595883 0.0837011826816979054 0.0852232267568197588 0.0899532197532659367 0.0894885518089337112 0.0958346480546612006 0.0857821218519718431 0.0914478410021243027 0.0910177787617019729 0.0888371373050060309 0.0863973972481843489 0.0859149352705648878 0.0911501112497726806 0.0935250257635063392 0.0888385233716558337 0.0921673857226443793 0.0874503485910435585 0.0886068150786280956 0.0832175599229860452 0.0923866976451308552 0.0904643653202136383 0.0902225674836392155 0.090163267675610545 0.0893460787478880109 0.0927602015627450316 0.0860875669223204326 0.0866369147670916218 0.0897427212560572424 0.0911794384643066641 0.0878912474958726958 0.0870217265658022876 0.089368170197947544 0.0939061363420257145 0.0859158022475398353 0.0952932285977265842 0.0894471842536392336 0.087739556635745064 0.0887800292092797427 0.0913084084576!
 754745 0.0893526585736332812 0.0890067708000333441 0.084833906!
 87493384
82 0.0872732569112319445 0.0863292264483929705 0.0887496467528825772 0.0841889624230460348 0.0885658836328168386 0.088839997890117095 0.0920015433771734364 ] ;
+bias = 100 [ -4.32711526727717022 -4.3372701645486238 0.0884972513670912453 0.0931047158365755634 0.0841804577845747942 0.0884423692492627839 0.0882859838041619854 0.0861706449956605863 0.0897606441010612016 0.0917997185462621629 0.0862172150073365201 0.0914492689853110274 0.0832344574341433024 0.0846255461316875179 0.086897933435698016 0.0902140465230860666 0.0913138521627376021 0.0839526766363259125 0.0885437376916390906 0.0832835086330831892 0.0917704045377616545 0.0900370148671758574 0.0865570415437719592 0.0889061535637738487 0.0901933883949007087 0.083855126464766741 0.0852683942355133007 0.083532078336021498 0.0903033950528041307 0.087349502272501281 0.0877663854611010485 0.0936361074968803525 0.0855698088882395069 0.0941943261400616239 0.0842970734238651792 0.0849357771997706346 0.0891822142770181264 0.0874879941894524393 0.0935054566357249178 0.0882701258129414135 0.0917743293773878727 0.0861543718441722672 0.0906792393721227302 0.0889428130004461664 0.091844875169!
 0769255 0.0838195887654502175 0.088673345702564188 0.0829929657926603698 0.0880565159446428936 0.0841799095124265706 0.0842893022098406414 0.0830294042875595467 0.0837011826816978638 0.0852232267568197449 0.0899532197532659367 0.0894885518089336696 0.0958346480546612284 0.0857821218519718293 0.0914478410021243027 0.0910177787617019729 0.0888371373050060309 0.0863973972481843211 0.0859149352705649016 0.0911501112497726668 0.0935250257635063392 0.0888385233716558198 0.0921673857226444071 0.0874503485910435724 0.0886068150786281233 0.0832175599229860452 0.0923866976451308552 0.0904643653202136522 0.0902225674836392155 0.0901632676756105172 0.089346078747887997 0.0927602015627450316 0.0860875669223204326 0.086636914767091594 0.0897427212560572563 0.0911794384643066502 0.0878912474958726958 0.0870217265658023015 0.089368170197947544 0.0939061363420257006 0.0859158022475398631 0.0952932285977266119 0.0894471842536392059 0.0877395566357450918 0.0887800292092797427 0.09130840845767!
 55023 0.089352658573633309 0.0890067708000333718 0.08483390687!
 49338343
 0.0872732569112319445 0.0863292264483929844 0.0887496467528825772 0.0841889624230460348 0.0885658836328168247 0.088839997890117095 0.0920015433771734364 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
--1.24792831633213641 	-1.39735089256445977 	
--1.26139566256338598 	-1.38981579762817442 	
-0.0761048699587760225 	0.00801901792621092548 	
--0.0437362423957267585 	0.0100258312296207294 	
-0.122341829544572858 	0.0843382980897796758 	
-0.100681906572919172 	-0.014297912333214367 	
-0.104194045090286055 	-0.0139423394592549255 	
-0.132369740996299834 	0.0170023995994674701 	
-0.0833785830127285188 	-0.0326773524247471192 	
--0.0326950623956562547 	0.0326269485130347633 	
-0.0983250247890316426 	0.0495618447986045374 	
--0.0492018798408611416 	0.0594823721799443497 	
-0.109567946177656472 	0.125725738863893627 	
-0.0840674972530368603 	0.110534760298992382 	
-0.123127060198002722 	0.00537144577668967425 	
-0.0499502891283985506 	-0.0115941354457338875 	
-0.0106130835455938329 	-0.000502204039447324223 	
-0.129155197592288162 	0.0843559859284681857 	
-0.0555094106422285502 	0.0279252235392405644 	
-0.131579496299484344 	0.102446402857112651 	
-0.014620781956967692 	-0.0165110880938397452 	
--0.0152098035716024937 	0.0611314970453571208 	
-0.0756480514037247004 	0.0624429547810556387 	
-0.0943313046294627644 	-0.0209162148785304799 	
--0.0387538020308657027 	0.0831676274390826531 	
+-1.24792831633213597 	-1.39735089256445932 	
+-1.2613956625633862 	-1.38981579762817375 	
+0.0761048699587760225 	0.00801901792621093762 	
+-0.0437362423957267515 	0.0100258312296207363 	
+0.122341829544572872 	0.0843382980897796897 	
+0.100681906572919158 	-0.0142979123332143618 	
+0.104194045090286028 	-0.0139423394592549325 	
+0.132369740996299834 	0.0170023995994674736 	
+0.0833785830127285188 	-0.0326773524247471261 	
+-0.0326950623956562686 	0.0326269485130347772 	
+0.0983250247890316564 	0.0495618447986045443 	
+-0.0492018798408611485 	0.0594823721799443636 	
+0.109567946177656486 	0.125725738863893627 	
+0.0840674972530368464 	0.110534760298992382 	
+0.12312706019800275 	0.00537144577668966644 	
+0.0499502891283985229 	-0.0115941354457338875 	
+0.0106130835455938467 	-0.000502204039447316742 	
+0.129155197592288079 	0.0843559859284681718 	
+0.0555094106422285502 	0.0279252235392405679 	
+0.131579496299484372 	0.102446402857112664 	
+0.0146207819569676885 	-0.0165110880938397521 	
+-0.0152098035716024798 	0.0611314970453571208 	
+0.0756480514037247004 	0.0624429547810556249 	
+0.0943313046294627644 	-0.0209162148785304661 	
+-0.0387538020308656958 	0.0831676274390826392 	
 0.111079692147122572 	0.106413612787671669 	
-0.0844859003750914883 	0.0909281536163424886 	
-0.110255534107209494 	0.117569964096915225 	
-0.0503402481910374122 	-0.0145363440385552316 	
-0.0882877347052690303 	0.0278747608862448711 	
-0.13579989243262322 	-0.0293047418304829138 	
--0.0208853376616612921 	-0.0270185613474963199 	
-0.0903303247636411122 	0.0767358706059570739 	
--0.049995372117030687 	-0.0111852725638232134 	
-0.0922451774733284713 	0.112807474834139387 	
-0.0557920505124698887 	0.131762040536686248 	
--0.0156166793851998647 	0.0854223662886001156 	
-0.121727203196954167 	-0.00852357251789233848 	
--0.0308060743325063342 	-0.0136561429394114215 	
-0.0233263322247821737 	0.0694023941057379939 	
-0.0405156370159508172 	-0.0424488523830184247 	
-0.0652001999783928116 	0.0848180390433775633 	
-0.0479443611943429771 	-0.0216705924806779616 	
--0.0520331275937088236 	0.133010909353283829 	
-0.0295060359707902987 	-0.0331018324098504776 	
-0.0973891803189232685 	0.121966636503981637 	
-0.0223348997781607182 	0.058958206082786728 	
-0.119610467943873061 	0.123597553887780973 	
-0.000476921336380212797 	0.09935506894421757 	
-0.116236242367810105 	0.0912979105807741331 	
+0.0844859003750915022 	0.0909281536163425025 	
+0.110255534107209507 	0.117569964096915211 	
+0.0503402481910374192 	-0.0145363440385552298 	
+0.0882877347052690442 	0.0278747608862448884 	
+0.13579989243262322 	-0.0293047418304829208 	
+-0.0208853376616612955 	-0.0270185613474963164 	
+0.0903303247636411538 	0.0767358706059570739 	
+-0.04999537211703068 	-0.0111852725638231943 	
+0.0922451774733284852 	0.112807474834139387 	
+0.0557920505124698748 	0.13176204053668622 	
+-0.0156166793851998647 	0.085422366288600074 	
+0.121727203196954181 	-0.00852357251789235063 	
+-0.0308060743325063273 	-0.013656142939411418 	
+0.0233263322247821842 	0.0694023941057379939 	
+0.0405156370159508103 	-0.0424488523830184525 	
+0.0652001999783928532 	0.0848180390433775772 	
+0.0479443611943429562 	-0.0216705924806779755 	
+-0.0520331275937088097 	0.133010909353283802 	
+0.0295060359707903092 	-0.0331018324098504846 	
+0.0973891803189232824 	0.121966636503981624 	
+0.0223348997781607252 	0.058958206082786721 	
+0.119610467943873047 	0.123597553887780973 	
+0.000476921336380206943 	0.0993550689442175561 	
+0.116236242367810105 	0.0912979105807741886 	
 0.132418046816531965 	0.0714706742660448091 	
-0.123533091063945707 	0.118700555327761512 	
-0.133945660716787401 	0.0871316762245619159 	
-0.0986966479771140132 	0.0772562696165317586 	
+0.123533091063945721 	0.118700555327761526 	
+0.133945660716787401 	0.087131676224561902 	
+0.0986966479771140132 	0.0772562696165317725 	
 0.0928489721357925596 	-0.0461591939206654617 	
-0.0674164705057713337 	-0.00995972789837363745 	
--0.0457363065061925367 	-0.0558365841657046738 	
+0.0674164705057713198 	-0.00995972789837364786 	
+-0.0457363065061925228 	-0.0558365841657046599 	
 0.0569120685437913346 	0.104659972014240071 	
-0.0144387081701442752 	-0.00799004058277002355 	
-0.0405014545351480917 	-0.0224470511098674094 	
-0.0156885394621569529 	0.0607053011427125255 	
-0.0385488185786774309 	0.106453340415823924 	
-0.0923477123362821534 	0.0638716009610263702 	
-0.0305985475582649033 	-0.0161337302924452197 	
--0.0503582362547266432 	0.00626888950394193043 	
--0.0403763892900047203 	0.122732268245548823 	
-0.0285286154269412222 	-0.0403524371964778528 	
-0.0505460324818351181 	0.0628354760976757298 	
-0.0524402592962677425 	0.0292665783345892501 	
-0.131792667337911978 	0.103591992774134042 	
+0.0144387081701442682 	-0.00799004058277001834 	
+0.0405014545351480848 	-0.022447051109867406 	
+0.0156885394621569495 	0.0607053011427125325 	
+0.0385488185786774379 	0.10645334041582398 	
+0.0923477123362821395 	0.0638716009610263563 	
+0.0305985475582648929 	-0.0161337302924452024 	
+-0.0503582362547266155 	0.0062688895039419313 	
+-0.0403763892900047272 	0.122732268245548809 	
+0.0285286154269412187 	-0.0403524371964778528 	
+0.0505460324818351181 	0.0628354760976757576 	
+0.0524402592962677147 	0.0292665783345892605 	
+0.131792667337911978 	0.103591992774134056 	
 0.0348344614551319917 	-0.0519031455810079895 	
--0.0405527523441988128 	0.0773380740705313791 	
--0.0154457459363229466 	0.0563698254393016143 	
-0.0355900832138369774 	0.00386744655745153596 	
-0.0656871478796531155 	-0.00408206902396727068 	
-0.00206405204327912055 	-0.0282454046662434892 	
-0.0902178471958940686 	0.0620365098886469582 	
-0.110386287835553543 	0.0255196613515526888 	
+-0.0405527523441988058 	0.0773380740705313652 	
+-0.0154457459363229466 	0.0563698254393016004 	
+0.0355900832138369844 	0.00386744655745152946 	
+0.0656871478796531016 	-0.00408206902396727328 	
+0.00206405204327912793 	-0.0282454046662434823 	
+0.0902178471958940686 	0.0620365098886469374 	
+0.110386287835553584 	0.0255196613515526888 	
 0.107379950901278345 	-0.0551879644255911656 	
--0.0230972154790073589 	0.0387229438492729394 	
-0.0444493219797162872 	0.0577741337594893933 	
-0.0816879272017193153 	0.0433059394724301938 	
--0.0320513493983501305 	0.0988418390166333621 	
--0.0336787933731876529 	-0.020710249645373352 	
-0.100614235785147138 	0.0558074948349462988 	
--0.0385665850321436701 	-0.0497831257878785238 	
-0.0713897633525990144 	-0.011968020460012975 	
-0.0264864934157091371 	0.0803429688797962643 	
-0.0527857984995237089 	0.0241688198442964533 	
--0.0372283475335263223 	0.0509203961830493809 	
-0.0107724740413416009 	0.0520979069590651045 	
-0.0576485034783195821 	0.0133097617517652107 	
-0.0706270442107162638 	0.118412798126190044 	
-0.0567206293871870618 	0.0621572219754824226 	
-0.0201604733232439125 	0.127871237627920936 	
-0.0895405314619520526 	-0.0116162226188489415 	
-0.0979661989894221319 	0.109552112759567388 	
--0.0133191795274148917 	0.100124728352003464 	
-0.0933963703177931454 	-0.0181627406802816124 	
-0.0294391786966599553 	-0.0367752161076390013 	
+-0.0230972154790073485 	0.0387229438492729464 	
+0.044449321979716315 	0.0577741337594893656 	
+0.0816879272017193014 	0.0433059394724301799 	
+-0.0320513493983501443 	0.0988418390166333621 	
+-0.033678793373187646 	-0.0207102496453733416 	
+0.100614235785147152 	0.0558074948349463126 	
+-0.0385665850321436632 	-0.0497831257878785238 	
+0.0713897633525990005 	-0.0119680204600129698 	
+0.026486493415709144 	0.0803429688797962643 	
+0.0527857984995237159 	0.0241688198442964464 	
+-0.0372283475335263153 	0.0509203961830493809 	
+0.0107724740413416079 	0.0520979069590650906 	
+0.057648503478319589 	0.0133097617517652072 	
+0.0706270442107162499 	0.118412798126190016 	
+0.0567206293871870756 	0.0621572219754823949 	
+0.020160473323243909 	0.127871237627920936 	
+0.0895405314619520387 	-0.0116162226188489346 	
+0.0979661989894221458 	0.109552112759567374 	
+-0.0133191795274148882 	0.100124728352003506 	
+0.0933963703177931454 	-0.0181627406802816228 	
+0.0294391786966599518 	-0.0367752161076390013 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -157,6 +159,7 @@
 input_size = 2 ;
 output_size = 100 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *6 ->PRandom(
@@ -178,14 +181,15 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0532386934498691039 	0.080626291410914 	-0.00322832263047528618 	-0.00910343074055995106 	0.000362212364114244121 	-0.0097499114166556787 	-0.00553090895961826957 	0.00203960694798923942 	-0.00690742803008835993 	-0.00394282619132612332 	0.0076661302644176893 	-0.0110215495992722432 	-0.0092472474209955649 	-0.00930398259817479673 	-0.0102344772667839652 	-0.00069601406606644052 	-0.00720779799897976973 	-0.00411788273161691355 	0.00709460022217443288 	-0.0038394426807942519 	-0.00933301520711203071 	-0.0014059560481970222 	-0.0115574854661330828 	-0.00373112402975689155 	0.00573110929815871979 	-0.00379954945814822828 	-0.00882869991630134444 	0.00553558683983973367 	-0.008310844063386379 	0.00652472469292016567 	0.00612490476129661301 	0.00142932511701680137 	-0.00199249906965874969 	-0.00874289066291808817 	0.00070810547166986473 	-0.00145190000566321892 	-0.00887602388839884258 	0.00757538439212778435 	-0.00083985907401753579 	0.000533248145951293207 	-0.0101225850172!
 894268 	-0.00660073099196189974 	-0.00452815842694289251 	0.00528765362002427998 	-0.000826747184378847982 	0.00322234347196983508 	0.00185752294625763747 	0.0011579056263633172 	-0.00787045050184466237 	-0.00335996956149444206 	0.00229723571371282529 	0.00504812464862783306 	-0.0109458737169871017 	-0.0111640298400931398 	0.00314859000401088119 	-0.010733386362684583 	-0.0099049628398422615 	-0.00252614321577472253 	-0.00276674250666862241 	0.00208774392453802034 	-0.00600209242006417062 	0.00262090610153620576 	-0.000361770208588623127 	0.00685332269176408982 	-0.00706721837785403463 	0.000315119951773163635 	0.00605543728940713746 	-0.0110544512908535425 	-0.00178970847570510063 	-0.0107338376993417629 	-0.00439294534012347332 	0.00496981016904087317 	0.00609072055761557003 	-0.0094892060786338165 	0.00821754095252105222 	0.00432696950688535222 	0.00487749096412092194 	-0.00741430702567230215 	-0.0111183804980227972 	-0.00145075801315799045 	0.00701661058502487094 	-0.00!
 903557532195551488 	0.00408713070637454264 	-0.004623164359064!
 14726 	0
.00550622837347920779 	-0.00274284051463609098 	0.00719964435159268969 	-0.000287628302747019787 	-0.000843487492493124485 	0.00859268413927025132 	0.00423301492662385252 	-0.00153895089395540544 	-0.000865210986956572504 	0.000232639542571764704 	-0.0114273613766537781 	0.0049815735378366862 	-0.00963711400233785431 	0.00411475321119588767 	-0.0039944488654932624 	0.00278501374815411009 	
--0.0520704852577663635 	-0.0720099785336388437 	0.00855327026891241488 	0.00256137368085467693 	0.00966773460663913203 	-0.00702625656653354299 	0.00804041437309091674 	0.0091973716909279548 	0.00433207859487812468 	-0.00549671079834038612 	0.0100638932705507927 	0.00245268478060984901 	0.00792565278503957853 	8.21142182019163135e-05 	0.00506714656774250906 	0.00951648896823437118 	0.00377202674322583238 	0.00343926532684308249 	0.00859143848707118879 	-0.00143083087427590722 	0.00450945688123279183 	0.00250107776527749891 	0.000754472478534290491 	0.00982217356681956699 	-0.00666156656700028684 	-0.00667464465315429408 	0.000294507627075601302 	-0.0039631881986930545 	0.00812795147635048294 	0.00607255939126580789 	-0.00233121261021206321 	-0.00345640859402216501 	-0.00597310617065646178 	0.0105646380525400863 	-0.00550924909835233834 	0.000743181659231372709 	-0.0074342344703180914 	0.0023690461183857041 	0.00349668191190673303 	-0.00660353514075127761 	-0.000413742368054!
 145467 	0.00750974509869004048 	0.00359064306012041789 	0.0112315221618441811 	0.00191514279203658074 	-0.00266908634654002041 	-0.000752204862829785074 	0.00522384984952284793 	0.00360425538155473507 	-0.00615876480420723092 	0.00296152177229528409 	0.00466300887242236325 	-0.004452949305855298 	0.00403813026723639362 	-0.00443644224566854158 	0.0026578631352313759 	0.00626920687621874539 	0.00861989030539164758 	0.0108246877329877689 	-0.00804917984137160786 	0.00969176548806604046 	0.0085143854767119314 	0.0103005394792546022 	0.00294127258703543674 	0.0067191538835347734 	0.00522731423676414068 	0.0102287747149861774 	0.00560620886959162148 	0.00125129794062288292 	0.00165939963870793614 	-0.00828607092868862208 	0.00174189046412734599 	0.00813909223674941923 	0.0111347269762160014 	0.0114204459533517721 	-0.00872668527598998399 	-0.00335734403632461254 	-0.00181801059335338347 	0.000599815417894182979 	0.000740816574758738585 	0.000972331592010053391 	-0.00111573694545!
 227136 	-0.00730086795481763896 	-0.00175818408229077715 	0.00!
 94422687
1049019811 	-0.00470023682608601296 	-0.00202050902250264274 	0.00943717337066829129 	0.00243318830031040434 	-0.0020230490115992106 	0.00884127313892139541 	-0.00416451049044531257 	0.00984195794749967087 	0.0009010739701096563 	0.00741039736017155316 	0.00626670424648849596 	-0.00683428938783131341 	0.0115880698121158966 	0.00717734592101487798 	-0.00309087678492820641 	
+0.053238693449869097 	0.0806262914109139861 	-0.00322832263047528444 	-0.0091034307405599528 	0.000362212364114244175 	-0.00974991141665568217 	-0.00553090895961827218 	0.00203960694798924419 	-0.00690742803008835907 	-0.00394282619132612245 	0.00766613026441769277 	-0.011021549599272245 	-0.00924724742099556664 	-0.00930398259817479673 	-0.0102344772667839687 	-0.000696014066066441171 	-0.00720779799897976366 	-0.00411788273161691094 	0.00709460022217443462 	-0.00383944268079425407 	-0.00933301520711202551 	-0.00140595604819702198 	-0.0115574854661330828 	-0.00373112402975689502 	0.00573110929815871806 	-0.00379954945814822871 	-0.00882869991630134097 	0.00553558683983973367 	-0.00831084406338638421 	0.0065247246929201622 	0.00612490476129661041 	0.00142932511701680202 	-0.00199249906965874666 	-0.00874289066291807949 	0.000708105471669862236 	-0.00145190000566321697 	-0.00887602388839884258 	0.00757538439212778955 	-0.000839859074017534706 	0.000533248145951293207 	-0.010!
 122585017289425 	-0.006600730991961898 	-0.00452815842694289077 	0.00528765362002428258 	-0.000826747184378848416 	0.00322234347196983725 	0.00185752294625764376 	0.00115790562636331872 	-0.0078704505018446589 	-0.00335996956149444293 	0.00229723571371282442 	0.00504812464862783739 	-0.0109458737169870982 	-0.0111640298400931398 	0.00314859000401087989 	-0.0107333863626845761 	-0.00990496283984226497 	-0.00252614321577472297 	-0.00276674250666862197 	0.00208774392453801947 	-0.00600209242006417235 	0.00262090610153620359 	-0.000361770208588622748 	0.00685332269176408115 	-0.00706721837785403203 	0.000315119951773165207 	0.00605543728940713833 	-0.0110544512908535408 	-0.00178970847570509976 	-0.0107338376993417629 	-0.00439294534012347158 	0.00496981016904086883 	0.00609072055761556916 	-0.0094892060786338113 	0.00821754095252104876 	0.00432696950688535569 	0.00487749096412092194 	-0.00741430702567230475 	-0.011118380498022792 	-0.00145075801315798915 	0.0070166105850248761!
 5 	-0.00903557532195551315 	0.00408713070637453744 	-0.0046231!
 64359064
14899 	0.00550622837347921126 	-0.00274284051463609185 	0.00719964435159268709 	-0.000287628302747018974 	-0.000843487492493124051 	0.00859268413927025305 	0.00423301492662385512 	-0.00153895089395540523 	-0.000865210986956573697 	0.000232639542571767957 	-0.0114273613766537781 	0.0049815735378366836 	-0.00963711400233785605 	0.00411475321119588507 	-0.00399444886549325893 	0.00278501374815410966 	
+-0.0520704852577664259 	-0.072009978533638816 	0.00855327026891241141 	0.00256137368085467649 	0.00966773460663913377 	-0.00702625656653353865 	0.00804041437309092194 	0.0091973716909279548 	0.00433207859487812468 	-0.00549671079834038699 	0.0100638932705507892 	0.00245268478060985074 	0.007925652785039582 	8.2114218201916571e-05 	0.00506714656774250906 	0.00951648896823437118 	0.00377202674322583281 	0.00343926532684308336 	0.00859143848707118879 	-0.00143083087427590722 	0.00450945688123279009 	0.00250107776527749674 	0.000754472478534290274 	0.00982217356681956526 	-0.00666156656700028771 	-0.00667464465315429755 	0.000294507627075602169 	-0.00396318819869305537 	0.00812795147635048988 	0.00607255939126580702 	-0.00233121261021206711 	-0.00345640859402216458 	-0.00597310617065646351 	0.0105646380525400863 	-0.00550924909835233834 	0.000743181659231372058 	-0.00743423447031809313 	0.00236904611838570497 	0.00349668191190673173 	-0.00660353514075127067 	-0.0004137423680541!
 4525 	0.00750974509869004048 	0.00359064306012042006 	0.0112315221618441846 	0.00191514279203657987 	-0.0026690863465400191 	-0.00075220486282978811 	0.00522384984952284966 	0.00360425538155473811 	-0.00615876480420723439 	0.00296152177229528453 	0.00466300887242236325 	-0.00445294930585530147 	0.00403813026723639189 	-0.00443644224566854591 	0.00265786313523137634 	0.00626920687621874192 	0.00861989030539164758 	0.0108246877329877655 	-0.00804917984137160265 	0.00969176548806604046 	0.00851438547671192966 	0.0103005394792546022 	0.00294127258703543544 	0.00671915388353477427 	0.00522731423676413808 	0.0102287747149861774 	0.00560620886959161801 	0.00125129794062288401 	0.00165939963870793657 	-0.00828607092868861861 	0.00174189046412734513 	0.00813909223674941749 	0.0111347269762160014 	0.011420445953351779 	-0.00872668527598998052 	-0.00335734403632461471 	-0.00181801059335338542 	0.000599815417894180702 	0.000740816574758737609 	0.000972331592010054584 	-0.00111573694545!
 227201 	-0.00730086795481763289 	-0.00175818408229077628 	0.00!
 94422687
1049019638 	-0.0047002368260860147 	-0.00202050902250264187 	0.00943717337066829302 	0.00243318830031040477 	-0.00202304901159921406 	0.00884127313892139541 	-0.00416451049044531257 	0.00984195794749967087 	0.000901073970109655432 	0.00741039736017155316 	0.00626670424648849683 	-0.00683428938783130994 	0.0115880698121159 	0.00717734592101487625 	-0.00309087678492820641 	
 ]
 ;
-bias = 2 [ -0.0480886388473655974 0.0480886388473659929 ] ;
+bias = 2 [ -0.0480886388473655835 0.0480886388473659929 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *9 ->PRandom(
@@ -195,12 +199,14 @@
 *10 ->SoftmaxModule(
 input_size = 2 ;
 name = "SoftmaxModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
 ] ;
 n_modules = 2 ;
 name = "ModuleStackModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -211,6 +217,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "NLLCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -219,6 +226,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "ClassErrorCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -229,6 +237,7 @@
 input_size = 2 ;
 output_size = 3 ;
 name = "CombiningCostsModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -238,6 +247,7 @@
 input_size = 100 ;
 output_size = 1 ;
 name = "NLLCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave	2007-06-07 14:43:22 UTC (rev 7549)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave	2007-06-07 15:42:58 UTC (rev 7550)
@@ -19,6 +19,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3 ->PRandom(
@@ -33,116 +34,117 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -3.67929816976774804 -3.66824487525477094 0.0750909232339263683 0.0795841166700319153 0.0708342668804167186 0.0750421938062638266 0.0749057540871664318 0.07281350621284427 0.0763556633275116886 0.0782689083009728281 0.0728246431246861031 0.0779183009350939149 0.069900033259396116 0.0712420215383051392 0.073542444248528932 0.0767792116507820116 0.0778439304287265044 0.0706109057057301009 0.0751034493617696725 0.0699400576141234881 0.0783087691931432239 0.0765286316068690131 0.073159353333281496 0.0755157467085580958 0.0766351376284479802 0.070479449352692139 0.0718806226985668067 0.0701457067916425731 0.0768776599755876117 0.0739384583091209358 0.074377412917112945 0.0801217793115618082 0.0721628274764351402 0.0806814897703578837 0.0708951835495260363 0.0715097933984782524 0.0756688379895598257 0.0740952778784406269 0.0799916253106679881 0.0747857117152906786 0.0783304419111606337 0.0727520148249376497 0.0772393904668482317 0.0753918317216105277 0.07838219159449!
 7691 0.0704261841624362062 0.0751946302171443148 0.0696449037214908473 0.0745767703531106385 0.070806479283302623 0.0709348311649994623 0.0696759908129132854 0.0703636071845372274 0.0718591719674553236 0.0765280528752297073 0.0760744758082740474 0.0823326087023871639 0.0723685391237849779 0.0779853133866276194 0.0775462066951605328 0.0753792470513584278 0.0729536446087890034 0.0725329279936358651 0.0776784049340394911 0.0800016351796151021 0.0753005795642522208 0.0787062629788652796 0.0740323485729651315 0.0751659610584929916 0.0698937309834748721 0.0789209919201119547 0.0769177427996143859 0.0767115469459719035 0.076730475615456234 0.0759158170119265813 0.079254120119077634 0.0726706650970279228 0.0732572878386490711 0.076356245513700613 0.0776622930565265546 0.0744300500286751571 0.0736199596196006334 0.0758170804080609134 0.0803895298645544015 0.0725294734781802286 0.0817744617003312163 0.0760032369969883548 0.0742856132339138608 0.0753395223184903928 0.07775838377451359!
 25 0.0758727019124877528 0.075561348375963483 0.07143844998902!
 13659 0.
0738360175084723652 0.0728876094213855852 0.0753375538997586996 0.070809484771196099 0.0750639894616385306 0.0754455419716839742 0.0785276661645328 ] ;
+bias = 100 [ -3.67929816976774715 -3.6682448752547705 0.0750909232339263683 0.0795841166700318875 0.0708342668804167325 0.0750421938062638405 0.0749057540871664457 0.07281350621284427 0.0763556633275116747 0.0782689083009728281 0.072824643124686117 0.0779183009350939287 0.0699000332593961438 0.0712420215383051392 0.073542444248528932 0.0767792116507820255 0.0778439304287265182 0.0706109057057301009 0.0751034493617696863 0.069940057614123502 0.0783087691931432239 0.0765286316068690409 0.0731593533332815099 0.0755157467085580819 0.0766351376284480079 0.070479449352692139 0.0718806226985668345 0.0701457067916425731 0.0768776599755875978 0.0739384583091209219 0.0743774129171129589 0.0801217793115617943 0.0721628274764351541 0.0806814897703578976 0.0708951835495260224 0.0715097933984782386 0.0756688379895598395 0.0740952778784406407 0.0799916253106679881 0.0747857117152906925 0.0783304419111606476 0.0727520148249376497 0.0772393904668482456 0.0753918317216105416 0.07838219159449!
 77049 0.0704261841624362062 0.0751946302171443287 0.0696449037214908612 0.0745767703531106385 0.070806479283302623 0.0709348311649994623 0.0696759908129132854 0.0703636071845372135 0.0718591719674553375 0.0765280528752296935 0.0760744758082740613 0.0823326087023871778 0.0723685391237849779 0.0779853133866276332 0.0775462066951605189 0.0753792470513583723 0.0729536446087890172 0.0725329279936358651 0.0776784049340394772 0.0800016351796151021 0.0753005795642522346 0.0787062629788652934 0.0740323485729651176 0.0751659610584930055 0.0698937309834748999 0.0789209919201119686 0.0769177427996144136 0.0767115469459718896 0.0767304756154562617 0.0759158170119265813 0.0792541201190776479 0.0726706650970279366 0.0732572878386490989 0.0763562455137006407 0.0776622930565265407 0.0744300500286751709 0.0736199596196006473 0.0758170804080609273 0.0803895298645543877 0.0725294734781802286 0.0817744617003312024 0.0760032369969883548 0.0742856132339138747 0.0753395223184904345 0.0777583837745!
 135925 0.0758727019124877389 0.0755613483759634968 0.071438449!
 98902137
98 0.0738360175084723652 0.0728876094213855713 0.0753375538997586996 0.0708094847711961267 0.0750639894616385306 0.0754455419716839742 0.0785276661645328 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
--0.520385419257819781 	-0.607667740189380523 	
--0.547335303272597051 	-0.574510722075016744 	
-0.0504029420063143579 	-0.0174374020209189066 	
+-0.52038541925781967 	-0.607667740189380523 	
+-0.547335303272597051 	-0.574510722075016633 	
+0.0504029420063143857 	-0.0174374020209189032 	
 -0.0726720177154327324 	-0.0166870493884907455 	
-0.0985284220175968173 	0.0611863005277123242 	
-0.0754034367023085489 	-0.0400349156555762711 	
-0.0789734219571437829 	-0.0396774616380166784 	
-0.108170790392739927 	-0.00765013072823587503 	
-0.0574395119975671942 	-0.0591070156090361884 	
--0.061007039891299214 	0.00667184762083029067 	
-0.0736287540104566957 	0.0253895592031331205 	
+0.0985284220175968173 	0.0611863005277123173 	
+0.0754034367023085489 	-0.0400349156555762642 	
+0.078973421957143769 	-0.0396774616380166784 	
+0.108170790392739913 	-0.00765013072823586982 	
+0.0574395119975671803 	-0.0591070156090361745 	
+-0.061007039891299214 	0.0066718476208302898 	
+0.0736287540104566957 	0.0253895592031331274 	
 -0.0777417566768631441 	0.033988152696632544 	
-0.0857932600234043719 	0.10336889613750673 	
-0.0595732996791215608 	0.0876320649751073794 	
-0.0985639370941749221 	-0.0196922373217131783 	
-0.0233660805474359655 	-0.0378252019980260187 	
+0.085793260023404358 	0.10336889613750673 	
+0.0595732996791215677 	0.0876320649751073655 	
+0.0985639370941749221 	-0.0196922373217131852 	
+0.023366080547435969 	-0.0378252019980260118 	
 -0.0169121891224506757 	-0.0268755261362537863 	
-0.105509436301355383 	0.0612729979063185298 	
-0.0295185514598919028 	0.00279503185259740924 	
+0.105509436301355369 	0.0612729979063185229 	
+0.0295185514598919063 	0.00279503185259742009 	
 0.108172318468795839 	0.0798166168810517801 	
--0.0129827416114516904 	-0.043273012491637404 	
--0.0427436926119571375 	0.0360769453606456375 	
-0.0504779492722270892 	0.0383236706034821711 	
-0.0687895317968882092 	-0.0469326547384686793 	
--0.0666313507642620623 	0.0584517370219266724 	
+-0.0129827416114516835 	-0.043273012491637404 	
+-0.0427436926119571375 	0.0360769453606456306 	
+0.0504779492722270823 	0.0383236706034821642 	
+0.0687895317968882231 	-0.0469326547384686793 	
+-0.0666313507642620761 	0.0584517370219266724 	
 0.0872457685108091324 	0.0837080805890952351 	
 0.0598201510048938442 	0.0675784349064649376 	
-0.0865288471252173008 	0.0951316360468640737 	
-0.0237140180718280573 	-0.0408629407609415601 	
-0.0631379942820747275 	0.00308499348420874066 	
-0.111272744450818944 	-0.0550198793991546892 	
--0.0495399255127684443 	-0.0544110084214884521 	
+0.0865288471252173008 	0.0951316360468640598 	
+0.0237140180718280608 	-0.0408629407609415601 	
+0.0631379942820747275 	0.00308499348420874673 	
+0.111272744450818958 	-0.0550198793991546961 	
+-0.0495399255127684165 	-0.0544110084214884313 	
 0.0657230040930184001 	0.0531562468007424349 	
 -0.0793895470235106288 	-0.0385651446308253806 	
 0.0680320003763357217 	0.0900858662837026314 	
-0.0308277344266279299 	0.109102377743976534 	
--0.0428935242523314705 	0.0609697782115297265 	
-0.0970561821731235064 	-0.0338666308471151228 	
+0.0308277344266279264 	0.10910237774397652 	
+-0.0428935242523314775 	0.0609697782115297127 	
+0.0970561821731235064 	-0.0338666308471151159 	
 -0.0596192092162303872 	-0.0408316442859519985 	
--0.00304223746333419842 	0.0449917804586676198 	
-0.0133410948519769677 	-0.0696020236616901877 	
-0.039977823239458643 	0.0611241954582473629 	
-0.0212044378814250332 	-0.0481804311018778769 	
+-0.00304223746333419452 	0.0449917804586676268 	
+0.0133410948519769729 	-0.0696020236616901877 	
+0.03997782323945865 	0.0611241954582473629 	
+0.0212044378814250263 	-0.0481804311018778769 	
 -0.0798320840154110589 	0.109307593668522079 	
-0.00216013628189292914 	-0.0601016310390021097 	
-0.0733821940828805058 	0.099497308314277666 	
--0.00418577457569986513 	0.034267274973199667 	
-0.0961049167980579933 	0.101328682742033521 	
--0.0262656255829718105 	0.0753752819357316889 	
-0.0923863548467529772 	0.0682949741016277007 	
+0.00216013628189293694 	-0.0601016310390021027 	
+0.0733821940828805336 	0.099497308314277666 	
+-0.00418577457569986339 	0.0342672749731996601 	
+0.0961049167980579794 	0.101328682742033507 	
+-0.0262656255829718105 	0.0753752819357317028 	
+0.0923863548467529772 	0.0682949741016277145 	
 0.108756281771888105 	0.0481458386888888659 	
-0.100093215436711416 	0.0963736142319992356 	
-0.110441408196684354 	0.0641597539760613289 	
-0.0742272888131287473 	0.0536989130170025392 	
+0.100093215436711402 	0.0963736142319992356 	
+0.11044140819668434 	0.064159753976061315 	
+0.0742272888131287334 	0.0536989130170025461 	
 0.0670662090171207786 	-0.0727680558490813728 	
-0.0412975622692688671 	-0.0359736262889523289 	
--0.07553369573227528 	-0.0843988974337768205 	
+0.0412975622692688532 	-0.035973626288952322 	
+-0.0755336957322752522 	-0.0843988974337767928 	
 0.031683218853364746 	0.0813611862515050827 	
--0.0130739435676640071 	-0.0345269882482395971 	
-0.0136140272169084781 	-0.0489930421672095315 	
--0.0110617238790147124 	0.0359297344246217523 	
-0.0128932585850817134 	0.0830364377215482702 	
-0.0676125872825022595 	0.039953970286474183 	
-0.00349451945220510905 	-0.042645481572695855 	
--0.0795372606084337602 	-0.0206274492374718503 	
--0.0679556269290350762 	0.0989090112668678245 	
-0.00106926327384114094 	-0.0675641733945039902 	
-0.0247260524176569838 	0.0384630055092971893 	
-0.0263782253284606909 	0.00413268501827161912 	
-0.108363320390186396 	0.0809527489082843138 	
-0.00744067734895160083 	-0.0793276345356500878 	
--0.0685828648959103254 	0.0524331672735785029 	
--0.0430299179219996233 	0.0311991517220432529 	
-0.00875455007682278755 	-0.0221527377393929692 	
-0.0396190903019199014 	-0.0299243858711141751 	
+-0.0130739435676640054 	-0.034526988248239604 	
+0.0136140272169084763 	-0.0489930421672095315 	
+-0.011061723879014709 	0.0359297344246217523 	
+0.0128932585850817186 	0.0830364377215482841 	
+0.0676125872825022733 	0.039953970286474183 	
+0.00349451945220510861 	-0.0426454815726958619 	
+-0.0795372606084337741 	-0.0206274492374718434 	
+-0.0679556269290350762 	0.0989090112668678106 	
+0.00106926327384114202 	-0.0675641733945039902 	
+0.0247260524176569838 	0.0384630055092971754 	
+0.026378225328460677 	0.00413268501827163039 	
+0.108363320390186382 	0.0809527489082843277 	
+0.00744067734895159909 	-0.0793276345356500878 	
+-0.0685828648959103254 	0.0524331672735784751 	
+-0.0430299179219996233 	0.0311991517220432564 	
+0.00875455007682279102 	-0.0221527377393929761 	
+0.0396190903019198737 	-0.0299243858711141855 	
 -0.0259299398002285457 	-0.0553743962254750755 	
-0.0654815998567056939 	0.0381233449386994203 	
-0.0857406226041131786 	0.000868266886166540347 	
-0.0818156967821113135 	-0.0819606571238428866 	
--0.0510867284702254815 	0.0130209282921754654 	
-0.0185073534397708732 	0.0332937538289315854 	
-0.0564972252169523848 	0.0187982842748549012 	
+0.0654815998567056939 	0.0381233449386994272 	
+0.0857406226041131925 	0.000868266886166538395 	
+0.0818156967821113412 	-0.0819606571238428866 	
+-0.0510867284702254815 	0.0130209282921754724 	
+0.0185073534397708767 	0.0332937538289315785 	
+0.0564972252169523639 	0.0187982842748548977 	
 -0.0595800360354162978 	0.0745776702050969659 	
--0.0626526219961449915 	-0.0481068295040314869 	
-0.0760290724928658762 	0.0318015323295316593 	
--0.0680175615269424105 	-0.0780292036804604849 	
-0.0454265273931732799 	-0.0379064295013739685 	
-0.000245257330670842243 	0.0561754669591406422 	
-0.0266789063222490318 	-0.00109026801003190839 	
--0.0654262101077237451 	0.0254095633358379092 	
--0.0161558737254881811 	0.0270927473390010463 	
-0.0315749936922243846 	-0.0121516449151251368 	
-0.0458707558030447271 	0.0955607640219145738 	
-0.0311126203336938266 	0.0378846159519728512 	
--0.00581538096481792107 	0.104733200172821811 	
-0.0640125283786390359 	-0.0373943635204284225 	
+-0.0626526219961449776 	-0.0481068295040314731 	
+0.0760290724928658901 	0.0318015323295316663 	
+-0.0680175615269424244 	-0.0780292036804604988 	
+0.045426527393173266 	-0.0379064295013739616 	
+0.000245257330670841593 	0.0561754669591406422 	
+0.0266789063222490352 	-0.00109026801003190752 	
+-0.065426210107723759 	0.0254095633358379196 	
+-0.0161558737254881811 	0.0270927473390010429 	
+0.0315749936922243776 	-0.0121516449151251385 	
+0.0458707558030447202 	0.0955607640219145738 	
+0.0311126203336938301 	0.0378846159519728373 	
+-0.00581538096481791413 	0.104733200172821811 	
+0.0640125283786390359 	-0.0373943635204284364 	
 0.0738255534250779194 	0.0867777875959012679 	
--0.0404066042800288877 	0.0760341306135597778 	
+-0.0404066042800288808 	0.0760341306135597778 	
 0.0678668982597476489 	-0.0441082122306330204 	
-0.00207766362528058642 	-0.0638437672563030162 	
+0.00207766362528058381 	-0.0638437672563030023 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -157,6 +159,7 @@
 input_size = 2 ;
 output_size = 100 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *6 ->PRandom(
@@ -178,14 +181,15 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0113262925564693046 	0.00641492602461607182 	-0.00159847381051862942 	-0.00789459485609226956 	0.00244219358215985944 	-0.00812615500197023596 	-0.00389482222121642196 	0.00389091998488833198 	-0.00541050652884287209 	-0.00262556755390811651 	0.00952868237595928119 	-0.00969494151094163654 	-0.00706508174692422036 	-0.00727612306607582256 	-0.00846038975666717902 	0.000773966054525185103 	-0.00583361570628981082 	-0.00201278590637816524 	0.00872572454424036872 	-0.00165701502752675081 	-0.00799891102918076635 	6.47096589051986929e-05 	-0.00973055555769559467 	-0.00215375660638213347 	0.00716715790392904832 	-0.001680404187211998 	-0.00686749154366765591 	0.00769175181889198388 	-0.00685037181174525441 	0.00827079361448006603 	0.00779395048534614783 	0.0026080574091930527 	-5.93787847705065132e-05 	-0.0076190130320924563 	0.00277648344971009181 	0.000531124508030603811 	-0.00733566395363501739 	0.00928862170370055307 	0.000345824306290504408 	0.00218347321736047728 	-0.008!
 79584583154094032 	-0.00473551449789763948 	-0.0031015010128094731 	0.00679600317669165534 	0.000498927731410179305 	0.00534312277919853432 	0.00347021641974513614 	0.00337220069515059215 	-0.00622143586125560415 	-0.00127655934433769662 	0.00436484410802257493 	0.00725999145664699315 	-0.00881265446020709428 	-0.00919797829396393901 	0.00462042560691856407 	-0.00919982500002390277 	-0.00889966122589246761 	-0.000627033368822296744 	-0.00140435990570596876 	0.00348752003687192004 	-0.00440996403305462647 	0.00445091226375020094 	0.0015319941118297879 	0.00824276166398773061 	-0.00589422826971325569 	0.00185054823285728774 	0.00735205392812524408 	-0.00932065008082455515 	-0.000164838547741596135 	-0.00854643924540077551 	-0.00311837434025635969 	0.00638201655108370895 	0.00754628751573688337 	-0.00801299059807394562 	0.00976747753260353083 	0.00557945832622598354 	0.00675698139843258425 	-0.00560185246073486697 	-0.00964174761582759464 	-7.80623115802627968e-05 	0.008710366!
 82392008384 	-0.00725561868064580808 	0.00559531193002933518 	!
 -0.00346
921625696461778 	0.00740028832034525172 	-0.00169509809182802015 	0.00873888335497757969 	0.00140994409075873408 	0.000764624405132731822 	0.00994480655086805825 	0.00577877274202991901 	4.74923655357142787e-05 	0.00113531960691515082 	0.00198775187931363737 	-0.00960880431385865812 	0.00658056776284521652 	-0.00755775334700503447 	0.00570729827446314186 	-0.00240907164842276648 	0.00409745863965042811 	
--0.0101580843643666735 	0.00220138685265899988 	0.00692342144895578479 	0.00135253779638701537 	0.00758775338859351851 	-0.00865001298121899007 	0.00640432763468906825 	0.0073460586540288822 	0.00283515709363264812 	-0.0068139694357583977 	0.0082013411590092164 	0.00112607669227921393 	0.00574348711096823051 	-0.0019457453138970579 	0.00329305905762568817 	0.00804650884764271379 	0.00239784445053587 	0.00133416850160433375 	0.00696031416500525121 	-0.00361325852754340766 	0.00317535270330150622 	0.00103041205817527566 	-0.00107245742990319201 	0.00824480614344477725 	-0.00809761517277060063 	-0.00879378992409048099 	-0.001666700745558108 	-0.00611935317774526567 	0.00666747922470935835 	0.0043264904697058338 	-0.00400025833426162318 	-0.00463514088619843001 	-0.00790622645554470496 	0.0094407604217145117 	-0.00757762707639256846 	-0.00123984285446245295 	-0.00897459440508197558 	0.000655808806812931914 	0.00231099853159869245 	-0.00825376021216044954 	-0.0017404815538026854!
 5 	0.00564452860462577415 	0.00216398564598699284 	0.00972317260517676155 	0.0005894678762475515 	-0.00478986565376872571 	-0.00236489833631728895 	0.00300955478073557494 	0.00195524074096567555 	-0.00824217502136400347 	0.000893913377985541715 	0.00245114206440323265 	-0.00658616856263531496 	0.00207207872110719196 	-0.0059082778485762379 	0.00112430177257065445 	0.00526390526226897665 	0.00672078045843925789 	0.00946230513202510858 	-0.0094489559537055054 	0.00809963710105650672 	0.00668437931449794315 	0.00840677515883614737 	0.00155183361481183758 	0.00554616377539399793 	0.00369188595568001798 	0.00893215807626812457 	0.003872407659562599 	-0.000373571987340624682 	-0.000527998815233036035 	-0.00956064192855578254 	0.00032968408208451953 	0.00668352527862810675 	0.00965851149565612362 	0.00987050937326928828 	-0.00997917409533064133 	-0.00523683447063628092 	-0.00363046515829083816 	-0.000876817464300998911 	-0.000631879126818990543 	-0.000721424646885172405 	-0.002895!
 69358676195973 	-0.00880904917847242369 	-0.002912132184390313!
 35 	0.00
754820876362416199 	-0.00574797924889403674 	-0.00355974802588750888 	0.00773960097716254111 	0.000825076402684549659 	-0.00337517142319696678 	0.00729551532351534193 	-0.00575095374993644676 	0.00784142735362793226 	-0.000854038366632214494 	0.00559184029737643234 	0.00466771002147996564 	-0.00891365004316409248 	0.00999552474884863457 	0.00559196870394438553 	-0.00440332167642452574 	
+0.0113262925564693168 	0.00641492602461605013 	-0.00159847381051862899 	-0.00789459485609227303 	0.00244219358215985814 	-0.0081261550019702377 	-0.00389482222121642152 	0.00389091998488833111 	-0.00541050652884287036 	-0.00262556755390811825 	0.00952868237595928119 	-0.00969494151094163828 	-0.00706508174692422209 	-0.00727612306607582256 	-0.00846038975666718249 	0.000773966054525186621 	-0.00583361570628981342 	-0.00201278590637816437 	0.00872572454424036352 	-0.00165701502752675124 	-0.00799891102918076635 	6.47096589051982863e-05 	-0.00973055555769559294 	-0.00215375660638213173 	0.00716715790392904832 	-0.00168040418721199583 	-0.00686749154366765677 	0.00769175181889198388 	-0.00685037181174525354 	0.00827079361448006777 	0.00779395048534614263 	0.00260805740919305227 	-5.93787847705055239e-05 	-0.00761901303209245716 	0.00277648344971009441 	0.000531124508030605003 	-0.00733566395363501652 	0.00928862170370055307 	0.000345824306290504029 	0.00218347321736047685 	-0.!
 00879584583154094379 	-0.00473551449789763775 	-0.0031015010128094731 	0.00679600317669165795 	0.000498927731410178546 	0.00534312277919853518 	0.00347021641974513528 	0.00337220069515059041 	-0.00622143586125560328 	-0.00127655934433769597 	0.00436484410802257493 	0.00725999145664699228 	-0.00881265446020709255 	-0.00919797829396393728 	0.00462042560691856407 	-0.00919982500002390104 	-0.00889966122589246934 	-0.000627033368822297178 	-0.00140435990570596876 	0.00348752003687191961 	-0.00440996403305462647 	0.00445091226375020181 	0.00153199411182978963 	0.00824276166398772887 	-0.00589422826971325482 	0.00185054823285728731 	0.00735205392812524582 	-0.00932065008082455862 	-0.000164838547741595403 	-0.00854643924540077898 	-0.00311837434025636013 	0.00638201655108370635 	0.00754628751573688424 	-0.00801299059807394388 	0.00976747753260353083 	0.00557945832622598354 	0.00675698139843258164 	-0.00560185246073486524 	-0.00964174761582759464 	-7.80623115802624038e-05 	0.00871!
 036682392008037 	-0.00725561868064580548 	0.005595311930029335!
 18 	-0.0
0346921625696461865 	0.00740028832034525432 	-0.00169509809182802015 	0.00873888335497757102 	0.00140994409075873494 	0.000764624405132729979 	0.00994480655086805825 	0.00577877274202991901 	4.74923655357136959e-05 	0.0011353196069151506 	0.00198775187931363694 	-0.00960880431385865638 	0.00658056776284521566 	-0.00755775334700503273 	0.00570729827446314446 	-0.00240907164842276734 	0.00409745863965042725 	
+-0.0101580843643666804 	0.00220138685265901506 	0.00692342144895578045 	0.00135253779638701581 	0.00758775338859351851 	-0.00865001298121899007 	0.00640432763468906565 	0.0073460586540288822 	0.00283515709363264898 	-0.00681396943575839856 	0.00820134115900921293 	0.00112607669227921393 	0.00574348711096823398 	-0.00194574531389705811 	0.00329305905762568817 	0.00804650884764271553 	0.00239784445053587304 	0.00133416850160433245 	0.00696031416500525382 	-0.00361325852754340766 	0.00317535270330150578 	0.00103041205817527436 	-0.00107245742990319179 	0.00824480614344477378 	-0.00809761517277059889 	-0.00879378992409047752 	-0.001666700745558108 	-0.00611935317774526394 	0.00666747922470936009 	0.00432649046970583293 	-0.00400025833426162231 	-0.00463514088619843348 	-0.0079062264555447067 	0.00944076042171451517 	-0.00757762707639256846 	-0.00123984285446245143 	-0.00897459440508197211 	0.000655808806812932781 	0.00231099853159869245 	-0.00825376021216044781 	-0.001740481553!
 80268415 	0.00564452860462577328 	0.00216398564598699068 	0.00972317260517675808 	0.0005894678762475515 	-0.00478986565376872571 	-0.00236489833631729068 	0.00300955478073557537 	0.00195524074096567512 	-0.008242175021364 	0.000893913377985542366 	0.00245114206440323395 	-0.00658616856263531582 	0.00207207872110719412 	-0.00590827784857623703 	0.0011243017725706564 	0.00526390526226897665 	0.00672078045843925789 	0.00946230513202510858 	-0.00944895595370550713 	0.00809963710105650325 	0.00668437931449794229 	0.00840677515883615084 	0.00155183361481183628 	0.00554616377539399619 	0.00369188595568001928 	0.00893215807626812804 	0.0038724076595626016 	-0.00037357198734062414 	-0.000527998815233036468 	-0.00956064192855578081 	0.000329684082084518717 	0.00668352527862810502 	0.00965851149565611841 	0.00987050937326929348 	-0.0099791740953306448 	-0.00523683447063627659 	-0.00363046515829083902 	-0.000876817464300999236 	-0.000631879126818990543 	-0.000721424646885170237 	-0.002!
 8956935867619593 	-0.00880904917847242369 	-0.0029121321843903!
 1248 	0.
00754820876362416286 	-0.00574797924889403674 	-0.00355974802588750845 	0.00773960097716254198 	0.000825076402684549767 	-0.00337517142319696765 	0.00729551532351534106 	-0.0057509537499364459 	0.00784142735362793053 	-0.000854038366632214711 	0.0055918402973764306 	0.00466771002147996824 	-0.00891365004316409248 	0.00999552474884863457 	0.0055919687039443864 	-0.00440332167642452834 	
 ]
 ;
-bias = 2 [ -0.00274640092009917911 0.00274640092009921337 ] ;
+bias = 2 [ -0.0027464009200991687 0.00274640092009921163 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *9 ->PRandom(
@@ -195,12 +199,14 @@
 *10 ->SoftmaxModule(
 input_size = 2 ;
 name = "SoftmaxModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
 ] ;
 n_modules = 2 ;
 name = "ModuleStackModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -211,6 +217,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "NLLCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -219,6 +226,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "ClassErrorCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -229,6 +237,7 @@
 input_size = 2 ;
 output_size = 3 ;
 name = "CombiningCostsModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -238,6 +247,7 @@
 input_size = 100 ;
 output_size = 1 ;
 name = "NLLCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave	2007-06-07 14:43:22 UTC (rev 7549)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave	2007-06-07 15:42:58 UTC (rev 7550)
@@ -19,6 +19,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3 ->PRandom(
@@ -33,116 +34,117 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -4.88348781864529524 -4.88377909666199628 0.0998136183975280034 0.105471817501073581 0.0944416603126441712 0.0997535283630023306 0.0995807079469193529 0.0969420280226542391 0.101407798986863071 0.103819719061399696 0.0969561262386221256 0.103379686484071684 0.0932604806258945929 0.0949581076326168616 0.0978612202826870531 0.101940630727569329 0.103281189924958716 0.0941595431112412368 0.0998305935628365643 0.0933123105594512864 0.103865433445130051 0.101629187737637211 0.0973778633969569346 0.100349460306733224 0.101767253426589127 0.0939952628691959613 0.0957642872874144568 0.0935745523905245069 0.102064003256194025 0.098361691863909001 0.0989175053269632093 0.10614746415871347 0.0961217644556267847 0.106849253706285979 0.0945216452223907599 0.0952991047808851438 0.100547639463152691 0.0985605833598848236 0.105983428725311607 0.0994327692125842183 0.103893097444509377 0.0968643024791368407 0.102520198423240214 0.100204431378448755 0.103958775859172015 0.093929!
 0211077105641 0.0999474375184054992 0.0929393923414386508 0.0991701323486275238 0.0944082201173512409 0.0945697106806672877 0.092979164059244182 0.093847082093861306 0.0957359700320323803 0.101626912170330402 0.101052854003951226 0.108921157875693647 0.0963814492151018748 0.103458632943262963 0.10290812806515362 0.100178659212683596 0.0971214500704266626 0.0965871618141980054 0.103073960705580364 0.105996494971855496 0.100087602274505388 0.104366272501995302 0.0984793481047083058 0.0999092875429416216 0.093252358955243822 0.104637234166777962 0.102122172258910154 0.10185947931508095 0.101878379680090225 0.100853886423779154 0.10505724942419796 0.0967631186351052552 0.0975018372905746139 0.101409956557437175 0.103055880161497665 0.0989832218691384935 0.0979591861849546941 0.100737252815511005 0.106483496226710883 0.0965834007256001076 0.108223002610933108 0.100965208043670845 0.0988008819013017625 0.100127932147128867 0.103179296286824859 0.100801354336986601 0.1004078925789!
 24268 0.0952068316396505465 0.0982331855345690008 0.0970389632!
 59577809
6 0.100125690380371446 0.094411978606329508 0.0997856098841084227 0.100261110879061907 0.104142637036895802 ] ;
+bias = 100 [ -4.88348781864529524 -4.8837790966619945 0.0998136183975280172 0.105471817501073567 0.0944416603126441573 0.0997535283630023167 0.0995807079469193807 0.0969420280226542391 0.101407798986863112 0.103819719061399696 0.0969561262386221118 0.103379686484071684 0.0932604806258945512 0.0949581076326168755 0.0978612202826870392 0.101940630727569342 0.103281189924958716 0.0941595431112412506 0.0998305935628365643 0.0933123105594513141 0.103865433445130023 0.101629187737637239 0.0973778633969569346 0.100349460306733224 0.101767253426589127 0.0939952628691959613 0.0957642872874144846 0.0935745523905245069 0.102064003256194066 0.098361691863909001 0.0989175053269632093 0.10614746415871347 0.0961217644556267709 0.106849253706285993 0.0945216452223907322 0.0952991047808851438 0.100547639463152719 0.0985605833598848236 0.105983428725311621 0.0994327692125842322 0.103893097444509405 0.0968643024791368545 0.102520198423240241 0.100204431378448769 0.103958775859172015 0.0939290!
 211077105641 0.099947437518405513 0.0929393923414386647 0.0991701323486275238 0.094408220117351227 0.0945697106806672877 0.0929791640592441959 0.0938470820938613198 0.0957359700320323803 0.101626912170330416 0.101052854003951254 0.108921157875693661 0.0963814492151018887 0.103458632943262935 0.10290812806515362 0.100178659212683582 0.0971214500704266626 0.0965871618141980054 0.103073960705580392 0.105996494971855509 0.100087602274505402 0.104366272501995289 0.0984793481047083058 0.0999092875429416077 0.0932523589552438636 0.104637234166777962 0.10212217225891014 0.10185947931508095 0.101878379680090239 0.100853886423779154 0.105057249424197946 0.0967631186351052691 0.0975018372905746139 0.101409956557437189 0.103055880161497679 0.0989832218691384519 0.0979591861849546941 0.100737252815511033 0.106483496226710855 0.096583400725600066 0.108223002610933108 0.100965208043670873 0.0988008819013017348 0.100127932147128867 0.103179296286824845 0.100801354336986573 0.10040789257892!
 4282 0.0952068316396505326 0.0982331855345689731 0.09703896325!
 95778512
 0.10012569038037146 0.0944119786063295219 0.0997856098841084088 0.100261110879061879 0.104142637036895788 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
--0.595479171538772722 	-0.64784326563401029 	
--0.6152383374774405 	-0.62664605667428297 	
+-0.595479171538772389 	-0.647843265634009846 	
+-0.615238337477441055 	-0.626646056674283858 	
 0.0538900294397439625 	-0.0137742833948165666 	
--0.06875053142052448 	-0.0128356405145059246 	
-0.101750482831042335 	0.0645179709928756923 	
-0.0788404196343822777 	-0.0363313352895024214 	
-0.0823984677889268713 	-0.0359772385695514424 	
-0.111455643794722073 	-0.0041048261939523184 	
-0.0609652140478198282 	-0.0553056639846301532 	
--0.0571686153486568316 	0.0104202988290012701 	
-0.0769767542086592094 	0.0288702909576745818 	
+-0.06875053142052448 	-0.0128356405145059228 	
+0.101750482831042349 	0.0645179709928756923 	
+0.0788404196343822639 	-0.0363313352895024075 	
+0.0823984677889268713 	-0.0359772385695514493 	
+0.111455643794722045 	-0.00410482619395231754 	
+0.0609652140478198074 	-0.0553056639846301532 	
+-0.0571686153486568316 	0.0104202988290012649 	
+0.0769767542086591816 	0.0288702909576745818 	
 -0.0738826113496724585 	0.0376651648655449978 	
-0.0890025291790312034 	0.10658630388745495 	
-0.0628827179229135558 	0.0909309318538381317 	
-0.10189692292911387 	-0.0160894138207842603 	
-0.0269756763337721632 	-0.0340491698234521578 	
--0.0131781351491743748 	-0.0230764112596843103 	
-0.108709363233458617 	0.0645946658963587189 	
-0.0330446970922479225 	0.00641644683858688409 	
+0.0890025291790311757 	0.10658630388745495 	
+0.0628827179229135558 	0.0909309318538381178 	
+0.10189692292911387 	-0.0160894138207842638 	
+0.0269756763337721528 	-0.0340491698234521578 	
+-0.0131781351491743713 	-0.0230764112596843068 	
+0.108709363233458603 	0.064594665896358705 	
+0.0330446970922479294 	0.00641644683858688929 	
 0.111338474011847738 	0.0830744128480884469 	
--0.00923663549878874814 	-0.039419952693748761 	
--0.0390168467135288966 	0.0396927629301102661 	
-0.053884855284114519 	0.04179601820019576 	
-0.0722589195882538532 	-0.0431916351119050954 	
+-0.00923663549878875507 	-0.039419952693748761 	
+-0.0390168467135288966 	0.039692762930110273 	
+0.053884855284114512 	0.0417960182001957531 	
+0.0722589195882538671 	-0.0431916351119050954 	
 -0.0628580553109818196 	0.0620282656863879525 	
 0.0904696456760091527 	0.0869795013810659712 	
-0.063155692471325367 	0.0709411651936545651 	
-0.089737672806708732 	0.0983663784623535764 	
-0.0273272832467084566 	-0.0370754352212264021 	
-0.0665515477695941549 	0.0066554552858713676 	
-0.114615148493610874 	-0.0513201825260645936 	
--0.0456422913795766633 	-0.0504568195824679211 	
-0.0690561056867186052 	0.0565546631705502456 	
--0.0754060455133410135 	-0.0346219815131878009 	
+0.063155692471325367 	0.070941165193654579 	
+0.089737672806708732 	0.0983663784623535487 	
+0.0273272832467084739 	-0.0370754352212263952 	
+0.0665515477695941687 	0.00665545528587137454 	
+0.114615148493610861 	-0.0513201825260646144 	
+-0.0456422913795766633 	-0.0504568195824679072 	
+0.0690561056867186052 	0.0565546631705502387 	
+-0.0754060455133409996 	-0.0346219815131878009 	
 0.0713063685070573078 	0.0933620354169992783 	
-0.0341980957421883225 	0.112370604562321522 	
--0.0392048865688329473 	0.0645005741897170942 	
-0.100413128501990342 	-0.0302192489756252305 	
--0.0557068816320919549 	-0.0369117849033234677 	
-0.000529733929197445555 	0.0485162644411036431 	
-0.0170355689113608696 	-0.0656936836103067801 	
-0.0433878525009998087 	0.0645360645558576557 	
-0.0248375275797735988 	-0.0443637944730375239 	
--0.0760831140867710143 	0.112732191490474212 	
-0.00587915934077326303 	-0.0562115972021530463 	
-0.0766272396820571744 	0.102736863234715051 	
--0.000592684940426663198 	0.0378303322767531494 	
-0.0992806567766493231 	0.104535562881183208 	
--0.0226534723321051186 	0.078833337275821494 	
-0.0956148861294622843 	0.0716084093953505973 	
+0.0341980957421883225 	0.112370604562321508 	
+-0.0392048865688329473 	0.0645005741897170803 	
+0.100413128501990329 	-0.030219248975625241 	
+-0.0557068816320919549 	-0.0369117849033234746 	
+0.000529733929197459758 	0.0485162644411036431 	
+0.0170355689113608801 	-0.0656936836103067801 	
+0.0433878525009998087 	0.0645360645558576418 	
+0.0248375275797735953 	-0.0443637944730375169 	
+-0.0760831140867710004 	0.112732191490474185 	
+0.00587915934077326736 	-0.0562115972021530394 	
+0.0766272396820571744 	0.102736863234715037 	
+-0.000592684940426658319 	0.0378303322767531494 	
+0.0992806567766492953 	0.104535562881183194 	
+-0.0226534723321051186 	0.0788333372758215217 	
+0.0956148861294622843 	0.071608409395350639 	
 0.111961983119019282 	0.051503114001920508 	
-0.103261697334856059 	0.0995893663062659279 	
+0.103261697334856045 	0.0995893663062659279 	
 0.113622132076437066 	0.0674654151128029206 	
-0.077537126133991538 	0.0570887588626447992 	
-0.0705810984356155524 	-0.0689366435996839194 	
-0.044843023134881678 	-0.0322309100088603792 	
--0.0714869712521966627 	-0.0802860339949844209 	
-0.0350917134068162717 	0.0847180859020596616 	
--0.00934136513864572242 	-0.0307058733146445689 	
+0.0775371261339915241 	0.0570887588626447992 	
+0.0705810984356155524 	-0.0689366435996839055 	
+0.044843023134881671 	-0.0322309100088603862 	
+-0.0714869712521966627 	-0.0802860339949844071 	
+0.0350917134068162787 	0.0847180859020596616 	
+-0.00934136513864572762 	-0.0307058733146445759 	
 0.017274684402242526 	-0.0451641166704254277 	
--0.00744338758825806822 	0.0394987995010958187 	
+-0.00744338758825805347 	0.0394987995010958187 	
 0.01635978807368689 	0.0864131668208693443 	
-0.070961277310940693 	0.043396773338633815 	
-0.00718060967436378282 	-0.0388226740324390598 	
--0.0755833033596572668 	-0.0167504198245917932 	
--0.0642340161003093629 	0.102350333481158032 	
-0.00480424872243435854 	-0.0636443844298318767 	
-0.0282189683208145629 	0.0419719166047329909 	
-0.0299132095892969477 	0.0077544045179868985 	
-0.111530306999090334 	0.0842099709862055845 	
-0.0111726585535115806 	-0.0753750532311086713 	
--0.0647904300854395543 	0.0560328728134857762 	
--0.0392948976134837868 	0.0348323569795132501 	
-0.012391516944369076 	-0.0184093328026520009 	
-0.0431605911084246147 	-0.0262028154072604068 	
--0.0221174993040646715 	-0.0514567367068857959 	
-0.0688357274057207064 	0.0415705062017272323 	
-0.089084785497041255 	0.00441555037709988676 	
-0.0852942013840160274 	-0.0781125130895272968 	
--0.0472940655022145579 	0.0167306727323274437 	
-0.0220238553939347825 	0.0368257958383149744 	
-0.0599111578444758708 	0.0223267883631605574 	
--0.0558570133839886712 	0.0780883336256121968 	
--0.058716698004954028 	-0.0441544608089798341 	
-0.0793606765007453807 	0.0352579600100396198 	
--0.0640094082428914329 	-0.0739527557808974945 	
-0.0489600429692438679 	-0.0341678748008232419 	
-0.00379375700322669715 	0.0596590822724617914 	
-0.0302207297680062159 	0.00254921219936410934 	
--0.0616037449568939341 	0.0290990160924210692 	
--0.0125089212557680796 	0.0306990708174415296 	
-0.0351162640727813863 	-0.00848184184674327137 	
-0.0492138445592737589 	0.0988530120564841547 	
-0.0345815650076597633 	0.0413837339984958752 	
--0.00231189629031252003 	0.1080671288555725 	
-0.0674832518044770763 	-0.0336835252022576678 	
-0.0770889815106832327 	0.0900588562201595916 	
--0.0367474445651055151 	0.0795105696210816892 	
+0.0709612773109407208 	0.0433967733386337942 	
+0.00718060967436377762 	-0.0388226740324390668 	
+-0.0755833033596572668 	-0.0167504198245917793 	
+-0.0642340161003093352 	0.102350333481158032 	
+0.0048042487224343542 	-0.0636443844298318767 	
+0.0282189683208145595 	0.0419719166047329909 	
+0.0299132095892969373 	0.00775440451798690283 	
+0.111530306999090348 	0.0842099709862055845 	
+0.0111726585535115806 	-0.0753750532311086435 	
+-0.0647904300854395682 	0.0560328728134857693 	
+-0.0392948976134838007 	0.0348323569795132501 	
+0.0123915169443690812 	-0.0184093328026520113 	
+0.0431605911084246008 	-0.0262028154072604068 	
+-0.022117499304064675 	-0.0514567367068857959 	
+0.068835727405720748 	0.0415705062017272323 	
+0.0890847854970412689 	0.00441555037709988676 	
+0.0852942013840160274 	-0.0781125130895272829 	
+-0.0472940655022145579 	0.0167306727323274472 	
+0.0220238553939347895 	0.0368257958383149606 	
+0.0599111578444758569 	0.0223267883631605608 	
+-0.055857013383988692 	0.0780883336256121829 	
+-0.058716698004954021 	-0.0441544608089798271 	
+0.0793606765007453946 	0.0352579600100396198 	
+-0.0640094082428914191 	-0.0739527557808974806 	
+0.0489600429692438541 	-0.0341678748008232419 	
+0.00379375700322669976 	0.0596590822724617845 	
+0.0302207297680062124 	0.00254921219936410327 	
+-0.0616037449568939341 	0.0290990160924210796 	
+-0.0125089212557680778 	0.0306990708174415226 	
+0.0351162640727813793 	-0.00848184184674327657 	
+0.049213844559273745 	0.0988530120564841547 	
+0.0345815650076597564 	0.0413837339984958544 	
+-0.00231189629031251787 	0.1080671288555725 	
+0.0674832518044770763 	-0.0336835252022576748 	
+0.0770889815106832049 	0.0900588562201595916 	
+-0.0367474445651055082 	0.0795105696210817031 	
 0.0713351233590624745 	-0.0403765780191503165 	
-0.00580328195072377442 	-0.0599406438638086306 	
+0.00580328195072376662 	-0.0599406438638086306 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -157,6 +159,7 @@
 input_size = 2 ;
 output_size = 100 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *6 ->PRandom(
@@ -178,14 +181,15 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0123420928014191557 	0.00916499543260136142 	-0.00164506058869937706 	-0.00792958713804811817 	0.00236549116291899161 	-0.00816839208649198199 	-0.00393729201208150355 	0.00383571983020708843 	-0.00544728800267464154 	-0.00266053897009083237 	0.00946810364372980065 	-0.00972470140984709429 	-0.00715049251794375867 	-0.00734869875953185047 	-0.00851045478860876919 	0.000733198497111745766 	-0.00587341695528064214 	-0.00209131432022617624 	0.00867743624742249239 	-0.0017424290544537477 	-0.00803777150919455716 	2.68947234888736322e-05 	-0.00978963001083593166 	-0.00219383685419234338 	0.00713629807466046543 	-0.00176071907009400916 	-0.00693551341442328901 	0.00760840838401072046 	-0.00689064362225262801 	0.00821779551345410038 	0.00775615975335418317 	0.00256994077777214979 	-0.000125487652430558499 	-0.00765557075172304709 	0.00270057028444748272 	0.00046445864750938883 	-0.00737340071900496018 	0.00924350722960627647 	0.000308359129016674426 	0.00213576384551587362 	-0.0!
 088316481921647573 	-0.00479667378996827178 	-0.00314050304959243281 	0.00677359813450965909 	0.000461574357387472775 	0.0052630551506867209 	0.00342377191963190138 	0.00328382212079000564 	-0.00626455935271803908 	-0.0013538271142407624 	0.00428995531271315206 	0.00717178746011403519 	-0.0088934328165411157 	-0.00926633853826809052 	0.00458758045120281812 	-0.00924171707330578331 	-0.00894077273613546816 	-0.000689382609061560837 	-0.00144386531203365576 	0.00344874682264902479 	-0.00445501595657440182 	0.004394399096584377 	0.00146877925949968121 	0.00820337445445518988 	-0.00592893418508044692 	0.00182255472740502585 	0.00731552216505707183 	-0.0093740454197580262 	-0.000212912355459384688 	-0.00863228712512623987 	-0.00315315105065477495 	0.00635133933025240714 	0.00750852663533482529 	-0.00805517934762344651 	0.00972441442848316455 	0.00554144065331621354 	0.0066946947510606019 	-0.00565723229911999602 	-0.00967100372334113619 	-0.00011446669224282846 	0.00865912090766!
 649163 	-0.00731143980482081186 	0.005563065733680898 	-0.0035!
 07002722
15526175 	0.00733748393075391286 	-0.00173505410219775436 	0.0086972297748011216 	0.00136029609347290388 	0.000717391705312010676 	0.00991167438484492555 	0.00573543123204991317 	1.6357813687929368e-06 	0.0010656448048303839 	0.00193302874506671692 	-0.00966095349810980504 	0.00653802734483208657 	-0.00763470234441813913 	0.00566876259818077221 	-0.00244991794631276147 	0.00406053318954284741 	
--0.011173884609316509 	-0.000548682555326316496 	0.00697000822713652116 	0.00138753007834285054 	0.00766445580783437982 	-0.00860777589669724404 	0.00644679742555415245 	0.00740125880871012228 	0.002871938567464418 	-0.00677899801957569919 	0.0082619198912386952 	0.00115583659118469271 	0.00582889788198777143 	-0.00187316962044102499 	0.00334312408956729222 	0.00808727640505616137 	0.00243764569952671997 	0.00141269691545234518 	0.00700860246182310586 	-0.00352784450061643289 	0.00321421318331525842 	0.00106822699359159987 	-0.0010133829767628522 	0.00828488639125499193 	-0.00806675534350202728 	-0.00871347504120848111 	-0.00159867887480246037 	-0.00603600974286399965 	0.00670775103521673542 	0.00437948857073182807 	-0.00396246760226966373 	-0.00459702425477752624 	-0.00784011758788464436 	0.00947731814134509035 	-0.00750171391112993986 	-0.00117317699394124133 	-0.00893685763971203972 	0.000700923280907215672 	0.0023484637088725213 	-0.00820605084031584588 	-0.001704679193!
 17889645 	0.00570568789669640732 	0.00220298768276995212 	0.00974557764735873959 	0.000626821250270258572 	-0.00470979802525691143 	-0.00231845383620405418 	0.00309793335509616231 	0.00199836423242812436 	-0.00816490725146091709 	0.000968802173294958623 	0.00253934606093618453 	-0.006505390206301284 	0.00214043896541134737 	-0.00587543269286049195 	0.00116619384585259201 	0.0053050167725119373 	0.00678312969867851581 	0.00950181053835282355 	-0.00941018273948261187 	0.00814468902457630116 	0.0067408924816637671 	0.00846999001116627986 	0.0015912208243443885 	0.00558086969076118568 	0.00371987946113228442 	0.00896868983933632111 	0.00392580299849605227 	-0.000325498179622836156 	-0.000442150935507603716 	-0.00952586521815736338 	0.000360361302915814029 	0.0067212861590301657 	0.0097007002452056193 	0.00991357247738965976 	-0.00994115642242088086 	-0.00517454782326429597 	-0.00357508531990572255 	-0.000847561356787462027 	-0.000595474746156425408 	-0.000670178730631581176 	-0!
 .00283987246258697937 	-0.00877680298212393101 	-0.00287434571!
 91996672
1 	0.00761101315321550433 	-0.00570802323852433353 	-0.00351809444571104602 	0.00778924897444837824 	0.000872309102505269395 	-0.00334203925717386965 	0.00733885683349534603 	-0.00570509716576950762 	0.00791110215571273409 	-0.000799315232385289817 	0.00564398948162753416 	0.00471025043949309646 	-0.00883670104575096267 	0.0100340604251309921 	0.00563281500183433932 	-0.0043663962263169433 	
+0.0123420928014191765 	0.00916499543260136662 	-0.00164506058869937728 	-0.0079295871380481147 	0.00236549116291899161 	-0.00816839208649198199 	-0.00393729201208150269 	0.00383571983020708452 	-0.00544728800267464067 	-0.00266053897009083237 	0.00946810364372980412 	-0.00972470140984709602 	-0.00715049251794375867 	-0.00734869875953184874 	-0.00851045478860876746 	0.000733198497111746091 	-0.00587341695528064475 	-0.00209131432022617667 	0.00867743624742248892 	-0.00174242905445374857 	-0.00803777150919455716 	2.68947234888729918e-05 	-0.00978963001083593166 	-0.00219383685419234295 	0.00713629807466046717 	-0.0017607190700940085 	-0.00693551341442329074 	0.00760840838401072306 	-0.00689064362225262714 	0.00821779551345409691 	0.00775615975335418317 	0.00256994077777214936 	-0.000125487652430557821 	-0.00765557075172304622 	0.00270057028444748229 	0.00046445864750938921 	-0.00737340071900495931 	0.00924350722960627473 	0.000308359129016673504 	0.00213576384551587232 	-0.00!
 88316481921647573 	-0.00479667378996826745 	-0.00314050304959243194 	0.00677359813450965909 	0.000461574357387472883 	0.0052630551506867209 	0.00342377191963190138 	0.00328382212079000434 	-0.00626455935271804255 	-0.00135382711424076175 	0.00428995531271315379 	0.00717178746011403606 	-0.00889343281654111917 	-0.00926633853826809052 	0.00458758045120281812 	-0.00924171707330578504 	-0.0089407727361354699 	-0.000689382609061561812 	-0.00144386531203365706 	0.00344874682264902609 	-0.00445501595657440443 	0.00439439909658437614 	0.00146877925949968099 	0.00820337445445518988 	-0.00592893418508044692 	0.00182255472740502824 	0.00731552216505707356 	-0.0093740454197580262 	-0.000212912355459383902 	-0.00863228712512624334 	-0.00315315105065477668 	0.00635133933025240193 	0.00750852663533482702 	-0.00805517934762344477 	0.00972441442848316628 	0.00554144065331621267 	0.00669469475106060277 	-0.00565723229911999689 	-0.00967100372334113445 	-0.000114466692242828325 	0.0086591209!
 0766649163 	-0.00731143980482081273 	0.00556306573368089453 	-!
 0.003507
00272215526306 	0.00733748393075391286 	-0.00173505410219775328 	0.0086972297748011216 	0.00136029609347290475 	0.00071739170531200905 	0.00991167438484492555 	0.00573543123204991317 	1.63578136879163428e-06 	0.00106564480483038434 	0.00193302874506671757 	-0.00966095349810980504 	0.00653802734483209004 	-0.00763470234441813479 	0.00566876259818077134 	-0.0024499179463127619 	0.00406053318954284741 	
+-0.0111738846093165402 	-0.000548682555326289933 	0.00697000822713652029 	0.00138753007834285184 	0.00766445580783438156 	-0.00860777589669724057 	0.00644679742555415332 	0.00740125880871012574 	0.00287193856746441843 	-0.00677899801957570006 	0.00826191989123869694 	0.00115583659118469336 	0.00582889788198777056 	-0.00187316962044102651 	0.00334312408956729005 	0.0080872764050561631 	0.00243764569952672171 	0.0014126969154523441 	0.00700860246182310413 	-0.00352784450061643332 	0.00321421318331525842 	0.00106822699359159965 	-0.00101338297676285176 	0.00828488639125498846 	-0.00806675534350202901 	-0.00871347504120848285 	-0.00159867887480246102 	-0.00603600974286399878 	0.00670775103521673542 	0.00437948857073182894 	-0.00396246760226966893 	-0.00459702425477752537 	-0.00784011758788464089 	0.00947731814134509208 	-0.00750171391112994246 	-0.00117317699394124133 	-0.00893685763971203626 	0.000700923280907214804 	0.00234846370887252 	-0.00820605084031584935 	-0.00170467919!
 317889645 	0.00570568789669640558 	0.00220298768276994995 	0.00974557764735873785 	0.000626821250270257813 	-0.00470979802525691143 	-0.00231845383620405462 	0.00309793335509616187 	0.00199836423242812523 	-0.00816490725146091882 	0.000968802173294960032 	0.0025393460609361854 	-0.00650539020630128747 	0.00214043896541134824 	-0.00587543269286049108 	0.00116619384585259374 	0.0053050167725119373 	0.00678312969867851494 	0.00950181053835282008 	-0.00941018273948261708 	0.00814468902457629769 	0.00674089248166376623 	0.00846999001116628507 	0.00159122082434438785 	0.00558086969076118829 	0.00371987946113228485 	0.00896868983933631937 	0.00392580299849605227 	-0.000325498179622835126 	-0.000442150935507603228 	-0.00952586521815736165 	0.000360361302915812836 	0.0067212861590301631 	0.00970070024520561756 	0.00991357247738965976 	-0.0099411564224208826 	-0.00517454782326429424 	-0.00357508531990571995 	-0.00084756135678746181 	-0.000595474746156425842 	-0.000670178730631579441 !
 	-0.00283987246258698067 	-0.00877680298212392754 	-0.00287434!
 57191996
6764 	0.00761101315321550259 	-0.00570802323852433267 	-0.00351809444571104472 	0.00778924897444837998 	0.000872309102505270263 	-0.00334203925717386661 	0.00733885683349534863 	-0.00570509716576950762 	0.00791110215571273062 	-0.000799315232385290142 	0.00564398948162753589 	0.00471025043949309472 	-0.00883670104575096267 	0.0100340604251309938 	0.00563281500183433759 	-0.00436639622631694244 	
 ]
 ;
-bias = 2 [ -0.00391578848611695649 0.00391578848611705884 ] ;
+bias = 2 [ -0.00391578848611695736 0.00391578848611706665 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *9 ->PRandom(
@@ -195,12 +199,14 @@
 *10 ->SoftmaxModule(
 input_size = 2 ;
 name = "SoftmaxModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
 ] ;
 n_modules = 2 ;
 name = "ModuleStackModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -211,6 +217,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "NLLCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -219,6 +226,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "ClassErrorCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -229,6 +237,7 @@
 input_size = 2 ;
 output_size = 3 ;
 name = "CombiningCostsModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )
@@ -238,6 +247,7 @@
 input_size = 100 ;
 output_size = 1 ;
 name = "NLLCostModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *3   )

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/PL_DBN_Mini-batch.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/PL_DBN_Mini-batch.pyplearn	2007-06-07 14:43:22 UTC (rev 7549)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/PL_DBN_Mini-batch.pyplearn	2007-06-07 15:42:58 UTC (rev 7550)
@@ -85,7 +85,8 @@
             layers = [
                 pl.RBMBinomialLayer(
                     learning_rate = rbm_lr,
-                    size = input_size
+                    size = input_size,
+                    use_fast_approximations = False
                 ),
 
                 pl.RBMMultinomialLayer(

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-06-07 14:43:22 UTC (rev 7549)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-06-07 15:42:58 UTC (rev 7550)
@@ -102,7 +102,7 @@
         ),
     arguments = "PL_DBN_Mini-batch.pyplearn",
     resources = [ "PL_DBN_Mini-batch.pyplearn" ],
-    precision = 1e-03,
+    precision = 1e-06,
     pfileprg = "__program__",
     disabled = False
     )



From nouiz at mail.berlios.de  Thu Jun  7 17:47:55 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Jun 2007 17:47:55 +0200
Subject: [Plearn-commits] r7551 - in branches/cgi-desjardin: commands
	commands/PLearnCommands plearn/base plearn/io plearn/sys
	plearn/vmat plearn_learners plearn_learners/hyper
	plearn_learners/online plearn_learners/second_iteration scripts
Message-ID: <200706071547.l57Fltwo002358@sheep.berlios.de>

Author: nouiz
Date: 2007-06-07 17:47:42 +0200 (Thu, 07 Jun 2007)
New Revision: 7551

Added:
   branches/cgi-desjardin/commands/plearngg.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.h
   branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h
   branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h
   branches/cgi-desjardin/plearn_learners/second_iteration/CheckDond2FileSequence.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/CheckDond2FileSequence.h
   branches/cgi-desjardin/plearn_learners/second_iteration/ComputeDond2Target.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/ComputeDond2Target.h
   branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.h
   branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.h
   branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.h
   branches/cgi-desjardin/plearn_learners/second_iteration/FixDond2BinaryVariables.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/FixDond2BinaryVariables.h
   branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.h
   branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.h
   branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationTester.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationTester.h
   branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.h
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
   branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.h
Modified:
   branches/cgi-desjardin/commands/PLearnCommands/ServerCommand.cc
   branches/cgi-desjardin/plearn/base/PrUtils.cc
   branches/cgi-desjardin/plearn/base/TypeTraits.h
   branches/cgi-desjardin/plearn/base/ms_hash_wrapper.cpp
   branches/cgi-desjardin/plearn/io/PPath.cc
   branches/cgi-desjardin/plearn/io/PStream.cc
   branches/cgi-desjardin/plearn/io/Poll.h
   branches/cgi-desjardin/plearn/io/PrPStreamBuf.cc
   branches/cgi-desjardin/plearn/io/fileutils.cc
   branches/cgi-desjardin/plearn/io/openFile.cc
   branches/cgi-desjardin/plearn/io/openSocket.cc
   branches/cgi-desjardin/plearn/io/pl_NSPR_io.h
   branches/cgi-desjardin/plearn/sys/Popen.cc
   branches/cgi-desjardin/plearn/vmat/AddMissingVMatrix.cc
   branches/cgi-desjardin/plearn/vmat/AddMissingVMatrix.h
   branches/cgi-desjardin/plearn/vmat/VariableDeletionVMatrix.cc
   branches/cgi-desjardin/plearn/vmat/VariableDeletionVMatrix.h
   branches/cgi-desjardin/plearn_learners/hyper/OptimizeOptionOracle.cc
   branches/cgi-desjardin/plearn_learners/online/GaussianDBNClassification.h
   branches/cgi-desjardin/plearn_learners/online/GaussianDBNRegression.cc
   branches/cgi-desjardin/plearn_learners/online/GaussianDBNRegression.h
   branches/cgi-desjardin/plearn_learners/online/RBMBinomialLayer.cc
   branches/cgi-desjardin/plearn_learners/online/RBMLLParameters.cc
   branches/cgi-desjardin/plearn_learners/online/RBMLLParameters.h
   branches/cgi-desjardin/plearn_learners/online/RBMLQParameters.cc
   branches/cgi-desjardin/plearn_learners/online/RBMLQParameters.h
   branches/cgi-desjardin/plearn_learners/online/RBMLayer.cc
   branches/cgi-desjardin/plearn_learners/online/RBMLayer.h
   branches/cgi-desjardin/plearn_learners/online/RBMQLParameters.cc
   branches/cgi-desjardin/plearn_learners/online/RBMQLParameters.h
   branches/cgi-desjardin/scripts/appStart.py
   branches/cgi-desjardin/scripts/extract_classes
   branches/cgi-desjardin/scripts/mnd.py
Log:
Commit the version of PLearn I got from Gilles in a branch


Modified: branches/cgi-desjardin/commands/PLearnCommands/ServerCommand.cc
===================================================================
--- branches/cgi-desjardin/commands/PLearnCommands/ServerCommand.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/commands/PLearnCommands/ServerCommand.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -49,9 +49,9 @@
 #include <plearn/io/pl_log.h>
 #include <plearn/io/PrPStreamBuf.h>
 #include <plearn/base/tostring.h>
-#include <mozilla/nspr/prio.h>
-#include <mozilla/nspr/prerror.h>
-#include <mozilla/nspr/prnetdb.h>
+#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prerror.h>
+#include <mozilla-1.7.13/nspr/prnetdb.h>
 
 #ifndef WIN32
 // POSIX includes for getpid() and gethostname()

Added: branches/cgi-desjardin/commands/plearngg.cc
===================================================================
--- branches/cgi-desjardin/commands/plearngg.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/commands/plearngg.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,54 @@
+// -*- C++ -*-
+
+// plearngg.cc
+// Copyright (C) 2005 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+   * $Id: plearn.cc 3636 2005-06-22 19:59:25Z godbout $
+   ******************************************************* */
+
+//! All includes should now go into plearn_inc.h and plearn_full_inc.h
+#include "plearn_inc_gg.h"
+#include "plearn_full_inc_gg.h"
+#include "plearn_gg_inc.h"
+#include "PLearnCommands/plearn_main.h"
+
+using namespace PLearn;
+
+int main(int argc, char** argv)
+{
+  return plearn_main( argc, argv, 
+                      PLEARN_MAJOR_VERSION, 
+                      PLEARN_MINOR_VERSION, 
+                      PLEARN_FIXLEVEL       );
+}
+

Modified: branches/cgi-desjardin/plearn/base/PrUtils.cc
===================================================================
--- branches/cgi-desjardin/plearn/base/PrUtils.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/base/PrUtils.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -42,7 +42,7 @@
 
 
 #include "PrUtils.h"
-#include <mozilla/nspr/prerror.h>
+#include <mozilla-1.7.13/nspr/prerror.h>
 
 namespace PLearn {
 using namespace std;

Modified: branches/cgi-desjardin/plearn/base/TypeTraits.h
===================================================================
--- branches/cgi-desjardin/plearn/base/TypeTraits.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/base/TypeTraits.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -50,7 +50,7 @@
 #include <list>
 #include <map>
 #include <set>
-#include <mozilla/nspr/prlong.h>
+#include <mozilla-1.7.13/nspr/prlong.h>
 
 namespace PLearn {
 using std::string;

Modified: branches/cgi-desjardin/plearn/base/ms_hash_wrapper.cpp
===================================================================
--- branches/cgi-desjardin/plearn/base/ms_hash_wrapper.cpp	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/base/ms_hash_wrapper.cpp	2007-06-07 15:47:42 UTC (rev 7551)
@@ -1,6 +1,6 @@
-
-// dummy file: just as a test.. It will be soon erased!
-#include <string>
-#include "ms_hash_wrapper.h"
-
-
+
+// dummy file: just as a test.. It will be soon erased!
+#include <string>
+#include "ms_hash_wrapper.h"
+
+

Modified: branches/cgi-desjardin/plearn/io/PPath.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/PPath.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/io/PPath.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -43,7 +43,7 @@
 // #define PL_LOG_MODULE_NAME "PPath"
 
 #include <ctype.h>
-#include <mozilla/nspr/prenv.h>
+#include <mozilla-1.7.13/nspr/prenv.h>
 
 #include "PPath.h"
 #include "PStream.h"

Modified: branches/cgi-desjardin/plearn/io/PStream.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/PStream.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/io/PStream.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -37,7 +37,7 @@
 #include "NullPStreamBuf.h"
 #include "PrPStreamBuf.h"
 #include <plearn/math/pl_math.h>
-#include <mozilla/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prio.h>
 #include <ctype.h>
 
 

Modified: branches/cgi-desjardin/plearn/io/Poll.h
===================================================================
--- branches/cgi-desjardin/plearn/io/Poll.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/io/Poll.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -45,7 +45,7 @@
 #define Poll_INC
 
 #include <vector>
-#include <mozilla/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prio.h>
 #include <plearn/io/PStream.h>
 
 

Modified: branches/cgi-desjardin/plearn/io/PrPStreamBuf.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/PrPStreamBuf.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/io/PrPStreamBuf.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -42,7 +42,7 @@
 
 
 #include "PrPStreamBuf.h"
-#include <mozilla/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prio.h>
 #include <stdio.h>
 
 namespace PLearn {

Modified: branches/cgi-desjardin/plearn/io/fileutils.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/fileutils.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/io/fileutils.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -65,10 +65,10 @@
 #include <plearn/math/pl_math.h>    //!< For 'real'.
 
 #include <plearn/base/PrUtils.h>
-#include <mozilla/nspr/prio.h>
-#include <mozilla/nspr/prtime.h>
-#include <mozilla/nspr/prerror.h>
-#include <mozilla/nspr/prlong.h>
+#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prtime.h>
+#include <mozilla-1.7.13/nspr/prerror.h>
+#include <mozilla-1.7.13/nspr/prlong.h>
 
 namespace PLearn {
 using namespace std;

Modified: branches/cgi-desjardin/plearn/io/openFile.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/openFile.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/io/openFile.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -42,7 +42,7 @@
 
 
 #include "openFile.h"
-#include <mozilla/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prio.h>
 #include <plearn/io/fileutils.h>
 #include <plearn/io/PrPStreamBuf.h>
 

Modified: branches/cgi-desjardin/plearn/io/openSocket.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/openSocket.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/io/openSocket.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -45,9 +45,9 @@
 #include <plearn/io/PStream.h>
 #include <plearn/io/PrPStreamBuf.h>
 #include "openSocket.h"
-#include <mozilla/nspr/prio.h>
-#include <mozilla/nspr/prerror.h>
-#include <mozilla/nspr/prnetdb.h>
+#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prerror.h>
+#include <mozilla-1.7.13/nspr/prnetdb.h>
 
 
 namespace PLearn {

Modified: branches/cgi-desjardin/plearn/io/pl_NSPR_io.h
===================================================================
--- branches/cgi-desjardin/plearn/io/pl_NSPR_io.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/io/pl_NSPR_io.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -46,7 +46,7 @@
 #ifndef pl_NSPR_io_INC
 #define pl_NSPR_io_INC
 
-#include <mozilla/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prio.h>
 
 namespace PLearn {
 using namespace std;

Modified: branches/cgi-desjardin/plearn/sys/Popen.cc
===================================================================
--- branches/cgi-desjardin/plearn/sys/Popen.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/sys/Popen.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -42,9 +42,9 @@
 
 #include "Popen.h"
 
-#include <mozilla/nspr/prio.h>
-#include <mozilla/nspr/prproces.h>
-#include <mozilla/nspr/prenv.h>
+#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla-1.7.13/nspr/prproces.h>
+#include <mozilla-1.7.13/nspr/prenv.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/base/PrUtils.h>
 #include <plearn/io/PrPStreamBuf.h>

Modified: branches/cgi-desjardin/plearn/vmat/AddMissingVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn/vmat/AddMissingVMatrix.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/vmat/AddMissingVMatrix.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -50,9 +50,10 @@
 // AddMissingVMatrix //
 //////////////////
 AddMissingVMatrix::AddMissingVMatrix():
+  random_gen(new PRandom()),
   missing_prop(0),
   only_on_first(-1),
-  random_gen(new PRandom()),
+  on_variables(-1),
   seed(-1)
 {}
 
@@ -72,8 +73,11 @@
       "Percentage of missing values.");
 
   declareOption(ol, "only_on_first", &AddMissingVMatrix::only_on_first, OptionBase::buildoption,
-      "Only add missing values in the first 'only_on_first' samples (ignored if < 0).");
+      "Only insert missing values in the first 'only_on_first' samples (ignored if < 0).");
 
+  declareOption(ol, "on_variables", &AddMissingVMatrix::on_variables, OptionBase::buildoption,
+      "Insert missing values in the first on_variables variables, if > 0.");
+
   declareOption(ol, "seed", &AddMissingVMatrix::seed, OptionBase::buildoption,
       "Random numbers seed.");
 
@@ -117,6 +121,7 @@
   if (only_on_first >= 0 && i >= only_on_first)
     return;
   int n = v.length();
+  if (on_variables > 0 && on_variables <= n) n = on_variables;
   for (int j = 0; j < n; j++)
     if (random_gen->uniform_sample() < missing_prop)
       v[j] = MISSING_VALUE;

Modified: branches/cgi-desjardin/plearn/vmat/AddMissingVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn/vmat/AddMissingVMatrix.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/vmat/AddMissingVMatrix.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -72,6 +72,7 @@
 
   real missing_prop;
   int  only_on_first;
+  int  on_variables;
   long seed;
 
   // ****************

Modified: branches/cgi-desjardin/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn/vmat/VariableDeletionVMatrix.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/vmat/VariableDeletionVMatrix.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -49,42 +49,30 @@
 PLEARN_IMPLEMENT_OBJECT(
     VariableDeletionVMatrix,
     "VMat class to select columns from a source VMat based on a given threshold percentage of non-missing variables.",
-    "This class will scan the VMat provided as the complete dataset and compute the percentage of non-missing values\n"
+    "This class will scan the VMat provided as the train set and compute the percentage of non-missing values\n"
     "of each variables. It then only selects the columns with  a percentage of non-missing higher than the given\n"
-    "threshod parameter.\n"
+    "threshod parameter from the complete dataset.\n"
     "Optionnaly, variable with non-missing constant values will also be removed.\n"
     "Note that the ending targets and weight columns are always kept.\n"
     "The targetsize and weightsize of the underlying matrix are kept.\n"
     );
 
 VariableDeletionVMatrix::VariableDeletionVMatrix()
-    : obtained_inputsize_from_source(false),
-      obtained_targetsize_from_source(false),
-      obtained_weightsize_from_source(false),
-      deletion_threshold(0),
+    : deletion_threshold(0),
       remove_columns_with_constant_value(0),
-      number_of_train_samples(0.0)
+      number_of_train_samples(0.0),
+      start_row(0)
 {
 }
 
-VariableDeletionVMatrix::VariableDeletionVMatrix(VMat the_complete_dataset, real the_threshold, bool the_remove_columns_with_constant_value, real the_number_of_train_samples)
-    : obtained_inputsize_from_source(false),
-      obtained_targetsize_from_source(false),
-      obtained_weightsize_from_source(false)
-{
-    complete_dataset = the_complete_dataset;
-    deletion_threshold = the_threshold;
-    remove_columns_with_constant_value = the_remove_columns_with_constant_value;
-    number_of_train_samples = the_number_of_train_samples;
-    build();
-}
-
 void VariableDeletionVMatrix::declareOptions(OptionList &ol)
 {
-
     declareOption(ol, "complete_dataset", &VariableDeletionVMatrix::complete_dataset, OptionBase::buildoption,
                   "The data set with all variables to select the columns from.");
 
+    declareOption(ol, "train_set", &VariableDeletionVMatrix::train_set, OptionBase::buildoption,
+                  "The train set in which to compute the percentage of missing values.");
+
     declareOption(ol, "deletion_threshold", &VariableDeletionVMatrix::deletion_threshold, OptionBase::buildoption,
                   "The percentage of non-missing values for a variable above which, the variable will be selected.");
 
@@ -92,25 +80,21 @@
                   "If set to 1, the columns with constant non-missing values will be removed.");
 
     declareOption(ol, "number_of_train_samples", &VariableDeletionVMatrix::number_of_train_samples, OptionBase::buildoption,
-                  "If equal to zero, all the underlying dataset samples are used to calculated the percentages and constant values.\n"
+                  "If equal to zero, all the train samples are used to calculated the percentages and constant values.\n"
                   "If it is a fraction between 0 and 1, this proportion of the samples will be used.\n"
                   "If greater or equal to 1, the integer portion will be interpreted as the number of samples to use.");
 
-    declareOption(ol, "obtained_inputsize_from_source", &VariableDeletionVMatrix::obtained_inputsize_from_source, OptionBase::learntoption,
-                  "Set to 1 when the inputsize was obtained from the source matrix.");
+    declareOption(ol, "start_row", &VariableDeletionVMatrix::start_row, OptionBase::buildoption,
+                  "The row at which, to start to calculate the percentages and constant values.");
 
-    declareOption(ol, "obtained_targetsize_from_source", &VariableDeletionVMatrix::obtained_targetsize_from_source, OptionBase::learntoption,
-                  "Set to 1 when the targetsize was obtained from the source matrix.");
+    declareOption(ol, "source", &VariableDeletionVMatrix::source, OptionBase::learntoption,
+                  "The resulting data set.");
 
-    declareOption(ol, "obtained_weightsize_from_source", &VariableDeletionVMatrix::obtained_weightsize_from_source, OptionBase::learntoption,
-                  "Set to 1 when the weightsize was obtained from the source matrix.");
-
     inherited::declareOptions(ol);
 }
 
 void VariableDeletionVMatrix::build()
 {
-    buildIndices();
     inherited::build();
     build_();
 }
@@ -119,11 +103,11 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(complete_dataset, copies);
+    deepCopyField(train_set, copies);
     deepCopyField(deletion_threshold, copies);
     deepCopyField(remove_columns_with_constant_value, copies);
-    deepCopyField(obtained_inputsize_from_source, copies);
-    deepCopyField(obtained_targetsize_from_source, copies);
-    deepCopyField(obtained_weightsize_from_source, copies);
+    deepCopyField(number_of_train_samples, copies);
+    deepCopyField(start_row, copies);
 }
 
 void VariableDeletionVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
@@ -163,7 +147,7 @@
 
 void VariableDeletionVMatrix::getRow(int i, Vec v) const
 {
-    source-> getRow(i, v);
+    source->getRow(i, v);
 }
 
 void VariableDeletionVMatrix::putRow(int i, Vec v)
@@ -173,58 +157,37 @@
 
 void VariableDeletionVMatrix::getColumn(int i, Vec v) const
 {
-    source-> getColumn(i, v);
+    source->getColumn(i, v);
 }
 
 void VariableDeletionVMatrix::build_()
 {
-    if (source) {
-        string error_msg =
-            "In VariableDeletionVMatrix::build_ - For safety reasons, it is forbidden to "
-            "re-use sizes obtained from a previous source VMatrix with a new source "
-            "VMatrix having different sizes";
-        length_ = source->length();
-        width_ = source->width();
-        if(inputsize_<0) {
-            inputsize_ = source->inputsize();
-            obtained_inputsize_from_source = true;
-        } else if (obtained_inputsize_from_source && inputsize_ != source->inputsize())
-            PLERROR(error_msg.c_str());
-        if(targetsize_<0) {
-            targetsize_ = source->targetsize();
-            obtained_targetsize_from_source = true;
-        } else if (obtained_targetsize_from_source && targetsize_ != source->targetsize())
-            PLERROR(error_msg.c_str());
-        if(weightsize_<0) {
-            weightsize_ = source->weightsize();
-            obtained_weightsize_from_source = true;
-        } else if (obtained_weightsize_from_source && weightsize_ != source->weightsize())
-            PLERROR(error_msg.c_str());
-        fieldinfos = source->fieldinfos;
-    } else {
-        // Restore the original undefined sizes if the current one had been obtained
-        // from the source VMatrix.
-        if (obtained_inputsize_from_source) {
-            inputsize_ = -1;
-            obtained_inputsize_from_source = false;
-        }
-        if (obtained_targetsize_from_source) {
-            targetsize_ = -1;
-            obtained_targetsize_from_source = false;
-        }
-        if (obtained_weightsize_from_source) {
-            weightsize_ = -1;
-            obtained_weightsize_from_source = false;
-        }
-    }
+    if (!train_set || !complete_dataset) PLERROR("In VariableDeletionVMatrix::train set and complete_dataset vmat must be supplied");
+    buildIndices();
 }
 
 void VariableDeletionVMatrix::buildIndices()
 {
+    int train_set_length = train_set->length();
+    if(train_set_length < 1) PLERROR("In VariableDeletionVMatrix::length of the number of train samples to use must be at least 1, got: %i", train_set_length);
+    int train_set_width = train_set->width();
+    int train_set_inputsize = train_set->inputsize();
+    if(train_set_inputsize < 1) PLERROR("In VariableDeletionVMatrix::inputsize of the train vmat must be supplied, got : %i", train_set_inputsize);
+    int train_set_targetsize = train_set->targetsize();
+    int train_set_weightsize = train_set->weightsize();
     int complete_dataset_length = complete_dataset->length();
     int complete_dataset_width = complete_dataset->width();
+    int complete_dataset_inputsize = complete_dataset->inputsize();
     int complete_dataset_targetsize = complete_dataset->targetsize();
     int complete_dataset_weightsize = complete_dataset->weightsize();
+    if (train_set_width != complete_dataset_width)
+        PLERROR("In VariableDeletionVMatrix::train set and complete_dataset width must agree, got : %i, %i", train_set_width, complete_dataset_width);
+    if (train_set_inputsize != complete_dataset_inputsize)
+        PLERROR("In VariableDeletionVMatrix::train set and complete_dataset inputsize must agree, got : %i, %i", train_set_inputsize, complete_dataset_inputsize);
+    if (train_set_targetsize != complete_dataset_targetsize)
+        PLERROR("In VariableDeletionVMatrix::train set and complete_dataset targetsize must agree, got : %i, %i", train_set_targetsize, complete_dataset_targetsize);
+    if (train_set_weightsize != complete_dataset_weightsize)
+        PLERROR("In VariableDeletionVMatrix::train set and complete_dataset weightsize must agree, got : %i, %i", train_set_weightsize, complete_dataset_weightsize);
     int row;
     int col;
     TVec<int>  selected_columns_indices;
@@ -235,25 +198,23 @@
     variable_present_count.resize(complete_dataset_width);
     variable_last_value.resize(complete_dataset_width);
     variable_constant.resize(complete_dataset_width);
-    for (col = 0; col < complete_dataset_width; col++)
-    {
-        variable_present_count[col] = 0;
-        variable_constant[col] = true;
-    }
+    variable_present_count.clear();
+    variable_constant.fill(true);
     real variable_value;
-    int scanned_length = complete_dataset_length;
+    int scanned_length = train_set_length;
     if (number_of_train_samples > 0.0)
     {
         if (number_of_train_samples >= 1.0) scanned_length = (int) number_of_train_samples;
-        else scanned_length = (int) ((double) complete_dataset_length * number_of_train_samples);
+        else scanned_length = (int) ((double) train_set_length * number_of_train_samples);
         if (scanned_length < 1) scanned_length = 1;
-        if (scanned_length > complete_dataset_length) scanned_length = complete_dataset_length;
     }
-    for (row = 0; row < scanned_length; row++)
+    if (start_row + scanned_length > train_set_length)        
+        PLERROR("In VariableDeletionVMatrix: start_row + number_of_train_samples must be less or equal to the train set length");
+    for (row = start_row; row < start_row + scanned_length; row++)
     {
-        for (col = 0; col < complete_dataset_width; col++)
+        for (col = 0; col < train_set_width; col++)
         {
-            variable_value = complete_dataset->get(row, col);
+            variable_value = train_set->get(row, col);
             if (!is_missing(variable_value))
             {
                 if (variable_present_count[col] > 0)
@@ -265,18 +226,21 @@
             }
         }
     }
-    real adjusted_threshold = deletion_threshold * (real) complete_dataset_length / 100.0;
-    if (complete_dataset_targetsize < 0) complete_dataset_targetsize = 0;
-    if (complete_dataset_weightsize < 0) complete_dataset_weightsize = 0;
-    int target_and_weight = complete_dataset_targetsize + complete_dataset_weightsize;
+    real adjusted_threshold = deletion_threshold * (real) scanned_length;
+    if (train_set_targetsize < 0) train_set_targetsize = 0;
+    if (train_set_weightsize < 0) train_set_weightsize = 0;
+    int target_and_weight = train_set_targetsize + train_set_weightsize;
     int new_width = target_and_weight;
-    for (col = 0; col < complete_dataset_width - target_and_weight; col++)
+    for (col = 0; col < train_set_width - target_and_weight; col++)
     {
         if ((real) variable_present_count[col] > adjusted_threshold && (!remove_columns_with_constant_value || !variable_constant[col])) new_width += 1;
     }
     selected_columns_indices.resize(new_width);
+    TVec<string> complete_dataset_names(complete_dataset_width);
+    TVec<string> new_names(new_width);
+    complete_dataset_names = complete_dataset->fieldNames();
     int selected_col = 0;
-    for (col = 0; col < complete_dataset_width - target_and_weight; col++)
+    for (col = 0; col < train_set_width - target_and_weight; col++)
     {
         if ((real) variable_present_count[col] > adjusted_threshold && (!remove_columns_with_constant_value || !variable_constant[col]))
         {
@@ -286,14 +250,23 @@
     }
     if (target_and_weight > 0)
     {
-        for (col = complete_dataset_width - target_and_weight; col < complete_dataset_width; col++)
+        for (col = train_set_width - target_and_weight; col < train_set_width; col++)
         {
             selected_columns_indices[selected_col] = col;
             selected_col += 1;
         }
     }
+    for (col = 0; col < new_width; col++)
+    {
+        new_names[col] = complete_dataset_names[selected_columns_indices[col]];
+    }
     source = new SelectColumnsVMatrix(complete_dataset, selected_columns_indices);
-    source->defineSizes(new_width - target_and_weight, complete_dataset_targetsize, complete_dataset_weightsize);
+    length_ = complete_dataset_length;
+    width_ = new_width;
+    inputsize_ = new_width - target_and_weight;
+    targetsize_ = complete_dataset_targetsize;
+    weightsize_ = complete_dataset_weightsize;
+    declareFieldNames(new_names);
 }
 
 } // end of namespace PLearn

Modified: branches/cgi-desjardin/plearn/vmat/VariableDeletionVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn/vmat/VariableDeletionVMatrix.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn/vmat/VariableDeletionVMatrix.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -45,7 +45,7 @@
 #ifndef VariableDeletionVMatrix_INC
 #define VariableDeletionVMatrix_INC
 
-#include "SourceVMatrix.h"
+#include "VMatrix.h"
 #include "SelectColumnsVMatrix.h"
 
 namespace PLearn {
@@ -53,27 +53,23 @@
 
 //!  provides mean imputation for missing variables
 
-class VariableDeletionVMatrix: public SourceVMatrix
+class VariableDeletionVMatrix: public VMatrix
 {
-    typedef SourceVMatrix inherited;
+    typedef VMatrix inherited;
 
-private:
-
-    bool obtained_inputsize_from_source;
-    bool obtained_targetsize_from_source;
-    bool obtained_weightsize_from_source;
-
 public:
 
     VMat       complete_dataset;
+    VMat       train_set;
+    VMat       source;
     real       deletion_threshold;
     bool       remove_columns_with_constant_value;
     real       number_of_train_samples;
+    int        start_row;
 
 public:
 
     VariableDeletionVMatrix();
-    VariableDeletionVMatrix(VMat the_complete_dataset, real the_threshold, bool the_remove_columns_with_constant_value, real the_number_of_train_samples);
 
     static void declareOptions(OptionList &ol);
 

Modified: branches/cgi-desjardin/plearn_learners/hyper/OptimizeOptionOracle.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/hyper/OptimizeOptionOracle.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/hyper/OptimizeOptionOracle.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -269,15 +269,20 @@
 void OptimizeOptionOracle::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields 
-    // ### that you wish to be deepCopied rather than 
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("OptimizeOptionOracle::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+    deepCopyField(option, copies);
+    deepCopyField(max_steps, copies);
+    deepCopyField(start_value, copies);
+    deepCopyField(min_value, copies);
+    deepCopyField(max_value, copies);
+    deepCopyField(relative_precision, copies);
+    deepCopyField(factor, copies);
+    deepCopyField(start_direction, copies);
+    deepCopyField(best, copies);
+    deepCopyField(best_objective, copies);
+    deepCopyField(current_direction, copies);
+    deepCopyField(lower_bound, copies);
+    deepCopyField(n_steps, copies);
+    deepCopyField(upper_bound, copies);
 }
 
 } // end of namespace PLearn

Modified: branches/cgi-desjardin/plearn_learners/online/GaussianDBNClassification.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/GaussianDBNClassification.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/GaussianDBNClassification.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -133,7 +133,6 @@
     // ### initializes all fields to reasonable default values.
     GaussianDBNClassification();
 
-
     //#####  PDistribution Member Functions  ##################################
 
     //! Return probability density p(y | x)

Modified: branches/cgi-desjardin/plearn_learners/online/GaussianDBNRegression.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/GaussianDBNRegression.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/GaussianDBNRegression.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -64,7 +64,8 @@
 GaussianDBNRegression::GaussianDBNRegression() :
     learning_rate(0.),
     weight_decay(0.),
-    use_sample_rather_than_expectation_in_positive_phase_statistics(false)
+    use_sample_rather_than_expectation_in_positive_phase_statistics(false),
+    skip_backprop_on_model(0)
 {
     random_gen = new PRandom();
 }
@@ -142,6 +143,10 @@
                   "In positive phase statistics use output->sample * input\n"
                   "rather than output->expectation * input.\n");
 
+    declareOption(ol, "skip_backprop_on_model", &GaussianDBNRegression::skip_backprop_on_model,
+                  OptionBase::buildoption,
+                  "If > 0, will only do backprop on target layer.");
+
     declareOption(ol, "n_layers", &GaussianDBNRegression::n_layers,
                   OptionBase::learntoption,
                   "Number of unsupervised layers, including input layer");
@@ -356,7 +361,7 @@
     target_layer->computeExpectation();
 
     mu << target_layer->expectation;
-
+    
 }
 
 /////////////
@@ -566,7 +571,6 @@
         {
             MODULE_LOG << "Fine-tuning all parameters, using method "
                 << fine_tuning_method << endl;
-
             if( fine_tuning_method == "" ) // do nothing
                 sample += n_samples_to_see;
             else if( fine_tuning_method == "EGD" )
@@ -612,9 +616,16 @@
         }
         train_stats->finalize(); // finalize statistics for this epoch
     }
+    checkLearner();
     MODULE_LOG << endl;
 }
 
+void GaussianDBNRegression::checkLearner()
+{
+// We do nothing here.
+// subclass may way to check various things.
+}
+
 void GaussianDBNRegression::greedyStep( const Vec& predictor, int index )
 {
     // deterministic propagation until we reach index
@@ -696,11 +707,26 @@
 {
     // split input in predictor_part and predicted_part
     splitCond(input);
+    
+//    cout << "fineTuneByGradientDescent: input: " << input << endl;
 
     // compute predicted_part expectation, conditioned on predictor_part
     // (forward pass)
     expectation( output_gradient );
 
+    /*    
+    cout << "we have gone up!" << endl;
+    cout << "layers[0]" << endl;
+    layers[0]->printActivation();
+    cout << "input_params" << endl;
+    input_params->printParams();
+    cout << "layers[1]" << endl;
+    layers[1]->printActivation();
+    cout << "target_params" << endl;
+    target_params->printParams();
+    cout << "target_layer" << endl;
+    target_layer->printActivation();
+*/
     int target_size = predicted_part.size() ; 
 
     for(int i=0 ; i < target_size ; ++i) { 
@@ -711,7 +737,7 @@
                                target_layer->expectation,
                                expectation_gradients[n_layers-1],
                                output_gradient );
-
+    if (skip_backprop_on_model > 0) return;
     for( int i=n_layers-2 ; i>1 ; i-- )
     {
         layers[i]->bpropUpdate( layers[i]->activations,
@@ -732,6 +758,20 @@
                                   layers[1]->activations,
                                   expectation_gradients[0],
                                   activation_gradients[1] );
+
+/*    
+    cout << "we have updated params!" << endl;
+    cout << "target_layer" << endl;
+    target_layer->printActivation();
+    cout << "target_params" << endl;
+    target_params->printParams();
+    cout << "layers[1]" << endl;
+    layers[1]->printActivation();
+    cout << "input_params" << endl;
+    input_params->printParams();
+    cout << "layers[0]" << endl;
+    layers[0]->printActivation();
+*/
     
 }
 

Modified: branches/cgi-desjardin/plearn_learners/online/GaussianDBNRegression.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/GaussianDBNRegression.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/GaussianDBNRegression.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -116,6 +116,8 @@
     string fine_tuning_method;
 
     bool use_sample_rather_than_expectation_in_positive_phase_statistics;
+    
+    int skip_backprop_on_model;
 
 public:
     //#####  Public Member Functions  #########################################
@@ -124,8 +126,9 @@
     // ### Make sure the implementation in the .cc
     // ### initializes all fields to reasonable default values.
     GaussianDBNRegression();
+    
+    virtual void checkLearner();
 
-
     //#####  PDistribution Member Functions  ##################################
 
     //! Return probability density p(y | x)

Modified: branches/cgi-desjardin/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMBinomialLayer.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMBinomialLayer.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -113,7 +113,7 @@
     for( int i=0 ; i<size ; i++ )
     {
         real output_i = output[i];
-        input_gradient[i] = output_i * (1-output_i) * output_gradient[i];
+        input_gradient[i] = -output_i * (1-output_i) * output_gradient[i];
     }
 }
 

Modified: branches/cgi-desjardin/plearn_learners/online/RBMLLParameters.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMLLParameters.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMLLParameters.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -266,21 +266,25 @@
 //! this version allows to obtain the input gradient as well
 void RBMLLParameters::bpropUpdate(const Vec& input, const Vec& output,
                                   Vec& input_gradient,
-                                  const Vec& output_gradient)
+                                  const Vec& output_gradient,
+                                  real fine_tuning_learning_rate)
 {
+    real bprop_learning_rate;
+    if (fine_tuning_learning_rate < 0.00000001) bprop_learning_rate = learning_rate;
+    else bprop_learning_rate = fine_tuning_learning_rate;
     assert( input.size() == down_layer_size );
     assert( output.size() == up_layer_size );
     assert( output_gradient.size() == up_layer_size );
     input_gradient.resize( down_layer_size );
+    
+    // input_gradient = weights' * output_gradient
+    transposeProduct( input_gradient, weights, output_gradient );
 
     // weights -= learning_rate * output_gradient * input'
-    externalProductScaleAcc( weights, output_gradient, input, -learning_rate );
+    externalProductScaleAcc( weights, output_gradient, input, -bprop_learning_rate );
 
     // (up) bias -= learning_rate * output_gradient
-    multiplyAcc( up_units_bias, output_gradient, -learning_rate );
-
-    // input_gradient = weights' * output_gradient
-    transposeProduct( input_gradient, weights, output_gradient );
+    multiplyAcc( up_units_bias, output_gradient, -bprop_learning_rate );
 }
 
 //! reset the parameters to the state they would be BEFORE starting training.

Modified: branches/cgi-desjardin/plearn_learners/online/RBMLLParameters.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMLLParameters.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMLLParameters.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -138,7 +138,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             real fine_tuning_learning_rate = 0.0);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: branches/cgi-desjardin/plearn_learners/online/RBMLQParameters.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMLQParameters.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMLQParameters.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -163,7 +163,14 @@
     build_();
 }
 
+void RBMLQParameters::printParams()
+{
+    cout << "weights: " << weights << endl;
+    cout << "down_units_bias: " << down_units_bias << endl;
+    cout << "up_units_params: " << up_units_params << endl;
+}
 
+
 void RBMLQParameters::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -304,8 +311,12 @@
 //! this version allows to obtain the input gradient as well
 void RBMLQParameters::bpropUpdate(const Vec& input, const Vec& output,
                                   Vec& input_gradient,
-                                  const Vec& output_gradient)
+                                  const Vec& output_gradient,
+                                  real fine_tuning_learning_rate)
 {
+    real bprop_learning_rate;
+    if (fine_tuning_learning_rate < 0.00000001) bprop_learning_rate = learning_rate;
+    else bprop_learning_rate = fine_tuning_learning_rate;
     //TODO: clean up the code a bit
     assert( input.size() == down_layer_size );
     assert( output.size() == up_layer_size );
@@ -317,37 +328,31 @@
 
     Vec scaled_out_grad(up_layer_size) ;  
     
-    Vec prod_w_input( up_layer_size ) ; 
+    Vec prod_w_input( up_layer_size ) ;
+    prod_w_input.clear(); 
+
+    //first, compute input_gradient = weights' * output_gradient.
+    for(int i=0 ; i<up_layer_size ; ++i) 
+    { 
+        scaled_out_grad[i] = -0.5 * output_gradient[i] / (up_units_params[1][i] * up_units_params[1][i]) ;
+    }
+    transposeProduct( input_gradient, weights, scaled_out_grad );
     
+    
+    //now, update parameters
     for(int i=0 ; i<up_layer_size ; ++i) 
     {
-        real a_i_square = up_units_params[1][i] * up_units_params[1][i] ; 
-        
-        scaled_out_grad[i] = -0.5 * output_gradient[i] / a_i_square ; 
-        
-        up_units_params[0][i] -= learning_rate * ( -0.5 / a_i_square ) *
-                                 output_gradient[i] ; 
-
-        
+        real partial_gradient = -0.5 * output_gradient[i] / (up_units_params[1][i] * up_units_params[1][i]);
         for(int j=0 ; j < down_layer_size ; ++j) {             
             prod_w_input[i] += weights[i][j] * input[j] ; 
-            weights[i][j] -= learning_rate * ( - 0.5 * input[j] / a_i_square ) * 
-                             output_gradient[i];
+            weights[i][j] -= bprop_learning_rate * input[j] * partial_gradient;
         }
-    }
 
-    for(int i=0 ; i<up_layer_size ; ++i) { 
-        up_units_params[1][i] -= learning_rate * ( up_units_params[0][i] +
-                prod_w_input[i] ) * output_gradient[i] ; 
+        up_units_params[1][i] -= bprop_learning_rate * ( up_units_params[0][i] +
+                prod_w_input[i] ) * output_gradient[i] / pow(up_units_params[1][i], 3.0);
+
+        up_units_params[0][i] -= bprop_learning_rate * partial_gradient;
     }
-
-    // (up) bias -= learning_rate * output_gradient
-//    multiplyAcc( up_units_params[0], output_gradient, -learning_rate );
-    
-
-
-    // input_gradient = weights' * output_gradient
-    transposeProduct( input_gradient, weights, scaled_out_grad );
 }
 
 //! reset the parameters to the state they would be BEFORE starting training.

Modified: branches/cgi-desjardin/plearn_learners/online/RBMLQParameters.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMLQParameters.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMLQParameters.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -140,12 +140,16 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             real fine_tuning_learning_rate = 0.0);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().
     virtual void forget();
+    
+    
+    void printParams();
 
     //! optionally perform some processing after training, or after a
     //! series of fprop/bpropUpdate calls to prepare the model for truly

Modified: branches/cgi-desjardin/plearn_learners/online/RBMLayer.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMLayer.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMLayer.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -104,7 +104,13 @@
     build_();
 }
 
+void RBMLayer::printActivation()
+{
+    cout << "activations: " << activations << endl;
+    cout << "expectation: " << expectation << endl;
+}
 
+
 void RBMLayer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);

Modified: branches/cgi-desjardin/plearn_learners/online/RBMLayer.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMLayer.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMLayer.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -127,6 +127,9 @@
     {
         return units_types;
     }
+    
+    
+    virtual void printActivation();
 
 
     //#####  PLearn::Object Protocol  #########################################

Modified: branches/cgi-desjardin/plearn_learners/online/RBMQLParameters.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMQLParameters.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMQLParameters.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -163,7 +163,14 @@
     build_();
 }
 
+void RBMQLParameters::printParams()
+{
+    cout << "weights: " << weights << endl;
+    cout << "up_units_bias: " << up_units_bias << endl;
+    cout << "down_units_params: " << down_units_params << endl;
+}
 
+
 void RBMQLParameters::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -310,21 +317,25 @@
 //! this version allows to obtain the input gradient as well
 void RBMQLParameters::bpropUpdate(const Vec& input, const Vec& output,
                                   Vec& input_gradient,
-                                  const Vec& output_gradient)
+                                  const Vec& output_gradient,
+                                  real fine_tuning_learning_rate)
 {
+    real bprop_learning_rate;
+    if (fine_tuning_learning_rate < 0.00000001) bprop_learning_rate = learning_rate;
+    else bprop_learning_rate = fine_tuning_learning_rate;
     assert( input.size() == down_layer_size );
     assert( output.size() == up_layer_size );
     assert( output_gradient.size() == up_layer_size );
     input_gradient.resize( down_layer_size );
+    
+    // input_gradient = weights' * output_gradient
+    transposeProduct( input_gradient, weights, output_gradient );
 
     // weights -= learning_rate * output_gradient * input'
-    externalProductAcc( weights, (-learning_rate)*output_gradient, input );
+    externalProductAcc( weights, (-bprop_learning_rate)*output_gradient, input );
 
     // (up) bias -= learning_rate * output_gradient
-    multiplyAcc( up_units_bias, output_gradient, -learning_rate );
-
-    // input_gradient = weights' * output_gradient
-    transposeProduct( input_gradient, weights, output_gradient );
+    multiplyAcc( up_units_bias, output_gradient, -bprop_learning_rate );
 }
 
 //! reset the parameters to the state they would be BEFORE starting training.

Modified: branches/cgi-desjardin/plearn_learners/online/RBMQLParameters.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/online/RBMQLParameters.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/online/RBMQLParameters.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -140,7 +140,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec& input, const Vec& output,
                              Vec& input_gradient,
-                             const Vec& output_gradient);
+                             const Vec& output_gradient,
+                             real fine_tuning_learning_rate = 0.0);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
@@ -163,6 +164,9 @@
 
     //! Transforms a shallow copy into a deep copy
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+    
+    
+    virtual void printParams();
 
 protected:
 

Added: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,221 @@
+// -*- C++ -*-
+
+// AnalyzeDond2DiscreteVariables.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file AnalyzeDond2DiscreteVariables.cc */
+
+#define PL_LOG_MODULE_NAME "AnalyzeDond2DiscreteVariables"
+#include <plearn/io/pl_log.h>
+
+#include "AnalyzeDond2DiscreteVariables.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    AnalyzeDond2DiscreteVariables,
+    "Computes correlation coefficient between various discrete values and the target.",
+    "name of the discrete variable, of the target and the values to check are options.\n"
+);
+
+/////////////////////////
+// AnalyzeDond2DiscreteVariables //
+/////////////////////////
+AnalyzeDond2DiscreteVariables::AnalyzeDond2DiscreteVariables()
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void AnalyzeDond2DiscreteVariables::declareOptions(OptionList& ol)
+{
+
+    declareOption(ol, "variable_name", &AnalyzeDond2DiscreteVariables::variable_name,
+                  OptionBase::buildoption,
+                  "The field name of the variable to be analyzed.");
+
+    declareOption(ol, "target_name", &AnalyzeDond2DiscreteVariables::target_name,
+                  OptionBase::buildoption,
+                  "The field name of the target.");
+
+    declareOption(ol, "values_to_analyze", &AnalyzeDond2DiscreteVariables::values_to_analyze,
+                  OptionBase::buildoption,
+                  "The vector of values to check the correlation with the target.\n"
+                  "The algorithm groups the values from, to of each pair specified.\n");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void AnalyzeDond2DiscreteVariables::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(values_to_analyze, copies);
+    deepCopyField(variable_name, copies);
+    deepCopyField(target_name, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void AnalyzeDond2DiscreteVariables::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void AnalyzeDond2DiscreteVariables::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        analyzeDiscreteVariable();
+        PLERROR("AnalyzeDond2DiscreteVariables: we are done here");
+    }
+}
+
+void AnalyzeDond2DiscreteVariables::analyzeDiscreteVariable()
+{    
+    // initialize primary dataset
+    main_row = 0;
+    main_col = 0;
+    main_length = train_set->length();
+    main_width = train_set->width();
+    main_input.resize(main_width);
+    main_names.resize(main_width);
+    main_names << train_set->fieldNames();
+    
+    // check for valid options
+    number_of_values = values_to_analyze.size();
+    variable_col = -1;
+    target_col = -1;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (variable_name == main_names[main_col]) variable_col = main_col;
+        if (target_name == main_names[main_col]) target_col = main_col;
+    }
+    if (variable_col < 0) PLERROR("In AnalyzeDond2DiscreteVariables: variable name not found: %s", variable_name.c_str());
+    if (target_col < 0) PLERROR("In AnalyzeDond2DiscreteVariables: target name not found: %s", target_name.c_str());
+    if (number_of_values <= 0) PLERROR("In AnalyzeDond2DiscreteVariables: invalid values_to_analyze");
+    
+    // initialize working variables
+    value_target_sum.resize(number_of_values);
+    value_present_count.resize(number_of_values);
+    value_target_sum.clear();
+    value_present_count.clear();
+    target_sum = 0.0;
+    target_squared_sum = 0.0;
+    variable_present_count = 0.0;
+    
+    //Now, we can process the discrete variable.
+    ProgressBar* pb = 0;
+    pb = new ProgressBar( "Analyzing discrete variable " + variable_name, main_length);
+    for (main_row = 0; main_row < main_length; main_row++)
+    {
+        train_set->getRow(main_row, main_input);
+        variable_value = main_input[variable_col];
+        if (is_missing(variable_value)) continue;
+        target_value = main_input[target_col];
+        target_sum += target_value;
+        target_squared_sum += target_value * target_value;
+        variable_present_count += 1.0;
+        for (value_col = 0; value_col < number_of_values; value_col++)
+        {
+            if (variable_value < values_to_analyze[value_col].first || variable_value > values_to_analyze[value_col].second) continue;
+            value_target_sum[value_col] += target_value;
+            value_present_count[value_col] += 1.0;
+        }
+        pb->update( main_row );
+    }
+    delete pb;
+    if (variable_present_count <= 0.0)
+    {
+        cout << "In AnalyzeDond2DiscreteVariables: no value present for this variable" << endl;
+        return;
+    }
+    target_mean = target_sum / variable_present_count;
+    cout << "In AnalyzeDond2DiscreteVariables, for variable:  " << variable_name << endl;
+    cout << variable_present_count << " values are present out of " << main_length << " samples." << endl;
+    for (value_col = 0; value_col < number_of_values; value_col++)
+    {
+        ssxy = value_target_sum[value_col] - value_present_count[value_col] * target_mean;
+        ss2xy = ssxy * ssxy;
+        ssxx = value_present_count[value_col] * (1.0 -  value_present_count[value_col] / variable_present_count);
+        ssyy = target_squared_sum - target_sum * target_mean;
+        correlation_coefficient = ss2xy / (ssxx * ssyy);
+        cout << "For value from: " << values_to_analyze[value_col].first << " to: " << values_to_analyze[value_col].second 
+             << " occurence: " << value_present_count[value_col] << " correlation coefficient: " << correlation_coefficient << endl;
+    }
+}
+
+int AnalyzeDond2DiscreteVariables::outputsize() const {return 0;}
+void AnalyzeDond2DiscreteVariables::train() {}
+void AnalyzeDond2DiscreteVariables::computeOutput(const Vec&, Vec&) const {}
+void AnalyzeDond2DiscreteVariables::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> AnalyzeDond2DiscreteVariables::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> AnalyzeDond2DiscreteVariables::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,167 @@
+// -*- C++ -*-
+
+// AnalyzeDond2DiscreteVariables.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file AnalyzeDond2DiscreteVariables.h */
+
+
+#ifndef AnalyzeDond2DiscreteVariables_INC
+#define AnalyzeDond2DiscreteVariables_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn/vmat/FileVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class AnalyzeDond2DiscreteVariables : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    
+    //! The field name of the variable to be analyzed.
+    string variable_name;
+    
+    //! The field name of the target.
+    string target_name;
+    
+    //! The vector of values to check the correlation with the target.
+    //! The algorithm groups the values from, to of each pair specified.
+    TVec< pair<real, real> > values_to_analyze;    
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    AnalyzeDond2DiscreteVariables();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(AnalyzeDond2DiscreteVariables);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void analyzeDiscreteVariable();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    
+    // input instructions variables
+    int value_col;
+    int number_of_values;
+    int variable_col;
+    int target_col;
+    Vec value_target_sum;
+    Vec value_present_count;
+    real target_sum;
+    real target_squared_sum;
+    real variable_present_count;
+    real ssxy;
+    real ss2xy;
+    real ssxx;
+    real ssyy;
+    real correlation_coefficient;
+    
+    // primary dataset variables
+    int main_length;
+    int main_width;
+    int main_row;
+    int main_col;
+    Vec main_input;
+    TVec<string> main_names;
+    real variable_value;
+    real target_value;
+    real target_mean;
+    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(AnalyzeDond2DiscreteVariables);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,454 @@
+// -*- C++ -*-
+
+// AnalyzeFieldStats.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file AnalyzeFieldStats.cc */
+
+#define PL_LOG_MODULE_NAME "AnalyzeFieldStats"
+#include <plearn/io/pl_log.h>
+
+#include "AnalyzeFieldStats.h"
+#include <plearn/io/load_and_save.h>          //!<  For save
+#include <plearn/io/fileutils.h>              //!<  For isfile()
+#include <plearn/math/random.h>               //!<  For the seed stuff.
+#include <plearn/vmat/ExplicitSplitter.h>     //!<  For the splitter stuff.
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    AnalyzeFieldStats,
+    "Computes correlation coefficient between various discrete values and the target.",
+    "name of the discrete variable, of the target and the values to check are options.\n"
+);
+
+/////////////////////////
+// AnalyzeFieldStats //
+/////////////////////////
+AnalyzeFieldStats::AnalyzeFieldStats() :
+  min_number_of_samples(5000),
+  max_number_of_samples(50000)
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void AnalyzeFieldStats::declareOptions(OptionList& ol)
+{
+
+    declareOption(ol, "min_number_of_samples", &AnalyzeFieldStats::min_number_of_samples,
+                  OptionBase::buildoption,
+                  "The minimum number of samples required to train the learner.");
+    declareOption(ol, "max_number_of_samples", &AnalyzeFieldStats::max_number_of_samples,
+                  OptionBase::buildoption,
+                  "The maximum number of samples used to train the learner");
+    declareOption(ol, "targeted_set", &AnalyzeFieldStats::targeted_set,
+                  OptionBase::buildoption,
+                  "The train and test data sets with the target field.");
+    declareOption(ol, "cond_mean_template", &AnalyzeFieldStats::cond_mean_template,
+                  OptionBase::buildoption,
+                  "The template of the script to learn the conditional mean.");
+    declareOption(ol, "fields", &AnalyzeFieldStats::fields,
+                  OptionBase::buildoption,
+                  "The vector of fields to consider by names.");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void AnalyzeFieldStats::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(min_number_of_samples, copies);
+    deepCopyField(max_number_of_samples, copies);
+    deepCopyField(targeted_set, copies);
+    deepCopyField(cond_mean_template, copies);
+    deepCopyField(fields, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void AnalyzeFieldStats::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void AnalyzeFieldStats::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        for (int iteration = 1; iteration <= 50; iteration++)
+        {
+            cout << "In AnalyzeFieldStats, Iteration # " << iteration << endl;
+            analyzeVariableStats();
+            train();
+        }
+        PLERROR("AnalyzeFieldStats: we are done here");
+    }
+}
+
+void AnalyzeFieldStats::analyzeVariableStats()
+{ 
+    // initialize primary dataset
+    main_row = 0;
+    main_col = 0;
+    main_length = train_set->length();
+    main_width = train_set->width();
+    main_input.resize(main_width);
+    main_names.resize(main_width);
+    main_names << train_set->fieldNames();
+    main_metadata = train_set->getMetaDataDir();
+    
+    // validate the field instructions
+    fields_width = fields.size();
+    fields_selected.resize(main_width);
+    fields_selected.clear();
+    for (fields_col = 0; fields_col < fields_width; fields_col++)
+    {
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            if (fields[fields_col] == main_names[main_col]) break;
+        }
+        if (main_col >= main_width) PLERROR("In AnalyzeFieldStats: no field with this name in input dataset: %", (fields[fields_col]).c_str());
+        fields_selected[main_col] = 1;
+    }
+    
+    // initialize targeted datasets
+    cout << "initialize train_test datasets" << endl;
+    targeted_length = targeted_set->length();
+    targeted_width = targeted_set->width();
+    targeted_input.resize(targeted_width);
+    targeted_names.resize(targeted_width);
+    targeted_names << targeted_set->fieldNames();
+    targeted_metadata = targeted_set->getMetaDataDir();
+    
+    // initialize the header file
+    cout << "initialize the header file" << endl;
+    train_set->lockMetaDataDir();
+    header_record.resize(main_width);
+    header_file_name = targeted_metadata + "/TreeCondMean/header.pmat";
+    if (!isfile(header_file_name)) createHeaderFile();
+    else getHeaderRecord();
+    
+    // choose variable to build a conditionnal function for
+    cout << "choose variable to build a conditionnal function for" << endl;
+    TVec<int> indices;
+    to_deal_with_total = 0;
+    to_deal_with_next = -1;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (header_record[main_col] != 2.0) continue;
+        to_deal_with_total += 1;
+        if (to_deal_with_next < 0) to_deal_with_next = main_col;
+    }
+    if (to_deal_with_next < 0)
+    {
+        train_set->unlockMetaDataDir();
+        reviewGlobalStats();
+        PLERROR("AnalyzeFieldStats: we are done here");
+    }
+    to_deal_with_name = main_names[to_deal_with_next];
+    cout << "total number of variable left to deal with: " << to_deal_with_total << endl;
+    cout << "next variable to deal with: " << main_names[to_deal_with_next] << endl;
+    updateHeaderRecord(to_deal_with_next);
+    train_set->unlockMetaDataDir();
+    
+    // find the available targeted records for this variable
+    ProgressBar* pb = 0;
+    main_stats = train_set->getStats(to_deal_with_next);
+    main_total = main_stats.n();
+    main_missing = main_stats.nmissing();
+    main_present = main_total - main_missing;
+    indices.resize((int) main_present);
+    ind_next = 0;
+    pb = new ProgressBar( "Building the indices for " + to_deal_with_name, main_length);
+    for (main_row = 0; main_row < main_length; main_row++)
+    {
+        to_deal_with_value = train_set->get(main_row, to_deal_with_next);
+        if (is_missing(to_deal_with_value)) continue;
+        if (ind_next >= indices.length()) PLERROR("AnalyzeFieldStats: There seems to be more present values than indicated by the stats file");
+        indices[ind_next] = main_row;
+        ind_next += 1;
+        pb->update( main_row );
+    }
+    delete pb;
+    
+    // shuffle the indices.
+    manual_seed(123456);
+    shuffleElements(indices);
+    
+    // initialize output datasets
+    output_length = (int) main_present;
+    if (output_length > max_number_of_samples) output_length = max_number_of_samples;
+    output_width = 0;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (header_record[main_col] != 1) output_width += 1;
+    }
+    output_variable_src.resize(output_width);
+    output_names.resize(output_width);
+    output_vec.resize(output_width);
+    output_path = main_metadata + "condmean_" + to_deal_with_name + ".pmat";
+    output_col = 0;
+    for (fields_col = 0; fields_col < fields_width; fields_col++)
+    {
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            if (fields[fields_col] == main_names[main_col]) break;
+        }
+        if (main_col >= main_width) PLERROR("In AnalyzeFieldStats: no field with this name in input dataset: %", (fields[fields_col]).c_str());
+        if (fields_col != to_deal_with_next && header_record[main_col] != 1)
+        {
+            output_variable_src[output_col] = main_col;
+            output_names[output_col] = fields[fields_col];
+            output_col += 1;
+        }
+    }
+    output_variable_src[output_col] = to_deal_with_next;
+    output_names[output_col] = to_deal_with_name;
+    output_file = new MemoryVMatrix(output_length, output_width);
+    output_file->declareFieldNames(output_names);
+    output_file->defineSizes(output_width - 1, 1, 0);
+    
+    //Now, we can build the training file
+    pb = new ProgressBar( "Building the training file for " + to_deal_with_name, output_length);
+    for (main_row = 0; main_row < output_length; main_row++)
+    {
+        train_set->getRow(indices[main_row], main_input);
+        for (output_col = 0; output_col < output_width; output_col++)
+        {
+            output_vec[output_col] = main_input[output_variable_src[output_col]];
+        }
+        output_file->putRow(main_row, output_vec);
+        pb->update( main_row );
+    }
+    delete pb;
+    
+    // initialize train_test datasets
+    train_test_length = targeted_length;
+    train_test_variable_src.resize(output_width);
+    train_test_path = targeted_metadata + "targeted_" + to_deal_with_name + ".pmat";
+    output_col = 0;
+    for (fields_col = 0; fields_col < fields_width; fields_col++)
+    {
+        for (main_col = 0; main_col < targeted_width; main_col++)
+        {
+            if (fields[fields_col] == targeted_names[main_col]) break;
+        }
+        if (main_col >= targeted_width) PLERROR("In AnalyzeFieldStats: no field with this name in targeted dataset: %", (fields[fields_col]).c_str());
+        if (fields_col != to_deal_with_next && header_record[main_col] != 1)
+        {
+            train_test_variable_src[output_col] = main_col;
+            output_col += 1;
+        }
+    }
+    train_test_variable_src[output_col] = to_deal_with_next;
+    train_test_file = new MemoryVMatrix(train_test_length, output_width);
+    train_test_file->declareFieldNames(output_names);
+    train_test_file->defineSizes(output_width - 1, 1, 0);
+    
+    //Now, we can build the targeted file
+    pb = new ProgressBar( "Building the targeted file for " + to_deal_with_name, train_test_length);
+    for (main_row = 0; main_row < train_test_length; main_row++)
+    {
+        targeted_set->getRow(main_row, targeted_input);
+        for (output_col = 0; output_col < output_width; output_col++)
+        {
+            output_vec[output_col] = targeted_input[train_test_variable_src[output_col]];
+        }
+        train_test_file->putRow(main_row, output_vec);
+        pb->update( main_row );
+    }
+    delete pb;
+}
+
+void AnalyzeFieldStats::createHeaderFile()
+{ 
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        targeted_stats = targeted_set->getStats(main_col);
+        targeted_missing = targeted_stats.nmissing();
+        main_stats = train_set->getStats(main_col);
+        main_total = main_stats.n();
+        main_missing = main_stats.nmissing();
+        main_present = main_total - main_missing;
+        if (fields_selected[main_col] < 1) header_record[main_col] = 1;                  // delete column, field not selected
+        else if (targeted_missing <= 0) header_record[main_col] = 0;                     // nothing to do
+        else if (main_present < min_number_of_samples) header_record[main_col] = 1;      // delete column
+        else header_record[main_col] = 2;                                                // build tree
+    }
+    header_file = new FileVMatrix(header_file_name, 1, main_names);
+    header_file->putRow(0, header_record);
+}
+
+void AnalyzeFieldStats::getHeaderRecord()
+{ 
+    header_file = new FileVMatrix(header_file_name, true);
+    header_file->getRow(0, header_record);
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (header_record[main_col] == 0) continue;
+        if (header_record[main_col] == 2) continue;
+        if (header_record[main_col] == 1 && fields_selected[main_col] < 1) continue;
+        if (header_record[main_col] == 1)
+        {
+            main_stats = train_set->getStats(main_col);
+            main_total = main_stats.n();
+            main_missing = main_stats.nmissing();
+            main_present = main_total - main_missing;
+            if (main_present >= min_number_of_samples) header_record[main_col] = 2;
+            continue;
+        }
+    }
+}
+
+void AnalyzeFieldStats::updateHeaderRecord(int var_col)
+{ 
+    header_file->put(0, var_col, 3.0);
+}
+
+void AnalyzeFieldStats::reviewGlobalStats()
+{ 
+    cout << "There is no more variable to deal with." << endl;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (header_record[main_col] == 0)
+        { 
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : no missing values for this variable in the targeted files." << endl;
+            continue;
+        }
+        if (header_record[main_col] == 1 && fields_selected[main_col] < 1)
+        {
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : field not selected." << endl;
+            continue;
+        }
+        if (header_record[main_col] == 1)
+        {
+            main_stats = train_set->getStats(main_col);
+            main_total = main_stats.n();
+            main_missing = main_stats.nmissing();
+            main_present = main_total - main_missing;
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : field deleted, only " << setw(6) << main_present << " records to train with." << endl;
+            continue;
+        }
+        results_file_name = targeted_metadata + "/TreeCondMean/dir/" + main_names[main_col] + "/Split0/LearnerExpdir/Strat0results.pmat";
+        if (!isfile(results_file_name))
+        {
+            header_file->put(0, main_col, 2.0);
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : missing results file." << endl;
+            continue;
+        }
+        test_output_file_name = targeted_metadata + "/TreeCondMean/dir/" + main_names[main_col] + "/Split0/test1_outputs.pmat";
+        if (!isfile(test_output_file_name))
+        {
+            header_file->put(0, main_col, 2.0);
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : missing test output file." << endl;
+            continue;
+        }
+        results_file = new FileVMatrix(results_file_name);
+        results_length = results_file->length();
+        results_nstages = results_file->get(results_length - 1, 2);
+        results_mse = results_file->get(results_length - 1, 6);
+        results_std_err = results_file->get(results_length - 1, 7);
+        test_output_file = new FileVMatrix(test_output_file_name);
+        test_output_length = test_output_file->length();
+        cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+        cout << " : tree built with " << setw(2) << (int) results_nstages << " leaves, "
+             << setw(6) << test_output_length << " test output records found, "
+             << "performance: " << setiosflags(ios::fixed) << setprecision(4) << results_mse
+             << " +/- " << setiosflags(ios::fixed) << setprecision(4) << results_std_err << endl;
+    }
+}
+
+void AnalyzeFieldStats::train()
+{
+    PP<ExplicitSplitter> explicit_splitter = new ExplicitSplitter();
+    explicit_splitter->splitsets.resize(1,2);
+    explicit_splitter->splitsets(0,0) = output_file;
+    explicit_splitter->splitsets(0,1) = train_test_file;
+    cond_mean = ::PLearn::deepCopy(cond_mean_template);
+    cond_mean->setOption("expdir", targeted_metadata + "/TreeCondMean/dir/" + to_deal_with_name);
+    cond_mean->splitter = new ExplicitSplitter();
+    cond_mean->splitter = explicit_splitter;
+    cond_mean->build();
+    Vec results = cond_mean->perform(true);
+}
+
+int AnalyzeFieldStats::outputsize() const {return 0;}
+void AnalyzeFieldStats::computeOutput(const Vec&, Vec&) const {}
+void AnalyzeFieldStats::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> AnalyzeFieldStats::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> AnalyzeFieldStats::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,200 @@
+// -*- C++ -*-
+
+// AnalyzeFieldStats.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file AnalyzeFieldStats.h */
+
+
+#ifndef AnalyzeFieldStats_INC
+#define AnalyzeFieldStats_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/testers/PTester.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/vmat/MemoryVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class AnalyzeFieldStats : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    
+    //! The minimum number of samples required to train the learner.
+    int min_number_of_samples;
+    //! The maximum number of samples used to train the learner.
+    int max_number_of_samples;
+    //! The train and test data sets with the target field.
+    VMat targeted_set;
+    //! The template of the script to learn the conditional mean
+    PP<PTester> cond_mean_template;
+    //! The field name of the variable to be analyzed.
+    TVec<string> fields;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    AnalyzeFieldStats();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(AnalyzeFieldStats);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void analyzeVariableStats();
+    void createHeaderFile();
+    void getHeaderRecord();
+    void updateHeaderRecord(int var_col);
+    void reviewGlobalStats();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    
+    int main_row;
+    int main_col;
+    int main_length;
+    int main_width;
+    Vec main_input;
+    TVec<string> main_names;
+    StatsCollector  main_stats;
+    PPath main_metadata;
+    TVec<int> main_ins;
+    real main_total;
+    real main_missing;
+    real main_present;
+    int targeted_length;
+    int targeted_width;
+    Vec targeted_input;
+    TVec<string> targeted_names;
+    StatsCollector  targeted_stats;
+    PPath targeted_metadata;
+    real targeted_missing;
+    PPath header_file_name;
+    VMat header_file;
+    Vec header_record;
+    int fields_col;
+    int fields_width;
+    TVec<int> fields_selected;
+    int to_deal_with_total;
+    int to_deal_with_next;
+    real to_deal_with_value;
+    string to_deal_with_name;
+    int ind_next;
+    int output_length;
+    int output_width;
+    int output_col;
+    string output_path;
+    TVec<string> output_names;
+    Vec output_vec;
+    TVec<int> output_variable_src;
+    VMat output_file;
+    int train_test_length;
+    string train_test_path;
+    TVec<int> train_test_variable_src;
+    VMat train_test_file;
+    PP<PTester> cond_mean;
+    PPath results_file_name;
+    VMat results_file;
+    int results_length;
+    real results_nstages;
+    real results_mse;
+    real results_std_err;
+    PPath test_output_file_name;
+    VMat test_output_file;
+    int test_output_length;
+    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(AnalyzeFieldStats);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,797 @@
+// -*- C++ -*-
+
+// BallTreeNearestNeighbors.cc
+//
+// Copyright (C) 2004 Pascal Lamblin & Marius Muja
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+ * $Id: BallTreeNearestNeighbors.cc 4911 2006-02-09 22:02:57Z lamblin $ 
+ ******************************************************* */
+
+// Authors: Pascal Lamblin & Marius Muja
+
+/*! \file BallTreeNearestNeighbors.cc */
+
+#include "BallTreeNearestNeighbors.h"
+#include <plearn/base/lexical_cast.h>
+
+namespace PLearn {
+using namespace std;
+
+BallTreeNearestNeighbors::BallTreeNearestNeighbors() 
+    : rmin( 1 ),
+      train_method( "anchor" )
+{
+    num_neighbors = 1;
+    expdir = "";
+    stage = 0;
+    nstages = -1;
+    report_progress = 0;
+}
+
+BallTreeNearestNeighbors::BallTreeNearestNeighbors( const VMat& tr_set, const BinBallTree& b_tree )
+    : rmin( 1 ),
+      train_method( "anchor" )
+{
+    num_neighbors = 1;
+    expdir = "";
+    stage = 1;
+    nstages = 1;
+    report_progress = 0;
+
+    setTrainingSet( tr_set );
+    ball_tree = b_tree;
+}
+
+PLEARN_IMPLEMENT_OBJECT( BallTreeNearestNeighbors, 
+                         "Organizes hierarchically a set of points to perform efficient  KNN search", 
+                         "This learner builds a Ball Tree, a hierarchized structure\n"
+                         "allowing to perform efficient KNN search.\n"
+                         "Output is formatted as in GenericNearestNeighbors.\n"
+                         "The square distance to this point can be computed as the error.\n" );
+
+void BallTreeNearestNeighbors::declareOptions( OptionList& ol )
+{
+    // build options
+    declareOption( ol, "point_indices", &BallTreeNearestNeighbors::point_indices, 
+                   OptionBase::buildoption,
+                   "Indices of the points we will consider" );
+
+    declareOption( ol, "rmin", &BallTreeNearestNeighbors::rmin, OptionBase::buildoption,
+                   "Max number of points in a leaf node of the tree" );
+
+    declareOption( ol, "train_method", &BallTreeNearestNeighbors::train_method, 
+                   OptionBase::buildoption,
+                   "Method used to build the tree. Just one is supported:\n"
+                   "  \"anchor\" (middle-out building based on Anchor\'s hierarchy\n"
+        );
+
+    declareOption( ol, "anchor_set", &BallTreeNearestNeighbors::anchor_set, 
+                   OptionBase::learntoption, 
+                   "Set of anchors, hierarchizing the set of points" );
+
+    declareOption( ol, "pivot_indices", &BallTreeNearestNeighbors::pivot_indices, 
+                   OptionBase::learntoption, "Indices of the anchors' centers" );
+
+    // saved options
+    declareOption( ol, "train_set", &BallTreeNearestNeighbors::train_set, 
+                   OptionBase::buildoption,
+                   "Indexed set of points we will be working with" );
+
+    declareOption( ol, "nb_train_points", &BallTreeNearestNeighbors::nb_train_points, 
+                   OptionBase::learntoption, "Number of points in train_set" );
+
+    declareOption( ol, "nb_points", &BallTreeNearestNeighbors::nb_points, 
+                   OptionBase::learntoption, "Number of points in point_indices" );
+
+    declareOption( ol, "ball_tree", &BallTreeNearestNeighbors::ball_tree, 
+                   OptionBase::learntoption, "Built ball-tree" );
+
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions( ol );
+}
+
+void BallTreeNearestNeighbors::build_()
+{
+    if (train_set) {
+        // initialize nb_train_points
+        nb_train_points = train_set.length();
+        
+        // if point_indices isn't specified, we take all the points in train_set
+        if( !point_indices )
+            point_indices = TVec<int>( 0, nb_train_points-1, 1 );
+
+        // initialize nb_points
+        nb_points = point_indices.size();
+    }
+}
+
+
+void BallTreeNearestNeighbors::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void BallTreeNearestNeighbors::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField( ball_tree, copies );
+    deepCopyField( point_indices, copies );
+    deepCopyField( anchor_set, copies );
+    deepCopyField( pivot_indices, copies );
+}
+
+
+void BallTreeNearestNeighbors::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend on the 'seed' option)
+    //! And sets 'stage' back to 0   (this is the stage of a fresh learner!)
+
+    anchor_set.resize( 0 );
+    pivot_indices.resize( 0 );
+    ball_tree = new BinaryBallTree;
+    stage = 0;
+    build();
+}
+
+
+
+
+void BallTreeNearestNeighbors::train()
+{
+    // The role of the train method is to bring the learner up to stage==nstages,
+    // updating train_stats with training costs measured on-line in the process.
+
+    if( train_method == "anchor" )
+    {
+        anchorTrain();
+    }
+    else
+        PLERROR( "train_method \"%s\" not implemented", train_method.c_str() );
+}
+
+
+void BallTreeNearestNeighbors::anchorTrain()
+{
+    /*  nstages and stage conventions, for "anchor" train method:
+     *
+     *  nstages == -1
+     *    We will construct ball_tree recursively,
+     *    until, for all leaf, nb_points <= rmin,
+     *    no matter how many iterations it will take.
+     *
+     *  nstages == 0
+     *    We want the PLearner il its fresh, blank state.
+     *
+     *  nstages == 1
+     *    We want ball_tree to be a unique leaf node,
+     *    containing all the point indices, with no children.
+     *
+     *  nstages > 1
+     *    We want to build ball_tree recursively,
+     *    but limiting the levels of recursion.
+     *    This means we will decrement this number at each recursive call,
+     *    the recursion will stop when nstages == 1 or nb_points <= rmin.
+     *
+     *  stage == 0
+     *    The learner is it its fresh, blank state.
+     *
+     *  stage == 1
+     *    The learner has one anchor, that's all.
+     *
+     *  Other values of stage might be used one day or anoter...
+     */
+
+    if( stage == 0 && nstages !=0 )
+    {
+        // That means we weren't provided with any anchor nor node parameter,
+        // or that they were just bullsh!t
+
+        // So, we build a single anchor
+        pivot_indices.resize( 1 );
+        pivot_indices[ 0 ] = 0;
+        Vec pivot = train_set.getSubRow( 0, inputsize() );
+
+        distance_kernel->setDataForKernelMatrix( train_set );
+        distance_kernel->build();
+        Vec distances_from_pivot( nb_train_points );
+        distance_kernel->evaluate_all_i_x( pivot, distances_from_pivot );
+
+        anchor_set.resize( 1 );
+        Mat* p_anchor = &anchor_set[ 0 ];
+        p_anchor->resize( nb_points, 2 );
+        p_anchor->column( 0 ) << Vec( 0, nb_points-1, 1 );
+        p_anchor->column( 1 ) << distances_from_pivot;
+        sortRows( *p_anchor, TVec<int>( 1, 1 ), false );
+
+        // then, we build the corresponding tree
+        ball_tree = leafFromAnchor( 0 );
+
+        ++stage;
+    }
+
+    if( nstages == 0 )
+    {
+        // we want a fresh, blank learner
+        forget();
+    }
+    else if( nstages == 1 )
+    {
+        // We have an anchor, and we want a leaf node
+        ball_tree = leafFromAnchor( 0 );
+    }
+    else
+    {
+        // nstages to be used on children learners
+        int new_nstages = nstages<0 ? -1 : nstages-1;
+
+        // First create sqrt( R )-1 anchors, from the initial anchor_set
+        int nb_anchors = (int) sqrt( (float) nb_points ) + 1 ;
+        nb_anchors = min( nb_anchors, nb_points );
+
+        createAnchors( nb_anchors-1 ); // because we already have one
+
+        // Convert them into leaf nodes
+        TVec< BinBallTree > leaf_set = TVec<BinBallTree>( nb_anchors );
+        for ( int i=0 ; i<nb_anchors ; i++ )
+        {
+            leaf_set[ i ] = leafFromAnchor( i );
+        }
+
+        // Then, group them to form the ball_tree
+        // keep an index of the leaves
+        ball_tree = treeFromLeaves( leaf_set );
+
+        // Now, recurse...
+        for( int i=0 ; i<leaf_set.size() ; i++ )
+        {
+            int rec_nb_points = anchor_set[ i ].length();
+
+            // if the leaf is too small, don't do anything
+            if( rec_nb_points > rmin )
+            {
+                // child learner
+                PP<BallTreeNearestNeighbors> p_rec_learner = new BallTreeNearestNeighbors();
+
+                // initializes child's nstages (see explanation above)
+                stringstream out;
+                out << new_nstages;
+                p_rec_learner->setOption( "nstages" , out.str() );
+
+                // keep the same training set: it give us all the point coordinates !
+                // but we don't want to call forget() after that
+                p_rec_learner->setTrainingSet( train_set, false );
+
+                // however, we only work on the points contained by current leaf
+                p_rec_learner->anchor_set.resize( 1 );
+                p_rec_learner->anchor_set[ 0 ].resize( rec_nb_points, 2 );
+                p_rec_learner->anchor_set[ 0 ] << anchor_set[ i ];
+
+                p_rec_learner->pivot_indices.resize( 1 );
+                p_rec_learner->pivot_indices[ 0 ] = pivot_indices[ i ];
+
+                p_rec_learner->point_indices.resize( rec_nb_points );
+                p_rec_learner->point_indices << 
+                    p_rec_learner->anchor_set[ 0 ].column( 0 );
+
+                p_rec_learner->stage = 1; 
+                // faudra peut-etre faire ?a plus subtilement
+
+                p_rec_learner->rmin = rmin;
+                p_rec_learner->train_method = train_method;
+                p_rec_learner->build();
+                p_rec_learner->train();
+
+                // once the child learner is trained, we can get the sub-tree,
+                // and link it correctly
+                BinBallTree subtree = p_rec_learner->getBallTree();
+                leaf_set[ i ]->pivot = subtree->pivot;
+                leaf_set[ i ]->radius = subtree->radius;
+                leaf_set[ i ]->point_set.resize( subtree->point_set.size() );
+                leaf_set[ i ]->point_set << subtree->point_set;
+                leaf_set[ i ]->setFirstChild( subtree->getFirstChild() );
+                leaf_set[ i ]->setSecondChild( subtree->getSecondChild() );
+
+            }
+        }
+    }
+}
+
+
+void BallTreeNearestNeighbors::createAnchors( int nb_anchors )
+{
+    // This method creates nb_anchors new anchors, and adds them to anchor_set
+
+    // Make room
+    int anchor_set_size = anchor_set.size();
+    anchor_set.resize( anchor_set_size, nb_anchors );
+
+    for( int i=0 ; i<nb_anchors ; i++ )
+    {
+        Mat new_anchor = Mat( 1, 2 );
+        int new_pivot_index;
+
+        // Search for the largest ball.
+        // pivot of the new anchor will be the point of this ball
+        // that is the furthest from the pivot.
+        int largest_index = 0;
+        real largest_radius = 0;
+        for( int j=0 ; j<anchor_set_size ; j++ )
+        {
+            // points are sorted in decreasing order of distance, 
+            // so anchor_set[ j ]( 0, 1 ) is the furthest point from 
+            // pivot_indices[ j ]
+            real current_radius = anchor_set[ j ]( 0, 1 );
+            if( current_radius > largest_radius )
+            {
+                largest_radius = current_radius;
+                largest_index = j;
+            }
+        }
+
+        Mat* p_largest_anchor = &anchor_set[ largest_index ];
+        new_pivot_index = (int) (*p_largest_anchor)( 0, 0 );
+
+        // assign the point to its new anchor
+        new_anchor( 0, 0 ) = new_pivot_index;
+        new_anchor( 0, 1 ) = 0;
+        Vec new_pivot = train_set.getSubRow( new_pivot_index, inputsize() );
+
+        int largest_anchor_length = p_largest_anchor->length();
+
+        // Verify that largest_anchor owns at least 2 points
+        if( largest_anchor_length <= 1 )
+        {
+            PLERROR("In BallTreeNearestNeighbors::createAnchors, more anchors asked than points");
+        }
+
+        // delete this point from its original anchor
+        *p_largest_anchor = p_largest_anchor->
+            subMatRows( 1, largest_anchor_length-1 );
+
+        // now, try to steal points from all the existing anchors
+        for( int j=0 ; j<anchor_set_size ; j++ )
+        {
+            Mat* p_anchor = &anchor_set[ j ];
+            int nb_points = p_anchor->length();
+            int pivot_index = pivot_indices[ j ];
+            Vec pivot = train_set.getSubRow( pivot_index, inputsize() );
+            real pivot_pow_dist = powdistance( new_pivot, pivot, 2 );
+
+            // loop on the anchor's points
+            for( int k=0 ; k<nb_points ; k++ )
+            {
+                int point_index = (int) (*p_anchor)( k, 0 );
+                real point_pow_dist = (*p_anchor)( k, 1 );
+
+                // if this inequality is verified,
+                // then we're sure that all the points closer to the pivot 
+                // belong to the pivot, and we don't need to check
+                if( 4*point_pow_dist < pivot_pow_dist )
+                {
+                    break;
+                }
+
+                Vec point = train_set.getSubRow( point_index, inputsize() );
+                real new_pow_dist = powdistance( new_pivot, point, 2 );
+
+                // if the point is closer to the new pivot, then steal it
+                if( new_pow_dist < point_pow_dist )
+                {
+                    Vec new_row( 2 );
+                    new_row[ 0 ] = point_index;
+                    new_row[ 1 ] = new_pow_dist;
+                    new_anchor.appendRow( new_row );
+
+                    *p_anchor = removeRow( *p_anchor, k );
+                    // bleaah, this is ugly !
+                    --k;
+                    --nb_points;
+                }
+            }
+        }
+
+        // sort the points by decreasing distance
+        sortRows( new_anchor, TVec<int>( 1, 1 ), false );
+
+        // append the new anchor to the anchor_set (and same for pivot)
+        anchor_set.append( new_anchor );
+        pivot_indices.append( new_pivot_index );
+        ++anchor_set_size;
+    }
+}
+
+BinBallTree BallTreeNearestNeighbors::leafFromAnchor( int anchor_index )
+{
+    BinBallTree leaf = new BinaryBallTree();
+
+    int pivot_index = pivot_indices[ anchor_index ];
+    leaf->pivot = train_set.getSubRow( pivot_index, inputsize() );
+
+    leaf->radius = anchor_set[ anchor_index ]( 0, 1 );
+
+    int nb_leaf_points = anchor_set[ anchor_index ].length();
+    leaf->point_set.resize( nb_leaf_points );
+    leaf->point_set << anchor_set[ anchor_index ].column( 0 );
+
+    return leaf;
+}
+
+
+BinBallTree BallTreeNearestNeighbors::treeFromLeaves( const TVec<BinBallTree>& leaves )
+{
+    int nb_nodes = leaves.size();
+    TVec<BinBallTree> nodes = TVec<BinBallTree>( nb_nodes );
+    nodes << leaves;
+
+    // if there is no leaf
+    if( nb_nodes < 1 )
+    {
+        PLERROR( "In BallTreeNearestNeighbors::treeFromLeaves(): no leaf existing" );
+    }
+
+    while( nb_nodes > 1 )
+    {
+        int min_i = 0;
+        int min_j = 0;
+        Vec min_center;
+        real min_radius = -1;
+
+        // we get the most "compatible" pair of nodes :
+        // the ball containing them both is the smallest
+        for( int i=0 ; i<nb_nodes ; i++ )
+        {
+            Vec center_i = nodes[ i ]->pivot;
+            real radius_i = nodes[ i ]->radius;
+
+            // to scan all pairs only once, and avoid i==j
+            for( int j=0 ; j<i ; j++ )
+            {
+                Vec center_j = nodes[ j ]->pivot;
+                real radius_j = nodes[ j ]->radius;
+
+                Vec t_center;
+                real t_radius;
+                smallestContainer( center_i, radius_i, center_j, radius_j, 
+                                   t_center, t_radius );
+
+                if( t_radius < min_radius || min_radius < 0 )
+                {
+                    min_i = i;
+                    min_j = j ;
+                    min_radius = t_radius;
+                    min_center = t_center;
+                }
+            }
+        }
+
+#ifdef DEBUG_CHECK_NAN
+        if (min_center.hasMissing())
+            PLERROR("In BallTreeNearestNeighbors::treeFromLeaves: min_center is NaN");
+#endif
+        
+        // Group these two nodes into a parent_node.
+        // TODO: something more sensible for the radius and center...
+        BinBallTree parent_node = new BinaryBallTree();
+        parent_node->pivot = min_center;
+        parent_node->radius = min_radius;
+        parent_node->setFirstChild( nodes[ min_i ] );
+        parent_node->setSecondChild( nodes[ min_j ] );
+
+        nodes[ min_j ] = parent_node;
+        nodes.remove( min_i );
+
+        --nb_nodes;
+    }
+
+    // then, we have only one anchor
+    BinBallTree root = nodes[ 0 ];
+    return root;
+}
+
+
+BinBallTree BallTreeNearestNeighbors::getBallTree()
+{
+    return ball_tree;
+}
+
+
+void BallTreeNearestNeighbors::computeOutputAndCosts(
+    const Vec& input, const Vec& target, Vec& output, Vec& costs ) const
+{
+    int nout = outputsize();
+    output.resize( nout );
+    costs.resize( num_neighbors );
+
+    // we launch a k-nearest-neighbors query on the root node (ball_tree)
+    priority_queue< pair<real,int> > q;
+    FindBallKNN( q, input, num_neighbors );
+
+    // dequeue the found nearest neighbors, beginning by the farthest away
+    int n_found = q.size();
+    TVec<int> neighbors( n_found );
+    for( int i=n_found-1 ; i>=0 ; i-- )
+    {
+        const pair<real,int>& cur_top = q.top();
+        costs[i] = cur_top.first;
+        neighbors[i] = cur_top.second;
+        q.pop();
+    }
+
+    // fill costs with missing values
+    for( int i= n_found ; i<num_neighbors ; i++ )
+        costs[i] = MISSING_VALUE;
+
+    constructOutputVector( neighbors, output );
+}
+
+void BallTreeNearestNeighbors::computeOutput(
+    const Vec& input, Vec& output ) const
+{
+    // Compute the output from the input.
+    // int nout = outputsize();
+    // output.resize(nout);
+
+    int nout = outputsize();
+    output.resize( nout );
+
+    // we launch a k-nearest-neighbors query on the root node (ball_tree)
+    priority_queue< pair<real,int> > q;
+    FindBallKNN( q, input, num_neighbors );
+
+    // dequeue the found nearest neighbors, beginning by the farthest away
+    int n_found = q.size();
+    TVec<int> neighbors( n_found );
+    for( int i=n_found-1 ; i>=0 ; i-- )
+    {
+        const pair<real,int>& cur_top = q.top();
+        neighbors[i] = cur_top.second;
+        q.pop();
+    }
+
+    constructOutputVector( neighbors, output );
+
+}
+
+
+void BallTreeNearestNeighbors::computeCostsFromOutputs(
+    const Vec& input, const Vec& output, const Vec& target, Vec& costs ) const
+{
+    // Compute the costs from *already* computed output.
+    costs.resize( num_neighbors );
+
+    int inputsize = train_set->inputsize();
+    int targetsize = train_set->targetsize();
+    int weightsize = train_set->weightsize();
+
+    Mat out( num_neighbors, inputsize );
+
+    if( copy_input )
+    {
+        for( int i=0 ; i<num_neighbors ; i++ )
+            out( i ) << output.subVec( i*outputsize(), inputsize );
+    }
+    else if( copy_index )
+    {
+        int offset = 0;
+
+        if( copy_target )
+            offset += targetsize;
+
+        if( copy_weight )
+            offset += weightsize;
+
+        for( int i=0 ; i<num_neighbors ; i++ )
+            out( i ) << train_set( (int) output[ i*outputsize() + offset ] );
+    }
+    else
+    {
+        PLERROR( "computeCostsFromOutput:\n"
+                 "neither indices nor coordinates of output computed\n" );
+    }
+
+    for( int i=0 ; i<num_neighbors ; i++ )
+        costs[ i ] = powdistance( input, out( i ) );
+}
+
+TVec<string> BallTreeNearestNeighbors::getTestCostNames() const
+{
+    return TVec<string>( num_neighbors, "squared_distance" );
+}
+
+TVec<string> BallTreeNearestNeighbors::getTrainCostNames() const
+{
+    return TVec<string>();
+}
+
+bool BallTreeNearestNeighbors::intersect(
+    const Vec& center1, const real& powrad1,
+    const Vec& center2, const real& powrad2 )
+{
+    real radius1 = sqrt( powrad1 );
+    real radius2 = sqrt( powrad2 );
+
+    real pow_dist = powdistance( center1, center2, 2 );
+    real rad_sum = radius1 + radius2;
+    bool result = ( pow_dist <= ( rad_sum * rad_sum ) );
+    return result;
+}
+
+bool BallTreeNearestNeighbors::contain(
+    const Vec& center1, const real& powrad1,
+    const Vec& center2, const real& powrad2 )
+{
+    real radius1 = sqrt( powrad1 );
+    real radius2 = sqrt( powrad2 );
+    real rad_dif = radius1 - radius2;
+
+    if( rad_dif >= 0 )
+    {
+        real pow_dist = powdistance( center1, center2, 2 );
+        bool result = ( pow_dist <= ( rad_dif * rad_dif ) );
+        return result;
+    }
+    else
+    {
+        return false;
+    }
+}
+
+void BallTreeNearestNeighbors::smallestContainer(
+    const Vec& center1, const real& powrad1,
+    const Vec& center2, const real& powrad2,
+    Vec& t_center, real& t_powrad )
+{
+    if( center1 == center2 )
+    {
+        t_center = center1;
+        t_powrad = max( powrad1, powrad2 );
+    }
+    else if( contain( center1, powrad1, center2, powrad2 ) )
+    {
+        t_center = center1;
+        t_powrad = powrad1;
+    }
+    else if( contain( center2, powrad2, center1, powrad1 ) )
+    {
+        t_center = center2;
+        t_powrad = powrad2;
+    }
+    else
+    {
+        real radius1 = sqrt( powrad1 );
+        real radius2 = sqrt( powrad2 );
+        real center_dist = dist( center1, center2, 2 ) ;
+        real coef = ( radius1 - radius2 ) / center_dist ;
+        t_center = real(0.5) * ( ( 1 + coef ) * center1  +  ( 1 - coef ) * center2 ) ;
+        real t_radius = real(0.5) * ( center_dist + radius1 + radius2 ) ;
+        t_powrad = t_radius * t_radius;
+    }
+
+#ifdef DEBUG_CHECK_NAN
+    if (t_center.hasMissing())
+        PLERROR("In BallTreeNearestNeighbors::smallestContainer: t_center is NaN.");
+#endif
+}
+
+
+
+void BallTreeNearestNeighbors::BallKNN(
+     priority_queue< pair<real,int> >& q, BinBallTree node,
+     const Vec& t, real& d2_sofar, real d2_pivot, const int k ) const
+{
+    real d_minp = max( sqrt(d2_pivot) - node->radius, 0.0 );
+#ifdef DEBUG_CHECK_NAN
+    if (isnan(d_minp))
+        PLERROR("BallTreeNearestNeighbors::BallKNN: d_minp is NaN");
+#endif
+
+    if (d_minp*d_minp > d2_sofar)
+    {
+        // no chance of finding anything closer around this node
+        return;
+    }
+    else if (node->point_set.size()!=0) // node is leaf
+    {
+        int n_points = node->point_set.size();
+        for( int i=0 ; i<n_points ; i++ )
+        {
+            int j = node->point_set[i];
+            real dist;
+            // last point is pivot, and we already now the distance
+            if( i==n_points-1 )
+            {
+                dist = d2_pivot;
+            }
+            else
+            {
+                Vec x = train_set.getSubRow(j, inputsize());
+                dist = powdistance(x, t, 2);
+            }
+            if( dist < d2_sofar )
+            {
+                q.push( make_pair(dist, j) );
+                int n_found = q.size();
+                if( n_found > k )
+                    q.pop();
+                if( n_found >= k )
+                    d2_sofar = q.top().first;
+            }
+        }
+    }
+    else if (!node->isEmpty()) // node is not leaf
+    {
+        BinBallTree node1 = node->getFirstChild();
+        BinBallTree node2 = node->getSecondChild();
+
+        real d2_pivot1 = powdistance(t, node1->pivot, 2);
+        real d2_pivot2 = powdistance(t, node2->pivot, 2);
+
+        if( d2_pivot1 > d2_pivot2 ) // node1 is closer to t
+        {
+            pl_swap(node1, node2);
+            pl_swap(d2_pivot1, d2_pivot2);
+        }
+
+        BallKNN(q, node1, t, d2_sofar, d2_pivot1, k);
+        BallKNN(q, node2, t, d2_sofar, d2_pivot2, k); 
+    }
+}
+
+
+void BallTreeNearestNeighbors::FindBallKNN(
+    priority_queue< pair<real,int> >& q, const Vec& point, const int k ) const
+{
+    real d2_sofar;
+    pl_isnumber("+inf", &d2_sofar);
+    real d2_pivot = powdistance(point, ball_tree->pivot, 2);
+//    real d_minp = 0;
+    BallKNN(q, ball_tree, point, d2_sofar, d2_pivot, k);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc
___________________________________________________________________
Name: svn:executable
   + *

Added: branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,227 @@
+// -*- C++ -*-
+
+// BallTreeNearestNeighbors.h
+//
+// Copyright (C) 2004 Pascal Lamblin & Marius Muja
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+ * $Id: BallTreeNearestNeighbors.h 4900 2006-02-05 08:42:31Z lamblin $ 
+ ******************************************************* */
+
+// Authors: Pascal Lamblin & Marius Muja
+
+/*! \file BallTreeNearestNeighbors.h */
+
+
+#ifndef BallTreeNearestNeighbors_INC
+#define BallTreeNearestNeighbors_INC
+
+#include <queue>
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/nearest_neighbors/GenericNearestNeighbors.h>
+#include <plearn/vmat/SelectRowsVMatrix.h>
+#include <plearn/ker/DistanceKernel.h>
+
+#include "BinaryBallTree.h"
+
+namespace PLearn {
+using namespace std;
+
+class BallTreeNearestNeighbors;
+typedef PP< BallTreeNearestNeighbors > BallTreeNN;
+
+class BallTreeNearestNeighbors: public GenericNearestNeighbors
+{
+
+private:
+
+    typedef GenericNearestNeighbors inherited;
+
+protected:
+
+    // *********************
+    // * protected options *
+    // *********************
+
+    BinBallTree ball_tree;
+    int nb_train_points;
+    int nb_points;
+
+public:
+
+    // ************************
+    // * public build options *
+    // ************************
+
+    TVec<int> point_indices;
+    int rmin;
+    string train_method;
+    TVec<Mat> anchor_set;
+    TVec<int> pivot_indices;
+
+    // ****************
+    // * Constructors *
+    // ****************
+
+    //! Default constructor.
+    BallTreeNearestNeighbors();
+
+    //! Constructor from a TrainSet and a BinBallTree.
+    BallTreeNearestNeighbors( const VMat& tr_set, const BinBallTree& b_tree );
+
+    // ********************
+    // * PLearner methods *
+    // ********************
+
+private: 
+
+    //! This does the actual building. 
+    // (Please implement in .cc)
+    void build_();
+    void anchorTrain();
+
+protected: 
+  
+    //! Declares this class' options.
+    // (Please implement in .cc)
+    static void declareOptions(OptionList& ol);
+
+public:
+
+    // ************************
+    // **** Static methods ****
+    // ************************
+    // Maybe should I put this somewhere else...
+
+    // Returns true if the balls defined by (center1, radius1) and
+    // (center2, radius2) have a common part
+
+    static bool intersect( const Vec& center1, const real& radius1,
+                           const Vec& center2, const real& radius2 );
+
+    // Returns true if the first ball contains the second one
+    static bool contain( const Vec& center1, const real& radius1,
+                         const Vec& center2, const real& radius2 );
+
+    // Returns the smallest ball containing two balls
+    static void smallestContainer( const Vec& center1, const real& radius1,
+                                   const Vec& center2, const real& radius2,
+                                   Vec& t_center, real& t_radius);
+
+    virtual void BallKNN( priority_queue< pair<real,int> >& q,
+                          BinBallTree node, const Vec& t,
+                          real& d_sofar, real d_minp, const int k ) const;
+
+    virtual void FindBallKNN( priority_queue< pair<real,int> >& q,
+                              const Vec& point, int k ) const;
+
+
+    // ************************
+    // **** Object methods ****
+    // ************************
+
+    //! Simply calls inherited::build() then build_().
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy.
+    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(BallTreeNearestNeighbors);
+
+
+    // **************************
+    // **** PLearner methods ****
+    // **************************
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may
+    //! depend on the 'seed' option) And sets 'stage' back to 0 (this is
+    //! the stage of a fresh learner!).
+    virtual void forget();
+
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training
+    //! costs measured on-line in the process.
+    virtual void train();
+
+    void createAnchors( int nb_anchors );
+
+
+    BinBallTree leafFromAnchor( int anchor_index );
+
+    BinBallTree treeFromLeaves( const TVec<BinBallTree>& leaves );
+
+    BinBallTree getBallTree();
+
+
+    //! Computes the output and costs from the input (more effectively)
+    virtual void computeOutputAndCosts( const Vec& input, const Vec& target,
+                                        Vec& output, Vec& costs ) const;
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output. 
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output, 
+                                         const Vec& target, Vec& costs) const;
+
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus
+    //! (and thus the test method).
+    virtual TVec<string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method
+    //computes and ! for which it updates the VecStatsCollector
+    //train_stats.  (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<string> getTrainCostNames() const;
+
+};
+
+// Declares a few other classes and functions related to this class.
+DECLARE_OBJECT_PTR(BallTreeNearestNeighbors);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h
___________________________________________________________________
Name: svn:executable
   + *

Added: branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,153 @@
+// -*- C++ -*-
+
+// BinaryBallTree.cc
+//
+// Copyright (C) 2004 Pascal Lamblin 
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+ * $Id: BinaryBallTree.cc 3994 2005-08-25 13:35:03Z chapados $ 
+ ******************************************************* */
+
+// Authors: Pascal Lamblin
+
+/*! \file BinaryBallTree.cc */
+
+
+#include "BinaryBallTree.h"
+
+namespace PLearn {
+using namespace std;
+
+BinaryBallTree::BinaryBallTree() 
+    : pivot( Vec() ),
+      radius( 0 )
+{}
+
+PLEARN_IMPLEMENT_OBJECT( BinaryBallTree,
+                         "Binary Tree, containing a point, a radius, and a set of points", 
+                         "Each node of the tree contains the parameters of a ball :\n"
+                         "a point and a radius.\n"
+                         "Each leaf node contains a list of indices of points,\n"
+                         "each non-leaf node has two children nodes.");
+
+void BinaryBallTree::declareOptions( OptionList& ol )
+{
+    declareOption( ol, "pivot", &BinaryBallTree::pivot, OptionBase::buildoption,
+                   "Center of the ball" );
+
+    declareOption(ol, "radius", &BinaryBallTree::radius, OptionBase::buildoption,
+                  "Radius of the ball" );
+
+    declareOption(ol, "point_set", &BinaryBallTree::point_set, OptionBase::buildoption,
+                  "List of indices of the points owned by this node (leaf only)" );
+
+    declareOption(ol, "child1", &BinaryBallTree::child1, OptionBase::tuningoption,
+                  "Pointer to first child (non-leaf only)" );
+
+    declareOption(ol, "child2", &BinaryBallTree::child2, OptionBase::tuningoption,
+                  "Pointer to second child (non-leaf only)" );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void BinaryBallTree::build_()
+{
+    if( child1 )
+    { child1->parent = this; }
+
+    if( child2 )
+    { child2->parent = this; }
+}
+
+void BinaryBallTree::build()
+{
+    inherited::build();
+    build_();
+}
+
+void BinaryBallTree::setFirstChild( const BinBallTree& first_child )
+{
+    this->child1 = first_child;
+    if( first_child )
+    {
+        first_child->parent = this;
+    }
+}
+
+void BinaryBallTree::setSecondChild( const BinBallTree& second_child )
+{
+    this->child2 = second_child;
+    if( second_child )
+    {
+        second_child->parent = this;
+    }
+}
+
+BinBallTree BinaryBallTree::getFirstChild()
+{
+    return this->child1;
+}
+
+BinBallTree BinaryBallTree::getSecondChild()
+{
+    return this->child2;
+}
+
+BinaryBallTree* BinaryBallTree::getParent()
+{
+    return this->parent;
+}
+
+void BinaryBallTree::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField( child1, copies );
+    deepCopyField( child2, copies );
+    deepCopyField( pivot, copies );
+    deepCopyField( point_set, copies );
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc
___________________________________________________________________
Name: svn:executable
   + *

Added: branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,147 @@
+// -*- C++ -*-
+
+// BinaryBallTree.h
+//
+// Copyright (C) 2004 Pascal Lamblin 
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+ * $Id: BinaryBallTree.h 3994 2005-08-25 13:35:03Z chapados $ 
+ ******************************************************* */
+
+// Authors: Pascal Lamblin
+
+/*! \file BinaryBallTree.h */
+
+
+#ifndef BinaryBallTree_INC
+#define BinaryBallTree_INC
+
+#include <plearn/base/Object.h>
+
+namespace PLearn {
+using namespace std;
+
+class BinaryBallTree;
+typedef PP<BinaryBallTree> BinBallTree;
+
+class BinaryBallTree: public Object
+{
+
+private:
+  
+    typedef Object inherited;
+
+protected:
+    // *********************
+    // * protected options *
+    // *********************
+
+    BinaryBallTree* parent;
+    BinBallTree child1;
+    BinBallTree child2;
+
+public:
+
+    // ************************
+    // * public build options *
+    // ************************
+
+    Vec pivot;
+    real radius;
+    TVec<int> point_set;
+
+    // ****************
+    // * Constructors *
+    // ****************
+
+    //! Default constructor.
+    BinaryBallTree();
+
+
+    // ******************
+    // * Object methods *
+    // ******************
+
+private: 
+    //! This does the actual building. 
+    void build_();
+
+protected: 
+    //! Declares this class' options.
+    static void declareOptions(OptionList& ol);
+
+public:
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(BinaryBallTree);
+
+    // simply calls inherited::build() then build_() 
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
+
+    virtual void setFirstChild( const BinBallTree& first_child );
+
+    virtual void setSecondChild( const BinBallTree& second_child );
+
+    virtual BinBallTree getFirstChild();
+
+    virtual BinBallTree getSecondChild();
+
+    virtual BinaryBallTree* getParent();
+
+    bool isEmpty() const
+    {
+        bool result = !pivot && !child1 && !child2 ;
+        return result;
+    }
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(BinaryBallTree);
+  
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h
___________________________________________________________________
Name: svn:executable
   + *

Added: branches/cgi-desjardin/plearn_learners/second_iteration/CheckDond2FileSequence.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/CheckDond2FileSequence.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/CheckDond2FileSequence.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,155 @@
+// -*- C++ -*-
+
+// CheckDond2FileSequence.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file CheckDond2FileSequence.cc */
+
+#define PL_LOG_MODULE_NAME "CheckDond2FileSequence"
+#include <plearn/io/pl_log.h>
+
+#include "CheckDond2FileSequence.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    CheckDond2FileSequence,
+    "Checks that the train set is in sequence",
+    ""
+);
+
+/////////////////////////
+// CheckDond2FileSequence //
+/////////////////////////
+CheckDond2FileSequence::CheckDond2FileSequence()
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void CheckDond2FileSequence::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "key_col", &CheckDond2FileSequence::key_col,
+                  OptionBase::buildoption,
+                  "The column of the sequence key.");
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void CheckDond2FileSequence::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(key_col, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void CheckDond2FileSequence::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void CheckDond2FileSequence::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        int row;
+        real prev_key;
+        Vec input(train_set->width());
+        train_set->getRow(0, input);
+        prev_key = input[key_col];
+        for (row = 1; row < train_set->length(); row++)
+        {
+            train_set->getRow(row, input);
+            if (input[key_col] < prev_key)
+            {
+                cout << "CheckDond2FileSequence: train set out of sequence" << endl;
+                cout << "CheckDond2FileSequence: row: " << row << " previous key: " << prev_key << " current key: " << input[key_col] << endl;
+                PLERROR("CheckDond2FileSequence: we are done here");
+            }
+            if (input[key_col] == prev_key)
+            {
+                cout << "CheckDond2FileSequence: row: " << row << " previous key: " << prev_key << " current key: " << input[key_col] << endl;
+            }
+            prev_key = input[key_col];
+            if (row % 25000 == 0) cout << "CheckDond2FileSequence: " << row << " records processed." << endl;
+        }
+        cout << "CheckDond2FileSequence: " << row << " records processed." << endl;
+        PLERROR("CheckDond2FileSequence: we are done here");
+    }
+}
+
+int CheckDond2FileSequence::outputsize() const {return 0;}
+void CheckDond2FileSequence::train() {}
+void CheckDond2FileSequence::computeOutput(const Vec&, Vec&) const {}
+void CheckDond2FileSequence::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> CheckDond2FileSequence::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> CheckDond2FileSequence::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/CheckDond2FileSequence.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/CheckDond2FileSequence.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/CheckDond2FileSequence.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,128 @@
+// -*- C++ -*-
+
+// CheckDond2FileSequence.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file CheckDond2FileSequence.h */
+
+
+#ifndef CheckDond2FileSequence_INC
+#define CheckDond2FileSequence_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class CheckDond2FileSequence : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    
+    int key_col;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    CheckDond2FileSequence();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(CheckDond2FileSequence);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(CheckDond2FileSequence);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/ComputeDond2Target.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ComputeDond2Target.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ComputeDond2Target.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,247 @@
+// -*- C++ -*-
+
+// ComputeDond2Target.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file ComputeDond2Target.cc */
+
+#define PL_LOG_MODULE_NAME "ComputeDond2Target"
+#include <plearn/io/pl_log.h>
+
+#include "ComputeDond2Target.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ComputeDond2Target,
+    "Computes the class target for the training and the test datasets.",
+    "the program reorders the input variables to group them between the binary variables,\n"
+    "the discrete variables and the continuous variables.\n"
+    "It reorders the variables in the order specified by the input vector option.\n"
+    "It also computes the predicted class target from the predicted annual sales figure\n"
+    "of the financial institution and computes the real class target.\n"
+);
+
+/////////////////////////
+// ComputeDond2Target //
+/////////////////////////
+ComputeDond2Target::ComputeDond2Target()
+  : unknown_sales(0)
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void ComputeDond2Target::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "input_vector", &ComputeDond2Target::input_vector,
+                  OptionBase::buildoption,
+                  "The variables to assemble in the input vector by names.\n"
+                  "To ease the following steps they are grouped with the binary variables first,\n"
+                  "the discrete variables, the continuous variables and finally some variables unused in the training.\n");
+
+    declareOption(ol, "unknown_sales", &ComputeDond2Target::unknown_sales,
+                  OptionBase::buildoption,
+                  "If set to 1 and annual sales attribute is missing, the class will be set to missing.");
+
+    declareOption(ol, "target_sales", &ComputeDond2Target::target_sales,
+                  OptionBase::buildoption,
+                  "The column of the real annual sales used to compute the real class target.");
+
+    declareOption(ol, "predicted_sales", &ComputeDond2Target::predicted_sales,
+                  OptionBase::buildoption,
+                  "The column of the predicted annual sales used to compute the predicted class target.");
+
+    declareOption(ol, "margin", &ComputeDond2Target::margin,
+                  OptionBase::buildoption,
+                  "The column of the total authorized margins including SLA.");
+
+    declareOption(ol, "loan", &ComputeDond2Target::loan,
+                  OptionBase::buildoption,
+                  "The column of the total loan balances excluding mortgages.");
+
+    declareOption(ol, "output_path", &ComputeDond2Target::output_path,
+                  OptionBase::buildoption,
+                  "The file path for the targeted output file.");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void ComputeDond2Target::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(input_vector, copies);
+    deepCopyField(unknown_sales, copies);
+    deepCopyField(target_sales, copies);
+    deepCopyField(predicted_sales, copies);
+    deepCopyField(margin, copies);
+    deepCopyField(loan, copies);
+    deepCopyField(output_path, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void ComputeDond2Target::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void ComputeDond2Target::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        computeTarget();
+    }
+}
+
+void ComputeDond2Target::computeTarget()
+{    
+    // initialize primary dataset
+    main_row = 0;
+    main_col = 0;
+    main_length = train_set->length();
+    main_width = train_set->width();
+    main_input.resize(main_width);
+    main_names.resize(main_width);
+    ins_width = input_vector.size();
+    predicted_class = ins_width;
+    target_class = ins_width + 1;
+    output_width = ins_width + 2;
+    output_variable_src.resize(ins_width);
+    output_names.resize(output_width);
+    output_vec.resize(output_width);
+    main_names << train_set->fieldNames();
+    for (ins_col = 0; ins_col < ins_width; ins_col++)
+    {
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            if (input_vector[ins_col] == main_names[main_col]) break;
+        }
+        if (main_col >= main_width) PLERROR("In ComputeDond2Target: no field with this name in input dataset: %", (input_vector[ins_col]).c_str());
+        output_variable_src[ins_col] = main_col;
+        output_names[ins_col] = input_vector[ins_col];
+    }
+    output_names[predicted_class] = "CLASSE_PRED";
+    output_names[target_class] = "CLASSE_REEL";
+    
+    // initialize output datasets
+    output_file = new FileVMatrix(output_path + ".pmat", main_length, output_names);
+    output_file->defineSizes(output_width, 0, 0);
+    
+    //Now, we can group the input and compute the class target
+    ProgressBar* pb = 0;
+    pb = new ProgressBar( "Computing target classes", main_length);
+    for (main_row = 0; main_row < main_length; main_row++)
+    {
+        train_set->getRow(main_row, main_input);
+        for (ins_col = 0; ins_col < ins_width; ins_col++)
+        {
+            output_vec[ins_col] = main_input[output_variable_src[ins_col]];
+        }
+        if (is_missing(main_input[predicted_sales])) main_input[predicted_sales] = 0.0;
+        commitment = 0.0;
+        if (!is_missing(main_input[margin])) commitment += main_input[margin];
+        if (!is_missing(main_input[loan])) commitment += main_input[loan];
+        if (main_input[predicted_sales] < 1000000.0 && commitment < 200000.0) output_vec[predicted_class] = 1.0;
+        else if (main_input[predicted_sales] < 10000000.0 && commitment < 1000000.0) output_vec[predicted_class] = 2.0;
+        else if (main_input[predicted_sales] < 100000000.0 && commitment < 20000000.0) output_vec[predicted_class] = 3.0;
+        else output_vec[predicted_class] = 4.0;
+        if (is_missing(main_input[target_sales]) && unknown_sales == 0)
+            PLERROR("In ComputeDond2Target: no target information for record: %i", main_row);
+        commitment = 0.0;
+        if (!is_missing(main_input[margin])) commitment += main_input[margin];
+        if (!is_missing(main_input[loan])) commitment += main_input[loan];
+        if (is_missing(main_input[target_sales]))  output_vec[target_class] = main_input[target_sales];
+        else if (main_input[target_sales] < 1000000.0 && commitment < 200000.0) output_vec[target_class] = 1.0;
+        else if (main_input[target_sales] < 10000000.0 && commitment < 1000000.0) output_vec[target_class] = 2.0;
+        else if (main_input[target_sales] < 100000000.0 && commitment < 20000000.0) output_vec[target_class] = 3.0;
+        else output_vec[target_class] = 4.0;
+        output_file->putRow(main_row, output_vec);
+        pb->update( main_row );
+    }
+    delete pb;
+}
+
+VMat ComputeDond2Target::getOutputFile()
+{
+    return output_file;
+}
+
+int ComputeDond2Target::outputsize() const {return 0;}
+void ComputeDond2Target::train()
+{
+    PLERROR("ComputeDond2Target: we are done here");
+}
+void ComputeDond2Target::computeOutput(const Vec&, Vec&) const {}
+void ComputeDond2Target::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> ComputeDond2Target::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> ComputeDond2Target::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/ComputeDond2Target.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ComputeDond2Target.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ComputeDond2Target.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,175 @@
+// -*- C++ -*-
+
+// ComputeDond2Target.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file ComputeDond2Target.h */
+
+
+#ifndef ComputeDond2Target_INC
+#define ComputeDond2Target_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn/vmat/FileVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class ComputeDond2Target : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! The variables to assemble in the input vector by names.
+    //! To ease the following steps they are grouped with the binary variables first,
+    //! the discrete variables, the continuous variables and finally some variables unused in the training.
+    TVec<string> input_vector;
+    
+    //! If set to 1 and annual sales attribute is missing, the class will be set to missing.
+    int unknown_sales;
+    
+    //! The column of the real annual sales used to compute the real class target.
+    int target_sales;
+    
+    //! The column of the predicted annual sales used to compute the predicted class target.
+    int predicted_sales;
+    
+    //! The column of the total authorized margins including SLA.
+    int margin;
+    
+    //! The column of the total loan balances excluding mortgages.
+    int loan;
+    
+    //! The file path for the targeted output file.
+    string output_path;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    ComputeDond2Target();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+    VMat getOutputFile();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(ComputeDond2Target);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void computeTarget();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    
+    // input instructions variables
+    int ins_width;
+    int ins_col;
+    
+    // primary dataset variables
+    int main_length;
+    int main_width;
+    int main_row;
+    int main_col;
+    Vec main_input;
+    TVec<string> main_names;
+    
+    // output dataset variables
+    int target_class;
+    int predicted_class;
+    int output_width;
+    real commitment;
+    Vec output_vec;
+    TVec<string> output_names;
+    TVec<int> output_variable_src;
+    VMat output_file;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ComputeDond2Target);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,162 @@
+// -*- C++ -*-
+
+// ComputePurenneError.cc
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ********************************************************************************    
+ * $Id: ComputePurenneError.cc, v 1.0 2004/07/19 10:00:00 Bengio/Kegl/Godbout        *
+ * This file is part of the PLearn library.                                     *
+ ******************************************************************************** */
+
+#include "ComputePurenneError.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(ComputePurenneError,
+                        "A PLearner to compute the prediction error of Vincent.", 
+                        "\n"
+    );
+
+ComputePurenneError::ComputePurenneError()  
+{
+}
+
+ComputePurenneError::~ComputePurenneError()
+{
+}
+
+void ComputePurenneError::declareOptions(OptionList& ol)
+{
+    inherited::declareOptions(ol);
+}
+
+void ComputePurenneError::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void ComputePurenneError::build()
+{
+    inherited::build();
+    build_();
+}
+
+void ComputePurenneError::build_()
+{
+}
+
+void ComputePurenneError::train()
+{
+    int row;
+    Vec sample_input(train_set->inputsize());
+    Vec sample_target(train_set->targetsize());
+    real sample_weight;
+    Vec sample_output(2);
+    Vec sample_costs(3);
+    ProgressBar* pb = NULL;
+    if (report_progress)
+    {
+        pb = new ProgressBar("Purenne error: computing the train statistics: ", train_set->length());
+    } 
+    train_stats->forget();
+    for (row = 0; row < train_set->length(); row++)
+    {  
+        train_set->getExample(row, sample_input, sample_target, sample_weight);
+        computeOutput(sample_input, sample_output);
+        computeCostsFromOutputs(sample_input, sample_output, sample_target, sample_costs); 
+        train_stats->update(sample_costs);
+        if (report_progress) pb->update(row);
+    }
+    train_stats->finalize();
+    if (report_progress) delete pb; 
+}
+
+void ComputePurenneError::forget()
+{
+}
+
+int ComputePurenneError::outputsize() const
+{
+    return 2;
+}
+
+TVec<string> ComputePurenneError::getTrainCostNames() const
+{
+    TVec<string> return_msg(3);
+    return_msg[0] = "mse";
+    return_msg[1] = "cse";
+    return_msg[2] = "cle";
+    return return_msg;
+}
+
+TVec<string> ComputePurenneError::getTestCostNames() const
+{ 
+    return getTrainCostNames();
+}
+
+void ComputePurenneError::computeOutput(const Vec& inputv, Vec& outputv) const
+{
+    outputv[0] = inputv[0];
+    outputv[1] = inputv[1];
+}
+
+void ComputePurenneError::computeOutputAndCosts(const Vec& inputv, const Vec& targetv, Vec& outputv, Vec& costsv) const
+{
+    computeOutput(inputv, outputv);
+    computeCostsFromOutputs(inputv, outputv, targetv, costsv);
+}
+
+void ComputePurenneError::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
+{
+    costsv[0] = pow((outputv[0] - targetv[0]), 2.0);
+    costsv[1] = pow((outputv[1] - targetv[1]), 2.0);
+    if (outputv[1] == targetv[1]) costsv[2] = 0.0;
+    else costsv[2] = 1.0;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ComputePurenneError.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,96 @@
+// -*- C++ -*-
+
+// ComputePurenneError.h
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ********************************************************************************    
+ * $Id: ComputePurenneError.h, v 1.0 2004/07/19 10:00:00 Bengio/Kegl/Godbout   *
+ * This file is part of the PLearn library.                                     *
+ ******************************************************************************** */
+
+/*! \file PLearnLibrary/PLearnAlgo/ComputePurenneError.h */
+
+#ifndef ComputePurenneError_INC
+#define ComputePurenneError_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn/base/stringutils.h>
+
+namespace PLearn {
+using namespace std;
+
+class ComputePurenneError: public PLearner
+{
+    typedef PLearner inherited;   
+  
+public:
+    ComputePurenneError();
+    virtual              ~ComputePurenneError();
+    
+    PLEARN_DECLARE_OBJECT(ComputePurenneError);
+
+    static  void         declareOptions(OptionList& ol);
+    virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
+    virtual void         build();
+    virtual void         train();
+    virtual void         forget();
+    virtual int          outputsize() const;
+    virtual TVec<string> getTrainCostNames() const;
+    virtual TVec<string> getTestCostNames() const;
+    virtual void         computeOutput(const Vec& input, Vec& output) const;
+    virtual void         computeOutputAndCosts(const Vec& input, const Vec& target, Vec& output, Vec& costs) const;
+    virtual void         computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const;
+  
+private:
+    void         build_();
+};
+
+DECLARE_OBJECT_PTR(ComputePurenneError);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,229 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************************    
+   * $Id: ConditionalMeanImputationVMatrix.cc 3658 2005-07-06 20:30:15  Godbout $
+   ******************************************************************* */
+
+
+#include "ConditionalMeanImputationVMatrix.h"
+#include <plearn/io/fileutils.h>              //!<  For isfile()
+
+namespace PLearn {
+using namespace std;
+
+/** ConditionalMeanImputationVMatrix **/
+
+PLEARN_IMPLEMENT_OBJECT(
+  ConditionalMeanImputationVMatrix,
+  "VMat class to impute the conditional mean to replace missing values in the source matrix.",
+  "This class will replace missing values in the underlying dataset with the estimated values\n"
+  "from a preceding machine learning step where each variable with missing value have to have.\n"
+  "been considered as target in turns.\n"
+  "The predictions are expected in the metadata directory of the data set.\n"
+  );
+
+ConditionalMeanImputationVMatrix::ConditionalMeanImputationVMatrix()
+{
+}
+
+ConditionalMeanImputationVMatrix::~ConditionalMeanImputationVMatrix()
+{
+}
+
+void ConditionalMeanImputationVMatrix::declareOptions(OptionList &ol)
+{
+  declareOption(ol, "source", &ConditionalMeanImputationVMatrix::source, OptionBase::buildoption, 
+                "The source VMatrix with missing values.\n");
+  declareOption(ol, "condmean_dir", &ConditionalMeanImputationVMatrix::condmean_dir, OptionBase::buildoption, 
+                "The directory in the source metadatadir housing the variable conditional mean files.\n");
+  declareOption(ol, "condmean", &ConditionalMeanImputationVMatrix::condmean, OptionBase::learntoption, 
+                "The matrix of conditional means.\n");
+  declareOption(ol, "condmean_col_ref", &ConditionalMeanImputationVMatrix::condmean_col_ref, OptionBase::learntoption, 
+                "The cross reference between columns of source and condmean.\n");
+  inherited::declareOptions(ol);
+}
+
+void ConditionalMeanImputationVMatrix::build()
+{
+  inherited::build();
+  build_();
+}
+
+void ConditionalMeanImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+  deepCopyField(source, copies);
+  deepCopyField(condmean_dir, copies);
+  deepCopyField(condmean, copies);
+  deepCopyField(condmean_col_ref, copies);
+  inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void ConditionalMeanImputationVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
+{
+  source->getExample(i, input, target, weight);
+  for (int source_col = 0; source_col < input->length(); source_col++)
+    if (is_missing(input[source_col]) && condmean_col_ref[source_col] >= 0) input[source_col] = condmean(i, condmean_col_ref[source_col]);
+    else if (is_missing(input[source_col])) cout << "getExample : " << i << " " << source_col << endl;
+}
+
+real ConditionalMeanImputationVMatrix::get(int i, int j) const
+{ 
+  real variable_value = source->get(i, j);
+  if (!is_missing(variable_value) && condmean_col_ref[j] >= 0) return condmean(i, condmean_col_ref[j]);
+    else if (is_missing(variable_value)) cout << "get : " << i << " " << j << endl;
+  return variable_value;
+}
+
+void ConditionalMeanImputationVMatrix::put(int i, int j, real value)
+{
+  PLERROR("In ConditionalMeanImputationVMatrix::put not implemented");
+}
+
+void ConditionalMeanImputationVMatrix::getSubRow(int i, int j, Vec v) const
+{  
+  source->getSubRow(i, j, v);
+  for (int source_col = 0; source_col < v->length(); source_col++) 
+    if (is_missing(v[source_col])) v[source_col] = condmean(i, condmean_col_ref[source_col + j]);
+    else if (is_missing(v[source_col])) cout << "getSubRow : " << i << " " << source_col + j << endl;
+}
+
+void ConditionalMeanImputationVMatrix::putSubRow(int i, int j, Vec v)
+{
+  PLERROR("In ConditionalMeanImputationVMatrix::putSubRow not implemented");
+}
+
+void ConditionalMeanImputationVMatrix::appendRow(Vec v)
+{
+  PLERROR("In ConditionalMeanImputationVMatrix::appendRow not implemented");
+}
+
+void ConditionalMeanImputationVMatrix::insertRow(int i, Vec v)
+{
+  PLERROR("In ConditionalMeanImputationVMatrix::insertRow not implemented");
+}
+
+void ConditionalMeanImputationVMatrix::getRow(int i, Vec v) const
+{  
+  source-> getRow(i, v);
+  for (int source_col = 0; source_col < v->length(); source_col++)
+    if (is_missing(v[source_col]) && condmean_col_ref[source_col] >= 0) v[source_col] = condmean(i, condmean_col_ref[source_col]);
+    else if (is_missing(v[source_col])) cout << "getRow : " << i << " " << source_col << endl;
+}
+
+void ConditionalMeanImputationVMatrix::putRow(int i, Vec v)
+{
+  PLERROR("In ConditionalMeanImputationVMatrix::putRow not implemented");
+}
+
+void ConditionalMeanImputationVMatrix::getColumn(int i, Vec v) const
+{  
+  source-> getColumn(i, v);
+  for (int source_row = 0; source_row < v->length(); source_row++)
+    if (is_missing(v[source_row]) && condmean_col_ref[i] >= 0) v[source_row] = condmean(source_row, condmean_col_ref[i]);
+    else if (is_missing(v[source_row])) cout << "getColumn : " << source_row << " " << i << endl;
+}
+
+
+
+void ConditionalMeanImputationVMatrix::build_()
+{
+    if (!source) PLERROR("In ConditionalMeanImputationVMatrix::source vmat must be supplied");
+    loadCondMeanMatrix(); 
+}
+
+void ConditionalMeanImputationVMatrix::loadCondMeanMatrix()
+/*  
+Imputation step:
+  count the # of variables with missing values in the train and test datasets.
+  create a matrix in memory with this number of columns and keep cross reference of columns.
+  at the build stage, for each variable of train and test:
+    if # of missing = 0 there is nothing to do.
+    look for the (cond_mean_dir (/TreeCondMean/dir/) + field_name + /Split0/test1_outputs.pmat) file in the metadatadir;
+    add its column 0 as a column of the matrix.
+  then, if is_missing(source[i,j]) replace it with matrix[i, cross_reference[j]]
+*/
+{
+    // initialize source dataset
+    source_length = source->length();
+    source_width = source->width();
+    source_inputsize = source->inputsize();
+    source_targetsize = source->targetsize();
+    source_weightsize = source->weightsize();
+    source_names.resize(source_width);
+    source_names = source->fieldNames();
+    source_metadata = source->getMetaDataDir();
+    length_ = source_length;
+    width_ = source_width;
+    inputsize_ = source_inputsize;
+    targetsize_ = source_targetsize;
+    weightsize_ = source_weightsize;
+    declareFieldNames(source_names);
+    
+    // count the # of variables with missing values in the source datasets.
+    // create a matrix in memory with this number of columns and keep cross reference of columns.
+    int count_variable_with_missing = 0;
+    condmean_col_ref.resize(source_width);
+    condmean_col_ref.fill(-1);
+    for (source_col = 0; source_col < source_width; source_col++)
+    {
+        source_stats = source->getStats(source_col);
+        if (source_stats.nmissing() <= 0) continue;
+        condmean_col_ref[source_col] = count_variable_with_missing;
+        count_variable_with_missing += 1;
+    }
+    condmean.resize(source_length, count_variable_with_missing);
+    
+    // for each variable with missing value, 
+    // look for the (cond_mean_dir (/TreeCondMean/dir/) + field_name + /Split0/test1_outputs.pmat) file in the metadatadir;
+    // add its column 0 as a column of the condmean matrix.
+    for (source_col = 0; source_col < source_width; source_col++)
+    {
+        source_stats = source->getStats(source_col);
+        if (source_stats.nmissing() <= 0) continue;
+        condmean_col = condmean_col_ref[source_col];
+        condmean_variable_file_name = source_metadata + "/" + condmean_dir + "/dir/" + source_names[source_col] + "/Split0/test1_outputs.pmat";
+        if (!isfile(condmean_variable_file_name)) PLERROR("In ConditionalMeanImputationVMatrix::A conditional mean file was not found for variable %s", source_names[source_col].c_str());
+        condmean_variable_file = new FileVMatrix(condmean_variable_file_name, false);
+        if (condmean_variable_file->length() != source_length)
+            PLERROR("In ConditionalMeanImputationVMatrix::Source and conditional mean file length are not equal for variable %s", source_names[source_col].c_str());
+        for (source_row = 0; source_row < source_length; source_row++)
+            condmean(source_row, condmean_col) = condmean_variable_file->get(source_row, 0);
+    }
+}
+
+} // end of namespcae PLearn

Added: branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,118 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ******************************************************************      
+   * $Id: ConditionalMeanImputationVMatrix.h 3658 2005-07-06 20:30:15  Godbout $
+   ****************************************************************** */
+
+/*! \file PLearnLibrary/PLearnCore/VMat.h */
+
+#ifndef ConditionalMeanImputationVMatrix_INC
+#define ConditionalMeanImputationVMatrix_INC
+
+#include <plearn/vmat/SourceVMatrix.h>
+#include <plearn/vmat/FileVMatrix.h>
+
+namespace PLearn {
+using namespace std;
+
+class ConditionalMeanImputationVMatrix: public VMatrix
+{
+  typedef VMatrix inherited;
+  
+public:
+
+  //! The source VMatrix with missing values.
+  VMat                 source;
+
+  //! The directory in the source metadatadir housing the variable conditional mean files.
+  string               condmean_dir;
+
+  //! The matrix of conditional means.
+  Mat                  condmean;
+
+  //! The cross reference between columns of source and condmean.
+  TVec<int>            condmean_col_ref;
+
+  
+                        ConditionalMeanImputationVMatrix();
+  virtual               ~ConditionalMeanImputationVMatrix();
+
+  static void           declareOptions(OptionList &ol);
+
+  virtual void          build();
+  virtual void          makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+  virtual void         getExample(int i, Vec& input, Vec& target, real& weight);
+  virtual real         get(int i, int j) const;
+  virtual void         put(int i, int j, real value);
+  virtual void         getSubRow(int i, int j, Vec v) const;
+  virtual void         putSubRow(int i, int j, Vec v);
+  virtual void         appendRow(Vec v);
+  virtual void         insertRow(int i, Vec v);  
+  virtual void         getRow(int i, Vec v) const;
+  virtual void         putRow(int i, Vec v);
+  virtual void         getColumn(int i, Vec v) const;
+
+private:
+  
+  int                  source_length;
+  int                  source_width;
+  int                  source_inputsize;
+  int                  source_targetsize;
+  int                  source_weightsize;
+  int                  source_row;
+  int                  source_col;
+  PPath                source_metadata;
+  TVec<string>         source_names;
+  StatsCollector       source_stats;
+  PPath                condmean_variable_file_name;
+  VMat                 condmean_variable_file;
+  int                  condmean_col;
+        
+
+          void         build_();
+          void         loadCondMeanMatrix();  
+  
+  PLEARN_DECLARE_OBJECT(ConditionalMeanImputationVMatrix);
+
+};
+
+DECLARE_OBJECT_PTR(ConditionalMeanImputationVMatrix);
+
+} // end of namespcae PLearn
+#endif

Added: branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,305 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************************    
+   * $Id: CovariancePreservationImputationVMatrix.cc 3658 2005-07-06 20:30:15  Godbout $
+   ******************************************************************* */
+
+
+#include "CovariancePreservationImputationVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+/** CovariancePreservationImputationVMatrix **/
+
+PLEARN_IMPLEMENT_OBJECT(
+  CovariancePreservationImputationVMatrix,
+  "VMat class to impute values preserving the observed relationships between variables on a global basis.",
+  "This class will replace a missing value in the underlying dataset with a value computed to minimized\n"
+  "the distance of the sample covariates with the global covariance vector of the observed data.\n"
+  );
+
+CovariancePreservationImputationVMatrix::CovariancePreservationImputationVMatrix()
+{
+}
+
+CovariancePreservationImputationVMatrix::~CovariancePreservationImputationVMatrix()
+{
+}
+
+void CovariancePreservationImputationVMatrix::declareOptions(OptionList &ol)
+{
+  declareOption(ol, "source", &CovariancePreservationImputationVMatrix::source, OptionBase::buildoption, 
+                "The source VMatrix with missing values.\n");
+
+  declareOption(ol, "train_set", &CovariancePreservationImputationVMatrix::train_set, OptionBase::buildoption, 
+                "A referenced train set.\n"
+                "The covariance imputation is computed with the observed values in this data set.\n");
+
+  inherited::declareOptions(ol);
+}
+
+void CovariancePreservationImputationVMatrix::build()
+{
+  inherited::build();
+  build_();
+}
+
+void CovariancePreservationImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+  deepCopyField(source, copies);
+  deepCopyField(train_set, copies);
+  inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void CovariancePreservationImputationVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
+{
+  source->getExample(i, input, target, weight);
+  for (int source_col = 0; source_col < input->length(); source_col++)
+  {
+    if (is_missing(input[source_col])) input[source_col] = computeImputation(i, source_col, input);
+  }  
+}
+
+real CovariancePreservationImputationVMatrix::get(int i, int j) const
+{ 
+  real variable_value = source->get(i, j);
+  if (is_missing(variable_value)) computeImputation(i, j);
+  return variable_value;
+}
+
+void CovariancePreservationImputationVMatrix::put(int i, int j, real value)
+{
+  PLERROR("In CovariancePreservationImputationVMatrix::put not implemented");
+}
+
+void CovariancePreservationImputationVMatrix::getSubRow(int i, int j, Vec v) const
+{  
+  source->getSubRow(i, j, v);
+  for (int source_col = 0; source_col < v->length(); source_col++) 
+    if (is_missing(v[source_col])) v[source_col] = computeImputation(i, source_col + j);
+}
+
+void CovariancePreservationImputationVMatrix::putSubRow(int i, int j, Vec v)
+{
+  PLERROR("In CovariancePreservationImputationVMatrix::putSubRow not implemented");
+}
+
+void CovariancePreservationImputationVMatrix::appendRow(Vec v)
+{
+  PLERROR("In CovariancePreservationImputationVMatrix::appendRow not implemented");
+}
+
+void CovariancePreservationImputationVMatrix::insertRow(int i, Vec v)
+{
+  PLERROR("In CovariancePreservationImputationVMatrix::insertRow not implemented");
+}
+
+void CovariancePreservationImputationVMatrix::getRow(int i, Vec v) const
+{  
+  source-> getRow(i, v);
+  for (int source_col = 0; source_col < v->length(); source_col++)
+    if (is_missing(v[source_col])) v[source_col] = computeImputation(i, source_col, v);
+}
+
+void CovariancePreservationImputationVMatrix::putRow(int i, Vec v)
+{
+  PLERROR("In CovariancePreservationImputationVMatrix::putRow not implemented");
+}
+
+void CovariancePreservationImputationVMatrix::getColumn(int i, Vec v) const
+{  
+  source-> getColumn(i, v);
+  for (int source_row = 0; source_row < v->length(); source_row++)
+    if (is_missing(v[source_row])) v[source_row] = computeImputation(source_row, i);
+}
+
+
+
+void CovariancePreservationImputationVMatrix::build_()
+{
+    if (!train_set || !source) PLERROR("In CovariancePreservationImputationVMatrix::train set and source vmat must be supplied");
+    train_length = train_set->length();
+    if(train_length < 1) PLERROR("In CovariancePreservationImputationVMatrix::length of the number of train samples to use must be at least 1, got: %i", train_length);
+    train_width = train_set->width();
+    train_targetsize = train_set->targetsize();
+    train_weightsize = train_set->weightsize();
+    train_inputsize = train_set->inputsize();
+    if(train_inputsize < 1) PLERROR("In CovariancePreservationImputationVMatrix::inputsize of the train vmat must be supplied, got : %i", train_inputsize);
+    source_width = source->width();
+    source_targetsize = source->targetsize();
+    source_weightsize = source->weightsize();
+    source_inputsize = source->inputsize();
+    if (train_width != source_width) PLERROR("In CovariancePreservationImputationVMatrix::train set and source width must agree, got : %i, %i", train_width, source_width);
+    if (train_targetsize != source_targetsize) PLERROR("In CovariancePreservationImputationVMatrix::train set and source targetsize must agree, got : %i, %i", train_targetsize, source_targetsize);
+    if (train_weightsize != source_weightsize) PLERROR("In CovariancePreservationImputationVMatrix::train set and source weightsize must agree, got : %i, %i", train_weightsize, source_weightsize);
+    if (train_inputsize != source_inputsize) PLERROR("In CovariancePreservationImputationVMatrix::train set and source inputsize must agree, got : %i, %i", train_inputsize, source_inputsize);
+    train_field_names.resize(train_width);
+    train_field_names = train_set->fieldNames();
+    source_length = source->length();
+    length_ = source_length;
+    width_ = source_width;
+    inputsize_ = source_inputsize;
+    targetsize_ = source_targetsize;
+    weightsize_ = source_weightsize;
+    declareFieldNames(train_field_names);
+    train_metadata = train_set->getMetaDataDir();
+    covariance_file_name = train_metadata + "covariance_file.pmat";
+    cov.resize(train_width, train_width);
+    mu.resize(train_width);
+    if (!isfile(covariance_file_name))
+    {
+        computeCovariances();
+        createCovarianceFile();
+    }
+    else loadCovarianceFile();
+}
+
+void CovariancePreservationImputationVMatrix::createCovarianceFile()
+{
+    covariance_file = new FileVMatrix(covariance_file_name, train_width + 1, train_field_names);
+    for (indj = 0; indj < train_width; indj++)
+    {
+        for (indk = 0; indk < train_width; indk++)
+        {
+            covariance_file->put(indj, indk, cov(indj, indk));
+        }
+    }
+    for (indk = 0; indk < train_width; indk++)
+    {
+        covariance_file->put(train_width, indk, mu[indk]);
+    }
+}
+
+void CovariancePreservationImputationVMatrix::loadCovarianceFile()
+{
+    covariance_file = new FileVMatrix(covariance_file_name);
+    for (indj = 0; indj < train_width; indj++)
+    {
+        for (indk = 0; indk < train_width; indk++)
+        {
+            cov(indj, indk) = covariance_file->get(indj, indk);
+        }
+    }
+    for (indk = 0; indk < train_width; indk++)
+    {
+        mu[indk] = covariance_file->get(train_width, indk);
+    }
+}
+
+VMat CovariancePreservationImputationVMatrix::getCovarianceFile()
+{
+    return covariance_file;
+}
+
+void CovariancePreservationImputationVMatrix::computeCovariances()
+{
+/*
+    We need to populate the matrix of COV for all combinations of input variables
+    we need in one pass to populate 4 matrices of dxd:
+    n(j,k) the number of samples where x(i, j) and x(i, k) are simultaneously observed.
+    sum_x(j)(k) the sum of the x(i, j) values where x(i, j) and x(i, k) are simultaneously observed.
+    sum_x(j)_x(k) the sum of the x(i, j)*x(i, k) values where x(i, j) and x(i, k) are simultaneously observed.
+    we can the calculate mu(k) = sum_x(k, k)/n(k, k)
+    COV(j, k) = (sum_x(j)_x(k) - sum_x(j)(k) * mu(k) - sum_x(k)(j) * mu(j) + mu(k) * mu(j)) (1 / n(j,k))
+    All we need after is the COV matrix to impute values on missing values.
+    
+*/
+    n_obs.resize(train_width, train_width);
+    sum_xj.resize(train_width, train_width);
+    sum_xj_xk.resize(train_width, train_width);
+    train_input.resize(train_width);
+    n_obs.clear();
+    sum_xj.clear();
+    sum_xj_xk.clear();
+    mu.clear();
+    cov.clear();
+    ProgressBar* pb = 0;
+    pb = new ProgressBar("Computing the covariance matrix", train_length);
+    for (train_row = 0; train_row < train_length; train_row++)
+    {
+        train_set->getRow(train_row, train_input);
+        for (indj = 0; indj < train_width; indj++)
+        {
+            for (indk = 0; indk < train_width; indk++)
+            {
+                if (is_missing(train_input[indj]) || is_missing(train_input[indk])) continue;
+                n_obs(indj, indk) += 1.0;
+                sum_xj(indj, indk) += train_input[indj];
+                sum_xj_xk(indj, indk) += train_input[indj] * train_input[indk];
+            }
+        }
+        pb->update( train_row ); 
+    }
+    delete pb;
+    for (indj = 0; indj < train_width; indj++)
+    {
+        mu[indj] = sum_xj(indj, indj) / n_obs(indj, indj); 
+    }
+    for (indj = 0; indj < train_width; indj++)
+    {
+        for (indk = 0; indk < train_width; indk++)
+        {
+            cov(indj, indk) = sum_xj_xk(indj, indk) - sum_xj(indj, indk) * mu[indk] - sum_xj(indk, indj) * mu[indj];
+            cov(indj, indk) = (cov(indj, indk) /  n_obs(indj, indk)) + mu[indk] * mu[indj];
+        }
+    }
+}
+
+real CovariancePreservationImputationVMatrix::computeImputation(int row, int col) const
+{
+    Vec input(source_width);
+    source->getRow(row, input);
+    return computeImputation(row, col, input);
+}
+
+real CovariancePreservationImputationVMatrix::computeImputation(int row, int col, Vec input) const
+{
+    real sum_cov_xl = 0;
+    real sum_xl_square = 0;
+    for (int indl = 0; indl < source_width; indl++)
+    {
+        if (is_missing(input[indl])) continue;
+        sum_cov_xl += cov(indl, col) * (input[indl] - mu[indl]);
+        sum_xl_square += (input[indl] - mu[indl]) * (input[indl] - mu[indl]);
+    }
+    if (sum_xl_square == 0.0) return mu[col];
+    return mu[col] + sum_cov_xl / sum_xl_square;
+}
+
+} // end of namespcae PLearn

Added: branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,129 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ******************************************************************      
+   * $Id: CovariancePreservationImputationVMatrix.h 3658 2005-07-06 20:30:15  Godbout $
+   ****************************************************************** */
+
+/*! \file PLearnLibrary/PLearnCore/VMat.h */
+
+#ifndef CovariancePreservationImputationVMatrix_INC
+#define CovariancePreservationImputationVMatrix_INC
+
+#include <plearn/vmat/SourceVMatrix.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/io/fileutils.h>                     //!<  For isfile()
+#include <plearn/math/BottomNI.h>
+
+namespace PLearn {
+using namespace std;
+
+class CovariancePreservationImputationVMatrix: public VMatrix
+{
+  typedef VMatrix inherited;
+  
+public:
+
+  //! The source VMatrix with missing values.
+  VMat                  source;
+  
+  //! A referenced train set.
+  //! The covariance imputation is computed with the observed values in this data set.
+  VMat                  train_set;
+  
+
+                        CovariancePreservationImputationVMatrix();
+  virtual               ~CovariancePreservationImputationVMatrix();
+
+  static void           declareOptions(OptionList &ol);
+
+  virtual void          build();
+  virtual void          makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+  virtual void         getExample(int i, Vec& input, Vec& target, real& weight);
+  virtual real         get(int i, int j) const;
+  virtual void         put(int i, int j, real value);
+  virtual void         getSubRow(int i, int j, Vec v) const;
+  virtual void         putSubRow(int i, int j, Vec v);
+  virtual void         appendRow(Vec v);
+  virtual void         insertRow(int i, Vec v);  
+  virtual void         getRow(int i, Vec v) const;
+  virtual void         putRow(int i, Vec v);
+  virtual void         getColumn(int i, Vec v) const;
+          VMat         getCovarianceFile();
+
+private:
+  
+  int                  train_length;
+  int                  train_width;
+  int                  train_inputsize;
+  int                  train_targetsize;
+  int                  train_weightsize;
+  int                  train_row;
+  Vec                  train_input;
+  TVec<string>         train_field_names;
+  PPath                train_metadata;
+  int                  source_length;
+  int                  source_width;
+  int                  source_inputsize;
+  int                  source_targetsize;
+  int                  source_weightsize;
+  PPath                covariance_file_name;
+  VMat                 covariance_file;
+  int                  indj;
+  int                  indk;
+  Mat                  n_obs;
+  Mat                  sum_xj;
+  Mat                  sum_xj_xk;
+  Vec                  mu;
+  Mat                  cov;
+
+          void         build_();
+          void         createCovarianceFile(); 
+          void         loadCovarianceFile(); 
+          void         computeCovariances();  
+          real         computeImputation(int row, int col) const;
+          real         computeImputation(int row, int col, Vec input) const;
+  
+  PLEARN_DECLARE_OBJECT(CovariancePreservationImputationVMatrix);
+
+};
+
+DECLARE_OBJECT_PTR(CovariancePreservationImputationVMatrix);
+
+} // end of namespcae PLearn
+#endif

Added: branches/cgi-desjardin/plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,243 @@
+// -*- C++ -*-
+
+// DichotomizeDond2DiscreteVariables.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file DichotomizeDond2DiscreteVariables.cc */
+
+#define PL_LOG_MODULE_NAME "DichotomizeDond2DiscreteVariables"
+#include <plearn/io/pl_log.h>
+
+#include "DichotomizeDond2DiscreteVariables.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    DichotomizeDond2DiscreteVariables,
+    "Dichotomize variables with discrete values.",
+    "Instructions are provided with the discrete_variable_instructions option.\n"
+);
+
+/////////////////////////
+// DichotomizeDond2DiscreteVariables //
+/////////////////////////
+DichotomizeDond2DiscreteVariables::DichotomizeDond2DiscreteVariables()
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void DichotomizeDond2DiscreteVariables::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "discrete_variable_instructions", &DichotomizeDond2DiscreteVariables::discrete_variable_instructions,
+                  OptionBase::buildoption,
+                  "The instructions to dichotomize the variables in the form of field_name : TVec<pair>.\n"
+                  "The pairs are values from : to, each creating a 0, 1 variable.\n"
+                  "Variables with no specification will be kept as_is.\n");
+
+    declareOption(ol, "output_path", &DichotomizeDond2DiscreteVariables::output_path,
+                  OptionBase::buildoption,
+                  "The file path for the fixed output file.");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void DichotomizeDond2DiscreteVariables::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(discrete_variable_instructions, copies);
+    deepCopyField(output_path, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void DichotomizeDond2DiscreteVariables::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void DichotomizeDond2DiscreteVariables::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        dichotomizeDiscreteVariables();
+    }
+}
+
+void DichotomizeDond2DiscreteVariables::dichotomizeDiscreteVariables()
+{    
+    // initialize primary dataset
+    main_row = 0;
+    main_col = 0;
+    main_length = train_set->length();
+    main_width = train_set->width();
+    main_input.resize(main_width);
+    main_names.resize(main_width);
+    main_ins.resize(main_width);
+    ins_width = discrete_variable_instructions.size();
+    main_names << train_set->fieldNames();
+    main_ins.fill(-1);
+    for (ins_col = 0; ins_col < ins_width; ins_col++)
+    {
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            if (discrete_variable_instructions[ins_col].first == main_names[main_col]) break;
+        }
+        if (main_col >= main_width) PLERROR("In DichotomizeDond2DiscreteVariables: no field with this name in data set: %s", (discrete_variable_instructions[ins_col].first).c_str());
+        else main_ins[main_col] = ins_col;
+    }
+    
+    // initialize output datasets
+    output_length = main_length;
+    output_width = 0;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (main_ins[main_col] < 0) output_width += 1;
+        else
+        {
+            instruction_ptr = discrete_variable_instructions[main_ins[main_col]].second;
+            output_width += instruction_ptr.size();
+        }
+    }
+    output_record.resize(output_width);
+    output_names.resize(output_width);
+    output_col = 0;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (main_ins[main_col] < 0)
+        {
+            output_names[output_col] = main_names[main_col];
+            output_col += 1;
+        }
+        else
+        {
+           instruction_ptr = discrete_variable_instructions[main_ins[main_col]].second;
+           if (instruction_ptr.size() == 0) continue;
+           for (ins_col = 0; ins_col < instruction_ptr.size(); ins_col++)
+           {
+               output_names[output_col] = main_names[main_col] + "_"
+                                        + tostring(instruction_ptr[ins_col].first) + "_" 
+                                        + tostring(instruction_ptr[ins_col].second);
+               output_col += 1;
+           }
+        }    
+    }
+    output_file = new FileVMatrix(output_path + ".pmat", output_length, output_names);
+    output_file->defineSizes(output_width, 0, 0);
+    
+    //Now, we can process the discrete variables.
+    ProgressBar* pb = 0;
+    pb = new ProgressBar( "Dichotomizing the discrete variables", main_length);
+    for (main_row = 0; main_row < main_length; main_row++)
+    {
+        train_set->getRow(main_row, main_input);
+        output_col = 0;
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            if (main_ins[main_col] < 0)
+            {
+                output_record[output_col] = main_input[main_col];
+                output_col += 1;
+            }
+            else
+            {
+               instruction_ptr = discrete_variable_instructions[main_ins[main_col]].second;
+               if (instruction_ptr.size() == 0) continue;
+               for (ins_col = 0; ins_col < instruction_ptr.size(); ins_col++)
+               {
+                   if (is_missing(main_input[main_col])) output_record[output_col] = MISSING_VALUE;
+                   else if (main_input[main_col] < instruction_ptr[ins_col].first || main_input[main_col] > instruction_ptr[ins_col].second) output_record[output_col] = 0.0;
+                   else output_record[output_col] = 1.0;
+                   output_col += 1;
+               }
+            }    
+        }
+        output_file->putRow(main_row, output_record);
+        pb->update( main_row );
+    }
+    delete pb;
+}
+
+VMat DichotomizeDond2DiscreteVariables::getOutputFile()
+{
+    return output_file;
+}
+
+int DichotomizeDond2DiscreteVariables::outputsize() const {return 0;}
+void DichotomizeDond2DiscreteVariables::train()
+{
+        PLERROR("DichotomizeDond2DiscreteVariables: we are done here");
+}
+void DichotomizeDond2DiscreteVariables::computeOutput(const Vec&, Vec&) const {}
+void DichotomizeDond2DiscreteVariables::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> DichotomizeDond2DiscreteVariables::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> DichotomizeDond2DiscreteVariables::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,160 @@
+// -*- C++ -*-
+
+// DichotomizeDond2DiscreteVariables.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file DichotomizeDond2DiscreteVariables.h */
+
+
+#ifndef DichotomizeDond2DiscreteVariables_INC
+#define DichotomizeDond2DiscreteVariables_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn/vmat/FileVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class DichotomizeDond2DiscreteVariables : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! The instructions to fix the binary variables in the form of field_name : instruction.
+    //! Supported instructions are 9_is_one, not_0_is_one, not_missing_is_one, not_1000_is_one.
+    //! Variables with no specification will be kept as_is.
+    TVec< pair<string, TVec< pair<real, real> > > > discrete_variable_instructions;
+    
+    //! The file path for the fixed output file.
+    string output_path;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    DichotomizeDond2DiscreteVariables();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+    VMat getOutputFile();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(DichotomizeDond2DiscreteVariables);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void dichotomizeDiscreteVariables();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    
+    // input instructions variables
+    int ins_width;
+    int ins_col;
+    TVec<pair<real, real> > instruction_ptr;
+    
+    // primary dataset variables
+    int main_length;
+    int main_width;
+    int main_row;
+    int main_col;
+    Vec main_input;
+    TVec<string> main_names;
+    TVec<int> main_ins;
+    
+    // output dataset variables
+    int output_length;
+    int output_width;
+    int output_col;
+    Vec output_record;
+    TVec<string> output_names;
+    VMat output_file;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(DichotomizeDond2DiscreteVariables);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,519 @@
+// -*- C++ -*-
+
+// Experimentation.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file Experimentation.cc */
+
+#define PL_LOG_MODULE_NAME "Experimentation"
+#include <plearn/io/pl_log.h>
+
+#include "Experimentation.h"
+#include <plearn/io/load_and_save.h>                 //!<  For save
+#include <plearn/io/fileutils.h>                     //!<  For isfile()
+#include <plearn/math/random.h>                      //!<  For the seed stuff.
+#include <plearn/vmat/ExplicitSplitter.h>            //!<  For the splitter stuff.
+#include <plearn/vmat/VariableDeletionVMatrix.h>     //!<  For the new_set stuff.
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    Experimentation,
+    "Computes correlation coefficient between various discrete values and the target.",
+    "name of the discrete variable, of the target and the values to check are options.\n"
+);
+
+/////////////////////////
+// Experimentation //
+/////////////////////////
+Experimentation::Experimentation()
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void Experimentation::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "save_files", &Experimentation::save_files,
+                  OptionBase::buildoption,
+                  "If set to 1, save the built train and test files instead of running the experiment.");
+    declareOption(ol, "experiment_without_missing_indicator", &Experimentation::experiment_without_missing_indicator,
+                  OptionBase::buildoption,
+                  "If set to 1, the missing_indicator_field_names will be excluded from the built training files.");
+    declareOption(ol, "target_field_name", &Experimentation::target_field_name,
+                  OptionBase::buildoption,
+                  "The name of the field to select from the target_set as target for the built training files.");
+    declareOption(ol, "missing_indicator_field_names", &Experimentation::missing_indicator_field_names,
+                  OptionBase::buildoption,
+                  "The field names of the missing indicators to exclude when we experiment without them.");
+    declareOption(ol, "experiment_name", &Experimentation::experiment_name,
+                  OptionBase::buildoption,
+                  "The name of the group of experiments to conduct.");
+    declareOption(ol, "number_of_test_samples", &Experimentation::number_of_test_samples,
+                  OptionBase::buildoption,
+                  "The number of test samples at the beginning of the train set.");
+    declareOption(ol, "number_of_train_samples", &Experimentation::number_of_train_samples,
+                  OptionBase::buildoption,
+                  "The number of train samples in the reference set to compute the % of missing.");
+    declareOption(ol, "reference_train_set", &Experimentation::reference_train_set,
+                  OptionBase::buildoption,
+                  "The train and valid set with missing values to compute the % of missing.");
+    declareOption(ol, "target_set", &Experimentation::target_set,
+                  OptionBase::buildoption,
+                  "The data set with the targets corresponding to the train set.");
+    declareOption(ol, "deletion_thresholds", &Experimentation::deletion_thresholds,
+                  OptionBase::buildoption,
+                  "The vector of the various deletion threshold to run this experiment with.");
+    declareOption(ol, "experiment_directory", &Experimentation::experiment_directory,
+                  OptionBase::buildoption,
+                  "The path in which to build the directories for the experiment's results.");
+    declareOption(ol, "experiment_template", &Experimentation::experiment_template,
+                  OptionBase::buildoption,
+                  "The template of the script to conduct the experiment.");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void Experimentation::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(experiment_name, copies);
+    deepCopyField(number_of_test_samples, copies);
+    deepCopyField(number_of_train_samples, copies);
+    deepCopyField(reference_train_set, copies);
+    deepCopyField(target_set, copies);
+    deepCopyField(deletion_thresholds, copies);
+    deepCopyField(experiment_directory, copies);
+    deepCopyField(experiment_template, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void Experimentation::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void Experimentation::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        for (int iteration = 1; iteration <= 50; iteration++)
+        {
+            cout << "In Experimentation, Iteration # " << iteration << endl;
+            experimentSetUp();
+            train();
+            ::PLearn::save(header_expdir + "/" + deletion_threshold_str + "/source_names.psave", source_names);
+        }
+        PLERROR("In Experimentation: we are done here");
+    }
+}
+
+void Experimentation::experimentSetUp()
+{ 
+    // initialize primary dataset
+    main_row = 0;
+    main_col = 0;
+    main_length = train_set->length();
+    main_width = train_set->width();
+    main_names.resize(main_width);
+    main_names << train_set->fieldNames();
+    if (train_set->hasMetaDataDir()) main_metadata = train_set->getMetaDataDir();
+    else if (experiment_directory == "") PLERROR("In Experimentation: we need one of experiment_directory or train_set->metadatadir");
+         else main_metadata = experiment_directory;
+    if (experiment_without_missing_indicator > 0)
+    {
+        fields_width = missing_indicator_field_names.size();
+        main_fields_selected.resize(main_width - fields_width);
+        for (fields_col = 0; fields_col < fields_width; fields_col++)
+        {
+            for (main_col = 0; main_col < main_width; main_col++)
+            {
+                if (missing_indicator_field_names[fields_col] == main_names[main_col]) break;
+            }
+            if (main_col >= main_width) PLERROR("In Experimentation: no field with this name in input dataset: %", (missing_indicator_field_names[fields_col]).c_str());
+        }
+        main_fields_selected_col = 0;
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            for (fields_col = 0; fields_col < fields_width; fields_col++)
+            {
+                if (missing_indicator_field_names[fields_col] == main_names[main_col]) break;
+            }
+            if (fields_col < fields_width) continue;
+            main_fields_selected[main_fields_selected_col] = main_names[main_col];
+            main_fields_selected_col += 1;
+        }
+    }
+    
+    // initialize target dataset
+    target_row = 0;
+    target_col = 0;
+    target_length = target_set->length();
+    target_width = target_set->width();
+    target_names.resize(target_width);
+    target_names << target_set->fieldNames();
+    if (target_length != main_length) PLERROR("In Experimentation: target and main train datasets should have equal length");
+    for (target_col = 0; target_col < target_width; target_col++)
+    {
+        if (target_field_name == target_names[target_col]) break;
+    }
+    if (target_col >= target_width) PLERROR("In Experimentation: no field with this name in target dataset: %", target_field_name.c_str());
+    
+    // initialize the header file
+    cout << "initialize the header file" << endl;
+    reference_train_set->lockMetaDataDir();
+    if (experiment_directory == "") header_expdir = main_metadata + "/Experiment/" + experiment_name;
+    else header_expdir = experiment_directory;
+    header_expdir += "/" + target_field_name;
+    if (experiment_without_missing_indicator > 0) header_expdir += "/no_ind/";
+    else header_expdir += "/ind/";
+    header_file_name = header_expdir + "header.pmat";
+    if (deletion_thresholds.length() <= 0)
+    {
+        deletion_thresholds.resize(20);
+        for (header_col = 0; header_col < 20; header_col++) deletion_thresholds[header_col] = (real) to_deal_with_next / 20.0;
+    } 
+    header_width = deletion_thresholds.length();
+    header_record.resize(header_width);
+    if (!isfile(header_file_name)) createHeaderFile();
+    else getHeaderRecord();
+    
+    // choose deletion threshold to experiment with
+    cout << "choose deletion threshold to experiment with" << endl;
+    to_deal_with_total = 0;
+    to_deal_with_next = -1;
+    for (header_col = 0; header_col < header_width; header_col++)
+    {
+        if (header_record[header_col] != 0.0) continue;
+        to_deal_with_total += 1;
+        if (to_deal_with_next < 0) to_deal_with_next = header_col;
+    }
+    if (to_deal_with_next < 0)
+    {
+        reference_train_set->unlockMetaDataDir();
+        reviewGlobalStats();
+        PLERROR("In Experimentation: we are done here");
+    }
+    deletion_threshold = deletion_thresholds[to_deal_with_next];
+    deletion_threshold_str = tostring(deletion_threshold + 0.005).substr(0,4);
+    cout << "total number of thresholds left to deal with: " << to_deal_with_total << endl;
+    cout << "next thresholds to deal with: " << deletion_threshold << endl;
+    updateHeaderRecord(to_deal_with_next);
+    reference_train_set->unlockMetaDataDir();
+    
+    // build the train and test sets
+    setSourceDataset();
+    cout << "source data set width: " << source_set->width() << endl;
+    main_input.resize(source_set->width());
+    source_names.resize(source_set->width());
+    source_names << source_set->fieldNames();
+    source_names.resize(source_set->width() + 1);
+    source_names[source_set->width()] = target_field_name;
+    
+    // load test data set
+    ProgressBar* pb = 0;
+    test_length = number_of_test_samples;
+    test_width = source_set->width() + 1;
+    test_file = new MemoryVMatrix(test_length, test_width);
+    test_file->defineSizes(test_width - 1, 1, 0);
+    test_record.resize(test_width);
+    pb = new ProgressBar( "loading the test file for threshold: " + deletion_threshold_str, test_length);
+    for (main_row = 0; main_row < test_length; main_row++)
+    {
+        source_set->getRow(main_row, main_input);
+        for (main_col = 0; main_col < source_set->width(); main_col++) test_record[main_col] = main_input[main_col];
+        test_record[source_set->width()] = target_set->get(main_row, target_col);
+        test_file->putRow(main_row, test_record);
+        pb->update( main_row );
+    }
+    delete pb;
+    
+    // load training and validation data set
+    train_valid_length = main_length - test_length;
+    train_valid_width = source_set->width() + 1;
+    train_valid_file = new MemoryVMatrix(train_valid_length, train_valid_width);
+    train_valid_file->defineSizes(train_valid_width - 1, 1, 0);
+    train_valid_record.resize(train_valid_width);
+    pb = new ProgressBar( "loading the training and validation file for threshold: " + deletion_threshold_str, train_valid_length);
+    for (main_row = test_length; main_row < main_length; main_row++)
+    {
+        source_set->getRow(main_row, main_input);
+        for (main_col = 0; main_col < source_set->width(); main_col++) train_valid_record[main_col] = main_input[main_col];
+        train_valid_record[source_set->width()] = target_set->get(main_row, target_col);
+        train_valid_file->putRow(main_row - test_length, train_valid_record);
+        pb->update( main_row - test_length );
+    }
+    delete pb;
+    
+    // save files if requested
+    if (save_files <= 0) return;
+    VMat save_test = new FileVMatrix(header_expdir + "/" + deletion_threshold_str + "/test.pmat", test_length, test_width);
+    save_test->declareFieldNames(source_names);
+    pb = new ProgressBar( "saving the test file for threshold: " + deletion_threshold_str, test_length);
+    for (main_row = 0; main_row < test_length; main_row++)
+    {
+        test_file->getRow(main_row, test_record);
+        save_test->putRow(main_row, test_record);
+        pb->update( main_row );
+    }
+    delete pb;
+    VMat save_train_valid = new FileVMatrix(header_expdir + "/" + deletion_threshold_str + "/train_valid.pmat", train_valid_length, train_valid_width);
+    save_train_valid->declareFieldNames(source_names);
+    pb = new ProgressBar( "saving the training and validation file for threshold: " + deletion_threshold_str, train_valid_length);
+    for (main_row = 0; main_row < train_valid_length; main_row++)
+    {
+        train_valid_file->getRow(main_row, train_valid_record);
+        save_train_valid->putRow(main_row, train_valid_record);
+        pb->update( main_row );
+    }
+    delete pb;
+    PLERROR("In Experimentation: we are done here");
+}
+
+void Experimentation::createHeaderFile()
+{ 
+    header_record.clear();
+    header_names.resize(header_width);
+    for (header_col = 0; header_col < header_width; header_col++) 
+        header_names[header_col] = tostring(deletion_thresholds[header_col] + 0.005).substr(0,4);
+    header_file = new FileVMatrix(header_file_name, 1, header_names);
+    header_file->putRow(0, header_record);
+}
+
+void Experimentation::getHeaderRecord()
+{ 
+    header_file = new FileVMatrix(header_file_name, true);
+    if (header_width != header_file->width()) PLERROR("In Experimentation: the existing header file does not match the deletion_thresholds width)");
+    header_names = header_file->fieldNames();
+    for (header_col = 0; header_col < header_width; header_col++) 
+        if (header_names[header_col] != tostring(deletion_thresholds[header_col] + 0.005).substr(0,4))
+            PLERROR("In Experimentation: the existing header file names does not match the deletion_thresholds values)");;
+    header_file->getRow(0, header_record);
+}
+
+void Experimentation::updateHeaderRecord(int var_col)
+{ 
+    header_file->put(0, var_col, 1.0);
+    header_file->flush();
+}
+
+void Experimentation::setSourceDataset()
+{
+    VMat selected_train_set = train_set;
+    VMat selected_reference_set= reference_train_set;
+    if (experiment_without_missing_indicator > 0)
+    {
+            SelectColumnsVMatrix* new_train_set = new SelectColumnsVMatrix();
+            new_train_set->source = train_set;
+            new_train_set->fields = main_fields_selected;
+            selected_train_set = new_train_set;
+            selected_train_set->build();
+            selected_train_set->defineSizes(selected_train_set->width(), 0, 0);
+            SelectColumnsVMatrix* new_reference_set = new SelectColumnsVMatrix();
+            new_reference_set->source = reference_train_set;
+            new_reference_set->fields = main_fields_selected;
+            selected_reference_set = new_reference_set;
+            selected_reference_set->build();
+            selected_reference_set->defineSizes(selected_reference_set->width(), 0, 0);
+    }
+    if (deletion_threshold <= 0.0)
+    {
+        source_set = selected_train_set;
+        return;
+    }
+    VariableDeletionVMatrix* new_set = new VariableDeletionVMatrix();
+    // VMat: The data set with all variables to select the columns from.
+    new_set->complete_dataset = selected_train_set;
+    // VMat: The train set in which to compute the percentage of missing values.
+    new_set->train_set = selected_reference_set;
+    // double: The percentage of non-missing values for a variable above which, the variable will be selected.
+    new_set->deletion_threshold = deletion_threshold;
+    // bool: If set to 1, the columns with constant non-missing values will be removed.
+    new_set->remove_columns_with_constant_value = 1;
+    // double: If equal to zero, all the train samples are used to calculated the percentages and constant values.
+    // If it is a fraction between 0 and 1, this proportion of the samples will be used.
+    // If greater or equal to 1, the integer portion will be interpreted as the number of samples to use.
+    new_set->number_of_train_samples = number_of_train_samples;
+    // int: The row at which, to start to calculate the percentages and constant values.
+    new_set->start_row = 0;
+    source_set = new_set;
+    source_set->build();
+}
+
+void Experimentation::reviewGlobalStats()
+{
+    cout << "There is no more variable to deal with." << endl;
+    bool missing_results_file = false;
+    for (header_col = 0; header_col < header_width; header_col++)
+    {
+        deletion_threshold = deletion_thresholds[header_col];
+        deletion_threshold_str = tostring(deletion_threshold + 0.005).substr(0,4);
+        PPath expdir = header_expdir + "/" + deletion_threshold_str;
+        PPath train_valid_results_file_name = expdir + "/Split0/LearnerExpdir/Strat0results.pmat";
+        PPath test_results_file_name = expdir + "/global_stats.pmat";
+        PPath source_names_file_name = expdir + "/source_names.psave";
+        if (!isfile(train_valid_results_file_name))
+        {
+            cout << "Missing training and validation results for threshold " << deletion_threshold_str << endl;
+            missing_results_file = true;
+        }
+        if (!isfile(test_results_file_name))
+        {
+            cout << "Missing test results for threshold " << deletion_threshold_str << endl;
+            missing_results_file = true;
+        }
+        if (!isfile(source_names_file_name))
+        {
+            cout << "Missing variable selected saved file for threshold " << deletion_threshold_str << endl;
+            missing_results_file = true;
+        }
+    }
+    if (missing_results_file) return;
+    cout << endl << endl;
+    cout << "Results for experiment " << experiment_name << endl;
+    cout << "       The file used for this experiment was " << main_metadata << endl;
+    cout << "       The target used was " << target_field_name << endl;
+    if (experiment_without_missing_indicator > 0) cout << "       The experiment was carried without missing indicators" << endl;
+    else cout << "       The experiment was carried with missing indicators" << endl;
+    cout << endl << endl;
+    cout << "           number                                                                  " << endl;
+    cout << "             of                                                                    " << endl;
+    cout << " deletion variable   weigth    train    valid     test     test       std      test" << endl;
+    cout << "threshold selected    decay     mse      mse      mse      cse       error     cle " << endl;
+    cout << endl;
+    cout << fixed << showpoint;
+    real best_valid_mse_threshold = -1.0;
+    real best_valid_mse_value;
+    for (header_col = 0; header_col < header_width; header_col++)
+    {
+        deletion_threshold = deletion_thresholds[header_col];
+        deletion_threshold_str = tostring(deletion_threshold + 0.005).substr(0,4);
+        PPath expdir = header_expdir + "/" + deletion_threshold_str;
+        PPath train_valid_results_file_name = expdir + "/Split0/LearnerExpdir/Strat0results.pmat";
+        PPath test_results_file_name = expdir + "/global_stats.pmat";
+        PPath source_names_file_name = expdir + "/source_names.psave";
+        ::PLearn::load(source_names_file_name, source_names);
+        VMat train_valid_results_file = new FileVMatrix(train_valid_results_file_name);
+        VMat test_results_file = new FileVMatrix(test_results_file_name);
+        int train_valid_last_row = train_valid_results_file->length() - 1;
+        real weight_decay = train_valid_results_file->get(train_valid_last_row, 2);
+        real train_mse =    train_valid_results_file->get(train_valid_last_row, 3);
+        real valid_mse =    train_valid_results_file->get(train_valid_last_row, 4);
+        if (best_valid_mse_threshold < 0.0)
+        {
+            best_valid_mse_threshold = deletion_threshold;
+            best_valid_mse_value = valid_mse;
+        }
+        else if (valid_mse < best_valid_mse_value)
+        {
+            best_valid_mse_threshold = deletion_threshold;
+            best_valid_mse_value = valid_mse;
+        }
+        real test_mse =     test_results_file->get(1, 0);
+        real test_cse =     test_results_file->get(1, 2);
+        real test_cse_std = test_results_file->get(1, 3);
+        real test_cle =     test_results_file->get(1, 4);
+        cout << setiosflags(ios::right) << setw(9) << deletion_threshold_str << "   "
+             << setw(4) << source_names.size() << "    "
+             << setw(6) << weight_decay << " "
+             << setw(6) << train_mse << " "
+             << setw(6) << valid_mse << " "
+             << setw(6) << test_mse << " "
+             << setw(6) << test_cse << "+/-"
+             << setw(6) << test_cse_std << " "
+             << setw(6) << test_cle << endl;
+    }
+    cout << endl << endl;
+    cout << "       Based on the validation mse, the suggested threshold is " << best_valid_mse_threshold << endl;
+    cout << endl << endl;
+}
+
+void Experimentation::train()
+{
+    PP<ExplicitSplitter> explicit_splitter = new ExplicitSplitter();
+    explicit_splitter->splitsets.resize(1,2);
+    explicit_splitter->splitsets(0,0) = train_valid_file;
+    explicit_splitter->splitsets(0,1) = test_file;
+    experiment = ::PLearn::deepCopy(experiment_template);
+    experiment->setOption("expdir", header_expdir + "/" + deletion_threshold_str);
+    experiment->splitter = new ExplicitSplitter();
+    experiment->splitter = explicit_splitter;
+    experiment->build();
+    Vec results = experiment->perform(true);
+}
+
+int Experimentation::outputsize() const {return 0;}
+void Experimentation::computeOutput(const Vec&, Vec&) const {}
+void Experimentation::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> Experimentation::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> Experimentation::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,221 @@
+// -*- C++ -*-
+
+// Experimentation.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file Experimentation.h */
+
+
+#ifndef Experimentation_INC
+#define Experimentation_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/testers/PTester.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/vmat/MemoryVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class Experimentation : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    
+    //! If set to 1, save the built train and test files instead of running the experiment.
+    int save_files;
+    //! If set to 1, the missing_indicator_field_names will be excluded from the built training files.
+    int experiment_without_missing_indicator;
+    //! The name of the field to select from the target_set as target for the built training files.
+    string target_field_name;
+    //! The field names of the missing indicators to exclude when we experiment without them.
+    TVec<string> missing_indicator_field_names;
+    //! The name of the group of experiments to conduct.
+    string experiment_name;
+    //! The number of test samples at the beginning of the train set.
+    int number_of_test_samples;
+    //! The number of train samples in the reference set to compute the % of missing.
+    int number_of_train_samples;
+    //! The train and valid set with missing values to compute the % of missing.
+    VMat reference_train_set;
+    //! The data set with the targets corresponding to the train set.
+    VMat target_set;
+    //! The vector of the various deletion threshold to run this experiment with.
+    Vec deletion_thresholds;
+    //! The path in which to build the directories for the experiment's results.
+    PPath experiment_directory;
+    //! The template of the script to conduct the experiment
+    PP<PTester> experiment_template;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    Experimentation();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(Experimentation);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void experimentSetUp();
+    void createHeaderFile();
+    void getHeaderRecord();
+    void updateHeaderRecord(int var_col);
+    void setSourceDataset();
+    void reviewGlobalStats();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    
+    int          main_row;
+    int          main_col;
+    int          main_length;
+    int          main_width;
+    Vec          main_input;
+    TVec<string> main_names;
+    PPath        main_metadata;
+    int          main_fields_selected_col;
+    TVec<string> main_fields_selected;
+    
+    int          fields_width;
+    int          fields_col;
+    
+    int          target_row;
+    int          target_col;
+    int          target_length;
+    int          target_width;
+    Vec          target_input;
+    TVec<string> target_names;
+    
+    int             header_col;
+    int             header_width;
+    Vec             header_record;
+    TVec<string>    header_names;
+    string          header_expdir;
+    PPath           header_file_name;
+    PP<FileVMatrix> header_file;
+    
+    int          to_deal_with_total;
+    int          to_deal_with_next;
+    real         deletion_threshold;
+    string       deletion_threshold_str;
+    
+    int          test_length;
+    int          test_width;
+    Vec          test_record;
+    VMat         test_file;
+    
+    int          train_valid_length;
+    int          train_valid_width;
+    Vec          train_valid_record;
+    VMat         train_valid_file;
+    
+    VMat         source_set;
+    TVec<string> source_names;
+    PP<PTester>  experiment;
+    
+/*
+    PPath results_file_name;
+    VMat results_file;
+    int results_length;
+    real results_nstages;
+    real results_mse;
+    real results_std_err;
+    PPath test_output_file_name;
+    VMat test_output_file;
+    int test_output_length;
+*/
+    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(Experimentation);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/FixDond2BinaryVariables.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/FixDond2BinaryVariables.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/FixDond2BinaryVariables.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,221 @@
+// -*- C++ -*-
+
+// FixDond2BinaryVariables.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file FixDond2BinaryVariables.cc */
+
+#define PL_LOG_MODULE_NAME "FixDond2BinaryVariables"
+#include <plearn/io/pl_log.h>
+
+#include "FixDond2BinaryVariables.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    FixDond2BinaryVariables,
+    "Fix binary variables with values 0, 1.0 or possibly is_missing.",
+    "Instructions are provided with the binary_variable_instructions option.\n"
+);
+
+/////////////////////////
+// FixDond2BinaryVariables //
+/////////////////////////
+FixDond2BinaryVariables::FixDond2BinaryVariables()
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void FixDond2BinaryVariables::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "binary_variable_instructions", &FixDond2BinaryVariables::binary_variable_instructions,
+                  OptionBase::buildoption,
+                  "The instructions to fix the binary variables in the form of field_name : instruction.\n"
+                  "Supported instructions are 9_is_one, not_0_is_one, not_missing_is_one, not_1000_is_one.\n"
+                  "Variables with no specification will be kept as_is.\n");
+
+    declareOption(ol, "output_path", &FixDond2BinaryVariables::output_path,
+                  OptionBase::buildoption,
+                  "The file path for the fixed output file.");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void FixDond2BinaryVariables::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(binary_variable_instructions, copies);
+    deepCopyField(output_path, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void FixDond2BinaryVariables::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void FixDond2BinaryVariables::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        fixBinaryVariables();
+    }
+}
+
+void FixDond2BinaryVariables::fixBinaryVariables()
+{    
+    // initialize primary dataset
+    main_row = 0;
+    main_col = 0;
+    main_length = train_set->length();
+    main_width = train_set->width();
+    main_input.resize(main_width);
+    main_names.resize(main_width);
+    main_ins.resize(main_width);
+    ins_width = binary_variable_instructions.size();
+    main_names << train_set->fieldNames();
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        main_ins[main_col] = "as_is";
+    }
+    for (ins_col = 0; ins_col < ins_width; ins_col++)
+    {
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            if (binary_variable_instructions[ins_col].first == main_names[main_col]) break;
+        }
+        if (main_col >= main_width) PLERROR("In FixDond2BinaryVariables: no field with this name in train data set: %s", (binary_variable_instructions[ins_col].first).c_str());
+        if (binary_variable_instructions[ins_col].second == "9_is_one") main_ins[main_col] = "9_is_one";
+        else if (binary_variable_instructions[ins_col].second == "not_0_is_one") main_ins[main_col] = "not_0_is_one";
+        else if (binary_variable_instructions[ins_col].second == "not_missing_is_one") main_ins[main_col] = "not_missing_is_one";
+        else if (binary_variable_instructions[ins_col].second == "not_1000_is_one") main_ins[main_col] = "not_1000_is_one";
+        else PLERROR("In FixDond2BinaryVariables: unsupported instruction: %s", (binary_variable_instructions[ins_col].second).c_str());
+    }
+    
+    // initialize output datasets
+    output_file = new FileVMatrix(output_path + ".pmat", main_length, main_names);
+    output_file->defineSizes(main_width, 0, 0);
+    
+    //Now, we can process the binary variables.
+    ProgressBar* pb = 0;
+    pb = new ProgressBar( "Fixing the binary variables", main_length);
+    for (main_row = 0; main_row < main_length; main_row++)
+    {
+        train_set->getRow(main_row, main_input);
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            if (main_ins[main_col] == "not_missing_is_one")
+            {
+                if (is_missing(main_input[main_col])) main_input[main_col] = 0.0;
+                else  main_input[main_col] = 1.0;
+            }
+            else if (main_ins[main_col] == "not_0_is_one")
+            {
+                if (is_missing(main_input[main_col])) continue;
+                if (main_input[main_col] != 0.0)  main_input[main_col] = 1.0;
+                else  main_input[main_col] = 0.0;
+            }
+            else if (main_ins[main_col] == "9_is_one")
+            {
+                if (is_missing(main_input[main_col])) continue;
+                if (main_input[main_col] == 9.0)  main_input[main_col] = 1.0;
+                else  main_input[main_col] = 0.0;
+            }
+            else if (main_ins[main_col] == "not_1000_is_one")
+            {
+                if (is_missing(main_input[main_col])) continue;
+                if (main_input[main_col] != -1000.0)  main_input[main_col] = 1.0;
+                else  main_input[main_col] = 0.0;
+            }
+        }
+        output_file->putRow(main_row, main_input);
+        pb->update( main_row );
+    }
+    delete pb;
+}
+
+VMat FixDond2BinaryVariables::getOutputFile()
+{
+    return output_file;
+}
+
+int FixDond2BinaryVariables::outputsize() const {return 0;}
+void FixDond2BinaryVariables::train()
+{
+    PLERROR("FixDond2BinaryVariables: we are done here");
+}
+void FixDond2BinaryVariables::computeOutput(const Vec&, Vec&) const {}
+void FixDond2BinaryVariables::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> FixDond2BinaryVariables::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> FixDond2BinaryVariables::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/FixDond2BinaryVariables.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/FixDond2BinaryVariables.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/FixDond2BinaryVariables.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,154 @@
+// -*- C++ -*-
+
+// FixDond2BinaryVariables.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file FixDond2BinaryVariables.h */
+
+
+#ifndef FixDond2BinaryVariables_INC
+#define FixDond2BinaryVariables_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn/vmat/FileVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class FixDond2BinaryVariables : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! The instructions to fix the binary variables in the form of field_name : instruction.
+    //! Supported instructions are 9_is_one, not_0_is_one, not_missing_is_one, not_1000_is_one.
+    //! Variables with no specification will be kept as_is.
+    TVec< pair<string, string> > binary_variable_instructions;
+    
+    //! The file path for the fixed output file.
+    string output_path;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    FixDond2BinaryVariables();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+    VMat getOutputFile();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(FixDond2BinaryVariables);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void fixBinaryVariables();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    
+    // input instructions variables
+    int ins_width;
+    int ins_col;
+    
+    // primary dataset variables
+    int main_length;
+    int main_width;
+    int main_row;
+    int main_col;
+    Vec main_input;
+    TVec<string> main_names;
+    TVec<string> main_ins;
+    
+    // output dataset variables
+    VMat output_file;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(FixDond2BinaryVariables);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,327 @@
+// -*- C++ -*-
+
+// GaussianizeVMatrix.cc
+//
+// Copyright (C) 2006 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file GaussianizeVMatrix.cc */
+
+
+#include "GaussianizeVMatrix.h"
+#include <plearn/math/pl_erf.h>
+#include <plearn/vmat/VMat_computeStats.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    GaussianizeVMatrix,
+    "Transforms its source VMatrix so that its features look Gaussian.",
+
+    "This VMat transforms the features of its source that are obviously non-\n"
+    "Gaussian, i.e. when the difference between the maximum and minimum\n"
+    "value is too large compared to the standard deviation (the meaning of\n"
+    "'too large' being controlled by the 'threshold_ratio' option).\n"
+    "\n"
+    "When this happens, the values of a features are sorted and their rank\n"
+    "is used to transform them through the inverse cumulative of a normal\n"
+    "Gaussian, resulting on a distribution that actually looks Gaussian.\n"
+    "Note that, unless specified otherwise through the options, only the\n"
+    "input features are transformed.\n"
+    "\n"
+    "An additional 'train_source' VMat can also be specified in order to\n"
+    "transform new data (in the 'source' option) while the transformation\n"
+    "parameters are learned on a fixed 'train_source' VMat (e.g. when new\n"
+    "test data are obtained and need to be properly Gaussianized).\n"
+);
+
+////////////////////////
+// GaussianizeVMatrix //
+////////////////////////
+GaussianizeVMatrix::GaussianizeVMatrix():
+    gaussianize_input(true),
+    gaussianize_target(false),
+    gaussianize_weight(false),
+    gaussianize_extra(false),
+    threshold_ratio(10)
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+void GaussianizeVMatrix::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "threshold_ratio", &GaussianizeVMatrix::threshold_ratio,
+                                         OptionBase::buildoption,
+        "A source's feature will be Gaussianized when the following holds:\n"
+        "(max - min) / stddev > threshold_ratio.");
+
+    declareOption(ol, "gaussianize_input",
+                  &GaussianizeVMatrix::gaussianize_input,
+                  OptionBase::buildoption,
+        "Whether or not to Gaussianize the input part.");
+
+    declareOption(ol, "gaussianize_target",
+                  &GaussianizeVMatrix::gaussianize_target,
+                  OptionBase::buildoption,
+        "Whether or not to Gaussianize the target part.");
+
+    declareOption(ol, "gaussianize_weight",
+                  &GaussianizeVMatrix::gaussianize_weight,
+                  OptionBase::buildoption,
+        "Whether or not to Gaussianize the weight part.");
+
+    declareOption(ol, "gaussianize_extra",
+                  &GaussianizeVMatrix::gaussianize_extra,
+                  OptionBase::buildoption,
+        "Whether or not to Gaussianize the extra part.");
+
+    declareOption(ol, "excluded_fields",
+                  &GaussianizeVMatrix::excluded_fields,
+                  OptionBase::buildoption,
+        "A list of fields to exclude from the process by field name.");
+
+    declareOption(ol, "train_source", &GaussianizeVMatrix::train_source,
+                                      OptionBase::buildoption,
+        "An optional VMat that will be used instead of 'source' to compute\n"
+        "the transformation parameters from the distribution statistics.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void GaussianizeVMatrix::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void GaussianizeVMatrix::build_()
+{
+    if (!source)
+        return;
+
+    if (train_source) {
+        assert( train_source->width() == source->width() );
+        assert( train_source->inputsize()  == source->inputsize() &&
+                train_source->targetsize() == source->targetsize() &&
+                train_source->weightsize() == source->weightsize() &&
+                train_source->extrasize()  == source->extrasize() );
+    }
+
+    VMat the_source = train_source ? train_source : source;
+
+    assert( the_source->inputsize() >= 0 && the_source->targetsize() >= 0 &&
+            the_source->weightsize() >= 0 && the_source->extrasize() >= 0 );
+
+    // Find which excluded_fields to exclude
+    int the_source_col;
+    int the_source_width = the_source->width();
+    TVec<string> the_source_names(the_source_width);
+    the_source_names << the_source->fieldNames();
+    int excluded_fields_col;
+    int excluded_fields_width = excluded_fields.size();
+    TVec<int> excluded_fields_selected(the_source_width);
+    excluded_fields_selected.clear();
+    for (excluded_fields_col = 0; excluded_fields_col < excluded_fields_width; excluded_fields_col++)
+    {
+        for (the_source_col = 0; the_source_col < the_source_width; the_source_col++)
+        {
+            if (excluded_fields[excluded_fields_col] == the_source_names[the_source_col]) break;
+        }
+        if (the_source_col >= the_source_width)
+            PLERROR("In GaussianizeVMatrix: no field with this name in input dataset: %s", (excluded_fields[excluded_fields_col]).c_str());
+        excluded_fields_selected[the_source_col] = 1;
+    }
+
+
+    // Find which dimensions to Gaussianize.
+    features_to_gaussianize.resize(0);
+    int col = 0;
+    if (gaussianize_input)
+        features_to_gaussianize.append(
+                TVec<int>(col, col + the_source->inputsize() - 1, 1));
+    col += the_source->inputsize();
+    if (gaussianize_target)
+        features_to_gaussianize.append(
+                TVec<int>(col, col + the_source->targetsize() - 1, 1));
+    col += the_source->targetsize();
+    if (gaussianize_weight)
+        features_to_gaussianize.append(
+                TVec<int>(col, col + the_source->weightsize() - 1, 1));
+    col += the_source->weightsize();
+    if (gaussianize_extra)
+        features_to_gaussianize.append(
+                TVec<int>(col, col + the_source->extrasize() - 1, 1));
+    col += the_source->extrasize();
+
+    // Compute source statistics.
+    TVec<StatsCollector> stats = PLearn::computeStats(the_source, -1, false);
+
+    // See which dimensions violate the Gaussian assumption and will be
+    // actually Gaussianized, and store the corresponding list of values.
+    TVec<int> candidates = features_to_gaussianize.copy();
+    features_to_gaussianize.resize(0);
+    counts.resize(0);
+    Vec row(2);
+    for (int i = 0; i < candidates.length(); i++) {
+        int j = candidates[i];
+        StatsCollector& stat = stats[j];
+        if (excluded_fields_selected[j] > 0)
+            continue;
+        if (fast_exact_is_equal(stat.stddev(), 0))
+            continue;
+        if ((stat.max() - stat.min()) > threshold_ratio * stat.stddev()) {
+            features_to_gaussianize.append(j);
+            counts.append(Mat());
+            Mat& counts_j = counts.lastElement();
+            // We use a dummy iterator to get rid of the last element in the
+            // counts, which is the max real value.
+            map<real, StatsCollectorCounts>::const_iterator it, it_dummy;
+            map<real,StatsCollectorCounts>* count_map = stat.getCounts();
+            it_dummy = count_map->begin();
+            it_dummy++;
+            int count_values = 0;
+            for (it = count_map->begin(); it_dummy != count_map->end();
+                                          it++, it_dummy++)
+            {
+                row[0] = it->first;
+                row[1] = count_values;
+                count_values += (int) it->second.n;
+                counts_j.appendRow(row);
+            }
+            // This scales the ranks so that they are between 0 and 1.
+            counts_j.column(1) /= row[1];
+        }
+    }
+
+    // Obtain meta information from source.
+    setMetaInfoFromSource();
+}
+
+///////////////
+// getNewRow //
+///////////////
+void GaussianizeVMatrix::getNewRow(int i, const Vec& v) const
+{
+    assert( source );
+    source->getRow(i, v);
+    for (int k = 0; k < features_to_gaussianize.length(); k++) {
+        int j = features_to_gaussianize[k];
+        real current_val = v[j];
+        if (is_missing(current_val))
+            continue;
+        // Find closest value in the training data.
+        Mat& counts_j = counts[k];
+        real closest;
+        if (current_val < counts_j(0, 0)) {
+            // Smaller than the minimum.
+            closest = 0;
+        } else if (current_val > counts_j(counts_j.length() - 1, 0)) {
+            // Higher than the maximum.
+            closest = 1;
+        } else {
+            int min = 0;
+            int max = counts_j.length() - 1;
+            while (max - min > 1) {
+                int mid = (max + min) / 2;
+                real mid_val = counts_j(mid, 0);
+                if (current_val < mid_val)
+                    max = mid;
+                else if (current_val > mid_val)
+                    min = mid;
+                else {
+                    // Found the exact value.
+                    min = max = mid;
+                }
+            }
+            if (min == max)
+                closest = counts_j(min, 1);
+            else {
+                assert( max - min == 1 );
+                if (fabs(current_val - counts_j(min, 0)) <
+                    fabs(current_val - counts_j(max, 0)))
+                {
+                    closest = counts_j(min, 1);
+                } else
+                    closest = counts_j(max, 1);
+            }
+        }
+        assert( closest >= 0 && closest <= 1 );
+        // The expectation of the minimum and maximum of n numbers taken from a
+        // uniform(0,1) distribution are respectively 1/n+1 and n/n+1: we shift
+        // and rescale 'closest' to be in [1/n+1, n/n+1] before using the
+        // inverse of the Gaussian cumulative function.
+        real n = counts_j.length();
+        closest = (n - 1) / (n + 1) * closest + 1 / (n + 1);
+        v[j] = gauss_01_quantile(closest);
+    }
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void GaussianizeVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("GaussianizeVMatrix::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,160 @@
+// -*- C++ -*-
+
+// GaussianizeVMatrix.h
+//
+// Copyright (C) 2006 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file GaussianizeVMatrix.h */
+
+
+#ifndef GaussianizeVMatrix_INC
+#define GaussianizeVMatrix_INC
+
+#include <plearn/vmat/SourceVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class GaussianizeVMatrix : public SourceVMatrix
+{
+    typedef SourceVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    bool gaussianize_input;
+    bool gaussianize_target;
+    bool gaussianize_weight;
+    bool gaussianize_extra;
+    real threshold_ratio;
+    TVec<string> excluded_fields;
+    VMat train_source;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    GaussianizeVMatrix();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(GaussianizeVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    //! List of features that need to be Gaussianized.
+    TVec<int> features_to_gaussianize;
+
+    //! Scaling factor to map the rank to [0,1].
+    Vec scaling_factor;
+
+    //! The j-th element is a two-column matrix whose first row is the (sorted)
+    //! list of values appearing in the variable features_to_gaussianize[j],
+    //! and the second row indicates how many occurences are strictly below
+    //! each of these values (i.e. gives their rank).
+    TVec<Mat> counts;
+
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+    //! Fill the vector 'v' with the content of the i-th row.
+    //! v is assumed to be the right size.
+    //! ### This function must be overridden in your class
+    virtual void getNewRow(int i, const Vec& v) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(GaussianizeVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,454 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************************    
+   * $Id: MeanMedianModeImputationVMatrix.cc 3658 2005-07-06 20:30:15  Godbout $
+   ******************************************************************* */
+
+
+#include "MeanMedianModeImputationVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+/** MeanMedianModeImputationVMatrix **/
+
+PLEARN_IMPLEMENT_OBJECT(
+  MeanMedianModeImputationVMatrix,
+  "VMat class to impute the observed variable mean to replace missing values in the source matrix.",
+  "This class will replace missing values in the underlying dataset with the mean, median or mode observed on the train set.\n"
+  "The imputed value is based on the imputation instruction option.\n"
+  );
+
+MeanMedianModeImputationVMatrix::MeanMedianModeImputationVMatrix()
+: number_of_train_samples_to_use(0.0)
+{
+}
+
+MeanMedianModeImputationVMatrix::~MeanMedianModeImputationVMatrix()
+{
+}
+
+void MeanMedianModeImputationVMatrix::declareOptions(OptionList &ol)
+{
+  declareOption(ol, "source", &MeanMedianModeImputationVMatrix::source, OptionBase::buildoption, 
+                "The source VMatrix with missing values.\n");
+
+  declareOption(ol, "train_set", &MeanMedianModeImputationVMatrix::train_set, OptionBase::buildoption, 
+                "A referenced train set.\n"
+                "The mean, median or mode is computed with the observed values in this data set.\n"
+                "It is used in combination with the option number_of_train_samples_to_use\n");
+
+  declareOption(ol, "number_of_train_samples_to_use", &MeanMedianModeImputationVMatrix::number_of_train_samples_to_use, OptionBase::buildoption, 
+                "The number of samples from the train set that will be examined to compute the required statistic for each variable.\n" 
+                "If equal to zero, all the samples from the train set are used to calculated the statistics.\n"
+                "If it is a fraction between 0 and 1, this proportion of the samples are used.\n"
+                "If greater or equal to 1, the integer portion is interpreted as the number of samples to use.");
+      
+  declareOption(ol, "imputation_spec", &MeanMedianModeImputationVMatrix::imputation_spec, OptionBase::buildoption, 
+                "Pairs of instruction of the form field_name : mean | median | mode.");
+
+  declareOption(ol, "variable_mean", &MeanMedianModeImputationVMatrix::variable_mean, OptionBase::learntoption, 
+                "The vector of variable means observed from the train set.");
+
+  declareOption(ol, "variable_median", &MeanMedianModeImputationVMatrix::variable_median, OptionBase::learntoption, 
+                "The vector of variable medians observed from the train set.");
+
+  declareOption(ol, "variable_mode", &MeanMedianModeImputationVMatrix::variable_mode, OptionBase::learntoption, 
+                "The vector of variable modes observed from the train set.");
+
+  declareOption(ol, "variable_present_count", &MeanMedianModeImputationVMatrix::variable_present_count, OptionBase::learntoption, 
+                "The vector of non missing variable counts from the train set.");
+
+  declareOption(ol, "variable_missing_count", &MeanMedianModeImputationVMatrix::variable_missing_count, OptionBase::learntoption, 
+                "The vector of missing variable counts from the train set.");
+
+  declareOption(ol, "variable_mode_count", &MeanMedianModeImputationVMatrix::variable_mode_count, OptionBase::learntoption, 
+                "The vector of variable mode counts from the train set.");
+
+  declareOption(ol, "variable_imputation_instruction", &MeanMedianModeImputationVMatrix::variable_imputation_instruction, OptionBase::learntoption, 
+                "The vector of coded instruction for each variables.");
+
+  inherited::declareOptions(ol);
+}
+
+void MeanMedianModeImputationVMatrix::build()
+{
+  inherited::build();
+  build_();
+}
+
+void MeanMedianModeImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+  deepCopyField(source, copies);
+  deepCopyField(train_set, copies);
+  deepCopyField(number_of_train_samples_to_use, copies);
+  deepCopyField(imputation_spec, copies);
+  deepCopyField(variable_mean, copies);
+  deepCopyField(variable_median, copies);
+  deepCopyField(variable_mode, copies);
+  deepCopyField(variable_present_count, copies);
+  deepCopyField(variable_missing_count, copies);
+  deepCopyField(variable_imputation_instruction, copies);
+  inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void MeanMedianModeImputationVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
+{
+  source->getExample(i, input, target, weight);
+  for (int source_col = 0; source_col < input->length(); source_col++)
+  {
+    if (is_missing(input[source_col]) && variable_imputation_instruction[source_col] > 0)
+      if (variable_imputation_instruction[source_col] == 1) input[source_col] = variable_mean[source_col];
+      else if (variable_imputation_instruction[source_col] == 2) input[source_col] = variable_median[source_col];
+      else if (variable_imputation_instruction[source_col] == 3) input[source_col] = variable_mode[source_col];
+  }  
+}
+
+real MeanMedianModeImputationVMatrix::get(int i, int j) const
+{ 
+  real variable_value = source->get(i, j);
+  if (!is_missing(variable_value)) return variable_value;
+  if (variable_imputation_instruction[j] == 1) return variable_mean[j];
+  if (variable_imputation_instruction[j] == 2) return variable_median[j];
+  if (variable_imputation_instruction[j] == 3) return variable_mode[j];
+  return variable_value;
+}
+
+void MeanMedianModeImputationVMatrix::put(int i, int j, real value)
+{
+  PLERROR("In MeanMedianModeImputationVMatrix::put not implemented");
+}
+
+void MeanMedianModeImputationVMatrix::getSubRow(int i, int j, Vec v) const
+{  
+  source->getSubRow(i, j, v);
+  for (int source_col = 0; source_col < v->length(); source_col++) 
+    if (is_missing(v[source_col]) && variable_imputation_instruction[source_col + j] > 0)
+      if (variable_imputation_instruction[source_col + j] == 1) v[source_col] = variable_mean[source_col + j];
+      else if (variable_imputation_instruction[source_col + j] == 2) v[source_col] = variable_median[source_col + j];
+      else if (variable_imputation_instruction[source_col + j] == 3) v[source_col] = variable_mode[source_col + j];
+}
+
+void MeanMedianModeImputationVMatrix::putSubRow(int i, int j, Vec v)
+{
+  PLERROR("In MeanMedianModeImputationVMatrix::putSubRow not implemented");
+}
+
+void MeanMedianModeImputationVMatrix::appendRow(Vec v)
+{
+  PLERROR("In MeanMedianModeImputationVMatrix::appendRow not implemented");
+}
+
+void MeanMedianModeImputationVMatrix::insertRow(int i, Vec v)
+{
+  PLERROR("In MeanMedianModeImputationVMatrix::insertRow not implemented");
+}
+
+void MeanMedianModeImputationVMatrix::getRow(int i, Vec v) const
+{  
+  source-> getRow(i, v);
+  for (int source_col = 0; source_col < v->length(); source_col++)
+    if (is_missing(v[source_col]) && variable_imputation_instruction[source_col] > 0)
+      if (variable_imputation_instruction[source_col] == 1) v[source_col] = variable_mean[source_col];
+      else if (variable_imputation_instruction[source_col] == 2) v[source_col] = variable_median[source_col];
+      else if (variable_imputation_instruction[source_col] == 3) v[source_col] = variable_mode[source_col]; 
+}
+
+void MeanMedianModeImputationVMatrix::putRow(int i, Vec v)
+{
+  PLERROR("In MeanMedianModeImputationVMatrix::putRow not implemented");
+}
+
+void MeanMedianModeImputationVMatrix::getColumn(int i, Vec v) const
+{  
+  source-> getColumn(i, v);
+  for (int source_row = 0; source_row < v->length(); source_row++)
+    if (is_missing(v[source_row]) && variable_imputation_instruction[i] > 0)
+      if (variable_imputation_instruction[i] == 1) v[source_row] = variable_mean[i];
+      else if (variable_imputation_instruction[i] == 2) v[source_row] = variable_median[i];
+      else if (variable_imputation_instruction[i] == 3) v[source_row] = variable_mode[i];
+}
+
+
+
+void MeanMedianModeImputationVMatrix::build_()
+{
+    if (!train_set || !source) PLERROR("In MeanMedianModeImputationVMatrix::train set and source vmat must be supplied");
+    train_length = train_set->length();
+    if (number_of_train_samples_to_use > 0.0)
+        if (number_of_train_samples_to_use < 1.0) train_length = (int) (number_of_train_samples_to_use * (real) train_length);
+        else train_length = (int) number_of_train_samples_to_use;
+    if (train_length > train_set->length()) train_length = train_set->length();
+    if(train_length < 1) PLERROR("In MeanMedianModeImputationVMatrix::length of the number of train samples to use must be at least 1, got: %i", train_length);
+    train_width = train_set->width();
+    train_targetsize = train_set->targetsize();
+    train_weightsize = train_set->weightsize();
+    train_inputsize = train_set->inputsize();
+    if(train_inputsize < 1) PLERROR("In MeanMedianModeImputationVMatrix::inputsize of the train vmat must be supplied, got : %i", train_inputsize);
+    source_width = source->width();
+    source_targetsize = source->targetsize();
+    source_weightsize = source->weightsize();
+    source_inputsize = source->inputsize();
+    if (train_width != source_width) PLERROR("In MeanMedianModeImputationVMatrix::train set and source width must agree, got : %i, %i", train_width, source_width);
+    if (train_targetsize != source_targetsize) PLERROR("In MeanMedianModeImputationVMatrix::train set and source targetsize must agree, got : %i, %i", train_targetsize, source_targetsize);
+    if (train_weightsize != source_weightsize) PLERROR("In MeanMedianModeImputationVMatrix::train set and source weightsize must agree, got : %i, %i", train_weightsize, source_weightsize);
+    if (train_inputsize != source_inputsize) PLERROR("In MeanMedianModeImputationVMatrix::train set and source inputsize must agree, got : %i, %i", train_inputsize, source_inputsize);
+    train_field_names.resize(train_width);
+    train_field_names = train_set->fieldNames();
+    source_length = source->length();
+    length_ = source_length;
+    width_ = source_width;
+    inputsize_ = source_inputsize;
+    targetsize_ = source_targetsize;
+    weightsize_ = source_weightsize;
+    declareFieldNames(train_field_names);
+    variable_mean.resize(train_width);
+    variable_median.resize(train_width);
+    variable_mode.resize(train_width);
+    variable_imputation_instruction.resize(train_width);
+    variable_imputation_instruction.clear();
+    for (spec_col = 0; spec_col < imputation_spec.size(); spec_col++)
+    {
+        for (train_col = 0; train_col < train_width; train_col++)
+        {
+            if (imputation_spec[spec_col].first == train_field_names[train_col]) break;
+        }
+        if (train_col >= train_width) PLERROR("In MeanMedianModeImputationVMatrix: no field with this name in train data set: %s", (imputation_spec[spec_col].first).c_str());
+        if (imputation_spec[spec_col].second == "mean") variable_imputation_instruction[train_col] = 1;
+        else if (imputation_spec[spec_col].second == "median") variable_imputation_instruction[train_col] = 2;
+        else if (imputation_spec[spec_col].second == "mode") variable_imputation_instruction[train_col] = 3;
+        else PLERROR("In MeanMedianModeImputationVMatrix: unsupported imputation instruction: %s : %s", (imputation_spec[spec_col].first).c_str(), (imputation_spec[spec_col].second).c_str());
+    }
+    train_metadata = train_set->getMetaDataDir();
+    mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
+    
+    if (!isfile(mean_median_mode_file_name))
+    {
+        computeMeanMedianModeVectors();
+        createMeanMedianModeFile();
+    }
+    else loadMeanMedianModeFile();
+}
+
+void MeanMedianModeImputationVMatrix::createMeanMedianModeFile()
+{
+    mean_median_mode_file = new FileVMatrix(mean_median_mode_file_name, 3, train_field_names);
+    mean_median_mode_file->putRow(0, variable_mean);
+    mean_median_mode_file->putRow(1, variable_median);
+    mean_median_mode_file->putRow(2, variable_mode);
+}
+
+void MeanMedianModeImputationVMatrix::loadMeanMedianModeFile()
+{
+    mean_median_mode_file = new FileVMatrix(mean_median_mode_file_name);
+    mean_median_mode_file->getRow(0, variable_mean);
+    mean_median_mode_file->getRow(1, variable_median);
+    mean_median_mode_file->getRow(2, variable_mode);
+}
+
+VMat MeanMedianModeImputationVMatrix::getMeanMedianModeFile()
+{
+    return mean_median_mode_file;
+}
+
+void MeanMedianModeImputationVMatrix::computeMeanMedianModeVectors()
+{
+    variable_present_count.resize(train_width);
+    variable_missing_count.resize(train_width);
+    variable_mode_count.resize(train_width);
+    variable_mean.clear();
+    variable_median.clear();
+    variable_mode.clear();
+    variable_present_count.clear();
+    variable_missing_count.clear();
+    variable_mode_count.clear();
+    variable_vec.resize(train_set->length());
+    cout << fixed << showpoint;
+    ProgressBar* pb = 0;
+    pb = new ProgressBar("Computing the mean, median and mode vectors", train_width);
+    for (train_col = 0; train_col < train_width; train_col++)
+    {
+        current_value = 0.0;
+        current_value_count = 0;
+        train_set->getColumn(train_col, variable_vec);
+        sortColumn(variable_vec, 0, train_length);
+        for (train_row = 0; train_row < train_length; train_row++)
+        {
+            if (is_missing(variable_vec[train_row]))
+            {
+                variable_missing_count[train_col] += 1;
+                continue;
+            }
+            variable_mean[train_col] += variable_vec[train_row];
+            variable_present_count[train_col] += 1;
+            if (variable_vec[train_row] != current_value)
+            {
+                if (current_value_count > variable_mode_count[train_col])
+                {
+                    variable_mode[train_col] = current_value;
+                    variable_mode_count[train_col] = current_value_count;
+                }
+                current_value_count = 0;
+                current_value = variable_vec[train_row];
+            }
+            current_value_count += 1;
+        }
+        if (variable_present_count[train_col] > 0)
+        {
+            variable_mean[train_col] = variable_mean[train_col] / variable_present_count[train_col];
+            variable_median[train_col] = variable_vec[(variable_present_count[train_col] / 2)];
+        }
+        if (current_value_count > variable_mode_count[train_col])
+        {
+            variable_mode[train_col] = current_value;
+            variable_mode_count[train_col] = current_value_count;
+        }
+        pb->update( train_col );
+        /*
+        cout << "col: "         << setw(3)  <<                     train_col
+             << " present: "    << setw(5)  <<                     variable_present_count[train_col]
+             << " missing: "    << setw(5)  <<                     variable_missing_count[train_col]
+             << " mean: "       << setw(11) << setprecision(2) <<  variable_mean[train_col]
+             << " median: "     << setw(11) << setprecision(2) <<  variable_median[train_col]
+             << " mode count: " << setw(5)  <<                     variable_mode_count[train_col]
+             << " mode: "       << setw(11) << setprecision(2) <<  variable_mode[train_col]
+             << " name: "       <<                                 train_field_names[train_col]
+             << endl;
+        */
+    }
+    delete pb;
+}
+
+void MeanMedianModeImputationVMatrix::sortColumn(Vec input_vec, int start, int end)
+{
+  int start_index = start;
+  int end_index = end - 1;
+  int forward_index;
+  int backward_index;
+  int stack_index = -1;
+  real pivot_value;
+  TVec<int> stack(50);
+  for (;;)
+  {
+    if ((end_index - start_index) < 7)
+    {
+      if (end_index > start_index)
+      {
+        sortSmallSubArray(input_vec, start_index, end_index);
+      }
+      if (stack_index < 0)
+      {
+        break;
+      }
+      end_index = stack[stack_index--];
+      start_index = stack[stack_index--];
+    }
+    else
+    {
+      swapValues(input_vec, start_index + 1, (start_index + end_index) / 2);
+      if (compare(input_vec[start_index], input_vec[end_index]) > 0.0) swapValues(input_vec, start_index, end_index);
+      if (compare(input_vec[start_index + 1], input_vec[end_index]) > 0.0) swapValues(input_vec, start_index + 1, end_index);
+      if (compare(input_vec[start_index], input_vec[start_index + 1]) > 0.0) swapValues(input_vec, start_index, start_index + 1);
+      forward_index = start_index + 1;
+      backward_index = end_index;
+      pivot_value = input_vec[start_index + 1];
+      for (;;)
+      {
+        do forward_index++; while (compare(input_vec[forward_index], pivot_value) < 0.0);
+        do backward_index--; while (compare(input_vec[backward_index], pivot_value) > 0.0);
+        if (backward_index < forward_index)
+        {
+          break;
+        }
+        swapValues(input_vec, forward_index, backward_index);
+      }
+      swapValues(input_vec, start_index + 1, backward_index);
+      stack_index += 2;
+      if (stack_index > 50)
+        PLERROR("RegressionTreeRegistersVMatrix: the stack for sorting the rows is too small");
+      if ((end_index - forward_index + 1) >= (backward_index - start_index))
+      {
+        stack[stack_index] = end_index;
+        stack[stack_index - 1] = forward_index;
+        end_index = backward_index - 1;
+      }
+      else
+      {
+        stack[stack_index] = backward_index - 1;
+        stack[stack_index - 1] = start_index;
+        start_index = forward_index;
+      }
+    }
+  }
+}
+  
+void MeanMedianModeImputationVMatrix::sortSmallSubArray(Vec input_vec, int start_index, int end_index)
+{
+  int index_i;
+  int index_j;
+  for (index_i = start_index + 1; index_i <= end_index; index_i++)
+  {
+    real saved_value = input_vec[index_i];
+    for (index_j = index_i - 1; index_j >= start_index; index_j--)
+    {
+      if (compare(input_vec[index_j], saved_value) <= 0.0)
+      {
+        break;
+      }
+      input_vec[index_j + 1] = input_vec[index_j];
+    }
+    input_vec[index_j + 1] = saved_value;
+  }  
+}
+
+void MeanMedianModeImputationVMatrix::swapValues(Vec input_vec, int index_i, int index_j)
+{
+  real saved_value = input_vec[index_i];
+  input_vec[index_i] = input_vec[index_j];
+  input_vec[index_j] = saved_value;
+}
+
+double MeanMedianModeImputationVMatrix::compare(double field1, double field2)
+{
+  if (is_missing(field1) && is_missing(field2)) return 0.0;
+  if (is_missing(field1)) return +1.0;
+  if (is_missing(field2)) return -1.0;
+  return field1 - field2;
+}
+
+} // end of namespcae PLearn

Added: branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,161 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ******************************************************************      
+   * $Id: MeanMedianModeImputationVMatrix.h 3658 2005-07-06 20:30:15  Godbout $
+   ****************************************************************** */
+
+/*! \file PLearnLibrary/PLearnCore/VMat.h */
+
+#ifndef MeanMedianModeImputationVMatrix_INC
+#define MeanMedianModeImputationVMatrix_INC
+
+#include <plearn/vmat/SourceVMatrix.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/io/fileutils.h>                     //!<  For isfile()
+#include <plearn/math/BottomNI.h>
+
+namespace PLearn {
+using namespace std;
+
+class MeanMedianModeImputationVMatrix: public VMatrix
+{
+  typedef VMatrix inherited;
+  
+public:
+
+  //! The source VMatrix with missing values.
+  VMat                          source;
+  
+  //! A referenced train set.
+  //! The mean, median or mode is computed with the observed values in this data set.
+  //! It is used in combination with the option number_of_train_samples_to_use.
+  VMat                          train_set;
+  
+  //! The number of samples from the train set that will be examined to compute the required statistic for each variable.
+  //! If equal to zero, all the samples from the train set are used to calculated the statistics.
+  //! If it is a fraction between 0 and 1, this proportion of the samples are used.
+  //! If greater or equal to 1, the integer portion is interpreted as the number of samples to use.
+  real                          number_of_train_samples_to_use;
+  
+  //! Pairs of instruction of the form field_name : mean | median | mode.
+  TVec< pair<string, string> >  imputation_spec;
+  
+  //! The vector of variable means observed from the train set.
+  Vec                           variable_mean;
+  
+  //! The vector of variable medians observed from the train set.
+  Vec                           variable_median;
+  
+  //! The vector of variable modes observed from the train set.
+  Vec                           variable_mode;
+  
+  //! The vector of non missing variable counts from the train set.
+  TVec<int>                     variable_present_count;
+  
+  //! The vector of missing variable counts from the train set.
+  TVec<int>                     variable_missing_count;
+  
+  //! The vector of variable mode counts from the train set.
+  TVec<int>                     variable_mode_count;
+  
+  //! The vector of coded instruction for each variables.
+  TVec<int>                     variable_imputation_instruction;
+  
+  //! Pairs of instruction of the form field_name : mean | median | mode.
+  
+
+                        MeanMedianModeImputationVMatrix();
+  virtual               ~MeanMedianModeImputationVMatrix();
+
+  static void           declareOptions(OptionList &ol);
+
+  virtual void          build();
+  virtual void          makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+  virtual void         getExample(int i, Vec& input, Vec& target, real& weight);
+  virtual real         get(int i, int j) const;
+  virtual void         put(int i, int j, real value);
+  virtual void         getSubRow(int i, int j, Vec v) const;
+  virtual void         putSubRow(int i, int j, Vec v);
+  virtual void         appendRow(Vec v);
+  virtual void         insertRow(int i, Vec v);  
+  virtual void         getRow(int i, Vec v) const;
+  virtual void         putRow(int i, Vec v);
+  virtual void         getColumn(int i, Vec v) const;
+          VMat         getMeanMedianModeFile();
+
+private:
+  
+  int                  train_length;
+  int                  train_width;
+  int                  train_inputsize;
+  int                  train_targetsize;
+  int                  train_weightsize;
+  int                  train_row;
+  int                  train_col;
+  TVec<string>         train_field_names;
+  PPath                train_metadata;
+  int                  source_length;
+  int                  source_width;
+  int                  source_inputsize;
+  int                  source_targetsize;
+  int                  source_weightsize;
+  Vec                  variable_vec;
+  int                  spec_col;
+  int                  current_value_count;
+  real                 current_value;
+  PPath                mean_median_mode_file_name;
+  VMat                 mean_median_mode_file;
+
+          void         build_();
+          void         createMeanMedianModeFile(); 
+          void         loadMeanMedianModeFile(); 
+          void         computeMeanMedianModeVectors();  
+          void         sortColumn(Vec input_vec, int start, int end);
+          void         sortSmallSubArray(Vec input_vec, int start_index, int end_index);
+          void         swapValues(Vec input_vec, int index_i, int index_j);
+          real         compare(real field1, real field2);
+  
+  PLEARN_DECLARE_OBJECT(MeanMedianModeImputationVMatrix);
+
+};
+
+DECLARE_OBJECT_PTR(MeanMedianModeImputationVMatrix);
+
+} // end of namespcae PLearn
+#endif

Added: branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,490 @@
+// -*- C++ -*-
+
+// MergeDond2Files.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file MergeDond2Files.cc */
+
+#define PL_LOG_MODULE_NAME "MergeDond2Files"
+#include <plearn/io/pl_log.h>
+
+#include "MergeDond2Files.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    MergeDond2Files,
+    "Merge two files on specified keys.",
+    "If more than 1 matching record is found in the secondary datatset,\n"
+    "mean, mode or variable presence will be appended based on variable merge instruction.\n"
+    "a variable from the secondary dataset may also be skipped.\n"
+);
+
+/////////////////////////
+// MergeDond2Files //
+/////////////////////////
+MergeDond2Files::MergeDond2Files()
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void MergeDond2Files::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "external_dataset", &MergeDond2Files::external_dataset,
+                  OptionBase::buildoption,
+                  "The secondary dataset to merge with the main one.\n"
+                  "The main one is provided as the train_set.");
+
+    declareOption(ol, "missing_instructions", &MergeDond2Files::missing_instructions,
+                  OptionBase::buildoption,
+                  "The variable missing regeneration instructions in the form of pairs field : instruction.\n"
+                  "Supported instructions are skip, as_is, zero_is_missing, 2436935_is_missing, present.");
+
+    declareOption(ol, "merge_instructions", &MergeDond2Files::merge_instructions,
+                  OptionBase::buildoption,
+                  "The variable merge instructions in the form of pairs field : instruction.\n"
+                  "Supported instructions are skip, mean, mode, present.");
+
+    declareOption(ol, "merge_path", &MergeDond2Files::merge_path,
+                  OptionBase::buildoption,
+                  "The root name of merge files to be created.\n"
+                  "3 files will be created: a root_train.pmat, a root_test.pmat and a root_uknown.pmat");
+
+    declareOption(ol, "sec_key", &MergeDond2Files::sec_key,
+                  OptionBase::buildoption,
+                  "The column of the merge key in the secondary dataset.");
+
+    declareOption(ol, "main_key", &MergeDond2Files::main_key,
+                  OptionBase::buildoption,
+                  "The column of the merge key in the main dataset.");
+
+    declareOption(ol, "train_ind", &MergeDond2Files::train_ind,
+                  OptionBase::buildoption,
+                  "The column of the indicator of a train record in the main dataset.");
+
+    declareOption(ol, "test_ind", &MergeDond2Files::test_ind,
+                  OptionBase::buildoption,
+                  "The column of the indicator of a test record in the main dataset.");
+
+    declareOption(ol, "train_file", &MergeDond2Files::train_file,
+                  OptionBase::learntoption,
+                  "The train file created.");
+
+    declareOption(ol, "test_file", &MergeDond2Files::test_file,
+                  OptionBase::learntoption,
+                  "The test file created.");
+
+    declareOption(ol, "unknown_file", &MergeDond2Files::unknown_file,
+                  OptionBase::learntoption,
+                  "The unknown target file created.");
+
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void MergeDond2Files::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(external_dataset, copies);
+    deepCopyField(missing_instructions, copies);
+    deepCopyField(merge_instructions, copies);
+    deepCopyField(merge_path, copies);
+    deepCopyField(sec_key, copies);
+    deepCopyField(main_key, copies);
+    deepCopyField(train_ind, copies);
+    deepCopyField(test_ind, copies);
+    deepCopyField(train_file, copies);
+    deepCopyField(test_file, copies);
+    deepCopyField(unknown_file, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void MergeDond2Files::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void MergeDond2Files::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        mergeFiles();
+        PLERROR("MergeDond2Files: we are done here");
+    }
+}
+
+void MergeDond2Files::mergeFiles()
+{
+    // initialization with merge instructions
+    sec_row = 0;
+    sec_col = 0;
+    sec_length = external_dataset->length();
+    sec_width = external_dataset->width();
+    sec_names.resize(sec_width);
+    sec_ins.resize(sec_width);
+    sec_input.resize(sec_width);
+    ins_col = 0;
+    extension_width = 0;
+    sec_names << external_dataset->fieldNames();
+    for (sec_col = 0; sec_col < sec_width; sec_col++)
+    {
+        sec_ins[sec_col] = "mean";
+    }
+    for (ins_col = 0; ins_col < merge_instructions.size(); ins_col++)
+    {
+        for (sec_col = 0; sec_col < sec_width; sec_col++)
+        {
+            if (merge_instructions[ins_col].first == sec_names[sec_col]) break;
+        }
+        if (sec_col >= sec_width) PLERROR("In MergeDond2Files: no field with this name in external_dataset data set: %", (merge_instructions[ins_col].first).c_str());
+        if (merge_instructions[ins_col].second == "skip") sec_ins[sec_col] = "skip";
+        else if (merge_instructions[ins_col].second == "mean") sec_ins[sec_col] = "mean";
+        else if (merge_instructions[ins_col].second == "mode") sec_ins[sec_col] = "mode";
+        else if (merge_instructions[ins_col].second == "present") sec_ins[sec_col] = "present";
+        else PLERROR("In MergeDond2Files: unsupported merge instruction: %", (merge_instructions[ins_col].second).c_str());
+        if (sec_ins[sec_col] != "skip") extension_width += 1;
+    }
+    ext_col = 0;
+    extension_pos.resize(sec_width);
+    extension_names.resize(extension_width);
+    for (sec_col = 0; sec_col < sec_width; sec_col++)
+    {
+        if (sec_ins[sec_col] == "skip")
+        {
+            extension_pos[sec_col] = -1; 
+        }
+        else
+        {
+            extension_pos[sec_col] = ext_col;
+            extension_names[ext_col] = sec_names[sec_col];
+            ext_col += 1;
+        }
+    }
+    sec_values.resize(extension_width, 10);
+    sec_value_cnt.resize(extension_width, 10);
+    sec_value_ind.resize(extension_width);
+    sec_values.clear();
+    sec_value_cnt.clear();
+    sec_value_ind.clear();
+    external_dataset->getRow(sec_row, sec_input);
+    
+    // initialize primary dataset
+    main_row = 0;
+    main_col = 0;
+    main_length = train_set->length();
+    main_width = train_set->width();
+    main_input.resize(main_width);
+    main_names.resize(main_width);
+    main_ins.resize(main_width);
+    main_names << train_set->fieldNames();
+    primary_width = 0;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        main_ins[main_col] = "as_is";
+    }
+    for (ins_col = 0; ins_col < missing_instructions.size(); ins_col++)
+    {
+        for (main_col = 0; main_col < main_width; main_col++)
+        {
+            if (missing_instructions[ins_col].first == main_names[main_col]) break;
+        }
+        if (main_col >= main_width) PLERROR("In MergeDond2Files: no field with this name in external_dataset data set: %", (missing_instructions[ins_col].first).c_str());
+        if (missing_instructions[ins_col].second == "skip") main_ins[main_col] = "skip";
+        else if (missing_instructions[ins_col].second == "as_is") main_ins[main_col] = "as_is";
+        else if (missing_instructions[ins_col].second == "zero_is_missing") main_ins[main_col] = "zero_is_missing";
+        else if (missing_instructions[ins_col].second == "2436935_is_missing") main_ins[main_col] = "2436935_is_missing";
+        else if (missing_instructions[ins_col].second == "present") main_ins[main_col] = "present";
+        else PLERROR("In MergeDond2Files: unsupported merge instruction: %", (missing_instructions[ins_col].second).c_str());
+        if (main_ins[main_col] != "skip") primary_width += 1;
+    }
+    prim_col = 0;
+    primary_names.resize(primary_width);
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (main_ins[main_col] != "skip")
+        {
+            primary_names[prim_col] = main_names[main_col];
+            prim_col += 1;
+        }
+    }
+    
+    // initialize output datasets
+    merge_col = 0;
+    merge_width = primary_width + extension_width;
+    merge_output.resize(merge_width);
+    merge_names.resize(merge_width);
+    for (prim_col = 0; prim_col < primary_width; prim_col++)
+    {
+       merge_names[merge_col] = primary_names[prim_col];
+       merge_col +=1;
+    }
+    for (ext_col = 0; ext_col < extension_width; ext_col++)
+    {
+       merge_names[merge_col] = extension_names[ext_col];
+       merge_col +=1;
+    }
+    train_length = 0;
+    test_length = 0;
+    unknown_length = 0;
+    ProgressBar* pb = 0;
+    pb = new ProgressBar( "Counting the number of records in the train, test and unknown datasets", main_length);
+    for (main_row = 0; main_row < main_length; main_row++)
+    {
+        train_set->getRow(main_row, main_input);
+        if (main_input[train_ind] > 0.0) train_length += 1;
+        else if (main_input[test_ind] > 0.0) test_length += 1;
+        else unknown_length += 1;
+        pb->update( main_row );
+    }
+    delete pb;
+    train_file = new FileVMatrix(merge_path + "_train.pmat", train_length, merge_width);
+    test_file = new FileVMatrix(merge_path + "_test.pmat", test_length, merge_width);
+    unknown_file = new FileVMatrix(merge_path + "_unknown.pmat", unknown_length, merge_width);
+    train_file->declareFieldNames(merge_names);
+    test_file->declareFieldNames(merge_names);
+    unknown_file->declareFieldNames(merge_names);
+    train_row = 0;
+    test_row = 0;
+    unknown_row = 0;
+    
+    //Now, we can merge
+    pb = new ProgressBar( "Merging primary and secondary datasets with instructions", main_length);
+    for (main_row = 0; main_row < main_length; main_row++)
+    {
+        train_set->getRow(main_row, main_input);
+        if (sec_row >= sec_length)
+        {
+            combineAndPut();
+            continue;
+        }
+        while (sec_input[sec_key] < main_input[main_key])
+        {
+            sec_row += 1;
+            if (sec_row >= sec_length) break;   
+            external_dataset->getRow(sec_row, sec_input);
+        }
+        while (sec_input[sec_key] == main_input[main_key])
+        {
+            accumulateVec();
+            sec_row += 1;
+            if (sec_row >= sec_length) break;   
+            external_dataset->getRow(sec_row, sec_input);
+        }
+        combineAndPut();
+        pb->update( main_row );
+    }
+    delete pb;
+}
+
+void MergeDond2Files::accumulateVec()
+{
+    for (sec_col = 0; sec_col < sec_width; sec_col++)
+    {
+        if (is_missing(sec_input[sec_col])) continue;
+        if (sec_ins[sec_col] == "skip") continue;
+        ext_col = extension_pos[sec_col];
+        if (sec_ins[sec_col] == "mean")
+        {
+            sec_values(ext_col, 0) += sec_input[sec_col];
+            sec_value_cnt(ext_col, 0) += 1.0;
+        }
+        if (sec_ins[sec_col] == "mode")
+        {
+            sec_value_found = false;
+            for (sec_value_col = 0; sec_value_col < sec_value_ind[sec_col]; sec_value_col++)
+            {
+                if (sec_values(ext_col, sec_value_col) == sec_input[sec_col])
+                {
+                    sec_value_found = true;
+                    sec_value_cnt(ext_col, sec_value_col) += 1;
+                    break;
+                }
+            }
+            if (!sec_value_found)
+            {
+                if (sec_value_ind[sec_col] >= 10)
+                {
+                    cout << "MergeDond2Files: main file row: " << main_row << " external file row: " << sec_row << endl;
+                    PLERROR("MergeDond2Files: we have exceeded the capacity of the value MAT.");
+                }
+                sec_values(ext_col, sec_value_ind[sec_col]) = sec_input[sec_col];
+                sec_value_ind[sec_col] += 1;
+            }
+        }
+        if (sec_ins[sec_col] == "present")
+        {
+            sec_value_cnt(ext_col, 0) = 1.0;
+        }
+    }
+}
+
+void MergeDond2Files::combineAndPut()
+{
+    merge_col = 0;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (main_ins[main_col] == "skip") continue;
+        if (main_ins[main_col] == "as_is")
+        {
+            merge_output[merge_col] = main_input[main_col];
+            merge_col +=1;
+            continue;
+        }
+        if (main_ins[main_col] == "zero_is_missing")
+        {
+            if (main_input[main_col] == 0.0) merge_output[merge_col] = MISSING_VALUE;
+            else merge_output[merge_col] = main_input[main_col];
+            merge_col +=1;
+            continue;
+        }
+        if (main_ins[main_col] == "2436935_is_missing")
+        {
+            if (main_input[main_col] == 2436935.0) merge_output[merge_col] = MISSING_VALUE;
+            else merge_output[merge_col] = main_input[main_col];
+            merge_col +=1;
+            continue;
+        }
+        if (main_ins[main_col] == "present")
+        {
+            if (is_missing(main_input[main_col])) merge_output[merge_col] = 0.0;
+            else merge_output[merge_col] = 1.0;
+            merge_col +=1;
+            continue;
+        }
+    }
+    for (sec_col = 0; sec_col < sec_width; sec_col++)
+    {
+        if (sec_ins[sec_col] == "skip") continue;
+        ext_col = extension_pos[sec_col];
+        if (sec_ins[sec_col] == "mean")
+        {
+            if (sec_value_cnt(ext_col, 0) <= 0.0)  merge_output[merge_col] = MISSING_VALUE;
+            else merge_output[merge_col] = sec_values(ext_col, 0) / sec_value_cnt(ext_col, 0);
+            merge_col +=1;
+            continue;
+        }
+        if (sec_ins[sec_col] == "mode")
+        {
+            if (sec_value_ind[sec_col] <= 0.0)
+            {
+                merge_output[merge_col] = MISSING_VALUE;
+                merge_col +=1;
+                continue;
+            }
+            merge_output[merge_col] = sec_values(ext_col, 0);
+            sec_value_count_max = sec_value_cnt(ext_col, 0);
+            for (sec_value_col = 1; sec_value_col < sec_value_ind[sec_col]; sec_value_col++)
+            {
+                if (sec_value_cnt(ext_col, sec_value_col) >= sec_value_count_max)
+                {
+                    merge_output[merge_col] = sec_values(ext_col, sec_value_col);
+                    sec_value_count_max = sec_value_cnt(ext_col, sec_value_col);
+                }
+            }
+            merge_col +=1;
+            continue;
+        }
+        if (sec_ins[sec_col] == "present")
+        {
+            merge_output[merge_col] = sec_value_cnt(ext_col, 0);
+            merge_col +=1;
+            continue;
+        }
+    }
+    if (main_input[train_ind] > 0.0)
+    {
+        train_file->putRow(train_row, merge_output);
+        train_row += 1;
+    }
+    else if (main_input[test_ind] > 0.0)
+    {
+        test_file->putRow(test_row, merge_output);
+        test_row += 1;
+    }
+    else
+    {
+        unknown_file->putRow(unknown_row, merge_output);
+        unknown_row += 1;
+    }
+    sec_values.clear();
+    sec_value_cnt.clear();
+    sec_value_ind.clear();
+}
+
+int MergeDond2Files::outputsize() const {return 0;}
+void MergeDond2Files::train() {}
+void MergeDond2Files::computeOutput(const Vec&, Vec&) const {}
+void MergeDond2Files::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> MergeDond2Files::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> MergeDond2Files::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,210 @@
+// -*- C++ -*-
+
+// MergeDond2Files.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file MergeDond2Files.h */
+
+
+#ifndef MergeDond2Files_INC
+#define MergeDond2Files_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn/vmat/FileVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class MergeDond2Files : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    
+    //! The secondary dataset to merge with the main one.
+    //! The main one is provided as the train_set.
+    VMat external_dataset;
+
+    //! The variable missing regeneration instructions in the form of pairs field : instruction.
+    //! Supported instructions are skip, as_is, zero_is_missing, 2436935_is_missing.
+    TVec< pair<string, string> >  missing_instructions;
+    
+    //! The variable merge instructions in the form of pairs field : instruction.
+    //! Supported instructions are skip, mean, mode, present.
+    TVec< pair<string, string> >  merge_instructions;
+    
+    //! The file name of the merge file to be created.
+    string merge_path;
+    
+    //! The column of the merge key in the secondary dataset.
+    int sec_key;
+    
+    //! The column of the merge key in the main dataset.
+    int main_key;
+    
+    //! The column of the indicator of a train record in the main dataset.
+    int train_ind;
+    
+    //! The column of the indicator of a test record in the main dataset.
+    int test_ind;
+    
+    //! The train file created.
+    VMat train_file;
+    
+    //! The test file created.
+    VMat test_file;
+    
+    //! The unknown target file created.
+    VMat unknown_file;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    MergeDond2Files();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(MergeDond2Files);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void mergeFiles();
+    void accumulateVec();
+    void combineAndPut();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    // secondary dataset variables
+    int sec_length;
+    int sec_width;
+    int sec_row;
+    int sec_col;
+    Vec sec_input;
+    TVec<string> sec_names;
+    TVec<string> sec_ins;
+    int ins_col;
+    int extension_width;
+    int ext_col;
+    TVec<int> extension_pos;
+    TVec<string> extension_names;
+    Mat sec_values;
+    Mat sec_value_cnt;
+    TVec<int> sec_value_ind;
+    bool sec_value_found;
+    real sec_value_count_max;
+    int sec_value_col;
+    
+    // primary dataset variables
+    int main_length;
+    int main_width;
+    int main_row;
+    int main_col;
+    Vec main_input;
+    TVec<string> main_names;
+    TVec<string> main_ins;
+    int primary_width;
+    int prim_col;
+    TVec<string> primary_names;
+    
+    // merge dataset variables
+    int train_length;
+    int test_length;
+    int unknown_length;
+    int merge_width;
+    int train_row;
+    int test_row;
+    int unknown_row;
+    int merge_col;
+    Vec merge_output;
+    TVec<string> merge_names;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(MergeDond2Files);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,257 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************************    
+   * $Id: MissingIndicatorVMatrix.cc 3658 2005-07-06 20:30:15  Godbout $
+   ******************************************************************* */
+
+
+#include "MissingIndicatorVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+/** MissingIndicatorVMatrix **/
+
+PLEARN_IMPLEMENT_OBJECT(
+  MissingIndicatorVMatrix,
+  "VMat class to add a missing indicator for each variable.",
+  "For each variable with a missing value in the referenced train set, an indicator is added.\n"
+  "It is set to 1 if the value of the corresponding variable`in the underlying dataset is missing.\n"
+  "It is set to 0 otherwise.\n"
+  );
+
+MissingIndicatorVMatrix::MissingIndicatorVMatrix()
+: number_of_train_samples_to_use(0.0)
+{
+}
+
+MissingIndicatorVMatrix::MissingIndicatorVMatrix(VMat the_source, VMat the_train_set, real the_number_of_train_samples_to_use)
+{
+  source = the_source;
+  train_set = the_train_set;
+  number_of_train_samples_to_use = the_number_of_train_samples_to_use;
+}
+
+MissingIndicatorVMatrix::~MissingIndicatorVMatrix()
+{
+}
+
+void MissingIndicatorVMatrix::declareOptions(OptionList &ol)
+{
+  declareOption(ol, "source", &MissingIndicatorVMatrix::source, OptionBase::buildoption, 
+                "The source VMatrix with missing values.\n");
+
+  declareOption(ol, "train_set", &MissingIndicatorVMatrix::train_set, OptionBase::buildoption, 
+                "A referenced train set.\n"
+                "A missing indicator is added for variables with missing values in this data set.\n"
+                "It is used in combination with the option number_of_train_samples_to_use\n");
+
+  declareOption(ol, "number_of_train_samples_to_use", &MissingIndicatorVMatrix::number_of_train_samples_to_use, OptionBase::buildoption, 
+                "The number of samples from the train set that will be examined to see\n"
+                "if an indicator should be added for each variable\n");
+
+  inherited::declareOptions(ol);
+}
+
+void MissingIndicatorVMatrix::build()
+{
+  inherited::build();
+  build_();
+}
+
+void MissingIndicatorVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+  deepCopyField(source, copies);
+  deepCopyField(train_set, copies);
+  deepCopyField(number_of_train_samples_to_use, copies);
+  inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void MissingIndicatorVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
+{
+    source->getExample(i, source_input, target, weight);
+    new_col = 0;
+    for (int source_col = 0; source_col < source_inputsize; source_col++)
+    {
+      input[new_col] = source_input[source_col];
+      new_col += 1;
+      if (train_var_missing[source_col] > 0)
+      {
+          if (is_missing(source_input[source_col])) input[new_col] = 1.0;
+          else input[new_col] = 0.0;
+          new_col += 1;
+      }
+    }
+}
+
+real MissingIndicatorVMatrix::get(int i, int j) const
+{
+  if (source_rel_pos[j] < 0)
+  {
+    if (is_missing(source->get(i, source_rel_pos[j - 1]))) return 1.0;
+    else return 0.0;
+  }
+  return source->get(i, source_rel_pos[j]);
+}
+
+void MissingIndicatorVMatrix::put(int i, int j, real value)
+{
+  PLERROR("In MissingIndicatorVMatrix::put not implemented");
+}
+
+void MissingIndicatorVMatrix::getSubRow(int i, int j, Vec v) const
+{  
+  for (int source_col = j; source_col < j + v.length(); source_col++) v[source_col] = get(i, source_col);
+}
+
+void MissingIndicatorVMatrix::putSubRow(int i, int j, Vec v)
+{
+  PLERROR("In MissingIndicatorVMatrix::putSubRow not implemented");
+}
+
+void MissingIndicatorVMatrix::appendRow(Vec v)
+{
+  PLERROR("In MissingIndicatorVMatrix::appendRow not implemented");
+}
+
+void MissingIndicatorVMatrix::insertRow(int i, Vec v)
+{
+  PLERROR("In MissingIndicatorVMatrix::insertRow not implemented");
+}
+
+void MissingIndicatorVMatrix::getRow(int i, Vec v) const
+{  
+  for (int source_col = 0; source_col < width_; source_col++) v[source_col] = get(i, source_col); 
+}
+
+void MissingIndicatorVMatrix::putRow(int i, Vec v)
+{
+  PLERROR("In MissingIndicatorVMatrix::putRow not implemented");
+}
+
+void MissingIndicatorVMatrix::getColumn(int i, Vec v) const
+{
+  if (source_rel_pos[i] < 0) source->getColumn(source_rel_pos[i - 1], v);
+  else source->getColumn(source_rel_pos[i], v);
+  if (source_rel_pos[i] >= 0) return;
+  for (int source_row = 0; source_row < v->length(); source_row++)
+  {
+    if (is_missing(v[source_row])) v[source_row] = 1.0;
+    else v[source_row] = 0.0;
+  } 
+}
+
+void MissingIndicatorVMatrix::build_()
+{
+    if (!train_set || !source) PLERROR("In MissingIndicatorVMatrix::train set and source vmat must be supplied");
+    buildNewRecordFormat(); 
+}
+
+void MissingIndicatorVMatrix::buildNewRecordFormat()
+{
+    train_length = train_set->length();
+    if (number_of_train_samples_to_use > 0.0)
+        if (number_of_train_samples_to_use < 1.0) train_length = (int) (number_of_train_samples_to_use * (real) train_length);
+        else train_length = (int) number_of_train_samples_to_use;
+    if (train_length > train_set->length()) train_length = train_set->length();
+    if(train_length < 1) PLERROR("In MissingIndicatorVMatrix::length of the number of train samples to use must be at least 1, got: %i", train_length);
+    train_width = train_set->width();
+    train_targetsize = train_set->targetsize();
+    train_weightsize = train_set->weightsize();
+    train_inputsize = train_set->inputsize();
+    if(train_inputsize < 1) PLERROR("In MissingIndicatorVMatrix::inputsize of the train vmat must be supplied, got : %i", train_inputsize);
+    source_width = source->width();
+    source_targetsize = source->targetsize();
+    source_weightsize = source->weightsize();
+    source_inputsize = source->inputsize();
+    if (train_width != source_width) PLERROR("In MissingIndicatorVMatrix::train set and source width must agree, got : %i, %i", train_width, source_width);
+    if (train_targetsize != source_targetsize) PLERROR("In MissingIndicatorVMatrix::train set and source targetsize must agree, got : %i, %i", train_targetsize, source_targetsize);
+    if (train_weightsize != source_weightsize) PLERROR("In MissingIndicatorVMatrix::train set and source weightsize must agree, got : %i, %i", train_weightsize, source_weightsize);
+    if (train_inputsize != source_inputsize) PLERROR("In MissingIndicatorVMatrix::train set and source inputsize must agree, got : %i, %i", train_inputsize, source_inputsize);
+    train_input.resize(train_width);
+    train_var_missing.resize(train_inputsize);
+    train_var_missing.clear();
+    for (train_row = 0; train_row < train_length; train_row++)
+    {
+        train_set->getRow(train_row, train_input);
+        for (train_col = 0; train_col < train_inputsize; train_col++)
+        {
+            if (is_missing(train_input[train_col])) train_var_missing[train_col] = 1;
+        }
+    }
+    new_width = train_width;
+    new_inputsize = train_inputsize;
+    for (train_col = 0; train_col < train_inputsize; train_col++)
+    {
+        new_width += train_var_missing[train_col];
+        new_inputsize += train_var_missing[train_col];
+    }
+    train_field_names.resize(train_width);
+    source_rel_pos.resize(new_width);
+    new_field_names.resize(new_width);
+    train_field_names = train_set->fieldNames();
+    new_col = 0;
+    for (train_col = 0; train_col < train_inputsize; train_col++)
+    {
+      new_field_names[new_col] = train_field_names[train_col];
+      source_rel_pos[new_col] = train_col;
+      new_col += 1;
+      if (train_var_missing[train_col] > 0)
+      {
+          new_field_names[new_col] = train_field_names[train_col] + "_MI";
+          source_rel_pos[new_col] = -1;
+          new_col += 1;
+      }
+    }
+    for (train_col = train_inputsize; train_col < train_width; train_col++)
+    {
+      new_field_names[new_col] = train_field_names[train_col];
+      source_rel_pos[new_col] = train_col;
+      new_col += 1;
+    }
+    source_length = source->length();
+    length_ = source_length;
+    width_ = new_width;
+    inputsize_ = new_inputsize;
+    targetsize_ = source_targetsize;
+    weightsize_ = train_weightsize;
+    source_input.resize(source_inputsize);
+    declareFieldNames(new_field_names);
+}
+
+} // end of namespcae PLearn

Added: branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,126 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ******************************************************************      
+   * $Id: MissingIndicatorVMatrix.h 3658 2005-07-06 20:30:15  Godbout $
+   ****************************************************************** */
+
+/*! \file PLearnLibrary/PLearnCore/VMat.h */
+
+#ifndef MissingIndicatorVMatrix_INC
+#define MissingIndicatorVMatrix_INC
+
+#include <plearn/vmat/SourceVMatrix.h>
+#include <plearn/math/BottomNI.h>
+
+namespace PLearn {
+using namespace std;
+
+class MissingIndicatorVMatrix: public VMatrix
+{
+  typedef VMatrix inherited;
+  
+public:
+
+  //! The source VMatrix with missing values.
+  VMat         source;
+  
+  //! A referenced train set.
+  //! A missing indicator is added for variables with missing values in this data set.
+  //! It is used in combination with the option number_of_train_samples_to_use.
+  VMat         train_set;
+  
+  //! The number of samples from the train set that will be examined to see
+  //! if an indicator should be added for each variable.
+  real         number_of_train_samples_to_use;
+  
+
+                        MissingIndicatorVMatrix();
+                        MissingIndicatorVMatrix(VMat the_source, VMat the_train_set, real the_number_of_train_samples_to_use);
+  virtual               ~MissingIndicatorVMatrix();
+
+  static void           declareOptions(OptionList &ol);
+
+  virtual void          build();
+  virtual void          makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+  virtual void         getExample(int i, Vec& input, Vec& target, real& weight);
+  virtual real         get(int i, int j) const;
+  virtual void         put(int i, int j, real value);
+  virtual void         getSubRow(int i, int j, Vec v) const;
+  virtual void         putSubRow(int i, int j, Vec v);
+  virtual void         appendRow(Vec v);
+  virtual void         insertRow(int i, Vec v);  
+  virtual void         getRow(int i, Vec v) const;
+  virtual void         putRow(int i, Vec v);
+  virtual void         getColumn(int i, Vec v) const;
+
+private:
+  
+  int          train_length;
+  int          train_width;
+  int          train_inputsize;
+  int          train_targetsize;
+  int          train_weightsize;
+  int          train_row;
+  int          train_col;
+  Vec          train_input;
+  TVec<string> train_field_names;
+  TVec<int>    train_var_missing;
+  int          source_length;
+  int          source_width;
+  int          source_inputsize;
+  int          source_targetsize;
+  int          source_weightsize;
+  Vec          source_input;
+  TVec<int>    source_rel_pos;
+  int          new_width;
+  int          new_inputsize;
+  int          new_col;
+  TVec<string> new_field_names;
+
+          void         build_();
+          void         buildNewRecordFormat();
+  
+  PLEARN_DECLARE_OBJECT(MissingIndicatorVMatrix);
+
+};
+
+DECLARE_OBJECT_PTR(MissingIndicatorVMatrix);
+
+} // end of namespcae PLearn
+#endif

Added: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,540 @@
+// -*- C++ -*-
+
+// NeighborhoodConditionalMean.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file NeighborhoodConditionalMean.cc */
+
+#define PL_LOG_MODULE_NAME "NeighborhoodConditionalMean"
+#include <plearn/io/pl_log.h>
+
+#include "NeighborhoodConditionalMean.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    NeighborhoodConditionalMean,
+    "Computes correlation coefficient between various discrete values and the target.",
+    "name of the discrete variable, of the target and the values to check are options.\n"
+);
+
+/////////////////////////
+// NeighborhoodConditionalMean //
+/////////////////////////
+NeighborhoodConditionalMean::NeighborhoodConditionalMean()
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void NeighborhoodConditionalMean::declareOptions(OptionList& ol)
+{
+
+    declareOption(ol, "test_train_input_set", &NeighborhoodConditionalMean::test_train_input_set,
+                  OptionBase::buildoption,
+                  "The concatenated test and train input vectors with missing values.");
+    declareOption(ol, "test_train_target_set", &NeighborhoodConditionalMean::test_train_target_set,
+                  OptionBase::buildoption,
+                  "The corresponding target vectors.");
+    declareOption(ol, "number_of_test_samples", &NeighborhoodConditionalMean::number_of_test_samples,
+                  OptionBase::buildoption,
+                  "The number of test samples at the beginning of the test train concatenated sets.");
+    declareOption(ol, "number_of_train_samples", &NeighborhoodConditionalMean::number_of_train_samples,
+                  OptionBase::buildoption,
+                  "The number of train samples in the reference set to compute the % of missing.");
+    declareOption(ol, "target_field_names", &NeighborhoodConditionalMean::target_field_names,
+                  OptionBase::buildoption,
+                  "The vector of names of the field to select from the target_set as target for the built training files.");
+    declareOption(ol, "dir_offset", &NeighborhoodConditionalMean::dir_offset,
+                  OptionBase::buildoption,
+                  "The directory offset where to find and/or create the various files.");
+    declareOption(ol, "various_ks", &NeighborhoodConditionalMean::various_ks,
+                  OptionBase::buildoption,
+                  "The vector of various Ks to experiment with. Values must be between 1 and 100.");
+    declareOption(ol, "deletion_thresholds", &NeighborhoodConditionalMean::deletion_thresholds,
+                  OptionBase::buildoption,
+                  "The vector of thresholds to be tested for each of the various Ks.");
+    declareOption(ol, "experiment_name", &NeighborhoodConditionalMean::experiment_name,
+                  OptionBase::buildoption,
+                  "The name of the group of experiments to conduct.");
+    declareOption(ol, "missing_indicator_field_names", &NeighborhoodConditionalMean::missing_indicator_field_names,
+                  OptionBase::buildoption,
+                  "The field names of the missing indicators to exclude when we experiment without them.");
+    declareOption(ol, "experiment_template", &NeighborhoodConditionalMean::experiment_template,
+                  OptionBase::buildoption,
+                  "The template of the script to conduct the experiment.");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void NeighborhoodConditionalMean::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(test_train_input_set, copies);
+    deepCopyField(test_train_target_set, copies);
+    deepCopyField(number_of_test_samples, copies);
+    deepCopyField(number_of_train_samples, copies);
+    deepCopyField(target_field_names, copies);
+    deepCopyField(dir_offset, copies);
+    deepCopyField(various_ks, copies);
+    deepCopyField(deletion_thresholds, copies);
+    deepCopyField(experiment_name, copies);
+    deepCopyField(missing_indicator_field_names, copies);
+    deepCopyField(deletion_thresholds, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void NeighborhoodConditionalMean::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void NeighborhoodConditionalMean::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        for (int iteration = 1; iteration <= 1; iteration++)
+        {
+            cout << "In NeighborhoodConditionalMean, Iteration # " << iteration << endl;
+            computeNeighborhood();
+            experimentWithVariousKs();
+            train();
+        }
+        PLERROR("In NeighborhoodConditionalMean: we are done here");
+    }
+}
+
+void NeighborhoodConditionalMean::computeNeighborhood()
+{
+/*
+    prepare correlation based versions of datatset: we have to write a VMatrix for that
+    use the ball tree nearest neighbor to build a ball tree using train only, with unknown it would be too long
+    find the 100 nearest neighbors of samples in train and test in order from the closest to the furthest
+    now we can create a neighborhood imputation for K from 1 up to 100 averaging 
+    the observed values of the the k closest input vectors.
+    If there is no observed values in the k closest, we have to use something else:
+    mean of the covariance preservation imputationof the the k closest input vectors.
+*/
+    cout << "In NeighborhoodConditionalMean:" << endl;
+    cout << endl << "****** STEP 1 ******" << endl;
+    cout << "The first thing to do is to impute an initial value the the missing values in order to be able" << endl;
+    cout << "to compute distance between samples." << endl;
+    cout << "This step uses the CovariancePreservationVMatrix to do that." << endl;
+    cout << "The Covariance PreservationVMatrix creates a covariance_file in the metadata of the  source file" << endl;
+    cout << "if it is not already there." << endl;
+    cout << "The file is kept in train_imputed_with_covariance_preservation.pmat." << endl;
+    if (dir_offset != "") dir_offset += "/";
+    train_covariance_name = dir_offset + "train_imputed_with_covariance_preservation.pmat";
+    if (isfile(train_covariance_name))
+    {
+        train_covariance_file = new FileVMatrix(train_covariance_name);
+        train_covariance_file->defineSizes(train_covariance_file->width(), 0, 0);
+        cout << train_covariance_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_covariance_vmatrix = new CovariancePreservationImputationVMatrix();
+        train_covariance_vmatrix->source = train_set;
+        train_covariance_vmatrix->train_set = train_set;
+        train_covariance_vmatrix->build();
+        train_covariance_vmat = train_covariance_vmatrix;
+        train_covariance_file = new FileVMatrix(train_covariance_name, train_covariance_vmat->length(), train_covariance_vmat->fieldNames());
+        train_covariance_file->defineSizes(train_covariance_vmat->width(), 0, 0);
+        pb = new ProgressBar("Saving the train file imputed with the covariance preservation", train_covariance_vmat->length());
+        train_covariance_vector.resize(train_covariance_vmat->width());
+        for (int train_covariance_row = 0; train_covariance_row < train_covariance_vmat->length(); train_covariance_row++)
+        {
+            train_covariance_vmat->getRow(train_covariance_row, train_covariance_vector);
+            train_covariance_file->putRow(train_covariance_row, train_covariance_vector);
+            pb->update( train_covariance_row );
+        }
+        delete pb;
+    }
+    cout << endl << "****** STEP 2 ******" << endl;
+    cout << "We do the same thing with the test_train dataset" << endl;
+    cout << "using the covariance file created at the previous step." << endl;
+    cout << "The file is kept in test_train_imputed_with_covariance_preservation.pmat." << endl;
+    test_train_covariance_file_name = dir_offset + "test_train_imputed_with_covariance_preservation.pmat";
+    if (isfile(test_train_covariance_file_name))
+    {
+        test_train_covariance_file = new FileVMatrix(test_train_covariance_file_name);
+        test_train_covariance_file->defineSizes(test_train_covariance_file->width(), 0, 0);
+        cout << test_train_covariance_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_train_covariance_vmatrix = new CovariancePreservationImputationVMatrix();
+        test_train_covariance_vmatrix->source = test_train_input_set;
+        test_train_covariance_vmatrix->train_set = train_set;
+        test_train_covariance_vmatrix->build();
+        test_train_covariance_vmat = test_train_covariance_vmatrix;
+        test_train_covariance_file = new FileVMatrix(test_train_covariance_file_name, test_train_covariance_vmat->length(), test_train_covariance_vmat->fieldNames());
+        test_train_covariance_file->defineSizes(test_train_covariance_vmat->width(), 0, 0);
+        pb = new ProgressBar("Saving the test_train file imputed with the covariance preservation", test_train_covariance_vmat->length());
+        test_train_covariance_vector.resize(test_train_covariance_vmat->width());
+        for (int test_train_covariance_row = 0; test_train_covariance_row < test_train_covariance_vmat->length(); test_train_covariance_row++)
+        {
+            test_train_covariance_vmat->getRow(test_train_covariance_row, test_train_covariance_vector);
+            test_train_covariance_file->putRow(test_train_covariance_row, test_train_covariance_vector);
+            pb->update( test_train_covariance_row );
+        }
+        delete pb;
+    }
+    cout << endl << "****** STEP 3 ******" << endl;
+    cout << "We this initial imputation, we find the 100 nearest neighbors of each sample in the test_train dataset." << endl;
+    cout << "Their indexes are kept in the neighborhood_file of the test_train dataset metadata." << endl;
+    cout << "The BallTreeNearestNeighbors learner is used to build a tree with the train set" << endl;
+    cout << "in order to speed up the identification of the 100 nearest neighbors of the test_train dataset." << endl;
+    test_train_neighborhood_file_name = test_train_covariance_file_name + ".metadata/neighborhood_file.pmat";
+    if (isfile(test_train_neighborhood_file_name))
+    {
+        test_train_neighborhood_file = new FileVMatrix(test_train_neighborhood_file_name);
+        cout << test_train_neighborhood_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_train_neighborhood_learner = new BallTreeNearestNeighbors();
+        test_train_neighborhood_learner->setOption("rmin", "1");
+        test_train_neighborhood_learner->setOption("train_method", "anchor");
+        test_train_neighborhood_learner->setOption("num_neighbors", "100");
+        test_train_neighborhood_learner->setOption("copy_input", "0");
+        test_train_neighborhood_learner->setOption("copy_target", "0");
+        test_train_neighborhood_learner->setOption("copy_weight", "0");
+        test_train_neighborhood_learner->setOption("copy_index", "1");
+        test_train_neighborhood_learner->setOption("nstages", "-1");
+        test_train_neighborhood_learner->setOption("report_progress", "1");
+        test_train_neighborhood_learner->setTrainingSet(train_covariance_file, true);
+        test_train_neighborhood_learner->train();
+        test_train_neighborhood_file = new FileVMatrix(test_train_neighborhood_file_name, test_train_covariance_file->length(), 100);
+        test_train_covariance_vector.resize(test_train_covariance_file->width());
+        test_train_neighborhood_vector.resize(100);
+        pb = new ProgressBar("Saving the test_train file with the index of the 100 nearest neighbors", test_train_covariance_file->length());
+        for (int test_train_neighborhood_row = 0; test_train_neighborhood_row < test_train_covariance_file->length(); test_train_neighborhood_row++)
+        {
+            test_train_covariance_file->getRow(test_train_neighborhood_row, test_train_covariance_vector);
+            test_train_neighborhood_learner->computeOutput(test_train_covariance_vector, test_train_neighborhood_vector);
+            test_train_neighborhood_file->putRow(test_train_neighborhood_row, test_train_neighborhood_vector);
+            pb->update( test_train_neighborhood_row );
+        }
+        delete pb;
+    }
+}
+
+void NeighborhoodConditionalMean::experimentWithVariousKs()
+{
+/*
+    We control the experiments using a  master header file giving the status for each ks.
+    If the file is not there, we create it.
+    An experiment directory is created for each ks to eexperiment with various level 
+    of variable deletion.
+*/
+    cout << endl << "****** STEP 4 ******" << endl;
+    cout << "We now prepare experimentation at various levels of Ks, the number of neighbors between 1 and 100." << endl;
+    cout << "The first thing is to load the master header file from the test_train_imputed_with_covariance_preservation.pmat metadata." << endl;
+    cout << "If it is not there, the file is created." << endl;
+    train_set->lockMetaDataDir();
+    master_header_file_name = test_train_covariance_file_name + ".metadata";
+    master_header_file_name += "/Experiment/" + experiment_name + "/";
+    master_header_file_name += "neighborhood_header.pmat";
+    if (!isfile(master_header_file_name)) createMasterHeaderFile();
+    else getMasterHeaderRecords();
+    cout << "With the master header data, we can choose which K to experiment with." << endl;
+    for (master_header_row = 0; master_header_row < master_header_length; master_header_row++)
+    {
+        for (master_header_col = 0; master_header_col < master_header_width; master_header_col++)
+            if (master_header_records(master_header_row, master_header_col) <= 0.0) break;
+        if (master_header_col < master_header_width) break;
+    }
+    if (master_header_row >= master_header_length)
+    {
+        train_set->unlockMetaDataDir();
+        //reviewGlobalStats();
+        PLERROR("In NeighborhoodConditionalMean: we are done here");
+    }
+    to_deal_with_k = various_ks[master_header_col];
+    to_deal_with_target = target_field_names[master_header_row / 2];
+    to_deal_with_ind = master_header_row % 2;
+    cout << "Next target to deal with: " << to_deal_with_target << endl;
+    cout << "Next experiment missing indicator: " << to_deal_with_ind << endl;
+    cout << "Next k (number of neighbors) to experiment with: " << to_deal_with_k << endl;
+    updateMasterHeaderRecords(master_header_row, master_header_col);
+    train_set->unlockMetaDataDir();
+    cout << endl << "****** STEP 5 ******" << endl;
+    cout << "We perform the imputaton with the selected number of neighbors." << endl;
+    cout << "The resulting file is loaded in memory to be passed to the experimentation script." << endl;
+    test_train_neighbor_imputation_vmatrix = new NeighborhoodImputationVMatrix();
+    test_train_neighbor_imputation_vmatrix->source_with_missing = test_train_input_set;
+    test_train_neighbor_imputation_vmatrix->reference_index = test_train_neighborhood_file;
+    test_train_neighbor_imputation_vmatrix->reference_with_missing = train_set;
+    test_train_neighbor_imputation_vmatrix->reference_with_covariance_preserved = train_covariance_file;
+    test_train_neighbor_imputation_vmatrix->number_of_neighbors = to_deal_with_k;
+    test_train_neighbor_imputation_vmatrix->build();
+    test_train_neighbor_imputation_vmat = test_train_neighbor_imputation_vmatrix;
+    test_train_neighbor_imputation_file = new MemoryVMatrix(test_train_neighbor_imputation_vmat->length(), test_train_neighbor_imputation_vmat->width());
+    test_train_neighbor_imputation_file->defineSizes(test_train_neighbor_imputation_vmat->width(), 0, 0);
+    test_train_neighbor_imputation_file->declareFieldNames(test_train_neighbor_imputation_vmat->fieldNames());
+    test_train_neighbor_imputation_vector.resize(test_train_neighbor_imputation_vmat->width());
+    pb = new ProgressBar("Loading the test_train file imputed with the selected # of neighbors", test_train_neighbor_imputation_vmat->length());
+    for (int  test_train_neighbor_imputation_row = 0;
+              test_train_neighbor_imputation_row < test_train_neighbor_imputation_vmat->length();
+              test_train_neighbor_imputation_row++)
+    {
+        test_train_neighbor_imputation_vmat->getRow(test_train_neighbor_imputation_row, test_train_neighbor_imputation_vector);
+        test_train_neighbor_imputation_file->putRow(test_train_neighbor_imputation_row, test_train_neighbor_imputation_vector);
+        pb->update( test_train_neighbor_imputation_row );
+    }
+     //       ::PLearn::save(header_expdir + "/" + deletion_threshold_str + "/source_names.psave", source_names);
+    delete pb;
+    cout << endl << "****** STEP 6 ******" << endl;
+    cout << "We are now ready to launch the experimentation for this k." << endl;
+    cout << "The Experimentation program will build learners for the specified deletion thresholds." << endl;
+    experimentation_learner = new Experimentation();
+    experimentation_learner->save_files = 0;
+    experimentation_learner->experiment_without_missing_indicator = to_deal_with_ind;
+    experimentation_learner->target_field_name = to_deal_with_target;
+    experimentation_learner->missing_indicator_field_names = missing_indicator_field_names;
+    experimentation_learner->experiment_name = experiment_name;
+    experimentation_learner->number_of_test_samples = number_of_test_samples;
+    experimentation_learner->number_of_train_samples = number_of_train_samples;
+    experimentation_learner->reference_train_set = train_set;
+    experimentation_learner->target_set = test_train_target_set;
+    experimentation_learner->experiment_template = experiment_template;
+    experimentation_learner->deletion_thresholds = deletion_thresholds;
+    experimentation_learner->experiment_directory = test_train_covariance_file_name + ".metadata";
+    experimentation_learner->experiment_directory += "/Experiment/" + experiment_name + "/";
+    experimentation_learner->experiment_directory += "K_" + tostring(to_deal_with_k);
+    experimentation_learner->setTrainingSet(test_train_neighbor_imputation_file);
+}
+
+void NeighborhoodConditionalMean::createMasterHeaderFile()
+{
+    master_header_length = target_field_names.length() * 2;
+    master_header_width = various_ks.length();
+    master_header_names.resize(master_header_width);
+    master_header_records.resize(master_header_length, master_header_width);
+    master_header_records.clear();
+    for (master_header_col = 0; master_header_col < master_header_width; master_header_col++)
+        master_header_names[master_header_col] = "K_" + tostring(master_header_col);
+    master_header_file = new FileVMatrix(master_header_file_name, master_header_length, master_header_names);
+    for (master_header_row = 0; master_header_row < master_header_length; master_header_row++)
+        for (master_header_col = 0; master_header_col < master_header_width; master_header_col++)
+            master_header_file->put(master_header_row, master_header_col, 0.0);
+}
+void NeighborhoodConditionalMean::getMasterHeaderRecords()
+{ 
+    master_header_file = new FileVMatrix(master_header_file_name, true);
+    master_header_length = master_header_file->length();
+    master_header_width = master_header_file->width();
+    if (master_header_length != target_field_names.length() * 2)
+        PLERROR("In NeighborhoodConditionalMean: master header file length and target_field_names do not agree");
+    if (master_header_width != various_ks.length())
+        PLERROR("In NeighborhoodConditionalMean: master header file width and various_ks do not agree");
+    master_header_records.resize(master_header_length, master_header_width);
+    for (master_header_row = 0; master_header_row < master_header_length; master_header_row++)
+        for (master_header_col = 0; master_header_col < master_header_width; master_header_col++)
+            master_header_records(master_header_row, master_header_col) = master_header_file->get(master_header_row, master_header_col);
+}
+
+void NeighborhoodConditionalMean::updateMasterHeaderRecords(int row, int col)
+{
+    master_header_records(row, col) += 1.0;
+    master_header_file->put(row, col, master_header_records(row, col));
+    master_header_file->flush();
+}
+
+/*
+void NeighborhoodConditionalMean::createHeaderFile()
+{ 
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        targeted_stats = targeted_set->getStats(main_col);
+        targeted_missing = targeted_stats.nmissing();
+        main_stats = train_set->getStats(main_col);
+        main_total = main_stats.n();
+        main_missing = main_stats.nmissing();
+        main_present = main_total - main_missing;
+        if (fields_selected[main_col] < 1) header_record[main_col] = 1;                  // delete column, field not selected
+        else if (targeted_missing <= 0) header_record[main_col] = 0;                     // nothing to do
+        else if (main_present < min_number_of_samples) header_record[main_col] = 1;      // delete column
+        else header_record[main_col] = 2;                                                // build tree
+    }
+    header_file = new FileVMatrix(header_file_name, 1, main_names);
+    header_file->putRow(0, header_record);
+}
+
+void NeighborhoodConditionalMean::getHeaderRecord()
+{ 
+    header_file = new FileVMatrix(header_file_name, true);
+    header_file->getRow(0, header_record);
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (header_record[main_col] == 0) continue;
+        if (header_record[main_col] == 2) continue;
+        if (header_record[main_col] == 1 && fields_selected[main_col] < 1) continue;
+        if (header_record[main_col] == 1)
+        {
+            main_stats = train_set->getStats(main_col);
+            main_total = main_stats.n();
+            main_missing = main_stats.nmissing();
+            main_present = main_total - main_missing;
+            if (main_present >= min_number_of_samples) header_record[main_col] = 2;
+            continue;
+        }
+    }
+}
+
+void NeighborhoodConditionalMean::updateHeaderRecord(int var_col)
+{ 
+    header_file->put(0, var_col, 3.0);
+}
+
+void NeighborhoodConditionalMean::reviewGlobalStats()
+{ 
+    cout << "There is no more variable to deal with." << endl;
+    for (main_col = 0; main_col < main_width; main_col++)
+    {
+        if (header_record[main_col] == 0)
+        { 
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : no missing values for this variable in the targeted files." << endl;
+            continue;
+        }
+        if (header_record[main_col] == 1 && fields_selected[main_col] < 1)
+        {
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : field not selected." << endl;
+            continue;
+        }
+        if (header_record[main_col] == 1)
+        {
+            main_stats = train_set->getStats(main_col);
+            main_total = main_stats.n();
+            main_missing = main_stats.nmissing();
+            main_present = main_total - main_missing;
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : field deleted, only " << setw(6) << main_present << " records to train with." << endl;
+            continue;
+        }
+        results_file_name = targeted_metadata + "/TreeCondMean/dir/" + main_names[main_col] + "/Split0/LearnerExpdir/Strat0results.pmat";
+        if (!isfile(results_file_name))
+        {
+            header_file->put(0, main_col, 2.0);
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : missing results file." << endl;
+            continue;
+        }
+        test_output_file_name = targeted_metadata + "/TreeCondMean/dir/" + main_names[main_col] + "/Split0/test1_outputs.pmat";
+        if (!isfile(test_output_file_name))
+        {
+            header_file->put(0, main_col, 2.0);
+            cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+            cout << " : missing test output file." << endl;
+            continue;
+        }
+        results_file = new FileVMatrix(results_file_name);
+        results_length = results_file->length();
+        results_nstages = results_file->get(results_length - 1, 2);
+        results_mse = results_file->get(results_length - 1, 6);
+        results_std_err = results_file->get(results_length - 1, 7);
+        test_output_file = new FileVMatrix(test_output_file_name);
+        test_output_length = test_output_file->length();
+        cout << setiosflags(ios::left) << setw(30) << main_names[main_col];
+        cout << " : tree built with " << setw(2) << (int) results_nstages << " leaves, "
+             << setw(6) << test_output_length << " test output records found, "
+             << "performance: " << setiosflags(ios::fixed) << setprecision(4) << results_mse
+             << " +/- " << setiosflags(ios::fixed) << setprecision(4) << results_std_err << endl;
+    }
+}
+*/
+
+void NeighborhoodConditionalMean::train()
+{
+/*
+    PP<ExplicitSplitter> explicit_splitter = new ExplicitSplitter();
+    explicit_splitter->splitsets.resize(1,2);
+    explicit_splitter->splitsets(0,0) = output_file;
+    explicit_splitter->splitsets(0,1) = train_test_file;
+    cond_mean = ::PLearn::deepCopy(cond_mean_template);
+    cond_mean->setOption("expdir", targeted_metadata + "/TreeCondMean/dir/" + to_deal_with_name);
+    cond_mean->splitter = new ExplicitSplitter();
+    cond_mean->splitter = explicit_splitter;
+    cond_mean->build();
+    Vec results = cond_mean->perform(true);
+*/
+}
+
+int NeighborhoodConditionalMean::outputsize() const {return 0;}
+void NeighborhoodConditionalMean::computeOutput(const Vec&, Vec&) const {}
+void NeighborhoodConditionalMean::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> NeighborhoodConditionalMean::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> NeighborhoodConditionalMean::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,252 @@
+// -*- C++ -*-
+
+// NeighborhoodConditionalMean.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file NeighborhoodConditionalMean.h */
+
+
+#ifndef NeighborhoodConditionalMean_INC
+#define NeighborhoodConditionalMean_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/testers/PTester.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/vmat/MemoryVMatrix.h>
+#include <plearn/io/load_and_save.h>          //!<  For save
+#include <plearn/io/fileutils.h>              //!<  For isfile()
+#include <plearn/math/random.h>               //!<  For the seed stuff.
+#include <plearn/vmat/ExplicitSplitter.h>     //!<  For the splitter stuff.
+#include <plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h>
+#include <plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h>
+#include <plearn_learners/second_iteration/BallTreeNearestNeighbors.h>
+#include <plearn_learners/second_iteration/Experimentation.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class NeighborhoodConditionalMean : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    
+    //! The concatenated test and train input vectors with missing values.
+    VMat test_train_input_set;
+    //! The corresponding target vectors.
+    VMat test_train_target_set;
+    //! The number of test samples at the beginning of the test train concatenated sets.
+    int number_of_test_samples;
+    //! The number of train samples in the reference set to compute the % of missing.
+    int number_of_train_samples;
+    //! The vector of names of the field to select from the target_set as target for the built training files.
+    TVec<string> target_field_names;
+    //! The directory offset where to find and/or create the various files.
+    string dir_offset;
+    //! The vector of various Ks to experiment with. Values must be between 1 and 100.
+    TVec<int> various_ks;
+    //! The vector of thresholds to be tested for each of the various Ks.
+    Vec deletion_thresholds;
+    //! The name of the group of experiments to conduct.
+    string experiment_name;
+    //!  The field names of the missing indicators to exclude when we experiment without them.
+    TVec< string > missing_indicator_field_names;
+    //! The template of the script to conduct the experiment.
+    PP< PTester > experiment_template;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    NeighborhoodConditionalMean();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(NeighborhoodConditionalMean);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void computeNeighborhood();
+    void experimentWithVariousKs();
+    void createMasterHeaderFile();
+    void getMasterHeaderRecords();
+    void updateMasterHeaderRecords(int row, int col);
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    ProgressBar*                              pb;
+    PPath                                     train_covariance_name;
+    VMat                                      train_covariance_file;
+    CovariancePreservationImputationVMatrix*  train_covariance_vmatrix;
+    VMat                                      train_covariance_vmat;
+    Vec                                       train_covariance_vector;
+    PPath                                     test_train_covariance_file_name;
+    VMat                                      test_train_covariance_file;
+    CovariancePreservationImputationVMatrix*  test_train_covariance_vmatrix;
+    VMat                                      test_train_covariance_vmat;
+    Vec                                       test_train_covariance_vector;
+    PPath                                     test_train_neighborhood_file_name;
+    BallTreeNearestNeighbors*                 test_train_neighborhood_learner;
+    VMat                                      test_train_neighborhood_file;
+    Vec                                       test_train_neighborhood_vector;
+    PPath                                     master_header_file_name;
+    VMat                                      master_header_file;
+    int                                       master_header_length;
+    int                                       master_header_width;
+    int                                       master_header_row;
+    int                                       master_header_col;
+    TVec<string>                              master_header_names;
+    Mat                                       master_header_records;
+    int                                       to_deal_with_k;
+    string                                    to_deal_with_target;
+    int                                       to_deal_with_ind;
+    NeighborhoodImputationVMatrix*            test_train_neighbor_imputation_vmatrix;
+    VMat                                      test_train_neighbor_imputation_vmat;
+    VMat                                      test_train_neighbor_imputation_file;
+    Vec                                       test_train_neighbor_imputation_vector;
+    Experimentation*                          experimentation_learner;
+    
+ /*   
+    int main_row;
+    int main_col;
+    int main_length;
+    int main_width;
+    Vec main_input;
+    TVec<string> main_names;
+    StatsCollector  main_stats;
+    PPath main_metadata;
+    TVec<int> main_ins;
+    real main_total;
+    real main_missing;
+    real main_present;
+    int targeted_length;
+    int targeted_width;
+    Vec targeted_input;
+    TVec<string> targeted_names;
+    StatsCollector  targeted_stats;
+    PPath targeted_metadata;
+    real targeted_missing;
+    PPath header_file_name;
+    VMat header_file;
+    Vec header_record;
+    int fields_col;
+    int fields_width;
+    TVec<int> fields_selected;
+    int to_deal_with_total;
+    int to_deal_with_next;
+    real to_deal_with_value;
+    string to_deal_with_name;
+    int ind_next;
+    int output_length;
+    int output_width;
+    int output_col;
+    string output_path;
+    TVec<string> output_names;
+    Vec output_vec;
+    TVec<int> output_variable_src;
+    VMat output_file;
+    int train_test_length;
+    string train_test_path;
+    TVec<int> train_test_variable_src;
+    VMat train_test_file;
+    PP<PTester> cond_mean;
+    PPath results_file_name;
+    VMat results_file;
+    int results_length;
+    real results_nstages;
+    real results_mse;
+    real results_std_err;
+    PPath test_output_file_name;
+    VMat test_output_file;
+    int test_output_length;
+*/    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(NeighborhoodConditionalMean);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,233 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************************    
+   * $Id: NeighborhoodImputationVMatrix.cc 3658 2005-07-06 20:30:15  Godbout $
+   ******************************************************************* */
+
+
+#include "NeighborhoodImputationVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+/** NeighborhoodImputationVMatrix **/
+
+PLEARN_IMPLEMENT_OBJECT(
+  NeighborhoodImputationVMatrix,
+  "VMat class to impute the observed variable mean to replace missing values in the source matrix.",
+  "This class will replace missing values in the underlying dataset with the mean, median or mode observed on the train set.\n"
+  "The imputed value is based on the imputation instruction option.\n"
+  );
+
+NeighborhoodImputationVMatrix::NeighborhoodImputationVMatrix()
+{
+}
+
+NeighborhoodImputationVMatrix::~NeighborhoodImputationVMatrix()
+{
+}
+
+void NeighborhoodImputationVMatrix::declareOptions(OptionList &ol)
+{
+  declareOption(ol, "source_with_missing", &NeighborhoodImputationVMatrix::source_with_missing, OptionBase::buildoption, 
+                "The source VMatrix with missing values.\n");
+
+  declareOption(ol, "reference_index", &NeighborhoodImputationVMatrix::reference_index, OptionBase::buildoption, 
+                "The set of pre-computed neighbors index.\n"
+                "his can be done with BallTreeNearestNeighbors.\n");
+
+  declareOption(ol, "reference_with_missing", &NeighborhoodImputationVMatrix::reference_with_missing, OptionBase::buildoption, 
+                "The reference set corresponding to the pre-computed index with missing values.");
+      
+  declareOption(ol, "reference_with_covariance_preserved", &NeighborhoodImputationVMatrix::reference_with_covariance_preserved, OptionBase::buildoption, 
+                "The reference set corresponding to the pre-computed index with the initial imputations.");
+
+  declareOption(ol, "number_of_neighbors", &NeighborhoodImputationVMatrix::number_of_neighbors, OptionBase::learntoption,
+                "This is usually called K, the number of neighbors to consider.\n"   
+                "It must be less or equal than the with of the reference index.");
+
+  inherited::declareOptions(ol);
+}
+
+void NeighborhoodImputationVMatrix::build()
+{
+  inherited::build();
+  build_();
+}
+
+void NeighborhoodImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+  deepCopyField(source_with_missing, copies);
+  deepCopyField(reference_index, copies);
+  deepCopyField(reference_with_missing, copies);
+  deepCopyField(reference_with_covariance_preserved, copies);
+  deepCopyField(number_of_neighbors, copies);
+  inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void NeighborhoodImputationVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
+{
+  source_with_missing->getExample(i, input, target, weight);
+  for (int source_col = 0; source_col < input->length(); source_col++)
+  {
+    if (is_missing(input[source_col])) input[source_col] = impute(i, source_col);
+  }  
+}
+
+real NeighborhoodImputationVMatrix::get(int i, int j) const
+{ 
+  real variable_value = source_with_missing->get(i, j);
+  if (!is_missing(variable_value)) return variable_value;
+  return impute(i, j);
+}
+
+void NeighborhoodImputationVMatrix::put(int i, int j, real value)
+{
+  PLERROR("In NeighborhoodImputationVMatrix::put not implemented");
+}
+
+void NeighborhoodImputationVMatrix::getSubRow(int i, int j, Vec v) const
+{  
+  source_with_missing->getSubRow(i, j, v);
+  for (int source_col = 0; source_col < v->length(); source_col++) 
+    if (is_missing(v[source_col])) v[source_col] = impute(i, source_col + j);
+}
+
+void NeighborhoodImputationVMatrix::putSubRow(int i, int j, Vec v)
+{
+  PLERROR("In NeighborhoodImputationVMatrix::putSubRow not implemented");
+}
+
+void NeighborhoodImputationVMatrix::appendRow(Vec v)
+{
+  PLERROR("In NeighborhoodImputationVMatrix::appendRow not implemented");
+}
+
+void NeighborhoodImputationVMatrix::insertRow(int i, Vec v)
+{
+  PLERROR("In NeighborhoodImputationVMatrix::insertRow not implemented");
+}
+
+void NeighborhoodImputationVMatrix::getRow(int i, Vec v) const
+{  
+  source_with_missing-> getRow(i, v);
+  for (int source_col = 0; source_col < v->length(); source_col++)
+    if (is_missing(v[source_col])) v[source_col] = impute(i, source_col); 
+}
+
+void NeighborhoodImputationVMatrix::putRow(int i, Vec v)
+{
+  PLERROR("In NeighborhoodImputationVMatrix::putRow not implemented");
+}
+
+void NeighborhoodImputationVMatrix::getColumn(int i, Vec v) const
+{  
+  source_with_missing-> getColumn(i, v);
+  for (int source_row = 0; source_row < v->length(); source_row++)
+    if (is_missing(v[source_row])) v[source_row] = impute(source_row, i);
+}
+
+void NeighborhoodImputationVMatrix::build_()
+{
+    if (!source_with_missing)                 PLERROR("In NeighborhoodImputationVMatrix::source with missing set must be supplied");
+    if (!reference_index)                     PLERROR("In NeighborhoodImputationVMatrix::reference index set must be supplied");
+    if (!reference_with_missing)              PLERROR("In NeighborhoodImputationVMatrix::reference with missing set must be supplied");
+    if (!reference_with_covariance_preserved) PLERROR("In NeighborhoodImputationVMatrix::reference with covariance preserved must be supplied");
+    src_length = source_with_missing->length();
+    if (src_length != reference_index->length())
+        PLERROR("In NeighborhoodImputationVMatrix::length of the source and its index must agree, got: %i - %i", src_length, reference_index->length());
+    ref_length = reference_with_missing->length();
+    if (ref_length != reference_with_covariance_preserved->length())
+        PLERROR("In NeighborhoodImputationVMatrix::length of the reference set with missing and with covariance preserved must agree, got: %i - %i",
+                ref_length, reference_with_covariance_preserved->length());
+    src_width = source_with_missing->width();
+    if (src_width != reference_with_missing->width())
+        PLERROR("In NeighborhoodImputationVMatrix::width of the source and the reference with missing must agree, got: %i - %i",
+                src_width, reference_with_missing->width());
+    if (src_width != reference_with_covariance_preserved->width())
+        PLERROR("In NeighborhoodImputationVMatrix::width of the source and the reference with missing must agree, got: %i - %i",
+                src_width, reference_with_covariance_preserved->width());
+    if (number_of_neighbors < 1)
+        PLERROR("In NeighborhoodImputationVMatrix::the index must contains at least as many reference as the specified number of neighbors, got: %i - %i",
+                number_of_neighbors, reference_index->width());
+    if (number_of_neighbors > reference_index->width())
+        PLERROR("In NeighborhoodImputationVMatrix::the index must contains at least as many reference as the specified number of neighbors, got: %i - %i",
+                number_of_neighbors, reference_index->width());
+    ref_idx.resize(reference_index->length(), reference_index->width());
+    ref_idx = reference_index->toMat();
+    ref_mis.resize(reference_with_missing->length(), reference_with_missing->width());
+    ref_mis = reference_with_missing->toMat();
+    ref_cov.resize(reference_with_covariance_preserved->length(), reference_with_covariance_preserved->width());
+    ref_cov = reference_with_covariance_preserved->toMat();
+    length_ = src_length;
+    width_ = src_width;
+    inputsize_ = source_with_missing->inputsize();
+    targetsize_ = source_with_missing->targetsize();
+    weightsize_ = source_with_missing->weightsize();
+    declareFieldNames(source_with_missing->fieldNames());
+}
+real NeighborhoodImputationVMatrix::impute(int i, int j) const
+{
+    int ref_idx_col;
+    int ref_row;
+    real return_value = 0.0;
+    real value_count = 0.0;
+    for (ref_idx_col = 0; ref_idx_col < number_of_neighbors; ref_idx_col++)
+    {
+        ref_row = (int) ref_idx(i, ref_idx_col);
+        if (ref_row < 0 || ref_row >= ref_length)
+            PLERROR("In NeighborhoodImputationVMatrix::invalid index reference, got: %i for sample %i", ref_row, i);
+        if (is_missing(ref_mis(ref_row, j))) continue;
+        return_value += ref_mis(ref_row, j);
+        value_count += 1.0;
+    }
+    if (value_count > 0) return return_value / value_count;
+    return_value = 0.0;
+    value_count = 0.0;
+    for (ref_idx_col = 0; ref_idx_col < number_of_neighbors; ref_idx_col++)
+    {
+        ref_row = (int) ref_idx(i, ref_idx_col);
+        if (is_missing(ref_cov(ref_row, j)))
+            PLERROR("In NeighborhoodImputationVMatrix::missing value found in the reference with covariance preserved at: %i , %i", ref_row, j);
+        return_value += ref_cov(ref_row, j);
+        value_count += 1.0;
+    }
+    return return_value / value_count;
+}
+
+} // end of namespcae PLearn

Added: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,117 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ******************************************************************      
+   * $Id: NeighborhoodImputationVMatrix.h 3658 2005-07-06 20:30:15  Godbout $
+   ****************************************************************** */
+
+/*! \file PLearnLibrary/PLearnCore/VMat.h */
+
+#ifndef NeighborhoodImputationVMatrix_INC
+#define NeighborhoodImputationVMatrix_INC
+
+#include <plearn/vmat/SourceVMatrix.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/io/fileutils.h>                     //!<  For isfile()
+#include <plearn/math/BottomNI.h>
+
+namespace PLearn {
+using namespace std;
+
+class NeighborhoodImputationVMatrix: public VMatrix
+{
+  typedef VMatrix inherited;
+  
+public:
+
+  //! The source VMatrix with missing values.
+  VMat                          source_with_missing;
+  
+  //! The set of pre-computed neighbors index.
+  //! This can be done with BallTreeNearestNeighbors.
+  VMat                          reference_index;
+  
+  //! The reference set corresponding to the pre-computed index with missing values.
+  VMat                          reference_with_missing;
+  
+  //! The reference set corresponding to the pre-computed index with the initial imputations.
+  VMat                          reference_with_covariance_preserved;
+   
+  //! This is usually called K, the number of neighbors to consider.
+  //! It must be less or equal than the with of the reference index.
+  int                           number_of_neighbors;
+  
+
+                        NeighborhoodImputationVMatrix();
+  virtual               ~NeighborhoodImputationVMatrix();
+
+  static void           declareOptions(OptionList &ol);
+
+  virtual void          build();
+  virtual void          makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+  virtual void         getExample(int i, Vec& input, Vec& target, real& weight);
+  virtual real         get(int i, int j) const;
+  virtual void         put(int i, int j, real value);
+  virtual void         getSubRow(int i, int j, Vec v) const;
+  virtual void         putSubRow(int i, int j, Vec v);
+  virtual void         appendRow(Vec v);
+  virtual void         insertRow(int i, Vec v);  
+  virtual void         getRow(int i, Vec v) const;
+  virtual void         putRow(int i, Vec v);
+  virtual void         getColumn(int i, Vec v) const;
+
+private:
+  
+          int          src_length;
+          int          src_width;
+          int          ref_length;
+          Mat          ref_idx;
+          Mat          ref_mis;
+          Mat          ref_cov;
+
+          void         build_();
+          real         impute(int i, int j) const;
+  
+  PLEARN_DECLARE_OBJECT(NeighborhoodImputationVMatrix);
+
+};
+
+DECLARE_OBJECT_PTR(NeighborhoodImputationVMatrix);
+
+} // end of namespcae PLearn
+#endif

Added: branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,990 @@
+// -*- C++ -*-
+
+// Preprocessing.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file Preprocessing.cc */
+
+#define PL_LOG_MODULE_NAME "Preprocessing"
+#include <plearn/io/pl_log.h>
+
+#include "Preprocessing.h"
+#include <plearn/io/load_and_save.h>                 //!<  For save
+#include <plearn/io/fileutils.h>                     //!<  For isfile()
+#include <plearn/math/random.h>                      //!<  For the seed stuff.
+#include <plearn/vmat/ExplicitSplitter.h>            //!<  For the splitter stuff.
+#include <plearn/vmat/VariableDeletionVMatrix.h>     //!<  For the new_set stuff.
+#include <plearn/vmat/BootstrapVMatrix.h>            //!<  For the shuffle stuff.
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    Preprocessing,
+    "Computes correlation coefficient between various discrete values and the target.",
+    "name of the discrete variable, of the target and the values to check are options.\n"
+);
+
+/////////////////////////
+// Preprocessing //
+/////////////////////////
+Preprocessing::Preprocessing()
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void Preprocessing::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "test_set", &Preprocessing::test_set,
+                  OptionBase::buildoption,
+                  "The test data set.\n");
+    declareOption(ol, "unknown_set", &Preprocessing::unknown_set,
+                  OptionBase::buildoption,
+                  "The unknown data set.\n");
+    declareOption(ol, "compute_target_learner_template", &Preprocessing::compute_target_learner_template,
+                  OptionBase::buildoption,
+                  "The template of the script to generate the class target.\n");
+    declareOption(ol, "fix_binary_variables_template", &Preprocessing::fix_binary_variables_template,
+                  OptionBase::buildoption,
+                  "The template of the script to fix the binary variables.\n");
+    declareOption(ol, "imputation_spec", &Preprocessing::imputation_spec,
+                  OptionBase::buildoption,
+                  "Pairs of instruction of the form field_name : mean | median | mode.\n");
+    declareOption(ol, "discrete_variable_instructions", &Preprocessing::discrete_variable_instructions,
+                  OptionBase::buildoption,
+                  "The instructions to dichotomize the variables in the form of field_name : TVec<pair>.\n"
+                  "The pairs are values from : to, each creating a 0, 1 variable.\n"
+                  "Variables with no specification will be kept as_is.\n");
+    declareOption(ol, "selected_variables_for_input", &Preprocessing::selected_variables_for_input,
+                  OptionBase::buildoption,
+                  "The list of variables selected as input vector.\n");
+    declareOption(ol, "selected_variables_for_target", &Preprocessing::selected_variables_for_target,
+                  OptionBase::buildoption,
+                  "The list of variables selected as target vector.\n");
+    declareOption(ol, "inputs_excluded_from_gaussianization", &Preprocessing::inputs_excluded_from_gaussianization,
+                  OptionBase::buildoption,
+                  "The list of input variables excluded from the gaussianization step.\n");
+    declareOption(ol, "targets_excluded_from_gaussianization", &Preprocessing::targets_excluded_from_gaussianization,
+                  OptionBase::buildoption,
+                  "The list of target variables excluded from the gaussianization step.\n");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void Preprocessing::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(test_set, copies);
+    deepCopyField(unknown_set, copies);
+    deepCopyField(compute_target_learner_template, copies);
+    deepCopyField(fix_binary_variables_template, copies);
+    deepCopyField(imputation_spec, copies);
+    deepCopyField(discrete_variable_instructions, copies);
+    deepCopyField(selected_variables_for_input, copies);
+    deepCopyField(selected_variables_for_target, copies);
+    deepCopyField(inputs_excluded_from_gaussianization, copies);
+    deepCopyField(targets_excluded_from_gaussianization, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void Preprocessing::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void Preprocessing::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        manageTrainTestUnknownSets();
+        PLERROR("In Preprocessing: Everything completed successfuly, we are done here");
+    }
+}
+
+void Preprocessing::manageTrainTestUnknownSets()
+{
+
+    // defining all the variables for the train set
+    PPath                                 output_path;
+    PPath                                 train_with_class_target_file_name;
+    VMat                                  train_with_class_target_file;
+    PP<ComputeDond2Target>                compute_target_learner;
+    BootstrapVMatrix*                     train_shufffled_vmatrix;
+    VMat                                  train_shuffled_file;
+    PPath                                 train_with_binary_fixed_file_name;
+    VMat                                  train_with_binary_fixed_file;
+    PP<FixDond2BinaryVariables>           fix_binary_variables_learner;
+    PPath                                 train_with_ind_file_name;
+    MissingIndicatorVMatrix*              train_with_ind_vmatrix;
+    VMat                                  train_with_ind_vmat;
+    VMat                                  train_with_ind_file;
+    Vec                                   train_with_ind_vector;
+    MeanMedianModeImputationVMatrix*      train_with_imp_vmatrix;
+    VMat                                  mean_median_mode_with_ind_file;
+    PPath                                 train_with_dichotomies_file_name;
+    VMat                                  train_with_dichotomies_file;
+    PPath                                 mean_median_mode_with_dichotmies_file_name;
+    VMat                                  mean_median_mode_with_dichotmies_file;
+    PP<DichotomizeDond2DiscreteVariables> dichotomize_discrete_variables_learner;
+    SelectColumnsVMatrix*                 train_with_selected_columns_vmatrix;
+    VMat                                  train_with_selected_columns_vmat;
+    SelectColumnsVMatrix*                 mean_median_mode_with_selected_columns_vmatrix;
+    VMat                                  mean_median_mode_with_selected_columns_vmat;
+    GaussianizeVMatrix*                   train_gaussianized_vmatrix;
+    VMat                                  train_gaussianized_vmat;
+    GaussianizeVMatrix*                   mean_median_mode_gaussianized_vmatrix;
+    VMat                                  mean_median_mode_gaussianized_vmat;
+    PPath                                 train_input_preprocessed_file_name;
+    VMat                                  train_input_preprocessed_file;
+    Vec                                   train_input_preprocessed_vector;
+    PPath                                 mean_median_mode_input_preprocessed_file_name;
+    VMat                                  mean_median_mode_input_preprocessed_file;
+    Vec                                   mean_median_mode_input_preprocessed_vector;
+    SelectColumnsVMatrix*                 train_target_with_selected_columns_vmatrix;
+    VMat                                  train_target_with_selected_columns_vmat;
+    GaussianizeVMatrix*                   train_target_gaussianized_vmatrix;
+    VMat                                  train_target_gaussianized_vmat;
+    PPath                                 train_target_preprocessed_file_name;
+    VMat                                  train_target_preprocessed_file;
+    Vec                                   train_target_preprocessed_vector;
+    ProgressBar*                          pb = 0;
+    
+    // managing the train set
+    cout << "In Preprocessing: we start by formatting the training set" << endl;
+    cout << endl << "****** STEP 1 ******" << endl;
+    cout << "The first step groups variables by type, skips untrustworthy variables, and generate class targets" << endl;
+    cout << "It uses ComputeDond2Target to transform base_train.pmat into step1_train_with_class_target.pmat" << endl;
+    output_path = "step1_train_with_class_target";
+    train_with_class_target_file_name = output_path + ".pmat";
+    if (isfile(train_with_class_target_file_name))
+    {
+        train_with_class_target_file = new FileVMatrix(train_with_class_target_file_name);
+        train_with_class_target_file->defineSizes(train_with_class_target_file->width(), 0, 0);
+        cout << train_with_class_target_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        compute_target_learner = ::PLearn::deepCopy(compute_target_learner_template);
+        compute_target_learner->unknown_sales = 0;
+        compute_target_learner->output_path = output_path;
+        compute_target_learner->setTrainingSet(train_set, true);
+        train_with_class_target_file = compute_target_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 2 ******" << endl;
+    cout << "This step shuffles the training set to get training data in random order." << endl;
+    cout << "It uses BootstrapVMatrix to transform step1_train_with_class_target.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 3" << endl;
+    output_path = "step3_train_with_binary_fixed";
+    train_with_binary_fixed_file_name = output_path + ".pmat";
+    if (isfile(train_with_binary_fixed_file_name))
+    {
+         cout << train_with_binary_fixed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_shufffled_vmatrix = new BootstrapVMatrix();
+        train_shufffled_vmatrix->shuffle = 1;
+        train_shufffled_vmatrix->frac = 1.0;
+        train_shufffled_vmatrix->own_seed = 123456;
+        train_shufffled_vmatrix->source = train_with_class_target_file;
+        train_shufffled_vmatrix->build();
+        train_shuffled_file = train_shufffled_vmatrix;
+    }
+    cout << endl << "****** STEP 3 ******" << endl;
+    cout << "For strictly binary variables, various situations arise: zero or non-zero, missing or not-missing, a given value or not, etc..." << endl;
+    cout << "This step uses FixDond2BinaryVariables to create step3_train_with_binary_fixed.pmat with 0-1 binary variables." << endl;
+    if (isfile(train_with_binary_fixed_file_name))
+    {
+        train_with_binary_fixed_file = new FileVMatrix(train_with_binary_fixed_file_name);
+        train_with_binary_fixed_file->defineSizes(train_with_binary_fixed_file->width(), 0, 0);
+        cout << train_with_binary_fixed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        fix_binary_variables_learner = ::PLearn::deepCopy(fix_binary_variables_template);
+        fix_binary_variables_learner->output_path = output_path;
+        fix_binary_variables_learner->setTrainingSet(train_shuffled_file, true);
+        train_with_binary_fixed_file = fix_binary_variables_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 4 ******" << endl;
+    cout << "This step adds missing indicators variables to each variable with missing values." << endl;
+    cout << "It uses MissingIndicatorVMatrix to transform step3_train_with_binary_fixed.pmat" << endl;
+    cout << "The resulting vitual view is stored in step4_train_with_ind.pmat." << endl;
+    train_with_ind_file_name = "step4_train_with_ind.pmat";
+    if (isfile(train_with_ind_file_name))
+    {
+        train_with_ind_file = new FileVMatrix(train_with_ind_file_name);
+        train_with_ind_file->defineSizes(train_with_ind_file->width(), 0, 0);
+        cout << train_with_ind_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_with_ind_vmatrix = new MissingIndicatorVMatrix();
+        train_with_ind_vmatrix->source = train_with_binary_fixed_file;
+        train_with_ind_vmatrix->train_set = train_with_binary_fixed_file;
+        train_with_ind_vmatrix->number_of_train_samples_to_use = 30000.0;
+        train_with_ind_vmatrix->build();
+        train_with_ind_vmat = train_with_ind_vmatrix;
+        train_with_ind_file = new FileVMatrix(train_with_ind_file_name, train_with_ind_vmat->length(), train_with_ind_vmat->fieldNames());
+        train_with_ind_file->defineSizes(train_with_ind_vmat->inputsize(), train_with_ind_vmat->targetsize(), train_with_ind_vmat->weightsize());
+        pb = new ProgressBar("Saving the train file with missing indicators", train_with_ind_vmat->length());
+        train_with_ind_vector.resize(train_with_ind_vmat->width());
+        for (int train_with_ind_row = 0; train_with_ind_row < train_with_ind_vmat->length(); train_with_ind_row++)
+        {
+            train_with_ind_vmat->getRow(train_with_ind_row, train_with_ind_vector);
+            train_with_ind_file->putRow(train_with_ind_row, train_with_ind_vector);
+            pb->update( train_with_ind_row );
+        }
+        delete pb;
+    }
+    cout << endl << "****** STEP 5 ******" << endl;
+    cout << "This step computes the mean, median and mode vectors on step4_train_with_ind.pmat." << endl;
+    cout << "The vectors are kept in the mean_median_mode_file.pmat of the metadata." << endl;
+    cout << "It uses MeanMedianModeImputationVMatrix to do that" << endl;
+    cout << "The resulting vitual view is not used." << endl;
+    cout << "But the mean, median and mode vectors have to go thru the same transformation than the training file" << endl;
+    cout << "from here on to the end of the preprocessing steps.." << endl;
+    train_with_imp_vmatrix = new MeanMedianModeImputationVMatrix();
+    train_with_imp_vmatrix->source = train_with_ind_file;
+    train_with_imp_vmatrix->train_set = train_with_ind_file;
+    train_with_imp_vmatrix->number_of_train_samples_to_use = 30000.0;
+    train_with_imp_vmatrix->imputation_spec = imputation_spec;
+    train_with_imp_vmatrix->build();
+    mean_median_mode_with_ind_file = train_with_imp_vmatrix->getMeanMedianModeFile();
+    cout << endl << "****** STEP 6 ******" << endl;
+    cout << "This steps generates as many dichotomized variables as there are significant code values." << endl;
+    cout << "It uses DichotomizeDond2DiscreteVariables to transform step4_train_with_ind.pmat into step6_train_with_dichotomies.pmat" << endl;
+    output_path = "step6_train_with_dichotomies";
+    train_with_dichotomies_file_name = output_path + ".pmat";
+    if (isfile(train_with_dichotomies_file_name))
+    {
+        train_with_dichotomies_file = new FileVMatrix(train_with_dichotomies_file_name);
+        train_with_dichotomies_file->defineSizes(train_with_dichotomies_file->width(), 0, 0);
+        cout << train_with_dichotomies_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        dichotomize_discrete_variables_learner = new DichotomizeDond2DiscreteVariables();
+        dichotomize_discrete_variables_learner->discrete_variable_instructions = discrete_variable_instructions;
+        dichotomize_discrete_variables_learner->output_path = output_path;
+        dichotomize_discrete_variables_learner->setTrainingSet(train_with_ind_file, true);
+        train_with_dichotomies_file = dichotomize_discrete_variables_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 7 ******" << endl;
+    cout << "This steps does the same thing to the mean, median and mode vectors." << endl;
+    cout << "It uses DichotomizeDond2DiscreteVariables to transform step4_train_with_ind.pmat.metadata/mean_median_mode_file.pmat "
+         << "into step6_train_with_dichotomies.pmat.metadata/mean_median_mode_file.pmat" << endl;
+    output_path = train_with_dichotomies_file_name + ".metadata/mean_median_mode_file";
+    mean_median_mode_with_dichotmies_file_name = output_path + ".pmat";
+    if (isfile(mean_median_mode_with_dichotmies_file_name))
+    {
+        mean_median_mode_with_dichotmies_file = new FileVMatrix(mean_median_mode_with_dichotmies_file_name);
+        mean_median_mode_with_dichotmies_file->defineSizes(mean_median_mode_with_dichotmies_file->width(), 0, 0);
+        cout << mean_median_mode_with_dichotmies_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        dichotomize_discrete_variables_learner = new DichotomizeDond2DiscreteVariables();
+        dichotomize_discrete_variables_learner->discrete_variable_instructions = discrete_variable_instructions;
+        dichotomize_discrete_variables_learner->output_path = output_path;
+        dichotomize_discrete_variables_learner->setTrainingSet(mean_median_mode_with_ind_file, true);
+        mean_median_mode_with_dichotmies_file = dichotomize_discrete_variables_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 8 ******" << endl;
+    cout << "This step select the desired columns from the training set to create the input records." << endl;
+    cout << "It uses SelectColumnsVMatrix to transform step6_train_with_dichotomies.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 10" << endl;
+    output_path = "final_train_input_preprocessed";
+    train_input_preprocessed_file_name = output_path + ".pmat";
+    if (isfile(train_input_preprocessed_file_name))
+    {
+        cout << train_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_with_selected_columns_vmatrix = new SelectColumnsVMatrix();
+        train_with_selected_columns_vmatrix->source = train_with_dichotomies_file;
+        train_with_selected_columns_vmatrix->fields_partial_match = 0;
+        train_with_selected_columns_vmatrix->extend_with_missing = 0;
+        train_with_selected_columns_vmatrix->fields = selected_variables_for_input;
+        train_with_selected_columns_vmatrix->build();
+        train_with_selected_columns_vmatrix->defineSizes(train_with_selected_columns_vmatrix->width(), 0, 0);
+        train_with_selected_columns_vmat = train_with_selected_columns_vmatrix;
+    }
+    cout << endl << "****** STEP 9 ******" << endl;
+    cout << "This step does the same thing to the mean, median and mode vectors." << endl;
+    cout << "It uses SelectColumnsVMatrix to transform step6_train_with_dichotomies.pmat.metadata/mean_median_mode_file.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 11" << endl;
+    output_path = train_input_preprocessed_file_name + ".metadata/mean_median_mode_file";
+    mean_median_mode_input_preprocessed_file_name = output_path + ".pmat";
+    if (isfile(mean_median_mode_input_preprocessed_file_name))
+    {
+        cout << mean_median_mode_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        mean_median_mode_with_selected_columns_vmatrix = new SelectColumnsVMatrix();
+        mean_median_mode_with_selected_columns_vmatrix->source = mean_median_mode_with_dichotmies_file;
+        mean_median_mode_with_selected_columns_vmatrix->fields_partial_match = 0;
+        mean_median_mode_with_selected_columns_vmatrix->extend_with_missing = 0;
+        mean_median_mode_with_selected_columns_vmatrix->fields = selected_variables_for_input;
+        mean_median_mode_with_selected_columns_vmatrix->build();
+        mean_median_mode_with_selected_columns_vmatrix->defineSizes(mean_median_mode_with_selected_columns_vmatrix->width(), 0, 0);
+        mean_median_mode_with_selected_columns_vmat = mean_median_mode_with_selected_columns_vmatrix;
+    }
+    cout << endl << "****** STEP 10 ******" << endl;
+    cout << "This gaussianizes the input records." << endl;
+    cout << "It uses GaussianizeVMatrix to transform the vmat from step 8" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 12" << endl;
+    if (isfile(train_input_preprocessed_file_name))
+    {
+        cout << train_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_gaussianized_vmatrix = new GaussianizeVMatrix();
+        train_gaussianized_vmatrix->source = train_with_selected_columns_vmat;
+        train_gaussianized_vmatrix->train_source = train_with_selected_columns_vmat;
+        train_gaussianized_vmatrix->threshold_ratio = 1;
+        train_gaussianized_vmatrix->gaussianize_input = 1;
+        train_gaussianized_vmatrix->gaussianize_target = 0;
+        train_gaussianized_vmatrix->gaussianize_weight = 0;
+        train_gaussianized_vmatrix->gaussianize_extra = 0;
+        train_gaussianized_vmatrix->excluded_fields = inputs_excluded_from_gaussianization;
+        train_gaussianized_vmatrix->build();
+        train_gaussianized_vmat = train_gaussianized_vmatrix;
+    }
+    cout << endl << "****** STEP 11 ******" << endl;
+    cout << "This step does the same thing to the mean, median and mode vectors." << endl;
+    cout << "It uses the vmat from step 9" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 13" << endl;
+    if (isfile(mean_median_mode_input_preprocessed_file_name))
+    {
+        cout << mean_median_mode_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        mean_median_mode_gaussianized_vmatrix = new GaussianizeVMatrix();
+        mean_median_mode_gaussianized_vmatrix->source = mean_median_mode_with_selected_columns_vmat;
+        mean_median_mode_gaussianized_vmatrix->train_source = train_with_selected_columns_vmat;
+        mean_median_mode_gaussianized_vmatrix->threshold_ratio = 1;
+        mean_median_mode_gaussianized_vmatrix->gaussianize_input = 1;
+        mean_median_mode_gaussianized_vmatrix->gaussianize_target = 0;
+        mean_median_mode_gaussianized_vmatrix->gaussianize_weight = 0;
+        mean_median_mode_gaussianized_vmatrix->gaussianize_extra = 0;
+        mean_median_mode_gaussianized_vmatrix->excluded_fields = inputs_excluded_from_gaussianization;
+        mean_median_mode_gaussianized_vmatrix->build();
+        mean_median_mode_gaussianized_vmat = mean_median_mode_gaussianized_vmatrix;
+    }
+    cout << endl << "****** STEP 12 ******" << endl;
+    cout << "Finaly, the preprocessed input vectors are store on disk." << endl;
+    cout << "The vmat from step 10 is converted to final_train_input_preprocessed.pmat." << endl;
+    if (isfile(train_input_preprocessed_file_name))
+    {
+        cout << train_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_input_preprocessed_file = new FileVMatrix(train_input_preprocessed_file_name, train_gaussianized_vmat->length(), train_gaussianized_vmat->fieldNames());
+        train_input_preprocessed_file->defineSizes(train_gaussianized_vmat->inputsize(), train_gaussianized_vmat->targetsize(), train_gaussianized_vmat->weightsize());
+        pb = new ProgressBar("Saving the final train preprocessed input records", train_gaussianized_vmat->length());
+        train_input_preprocessed_vector.resize(train_gaussianized_vmat->width());
+        for (int train_gaussianized_row = 0; train_gaussianized_row < train_gaussianized_vmat->length(); train_gaussianized_row++)
+        {
+            train_gaussianized_vmat->getRow(train_gaussianized_row, train_input_preprocessed_vector);
+            train_input_preprocessed_file->putRow(train_gaussianized_row, train_input_preprocessed_vector);
+            pb->update( train_gaussianized_row );
+        }
+        delete pb;
+    }
+    cout << endl << "****** STEP 13 ******" << endl;
+    cout << "And we do the same for the mean, median and mode vectors." << endl;
+    cout << "The vmat from step 11 is converted to final_train_input_preprocessed.pmat.metadata/men_median_mode_file.pmat" << endl;
+    if (isfile(mean_median_mode_input_preprocessed_file_name))
+    {
+        cout << mean_median_mode_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        mean_median_mode_input_preprocessed_file = 
+            new FileVMatrix(mean_median_mode_input_preprocessed_file_name, mean_median_mode_gaussianized_vmat->length(), mean_median_mode_gaussianized_vmat->fieldNames());
+        mean_median_mode_input_preprocessed_file->defineSizes(mean_median_mode_gaussianized_vmat->inputsize(),
+                                                              mean_median_mode_gaussianized_vmat->targetsize(), mean_median_mode_gaussianized_vmat->weightsize());
+        pb = new ProgressBar("Saving the final mean,median and mode preprocessed input vectors", mean_median_mode_gaussianized_vmat->length());
+        mean_median_mode_input_preprocessed_vector.resize(mean_median_mode_gaussianized_vmat->width());
+        for (int mean_median_mode_gaussianized_row = 0; mean_median_mode_gaussianized_row < mean_median_mode_gaussianized_vmat->length(); mean_median_mode_gaussianized_row++)
+        {
+            mean_median_mode_gaussianized_vmat->getRow(mean_median_mode_gaussianized_row, mean_median_mode_input_preprocessed_vector);
+            mean_median_mode_input_preprocessed_file->putRow(mean_median_mode_gaussianized_row, mean_median_mode_input_preprocessed_vector);
+            pb->update( mean_median_mode_gaussianized_row );
+        }
+        delete pb;
+    }
+    cout << endl << "****** STEP 14 ******" << endl;
+    cout << "This step select the desired columns from the training set to create the target records." << endl;
+    cout << "It uses SelectColumnsVMatrix to transform step6_train_with_dichotomies.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 15" << endl;
+    output_path = "final_train_target_preprocessed";
+    train_target_preprocessed_file_name = output_path + ".pmat";
+    if (isfile(train_target_preprocessed_file_name))
+    {
+        cout << train_target_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_target_with_selected_columns_vmatrix = new SelectColumnsVMatrix();
+        train_target_with_selected_columns_vmatrix->source = train_with_dichotomies_file;
+        train_target_with_selected_columns_vmatrix->fields_partial_match = 0;
+        train_target_with_selected_columns_vmatrix->extend_with_missing = 0;
+        train_target_with_selected_columns_vmatrix->fields = selected_variables_for_target;
+        train_target_with_selected_columns_vmatrix->build();
+        train_target_with_selected_columns_vmatrix->defineSizes(train_target_with_selected_columns_vmatrix->width(), 0, 0);
+        train_target_with_selected_columns_vmat = train_target_with_selected_columns_vmatrix;
+    }
+    cout << endl << "****** STEP 15 ******" << endl;
+    cout << "This gaussianizes the input records." << endl;
+    cout << "It uses GaussianizeVMatrix to transform the vmat from step 14" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 16" << endl;
+    if (isfile(train_target_preprocessed_file_name))
+    {
+        cout << train_target_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_target_gaussianized_vmatrix = new GaussianizeVMatrix();
+        train_target_gaussianized_vmatrix->source = train_target_with_selected_columns_vmat;
+        train_target_gaussianized_vmatrix->train_source = train_target_with_selected_columns_vmat;
+        train_target_gaussianized_vmatrix->threshold_ratio = 1;
+        train_target_gaussianized_vmatrix->gaussianize_input = 1;
+        train_target_gaussianized_vmatrix->gaussianize_target = 0;
+        train_target_gaussianized_vmatrix->gaussianize_weight = 0;
+        train_target_gaussianized_vmatrix->gaussianize_extra = 0;
+        train_target_gaussianized_vmatrix->excluded_fields = targets_excluded_from_gaussianization;;
+        train_target_gaussianized_vmatrix->build();
+        train_target_gaussianized_vmat = train_target_gaussianized_vmatrix;
+    }
+    cout << endl << "****** STEP 16 ******" << endl;
+    cout << "Finaly, the preprocessed input vectors are store on disk." << endl;
+    cout << "The vmat from step 15 is converted to final_train_target_preprocessed.pmat." << endl;
+    if (isfile(train_target_preprocessed_file_name))
+    {
+        cout << train_target_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        train_target_preprocessed_file = new FileVMatrix(train_target_preprocessed_file_name, train_target_gaussianized_vmat->length(),
+                                                         train_target_gaussianized_vmat->fieldNames());
+        train_target_preprocessed_file->defineSizes(train_target_gaussianized_vmat->inputsize(), train_target_gaussianized_vmat->targetsize(),
+                                                   train_target_gaussianized_vmat->weightsize());
+        pb = new ProgressBar("Saving the final train preprocessed target records", train_target_gaussianized_vmat->length());
+        train_target_preprocessed_vector.resize(train_target_gaussianized_vmat->width());
+        for (int train_gaussianized_row = 0; train_gaussianized_row < train_target_gaussianized_vmat->length(); train_gaussianized_row++)
+        {
+            train_target_gaussianized_vmat->getRow(train_gaussianized_row, train_target_preprocessed_vector);
+            train_target_preprocessed_file->putRow(train_gaussianized_row, train_target_preprocessed_vector);
+            pb->update( train_gaussianized_row );
+        }
+        delete pb;
+    }
+    
+    // defining all the variables for the test set
+    PPath                                 test_with_class_target_file_name;
+    VMat                                  test_with_class_target_file;
+    PPath                                 test_with_binary_fixed_file_name;
+    VMat                                  test_with_binary_fixed_file;
+    PPath                                 test_with_ind_file_name;
+    MissingIndicatorVMatrix*              test_with_ind_vmatrix;
+    VMat                                  test_with_ind_vmat;
+    PPath                                 test_with_dichotomies_file_name;
+    VMat                                  test_with_dichotomies_file;
+    SelectColumnsVMatrix*                 test_with_selected_columns_vmatrix;
+    VMat                                  test_with_selected_columns_vmat;
+    GaussianizeVMatrix*                   test_gaussianized_vmatrix;
+    VMat                                  test_gaussianized_vmat;
+    PPath                                 test_input_preprocessed_file_name;
+    VMat                                  test_input_preprocessed_file;
+    Vec                                   test_input_preprocessed_vector;
+    SelectColumnsVMatrix*                 test_target_with_selected_columns_vmatrix;
+    VMat                                  test_target_with_selected_columns_vmat;
+    GaussianizeVMatrix*                   test_target_gaussianized_vmatrix;
+    VMat                                  test_target_gaussianized_vmat;
+    PPath                                 test_target_preprocessed_file_name;
+    VMat                                  test_target_preprocessed_file;
+    Vec                                   test_target_preprocessed_vector;
+    
+    // managing the test set
+    cout << endl << "********************" << endl;
+    cout << "In Preprocessing: now, we format the test set" << endl;
+    cout << endl << "****** STEP 1 ******" << endl;
+    cout << "The first step groups variables by type, skips untrustworthy variables, and generate class targets" << endl;
+    cout << "It uses ComputeDond2Target to transform base_test.pmat into step1_test_with_class_target.pmat" << endl;
+    output_path = "step1_test_with_class_target";
+    test_with_class_target_file_name = output_path + ".pmat";
+    if (isfile(test_with_class_target_file_name))
+    {
+        test_with_class_target_file = new FileVMatrix(test_with_class_target_file_name);
+        test_with_class_target_file->defineSizes(test_with_class_target_file->width(), 0, 0);
+        cout << test_with_class_target_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        compute_target_learner = ::PLearn::deepCopy(compute_target_learner_template);
+        compute_target_learner->unknown_sales = 0;
+        compute_target_learner->output_path = output_path;
+        compute_target_learner->setTrainingSet(test_set, true);
+        test_with_class_target_file = compute_target_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 2 ******" << endl;
+    cout << "Shuffling is not required for the test set, skipped." << endl;
+    cout << endl << "****** STEP 3 ******" << endl;
+    cout << "For strictly binary variables, various situations arise: zero or non-zero, missing or not-missing, a given value or not, etc..." << endl;
+    cout << "This step uses FixDond2BinaryVariables to create step3_test_with_binary_fixed.pmat with 0-1 binary variables." << endl;
+    output_path = "step3_test_with_binary_fixed";
+    test_with_binary_fixed_file_name = output_path + ".pmat";
+    if (isfile(test_with_binary_fixed_file_name))
+    {
+        test_with_binary_fixed_file = new FileVMatrix(test_with_binary_fixed_file_name);
+        test_with_binary_fixed_file->defineSizes(test_with_binary_fixed_file->width(), 0, 0);
+        cout << test_with_binary_fixed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        fix_binary_variables_learner = ::PLearn::deepCopy(fix_binary_variables_template);
+        fix_binary_variables_learner->output_path = output_path;
+        fix_binary_variables_learner->setTrainingSet(test_with_class_target_file, true);
+        test_with_binary_fixed_file = fix_binary_variables_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 4 ******" << endl;
+    cout << "This step adds missing indicators variables to each variable with missing values." << endl;
+    cout << "It uses MissingIndicatorVMatrix to transform step3_test_with_binary_fixed.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 6" << endl;
+    output_path = "step6_test_with_dichotomies";
+    test_with_dichotomies_file_name = output_path + ".pmat";
+    if (isfile(test_with_dichotomies_file_name))
+    {
+        cout << test_with_dichotomies_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_with_ind_vmatrix = new MissingIndicatorVMatrix();
+        test_with_ind_vmatrix->source = test_with_binary_fixed_file;
+        test_with_ind_vmatrix->train_set = train_with_binary_fixed_file;
+        test_with_ind_vmatrix->number_of_train_samples_to_use = 30000.0;
+        test_with_ind_vmatrix->build();
+        test_with_ind_vmat = test_with_ind_vmatrix;
+    }
+    cout << endl << "****** STEP 5 ******" << endl;
+    cout << "Computing mean, median and mode is not required for the test set, skipped." << endl;
+    cout << endl << "****** STEP 6 ******" << endl;
+    cout << "This steps generates as many dichotomized variables as there are significant code values." << endl;
+    cout << "It uses DichotomizeDond2DiscreteVariables to transform the vmat from step 4 into step6_test_with_dichotomies.pmat" << endl;
+    if (isfile(test_with_dichotomies_file_name))
+    {
+        test_with_dichotomies_file = new FileVMatrix(test_with_dichotomies_file_name);
+        test_with_dichotomies_file->defineSizes(test_with_dichotomies_file->width(), 0, 0);
+        cout << test_with_dichotomies_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        dichotomize_discrete_variables_learner = new DichotomizeDond2DiscreteVariables();
+        dichotomize_discrete_variables_learner->discrete_variable_instructions = discrete_variable_instructions;
+        dichotomize_discrete_variables_learner->output_path = output_path;
+        dichotomize_discrete_variables_learner->setTrainingSet(test_with_ind_vmat, true);
+        test_with_dichotomies_file = dichotomize_discrete_variables_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 7 ******" << endl;
+    cout << "Dichotomizing mean median and mode is not required for the test set, skipped." << endl;
+    cout << endl << "****** STEP 8 ******" << endl;
+    cout << "This step select the desired columns from the test set to create the input records." << endl;
+    cout << "It uses SelectColumnsVMatrix to transform step6_test_with_dichotomies.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 10" << endl;
+    output_path = "final_test_input_preprocessed";
+    test_input_preprocessed_file_name = output_path + ".pmat";
+    if (isfile(test_input_preprocessed_file_name))
+    {
+        cout << test_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_with_selected_columns_vmatrix = new SelectColumnsVMatrix();
+        test_with_selected_columns_vmatrix->source = test_with_dichotomies_file;
+        test_with_selected_columns_vmatrix->fields_partial_match = 0;
+        test_with_selected_columns_vmatrix->extend_with_missing = 0;
+        test_with_selected_columns_vmatrix->fields = selected_variables_for_input;
+        test_with_selected_columns_vmatrix->build();
+        test_with_selected_columns_vmatrix->defineSizes(test_with_selected_columns_vmatrix->width(), 0, 0);
+        test_with_selected_columns_vmat = test_with_selected_columns_vmatrix;
+    }
+    cout << endl << "****** STEP 9 ******" << endl;
+    cout << "Selecting variables for the mean, median and mode is not required for the test set, skipped." << endl;
+    cout << endl << "****** STEP 10 ******" << endl;
+    cout << "This gaussianizes the input records." << endl;
+    cout << "It uses GaussianizeVMatrix to transform the vmat from step 8" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 12" << endl;
+    if (isfile(test_input_preprocessed_file_name))
+    {
+        cout << test_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_gaussianized_vmatrix = new GaussianizeVMatrix();
+        test_gaussianized_vmatrix->source = test_with_selected_columns_vmat;
+        test_gaussianized_vmatrix->train_source = train_with_selected_columns_vmat;
+        test_gaussianized_vmatrix->threshold_ratio = 1;
+        test_gaussianized_vmatrix->gaussianize_input = 1;
+        test_gaussianized_vmatrix->gaussianize_target = 0;
+        test_gaussianized_vmatrix->gaussianize_weight = 0;
+        test_gaussianized_vmatrix->gaussianize_extra = 0;
+        test_gaussianized_vmatrix->excluded_fields = inputs_excluded_from_gaussianization;
+        test_gaussianized_vmatrix->build();
+        test_gaussianized_vmat = test_gaussianized_vmatrix;
+    }
+    cout << endl << "****** STEP 11 ******" << endl;
+    cout << "Gaussianizing the mean, meadian and mode is not required for the test set, skipped." << endl;
+    cout << endl << "****** STEP 12 ******" << endl;
+    cout << "Finaly, the preprocessed input vectors are store on disk." << endl;
+    cout << "The vmat from step 10 is converted to final_test_input_preprocessed.pmat." << endl;
+    if (isfile(test_input_preprocessed_file_name))
+    {
+        cout << test_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_input_preprocessed_file = new FileVMatrix(test_input_preprocessed_file_name, test_gaussianized_vmat->length(), test_gaussianized_vmat->fieldNames());
+        test_input_preprocessed_file->defineSizes(test_gaussianized_vmat->inputsize(), test_gaussianized_vmat->targetsize(), test_gaussianized_vmat->weightsize());
+        pb = new ProgressBar("Saving the final test preprocessed input records", test_gaussianized_vmat->length());
+        test_input_preprocessed_vector.resize(test_gaussianized_vmat->width());
+        for (int test_gaussianized_row = 0; test_gaussianized_row < test_gaussianized_vmat->length(); test_gaussianized_row++)
+        {
+            test_gaussianized_vmat->getRow(test_gaussianized_row, test_input_preprocessed_vector);
+            test_input_preprocessed_file->putRow(test_gaussianized_row, test_input_preprocessed_vector);
+            pb->update( test_gaussianized_row );
+        }
+        delete pb;
+    }
+    cout << endl << "****** STEP 13 ******" << endl;
+    cout << "Saving the final mean, median and mode is not required for the test set, skipped." << endl;
+    cout << endl << "****** STEP 14 ******" << endl;
+    cout << "This step select the desired columns from the testing set to create the target records." << endl;
+    cout << "It uses SelectColumnsVMatrix to transform step6_test_with_dichotomies.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 15" << endl;
+    output_path = "final_test_target_preprocessed";
+    test_target_preprocessed_file_name = output_path + ".pmat";
+    if (isfile(test_target_preprocessed_file_name))
+    {
+        cout << test_target_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_target_with_selected_columns_vmatrix = new SelectColumnsVMatrix();
+        test_target_with_selected_columns_vmatrix->source = test_with_dichotomies_file;
+        test_target_with_selected_columns_vmatrix->fields_partial_match = 0;
+        test_target_with_selected_columns_vmatrix->extend_with_missing = 0;
+        test_target_with_selected_columns_vmatrix->fields = selected_variables_for_target;
+        test_target_with_selected_columns_vmatrix->build();
+        test_target_with_selected_columns_vmatrix->defineSizes(test_target_with_selected_columns_vmatrix->width(), 0, 0);
+        test_target_with_selected_columns_vmat = test_target_with_selected_columns_vmatrix;
+    }
+    cout << endl << "****** STEP 15 ******" << endl;
+    cout << "This gaussianizes the input records." << endl;
+    cout << "It uses GaussianizeVMatrix to transform the vmat from step 14" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 16" << endl;
+    if (isfile(test_target_preprocessed_file_name))
+    {
+        cout << test_target_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_target_gaussianized_vmatrix = new GaussianizeVMatrix();
+        test_target_gaussianized_vmatrix->source = test_target_with_selected_columns_vmat;
+        test_target_gaussianized_vmatrix->train_source = train_target_with_selected_columns_vmat;
+        test_target_gaussianized_vmatrix->threshold_ratio = 1;
+        test_target_gaussianized_vmatrix->gaussianize_input = 1;
+        test_target_gaussianized_vmatrix->gaussianize_target = 0;
+        test_target_gaussianized_vmatrix->gaussianize_weight = 0;
+        test_target_gaussianized_vmatrix->gaussianize_extra = 0;
+        test_target_gaussianized_vmatrix->excluded_fields = targets_excluded_from_gaussianization;;
+        test_target_gaussianized_vmatrix->build();
+        test_target_gaussianized_vmat = test_target_gaussianized_vmatrix;
+    }
+    cout << endl << "****** STEP 16 ******" << endl;
+    cout << "Finaly, the preprocessed input vectors are store on disk." << endl;
+    cout << "The vmat from step 15 is converted to final_test_target_preprocessed.pmat." << endl;
+    if (isfile(test_target_preprocessed_file_name))
+    {
+        cout << test_target_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        test_target_preprocessed_file = new FileVMatrix(test_target_preprocessed_file_name, test_target_gaussianized_vmat->length(),
+                                                         test_target_gaussianized_vmat->fieldNames());
+        test_target_preprocessed_file->defineSizes(test_target_gaussianized_vmat->inputsize(), test_target_gaussianized_vmat->targetsize(),
+                                                   test_target_gaussianized_vmat->weightsize());
+        pb = new ProgressBar("Saving the final test preprocessed target records", test_target_gaussianized_vmat->length());
+        test_target_preprocessed_vector.resize(test_target_gaussianized_vmat->width());
+        for (int test_gaussianized_row = 0; test_gaussianized_row < test_target_gaussianized_vmat->length(); test_gaussianized_row++)
+        {
+            test_target_gaussianized_vmat->getRow(test_gaussianized_row, test_target_preprocessed_vector);
+            test_target_preprocessed_file->putRow(test_gaussianized_row, test_target_preprocessed_vector);
+            pb->update( test_gaussianized_row );
+        }
+        delete pb;
+    }
+    
+    // defining all the variables for the unknown set
+    PPath                                 unknown_with_class_target_file_name;
+    VMat                                  unknown_with_class_target_file;
+    PPath                                 unknown_with_binary_fixed_file_name;
+    VMat                                  unknown_with_binary_fixed_file;
+    PPath                                 unknown_with_ind_file_name;
+    MissingIndicatorVMatrix*              unknown_with_ind_vmatrix;
+    VMat                                  unknown_with_ind_vmat;
+    PPath                                 unknown_with_dichotomies_file_name;
+    VMat                                  unknown_with_dichotomies_file;
+    SelectColumnsVMatrix*                 unknown_with_selected_columns_vmatrix;
+    VMat                                  unknown_with_selected_columns_vmat;
+    GaussianizeVMatrix*                   unknown_gaussianized_vmatrix;
+    VMat                                  unknown_gaussianized_vmat;
+    PPath                                 unknown_input_preprocessed_file_name;
+    VMat                                  unknown_input_preprocessed_file;
+    Vec                                   unknown_input_preprocessed_vector;
+    
+    // managing the unknown set
+    cout << endl << "********************" << endl;
+    cout << "In Preprocessing: finally, we format the unknown set" << endl;
+    cout << endl << "****** STEP 1 ******" << endl;
+    cout << "The first step groups variables by type, skips untrustworthy variables, and generate class targets" << endl;
+    cout << "It uses ComputeDond2Target to transform base_unknown.pmat into step1_unknown_with_class_target.pmat" << endl;
+    output_path = "step1_unknown_with_class_target";
+    unknown_with_class_target_file_name = output_path + ".pmat";
+    if (isfile(unknown_with_class_target_file_name))
+    {
+        unknown_with_class_target_file = new FileVMatrix(unknown_with_class_target_file_name);
+        unknown_with_class_target_file->defineSizes(unknown_with_class_target_file->width(), 0, 0);
+        cout << unknown_with_class_target_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        compute_target_learner = ::PLearn::deepCopy(compute_target_learner_template);
+        compute_target_learner->unknown_sales = 1;
+        compute_target_learner->output_path = output_path;
+        compute_target_learner->setTrainingSet(unknown_set, true);
+        unknown_with_class_target_file = compute_target_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 2 ******" << endl;
+    cout << "Shuffling is not required for the unknown set, skipped." << endl;
+    cout << endl << "****** STEP 3 ******" << endl;
+    cout << "For strictly binary variables, various situations arise: zero or non-zero, missing or not-missing, a given value or not, etc..." << endl;
+    cout << "This step uses FixDond2BinaryVariables to create step3_unknown_with_binary_fixed.pmat with 0-1 binary variables." << endl;
+    output_path = "step3_unknown_with_binary_fixed";
+    unknown_with_binary_fixed_file_name = output_path + ".pmat";
+    if (isfile(unknown_with_binary_fixed_file_name))
+    {
+        unknown_with_binary_fixed_file = new FileVMatrix(unknown_with_binary_fixed_file_name);
+        unknown_with_binary_fixed_file->defineSizes(unknown_with_binary_fixed_file->width(), 0, 0);
+        cout << unknown_with_binary_fixed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        fix_binary_variables_learner = ::PLearn::deepCopy(fix_binary_variables_template);
+        fix_binary_variables_learner->output_path = output_path;
+        fix_binary_variables_learner->setTrainingSet(unknown_with_class_target_file, true);
+        unknown_with_binary_fixed_file = fix_binary_variables_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 4 ******" << endl;
+    cout << "This step adds missing indicators variables to each variable with missing values." << endl;
+    cout << "It uses MissingIndicatorVMatrix to transform step3_unknown_with_binary_fixed.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 6" << endl;
+    output_path = "step6_unknown_with_dichotomies";
+    unknown_with_dichotomies_file_name = output_path + ".pmat";
+    if (isfile(unknown_with_dichotomies_file_name))
+    {
+        cout << unknown_with_dichotomies_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        unknown_with_ind_vmatrix = new MissingIndicatorVMatrix();
+        unknown_with_ind_vmatrix->source = unknown_with_binary_fixed_file;
+        unknown_with_ind_vmatrix->train_set = train_with_binary_fixed_file;
+        unknown_with_ind_vmatrix->number_of_train_samples_to_use = 30000.0;
+        unknown_with_ind_vmatrix->build();
+        unknown_with_ind_vmat = unknown_with_ind_vmatrix;
+    }
+    cout << endl << "****** STEP 5 ******" << endl;
+    cout << "Computing mean, median and mode is not required for the unknown set, skipped." << endl;
+    cout << endl << "****** STEP 6 ******" << endl;
+    cout << "This steps generates as many dichotomized variables as there are significant code values." << endl;
+    cout << "It uses DichotomizeDond2DiscreteVariables to transform the vmat from step 4 into step6_unknown_with_dichotomies.pmat" << endl;
+    if (isfile(unknown_with_dichotomies_file_name))
+    {
+        unknown_with_dichotomies_file = new FileVMatrix(unknown_with_dichotomies_file_name);
+        unknown_with_dichotomies_file->defineSizes(unknown_with_dichotomies_file->width(), 0, 0);
+        cout << unknown_with_dichotomies_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        dichotomize_discrete_variables_learner = new DichotomizeDond2DiscreteVariables();
+        dichotomize_discrete_variables_learner->discrete_variable_instructions = discrete_variable_instructions;
+        dichotomize_discrete_variables_learner->output_path = output_path;
+        dichotomize_discrete_variables_learner->setTrainingSet(unknown_with_ind_vmat, true);
+        unknown_with_dichotomies_file = dichotomize_discrete_variables_learner->getOutputFile();
+    }
+    cout << endl << "****** STEP 7 ******" << endl;
+    cout << "Dichotomizing mean median and mode is not required for the unknown set, skipped." << endl;
+    cout << endl << "****** STEP 8 ******" << endl;
+    cout << "This step select the desired columns from the unknown set to create the input records." << endl;
+    cout << "It uses SelectColumnsVMatrix to transform step6_unknown_with_dichotomies.pmat" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 10" << endl;
+    output_path = "final_unknown_input_preprocessed";
+    unknown_input_preprocessed_file_name = output_path + ".pmat";
+    if (isfile(unknown_input_preprocessed_file_name))
+    {
+        cout << unknown_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        unknown_with_selected_columns_vmatrix = new SelectColumnsVMatrix();
+        unknown_with_selected_columns_vmatrix->source = unknown_with_dichotomies_file;
+        unknown_with_selected_columns_vmatrix->fields_partial_match = 0;
+        unknown_with_selected_columns_vmatrix->extend_with_missing = 0;
+        unknown_with_selected_columns_vmatrix->fields = selected_variables_for_input;
+        unknown_with_selected_columns_vmatrix->build();
+        unknown_with_selected_columns_vmatrix->defineSizes(unknown_with_selected_columns_vmatrix->width(), 0, 0);
+        unknown_with_selected_columns_vmat = unknown_with_selected_columns_vmatrix;
+    }
+    cout << endl << "****** STEP 9 ******" << endl;
+    cout << "Selecting variables for the mean, median and mode is not required for the unknown set, skipped." << endl;
+    cout << endl << "****** STEP 10 ******" << endl;
+    cout << "This gaussianizes the input records." << endl;
+    cout << "It uses GaussianizeVMatrix to transform the vmat from step 8" << endl;
+    cout << "The resulting vitual view is not stored on disk, it is fed as input to step 12" << endl;
+    if (isfile(unknown_input_preprocessed_file_name))
+    {
+        cout << unknown_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        unknown_gaussianized_vmatrix = new GaussianizeVMatrix();
+        unknown_gaussianized_vmatrix->source = unknown_with_selected_columns_vmat;
+        unknown_gaussianized_vmatrix->train_source = train_with_selected_columns_vmat;
+        unknown_gaussianized_vmatrix->threshold_ratio = 1;
+        unknown_gaussianized_vmatrix->gaussianize_input = 1;
+        unknown_gaussianized_vmatrix->gaussianize_target = 0;
+        unknown_gaussianized_vmatrix->gaussianize_weight = 0;
+        unknown_gaussianized_vmatrix->gaussianize_extra = 0;
+        unknown_gaussianized_vmatrix->excluded_fields = inputs_excluded_from_gaussianization;
+        unknown_gaussianized_vmatrix->build();
+        unknown_gaussianized_vmat = unknown_gaussianized_vmatrix;
+    }
+    cout << endl << "****** STEP 11 ******" << endl;
+    cout << "Gaussianizing the mean, meadian and mode is not required for the unknown set, skipped." << endl;
+    cout << endl << "****** STEP 12 ******" << endl;
+    cout << "Finaly, the preprocessed input vectors are store on disk." << endl;
+    cout << "The vmat from step 10 is converted to final_unknown_input_preprocessed.pmat." << endl;
+    if (isfile(unknown_input_preprocessed_file_name))
+    {
+        cout << unknown_input_preprocessed_file_name << " already exist, we are skipping this step." << endl;
+    }
+    else 
+    {
+        unknown_input_preprocessed_file = new FileVMatrix(unknown_input_preprocessed_file_name, unknown_gaussianized_vmat->length(), unknown_gaussianized_vmat->fieldNames());
+        unknown_input_preprocessed_file->defineSizes(unknown_gaussianized_vmat->inputsize(), unknown_gaussianized_vmat->targetsize(), unknown_gaussianized_vmat->weightsize());
+        pb = new ProgressBar("Saving the final unknown preprocessed input records", unknown_gaussianized_vmat->length());
+        unknown_input_preprocessed_vector.resize(unknown_gaussianized_vmat->width());
+        for (int unknown_gaussianized_row = 0; unknown_gaussianized_row < unknown_gaussianized_vmat->length(); unknown_gaussianized_row++)
+        {
+            unknown_gaussianized_vmat->getRow(unknown_gaussianized_row, unknown_input_preprocessed_vector);
+            unknown_input_preprocessed_file->putRow(unknown_gaussianized_row, unknown_input_preprocessed_vector);
+            pb->update( unknown_gaussianized_row );
+        }
+        delete pb;
+    }
+    cout << endl << "****** STEP 13 ******" << endl;
+    cout << "Saving the final mean, median and mode is not required for the unknown set, skipped." << endl;
+    cout << endl << "****** STEP 14 ******" << endl;
+    cout << "Saving the final target preprocessed records is not required for the unknown set, skipped." << endl;
+    cout << endl << "****** STEP 15 ******" << endl;
+    cout << "Saving the final target preprocessed records is not required for the unknown set, skipped." << endl;
+    cout << endl << "****** STEP 16 ******" << endl;
+    cout << "Saving the final target preprocessed records is not required for the unknown set, skipped." << endl;
+}
+
+void Preprocessing::train()
+{
+}
+
+int Preprocessing::outputsize() const {return 0;}
+void Preprocessing::computeOutput(const Vec&, Vec&) const {}
+void Preprocessing::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> Preprocessing::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> Preprocessing::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,161 @@
+// -*- C++ -*-
+
+// Preprocessing.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file Preprocessing.h */
+
+
+#ifndef Preprocessing_INC
+#define Preprocessing_INC
+
+#include <plearn_learners/second_iteration/ComputeDond2Target.h>
+#include <plearn_learners/second_iteration/FixDond2BinaryVariables.h>
+#include <plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.h>
+#include <plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h>
+#include <plearn_learners/second_iteration/GaussianizeVMatrix.h>
+#include <plearn_learners/second_iteration/MissingIndicatorVMatrix.h>     //!<  For the missing indicators stuff.
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/testers/PTester.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/vmat/MemoryVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class Preprocessing : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! The test data set.
+    VMat test_set;
+    //! The unknown data set.
+    VMat unknown_set;
+    //! The template of the script to generate the class target.
+    PP<ComputeDond2Target> compute_target_learner_template;
+    //! The template of the script to fix the binary variables.
+    PP<FixDond2BinaryVariables> fix_binary_variables_template;
+    //! Pairs of instruction of the form field_name : mean | median | mode.
+    TVec< pair<string, string> >  imputation_spec;
+    //! The instructions to fix the binary variables in the form of field_name : instruction.
+    //! Supported instructions are 9_is_one, not_0_is_one, not_missing_is_one, not_1000_is_one.
+    //! Variables with no specification will be kept as_is.
+    TVec< pair<string, TVec< pair<real, real> > > > discrete_variable_instructions;
+    //! The list of variables selected as input vector.
+    TVec< string > selected_variables_for_input;
+    //! The list of variables selected as target vector.
+    TVec< string > selected_variables_for_target;
+    //! The list of variables excluded from the gaussianization step.
+    TVec< string > inputs_excluded_from_gaussianization;
+    //! The list of variables excluded from the gaussianization step.
+    TVec< string > targets_excluded_from_gaussianization;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    Preprocessing();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(Preprocessing);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void manageTrainTestUnknownSets();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(Preprocessing);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationTester.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationTester.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationTester.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,113 @@
+// -*- C++ -*-
+
+// SecondIterationTester.cc
+// 
+// Copyright (C) 2002 Pascal Vincent, Frederic Morin
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+ * $Id: SecondIterationTester.cc 5587 2006-05-12 16:31:54Z plearner $ 
+ ******************************************************* */
+
+/*! \file SecondIterationTester.cc */
+#include "SecondIterationTester.h"
+
+
+namespace PLearn {
+using namespace std;
+
+SecondIterationTester::SecondIterationTester() 
+{}
+
+PLEARN_IMPLEMENT_OBJECT(
+    SecondIterationTester,
+    "Manages a learning experiment, with training and estimation of generalization error.", 
+    "The SecondIterationTester class allows you to describe a typical learning experiment that you wish to perform, \n"
+    "as a training/testing of a learning algorithm on a particular dataset.\n"
+    "The splitter is used to obtain one or several (such as for k-fold) splits of the dataset \n"
+    "and training/testing is performed on each split. \n"
+    "Requested statistics are computed, and all requested results are written in an appropriate \n"
+    "file inside the specified experiment directory. \n"
+    "Statistics can be either specified entirely from the 'statnames' option, or built from\n"
+    "'statnames' and 'statmask'. For instance, one may set:\n"
+    "   statnames = [ \"NLL\" \"mse\" ]\n"
+    "   statmask  = [ [ \"E[*]\" ] [ \"test#1-2#.*\" ] [ \"E[*]\" \"STDERROR[*]\" ] ]\n"
+    "and this will compute:\n"
+    "   E[test1.E[NLL]], STDERROR[test1.E[NLL]], E[test2.E[NLL]], STDERROR[test2.E[NLL]]\n"
+    "   E[test1.E[mse]], STDERROR[test1.E[mse]], E[test2.E[mse]], STDERROR[test2.E[mse]]\n"
+    );
+
+
+void SecondIterationTester::declareOptions(OptionList& ol)
+{
+    inherited::declareOptions(ol);
+}
+
+void SecondIterationTester::build_()
+{
+}
+
+// ### Nothing to add here, simply calls build_
+void SecondIterationTester::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////////////////////
+// setExperimentDirectory //
+////////////////////////////
+void SecondIterationTester::setSplitter(string splitter_template)
+{ 
+ //   splitter = ::PLearn::deepCopy(splitter_template);
+    splitter->build();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void SecondIterationTester::makeDeepCopyFromShallowCopy(CopiesMap& copies) {
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationTester.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationTester.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationTester.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,117 @@
+// -*- C++ -*-
+
+// SecondIterationTester.h
+// 
+// Copyright (C) 2002 Pascal Vincent, Frederic Morin
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+ * $Id: SecondIterationTester.h 5587 2006-05-12 16:31:54Z plearner $ 
+ ******************************************************* */
+
+/*! \file SecondIterationTester.h */
+#ifndef SecondIterationTester_INC
+#define SecondIterationTester_INC
+
+#include <plearn/vmat/ExplicitSplitter.h>
+#include <plearn_learners/testers/PTester.h>
+
+namespace PLearn {
+using namespace std;
+
+class SecondIterationTester: public PTester
+{    
+
+private:
+
+    typedef PTester inherited;
+
+public:
+
+    // ************************
+    // * public build options *
+    // ************************
+  
+    // See declareOptions method in .cc for the role of these options.
+
+    // ****************
+    // * Constructors *
+    // ****************
+
+    // Default constructor
+    SecondIterationTester();
+
+
+    // ******************
+    // * Object methods *
+    // ******************
+
+private: 
+    //! This does the actual building. 
+    // (Please implement in .cc)
+    void build_();
+
+protected: 
+    //! Declares this class' options
+    static void declareOptions(OptionList& ol);
+
+public:
+    // simply calls inherited::build() then build_() 
+    virtual void build();
+
+    //! Declares name and deepCopy methods
+    PLEARN_DECLARE_OBJECT(SecondIterationTester);
+    
+  
+    //! This returns the currently set expdir (see setExperimentDirectory)
+    void setSplitter(string splitter_template);
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+};
+
+DECLARE_OBJECT_PTR(SecondIterationTester);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,381 @@
+// -*- C++ -*-
+
+// SecondIterationWrapper.cc
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ********************************************************************************    
+ * $Id: SecondIterationWrapper.cc, v 1.0 2004/07/19 10:00:00 Bengio/Kegl/Godbout        *
+ * This file is part of the PLearn library.                                     *
+ ******************************************************************************** */
+
+#include "SecondIterationWrapper.h"
+#include <plearn/vmat/FileVMatrix.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(SecondIterationWrapper,
+                        "A PLearner to wrap around a generic regressor to calculate the predicted class.", 
+                        "Algorithm built to wrap around a generic regressor in the context of the second iteration\n"
+                        "of the annual sales estimation project.\n"
+                        "It calculates the predicted class and computes the cse error.\n"
+    );
+
+SecondIterationWrapper::SecondIterationWrapper()  
+  : class_prediction(1)
+{
+}
+
+SecondIterationWrapper::~SecondIterationWrapper()
+{
+}
+
+void SecondIterationWrapper::declareOptions(OptionList& ol)
+{ 
+    declareOption(ol, "class_prediction", &SecondIterationWrapper::class_prediction, OptionBase::buildoption,
+                  "When set to 1 (default), indicates the base regression is on the class target.\n"
+                  "Otherwise, we assume the regression is on the sales target.\n"); 
+ 
+    declareOption(ol, "base_regressor_template", &SecondIterationWrapper::base_regressor_template, OptionBase::buildoption,
+                  "The template for the base regressor to be used.\n");  
+ 
+    declareOption(ol, "ref_train", &SecondIterationWrapper::ref_train, OptionBase::buildoption,
+                  "The reference set to compute train statistics.\n");  
+ 
+    declareOption(ol, "ref_test", &SecondIterationWrapper::ref_test, OptionBase::buildoption,
+                  "The reference set to compute test statistice.\n");  
+ 
+    declareOption(ol, "ref_sales", &SecondIterationWrapper::ref_sales, OptionBase::buildoption,
+                  "The reference set to de-gaussianize the prediction.\n"); 
+ 
+    declareOption(ol, "train_dataset", &SecondIterationWrapper::train_dataset, OptionBase::buildoption,
+                  "The train data set.\n"); 
+ 
+    declareOption(ol, "test_dataset", &SecondIterationWrapper::test_dataset, OptionBase::buildoption,
+                  "The test data set.\n"); 
+      
+    declareOption(ol, "base_regressor", &SecondIterationWrapper::base_regressor, OptionBase::learntoption,
+                  "The base regressor built from the template.\n");
+    inherited::declareOptions(ol);
+}
+
+void SecondIterationWrapper::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(class_prediction, copies);
+    deepCopyField(ref_train, copies);
+    deepCopyField(ref_test, copies);
+    deepCopyField(ref_sales, copies);
+    deepCopyField(test_dataset, copies);
+    deepCopyField(base_regressor_template, copies);
+    deepCopyField(base_regressor, copies);
+}
+
+void SecondIterationWrapper::build()
+{
+    inherited::build();
+    build_();
+}
+
+void SecondIterationWrapper::build_()
+{
+    if (train_set)
+    {
+        if (class_prediction == 0)
+            if(!ref_train || !ref_test || !ref_test || !train_dataset || !test_dataset)
+                PLERROR("In SecondIterationWrapper: missing reference data sets to compute statistics");
+        margin = 0;
+        loan = 1;
+        sales = 2;
+        tclass = 3;
+        base_regressor = ::PLearn::deepCopy(base_regressor_template);
+        base_regressor->setTrainingSet(train_set, true);
+        base_regressor->setTrainStatsCollector(new VecStatsCollector);
+        if (class_prediction == 0) search_table = ref_sales->toMat();
+    }
+}
+
+void SecondIterationWrapper::train()
+{
+    base_regressor->setOption("nstages", tostring(nstages));
+    base_regressor->train();
+    if (class_prediction == 1) computeClassStatistics();
+    else computeSalesStatistics();
+}
+
+void SecondIterationWrapper::computeClassStatistics()
+{
+    int row;
+    Vec sample_input(train_set->inputsize());
+    Vec sample_target(train_set->targetsize());
+    real sample_weight;
+    Vec sample_output(base_regressor->outputsize());
+    Vec sample_costs(3);
+    ProgressBar* pb = NULL;
+    if (report_progress)
+    {
+        pb = new ProgressBar("Second Iteration : computing the train statistics: ", train_set->length());
+    } 
+    train_stats->forget();
+    for (row = 0; row < train_set->length(); row++)
+    {  
+        train_set->getExample(row, sample_input, sample_target, sample_weight);
+        computeOutput(sample_input, sample_output);
+        computeCostsFromOutputs(sample_input, sample_output, sample_target, sample_costs); 
+        train_stats->update(sample_costs);
+        if (report_progress) pb->update(row);
+    }
+    train_stats->finalize();
+    if (report_progress) delete pb; 
+}
+
+void SecondIterationWrapper::computeSalesStatistics()
+{
+    int row;
+    Vec sample_input(train_set->inputsize());
+    Vec sample_target(train_set->targetsize());
+    Vec reference_vector(ref_train->width());
+    real sample_weight;
+    Vec sample_output(base_regressor->outputsize());
+    Vec sample_costs(3);
+    Vec train_mean(3);
+    Vec train_std_error(3);
+    Vec valid_mean(3);
+    Vec valid_std_error(3);
+    Vec test_mean(3);
+    Vec test_std_error(3);
+    real sales_prediction;
+    real commitment;
+    real predicted_class;
+    ProgressBar* pb = NULL;
+    if (report_progress)
+    {
+        pb = new ProgressBar("Second Iteration : computing the train statistics: ", train_set->length());
+    } 
+    train_stats->forget();
+    for (row = 0; row < train_set->length(); row++)
+    {  
+        train_set->getExample(row, sample_input, sample_target, sample_weight);
+        ref_train->getRow(row, reference_vector);
+        computeOutput(sample_input, sample_output);
+        sales_prediction = deGaussianize(sample_output[0]);
+        commitment = 0.0;
+        if (!is_missing(reference_vector[margin])) commitment += reference_vector[margin];
+        if (!is_missing(reference_vector[loan])) commitment += reference_vector[loan];
+        if (sales_prediction < 1000000.0 && commitment < 200000.0) predicted_class = 1.0;
+        else if (sales_prediction < 10000000.0 && commitment < 1000000.0) predicted_class = 2.0;
+        else if (sales_prediction < 100000000.0 && commitment < 20000000.0) predicted_class = 3.0;
+        else predicted_class = 4.0;
+        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2.0);
+        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2.0);
+        if (predicted_class == reference_vector[tclass]) sample_costs[2] = 0.0;
+        else sample_costs[2] = 1.0;
+        train_stats->update(sample_costs);
+        if (report_progress) pb->update(row);
+    }
+    train_stats->finalize();
+    train_mean << train_stats->getMean();
+    train_std_error << train_stats->getStdError();
+    if (report_progress) delete pb; 
+    if (report_progress)
+    {
+        pb = new ProgressBar("Second Iteration : computing the valid statistics: ", train_dataset->length() - train_set->length());
+    } 
+    train_stats->forget();
+    for (row = train_set->length(); row < train_dataset->length(); row++)
+    {  
+        train_dataset->getExample(row, sample_input, sample_target, sample_weight);
+        ref_train->getRow(row, reference_vector);
+        computeOutput(sample_input, sample_output);
+        sales_prediction = deGaussianize(sample_output[0]);
+        commitment = 0.0;
+        if (!is_missing(reference_vector[margin])) commitment += reference_vector[margin];
+        if (!is_missing(reference_vector[loan])) commitment += reference_vector[loan];
+        if (sales_prediction < 1000000.0 && commitment < 200000.0) predicted_class = 1.0;
+        else if (sales_prediction < 10000000.0 && commitment < 1000000.0) predicted_class = 2.0;
+        else if (sales_prediction < 100000000.0 && commitment < 20000000.0) predicted_class = 3.0;
+        else predicted_class = 4.0;
+        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2.0);
+        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2.0);
+        if (predicted_class == reference_vector[tclass]) sample_costs[2] = 0.0;
+        else sample_costs[2] = 1.0;
+        train_stats->update(sample_costs);
+        if (report_progress) pb->update(row);
+    }
+    train_stats->finalize();
+    valid_mean << train_stats->getMean();
+    valid_std_error << train_stats->getStdError();
+    if (report_progress) delete pb; 
+    if (report_progress)
+    {
+        pb = new ProgressBar("Second Iteration : computing the test statistics: ", test_dataset->length());
+    } 
+    train_stats->forget();
+    for (row = 0; row < test_dataset->length(); row++)
+    {  
+        test_dataset->getExample(row, sample_input, sample_target, sample_weight);
+        ref_test->getRow(row, reference_vector);
+        computeOutput(sample_input, sample_output);
+        sales_prediction = deGaussianize(sample_output[0]);
+        commitment = 0.0;
+        if (!is_missing(reference_vector[margin])) commitment += reference_vector[margin];
+        if (!is_missing(reference_vector[loan])) commitment += reference_vector[loan];
+        if (sales_prediction < 1000000.0 && commitment < 200000.0) predicted_class = 1.0;
+        else if (sales_prediction < 10000000.0 && commitment < 1000000.0) predicted_class = 2.0;
+        else if (sales_prediction < 100000000.0 && commitment < 20000000.0) predicted_class = 3.0;
+        else predicted_class = 4.0;
+        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2.0);
+        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2.0);
+        if (predicted_class == reference_vector[tclass]) sample_costs[2] = 0.0;
+        else sample_costs[2] = 1.0;
+        train_stats->update(sample_costs);
+        if (report_progress) pb->update(row);
+    }
+    train_stats->finalize();
+    test_mean << train_stats->getMean();
+    test_std_error << train_stats->getStdError();
+    if (report_progress) delete pb;
+    TVec<string> stat_names(6);
+    stat_names[0] = "mse";
+    stat_names[1] = "mse_stderr";
+    stat_names[2] = "cse";
+    stat_names[3] = "cse_stderr";
+    stat_names[4] = "cle";
+    stat_names[5] = "cle_stderr";
+    VMat stat_file = new FileVMatrix(expdir + "class_stats.pmat", 3, stat_names);
+    stat_file->put(0, 0, train_mean[0]);
+    stat_file->put(0, 1, train_std_error[0]);
+    stat_file->put(0, 2, train_mean[1]);
+    stat_file->put(0, 3, train_std_error[1]);
+    stat_file->put(0, 4, train_mean[2]);
+    stat_file->put(0, 5, train_std_error[2]);
+    stat_file->put(1, 0, valid_mean[0]);
+    stat_file->put(1, 1, valid_std_error[0]);
+    stat_file->put(1, 2, valid_mean[1]);
+    stat_file->put(1, 3, valid_std_error[1]);
+    stat_file->put(1, 4, valid_mean[2]);
+    stat_file->put(1, 5, valid_std_error[2]);
+    stat_file->put(2, 0, test_mean[0]);
+    stat_file->put(2, 1, test_std_error[0]);
+    stat_file->put(2, 2, test_mean[1]);
+    stat_file->put(2, 3, test_std_error[1]);
+    stat_file->put(2, 4, test_mean[2]);
+    stat_file->put(2, 5, test_std_error[2]);
+}
+
+real SecondIterationWrapper::deGaussianize(real prediction)
+{
+    if (prediction < search_table(0, 0)) return search_table(0, 1);
+    if (prediction > search_table(search_table.length() - 1, 0)) return search_table(search_table.length() - 1, 1);
+    int mid;
+    int min = 0;
+    int max = search_table.length() - 1;
+    while (max - min > 1)
+    {
+        mid = (max + min) / 2;
+        real mid_val = search_table(mid, 0);
+        if (prediction < mid_val) max = mid;
+        else if (prediction > mid_val) min = mid;
+        else min = max = mid;
+    }
+    if (min == max) return search_table(min, 1);
+    return (search_table(min, 1) + search_table(max, 1)) / 2.0;
+}
+
+void SecondIterationWrapper::forget()
+{
+}
+
+int SecondIterationWrapper::outputsize() const
+{
+    return base_regressor->outputsize();
+}
+
+TVec<string> SecondIterationWrapper::getTrainCostNames() const
+{
+    TVec<string> return_msg(3);
+    return_msg[0] = "mse";
+    return_msg[1] = "cse";
+    return_msg[2] = "cle";
+    return return_msg;
+}
+
+TVec<string> SecondIterationWrapper::getTestCostNames() const
+{ 
+    return getTrainCostNames();
+}
+
+void SecondIterationWrapper::computeOutput(const Vec& inputv, Vec& outputv) const
+{
+    base_regressor->computeOutput(inputv, outputv);
+}
+
+void SecondIterationWrapper::computeOutputAndCosts(const Vec& inputv, const Vec& targetv, Vec& outputv, Vec& costsv) const
+{
+    computeOutput(inputv, outputv);
+    computeCostsFromOutputs(inputv, outputv, targetv, costsv);
+}
+
+void SecondIterationWrapper::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
+{
+    costsv[0] = pow((outputv[0] - targetv[0]), 2.0);
+    if (class_prediction == 1)
+    {
+        real class_pred;
+        if (outputv[0] <= 1.5) class_pred = 1.0;
+        else if (outputv[0] <= 2.5) class_pred = 2.0;
+        else class_pred = 3.0;
+        costsv[1] = pow((class_pred - targetv[0]), 2.0);
+        if (class_pred == targetv[0]) costsv[2] = 0.0;
+        else costsv[2] = 1.0;
+        return;
+    }
+    costsv[1] = 0.0;
+    costsv[2] = 0.0;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,131 @@
+// -*- C++ -*-
+
+// SecondIterationWrapper.h
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ********************************************************************************    
+ * $Id: SecondIterationWrapper.h, v 1.0 2004/07/19 10:00:00 Bengio/Kegl/Godbout   *
+ * This file is part of the PLearn library.                                     *
+ ******************************************************************************** */
+
+/*! \file PLearnLibrary/PLearnAlgo/SecondIterationWrapper.h */
+
+#ifndef SecondIterationWrapper_INC
+#define SecondIterationWrapper_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn/base/stringutils.h>
+
+namespace PLearn {
+using namespace std;
+
+class SecondIterationWrapper: public PLearner
+{
+    typedef PLearner inherited;
+  
+private:
+
+/*
+  Build options: they have to be set before training
+*/
+
+    int class_prediction;
+    VMat ref_train;
+    VMat ref_test;
+    VMat ref_sales;
+    VMat train_dataset;
+    VMat test_dataset;
+    PP<PLearner> base_regressor_template;                     // template for a generic regressor as the base learner to be boosted 
+  
+/*
+  Learnt options: they are sized and initialized if need be, at stage 0
+*/
+
+
+    PP<PLearner> base_regressor;                              // base regressors built at each boosting stage 
+ 
+/*
+  Work fields: they are sized and initialized if need be, at buid time
+*/ 
+    int margin;
+    int loan;
+    int sales;
+    int tclass;
+    Mat search_table;
+    
+   
+  
+public:
+    SecondIterationWrapper();
+    virtual              ~SecondIterationWrapper();
+    
+    PLEARN_DECLARE_OBJECT(SecondIterationWrapper);
+
+    static  void         declareOptions(OptionList& ol);
+    virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
+    virtual void         build();
+    virtual void         train();
+    virtual void         forget();
+    virtual int          outputsize() const;
+    virtual TVec<string> getTrainCostNames() const;
+    virtual TVec<string> getTestCostNames() const;
+    virtual void         computeOutput(const Vec& input, Vec& output) const;
+    virtual void         computeOutputAndCosts(const Vec& input, const Vec& target, Vec& output, Vec& costs) const;
+    virtual void         computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const;
+  
+private:
+    void         build_();
+    void         computeClassStatistics();
+    void         computeSalesStatistics();
+    real         deGaussianize(real prediction);
+};
+
+DECLARE_OBJECT_PTR(SecondIterationWrapper);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,609 @@
+// -*- C++ -*-
+
+// TestImputations.cc
+//
+// Copyright (C) 2006 Dan Popovici, Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file TestImputations.cc */
+
+#define PL_LOG_MODULE_NAME "TestImputations"
+#include <plearn/io/pl_log.h>
+
+#include "TestImputations.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    TestImputations,
+    "Computes imputations errors using various imputation methods.",
+    "name of the discrete variable, of the target and the values to check are options.\n"
+);
+
+/////////////////////////
+// TestImputations //
+/////////////////////////
+TestImputations::TestImputations()
+{
+}
+    
+////////////////////
+// declareOptions //
+////////////////////
+void TestImputations::declareOptions(OptionList& ol)
+{
+
+    declareOption(ol, "min_number_of_samples", &TestImputations::min_number_of_samples,
+                  OptionBase::buildoption,
+                  "The minimum number of samples required to test imputations for a variable.");
+    declareOption(ol, "max_number_of_samples", &TestImputations::max_number_of_samples,
+                  OptionBase::buildoption,
+                  "The maximum number of samples used to test imputations for a variable.");
+    declareOption(ol, "mean_median_mode_file_name", &TestImputations::mean_median_mode_file_name,
+                  OptionBase::buildoption,
+                  "The Path of the file with those statistics for all the variables.");
+    declareOption(ol, "tree_conditional_mean_directory", &TestImputations::tree_conditional_mean_directory,
+                  OptionBase::buildoption,
+                  "The Path of the dircetory containing the tree conditional means computed for each variable.");
+    declareOption(ol, "covariance_preservation_file_name", &TestImputations::covariance_preservation_file_name,
+                  OptionBase::buildoption,
+                  "The Path of the file with the train_set empirically observed covariances and means.");
+    declareOption(ol, "reference_set_with_covpres", &TestImputations::reference_set_with_covpres,
+                  OptionBase::buildoption,
+                  "The reference set corresponding to the index computed with the ball_tree, with the initial imputations.");
+    declareOption(ol, "reference_set_with_missing", &TestImputations::reference_set_with_missing,
+                  OptionBase::buildoption,
+                  "The reference set corresponding to the index computed with the ball_tree, with missing values.");
+    declareOption(ol, "missing_indicators", &TestImputations::missing_indicators,
+                  OptionBase::buildoption,
+                  "The vector of missing indicator field names to be excluded in the distance computation.");
+
+    inherited::declareOptions(ol);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void TestImputations::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(min_number_of_samples, copies);
+    deepCopyField(max_number_of_samples, copies);
+    deepCopyField(mean_median_mode_file_name, copies);
+    deepCopyField(tree_conditional_mean_directory, copies);
+    deepCopyField(covariance_preservation_file_name, copies);
+    deepCopyField(reference_set_with_covpres, copies);
+    deepCopyField(reference_set_with_missing, copies);
+    deepCopyField(missing_indicators, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+}
+
+///////////
+// build //
+///////////
+void TestImputations::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void TestImputations::build_()
+{
+/*
+  for each variable with missing values in the train set(which is in this case the test set)
+    - radomly choose up to n samples with a value in the variable
+    - build a set with these samples replacing the value with missing
+    - perform the various type of imputations and compute the errors
+  valider meanmedianmode, treeconditionalmean  covariancepreservation, neighborhood
+  create a Mat: width is #of variables with missing values
+  row 0: nb_present
+  row 1: mean/mode imputation from preprocessing/final_train_input_preprocessed.pmat.metadata/mean_median_mode_file.pmat
+  row 2: median/mode imputation from preprocessing/final_train_input_preprocessed.pmat.metadata/mean_median_mode_file.pmat
+  row 3: treeconditionalmean imputation from prep/data/targeted_ind_no_imp.vmat.metadata/TreeCondMean/dir/'field_names'/Split0/test1_outputs.pmat
+  row 4: covariance preservation imputation from preprocessing/final_train_input_preprocessed.pmat.metadata/covariance_file.pmat
+  row 5 to 24: (row - 4) * i neighbors imputation from neighborhood/test_train_imputed_with_covariance_preservation.pmat.metadata/neighborhood_file.pmat
+  lire le train_set
+*/
+    MODULE_LOG << "build_() called" << endl;
+    if (train_set)
+    {
+        build_ball_tree();
+        for (int iteration = 1; iteration <= 50; iteration++)
+        {
+            cout << "In TestImputations, Iteration # " << iteration << endl;
+            initialize();
+            computeMeanMedianModeStats();
+            computeTreeCondMeanStats();
+            computeCovPresStats();
+            computeNeighborhoodStats();
+            train();
+        }
+        PLERROR("In TestImputations: we are done here");
+    }
+}
+
+void TestImputations::build_ball_tree()
+{
+    // initialize primary dataset
+    cout << "initialize the train set" << endl;
+    train_row = 0;
+    train_col = 0;
+    train_length = train_set->length();
+    train_width = train_set->width();
+    train_input.resize(train_width);
+    train_names.resize(train_width);
+    train_names << train_set->fieldNames();
+    train_metadata = train_set->getMetaDataDir();
+    weights.resize(train_width);
+    weights.fill(1.0);
+    for (mi_col = 0; mi_col < missing_indicators.length(); mi_col++)
+    {
+        for (train_col = 0; train_col < train_width; train_col++)
+        {
+            if (missing_indicators[mi_col] != train_names[train_col]) continue;
+            weights[train_col] = 0.0;
+            break;
+        }
+        if (train_col >= train_width)
+            PLERROR("In TestImputations:: no field with this name in input dataset: %s", (missing_indicators[mi_col]).c_str());
+    }
+    weighted_distance_kernel = new WeightedDistance(weights);
+/*
+    if (!reference_set_with_covpres) PLERROR("In TestImputations:: no reference_set_with_covpres provided.");
+    if (!reference_set_with_missing) PLERROR("In TestImputations:: no reference_set_with_missing provided.");
+    ball_tree = new BallTreeNearestNeighbors();
+    ball_tree->setOption("rmin", "1");
+    ball_tree->setOption("train_method", "anchor");
+    ball_tree->setOption("num_neighbors", "100");
+    ball_tree->setOption("copy_input", "0");
+    ball_tree->setOption("copy_target", "0");
+    ball_tree->setOption("copy_weight", "0");
+    ball_tree->setOption("copy_index", "1");
+    ball_tree->setOption("nstages", "-1");
+    ball_tree->setOption("report_progress", "1");
+    ball_tree->setTrainingSet(reference_set_with_covpres, true);
+    ball_tree->train();
+    ref_cov = reference_set_with_covpres->toMat();
+    ref_mis = reference_set_with_missing->toMat();
+*/
+    if (!reference_set_with_covpres) PLERROR("In TestImputations:: no reference_set_with_covpres provided.");
+    if (!reference_set_with_missing) PLERROR("In TestImputations:: no reference_set_with_missing provided.");
+    ball_tree = new ExhaustiveNearestNeighbors();
+    ball_tree->setOption("num_neighbors", "100");
+    ball_tree->setOption("copy_input", "0");
+    ball_tree->setOption("copy_target", "0");
+    ball_tree->setOption("copy_weight", "0");
+    ball_tree->setOption("copy_index", "1");
+    ball_tree->setOption("nstages", "-1");
+    ball_tree->setOption("report_progress", "1");
+    ball_tree->distance_kernel = weighted_distance_kernel;
+    ball_tree->setTrainingSet(reference_set_with_covpres, true);
+    ball_tree->train();
+    ref_cov = reference_set_with_covpres->toMat();
+    ref_mis = reference_set_with_missing->toMat();
+/*
+ExhaustiveNearestNeighbors(
+# bool: Whether the kernel defined by the 'distance_kernel' option should be
+# interpreted as a (pseudo-)distance measure (true) or a similarity
+# measure (false). Default = true.  Note that this interpretation is
+# strictly specific to the class ExhaustiveNearestNeighbors.
+kernel_is_pseudo_distance = 1  ;
+
+# Ker: Alternate name for 'distance_kernel'.  (Deprecated; use only so that
+# existing scripts can run.)
+kernel = *1 ->DistanceKernel(
+n = 2 ;
+pow_distance = 0 ;
+optimized = 0 ;
+is_symmetric = 1 ;
+report_progress = 0 ;
+specify_dataset = *0 ;
+cache_gram_matrix = 0 ;
+data_inputsize = -1 ;
+n_examples = -1  )
+ ;
+
+# int: Number of nearest-neighbors to compute.  This is usually called "K".
+# The output vector is simply the concatenation of all found neighbors.
+# (Default = 1)
+num_neighbors = 1  ;
+
+# bool: If true, the output contains a copy of the found input vector(s).
+# (Default = false)
+copy_input = 0  ;
+
+# bool: If true, the output contains a copy of the found target vector(s).
+# (Default = true)
+copy_target = 1  ;
+
+# bool: If true, the output contains a copy of the found weight.  If no
+# weight is present in the training set, a weight of 1.0 is put.
+# (Default = true)
+copy_weight = 0  ;
+
+# bool: If true, the output contains the index of the found neighbor
+# (as the row number, zero-based, in the training set.)
+# (Default = false)
+copy_index = 0  ;
+
+# Ker: An optional alternative to the Euclidean distance (DistanceKernel with
+# n=2 and pow_distance=1).  It should be a 'distance-like' kernel rather
+# than a 'dot-product-like' kernel, i.e. small when the arguments are
+# similar, and it should always be non-negative, and 0 only if arguments
+# are equal.
+distance_kernel = *1 ->DistanceKernel(
+n = 2 ;
+pow_distance = 0 ;
+optimized = 0 ;
+is_symmetric = 1 ;
+report_progress = 0 ;
+specify_dataset = *0 ;
+cache_gram_matrix = 0 ;
+data_inputsize = -1 ;
+n_examples = -1  )
+ ;
+*/
+
+}
+
+void TestImputations::initialize()
+{
+    
+    // initialize the header file
+    cout << "initialize the header file" << endl;
+    train_set->lockMetaDataDir();
+    header_record.resize(train_width);
+    header_file_name = train_metadata + "/TestImputation/header.pmat";
+    if (!isfile(header_file_name)) createHeaderFile();
+    else getHeaderRecord();
+    
+    // choose a variable to test imputations for
+    cout << "choose a variable to test imputations for" << endl;
+    to_deal_with_total = 0;
+    to_deal_with_next = -1;
+    for (train_col = 0; train_col < train_width; train_col++)
+    {
+        if (header_record[train_col] != 1.0) continue;
+        to_deal_with_total += 1;
+        if (to_deal_with_next < 0) to_deal_with_next = train_col;
+    }
+    if (to_deal_with_next < 0)
+    {
+        train_set->unlockMetaDataDir();
+        // reviewGlobalStats();
+        PLERROR("In TestImputations:: we are done here");
+    }
+    to_deal_with_name = train_names[to_deal_with_next];
+    cout << "total number of variable left to deal with: " << to_deal_with_total << endl;
+    cout << "next variable to deal with: " << train_names[to_deal_with_next] << endl;
+    updateHeaderRecord(to_deal_with_next);
+    train_set->unlockMetaDataDir();
+    
+    // find the available samples with non-missing values for this variable
+    pb = 0;
+    train_stats = train_set->getStats(to_deal_with_next);
+    train_total = train_stats.n();
+    train_missing = train_stats.nmissing();
+    train_present = train_total - train_missing;
+    indices.resize((int) train_present);
+    ind_next = 0;
+    pb = new ProgressBar( "Building the indices for " + to_deal_with_name, train_length);
+    for (train_row = 0; train_row < train_length; train_row++)
+    {
+        to_deal_with_value = train_set->get(train_row, to_deal_with_next);
+        if (is_missing(to_deal_with_value)) continue;
+        if (ind_next >= indices.length()) PLERROR("In TestImputations:: There seems to be more present values than indicated by the stats file");
+        indices[ind_next] = train_row;
+        ind_next += 1;
+        pb->update( train_row );
+    }
+    delete pb;
+    
+    // shuffle the indices.
+    manual_seed(123456);
+    shuffleElements(indices);
+    
+    // load the test samples for this variable
+    if (indices.length() > max_number_of_samples) test_length = max_number_of_samples;
+    else test_length = indices.length();
+    test_width = train_width;
+    test_samples_set = new MemoryVMatrix(test_length, test_width);
+    pb = new ProgressBar( "Loading the test samples for " + to_deal_with_name, test_length);
+    for (test_row = 0; test_row < test_length; test_row++)
+    {
+        train_set->getRow(indices[test_row], train_input);
+        test_samples_set->putRow(test_row, train_input);
+        pb->update( test_row );
+    }
+    delete pb;
+}
+
+void TestImputations::computeMeanMedianModeStats()
+{
+    if (!isfile(mean_median_mode_file_name)) PLERROR("In TestImputations:: a valid mean_median_mode_file path must be provided.");
+    mmmf_file = new FileVMatrix(mean_median_mode_file_name);
+    mmmf_length = mmmf_file->length();
+    mmmf_width = mmmf_file->width();
+    if (mmmf_length != 3) PLERROR("In TestImputations:: there should be exactly 3 records in the mmm file, got %i.", mmmf_length);
+    if (mmmf_width != train_width) PLERROR("In TestImputations:: train set and mmm width should be the same, got %i.", mmmf_width);
+    mmmf_mean = mmmf_file->get(0, to_deal_with_next);
+    mmmf_median = mmmf_file->get(1, to_deal_with_next);
+    mmmf_mode = mmmf_file->get(2, to_deal_with_next);
+    mmmf_mean_err = 0.0;
+    mmmf_median_err = 0.0;
+    mmmf_mode_err = 0.0;
+    pb = new ProgressBar( "computing the mean, median and mode imputation errors for " + to_deal_with_name, test_length);
+    for (test_row = 0; test_row < test_length; test_row++)
+    {
+        to_deal_with_value = test_samples_set->get(test_row, to_deal_with_next);
+        mmmf_mean_err += pow(to_deal_with_value - mmmf_mean, 2.0);
+        mmmf_median_err += pow(to_deal_with_value - mmmf_median, 2.0);
+        mmmf_mode_err += pow(to_deal_with_value - mmmf_mode, 2.0);
+        pb->update( test_row );
+    }
+    delete pb;
+    mmmf_mean_err = mmmf_mean_err / (real) test_length;
+    mmmf_median_err = mmmf_median_err / (real) test_length;
+    mmmf_mode_err = mmmf_mode_err / (real) test_length;
+}
+
+void TestImputations::computeTreeCondMeanStats()
+{
+    tcmf_file_name = tree_conditional_mean_directory + "/" + to_deal_with_name + "/Split0/test1_outputs.pmat";
+    if (!isfile(tcmf_file_name)) PLERROR("In TestImputations:: a file was not found in the tcf directory.");
+    tcmf_file = new FileVMatrix(tcmf_file_name);
+    tcmf_length = tcmf_file->length();
+    tcmf_width = tcmf_file->width();
+    if (tcmf_length < train_length) PLERROR("In TestImputations:: there are not enough records in the tree conditional output file.");
+    tcmf_mean_err = 0.0;
+    pb = new ProgressBar( "computing the tree conditional mean imputation errors for " + to_deal_with_name, test_length);
+    for (test_row = 0; test_row < test_length; test_row++)
+    {
+        to_deal_with_value = test_samples_set->get(test_row, to_deal_with_next);
+        tcmf_mean_err += pow(to_deal_with_value - tcmf_file->get(indices[test_row], 0), 2.0);
+        pb->update( test_row );
+    }
+    delete pb;
+    tcmf_mean_err = tcmf_mean_err / (real) test_length;
+}
+
+void TestImputations::computeCovPresStats()
+{
+    if (!isfile(covariance_preservation_file_name)) PLERROR("In TestImputations:: a valid covariance_preservation_file path must be provided.");
+    cvpf_file = new FileVMatrix(covariance_preservation_file_name);
+    cvpf_length = cvpf_file->length();
+    cvpf_width = cvpf_file->width();
+    if (cvpf_length != train_width + 1) PLERROR("In TestImputations:: there should be %i records in the cvp file, got %i.", train_width + 1, cvpf_length);
+    if (cvpf_width != train_width) PLERROR("In TestImputations:: train set and cvp width should be the same, got %i.", cvpf_width);
+    cvpf_file = new FileVMatrix(covariance_preservation_file_name);
+    cvpf_cov.resize(train_width, train_width);
+    cvpf_mu.resize(train_width);
+    for (cvpf_row = 0; cvpf_row < train_width; cvpf_row++)
+    {
+        for (cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
+        {
+            cvpf_cov(cvpf_row, cvpf_col) = cvpf_file->get(cvpf_row, cvpf_col);
+        }
+    }
+    for (cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
+    {
+        cvpf_mu[cvpf_col] = cvpf_file->get(train_width, cvpf_col);
+    }
+    cvpf_mean_err = 0.0;
+    pb = new ProgressBar( "computing the covariance preservation imputation errors for " + to_deal_with_name, test_length);
+    for (test_row = 0; test_row < test_length; test_row++)
+    {
+        test_samples_set->getRow(test_row, train_input);
+        cvpf_mean_err += pow(to_deal_with_value - covariancePreservationValue(to_deal_with_next), 2.0);
+        pb->update( test_row );
+    }
+    delete pb;
+    cvpf_mean_err = cvpf_mean_err / (real) test_length;
+}
+
+real TestImputations::covariancePreservationValue(int col)
+{
+    cvpf_sum_cov_xl = 0;
+    cvpf_sum_xl_square = 0;
+    for (cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
+    {
+        if (cvpf_col == col) continue;
+        if (is_missing(train_input[cvpf_col])) continue;
+        cvpf_sum_cov_xl += cvpf_cov(cvpf_col, col) * (train_input[cvpf_col] - cvpf_mu[cvpf_col]);
+        cvpf_sum_xl_square += (train_input[cvpf_col] - cvpf_mu[cvpf_col]) * (train_input[cvpf_col] - cvpf_mu[cvpf_col]);
+    }
+    if (cvpf_sum_xl_square == 0.0) cvpf_value = cvpf_mu[col];
+    else cvpf_value = cvpf_mu[col] + cvpf_sum_cov_xl / cvpf_sum_xl_square;
+    return cvpf_value;
+}
+
+void TestImputations::computeNeighborhoodStats()
+{
+    knnf_input.resize(train_width);
+    knnf_neighbors.resize(100);
+    knnf_mean_err.resize(100);
+    knnf_mean_err.clear();
+    pb = new ProgressBar( "computing the neighborhood imputation errors for " + to_deal_with_name, test_length);
+    for (test_row = 0; test_row < test_length; test_row++)
+    {
+        test_samples_set->getRow(test_row, train_input);
+        for (test_col = 0; test_col < train_width; test_col++)
+        {
+            if (test_col == to_deal_with_next) knnf_input[test_col] = covariancePreservationValue(test_col);
+            else if (is_missing(train_input[test_col])) knnf_input[test_col] = covariancePreservationValue(test_col);
+            else knnf_input[test_col] = train_input[test_col];
+        }
+        ball_tree->computeOutput(knnf_input, knnf_neighbors);
+        knnf_sum_value = 0.0;
+        knnf_sum_cov = 0.0;
+        knnv_value_count = 0.0;
+        for (knnf_row = 0; knnf_row < 100; knnf_row++)
+        {
+            knnf_value = ref_mis((int) knnf_neighbors[knnf_row], to_deal_with_next);
+            if (!is_missing(knnf_value))
+            {
+                knnf_sum_value += knnf_value;
+                knnv_value_count += 1.0;
+            }
+            if (knnv_value_count > 0.0)
+            {
+                knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_value / knnv_value_count), 2.0);
+                continue;
+            }
+            knnf_value = ref_cov((int) knnf_neighbors[knnf_row], to_deal_with_next);
+            if (is_missing(knnf_value))
+                PLERROR("In TestImputations::missing value found in the reference with covariance preserved at: %i , %i",
+                         (int) knnf_neighbors[knnf_row], to_deal_with_next);
+            knnf_sum_cov += knnf_value;
+            knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_cov / (knnf_row + 1)), 2.0);
+        }
+        pb->update( test_row );
+    }
+    delete pb;
+    for (knnf_row = 0; knnf_row < 100; knnf_row++) knnf_mean_err[knnf_row] = knnf_mean_err[knnf_row] /  (real) test_length;
+}
+
+void TestImputations::createHeaderFile()
+{ 
+    for (train_col = 0; train_col < train_width; train_col++)
+    {
+        train_stats = train_set->getStats(train_col);
+        train_total = train_stats.n();
+        train_missing = train_stats.nmissing();
+        train_present = train_total - train_missing;
+        if (train_missing <= 0.0) header_record[train_col] = 0.0;                       // no missing, noting to do.
+        else if (train_present < min_number_of_samples) header_record[train_col] = 0.0; // should not happen
+        else header_record[train_col] = 1.0;                                            // test imputations
+    }
+    header_file = new FileVMatrix(header_file_name, 1, train_names);
+    header_file->putRow(0, header_record);
+}
+
+void TestImputations::getHeaderRecord()
+{ 
+    header_file = new FileVMatrix(header_file_name, true);
+    header_file->getRow(0, header_record);
+}
+
+void TestImputations::updateHeaderRecord(int var_col)
+{ 
+    header_file->put(0, var_col, 2.0);
+    header_file->flush();
+}
+
+void TestImputations::train()
+{
+    // initialize the output file
+    cout << "initialize the output file" << endl;
+    train_set->lockMetaDataDir();
+    output_record.resize(105);
+    output_file_name = train_metadata + "/TestImputation/output.pmat";
+    if (!isfile(output_file_name)) createOutputFile();
+    else getOutputRecord(to_deal_with_next);
+    output_record[0] = mmmf_mean_err;
+    output_record[1] = mmmf_median_err;
+    output_record[2] = mmmf_mode_err;
+    output_record[3] = tcmf_mean_err;
+    output_record[4] = cvpf_mean_err;
+    for (knnf_row = 0; knnf_row < 100; knnf_row++)
+    {
+       output_record[knnf_row + 5] = knnf_mean_err[knnf_row];
+    }
+    updateOutputRecord(to_deal_with_next);
+    train_set->unlockMetaDataDir();
+}
+
+void TestImputations::createOutputFile()
+{
+    output_names.resize(105);
+    output_names[0] = "mean";
+    output_names[1] = "median";
+    output_names[2] = "mode";
+    output_names[3] = "tree_cond";
+    output_names[4] = "cov_pres";
+    for (knnf_row = 0; knnf_row < 100; knnf_row++)
+    {
+       output_names[knnf_row + 5] = "KNN_" + tostring(knnf_row);
+    }
+    output_record.clear();
+    output_file = new FileVMatrix(output_file_name, train_width, output_names);
+    for (train_col = 0; train_col < train_width; train_col++)
+        output_file->putRow(train_col, output_record);
+}
+
+void TestImputations::getOutputRecord(int var_col)
+{ 
+    output_file = new FileVMatrix(output_file_name, true);
+    output_file->getRow(var_col, output_record);
+}
+
+void TestImputations::updateOutputRecord(int var_col)
+{ 
+    output_file->putRow(var_col, output_record);
+    output_file->flush();
+}
+
+int TestImputations::outputsize() const {return 0;}
+void TestImputations::computeOutput(const Vec&, Vec&) const {}
+void TestImputations::computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const {}
+TVec<string> TestImputations::getTestCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+TVec<string> TestImputations::getTrainCostNames() const
+{
+    TVec<string> result;
+    result.append( "MSE" );
+    return result;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,239 @@
+// -*- C++ -*-
+
+// TestImputations.h
+//
+// Copyright (C) 2006 Dan Popovici
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dan Popovici
+
+/*! \file TestImputations.h */
+
+
+#ifndef TestImputations_INC
+#define TestImputations_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/testers/PTester.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/vmat/MemoryVMatrix.h>
+#include <plearn/io/load_and_save.h>          //!<  For save
+#include <plearn/io/fileutils.h>              //!<  For isfile()
+#include <plearn/math/random.h>               //!<  For the seed stuff.
+#include <plearn/vmat/ExplicitSplitter.h>     //!<  For the splitter stuff.
+#include <plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h>
+#include <plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h>
+#include <plearn_learners/second_iteration/BallTreeNearestNeighbors.h>
+#include <plearn_learners/second_iteration/WeightedDistance.h>
+#include <plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h>
+#include <plearn_learners/second_iteration/Experimentation.h>
+
+namespace PLearn {
+
+/**
+ * Generate samples from a mixture of two gaussians
+ *
+ */
+class TestImputations : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    
+    //! The minimum number of samples required to test imputations for a variable.
+    int min_number_of_samples;
+    //! The maximum number of samples used to test imputations for a variable.
+    int max_number_of_samples;
+    //! The Path of the file with those statistics for all the variables.
+    PPath mean_median_mode_file_name;
+    //! The Path of the dircetory containing the tree conditional means computed for each variable.
+    PPath tree_conditional_mean_directory;
+    //! The Path of the file with the train_set empirically observed covariances and means.
+    PPath covariance_preservation_file_name;
+    //! The reference set corresponding to the index computed with the ball_tree, with the initial imputations.
+    VMat reference_set_with_covpres;
+    //! The reference set corresponding to the index computed with the ball_tree, with missing values.
+    VMat reference_set_with_missing;
+    //! The vector of missing indicator field names to be excluded in the distance computation.
+    TVec<string> missing_indicators;
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    TestImputations();
+    int outputsize() const;
+    void train();
+    void computeOutput(const Vec&, Vec&) const;
+    void computeCostsFromOutputs(const Vec&, const Vec&, const Vec&, Vec&) const;
+    TVec<string> getTestCostNames() const;
+    TVec<string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(TestImputations);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+    void build_ball_tree();
+    void initialize();
+    void computeMeanMedianModeStats();
+    void computeTreeCondMeanStats();
+    void computeCovPresStats();
+    real covariancePreservationValue(int col);
+    void computeNeighborhoodStats();
+    void createHeaderFile();
+    void getHeaderRecord();
+    void updateHeaderRecord(int var_col);
+    void createOutputFile();
+    void getOutputRecord(int var_col);
+    void updateOutputRecord(int var_col);
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    ProgressBar* pb;
+    ExhaustiveNearestNeighbors* ball_tree;
+    Mat ref_cov;
+    Mat ref_mis;
+    int train_length;
+    int train_width;
+    Vec train_input;
+    TVec<string> train_names;
+    PPath train_metadata;
+    StatsCollector train_stats;
+    real train_total;
+    real train_missing;
+    real train_present;
+    int train_row;
+    int train_col;
+    PPath header_file_name;
+    VMat header_file;
+    Vec header_record;
+    int to_deal_with_total;
+    int to_deal_with_next;
+    string to_deal_with_name;
+    real to_deal_with_value;
+    VMat test_samples_set;
+    TVec<int> indices;
+    int ind_next;
+    int test_length;
+    int test_width;
+    int test_row;
+    int test_col;
+    VMat mmmf_file;
+    int mmmf_length;
+    int mmmf_width;
+    real mmmf_mean;
+    real mmmf_median;
+    real mmmf_mode;
+    real mmmf_mean_err;
+    real mmmf_median_err;
+    real mmmf_mode_err;
+    PPath tcmf_file_name;
+    VMat tcmf_file;
+    int tcmf_length;
+    int tcmf_width;
+    real tcmf_mean_err;
+    VMat cvpf_file;
+    int cvpf_length;
+    int cvpf_width;
+    int cvpf_row;
+    int cvpf_col;
+    Mat cvpf_cov;
+    Vec cvpf_mu;
+    real cvpf_sum_cov_xl;
+    real cvpf_sum_xl_square;
+    real cvpf_value;
+    real cvpf_mean_err;
+    Vec knnf_input;
+    Vec knnf_neighbors;
+    Vec knnf_mean_err;
+    real knnf_value;
+    real knnf_sum_value;
+    real knnf_sum_cov;
+    real knnv_value_count;
+    int knnf_row;
+    Vec weights;
+    int mi_col;
+    WeightedDistance* weighted_distance_kernel;
+    PPath output_file_name;
+    VMat output_file;
+    Vec output_record;
+    TVec<string> output_names;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(TestImputations);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.cc	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.cc	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,125 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2002 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2001-2002 Nicolas Chapados, Ichiro Takeuchi, Jean-Sebastien Senecal
+// Copyright (C) 2002 Xiangdong Wang, Christian Dorion
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: WeightedDistance.cc 4253 2005-10-18 19:02:25Z tihocan $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#include "WeightedDistance.h"
+
+namespace PLearn {
+using namespace std;
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    WeightedDistance,
+    "Implements an weighted distance.",
+    "Output is as follows:\n"
+    "- k(x1,x2) = \\sum_i w[i]*(x1[i]-x2[i])^2\n"
+);
+
+////////////////////
+// WeightedDistance //
+////////////////////
+WeightedDistance::WeightedDistance()
+{
+}
+
+WeightedDistance::WeightedDistance(Vec the_weights)
+ : weights(the_weights)
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void WeightedDistance::declareOptions(OptionList& ol)
+{
+
+    declareOption(ol, "weights", &WeightedDistance::weights, OptionBase::buildoption, 
+                  "The vector of weights to apply to the distance computation.");
+
+    inherited::declareOptions(ol);
+}
+
+//////////////
+// evaluate //
+//////////////
+real WeightedDistance::evaluate(const Vec& x1, const Vec& x2) const
+{
+    if (weights.length() != x1.length()) PLERROR("In WeightedDistance: inconsistent length between weigths and x1");
+    if (weights.length() != x2.length()) PLERROR("In WeightedDistance: inconsistent length between weigths and x2");
+    real return_value = 0.0;
+    for (int i = 0; i < weights.length(); i++)
+    {
+        return_value += weights[i] * pow(x1[i] - x2[i], 2.0);
+    }
+    return return_value;
+}
+
+//////////////////
+// evaluate_i_j //
+//////////////////
+real WeightedDistance::evaluate_i_j(int i, int j) const
+{
+    PLERROR("In WeightedDistance: evaluate_i_j not implemented");
+    return 0.0;
+}
+
+////////////////////////////
+// setDataForKernelMatrix //
+////////////////////////////
+void WeightedDistance::setDataForKernelMatrix(VMat the_data)
+{
+    PLERROR("In WeightedDistance: setDataForKernelMatrix not implemented");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.h	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/WeightedDistance.h	2007-06-07 15:47:42 UTC (rev 7551)
@@ -0,0 +1,100 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2002 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2001-2002 Nicolas Chapados, Ichiro Takeuchi, Jean-Sebastien Senecal
+// Copyright (C) 2002 Xiangdong Wang, Christian Dorion
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: WeightedDistance.h 3994 2005-08-25 13:35:03Z chapados $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#ifndef WeightedDistance_INC
+#define WeightedDistance_INC
+
+#include <plearn/ker/Kernel.h>
+#include <plearn/base/tostring.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This class implements an Ln distance (defaults to L2 i.e. euclidean distance).
+class WeightedDistance: public Kernel
+{
+
+private:
+
+    typedef Kernel inherited;
+
+public:
+
+    Vec weights;
+
+    WeightedDistance();
+    WeightedDistance(Vec the_weights);
+    
+    PLEARN_DECLARE_OBJECT(WeightedDistance);
+
+    virtual string info() const
+    { return "WL2"; }
+
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+    virtual real evaluate_i_j(int i, int j) const;
+
+    //!  This method precomputes the squared norm for all the data to
+    //! later speed up evaluate methods, if n == 2.
+    virtual void setDataForKernelMatrix(VMat the_data);
+
+protected:
+    static void declareOptions(OptionList& ol);
+};
+
+DECLARE_OBJECT_PTR(WeightedDistance);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: branches/cgi-desjardin/scripts/appStart.py
===================================================================
--- branches/cgi-desjardin/scripts/appStart.py	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/scripts/appStart.py	2007-06-07 15:47:42 UTC (rev 7551)
@@ -1,4 +1,4 @@
-__cvs_id__ = "$Id: appStart.py,v 1.3 2004/12/21 16:23:47 dorionc Exp $"
+__cvs_id__ = "$Id$"
 
 import string, sys, fpformat, os, copy, time
 from popen2 import *

Modified: branches/cgi-desjardin/scripts/extract_classes
===================================================================
--- branches/cgi-desjardin/scripts/extract_classes	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/scripts/extract_classes	2007-06-07 15:47:42 UTC (rev 7551)
@@ -593,7 +593,7 @@
 // by a prior agreement signed with ApSTAT Technologies Inc.
 
 /* *******************************************************      
-   * $Id: extract_classes,v 1.3 2004/06/28 19:19:57 dorionc Exp $ 
+   * $Id$ 
    ******************************************************* */
 
 // Authors: Nicolas Chapados

Modified: branches/cgi-desjardin/scripts/mnd.py
===================================================================
--- branches/cgi-desjardin/scripts/mnd.py	2007-06-07 15:42:58 UTC (rev 7550)
+++ branches/cgi-desjardin/scripts/mnd.py	2007-06-07 15:47:42 UTC (rev 7551)
@@ -1,4 +1,4 @@
-__cvs_id__ = "$Id: mnd.py,v 1.2 2004/12/21 16:23:47 dorionc Exp $"
+__cvs_id__ = "$Id$"
 
 from Numeric import *
 from LinearAlgebra import *



From nouiz at mail.berlios.de  Thu Jun  7 18:04:30 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Jun 2007 18:04:30 +0200
Subject: [Plearn-commits] r7552 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706071604.l57G4UWI004005@sheep.berlios.de>

Author: nouiz
Date: 2007-06-07 18:04:29 +0200 (Thu, 07 Jun 2007)
New Revision: 7552

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
Log:
My initial modification:
-better message for PLERROR
-Iterate over all variables
-Print more information and at better moment
-Partially made the number of neighbourg to find parametrable


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-06-07 15:47:42 UTC (rev 7551)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-06-07 16:04:29 UTC (rev 7552)
@@ -142,7 +142,7 @@
     if (train_set)
     {
         build_ball_tree();
-        for (int iteration = 1; iteration <= 50; iteration++)
+        for (int iteration = 1; iteration <= train_set->width(); iteration++)
         {
             cout << "In TestImputations, Iteration # " << iteration << endl;
             initialize();
@@ -152,7 +152,7 @@
             computeNeighborhoodStats();
             train();
         }
-        PLERROR("In TestImputations: we are done here");
+        PLERROR("In TestImputations::build_(): we are done here");
     }
 }
 
@@ -179,12 +179,12 @@
             break;
         }
         if (train_col >= train_width)
-            PLERROR("In TestImputations:: no field with this name in input dataset: %s", (missing_indicators[mi_col]).c_str());
+            PLERROR("In TestImputations::build_ball_tree():: no field with this name in input dataset: %s", (missing_indicators[mi_col]).c_str());
     }
     weighted_distance_kernel = new WeightedDistance(weights);
 /*
-    if (!reference_set_with_covpres) PLERROR("In TestImputations:: no reference_set_with_covpres provided.");
-    if (!reference_set_with_missing) PLERROR("In TestImputations:: no reference_set_with_missing provided.");
+    if (!reference_set_with_covpres) PLERROR("In TestImputations::build_ball_tree() no reference_set_with_covpres provided.");
+    if (!reference_set_with_missing) PLERROR("In TestImputations::build_ball_tree() no reference_set_with_missing provided.");
     ball_tree = new BallTreeNearestNeighbors();
     ball_tree->setOption("rmin", "1");
     ball_tree->setOption("train_method", "anchor");
@@ -200,8 +200,8 @@
     ref_cov = reference_set_with_covpres->toMat();
     ref_mis = reference_set_with_missing->toMat();
 */
-    if (!reference_set_with_covpres) PLERROR("In TestImputations:: no reference_set_with_covpres provided.");
-    if (!reference_set_with_missing) PLERROR("In TestImputations:: no reference_set_with_missing provided.");
+    if (!reference_set_with_covpres) PLERROR("In TestImputations::build_ball_tree() no reference_set_with_covpres provided.");
+    if (!reference_set_with_missing) PLERROR("In TestImputations::build_ball_tree() no reference_set_with_missing provided.");
     ball_tree = new ExhaustiveNearestNeighbors();
     ball_tree->setOption("num_neighbors", "100");
     ball_tree->setOption("copy_input", "0");
@@ -288,6 +288,7 @@
     train_set->lockMetaDataDir();
     header_record.resize(train_width);
     header_file_name = train_metadata + "/TestImputation/header.pmat";
+    cout << "header_file_name: " << header_file_name << endl;
     if (!isfile(header_file_name)) createHeaderFile();
     else getHeaderRecord();
     
@@ -295,21 +296,22 @@
     cout << "choose a variable to test imputations for" << endl;
     to_deal_with_total = 0;
     to_deal_with_next = -1;
+
     for (train_col = 0; train_col < train_width; train_col++)
     {
         if (header_record[train_col] != 1.0) continue;
         to_deal_with_total += 1;
         if (to_deal_with_next < 0) to_deal_with_next = train_col;
     }
+    cout << "total number of variable left to deal with: " << to_deal_with_total << endl;
     if (to_deal_with_next < 0)
     {
         train_set->unlockMetaDataDir();
         // reviewGlobalStats();
-        PLERROR("In TestImputations:: we are done here");
+        PLERROR("In TestImputations::initialize() we are done here");
     }
+    cout << "next variable to deal with: " << train_names[to_deal_with_next] << endl;
     to_deal_with_name = train_names[to_deal_with_next];
-    cout << "total number of variable left to deal with: " << to_deal_with_total << endl;
-    cout << "next variable to deal with: " << train_names[to_deal_with_next] << endl;
     updateHeaderRecord(to_deal_with_next);
     train_set->unlockMetaDataDir();
     
@@ -326,7 +328,7 @@
     {
         to_deal_with_value = train_set->get(train_row, to_deal_with_next);
         if (is_missing(to_deal_with_value)) continue;
-        if (ind_next >= indices.length()) PLERROR("In TestImputations:: There seems to be more present values than indicated by the stats file");
+        if (ind_next >= indices.length()) PLERROR("In TestImputations::initialize() There seems to be more present values than indicated by the stats file");
         indices[ind_next] = train_row;
         ind_next += 1;
         pb->update( train_row );
@@ -354,12 +356,12 @@
 
 void TestImputations::computeMeanMedianModeStats()
 {
-    if (!isfile(mean_median_mode_file_name)) PLERROR("In TestImputations:: a valid mean_median_mode_file path must be provided.");
+    if (!isfile(mean_median_mode_file_name)) PLERROR("In TestImputations::computeMeanMedianModeStats() a valid mean_median_mode_file path must be provided.");
     mmmf_file = new FileVMatrix(mean_median_mode_file_name);
     mmmf_length = mmmf_file->length();
     mmmf_width = mmmf_file->width();
-    if (mmmf_length != 3) PLERROR("In TestImputations:: there should be exactly 3 records in the mmm file, got %i.", mmmf_length);
-    if (mmmf_width != train_width) PLERROR("In TestImputations:: train set and mmm width should be the same, got %i.", mmmf_width);
+    if (mmmf_length != 3) PLERROR("In TestImputations::computeMeanMedianModeStats() there should be exactly 3 records in the mmm file, got %i.", mmmf_length);
+    if (mmmf_width != train_width) PLERROR("In TestImputations::computeMeanMedianModeStats() train set and mmm width should be the same, got %i.", mmmf_width);
     mmmf_mean = mmmf_file->get(0, to_deal_with_next);
     mmmf_median = mmmf_file->get(1, to_deal_with_next);
     mmmf_mode = mmmf_file->get(2, to_deal_with_next);
@@ -384,11 +386,13 @@
 void TestImputations::computeTreeCondMeanStats()
 {
     tcmf_file_name = tree_conditional_mean_directory + "/" + to_deal_with_name + "/Split0/test1_outputs.pmat";
-    if (!isfile(tcmf_file_name)) PLERROR("In TestImputations:: a file was not found in the tcf directory.");
+    if (!isfile(tcmf_file_name)) 
+        PLERROR("In TestImputations::computeTreeCondMeanStats(): The '%s' file was not found in the tcf directory.",tcmf_file_name.c_str());
     tcmf_file = new FileVMatrix(tcmf_file_name);
     tcmf_length = tcmf_file->length();
     tcmf_width = tcmf_file->width();
-    if (tcmf_length < train_length) PLERROR("In TestImputations:: there are not enough records in the tree conditional output file.");
+    if (tcmf_length < train_length) 
+        PLERROR("In TestImputations::computeTreeCondMeanStats(): there are only %d records in the tree conditional output file. We need %d.",tcmf_length,train_length);
     tcmf_mean_err = 0.0;
     pb = new ProgressBar( "computing the tree conditional mean imputation errors for " + to_deal_with_name, test_length);
     for (test_row = 0; test_row < test_length; test_row++)
@@ -403,12 +407,13 @@
 
 void TestImputations::computeCovPresStats()
 {
-    if (!isfile(covariance_preservation_file_name)) PLERROR("In TestImputations:: a valid covariance_preservation_file path must be provided.");
+    if (!isfile(covariance_preservation_file_name)) PLERROR("In TestImputations::computeCovPresStats() a valid covariance_preservation_file path must be provided.");
     cvpf_file = new FileVMatrix(covariance_preservation_file_name);
     cvpf_length = cvpf_file->length();
     cvpf_width = cvpf_file->width();
-    if (cvpf_length != train_width + 1) PLERROR("In TestImputations:: there should be %i records in the cvp file, got %i.", train_width + 1, cvpf_length);
-    if (cvpf_width != train_width) PLERROR("In TestImputations:: train set and cvp width should be the same, got %i.", cvpf_width);
+    if (cvpf_length != train_width + 1) PLERROR("In TestImputations::computeCovPresStats() there should be %i records in the cvp file, got %i.",
+                                                train_width + 1, cvpf_length);
+    if (cvpf_width != train_width) PLERROR("In TestImputations::computeCovPresStats() train set and cvp width should be the same, got %i.", cvpf_width);
     cvpf_file = new FileVMatrix(covariance_preservation_file_name);
     cvpf_cov.resize(train_width, train_width);
     cvpf_mu.resize(train_width);
@@ -471,7 +476,7 @@
         knnf_sum_value = 0.0;
         knnf_sum_cov = 0.0;
         knnv_value_count = 0.0;
-        for (knnf_row = 0; knnf_row < 100; knnf_row++)
+        for (knnf_row = 0; knnf_row < knnf_neighbors.size(); knnf_row++)
         {
             knnf_value = ref_mis((int) knnf_neighbors[knnf_row], to_deal_with_next);
             if (!is_missing(knnf_value))
@@ -486,7 +491,7 @@
             }
             knnf_value = ref_cov((int) knnf_neighbors[knnf_row], to_deal_with_next);
             if (is_missing(knnf_value))
-                PLERROR("In TestImputations::missing value found in the reference with covariance preserved at: %i , %i",
+                PLERROR("In TestImputations::computeNeighborhoodStats(): missing value found in the reference with covariance preserved at: %i , %i",
                          (int) knnf_neighbors[knnf_row], to_deal_with_next);
             knnf_sum_cov += knnf_value;
             knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_cov / (knnf_row + 1)), 2.0);
@@ -494,11 +499,12 @@
         pb->update( test_row );
     }
     delete pb;
-    for (knnf_row = 0; knnf_row < 100; knnf_row++) knnf_mean_err[knnf_row] = knnf_mean_err[knnf_row] /  (real) test_length;
+    for (knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++) knnf_mean_err[knnf_row] = knnf_mean_err[knnf_row] /  (real) test_length;
 }
 
 void TestImputations::createHeaderFile()
 { 
+    cout << "in createHeaderFile()" << endl;
     for (train_col = 0; train_col < train_width; train_col++)
     {
         train_stats = train_set->getStats(train_col);
@@ -528,10 +534,10 @@
 void TestImputations::train()
 {
     // initialize the output file
-    cout << "initialize the output file" << endl;
+    output_file_name = train_metadata + "/TestImputation/output.pmat";
+    cout << "initialize the output file: " << output_file_name << endl;
     train_set->lockMetaDataDir();
-    output_record.resize(105);
-    output_file_name = train_metadata + "/TestImputation/output.pmat";
+    output_record.resize(knnf_mean_err.size()+5);
     if (!isfile(output_file_name)) createOutputFile();
     else getOutputRecord(to_deal_with_next);
     output_record[0] = mmmf_mean_err;
@@ -539,7 +545,7 @@
     output_record[2] = mmmf_mode_err;
     output_record[3] = tcmf_mean_err;
     output_record[4] = cvpf_mean_err;
-    for (knnf_row = 0; knnf_row < 100; knnf_row++)
+    for (knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++)
     {
        output_record[knnf_row + 5] = knnf_mean_err[knnf_row];
     }
@@ -549,13 +555,13 @@
 
 void TestImputations::createOutputFile()
 {
-    output_names.resize(105);
+    output_names.resize(knnf_mean_err.size()+5);
     output_names[0] = "mean";
     output_names[1] = "median";
     output_names[2] = "mode";
     output_names[3] = "tree_cond";
     output_names[4] = "cov_pres";
-    for (knnf_row = 0; knnf_row < 100; knnf_row++)
+    for (knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++)
     {
        output_names[knnf_row + 5] = "KNN_" + tostring(knnf_row);
     }



From nouiz at mail.berlios.de  Thu Jun  7 18:10:30 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Jun 2007 18:10:30 +0200
Subject: [Plearn-commits] r7553 - branches/cgi-desjardin/commands
Message-ID: <200706071610.l57GAUhx004475@sheep.berlios.de>

Author: nouiz
Date: 2007-06-07 18:10:30 +0200 (Thu, 07 Jun 2007)
New Revision: 7553

Added:
   branches/cgi-desjardin/commands/plearn_full_inc_gg.h
   branches/cgi-desjardin/commands/plearn_gg_inc.h
   branches/cgi-desjardin/commands/plearn_inc_gg.h
Log:
Add missin header file


Added: branches/cgi-desjardin/commands/plearn_full_inc_gg.h
===================================================================
--- branches/cgi-desjardin/commands/plearn_full_inc_gg.h	2007-06-07 16:04:29 UTC (rev 7552)
+++ branches/cgi-desjardin/commands/plearn_full_inc_gg.h	2007-06-07 16:10:30 UTC (rev 7553)
@@ -0,0 +1,85 @@
+// -*- C++ -*-
+
+// plearn_full_inc_gg.h
+//
+// Copyright (C) 2005 Olivier Delalleau 
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+ * $Id: plearn_full_inc_gg.h 4594 2005-11-18 18:16:39Z larocheh $ 
+ ******************************************************* */
+
+// Authors: Olivier Delalleau
+
+/*! \file plearn_full_inc_gg.h */
+
+/*! Include here all classes available in the PLearn CVS repository
+  that are dependent upon some external libraries.
+*/
+
+#ifndef plearn_full_inc_gg_INC
+#define plearn_full_inc_gg_INC
+
+/*********
+ * Boost *
+ *********/
+//gg#include <commands/PLearnCommands/HTMLHelpCommand.h>
+#include <plearn/math/PRandom.h>
+
+/**********
+ * Curses *
+ **********/
+#include <commands/PLearnCommands/VMatCommand.h>
+
+/**************
+ * Dictionary *
+ **************/
+//#include <plearn_learners/language/Dictionary/WordNetSenseDictionary.h>
+
+/*********
+ * Torch *
+ *********/
+//#include <plearn_learners/classifiers/SVMClassificationTorch.h>
+
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: branches/cgi-desjardin/commands/plearn_gg_inc.h
===================================================================
--- branches/cgi-desjardin/commands/plearn_gg_inc.h	2007-06-07 16:04:29 UTC (rev 7552)
+++ branches/cgi-desjardin/commands/plearn_gg_inc.h	2007-06-07 16:10:30 UTC (rev 7553)
@@ -0,0 +1,89 @@
+// -*- C++ -*-
+
+// plearn_gg_inc.h
+//
+// Copyright (C) 2005 Olivier Delalleau 
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+   * $Id: plearn_gg_inc.h 3110 2005-02-23 01:32:46Z tihocan $ 
+   ******************************************************* */
+
+// Authors: Olivier Delalleau
+
+/*! \file plearn_gg_inc.h */
+
+/*! Include here all classes available in the PLearn CVS repository
+    that are dependent upon some external libraries.
+ */
+
+#ifndef plearn_gg_inc_INC
+#define plearn_gg_inc_INC
+
+#include <plearn_learners/second_iteration/Experimentation.h>
+#include <plearn_learners/second_iteration/Preprocessing.h>
+#include <plearn_learners/second_iteration/CheckDond2FileSequence.h>
+#include <plearn_learners/second_iteration/MergeDond2Files.h>
+#include <plearn_learners/second_iteration/ComputeDond2Target.h>
+#include <plearn_learners/second_iteration/FixDond2BinaryVariables.h>
+#include <plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h>
+#include <plearn_learners/second_iteration/DichotomizeDond2DiscreteVariables.h>
+#include <plearn_learners/second_iteration/SecondIterationWrapper.h>
+#include <plearn_learners/second_iteration/ComputePurenneError.h>
+#include <plearn_learners/second_iteration/AnalyzeFieldStats.h>
+#include <plearn_learners/second_iteration/NeighborhoodConditionalMean.h>
+#include <plearn_learners/second_iteration/TestImputations.h>
+#include <plearn_learners/second_iteration/GaussianizeVMatrix.h>
+#include <plearn_learners/second_iteration/MissingIndicatorVMatrix.h>
+#include <plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h>
+#include <plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h>
+#include <plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h>
+#include <plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h>
+#include <plearn_learners/second_iteration/BallTreeNearestNeighbors.h>
+#include <plearn_learners/second_iteration/WeightedDistance.h>
+#include <plearn_learners/regressors/RegressionTree.h>
+#include <plearn/vmat/VariableDeletionVMatrix.h>
+/*
+#include <plearn/vmat/MeanMedianModeImputationVMatrix.h>
+#include <plearn/vmat/LocalMeanImputationVMatrix.h>
+#include <plearn/vmat/VariableDeletionVMatrix.h>
+#include <plearn/vmat/MissingIndicatorVMatrix.h>
+#include <plearn_learners/regressors/LocalMedBoost.h>
+#include <plearn_learners/regressors/BaseRegressorWrapper.h>
+#include <plearn_learners/regressors/BaseRegressorManifoldWrapper.h>
+#include <plearn_learners/regressors/BaseRegressorManifoldConstant.h>
+#include <plearn_learners/regressors/CompareRegressor.h>
+#include <plearn_learners/regressors/DistanceLearner.h>
+#include <plearn_learners/regressors/MultiClassDistanceLearner.h>
+#include <plearn_learners/regressors/OnlineNNet.h>
+*/
+
+#endif
+

Added: branches/cgi-desjardin/commands/plearn_inc_gg.h
===================================================================
--- branches/cgi-desjardin/commands/plearn_inc_gg.h	2007-06-07 16:04:29 UTC (rev 7552)
+++ branches/cgi-desjardin/commands/plearn_inc_gg.h	2007-06-07 16:10:30 UTC (rev 7553)
@@ -0,0 +1,324 @@
+// -*- C++ -*-
+
+// plearn_inc_gg.h
+//
+// Copyright (C) 2004-2005 Olivier Delalleau 
+// 
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************      
+ * $Id: plearn_inc_gg.h 5606 2006-05-16 15:03:12Z lheureup $ 
+ ******************************************************* */
+
+// Authors: Olivier Delalleau
+
+/*! \file plearn_inc_gg.h */
+
+/*! Include here all classes available in the PLearn CVS repository 
+  that do NOT depend upon fancy external libraries.
+*/
+
+#ifndef plearn_inc_gg_INC
+#define plearn_inc_gg_INC
+
+// Version number.
+#define PLEARN_MAJOR_VERSION 0
+#define PLEARN_MINOR_VERSION 92
+#define PLEARN_FIXLEVEL 0
+
+/******************************************************
+ * Python includes must come FIRST, as per Python doc *
+ ******************************************************/
+#include <plearn/python/PythonIncludes.h>
+
+/*****************
+ * Miscellaneous *
+ *****************/
+//gg#include <plearn_learners/misc/Grapher.h>
+//gg#include <plearn_learners/misc/VariableSelectionWithDirectedGradientDescent.h>
+//gg#include <plearn/math/ManualBinner.h>
+//gg#include <plearn/math/SoftHistogramBinner.h>
+#include <plearn/misc/ShellScript.h>
+#include <plearn/misc/RunObject.h>
+#include <plearn/db/UCISpecification.h>
+#include <plearn_learners/testers/PTester.h>
+
+/***********
+ * Command *
+ ***********/
+#include <commands/PLearnCommands/AutoRunCommand.h>
+#include <commands/PLearnCommands/DiffCommand.h>
+#include <commands/PLearnCommands/FieldConvertCommand.h>
+#include <commands/PLearnCommands/HelpCommand.h>
+#include <commands/PLearnCommands/JulianDateCommand.h>
+//gg#include <commands/PLearnCommands/KolmogorovSmirnovCommand.h>
+#include <commands/PLearnCommands/LearnerCommand.h>
+#include <commands/PLearnCommands/PairwiseDiffsCommand.h>
+#include <commands/PLearnCommands/ReadAndWriteCommand.h>
+#include <commands/PLearnCommands/RunCommand.h>
+#include <commands/PLearnCommands/ServerCommand.h>
+#include <commands/PLearnCommands/TestDependenciesCommand.h>
+#include <commands/PLearnCommands/TestDependencyCommand.h>
+//#include <commands/PLearnCommands/TxtmatCommand.h>
+
+
+/**************
+ * Dictionary *
+ **************/
+//gg#include <plearn_learners/language/Dictionary/Dictionary.h>
+//gg#include <plearn_learners/language/Dictionary/FileDictionary.h>
+//gg#include <plearn_learners/language/Dictionary/VecDictionary.h>
+//gg#include <plearn_learners/language/Dictionary/ConditionalDictionary.h>
+
+/****************
+ * HyperCommand *
+ ****************/
+#include <plearn_learners/hyper/HyperOptimize.h>
+#include <plearn_learners/hyper/HyperRetrain.h>
+
+/**********
+ * Kernel *
+ **********/
+//gg#include <plearn/ker/AdditiveNormalizationKernel.h>
+//gg#include <plearn/ker/DistanceKernel.h>
+//gg#include <plearn/ker/DotProductKernel.h>
+//gg#include <plearn/ker/EpanechnikovKernel.h>
+//gg#include <plearn/ker/GaussianKernel.h>
+//gg#include <plearn/ker/GeodesicDistanceKernel.h>
+//gg#include <plearn/ker/LLEKernel.h>
+//gg#include <plearn/ker/NegOutputCostFunction.h>
+//#include <plearn/ker/PolynomialKernel.h>
+//gg#include <plearn/ker/ReconstructionWeightsKernel.h>
+//gg#include <plearn/ker/ThresholdedKernel.h>
+//gg#include <plearn/ker/VMatKernel.h>
+
+/*************
+ * Optimizer *
+ *************/
+#include <plearn/opt/AdaptGradientOptimizer.h>
+#include <plearn/opt/ConjGradientOptimizer.h>
+#include <plearn/opt/GradientOptimizer.h>
+
+/****************
+ * OptionOracle *
+ ****************/
+#include <plearn_learners/hyper/CartesianProductOracle.h>
+#include <plearn_learners/hyper/EarlyStoppingOracle.h>
+#include <plearn_learners/hyper/ExplicitListOracle.h>
+#include <plearn_learners/hyper/OptimizeOptionOracle.h>
+
+/************
+ * PLearner *
+ ************/
+
+// Classifiers
+//gg#include <plearn_learners/meta/AdaBoost.h>
+//gg#include <plearn_learners/classifiers/BinaryStump.h>
+#include <plearn_learners/classifiers/ClassifierFromConditionalPDistribution.h>
+#include <plearn_learners/classifiers/ClassifierFromDensity.h>
+#include <plearn_learners/classifiers/KNNClassifier.h>
+#include <plearn_learners/classifiers/MultiInstanceNNet.h>
+//#include <plearn_learners/classifiers/OverlappingAdaBoost.h> // Does not currently compile.
+
+// Generic
+#include <plearn_learners/generic/AddCostToLearner.h>
+#include <plearn_learners/generic/AddLayersNNet.h>
+#include <plearn_learners/generic/DistRepNNet.h>
+#include <plearn_learners/generic/NNet.h>
+#include <plearn_learners/generic/SelectInputSubsetLearner.h>
+#include <plearn_learners/generic/StackedLearner.h>
+#include <plearn_learners/generic/TestingLearner.h>
+#include <plearn_learners/generic/VPLPreprocessedLearner.h>
+
+// Hyper
+#include <plearn_learners/hyper/HyperLearner.h>
+
+// Regressors
+#include <plearn_learners/regressors/ConstantRegressor.h>
+//gg#include <plearn_learners/regressors/KernelRidgeRegressor.h>
+#include <plearn_learners/regressors/KNNRegressor.h>
+#include <plearn_learners/regressors/LinearRegressor.h>
+//gg#include <plearn_learners/regressors/PLS.h>
+//gg#include <plearn_learners/regressors/RankLearner.h>
+
+// Unsupervised/KernelProjection
+//gg#include <plearn_learners/unsupervised/Isomap.h>
+//gg#include <plearn_learners/unsupervised/KernelPCA.h>
+//gg#include <plearn_learners/unsupervised/LLE.h>
+//gg#include <plearn_learners/unsupervised/PCA.h>
+//gg#include <plearn_learners/unsupervised/SpectralClustering.h>
+//gg#include <plearn_learners/unsupervised/UniformizeLearner.h>
+
+// PDistribution
+#include <plearn_learners/distributions/GaussianDistribution.h>
+#include <plearn_learners/distributions/GaussMix.h>
+#include <plearn_learners/distributions/ManifoldParzen2.h>
+#include <plearn_learners/distributions/ParzenWindow.h>
+#include <plearn_learners/distributions/RandomGaussMix.h>
+#include <plearn_learners/distributions/SpiralDistribution.h>
+#include <plearn_learners/distributions/UniformDistribution.h>
+
+// Nearest-Neighbors
+//gg#include <plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h>
+//gg#include <plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h>
+//gg#include <plearn_learners/nearest_neighbors/GenericNearestNeighbors.h>
+
+// Online
+#include <plearn_learners/online/GaussianDBNClassification.h>
+#include <plearn_learners/online/GradNNetLayerModule.h>
+#include <plearn_learners/online/HintonDeepBeliefNet.h>
+#include <plearn_learners/online/NLLErrModule.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/RBMParameters.h>
+#include <plearn_learners/online/RBMGenericParameters.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMBinomialLayer.h>
+#include <plearn_learners/online/RBMMultinomialLayer.h>
+#include <plearn_learners/online/RBMGaussianLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMJointGenericParameters.h>
+
+//gg#include <plearn_learners/online/SquaredErrModule.h>
+//gg#include <plearn_learners/online/TanhModule.h>
+//gg#include <plearn_learners/online/StackedModulesLearner.h>
+
+
+/**********
+ * Python *
+ **********/
+//gg#include <plearn/python/PythonCodeSnippet.h>
+//gg#include <plearn/python/PythonProcessedVMatrix.h>
+
+/************
+ * Splitter *
+ ************/
+#include <plearn/vmat/BinSplitter.h>
+#include <plearn/vmat/ConcatSetsSplitter.h>
+#include <plearn/vmat/DBSplitter.h>
+#include <plearn/vmat/ExplicitSplitter.h>
+#include <plearn/vmat/FilterSplitter.h>
+#include <plearn/vmat/FractionSplitter.h>
+#include <plearn/vmat/KFoldSplitter.h>
+#include <plearn/vmat/NoSplitSplitter.h>
+#include <plearn/vmat/RepeatSplitter.h>
+#include <plearn/vmat/SourceVMatrixSplitter.h>
+#include <plearn/vmat/StackedSplitter.h>
+#include <plearn/vmat/TestInTrainSplitter.h>
+#include <plearn/vmat/ToBagSplitter.h>
+#include <plearn/vmat/TrainTestSplitter.h>
+#include <plearn/vmat/TrainValidTestSplitter.h>
+
+/************
+ * Variable *
+ ************/
+#include <plearn/var/MatrixElementsVariable.h>
+
+/*********************
+ * VecStatsCollector *
+ *********************/
+#include <plearn/math/LiftStatsCollector.h>
+
+/***********
+ * VMatrix *
+ ***********/
+#include <plearn/vmat/AddMissingVMatrix.h>
+#include <plearn/vmat/AsciiVMatrix.h>
+#include <plearn/vmat/AutoVMatrix.h>
+#include <plearn/vmat/BootstrapVMatrix.h>
+#include <plearn/vmat/CenteredVMatrix.h>
+#include <plearn/vmat/CompactVMatrix.h>
+#include <plearn/vmat/CompressedVMatrix.h>
+#include <plearn/vmat/CumVMatrix.h>
+#include <plearn/vmat/DatedJoinVMatrix.h>
+#include <plearn/vmat/DictionaryVMatrix.h>
+#include <plearn/vmat/DisregardRowsVMatrix.h>
+#include <plearn/vmat/ExtractNNetParamsVMatrix.h>
+#include <plearn/vmat/FilteredVMatrix.h>
+#include <plearn/vmat/FinancePreprocVMatrix.h>
+#include <plearn/vmat/GeneralizedOneHotVMatrix.h>
+#include <plearn/vmat/GetInputVMatrix.h>
+#include <plearn/vmat/GramVMatrix.h>
+#include <plearn/vmat/IndexedVMatrix.h>
+#include <plearn/vmat/JulianizeVMatrix.h>
+#include <plearn/vmat/KNNVMatrix.h>
+// Commented out because triggers WordNet, which does not work really fine yet.
+//#include <plearn/vmat/LemmatizeVMatrix.h>
+#include <plearn/vmat/LocalNeighborsDifferencesVMatrix.h>
+#include <plearn/vmat/LocallyPrecomputedVMatrix.h>
+//#include <plearn/vmat/MixUnlabeledNeighbourVMatrix.h>
+#include <plearn/vmat/MultiInstanceVMatrix.h>
+#include <plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.h>
+#include <plearn/vmat/OneHotVMatrix.h>
+#include <plearn/vmat/PLearnerOutputVMatrix.h>
+#include <plearn/vmat/PairsVMatrix.h>
+#include <plearn/vmat/PrecomputedVMatrix.h>
+#include <plearn/vmat/ProcessDatasetVMatrix.h>
+#include <plearn/vmat/ProcessingVMatrix.h>
+#include <plearn/vmat/ProcessSymbolicSequenceVMatrix.h>
+#include <plearn/vmat/RandomSamplesVMatrix.h>
+#include <plearn/vmat/RankedVMatrix.h>
+#include <plearn/vmat/RegularGridVMatrix.h>
+#include <plearn/vmat/RemoveDuplicateVMatrix.h>
+#include <plearn/vmat/ReorderByMissingVMatrix.h>
+//#include <plearn/vmat/SelectAttributsSequenceVMatrix.h>
+#include <plearn/vmat/SelectRowsMultiInstanceVMatrix.h>
+#include <plearn/vmat/ShuffleColumnsVMatrix.h>
+#include <plearn/vmat/SortRowsVMatrix.h>
+#include <plearn/vmat/SparseVMatrix.h>
+#include <plearn/vmat/SubInputVMatrix.h>
+#include <plearn/vmat/TemporaryDiskVMatrix.h>
+#include <plearn/vmat/TemporaryFileVMatrix.h>
+#include <plearn/vmat/TextFilesVMatrix.h>
+#include <plearn/vmat/ThresholdVMatrix.h>
+#include <plearn/vmat/TransposeVMatrix.h>
+#include <plearn/vmat/UCIDataVMatrix.h>
+#include <plearn/vmat/ViewSplitterVMatrix.h>
+#include <plearn/vmat/VMatrixFromDistribution.h>
+
+/*******************
+ * SurfaceTemplate *
+ ******************/
+//gg#include <plearn_learners_experimental/SurfaceTemplate/ScoreLayerVariable.h>
+//gg#include <plearn_learners_experimental/SurfaceTemplate/SurfaceTemplateLearner.h>
+
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Thu Jun  7 21:38:13 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Jun 2007 21:38:13 +0200
Subject: [Plearn-commits] r7554 - trunk/scripts
Message-ID: <200706071938.l57JcDmo006616@sheep.berlios.de>

Author: nouiz
Date: 2007-06-07 21:38:12 +0200 (Thu, 07 Jun 2007)
New Revision: 7554

Modified:
   trunk/scripts/cdispatch
Log:
Added a parameter to send to the cluster or to be executed localy. The local version is not implemented by DBI.py right now.


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-06-07 16:10:30 UTC (rev 7553)
+++ trunk/scripts/cdispatch	2007-06-07 19:38:12 UTC (rev 7554)
@@ -35,7 +35,7 @@
 ## library, go to the PLearn Web site at www.plearn.org
 
 $ScriptName="lauchdbi.py";
-$ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--test] [--file=FILEPATH] [--req="CONDOR_REQUIREMENT"]<command-template>'."\n";
+$ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--file=FILEPATH] [--req="CONDOR_REQUIREMENT"]<command-template>'."\n";
 $LongHelp=<<EOUSAGE;
 Usage: cdispatch [--help|-h] [--log|--nolog] [--test] [--file=FILEPATH] [--req="CONDOR_REQUIREMENT"]<command-template>
 Dispatches jobs on Condor with dbi.py.  
@@ -104,6 +104,19 @@
     $LOG = 0;
 }
 
+if ($ARGV[0] eq "--cluster") {
+    $CLUSTER = 1;
+    $LOCAL = 0;
+    shift;
+} elsif ($ARGV[0] eq "--local") {
+    $CLUSTER = 0;
+    $LOCAL = 1;
+    shift;
+} else {
+    $CLUSTER = 0;
+    $LOCAL = 0;
+}
+
 if ($ARGV[0] eq "--test") {
     $TEST = 1;
     shift;
@@ -138,9 +151,10 @@
 #    "\tArglist = (", join(",",@{$repl[$i]}),")\n";
 #}
 open(SCRIPT,">$ScriptName");
-print SCRIPT "#! /usr/bin/env python\n".
-    "from plearn.parallel.dbi import DBI\n".
-    "jobs = DBI([\n";
+    print SCRIPT "#! /usr/bin/env python\n".
+	"from plearn.parallel.dbi import DBI\n".
+	"jobs = DBI([\n";
+
 $nbcommand=0;
 
 #print the command
@@ -156,7 +170,13 @@
     $nbcommand=printAndExpendArgs(@ARGV)
 }
 
-print SCRIPT "   ],'Condor'";
+if($CLUSTER){
+    print SCRIPT "   ],'Cluster'";
+}elsif($LOCAL){
+    print SCRIPT "   ],'Local'";
+}else{
+    print SCRIPT "   ],'Condor'";
+}
 
 if ($REQ ne "") {
     print SCRIPT ", requirements=\"$REQ\"";



From nouiz at mail.berlios.de  Thu Jun  7 21:59:30 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Jun 2007 21:59:30 +0200
Subject: [Plearn-commits] r7555 - trunk/python_modules/plearn/parallel
Message-ID: <200706071959.l57JxUea008981@sheep.berlios.de>

Author: nouiz
Date: 2007-06-07 21:59:30 +0200 (Thu, 07 Jun 2007)
New Revision: 7555

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
The cluster mode now respect the test mode


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-06-07 19:38:12 UTC (rev 7554)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-06-07 19:59:30 UTC (rev 7555)
@@ -219,9 +219,12 @@
             output = file(task.log_file + '.out','w')
         if int(self.file_redirect_stderr):
             error = file(task.log_file + '.err','w')
-        task.p = Popen(command, shell=True,stdout=output,stderr=error)
+        if self.test == False:
+            task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
     def run(self):
+        if self.test:
+            print "Test mode, we only print the command to be executed, we don't execute them"
         # Execute pre-batch
         pre_batch_command = ';'.join( self.pre_batch )
         output = PIPE
@@ -230,10 +233,11 @@
             output = file(self.log_file + '.pre_batch.out', 'w')
         if int(self.file_redirect_stderr):
             error = file(self.log_file + '.pre_batch.err', 'w')
-        self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
+        if self.test:
+            print pre_batch_command
+        else:
+            self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
 
-        #print 'pre_batch_command =', pre_batch_command
-
         # Execute all Tasks (including pre_tasks and post_tasks if any)
         for task in self.tasks:
             self.run_one_job(task)
@@ -244,9 +248,11 @@
             output = file(self.log_file + '.post_batch.out', 'w')
         if int(self.file_redirect_stderr):
             error = file(self.log_file + '.post_batch.err', 'w')
-        self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
-        #print 'post_batch_command =', post_batch_command
-
+        if self.test:
+            print post_batch_command
+        else:
+            self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
+            
     def clean(self):
         #TODO: delete all log files for the current batch
         pass



From nouiz at mail.berlios.de  Thu Jun  7 22:57:05 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Jun 2007 22:57:05 +0200
Subject: [Plearn-commits] r7556 - trunk/python_modules/plearn/parallel
Message-ID: <200706072057.l57Kv5DE015091@sheep.berlios.de>

Author: nouiz
Date: 2007-06-07 22:57:04 +0200 (Thu, 07 Jun 2007)
New Revision: 7556

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Modified to read the jobs to execute from stdin as another script from James was doing


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-06-07 19:59:30 UTC (rev 7555)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-06-07 20:57:04 UTC (rev 7556)
@@ -719,14 +719,11 @@
     return jobs
 
 def main():
-    #    jobs = DBICluster(['ls','sleep 2'])
-    jobs = DBI([
-        'plearn_exp ${PLEARNDIR}/speedtest/sgrad.plearn task=letter_memvmat nout=26 nh1=100 nh2=100 nh3=100 slr=1e-1 dc=0 n=16001 epoch=16000 seed=1 mbs=10',
-        'plearn_exp ${PLEARNDIR}/speedtest/sgrad.plearn task=letter_memvmat nout=26 nh1=100 nh2=100 nh3=100 slr=1e-1 dc=0 n=16001 epoch=16000 seed=1 mbs=20'
-        #   './plearn dbn.pyplearn n_epochs_grad=250 n_hidden=25 grad_learning_rate=0.05 n_epochs_cd=0 cd_learning_rate=0.00001 unique_id=x25 recons_file=r.txt',
-        #   './plearn dbn.pyplearn n_epochs_grad=250 n_hidden=35 grad_learning_rate=0.05 n_epochs_cd=0 cd_learning_rate=0.00001 unique_id=x35 recons_file=r.txt'
-        ],'Ssh')
-    jobs.run()
+    if len(sys.argv)!=2:
+        print "Usage:dbi.py {Condor|Cluster|Ssh|Local|bqtools} < joblist"
+        print "Where joblist is a file containing one exeperiement by line"
+        sys.exit(0)
+    DBI([ s[0:-1] for s in sys.stdin.readlines() ], sys.argv[1]).run()
 #    jobs.clean()
 
 #    config['LOG_DIRECTORY'] = 'LOGS/'



From nouiz at mail.berlios.de  Fri Jun  8 15:57:42 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 Jun 2007 15:57:42 +0200
Subject: [Plearn-commits] r7557 - trunk/python_modules/plearn/parallel
Message-ID: <200706081357.l58Dvgh6008614@sheep.berlios.de>

Author: nouiz
Date: 2007-06-08 15:57:42 +0200 (Fri, 08 Jun 2007)
New Revision: 7557

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Added a Local mode to dbi that will make all command to be executed on the localhost serially


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-06-07 20:57:04 UTC (rev 7556)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-06-08 13:57:42 UTC (rev 7557)
@@ -63,6 +63,7 @@
         self.requirements = ''
         self.test = False
         self.dolog = False
+        self.temp_files = []
         for key in args.keys():
             self.__dict__[key] = args[key]
 
@@ -275,8 +276,6 @@
         if self.dolog and not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir)
 
-        self.log_file = os.path.join( self.parent_dir, self.log_file )
-
         # create the information about the tasks
         args['temp_dir'] = self.temp_dir
         for command in commands:
@@ -371,8 +370,6 @@
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
             
-#        self.log_file = os.path.join( self.parent_dir, self.log_file )
-
         # create the information about the tasks
 #        args['temp_dir'] = self.temp_dir
         for command in commands:
@@ -440,7 +437,6 @@
                                    self.post_tasks,self.dolog,args))
 
             #keeps a list of the temporary files created, so that they can be deleted at will            
-        self.temp_files = []
 
     def run_all_job(self):
         if len(self.tasks)==0:
@@ -577,8 +573,82 @@
     def clean(self):
         pass
 
+class DBILocal(DBIBase):
 
+    def __init__( self, commands, **args ):
+        DBIBase.__init__(self, commands, **args)
 
+        # check if log directory exists, if not create it
+        if self.dolog and not os.path.exists(self.log_dir):
+            os.mkdir(self.log_dir)
+
+        for command in commands:
+            pos = string.find(command,' ')
+            if pos>=0:
+                c = command[0:pos]
+                c2 = command[pos:]
+            else:
+                c=command
+                c2=""
+
+        # We use the absolute path so that we don't have corner case as with ./ 
+            c = os.path.normpath(os.path.join(os.getcwd(), c))
+            command = c + c2
+            
+            # We will execute the command on the specified architecture
+            # if it is specified. If the executable exist for both
+            # architecture we execute on both. Otherwise we execute on the
+            # same architecture as the architecture of the launch computer
+            
+            if not os.path.exists(c):
+                raise Exception("The command '"+c+"' do not exist!")
+            elif not os.access(c, os.X_OK):
+                raise Exception("The command '"+c+"' do not have execution permission!")
+            self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
+                                   self.time_format, self.pre_tasks,
+                                   self.post_tasks,self.dolog,args))
+
+            #keeps a list of the temporary files created, so that they can be deleted at will            
+
+    def run_one_job(self,task):
+        launch_file = os.path.join(self.tmp_dir, 'launch.sh')
+
+        # Launch condor
+        output = PIPE
+        error = PIPE
+        if int(self.file_redirect_stdout):
+            output = file(self.log_file + '.out', 'w')
+        if int(self.file_redirect_stderr):
+            error = file(self.log_file + '.err', 'w')
+
+        c = (';'.join(task.commands))
+        if self.test == False:
+            print c
+            os.system(c)
+
+        else:
+            print c
+            
+    def clean(self):
+                
+        sleep(20)
+        for file_name in self.temp_files:
+            try:
+                os.remove(file_name)
+            except os.error:
+                pass
+            pass    
+
+
+    def run(self):
+        if self.test:
+            print "Test mode, we only print the command to be executed, we don't execute them"
+        for task in self.tasks:
+            self.run_one_job(task)
+
+    def clean(self):
+        pass
+
 class SshHost:
     def __init__(self, hostname):
         self.hostname= hostname



From tihocan at mail.berlios.de  Fri Jun  8 16:15:18 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 8 Jun 2007 16:15:18 +0200
Subject: [Plearn-commits] r7558 - in
	trunk/plearn_learners/online/test/DeepBeliefNet: .
	.pytest/PL_DBN_SimpleRBM/expected_results/expdir
	.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0
	.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir
Message-ID: <200706081415.l58EFILk010863@sheep.berlios.de>

Author: tihocan
Date: 2007-06-08 16:15:17 +0200 (Fri, 08 Jun 2007)
New Revision: 7558

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/SimpleRBM_test.pyplearn
   trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
Log:
Regenerated test PL_DBN_SimpleRBM without using fast approximations

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2007-06-08 13:57:42 UTC (rev 7557)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2007-06-08 14:15:17 UTC (rev 7558)
@@ -19,6 +19,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *2 ->PRandom(
@@ -37,6 +38,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *2   )
@@ -59,6 +61,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *2   )
@@ -84,6 +87,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *2   )
@@ -100,6 +104,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *2   )
@@ -120,6 +125,7 @@
 input_size = 4 ;
 output_size = 2 ;
 name = "RBMMixedConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *2   )
@@ -128,6 +134,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMClassificationModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *2   )

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2007-06-08 13:57:42 UTC (rev 7557)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2007-06-08 14:15:17 UTC (rev 7558)
@@ -50,6 +50,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *7 ->PRandom(
@@ -68,6 +69,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *7   )
@@ -90,6 +92,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *7   )
@@ -115,6 +118,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *7   )
@@ -131,6 +135,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *7   )
@@ -151,6 +156,7 @@
 input_size = 4 ;
 output_size = 2 ;
 name = "RBMMixedConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *7   )
@@ -159,6 +165,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMClassificationModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *7   )

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/experiment.plearn	2007-06-08 13:57:42 UTC (rev 7557)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/experiment.plearn	2007-06-08 14:15:17 UTC (rev 7558)
@@ -26,8 +26,14 @@
             forget_when_training_set_changes = 0,
             grad_learning_rate = 0.1,
             layers = [
-                *3 -> RBMBinomialLayer( size = 2 ),
-                *4 -> RBMBinomialLayer( size = 2 )
+                *3 -> RBMBinomialLayer(
+                    size = 2,
+                    use_fast_approximations = False
+                    ),
+                *4 -> RBMBinomialLayer(
+                    size = 2,
+                    use_fast_approximations = False
+                    )
                 ],
             n_classes = 2,
             nstages = 0,

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2007-06-08 13:57:42 UTC (rev 7557)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2007-06-08 14:15:17 UTC (rev 7558)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL7398"
+__REVISION__ = "PL7547"
 

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2007-06-08 13:57:42 UTC (rev 7557)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2007-06-08 14:15:17 UTC (rev 7558)
@@ -59,6 +59,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *8 ->PRandom(
@@ -77,6 +78,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *8   )
@@ -99,6 +101,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *8   )
@@ -124,6 +127,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *8   )
@@ -140,6 +144,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *8   )
@@ -160,6 +165,7 @@
 input_size = 4 ;
 output_size = 2 ;
 name = "RBMMixedConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *8   )
@@ -168,6 +174,7 @@
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMClassificationModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *8   )

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/SimpleRBM_test.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/SimpleRBM_test.pyplearn	2007-06-08 13:57:42 UTC (rev 7557)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/SimpleRBM_test.pyplearn	2007-06-08 14:15:17 UTC (rev 7558)
@@ -20,8 +20,8 @@
         )
 
 layers = [
-    pl.RBMBinomialLayer( size = input_size ),
-    pl.RBMBinomialLayer( size = hidden_size )
+    pl.RBMBinomialLayer( size = input_size, use_fast_approximations = False ),
+    pl.RBMBinomialLayer( size = hidden_size, use_fast_approximations = False )
     ]
 
 connections = [

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-06-08 13:57:42 UTC (rev 7557)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/pytest.config	2007-06-08 14:15:17 UTC (rev 7558)
@@ -117,7 +117,7 @@
         ),
     arguments = "--enable-logging DeepBeliefNet,RBMClassificationModule SimpleRBM_test.pyplearn",
     resources = [ "SimpleRBM_test.pyplearn" ],
-    precision = 1e-03,
+    precision = 1e-06,
     pfileprg = "__program__",
     disabled = False
     )



From tihocan at mail.berlios.de  Fri Jun  8 16:47:33 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 8 Jun 2007 16:47:33 +0200
Subject: [Plearn-commits] r7559 - in
	trunk/plearn_learners/online/test/ModuleLearner: .
	.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0
	.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0
Message-ID: <200706081447.l58ElWw5014484@sheep.berlios.de>

Author: tihocan
Date: 2007-06-08 16:47:32 +0200 (Fri, 08 Jun 2007)
New Revision: 7559

Modified:
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_Greedy.pyplearn
   trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.plearn
   trunk/plearn_learners/online/test/ModuleLearner/pytest.config
Log:
Not using fast approximations for stabler tests

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-08 14:15:17 UTC (rev 7558)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-08 14:47:32 UTC (rev 7559)
@@ -39,6 +39,7 @@
 input_size = 5 ;
 output_size = 5 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10 ->PRandom(
@@ -58,6 +59,7 @@
 input_size = 15 ;
 output_size = 15 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -93,6 +95,7 @@
 input_size = 5 ;
 output_size = 15 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -101,6 +104,9 @@
 grad_learning_rate = 0.00100000000000000002 ;
 cd_learning_rate = 0 ;
 compute_contrastive_divergence = 0 ;
+standard_cd_grad = 1 ;
+standard_cd_bias_grad = 1 ;
+standard_cd_weights_grad = 1 ;
 n_Gibbs_steps_CD = 1 ;
 min_n_Gibbs_steps = 1 ;
 n_Gibbs_steps_per_generated_sample = 1 ;
@@ -111,6 +117,7 @@
 input_size = -1 ;
 output_size = -1 ;
 name = "rbm_0" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -120,6 +127,7 @@
 ports = 2 [ ("input" , "rbm_0.visible" )("output" , "rbm_0.hidden.state" )] ;
 save_states = 1 ;
 name = "network_0" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -137,6 +145,7 @@
 input_size = 15 ;
 output_size = 15 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -153,6 +162,7 @@
 input_size = 20 ;
 output_size = 20 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -193,6 +203,7 @@
 input_size = 15 ;
 output_size = 20 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -201,6 +212,9 @@
 grad_learning_rate = 0.00100000000000000002 ;
 cd_learning_rate = 0 ;
 compute_contrastive_divergence = 0 ;
+standard_cd_grad = 1 ;
+standard_cd_bias_grad = 1 ;
+standard_cd_weights_grad = 1 ;
 n_Gibbs_steps_CD = 1 ;
 min_n_Gibbs_steps = 1 ;
 n_Gibbs_steps_per_generated_sample = 1 ;
@@ -211,6 +225,7 @@
 input_size = -1 ;
 output_size = -1 ;
 name = "rbm_1" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -223,6 +238,7 @@
 ports = 2 [ ("input" , "rbm_0.visible" )("output" , "rbm_1.hidden.state" )] ;
 save_states = 1 ;
 name = "network_1" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -247,13 +263,14 @@
 input_size = 20 ;
 output_size = 2 ;
 name = "affine_net" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
 *21 ->SoftmaxModule(
 input_size = 2 ;
-output_size = 2 ;
 name = "softmax" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -262,6 +279,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "nll" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -286,6 +304,7 @@
 ports = 4 [ ("input" , "rbm_0.visible" )("output" , "affine_net.output" )("target" , "nll.target" )("NLL" , "nll.cost" )] ;
 save_states = 1 ;
 name = "network_2" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
@@ -295,11 +314,13 @@
 input_size = -1 ;
 output_size = -1 ;
 name = "network" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *10   )
 ;
 batch_size = 10 ;
+reset_seed_upon_train = 0 ;
 cost_ports = 1 [ "NLL" ] ;
 input_ports = 1 [ "input" ] ;
 target_ports = 1 [ "target" ] ;

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-08 14:15:17 UTC (rev 7558)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-08 14:47:32 UTC (rev 7559)
@@ -13,6 +13,7 @@
 input_size = 5 ;
 output_size = 5 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5 ->PRandom(
@@ -32,6 +33,7 @@
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -62,6 +64,7 @@
 input_size = 5 ;
 output_size = 10 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -70,6 +73,9 @@
 grad_learning_rate = 0.0100000000000000002 ;
 cd_learning_rate = 0.00100000000000000002 ;
 compute_contrastive_divergence = 0 ;
+standard_cd_grad = 1 ;
+standard_cd_bias_grad = 1 ;
+standard_cd_weights_grad = 1 ;
 n_Gibbs_steps_CD = 1 ;
 min_n_Gibbs_steps = 1 ;
 n_Gibbs_steps_per_generated_sample = 1 ;
@@ -80,6 +86,7 @@
 input_size = -1 ;
 output_size = -1 ;
 name = "rbm_1" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -96,6 +103,7 @@
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -112,6 +120,7 @@
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -232,6 +241,7 @@
 input_size = 10 ;
 output_size = 100 ;
 name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -240,6 +250,9 @@
 grad_learning_rate = 0.0100000000000000002 ;
 cd_learning_rate = 0.00100000000000000002 ;
 compute_contrastive_divergence = 0 ;
+standard_cd_grad = 1 ;
+standard_cd_bias_grad = 1 ;
+standard_cd_weights_grad = 1 ;
 n_Gibbs_steps_CD = 1 ;
 min_n_Gibbs_steps = 1 ;
 n_Gibbs_steps_per_generated_sample = 1 ;
@@ -250,6 +263,7 @@
 input_size = -1 ;
 output_size = -1 ;
 name = "rbm_2" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -273,13 +287,14 @@
 input_size = 100 ;
 output_size = 2 ;
 name = "affine_net" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
 *13 ->SoftmaxModule(
 input_size = 2 ;
-output_size = 2 ;
 name = "softmax" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -288,6 +303,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "nll" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -296,6 +312,7 @@
 input_size = 2 ;
 output_size = 1 ;
 name = "class_error" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -303,6 +320,7 @@
 input_size = -1 ;
 output_size = 1 ;
 name = "argmax_class" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -311,6 +329,7 @@
 input_size = 1 ;
 output_size = 1 ;
 name = "mse" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
@@ -347,11 +366,13 @@
 ports = 8 [ ("input" , "rbm_1.visible" )("" , "nll.target" )("" , "class_error.target" )("" , "mse.target" )("output" , "affine_net.output" )("NLL" , "nll.cost" )("class_error" , "class_error.cost" )("mse" , "mse.cost" )] ;
 save_states = 1 ;
 name = "NetworkModule" ;
+use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
 random_gen = *5   )
 ;
 batch_size = 11 ;
+reset_seed_upon_train = 0 ;
 cost_ports = 3 [ "NLL" "class_error" "mse" ] ;
 input_ports = 1 [ "input" ] ;
 target_ports = 3 [ "nll.target" "class_error.target" "mse.target" ] ;

Modified: trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_Greedy.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_Greedy.pyplearn	2007-06-08 14:15:17 UTC (rev 7558)
+++ trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_Greedy.pyplearn	2007-06-08 14:47:32 UTC (rev 7559)
@@ -35,8 +35,8 @@
             name = name,
             cd_learning_rate = cd_learning_rate,
             grad_learning_rate = grad_learning_rate,
-            visible_layer = pl.RBMBinomialLayer(size = visible_size),
-            hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
+            visible_layer = pl.RBMBinomialLayer(size = visible_size, use_fast_approximations = False),
+            hidden_layer = pl.RBMBinomialLayer(size = hidden_size, use_fast_approximations = False),
             connection = pl.RBMMatrixConnection(
                 down_size = visible_size,
                 up_size = hidden_size))

Modified: trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.plearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.plearn	2007-06-08 14:15:17 UTC (rev 7558)
+++ trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.plearn	2007-06-08 14:47:32 UTC (rev 7559)
@@ -24,10 +24,12 @@
                             visible_layer =
                                 RBMBinomialLayer(
                                     size = ${inputsize}
+                                    use_fast_approximations = 0
                                 )
                             hidden_layer =
                                 RBMBinomialLayer(
                                     size = ${rbm_size_1}
+                                    use_fast_approximations = 0
                                 )
                             connection =
                                 RBMMatrixConnection(
@@ -43,9 +45,11 @@
                             visible_layer =
                                 RBMBinomialLayer(
                                     size = ${rbm_size_1}
+                                    use_fast_approximations = 0
                                 )
                             hidden_layer =
                                 RBMBinomialLayer(
+                                    use_fast_approximations = 0
                                     size = ${rbm_size_2}
                                 )
                             connection =

Modified: trunk/plearn_learners/online/test/ModuleLearner/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/pytest.config	2007-06-08 14:15:17 UTC (rev 7558)
+++ trunk/plearn_learners/online/test/ModuleLearner/pytest.config	2007-06-08 14:47:32 UTC (rev 7559)
@@ -117,7 +117,7 @@
         ),
     arguments = "PL_ModuleLearner_TwoRBMs.plearn grad_lr=1e-2 cd_lr=1e-3 batch_size=11 nstages=1001",
     resources = [ "PL_ModuleLearner_TwoRBMs.plearn" ],
-    precision = 1e-03,
+    precision = 1e-06,
     pfileprg = "__program__",
     disabled = False
     )
@@ -132,7 +132,7 @@
         ),
     arguments = "PL_ModuleLearner_Greedy.pyplearn",
     resources = [ "PL_ModuleLearner_Greedy.pyplearn" ],
-    precision = 1e-03,
+    precision = 1e-06,
     pfileprg = "__program__",
     disabled = False
     )



From yoshua at mail.berlios.de  Fri Jun  8 19:51:48 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 8 Jun 2007 19:51:48 +0200
Subject: [Plearn-commits] r7560 - trunk/plearn/math
Message-ID: <200706081751.l58Hpmqk008906@sheep.berlios.de>

Author: yoshua
Date: 2007-06-08 19:51:47 +0200 (Fri, 08 Jun 2007)
New Revision: 7560

Modified:
   trunk/plearn/math/pl_math.h
Log:
fasttanh, fastsigmoid and tabulated_softmax should now handle nan's and inf's properly


Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2007-06-08 14:47:32 UTC (rev 7559)
+++ trunk/plearn/math/pl_math.h	2007-06-08 17:51:47 UTC (rev 7560)
@@ -275,6 +275,10 @@
 
 inline real fasttanh(const real& x)
 {
+    if (isnan(x)) return x; // tanh(nan)=nan
+    int is_inf=isinf(x);
+    if (is_inf>0) return 1; // tanh(inf)=1
+    if (is_inf<0) return -1; // tanh(-inf)=-1
     if(x>0)
     {
         if(x>MAXTANHX)
@@ -444,6 +448,10 @@
     static const real softplus_delta = (n_softplus_values-1)/(max_softplus_arg-min_softplus_arg);
     static real softplus_values[n_softplus_values];
     static bool computed_softplus_table = false;
+    if (isnan(x)) return x; // softplus(nan)=nan
+    int is_inf=isinf(x);
+    if (is_inf>0) return x; // softplus(inf)=inf
+    if (is_inf<0) return 0; // softplus(-inf)=0
     if (!computed_softplus_table)
     {
         real y=min_softplus_arg;



From lamblin at mail.berlios.de  Fri Jun  8 20:46:58 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 8 Jun 2007 20:46:58 +0200
Subject: [Plearn-commits] r7561 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200706081846.l58Ikwtp013365@sheep.berlios.de>

Author: lamblin
Date: 2007-06-08 20:46:57 +0200 (Fri, 08 Jun 2007)
New Revision: 7561

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Don't skip last schedule


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-08 17:51:47 UTC (rev 7560)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-08 18:46:57 UTC (rev 7561)
@@ -171,7 +171,7 @@
         clf()
     colors="bgrcmyk"
     styles=['-', '--', '-.', ':', '.', ',', 'o', '^', 'v', '<', '>', 's', '+', 'x', 'D']
-    for i in range(n_train-1):
+    for i in range(n_train):
         learner.nstages = int(stages[i])
         options = {}
         for s in range(n_schedules):



From simonl at mail.berlios.de  Fri Jun  8 23:03:12 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Fri, 8 Jun 2007 23:03:12 +0200
Subject: [Plearn-commits] r7562 - in trunk: plearn/var/EXPERIMENTAL
	plearn_learners/generic/EXPERIMENTAL
	python_modules/plearn/plotting python_modules/plearn/var
Message-ID: <200706082103.l58L3CeO022939@sheep.berlios.de>

Author: simonl
Date: 2007-06-08 23:03:11 +0200 (Fri, 08 Jun 2007)
New Revision: 7562

Added:
   trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.cc
   trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.h
Modified:
   trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
   trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.h
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/var/Var.py
Log:
minor improvements...


Added: trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.cc	2007-06-08 18:46:57 UTC (rev 7561)
+++ trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.cc	2007-06-08 21:03:11 UTC (rev 7562)
@@ -0,0 +1,182 @@
+// -*- C++ -*-
+
+// ProbabilityPairsInverseVariable.cc
+//
+// Copyright (C) 2007 Simon Lemieux, Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Simon Lemieux
+
+/*! \file ProbabilityPairsInverseVariable.cc */
+
+
+#include "ProbabilityPairsInverseVariable.h"
+
+namespace PLearn {
+using namespace std;
+
+/** ProbabilityPairsInverseVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    ProbabilityPairsInverseVariable,
+    "[x1,x2,x3,...,xn] -> [f(x1), f(x3), ..., f(xn)] with f(x) = (max-min)*x - min and with n even",
+    "MULTI LINE\nHELP FOR USERS"
+    );
+
+
+ProbabilityPairsInverseVariable::ProbabilityPairsInverseVariable()
+    : min(0.), max(1.)
+{}
+
+ProbabilityPairsInverseVariable::ProbabilityPairsInverseVariable(Variable* input, real min, real max)
+    : inherited(input, input->length(), input->width()/2),
+      min(min),max(max)
+{
+    build_();
+}
+
+ProbabilityPairsInverseVariable::ProbabilityPairsInverseVariable(Variable* input, real max)
+    :inherited(input, input->length(), input->width()/2),
+     min(0.), max(max)
+{
+    build_();
+}
+
+ProbabilityPairsInverseVariable::ProbabilityPairsInverseVariable(Variable* input)
+    :inherited(input, input->length(), input->width()/2),
+     min(0.), max(1.)
+{
+    build_();
+}
+
+void ProbabilityPairsInverseVariable::recomputeSize(int& l, int& w) const
+{
+        if (input) {
+            l = input->length(); // the computed length of this Var
+            w = input->width()/2; // the computed width
+        } else
+            l = w = 0;
+}
+
+// ### computes value from input's value
+void ProbabilityPairsInverseVariable::fprop()
+{
+    for(int n=0; n<input->length(); n++)
+        for(int i=0; i<input->width()/2; i++)
+            matValue(n,i) = input->matValue(n,i*2)*(max-min) + min;
+}
+
+// ### computes input's gradient from gradient
+void ProbabilityPairsInverseVariable::bprop()
+{
+    for(int n=0; n<input->length(); n++)
+        for(int i=0; i<input->width()/2; i++)
+            input->matGradient(n,i*2) += matGradient(n,i)*(max-min);
+}
+
+// ### You can implement these methods:
+// void ProbabilityPairsInverseVariable::bbprop() {}
+// void ProbabilityPairsInverseVariable::symbolicBprop() {}
+// void ProbabilityPairsInverseVariable::rfprop() {}
+
+
+// ### Nothing to add here, simply calls build_
+void ProbabilityPairsInverseVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+void ProbabilityPairsInverseVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### If you want to deepCopy a Var field:
+    // varDeepCopyField(somevariable, copies);
+}
+
+void ProbabilityPairsInverseVariable::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+  
+    declareOption(ol, "min", &ProbabilityPairsInverseVariable::min,
+                  OptionBase::buildoption,
+                  "The lower bound a value of the input should be. It will be used to calculate the corresponding probability of each input.");
+
+    declareOption(ol, "max", &ProbabilityPairsInverseVariable::max,
+                  OptionBase::buildoption,
+                  "analog to min");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void ProbabilityPairsInverseVariable::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.h	2007-06-08 18:46:57 UTC (rev 7561)
+++ trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.h	2007-06-08 21:03:11 UTC (rev 7562)
@@ -0,0 +1,163 @@
+// -*- C++ -*-
+
+// ProbabilityPairsInverseVariable.h
+//
+// Copyright (C) 2007 Simon Lemieux, Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Simon Lemieux
+
+/*! \file ProbabilityPairsInverseVariable.h */
+
+
+#ifndef ProbabilityPairsInverseVariable_INC
+#define ProbabilityPairsInverseVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+/*! * ProbabilityPairsInverseVariable * */
+
+/**
+ * [x1,x2,x3,...,xn] -> [f(x1), f(x3), ..., f(xn)] with f(x) = (max-min)*x - min and with n even
+ * It is the inverse of ProbabilityPairsVariable
+ *
+ *
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class ProbabilityPairsInverseVariable : public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! see ProbabilityPairsVariable documentation
+    real min;
+    
+    //! see ProbabilityPairsVariable documentation
+    real max;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor, usually does nothing
+    ProbabilityPairsInverseVariable();
+     
+    ProbabilityPairsInverseVariable(Variable* input, real min, real max);
+    ProbabilityPairsInverseVariable(Variable* input, real max);
+    ProbabilityPairsInverseVariable(Variable* input);
+
+
+    //#####  PLearn::Variable methods #########################################
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+
+    // ### These ones are not always implemented
+    // virtual void bbprop();
+    // virtual void symbolicBprop();
+    // virtual void rfprop();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(ProbabilityPairsInverseVariable);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ProbabilityPairsInverseVariable);
+
+// ### Put here a convenient method for building your variable.
+// ### e.g., if your class is TotoVariable, with two parameters foo_type foo
+// ### and bar_type bar, you could write:
+// inline Var toto(Var v, foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v, foo, bar); }
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-08 18:46:57 UTC (rev 7561)
+++ trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-08 21:03:11 UTC (rev 7562)
@@ -57,7 +57,6 @@
 ProbabilityPairsVariable::ProbabilityPairsVariable(Variable* input, real min, real max)
     : inherited(input, input->length(), input->width()*2),
       min(min),max(max)
-
 {
     build_();
 }
@@ -103,9 +102,7 @@
 {
     for(int n=0; n<length(); n++)
         for(int i=0; i<input->width(); i++)
-        {
             input->matGradient(n,i) += 1./(max-min)*(matGradient(n,2*i) - matGradient(n,2*i+1));
-        }
 }
 
 // ### You can implement these methods:

Modified: trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.h	2007-06-08 18:46:57 UTC (rev 7561)
+++ trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.h	2007-06-08 21:03:11 UTC (rev 7562)
@@ -50,7 +50,7 @@
 /**
  *  Let define f(x) = (x-min)/(max-min) for min<=x<=max, then this variable is defined by [x1,x2,...,xn]  |->  [ f(x1), 1-f(x1), f(x2), 1-f(x2), ... , f(xn), 1-f(xn) ]
  *  
- *  This can seen as pairs of probabilities 
+ *  This can be interpreted  as pairs of probabilities 
  *
  *
  * @todo Write class to-do's here if there are any.
@@ -82,10 +82,6 @@
     //! Default constructor, usually does nothing
     ProbabilityPairsVariable();
 
-
-    // ### If your class has parameters, you probably want a constructor that
-    // ### initializes them
-    //! constructor
     ProbabilityPairsVariable(Variable* input, real min, real max);
     ProbabilityPairsVariable(Variable* input, real max);
     ProbabilityPairsVariable(Variable* input);

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-08 18:46:57 UTC (rev 7561)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-08 21:03:11 UTC (rev 7562)
@@ -134,6 +134,9 @@
                   OptionBase::buildoption,
                   "");
 
+    declareOption(ol, "group_sizes", &DeepReconstructorNet::group_sizes,
+                  OptionBase::buildoption,
+                  "");
     
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -267,6 +270,7 @@
     deepCopyField(compute_output, copies);
     deepCopyField(output_and_target_to_cost, copies);
     deepCopyField(outmat, copies);
+    deepCopyField(group_sizes, copies);
 }
 
 int DeepReconstructorNet::outputsize() const

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-08 18:46:57 UTC (rev 7561)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-08 21:03:11 UTC (rev 7562)
@@ -107,6 +107,9 @@
 
     PP<Optimizer> fine_tuning_optimizer;
 
+
+    TVec<int> group_sizes;
+
 protected:
     // protected members (not options)
 

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-06-08 18:46:57 UTC (rev 7561)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-06-08 21:03:11 UTC (rev 7562)
@@ -42,7 +42,7 @@
     for el in cbarh:
         cbarh_str.append(str(el)[0:5])
     yticks(arange(len(cbarh)),cbarh_str)
-    xticks([],[])                              
+    xticks([],[])
     imshow(cbar, cmap = color_map, vmin=min, vmax=max)
 
 
@@ -119,6 +119,7 @@
     #THE plotting
     
     x,y = sbi,sbi
+    toReturn = -1
     
     for i in arange(start,length):
     
@@ -128,7 +129,7 @@
             row = apply_to_rows(row)
         
 
-        print x,y,plotWidth, plotHeight
+       # print x,y,plotWidth, plotHeight
         
         axes((x, y, plotWidth, plotHeight))
         imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap, vmin = mi, vmax = ma)
@@ -142,10 +143,12 @@
             y = y + plotHeight + sbi
         if y + plotHeight > 1:
             # images that follows would be out of the figure...
+            toReturn = i
             break
 
     #custom color bar
-    customColorBar(mi,ma,(1.-cbw-sbi, sbi, sbi, 1.-2.*cbw))    
+    customColorBar(mi,ma,(1.-cbw-sbi, sbi, sbi, 1.-2.*cbw))
+    return toReturn
         
 
         #1 sur 2 -
@@ -163,69 +166,11 @@
         # setPlotParams(str(i), False, True)
             
 
-def plotCharOLD(char, layers=[], space_between_layers = 5):
-    '''plots a caracter and hidden layers
-    '''  
-    #some 'plotting' consts
-    nbLayers = len(layers)
-    print 'nbLayers', nbLayers
-    totalLayersWidth = 0
-    for layer in layers:
-        totalLayersWidth += len(layer[0]) + space_between_layers
-    totalLayersWidth += space_between_layers
-    print 'totalLayersWidth', totalLayersWidth
-    
-    unit = .9/totalLayersWidth
-    print 'unit', unit
-    sbl = space_between_layers*unit
-    print 'sbl', sbl
-    plotCharWidth = .099
-    plotCharHeight = plotCharWidth
-    #plotLayerWidth = unit
-    plotCharBottom = (1.-plotCharHeight)/2.
-    
-    #plot of the char
-    axes((sbl,          
-          plotCharBottom,
-          plotCharWidth,
-          plotCharHeight))
-    imshow(char, interpolation="nearest", cmap = defaultColorMap)
-    setPlotParams("", True, True)
-    print 'char ok'
-    #plots of the layers
 
 
-    x,y=sbl,1.-2*len(layers[0])
-    axes((x,y,len(layers[0][0]), len(layers[0])))
-    imshow(layer, interpolation = "nearest", cmap = defaultColorMap)
-    print 'layer[0] ok'
-    
-    k=1
-    x+=len(layers[0][0])
-    
-    while k < nbLayers:
-
-       
-        
-        plotLayerHeight = unit*len(layers[k])
-        plotLayerWidth = unit*len(layers[k][0])
-        if plotLayerHeight > 1:
-            plotLayerHeight = 1.-2.*sbl
-            
-        plotLayerBottom = (1.-plotLayerHeight)/2.
-
-        print 'k',k
-        print 'x',x
-        print 'plotLayerBottom', plotLayerBottom
-        print 'plotLayerWidth', plotLayer
-        axes((x,plotLayerBottom, plotLayerWidth, plotLayerHeight))
-        imshow(layers[k], interpolation="nearest", cmap = defaultColorMap)
-        setPlotParams("",True,True)
-        x += plotLayerWidth
-        k += 1
-
 def plotMatrices(matrices, same_color_bar = False, space_between_matrices = 5):
     '''plot matrices from left to right
+    TODO : same_color_bar does nothing !!
     '''
     colorMap = cm.gray
     nbMatrices = len(matrices)
@@ -244,6 +189,7 @@
     sbm = space_between_matrices*unit
 
     x=sbm
+    the_axes = []
     for matrix in matrices:
         
         h = len(matrix)*unit
@@ -252,12 +198,16 @@
             h = 1.-2*sbm
         bottom = (1.-h)/2.
 
-        axes(( x,bottom, w,h))
+        temp = axes(( x,bottom, w,h))
+        the_axes.append(temp)
         imshow(matrix, interpolation = 'nearest', cmap = colorMap)
         colorbar()
         x += w+sbm
 
+    return the_axes
 
+
+
 def truncate_imshow(mat, max_height_or_width = 200, width_height_ratio = 1, space_between_submatrices=5):
 
     matWidth = float(len(mat[0]))
@@ -348,7 +298,7 @@
         row2.append(row[i])
     return row2
 
-def rowToMatrix(row, width, validate_size = True, fill_value = 0.):
+def rowToMatrixOld(row, width, validate_size = True, fill_value = 0.):
     '''change a row [1,2,3,4,5,6] into a matrix [[1,2],[3,4],[5,6]] if width = 2
        or [1,2,3,4,5,6] -> [[1,2,3,4],[5,6,fill_value,fill_value]] if width = 4 and validate_size = False
     '''
@@ -371,13 +321,24 @@
     
     return m
 
+def rowToMatrix(row, width, validate_size = True, fill_value = 0.):
+    '''change a row [1,2,3,4,5,6] into a matrix [[1,2],[3,4],[5,6]] if width = 2
+       or [1,2,3,4,5,6] -> [[1,2,3,4],[5,6,fill_value,fill_value]] if width = 4 and validate_size = False
+    '''
+    copy = list(row)
+    if len(copy)%width != 0:
+        if validate_size:
+            raise Exception, "dimensions does not fit ( " + str(width) + " does not divide " + str(len(copy)) + ")"
+        for i in arange(len(copy)%width-1):
+            copy.append(fill_value)
+
+    return reshape(copy, (-1,width))
+
+
 def vecToVerticalMatrix(vec):
     '''ex : [1,2,3,4,5] --> [[1],[2],[3],[4],[5]]
     '''
-    mat = []
-    for elem in vec:
-        mat.append([elem])
-    return mat
+    return reshape(array(vec), (len(vec),-1))
 
 def truncateMatrixOld(mat, maxHeight=10, maxWidth=10):
     

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-06-08 18:46:57 UTC (rev 7561)
+++ trunk/python_modules/plearn/var/Var.py	2007-06-08 21:03:11 UTC (rev 7562)
@@ -71,6 +71,9 @@
 
     def probabilityPairs(self, min, max, varname=""):
         return Var(pl.ProbabilityPairsVariable(input=self.v, min=min, max=max, varname=varname))
+    
+    def probabilityPairsInverse(self, min, max, varname=""):
+        return Var(pl.ProbabilityPairsInverseVariable(input=self.v, min=min, max=max, varname=varname))
 
     def transpose(self):
         return Var(pl.TransposeVariable(input=self.v))
@@ -150,7 +153,7 @@
     
 
 ###################################################    
-# RLayer stands for reconstruciton hidden layer
+# RLayer stands for reconstruction hidden layer
 
 
 def addSigmoidTiedRLayer(input, iw, ow, add_bias=True, basename=""):
@@ -211,10 +214,37 @@
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, basename=""):
+def addMultiSoftMaxMixedProductRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=""):
+    """iw is the input's width
+    igs is the input's group size
+    ow and ogs analog but for output"""
+    M = Var(ow/ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_M")
+    W = Var(ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_W")
+    Wr = Var(ow, iw, "uniform", -1./ow, 1./ow, varname=basename+'_Wr')
+
+    if add_bias:
+        b = Var(1,ow,"fill",0, varname=basename+'_b')
+        hidden = input.doubleProduct(W,M).add(b).multiSoftMax(ogs)
+        br = Var(1,iw,"fill",0, varname=basename+'_br')
+        log_reconstructed = hidden.matrixProduct(Wr).add(br).multiLogSoftMax(igs)
+    else:
+        hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
+        log_reconstructed = hidden.matrixProduct(Wr).multiLogSoftMax(igs)
+
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
+
+def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=""):
     W = Var(ow, iw, "uniform", -1./iw, 1./iw, varname=basename+'_W')
-    hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
-    log_reconstructed = hidden.matrixProduct(W).multiLogSoftMax(igs)
+    if add_bias:
+        b = Var(1,ow,"fill",0, varname=basename+'_b')
+        hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)
+        br = Var(1,iw,"fill",0, varname=basename+'_br')
+        log_reconstructed = hidden.matrixProduct(W).add(br).multiLogSoftMax(igs)
+    else:        
+        hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
+        log_reconstructed = hidden.matrixProduct(W).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input



From dorionc at mail.berlios.de  Mon Jun 11 15:00:47 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Mon, 11 Jun 2007 15:00:47 +0200
Subject: [Plearn-commits] r7563 - trunk/python_modules/plearn/parallel
Message-ID: <200706111300.l5BD0lld004979@sheep.berlios.de>

Author: dorionc
Date: 2007-06-11 15:00:47 +0200 (Mon, 11 Jun 2007)
New Revision: 7563

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
Changed the LOADAVG_DELAY from 15 to 60 seconds...

Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-06-08 21:03:11 UTC (rev 7562)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-06-11 13:00:47 UTC (rev 7563)
@@ -64,7 +64,7 @@
 
 # Do not perform a new query for the loadavg until recently launched
 # processes are likely to have started. 
-LOADAVG_DELAY = timedelta(seconds=15)
+LOADAVG_DELAY = timedelta(seconds=60)
 BUFSIZE       = 4096
 SLEEP_TIME    = 15
 LOGDIR        = None  # May be set by set_logdir()



From dorionc at mail.berlios.de  Mon Jun 11 20:20:13 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Mon, 11 Jun 2007 20:20:13 +0200
Subject: [Plearn-commits] r7564 - in trunk: commands plearn/math
	plearn_learners/regressors
Message-ID: <200706111820.l5BIKDsI012302@sheep.berlios.de>

Author: dorionc
Date: 2007-06-11 20:20:13 +0200 (Mon, 11 Jun 2007)
New Revision: 7564

Added:
   trunk/plearn_learners/regressors/CubicSpline.cc
   trunk/plearn_learners/regressors/CubicSpline.h
Modified:
   trunk/commands/plearn_light_inc.h
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn/math/TMat_impl.h
   trunk/plearn/math/TVec_decl.h
Log:
Adding a new CubicSpline learner


Modified: trunk/commands/plearn_light_inc.h
===================================================================
--- trunk/commands/plearn_light_inc.h	2007-06-11 13:00:47 UTC (rev 7563)
+++ trunk/commands/plearn_light_inc.h	2007-06-11 18:20:13 UTC (rev 7564)
@@ -160,6 +160,7 @@
 
 // Regressors
 #include <plearn_learners/regressors/ConstantRegressor.h>
+#include <plearn_learners/regressors/CubicSpline.h>
 #include <plearn_learners/regressors/KernelRidgeRegressor.h>
 #include <plearn_learners/regressors/KNNRegressor.h>
 #include <plearn_learners/regressors/GaussianProcessRegressor.h>

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-06-11 13:00:47 UTC (rev 7563)
+++ trunk/commands/plearn_noblas_inc.h	2007-06-11 18:20:13 UTC (rev 7564)
@@ -172,6 +172,7 @@
 
 // Regressors
 #include <plearn_learners/regressors/ConstantRegressor.h>
+#include <plearn_learners/regressors/CubicSpline.h>
 #include <plearn_learners/regressors/GaussianProcessRegressor.h>
 #include <plearn_learners/regressors/KernelRidgeRegressor.h>
 #include <plearn_learners/regressors/KNNRegressor.h>

Modified: trunk/plearn/math/TMat_impl.h
===================================================================
--- trunk/plearn/math/TMat_impl.h	2007-06-11 13:00:47 UTC (rev 7563)
+++ trunk/plearn/math/TMat_impl.h	2007-06-11 18:20:13 UTC (rev 7564)
@@ -53,6 +53,8 @@
 #include "TMatRowsAsArraysIterator_impl.h"
 #include "TMatColRowsIterator_impl.h"
 
+#include "algo.h"
+
 namespace PLearn {
 using namespace std;
 
@@ -227,7 +229,28 @@
                       v.subVec(elemnum+1,v.length()-(elemnum+1)));
 }
 
+// Returns an index vector I so that (*this)(I) returns a sorted version
+// of this vec in ascending order.
+namespace {
+  template <class T>
+  struct index_cmp : public binary_function<int, int, bool>
+  {
+      const TVec<T>& m_values;
+      index_cmp(const Vec& values): m_values(values) { }        
+      bool operator()(int x, int y) { return m_values[x] < m_values[y]; }
+  };
+}
+// Actual body of the method
+template <class T>
+TVec<int> TVec<T>::sortingPermutation() const
+{    
+    TVec<int> indices(length_);
+    for (int i=0; i < length_; i++) indices[i] = i;
+    sort(indices.begin(), indices.end(), index_cmp<T>(*this));
+    return indices;
+}
 
+
 // **************
 // **** TMat ****
 // **************

Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2007-06-11 13:00:47 UTC (rev 7563)
+++ trunk/plearn/math/TVec_decl.h	2007-06-11 18:20:13 UTC (rev 7564)
@@ -539,7 +539,11 @@
         resize(length()-1);
     }
 
-    int findSorted(T value)
+    //! Returns an index vector I so that (*this)(I) returns a sorted version
+    //! of this vec in ascending order.
+    TVec<int> sortingPermutation() const;
+    
+    int findSorted(T value) const
     {
         if (isEmpty())
             return 0;

Added: trunk/plearn_learners/regressors/CubicSpline.cc
===================================================================
--- trunk/plearn_learners/regressors/CubicSpline.cc	2007-06-11 13:00:47 UTC (rev 7563)
+++ trunk/plearn_learners/regressors/CubicSpline.cc	2007-06-11 18:20:13 UTC (rev 7564)
@@ -0,0 +1,234 @@
+// -*- C++ -*-
+
+// CubicSpline.cc
+//
+// Copyright (C) 2007 Christian Dorion
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Christian Dorion
+
+/*! \file CubicSpline.cc */
+
+
+#include "CubicSpline.h"
+#include "algo.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    CubicSpline,
+    "Unidimensional cubic spline learner.",
+    "This learner fits a unidimensional cubic spline to a given set of\n"
+    "points. That is, inputsize() must be one and the inputs are considered to be\n"
+    "the x values, while targetsize() must also be one and the targets are\n"
+    "considered to be the y values. The spline is fitted to the (x,y)-pairs so\n"
+    "formed. X values don't need to be ordered; the ordering is ensured within\n"
+    "the train method.\n");
+
+CubicSpline::CubicSpline()
+    : m_low_slope(MISSING_VALUE),
+      m_high_slope(MISSING_VALUE)
+{
+    // Nothing to do here
+} 
+
+void CubicSpline::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "low_slope", &CubicSpline::m_low_slope,
+                  OptionBase::buildoption,
+                  "The slope to enforce at the leftmost node -- Default: NaN [None]");
+    
+    declareOption(ol, "high_slope", &CubicSpline::m_high_slope,
+                  OptionBase::buildoption,
+                  "The slope to enforce at the rightmost node -- Default: NaN [None]");
+    
+    // Learnt options
+    declareOption(ol, "inputs", &CubicSpline::m_inputs,
+                  OptionBase::learntoption,
+                  "A buffer containing the last training set inputs");
+
+    declareOption(ol, "targets", &CubicSpline::m_targets,
+                  OptionBase::learntoption,
+                  "A buffer containing the last training set targets");
+
+    declareOption(ol, "coefficients", &CubicSpline::m_coefficients,
+                  OptionBase::learntoption,
+                  "The learnt coefficients");
+    
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void CubicSpline::build_()
+{
+    // Nothing to do here    
+}
+
+// ### Nothing to add here, simply calls build_
+void CubicSpline::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void CubicSpline::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(m_inputs, copies);
+    deepCopyField(m_targets, copies);
+    deepCopyField(m_coefficients, copies);
+}
+
+
+int CubicSpline::outputsize() const
+{
+    return inputsize();
+}
+
+void CubicSpline::forget()
+{
+    m_coefficients->resize(0);
+    inherited::forget();
+}
+
+void CubicSpline::train()
+{
+    // This learner fits unidimensional inputs/targets
+    PLASSERT( inputsize() == 1 );
+    PLASSERT( targetsize() == 1 );
+    
+    // Train set is a member of PLearner; set through setTrainingSet()
+    int n = train_set->length();
+    m_inputs = train_set.getColumn(0);
+    m_targets = train_set.getColumn(1);
+    PLASSERT( n >= 2 && train_set->width() == 2 );
+
+    // Sort the inputs and targets along the inputs values
+    TVec<int> indices = m_inputs.sortingPermutation();
+    m_inputs          = m_inputs(indices);
+    m_targets         = m_targets(indices);
+    
+    Vec u(n-1, 0.0);
+    m_coefficients.resize(n);
+    m_coefficients.fill(0.0);
+
+    // Low slope hack
+    if ( !is_missing(m_low_slope) ) {
+        u[0] = (3.0/(m_inputs[1]-m_inputs[0])) *
+            ( (m_targets[1]-m_targets[0])/
+              (m_inputs[1] - m_inputs[0]) - m_low_slope );
+        m_coefficients[0] = -0.5;
+    }
+
+    // Forward pass on coefficients
+    for (int i=1; i < (n-1); i++) {
+        real sig = (m_inputs[i]-m_inputs[i-1]) / (m_inputs[i+1]-m_inputs[i-1]);        
+        real p   = sig*m_coefficients[i-1]+2.0;
+        
+        m_coefficients[i] = (sig-1.0)/p;
+        
+        u[i] = (m_targets[i+1]-m_targets[i]) / (m_inputs[i+1]-m_inputs[i])
+            - (m_targets[i]-m_targets[i-1])  / (m_inputs[i]-m_inputs[i-1]);
+        
+        u[i] = (6.0*u[i]/(m_inputs[i+1]-m_inputs[i-1]) - sig*u[i-1]) / p;
+    }
+
+    // High slope hack
+    real un=0, qn=0;
+    if ( !is_missing(m_high_slope) ) {
+        qn = 0.5;
+        un = (3.0/(m_inputs[n-1]-m_inputs[n-2])) *
+            (m_high_slope  -  (m_targets[n-1]-m_targets[n-2]) / (m_inputs[n-1]-m_inputs[n-2]));
+    }
+
+    // Compute the last coefficient
+    m_coefficients[n-1] = (un-qn*u[n-2])/(qn*m_coefficients[n-1]+1.0);
+
+    // Backsubstitution step
+    for (int k=n-2; k >= 0; k--)
+        m_coefficients[k] = m_coefficients[k]*m_coefficients[k+1]+u[k];
+}
+
+
+void CubicSpline::computeOutput(const Vec& input, Vec& output) const
+{
+    PLASSERT( input.length() == 1 );
+    output.resize(1);
+ 
+    int n = m_inputs.length();
+    real x = input[0];
+    int pos = min( max(1, m_inputs.findSorted(x)),  n-1 );
+
+    real h = m_inputs[pos] - m_inputs[pos-1];
+    PLASSERT( h > 0.0 );
+
+    real a = (m_inputs[pos] - x) / h;
+    real b = (x - m_inputs[pos-1]) / h;
+    output[0] = a*m_targets[pos-1] + b*m_targets[pos]
+        + ((a*a*a - a)*m_coefficients[pos-1]
+           + (b*b*b - b)*m_coefficients[pos]) * h*h/6.0;
+}
+
+void CubicSpline::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    // No costs...
+}
+
+TVec<string> CubicSpline::getTestCostNames() const
+{
+    // None for now
+    return TVec<string>();
+}
+
+TVec<string> CubicSpline::getTrainCostNames() const
+{
+    // None for now
+    return TVec<string>();
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/regressors/CubicSpline.h
===================================================================
--- trunk/plearn_learners/regressors/CubicSpline.h	2007-06-11 13:00:47 UTC (rev 7563)
+++ trunk/plearn_learners/regressors/CubicSpline.h	2007-06-11 18:20:13 UTC (rev 7564)
@@ -0,0 +1,169 @@
+// -*- C++ -*-
+
+// CubicSpline.h
+//
+// Copyright (C) 2007 Christian Dorion
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Christian Dorion
+//   Migrated from Johann Hibschman's <johann at physics.berkeley.edu> Python
+//   'spline' module.
+
+
+/*! \file CubicSpline.h */
+
+
+#ifndef CubicSpline_INC
+#define CubicSpline_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ * Unidimensional cubic spline learner.
+ *
+ * This learner fits a unidimensional cubic spline to a given set of
+ * points. That is, inputsize() must be one and the inputs are considered to be
+ * the x values, while targetsize() must also be one and the targets are
+ * considered to be the y values. The spline is fitted to the (x,y)-pairs so
+ * formed. X values don't need to be ordered; the ordering is ensured within
+ * the train method.
+ */
+class CubicSpline : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The slope to enforce at the leftmost node -- Default: NaN [None]
+    bool m_low_slope;
+
+    //! The slope to enforce at the rightmost node -- Default: NaN [None]
+    bool m_high_slope;
+    
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    CubicSpline();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output: 1 -> a single interpolated value
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! Fit the splines to the *last* input point
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method). [Empty]
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats. [Empty]
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(CubicSpline);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    //! A buffer containing the last training set inputs
+    Vec m_inputs;
+
+    //! A buffer containing the last training set targets
+    Vec m_targets;    
+    
+    //! The learnt coefficients
+    Vec m_coefficients;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // ...
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(CubicSpline);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From manzagop at mail.berlios.de  Mon Jun 11 20:49:58 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 11 Jun 2007 20:49:58 +0200
Subject: [Plearn-commits] r7565 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200706111849.l5BInwxI014071@sheep.berlios.de>

Author: manzagop
Date: 2007-06-11 20:49:57 +0200 (Mon, 11 Jun 2007)
New Revision: 7565

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
Log:
The class now allows for "cross_entropy" output for doing binary classification.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-06-11 18:20:13 UTC (rev 7564)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-06-11 18:49:57 UTC (rev 7565)
@@ -214,8 +214,8 @@
     declareOption(ol, "output_type", 
                   &NatGradNNet::output_type,
                   OptionBase::buildoption,
-                  "type of output cost: 'NLL' for classification problems,\n"
-                  "or 'MSE' for regression.\n");
+                  "type of output cost: 'cross_entropy' for binary classification,\n"
+                  "'NLL' for classification problems, or 'MSE' for regression.\n");
 
     declareOption(ol, "input_size_lrate_normalization_power", 
                   &NatGradNNet::input_size_lrate_normalization_power, 
@@ -340,7 +340,13 @@
                     "should provide noutputs = number of classes, or possibly\n"
                     "1 when 2 classes\n");
     }
-    else PLERROR("NatGradNNet: output_type should be NLL or MSE\n");
+    else if (output_type=="cross_entropy")
+    {
+        if(noutputs!=1)
+            PLERROR("NatGradNNet: if output_type=cross_entropy, then \n"
+                    "noutputs should be 1.\n");
+    }
+    else PLERROR("NatGradNNet: output_type should be cross_entropy, NLL or MSE\n");
 
     if( output_layer_L1_penalty_factor < 0. )
         PLWARNING("NatGradNNet::build_ - output_layer_L1_penalty_factor is negative!\n");
@@ -985,6 +991,15 @@
                 log_softmax(L,L);
                 Profiler::pl_profile_end("activation function");
             }
+        else if (output_type=="cross_entropy")  {
+            for (int k=0;k<n_examples;k++)
+            {
+                Vec L=next_layer(k);
+                Profiler::pl_profile_start("activation function");
+                log_sigmoid(L,L);
+                Profiler::pl_profile_end("activation function");
+            }
+         }
     }
 }
 
@@ -1010,6 +1025,28 @@
                 costs(i,0) *= example_weight[i];
         }
     }
+    else if(output_type=="cross_entropy")   {
+        for (int i=0;i<n_examples;i++)
+        {
+            int target_class = int(round(target(i,0)));
+            Vec outp = output(i);
+            Vec grad = out_grad(i);
+            exp(outp,grad); // map log-prob to prob
+            if( target_class == 1 ) {
+                costs(i,0) = - outp[0];
+                costs(i,1) = (grad[0]>0.5)?0:1;
+            }   else    {
+                costs(i,0) = - pl_log( 1.0 - grad[0] );
+                costs(i,1) = (grad[0]>0.5)?1:0;
+            }
+            grad[0] -= (real)target_class;
+            if (example_weight[i]!=1.0)
+                costs(i,0) *= example_weight[i];
+        }
+//cout << "costs\t" << costs(0) << endl;
+//cout << "gradient\t" << out_grad(0) << endl;
+
+    }
     else // if (output_type=="MSE")
     {
         substract(output,target,out_grad);
@@ -1045,6 +1082,11 @@
         costs[0]="NLL";
         costs[1]="class_error";
     }
+    else if (output_type=="cross_entropy")  {
+        costs.resize(2);
+        costs[0]="cross_entropy";
+        costs[1]="class_error";
+    }
     else if (output_type=="MSE")
     {
         costs.resize(1);



From chapados at mail.berlios.de  Tue Jun 12 01:46:24 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 12 Jun 2007 01:46:24 +0200
Subject: [Plearn-commits] r7566 - trunk/python_modules/plearn/math
Message-ID: <200706112346.l5BNkOZZ018542@sheep.berlios.de>

Author: chapados
Date: 2007-06-12 01:46:24 +0200 (Tue, 12 Jun 2007)
New Revision: 7566

Added:
   trunk/python_modules/plearn/math/missings_robust_covariance.py
Log:
Compute covariance matrix in a way that is robust to missing values

Added: trunk/python_modules/plearn/math/missings_robust_covariance.py
===================================================================
--- trunk/python_modules/plearn/math/missings_robust_covariance.py	2007-06-11 18:49:57 UTC (rev 7565)
+++ trunk/python_modules/plearn/math/missings_robust_covariance.py	2007-06-11 23:46:24 UTC (rev 7566)
@@ -0,0 +1,75 @@
+# missings_robust_covariance
+# Copyright (C) 2007 by Nicolas Chapados
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+
+# Author: Nicolas Chapados
+
+
+import sys
+from fpconst     import NaN
+from numarray.ma import *
+from plearn.math import isNaN
+
+def missingsRobustCovariance(arr):
+    """Compute the covariance matrix in a way that's robust to missing values.
+
+    This function computes the covariance matrix between the columns of
+    'arr' in a way that is resistant to missing values (NaN).  It does not
+    just 'drop missing rows', like would be the easy thing to do.  Instead,
+    it uses as much information as it can, and can be used, for instance,
+    when EVERY ROW contains one or more missings.
+
+    If the matrix contains no missings whatsoever, it produces the same
+    results as the standard bias-corrected maximum likelihood estimator.
+    """
+    ## Second-order statistics
+    mask   = isNaN(arr)
+    arr_ma = array(arr, mask=mask)
+    count  = array(1 - mask)
+
+    ## First-order statistics to center
+    rowsum  = sum(arr_ma)
+    row_c   = sum(count)
+    rowmean = rowsum / row_c
+
+    ## Maximum likelihood cov
+    xxt       = dot(transpose(arr_ma),arr_ma)
+    cov_count = dot(transpose(count),count)
+    e_xxt     = xxt / cov_count
+    outer_m   = outerproduct(rowmean, rowmean)
+    cov_mle   = e_xxt - outer_m
+
+    ## Unbias the thing.  Subtle since the count may be 1 in some places
+    ## and we don't want to divide by zero.  Use masking again.
+    masked_count = masked_less_equal(cov_count, 1.0)
+    cov_mle *= masked_count / (masked_count-1.)
+
+    return filled(cov_mle, NaN)



From lamblin at mail.berlios.de  Tue Jun 12 02:21:01 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 12 Jun 2007 02:21:01 +0200
Subject: [Plearn-commits] r7567 - trunk/commands
Message-ID: <200706120021.l5C0L1pw022300@sheep.berlios.de>

Author: lamblin
Date: 2007-06-12 02:20:59 +0200 (Tue, 12 Jun 2007)
New Revision: 7567

Modified:
   trunk/commands/plearn_lapack_inc.h
   trunk/commands/plearn_noblas_inc.h
Log:
Move RBMMatrixConnectionNatGrad.h to plearn_lapack because it uses lapack.


Modified: trunk/commands/plearn_lapack_inc.h
===================================================================
--- trunk/commands/plearn_lapack_inc.h	2007-06-11 23:46:24 UTC (rev 7566)
+++ trunk/commands/plearn_lapack_inc.h	2007-06-12 00:20:59 UTC (rev 7567)
@@ -75,7 +75,10 @@
 #include <plearn_learners_experimental/LinearInductiveTransferClassifier.h>
 #include <plearn_learners_experimental/SurfaceTemplate/SurfaceTemplateLearner.h>
 
+// Online
+#include <plearn_learners/online/RBMMatrixConnectionNatGrad.h>
 
+
 #endif
 
 

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-06-11 23:46:24 UTC (rev 7566)
+++ trunk/commands/plearn_noblas_inc.h	2007-06-12 00:20:59 UTC (rev 7567)
@@ -223,7 +223,6 @@
 #include <plearn_learners/online/RBMGaussianLayer.h>
 #include <plearn_learners/online/RBMLayer.h>
 #include <plearn_learners/online/RBMMatrixConnection.h>
-#include <plearn_learners/online/RBMMatrixConnectionNatGrad.h>
 #include <plearn_learners/online/RBMMatrixTransposeConnection.h>
 #include <plearn_learners/online/RBMMixedConnection.h>
 #include <plearn_learners/online/RBMMixedLayer.h>



From lamblin at mail.berlios.de  Tue Jun 12 02:31:55 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 12 Jun 2007 02:31:55 +0200
Subject: [Plearn-commits] r7568 - trunk/plearn_learners/online
Message-ID: <200706120031.l5C0VtcQ022818@sheep.berlios.de>

Author: lamblin
Date: 2007-06-12 02:31:53 +0200 (Tue, 12 Jun 2007)
New Revision: 7568

Modified:
   trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc
Log:
Fix -noblas compilation


Modified: trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc	2007-06-12 00:20:59 UTC (rev 7567)
+++ trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc	2007-06-12 00:31:53 UTC (rev 7568)
@@ -119,9 +119,9 @@
         // We use the average gradient over a mini-batch.
         real mbnorm = 1. / pos_down_values.length();
         productScaleAcc(weights_gradient, pos_up_values, true, pos_down_values, false,
-                        mbnorm, 0);
+                        mbnorm, 0.);
         productScaleAcc(weights_gradient, neg_up_values, true, neg_down_values, false,
-                        -mbnorm, 1);
+                        -mbnorm, 1.);
 
         for (int i=0;i<up_size;i++)
         {
@@ -163,7 +163,7 @@
 
     // weights_gradient = 1/n * output_gradients' * inputs
     productScaleAcc(weights_gradient, output_gradients, true, inputs, false,
-                    1. / inputs.length(), 0);
+                    1. / inputs.length(), 0.);
     for (int i=0;i<up_size;i++)
     {
         (*bp_natgrad[i])(pos_count,weights_gradient(i),natural_gradient);



From lamblin at mail.berlios.de  Tue Jun 12 02:35:51 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 12 Jun 2007 02:35:51 +0200
Subject: [Plearn-commits] r7569 - trunk/plearn_learners/online
Message-ID: <200706120035.l5C0ZpMT023026@sheep.berlios.de>

Author: lamblin
Date: 2007-06-12 02:35:48 +0200 (Tue, 12 Jun 2007)
New Revision: 7569

Modified:
   trunk/plearn_learners/online/NLLCostModule.cc
Log:
Relaxed a bit the constraint, so we can handle NaN in input.


Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-06-12 00:31:53 UTC (rev 7568)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-06-12 00:35:48 UTC (rev 7569)
@@ -93,13 +93,19 @@
     PLASSERT( target.size() == target_size );
     cost.resize( output_size );
 
-    PLASSERT_MSG( min(input) >= 0.,
-                  "Elements of \"input\" should be positive" );
-    PLASSERT_MSG( is_equal( sum(input), 1. ),
-                  "Elements of \"input\" should sum to 1" );
+    if( input.hasMissing() )
+        // TODO: should we put something else? infinity?
+        cost[0] = MISSING_VALUE;
+    else
+    {
+        PLASSERT_MSG( min(input) >= 0.,
+                      "Elements of \"input\" should be positive" );
+        PLASSERT_MSG( is_equal( sum(input), 1. ),
+                      "Elements of \"input\" should sum to 1" );
 
-    int the_target = (int) round( target[0] );
-    cost[0] = -pl_log( input[ the_target ] );
+        int the_target = (int) round( target[0] );
+        cost[0] = -pl_log( input[ the_target ] );
+    }
 }
 
 void NLLCostModule::fprop(const Mat& inputs, const Mat& targets, Mat& costs)
@@ -143,26 +149,32 @@
 
         cost->resize(batch_size, port_sizes(2, 1));
 
-        PLASSERT_MSG( min(*prediction) >= 0.,
-                "Elements of \"prediction\" should be positive" );
 
+        for( int i=0; i<batch_size; i++ )
+        {
+            if( (*prediction)(i).hasMissing() )
+            {
+                // TODO: should we put something else? infinity?
+                (*cost)(i,0) = MISSING_VALUE;
+            }
+            else
+            {
 #ifdef BOUNDCHECK
-        for (int i = 0; i < prediction->length(); i++) {
-            // Ensure the distribution probabilities sum to 1. We relax a bit
-            // the default tolerance as probabilities using exponentials could
-            // suffer numerical imprecisions.
-            if (!is_equal( sum((*prediction)(i)), 1., 1., 1e-5, 1e-5 ))
-                PLERROR("In NLLCostModule::fprop - Elements of \"prediction\" "
-                        "should sum to 1 (found a sum = %f)",
-                        sum((*prediction)(i)));
-        }
+                PLASSERT_MSG( min((*prediction)(i)) >= 0.,
+                    "Elements of \"prediction\" should be positive" );
+                // Ensure the distribution probabilities sum to 1. We relax a
+                // bit the default tolerance as probabilities using
+                // exponentials could suffer numerical imprecisions.
+                if (!is_equal( sum((*prediction)(i)), 1., 1., 1e-5, 1e-5 ))
+                    PLERROR("In NLLCostModule::fprop - Elements of"
+                            " \"prediction\" should sum to 1"
+                            " (found a sum = %f)",
+                            sum((*prediction)(i)));
 #endif
-
-        for( int k=0; k<batch_size; k++ )
-        {
-            int target_k = (int) round( (*target)(k,0) );
-            PLASSERT( is_equal( (*target)(k, 0), target_k ) );
-            (*cost)(k,0) = -pl_log( (*prediction)(k, target_k) );
+                int target_i = (int) round( (*target)(i,0) );
+                PLASSERT( is_equal( (*target)(i, 0), target_i ) );
+                (*cost)(i,0) = -pl_log( (*prediction)(i, target_i) );
+            }
         }
     }
     else if( !prediction && !target && !cost )



From lamblin at mail.berlios.de  Tue Jun 12 02:36:50 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 12 Jun 2007 02:36:50 +0200
Subject: [Plearn-commits] r7570 - trunk/plearn_learners/online
Message-ID: <200706120036.l5C0aoWn023158@sheep.berlios.de>

Author: lamblin
Date: 2007-06-12 02:36:49 +0200 (Tue, 12 Jun 2007)
New Revision: 7570

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.cc
Log:
Fix for -noblas compilation


Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-12 00:35:48 UTC (rev 7569)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-12 00:36:49 UTC (rev 7570)
@@ -321,7 +321,7 @@
     if (neg_count==0)
         productScaleAcc(weights_neg_stats,
                         gibbs_neg_up_values,true,
-                        gibbs_neg_down_values,false,normalize_factor,0);
+                        gibbs_neg_down_values,false,normalize_factor,0.);
     else
         productScaleAcc(weights_neg_stats,
                         gibbs_neg_up_values,true,
@@ -357,7 +357,7 @@
     //               * gibbs_neg_up_values'*gibbs_neg_down_values
     static Mat tmp;
     tmp.resize(weights.length(),weights.width());
-    productScaleAcc(tmp,gibbs_neg_up_values,true,gibbs_neg_down_values,false,1,0);
+    productScaleAcc(tmp,gibbs_neg_up_values,true,gibbs_neg_down_values,false,1.,0.);
     if (neg_count==0)
         multiply(weights_neg_stats,tmp,normalize_factor);
     else



From lamblin at mail.berlios.de  Tue Jun 12 02:38:06 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 12 Jun 2007 02:38:06 +0200
Subject: [Plearn-commits] r7571 - trunk/plearn_learners/online
Message-ID: <200706120038.l5C0c6lG023264@sheep.berlios.de>

Author: lamblin
Date: 2007-06-12 02:38:05 +0200 (Tue, 12 Jun 2007)
New Revision: 7571

Modified:
   trunk/plearn_learners/online/RBMConv2DConnection.cc
Log:
Initialize some parameters


Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-12 00:36:49 UTC (rev 7570)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-12 00:38:05 UTC (rev 7571)
@@ -52,7 +52,15 @@
     "");
 
 RBMConv2DConnection::RBMConv2DConnection( real the_learning_rate ) :
-    inherited(the_learning_rate)
+    inherited(the_learning_rate),
+    down_image_length(-1),
+    down_image_width(-1),
+    up_image_length(-1),
+    up_image_width(-1),
+    kernel_step1(1),
+    kernel_step2(1),
+    kernel_length(-1),
+    kernel_width(-1)
 {
 }
 



From lamblin at mail.berlios.de  Tue Jun 12 02:38:33 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 12 Jun 2007 02:38:33 +0200
Subject: [Plearn-commits] r7572 - trunk/plearn_learners/online
Message-ID: <200706120038.l5C0cXtP023286@sheep.berlios.de>

Author: lamblin
Date: 2007-06-12 02:38:32 +0200 (Tue, 12 Jun 2007)
New Revision: 7572

Modified:
   trunk/plearn_learners/online/Convolution2DModule.cc
   trunk/plearn_learners/online/Convolution2DModule.h
Log:
Implementation of new fprop() and bpropAccUpdate()


Modified: trunk/plearn_learners/online/Convolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.cc	2007-06-12 00:38:05 UTC (rev 7571)
+++ trunk/plearn_learners/online/Convolution2DModule.cc	2007-06-12 00:38:32 UTC (rev 7572)
@@ -257,6 +257,16 @@
     output_gradients.resize(n_output_images);
     input_diag_hessians.resize(n_input_images);
     output_diag_hessians.resize(n_output_images);
+
+    // port stuff
+    ports.resize(2);
+    ports[0] = "input";
+    ports[1] = "output";
+
+    port_sizes.resize(nPorts(), 2);
+    port_sizes.column(0).fill(-1);
+    port_sizes(0, 1) = input_size;
+    port_sizes(1, 1) = output_size;
 }
 
 void Convolution2DModule::build_kernels()
@@ -296,6 +306,7 @@
         forget();
 
     kernel_gradient.resize(kernel_length, kernel_width);
+    kernel_gradients.resize(n_input_images, n_output_images);
     squared_kernel.resize(kernel_length, kernel_width);
 }
 
@@ -357,6 +368,52 @@
     }
 }
 
+void Convolution2DModule::fprop(const TVec<Mat*>& ports_value)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+
+    Mat* input = ports_value[0];
+    Mat* output = ports_value[1];
+
+    if( input && !input->isEmpty() && output && output->isEmpty() )
+    {
+        PLASSERT( input->width() == port_sizes(0,1) );
+
+        int batch_size = input->length();
+        output->resize(batch_size, port_sizes(1,1));
+
+        // TODO: optimize
+        for( int k=0; k<batch_size; k++ )
+        {
+            for( int i=0; i<n_input_images; i++ )
+                input_images[i] = (*input)(k)
+                    .subVec(i*input_images_size, input_images_size)
+                    .toMat(input_images_length, input_images_width);
+
+            for( int j=0; j<n_output_images; j++ )
+                output_images[j] = (*output)(k)
+                    .subVec(j*output_images_size, output_images_size)
+                    .toMat(output_images_length, output_images_width);
+
+            for( int j=0; j<n_output_images; j++ )
+            {
+                output_images[j].fill( bias[j] );
+                for( int i=0; i<n_input_images; i++ )
+                    if( connection_matrix(i,j) != 0 )
+                        convolve2D( input_images[i], kernels(i,j),
+                                    output_images[j], kernel_step1,
+                                    kernel_step2, true );
+            }
+        }
+    }
+    else if (!input && !output)
+    {
+        // Nothing to do
+    }
+    else
+        PLCHECK_MSG( false, "Unknown port configuration" );
+}
+
 /* THIS METHOD IS OPTIONAL
 //! Adapt based on the output gradient: this method should only
 //! be called just after a corresponding fprop; it should be
@@ -439,6 +496,102 @@
 
 }
 
+void Convolution2DModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                         const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts()
+              && ports_gradient.length() == nPorts() );
+
+    Mat* input = ports_value[0];
+    Mat* output = ports_value[1];
+    Mat* input_grad = ports_gradient[0];
+    Mat* output_grad = ports_gradient[1];
+
+    // If we want input_grad and we have output_grad
+    if( input_grad && input_grad->isEmpty()
+        && output_grad && !output_grad->isEmpty() )
+    {
+        PLASSERT( input );
+        PLASSERT( output );
+
+        PLASSERT( input->width() == port_sizes(0,1) );
+        PLASSERT( output->width() == port_sizes(1,1) );
+        PLASSERT( input_grad->width() == port_sizes(0,1) );
+        PLASSERT( output_grad->width() == port_sizes(1,1) );
+
+        int batch_size = input->length();
+        PLASSERT( output->length() == batch_size );
+        PLASSERT( output_grad->length() == batch_size );
+
+        input_grad->resize(batch_size, port_sizes(0,1));
+        learning_rate = start_learning_rate /
+            (1.+decrease_constant*step_number);
+        real avg_lr = learning_rate / batch_size;
+
+        // clear kernel gradient
+        for( int i=0; i<n_input_images; i++ )
+            for( int j=0; j<n_output_images; j++ )
+                if( connection_matrix(i,j) != 0 )
+                {
+                    kernel_gradients(i,j).resize(kernel_length, kernel_width);
+                    kernel_gradients(i,j).clear();
+                }
+
+        // TODO: optimize
+        for( int k=0; k<batch_size; k++ )
+        {
+            for( int i=0; i<n_input_images; i++ )
+            {
+                input_images[i] = (*input)(k)
+                    .subVec(i*input_images_size, input_images_size)
+                    .toMat(input_images_length, input_images_width);
+                input_gradients[i] = (*input_grad)(k)
+                    .subVec(i*input_images_size, input_images_size)
+                    .toMat(input_images_length, input_images_width);
+            }
+
+            for( int j=0; j<n_output_images; j++ )
+            {
+                output_images[j] = (*output)(k)
+                    .subVec(j*output_images_size, output_images_size)
+                    .toMat(output_images_length, output_images_width);
+                output_gradients[j] = (*output_grad)(k)
+                    .subVec(j*output_images_size, output_images_size)
+                    .toMat(output_images_length, output_images_width);
+            }
+
+            for( int j=0; j<n_output_images; j++ )
+                for( int i=0; i<n_input_images; i++ )
+                    if( connection_matrix(i,j) != 0 )
+                        convolve2Dbackprop( input_images[i],
+                                            kernels(i,j),
+                                            output_gradients[j],
+                                            input_gradients[i],
+                                            kernel_gradients(i,j),
+                                            kernel_step1, kernel_step2,
+                                            true );
+        }
+
+        for( int j=0; j<n_output_images; j++ )
+        {
+            for( int i=0; i<n_input_images; i++ )
+                if( connection_matrix(i,j) != 0 )
+                    multiplyAcc(kernels(i,j), kernel_gradients(i,j), -avg_lr);
+
+            bias[j] -= avg_lr * sum( (*output_grad)
+                .subMatColumns(j*output_images_size, output_images_size) );
+        }
+    }
+    else if( !input_grad )
+    {
+        PLASSERT( !output_grad || !output_grad->isEmpty() );
+        PLASSERT( !input || !input->isEmpty() );
+        PLASSERT( !output || !output->isEmpty() );
+    }
+    else
+        PLCHECK_MSG( false, "Port configuration not implemented" );
+}
+
 //! reset the parameters to the state they would be BEFORE starting training.
 //! Note that this method is necessarily called from build().
 void Convolution2DModule::forget()

Modified: trunk/plearn_learners/online/Convolution2DModule.h
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.h	2007-06-12 00:38:05 UTC (rev 7571)
+++ trunk/plearn_learners/online/Convolution2DModule.h	2007-06-12 00:38:32 UTC (rev 7572)
@@ -134,6 +134,9 @@
     //! given the input, compute the output (possibly resize it  appropriately)
     virtual void fprop(const Vec& input, Vec& output) const;
 
+    //! New version
+    virtual void fprop(const TVec<Mat*>& ports_value);
+
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop; it should be
     //! called with the same arguments as fprop for the first two arguments
@@ -154,6 +157,10 @@
                              const Vec& output_gradient,
                              bool accumulate=false);
 
+    //! New version
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
     //! back. If these methods are defined, you can use them INSTEAD of
@@ -238,8 +245,12 @@
 
     // Will store gradients wrt kernels
     mutable Mat kernel_gradient;
+    TMat<Mat> kernel_gradients;
+
     // Will store the term-to-term squared kernels (for bbprop)
     mutable Mat squared_kernel;
+
+    TVec<string> ports;
 };
 
 // Declares a few other classes and functions related to this class



From lamblin at mail.berlios.de  Tue Jun 12 02:42:34 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 12 Jun 2007 02:42:34 +0200
Subject: [Plearn-commits] r7573 - trunk/python_modules/plearn/learners/online
Message-ID: <200706120042.l5C0gYQe023619@sheep.berlios.de>

Author: lamblin
Date: 2007-06-12 02:42:34 +0200 (Tue, 12 Jun 2007)
New Revision: 7573

Modified:
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Add convenient function that returns a classification MLP


Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-06-12 00:38:32 UTC (rev 7572)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-06-12 00:42:34 UTC (rev 7573)
@@ -37,14 +37,26 @@
                                            connection('softmax.output','clerr.prediction',False),
                                            connection('target.output','nll.target',False),
                                            connection('target.output','clerr.target',False)],
-                            ports = [ ('in', 'a1.input'),
+                            ports = [ ('input', 'a1.input'),
                                       ('target', 'target.input'),
-                                      ('out', 'softmax.output'),
+                                      ('output', 'softmax.output'),
                                       ('nll', 'nll.cost'),
                                       ('class_err','clerr.cost') ] )
-                            
 
 
+def supervised_classification_mlp_learner(input_size, n_hidden, n_classes,
+        L1wd=0,L2wd=0,lrate=0.01):
+    return (pl.ModuleLearner(
+            module = supervised_classification_mlp(
+                'MLP', input_size, n_hidden, n_classes, L1wd, L2wd, lrate
+                ),
+            target_ports = [ 'target' ],
+            cost_ports = [ 'nll', 'class_err' ]
+            ),
+            [ 'module.modules[%d].start_learning_rate' % i for i in [0,2] ]
+            )
+
+
 def rbm(name,
         visible_size,
         hidden_size,



From nouiz at mail.berlios.de  Tue Jun 12 15:15:01 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 12 Jun 2007 15:15:01 +0200
Subject: [Plearn-commits] r7574 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706121315.l5CDF1Vn006724@sheep.berlios.de>

Author: nouiz
Date: 2007-06-12 15:15:01 +0200 (Tue, 12 Jun 2007)
New Revision: 7574

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.h
   branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc
Log:
-Added error message
-Ameliorated some error message
-localised many global variable when they are used in only one function



Modified: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc	2007-06-12 00:42:34 UTC (rev 7573)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc	2007-06-12 13:15:01 UTC (rev 7574)
@@ -126,17 +126,16 @@
             analyzeVariableStats();
             train();
         }
-        PLERROR("AnalyzeFieldStats: we are done here");
+        PLERROR("AnalyzeFieldStats::build_() we are done here");
     }
 }
 
 void AnalyzeFieldStats::analyzeVariableStats()
 { 
     // initialize primary dataset
-    main_row = 0;
-    main_col = 0;
-    main_length = train_set->length();
+    int main_length = train_set->length();
     main_width = train_set->width();
+    Vec main_input;
     main_input.resize(main_width);
     main_names.resize(main_width);
     main_names << train_set->fieldNames();
@@ -148,11 +147,13 @@
     fields_selected.clear();
     for (fields_col = 0; fields_col < fields_width; fields_col++)
     {
+        int main_col;
         for (main_col = 0; main_col < main_width; main_col++)
         {
             if (fields[fields_col] == main_names[main_col]) break;
         }
-        if (main_col >= main_width) PLERROR("In AnalyzeFieldStats: no field with this name in input dataset: %", (fields[fields_col]).c_str());
+        if (main_col >= main_width) 
+            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in input dataset: %", (fields[fields_col]).c_str());
         fields_selected[main_col] = 1;
     }
     
@@ -178,7 +179,7 @@
     TVec<int> indices;
     to_deal_with_total = 0;
     to_deal_with_next = -1;
-    for (main_col = 0; main_col < main_width; main_col++)
+    for (int main_col = 0; main_col < main_width; main_col++)
     {
         if (header_record[main_col] != 2.0) continue;
         to_deal_with_total += 1;
@@ -188,7 +189,7 @@
     {
         train_set->unlockMetaDataDir();
         reviewGlobalStats();
-        PLERROR("AnalyzeFieldStats: we are done here");
+        PLERROR("AnalyzeFieldStats::analyzeVariableStats() we are done here");
     }
     to_deal_with_name = main_names[to_deal_with_next];
     cout << "total number of variable left to deal with: " << to_deal_with_total << endl;
@@ -205,11 +206,12 @@
     indices.resize((int) main_present);
     ind_next = 0;
     pb = new ProgressBar( "Building the indices for " + to_deal_with_name, main_length);
-    for (main_row = 0; main_row < main_length; main_row++)
+    for (int main_row = 0; main_row < main_length; main_row++)
     {
         to_deal_with_value = train_set->get(main_row, to_deal_with_next);
         if (is_missing(to_deal_with_value)) continue;
-        if (ind_next >= indices.length()) PLERROR("AnalyzeFieldStats: There seems to be more present values than indicated by the stats file");
+        if (ind_next >= indices.length()) 
+            PLERROR("AnalyzeFieldStats::analyzeVariableStats() There seems to be more present values than indicated by the stats file");
         indices[ind_next] = main_row;
         ind_next += 1;
         pb->update( main_row );
@@ -224,7 +226,7 @@
     output_length = (int) main_present;
     if (output_length > max_number_of_samples) output_length = max_number_of_samples;
     output_width = 0;
-    for (main_col = 0; main_col < main_width; main_col++)
+    for (int main_col = 0; main_col < main_width; main_col++)
     {
         if (header_record[main_col] != 1) output_width += 1;
     }
@@ -235,11 +237,13 @@
     output_col = 0;
     for (fields_col = 0; fields_col < fields_width; fields_col++)
     {
+        int main_col;
         for (main_col = 0; main_col < main_width; main_col++)
         {
             if (fields[fields_col] == main_names[main_col]) break;
         }
-        if (main_col >= main_width) PLERROR("In AnalyzeFieldStats: no field with this name in input dataset: %", (fields[fields_col]).c_str());
+        if (main_col >= main_width) 
+            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in input dataset: %", (fields[fields_col]).c_str());
         if (fields_col != to_deal_with_next && header_record[main_col] != 1)
         {
             output_variable_src[output_col] = main_col;
@@ -255,7 +259,7 @@
     
     //Now, we can build the training file
     pb = new ProgressBar( "Building the training file for " + to_deal_with_name, output_length);
-    for (main_row = 0; main_row < output_length; main_row++)
+    for (int main_row = 0; main_row < output_length; main_row++)
     {
         train_set->getRow(indices[main_row], main_input);
         for (output_col = 0; output_col < output_width; output_col++)
@@ -274,11 +278,13 @@
     output_col = 0;
     for (fields_col = 0; fields_col < fields_width; fields_col++)
     {
+        int main_col;
         for (main_col = 0; main_col < targeted_width; main_col++)
         {
             if (fields[fields_col] == targeted_names[main_col]) break;
         }
-        if (main_col >= targeted_width) PLERROR("In AnalyzeFieldStats: no field with this name in targeted dataset: %", (fields[fields_col]).c_str());
+        if (main_col >= targeted_width) 
+            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in targeted dataset: %", (fields[fields_col]).c_str());
         if (fields_col != to_deal_with_next && header_record[main_col] != 1)
         {
             train_test_variable_src[output_col] = main_col;
@@ -292,7 +298,7 @@
     
     //Now, we can build the targeted file
     pb = new ProgressBar( "Building the targeted file for " + to_deal_with_name, train_test_length);
-    for (main_row = 0; main_row < train_test_length; main_row++)
+    for (int main_row = 0; main_row < train_test_length; main_row++)
     {
         targeted_set->getRow(main_row, targeted_input);
         for (output_col = 0; output_col < output_width; output_col++)
@@ -307,7 +313,7 @@
 
 void AnalyzeFieldStats::createHeaderFile()
 { 
-    for (main_col = 0; main_col < main_width; main_col++)
+    for (int main_col = 0; main_col < main_width; main_col++)
     {
         targeted_stats = targeted_set->getStats(main_col);
         targeted_missing = targeted_stats.nmissing();
@@ -328,7 +334,7 @@
 { 
     header_file = new FileVMatrix(header_file_name, true);
     header_file->getRow(0, header_record);
-    for (main_col = 0; main_col < main_width; main_col++)
+    for (int main_col = 0; main_col < main_width; main_col++)
     {
         if (header_record[main_col] == 0) continue;
         if (header_record[main_col] == 2) continue;
@@ -353,7 +359,7 @@
 void AnalyzeFieldStats::reviewGlobalStats()
 { 
     cout << "There is no more variable to deal with." << endl;
-    for (main_col = 0; main_col < main_width; main_col++)
+    for (int main_col = 0; main_col < main_width; main_col++)
     {
         if (header_record[main_col] == 0)
         { 
@@ -414,7 +420,7 @@
     explicit_splitter->splitsets.resize(1,2);
     explicit_splitter->splitsets(0,0) = output_file;
     explicit_splitter->splitsets(0,1) = train_test_file;
-    cond_mean = ::PLearn::deepCopy(cond_mean_template);
+    PP<PTester> cond_mean = ::PLearn::deepCopy(cond_mean_template);
     cond_mean->setOption("expdir", targeted_metadata + "/TreeCondMean/dir/" + to_deal_with_name);
     cond_mean->splitter = new ExplicitSplitter();
     cond_mean->splitter = explicit_splitter;

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.h	2007-06-12 00:42:34 UTC (rev 7573)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.h	2007-06-12 13:15:01 UTC (rev 7574)
@@ -124,11 +124,7 @@
 
     // The rest of the private stuff goes here
     
-    int main_row;
-    int main_col;
-    int main_length;
     int main_width;
-    Vec main_input;
     TVec<string> main_names;
     StatsCollector  main_stats;
     PPath main_metadata;
@@ -166,7 +162,6 @@
     string train_test_path;
     TVec<int> train_test_variable_src;
     VMat train_test_file;
-    PP<PTester> cond_mean;
     PPath results_file_name;
     VMat results_file;
     int results_length;

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-12 00:42:34 UTC (rev 7573)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-12 13:15:01 UTC (rev 7574)
@@ -98,14 +98,14 @@
   source->getExample(i, input, target, weight);
   for (int source_col = 0; source_col < input->length(); source_col++)
     if (is_missing(input[source_col]) && condmean_col_ref[source_col] >= 0) input[source_col] = condmean(i, condmean_col_ref[source_col]);
-    else if (is_missing(input[source_col])) cout << "getExample : " << i << " " << source_col << endl;
+    else if (is_missing(input[source_col])) PLERROR("In ConditionalMeanImputationVMatrix::getExample() we have a missing value in column %d that haven't been assigned a value",source_col);
 }
 
 real ConditionalMeanImputationVMatrix::get(int i, int j) const
 { 
   real variable_value = source->get(i, j);
   if (!is_missing(variable_value) && condmean_col_ref[j] >= 0) return condmean(i, condmean_col_ref[j]);
-    else if (is_missing(variable_value)) cout << "get : " << i << " " << j << endl;
+  else if (is_missing(variable_value)) PLERROR("In ConditionalMeanImputationVMatrix::getExample(%d,%d) we have a missing value that haven't been assigned a value",i,j);
   return variable_value;
 }
 

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc	2007-06-12 00:42:34 UTC (rev 7573)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc	2007-06-12 13:15:01 UTC (rev 7574)
@@ -149,7 +149,7 @@
             train();
             ::PLearn::save(header_expdir + "/" + deletion_threshold_str + "/source_names.psave", source_names);
         }
-        PLERROR("In Experimentation: we are done here");
+        PLERROR("In Experimentation::build_() we are done here");
     }
 }
 
@@ -163,7 +163,7 @@
     main_names.resize(main_width);
     main_names << train_set->fieldNames();
     if (train_set->hasMetaDataDir()) main_metadata = train_set->getMetaDataDir();
-    else if (experiment_directory == "") PLERROR("In Experimentation: we need one of experiment_directory or train_set->metadatadir");
+    else if (experiment_directory == "") PLERROR("In Experimentation::experimentSetUp() we need one of experiment_directory or train_set->metadatadir");
          else main_metadata = experiment_directory;
     if (experiment_without_missing_indicator > 0)
     {
@@ -197,12 +197,12 @@
     target_width = target_set->width();
     target_names.resize(target_width);
     target_names << target_set->fieldNames();
-    if (target_length != main_length) PLERROR("In Experimentation: target and main train datasets should have equal length");
+    if (target_length != main_length) PLERROR("In Experimentation::experimentSetUp() target and main train datasets should have equal length");
     for (target_col = 0; target_col < target_width; target_col++)
     {
         if (target_field_name == target_names[target_col]) break;
     }
-    if (target_col >= target_width) PLERROR("In Experimentation: no field with this name in target dataset: %", target_field_name.c_str());
+    if (target_col >= target_width) PLERROR("In Experimentation::experimentSetUp() no field with this name in target dataset: %", target_field_name.c_str());
     
     // initialize the header file
     cout << "initialize the header file" << endl;
@@ -237,7 +237,7 @@
     {
         reference_train_set->unlockMetaDataDir();
         reviewGlobalStats();
-        PLERROR("In Experimentation: we are done here");
+        PLERROR("In Experimentation::experimentSetUp() we are done here");
     }
     deletion_threshold = deletion_thresholds[to_deal_with_next];
     deletion_threshold_str = tostring(deletion_threshold + 0.005).substr(0,4);
@@ -328,11 +328,12 @@
 void Experimentation::getHeaderRecord()
 { 
     header_file = new FileVMatrix(header_file_name, true);
-    if (header_width != header_file->width()) PLERROR("In Experimentation: the existing header file does not match the deletion_thresholds width)");
+    if (header_width != header_file->width()) 
+        PLERROR("In Experimentation::getHeaderRecord() the existing header file does not match the deletion_thresholds width)");
     header_names = header_file->fieldNames();
     for (header_col = 0; header_col < header_width; header_col++) 
         if (header_names[header_col] != tostring(deletion_thresholds[header_col] + 0.005).substr(0,4))
-            PLERROR("In Experimentation: the existing header file names does not match the deletion_thresholds values)");;
+            PLERROR("In Experimentation::getHeaderRecord() the existing header file names does not match the deletion_thresholds values)");;
     header_file->getRow(0, header_record);
 }
 



From dorionc at mail.berlios.de  Tue Jun 12 20:08:25 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Tue, 12 Jun 2007 20:08:25 +0200
Subject: [Plearn-commits] r7575 - in trunk: plearn_learners/regressors
	python_modules/plearn/utilities
Message-ID: <200706121808.l5CI8P8r018334@sheep.berlios.de>

Author: dorionc
Date: 2007-06-12 20:08:25 +0200 (Tue, 12 Jun 2007)
New Revision: 7575

Modified:
   trunk/plearn_learners/regressors/CubicSpline.h
   trunk/python_modules/plearn/utilities/pldatetime.py
Log:
- Added Matlab-like tic()-toc() functions to pldatetime
- Fixed the declaration of the slope options in CubicSpline


Modified: trunk/plearn_learners/regressors/CubicSpline.h
===================================================================
--- trunk/plearn_learners/regressors/CubicSpline.h	2007-06-12 13:15:01 UTC (rev 7574)
+++ trunk/plearn_learners/regressors/CubicSpline.h	2007-06-12 18:08:25 UTC (rev 7575)
@@ -65,10 +65,10 @@
     //#####  Public Build Options  ############################################
 
     //! The slope to enforce at the leftmost node -- Default: NaN [None]
-    bool m_low_slope;
+    real m_low_slope;
 
     //! The slope to enforce at the rightmost node -- Default: NaN [None]
-    bool m_high_slope;
+    real m_high_slope;
     
 
 public:

Modified: trunk/python_modules/plearn/utilities/pldatetime.py
===================================================================
--- trunk/python_modules/plearn/utilities/pldatetime.py	2007-06-12 13:15:01 UTC (rev 7574)
+++ trunk/python_modules/plearn/utilities/pldatetime.py	2007-06-12 18:08:25 UTC (rev 7575)
@@ -43,6 +43,22 @@
 
     return datetime(yyyy, mm, dd, hh, mn, ss)
 
+__tic = []
+def tic():
+    import time
+    global __tic
+    __tic.append( datetime(*time.localtime()[:6]) )
+
+def toc():
+    import time
+    global __tic
+    now = datetime(*time.localtime()[:6])
+    if len(__tic) == 0:
+        import sys
+        print >>sys.stderr, "Call to toc() corresponding to no tic()..."
+        return -1
+    return now - __tic.pop(-1)
+
 #
 #  Module classes
 #



From lamblin at mail.berlios.de  Wed Jun 13 04:59:08 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 13 Jun 2007 04:59:08 +0200
Subject: [Plearn-commits] r7576 - trunk/plearn/misc
Message-ID: <200706130259.l5D2x83F009708@sheep.berlios.de>

Author: lamblin
Date: 2007-06-13 04:59:07 +0200 (Wed, 13 Jun 2007)
New Revision: 7576

Modified:
   trunk/plearn/misc/viewVMat.cc
Log:
Fix the display of values in plearn vmat view.


Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2007-06-12 18:08:25 UTC (rev 7575)
+++ trunk/plearn/misc/viewVMat.cc	2007-06-13 02:59:07 UTC (rev 7576)
@@ -257,15 +257,11 @@
                     string s = vm_showed->getValString(j,val);
                     if (!view_strings || s == "")
                     {
-                        // problem of display, because if val is a double,
-                        // tostring(val) has 18 digits, which is more than the
-                        // 14 we have for display. Cropping the end doesn't
-                        // solve the problem when the last characters are
-                        // "e-6"...
-                        //s = tostring(val);
-                        // TODO: find a better solution
+                        // We only have 14 characters, and have to display
+                        // correctly numbers like "18270326".
+                        // Best compromise found is "%14.8g".
                         char tmp[1000];
-                        sprintf(tmp, "%14.12g", val);
+                        sprintf(tmp, "%14.8g", val);
                         s = tmp;
                     }
                     else {



From lamblin at mail.berlios.de  Wed Jun 13 05:07:31 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 13 Jun 2007 05:07:31 +0200
Subject: [Plearn-commits] r7577 - trunk/plearn/misc
Message-ID: <200706130307.l5D37VvU010115@sheep.berlios.de>

Author: lamblin
Date: 2007-06-13 05:07:30 +0200 (Wed, 13 Jun 2007)
New Revision: 7577

Modified:
   trunk/plearn/misc/viewVMat.cc
Log:
Cosmetic changes


Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2007-06-13 02:59:07 UTC (rev 7576)
+++ trunk/plearn/misc/viewVMat.cc	2007-06-13 03:07:30 UTC (rev 7577)
@@ -5,18 +5,18 @@
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
-// 
+//
 //  1. Redistributions of source code must retain the above copyright
 //     notice, this list of conditions and the following disclaimer.
-// 
+//
 //  2. Redistributions in binary form must reproduce the above copyright
 //     notice, this list of conditions and the following disclaimer in the
 //     documentation and/or other materials provided with the distribution.
-// 
+//
 //  3. The name of the authors may not be used to endorse or promote
 //     products derived from this software without specific prior written
 //     permission.
-// 
+//
 // THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 // IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 // OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
@@ -27,11 +27,11 @@
 // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
+//
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-/* *******************************************************      
+/* *******************************************************
  * $Id: vmatmain.cc 6316 2006-10-16 23:22:54Z lamblin $
  ******************************************************* */
 
@@ -44,8 +44,8 @@
 #include <plearn/db/getDataSet.h>
 
 #if defined(WIN32) && !defined(__CYGWIN__)
-// There does not seem to be a Windows implementation of 'ncurses', thus we use
-// 'pdcurses' instead.
+// There does not seem to be a Windows implementation of 'ncurses', thus we
+// use 'pdcurses' instead.
 #include <pdcurses/curses.h>
 #else
 #include "curses.h"
@@ -68,7 +68,8 @@
 
 
 // returns false if the input is invalid and write in strReason the reason
-bool getList(char* str, int curj, const VMat& vm, Vec& outList, char* strReason)
+bool getList(char* str, int curj, const VMat& vm, Vec& outList,
+             char* strReason)
 {
     vector<string>columnList;
     if (str[0] == '\0')
@@ -76,7 +77,7 @@
         // nothing was inserted, then gets the current column
         char strj[10];
         sprintf(strj, "%d", curj);
-        columnList.push_back(strj);				
+        columnList.push_back(strj);
     }
     else
     {
@@ -97,7 +98,8 @@
             if (colVal > toint(*vsIt) && separator == '-')
             {
                 invalidInput = true;
-                strcpy(strReason, "Second element in range smaller than the first");
+                strcpy(strReason,
+                       "Second element in range smaller than the first");
                 break;
             }
             colVal = toint(*vsIt);
@@ -161,22 +163,25 @@
     keypad(stdscr,TRUE);
 
     VMat vm_showed = vm;
-  
+
     int key = 0;
     bool view_strings = true;
-    // If 'indent_strings_left' is set to false, then strings will be indented to the right.
+    // If 'indent_strings_left' is set to false, then strings will be indented
+    // to the right.
     bool indent_strings_left = true;
     //! Can take three values:
     //! 0 - usual display
-    //! 1 - values that are *exactly* the same as the one of the previous vmat line will be replaced by ...
-    //! 2 - values that are *approximately* the same as the one of the previous vmat line will be replaced by ...
+    //! 1 - values that are *exactly* the same as the one of the previous vmat
+    //!     line will be replaced by ...
+    //! 2 - values that are *approximately* the same as the one of the
+    //!     previous vmat line will be replaced by ...
     int hide_sameval = 0;
     bool transposed = false;
-  
+
     int namewidth = 0;
     for(int j=0; j<vm->width(); j++)
         namewidth = max(namewidth, (int) vm->fieldName(j).size());
-    int namewidth_ = namewidth; 
+    int namewidth_ = namewidth;
 
     int valwidth = 15;
     int valstrwidth = valwidth-1;
@@ -189,7 +194,7 @@
     int startj = 0;
 
     int vStartHelp = 0;
-  
+
     bool onError=false;
 
     map<int,Vec> cached_columns;
@@ -199,7 +204,7 @@
         while(key != 'q' && key != 'Q')
         {
             erase();
-      
+
             int leftcolwidth = transposed ?1+namewidth :10;
 
             int nj = transposed ? LINES-3 : (COLS-leftcolwidth)/valwidth;
@@ -210,7 +215,7 @@
 
             int x=0, y=0; // (curses coordinates are (y,x) )
 
-            // print field names 
+            // print field names
             for(int j=startj; j<endj; j++)
             {
                 string s = vm_showed->fieldName(j);
@@ -224,9 +229,11 @@
                 else
                 {
                     x = 1+leftcolwidth+(j-startj)*valwidth;
-                    mvprintw(0, x, valstrformat, s.substr(0,valstrwidth).c_str() );
+                    mvprintw(0, x, valstrformat,
+                             s.substr(0,valstrwidth).c_str() );
                     if((int)s.length() > valstrwidth)
-                        mvprintw(1, x, valstrformat, s.substr(valstrwidth,valstrwidth).c_str() );
+                        mvprintw(1, x, valstrformat,
+                                 s.substr(valstrwidth,valstrwidth).c_str() );
                 }
                 // attroff(A_REVERSE);
             }
@@ -235,7 +242,7 @@
             Vec oldv(vm_showed.width());
 
             for(int i=starti; i<endi; i++)
-            { 
+            {
                 if(transposed)
                 {
                     y = 0;
@@ -248,9 +255,9 @@
                     x = 0;
                     mvprintw(y,x,"%d",i);
                 }
-          
+
                 vm_showed->getRow(i,v);
-          
+
                 for(int j=startj; j<endj; j++)
                 {
                     real val = v[j];
@@ -265,47 +272,51 @@
                         s = tmp;
                     }
                     else {
-                        // This is a string. Maybe we want to indent it to the right.
+                        // This is a string. Maybe we want to indent it to the
+                        // right.
                         // In this case we truncate it to its last characters.
                         if (!indent_strings_left) {
                             if (s.size() >= (size_t) valstrwidth) {
-                                s = s.substr(s.size() - valstrwidth, valstrwidth);
+                                s = s.substr(s.size() - valstrwidth,
+                                             valstrwidth);
                             } else {
                                 string added_spaces((size_t) (valstrwidth - s.size()), ' ');
                                 s = added_spaces + s;
                             }
                         }
                     }
-              
+
                     if(transposed)
                         y = 1+(j-startj);
                     else
                         x = 1+leftcolwidth+(j-startj)*valwidth;
-              
+
                     if( i == curi || (vm_showed.width() > 1 && j == curj) )
                         attron(A_REVERSE);
                     //else if ()
                     //  attron(A_REVERSE);
-              
-                    if(hide_sameval == 2 && i>starti && (is_equal(val,oldv[j])) )
-                        mvprintw(y, x, valstrformat, "...");                
+
+                    if(hide_sameval== 2 && i>starti && (is_equal(val,oldv[j])) )
+                        mvprintw(y, x, valstrformat, "...");
                     else if(fast_exact_is_equal(hide_sameval, 1) && i>starti &&
                             (fast_exact_is_equal(val, oldv[j]) ||
                              is_missing(val) && is_missing(oldv[j])))
-                        mvprintw(y, x, valstrformat, "...");                
+                        mvprintw(y, x, valstrformat, "...");
                     else
-                        mvprintw(y, x, valstrformat, s.substr(0,valstrwidth).c_str());
+                        mvprintw(y, x, valstrformat,
+                                 s.substr(0,valstrwidth).c_str());
 
                     attroff(A_REVERSE);
                 }
-                oldv << v;          
+                oldv << v;
             }
 
             string strval = vm_showed->getString(curi, curj);
             mvprintw(0,0,"Cols[%d-%d]", 0, vm_showed.width()-1);
-            mvprintw(LINES-1,0," %dx%d   line= %d   col= %d     %s = %s (%f)", 
+            mvprintw(LINES-1,0," %dx%d   line= %d   col= %d     %s = %s (%f)",
                      vm_showed->length(), vm_showed->width(),
-                     curi, curj, vm_showed->fieldName(curj).c_str(), strval.c_str(), vm_showed(curi,curj));
+                     curi, curj, vm_showed->fieldName(curj).c_str(),
+                     strval.c_str(), vm_showed(curi,curj));
 
             refresh();
             if (!onError)
@@ -316,7 +327,7 @@
             ///////////////////////////////////////////////////////////////
             switch(key)
             {
-            case KEY_LEFT: 
+            case KEY_LEFT:
                 if(transposed)
                 {
                     if(curi>0)
@@ -332,8 +343,8 @@
                         startj=curj;
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
-            case KEY_RIGHT: 
+                //////////////////////////////////////////////////////////////
+            case KEY_RIGHT:
                 if(transposed)
                 {
                     if(curi<vm_showed->length()-1)
@@ -349,8 +360,8 @@
                         ++startj;
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
-            case KEY_UP: 
+                //////////////////////////////////////////////////////////////
+            case KEY_UP:
                 if(transposed)
                 {
                     if(curj>0)
@@ -366,8 +377,8 @@
                         starti = curi;
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
-            case KEY_DOWN: 
+                //////////////////////////////////////////////////////////////
+            case KEY_DOWN:
                 if(transposed)
                 {
                     if(curj<vm_showed->width()-1)
@@ -383,8 +394,8 @@
                         ++starti;
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
-            case KEY_PPAGE: 
+                //////////////////////////////////////////////////////////////
+            case KEY_PPAGE:
                 if(transposed)
                 {
                     curj -= nj;
@@ -404,8 +415,8 @@
                         curi = 0;
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
-            case KEY_NPAGE: 
+                //////////////////////////////////////////////////////////////
+            case KEY_NPAGE:
                 if(transposed)
                 {
                     curj += nj;
@@ -425,9 +436,10 @@
                         starti = max(0,vm_showed->length()-ni);
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
-            case KEY_HOME: 
-                // not working on unix for the moment: see http://dickey.his.com/xterm/xterm.faq.html#xterm_pc_style
+                //////////////////////////////////////////////////////////////
+            case KEY_HOME:
+                // not working on unix for the moment: see
+                // http://dickey.his.com/xterm/xterm.faq.html#xterm_pc_style
                 if(transposed)
                 {
                     curi = 0;
@@ -439,9 +451,10 @@
                     startj = 0;
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
-            case KEY_END: 
-                // not working on unix for the moment: see http://dickey.his.com/xterm/xterm.faq.html#xterm_pc_style
+                //////////////////////////////////////////////////////////////
+            case KEY_END:
+                // not working on unix for the moment: see
+                // http://dickey.his.com/xterm/xterm.faq.html#xterm_pc_style
                 if(transposed)
                 {
                     curi = vm_showed->length()-1;
@@ -453,22 +466,22 @@
                     startj = max(curj-nj + 1, 0);
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
+                //////////////////////////////////////////////////////////////
             case '.':
                 if (hide_sameval == 1)
                     hide_sameval = 0;
                 else
                     hide_sameval = 1;
                 break;
-                ///////////////////////////////////////////////////////////////
+                //////////////////////////////////////////////////////////////
             case ',':
                 if (hide_sameval == 2)
                     hide_sameval = 0;
                 else
                     hide_sameval = 2;
                 break;
-                ///////////////////////////////////////////////////////////////
-            case 't': case 'T':          
+                //////////////////////////////////////////////////////////////
+            case 't': case 'T':
                 transposed = !transposed;
                 nj = transposed ? LINES-3 : (COLS-leftcolwidth)/valwidth;
                 ni = transposed ? (COLS-leftcolwidth)/valwidth : LINES-4;
@@ -477,7 +490,7 @@
                 //endj = min(vm_showed->width(), startj+nj);
                 //endi = min(vm_showed->length(), starti+ni);
                 break;
-                ///////////////////////////////////////////////////////////////
+                //////////////////////////////////////////////////////////////
             case '/':  // search for value
             {
                 echo();
@@ -491,7 +504,7 @@
                 string searchme = removeblanks(l);
                 real searchval = vm_showed(curi,curj);
                 if(searchme!="")
-                { 
+                {
                     searchval = vm_showed->getStringVal(curj, searchme);
                     if(is_missing(searchval))
                     {
@@ -512,7 +525,7 @@
                     clrtoeol();
                     refresh();
                     cached.resize(vm_showed->length());
-                    vm_showed->getColumn(curj,cached);                
+                    vm_showed->getColumn(curj,cached);
                     cached_columns[curj] = cached;
                 }
 
@@ -539,7 +552,7 @@
             }
             break;
             ///////////////////////////////////////////////////////////////
-            case (int)'l': case (int)'L': 
+            case (int)'l': case (int)'L':
             {
                 echo();
                 char strmsg[] = {"Goto line: "};
@@ -567,7 +580,7 @@
             }
             break;
             ///////////////////////////////////////////////////////////////
-            case (int)'c': case (int)'C': 
+            case (int)'c': case (int)'C':
             {
                 echo();
                 char strmsg[] = {"Goto column: "};
@@ -599,7 +612,7 @@
             }
             break;
             ///////////////////////////////////////////////////////////////
-            case (int)'v': case (int)'V': 
+            case (int)'v': case (int)'V':
             {
                 echo();
                 char strmsg[] = {"View dataset ('Enter' = reload last dataset): "};
@@ -637,7 +650,7 @@
             }
             break;
             ///////////////////////////////////////////////////////////////
-            case (int)'i': case (int)'I': 
+            case (int)'i': case (int)'I':
             {
                 echo();
                 char strmsg[] = {"Insert before column ('Enter' = current, '-1' = insert at the end): "};
@@ -685,7 +698,8 @@
                 }
                 TVec<VMat> vmats;
                 if (ins_col > 0)
-                    vmats.append(new SubVMatrix(vm_showed, 0, 0, vm_showed->length(), ins_col));
+                    vmats.append(new SubVMatrix(vm_showed, 0, 0,
+                                                vm_showed->length(), ins_col));
                 Mat col_mat(vm_showed->length(), 1);
                 VMat col_vmat(col_mat);
                 col_vmat->declareFieldNames(TVec<string>(1, ins_name));
@@ -698,7 +712,9 @@
                 }
                 vmats.append(col_vmat);
                 if (ins_col < vm_showed->width())
-                    vmats.append(new SubVMatrix(vm_showed, 0, ins_col, vm_showed->length(), vm_showed->width() - ins_col));
+                    vmats.append(new SubVMatrix(vm_showed, 0, ins_col,
+                                                vm_showed->length(),
+                                                vm_showed->width() - ins_col));
                 vm_showed = new ConcatColumnsVMatrix(vmats);
                 mvprintw(LINES-1,0,"*** Inserted column '%s' at position %d with default value %s ***",
                          ins_name.c_str(), ins_col, default_val.c_str());
@@ -710,7 +726,7 @@
             }
             break;
             ///////////////////////////////////////////////////////////////
-            case (int)'e': case (int)'E': 
+            case (int)'e': case (int)'E':
             {
                 echo();
                 char strmsg[100];
@@ -724,7 +740,8 @@
 
                 Vec indexs;
                 char strReason[100] = {"\0"};
-                bool invalidInput = getList(strRange, curj, vm_showed, indexs, strReason);
+                bool invalidInput = getList(strRange, curj, vm_showed, indexs,
+                                            strReason);
 
                 if (invalidInput)
                 {
@@ -754,9 +771,11 @@
                     clrtoeol();
                     refresh();
 
-                    // Save the selected columns to the desired file, keeping the string values
-                    // if 'view_strings' is currently true (can be toggled with 's'/'S' keys).
-                    vm_showed.columns(indexs)->saveAMAT(filename, false, false, view_strings);
+                    // Save the selected columns to the desired file, keeping
+                    // the string values if 'view_strings' is currently true
+                    // (can be toggled with 's'/'S' keys).
+                    vm_showed.columns(indexs)->saveAMAT(filename, false,
+                                                        false, view_strings);
 
                     mvprintw(LINES-1,0,"*** Output written on: %s ***", fname);
                     clrtoeol();
@@ -770,7 +789,7 @@
             }
             break;
             ///////////////////////////////////////////////////////////////
-            case (int)'r': case (int)'R': 
+            case (int)'r': case (int)'R':
             {
                 echo();
                 char strmsg[100];
@@ -784,7 +803,8 @@
 
                 Vec indexs;
                 char strReason[100] = {"\0"};
-                bool invalidInput = getList(c, curj, vm_showed, indexs, strReason);
+                bool invalidInput = getList(c, curj, vm_showed, indexs,
+                                            strReason);
 
                 if (invalidInput)
                 {
@@ -809,7 +829,7 @@
             }
             break;
             ///////////////////////////////////////////////////////////////
-            case (int)'x': case (int)'X': 
+            case (int)'x': case (int)'X':
                 // Hide the currently selected row.
             {
                 //echo();
@@ -830,11 +850,11 @@
             }
             break;
             ///////////////////////////////////////////////////////////////
-            case (int)'a': case (int)'A': 
+            case (int)'a': case (int)'A':
                 vm_showed = vm;
                 break;
                 ///////////////////////////////////////////////////////////////
-            case (int)'n': case (int)'N': 
+            case (int)'n': case (int)'N':
                 if ( namewidth != namewidth_ )
                     namewidth = namewidth_;
                 else
@@ -843,7 +863,8 @@
 
                     echo();
                     char strmsg[100];
-                    sprintf(strmsg, "Enter namewidth to use (between 10 and %d -- enter=%d): ", namewidth, def);
+                    sprintf(strmsg, "Enter namewidth to use (between 10 and %d -- enter=%d): ",
+                            namewidth, def);
                     mvprintw(LINES-1,0,strmsg);
                     clrtoeol();
 
@@ -870,19 +891,20 @@
                     noecho();
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
+                //////////////////////////////////////////////////////////////
             case (int)'s':
                 if (indent_strings_left)
                     // Toggle display.
                     view_strings = !view_strings;
                 else {
-                    // Do not remove display if we only asked to change indentation.
+                    // Do not remove display if we only asked to change
+                    // indentation.
                     indent_strings_left = true;
                     if (!view_strings)
                         view_strings = true;
                 }
                 break;
-            case (int)'S': 
+            case (int)'S':
                 // Same as above, except we indent to the right.
                 if (!indent_strings_left)
                     view_strings = !view_strings;
@@ -892,7 +914,7 @@
                         view_strings = true;
                 }
                 break;
-                ///////////////////////////////////////////////////////////////
+                //////////////////////////////////////////////////////////////
             case (int)'h': case (int)'H':
                 erase();
 
@@ -924,7 +946,7 @@
                 mvprintw(vStartHelp++,10," - ','       : toggle displaying of ... for values that do not change (approximate match)");
                 mvprintw(vStartHelp++,10," - '/'       : search for a value of the current field");
                 mvprintw(vStartHelp++,10," - 'h' or 'H': display this screen");
-                mvprintw(vStartHelp++,10," - 'q' or 'Q': quit program");          
+                mvprintw(vStartHelp++,10," - 'q' or 'Q': quit program");
                 mvprintw(vStartHelp++,COLS/2-13,"(press any key to continue)");
 
                 refresh();
@@ -932,10 +954,10 @@
 
                 break;
 
-            case (int)'q': case (int)'Q': 
+            case (int)'q': case (int)'Q':
                 break;
 
-                ///////////////////////////////////////////////////////////////
+                //////////////////////////////////////////////////////////////
             default:
                 mvprintw(LINES-1,0,"*** Invalid command (type 'h' for help) ***");
                 // clear the rest of the line
@@ -957,7 +979,7 @@
         endwin();
         throw(e);
     }
-  
+
     // make sure it is clean
     mvprintw(LINES-1,0,"");
     clrtoeol();
@@ -971,9 +993,9 @@
 
     declareFunction("viewVMat", &viewVMat,
                     (BodyDoc("Displays a VMat's contents using curses.\n"),
-                     ArgDoc("vm", 
+                     ArgDoc("vm",
                             "the VMat to display"),
-                     ArgDoc("dataset_spec", 
+                     ArgDoc("dataset_spec",
                             "optional specification of the dataset that will be used to 'reload' it (\"\" works just fine)")));
 
 END_DECLARE_REMOTE_FUNCTIONS



From louradou at mail.berlios.de  Wed Jun 13 23:15:58 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 13 Jun 2007 23:15:58 +0200
Subject: [Plearn-commits] r7578 - in trunk/python_modules/plearn/learners: .
	modulelearners modulelearners/examples
Message-ID: <200706132115.l5DLFwWY007639@sheep.berlios.de>

Author: louradou
Date: 2007-06-13 23:15:55 +0200 (Wed, 13 Jun 2007)
New Revision: 7578

Added:
   trunk/python_modules/plearn/learners/modulelearners/
   trunk/python_modules/plearn/learners/modulelearners/__init__.py
   trunk/python_modules/plearn/learners/modulelearners/__init__.pyc
   trunk/python_modules/plearn/learners/modulelearners/examples/
   trunk/python_modules/plearn/learners/modulelearners/examples/plugNetwork2SVM.py
   trunk/python_modules/plearn/learners/modulelearners/network_view.py
   trunk/python_modules/plearn/learners/modulelearners/network_view.pyc
   trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py
   trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.pyc
Modified:
   trunk/python_modules/plearn/learners/discr_power_SVM.py
Log:
Some tools to "easily" handle NetworkModule and ModuleLearner objects.
In particular, to "plug" SVM on some module(s) of a network (see in modulelearners/examples).
run network_view.py to draw a network (or several) declared in a .pyplearn
script or saved in a .psave file (.py file will be functional soon...)



Modified: trunk/python_modules/plearn/learners/discr_power_SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-13 03:07:30 UTC (rev 7577)
+++ trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-13 21:15:55 UTC (rev 7578)
@@ -68,7 +68,7 @@
 	  self.parameters_names += ['gamma']
 
       def init_gamma(self, gamma):
-          return [gamma/9., gamma, gamma*9.]
+          return [gamma, gamma/9., gamma*9.]
 	  
       def init_parameters( self, samples ):
           dim = len(samples[0])
@@ -131,7 +131,8 @@
       __attributes__ = ['accuracy',
                         'valid_accuracy',
 			'best_parameters',
-			'tried_parameters'
+			'tried_parameters',
+			'save_filename'
 			]
        
       def __init__( self ):
@@ -146,6 +147,8 @@
 	  self.best_parameters      = None  
 	  self.tried_parameters     = {}
 	  
+	  self.save_filename        = None
+	  
           # For cross-validation
 	  self.nr_fold        = 5
 
@@ -197,6 +200,16 @@
 		     train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
 		     model = svm_model(train_problem, param)
 		     accuracy = do_simple_validation(model, samples_target_list[1][0], samples_target_list[1][1], param)
+		     
+		     if self.save_filename != None:
+		        try:
+		           FID=open(self.save_filename,'a')
+		           FID.write('------------\nTry with '+kernel_type+' kernel :\n')
+		           FID.write('parameters : '+str(parameters)+'\n')
+		           FID.write(' --> Accuracy = '+str(accuracy)+'\n')
+		           FID.close()
+		        except:
+		           print "COULD not write in save_filename"   
 		     	     
 		  if accuracy > best_accuracy:
 		         best_parameters = parameters
@@ -232,7 +245,26 @@
 	     model = svm_model(train_problem, param)
 	     accuracy = do_simple_validation(model, samples_target_list[1][0], samples_target_list[1][1], param)
 	  return accuracy
-    
+
+def normalize(data,mean,std):
+    if mean == None:
+       mean=[]
+       for i in range(len(data[0])):
+           mean.append( get_mean_cmp(data,i) )
+    if std == None:
+       std=[]
+       for i in range(len(data[0])):
+           std_tmp=get_std_cmp(data,i)
+	   if std_tmp == 0.:
+	      print "WARNING : standard deviation is 0 on component "+str(i)
+	      std.append( 1. )
+	   else:
+              std.append( std_tmp )
+    for i in range(len(data[0])):
+        for j in range(len(data)):
+	    data[j][i]=(data[j][i]-mean[i])/std[i]
+    return mean, std
+
 def mean_std(data):
     stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
     return sum(stds)/len(stds)
@@ -242,6 +274,9 @@
     avg = tot*1.0/len(values)
     sdsq = sum([(i-avg)**2 for i in values])
     return (sdsq*1.0/(len(values)-1 or 1))**.5
+def get_mean_cmp(data,i):
+    values=[vec[i] for vec in data]
+    return  sum(values)/len(values)
 
 def arithm_mean(data):
     if type(data[0]) == list:

Added: trunk/python_modules/plearn/learners/modulelearners/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-06-13 03:07:30 UTC (rev 7577)
+++ trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-06-13 21:15:55 UTC (rev 7578)
@@ -0,0 +1,647 @@
+import sys, os, os.path
+import plearn.bridgemode
+plearn.bridgemode.interactive = False
+plearn.bridgemode.useserver= False
+from plearn.bridge import *
+#from plearn.pyplearn import *
+
+from plearn.learners.autolr import deepcopy
+
+from plearn.learners.modulelearners.network_view import *
+
+tmp_file='/tmp/modulelearner.py'
+
+def dirtydeepcopy( myObject ):
+    myObject.save(tmp_file,'plearn_ascii')
+    return loadObject(tmp_file)
+
+if plearn.bridgemode.useserver:
+    learner  = serv.new(learner)
+    trainset = serv.new(trainset)
+    validset = serv.new(validset)
+    testset  = serv.new(testset)
+
+
+# examples:
+#   isModule(module,'RBM')
+#   isModule(module,'Split')
+#
+def isModule(module,name):
+    return name+'Module' in str(type(module))
+
+
+# Handling files...
+
+def loadNetworkModule(filename):
+    modulelearner = loadModuleLearner(filename)
+    if 'module' not in modulelearner._optionnames:
+       raise TypeError, "ModuleLearner has no option module"
+    if 'NetworkModule' not in str(type(modulelearner.module)):
+       raise TypeError, "ModuleLearner.module is not a NetworkModule"
+    return modulelearner.module
+    
+
+def loadModuleLearner(filename):
+    if os.path.isfile(filename) == False:
+       raise TypeError, "could not find the file "+filename
+    extension = os.path.splitext(filename)[1]
+    
+    if extension == '.pyplearn':
+       from plearn.learners.modulelearners.pyplearn_read import *
+       object_dict = read_objects( filename, ['HyperLearner', 'PTester', 'MemoryVMatrix', 'AutoVMatrix'] , tmp_file)
+       execfile(tmp_file)
+       modules=[]
+       for i in object_dict['ModuleLearner']:
+           modules += eval(i)
+       if len(modules) == 0:
+          raise TypeError, "could not find a NetworkModule in "+filename
+       if len(modules) == 1:
+          return modules[0]
+       else:
+          print "WARNING: found several NetworkModules in "+filename+" (a list is returned)"
+	  return modules
+	  
+    elif extension == '.psave':
+       myObject = loadObject(filename)
+       if 'ModuleLearner' in str(type(myObject)):
+          return myObject
+       elif 'HyperLearner' in str(type(myObject)):
+          if 'ModuleLearner' in str(type(myObject.learner)):
+	     return myObject.learner
+	  else:
+	     raise TypeError, "The learner of HyperLearner in "+filename+" has type "+str(type(myObject.learner))
+       else:
+          raise TypeError, "Could not recognize the type "+str(type(myObject))
+
+    else:
+       raise TypeError, "could not recognize the extension ("+extension+") of "+filename
+
+# String tools...
+
+def port2moduleName( portName ):
+    return portName.split('.')[0]
+
+
+# Getting information about the network...
+
+
+def getModules( myObject ):
+    # Network module
+    if 'NetworkModule' in str(type(myObject)):
+       return copy.copy( myObject.modules )
+    # Module Learner
+    elif 'ModuleLearner' in str(type(myObject)):
+       return copy.copy( myObject.module.modules )
+    # List of modules
+    elif type(myObject) == list and 'Module' in str(type(myObject[0])):
+         return myObject
+    else:
+        raise TypeError, "Please give a ModuleLearner or NetworkModule"
+
+def setModules( myObject , new_connections_list):
+    if 'NetworkModule' in str(type(myObject)):
+       myObject.setOptionFromPython('modules',new_connections_list)
+    elif 'ModuleLearner' in str(type(myObject)):
+       myObject.module.setOptionFromPython('modules',new_connections_list)
+    else:
+        raise TypeError, "Please give a ModuleLearner or NetworkModule"
+
+
+def getModulesNames( myObject ):
+    modules_names=[]
+    for module in getModules(myObject):
+        modules_names.append(module.name)
+    return modules_names
+
+
+def getModule( myObject, modulename ):
+    modules_names = getModulesNames( myObject )
+    if modulename not in modules_names:
+       raise TypeError, "Could not find module "+modulename
+    index = modules_names.index( modulename )
+    # Network module
+    if 'NetworkModule' in str(type(myObject)):
+       return copy.copy( myObject.modules[index] )
+    # Module Learner
+    elif 'ModuleLearner' in str(type(myObject)):
+       return copy.copy( myObject.module.modules[index] )
+    # List of modules
+    elif type(myObject) == list and 'Module' in str(type(myObject[0])):
+       return myObject[index]
+    else:
+       raise TypeError, "Please give a ModuleLearner or NetworkModule"
+    
+def getConnections( myObject ):
+    if 'NetworkModule' in str(type(myObject)):
+       return copy.copy( myObject.connections )
+    elif 'ModuleLearner' in str(type(myObject)):
+       return copy.copy( myObject.module.connections )
+    else:
+        raise TypeError, "Please give a ModuleLearner or NetworkModule"
+
+def setConnections( myObject , new_connections_list):
+    if 'NetworkModule' in str(type(myObject)):
+       myObject.setOptionFromPython('connections',new_connections_list)
+    elif 'ModuleLearner' in str(type(myObject)):
+       myObject.module.setOptionFromPython('connections',new_connections_list)
+    else:
+        raise TypeError, "Please give a ModuleLearner or NetworkModule"
+
+def getPorts( myObject ):
+    if 'NetworkModule' in str(type(myObject)):
+       return copy.copy( myObject.ports )
+    elif 'ModuleLearner' in str(type(myObject)):
+       return copy.copy( myObject.module.ports )
+    else:
+        raise TypeError, "Please give a ModuleLearner or NetworkModule"
+
+def setPorts( myObject , new_connections_list):
+    if 'NetworkModule' in str(type(myObject)):
+       myObject.setOptionFromPython('ports',new_connections_list)
+    elif 'ModuleLearner' in str(type(myObject)):
+       myObject.module.setOptionFromPython('ports',new_connections_list)
+    else:
+        raise TypeError, "Please give a ModuleLearner or NetworkModule"
+
+def get_last_layer_module_name( learner ):
+    last_layer = []
+    output_layer = getOutputModuleName(learner)
+    connections_list = getConnections(learner)
+    for connection in connections_list:
+        if output_layer in connection.destination:
+	   last_layer.append( port2moduleName( connection.source ) )
+    if len(last_layer) != 1:
+       raise TypeError, "find several layers before output layer\n(" + str(last_layer) + ")"
+    return last_layer[0]
+
+def get_last_layer_module( learner ):
+    last_module_name = get_last_layer_module_name(learner)
+    modules_list = getModules( learner )
+    for module in modules_list:
+        if module.name == last_module_name:
+	   return module
+
+def getOutputModuleName( learner ):
+    outputPort = getOutputPort(learner)
+    return port2moduleName(outputPort[1])
+
+def getOutputPort( learner ):
+    ports_list = getPorts(learner)
+    for port in ports_list:
+        if 'output' in port:
+           return port
+    raise 'cannot find output port of learner'
+
+def getCostModuleName( learner ):
+    costPort = getCostPort(learner)
+    return port2moduleName(costPort[1])
+
+def getCostPort( learner ):
+    cost_port = learner.cost_ports[0]
+    ports_list = getPorts(learner)
+    for port in ports_list:
+        if cost_port in port:
+           return port
+    raise 'cannot find output port of learner'
+
+
+def getSourcePorts( learner, modulename ):
+    inputports = []
+    for connection in getConnections(learner):
+        if modulename in connection.destination:
+	   inputports.append( connection.source )
+    return inputports
+
+def getAllPrevious( myObject, modulename ):
+    connections_list = getConnections(myObject)
+    previous_connections = []
+    previous_modulenames = []
+    for connection in connections_list:
+        if modulename in connection.destination:
+	   previous_connections.append(connection)
+	   previous_modulenames.append(port2moduleName(connection.source))
+    for sourcename in previous_modulenames:
+        connections, sources = getAllPrevious(myObject, sourcename)
+        for connection in connections:
+	    if connection not in previous_connections:
+               previous_connections.append(connection)
+	       source = port2moduleName(connection.source)
+	       if source not in previous_modulenames:
+	          previous_modulenames.append(source)
+    return previous_connections, previous_modulenames
+
+def getAllPreviousConnection( myObject, modulename ):
+    previous_connections, previous_modulenames = getAllPrevious( myObject, modulename )
+    return previous_connections
+
+
+def getPrevious( myObject, modulename ):
+    connections_list = getConnections(myObject)
+    previous_connections = []
+    previous_modulenames = []
+    for connection in connections_list:
+        if modulename in connection.destination:
+	   previous_connections.append(connection)
+	   previous_modulenames.append(port2moduleName(connection.source))
+    return previous_connections, previous_modulenames
+
+def getPreviousConnection( myObject, modulename ):
+    connections_list = getConnections(myObject)
+    previous_connections = []
+    for connection in connections_list:
+        if modulename in connection.destination:
+	   previous_connections.append(connection)
+    return previous_connections
+
+def getAllNext( myObject, modulename ):
+    connections_list = getConnections(myObject)
+    next_connections = []
+    next_modulenames = []
+    for connection in connections_list:
+        if modulename in connection.source:
+	   next_connections.append(connection)
+	   next_modulenames.append(port2moduleName(connection.destination))
+    for sourcename in next_modulenames:
+        connections, sources = getAllNext(myObject, sourcename)
+        for connection in connections:
+	    if connection not in next_connections:
+               next_connections.append(connection)
+	       source = port2moduleName(connection.destination)
+	       if source not in next_modulenames:
+	          next_modulenames.append(source)
+    return next_connections, next_modulenames
+
+def getNext( myObject, modulename ):
+    connections_list = getConnections(myObject)
+    next_connections = []
+    next_modulenames = []
+    for connection in connections_list:
+        if modulename in connection.source:
+	   next_connections.append(connection)
+	   next_modulenames.append(port2moduleName(connection.destination))
+    return next_connections, next_modulenames
+
+
+def removeLastModule( learner , modulename ):
+    return removeModule( learner , getOutputModuleName(learner) )
+
+def removeModuleFromNetwork( network , modulename ):
+    modules_name_list = getModulesNames(network)
+    if modulename not in modules_name_list:
+       raise TypeError, "Module's name "+modulename+" not found in ModuleLearner" 
+
+    new_network = deepcopy(network)
+
+    connections_list     = getConnections(network)
+    new_connections_list = getConnections(network)
+    ports_list     = getPorts(network)
+    new_ports_list = getPorts(network)
+    modules_list     = getModules(network)
+    new_modules_list = getModules(network)
+
+
+           
+    # Creating the list of top modules to remove
+    # (looking at the connections)
+    #        
+    connections_to_reject, modules_to_reject = getAllNext( network, modulename )
+    modules_to_reject.append(modulename)
+    #connections_to_reject.append(getPreviousConnection(network, modulename))
+
+    # Rejecting modules
+    #
+    for module in modules_list:
+        if module.name in modules_to_reject:
+	   new_modules_list.remove(module)
+
+
+    # Rejecting connections
+    #
+    for module_being_rejected in modules_to_reject:
+          for connection in connections_list:
+              if module_being_rejected in connection.source+" "+connection.destination and connection in new_connections_list:
+                 new_connections_list.remove(connection)
+		 
+    # Rejecting ports
+    #
+    new_output = None
+    for port in ports_list:
+        if port2moduleName(port[1]) in modules_to_reject:
+           new_ports_list.remove(port)
+           if port[0] in ['output']:
+	      module_being_rejected = port2moduleName(port[1])
+	      print module_being_rejected
+	      #
+	      # Getting the inputs (to connect to ports)
+              #
+              if new_output == None:
+	         new_outputs = getSourcePorts( network, module_being_rejected )
+		 test_new_outputs = True
+		 while test_new_outputs:
+		       test_new_outputs = False
+		       for checked_output in new_outputs:
+		           checked_module = port2moduleName(checked_output)
+		           if checked_module in modules_to_reject:
+			      new_outputs.remove(checked_output)
+			      new_outputs += getSourcePorts( network, checked_module )
+		              test_new_outputs = True
+			      break
+		 print new_outputs
+                 if len(new_outputs)>1:
+	            output_sizes = []
+		    name_join = '_'+port[0]
+	            for i in range(len(new_outputs)):
+		        port_tmp = new_outputs[i]
+		        new_connections_list.append(pl.NetworkConnection(source = port_tmp,
+		   			                                 destination = name_join+'.in'+str(i),
+		   				                         propagate_gradient = connections_to_reject[0].propagate_gradient
+		   				   ))
+		        module = learner.module.modules[modules_name_list.index(port2moduleName(port_tmp))]
+		        output_size = module.output_size
+		        if output_size == -1:
+		           if 'connection' in module._optionnames:
+			      output_size = module.connection.output_size
+			   else:
+			      raise TypeError, "could not find outputsize of module "+module.name
+                    new_modules_list.append(pl.SplitModule(
+							name = name_join,
+							down_port_name = 'output',
+							up_port_names = ['in'+str(j) for j in range(len(new_outputs))],
+							up_port_sizes = output_sizes
+                                        ))
+                    new_output = name_join+'.output'
+                 else:
+                    new_output = new_outputs[0]
+	      new_port=(port[0],new_output)
+	      new_ports_list.append(new_port)
+
+	   
+    setConnections(new_network, new_connections_list)
+    setPorts(new_network, new_ports_list)
+    setModules(new_network, new_modules_list)
+#    new_network.setOptionFromPython('connections',new_connections_list)
+#    new_network.setOptionFromPython('ports',new_ports_list)
+#    new_network.setOptionFromPython('modules',new_modules_list)
+    return dirtydeepcopy( new_network )
+    return new_network
+    
+
+def removeModuleFromLearner( learner , modulename ):
+    new_learner = deepcopy( learner )
+    new_network = removeModuleFromNetwork( learner.module , modulename )
+    new_ports_list = [ port[0] for port in getPorts(new_network)  ]
+    
+    print new_ports_list
+
+    # Rejecting ports
+    #
+    for port_option_name in ['target_ports', 'cost_ports', 'weight_ports']:
+        output_ports_list     = copy.copy( learner.getOption(port_option_name) )
+	new_output_ports_list = copy.copy( learner.getOption(port_option_name) )
+        for port in output_ports_list:
+            if port not in new_ports_list:
+	       new_output_ports_list.remove(port)
+        new_learner.setOptionFromPython(port_option_name, new_output_ports_list)
+    new_learner.setOptionFromPython('module',new_network)
+
+    return dirtydeepcopy( new_learner )
+    return new_learner
+
+
+def plug2output(myObject, portslist):
+    output_port_tuple = getOutputPort(myObject)
+    output_port = output_port_tuple[1]
+    if len(portslist) == 1 and portslist[0] == output_port:
+       print "WARNING: plug2output return the same myObject"
+       return myObject
+       
+    mynewObject = deepcopy(myObject)
+    new_ports_list = getPorts(myObject)
+    new_ports_list.remove(output_port_tuple)
+    if len(portslist) == 1:
+       new_ports_list.append((output_port_tuple[0],portslist[0]))
+       setPorts(mynewObject, new_ports_list)
+       return dirtydeepcopy( mynewObject )
+       return mynewObject
+    new_modules_list = getModules(myObject)
+    new_connections_list = getConnections(myObject)
+    name_join = "_".join(portslist).replace('.','')
+    output_sizes = []
+    modules_name_list = getModulesNames(myObject)
+    for i in range(len(portslist)):
+        port = portslist[i]
+        new_connections_list.append(pl.NetworkConnection(source = port,
+		   			                 destination = name_join+'.in'+str(i),
+		   				         propagate_gradient = 0
+		   		    ))
+        module = myObject.module.modules[modules_name_list.index(port2moduleName(port))]
+        output_size = module.output_size
+	optionnames = module._optionnames
+        if 'connection' in optionnames: # RBM
+           output_size = module.connection.output_size
+	elif 'up_port_sizes' in optionnames: # splitModule
+           output_size = module.up_port_sizes[module.up_port_names.index(port.split('.')[1])]
+	else:
+	   output_size = module.output_size
+	   if output_size == -1:      
+              raise TypeError, "could not find outputsize of module "+module.name
+	output_sizes.append(output_size)
+    new_modules_list.append(pl.SplitModule(
+							name = name_join,
+							down_port_name = 'output',
+							up_port_names = ['in'+str(j) for j in range(len(portslist))],
+							up_port_sizes = output_sizes
+                           ))
+    new_ports_list.append((output_port_tuple[0],name_join+'.output'))
+    setConnections(mynewObject, new_connections_list)
+    setPorts(mynewObject, new_ports_list)
+    setModules(mynewObject, new_modules_list)
+    return dirtydeepcopy( mynewObject )
+    return mynewObject
+
+
+
+
+def mean_std(data):
+    stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
+    return sum(stds)/len(stds)
+def get_std_cmp(data,i):
+    values=[vec[i] for vec in data]
+    tot = sum(values)
+    avg = tot*1.0/len(values)
+    sdsq = sum([(i-avg)**2 for i in values])
+    return (sdsq*1.0/(len(values)-1 or 1))**.5
+def proposed_gamma(samples):
+          dim = len(samples[0])
+	  std = mean_std(samples)
+	  rho=sqrt(dim)*std
+          return 1/(2*rho**2)
+	  
+def compute_entries_SVM_write_libSVM_file( learner, dataSet, output_filename ):
+    if os.path.isfile( output_filename ):
+       print "WARNING: file "+output_filename+"already exists"
+       pass
+    temp_file='/tmp/new_learner.psave'
+    learner.save(temp_file,'plearn_ascii')
+    learner = loadObject(temp_file)
+    connect_last_layer2outputs( learner )
+    learner.save(temp_file,'plearn_ascii')
+    learner = loadObject(temp_file)
+    outputs=make_test_output(learner,dataSet)
+    nsamples = dataSet.length
+    outputsize = len(outputs[0])
+    if nsamples != len(outputs):
+       raise "ERROR: conflict in dimensions"
+    else:
+       print str(nsamples)+" samples, new input size ="+str(outputsize)
+    if dataSet.targetsize != 1:
+       raise "ERROR: target size not equal to 1 in "+data_filename
+    dim = len(dataSet.getRow(0))-1
+    data = [ dataSet.getRow(i)[0:dim] for i in range(dataSet.length) ]
+    print "SUGGESTED gamma: "+str(proposed_gamma(data))
+    FID = open(output_filename, 'w')
+    for i in range(nsamples):
+        X = dataSet.getRow(i)
+        target = X[dataSet.inputsize]
+        FID.write(str(int(target))+" ")
+	for j in range(outputsize):
+            FID.write(str(j+1)+":"+str(output[i][j])+" ")
+	FID.write("\n")
+    FID.close()
+
+
+
+def compute_entries_SVM( learner, dataSet):
+    temp_file='/tmp/new_learner.psave'
+#    learner.save(temp_file,'plearn_ascii')
+#    learner = loadObject(temp_file)   
+    learner = deepcopy(learner)
+    connect_last_layer2outputs( learner )
+    learner.save(temp_file,'plearn_ascii')
+    learner = loadObject(temp_file)
+    outputs=make_test_output(learner,dataSet)
+    nsamples = dataSet.length
+    outputsize = len(outputs[0])
+    if nsamples != len(outputs):
+       raise "ERROR: conflict in dimensions"
+    else:
+       print str(nsamples)+" samples, new input size ="+str(outputsize)
+    if dataSet.targetsize != 1:
+       raise "ERROR: target size not equal to 1 in "+data_filename
+    return outputs, [ dataSet.getRow(i)[dataSet.inputsize] for i in range(nsamples) ]
+
+def computeOutputsTargets(learner,dataSet):
+    ts=pl.VecStatsCollector()
+    (stats,outputs,costs)=learner.test(dataSet,ts,True,False)
+    targets=pl.SelectColumnsVMatrix(
+       source = dataSet.getObject(),
+       inputsize=0,
+       targetsize = dataSet.targetsize,
+       weightsize=0,
+       indices = range(dataSet.inputsize, dataSet.inputsize+dataSet.targetsize)
+    ).getMat()
+    return outputs, [int(target) for target in targets]
+
+    
+if __name__ == '__main__':
+
+    from plearn.learners.discr_power_SVM import *
+
+    learner_filename = "/u/louradoj/PRGM/blocksworld/res/textual_v2/BESTdbn/final_learner.psave"
+    if os.path.isfile(learner_filename) == False:
+       raise TypeError, "ERROR : Learner file cannot be find\n\tCould not find file "+learner_filename
+    learner = loadModuleLearner(learner_filename)
+        
+    #networkview( learner )
+
+    new_learner = removeModuleFromLearner( learner, "rbm_21" )
+    networkview( new_learner )
+
+    raise TypeError, "OK"
+
+    data_filename = "/cluster/opter/data/babyAI/textual_v2/BABYAI_gray_10x2obj_32x32.color-size-location-shape.3gram.amat"
+    if os.path.isfile(data_filename) == False:
+       raise TypeError, "ERROR : Data file cannot be find\n\tCould not find file "+data_filename
+    dataSet = pl.AutoVMatrix( filename = data_filename )
+    
+    new_learner = plug2output( learner, ['split.out1', 'rbm_3.hidden.state', 'rbm_12.hidden.state'])
+    getModulesNames(new_learner)  
+    outputs, targets = computeOutputsTargets( new_learner, dataSet)
+    #networkview( new_learner )
+    
+
+#    learner_filename = "/u/louradoj/PRGM/blocksworld/res/textual_v2/BESTdbn/final_learner.psave"
+#    learner = loadObject(learner_filename)
+#    data_filename = '/cluster/opter/data/babyAI/textual_v2/BABYAI_gray_10x2obj_32x32.color-size-location-shape.3gram.amat'
+#    dataSet = pl.AutoVMatrix( filename = data_filename )
+    
+#    outputs2, targets2 = compute_entries_SVM( learner, dataSet)
+#    outputs3, targets3 = compute_entries_SVM( learner, dataSet)    
+#    raise TypeError, "douuuu"
+
+    dataPath='/u/lisa/db/babyAI/textual_v2/BACKUP' #'/cluster/opter/data/babyAI/textual_v2/'
+    
+        
+    
+    dataTrain_filename = dataPath+'/BABYAI_gray_10000x2obj_32x32.color-size-location-shape.train.3gram.vmat'
+    dataValid_filename = dataPath+'/BABYAI_gray_5000x2obj_32x32.color-size-location-shape.valid.3gram.vmat'
+    dataTest_filename = dataPath+'/BABYAI_gray_5000x2obj_32x32.color-size-location-shape.test.3gram.vmat'
+    result_dir = os.path.dirname(learner_filename)
+
+    if 'ModuleLearner' not in str(type(learner)):
+        if learner.hasOption('learner') and 'ModuleLearner' in str(type(learner.learner)):
+	   learner=learner.learner
+        else:
+           raise TypeError,  'Sorry, but this code can only be used with ModuleLearner !!!'
+    learner_nickname = 'DBN-2-2-1_'+get_last_layer_module_name( learner ) #os.path.basename(learner_filename)
+    
+  
+    for typeDataSet in ['Train','Valid','Test']:
+        data_filename = globals()['data'+typeDataSet+'_filename']
+        if os.path.isfile(data_filename) == False:
+           raise TypeError, "ERROR : Data file cannot be find\n\tCould not find file "+data_filename
+        print "CONVERSION "+data_filename
+	dataSet = pl.AutoVMatrix( filename = data_filename )
+        globals()[typeDataSet+'_outputs'], globals()[typeDataSet+'_targets'] = compute_entries_SVM( learner, dataSet)
+
+    
+    E=discr_power_SVM_eval()
+    output_filename = result_dir+'/SVM_results_'+"_"+learner_nickname+os.path.basename(data_filename).replace(".vmat","").replace(".amat","")
+    
+    print "Writing results in "+output_filename
+    if os.path.isfile(output_filename):
+       print "WARNING : output "+output_filename+" already exists"
+       FID = open(output_filename, 'a')
+       abspath = os.path.realpath(learner_filename)
+       FID.write('LEARNER.: '+abspath+'\n')
+       for i in range(3):
+           abspath = os.path.dirname(abspath)
+       global_results = abspath+'/global_stats.pmat'
+       if os.path.isfile(global_results):
+          os.system("echo   baseline test error rate : `plearn vmat cat "+global_results+" | tail -1 | awk '{print $NF}'` \%   >> "+output_filename )
+       else:
+          print "WARNING : could not find global_stats.pmat\n\t( "+abspath+"/global_stats.pmat )"
+       FID.write('Train...: '+os.path.realpath(dataTrain_filename)+'\n')
+       FID.write('Valid...: '+os.path.realpath(dataValid_filename)+'\n')
+       FID.write('Test....: '+os.path.realpath(dataTest_filename)+'\n')
+    else:
+       FID = open(output_filename, 'w')
+    FID.write('--------\n')
+
+    E.save_filename = output_filename
+    E.valid_and_compute_accuracy( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    FID = open(output_filename, 'a')
+    FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
+    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.close()
+    E.valid_and_compute_accuracy( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    FID = open(output_filename, 'a')
+    FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
+    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.close()
+    E.valid_and_compute_accuracy( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    FID = open(output_filename, 'a')
+    FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
+    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.close()
+    print "Results written in "+output_filename

Added: trunk/python_modules/plearn/learners/modulelearners/__init__.pyc
===================================================================
(Binary files differ)


Property changes on: trunk/python_modules/plearn/learners/modulelearners/__init__.pyc
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/python_modules/plearn/learners/modulelearners/examples/plugNetwork2SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/plugNetwork2SVM.py	2007-06-13 03:07:30 UTC (rev 7577)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/plugNetwork2SVM.py	2007-06-13 21:15:55 UTC (rev 7578)
@@ -0,0 +1,99 @@
+import sys
+from plearn.learners.modulelearners  import *
+from plearn.learners.discr_power_SVM import *
+
+if __name__ == '__main__':
+
+
+    ports_list = sys.argv[1:]
+    # ex: 'split.out1', 'split.out2',  'rbm_3.hidden.state', 'rbm_12.hidden.state'
+
+
+    learner_filename = "/u/louradoj/PRGM/blocksworld/res/textual_v2/BESTdbn/final_learner.psave"
+    if os.path.isfile(learner_filename) == False:
+       raise TypeError, "ERROR : Learner file cannot be find\n\tCould not find file "+learner_filename
+    learner = loadModuleLearner(learner_filename)
+    learner_nickname = 'DBN-2-2-1_'+"_".join(ports_list).replace(".","")
+
+    dataPath='/cluster/opter/data/babyAI/textual_v2/'    
+    dataTrain_filename = dataPath+'/BABYAI_gray_10000x2obj_32x32.color-size-location-shape.train.3gram.vmat'
+    dataValid_filename = dataPath+'/BABYAI_gray_5000x2obj_32x32.color-size-location-shape.valid.3gram.vmat'
+    dataTest_filename = dataPath+'/BABYAI_gray_5000x2obj_32x32.color-size-location-shape.test.3gram.vmat'
+
+
+#
+#    A small dataset for debug...
+#
+#    data_filename = "/cluster/opter/data/babyAI/textual_v2/BABYAI_gray_10x2obj_32x32.color-size-location-shape.3gram.amat"
+#    dataTrain_filename = data_filename
+#    dataTest_filename = data_filename
+#    dataValid_filename = data_filename
+
+    result_dir = os.path.dirname(learner_filename)
+    output_filename = result_dir+'/SVM_results_'+"_"+learner_nickname+"-"+os.path.basename(dataTrain_filename).replace(".vmat","").replace(".amat","")
+
+
+#               #
+#   MAIN PART   #
+#               #
+
+
+    new_learner = plug2output( learner, ports_list)
+
+    
+  
+    for typeDataSet in ['Train','Valid','Test']:
+        data_filename = globals()['data'+typeDataSet+'_filename']
+        if os.path.isfile(data_filename) == False:
+           print "ERROR : Data file cannot be find\n\tCould not find file "+data_filename
+           sys.exit(0)
+        print "CONVERSION "+data_filename
+	dataSet = pl.AutoVMatrix( filename = data_filename )
+        globals()[typeDataSet+'_outputs'], globals()[typeDataSet+'_targets'] = computeOutputsTargets( new_learner, dataSet)
+	#
+	# Normalizing the data (/!\ compute statistics on the training data and assumes it comes first)
+	#
+        if typeDataSet == 'Train':
+           mean, std = normalize(globals()[typeDataSet+'_outputs'],None,None)
+	else:
+	   normalize(globals()[typeDataSet+'_outputs'],mean,std)
+
+    E=discr_power_SVM_eval()
+    
+    print "Writing results in "+output_filename
+    if os.path.isfile(output_filename):
+       print "WARNING : output "+output_filename+" already exists"
+       FID = open(output_filename, 'a')
+       abspath = os.path.realpath(learner_filename)
+       FID.write('LEARNER.: '+abspath+'\n')
+       for i in range(3):
+           abspath = os.path.dirname(abspath)
+       global_results = abspath+'/global_stats.pmat'
+       if os.path.isfile(global_results):
+          os.system("echo   baseline test error rate : `plearn vmat cat "+global_results+" | tail -1 | awk '{print $NF}'` \%   >> "+output_filename )
+       else:
+          print "WARNING : could not find global_stats.pmat\n\t( "+abspath+"/global_stats.pmat )"
+       FID.write('Train...: '+os.path.realpath(dataTrain_filename)+'\n')
+       FID.write('Valid...: '+os.path.realpath(dataValid_filename)+'\n')
+       FID.write('Test....: '+os.path.realpath(dataTest_filename)+'\n')
+    else:
+       FID = open(output_filename, 'w')
+    FID.write('--------\n')
+
+    E.save_filename = output_filename
+    E.valid_and_compute_accuracy( 'LINEAR' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    FID = open(output_filename, 'a')
+    FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
+    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.close()
+    E.valid_and_compute_accuracy( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    FID = open(output_filename, 'a')
+    FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
+    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.close()
+    E.valid_and_compute_accuracy( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    FID = open(output_filename, 'a')
+    FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
+    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.close()
+    print "Results written in "+output_filename

Added: trunk/python_modules/plearn/learners/modulelearners/network_view.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-06-13 03:07:30 UTC (rev 7577)
+++ trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-06-13 21:15:55 UTC (rev 7578)
@@ -0,0 +1,287 @@
+#!/usr/bin/env python
+
+import os, os.path
+import sys
+
+from plearn.pymake.pymake import *
+
+try:
+  from plearn.pyext import *
+except:
+  PLEARNDIR = os.environ.get('PLEARNDIR', os.getcwd())
+  PLEARNDIRpyext = os.path.join(PLEARNDIR,'python_modules','plearn','pyext')
+  PLEARNDIRpyextOBJ =  os.path.join(PLEARNDIRpyext,'OBJS')
+  DIRS=os.listdir(PLEARNDIRpyextOBJ)
+  l=len(DIRS)
+  for dirname in DIRS:
+      l -= 1
+      if l>0 and 'double' in dirname:
+         DIRS.append(dirname)
+         continue
+      elif l>0 and 'dbg' in dirname:
+         DIRS.append(dirname)
+         continue
+      dirname = os.path.join(PLEARNDIRpyextOBJ, dirname)      
+      if 'libplext.so' in os.listdir(dirname):
+         linux_command = 'ln -sf '+ os.path.join(dirname,'libplext.so') + ' ' + os.path.join(PLEARNDIRpyext, 'libplext.so') 
+         os.system(linux_command)
+      try :
+           from plearn.pyext import *
+	   break
+      except: pass
+  
+  
+
+
+
+printAllPorts=False
+
+
+from pyplearn_read import *
+
+import pydot
+
+# global variables:
+modules_dict = {}
+
+    
+
+def countModules(mynetwork):
+    n=0
+    for module in mynetwork.modules:
+        if is_SplitModule(module) == False:
+	   n += 1
+    return n
+
+def Network2dict(mynetwork):
+    mydict={}
+    for module in mynetwork.modules:
+        mydict[module.name]=module
+    return mydict
+
+def Modules2dict(modules):
+    mydict={}
+    for module in modules:
+        mydict[module.name]=module
+    return mydict
+
+def getPortDict(ports):
+    mydict={}
+    if type(ports[0]) == tuple:
+        for i in range(0,len(ports)):
+	    mydict[ports[i][1].split('.')[0]] = ports[i][0]
+    else:
+        for i in range(0,len(ports),2):
+            mydict[ports[i+1].split('.')[0]] = ports[i]
+    return mydict
+
+def getPortLists(ports):
+    mylist=[]
+    if type(ports[0]) == tuple:
+        for i in range(0,len(ports)):
+	    mylist.append( [ ports[i][1].split('.')[0] , ports[i][0] ] )
+    else:
+       for i in range(0,len(ports),2):
+           mylist.append( [ ports[i+1].split('.')[0] , ports[i] ] )
+    return mylist
+    
+def formatModulesNames(name,modules_dict):
+    if modules_dict.has_key(name):
+       return name.upper()
+       #return str(type(modules_dict[name])).split("<class '")[1].split("Module")[0].upper()
+    print "ERROR: while executing formatModulesNames(name):\n\tCannot find "+name+" in modules_dict"
+    return name
+
+def formatPortNames(name,portname,modules_dict,printAllPorts):
+    if portname in ['input','target','cost','weight']:
+       return '*'+portname.lower()+'*'
+    elif portname in ['output']:
+       return formatModulesNames(name,modules_dict)
+    else:
+       if printAllPorts:
+          return '-*'+portname.lower()+'*'
+       else:
+          return formatModulesNames(name,modules_dict)
+       
+
+def checkName(ModuleName, ports_dict, modules_dict):
+    if ports_dict.has_key(ModuleName):
+       return formatPortNames(ModuleName,ports_dict[ModuleName],modules_dict,False)
+    return formatModulesNames(ModuleName,modules_dict)
+
+
+def is_SplitModule(module):
+    return 'SplitModule' in str(type(module))
+       
+def module_type(module):
+    return str(type(module)).upper()
+
+def getInputOutputSize(mymodule):
+    inputSize = ['?']
+    outputSize = ['?']
+    if 'connection' in mymodule.__dict__:
+       inputSize = [mymodule.connection.down_size]
+       outputSize = [mymodule.connection.up_size]
+    elif 'input_size' in mymodule.__dict__:
+       inputSize = [mymodule.input_size]
+       outputSize = [mymodule.output_size]
+    elif 'up_port_sizes' in mymodule.__dict__:
+       inputSize = [mymodule.up_port_sizes]
+    elif 'weights' in mymodule.__dict__:
+       inputSize = [len(mymodule.weights)]
+    return inputSize, outputSize 
+
+        
+def getInputOutput(connection):
+    nameport = connection.source.split('.')[1]
+    inputModuleName = connection.source.split('.')[0]
+    outputModuleName = connection.destination.split('.')[0]
+    if nameport in ['hidden','hidden.state','output','target']:
+       #nameport = connection.destination.split('.')[1]
+       #if nameport in ['visible','input','target']:
+          nameport = ' '
+    #tp, outSize = getInputOutputSize(modules_dict[inputModuleName])
+    #inSize, tp  = getInputOutputSize(modules_dict[outputModuleName])
+    #nameport = str(outSize) + nameport + str(inSize)
+    return inputModuleName, outputModuleName, nameport
+
+def get_graph(modules, connections, ports):
+    edges=[]
+    edges_toAdd={}
+    globals()['modules_dict'] = Modules2dict(modules)
+    ports_dict = getPortDict(ports)
+    for connection in connections:
+        inputModuleName , outputModuleName, inputModulePort = getInputOutput(connection)
+	inputModuleNameToplot = checkName(inputModuleName,ports_dict,modules_dict)
+	outputModuleNameToplot = checkName(outputModuleName,ports_dict,modules_dict)
+	
+	if is_SplitModule(modules_dict[inputModuleName]):
+	   if edges_toAdd.has_key(inputModuleName):
+	      edges_toAdd[inputModuleName][1].append(outputModuleNameToplot)
+	   else:
+	      edges_toAdd[inputModuleName]=[[],[]]
+	      edges_toAdd[inputModuleName][1]=[outputModuleNameToplot]
+	elif is_SplitModule(modules_dict[outputModuleName]):
+	   if edges_toAdd.has_key(outputModuleName):
+	      edges_toAdd[outputModuleName][0].append(inputModuleNameToplot)
+	   else:
+	      edges_toAdd[outputModuleName]=[[],[]]
+	      edges_toAdd[outputModuleName][0]=[inputModuleNameToplot]
+	else:
+	   edges.append( [inputModuleNameToplot, outputModuleNameToplot, inputModulePort] )
+	   
+    for split_module in edges_toAdd:
+        # split from/to a port (input, output)
+	if len(edges_toAdd[split_module][0]) == 0: 
+	   edges_toAdd[split_module][0]=[checkName(split_module,ports_dict,modules_dict)]
+	elif len(edges_toAdd[split_module][1]) == 0:
+	   edges_toAdd[split_module][1]=[checkName(split_module,ports_dict,modules_dict)]
+	for i in edges_toAdd[split_module][0]:
+	    for j in edges_toAdd[split_module][1]:
+                edges.append( [ i, j, ' ' ])
+
+    port_list = getPortLists(ports)
+
+    graphSIZE = len(modules_dict)
+    graph=pydot.Dot(size=str(graphSIZE)+','+str(graphSIZE))
+    for edge in edges:
+        myedge=pydot.Edge(edge[0],edge[1])
+	myedge.set_label(edge[2])
+        graph.add_edge(myedge)
+
+
+    for port in port_list:
+        inputPort = checkName(port[0],ports_dict,modules_dict)
+#        outputPort = formatPortNames(port[0],port[1],modules_dict)
+	outputPort = formatPortNames(port[0],port[1],modules_dict,printAllPorts)
+	if outputPort != inputPort:
+	   #graph.add_node(pydot.Node(inputPort+'->'+outputPort))
+	   #for edge in edges:
+	   #    for i in range(len(edge)):
+	   #        if inputPort in edge[i]:
+	   #           edge[i] += ' '+outputPort	          
+	   myedge=pydot.Edge(outputPort,inputPort)
+   	   myedge.set_arrowsize(1)
+#  	   myedge.set_label('label')
+           graph.add_edge(myedge)
+
+	
+    return graph
+    
+
+def save_graph( outputname, modules, connections, ports ):
+    graph = get_graph( modules, connections, ports )
+    graph.write_jpeg(outputname, prog='neato') #'dot', 'twopi' and 'neato'
+    #import Image
+    #im=Image.open(outputname,"r")
+    #im.show()
+    print "to see the network: kuickview "+outputname
+
+def show_graph( modules, connections, ports ):
+    outputname='temp.jpeg'
+    graph = get_graph( modules, connections, ports )
+    graph.write_jpeg(outputname, prog='dot')
+    im=Image.open(outputname,"r")
+    im.show()
+
+def networkview( myObject ):
+    if 'NetworkModule' in str(type(myObject)):
+       graph = get_graph( myObject.modules, myObject.connections, myObject.ports )
+    elif 'ModuleLearner' in str(type(myObject)):
+       graph = get_graph( myObject.module.modules, myObject.module.connections, myObject.module.ports )
+    else:
+        raise TypeError, "Please give a ModuleLearner or NetworkModule"
+    outputname = '/tmp/networkview.jpeg'
+    graph.write_jpeg(outputname, prog='dot')
+    os.system('kuickshow '+outputname+' &')
+
+if __name__ == '__main__':
+
+    inputname  = sys.argv[1]
+    outputname = inputname+'.network.jpeg'
+    extension = os.path.splitext(inputname)[1]
+    if extension == '.pyplearn' or extension == '.py':
+       types_to_discard = ['HyperLearner', 'PTester', 'MemoryVMatrix', 'AutoVMatrix']
+       if extension == '.py':
+          commands_to_discard = ['.run()', '.test()', '.train()', '.save(',
+	                         'choose_initial_lr', 'train_adapting_lr', 'train_with_schedule']
+          types_to_discard.append('ModuleLearner')
+       else:
+          commands_to_discard = []
+       tmp_file = '/tmp/tp.py'
+       object_dict = write_objects_from_pyplearn( inputname, types_to_discard ,commands_to_discard, tmp_file)
+#      for debug...
+#       os.system('nedit '+tmp_file+' &')
+       execfile(tmp_file)
+       nets=[]
+       names=[]
+       print object_dict
+       for variable_name in object_dict['NetworkModule']:
+           names += variable_name
+	   nets += eval(variable_name)
+    elif extension == '.psave':
+       myObject = loadObject(inputname)
+       if 'ModuleLearner' in str(type(myObject)):
+          net = myObject.module
+       else:
+          raise TypeError, "Could not recognize the type "+str(type(myObject))
+       nets = [net]
+    else:
+       raise TypeError, "could not recognize the extension ("+extension+") of "+inputname
+    
+    for i in range(len(nets)):
+        net = nets[i]
+        if len(nets)==1:
+	   outputname_i = outputname
+	else:
+	   print "making the graph of " + names[i]
+	   outputname_i = outputname+'_'+str(i)
+        graph = get_graph( net.modules, net.connections, net.ports )
+        graph.write_jpeg(outputname_i, prog='dot')
+        if os.path.isfile(outputname_i) == False:
+           print "ERROR: could not write "+outputname_i
+           sys.exit(0)
+        print "to see the network: kuickshow "+outputname_i
+        os.system('kuickshow '+outputname_i+' &')
+    
+#    show_graph_network( modules, connections, ports )


Property changes on: trunk/python_modules/plearn/learners/modulelearners/network_view.py
___________________________________________________________________
Name: svn:executable
   + *

Added: trunk/python_modules/plearn/learners/modulelearners/network_view.pyc
===================================================================
(Binary files differ)


Property changes on: trunk/python_modules/plearn/learners/modulelearners/network_view.pyc
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py	2007-06-13 03:07:30 UTC (rev 7577)
+++ trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py	2007-06-13 21:15:55 UTC (rev 7578)
@@ -0,0 +1,231 @@
+#!/usr/bin/env python
+
+import sys, os, os.path
+
+FID=0
+
+class pyplearnFile:
+
+#      Nbrack
+#      Nparen
+#      FID
+#      cline
+#      cline_previous
+#      ctype
+#      cname
+#      islocal
+
+      def __init__(self,fname):
+            self.FID = open(fname, 'r')
+	    self.cline = ''  # current line
+            self.Nbrack  = [] # number of opened brackets [
+            self.Nparen  = [] # number of opened parenthesis (
+	    self.islocal = 0 # being in a subfunction?
+	    self.ctypes = [] # type that i being defined (hierarchical)
+	    self.cnames = [] # ...and corresponding names
+	    self.isaffect = False # is the previous line affecting something?
+      def close(self):
+          self.FID.close()
+
+      def read(self):
+          self.cline_previous = self.cline
+          self.cline = self.FID.readline()
+	  if len(self.cline) == 0:
+	     return False
+	  
+	  comment = self.cline.find("#")
+	  if comment > -1:
+   	     # The '#' may be in a string...
+  	     prev = 0
+ 	     while is_odd(number_occurence(self.cline[prev:comment],"'")) or is_odd(number_occurence(self.cline[prev:comment],'"')):
+	        prev = comment
+		tp = self.cline[comment+1:].find("#")
+		if tp > -1:
+		   comment += tp+1 
+		else:
+		   comment = len(self.cline)
+	     self.cline = self.cline[0:comment]+'\n'
+	     
+	  return True
+
+      def actualize_previous(self):
+          if is_def(self.cline):
+	     self.islocal = indent_level(self.cline)
+	  elif self.islocal > 0:
+	     if indent_level(self.cline) <= self.islocal:
+	        self.islocal = 0
+	  if self.islocal == 0:
+  	     closed_paren = number_occurence(self.cline_previous,")") - number_occurence(self.cline_previous,"(") + number_occurence(self.cline_previous,"]") - number_occurence(self.cline_previous,"[")
+	     if closed_paren == 0 and self.isaffect:
+	        self.isaffect = False
+		closed_paren = 1
+	     self.cnames = self.cnames[0:len(self.cnames)-closed_paren]
+	     self.ctypes = self.ctypes[0:len(self.ctypes)-closed_paren]
+
+	     being_affect = self.name_affect()
+	     if len(being_affect[0]) > 0:
+	        self.cnames.append(being_affect[0])
+		self.ctypes.append(being_affect[1])
+	  
+          self.Nbrack += number_occurence(self.cline,"[") -  number_occurence(self.cline,"]")
+          self.Nparen += number_occurence(self.cline,"(") -  number_occurence(self.cline,")")
+
+      def actualize(self):
+          if is_def(self.cline):
+	     self.islocal = indent_level(self.cline)
+	  elif self.islocal > 0:
+	     if indent_level(self.cline) <= self.islocal:
+	        self.islocal = 0
+	  if self.islocal == 0:
+	     assend = 0
+	     level = len(self.Nbrack)-1
+	     N = 0
+	     while N == 0 and level >= 0:
+  	        if len(self.Nbrack) > 0:
+	           N = self.Nbrack[level]
+                else:
+	           N = 0
+	        if len(self.Nparen) > 0:
+	           N += self.Nparen[level]
+                if N > 0:
+		   break
+	        assend += 1
+		level  -= 1
+		N -= N
+	     
+             self.cnames = self.cnames[0:len(self.cnames)-assend]
+             self.ctypes = self.ctypes[0:len(self.ctypes)-assend]
+             self.Nbrack = self.Nbrack[0:len(self.Nbrack)-assend]
+             self.Nparen = self.Nparen[0:len(self.Nparen)-assend]
+
+	     being_affect = self.name_affect()
+	     if len(being_affect[0]) > 0:
+	        self.cnames.append(being_affect[0])
+		self.ctypes.append(being_affect[1])
+		self.Nbrack.append(number_occurence(self.cline,"[") -  number_occurence(self.cline,"]"))
+		self.Nparen.append(number_occurence(self.cline,"(") -  number_occurence(self.cline,")"))
+             elif len(self.Nbrack) > 0:
+	        self.Nbrack[len(self.Nbrack)-1] += number_occurence(self.cline,"[") -  number_occurence(self.cline,"]")
+	        self.Nparen[len(self.Nparen)-1] += number_occurence(self.cline,"(") -  number_occurence(self.cline,")")
+
+
+       # is there an affectation in the line?
+      def name_affect(self):
+          tp = self.cline.split('=')
+          if len(tp) > 1:
+             if '' not in tp: # discard the case of '=='
+	        tp[1] = '='.join(tp[1:])
+                first_term = tp[0].strip()
+                second_term = tp[1].split('(')[0].strip()
+		if len(second_term) == 0:
+		   while len(second_term) < 1:
+		         line = self.cline
+		         self.read()
+			 self.cline = line + self.cline
+		         temp, second_term = self.name_affect()
+		   return first_term, second_term
+		self.isaffect = True
+                if len(second_term) > 3 and second_term[0:3] == 'pl.':
+                   return first_term, second_term[3:]
+		elif '[' in second_term:
+		   return first_term, 'list'
+		else:
+		   self.isaffect = False
+		   return '', type(second_term)
+          self.isaffect = False
+	  return '',''
+
+# is it a definition line?
+def is_def(line):
+    tp = line.split('def')
+    if len(tp) > 1:
+       if len(tp[1]) > 0:
+          if len(tp[0]) == 0 and tp[1][0] == ' ':
+             return True
+    return False
+
+
+
+# the level of indentation of the line    
+def indent_level(line):
+    return line.replace("\t","        ").find(line.lstrip()) + 1
+
+def number_occurence(line,string):
+    n = 0
+    while line.find(string) > -1  and len(string) > 0:
+          n += 1
+          line = line[line.find(string)+1:]
+    return n
+    
+def is_odd(number): # nombre impair?
+    return int(number/2.0) != number/2.0
+    
+    
+    
+    
+	  
+# fname           : name of the *.pyplearn or *.py file to read
+# types_to_discard : list of module names we don't want to construct
+#                     (ex: ['ModuleLearner', 'NetworkModule', 'MemoryVMatrix', 'AutoVMatrix'])
+#                   but we will get their components
+def write_objects_from_pyplearn( fname, types_to_discard, commands_to_discard, fname_out):
+    if os.path.isfile(fname) == False:
+       print "ERROR: cannot find file "+fname
+       sys.exit(0)
+       
+    module_dict={}
+    
+    FILE = pyplearnFile(fname)
+    
+    globals()['FID'] = open(fname_out,'w')
+    FID=globals()['FID']
+    
+    cnames=[]
+    
+    while FILE.read():
+       FILE.actualize()
+	  
+       if len(FILE.ctypes) == 0 or FILE.ctypes[0] not in types_to_discard:
+          
+	  print FILE.cline
+	  do_not_consider = False
+	  for command in commands_to_discard:
+	      print command
+	      if command in FILE.cline:
+	         do_not_consider = True
+              print do_not_consider
+	  if do_not_consider:
+	     continue
+	  
+	  if cnames != FILE.cnames:
+	     if len(cnames) < len(FILE.cnames):
+	        cname = '.'.join(FILE.cnames)
+		ctype = FILE.ctypes[len(FILE.ctypes)-1]
+                if module_dict.has_key(ctype):
+		   if cname not in module_dict[ctype]:
+		      module_dict[ctype].append( cname )
+		else:
+		   module_dict[ctype] = [ cname ]
+	        FID.write("#"+"/"*10*len(FILE.cnames)+"\n")
+                FID.write("# "+' / '.join(FILE.cnames)+"\n# "+' / '.join(FILE.ctypes)+"\n")
+	     else: 
+                FID.write("# "+' / '.join(FILE.cnames)+"\n")
+		if len(cnames) != len(FILE.cnames):
+		   FID.write("# "+' / '.join(FILE.ctypes)+"\n")
+	           FID.write("#"+"\\"*10*len(FILE.cnames)+"\n")
+	     cnames = FILE.cnames
+	  FID.write(FILE.cline)
+   
+    FILE.close()
+    FID.close()
+    
+    return module_dict
+    
+if __name__ == '__main__':
+
+    object_dict = write_objects_from_pyplearn( sys.argv[1], ['HyperLearner', 'PTester', 'MemoryVMatrix'] , [] , '/tmp/pyplearn_read.py')
+    print str(object_dict)
+    execfile('/tmp/pyplearn_read.py')
+    for i in object_dict['NetworkModule']:
+        m = eval(i)
+        print m


Property changes on: trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py
___________________________________________________________________
Name: svn:executable
   + *

Added: trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.pyc
===================================================================
(Binary files differ)


Property changes on: trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.pyc
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream



From louradou at mail.berlios.de  Thu Jun 14 01:25:25 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 14 Jun 2007 01:25:25 +0200
Subject: [Plearn-commits] r7579 -
	trunk/python_modules/plearn/learners/modulelearners
Message-ID: <200706132325.l5DNPPFL032394@sheep.berlios.de>

Author: louradou
Date: 2007-06-14 01:25:25 +0200 (Thu, 14 Jun 2007)
New Revision: 7579

Modified:
   trunk/python_modules/plearn/learners/modulelearners/network_view.py
   trunk/python_modules/plearn/learners/modulelearners/network_view.pyc
   trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py
   trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.pyc
Log:


Modified: trunk/python_modules/plearn/learners/modulelearners/network_view.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-06-13 21:15:55 UTC (rev 7578)
+++ trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-06-13 23:25:25 UTC (rev 7579)
@@ -209,19 +209,19 @@
     return graph
     
 
-def save_graph( outputname, modules, connections, ports ):
+def save_graph( output_name, modules, connections, ports ):
     graph = get_graph( modules, connections, ports )
-    graph.write_jpeg(outputname, prog='neato') #'dot', 'twopi' and 'neato'
+    graph.write_jpeg(output_name, prog='neato') #'dot', 'twopi' and 'neato'
     #import Image
-    #im=Image.open(outputname,"r")
+    #im=Image.open(output_name,"r")
     #im.show()
-    print "to see the network: kuickview "+outputname
+    print "to see the network: kuickview "+output_name
 
 def show_graph( modules, connections, ports ):
-    outputname='temp.jpeg'
+    output_name='temp.jpeg'
     graph = get_graph( modules, connections, ports )
-    graph.write_jpeg(outputname, prog='dot')
-    im=Image.open(outputname,"r")
+    graph.write_jpeg(output_name, prog='dot')
+    im=Image.open(output_name,"r")
     im.show()
 
 def networkview( myObject ):
@@ -231,57 +231,79 @@
        graph = get_graph( myObject.module.modules, myObject.module.connections, myObject.module.ports )
     else:
         raise TypeError, "Please give a ModuleLearner or NetworkModule"
-    outputname = '/tmp/networkview.jpeg'
-    graph.write_jpeg(outputname, prog='dot')
-    os.system('kuickshow '+outputname+' &')
+    output_name = '/tmp/networkview.jpeg'
+    graph.write_jpeg(output_name, prog='dot')
+    os.system('kuickshow '+output_name+' &')
 
 if __name__ == '__main__':
 
     inputname  = sys.argv[1]
-    outputname = inputname+'.network.jpeg'
-    extension = os.path.splitext(inputname)[1]
-    if extension == '.pyplearn' or extension == '.py':
+    output_extension = '.network.jpeg'
+    output_name = os.path.splitext(inputname)[0]
+    input_extension = os.path.splitext(inputname)[1]
+    
+    
+    if input_extension == '.pyplearn':
+       commands_to_discard = []
        types_to_discard = ['HyperLearner', 'PTester', 'MemoryVMatrix', 'AutoVMatrix']
-       if extension == '.py':
-          commands_to_discard = ['.run()', '.test()', '.train()', '.save(',
-	                         'choose_initial_lr', 'train_adapting_lr', 'train_with_schedule']
-          types_to_discard.append('ModuleLearner')
-       else:
-          commands_to_discard = []
        tmp_file = '/tmp/tp.py'
        object_dict = write_objects_from_pyplearn( inputname, types_to_discard ,commands_to_discard, tmp_file)
 #      for debug...
 #       os.system('nedit '+tmp_file+' &')
        execfile(tmp_file)
+       names=[]
        nets=[]
+#      for debug...
+#       print object_dict
+       for variable_name in object_dict['NetworkModule']:
+           names.append(variable_name)
+	   nets.append(eval(variable_name))
+	   
+    elif  input_extension == '.py':  
+       commands_to_discard = ['.run()', '.test()', '.train()', '.save(',
+                              'choose_initial_lr', 'train_adapting_lr', 'train_with_schedule']
+       types_to_discard = []
+       tmp_file = '/tmp/tp.py'
+       write_objects_from_pyplearn( inputname, types_to_discard ,commands_to_discard, tmp_file)
+#      for debug...
+#       os.system('nedit '+tmp_file+' &')
+       execfile(tmp_file)
        names=[]
-       print object_dict
-       for variable_name in object_dict['NetworkModule']:
-           names += variable_name
-	   nets += eval(variable_name)
-    elif extension == '.psave':
+       nets=[]
+       global_variable=''
+       for global_variable in globals():
+           if 'ModuleLearner' in str(type(globals()[global_variable])):
+              names.append(global_variable+'.module')
+              nets.append(eval(global_variable+'.module'))
+       
+    elif input_extension == '.psave':
        myObject = loadObject(inputname)
        if 'ModuleLearner' in str(type(myObject)):
           net = myObject.module
        else:
           raise TypeError, "Could not recognize the type "+str(type(myObject))
        nets = [net]
+       
+       
     else:
-       raise TypeError, "could not recognize the extension ("+extension+") of "+inputname
+       raise TypeError, "could not recognize the input_extension ("+input_extension+") of "+inputname
     
+    
+    
+    
     for i in range(len(nets)):
         net = nets[i]
         if len(nets)==1:
-	   outputname_i = outputname
+	   output_name_i = output_name + output_extension
 	else:
 	   print "making the graph of " + names[i]
-	   outputname_i = outputname+'_'+str(i)
+	   output_name_i = output_name+'.'+names[i]+output_extension
         graph = get_graph( net.modules, net.connections, net.ports )
-        graph.write_jpeg(outputname_i, prog='dot')
-        if os.path.isfile(outputname_i) == False:
-           print "ERROR: could not write "+outputname_i
+        graph.write_jpeg(output_name_i, prog='dot')
+        if os.path.isfile(output_name_i) == False:
+           print "ERROR: could not write "+output_name_i
            sys.exit(0)
-        print "to see the network: kuickshow "+outputname_i
-        os.system('kuickshow '+outputname_i+' &')
+        print "to see the network: kuickshow "+output_name_i
+        os.system('kuickshow '+output_name_i+' &')
     
 #    show_graph_network( modules, connections, ports )

Modified: trunk/python_modules/plearn/learners/modulelearners/network_view.pyc
===================================================================
(Binary files differ)

Modified: trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py	2007-06-13 21:15:55 UTC (rev 7578)
+++ trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.py	2007-06-13 23:25:25 UTC (rev 7579)
@@ -187,13 +187,10 @@
 	  
        if len(FILE.ctypes) == 0 or FILE.ctypes[0] not in types_to_discard:
           
-	  print FILE.cline
 	  do_not_consider = False
 	  for command in commands_to_discard:
-	      print command
 	      if command in FILE.cline:
 	         do_not_consider = True
-              print do_not_consider
 	  if do_not_consider:
 	     continue
 	  

Modified: trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.pyc
===================================================================
(Binary files differ)



From lamblin at mail.berlios.de  Thu Jun 14 04:54:11 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 14 Jun 2007 04:54:11 +0200
Subject: [Plearn-commits] r7580 - trunk/plearn/math
Message-ID: <200706140254.l5E2sBgH013900@sheep.berlios.de>

Author: lamblin
Date: 2007-06-14 04:54:09 +0200 (Thu, 14 Jun 2007)
New Revision: 7580

Modified:
   trunk/plearn/math/convolutions.cc
Log:
Notation changes


Modified: trunk/plearn/math/convolutions.cc
===================================================================
--- trunk/plearn/math/convolutions.cc	2007-06-13 23:25:25 UTC (rev 7579)
+++ trunk/plearn/math/convolutions.cc	2007-06-14 02:54:09 UTC (rev 7580)
@@ -342,48 +342,50 @@
                 const Mat& dest_image,
                 int step1, int step2, bool accumulate)
 {
-    int n1k=kernel.length();
-    int n2k=kernel.width();
-    int n1d=dest_image.length();
-    int n2d=dest_image.width();
+    int kl = kernel.length();
+    int kw = kernel.width();
+    int dl = dest_image.length();
+    int dw = dest_image.width();
+
 #ifdef BOUNDCHECK
-    int n1s=source_image.length();
-    int n2s=source_image.width();
+    int sl = source_image.length();
+    int sw = source_image.width();
+
     if (step1<1)
         PLERROR("convolve2D: step1 (%d) should be a positive integer\n",step1);
-    if (n1s!=step1*(n1d-1)+n1k)
+    if (sl != step1*(dl-1)+kl)
         PLERROR("convolve2D: source_image.length() (%d) should equal %d:\n"
                 "step1 (%d) * (dest_image.length() (%d) - 1) + kernel.length()"
                 " (%d)\n",
-                n1s,step1*(n1d-1)+n1k,step1,n1d,n1k);
+                sl, step1*(dl-1)+kl, step1, dl, kl);
 
     if (step2<1)
         PLERROR("convolve2D: step2 (%d) should be a positive integer\n",step2);
-    if (n2s!=step2*(n2d-1)+n2k)
+    if (sw != step2*(dw-1)+kw)
         PLERROR("convolve2D: source_image.width() (%d) should equal %d:\n"
                 "step2 (%d) * (dest_image.width() (%d) - 1) + kernel.width()"
                 " (%d)\n",
-                n2s,step2*(n2d-1)+n2k,step2,n2d,n2k);
+                sw, step2*(dw-1)+kw, step2, dw, kw);
 #endif
     if (!accumulate)
         dest_image.clear();
     int sm = source_image.mod();
     int dm = dest_image.mod();
     int km = kernel.mod();
-    real* s = source_image.data();
-    real* d = dest_image.data();
-    for (int i=0;i<n1d;i++,s+=sm*step1,d+=dm)
+    real* source_i = source_image.data(); // source_image[i*step1]
+    real* dest_i = dest_image.data(); // dest_image[i]
+    for (int i=0; i<dl; i++, source_i+=sm*step1, dest_i+=dm)
     {
-        real* s1 = s; // copy to iterate over columns
-        for (int j=0;j<n2d;j++,s1+=step2)
+        real* source_i_j = source_i; // source_image[i*step1][j*step2]
+        for (int j=0; j<dw; j++, source_i_j+=step2)
         {
-            real somme=0;
-            real* k = kernel.data();
-            real* ss = s1; // copy to iterate over sub-rows
-            for (int l=0;l<n1k;l++,ss+=sm,k+=km)
-                for (int m=0;m<n2k;m++)
-                    somme += ss[m]*k[m];
-            d[j]+=somme;
+            real sum = 0;
+            real* kernel_k = kernel.data(); // kernel[k]
+            real* source_ik_j = source_i_j; // source_image[i*step1+k][j*step2]
+            for (int k=0; k<kl; k++, source_ik_j+=sm, kernel_k+=km)
+                for (int l=0; l<kw; l++)
+                    sum += source_ik_j[l] * kernel_k[l];
+            dest_i[j] += sum;
         }
     }
 }



From larocheh at mail.berlios.de  Thu Jun 14 21:50:31 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 14 Jun 2007 21:50:31 +0200
Subject: [Plearn-commits] r7581 - trunk/plearn_learners/online
Message-ID: <200706141950.l5EJoVhd023565@sheep.berlios.de>

Author: larocheh
Date: 2007-06-14 21:50:30 +0200 (Thu, 14 Jun 2007)
New Revision: 7581

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Changed some comments...


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-06-14 02:54:09 UTC (rev 7580)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-06-14 19:50:30 UTC (rev 7581)
@@ -116,11 +116,8 @@
     declareOption(ol, "training_schedule", 
                   &StackedAutoassociatorsNet::training_schedule,
                   OptionBase::buildoption,
-                  "Number of examples to use during each phase of learning:\n"
-                  "first the greedy phases, and then the gradient descent.\n"
-                  "Unlike for DeepBeliefNet, these numbers should not be\n"
-                  "cumulative. They correspond to the number of seen training\n"
-                  "examples for each phase.\n"
+                  "Number of examples to use during each phase of greedy pre-training.\n"
+                  "The number of fine-tunig steps is defined by nstages.\n"
         );
 
     declareOption(ol, "layers", &StackedAutoassociatorsNet::layers,
@@ -128,7 +125,7 @@
                   "The layers of units in the network. The first element\n"
                   "of this vector should be the input layer and the\n"
                   "subsequent elements should be the hidden layers. The\n"
-                  "should not be included in this layer.\n");
+                  "output should not be included in this layer.\n");
 
     declareOption(ol, "connections", &StackedAutoassociatorsNet::connections,
                   OptionBase::buildoption,

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-06-14 02:54:09 UTC (rev 7580)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-06-14 19:50:30 UTC (rev 7581)
@@ -92,11 +92,8 @@
     //! where h is the value of the neurons.
     real l1_neuron_decay_center;
 
-    //! Number of examples to use during each phase of learning:
-    //! first the greedy phases, and then the gradient descent.
-    //! Unlike for DeepBeliefNet, these numbers should not be
-    //! cumulative. They correspond to the number of seen training
-    //! examples for each phase.
+    //! Number of examples to use during each phase of greedy pre-training.
+    //! The number of fine-tunig steps is defined by nstages.
     TVec<int> training_schedule;
 
     //! The layers of units in the network



From nouiz at mail.berlios.de  Fri Jun 15 15:43:33 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Jun 2007 15:43:33 +0200
Subject: [Plearn-commits] r7582 - trunk/plearn_learners/regressors
Message-ID: <200706151343.l5FDhXPc026293@sheep.berlios.de>

Author: nouiz
Date: 2007-06-15 15:43:33 +0200 (Fri, 15 Jun 2007)
New Revision: 7582

Modified:
   trunk/plearn_learners/regressors/LinearRegressor.cc
Log:
Added a test that check that the matrice have a trainset > 0


Modified: trunk/plearn_learners/regressors/LinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/LinearRegressor.cc	2007-06-14 19:50:30 UTC (rev 7581)
+++ trunk/plearn_learners/regressors/LinearRegressor.cc	2007-06-15 13:43:33 UTC (rev 7582)
@@ -224,6 +224,8 @@
 
 void LinearRegressor::train()
 {
+    if(targetsize()<=0)
+        PLERROR("In LinearRegressor::train(), we are doing a regression with a trainsize for the train_set of %d(should he higher then 0)",targetsize());
     // Preparatory buffer allocation
     bool recompute_XXXY = (XtX.length()==0);
     extendedinput.resize(effective_inputsize());



From nouiz at mail.berlios.de  Fri Jun 15 15:45:37 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Jun 2007 15:45:37 +0200
Subject: [Plearn-commits] r7583 - trunk/commands
Message-ID: <200706151345.l5FDjbcg026478@sheep.berlios.de>

Author: nouiz
Date: 2007-06-15 15:45:37 +0200 (Fri, 15 Jun 2007)
New Revision: 7583

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Include two existing vmat that I need


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-06-15 13:43:33 UTC (rev 7582)
+++ trunk/commands/plearn_noblas_inc.h	2007-06-15 13:45:37 UTC (rev 7583)
@@ -283,6 +283,8 @@
 #include <plearn/vmat/CompactFileVMatrix.h>
 #include <plearn/vmat/CompressedVMatrix.h>
 #include <plearn/vmat/CumVMatrix.h>
+#include <plearn/vmat/ConcatColumnsVMatrix.h>
+#include <plearn/vmat/ConstantVMatrix.h>
 #include <plearn/vmat/DatedJoinVMatrix.h>
 // #include <plearn/vmat/DictionaryVMatrix.h>
 #include <plearn/vmat/DisregardRowsVMatrix.h>



From nouiz at mail.berlios.de  Fri Jun 15 15:49:54 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Jun 2007 15:49:54 +0200
Subject: [Plearn-commits] r7584 - trunk/scripts
Message-ID: <200706151349.l5FDnsS8026641@sheep.berlios.de>

Author: nouiz
Date: 2007-06-15 15:49:54 +0200 (Fri, 15 Jun 2007)
New Revision: 7584

Modified:
   trunk/scripts/cdispatch
Log:
-Added a test
-Corrected the long help message


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-06-15 13:45:37 UTC (rev 7583)
+++ trunk/scripts/cdispatch	2007-06-15 13:49:54 UTC (rev 7584)
@@ -37,7 +37,7 @@
 $ScriptName="lauchdbi.py";
 $ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--file=FILEPATH] [--req="CONDOR_REQUIREMENT"]<command-template>'."\n";
 $LongHelp=<<EOUSAGE;
-Usage: cdispatch [--help|-h] [--log|--nolog] [--test] [--file=FILEPATH] [--req="CONDOR_REQUIREMENT"]<command-template>
+Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--file=FILEPATH] [--req="CONDOR_REQUIREMENT"]<command-template>
 Dispatches jobs on Condor with dbi.py.  
 
 where <command-template> is interpreted as follows: the first argument
@@ -131,6 +131,10 @@
     $FILE = "";
 }
 
+if (scalar(@ARGV) == 0) {
+    die $ShortHelp;
+}
+
 if(scalar(@ARGV) != 0) {
     if (substr($ARGV[0],0,6) eq "--req=") {
 	$REQ = substr($ARGV[0],6);
@@ -151,9 +155,9 @@
 #    "\tArglist = (", join(",",@{$repl[$i]}),")\n";
 #}
 open(SCRIPT,">$ScriptName");
-    print SCRIPT "#! /usr/bin/env python\n".
-	"from plearn.parallel.dbi import DBI\n".
-	"jobs = DBI([\n";
+print SCRIPT "#! /usr/bin/env python\n".
+    "from plearn.parallel.dbi import DBI\n".
+    "jobs = DBI([\n";
 
 $nbcommand=0;
 



From nouiz at mail.berlios.de  Fri Jun 15 15:51:36 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Jun 2007 15:51:36 +0200
Subject: [Plearn-commits] r7585 - trunk/python_modules/plearn/parallel
Message-ID: <200706151351.l5FDpamK026878@sheep.berlios.de>

Author: nouiz
Date: 2007-06-15 15:51:35 +0200 (Fri, 15 Jun 2007)
New Revision: 7585

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Factorised in the parent class some fonctionnality


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-06-15 13:49:54 UTC (rev 7584)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-06-15 13:51:35 UTC (rev 7585)
@@ -56,8 +56,8 @@
         self.tmp_dir = 'TMP_DBI'
 
         #
-        self.file_redirect_stdout = 0
-        self.file_redirect_stderr = 0
+        self.file_redirect_stdout = False
+        self.file_redirect_stderr = False
 
         # Initialize the namespace
         self.requirements = ''
@@ -67,6 +67,10 @@
         for key in args.keys():
             self.__dict__[key] = args[key]
 
+        # check if log directory exists, if not create it
+        if self.dolog and not os.path.exists(self.log_dir):
+            os.mkdir(self.log_dir)
+
         # If some arguments aren't lists, put them in a list
         if not isinstance(commands, list):
             commands = [commands]
@@ -194,9 +198,6 @@
     def __init__(self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
 
-        # check if log directory exists, if not create it
-        if self.dolog and not os.path.exists(self.log_dir):
-            os.mkdir(self.log_dir)
 
         # create the information about the tasks
         for command in commands:
@@ -272,10 +273,6 @@
         self.parent_dir = 'parent'
         os.symlink( '..', self.parent_dir )
 
-        # check if log directory exists, if not create it
-        if self.dolog and not os.path.exists(self.log_dir):
-            os.mkdir(self.log_dir)
-
         # create the information about the tasks
         args['temp_dir'] = self.temp_dir
         for command in commands:
@@ -363,10 +360,6 @@
     def __init__( self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
 
-        # check if log directory exists, if not create it
-        if self.dolog and not os.path.exists(self.log_dir):
-            os.mkdir(self.log_dir)
-
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
             
@@ -578,10 +571,6 @@
     def __init__( self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
 
-        # check if log directory exists, if not create it
-        if self.dolog and not os.path.exists(self.log_dir):
-            os.mkdir(self.log_dir)
-
         for command in commands:
             pos = string.find(command,' ')
             if pos>=0:
@@ -706,10 +695,6 @@
         print "Use at your own risk!"
         DBIBase.__init__(self, commands, **args)
 
-        # check if log directory exists, if not create it
-        if self.dolog and not os.path.exists(self.log_dir):
-            os.mkdir(self.log_dir)
-
         # create the information about the tasks
         for command in commands:
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,



From nouiz at mail.berlios.de  Fri Jun 15 15:53:08 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Jun 2007 15:53:08 +0200
Subject: [Plearn-commits] r7586 - trunk/plearn/math
Message-ID: <200706151353.l5FDr8ha026976@sheep.berlios.de>

Author: nouiz
Date: 2007-06-15 15:53:08 +0200 (Fri, 15 Jun 2007)
New Revision: 7586

Modified:
   trunk/plearn/math/TMat_maths_specialisation.h
Log:
-Added a BoundCheck


Modified: trunk/plearn/math/TMat_maths_specialisation.h
===================================================================
--- trunk/plearn/math/TMat_maths_specialisation.h	2007-06-15 13:51:35 UTC (rev 7585)
+++ trunk/plearn/math/TMat_maths_specialisation.h	2007-06-15 13:53:08 UTC (rev 7586)
@@ -305,6 +305,8 @@
         PLERROR("In externalProductScaleAcc, incompatible dimensions:\n"
                 "Mat(%d,%d) <- Vec(%d).Vec(%d)'",
                 A.length(), A.width(), x.length(), y.length());
+    if(A.mod()<=0 || A.width()<=0)
+        PLERROR("In externalProductScaleAcc, destination matrice hava a width(%d) or a mod(%d) <= 0",A.width(),A.mod());
 #endif
     int one = 1;
     int lda = A.mod();



From ducharme at mail.berlios.de  Fri Jun 15 20:22:23 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 15 Jun 2007 20:22:23 +0200
Subject: [Plearn-commits] r7587 - in tags: . OPAL-3.1.4/scripts
Message-ID: <200706151822.l5FIMNVs026749@sheep.berlios.de>

Author: ducharme
Date: 2007-06-15 20:22:23 +0200 (Fri, 15 Jun 2007)
New Revision: 7587

Added:
   tags/OPAL-3.1.4/
Modified:
   tags/OPAL-3.1.4/scripts/perlgrep
Log:
Tag pour release OPAL 3.1.4

Copied: tags/OPAL-3.1.4 (from rev 7581, trunk)

Modified: tags/OPAL-3.1.4/scripts/perlgrep
===================================================================
--- trunk/scripts/perlgrep	2007-06-14 19:50:30 UTC (rev 7581)
+++ tags/OPAL-3.1.4/scripts/perlgrep	2007-06-15 18:22:23 UTC (rev 7587)
@@ -102,7 +102,7 @@
             { 
                 my @flist = lsdir($fname);
                 
-                @flist = grep { -d $_ or /Makefile|makefile|
+                @flist = grep { -d $_ or /Makefile|makefile|pytest\.config|
                                     \.c$|\.cc$|\.cpp$|\.CC$|\.h$|\.hpp$
                                    |\.plearn$|\.pyplearn$|\.vmat$|\.py$|\.pymat$
                                    |\.txt$|^readme|^Readme|^README/x } @flist;



From tihocan at mail.berlios.de  Fri Jun 15 21:54:18 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Jun 2007 21:54:18 +0200
Subject: [Plearn-commits] r7588 - trunk/plearn_learners/online
Message-ID: <200706151954.l5FJsIKb000460@sheep.berlios.de>

Author: tihocan
Date: 2007-06-15 21:54:18 +0200 (Fri, 15 Jun 2007)
New Revision: 7588

Modified:
   trunk/plearn_learners/online/ModuleTester.cc
Log:
Got rid of compiler warning

Modified: trunk/plearn_learners/online/ModuleTester.cc
===================================================================
--- trunk/plearn_learners/online/ModuleTester.cc	2007-06-15 18:22:23 UTC (rev 7587)
+++ trunk/plearn_learners/online/ModuleTester.cc	2007-06-15 19:54:18 UTC (rev 7588)
@@ -427,14 +427,14 @@
                             int out_idx = module->getPortIndex(out_grad[r]);
                             Mat* out_val = fprop_data[out_idx];
                             Mat* out_prev = fprop_check[out_idx];
-                            Mat* out_grad = bprop_data[out_idx];
-                            PLASSERT( out_val && out_prev && out_grad );
+                            Mat* out_grad_ = bprop_data[out_idx];
+                            PLASSERT( out_val && out_prev && out_grad_ );
                             for (int oi = 0; oi < out_val->length(); oi++)
                                 for (int oj = 0; oj < out_val->width(); oj++) {
                                     real diff = (*out_val)(oi, oj) - 
                                         (*out_prev)(oi, oj);
                                     (*grad)(p, q) +=
-                                        diff * (*out_grad)(oi, oj) / step;
+                                        diff * (*out_grad_)(oi, oj) / step;
                                 }
                         }
                     }



From chrish at mail.berlios.de  Fri Jun 15 21:55:30 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Fri, 15 Jun 2007 21:55:30 +0200
Subject: [Plearn-commits] r7589 - trunk/python_modules/plearn/parallel
Message-ID: <200706151955.l5FJtUb0000586@sheep.berlios.de>

Author: chrish
Date: 2007-06-15 21:55:30 +0200 (Fri, 15 Jun 2007)
New Revision: 7589

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
Bump LOADAVG_DELAY to 120 seconds in attempt to solve too many tasks started
on machine problem.


Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-06-15 19:54:18 UTC (rev 7588)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-06-15 19:55:30 UTC (rev 7589)
@@ -64,7 +64,7 @@
 
 # Do not perform a new query for the loadavg until recently launched
 # processes are likely to have started. 
-LOADAVG_DELAY = timedelta(seconds=60)
+LOADAVG_DELAY = timedelta(seconds=120)
 BUFSIZE       = 4096
 SLEEP_TIME    = 15
 LOGDIR        = None  # May be set by set_logdir()



From tihocan at mail.berlios.de  Fri Jun 15 22:51:15 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Jun 2007 22:51:15 +0200
Subject: [Plearn-commits] r7590 - trunk/commands
Message-ID: <200706152051.l5FKpFfp004176@sheep.berlios.de>

Author: tihocan
Date: 2007-06-15 22:51:15 +0200 (Fri, 15 Jun 2007)
New Revision: 7590

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Disabled include of HTMLHelpCommand. This is because of temporary problems with boost_regex on Mammouth cluster. Hopefully we do not need it anymore since it is supposed to be deprecated

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-06-15 19:55:30 UTC (rev 7589)
+++ trunk/commands/plearn_noblas_inc.h	2007-06-15 20:51:15 UTC (rev 7590)
@@ -81,7 +81,7 @@
 #include <commands/PLearnCommands/TestDependencyCommand.h>
 
 // * extra stuff from Boost to generate help *
-#include <commands/PLearnCommands/HTMLHelpCommand.h>//<! DEPRECATED (will disappear soon)
+//#include <commands/PLearnCommands/HTMLHelpCommand.h>//<! DEPRECATED (will disappear soon)
 
 //#include <commands/PLearnCommands/TxtmatCommand.h>
 



From tihocan at mail.berlios.de  Fri Jun 15 22:53:48 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Jun 2007 22:53:48 +0200
Subject: [Plearn-commits] r7591 - in trunk: . plearn/misc
Message-ID: <200706152053.l5FKrmBT004416@sheep.berlios.de>

Author: tihocan
Date: 2007-06-15 22:53:47 +0200 (Fri, 15 Jun 2007)
New Revision: 7591

Modified:
   trunk/plearn/misc/HTMLUtils.cc
   trunk/pymake.config.model
Log:
Upgraded to Python 2.5 on mammouth (some HTML functions are temporarily broken until I fix a link issue with boost_regex)

Modified: trunk/plearn/misc/HTMLUtils.cc
===================================================================
--- trunk/plearn/misc/HTMLUtils.cc	2007-06-15 20:51:15 UTC (rev 7590)
+++ trunk/plearn/misc/HTMLUtils.cc	2007-06-15 20:53:47 UTC (rev 7591)
@@ -54,15 +54,18 @@
 
 string HTMLUtils::quote(string s)
 {
+#ifndef __INTEL_COMPILER
     search_replace(s, "&", "&amp;");
     search_replace(s, "<", "&lt;");
     search_replace(s, ">", "&gt;");
     search_replace(s, "\"", "&quot;");
+#endif
     return s;
 }
 
 string HTMLUtils::highlight_known_classes(string typestr)
 {
+#ifndef __INTEL_COMPILER
     vector<string> tokens = split(typestr, " \t\n\r<>,.';:\"");
     set<string> replaced; // Carry out replacements for a given token only once
     const TypeMap& type_map = TypeFactory::instance().getTypeMap();
@@ -80,6 +83,7 @@
             typestr = regex_replace(typestr, e, repl_str, boost::match_default | boost::format_default);
         }
     }
+#endif
     return typestr;
 }
 
@@ -92,6 +96,7 @@
     bool ul_active = false;
     bool start_paragraph = false;
     string finallines;
+#ifndef __INTEL_COMPILER
     for ( ; curpos != string::npos ;
           curpos = curnl+(curnl!=string::npos), curnl = text.find('\n', curpos) ) {
         string curline = text.substr(curpos, curnl-curpos);
@@ -152,10 +157,13 @@
         finallines += "</ul>\n";
   
     // Finally join the lines
+#endif
     return make_http_hyperlinks(finallines);
 }
+
 string HTMLUtils::make_http_hyperlinks(string text)
 {
+#ifndef __INTEL_COMPILER
     // Find elements of the form XYZ://x.y.z/a/b/c and make them into
     // hyperlink. An issue is to determine when
     static const char* recognized_protocols[] = 
@@ -176,6 +184,7 @@
                           + tostring(OptionBase::getCurrentOptionLevel())
                           +"\">$1</a>$2");
     text = regex_replace(text, e, repl_str, boost::match_default | boost::format_default);
+#endif
     return text;
 }
 

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-06-15 20:51:15 UTC (rev 7590)
+++ trunk/pymake.config.model	2007-06-15 20:53:47 UTC (rev 7591)
@@ -267,14 +267,12 @@
             ### NB: The '-lutil' is necessary on i386 LISA computers.
             numpy_site_packages = '/u/lisa/local/' + target_platform + '/lib/python2.4/site-packages/numarray -lutil'
     elif domain_name.endswith('.ms'):
-        numpy_includedirs = [ '/opt/python2.4/include' ]
-        numpy_site_packages = join(homedir, '../delallea/local/lib/python2.4/site-packages/numarray -lutil')
-        optionargs += [ 'python24' ]
-        python_version = '2.4'
-        python_lib_root = '/opt/python2.4/lib'
-        #linkeroptions_tail += '-lgcc_eh -lunwind -lcprts'
-        linkeroptions_tail += '-lunwind -lcprts'
-
+        numpy_includedirs = []
+        numpy_site_packages = join(homedir, '../delallea/local/lib/python2.5/site-packages/numarray -lutil')
+        optionargs += [ 'python25' ]
+        python_version = '2.5'
+        python_lib_root = '/home/delallea/local/lib'
+        linkeroptions_tail += '-lunwind -lcprts' # -lgcc_eh
     elif domain_name.endswith('.rqchp.qc.ca'):
         numpy_includedirs   = [ '/usr/network.ALTIX/python-2.4.1/include' ]
         numpy_site_packages = join(homedir, '../delallea/local/lib/python2.4/site-packages/numarray -lutil')



From dorionc at mail.berlios.de  Sat Jun 16 00:05:25 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Sat, 16 Jun 2007 00:05:25 +0200
Subject: [Plearn-commits] r7592 - in trunk/python_modules/plearn: . analysis
	report
Message-ID: <200706152205.l5FM5Plh010472@sheep.berlios.de>

Author: dorionc
Date: 2007-06-16 00:05:24 +0200 (Sat, 16 Jun 2007)
New Revision: 7592

Added:
   trunk/python_modules/plearn/analysis/
   trunk/python_modules/plearn/analysis/__init__.py
   trunk/python_modules/plearn/analysis/experiment_results.py
Modified:
   trunk/python_modules/plearn/report/graphical_tools.py
Log:
The 'analysis' package is meant to contain improved versions of the material
currently under 'xp' and 'report'. 


Added: trunk/python_modules/plearn/analysis/__init__.py
===================================================================
--- trunk/python_modules/plearn/analysis/__init__.py	2007-06-15 20:53:47 UTC (rev 7591)
+++ trunk/python_modules/plearn/analysis/__init__.py	2007-06-15 22:05:24 UTC (rev 7592)
@@ -0,0 +1,30 @@
+# analysis/__init__.py
+# Copyright (C) 2007 Christian Dorion
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org

Added: trunk/python_modules/plearn/analysis/experiment_results.py
===================================================================
--- trunk/python_modules/plearn/analysis/experiment_results.py	2007-06-15 20:53:47 UTC (rev 7591)
+++ trunk/python_modules/plearn/analysis/experiment_results.py	2007-06-15 22:05:24 UTC (rev 7592)
@@ -0,0 +1,317 @@
+# experiment_results.py
+# Copyright (C) 2007 Nicolas Chapados, Christian Dorion
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+import sys, os, os.path, glob, fnmatch, csv, numarray
+
+from plearn.vmat.PMat import PMat
+from plearn.vmat.readAMat import readAMat
+from plearn.utilities.Bindings import Bindings
+
+#####  ExperimentDirectory  #################################################
+
+class ExperimentDirectory( object ):
+    """Class representing a subdirectory within a Finlearn3 experiment.
+
+    This object loads on-demand the .pmat, .amat and .csv making up the
+    subdirectory, and allows recursive access to sub-subdirectory through
+    simple member access. So if you have, for instance, an expdir
+    containing a PerformanceReport, you can write:
+
+        xp.PerformanceByKind.test_raw_costs['rel_return']
+
+    and if test_raw_costs is not yet loaded, it is at that point, and you
+    can access all columns of the underlying file by name.  Note that the
+    returned data is a VECTOR (1-D).
+    """
+    def __init__(self, expdir):
+        if os.path.isdir(expdir):
+            self.expdir = expdir
+        else:
+            raise RuntimeError, "Cannot construct ExperimentDirectory because " \
+                  "path '%s' is not a readable directory" % expdir
+
+        ## Remember files of interest
+        self.pmats = self._get_files('pmat')
+        self.csv = self._get_files('csv')
+        self.amats = self._get_files('amat')
+
+        ## Remember the subdirectories
+        self.subdirs = [ f for f in os.listdir(expdir)
+                         if not f.endswith('.metadata')
+                            and os.path.isdir(os.path.join(expdir, f)) ]
+
+    def _get_files(self, extension):
+        return [ os.path.splitext(os.path.basename(fname))[0]
+                 for fname in glob.glob(os.path.join(self.expdir, '*.' + extension)) ]
+
+    def find(self, pattern):
+        """Recursively find files that match a shell-like pattern in the
+        expdir tree
+        
+        For example, to find all generated png files, you can write:
+
+            xp.find('*.png')
+        """
+        return [ os.path.join(r,f)
+                 for (r,d,files) in os.walk(self.expdir)
+                   for f in files if fnmatch.fnmatch(f, pattern) ]
+
+    def _add_array(self, name, arr, fieldnames):
+            columns = {}
+            for i, f in enumerate(fieldnames):
+                columns[f] = arr[:,i]
+            setattr(self, name, columns)
+            return columns
+
+    def __getattr__(self, name):
+        if name in self.subdirs:
+            subdir = ExperimentDirectory(os.path.join(self.expdir, name))
+            setattr(self, name, subdir)
+            return subdir
+        elif name in self.pmats:
+            pmat = PMat(os.path.join(self.expdir, name+'.pmat'))
+            arr  = pmat.getRows(0, pmat.length)
+            fieldnames = pmat.fieldnames
+            return self._add_array(name, arr, fieldnames)
+        elif name in self.amats:
+            arr, fieldnames = readAMat(os.path.join(self.expdir, name+'.amat'))
+            return self._add_array(name, arr, fieldnames)
+        elif name in self.csv:
+            fname = os.path.join(self.expdir, name+'.csv')
+
+            # Use CSV sniffer to detect presence of header.
+            sniffer = csv.Sniffer()
+            f = open(fname)
+            sample = f.read(1000)
+            has_header = sniffer.has_header(sample)
+            f.seek(0)
+
+            # Load csv into array
+            csv_reader = csv.reader(f)
+            if has_header:
+                fieldnames = csv_reader.next()
+            arr = numarray.array([[float(value) for value in fields] for fields in csv_reader])
+            if not has_header:
+                # Generate fake fieldnames
+                fieldnames = ['field%d' % (i + 1) for i in range(arr.shape[1])]
+                
+            f.close()
+            return self._add_array(name, arr, fieldnames)
+        else:
+            raise ValueError, "ExperimentDirectory '%s' does not contain a component '%s'" \
+                  % (self.expdir, name)
+
+            
+#####  ExperimentResults  ###################################################
+
+class ExperimentResults( ExperimentDirectory ):
+    """Class allowing the inspection of Finlearn3 experiment results.
+
+    Note: an expdir is considered to contain valid results only if contains
+    a 'metainfos.txt' file, indicating that the experiment completed
+    successfully.
+    """
+    cached = []
+    metainfos_path = 'metainfos.txt'
+    
+    def __init__(self, expdir):
+        super(ExperimentResults,self).__init__(expdir)
+        metainfos_name = os.path.join(expdir, self.metainfos_path)
+        if os.path.isfile(metainfos_name):
+            self.metainfos = self.parseMetaInfos(metainfos_name)
+
+        else:
+            raise RuntimeError, "Cannot construct ExperimentResults because " \
+                  "expdir '%s' does not contain 'metainfos.txt'." % expdir
+
+        self.path = expdir
+        self.cached.append(self)
+
+    def __cmp__(self, other):
+        raise ExpKey.keycmp(x1, x2, None)
+
+    def getSubKey(self, keys=None):
+        """Returns a subset of the metainfos dict for the given keys."""
+        if keys is None:
+            return self.metainfos.copy()
+
+        subset = ExpKey()
+        for key in keys:
+            if key in self.metainfos:
+                subset[key] = self.metainfos[key]
+            else:
+                subset[key] = None
+        return subset
+        
+    def isMatched(self, expkey=[]):
+        # Always matching empty expkey
+        if not expkey:
+            return True 
+
+        # User should probably become aware of the concept of ExpKey.
+        if not isinstance(expkey, ExpKey): 
+            expkey = ExpKey(expkey)
+
+        # For efficiency
+        if len(expkey) > len(self.metainfos):
+            return False
+
+        # A key element from the expkey matches this experiement if
+        #
+        # 1) the key exists within this experiment ExpKey
+        #
+        # 2) the value is not restricted (None) or is restricted the same
+        #    value than the one in this experiment
+        match_predicate = lambda lhs,rhs: \
+            lhs in self.metainfos and \
+            ( rhs is None or self.metainfos[lhs]==rhs )                       
+
+        # All key element must match (match_predicate)
+        for lhs, rhs in expkey.iteritems():
+            if not match_predicate(lhs,rhs):
+                return False
+        
+        # All key element matched
+        return True
+
+
+    #####  Static Methods  ##############################################
+
+    def loadResults(exproot=None, forget=True):
+        # Clean cache if required
+        if forget:
+            ExperimentResults.cached = []
+    
+        # Use CWD as default exproot
+        if exproot is None:
+            exproot = os.getcwd()
+    
+        # Don't barf on inexistant exproots
+        if not os.path.exists(exproot):
+            return
+    
+        # Load all experiments in the provided exproot
+        dirlist = os.listdir(exproot)
+        for fname in dirlist:
+            candidate_path = os.path.join(exproot, fname)
+            candidate_infopath = os.path.join(candidate_path,
+                                              ExperimentResults.metainfos_path)
+            if fname.startswith("expdir") and os.path.exists(candidate_infopath):
+                ExperimentResults(candidate_path) # Cached in __init__ 
+    loadResults = staticmethod(loadResults)
+
+    def match(expkey=[]):
+        if not ExperimentResults.cached:
+            ExperimentResults.cache_experiments()
+        return [ exp for exp in ExperimentResults.cached if exp.isMatched(expkey) ]
+    match = staticmethod(match)
+
+    def match_one(expkey=[]):
+        matches = ExperimentResults.match(expkey)
+        assert len(matches) == 1, \
+               "Key matches %d experiments\n%s" % (len(matches), expkey)
+        return matches[0]
+    match_one = staticmethod(match_one)
+
+    def parseMetaInfos(metainfos_name):
+        metainfos = ExpKey()
+        f = open(metainfos_name)
+        for line in f:
+            [lhs,rhs] = line.split('=',1)
+            metainfos[lhs.strip()] = rhs.strip()
+        return metainfos
+    parseMetaInfos = staticmethod(parseMetaInfos)
+    
+
+#####  ExpKey  ##############################################################
+
+class ExpKey( Bindings ):
+    """Utility class for matching experiments (ExperimentResults.isMatched())"""
+    _neglected = [ 'expdir', 'expdir_root' ]
+    
+    def __init__(self, keysrc=[]):
+        expkey = []
+        if isinstance(keysrc, str): # Handle single string
+            keysrc = [ keysrc ]
+        assert isinstance(keysrc, list)
+        
+        # List of tuples
+        if keysrc and isinstance(keysrc[0], tuple):
+            expkey = keysrc
+
+        # List of strings
+        elif keysrc and isinstance(keysrc[0], str):
+            for key in keysrc:
+                lhs_rhs = key.split('=',1)
+                if len(lhs_rhs)==1:
+                    expkey.append( (lhs_rhs[0].strip(), None) )
+                else:
+                    expkey.append( (lhs_rhs[0].strip(), lhs_rhs[1].strip()) )
+
+        # Create the dict
+        Bindings.__init__(self, expkey)
+        for neg in self._neglected:
+            if neg in self:
+                del self[neg]
+
+    def listkey(self):
+        keyjoin = lambda key,val: (val is None and key) or "%s=%s"%(key,val)
+        return [ keyjoin(key, value) for key,value in self.iteritems() ]
+    
+    def strkey(self):
+        return " ".join(self.listkey())
+
+
+    #####  Static Methods  #################################################
+
+    def keycmp(exp, other, expkey):
+        """Compare two experiments along a given key"""
+        if exp.path == other.path:
+            return 0
+        
+        exp_subkey   = exp.getSubKey(expkey)
+        exp_it       = exp_subkey.iteritems()
+        
+        other_subkey = other.getSubKey(expkey)
+        other_it     = other_subkey.iteritems()
+        
+        for item in exp_it:
+            try:
+                compare = cmp(item, other_it.next())
+                if compare != 0:
+                    return compare
+            except StopIteration:
+                return 1 ## >
+        
+        ## Non-Positive ( < or == )
+        return len(exp_subkey) - len(other_subkey)
+    keycmp = staticmethod(keycmp)
+
+

Modified: trunk/python_modules/plearn/report/graphical_tools.py
===================================================================
--- trunk/python_modules/plearn/report/graphical_tools.py	2007-06-15 20:53:47 UTC (rev 7591)
+++ trunk/python_modules/plearn/report/graphical_tools.py	2007-06-15 22:05:24 UTC (rev 7592)
@@ -31,6 +31,7 @@
 
 # Author: Christian Dorion
 import pylab, os
+from matplotlib.font_manager import FontProperties
 from plearn.report import GRID_COL, FONTSIZE, LEGEND_FONTPROP, TICK_LABEL_FONTPROP
 
 LEFT,   WIDTH  = 0.125, 0.800
@@ -69,21 +70,29 @@
         axes.set_xlim(m, M)
     return m, M
 
-def same_ylim(*ax_list):
+def same_ylim(*ax_list, **kwargs):
     m, M = float('inf'), -float('inf')
     for axes in ax_list:
         ylim = axes.get_ylim()
         m, M = min(m, ylim[0]), max(M, ylim[1]), 
 
+    if 'ymin' in kwargs:
+        m = max(m, kwargs.pop('ymin'))
+    if 'ymax' in kwargs:
+        M = min(M, kwargs.pop('ymax'))
+
     for axes in ax_list:
         axes.set_ylim(m, M)
+
+    assert len(kwargs)==0, "Unexpected keyword arguments: %s"%repr(kwargs.keys())
     return m, M
 
 def setLegend(axes, legend_map, sorted_keys=None, loc=0):
     if not sorted_keys:
         sorted_keys = legend_map.keys(); sorted_keys.sort()
     values = [ legend_map[k] for k in sorted_keys ]
-    legend = axes.legend(values, sorted_keys, loc=loc, shadow=False)
+    legend = axes.legend(values, sorted_keys,
+                         loc=loc, shadow=False, prop = FontProperties(size=13))
     legend.set_zorder(100)
 
 class Struct(dict):



From yoshua at mail.berlios.de  Mon Jun 18 17:53:23 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 18 Jun 2007 17:53:23 +0200
Subject: [Plearn-commits] r7593 - trunk/plearn_learners/classifiers
Message-ID: <200706181553.l5IFrNoL031476@sheep.berlios.de>

Author: yoshua
Date: 2007-06-18 17:53:23 +0200 (Mon, 18 Jun 2007)
New Revision: 7593

Modified:
   trunk/plearn_learners/classifiers/KNNClassifier.cc
Log:
To avoid stupid warnings when k is small and KNN
overconfidently says prob=0 for the correct class.


Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2007-06-15 22:05:24 UTC (rev 7592)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2007-06-18 15:53:23 UTC (rev 7593)
@@ -273,7 +273,7 @@
     costs.resize(nTestCosts());
     int sel_class = argmax(output);
     costs[0] = sel_class != int(target[0]);
-    costs[1] = -pl_log(output[int(target[0])]);
+    costs[1] = -pl_log(1e-10+output[int(target[0])]);
 }
 
 TVec<string> KNNClassifier::getTestCostNames() const



From tihocan at mail.berlios.de  Mon Jun 18 20:38:17 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 18 Jun 2007 20:38:17 +0200
Subject: [Plearn-commits] r7594 - in trunk: plearn/math
	plearn_learners/regressors
Message-ID: <200706181838.l5IIcHhu023938@sheep.berlios.de>

Author: tihocan
Date: 2007-06-18 20:38:17 +0200 (Mon, 18 Jun 2007)
New Revision: 7594

Modified:
   trunk/plearn/math/TMat_impl.h
   trunk/plearn_learners/regressors/CubicSpline.cc
Log:
Commented out includes of 'algo.h' which is apparently deprecated (and should be replaced with <algorithm>, but since PLearn compiles without these includes and the standard tests still pass, this include may not have been needed in the first place)

Modified: trunk/plearn/math/TMat_impl.h
===================================================================
--- trunk/plearn/math/TMat_impl.h	2007-06-18 15:53:23 UTC (rev 7593)
+++ trunk/plearn/math/TMat_impl.h	2007-06-18 18:38:17 UTC (rev 7594)
@@ -53,7 +53,7 @@
 #include "TMatRowsAsArraysIterator_impl.h"
 #include "TMatColRowsIterator_impl.h"
 
-#include "algo.h"
+//#include "algo.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/CubicSpline.cc
===================================================================
--- trunk/plearn_learners/regressors/CubicSpline.cc	2007-06-18 15:53:23 UTC (rev 7593)
+++ trunk/plearn_learners/regressors/CubicSpline.cc	2007-06-18 18:38:17 UTC (rev 7594)
@@ -38,7 +38,7 @@
 
 
 #include "CubicSpline.h"
-#include "algo.h"
+//#include "algo.h"
 
 namespace PLearn {
 using namespace std;



From tihocan at mail.berlios.de  Mon Jun 18 21:02:59 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 18 Jun 2007 21:02:59 +0200
Subject: [Plearn-commits] r7595 -
	trunk/python_modules/plearn/learners/modulelearners
Message-ID: <200706181902.l5IJ2x4p027021@sheep.berlios.de>

Author: tihocan
Date: 2007-06-18 21:02:59 +0200 (Mon, 18 Jun 2007)
New Revision: 7595

Removed:
   trunk/python_modules/plearn/learners/modulelearners/__init__.pyc
   trunk/python_modules/plearn/learners/modulelearners/network_view.pyc
   trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.pyc
Log:
Removed .pyc files

Deleted: trunk/python_modules/plearn/learners/modulelearners/__init__.pyc
===================================================================
(Binary files differ)

Deleted: trunk/python_modules/plearn/learners/modulelearners/network_view.pyc
===================================================================
(Binary files differ)

Deleted: trunk/python_modules/plearn/learners/modulelearners/pyplearn_read.pyc
===================================================================
(Binary files differ)



From tihocan at mail.berlios.de  Mon Jun 18 21:04:35 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 18 Jun 2007 21:04:35 +0200
Subject: [Plearn-commits] r7596 -
	trunk/python_modules/plearn/learners/modulelearners
Message-ID: <200706181904.l5IJ4Z0q027162@sheep.berlios.de>

Author: tihocan
Date: 2007-06-18 21:04:34 +0200 (Mon, 18 Jun 2007)
New Revision: 7596

Modified:
   trunk/python_modules/plearn/learners/modulelearners/
Log:
Ignoring .pyc files


Property changes on: trunk/python_modules/plearn/learners/modulelearners
___________________________________________________________________
Name: svn:ignore
   + *.pyc




From tihocan at mail.berlios.de  Mon Jun 18 21:10:12 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 18 Jun 2007 21:10:12 +0200
Subject: [Plearn-commits] r7597 - trunk/plearn_learners/regressors
Message-ID: <200706181910.l5IJACfo027599@sheep.berlios.de>

Author: tihocan
Date: 2007-06-18 21:10:12 +0200 (Mon, 18 Jun 2007)
New Revision: 7597

Modified:
   trunk/plearn_learners/regressors/LinearRegressor.cc
Log:
Fixed typo in error message

Modified: trunk/plearn_learners/regressors/LinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/LinearRegressor.cc	2007-06-18 19:04:34 UTC (rev 7596)
+++ trunk/plearn_learners/regressors/LinearRegressor.cc	2007-06-18 19:10:12 UTC (rev 7597)
@@ -225,7 +225,8 @@
 void LinearRegressor::train()
 {
     if(targetsize()<=0)
-        PLERROR("In LinearRegressor::train(), we are doing a regression with a trainsize for the train_set of %d(should he higher then 0)",targetsize());
+        PLERROR("In LinearRegressor::train() -  Targetsize (%s) must be "
+                "positive", targetsize());
     // Preparatory buffer allocation
     bool recompute_XXXY = (XtX.length()==0);
     extendedinput.resize(effective_inputsize());



From tihocan at mail.berlios.de  Mon Jun 18 21:13:57 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 18 Jun 2007 21:13:57 +0200
Subject: [Plearn-commits] r7598 - trunk/plearn/math
Message-ID: <200706181913.l5IJDvx1027883@sheep.berlios.de>

Author: tihocan
Date: 2007-06-18 21:13:57 +0200 (Mon, 18 Jun 2007)
New Revision: 7598

Modified:
   trunk/plearn/math/TMat_maths_specialisation.h
Log:
Typo fix in error message

Modified: trunk/plearn/math/TMat_maths_specialisation.h
===================================================================
--- trunk/plearn/math/TMat_maths_specialisation.h	2007-06-18 19:10:12 UTC (rev 7597)
+++ trunk/plearn/math/TMat_maths_specialisation.h	2007-06-18 19:13:57 UTC (rev 7598)
@@ -306,7 +306,8 @@
                 "Mat(%d,%d) <- Vec(%d).Vec(%d)'",
                 A.length(), A.width(), x.length(), y.length());
     if(A.mod()<=0 || A.width()<=0)
-        PLERROR("In externalProductScaleAcc, destination matrice hava a width(%d) or a mod(%d) <= 0",A.width(),A.mod());
+        PLERROR("In externalProductScaleAcc, destination matrix has a width "
+                "(%d) or a mod (%d) <= 0", A.width(), A.mod());
 #endif
     int one = 1;
     int lda = A.mod();



From chapados at mail.berlios.de  Tue Jun 19 17:55:18 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 19 Jun 2007 17:55:18 +0200
Subject: [Plearn-commits] r7600 - trunk/python_modules/plearn/vmat
Message-ID: <200706191555.l5JFtIx9031592@sheep.berlios.de>

Author: chapados
Date: 2007-06-19 17:55:18 +0200 (Tue, 19 Jun 2007)
New Revision: 7600

Added:
   trunk/python_modules/plearn/vmat/smartReadMat.py
Log:
Utility function to read a matrix as a numarray (for now) regardless of its types; .amat, .pmat and .csv are supported

Added: trunk/python_modules/plearn/vmat/smartReadMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/smartReadMat.py	2007-06-18 23:35:05 UTC (rev 7599)
+++ trunk/python_modules/plearn/vmat/smartReadMat.py	2007-06-19 15:55:18 UTC (rev 7600)
@@ -0,0 +1,176 @@
+# smartReadMat.py
+# Copyright (C) 2007 by Nicolas Chapados
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+
+# Author: Nicolas Chapados
+
+import csv
+import numarray
+
+from plearn.vmat.PMat import PMat
+from plearn.vmat.readAMat import readAMat
+
+
+### BEGIN ugly hack
+
+# Code copied from module csv.py, but with the rdr = ... line changed
+# so we don't get an exception
+try:
+    from cStringIO import StringIO
+except ImportError:
+    from StringIO import StringIO
+
+from _csv import reader
+
+def has_header(self, sample):
+    # Creates a dictionary of types of data in each column. If any
+    # column is of a single type (say, integers), *except* for the first
+    # row, then the first row is presumed to be labels. If the type
+    # can't be determined, it is assumed to be a string in which case
+    # the length of the string is the determining factor: if all of the
+    # rows except for the first are the same length, it's a header.
+    # Finally, a 'vote' is taken at the end for each column, adding or
+    # subtracting from the likelihood of the first row being a header.
+
+    rdr = reader(StringIO(sample))
+
+    header = rdr.next() # assume first row is header
+
+    columns = len(header)
+    columnTypes = {}
+    for i in range(columns): columnTypes[i] = None
+
+    checked = 0
+    for row in rdr:
+        # arbitrary number of rows to check, to keep it sane
+        if checked > 20:
+            break
+        checked += 1
+
+        if len(row) != columns:
+            continue # skip rows that have irregular number of columns
+
+        for col in columnTypes.keys():
+
+            for thisType in [int, long, float, complex]:
+                try:
+                    thisType(row[col])
+                    break
+                except (ValueError, OverflowError):
+                    pass
+            else:
+                # fallback to length of string
+                thisType = len(row[col])
+
+            # treat longs as ints
+            if thisType == long:
+                thisType = int
+
+            if thisType != columnTypes[col]:
+                if columnTypes[col] is None: # add new column type
+                    columnTypes[col] = thisType
+                else:
+                    # type is inconsistent, remove column from
+                    # consideration
+                    del columnTypes[col]
+
+    # finally, compare results against first row and "vote"
+    # on whether it's a header
+    hasHeader = 0
+    for col, colType in columnTypes.items():
+        if type(colType) == type(0): # it's a length
+            if len(header[col]) != colType:
+                hasHeader += 1
+            else:
+                hasHeader -= 1
+        else: # attempt typecast
+            try:
+                colType(header[col])
+            except (ValueError, TypeError):
+                hasHeader += 1
+            else:
+                hasHeader -= 1
+
+    return hasHeader > 0
+
+
+csv.Sniffer.has_header = has_header
+
+### END ugly hack
+
+
+#####  smartReadMat  ########################################################
+
+def smartReadMat(filename):
+    """Read a matrix from a file, determining the file type automatically.
+
+    This function returns a pair (matrix,fieldnames).  The filetype is
+    determined automatically from the extension.  The following formats are
+    currently supported:
+
+    - '.amat' : PLearn ascii matrix format
+    - '.pmat' : PLearn binary matrix format
+    - '.csv'  : Text-file comma-separated values format
+    """
+
+    if filename.endswith(".amat"):
+        arr, fieldnames = readAMat(filename)
+
+    elif filename.endswith(".pmat"):
+        pmat = PMat(filename)
+        arr  = pmat.getRows(0, pmat.length)
+        fieldnames = pmat.fieldnames
+        pmat.close()
+
+    elif filename.endswith(".csv"):
+        # Use CSV sniffer to detect presence of header.
+        sniffer = csv.Sniffer()
+        f = open(filename)
+        sample = f.read(1000)
+        has_header = sniffer.has_header(sample)
+        f.seek(0)
+
+        # Load csv into array
+        csv_reader = csv.reader(f)
+        if has_header:
+            fieldnames = csv_reader.next()
+        arr = numarray.array([[float(value) for value in fields] for fields in csv_reader])
+        if not has_header:
+            # Generate fake fieldnames
+            fieldnames = ['field%d' % (i + 1) for i in range(arr.shape[1])]
+            
+        f.close()
+
+    else:
+        raise ValueError, "Unrecognized file type for '%s'; valid extensions are: " \
+                          "{'.amat', '.pmat', '.csv'}" % filename
+
+    return arr, fieldnames



From chapados at mail.berlios.de  Tue Jun 19 17:57:10 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 19 Jun 2007 17:57:10 +0200
Subject: [Plearn-commits] r7601 - trunk/python_modules/plearn/math
Message-ID: <200706191557.l5JFvA4e031819@sheep.berlios.de>

Author: chapados
Date: 2007-06-19 17:57:10 +0200 (Tue, 19 Jun 2007)
New Revision: 7601

Modified:
   trunk/python_modules/plearn/math/missings_robust_covariance.py
Log:
Additional option for missing-robust covariance estimation

Modified: trunk/python_modules/plearn/math/missings_robust_covariance.py
===================================================================
--- trunk/python_modules/plearn/math/missings_robust_covariance.py	2007-06-19 15:55:18 UTC (rev 7600)
+++ trunk/python_modules/plearn/math/missings_robust_covariance.py	2007-06-19 15:57:10 UTC (rev 7601)
@@ -38,7 +38,7 @@
 from numarray.ma import *
 from plearn.math import isNaN
 
-def missingsRobustCovariance(arr):
+def missingsRobustCovariance(arr, unbiased=False, epsilon=1e-8):
     """Compute the covariance matrix in a way that's robust to missing values.
 
     This function computes the covariance matrix between the columns of
@@ -47,8 +47,14 @@
     it uses as much information as it can, and can be used, for instance,
     when EVERY ROW contains one or more missings.
 
-    If the matrix contains no missings whatsoever, it produces the same
-    results as the standard bias-corrected maximum likelihood estimator.
+    If the matrix contains no missings whatsoever, it makes sense to use
+    the option 'unbiased=True', which produces the same results as the
+    standard bias-corrected maximum likelihood estimator.  Otherwise,
+    numerical problems may ensue with the covariance matrix (e.g. lack of
+    positive-definiteness).
+
+    The option 'epsilon' is used to add a small regularization constant to
+    the diagonal.
     """
     ## Second-order statistics
     mask   = isNaN(arr)
@@ -72,4 +78,7 @@
     masked_count = masked_less_equal(cov_count, 1.0)
     cov_mle *= masked_count / (masked_count-1.)
 
+    for i in range(cov_mle.shape[0]):
+        cov_mle[i,i] += epsilon         # numarray cannot assign to diagonal!!!!
+
     return filled(cov_mle, NaN)



From dorionc at mail.berlios.de  Tue Jun 19 21:12:11 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Tue, 19 Jun 2007 21:12:11 +0200
Subject: [Plearn-commits] r7602 - in trunk: python_modules/plearn/analysis
	python_modules/plearn/xp scripts
Message-ID: <200706191912.l5JJCBUf026817@sheep.berlios.de>

Author: dorionc
Date: 2007-06-19 21:12:10 +0200 (Tue, 19 Jun 2007)
New Revision: 7602

Modified:
   trunk/python_modules/plearn/analysis/experiment_results.py
   trunk/python_modules/plearn/xp/Experiment.py
   trunk/scripts/xp
Log:
One step further in deprecating python_modules/plearn/xp/ 


Modified: trunk/python_modules/plearn/analysis/experiment_results.py
===================================================================
--- trunk/python_modules/plearn/analysis/experiment_results.py	2007-06-19 15:57:10 UTC (rev 7601)
+++ trunk/python_modules/plearn/analysis/experiment_results.py	2007-06-19 19:12:10 UTC (rev 7602)
@@ -32,6 +32,7 @@
 
 from plearn.vmat.PMat import PMat
 from plearn.vmat.readAMat import readAMat
+from plearn.utilities.moresh import relative_path
 from plearn.utilities.Bindings import Bindings
 
 #####  ExperimentDirectory  #################################################
@@ -67,6 +68,9 @@
                          if not f.endswith('.metadata')
                             and os.path.isdir(os.path.join(expdir, f)) ]
 
+    def __str__(self):        
+        return "\n    ".join([ self.expdir ] + self.subdirs)
+
     def _get_files(self, extension):
         return [ os.path.splitext(os.path.basename(fname))[0]
                  for fname in glob.glob(os.path.join(self.expdir, '*.' + extension)) ]
@@ -146,10 +150,9 @@
         metainfos_name = os.path.join(expdir, self.metainfos_path)
         if os.path.isfile(metainfos_name):
             self.metainfos = self.parseMetaInfos(metainfos_name)
-
         else:
-            raise RuntimeError, "Cannot construct ExperimentResults because " \
-                  "expdir '%s' does not contain 'metainfos.txt'." % expdir
+            # Experiment is still running
+            self.metainfos = ExpKey()
 
         self.path = expdir
         self.cached.append(self)
@@ -157,6 +160,21 @@
     def __cmp__(self, other):
         raise ExpKey.keycmp(x1, x2, None)
 
+    def __str__( self ):
+        return self.toString()
+
+    def getKey(self, expkey=None):
+        if expkey is None:
+            return self.metainfos
+
+        subset = ExpKey()
+        for key in expkey:
+            if key in self.metainfos:
+                subset[key] = self.metainfos[key]
+            else:
+                subset[key] = None
+        return subset
+        
     def getSubKey(self, keys=None):
         """Returns a subset of the metainfos dict for the given keys."""
         if keys is None:
@@ -201,7 +219,19 @@
         # All key element matched
         return True
 
+    def isRunning(self):
+        return len(self.metainfos) == 0
 
+    def toString(self, expkey=None, short=False):
+        s = '%s\n' % relative_path(self.path)
+        if short and expkey is None:
+            return s
+        
+        for key, value in self.getKey(expkey).iteritems():
+            s += '    %s= %s\n' % (key.ljust(30), value)
+        return s
+
+
     #####  Static Methods  ##############################################
 
     def loadResults(exproot=None, forget=True):
@@ -229,7 +259,7 @@
 
     def match(expkey=[]):
         if not ExperimentResults.cached:
-            ExperimentResults.cache_experiments()
+            ExperimentResults.loadResults()
         return [ exp for exp in ExperimentResults.cached if exp.isMatched(expkey) ]
     match = staticmethod(match)
 

Modified: trunk/python_modules/plearn/xp/Experiment.py
===================================================================
--- trunk/python_modules/plearn/xp/Experiment.py	2007-06-19 15:57:10 UTC (rev 7601)
+++ trunk/python_modules/plearn/xp/Experiment.py	2007-06-19 19:12:10 UTC (rev 7602)
@@ -72,16 +72,13 @@
             raise ValueError( obj )
     return function
 
-def migrate( path, dest, move=False ):
-    abspath = os.path.abspath( path )
+def migrate(path, dest, move=False):
+    abspath = os.path.abspath(path)
     if move:
-        absdest = os.path.abspath( dest )
-        os.system( 'mv %s %s'
-                   % ( abspath, absdest )
-                   )
-
+        absdest = os.path.abspath(dest)
+        os.system('mv %s %s' % (abspath, absdest))
     else:
-        relative_link( abspath, dest )            
+        relative_link(abspath, dest)            
 migrate = xpathfunction( migrate )
 
 class ExpKey( Bindings ):

Modified: trunk/scripts/xp
===================================================================
--- trunk/scripts/xp	2007-06-19 15:57:10 UTC (rev 7601)
+++ trunk/scripts/xp	2007-06-19 19:12:10 UTC (rev 7602)
@@ -1,8 +1,8 @@
 #!/usr/bin/env python
 
 import os
-from plearn           import xp
-#from plearn.utilities import toolkit
+#from plearn           import xp
+from plearn.analysis  import xp
 from plearn.utilities import svn
 
 # Builds are tuples of the form (major, minor, plearn_revision) where



From dorionc at mail.berlios.de  Tue Jun 19 21:21:30 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Tue, 19 Jun 2007 21:21:30 +0200
Subject: [Plearn-commits] r7603 - trunk/python_modules/plearn/analysis
Message-ID: <200706191921.l5JJLU1v027891@sheep.berlios.de>

Author: dorionc
Date: 2007-06-19 21:21:30 +0200 (Tue, 19 Jun 2007)
New Revision: 7603

Added:
   trunk/python_modules/plearn/analysis/xp.py
Log:


Added: trunk/python_modules/plearn/analysis/xp.py
===================================================================
--- trunk/python_modules/plearn/analysis/xp.py	2007-06-19 19:12:10 UTC (rev 7602)
+++ trunk/python_modules/plearn/analysis/xp.py	2007-06-19 19:21:30 UTC (rev 7603)
@@ -0,0 +1,304 @@
+"""xp 0.1
+
+usage::
+    xp [mode [options]] expkey*
+
+Default mode is listdir
+
+Supported modes are:
+
+    1. L{duplicates}:
+    2. L{listdir}:  
+    3. L{group}:    
+    4. L{running}:  
+
+Type 'xp mode --help' for further help on a mode
+"""
+import new, os, sys
+from ConfigParser import ConfigParser
+
+from plearn                               import pyplearn
+from experiment_results                   import *
+from plearn.vmat.PMat                     import PMat
+from plearn.utilities                     import toolkit
+from plearn.utilities.toolkit             import vsystem
+from plearn.utilities.moresh              import *
+from plearn.utilities.ModeAndOptionParser import *
+
+#
+#  Classes
+#
+class XpMode( Mode ):
+    pass
+
+## class link( XpMode ):
+##     def __init__( self, targets, options ):
+##         for target in targets:
+##             dirlist = os.listdir( target )
+##             for fname in dirlist:
+##                 if fname.startswith( "expdir" ):
+##                     exppath = os.path.join( target, fname )
+##                     os.symlink
+                        
+class merge( XpMode ):
+    """\ls -1 Rank*/expdir* | grep expdir > tmp.sh"""
+    def option_groups( cls, parser ):
+        return directory_options( cls, parser )
+    option_groups = classmethod( option_groups )
+    
+
+    def __init__( self, targets, options ):
+        super(merge, self).__init__(targets, options)
+        if options.name:
+            dirname = options.name
+        else:
+            dirname = 'Merge_%s' % '_'.join(targets)
+
+        if not os.path.isdir( dirname ):
+            os.makedirs( dirname )
+
+        for target in targets:
+            dirlist = os.listdir( target )
+            for fname in dirlist:
+                if fname.startswith("expdir"):
+                    migrate( os.path.join(target, fname), dirname, options.move )
+
+class mkdir( XpMode ):
+    def __init__( self, targets, options ):
+        super(mkdir, self).__init__(targets, options)
+        dirname = targets[0]
+        
+        vsystem( 'mkdir %s'             % dirname )
+        vsystem( 'mkdir %s/Experiments' % dirname )
+        vsystem( 'mkdir %s/Reports'     % dirname )
+        
+        config = ConfigParser( )
+        config.add_section( 'EXPERIMENTS' )
+        config.set( 'EXPERIMENTS', 'expdir_root', 'Experiments' )
+        config.set( 'EXPERIMENTS', 'report_root', 'Reports'     )
+
+        config_path = '%s/.pyplearn' % dirname
+        config_fp   = open( config_path, 'w' )
+        config.write( config_fp )        
+        config_fp.close()
+        print '+++', config_path, 'created.'
+        
+        lname  = '%s/dispatch' % dirname
+        import dispatch_template
+        template_file = dispatch_template.__file__.replace( '.pyc', '.py' )
+        vsystem( 'cp %s %s' % (template_file, lname) )
+        vsystem( 'chmod u=rwx %s' % lname )
+        print '+++', lname, 'created.'
+        
+
+class running( XpMode ):
+    def __init__( self, targets, options ):
+        super(running, self).__init__(targets, options)
+        assert not targets
+        
+        r = 0
+        for exp in ls():
+            if exp.startswith("expdir") and \
+               not os.path.exists( os.path.join(exp, ExperimentResults.metainfos_path) ):
+                if r == 0:
+                    print
+                r += 1
+                print exp
+
+        print( "\n(%d experiment%s %s running)"
+               % ( r, toolkit.plural(r),
+                   toolkit.plural(r, 'is', 'are') )
+               )
+        
+
+#
+#  Modes Parsing ExpKeys
+#
+class ExpKeyMode( XpMode ):
+    def __init__( self, targets, options ):
+        super(ExpKeyMode, self).__init__(targets, options)
+        self.routine( ExpKey(targets), options, ExperimentResults.match(targets) )
+
+class duplicates( ExpKeyMode ):
+    def aliases( cls ):
+        return ['dup']
+    aliases = classmethod( aliases )
+
+    def option_groups( cls, parser ):
+        duplicates_options = OptionGroup( parser, "Mode Specific Options --- %s" % cls.__name__,
+                                          "Available under %s mode only." % cls.__name__ )
+
+        duplicates_options.add_option( "--sh",
+                                       default = False,
+                                       action  = 'store_true',
+                                       help    = ""
+                                       )
+
+        return [ duplicates_options ]
+    option_groups = classmethod( option_groups )
+
+    def routine( self, expkey, options, experiments ):
+        if options.sh:
+            shfile = open('duplicates.sh', 'w')
+            
+        while experiments:
+            exp = experiments.pop()
+            duplicates = []
+            for x in experiments:
+                if ExpKey.keycmp(x, exp, expkey) == 0:
+                    duplicates.append( x.path )
+            if duplicates:
+                if options.sh:
+                    shfile.write( 'rm -rf ' + "\nrm -rf ".join(duplicates) + '\n' )
+                else:
+                    print exp.toString( expkey, True )
+                    print "Duplicated by", " ".join(duplicates)
+                    print
+
+class listdir( ExpKeyMode ):
+    """List matching experiments directory. B{Default mode.}
+    
+    Among experiments in the current directory, this mode lists the
+    experiments that matches the experiment key::
+
+        xp [listdir [key[=value]]*]
+
+    Providing the listdir mode name is optional since the listdir mode is
+    the default one.
+
+    If no key is provided, all experiments in the directory are listed and
+    their settings reported. If a key is provided without value, all
+    experiment having the given key in their settings are matched.
+    """
+    def aliases( cls ):
+        return ['ls']
+    aliases = classmethod( aliases )
+    
+    def routine( self, expkey, options, experiments ):
+        experiments.sort( lambda x1, x2: ExpKey.keycmp(x1, x2, expkey) )
+        if options.full or not expkey:
+            print "\n","\n".join([ str(x) for x in experiments ])
+        else:
+            for x in experiments:
+                print '\n%s' % x.toString( expkey )
+            
+        print "(%d experiments)" % len(experiments)
+
+    def option_groups( cls, parser ):
+        listdir_options = OptionGroup( parser, "Mode Specific Options --- %s" % cls.__name__,
+                                     "Available under listdir mode only." )
+
+        listdir_options.add_option( "--full",
+                                  default = False,
+                                  action  = 'store_true',
+                                  help    = "Listdir's default behaviour is to restrict the printing of the metainfos to "
+                                    "the key provided as target. Use this option to print all metainfos."                       
+                                  )
+        
+        return [ listdir_options ]
+    option_groups = classmethod( option_groups )
+
+
+class group( ExpKeyMode ):
+    def option_groups( cls, parser ):
+        return directory_options( cls, parser )
+    option_groups = classmethod( option_groups )
+    
+    def routine( self, expkey, options, experiments ):    
+        reffunc = os.symlink
+        if options.move:
+            reffunc = lambda src,dest: os.system("mv %s %s"%(src,dest))        
+
+        for exp in experiments:
+            subkey = exp.getKey( expkey )
+            if options.name is None:
+                dirname = "_".join([ "%s=%s" % (lhs, str(rhs))
+                                     for (lhs, rhs) in subkey.iteritems() ])
+            else:
+                dirname = options.name
+
+            if not exp.path.startswith("expdir"):
+                dirname = os.path.join( os.path.dirname(exp.path), dirname )
+
+            if not os.path.exists( dirname ):
+                os.mkdir( dirname )
+
+            pushd( dirname )
+            if not os.path.exists( exp.path ):
+                expdir = os.path.basename( exp.path )
+                assert expdir.startswith("expdir"), expdir
+                try:
+                    reffunc( os.path.join('..',expdir), expdir )
+                except OSError, err:
+                    raise OSError( '%s\n in %s\n with expdir=%s'
+                                   % (str(err), os.getcwd(), expdir)
+                                   )
+            popd( )
+
+#
+#  Helper functions
+#
+def directory_options( cls, parser ):
+    directory_options = OptionGroup( parser, "Mode Specific Options --- %s" % cls.__name__,
+                                     "Available under %s mode only." % cls.__name__ )
+
+    directory_options.add_option( "--move",
+                                  default = False,
+                                  action  = 'store_true',
+                                  help    = "Should the experiments directory be moved (instead of linked) in "
+                                  "the created directory."                       
+                                  )
+    
+    directory_options.add_option( "--name",
+                                  default = None,
+                                  help    = "The name that should be given the created directory. The default "
+                                  "name is built from the experiment key."                       
+                                  )
+
+    return [ directory_options ]
+
+def inexistence_predicate(arguments, forget=False):
+    """Predicate checking that the experiment doesn't exist."""
+    if forget:
+        ExperimentResults.loadResults(forget=forget)
+    matches = Experiment.match(arguments)
+    return len(matches) == 0
+    
+def xpathfunction( func ):
+    def function( obj, *args, **kwargs ):
+        if isinstance( obj, Experiment ):
+            func( obj.path, *args, **kwargs )
+        elif len(args) and isinstance( obj, str ):
+            func( obj, *args, **kwargs )
+        else:
+            raise ValueError( obj )
+    return function
+
+def migrate(path, dest, move=False):
+    abspath = os.path.abspath(path)
+    if move:
+        absdest = os.path.abspath(dest)
+        os.system('mv %s %s' % (abspath, absdest))
+    else:
+        relative_link(abspath, dest)            
+migrate = xpathfunction( migrate )
+
+
+#            
+#  Main program
+#
+def main( xp_version = lambda : '0.1', default_mode_name='listdir' ):
+    parser = ModeAndOptionParser( usage = ( "%prog [mode [options]] expkey*\n" +
+                                            (' '*7) +"Default mode is listdir" ),
+                                  version = "%prog " + xp_version( )   )
+
+    #
+    # Actual Launch
+    #
+    options, targets = parser.parse_args( default_mode_name=default_mode_name )
+
+    print "xp %s" % xp_version( )    
+    parser.selected_mode( targets, options )
+
+if __name__ == '__main__':
+    main( )



From dorionc at mail.berlios.de  Tue Jun 19 21:38:00 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Tue, 19 Jun 2007 21:38:00 +0200
Subject: [Plearn-commits] r7604 - trunk/python_modules/plearn/analysis
Message-ID: <200706191938.l5JJc0sK029577@sheep.berlios.de>

Author: dorionc
Date: 2007-06-19 21:38:00 +0200 (Tue, 19 Jun 2007)
New Revision: 7604

Modified:
   trunk/python_modules/plearn/analysis/experiment_results.py
Log:
Added slight modifications from chrish and chapados


Modified: trunk/python_modules/plearn/analysis/experiment_results.py
===================================================================
--- trunk/python_modules/plearn/analysis/experiment_results.py	2007-06-19 19:21:30 UTC (rev 7603)
+++ trunk/python_modules/plearn/analysis/experiment_results.py	2007-06-19 19:38:00 UTC (rev 7604)
@@ -30,9 +30,10 @@
 #  library, go to the PLearn Web site at www.plearn.org
 import sys, os, os.path, glob, fnmatch, csv, numarray
 
-from plearn.vmat.PMat import PMat
-from plearn.vmat.readAMat import readAMat
-from plearn.utilities.moresh import relative_path
+import os, glob, fnmatch, numarray
+from plearn.vmat.PMat          import PMat
+from plearn.vmat.smartReadMat  import smartReadMat
+from plearn.utilities.moresh   import relative_path
 from plearn.utilities.Bindings import Bindings
 
 #####  ExperimentDirectory  #################################################
@@ -50,6 +51,9 @@
     and if test_raw_costs is not yet loaded, it is at that point, and you
     can access all columns of the underlying file by name.  Note that the
     returned data is a VECTOR (1-D).
+
+    As a special case, passing the string '__all__' returns the whole
+    array.
     """
     def __init__(self, expdir):
         if os.path.isdir(expdir):
@@ -60,7 +64,7 @@
 
         ## Remember files of interest
         self.pmats = self._get_files('pmat')
-        self.csv = self._get_files('csv')
+        self.csv   = self._get_files('csv')
         self.amats = self._get_files('amat')
 
         ## Remember the subdirectories
@@ -88,46 +92,27 @@
                    for f in files if fnmatch.fnmatch(f, pattern) ]
 
     def _add_array(self, name, arr, fieldnames):
-            columns = {}
-            for i, f in enumerate(fieldnames):
-                columns[f] = arr[:,i]
-            setattr(self, name, columns)
-            return columns
+        columns = {'__all__': arr}
+        for i, f in enumerate(fieldnames):
+            columns[f] = arr[:,i]
+        setattr(self, name, columns)
+        return columns
 
     def __getattr__(self, name):
         if name in self.subdirs:
             subdir = ExperimentDirectory(os.path.join(self.expdir, name))
             setattr(self, name, subdir)
             return subdir
-        elif name in self.pmats:
-            pmat = PMat(os.path.join(self.expdir, name+'.pmat'))
-            arr  = pmat.getRows(0, pmat.length)
-            fieldnames = pmat.fieldnames
-            return self._add_array(name, arr, fieldnames)
-        elif name in self.amats:
-            arr, fieldnames = readAMat(os.path.join(self.expdir, name+'.amat'))
-            return self._add_array(name, arr, fieldnames)
-        elif name in self.csv:
-            fname = os.path.join(self.expdir, name+'.csv')
 
-            # Use CSV sniffer to detect presence of header.
-            sniffer = csv.Sniffer()
-            f = open(fname)
-            sample = f.read(1000)
-            has_header = sniffer.has_header(sample)
-            f.seek(0)
+        elif name in self.pmats or name in self.amats or name in self.csv:
+            # minorly ugly, but will do for now...
+            if name in self.pmats: filename = os.path.join(self.expdir, name+'.pmat')
+            if name in self.amats: filename = os.path.join(self.expdir, name+'.amat')
+            if name in self.csv  : filename = os.path.join(self.expdir, name+'.csv' )
 
-            # Load csv into array
-            csv_reader = csv.reader(f)
-            if has_header:
-                fieldnames = csv_reader.next()
-            arr = numarray.array([[float(value) for value in fields] for fields in csv_reader])
-            if not has_header:
-                # Generate fake fieldnames
-                fieldnames = ['field%d' % (i + 1) for i in range(arr.shape[1])]
-                
-            f.close()
+            arr, fieldnames = smartReadMat(filename)
             return self._add_array(name, arr, fieldnames)
+
         else:
             raise ValueError, "ExperimentDirectory '%s' does not contain a component '%s'" \
                   % (self.expdir, name)



From nouiz at mail.berlios.de  Tue Jun 19 21:47:16 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Jun 2007 21:47:16 +0200
Subject: [Plearn-commits] r7605 - in branches/cgi-desjardin:
	commands/PLearnCommands plearn/base plearn/io plearn/sys
Message-ID: <200706191947.l5JJlGkm030199@sheep.berlios.de>

Author: nouiz
Date: 2007-06-19 21:47:15 +0200 (Tue, 19 Jun 2007)
New Revision: 7605

Modified:
   branches/cgi-desjardin/commands/PLearnCommands/ServerCommand.cc
   branches/cgi-desjardin/plearn/base/PrUtils.cc
   branches/cgi-desjardin/plearn/base/TypeTraits.h
   branches/cgi-desjardin/plearn/io/PPath.cc
   branches/cgi-desjardin/plearn/io/PStream.cc
   branches/cgi-desjardin/plearn/io/Poll.h
   branches/cgi-desjardin/plearn/io/PrPStreamBuf.cc
   branches/cgi-desjardin/plearn/io/fileutils.cc
   branches/cgi-desjardin/plearn/io/openFile.cc
   branches/cgi-desjardin/plearn/io/openSocket.cc
   branches/cgi-desjardin/plearn/io/pl_NSPR_io.h
   branches/cgi-desjardin/plearn/sys/Popen.cc
Log:
removed some difference that are not necessarey between the trunk version and this branch


Modified: branches/cgi-desjardin/commands/PLearnCommands/ServerCommand.cc
===================================================================
--- branches/cgi-desjardin/commands/PLearnCommands/ServerCommand.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/commands/PLearnCommands/ServerCommand.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -49,9 +49,9 @@
 #include <plearn/io/pl_log.h>
 #include <plearn/io/PrPStreamBuf.h>
 #include <plearn/base/tostring.h>
-#include <mozilla-1.7.13/nspr/prio.h>
-#include <mozilla-1.7.13/nspr/prerror.h>
-#include <mozilla-1.7.13/nspr/prnetdb.h>
+#include <mozilla/nspr/prio.h>
+#include <mozilla/nspr/prerror.h>
+#include <mozilla/nspr/prnetdb.h>
 
 #ifndef WIN32
 // POSIX includes for getpid() and gethostname()

Modified: branches/cgi-desjardin/plearn/base/PrUtils.cc
===================================================================
--- branches/cgi-desjardin/plearn/base/PrUtils.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/base/PrUtils.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -42,7 +42,7 @@
 
 
 #include "PrUtils.h"
-#include <mozilla-1.7.13/nspr/prerror.h>
+#include <mozilla/nspr/prerror.h>
 
 namespace PLearn {
 using namespace std;

Modified: branches/cgi-desjardin/plearn/base/TypeTraits.h
===================================================================
--- branches/cgi-desjardin/plearn/base/TypeTraits.h	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/base/TypeTraits.h	2007-06-19 19:47:15 UTC (rev 7605)
@@ -50,7 +50,7 @@
 #include <list>
 #include <map>
 #include <set>
-#include <mozilla-1.7.13/nspr/prlong.h>
+#include <mozilla/nspr/prlong.h>
 
 namespace PLearn {
 using std::string;

Modified: branches/cgi-desjardin/plearn/io/PPath.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/PPath.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/io/PPath.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -43,7 +43,7 @@
 // #define PL_LOG_MODULE_NAME "PPath"
 
 #include <ctype.h>
-#include <mozilla-1.7.13/nspr/prenv.h>
+#include <mozilla/nspr/prenv.h>
 
 #include "PPath.h"
 #include "PStream.h"

Modified: branches/cgi-desjardin/plearn/io/PStream.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/PStream.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/io/PStream.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -37,7 +37,7 @@
 #include "NullPStreamBuf.h"
 #include "PrPStreamBuf.h"
 #include <plearn/math/pl_math.h>
-#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla/nspr/prio.h>
 #include <ctype.h>
 
 

Modified: branches/cgi-desjardin/plearn/io/Poll.h
===================================================================
--- branches/cgi-desjardin/plearn/io/Poll.h	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/io/Poll.h	2007-06-19 19:47:15 UTC (rev 7605)
@@ -45,7 +45,7 @@
 #define Poll_INC
 
 #include <vector>
-#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla/nspr/prio.h>
 #include <plearn/io/PStream.h>
 
 

Modified: branches/cgi-desjardin/plearn/io/PrPStreamBuf.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/PrPStreamBuf.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/io/PrPStreamBuf.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -42,7 +42,7 @@
 
 
 #include "PrPStreamBuf.h"
-#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla/nspr/prio.h>
 #include <stdio.h>
 
 namespace PLearn {

Modified: branches/cgi-desjardin/plearn/io/fileutils.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/fileutils.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/io/fileutils.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -65,10 +65,10 @@
 #include <plearn/math/pl_math.h>    //!< For 'real'.
 
 #include <plearn/base/PrUtils.h>
-#include <mozilla-1.7.13/nspr/prio.h>
-#include <mozilla-1.7.13/nspr/prtime.h>
-#include <mozilla-1.7.13/nspr/prerror.h>
-#include <mozilla-1.7.13/nspr/prlong.h>
+#include <mozilla/nspr/prio.h>
+#include <mozilla/nspr/prtime.h>
+#include <mozilla/nspr/prerror.h>
+#include <mozilla/nspr/prlong.h>
 
 namespace PLearn {
 using namespace std;

Modified: branches/cgi-desjardin/plearn/io/openFile.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/openFile.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/io/openFile.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -42,7 +42,7 @@
 
 
 #include "openFile.h"
-#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla/nspr/prio.h>
 #include <plearn/io/fileutils.h>
 #include <plearn/io/PrPStreamBuf.h>
 

Modified: branches/cgi-desjardin/plearn/io/openSocket.cc
===================================================================
--- branches/cgi-desjardin/plearn/io/openSocket.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/io/openSocket.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -45,9 +45,9 @@
 #include <plearn/io/PStream.h>
 #include <plearn/io/PrPStreamBuf.h>
 #include "openSocket.h"
-#include <mozilla-1.7.13/nspr/prio.h>
-#include <mozilla-1.7.13/nspr/prerror.h>
-#include <mozilla-1.7.13/nspr/prnetdb.h>
+#include <mozilla/nspr/prio.h>
+#include <mozilla/nspr/prerror.h>
+#include <mozilla/nspr/prnetdb.h>
 
 
 namespace PLearn {

Modified: branches/cgi-desjardin/plearn/io/pl_NSPR_io.h
===================================================================
--- branches/cgi-desjardin/plearn/io/pl_NSPR_io.h	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/io/pl_NSPR_io.h	2007-06-19 19:47:15 UTC (rev 7605)
@@ -46,7 +46,7 @@
 #ifndef pl_NSPR_io_INC
 #define pl_NSPR_io_INC
 
-#include <mozilla-1.7.13/nspr/prio.h>
+#include <mozilla/nspr/prio.h>
 
 namespace PLearn {
 using namespace std;

Modified: branches/cgi-desjardin/plearn/sys/Popen.cc
===================================================================
--- branches/cgi-desjardin/plearn/sys/Popen.cc	2007-06-19 19:38:00 UTC (rev 7604)
+++ branches/cgi-desjardin/plearn/sys/Popen.cc	2007-06-19 19:47:15 UTC (rev 7605)
@@ -42,9 +42,9 @@
 
 #include "Popen.h"
 
-#include <mozilla-1.7.13/nspr/prio.h>
-#include <mozilla-1.7.13/nspr/prproces.h>
-#include <mozilla-1.7.13/nspr/prenv.h>
+#include <mozilla/nspr/prio.h>
+#include <mozilla/nspr/prproces.h>
+#include <mozilla/nspr/prenv.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/base/PrUtils.h>
 #include <plearn/io/PrPStreamBuf.h>



From nouiz at mail.berlios.de  Tue Jun 19 21:56:56 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Jun 2007 21:56:56 +0200
Subject: [Plearn-commits] r7606 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706191956.l5JJuuEA030718@sheep.berlios.de>

Author: nouiz
Date: 2007-06-19 21:56:56 +0200 (Tue, 19 Jun 2007)
New Revision: 7606

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.h
Log:
Made some variable local if they are used only locally


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc	2007-06-19 19:47:15 UTC (rev 7605)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc	2007-06-19 19:56:56 UTC (rev 7606)
@@ -142,7 +142,7 @@
     MODULE_LOG << "build_() called" << endl;
     if (train_set)
     {
-        for (int iteration = 1; iteration <= 50; iteration++)
+        for (int iteration = 1; iteration <= train_set->width(); iteration++)
         {
             cout << "In Experimentation, Iteration # " << iteration << endl;
             experimentSetUp();
@@ -216,7 +216,7 @@
     if (deletion_thresholds.length() <= 0)
     {
         deletion_thresholds.resize(20);
-        for (header_col = 0; header_col < 20; header_col++) deletion_thresholds[header_col] = (real) to_deal_with_next / 20.0;
+        for (int header_col = 0; header_col < 20; header_col++) deletion_thresholds[header_col] = (real) to_deal_with_next / 20.0;
     } 
     header_width = deletion_thresholds.length();
     header_record.resize(header_width);
@@ -227,7 +227,7 @@
     cout << "choose deletion threshold to experiment with" << endl;
     to_deal_with_total = 0;
     to_deal_with_next = -1;
-    for (header_col = 0; header_col < header_width; header_col++)
+    for (int header_col = 0; header_col < header_width; header_col++)
     {
         if (header_record[header_col] != 0.0) continue;
         to_deal_with_total += 1;
@@ -319,7 +319,7 @@
 { 
     header_record.clear();
     header_names.resize(header_width);
-    for (header_col = 0; header_col < header_width; header_col++) 
+    for (int header_col = 0; header_col < header_width; header_col++) 
         header_names[header_col] = tostring(deletion_thresholds[header_col] + 0.005).substr(0,4);
     header_file = new FileVMatrix(header_file_name, 1, header_names);
     header_file->putRow(0, header_record);
@@ -331,7 +331,7 @@
     if (header_width != header_file->width()) 
         PLERROR("In Experimentation::getHeaderRecord() the existing header file does not match the deletion_thresholds width)");
     header_names = header_file->fieldNames();
-    for (header_col = 0; header_col < header_width; header_col++) 
+    for (int header_col = 0; header_col < header_width; header_col++) 
         if (header_names[header_col] != tostring(deletion_thresholds[header_col] + 0.005).substr(0,4))
             PLERROR("In Experimentation::getHeaderRecord() the existing header file names does not match the deletion_thresholds values)");;
     header_file->getRow(0, header_record);
@@ -390,7 +390,7 @@
 {
     cout << "There is no more variable to deal with." << endl;
     bool missing_results_file = false;
-    for (header_col = 0; header_col < header_width; header_col++)
+    for (int header_col = 0; header_col < header_width; header_col++)
     {
         deletion_threshold = deletion_thresholds[header_col];
         deletion_threshold_str = tostring(deletion_threshold + 0.005).substr(0,4);
@@ -430,7 +430,7 @@
     cout << fixed << showpoint;
     real best_valid_mse_threshold = -1.0;
     real best_valid_mse_value;
-    for (header_col = 0; header_col < header_width; header_col++)
+    for (int header_col = 0; header_col < header_width; header_col++)
     {
         deletion_threshold = deletion_thresholds[header_col];
         deletion_threshold_str = tostring(deletion_threshold + 0.005).substr(0,4);

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.h	2007-06-19 19:47:15 UTC (rev 7605)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.h	2007-06-19 19:56:56 UTC (rev 7606)
@@ -159,7 +159,6 @@
     Vec          target_input;
     TVec<string> target_names;
     
-    int             header_col;
     int             header_width;
     Vec             header_record;
     TVec<string>    header_names;



From dorionc at mail.berlios.de  Tue Jun 19 22:05:26 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Tue, 19 Jun 2007 22:05:26 +0200
Subject: [Plearn-commits] r7607 - trunk/python_modules/plearn/pyplearn
Message-ID: <200706192005.l5JK5Q3W031422@sheep.berlios.de>

Author: dorionc
Date: 2007-06-19 22:05:26 +0200 (Tue, 19 Jun 2007)
New Revision: 7607

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
Now allows to pass arbitrary python lists on command-line ([ ==> eval())


Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-19 19:56:56 UTC (rev 7606)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-19 20:05:26 UTC (rev 7607)
@@ -228,8 +228,12 @@
     The element cast is made using the I{elem_cast} argument.
     """
     # CSV (with or without brackets)
+    slist = slist.strip()
     if isinstance(slist,str):
-        slist = slist.lstrip(' [').rstrip(' ]')
+        if slist.startswith("["):
+            assert slist.endswith("]")
+            return eval(slist)
+        
         if slist=="":
             return []
         return [ elem_cast(e) for e in slist.split(",") ]



From simonl at mail.berlios.de  Tue Jun 19 22:20:04 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Tue, 19 Jun 2007 22:20:04 +0200
Subject: [Plearn-commits] r7608 - in trunk:
	plearn_learners/generic/EXPERIMENTAL
	python_modules/plearn/plotting python_modules/plearn/var
	scripts scripts/EXPERIMENTAL
Message-ID: <200706192020.l5JKK4K6000117@sheep.berlios.de>

Author: simonl
Date: 2007-06-19 22:20:02 +0200 (Tue, 19 Jun 2007)
New Revision: 7608

Added:
   trunk/scripts/EXPERIMENTAL/
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/var/Var.py
Log:
Many improvements to deepreconstructorenet stuff


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-19 20:20:02 UTC (rev 7608)
@@ -102,6 +102,10 @@
                   OptionBase::buildoption,
                   "reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]");
 
+    declareOption(ol, "reconstruction_optimizers", &DeepReconstructorNet::reconstruction_optimizers,
+                  OptionBase::buildoption,
+                  "");
+
     declareOption(ol, "reconstruction_optimizer", &DeepReconstructorNet::reconstruction_optimizer,
                   OptionBase::buildoption,
                   "");
@@ -153,7 +157,16 @@
                    ArgDoc("varname", "name of the variable searched for"),
                    RetDoc("Returns the value of the parameter as a Mat")));
 
+    declareMethod(rmm,
+                  "getParameterRow",
+                  &DeepReconstructorNet::getParameterRow,
+                  (BodyDoc("Returns the matValue of the parameter variable with the given name"),
+                   ArgDoc("varname", "name of the variable searched for"),
+                   ArgDoc("n", "row number"),
+                   RetDoc("Returns the nth row of the value of the parameter as a Mat")));
 
+
+
     declareMethod(rmm,
                   "listParameterNames",
                   &DeepReconstructorNet::listParameterNames,
@@ -179,6 +192,35 @@
                   (BodyDoc("Compute the reconstructions of the input of each hidden layer"),
                    ArgDoc("input", "the input"),
                    RetDoc("The reconstructions")));
+
+    declareMethod(rmm,
+                   "getMatValue",
+                   &DeepReconstructorNet::getMatValue,
+                   (BodyDoc(""),
+                    ArgDoc("layer", "no of the layer"),
+                    RetDoc("the matValue")));
+
+    declareMethod(rmm,
+                   "setMatValue",
+                   &DeepReconstructorNet::setMatValue,
+                   (BodyDoc(""),
+                    ArgDoc("layer", "no of the layer"),
+                    ArgDoc("values", "the values")));
+
+    declareMethod(rmm,
+                   "fpropOneLayer",
+                   &DeepReconstructorNet::fpropOneLayer,
+                   (BodyDoc(""),
+                    ArgDoc("layer", "no of the layer"),
+                    RetDoc("")));
+
+
+    declareMethod(rmm,
+                   "reconstructOneLayer",
+                   &DeepReconstructorNet::reconstructOneLayer,
+                   (BodyDoc(""),
+                    ArgDoc("layer", "no of the layer"),
+                    RetDoc("")));
 }
 
 void DeepReconstructorNet::build_()
@@ -256,6 +298,7 @@
     deepCopyField(layers, copies);
     deepCopyField(reconstruction_costs, copies);
     deepCopyField(reconstructed_layers, copies);
+    deepCopyField(reconstruction_optimizers, copies);
     deepCopyField(reconstruction_optimizer, copies);
     varDeepCopyField(target, copies);
     deepCopyField(supervised_costs, copies);
@@ -562,8 +605,17 @@
     //displayFunction(f,false,false, 333, "train_func");
     Var totalcost = sumOf(inputs, f, minibatch_size);
     VarArray params = totalcost->parents();
-    reconstruction_optimizer->setToOptimize(params, totalcost);
-    reconstruction_optimizer->reset();
+    
+    if ( reconstruction_optimizers.size() !=0 )
+    {
+        reconstruction_optimizers[which_input_layer]->setToOptimize(params, totalcost);
+        reconstruction_optimizers[which_input_layer]->reset();    
+    }
+    else 
+    {
+        reconstruction_optimizer->setToOptimize(params, totalcost);
+        reconstruction_optimizer->reset();    
+    }
 
     TVec<string> colnames(4);
     colnames[0] = "nepochs";
@@ -579,8 +631,16 @@
     for(int n=0; n<nepochs.first || (n<nepochs.second && relative_improvement >= min_improvement); n++)
     {
         st.forget();
-        reconstruction_optimizer->nstages = l/minibatch_size;
-        reconstruction_optimizer->optimizeN(st);
+        if ( reconstruction_optimizers.size() !=0 )
+        {
+            reconstruction_optimizers[which_input_layer]->nstages = l/minibatch_size;
+            reconstruction_optimizers[which_input_layer]->optimizeN(st);
+        }
+        else 
+        {
+            reconstruction_optimizer->nstages = l/minibatch_size;
+            reconstruction_optimizer->optimizeN(st);
+        }        
         const StatsCollector& s = st.getStats(0);
         real m = s.mean();
         real er = s.stderror();
@@ -634,6 +694,16 @@
     return Mat(0,0);
 }
 
+
+Vec DeepReconstructorNet::getParameterRow(const string& varname, int n)
+{
+    for(int i=0; i<parameters.length(); i++)
+        if(parameters[i]->getName() == varname)
+            return parameters[i]->matValue(n);
+    PLERROR("There is no parameter  named %s", varname.c_str());
+    return Vec(0);
+}
+
 TVec<string> DeepReconstructorNet::listParameterNames()
 {
     TVec<string> nameListe(0);
@@ -652,6 +722,31 @@
 }
 
 
+Mat DeepReconstructorNet::getMatValue(int layer)
+{
+    return layers[layer]->matValue;
+}
+
+void DeepReconstructorNet::setMatValue(int layer, Mat values)
+{
+    layers[layer]->matValue << values;
+}
+
+Mat DeepReconstructorNet::fpropOneLayer(int layer)
+{
+    VarArray proppath = propagationPath( layers[layer], layers[layer+1] );
+    proppath.fprop();
+    return getMatValue(layer+1);
+}
+
+Mat DeepReconstructorNet::reconstructOneLayer(int layer)
+{
+    VarArray proppath = propagationPath(layers[layer],reconstructed_layers[layer-1]);
+    proppath.fprop();       
+    return reconstructed_layers[layer-1]->matValue;
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-19 20:20:02 UTC (rev 7608)
@@ -86,6 +86,10 @@
     // reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]
     VarArray reconstructed_layers;
 
+    // optimizers if we use different ones for each layer
+    TVec< PP<Optimizer> > reconstruction_optimizers;
+    
+    // if we use always the same optimizer
     PP<Optimizer> reconstruction_optimizer;
 
 
@@ -172,6 +176,9 @@
     //! Returns the matValue of the parameter variable with the given name
     Mat getParameterValue(const string& varname);
 
+    //! Returns the nth row of the matValue of the parameter variable with the given name
+    Vec getParameterRow(const string& varname, int n);
+
     //! Returns a list of the names of the parameters (in the same order as in listParameter)
     TVec<string> listParameterNames();
 
@@ -188,6 +195,12 @@
     void reconstructInputFromLayer(int layer);
     TVec<Mat> computeReconstructions(Mat input);
 
+    Mat getMatValue(int layer);
+    void setMatValue(int layer, Mat values);
+    Mat fpropOneLayer(int layer);
+    Mat reconstructOneLayer(int layer);
+       
+
     
 
     // *** SUBCLASS WRITING: ***

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-06-19 20:20:02 UTC (rev 7608)
@@ -1,4 +1,5 @@
 from pylab import *
+from numarray import *
 
 
 #################
@@ -149,61 +150,65 @@
     #custom color bar
     customColorBar(mi,ma,(1.-cbw-sbi, sbi, sbi, 1.-2.*cbw))
     return toReturn
-        
 
-        #1 sur 2 -
-        
-        #subplot(subPlotHeight, subPlotWidth, (2+i%2)*subPlotWidth + j + 1)
-        #axes((i*axesWidth, 2*axesHeight, axesWidth, axesHeight))
-        #imshow(rowToMatrix(toMinusRow(row),width), interpolation="nearest", cmap = colorMap)
-        #setPlotParams(str(i), False, True)
-        
-        #1 sur 2 +
-        
-        #subplot(subPlotHeight, subPlotWidth, (4+i%2)*subPlotWidth + i + 1)
-        #axes((i*axesWidth, 4*axesHeight, axesWidth, axesHeight))
-        #imshow(rowToMatrix(toPlusRow(row),width), interpolation="nearest", cmap = colorMap)
-        # setPlotParams(str(i), False, True)
-            
 
 
 
-def plotMatrices(matrices, same_color_bar = False, space_between_matrices = 5):
+def plotMatrices(matrices, names = None, ticks = False, same_color_bar = False, space_between_matrices = 5):
     '''plot matrices from left to right
     TODO : same_color_bar does nothing !!
     '''
+    
     colorMap = cm.gray
     nbMatrices = len(matrices)
-    #print 'plotting ' + str(nbMatrices) + ' matrices'
+    print 'plotting ' + str(nbMatrices) + ' matrices'
 
     totalWidth = 0
     maxHeight = 0
     
     for matrix in matrices:
-        if len(matrix) > maxHeight:
-            maxHeight = len(matrix)
-        totalWidth += len(matrix[0])
+        #print matrix.info()
+        if matrix.shape[0] > maxHeight:
+            maxHeight = matrix.shape[0]
+        totalWidth += matrix.shape[1]
+    print maxHeight, totalWidth
+    
 
+    #to prevent a little bug   
+    space_between_matrices = min(space_between_matrices, maxHeight, totalWidth)
 
-    unit = min(1./((nbMatrices+1)*space_between_matrices + totalWidth), 1./(maxHeight-2.*space_between_matrices))
+    unit = min(1./((nbMatrices+1)*space_between_matrices + totalWidth), 1./(maxHeight+2.*space_between_matrices))
     sbm = space_between_matrices*unit
 
+
     x=sbm
     the_axes = []
-    for matrix in matrices:
+    
+    if names != None:
+        if len(names) != len(matrices):
+            raise Exception, "nb of matrices and nb of names must be equals in plotMatrices()"
+    else:
+        names = ['']*len(matrices)
         
-        h = len(matrix)*unit
-        w = len(matrix[0])*unit
+    for matrix,name in zip(matrices,names):
+        
+        h = matrix.shape[0]*unit
+        w = matrix.shape[1]*unit
+
         if h>1 :
             h = 1.-2*sbm
         bottom = (1.-h)/2.
 
+        #print x,bottom, w, h
         temp = axes(( x,bottom, w,h))
         the_axes.append(temp)
         imshow(matrix, interpolation = 'nearest', cmap = colorMap)
+        title(name)       
+        if ticks == False:
+            xticks([],[])
+            yticks([],[])
         colorbar()
         x += w+sbm
-
     return the_axes
 
 

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/python_modules/plearn/var/Var.py	2007-06-19 20:20:02 UTC (rev 7608)
@@ -164,7 +164,8 @@
     Then output = sigmoid(input.W^T + b)
     
     Returns a triple (hidden, reconstruciton_cost, reconstructed_input)"""
-    W = Var(ow,iw,"uniform", -1./sqrt(iw), 1./sqrt(iw), varname=basename+'_W')
+    ra = 1./max(iw,ow)
+    W = Var(ow,iw,"uniform", -ra, 1./ra, varname=basename+'_W')
     
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')        
@@ -183,10 +184,12 @@
     """iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output"""
-    M = Var(ow/ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_M")
+    ra = 1./max(iw,ow)
+    sqra = sqrt(ra)
+    M = Var(ow/ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_M")
     if constrain_mask:
         M = M.sigmoid()
-    W = Var(ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_W")
+    W = Var(ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_W")
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')
         hidden = input.doubleProduct(W,M).add(b).multiSoftMax(ogs)
@@ -203,11 +206,13 @@
     """iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output"""
-    M = Var(ow/ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_M")
-    W = Var(ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_W")
+    ra = 1./max(iw,ow)
+    sqra = sqrt(ra)
+    M = Var(ow/ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_M")
+    W = Var(ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_W")
     hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
-    Mr = Var(iw/igs, ow, "uniform", -1./ow, 1./ow, False, varname=basename+"_Mr")
-    Wr = Var(igs, ow, "uniform", -1./ow, 1./ow, False, varname=basename+"_Wr")
+    Mr = Var(iw/igs, ow, "uniform", -sqra, sqra, False, varname=basename+"_Mr")
+    Wr = Var(igs, ow, "uniform", -sqra, sqra, False, varname=basename+"_Wr")
     # TODO: a repenser s'il faut un transpose ou non
     log_reconstructed = hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
@@ -218,9 +223,11 @@
     """iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output"""
-    M = Var(ow/ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_M")
-    W = Var(ogs, iw, "uniform", -1./iw, 1./iw, False, varname=basename+"_W")
-    Wr = Var(ow, iw, "uniform", -1./ow, 1./ow, varname=basename+'_Wr')
+    ra = 1./max(iw,ow)
+    sqra = sqrt(ra)
+    M = Var(ow/ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_M")
+    W = Var(ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_W")
+    Wr = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_Wr')
 
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')
@@ -236,7 +243,8 @@
     return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=""):
-    W = Var(ow, iw, "uniform", -1./iw, 1./iw, varname=basename+'_W')
+    ra = 1./max(iw,ow)
+    W = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_W')
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')
         hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)
@@ -250,8 +258,9 @@
     return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=""):
-    W = Var(ow, iw, "uniform", -1./iw, 1./iw, varname=basename+'_W')
-    Wr = Var(ow, iw, "uniform", -1./ow, 1./ow, varname=basename+'_Wr')
+    ra = 1./max(iw,ow)
+    W = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_W')
+    Wr = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_Wr')
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')
         hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)

Added: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-06-19 20:20:02 UTC (rev 7608)
@@ -0,0 +1,789 @@
+#!/usr/bin/env python
+
+import sys
+from pylab import *
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting.netplot import *
+from numarray import *
+from numarray.random_array import *
+import numpy.random
+
+
+################
+### methods ###
+################
+
+def print_usage_and_exit():
+    print "Usage: drnPlot <task> <file> [<other arguments>]",    
+    "drnPlot plotSingleMatrix x.psave ",
+    "drnPlot plotEachRow learner.psave chars.pmat"
+    "drnPlot plotRepAndRec learner.psave chars.pmat"
+    sys.exit()
+
+
+def appendMatrixToFile(file, matrix, matrix_name=""):
+    file.write("\n\n" + matrix_name + ' ('+ str(len(matrix)) + 'x' + str(len(matrix[0])) + ')\n\n')
+    for i, row in enumerate(matrix):
+        file.write('[')
+        for j, el in enumerate(row):
+            file.write(str(el) + ', ')
+        file.write(']\n')
+        
+
+class HiddenLayer:
+    
+    def __init__(self,hidden_Layer, groupsize):
+        self.hidden_layer = hidden_Layer
+        self.groupsize = groupsize
+
+        self.max_height = 150
+
+        #if it's too tall, we do this little tweak
+        gs = self.groupsize
+        height = self.hidden_layer.size()/gs
+        self.nbgroups = 1
+        while height > self.max_height:
+            self.nbgroups+=1
+            while (self.hidden_layer.size()/gs)%self.nbgroups != 0 :
+                self.nbgroups+=1                       
+            height = self.hidden_layer.size()/gs/self.nbgroups
+        print 'nbgroups', self.nbgroups
+        
+    def getMatrix(self):        
+        return reshape(self.hidden_layer, (-1,self.groupsize*self.nbgroups))
+    
+    def matrixToLayer(self, x, y):
+        gs = self.groupsize
+        x,y = self.correctXY(x,y)
+        return x + gs*self.nbgroups*y
+
+    def correctXY(self,x,y):
+        return int(x + .4), int(y + .3)        
+
+    def getElement(self, x, y):
+        n = self.matrixToLayer(x,y)
+        return self.hidden_layer[n]
+
+    def setElement(self, x,y, value):
+        n = self.matrixToLayer(x,y)
+        self.hidden_layer[n] = value
+
+    def getRow(self,n):
+        return self.hidden_layer[n*self.groupsize:(n+1)*self.groupsize]
+
+    def setRow(self,n,row):
+        c=0
+        for i in arange(n*self.groupsize,(n+1)*self.groupsize):
+           self.hidden_layer[i] = row[c]
+           c+=1
+
+
+
+class InteractiveRepRecPlotter:
+    
+    def __init__(self, learner, vmat, image_width=28, char_indice=0):
+        '''constructor'''
+        self.current = char_indice-1#-1 because it's the first time
+        self.learner = learner
+        self.vmat = vmat
+        self.char = -1#the char we're looking for, -1 for anyone (it can be -1,0,1,2,3,4,5,6,7,8,9)
+        self.image_width = image_width
+
+        self.fig_rec = 0
+        self.fig_rep = 1
+        figure(self.fig_rec)
+        figure(self.fig_rep)
+
+        #self.current_fig = None
+        #self.current_axes = None
+        self.current_hl = None#current hidden layer
+
+        #plotting constants
+        self.interpolation = 'nearest'
+        self.cmap = cm.gray
+
+        self.plotNext()#starting with a plot...
+        self.__linkEvents()
+               
+
+
+    def size(self):
+        return len(self.originals_hl)
+        
+
+    ###
+    ### getting char from vmat
+    ###
+    
+    def __rowToClassInput(self, row):
+        '''we put the last element of row in self.classe, the rest in self.input'''
+        self.classe = row[-1:][0]
+        self.input = row[:-1]
+
+    def __getNextChar(self):
+        '''get next input row from the vmat'''        
+        while True:
+            self.current+=1
+            raw_input = vmat.getRow(self.current)
+            classe = int(raw_input[-1:])
+            if classe == self.char or self.char == -1:
+                break            
+        self.__rowToClassInput(raw_input)
+
+    def __getPrevChar(self):
+        '''get next input row from the vmat'''        
+        while True:
+            self.current-=1
+            raw_input = vmat.getRow(self.current)
+            classe = int(raw_input[-1:])
+            if classe == self.char or self.char == -1:
+                break            
+        self.__rowToClassInput(raw_input)
+
+    ###
+    ### computings
+    ###
+
+    def __computeRepresentation(self):
+        
+        # we convert list to tmat
+        imagetmat = TMat([self.input])
+        
+        #representation
+        print 'computing representations...'
+        raw_rep = learner.computeRepresentations(imagetmat)        
+        print '...done.'
+
+        try:
+            #groupsizes = self.learner.groupsizes[1:]
+            groupsizes = list(self.learner.getOption('group_sizes')[1:])
+        except:
+            groupsizes = [10,20,40]
+        if groupsizes == []:
+            groupsizes = [10, 20, 40]
+        groupsizes.insert(0, self.image_width)
+        groupsizes.append(1)
+
+        print groupsizes
+
+        self.hidden_layers = []
+
+        for gs,el in zip(groupsizes,raw_rep):
+            self.hidden_layers.append(HiddenLayer(el[0], gs))
+
+    def __computeReconstructions(self):
+
+        imagetmat = TMat([self.input])
+        print 'computing reconstructions...'
+        rec = learner.computeReconstructions(imagetmat)
+        print '...done.'
+
+        matrices = [rowToMatrix(self.input, self.image_width)]
+        for el in rec:
+            row = el[0]
+            matrices.append(rowToMatrix(row,self.image_width))
+
+        self.reconstructions = matrices
+       
+
+    ###
+    ### events
+    ###
+
+        
+    def __changeChar(self, event):
+        char = event.key
+        if char in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']:
+            self.char = int(char)
+            print 'now plotting only this digit :', char
+        elif char == '.':
+            self.char = -1
+            print 'now plotting any digit'
+        elif char == 'right':
+            self.plotNext()
+        elif char == 'left':
+            self.plotPrev()
+        elif char == '':
+            pass            
+
+
+    def __repCommands(self,event):
+
+        #met a jour self.current_hl
+        self.__findCurrentLayer(event)
+        
+        char = event.key        
+        i = self.current_hl
+               
+        if i != -1:
+
+            #commun
+
+            hl1 = self.hidden_layers[i]
+            if i >  0:
+                hl0 = self.hidden_layers[i-1]
+            if i < self.size()-1:
+                hl2 = self.hidden_layers[i+1]
+
+            hl = hl1
+                        
+            # fprop -- f
+
+            if char == 'f':                
+                print 'fproping...'
+                #update                
+                self.learner.setMatValue(i-1, reshape(hl0.hidden_layer, (1,-1)))
+                #fprop
+                row = self.learner.fpropOneLayer(i-1)[0]
+                #print
+                hl1.hidden_layer = row
+                self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                print '...done'
+
+            # big-fprop -- F
+
+            elif char == 'F':
+                print 'big-fproping...'                
+                for k in arange(i-1, self.size()-1):
+                    print 'k',k
+                    self.learner.setMatValue(k, reshape(self.hidden_layers[k].hidden_layer, (1,-1)))
+                    row = self.learner.fpropOneLayer(k)[0]
+                    self.hidden_layers[k+1].hidden_layer = row
+                    self.rep_axes[k+1].imshow(self.hidden_layers[k+1].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                    draw()
+                print '...done'
+                
+            # reconstruction -- r
+                
+            elif char == 'r':
+                print 'reconstructing...'
+                self.learner.setMatValue(i+1, reshape(hl2.hidden_layer, (1,-1)))
+                #reconstruct
+                row = self.learner.reconstructOneLayer(i+1)[0]
+                #HACK
+                if len(row) == 28*28*2:
+                    print 'hacking...'
+                    row = array(toMinusRow(row))
+                #print the new layer
+                hl1.hidden_layer = row
+                self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)                 
+                draw()
+                print '...done'
+
+            # big-reconstruction -- R
+
+            elif char == 'R':
+                print 'big-reconstructing...'
+                for k in arange(i+1, 0, -1):
+                    self.learner.setMatValue(k, reshape(self.hidden_layers[k].hidden_layer, (1,-1)))
+                    row = self.learner.reconstructOneLayer(k)[0]
+                    #HACK
+                    if len(row) == 28*28*2:
+                        print 'hacking...'
+                        row = array(toMinusRow(row))
+                    self.hidden_layers[k-1].hidden_layer = row
+                    self.rep_axes[k-1].imshow(self.hidden_layers[k-1].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                    draw()
+                print '...done'                
+
+            # max -- m
+
+            elif char == 'm':
+                print 'maximum...'
+
+                for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
+
+                    row = hl.getRow(n)
+
+                    #finding out the max
+                    indmax = 0
+                    for el in arange(1,len(row)):
+                        if row[el] > row[indmax]:                            
+                            indmax = el                    
+
+                    #set max = 1, other = 0
+                    for el in arange(len(row)):
+                        if el == indmax:
+                            row[el] = 1.
+                        else:
+                           row[el] = 0.
+
+                    hl.setRow(n,row)
+                            
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                
+                print '...done'
+
+            # sampling -- s
+
+            elif char == 's':
+                print 'sampling...'
+                for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
+                    print 'sum', hl.getRow(n).sum()
+                    multi = numpy.random.multinomial(1,hl.getRow(n))                    
+                    hl.setRow(n,multi)                            
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                print '...done'
+                
+            # set pixel -- z,x,c,v,b
+
+            elif char in ['z', 'x', 'c', 'v', 'b']:
+                
+                x,y = event.xdata, event.ydata
+                
+                if char == 'z':
+                    hl.setElement(x,y,0.)
+                elif char == 'x':
+                    hl.setElement(x,y,.25)
+                elif char == 'c':
+                    hl.setElement(x,y,.5)
+                elif char == 'v':
+                    hl.setElement(x,y,.75)
+                elif char == 'b':
+                    hl.setElement(x,y,1.)
+
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                                            
+            # infos -- ' '
+
+            elif char == ' ':
+                x,y = event.xdata, event.ydata
+                print
+                print 'Position', hl.matrixToLayer(x,y), '(', x, y, ')'
+                print 'Value', hl.getElement(x,y)                
+
+            # plot W and M -- w
+
+            elif char == 'w':
+
+                prefixe = 'Layer' + str(i)
+                nameW = prefixe + '_W'
+                nameWr = nameW + 'r'
+                nameM = prefixe + '_M'
+                nameMr = nameM + 'r'
+                nameB = prefixe + '_b'
+                nameBr = nameB + 'r'
+             
+                listNames = learner.listParameterNames()
+
+                matricesToPlot = []
+                namesToPlot = []
+
+                x,y = hl.correctXY(event.xdata,event.ydata)
+                n = hl.matrixToLayer(x, y)
+                
+                
+                if nameW in listNames and nameM not in listNames:
+                    
+                    row = learner.getParameterRow(nameW,n)
+                    
+                    #HACK !!!
+                    print 'just hacked...'
+                    if i==1 and len(row) == 28*28*2:
+                        row = array(toMinusRow(list(row)))
+                    #END OF HACK
+
+                    matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                    namesToPlot.append(nameW)
+
+                    if nameWr in listNames:
+
+                        row = learner.getParameterRow(nameWr,n)
+                    
+                        #HACK !!!
+                        print 'just hacked...'
+                        if i==1 and len(row) == 28*28*2:
+                            row = array(toMinusRow(list(row)))
+                        #END OF HACK 
+                        
+                        matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                        namesToPlot.append(nameWr)
+
+                    figure(3)
+                    clf()
+                    plotMatrices(matricesToPlot, namesToPlot)
+                    draw()
+
+                if nameW in listNames and nameM in listNames:
+
+                    rowW = learner.getParameterRow(nameW,x)
+                    rowM = learner.getParameterRow(nameM,y)
+                    
+                    #HACK !!!
+                    #print 'just hacked...'
+                    #if i==1 and len(row) == 28*28*2:
+                    #    row = array(toMinusRow(row))
+                    #END OF HACK
+                                      
+                    rowW = reshape(rowW, (-1,self.hidden_layers[i-1].groupsize))
+                    rowM = reshape(rowM,(-1,self.hidden_layers[i-1].groupsize))
+
+                    #TODO: rajouter les deux cas  : juste un Wr et lautre : Mr ET Wr
+
+                    produit = rowW*rowM                   
+
+                    figure(3)
+                    clf()                  
+                    plotMatrices([rowW,rowM,produit], [nameW,nameM, 'term-to-term product'])
+                    draw()
+
+                #BIAS
+
+                if nameB in listNames:
+
+                    row = learner.getParameterValue(nameB)
+                    print nameB,row.shape
+
+                    print 'i',i
+                    matricesToPlot = [reshape(row, (-1, self.hidden_layers[i].groupsize))]
+                                           
+                    namesToPlot = [nameB]
+
+                    if nameBr in listNames:
+
+                        row = learner.getParameterValue(nameBr)
+                        print nameBr,row.shape
+
+                        #HACK !!!
+                        print i, row.shape[1]
+                        if i==1 and row.shape[1] == 28*28*2:
+                            row = array(toMinusRow(list(row[0])))
+                            print 'just hacked...'
+                        #END OF HACK
+
+                        matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                        
+                        namesToPlot.append(nameBr)
+
+                    figure(4)
+                    clf()
+                    plotMatrices(matricesToPlot, namesToPlot)
+                    draw()
+                
+
+            # back to Original -- o
+            
+            elif char == 'o':
+                print 'getting original layer...'                
+                self.hidden_layers[i].hidden_layer = copy.copy(self.originals_hl[i].hidden_layer)
+                self.rep_axes[i].imshow(self.hidden_layers[i].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                print '...done'
+                
+       
+
+        # big-back to Original -- O
+        if char == 'O':
+            print 'getting all original layers...'
+            for k in arange(0,self.size()):
+                self.hidden_layers[k].hidden_layer = copy.copy(self.originals_hl[k].hidden_layer)
+                self.rep_axes[k].imshow(self.hidden_layers[k].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+            print '...done'
+        
+
+
+    def __clicked(self, event):
+            
+        self.__findCurrentLayer(event)
+        
+        print 'current hidden layer is now', self.current_hl
+
+        if event.key == 'control' and self.current_hl != -1:
+            
+            hidden_layer = self.hidden_layers[self.current_hl]
+            
+            n = hidden_layer.matrixToLayer(x,y)
+           # print 'n', n
+            hidden_layer.hidden_layer[n] = 1 - hidden_layer.hidden_layer[n]
+            axes.imshow(hidden_layer.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)        
+            draw()
+
+        if event.button == 3:
+            print 'Layer', self.current_hl, ', position (',x,y,'), value', xself.hidden_layers[self.current_hl].getElement(x,y)
+
+    def __findCurrentLayer(self, event):
+
+        fig = event.canvas.figure.number
+        axes = event.inaxes
+
+        self.current_hl = -1
+        if axes != None:
+            figure(fig)
+            
+            #we find to which hidden layer corresponds our axes
+            for i,a in enumerate(self.rep_axes):                
+                if a == axes:
+                    self.current_hl = i
+    
+
+    def __linkEvents(self):
+
+        figure(self.fig_rep)
+        connect('key_press_event', self.__changeChar)
+        #connect('button_press_event', self.__clicked)
+        connect('key_press_event', self.__repCommands)
+
+        figure(self.fig_rec)
+        connect('key_press_event', self.__changeChar)        
+        #connect('button_press_event', self.__clicked)        
+        
+    ###
+    ### plotting
+    ###
+
+    def __plotReconstructions(self):
+        print 'plotting reconstructions...'
+        figure(self.fig_rec)
+        clf()
+        plotMatrices(self.reconstructions)
+        draw()
+        print '...done.'
+
+    def __plotRepresentations(self):
+        print 'plotting representations...'
+        figure(self.fig_rep)
+        clf()
+        temp = []
+        for x in self.hidden_layers:
+            temp.append( x.getMatrix() )        
+        draw()
+
+        self.rep_axes = plotMatrices(temp)
+        print '...done.'
+
+    def __computeAndPlot(self):        
+        self.__computeRepresentation()
+        self.__computeReconstructions()
+        self.__plotReconstructions()
+        self.__plotRepresentations()
+
+    def plotNext(self):
+        self.__getNextChar()
+        self.__computeAndPlot()
+
+        #saving "original" matrices
+        #print 'copying originals'
+        self.originals_hl = copy.deepcopy(self.hidden_layers)
+        
+        print 'a', self.classe, 'was plotted'
+        
+
+    def plotPrev(self):
+        self.__getPrevChar()
+        self.__computeAndPlot()
+
+        #saving "original" matrices
+        #print 'copying originals'
+        self.originals_hl = copy.deepcopy(self.hidden_layers)
+        
+        print 'a', self.classe, 'was plotted'
+    
+            
+    def plotOld(self):
+
+        print 'entered in plot method'
+               
+
+        #reconstruction
+        print 'computing reconstructions...'
+        rec = learner.computeReconstructions(imagetmat)
+        
+        print 'executing some matrix manipulations...'
+        row = raw_rep[0][0]
+        
+        image = rowToMatrix(row,28)
+        
+        listDeMatrices = [image]        
+        for el in raw_rep[1:]:
+            listDeMatrices.append(rowToMatrix(el[0],max(len(el[0])/100.,1), False))
+
+        listDeMatrices2 = [image]
+        for el in rec:
+            row = el[0]
+            listDeMatrices2.append(rowToMatrix(row,28))
+        
+    
+        print 'plotting'
+        #plotting
+        figure(self.nofig)
+        clf()
+        plotMatrices(listDeMatrices)        
+        draw()
+
+        figure(self.nofig+1)
+        clf()
+        plotMatrices(listDeMatrices2)
+        draw()       
+
+        self.i+=1
+
+        if self.log_file != "":
+            file = open(self.log_file,'a')
+
+            file.write("\n\n\n\n\n\n--------------------------------------------------------------------")
+            file.write("---------- REP ------------------------------------------------------\n")
+
+
+            appendMatrixToFile(file, image, "input")
+            
+            for i,mat in enumerate(raw_rep[1:]):
+                appendMatrixToFile(file, mat, 'rep of hidden layer ' + str(i+1))
+
+
+            file.write("\n\n\n---------- REC ------------------------------------------------------\n")            
+            for i, mat in enumerate(rec):
+                appendMatrixToFile(file,  rowToMatrix(mat[0],28), 'rec of hidden layer ' + str(i+1))
+            
+
+
+class EachRowPlotter:
+    
+    def __init__(self, matrix, width = 28, plot_width = .1, space_between_images = .01, do_to_rows = None, i=0, nofig=0):
+        self.matrix = matrix
+        self.i = i
+        self.nofig = nofig
+        self.do_to_rows = do_to_rows
+        self.last_element = -1
+        self.plot_width = plot_width
+        self.sbi = space_between_images
+        self.width = width
+
+    def plotNext(self, event):
+        clf()
+        if self.last_element > 0:
+            self.last_element = plotLayer1(matrix, self.width, self.plot_width, self.last_element+1, -1, self.sbi, doToRow)
+        else:
+            self.last_element = plotLayer1(matrix, self.width, self.plot_width, 0, -1, self.sbi, doToRow)
+        draw()
+
+    def plot(self):        
+        print 'Plotting matrix ' +  matrixName + ' (' + str(len(matrix)) + 'x' + str(len(matrix[0])) + ')'        
+        self.last_element = plotLayer1(matrix,self.width, self.plot_width, 0, -1, self.sbi, self.do_to_rows)                   
+        connect("button_press_event",self.plotNext)
+        
+    
+
+        
+
+############
+### main ###
+############
+
+server_command = "slearn server"
+serv = launch_plearn_server(command = server_command)
+
+#print "Press Enter to continue"
+#raw_input()
+
+if len(sys.argv)<2:
+    print_usage_and_exit()
+
+task = sys.argv[1]
+
+def openVMat(vmatspec):
+    if vmatspec.endswith(".amat") or vmatspec.endswith(".pmat"):
+        vmat = serv.new('AutoVMatrix(specification ="'+vmatspec+'");')
+    else:
+        vmat = serv.load(vmatspec)
+    return vmat
+
+if task == 'plotEachRow':
+
+    psave = sys.argv[2]
+    
+    learner = serv.load(psave)
+        
+    matrices = learner.listParameter() 
+    names = learner.listParameterNames()
+    
+    #doToRow = None
+    doToRow = toPlusRow
+    #doToRow = toMinusRow
+
+    matrixName = ''
+    while matrixName != 'exit':
+        print
+        print 'Matrix list :'
+        print names
+
+        print
+        matrixName = raw_input('Choose a matrix to be plotted (or \'exit\')>>>')
+        
+        if matrixName in names:
+
+            matrix = learner.getParameterValue(matrixName)
+            #matrix = rand(500,28*28*2)
+            plotter = EachRowPlotter(matrix, 28, .1, .01, doToRow)
+            plotter.plot()
+            show()          
+            
+        elif matrixName != 'exit':
+            print
+            print 'This matrix does not exist !'
+
+
+elif task == 'plotRepAndRec':
+    
+    psave = sys.argv[2]
+    datafname = sys.argv[3]
+    #test = sys.argv[5]
+    #print test
+   
+    #loading learner
+    learner = serv.load(psave)
+    
+    #taking an input
+    vmat = openVMat(datafname)
+    
+    matrix_plot = InteractiveRepRecPlotter(learner, vmat)
+    
+    show()
+
+    
+elif task == 'plotSingleMatrix':
+
+    psave = sys.argv[2]
+
+    learner = serv.load(psave)
+    
+    nameList = learner.listParameterNames()
+
+    matrixName = ''
+    while matrixName != 'exit':
+        print
+        print 'Matrix list :'
+        print nameList
+
+        print
+        matrixName = raw_input('Choose a matrix to be plotted (or \'exit\')>>>')
+        
+        if matrixName in nameList:
+            matrix = learner.getParameterValue(matrixName)
+            figure(0)
+            #cadre = .05
+            #axes((cadre,cadre,1.-2*cadre,1-2*cadre))
+            #imshow(matrix, interpolation = defaultInterpolation, cmap = defaultColorMap)
+            #setPlotParams(matrixName, True, True)            
+            #figure(1, figsize=(1000,1000), dpi=40)
+            truncate_imshow(matrix)
+            show()
+            
+        elif matrixName != 'exit':
+            print
+            print 'This matrix does not exist !'
+
+elif task == 'test':
+    #jutilise cet endroit pour faire des tests
+
+    pass
+
+
+
+else:
+    print_usage_and_exit()


Property changes on: trunk/scripts/EXPERIMENTAL/deepnetplot.py
___________________________________________________________________
Name: svn:executable
   + *



From louradou at mail.berlios.de  Tue Jun 19 22:31:29 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 19 Jun 2007 22:31:29 +0200
Subject: [Plearn-commits] r7609 - trunk/plearn_learners/online
Message-ID: <200706192031.l5JKVTRU000959@sheep.berlios.de>

Author: louradou
Date: 2007-06-19 22:31:29 +0200 (Tue, 19 Jun 2007)
New Revision: 7609

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMModule.cc
Log:
RBMGaussianLayer: added functions energy, fpropNLL and bpropNLL
RBMModule: slight modifications for sampling (it's being tested!)



Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-19 20:20:02 UTC (rev 7608)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-19 20:31:29 UTC (rev 7609)
@@ -302,6 +302,8 @@
     deepCopyField(sigma, copies);
 }
 
+
+
 void RBMGaussianLayer::accumulatePosStats( const Vec& pos_values )
 {
     for( int i=0 ; i<size ; i++ )
@@ -402,6 +404,100 @@
     inherited::update( pos_values, neg_values );
 }
 
+real RBMGaussianLayer::energy(const Vec& unit_values) const
+{
+    if(unit_values.length()!=quad_coeff.length())
+        PLERROR("InRBMGaussianLayer::unit_values and quadratic_coeff Vecs must have the same length.");
+    real quad_dot=0.;
+    if (unit_values.size() > 0 && quad_coeff.size() > 0) {
+        real* v1 = unit_values.data();
+        real* v2 = quad_coeff.data();
+        for(register int i=0; i<unit_values.length(); i++)
+            quad_dot += v1[i]*v2[i];
+    }
+    return dot(unit_values,bias) + quad_dot;
+}
+
+real RBMGaussianLayer::fpropNLL(const Vec& target)
+{
+    PLASSERT( target.size() == input_size );
+
+    real ret = 0;
+    real target_i, activation_i, a_i;
+    for( int i=0 ; i<size ; i++ )
+    {
+        target_i = target[i];
+        activation_i = activation[i];
+	a_i = quad_coeff[i];
+        if(!fast_exact_is_equal(target_i,0.0))
+            // ret -= target[i] * pl_log(expectations[i]); 
+            ret -= target_i * pl_log( - activation_i / (2 * a_i * a_i) );
+        if(!fast_exact_is_equal(target_i,1.0))
+            // ret -= (1-target_i) * pl_log(1-expectation_i);
+            ret -= (1-target_i) * pl_log( 1 + activation_i / (2 * a_i * a_i) );
+    }
+    return ret;
+}
+
+
+void RBMGaussianLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
+{
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+
+    real nll, a_i;
+    real *activation, *target;
+    
+    for (int k=0;k<batch_size;k++) // loop over minibatch
+    {
+        nll = 0;
+        activation = activations[k];
+        target = targets[k];
+        for( register int i=0 ; i<size ; i++ ) // loop over outputs
+        {
+            a_i = quad_coeff[i];
+	    if(!fast_exact_is_equal(target[i],0.0))
+                // nll -= target[i] * pl_log(expectations[i]); 
+                nll -= target[i] * pl_log( - activation[i] / (2 * a_i * a_i) );
+            if(!fast_exact_is_equal(target[i],1.0))
+                // nll -= (1-target[i]) * pl_log(1-expectations[i]); 
+                nll -= (1 - target[i]) * pl_log( 1 + activation[i] / (2 * a_i * a_i) );
+        }
+        costs_column(k,0) = nll;
+    }
+}
+
+void RBMGaussianLayer::bpropNLL(const Vec& target, real nll, Vec& bias_gradient)
+{
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+    bias_gradient.resize( size );
+
+    for( int i=0 ; i<size ; i++ )
+    {
+        bias_gradient[i] = target[i]-expectation[i];
+    }
+}
+
+void RBMGaussianLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
+                                Mat& bias_gradients)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+
+    substract(targets,expectations,bias_gradients);
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-19 20:20:02 UTC (rev 7608)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-19 20:31:29 UTC (rev 7609)
@@ -128,6 +128,20 @@
     //! forgets everything
     virtual void forget();
 
+    //! compute bias' unit_values + min_quad_coeff.^2' unit_values.^2
+    virtual real energy(const Vec& unit_values) const;
+
+    //! Computes the negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec& target);
+    virtual void fpropNLL(const Mat& targets, const Mat& costs_column);
+    
+    //! Computes the gradient of the negative log-likelihood of target
+    //! with respect to the layer's bias, given the internal activations
+    virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
+    virtual void bpropNLL(const Mat& targets, const Mat& costs_column,
+                          Mat& bias_gradients);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-19 20:20:02 UTC (rev 7608)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-19 20:31:29 UTC (rev 7609)
@@ -772,21 +772,21 @@
         found_a_valid_configuration = true;
     } 
     
-    // SAMPLING
+    // SAMPLING inputs
     if ((visible_sample && visible_sample->isEmpty()) // it is asked to sample the visible units
-        || (hidden_sample && hidden_sample->isEmpty())) // or to sample the hidden units
+        && (!visible || !visible->isEmpty() ))
+//        || (hidden_sample && hidden_sample->isEmpty())) // or to sample the hidden units
     {
-        if (hidden && !hidden->isEmpty()) // sample visible conditionally on hidden
+        if (hidden_sample && !hidden_sample->isEmpty()) // sample visible conditionally on hidden
         {
-            sampleVisibleGivenHidden(*hidden);
-            visible_sample->resize(visible_layer->samples.length(),visible_layer->samples.width());
-            *visible_sample << visible_layer->samples;
-            Gibbs_step = 0; // that would restart the chain for unconditional sampling
+            sampleVisibleGivenHidden(*hidden_sample);
+	    cout << "(discrete visible) sampling init (from hidden)" << endl;
         }
         else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
         {
-            sampleHiddenGivenVisible(visible_layer->samples);
-            Gibbs_step = 0; // that would restart the chain for unconditional sampling
+            sampleHiddenGivenVisible(*visible);
+            sampleVisibleGivenHidden(hidden_layer->samples);
+	    cout << "(discrete visible) sampling init (from visible)" << endl;
         }
         else // sample unconditionally: Gibbs sample after k steps
         {
@@ -794,63 +794,45 @@
             // start or continue the chain
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
+            cout << "(discrete visible) gibbs " << Gibbs_step;
             for (;Gibbs_step<min_n;Gibbs_step++)
             {
                 sampleHiddenGivenVisible(visible_layer->samples);
                 sampleVisibleGivenHidden(hidden_layer->samples);
             }
-  	    cout << "gibbs " << Gibbs_step << endl;
+  	    cout << " -> " << Gibbs_step-1 << endl;
         }
-        if (visible_sample && visible_sample->isEmpty()) // provide sample of the visible units
-        {
-            visible_sample->resize(visible_layer->samples.length(),
-                                   visible_layer->samples.width());
-            *visible_sample << visible_layer->samples;
-        }
-        if (hidden_sample && hidden_sample->isEmpty()) // provide sample of the hidden units
-        {
-            hidden_sample->resize(hidden_layer->samples.length(),
-                                  hidden_layer->samples.width());
-            *hidden_sample << hidden_layer->samples;
-        }
         found_a_valid_configuration = true;
     }
     // SAMPLING continuous inputs (computing visible expectation)
-    if (visible && visible->isEmpty()) {
+    if (visible && visible->isEmpty()) { // it is asked the expectations of visible units
     
         if (visible_sample && !visible_sample->isEmpty())
         {
             sampleHiddenGivenVisible(*visible_sample);
             sampleVisibleGivenHidden(hidden_layer->samples);
-	    cout << "init (1)" << endl;
+	    cout << "(continuous visible) sampling init (from visible)" << endl;
 	}
-	else if ( hidden && !hidden->isEmpty() )
+	else if ( hidden_sample && !hidden_sample->isEmpty() )
 	{
-	   sampleVisibleGivenHidden(*hidden);
-           Gibbs_step = 0; // that would restart the chain for unconditional sampling
-	   cout << "init" << endl;
+	   sampleVisibleGivenHidden(*hidden_sample);
+	   cout << "(continuous visible) sampling init (from hidden)" << endl;	   
 	}
 	else
 	{
            int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
 	   PLASSERT( min_n > 0 );
-           cout << "gibbs " << Gibbs_step;
+           cout << "(continuous visible) gibbs " << Gibbs_step;
 	   for (;Gibbs_step<min_n;Gibbs_step++)
            {
                 sampleHiddenGivenVisible(visible_layer->samples);
                 sampleVisibleGivenHidden(hidden_layer->samples);
            }
   	   cout << " -> " << Gibbs_step-1 << endl;
-           if ( hidden )   // fill hidden.state with expectations
-	   {
-	      const Mat& hidden_expect = hidden_layer->getExpectations();
-              hidden->resize(hidden_expect.length(), hidden_expect.width());
-              *hidden << hidden_expect;
-	   }
 	}
         
-	if ( hidden )   // fill hidden.state with expectations
+	if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
         {
    	      const Mat& hidden_expect = hidden_layer->getExpectations();
               hidden->resize(hidden_expect.length(), hidden_expect.width());
@@ -858,12 +840,23 @@
         }
 	 
         const Mat& to_store = visible_layer->getExpectations();
-	cout << to_store.length() << " x " << to_store.width() << endl;
         visible->resize(to_store.length(), to_store.width());
         *visible << to_store;
 
 	found_a_valid_configuration = true;
     }
+    if (visible_sample && visible_sample->isEmpty()) // provide sample of the visible units
+    {
+        visible_sample->resize(visible_layer->samples.length(),
+                               visible_layer->samples.width());
+        *visible_sample << visible_layer->samples;
+    }
+    if (hidden_sample && hidden_sample->isEmpty()) // provide sample of the hidden units
+    {
+        hidden_sample->resize(hidden_layer->samples.length(),
+                              hidden_layer->samples.width());
+        *hidden_sample << hidden_layer->samples;
+    }
 
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)



From yoshua at mail.berlios.de  Tue Jun 19 22:46:48 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Tue, 19 Jun 2007 22:46:48 +0200
Subject: [Plearn-commits] r7610 - trunk/plearn_learners/online
Message-ID: <200706192046.l5JKkmAD002640@sheep.berlios.de>

Author: yoshua
Date: 2007-06-19 22:46:48 +0200 (Tue, 19 Jun 2007)
New Revision: 7610

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Added (optional) computation of exact gradient of log-likelihood in RBMs.


Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-06-19 20:31:29 UTC (rev 7609)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-06-19 20:46:48 UTC (rev 7610)
@@ -336,6 +336,12 @@
     bias_pos_stats += pos_values;
     pos_count++;
 }
+void RBMLayer::accumulatePosStats( const Mat& pos_values )
+{
+    for (int i=0;i<pos_values.length();i++)
+        bias_pos_stats += pos_values(i);
+    pos_count+=pos_values.length();
+}
 
 ////////////////////////
 // accumulateNegStats //
@@ -345,6 +351,12 @@
     bias_neg_stats += neg_values;
     neg_count++;
 }
+void RBMLayer::accumulateNegStats( const Mat& neg_values )
+{
+    for (int i=0;i<neg_values.length();i++)
+        bias_neg_stats += neg_values(i);
+    neg_count+=neg_values.length();
+}
 
 ////////////
 // update //

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-06-19 20:31:29 UTC (rev 7609)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-06-19 20:46:48 UTC (rev 7610)
@@ -204,9 +204,11 @@
 
     //! Accumulates positive phase statistics
     virtual void accumulatePosStats( const Vec& pos_values );
+    virtual void accumulatePosStats( const Mat& ps_values);
 
     //! Accumulates negative phase statistics
     virtual void accumulateNegStats( const Vec& neg_values );
+    virtual void accumulateNegStats( const Mat& neg_values );
 
     //! Update parameters according to accumulated statistics
     virtual void update();

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-19 20:31:29 UTC (rev 7609)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-19 20:46:48 UTC (rev 7610)
@@ -135,6 +135,16 @@
     pos_count++;
 }
 
+void RBMMatrixConnection::accumulatePosStats( const Mat& down_values,
+                                              const Mat& up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+    // weights_pos_stats += up_values * down_values'
+    productScaleAcc(weights_pos_stats, up_values, true, down_values, false, 1., 1.);
+    pos_count+=mbs;
+}
+
 ////////////////////////
 // accumulateNegStats //
 ////////////////////////
@@ -147,6 +157,16 @@
     neg_count++;
 }
 
+void RBMMatrixConnection::accumulateNegStats( const Mat& down_values,
+                                              const Mat& up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+    // weights_neg_stats += up_values * down_values'
+     productScaleAcc(weights_neg_stats, up_values, true, down_values, false, 1., 1.);
+    neg_count+=mbs;
+}
+
 ////////////
 // update //
 ////////////

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-06-19 20:31:29 UTC (rev 7609)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-06-19 20:46:48 UTC (rev 7610)
@@ -100,20 +100,14 @@
                                      const Vec& up_values );
 
     virtual void accumulatePosStats( const Mat& down_values,
-                                     const Mat& up_values )
-    {
-        PLASSERT_MSG( false, "Not implemented" );
-    }
+                                     const Mat& up_values );
 
     //! Accumulates negative phase statistics to *_neg_stats
     virtual void accumulateNegStats( const Vec& down_values,
                                      const Vec& up_values );
 
     virtual void accumulateNegStats( const Mat& down_values,
-                                     const Mat& up_values )
-    {
-        PLASSERT_MSG( false, "Not implemented" );
-    }
+                                     const Mat& up_values );
 
     //! Updates parameters according to contrastive divergence gradient
     virtual void update();

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-19 20:31:29 UTC (rev 7609)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-19 20:46:48 UTC (rev 7610)
@@ -98,6 +98,7 @@
     min_n_Gibbs_steps(1),
     n_Gibbs_steps_per_generated_sample(1),
     compute_log_likelihood(false),
+    minimize_log_likelihood(false),
     Gibbs_step(0),
     log_partition_function(0),
     partition_function_is_stale(true),
@@ -199,9 +200,16 @@
     declareOption(ol, "compute_log_likelihood",
                   &RBMModule::compute_log_likelihood,
                   OptionBase::buildoption,
-                  "Whether to compute the RBM generative model's log-likelihood\n"
+                  "Whether to compute the exact RBM generative model's log-likelihood\n"
                   "(on the neg_log_likelihood port). If false then the neg_log_likelihood\n"
                   "port just computes the input visible's free energy.\n");
+    
+    declareOption(ol, "minimize_log_likelihood",
+                  &RBMModule::minimize_log_likelihood,
+                  OptionBase::buildoption,
+                  "Whether to minimize the exact RBM generative model's log-likelihood\n"
+                  "i.e. take stochastic gradient steps w.r.t. the log-likelihood instead\n"
+                  "of w.r.t. the contrastive divergence.\n");
 
     declareOption(ol, "Gibbs_step", 
                   &RBMModule::Gibbs_step,
@@ -1020,6 +1028,7 @@
 void RBMModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
                                const TVec<Mat*>& ports_gradient)
 {
+    PLASSERT( ports_value.length() == nPorts() );
     PLASSERT( ports_gradient.length() == nPorts() );
     Mat* visible_grad = ports_gradient[getPortIndex("visible")];
     Mat* hidden_grad = ports_gradient[getPortIndex("hidden.state")];
@@ -1130,13 +1139,83 @@
             partition_function_is_stale = true;
     }
 
-    if (cd_learning_rate > 0) {
+    if (cd_learning_rate > 0 && minimize_log_likelihood) {
+        PLASSERT( visible && !visible->isEmpty() );
+        PLASSERT( hidden && !hidden->isEmpty() );
+        setAllLearningRates(cd_learning_rate);
+
+        // positive phase
+        visible_layer->accumulatePosStats(*visible);
+        hidden_layer->accumulatePosStats(*hidden);
+        connection->accumulatePosStats(*visible,*hidden);
+
+        // negative phase
+        PLASSERT_MSG(hidden_layer->size<32 || visible_layer->size<32,
+                     "To minimize exact log-likelihood of an RBM, hidden_layer->size "
+                     "or visible_layer->size must be <32");
+        // gradient of partition function
+        if (hidden_layer->size > visible_layer->size)
+            // do it by summing over visible configurations
+        {
+            PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
+            // assuming a binary input we sum over all bit configurations
+            int n_configurations = 1 << visible_layer->size; // = 2^{visible_layer->size}
+            energy_inputs.resize(1, visible_layer->size);
+            Vec input = energy_inputs(0);
+            // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+            // AT ONCE IN A 'MINIBATCH'
+            for (int c=0;c<n_configurations;c++)
+            {
+                // convert integer c into a bit-wise visible representation
+                int x=c;
+                for (int i=0;i<visible_layer->size;i++)
+                {
+                    input[i]= x & 1; // take least significant bit
+                    x >>= 1; // and shift right (divide by 2)
+                }
+                connection->setAsDownInput(input);
+                hidden_layer->getAllActivations(connection,0,false);
+                hidden_layer->computeExpectation();
+                visible_layer->accumulateNegStats(input);
+                hidden_layer->accumulateNegStats(hidden_layer->expectation);
+                connection->accumulateNegStats(input,hidden_layer->expectation);
+            }
+        }
+        else
+        {
+            PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
+            // assuming a binary hidden we sum over all bit configurations
+            int n_configurations = 1 << hidden_layer->size; // = 2^{hidden_layer->size}
+            energy_inputs.resize(1, hidden_layer->size);
+            Vec h = energy_inputs(0);
+            for (int c=0;c<n_configurations;c++)
+            {
+                // convert integer c into a bit-wise hidden representation
+                int x=c;
+                for (int i=0;i<hidden_layer->size;i++)
+                {
+                    h[i]= x & 1; // take least significant bit
+                    x >>= 1; // and shift right (divide by 2)
+                }
+                connection->setAsUpInput(h);
+                visible_layer->getAllActivations(connection,0,false);
+                visible_layer->computeExpectation();
+                visible_layer->accumulateNegStats(visible_layer->expectation);
+                hidden_layer->accumulateNegStats(h);
+                connection->accumulateNegStats(visible_layer->expectation,h);
+            }
+        }
+        // update
+        visible_layer->update();
+        hidden_layer->update();
+        connection->update();
+    }
+    if (cd_learning_rate > 0 && !minimize_log_likelihood) {
         EXTREME_MODULE_LOG << "Performing contrastive divergence step in RBM '"
                            << name << "'" << endl;
         // Perform a step of contrastive divergence.
         PLASSERT( visible && !visible->isEmpty() );
         setAllLearningRates(cd_learning_rate);
-        PLASSERT( ports_value.length() == nPorts() );
         Mat* negative_phase_visible_samples = 
             compute_contrastive_divergence?ports_value[getPortIndex("negative_phase_visible_samples.state")]:0;
         const Mat* negative_phase_hidden_expectations =
@@ -1330,7 +1409,6 @@
         setAllLearningRates(grad_learning_rate);
         PLASSERT( reconstruction_connection != 0 );
         // Perform gradient descent on Autoassociator reconstruction cost
-        PLASSERT( ports_value.length() == nPorts() );
         Mat* visible_reconstruction = ports_value[getPortIndex("visible_reconstruction.state")];
         Mat* visible_reconstruction_activations = ports_value[getPortIndex("visible_reconstruction_activations.state")];
         Mat* reconstruction_error = ports_value[getPortIndex("reconstruction_error.state")];

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-06-19 20:31:29 UTC (rev 7609)
+++ trunk/plearn_learners/online/RBMModule.h	2007-06-19 20:46:48 UTC (rev 7610)
@@ -83,6 +83,7 @@
     int n_Gibbs_steps_per_generated_sample;
 
     bool compute_log_likelihood;
+    bool minimize_log_likelihood;
 
     //#####  Public Learnt Options  ############################################
     //! used to generate samples from the RBM



From dorionc at mail.berlios.de  Wed Jun 20 00:19:03 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Wed, 20 Jun 2007 00:19:03 +0200
Subject: [Plearn-commits] r7611 - trunk/python_modules/plearn/pyplearn
Message-ID: <200706192219.l5JMJ3T1016378@sheep.berlios.de>

Author: dorionc
Date: 2007-06-20 00:19:02 +0200 (Wed, 20 Jun 2007)
New Revision: 7611

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
Minor fix

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-19 20:46:48 UTC (rev 7610)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-06-19 22:19:02 UTC (rev 7611)
@@ -229,14 +229,16 @@
     """
     # CSV (with or without brackets)
     slist = slist.strip()
+    #print >>sys.stderr, repr(slist), type(slist)
     if isinstance(slist,str):
-        if slist.startswith("["):
+        if slist=="":
+            slist = []
+        elif slist.startswith("["):
             assert slist.endswith("]")
-            return eval(slist)
-        
-        if slist=="":
-            return []
-        return [ elem_cast(e) for e in slist.split(",") ]
+            slist = eval(slist)
+        else:
+            slist = slist.split(",")
+        return [ elem_cast(e) for e in slist ]
 
     # List of strings
     elif isinstance(slist, list):



From lamblin at mail.berlios.de  Wed Jun 20 02:33:17 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 02:33:17 +0200
Subject: [Plearn-commits] r7612 - trunk/plearn_learners/online
Message-ID: <200706200033.l5K0XHsl011551@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 02:33:16 +0200 (Wed, 20 Jun 2007)
New Revision: 7612

Modified:
   trunk/plearn_learners/online/Convolution2DModule.cc
Log:
Fix the case where the module were not asked to provide an input gradient:
do the parameter update anyway.


Modified: trunk/plearn_learners/online/Convolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.cc	2007-06-19 22:19:02 UTC (rev 7611)
+++ trunk/plearn_learners/online/Convolution2DModule.cc	2007-06-20 00:33:16 UTC (rev 7612)
@@ -43,6 +43,7 @@
 #include "Convolution2DModule.h"
 #include <plearn/math/convolutions.h>
 #include <plearn/math/TMat_maths.h>
+#include <plearn/sys/Profiler.h>
 
 namespace PLearn {
 using namespace std;
@@ -370,6 +371,7 @@
 
 void Convolution2DModule::fprop(const TVec<Mat*>& ports_value)
 {
+    Profiler::pl_profile_start( "Convolution2DModule::fprop" );
     PLASSERT( ports_value.length() == nPorts() );
 
     Mat* input = ports_value[0];
@@ -382,6 +384,7 @@
         int batch_size = input->length();
         output->resize(batch_size, port_sizes(1,1));
 
+        Profiler::pl_profile_start( "convolve2D" );
         // TODO: optimize
         for( int k=0; k<batch_size; k++ )
         {
@@ -405,6 +408,7 @@
                                     kernel_step2, true );
             }
         }
+        Profiler::pl_profile_end( "convolve2D" );
     }
     else if (!input && !output)
     {
@@ -412,6 +416,8 @@
     }
     else
         PLCHECK_MSG( false, "Unknown port configuration" );
+
+    Profiler::pl_profile_end( "Convolution2DModule::fprop" );
 }
 
 /* THIS METHOD IS OPTIONAL
@@ -499,6 +505,7 @@
 void Convolution2DModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
                                          const TVec<Mat*>& ports_gradient)
 {
+    Profiler::pl_profile_start("Convolution2DModule::bpropAccUpdate");
     PLASSERT( ports_value.length() == nPorts()
               && ports_gradient.length() == nPorts() );
 
@@ -507,26 +514,33 @@
     Mat* input_grad = ports_gradient[0];
     Mat* output_grad = ports_gradient[1];
 
-    // If we want input_grad and we have output_grad
-    if( input_grad && input_grad->isEmpty()
-        && output_grad && !output_grad->isEmpty() )
+    // If we have output_grad and we want to update
+    if( output_grad && !output_grad->isEmpty()
+        && (!input_grad || input_grad->isEmpty() ) )
     {
+        // If we have to compute input_grad
+        bool compute_input_grad = false;
+        if( input_grad )
+            compute_input_grad = true;
+
         PLASSERT( input );
         PLASSERT( output );
 
         PLASSERT( input->width() == port_sizes(0,1) );
         PLASSERT( output->width() == port_sizes(1,1) );
-        PLASSERT( input_grad->width() == port_sizes(0,1) );
         PLASSERT( output_grad->width() == port_sizes(1,1) );
+        if( compute_input_grad )
+            PLASSERT( input_grad->width() == port_sizes(0,1) );
 
         int batch_size = input->length();
         PLASSERT( output->length() == batch_size );
         PLASSERT( output_grad->length() == batch_size );
 
-        input_grad->resize(batch_size, port_sizes(0,1));
         learning_rate = start_learning_rate /
             (1.+decrease_constant*step_number);
         real avg_lr = learning_rate / batch_size;
+        if( compute_input_grad )
+            input_grad->resize(batch_size, port_sizes(0,1));
 
         // clear kernel gradient
         for( int i=0; i<n_input_images; i++ )
@@ -538,6 +552,11 @@
                 }
 
         // TODO: optimize
+        if( compute_input_grad )
+            Profiler::pl_profile_start("convolve2Dbackprop");
+        else
+            Profiler::pl_profile_start("convolve2Dbackprop (update only)");
+
         for( int k=0; k<batch_size; k++ )
         {
             for( int i=0; i<n_input_images; i++ )
@@ -545,9 +564,11 @@
                 input_images[i] = (*input)(k)
                     .subVec(i*input_images_size, input_images_size)
                     .toMat(input_images_length, input_images_width);
-                input_gradients[i] = (*input_grad)(k)
-                    .subVec(i*input_images_size, input_images_size)
-                    .toMat(input_images_length, input_images_width);
+
+                if( compute_input_grad )
+                    input_gradients[i] = (*input_grad)(k)
+                        .subVec(i*input_images_size, input_images_size)
+                        .toMat(input_images_length, input_images_width);
             }
 
             for( int j=0; j<n_output_images; j++ )
@@ -563,15 +584,29 @@
             for( int j=0; j<n_output_images; j++ )
                 for( int i=0; i<n_input_images; i++ )
                     if( connection_matrix(i,j) != 0 )
-                        convolve2Dbackprop( input_images[i],
-                                            kernels(i,j),
-                                            output_gradients[j],
-                                            input_gradients[i],
-                                            kernel_gradients(i,j),
-                                            kernel_step1, kernel_step2,
-                                            true );
+                    {
+                        if( compute_input_grad )
+                            convolve2Dbackprop( input_images[i],
+                                                kernels(i,j),
+                                                output_gradients[j],
+                                                input_gradients[i],
+                                                kernel_gradients(i,j),
+                                                kernel_step1, kernel_step2,
+                                                true );
+                        else
+                            convolve2Dbackprop( input_images[i],
+                                                output_gradients[j],
+                                                kernel_gradients(i,j),
+                                                kernel_step1, kernel_step2,
+                                                true );
+                    }
         }
 
+        if( compute_input_grad )
+            Profiler::pl_profile_end("convolve2Dbackprop");
+        else
+            Profiler::pl_profile_end("convolve2Dbackprop (update only)");
+
         for( int j=0; j<n_output_images; j++ )
         {
             for( int i=0; i<n_input_images; i++ )
@@ -582,14 +617,21 @@
                 .subMatColumns(j*output_images_size, output_images_size) );
         }
     }
-    else if( !input_grad )
+    else if( !input_grad
+             && output_grad && !output_grad->isEmpty() )
     {
-        PLASSERT( !output_grad || !output_grad->isEmpty() );
+        PLASSERT( input && !input->isEmpty() );
+        PLASSERT( output && !output->isEmpty() );
+    }
+    else if( !input_grad && !output_grad )
+    {
         PLASSERT( !input || !input->isEmpty() );
         PLASSERT( !output || !output->isEmpty() );
     }
     else
         PLCHECK_MSG( false, "Port configuration not implemented" );
+
+    Profiler::pl_profile_end("Convolution2DModule::bpropAccUpdate");
 }
 
 //! reset the parameters to the state they would be BEFORE starting training.



From lamblin at mail.berlios.de  Wed Jun 20 02:41:45 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 02:41:45 +0200
Subject: [Plearn-commits] r7613 - trunk/plearn_learners/online
Message-ID: <200706200041.l5K0fjlf012052@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 02:41:45 +0200 (Wed, 20 Jun 2007)
New Revision: 7613

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
Log:
Fix computation of fpropNLL and energy (hopefully)


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-20 00:33:16 UTC (rev 7612)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-20 00:41:45 UTC (rev 7613)
@@ -82,7 +82,7 @@
 
     PLCHECK_MSG(expectation_is_up_to_date, "Expectation should be computed "
             "before calling generateSample()");
-    
+
     computeStdDeviation();
     for( int i=0 ; i<size ; i++ )
         sample[i] = random_gen->gaussian_mu_sigma( expectation[i], sigma[i] );
@@ -406,36 +406,40 @@
 
 real RBMGaussianLayer::energy(const Vec& unit_values) const
 {
-    if(unit_values.length()!=quad_coeff.length())
-        PLERROR("InRBMGaussianLayer::unit_values and quadratic_coeff Vecs must have the same length.");
-    real quad_dot=0.;
-    if (unit_values.size() > 0 && quad_coeff.size() > 0) {
-        real* v1 = unit_values.data();
-        real* v2 = quad_coeff.data();
-        for(register int i=0; i<unit_values.length(); i++)
-            quad_dot += v1[i]*v2[i];
+    PLASSERT( unit_values.length() == size );
+
+    real en = 0.;
+    real tmp;
+    if (size > 0)
+    {
+        real* v = unit_values.data();
+        real* a = quad_coeff.data();
+        real* b = bias.data();
+        for(register int i=0; i<size; i++)
+        {
+            tmp = a[i]*v[i];
+            en += tmp*tmp + b[i]*v[i];
+        }
     }
-    return dot(unit_values,bias) + quad_dot;
+    return en;
 }
 
 real RBMGaussianLayer::fpropNLL(const Vec& target)
 {
     PLASSERT( target.size() == input_size );
+    computeExpectation();
+    computeStdDeviation();
 
     real ret = 0;
-    real target_i, activation_i, a_i;
     for( int i=0 ; i<size ; i++ )
     {
-        target_i = target[i];
-        activation_i = activation[i];
-	a_i = quad_coeff[i];
-        if(!fast_exact_is_equal(target_i,0.0))
-            // ret -= target[i] * pl_log(expectations[i]); 
-            ret -= target_i * pl_log( - activation_i / (2 * a_i * a_i) );
-        if(!fast_exact_is_equal(target_i,1.0))
-            // ret -= (1-target_i) * pl_log(1-expectation_i);
-            ret -= (1-target_i) * pl_log( 1 + activation_i / (2 * a_i * a_i) );
+        // ret += (target[i]-expectation[i])^2/(2 sigma[i]^2)
+        //      + log(sqrt(2*Pi) * sigma[i])
+
+        real r = (target[i] - expectation[i]) * quad_coeff[i];
+        ret += r * r + pl_log(sigma[i]);
     }
+    ret += 0.5*size*Log2Pi;
     return ret;
 }
 
@@ -448,24 +452,25 @@
     PLASSERT( costs_column.width() == 1 );
     PLASSERT( costs_column.length() == batch_size );
 
-    real nll, a_i;
-    real *activation, *target;
-    
+    computeExpectations();
+    computeStdDeviation();
+
+    real nll;
+    real *expectation, *target;
+
     for (int k=0;k<batch_size;k++) // loop over minibatch
     {
         nll = 0;
-        activation = activations[k];
+        expectation = expectations[k];
         target = targets[k];
         for( register int i=0 ; i<size ; i++ ) // loop over outputs
         {
-            a_i = quad_coeff[i];
-	    if(!fast_exact_is_equal(target[i],0.0))
-                // nll -= target[i] * pl_log(expectations[i]); 
-                nll -= target[i] * pl_log( - activation[i] / (2 * a_i * a_i) );
-            if(!fast_exact_is_equal(target[i],1.0))
-                // nll -= (1-target[i]) * pl_log(1-expectations[i]); 
-                nll -= (1 - target[i]) * pl_log( 1 + activation[i] / (2 * a_i * a_i) );
+            // nll += (target[i]-expectation[i])^2/(2 sigma[i]^2)
+            //      + log(sqrt(2*Pi) * sigma[i])
+            real r = (target[i] - expectation[i]) * quad_coeff[i];
+            nll += r * r + pl_log(sigma[i]);
         }
+        nll += 0.5*size*Log2Pi;
         costs_column(k,0) = nll;
     }
 }



From lamblin at mail.berlios.de  Wed Jun 20 03:15:34 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 03:15:34 +0200
Subject: [Plearn-commits] r7614 - trunk/plearn_learners/online
Message-ID: <200706200115.l5K1FYN6013863@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 03:15:33 +0200 (Wed, 20 Jun 2007)
New Revision: 7614

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
Log:
Implement batch version of update( pos_values, neg_values )


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-20 00:41:45 UTC (rev 7613)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-20 01:15:33 UTC (rev 7614)
@@ -365,7 +365,7 @@
     inherited::update();
 }
 
-void RBMGaussianLayer::update( const Vec& pos_values, const Vec& neg_values)
+void RBMGaussianLayer::update( const Vec& pos_values, const Vec& neg_values )
 {
     // quad_coeff[i] -= learning_rate * 2 * quad_coeff[i] * (pos_values[i]^2
     //                                                       - neg_values[i]^2)
@@ -404,6 +404,45 @@
     inherited::update( pos_values, neg_values );
 }
 
+void RBMGaussianLayer::update( const Mat& pos_values, const Mat& neg_values )
+{
+    PLASSERT( pos_values.width() == size );
+    PLASSERT( neg_values.width() == size );
+
+    int batch_size = pos_values.length();
+    PLASSERT( neg_values.length() == batch_size );
+
+    // quad_coeff[i] -= learning_rate * 2 * quad_coeff[i] * (pos_values[i]^2
+    //                                                       - neg_values[i]^2)
+
+    real two_lr = 2 * learning_rate;
+    real* a = quad_coeff.data();
+
+    if( momentum == 0. )
+    {
+        for( int k=0; k<batch_size; k++ )
+        {
+            real *pv_k = pos_values[k];
+            real *nv_k = neg_values[k];
+            for( int i=0; i<size; i++ )
+            {
+                a[i] += two_lr * a[i] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
+                if( a[i] < min_quad_coeff )
+                    a[i] = min_quad_coeff;
+            }
+        }
+    }
+    else
+        PLCHECK_MSG( false,
+                     "momentum and minibatch are not compatible yet" );
+
+    // We will need to recompute sigma
+    sigma_is_up_to_date = false;
+
+    // Update the bias
+    inherited::update( pos_values, neg_values );
+}
+
 real RBMGaussianLayer::energy(const Vec& unit_values) const
 {
     PLASSERT( unit_values.length() == size );



From lamblin at mail.berlios.de  Wed Jun 20 03:29:58 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 03:29:58 +0200
Subject: [Plearn-commits] r7615 - trunk/plearn_learners/online
Message-ID: <200706200129.l5K1Twee014454@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 03:29:58 +0200 (Wed, 20 Jun 2007)
New Revision: 7615

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.h
Log:
Should work better with up-to-date .h file


Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-20 01:15:33 UTC (rev 7614)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-20 01:29:58 UTC (rev 7615)
@@ -113,11 +113,8 @@
     //! Update parameters according to one pair of vectors
     virtual void update( const Vec& pos_values, const Vec& neg_values );
 
-    //! Not implemented.
-    virtual void update( const Mat& pos_values, const Mat& neg_values )
-    {
-        PLASSERT_MSG(false, "Not implemented");
-    }
+    //! Batch version
+    virtual void update( const Mat& pos_values, const Mat& neg_values );
 
     //! resets activations, sample, expectation and sigma fields
     virtual void reset();
@@ -135,7 +132,7 @@
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
     virtual void fpropNLL(const Mat& targets, const Mat& costs_column);
-    
+
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);



From lamblin at mail.berlios.de  Wed Jun 20 03:50:23 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 03:50:23 +0200
Subject: [Plearn-commits] r7616 - trunk/plearn_learners/online
Message-ID: <200706200150.l5K1oN4E015586@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 03:50:21 +0200 (Wed, 20 Jun 2007)
New Revision: 7616

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.cc
Log:
Fix compilation with -float -noblas


Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-20 01:29:58 UTC (rev 7615)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-20 01:50:21 UTC (rev 7616)
@@ -141,7 +141,7 @@
     int mbs=down_values.length();
     PLASSERT(up_values.length()==mbs);
     // weights_pos_stats += up_values * down_values'
-    productScaleAcc(weights_pos_stats, up_values, true, down_values, false, 1., 1.);
+    transposeProductAcc(weights_pos_stats, up_values, down_values);
     pos_count+=mbs;
 }
 
@@ -163,7 +163,7 @@
     int mbs=down_values.length();
     PLASSERT(up_values.length()==mbs);
     // weights_neg_stats += up_values * down_values'
-     productScaleAcc(weights_neg_stats, up_values, true, down_values, false, 1., 1.);
+    transposeProductAcc(weights_neg_stats, up_values, down_values);
     neg_count+=mbs;
 }
 
@@ -293,11 +293,11 @@
         // We use the average gradient over a mini-batch.
         real avg_lr = learning_rate / pos_down_values.length();
 
-        productScaleAcc(weights, pos_up_values, true, pos_down_values, false,
-                -avg_lr, 1.);
+        transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
+                                 -avg_lr, real(1));
 
-        productScaleAcc(weights, neg_up_values, true, neg_down_values, false,
-                avg_lr, 1.);
+        transposeProductScaleAcc(weights, neg_up_values, neg_down_values,
+                                 avg_lr, real(1));
     }
     else
     {
@@ -339,30 +339,28 @@
     //              +(1-gibbs_chain_statistics_forgetting_factor)
     //               * gibbs_neg_up_values'*gibbs_neg_down_values/minibatch_size
     if (neg_count==0)
-        productScaleAcc(weights_neg_stats,
-                        gibbs_neg_up_values,true,
-                        gibbs_neg_down_values,false,normalize_factor,0.);
+        transposeProductScaleAcc(weights_neg_stats, gibbs_neg_up_values,
+                                 gibbs_neg_down_values,
+                                 normalize_factor, real(0));
     else
-        productScaleAcc(weights_neg_stats,
-                        gibbs_neg_up_values,true,
-                        gibbs_neg_down_values,false,
-                        normalize_factor*(1-gibbs_ma_coefficient),
-                        gibbs_ma_coefficient);
+        transposeProductScaleAcc(weights_neg_stats,
+                                 gibbs_neg_up_values,
+                                 gibbs_neg_down_values,
+                                 normalize_factor*(1-gibbs_ma_coefficient),
+                                 gibbs_ma_coefficient);
     neg_count++;
 
     // delta w = -lrate * ( pos_up_values'*pos_down_values
     //                   - ( background_gibbs_update_ratio*neg_stats
     //                      +(1-background_gibbs_update_ratio)
     //                       * cd_neg_up_values'*cd_neg_down_values/minibatch_size))
-    productScaleAcc(weights,
-                    pos_up_values, true,
-                    pos_down_values, false,-learning_rate*normalize_factor,1.);
+    transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
+                             -learning_rate*normalize_factor, real(1));
     multiplyAcc(weights, weights_neg_stats,
                 learning_rate*background_gibbs_update_ratio);
-    productScaleAcc(weights,
-                    cd_neg_up_values, true,
-                    cd_neg_down_values, false,
-                    learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,1.);
+    transposeProductScaleAcc(weights, cd_neg_up_values, cd_neg_down_values,
+        learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,
+        real(1));
 }
 
 void RBMMatrixConnection::updateGibbs( const Mat& pos_down_values,
@@ -377,7 +375,8 @@
     //               * gibbs_neg_up_values'*gibbs_neg_down_values
     static Mat tmp;
     tmp.resize(weights.length(),weights.width());
-    productScaleAcc(tmp,gibbs_neg_up_values,true,gibbs_neg_down_values,false,1.,0.);
+    transposeProduct(tmp, gibbs_neg_up_values, gibbs_neg_down_values);
+
     if (neg_count==0)
         multiply(weights_neg_stats,tmp,normalize_factor);
     else
@@ -401,9 +400,8 @@
     }
 
     // delta w = -lrate * ( pos_up_values'*pos_down_values/minibatch_size - neg_stats )
-    productScaleAcc(weights,
-                    pos_up_values, true,
-                    pos_down_values, false,-learning_rate*normalize_factor,1.);
+    transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
+                             -learning_rate*normalize_factor, real(1));
     multiplyAcc(weights, weights_neg_stats,learning_rate);
 }
 
@@ -579,8 +577,8 @@
     }
 
     // weights -= learning_rate/n * output_gradients' * inputs
-    productScaleAcc(weights, output_gradients, true, inputs, false,
-            -learning_rate / inputs.length(), 1.);
+    transposeProductScaleAcc(weights, output_gradients, inputs,
+                             -learning_rate / inputs.length(), real(1));
 }
 
 void RBMMatrixConnection::petiteCulotteOlivierUpdate(



From lamblin at mail.berlios.de  Wed Jun 20 03:54:38 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 03:54:38 +0200
Subject: [Plearn-commits] r7617 - trunk/plearn_learners/online
Message-ID: <200706200154.l5K1scQj015841@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 03:54:37 +0200 (Wed, 20 Jun 2007)
New Revision: 7617

Modified:
   trunk/plearn_learners/online/GradNNetLayerModule.cc
Log:
Fix compilation wit -noblas -float


Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-06-20 01:50:21 UTC (rev 7616)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-06-20 01:54:37 UTC (rev 7617)
@@ -95,7 +95,7 @@
 
     // Add bias.
     resizeOnes(n);
-    externalProductScaleAcc(outputs, ones, bias, 1.); // could be more efficient, but not critical 
+    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical 
 }
 
 /////////////////
@@ -256,11 +256,11 @@
 
     // Update bias.
     resizeOnes(n);
-    productScaleAcc(bias, output_gradients, true, ones, -avg_lr, 1.);
+    transposeProductScaleAcc(bias, output_gradients, ones, -avg_lr, real(1));
 
     // Update weights.
-    productScaleAcc(weights, output_gradients, true, inputs, false,
-            -avg_lr, l2_scaling);
+    transposeProductScaleAcc(weights, output_gradients, inputs,
+                             -avg_lr, l2_scaling);
 
     // Apply L1 penalty if needed (note: this is not very efficient).
     if (L1_penalty_factor > 0) {



From lamblin at mail.berlios.de  Wed Jun 20 03:59:23 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 03:59:23 +0200
Subject: [Plearn-commits] r7618 - trunk/plearn_learners/online
Message-ID: <200706200159.l5K1xNqn015986@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 03:59:22 +0200 (Wed, 20 Jun 2007)
New Revision: 7618

Modified:
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
Log:
Fix compilation with -float -noblas


Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2007-06-20 01:54:37 UTC (rev 7617)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2007-06-20 01:59:22 UTC (rev 7618)
@@ -372,18 +372,18 @@
                       "Cannot resize input_gradients and accumulate into it" );
 
         // input_gradients += output_gradient * weights
-        productScaleAcc(input_gradients, output_gradients, false, weights, true, 1., 1.);
+        productTransposeAcc(input_gradients, output_gradients, weights);
     }
     else
     {
         input_gradients.resize(inputs.length(), down_size);
         // input_gradients = output_gradient * weights
-        productScaleAcc(input_gradients, output_gradients, false, weights, true, 1., 0.);
+        productTranspose(input_gradients, output_gradients, weights);
     }
 
     // weights -= learning_rate/n * output_gradients' * inputs
-    productScaleAcc(weights, inputs, true, output_gradients, false, 
-                    -learning_rate / inputs.length(), 1.);
+    transposeProductScaleAcc(weights, inputs, output_gradients,
+                             -learning_rate / inputs.length(), real(1));
 }
 
 



From lamblin at mail.berlios.de  Wed Jun 20 04:02:36 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 04:02:36 +0200
Subject: [Plearn-commits] r7619 - trunk/plearn_learners/online
Message-ID: <200706200202.l5K22a0g016339@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 04:02:35 +0200 (Wed, 20 Jun 2007)
New Revision: 7619

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
Log:
Fix compilation with -float -noblas


Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-06-20 01:59:22 UTC (rev 7618)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-06-20 02:02:35 UTC (rev 7619)
@@ -466,8 +466,8 @@
 
     if( momentum == 0. )
     {
-        productScaleAcc(bias, pos_values, true, ones, -avg_lr, 1.);
-        productScaleAcc(bias, neg_values, true, ones,  avg_lr, 1.);
+        transposeProductScaleAcc(bias, pos_values, ones, -avg_lr, real(1));
+        transposeProductScaleAcc(bias, neg_values, ones,  avg_lr, real(1));
     }
     else
     {



From nouiz at mail.berlios.de  Wed Jun 20 20:51:02 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 20 Jun 2007 20:51:02 +0200
Subject: [Plearn-commits] r7620 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706201851.l5KIp2j5000046@sheep.berlios.de>

Author: nouiz
Date: 2007-06-20 20:51:02 +0200 (Wed, 20 Jun 2007)
New Revision: 7620

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc
Log:
Better comment


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc	2007-06-20 02:02:35 UTC (rev 7619)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc	2007-06-20 18:51:02 UTC (rev 7620)
@@ -49,7 +49,7 @@
                         "A PLearner to wrap around a generic regressor to calculate the predicted class.", 
                         "Algorithm built to wrap around a generic regressor in the context of the second iteration\n"
                         "of the annual sales estimation project.\n"
-                        "It calculates the predicted class and computes the cse error.\n"
+                        "It calculates the predicted class and computes the cse error. It do not work with classifier\n"
     );
 
 SecondIterationWrapper::SecondIterationWrapper()  



From nouiz at mail.berlios.de  Wed Jun 20 20:56:31 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 20 Jun 2007 20:56:31 +0200
Subject: [Plearn-commits] r7621 - branches/cgi-desjardin/commands
Message-ID: <200706201856.l5KIuVHP000369@sheep.berlios.de>

Author: nouiz
Date: 2007-06-20 20:56:31 +0200 (Wed, 20 Jun 2007)
New Revision: 7621

Modified:
   branches/cgi-desjardin/commands/plearn_inc_gg.h
Log:
Added two VMatrix that I use


Modified: branches/cgi-desjardin/commands/plearn_inc_gg.h
===================================================================
--- branches/cgi-desjardin/commands/plearn_inc_gg.h	2007-06-20 18:51:02 UTC (rev 7620)
+++ branches/cgi-desjardin/commands/plearn_inc_gg.h	2007-06-20 18:56:31 UTC (rev 7621)
@@ -254,6 +254,8 @@
 #include <plearn/vmat/CenteredVMatrix.h>
 #include <plearn/vmat/CompactVMatrix.h>
 #include <plearn/vmat/CompressedVMatrix.h>
+#include <plearn/vmat/ConcatColumnsVMatrix.h>
+#include <plearn/vmat/ConstantVMatrix.h>
 #include <plearn/vmat/CumVMatrix.h>
 #include <plearn/vmat/DatedJoinVMatrix.h>
 #include <plearn/vmat/DictionaryVMatrix.h>



From lamblin at mail.berlios.de  Wed Jun 20 21:04:59 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 20 Jun 2007 21:04:59 +0200
Subject: [Plearn-commits] r7622 - trunk/plearn_learners/online
Message-ID: <200706201904.l5KJ4xhl000995@sheep.berlios.de>

Author: lamblin
Date: 2007-06-20 21:04:58 +0200 (Wed, 20 Jun 2007)
New Revision: 7622

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
Log:
Bug fix: we should not always reinitialize quad_coeff in build_()


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-20 18:56:31 UTC (rev 7621)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-20 19:04:58 UTC (rev 7622)
@@ -255,6 +255,7 @@
 void RBMGaussianLayer::forget()
 {
     quad_coeff.fill( 2. );
+//    quad_coeff.fill( 1. );
 
     inherited::forget();
 }
@@ -279,8 +280,6 @@
     sigma_is_up_to_date = false;
 
     quad_coeff.resize( size );
-    quad_coeff.fill( 2. );
-//    quad_coeff.fill( 1. );
     quad_coeff_pos_stats.resize( size );
     quad_coeff_neg_stats.resize( size );
 }



From louradou at mail.berlios.de  Thu Jun 21 02:14:36 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 21 Jun 2007 02:14:36 +0200
Subject: [Plearn-commits] r7623 - trunk/plearn_learners/online
Message-ID: <200706210014.l5L0EalF014041@sheep.berlios.de>

Author: louradou
Date: 2007-06-21 02:14:35 +0200 (Thu, 21 Jun 2007)
New Revision: 7623

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMModule.cc
Log:
RBMGaussianLayer: added an option share_quad_coeff
                  fixed a bug concerning the learning rate
		        for quadratic coefficients with batch_size>1
RBMModule: things a bit improved for sampling



Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-20 19:04:58 UTC (rev 7622)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-21 00:14:35 UTC (rev 7623)
@@ -53,6 +53,7 @@
 RBMGaussianLayer::RBMGaussianLayer( real the_learning_rate ) :
     inherited( the_learning_rate ),
     min_quad_coeff( 0. ),
+    share_quad_coeff( false ),
     sigma_is_up_to_date( false )
 {
 }
@@ -60,6 +61,7 @@
 RBMGaussianLayer::RBMGaussianLayer( int the_size, real the_learning_rate ) :
     inherited( the_learning_rate ),
     min_quad_coeff( 0. ),
+    share_quad_coeff( false ),
     quad_coeff( the_size, 1. ), // or 1./M_SQRT2 ?
     quad_coeff_pos_stats( the_size ),
     quad_coeff_neg_stats( the_size ),
@@ -75,6 +77,30 @@
     bias_neg_stats.resize( the_size );
 }
 
+RBMGaussianLayer::RBMGaussianLayer( int the_size, real the_learning_rate, bool do_share_quad_coeff ) :
+    inherited( the_learning_rate ),
+    min_quad_coeff( 0. ),
+    sigma_is_up_to_date( false ),
+    quad_coeff_pos_stats( the_size ),
+    quad_coeff_neg_stats( the_size )
+{
+    size = the_size;
+    activation.resize( the_size );
+    sample.resize( the_size );
+    expectation.resize( the_size );
+    bias.resize( the_size );
+    bias_pos_stats.resize( the_size );
+    bias_neg_stats.resize( the_size );
+    share_quad_coeff = do_share_quad_coeff;
+    if ( share_quad_coeff )
+       size_quad_coeff=1;
+    else
+       size_quad_coeff=size;
+    quad_coeff=Vec(size_quad_coeff,1.);
+    sigma=Vec(size_quad_coeff);
+}
+
+
 void RBMGaussianLayer::generateSample()
 {
     PLASSERT_MSG(random_gen,
@@ -84,8 +110,12 @@
             "before calling generateSample()");
 
     computeStdDeviation();
-    for( int i=0 ; i<size ; i++ )
-        sample[i] = random_gen->gaussian_mu_sigma( expectation[i], sigma[i] );
+    if(share_quad_coeff)
+        for( int i=0 ; i<size ; i++ )
+            sample[i] = random_gen->gaussian_mu_sigma( expectation[i], sigma[0] );
+    else
+        for( int i=0 ; i<size ; i++ )
+            sample[i] = random_gen->gaussian_mu_sigma( expectation[i], sigma[i] );
 }
 
 void RBMGaussianLayer::generateSamples()
@@ -99,10 +129,14 @@
     computeStdDeviation();
     PLASSERT( samples.width() == size && samples.length() == batch_size );
 
-    for (int k = 0; k < batch_size; k++) {
-        for (int i=0 ; i<size ; i++)
-            samples(k, i) = random_gen->gaussian_mu_sigma( expectations(k, i), sigma[i] );
-    }
+    if(share_quad_coeff)
+        for (int k = 0; k < batch_size; k++)
+            for (int i=0 ; i<size ; i++)
+                samples(k, i) = random_gen->gaussian_mu_sigma( expectations(k, i), sigma[0] );
+    else
+        for (int k = 0; k < batch_size; k++)
+            for (int i=0 ; i<size ; i++)
+                samples(k, i) = random_gen->gaussian_mu_sigma( expectations(k, i), sigma[i] );
 }
 
 
@@ -112,11 +146,20 @@
         return;
 
     // mu = activations[i] / (2 * quad_coeff[i]^2)
-    for( int i=0 ; i<size ; i++ )
+    if(share_quad_coeff)
     {
-        real a_i = quad_coeff[i];
-        expectation[i] = - activation[i] / (2 * a_i * a_i);
+        real a_i = quad_coeff[0];
+        for( int i=0 ; i<size ; i++ )
+        {
+            expectation[i] = - activation[i] / (2 * a_i * a_i);
+        }
     }
+    else
+        for( int i=0 ; i<size ; i++ )
+        {
+            real a_i = quad_coeff[i];
+            expectation[i] = - activation[i] / (2 * a_i * a_i);
+        }
 
     expectation_is_up_to_date = true;
 }
@@ -129,13 +172,22 @@
     PLASSERT( expectations.width() == size
               && expectations.length() == batch_size );
 
-    for (int k = 0; k < batch_size; k++)
-        for (int i = 0 ; i < size ; i++)
-            {
-                real a_i = quad_coeff[i];
-                expectations(k, i) = -activations(k, i) / (2 * a_i * a_i) ;
-            }
-
+    if(share_quad_coeff)
+    {
+        real a_i = quad_coeff[0];
+        for (int k = 0; k < batch_size; k++)
+            for (int i = 0 ; i < size ; i++)
+                {
+                    expectations(k, i) = -activations(k, i) / (2 * a_i * a_i) ;
+                }
+    }
+    else
+        for (int k = 0; k < batch_size; k++)
+            for (int i = 0 ; i < size ; i++)
+                {
+                    real a_i = quad_coeff[i];
+                    expectations(k, i) = -activations(k, i) / (2 * a_i * a_i) ;
+                }
     expectations_are_up_to_date = true;
 }
 
@@ -145,9 +197,12 @@
     if( sigma_is_up_to_date )
         return;
 
-    // sigma = 1 / (sqrt(2) * quad_coeff[i])
-    for( int i=0 ; i<size ; i++ )
-        sigma[i] = 1 / (M_SQRT2 * quad_coeff[i]);
+    // sigma = 1 / (sqrt(2) * quad_coeff[i])    
+    if(share_quad_coeff)
+        sigma[0] = 1 / (M_SQRT2 * quad_coeff[0]);
+    else
+        for( int i=0 ; i<size ; i++ )
+            sigma[i] = 1 / (M_SQRT2 * quad_coeff[i]);
 
     sigma_is_up_to_date = true;
 }
@@ -157,11 +212,20 @@
     PLASSERT( input.size() == input_size );
     output.resize( output_size );
 
-    for( int i=0 ; i<size ; i++ )
+    if(share_quad_coeff)
     {
-        real a_i = quad_coeff[i];
-        output[i] = - (input[i] + bias[i]) / (2 * a_i * a_i);
+        real a_i = quad_coeff[0];
+        for( int i=0 ; i<size ; i++ )
+        {
+            output[i] = - (input[i] + bias[i]) / (2 * a_i * a_i);
+        }
     }
+    else
+        for( int i=0 ; i<size ; i++ )
+        {
+            real a_i = quad_coeff[i];
+            output[i] = - (input[i] + bias[i]) / (2 * a_i * a_i);
+        }
 }
 
 void RBMGaussianLayer::bpropUpdate(const Vec& input, const Vec& output,
@@ -187,13 +251,15 @@
     if( momentum != 0. )
     {
         bias_inc.resize( size );
-        //quad_coeff_inc.resize( size );
+        //quad_coeff_inc.resize( size );//quad_coeff_inc.resize( 1 );
     }
 
     // real two_lr = 2 * learning_rate;
+    real a_i = quad_coeff[0];
     for( int i=0 ; i<size ; ++i )
     {
-        real a_i = quad_coeff[i];
+        if(!share_quad_coeff)
+            a_i = quad_coeff[i];
         real in_grad_i = - output_gradient[i] / (2 * a_i * a_i);
         input_gradient[i] += in_grad_i;
 
@@ -208,10 +274,10 @@
             // update the quadratic coefficient:
             // a_i += learning_rate * out_grad_i * (b_i + input_i) / a_i^3
             // (or a_i += 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
-            quad_coeff[i] += two_lr * in_grad_i * (bias[i] + input[i])
-                                                    / quad_coeff[i];
-            if( quad_coeff[i] < min_quad_coeff )
-                quad_coeff[i] = min_quad_coeff;
+            a_i += two_lr * in_grad_i * (bias[i] + input[i])
+                                                    / a_i;
+            if( a_i < min_quad_coeff )
+                a_i = min_quad_coeff;
             */
         }
         else
@@ -228,10 +294,10 @@
             // a_i += a_inc_i
             quad_coeff_inc[i] += momentum * quad_coeff_inc[i]
                 + two_lr * in_grad_i * (bias[i] + input[i])
-                                         / quad_coeff[i];
-            quad_coeff[i] += quad_coeff_inc[i];
-            if( quad_coeff[i] < min_quad_coeff )
-                quad_coeff[i] = min_quad_coeff;
+                                         / a_i;
+            a_i += quad_coeff_inc[i];
+            if( a_i < min_quad_coeff )
+                a_i = min_quad_coeff;
             */
         }
     }
@@ -254,9 +320,7 @@
 
 void RBMGaussianLayer::forget()
 {
-    quad_coeff.fill( 2. );
-//    quad_coeff.fill( 1. );
-
+    quad_coeff.fill( 1. );
     inherited::forget();
 }
 
@@ -270,16 +334,28 @@
                   OptionBase::learntoption,
                   "Quadratic coefficients of the units.");
 
+    declareOption(ol, "share_quad_coeff", &RBMGaussianLayer::share_quad_coeff,
+                  OptionBase::buildoption,
+                  "Should all the units share the same quadratic coefficients?\n"
+		  "Suitable to avoid unstability (overfitting)  in cases where\n"
+		  "all the units have the same 'meaning'  (pixels of an image)");
+
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
 
 void RBMGaussianLayer::build_()
 {
-    sigma.resize( size );
+    if(share_quad_coeff)
+        size_quad_coeff=1;
+    else
+        size_quad_coeff=size;
+
+    sigma.resize( size_quad_coeff );
     sigma_is_up_to_date = false;
 
-    quad_coeff.resize( size );
+    quad_coeff.resize( size_quad_coeff );
     quad_coeff_pos_stats.resize( size );
     quad_coeff_neg_stats.resize( size );
 }
@@ -305,21 +381,36 @@
 
 void RBMGaussianLayer::accumulatePosStats( const Vec& pos_values )
 {
-    for( int i=0 ; i<size ; i++ )
-    {
-        real x_i = pos_values[i];
-        quad_coeff_pos_stats[i] += 2 * quad_coeff[i] * x_i * x_i;
-    }
+    if (share_quad_coeff)
+       for( int i=0 ; i<size ; i++ )
+       {
+           real x_i = pos_values[i];
+           quad_coeff_pos_stats[i] += 2 * quad_coeff[0] * x_i * x_i;
+       }
+    else
+       for( int i=0 ; i<size ; i++ )
+       {
+           real x_i = pos_values[i];
+           quad_coeff_pos_stats[i] += 2 * quad_coeff[i] * x_i * x_i;
+       }
+
     inherited::accumulatePosStats( pos_values );
 }
 
 void RBMGaussianLayer::accumulateNegStats( const Vec& neg_values )
 {
-    for( int i=0 ; i<size ; i++ )
-    {
-        real x_i = neg_values[i];
-        quad_coeff_neg_stats[i] += 2 * quad_coeff[i] * x_i * x_i;
-    }
+    if (share_quad_coeff)
+        for( int i=0 ; i<size ; i++ )
+        {
+            real x_i = neg_values[i];
+            quad_coeff_neg_stats[i] += 2 * quad_coeff[0] * x_i * x_i;
+        }
+    else
+        for( int i=0 ; i<size ; i++ )
+        {
+            real x_i = neg_values[i];
+            quad_coeff_neg_stats[i] += 2 * quad_coeff[i] * x_i * x_i;
+        }
     inherited::accumulateNegStats( neg_values );
 }
 
@@ -336,25 +427,52 @@
 
     if( momentum == 0. )
     {
-        // no need to use bias_inc
-        for( int i=0 ; i<size ; i++ )
-        {
-            a[i] += pos_factor * aps[i] + neg_factor * ans[i];
-            if( a[i] < min_quad_coeff )
-                a[i] = min_quad_coeff;
+        if(share_quad_coeff)
+	{
+	    real update=0;
+            for( int i=0 ; i<size ; i++ )
+            {
+                update += pos_factor * aps[i] + neg_factor * ans[i];
+            }
+	    a[0] += update/(real)size;
+            if( a[0] < min_quad_coeff )
+                a[0] = min_quad_coeff;
         }
+	else
+            for( int i=0 ; i<size ; i++ )
+            {
+                a[i] += pos_factor * aps[i] + neg_factor * ans[i];
+                if( a[i] < min_quad_coeff )
+                    a[i] = min_quad_coeff;
+            }
     }
     else
     {
-        quad_coeff_inc.resize( size );
-        real* ainc = quad_coeff_inc.data();
-        for( int i=0 ; i<size ; i++ )
-        {
-            ainc[i] = momentum*ainc[i] + pos_factor*aps[i] + neg_factor*ans[i];
-            a[i] += ainc[i];
-            if( a[i] < min_quad_coeff )
-                a[i] = min_quad_coeff;
+        if(share_quad_coeff)
+	{
+            quad_coeff_inc.resize( 1 );
+            real* ainc = quad_coeff_inc.data();
+            for( int i=0 ; i<size ; i++ )
+            {
+                ainc[0] = momentum*ainc[0] + pos_factor*aps[i] + neg_factor*ans[i];
+		ainc[0] /= (real)size;
+                a[0] += ainc[0];
+            }
+            if( a[0] < min_quad_coeff )
+                a[0] = min_quad_coeff;
         }
+	else
+	{
+            quad_coeff_inc.resize( size );
+            real* ainc = quad_coeff_inc.data();
+            for( int i=0 ; i<size ; i++ )
+            {
+                ainc[i] = momentum*ainc[i] + pos_factor*aps[i] + neg_factor*ans[i];
+                a[i] += ainc[i];
+                if( a[i] < min_quad_coeff )
+                    a[i] = min_quad_coeff;
+            }
+        }
     }
 
     // We will need to recompute sigma
@@ -375,25 +493,54 @@
 
     if( momentum == 0. )
     {
-        for( int i=0 ; i<size ; i++ )
-        {
-            a[i] += two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
-            if( a[i] < min_quad_coeff )
-                a[i] = min_quad_coeff;
+        if (share_quad_coeff)
+	{
+	    real update=0;
+            for( int i=0 ; i<size ; i++ )
+            {
+                update += two_lr * a[0] * (nv[i]*nv[i] - pv[i]*pv[i]);
+            }
+	    a[0] += update/(real)size;
+            if( a[0] < min_quad_coeff )
+                a[0] = min_quad_coeff;
         }
+	else
+            for( int i=0 ; i<size ; i++ )
+            {
+                a[i] += two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
+                if( a[i] < min_quad_coeff )
+                    a[i] = min_quad_coeff;
+            }	
     }
     else
     {
-        quad_coeff_inc.resize( size );
+        
         real* ainc = quad_coeff_inc.data();
-        for( int i=0 ; i<size ; i++ )
+        if(share_quad_coeff)
         {
-            ainc[i] = momentum*ainc[i]
-                + two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
-            a[i] += ainc[i];
-            if( a[i] < min_quad_coeff )
-                a[i] = min_quad_coeff;
+	   quad_coeff_inc.resize( 1 );
+           for( int i=0 ; i<size ; i++ )
+           {
+                ainc[0] = momentum*ainc[0]
+                    + two_lr * a[0] * (nv[i]*nv[i] - pv[i]*pv[i]);
+		ainc[0] /= (real)size;
+                a[0] += ainc[0];
+           }
+           if( a[0] < min_quad_coeff )
+               a[0] = min_quad_coeff;
         }
+        else
+	{
+	   quad_coeff_inc.resize( size );
+           for( int i=0 ; i<size ; i++ )
+           {
+                ainc[i] = momentum*ainc[i]
+                    + two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
+                a[i] += ainc[i];
+                if( a[i] < min_quad_coeff )
+                    a[i] = min_quad_coeff;
+           }
+        }
     }
 
     // We will need to recompute sigma
@@ -405,6 +552,7 @@
 
 void RBMGaussianLayer::update( const Mat& pos_values, const Mat& neg_values )
 {
+    
     PLASSERT( pos_values.width() == size );
     PLASSERT( neg_values.width() == size );
 
@@ -414,22 +562,37 @@
     // quad_coeff[i] -= learning_rate * 2 * quad_coeff[i] * (pos_values[i]^2
     //                                                       - neg_values[i]^2)
 
-    real two_lr = 2 * learning_rate;
+    real two_lr = 2 * learning_rate / batch_size;
     real* a = quad_coeff.data();
 
     if( momentum == 0. )
     {
-        for( int k=0; k<batch_size; k++ )
-        {
-            real *pv_k = pos_values[k];
-            real *nv_k = neg_values[k];
-            for( int i=0; i<size; i++ )
+        if (share_quad_coeff)
+            for( int k=0; k<batch_size; k++ )
             {
-                a[i] += two_lr * a[i] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
-                if( a[i] < min_quad_coeff )
-                    a[i] = min_quad_coeff;
+                real *pv_k = pos_values[k];
+                real *nv_k = neg_values[k];
+		real update=0;
+                for( int i=0; i<size; i++ )
+                {
+                    update += two_lr * a[0] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
+                }
+		a[0] += update/(real)size;
+                if( a[0] < min_quad_coeff )
+                    a[0] = min_quad_coeff;
             }
-        }
+	else
+            for( int k=0; k<batch_size; k++ )
+            {
+                real *pv_k = pos_values[k];
+                real *nv_k = neg_values[k];
+                for( int i=0; i<size; i++ )
+                {
+                    a[i] += two_lr * a[i] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
+                    if( a[i] < min_quad_coeff )
+                        a[i] = min_quad_coeff;
+                }
+            }
     }
     else
         PLCHECK_MSG( false,
@@ -453,11 +616,18 @@
         real* v = unit_values.data();
         real* a = quad_coeff.data();
         real* b = bias.data();
-        for(register int i=0; i<size; i++)
-        {
-            tmp = a[i]*v[i];
-            en += tmp*tmp + b[i]*v[i];
-        }
+	if(share_quad_coeff)
+            for(register int i=0; i<size; i++)
+            {
+                tmp = a[0]*v[i];
+                en += tmp*tmp + b[i]*v[i];
+            }
+	else
+            for(register int i=0; i<size; i++)
+            {
+                tmp = a[i]*v[i];
+                en += tmp*tmp + b[i]*v[i];
+            }
     }
     return en;
 }
@@ -469,14 +639,20 @@
     computeStdDeviation();
 
     real ret = 0;
-    for( int i=0 ; i<size ; i++ )
-    {
-        // ret += (target[i]-expectation[i])^2/(2 sigma[i]^2)
-        //      + log(sqrt(2*Pi) * sigma[i])
-
-        real r = (target[i] - expectation[i]) * quad_coeff[i];
-        ret += r * r + pl_log(sigma[i]);
-    }
+    if(share_quad_coeff)
+        for( int i=0 ; i<size ; i++ )
+        {
+            real r = (target[i] - expectation[i]) * quad_coeff[0];
+            ret += r * r + pl_log(sigma[0]);
+        }
+    else
+        for( int i=0 ; i<size ; i++ )
+        {
+            // ret += (target[i]-expectation[i])^2/(2 sigma[i]^2)
+            //      + log(sqrt(2*Pi) * sigma[i])
+            real r = (target[i] - expectation[i]) * quad_coeff[i];
+            ret += r * r + pl_log(sigma[i]);
+        }
     ret += 0.5*size*Log2Pi;
     return ret;
 }
@@ -496,21 +672,36 @@
     real nll;
     real *expectation, *target;
 
-    for (int k=0;k<batch_size;k++) // loop over minibatch
-    {
-        nll = 0;
-        expectation = expectations[k];
-        target = targets[k];
-        for( register int i=0 ; i<size ; i++ ) // loop over outputs
+    if(share_quad_coeff)
+        for (int k=0;k<batch_size;k++) // loop over minibatch
         {
-            // nll += (target[i]-expectation[i])^2/(2 sigma[i]^2)
-            //      + log(sqrt(2*Pi) * sigma[i])
-            real r = (target[i] - expectation[i]) * quad_coeff[i];
-            nll += r * r + pl_log(sigma[i]);
+            nll = 0;
+            expectation = expectations[k];
+            target = targets[k];
+            for( register int i=0 ; i<size ; i++ ) // loop over outputs
+            {
+                real r = (target[i] - expectation[i]) * quad_coeff[0];
+                nll += r * r + pl_log(sigma[0]);
+            }
+            nll += 0.5*size*Log2Pi;
+            costs_column(k,0) = nll;
         }
-        nll += 0.5*size*Log2Pi;
-        costs_column(k,0) = nll;
-    }
+    else
+        for (int k=0;k<batch_size;k++) // loop over minibatch
+        {
+            nll = 0;
+            expectation = expectations[k];
+            target = targets[k];
+            for( register int i=0 ; i<size ; i++ ) // loop over outputs
+            {
+                // nll += (target[i]-expectation[i])^2/(2 sigma[i]^2)
+                //      + log(sqrt(2*Pi) * sigma[i])
+                real r = (target[i] - expectation[i]) * quad_coeff[i];
+                nll += r * r + pl_log(sigma[i]);
+            }
+            nll += 0.5*size*Log2Pi;
+            costs_column(k,0) = nll;
+        }
 }
 
 void RBMGaussianLayer::bpropNLL(const Vec& target, real nll, Vec& bias_gradient)

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-20 19:04:58 UTC (rev 7622)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-21 00:14:35 UTC (rev 7623)
@@ -58,6 +58,12 @@
     //#####  Public Build Options  ############################################
 
     real min_quad_coeff;
+    
+    bool share_quad_coeff;
+    
+    //! Number of units when share_quad_coeff is False
+    //! or 1 when share_quad_coeff is True
+    int size_quad_coeff;
 
 
 public:
@@ -69,6 +75,8 @@
     //! Constructor from the number of units in the multinomial
     RBMGaussianLayer( int the_size, real the_learning_rate=0. );
 
+    //! Constructor from the number of units in the multinomial, with an aditional option
+    RBMGaussianLayer( int the_size, real the_learning_rate=0., bool do_share_quad_coeff=false );
 
     //! compute a sample, and update the sample field
     virtual void generateSample() ;

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-06-20 19:04:58 UTC (rev 7622)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-06-21 00:14:35 UTC (rev 7623)
@@ -471,7 +471,7 @@
     }
     else
     {
-        PLERROR("RBMLayer::update - Not implemented yet");
+        PLERROR("RBMLayer::update - Not implemented yet with momentum");
         /*
         bias_inc.resize( size );
         real* binc = bias_inc.data();

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-20 19:04:58 UTC (rev 7622)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-21 00:14:35 UTC (rev 7623)
@@ -57,7 +57,8 @@
     "  - 'visible' : expectations of the visible (normally input) layer\n"
     "  - 'hidden.state' : expectations of the hidden (normally output) layer\n"
     "  - 'hidden_activations.state' : activations of hidden units (given visible)\n"
-    "  - 'visible_sample' : random sample obtained on visible units\n"
+    "  - 'visible_sample' : random sample obtained on visible units (input or output port)\n"
+    "  - 'visible_expectation' : expectation of visible units (output port ONLY, for sampling)\n"
     "  - 'hidden_sample' : random sample obtained on hidden units\n"
     "  - 'energy' : energy of the joint (visible,hidden) pair or free-energy\n"
     "               of the visible (if given) or of the hidden (if given).\n"
@@ -279,6 +280,7 @@
     addPortName("hidden.state");
     addPortName("hidden_activations.state");
     addPortName("visible_sample");
+    addPortName("visible_expectation");
     addPortName("hidden_sample");
     addPortName("energy");
     addPortName("hidden_bias"); 
@@ -303,6 +305,7 @@
     if (visible_layer) {
         port_sizes(getPortIndex("visible"), 1) = visible_layer->size;
         port_sizes(getPortIndex("visible_sample"), 1) = visible_layer->size;
+        port_sizes(getPortIndex("visible_expectation"), 1) = visible_layer->size;
     }
     if (hidden_layer) {
         port_sizes(getPortIndex("hidden.state"), 1) = hidden_layer->size;
@@ -586,6 +589,7 @@
     Mat* hidden = ports_value[getPortIndex("hidden.state")];
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
     Mat* visible_sample = ports_value[getPortIndex("visible_sample")];
+    Mat* visible_expectation = ports_value[getPortIndex("visible_expectation")];
     Mat* hidden_sample = ports_value[getPortIndex("hidden_sample")];
     Mat* energy = ports_value[getPortIndex("energy")];
     Mat* neg_log_likelihood = ports_value[getPortIndex("neg_log_likelihood")];
@@ -780,29 +784,41 @@
         found_a_valid_configuration = true;
     } 
     
-    // SAMPLING inputs
-    if ((visible_sample && visible_sample->isEmpty()) // it is asked to sample the visible units
-        && (!visible || !visible->isEmpty() ))
-//        || (hidden_sample && hidden_sample->isEmpty())) // or to sample the hidden units
+    // SAMPLING
+    if ((visible_sample && visible_sample->isEmpty())               // is asked to sample visible units (discrete)
+        || (visible_expectation && visible_expectation->isEmpty())  //              "                   (continous)
+	|| (hidden_sample && hidden_sample->isEmpty()))             // or to sample hidden units
     {
         if (hidden_sample && !hidden_sample->isEmpty()) // sample visible conditionally on hidden
         {
             sampleVisibleGivenHidden(*hidden_sample);
-	    cout << "(discrete visible) sampling init (from hidden)" << endl;
+	    Gibbs_step=0;
+	    cout << "sampling init (from hidden)" << endl;
         }
+        else if (visible_sample && !visible_sample->isEmpty()) // if an input is provided, sample hidden conditionally
+        {
+            sampleHiddenGivenVisible(*visible_sample);
+	    Gibbs_step=0;
+	    cout << "sampling init (from visible)" << endl;
+	}
         else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
         {
-            sampleHiddenGivenVisible(*visible);
-            sampleVisibleGivenHidden(hidden_layer->samples);
-	    cout << "(discrete visible) sampling init (from visible)" << endl;
+            visible_layer->generateSamples();
+	    sampleHiddenGivenVisible(visible_layer->samples);
+	    Gibbs_step=0;
+	    cout << "sampling init (from visible)" << endl;
         }
+        else if (visible_expectation && !visible_expectation->isEmpty()) 
+        {
+	     PLERROR("In RBMModule::fprop visible_expectation can only be an output port (use visible as input port");
+	}
         else // sample unconditionally: Gibbs sample after k steps
         {
             // the visible_layer->expectations contain the "state" from which we
             // start or continue the chain
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
-            cout << "(discrete visible) gibbs " << Gibbs_step;
+            cout << "Gibbs sampling " << Gibbs_step;
             for (;Gibbs_step<min_n;Gibbs_step++)
             {
                 sampleHiddenGivenVisible(visible_layer->samples);
@@ -810,62 +826,37 @@
             }
   	    cout << " -> " << Gibbs_step-1 << endl;
         }
-        found_a_valid_configuration = true;
-    }
-    // SAMPLING continuous inputs (computing visible expectation)
-    if (visible && visible->isEmpty()) { // it is asked the expectations of visible units
-    
-        if (visible_sample && !visible_sample->isEmpty())
-        {
-            sampleHiddenGivenVisible(*visible_sample);
-            sampleVisibleGivenHidden(hidden_layer->samples);
-	    cout << "(continuous visible) sampling init (from visible)" << endl;
-	}
-	else if ( hidden_sample && !hidden_sample->isEmpty() )
-	{
-	   sampleVisibleGivenHidden(*hidden_sample);
-	   cout << "(continuous visible) sampling init (from hidden)" << endl;	   
-	}
-	else
-	{
-           int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
-                            min_n_Gibbs_steps);
-	   PLASSERT( min_n > 0 );
-           cout << "(continuous visible) gibbs " << Gibbs_step;
-	   for (;Gibbs_step<min_n;Gibbs_step++)
-           {
-                sampleHiddenGivenVisible(visible_layer->samples);
-                sampleVisibleGivenHidden(hidden_layer->samples);
-           }
-  	   cout << " -> " << Gibbs_step-1 << endl;
-	}
-        
+
 	if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
         {
    	      const Mat& hidden_expect = hidden_layer->getExpectations();
               hidden->resize(hidden_expect.length(), hidden_expect.width());
               *hidden << hidden_expect;
         }
-	 
-        const Mat& to_store = visible_layer->getExpectations();
-        visible->resize(to_store.length(), to_store.width());
-        *visible << to_store;
+        if (visible_sample && visible_sample->isEmpty()) // provide sample of the visible units
+        {
+            visible_sample->resize(visible_layer->samples.length(),
+                                   visible_layer->samples.width());
+            *visible_sample << visible_layer->samples;
+        }
+        if (hidden_sample && hidden_sample->isEmpty()) // provide sample of the hidden units
+        {
+            hidden_sample->resize(hidden_layer->samples.length(),
+                                  hidden_layer->samples.width());
+            *hidden_sample << hidden_layer->samples;
+        }
+        if (visible_expectation && visible_expectation->isEmpty()) // provide expectation of the visible units
+        {
+	    const Mat& to_store = visible_layer->getExpectations();
+            visible_expectation->resize(to_store.length(),
+	                                to_store.width());
+            *visible_expectation << to_store;
+        }
 
 	found_a_valid_configuration = true;
-    }
-    if (visible_sample && visible_sample->isEmpty()) // provide sample of the visible units
-    {
-        visible_sample->resize(visible_layer->samples.length(),
-                               visible_layer->samples.width());
-        *visible_sample << visible_layer->samples;
-    }
-    if (hidden_sample && hidden_sample->isEmpty()) // provide sample of the hidden units
-    {
-        hidden_sample->resize(hidden_layer->samples.length(),
-                              hidden_layer->samples.width());
-        *hidden_sample << hidden_layer->samples;
-    }
+    } // END SAMPLING
 
+
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)
     {



From dorionc at mail.berlios.de  Thu Jun 21 16:19:28 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 21 Jun 2007 16:19:28 +0200
Subject: [Plearn-commits] r7624 - trunk/python_modules/plearn/analysis
Message-ID: <200706211419.l5LEJSBa009733@sheep.berlios.de>

Author: dorionc
Date: 2007-06-21 16:19:28 +0200 (Thu, 21 Jun 2007)
New Revision: 7624

Added:
   trunk/python_modules/plearn/analysis/latex.py
Log:
First version of what shall replace report.formatter

Added: trunk/python_modules/plearn/analysis/latex.py
===================================================================
--- trunk/python_modules/plearn/analysis/latex.py	2007-06-21 00:14:35 UTC (rev 7623)
+++ trunk/python_modules/plearn/analysis/latex.py	2007-06-21 14:19:28 UTC (rev 7624)
@@ -0,0 +1,299 @@
+# latex.py
+# Copyright (C) 2006-2007 Christian Dorion
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Christian Dorion
+import os, sys
+from plearn.pyplearn.plearn_repr import plearn_repr
+
+DEFAULT_WRITER = sys.stdout.write
+
+def formatTable(table, headers=[], 
+                align="", super_headers=[], padding=0.5, vpadding=0.0, 
+                is_float=True, caption="", label="", tabular_pos="",
+                fontsize="", landscape=False, targetpos="", writer=DEFAULT_WRITER):
+    lwriter = lambda line : writer("%s\n"%line)
+    if align:
+        assert len(align)==len(table[0]), \
+               "%d != %d --- %s"%(len(align), len(table[0]), table[0])
+    else:
+        align = "c"*len(table[0])
+
+    if padding:
+        vpad = ""
+        if vpadding > 0:
+            vpad = r"\raisebox{%.3fcm}{\rule{0pt}{%.3fcm}}"%(-0.5*vpadding, vpadding)
+            
+        padded = r"%s@{\hspace{%.3fcm}%s}"%(align[0],padding,vpad)        
+        for a in align[1:-1]:
+            padded += "%s@{\\hspace{%.3fcm}}"%(a,2*padding)
+        if len(align) > 1:
+            padded += "%s@{\\hspace{%.3fcm}}"%(align[-1],padding)
+        align = padded    
+
+    if landscape:
+        lwriter(r"\begin{landscape}")
+        lwriter(r"\addtocounter{@inlandscapetable}{1}")
+
+    if is_float:
+        lwriter("\\begin{table}%s"%targetpos)    
+        lwriter("\\begin{center}")
+    else:
+        assert caption+label == ""
+    
+    if fontsize:
+        lwriter(fontsize)
+    lwriter("\\begin{tabular}%s{%s}"%(tabular_pos, align))
+    
+    if super_headers:
+        writer("  ")
+        formatTableLine(super_headers, writer)
+
+    if headers:
+        writer("  ")
+        formatTableLine(headers, writer)
+        lwriter("\\hline\\hline")
+    else:
+        lwriter("\\hline")
+ 
+    for line in table:
+        writer("  ")
+        if isinstance(line, str):
+            lwriter(line) # Single string is wrote as is...
+        else:
+            formatTableLine(line, writer)
+    lwriter("\\hline")
+       
+    lwriter("\\end{tabular}")
+
+    if is_float:
+        lwriter("\\end{center}")
+        if caption:
+            lwriter("    \\tabcaption{%s}"%caption)
+        if label:
+            lwriter("    \\label{table:%s}"%label)        
+        lwriter("\\end{table}")
+        
+    if landscape:
+        lwriter(r"\addtocounter{@inlandscapetable}{-1}")
+        lwriter(r"\end{landscape}")
+
+def formatTableLine(line, writer=DEFAULT_WRITER):
+    endl = r"\\"
+    handling_multicol = [] # For \multicolumn...
+    for elem in line: 
+        if elem is None:
+            assert handling_multicol \
+                and ( handling_multicol[-1].find("multicol") != -1
+                      or handling_multicol[-1].find("hline") != -1 )
+        elif elem == "NOENDL":
+            endl = ""
+        else:
+            handling_multicol.append(elem)            
+    writer('&'.join(handling_multicol) + endl + "\n")
+
+def strictlyUpperTriangularTable(table, headers=[], format="%s"):
+    """Returns a table of strings and modified headers suitable for latex/twikiTable.
+
+    The 'table' is assumed to be a square matrix of numbers in which the
+    subdiagonal AND diagonal elements are to be neglected. It can also be a
+    pair of tables, in which case the format is assumed to handles pairs.
+    """
+    N = len(table)
+    rows = iter(table)
+    columns = lambda row: iter(row)
+    if isinstance(table, tuple):
+        table, subtable = table
+        N = len(table)
+        rows = iter( zip(table,subtable) )
+        columns = lambda row: iter( zip(row[0], row[1]) )        
+    assert N==len(table[0])
+
+    formatted = []
+    for i, row in enumerate(rows):
+        if i == N-1:
+            break
+        
+        formatted_row = [ ]
+        if headers:
+            formatted_row.append(headers[i])
+            
+        for j, col in enumerate( columns(row) ):
+            if j == 0:
+                continue
+            elif j <= i:
+                formatted_row.append('-'),
+            else:
+                formatted_row.append( format%col )
+        formatted.append(formatted_row)
+
+    return formatted, ['']+headers[1:]
+
+def sideBySideTables(tables, headers, shared_headers):
+    """Puts tables side by side in a single table.
+
+    Tables are to be provided as a list ('tables') and 'headers' must be a
+    list of lists of headers. The shared headers will be written only once
+    at the begging of the merged table.
+    """
+    rows = len(tables[0])    
+    columns = len(headers[0])
+    shared_columns = len(shared_headers)
+    
+    merged_table = []
+    for r in range(rows):
+        row = tables[0][r]
+        if isinstance(row, str): # Single string is wrote as is...
+            merged_table.append(row)
+            continue
+        
+        merged_row = list(row[:shared_columns])
+        for table in tables:
+            row = table[r]            
+            merged_row.extend( row[shared_columns:] )
+            merged_row.append("")
+        assert merged_row.pop()=="" # Don't add empty col after last table
+        merged_table.append(merged_row)
+
+    merged_headers = list(shared_headers)
+    for hdrs in headers:
+        merged_headers.extend(hdrs[shared_columns:])
+        merged_headers.append("")
+    assert merged_headers.pop()=="" # Don't add empty col after last table
+
+    return merged_table, merged_headers
+
+def vpaddingLine(vpadding, length):
+    vpad = r"\raisebox{%.3fcm}{\rule{0pt}{%.3fcm}}"%(-0.5*vpadding, vpadding)
+    return [vpad]+[""]*(length-1)
+        
+#####  PDF creator  #########################################################
+
+TEX_BEGIN = r"""
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+%% Automatically generated by 'plearn.analysis.latex.createPDF()'
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+
+\documentclass[11pt]{article}
+\usepackage{apstat_article_style}
+\usepackage{lscape}
+
+%%%%%  My caption settings  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+
+\renewcommand{\tabcaption}[1]{%
+  \refstepcounter{table}%
+  {\parbox[t]{0.95\textwidth}{\raggedright\hspace{0pt}\small\it 
+      \captionheadfont\caparrowup%
+      \tablename~\mbox{\arabic{table}}.~%
+      \captionbodyfont{#1}}%
+  }}%
+
+\newcommand{\adjustedfigcaption}[2]{%
+  \refstepcounter{figure}%
+  {\parbox[t]{#1}{\raggedright\hspace{0pt}\small\it 
+      \captionheadfont\caparrowup%
+      \figurename~\mbox{\arabic{figure}}.~%
+      \captionbodyfont{#2}}%
+  }}%
+
+\newcommand{\figinclude}[3]{%
+  \begin{figure}[h]%
+    \centering%
+    \includegraphics[width={#1}]{#2} \\%
+    \adjustedfigcaption{#1}{#3}%
+  \end{figure}%
+}
+
+
+%%%%%  Settings for 2x2 figure pages  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+
+\newcommand{\multifiginclude}[3]{
+  \parbox[t]{#1}{%
+    \includegraphics[width={#1}]{#2} \\%
+    \adjustedfigcaption{#1}{#3}%
+  }}
+
+\newcommand{\TwoByTwoFigInclude}[2]{\multifiginclude{0.5\textwidth}{#1}{#2}}
+
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+
+\begin{document}
+
+"""
+
+FULL_SIZE = r"""
+%%% Page style and headers
+\setlength{\textheight}{\paperheight}
+% \setlength{\voffset}{-0.5in}
+\setlength{\topmargin}{0in}
+\setlength{\headheight}{0in}
+\setlength{\headsep}{0in}
+\setlength{\footskip}{0in}
+
+\setlength{\textwidth}{\paperwidth}
+% \setlength{\hoffset}{-0.5in}
+
+\setlength{\oddsidemargin}{0in}
+\setlength{\marginparwidth}{0pt}
+\setlength{\marginparsep}{0pt}
+\setlength{\evensidemargin}{0in}
+"""
+
+TEX_END = r"""
+
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+\cleardoublepage
+\end{document}
+
+"""
+
+def createPDF(file_name, content, landscape=False):
+    assert file_name.endswith('.tex')    
+    if isinstance(content, list):
+        content = '\n'.join(content)    
+    
+    tex_file = open(file_name, 'w')
+
+    lscape = ''
+    if landscape:
+        lscape = ',landscape'
+    tex_file.write(TEX_BEGIN)# % lscape)
+
+    tex_file.write(content)
+    tex_file.write(TEX_END)
+    tex_file.close()
+
+    #TBA: pdf_name = file_name.replace(".tex", '.pdf')
+    #TBA: os.system("rm -f %s"%pdf_name)
+    #TBA: os.system("pdflatex -interaction=nonstopmode %s >& /dev/null"%file_name)
+    #TBA: assert os.path.exists(pdf_name), "PDF could not be created!"
+    #TBA: os.system("pdflatex %s >& /dev/null"%file_name)
+    #TBA: os.system("pdflatex %s >& /dev/null"%file_name)



From nouiz at mail.berlios.de  Thu Jun 21 17:16:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Jun 2007 17:16:34 +0200
Subject: [Plearn-commits] r7625 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706211516.l5LFGYZq015035@sheep.berlios.de>

Author: nouiz
Date: 2007-06-21 17:16:33 +0200 (Thu, 21 Jun 2007)
New Revision: 7625

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.cc
Log:
-Made some global variable local
-The file created are put in the expdir directory


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.cc	2007-06-21 14:19:28 UTC (rev 7624)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Preprocessing.cc	2007-06-21 15:16:33 UTC (rev 7625)
@@ -154,17 +154,14 @@
     PPath                                 train_with_class_target_file_name;
     VMat                                  train_with_class_target_file;
     PP<ComputeDond2Target>                compute_target_learner;
-    BootstrapVMatrix*                     train_shufffled_vmatrix;
     VMat                                  train_shuffled_file;
     PPath                                 train_with_binary_fixed_file_name;
     VMat                                  train_with_binary_fixed_file;
     PP<FixDond2BinaryVariables>           fix_binary_variables_learner;
     PPath                                 train_with_ind_file_name;
-    MissingIndicatorVMatrix*              train_with_ind_vmatrix;
     VMat                                  train_with_ind_vmat;
     VMat                                  train_with_ind_file;
     Vec                                   train_with_ind_vector;
-    MeanMedianModeImputationVMatrix*      train_with_imp_vmatrix;
     VMat                                  mean_median_mode_with_ind_file;
     PPath                                 train_with_dichotomies_file_name;
     VMat                                  train_with_dichotomies_file;
@@ -173,9 +170,7 @@
     PP<DichotomizeDond2DiscreteVariables> dichotomize_discrete_variables_learner;
     SelectColumnsVMatrix*                 train_with_selected_columns_vmatrix;
     VMat                                  train_with_selected_columns_vmat;
-    SelectColumnsVMatrix*                 mean_median_mode_with_selected_columns_vmatrix;
     VMat                                  mean_median_mode_with_selected_columns_vmat;
-    GaussianizeVMatrix*                   train_gaussianized_vmatrix;
     VMat                                  train_gaussianized_vmat;
     GaussianizeVMatrix*                   mean_median_mode_gaussianized_vmatrix;
     VMat                                  mean_median_mode_gaussianized_vmat;
@@ -199,7 +194,8 @@
     cout << endl << "****** STEP 1 ******" << endl;
     cout << "The first step groups variables by type, skips untrustworthy variables, and generate class targets" << endl;
     cout << "It uses ComputeDond2Target to transform base_train.pmat into step1_train_with_class_target.pmat" << endl;
-    output_path = "step1_train_with_class_target";
+    output_path = expdir+"step1_train_with_class_target";
+    cout << "output_path" << output_path;
     train_with_class_target_file_name = output_path + ".pmat";
     if (isfile(train_with_class_target_file_name))
     {
@@ -219,7 +215,7 @@
     cout << "This step shuffles the training set to get training data in random order." << endl;
     cout << "It uses BootstrapVMatrix to transform step1_train_with_class_target.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 3" << endl;
-    output_path = "step3_train_with_binary_fixed";
+    output_path = expdir+"step3_train_with_binary_fixed";
     train_with_binary_fixed_file_name = output_path + ".pmat";
     if (isfile(train_with_binary_fixed_file_name))
     {
@@ -227,7 +223,7 @@
     }
     else 
     {
-        train_shufffled_vmatrix = new BootstrapVMatrix();
+        BootstrapVMatrix* train_shufffled_vmatrix = new BootstrapVMatrix();
         train_shufffled_vmatrix->shuffle = 1;
         train_shufffled_vmatrix->frac = 1.0;
         train_shufffled_vmatrix->own_seed = 123456;
@@ -264,7 +260,7 @@
     }
     else 
     {
-        train_with_ind_vmatrix = new MissingIndicatorVMatrix();
+        MissingIndicatorVMatrix* train_with_ind_vmatrix = new MissingIndicatorVMatrix();
         train_with_ind_vmatrix->source = train_with_binary_fixed_file;
         train_with_ind_vmatrix->train_set = train_with_binary_fixed_file;
         train_with_ind_vmatrix->number_of_train_samples_to_use = 30000.0;
@@ -289,17 +285,19 @@
     cout << "The resulting vitual view is not used." << endl;
     cout << "But the mean, median and mode vectors have to go thru the same transformation than the training file" << endl;
     cout << "from here on to the end of the preprocessing steps.." << endl;
-    train_with_imp_vmatrix = new MeanMedianModeImputationVMatrix();
-    train_with_imp_vmatrix->source = train_with_ind_file;
-    train_with_imp_vmatrix->train_set = train_with_ind_file;
-    train_with_imp_vmatrix->number_of_train_samples_to_use = 30000.0;
-    train_with_imp_vmatrix->imputation_spec = imputation_spec;
-    train_with_imp_vmatrix->build();
-    mean_median_mode_with_ind_file = train_with_imp_vmatrix->getMeanMedianModeFile();
+    { 
+        MeanMedianModeImputationVMatrix* train_with_imp_vmatrix = new MeanMedianModeImputationVMatrix();
+        train_with_imp_vmatrix->source = train_with_ind_file;
+        train_with_imp_vmatrix->train_set = train_with_ind_file;
+        train_with_imp_vmatrix->number_of_train_samples_to_use = 30000.0;
+        train_with_imp_vmatrix->imputation_spec = imputation_spec;
+        train_with_imp_vmatrix->build();
+        mean_median_mode_with_ind_file = train_with_imp_vmatrix->getMeanMedianModeFile();
+    }
     cout << endl << "****** STEP 6 ******" << endl;
     cout << "This steps generates as many dichotomized variables as there are significant code values." << endl;
     cout << "It uses DichotomizeDond2DiscreteVariables to transform step4_train_with_ind.pmat into step6_train_with_dichotomies.pmat" << endl;
-    output_path = "step6_train_with_dichotomies";
+    output_path = expdir+"step6_train_with_dichotomies";
     train_with_dichotomies_file_name = output_path + ".pmat";
     if (isfile(train_with_dichotomies_file_name))
     {
@@ -319,7 +317,7 @@
     cout << "This steps does the same thing to the mean, median and mode vectors." << endl;
     cout << "It uses DichotomizeDond2DiscreteVariables to transform step4_train_with_ind.pmat.metadata/mean_median_mode_file.pmat "
          << "into step6_train_with_dichotomies.pmat.metadata/mean_median_mode_file.pmat" << endl;
-    output_path = train_with_dichotomies_file_name + ".metadata/mean_median_mode_file";
+    output_path = expdir+train_with_dichotomies_file_name + ".metadata/mean_median_mode_file";
     mean_median_mode_with_dichotmies_file_name = output_path + ".pmat";
     if (isfile(mean_median_mode_with_dichotmies_file_name))
     {
@@ -339,7 +337,7 @@
     cout << "This step select the desired columns from the training set to create the input records." << endl;
     cout << "It uses SelectColumnsVMatrix to transform step6_train_with_dichotomies.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 10" << endl;
-    output_path = "final_train_input_preprocessed";
+    output_path = expdir+"final_train_input_preprocessed";
     train_input_preprocessed_file_name = output_path + ".pmat";
     if (isfile(train_input_preprocessed_file_name))
     {
@@ -360,7 +358,7 @@
     cout << "This step does the same thing to the mean, median and mode vectors." << endl;
     cout << "It uses SelectColumnsVMatrix to transform step6_train_with_dichotomies.pmat.metadata/mean_median_mode_file.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 11" << endl;
-    output_path = train_input_preprocessed_file_name + ".metadata/mean_median_mode_file";
+    output_path = expdir+train_input_preprocessed_file_name + ".metadata/mean_median_mode_file";
     mean_median_mode_input_preprocessed_file_name = output_path + ".pmat";
     if (isfile(mean_median_mode_input_preprocessed_file_name))
     {
@@ -368,7 +366,7 @@
     }
     else 
     {
-        mean_median_mode_with_selected_columns_vmatrix = new SelectColumnsVMatrix();
+        SelectColumnsVMatrix* mean_median_mode_with_selected_columns_vmatrix = new SelectColumnsVMatrix();
         mean_median_mode_with_selected_columns_vmatrix->source = mean_median_mode_with_dichotmies_file;
         mean_median_mode_with_selected_columns_vmatrix->fields_partial_match = 0;
         mean_median_mode_with_selected_columns_vmatrix->extend_with_missing = 0;
@@ -387,7 +385,7 @@
     }
     else 
     {
-        train_gaussianized_vmatrix = new GaussianizeVMatrix();
+        GaussianizeVMatrix* train_gaussianized_vmatrix = new GaussianizeVMatrix();
         train_gaussianized_vmatrix->source = train_with_selected_columns_vmat;
         train_gaussianized_vmatrix->train_source = train_with_selected_columns_vmat;
         train_gaussianized_vmatrix->threshold_ratio = 1;
@@ -469,7 +467,7 @@
     cout << "This step select the desired columns from the training set to create the target records." << endl;
     cout << "It uses SelectColumnsVMatrix to transform step6_train_with_dichotomies.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 15" << endl;
-    output_path = "final_train_target_preprocessed";
+    output_path = expdir+"final_train_target_preprocessed";
     train_target_preprocessed_file_name = output_path + ".pmat";
     if (isfile(train_target_preprocessed_file_name))
     {
@@ -563,7 +561,7 @@
     cout << endl << "****** STEP 1 ******" << endl;
     cout << "The first step groups variables by type, skips untrustworthy variables, and generate class targets" << endl;
     cout << "It uses ComputeDond2Target to transform base_test.pmat into step1_test_with_class_target.pmat" << endl;
-    output_path = "step1_test_with_class_target";
+    output_path = expdir+"step1_test_with_class_target";
     test_with_class_target_file_name = output_path + ".pmat";
     if (isfile(test_with_class_target_file_name))
     {
@@ -584,7 +582,7 @@
     cout << endl << "****** STEP 3 ******" << endl;
     cout << "For strictly binary variables, various situations arise: zero or non-zero, missing or not-missing, a given value or not, etc..." << endl;
     cout << "This step uses FixDond2BinaryVariables to create step3_test_with_binary_fixed.pmat with 0-1 binary variables." << endl;
-    output_path = "step3_test_with_binary_fixed";
+    output_path = expdir+"step3_test_with_binary_fixed";
     test_with_binary_fixed_file_name = output_path + ".pmat";
     if (isfile(test_with_binary_fixed_file_name))
     {
@@ -603,7 +601,7 @@
     cout << "This step adds missing indicators variables to each variable with missing values." << endl;
     cout << "It uses MissingIndicatorVMatrix to transform step3_test_with_binary_fixed.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 6" << endl;
-    output_path = "step6_test_with_dichotomies";
+    output_path = expdir+"step6_test_with_dichotomies";
     test_with_dichotomies_file_name = output_path + ".pmat";
     if (isfile(test_with_dichotomies_file_name))
     {
@@ -643,7 +641,7 @@
     cout << "This step select the desired columns from the test set to create the input records." << endl;
     cout << "It uses SelectColumnsVMatrix to transform step6_test_with_dichotomies.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 10" << endl;
-    output_path = "final_test_input_preprocessed";
+    output_path = expdir+"final_test_input_preprocessed";
     test_input_preprocessed_file_name = output_path + ".pmat";
     if (isfile(test_input_preprocessed_file_name))
     {
@@ -713,7 +711,7 @@
     cout << "This step select the desired columns from the testing set to create the target records." << endl;
     cout << "It uses SelectColumnsVMatrix to transform step6_test_with_dichotomies.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 15" << endl;
-    output_path = "final_test_target_preprocessed";
+    output_path = expdir+"final_test_target_preprocessed";
     test_target_preprocessed_file_name = output_path + ".pmat";
     if (isfile(test_target_preprocessed_file_name))
     {
@@ -800,7 +798,7 @@
     cout << endl << "****** STEP 1 ******" << endl;
     cout << "The first step groups variables by type, skips untrustworthy variables, and generate class targets" << endl;
     cout << "It uses ComputeDond2Target to transform base_unknown.pmat into step1_unknown_with_class_target.pmat" << endl;
-    output_path = "step1_unknown_with_class_target";
+    output_path = expdir+"step1_unknown_with_class_target";
     unknown_with_class_target_file_name = output_path + ".pmat";
     if (isfile(unknown_with_class_target_file_name))
     {
@@ -821,7 +819,7 @@
     cout << endl << "****** STEP 3 ******" << endl;
     cout << "For strictly binary variables, various situations arise: zero or non-zero, missing or not-missing, a given value or not, etc..." << endl;
     cout << "This step uses FixDond2BinaryVariables to create step3_unknown_with_binary_fixed.pmat with 0-1 binary variables." << endl;
-    output_path = "step3_unknown_with_binary_fixed";
+    output_path = expdir+"step3_unknown_with_binary_fixed";
     unknown_with_binary_fixed_file_name = output_path + ".pmat";
     if (isfile(unknown_with_binary_fixed_file_name))
     {
@@ -840,7 +838,7 @@
     cout << "This step adds missing indicators variables to each variable with missing values." << endl;
     cout << "It uses MissingIndicatorVMatrix to transform step3_unknown_with_binary_fixed.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 6" << endl;
-    output_path = "step6_unknown_with_dichotomies";
+    output_path = expdir+"step6_unknown_with_dichotomies";
     unknown_with_dichotomies_file_name = output_path + ".pmat";
     if (isfile(unknown_with_dichotomies_file_name))
     {
@@ -880,7 +878,7 @@
     cout << "This step select the desired columns from the unknown set to create the input records." << endl;
     cout << "It uses SelectColumnsVMatrix to transform step6_unknown_with_dichotomies.pmat" << endl;
     cout << "The resulting vitual view is not stored on disk, it is fed as input to step 10" << endl;
-    output_path = "final_unknown_input_preprocessed";
+    output_path = expdir+"final_unknown_input_preprocessed";
     unknown_input_preprocessed_file_name = output_path + ".pmat";
     if (isfile(unknown_input_preprocessed_file_name))
     {



From nouiz at mail.berlios.de  Thu Jun 21 17:19:55 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Jun 2007 17:19:55 +0200
Subject: [Plearn-commits] r7626 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706211519.l5LFJti6015360@sheep.berlios.de>

Author: nouiz
Date: 2007-06-21 17:19:54 +0200 (Thu, 21 Jun 2007)
New Revision: 7626

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h
Log:
Changed buildoption to take directly the filename instead of a directory, that way the filename can be anything


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.cc	2007-06-21 15:16:33 UTC (rev 7625)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.cc	2007-06-21 15:19:54 UTC (rev 7626)
@@ -78,9 +78,12 @@
     declareOption(ol, "target_field_names", &NeighborhoodConditionalMean::target_field_names,
                   OptionBase::buildoption,
                   "The vector of names of the field to select from the target_set as target for the built training files.");
-    declareOption(ol, "dir_offset", &NeighborhoodConditionalMean::dir_offset,
+    declareOption(ol, "train_covariance_file_name", &NeighborhoodConditionalMean::train_covariance_file_name,
                   OptionBase::buildoption,
-                  "The directory offset where to find and/or create the various files.");
+                  "The path to the file train set where missing value are imputed by the covariance preservation algo.");
+    declareOption(ol, "test_train_covariance_file_name", &NeighborhoodConditionalMean::test_train_covariance_file_name,
+                  OptionBase::buildoption,
+                  "The path to the file test_train set where missing value are imputed by the covariance preservation algo.");
     declareOption(ol, "various_ks", &NeighborhoodConditionalMean::various_ks,
                   OptionBase::buildoption,
                   "The vector of various Ks to experiment with. Values must be between 1 and 100.");
@@ -110,7 +113,8 @@
     deepCopyField(number_of_test_samples, copies);
     deepCopyField(number_of_train_samples, copies);
     deepCopyField(target_field_names, copies);
-    deepCopyField(dir_offset, copies);
+    deepCopyField(test_train_covariance_file_name, copies);
+    deepCopyField(train_covariance_file_name, copies);
     deepCopyField(various_ks, copies);
     deepCopyField(deletion_thresholds, copies);
     deepCopyField(experiment_name, copies);
@@ -168,13 +172,13 @@
     cout << "The Covariance PreservationVMatrix creates a covariance_file in the metadata of the  source file" << endl;
     cout << "if it is not already there." << endl;
     cout << "The file is kept in train_imputed_with_covariance_preservation.pmat." << endl;
-    if (dir_offset != "") dir_offset += "/";
-    train_covariance_name = dir_offset + "train_imputed_with_covariance_preservation.pmat";
-    if (isfile(train_covariance_name))
+    if( train_covariance_file_name == "" )
+        PLERROR("In NeighborhoodConditionalMean::computeNeighborhood() train_covariance_file_name must not be empty",train_covariance_file_name.c_str());
+    if (isfile(train_covariance_file_name))
     {
-        train_covariance_file = new FileVMatrix(train_covariance_name);
+        train_covariance_file = new FileVMatrix(train_covariance_file_name);
         train_covariance_file->defineSizes(train_covariance_file->width(), 0, 0);
-        cout << train_covariance_name << " already exist, we are skipping this step." << endl;
+        cout << train_covariance_file_name << " already exist, we are skipping this step." << endl;
     }
     else 
     {
@@ -183,7 +187,7 @@
         train_covariance_vmatrix->train_set = train_set;
         train_covariance_vmatrix->build();
         train_covariance_vmat = train_covariance_vmatrix;
-        train_covariance_file = new FileVMatrix(train_covariance_name, train_covariance_vmat->length(), train_covariance_vmat->fieldNames());
+        train_covariance_file = new FileVMatrix(train_covariance_file_name, train_covariance_vmat->length(), train_covariance_vmat->fieldNames());
         train_covariance_file->defineSizes(train_covariance_vmat->width(), 0, 0);
         pb = new ProgressBar("Saving the train file imputed with the covariance preservation", train_covariance_vmat->length());
         train_covariance_vector.resize(train_covariance_vmat->width());
@@ -199,7 +203,8 @@
     cout << "We do the same thing with the test_train dataset" << endl;
     cout << "using the covariance file created at the previous step." << endl;
     cout << "The file is kept in test_train_imputed_with_covariance_preservation.pmat." << endl;
-    test_train_covariance_file_name = dir_offset + "test_train_imputed_with_covariance_preservation.pmat";
+    if( test_train_covariance_file_name == "" )
+        PLERROR("In NeighborhoodConditionalMean::computeNeighborhood() test_train_covariance_file_name must not be empty",test_train_covariance_file_name.c_str());
     if (isfile(test_train_covariance_file_name))
     {
         test_train_covariance_file = new FileVMatrix(test_train_covariance_file_name);
@@ -308,7 +313,7 @@
     cout << "We perform the imputaton with the selected number of neighbors." << endl;
     cout << "The resulting file is loaded in memory to be passed to the experimentation script." << endl;
     test_train_neighbor_imputation_vmatrix = new NeighborhoodImputationVMatrix();
-    test_train_neighbor_imputation_vmatrix->source_with_missing = test_train_input_set;
+    test_train_neighbor_imputation_vmatrix->source = test_train_input_set;
     test_train_neighbor_imputation_vmatrix->reference_index = test_train_neighborhood_file;
     test_train_neighbor_imputation_vmatrix->reference_with_missing = train_set;
     test_train_neighbor_imputation_vmatrix->reference_with_covariance_preserved = train_covariance_file;

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h	2007-06-21 15:16:33 UTC (rev 7625)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h	2007-06-21 15:19:54 UTC (rev 7626)
@@ -80,8 +80,8 @@
     int number_of_train_samples;
     //! The vector of names of the field to select from the target_set as target for the built training files.
     TVec<string> target_field_names;
-    //! The directory offset where to find and/or create the various files.
-    string dir_offset;
+    PPath train_covariance_file_name;
+    PPath test_train_covariance_file_name;    
     //! The vector of various Ks to experiment with. Values must be between 1 and 100.
     TVec<int> various_ks;
     //! The vector of thresholds to be tested for each of the various Ks.
@@ -144,12 +144,10 @@
 
     // The rest of the private stuff goes here
     ProgressBar*                              pb;
-    PPath                                     train_covariance_name;
     VMat                                      train_covariance_file;
     CovariancePreservationImputationVMatrix*  train_covariance_vmatrix;
     VMat                                      train_covariance_vmat;
     Vec                                       train_covariance_vector;
-    PPath                                     test_train_covariance_file_name;
     VMat                                      test_train_covariance_file;
     CovariancePreservationImputationVMatrix*  test_train_covariance_vmatrix;
     VMat                                      test_train_covariance_vmat;



From nouiz at mail.berlios.de  Thu Jun 21 17:24:11 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Jun 2007 17:24:11 +0200
Subject: [Plearn-commits] r7627 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706211524.l5LFOB8T015888@sheep.berlios.de>

Author: nouiz
Date: 2007-06-21 17:24:11 +0200 (Thu, 21 Jun 2007)
New Revision: 7627

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
Log:
-Made some global variable local when it make sense
-Added a test function at the end that verify if some variable have no stats in the output file


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-06-21 15:19:54 UTC (rev 7626)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-06-21 15:24:11 UTC (rev 7627)
@@ -142,6 +142,7 @@
     if (train_set)
     {
         build_ball_tree();
+        output_file_name = train_metadata + "/TestImputation/output.pmat";
         for (int iteration = 1; iteration <= train_set->width(); iteration++)
         {
             cout << "In TestImputations, Iteration # " << iteration << endl;
@@ -152,16 +153,14 @@
             computeNeighborhoodStats();
             train();
         }
-        PLERROR("In TestImputations::build_(): we are done here");
+        endtestimputation("In TestImputations::build_(): we are done here");
     }
 }
 
 void TestImputations::build_ball_tree()
-{
-    // initialize primary dataset
+{ 
+   // initialize primary dataset
     cout << "initialize the train set" << endl;
-    train_row = 0;
-    train_col = 0;
     train_length = train_set->length();
     train_width = train_set->width();
     train_input.resize(train_width);
@@ -170,8 +169,9 @@
     train_metadata = train_set->getMetaDataDir();
     weights.resize(train_width);
     weights.fill(1.0);
-    for (mi_col = 0; mi_col < missing_indicators.length(); mi_col++)
+    for (int mi_col = 0; mi_col < missing_indicators.length(); mi_col++)
     {
+        int train_col;
         for (train_col = 0; train_col < train_width; train_col++)
         {
             if (missing_indicators[mi_col] != train_names[train_col]) continue;
@@ -279,7 +279,22 @@
 */
 
 }
-
+void TestImputations::endtestimputation(const char* msg, ...){
+    va_list args;
+    va_start(args,msg);
+    getHeaderRecord();
+    for (int train_col = 0; train_col < train_width; train_col++)
+    {
+        if (header_record[train_col] == 1.0) 
+            PLWARNING("No all variable done!!!");
+        else if (header_record[train_col] == 2.0){
+            getOutputRecord(train_col);
+            if(output_record[100]==0.0)
+                PLWARNING("Element %d,%d is at zero in the output file. Meaby this variable was not treated",train_col,100);
+        }
+    }
+    PLERROR(msg,args);
+}
 void TestImputations::initialize()
 {
     
@@ -297,7 +312,7 @@
     to_deal_with_total = 0;
     to_deal_with_next = -1;
 
-    for (train_col = 0; train_col < train_width; train_col++)
+    for (int train_col = 0; train_col < train_width; train_col++)
     {
         if (header_record[train_col] != 1.0) continue;
         to_deal_with_total += 1;
@@ -308,27 +323,27 @@
     {
         train_set->unlockMetaDataDir();
         // reviewGlobalStats();
-        PLERROR("In TestImputations::initialize() we are done here");
+        endtestimputation("In TestImputations::initialize() we are done here");
     }
-    cout << "next variable to deal with: " << train_names[to_deal_with_next] << endl;
+    cout << "next variable to deal with: " << train_names[to_deal_with_next] << "("<<to_deal_with_next<<")"<<endl;
     to_deal_with_name = train_names[to_deal_with_next];
     updateHeaderRecord(to_deal_with_next);
     train_set->unlockMetaDataDir();
     
     // find the available samples with non-missing values for this variable
-    pb = 0;
     train_stats = train_set->getStats(to_deal_with_next);
     train_total = train_stats.n();
     train_missing = train_stats.nmissing();
     train_present = train_total - train_missing;
     indices.resize((int) train_present);
-    ind_next = 0;
-    pb = new ProgressBar( "Building the indices for " + to_deal_with_name, train_length);
-    for (train_row = 0; train_row < train_length; train_row++)
+    int ind_next = 0;
+    ProgressBar* pb = new ProgressBar( "Building the indices for " + to_deal_with_name, train_length);
+    for (int train_row = 0; train_row < train_length; train_row++)
     {
         to_deal_with_value = train_set->get(train_row, to_deal_with_next);
         if (is_missing(to_deal_with_value)) continue;
-        if (ind_next >= indices.length()) PLERROR("In TestImputations::initialize() There seems to be more present values than indicated by the stats file");
+        if (ind_next >= indices.length()) 
+            PLERROR("In TestImputations::initialize() There seems to be more present values than indicated by the stats file");
         indices[ind_next] = train_row;
         ind_next += 1;
         pb->update( train_row );
@@ -341,11 +356,14 @@
     
     // load the test samples for this variable
     if (indices.length() > max_number_of_samples) test_length = max_number_of_samples;
+    else if (indices.length() < min_number_of_samples)
+        PLERROR("TestImputations::initialize() Their is less examples for the variable %s then the min_number_of semples(%d)",
+                to_deal_with_name.c_str(),min_number_of_samples);
     else test_length = indices.length();
     test_width = train_width;
     test_samples_set = new MemoryVMatrix(test_length, test_width);
     pb = new ProgressBar( "Loading the test samples for " + to_deal_with_name, test_length);
-    for (test_row = 0; test_row < test_length; test_row++)
+    for (int test_row = 0; test_row < test_length; test_row++)
     {
         train_set->getRow(indices[test_row], train_input);
         test_samples_set->putRow(test_row, train_input);
@@ -357,19 +375,19 @@
 void TestImputations::computeMeanMedianModeStats()
 {
     if (!isfile(mean_median_mode_file_name)) PLERROR("In TestImputations::computeMeanMedianModeStats() a valid mean_median_mode_file path must be provided.");
-    mmmf_file = new FileVMatrix(mean_median_mode_file_name);
-    mmmf_length = mmmf_file->length();
-    mmmf_width = mmmf_file->width();
+    VMat mmmf_file = new FileVMatrix(mean_median_mode_file_name);
+    int mmmf_length = mmmf_file->length();
+    int mmmf_width = mmmf_file->width();
     if (mmmf_length != 3) PLERROR("In TestImputations::computeMeanMedianModeStats() there should be exactly 3 records in the mmm file, got %i.", mmmf_length);
     if (mmmf_width != train_width) PLERROR("In TestImputations::computeMeanMedianModeStats() train set and mmm width should be the same, got %i.", mmmf_width);
-    mmmf_mean = mmmf_file->get(0, to_deal_with_next);
-    mmmf_median = mmmf_file->get(1, to_deal_with_next);
-    mmmf_mode = mmmf_file->get(2, to_deal_with_next);
+    real mmmf_mean = mmmf_file->get(0, to_deal_with_next);
+    real mmmf_median = mmmf_file->get(1, to_deal_with_next);
+    real mmmf_mode = mmmf_file->get(2, to_deal_with_next);
     mmmf_mean_err = 0.0;
     mmmf_median_err = 0.0;
     mmmf_mode_err = 0.0;
-    pb = new ProgressBar( "computing the mean, median and mode imputation errors for " + to_deal_with_name, test_length);
-    for (test_row = 0; test_row < test_length; test_row++)
+    ProgressBar* pb = new ProgressBar( "computing the mean, median and mode imputation errors for " + to_deal_with_name, test_length);
+    for (int test_row = 0; test_row < test_length; test_row++)
     {
         to_deal_with_value = test_samples_set->get(test_row, to_deal_with_next);
         mmmf_mean_err += pow(to_deal_with_value - mmmf_mean, 2.0);
@@ -389,13 +407,13 @@
     if (!isfile(tcmf_file_name)) 
         PLERROR("In TestImputations::computeTreeCondMeanStats(): The '%s' file was not found in the tcf directory.",tcmf_file_name.c_str());
     tcmf_file = new FileVMatrix(tcmf_file_name);
-    tcmf_length = tcmf_file->length();
-    tcmf_width = tcmf_file->width();
+    int tcmf_length = tcmf_file->length();
+    int tcmf_width = tcmf_file->width();
     if (tcmf_length < train_length) 
         PLERROR("In TestImputations::computeTreeCondMeanStats(): there are only %d records in the tree conditional output file. We need %d.",tcmf_length,train_length);
     tcmf_mean_err = 0.0;
-    pb = new ProgressBar( "computing the tree conditional mean imputation errors for " + to_deal_with_name, test_length);
-    for (test_row = 0; test_row < test_length; test_row++)
+    ProgressBar* pb = new ProgressBar( "computing the tree conditional mean imputation errors for " + to_deal_with_name, test_length);
+    for (int test_row = 0; test_row < test_length; test_row++)
     {
         to_deal_with_value = test_samples_set->get(test_row, to_deal_with_next);
         tcmf_mean_err += pow(to_deal_with_value - tcmf_file->get(indices[test_row], 0), 2.0);
@@ -408,29 +426,30 @@
 void TestImputations::computeCovPresStats()
 {
     if (!isfile(covariance_preservation_file_name)) PLERROR("In TestImputations::computeCovPresStats() a valid covariance_preservation_file path must be provided.");
-    cvpf_file = new FileVMatrix(covariance_preservation_file_name);
-    cvpf_length = cvpf_file->length();
-    cvpf_width = cvpf_file->width();
-    if (cvpf_length != train_width + 1) PLERROR("In TestImputations::computeCovPresStats() there should be %i records in the cvp file, got %i.",
-                                                train_width + 1, cvpf_length);
-    if (cvpf_width != train_width) PLERROR("In TestImputations::computeCovPresStats() train set and cvp width should be the same, got %i.", cvpf_width);
-    cvpf_file = new FileVMatrix(covariance_preservation_file_name);
+    VMat cvpf_file = new FileVMatrix(covariance_preservation_file_name);
+    int cvpf_length = cvpf_file->length();
+    int cvpf_width = cvpf_file->width();
+    if (cvpf_length != train_width + 1)
+        PLERROR("In TestImputations::computeCovPresStats() there should be %i records in the cvp file, got %i.", train_width + 1, cvpf_length);
+    if (cvpf_width != train_width)
+        PLERROR("In TestImputations::computeCovPresStats() train set and cvp width should be the same, got %i.", cvpf_width);
+    //cvpf_file = new FileVMatrix(covariance_preservation_file_name);
     cvpf_cov.resize(train_width, train_width);
     cvpf_mu.resize(train_width);
-    for (cvpf_row = 0; cvpf_row < train_width; cvpf_row++)
+    for (int cvpf_row = 0; cvpf_row < train_width; cvpf_row++)
     {
-        for (cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
+        for (int cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
         {
             cvpf_cov(cvpf_row, cvpf_col) = cvpf_file->get(cvpf_row, cvpf_col);
         }
     }
-    for (cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
+    for (int cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
     {
         cvpf_mu[cvpf_col] = cvpf_file->get(train_width, cvpf_col);
     }
     cvpf_mean_err = 0.0;
-    pb = new ProgressBar( "computing the covariance preservation imputation errors for " + to_deal_with_name, test_length);
-    for (test_row = 0; test_row < test_length; test_row++)
+    ProgressBar* pb = new ProgressBar( "computing the covariance preservation imputation errors for " + to_deal_with_name, test_length);
+    for (int test_row = 0; test_row < test_length; test_row++)
     {
         test_samples_set->getRow(test_row, train_input);
         cvpf_mean_err += pow(to_deal_with_value - covariancePreservationValue(to_deal_with_next), 2.0);
@@ -442,15 +461,16 @@
 
 real TestImputations::covariancePreservationValue(int col)
 {
-    cvpf_sum_cov_xl = 0;
-    cvpf_sum_xl_square = 0;
-    for (cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
+    real cvpf_sum_cov_xl = 0;
+    real cvpf_sum_xl_square = 0;
+    for (int cvpf_col = 0; cvpf_col < train_width; cvpf_col++)
     {
         if (cvpf_col == col) continue;
         if (is_missing(train_input[cvpf_col])) continue;
         cvpf_sum_cov_xl += cvpf_cov(cvpf_col, col) * (train_input[cvpf_col] - cvpf_mu[cvpf_col]);
         cvpf_sum_xl_square += (train_input[cvpf_col] - cvpf_mu[cvpf_col]) * (train_input[cvpf_col] - cvpf_mu[cvpf_col]);
     }
+    real cvpf_value;
     if (cvpf_sum_xl_square == 0.0) cvpf_value = cvpf_mu[col];
     else cvpf_value = cvpf_mu[col] + cvpf_sum_cov_xl / cvpf_sum_xl_square;
     return cvpf_value;
@@ -462,23 +482,23 @@
     knnf_neighbors.resize(100);
     knnf_mean_err.resize(100);
     knnf_mean_err.clear();
-    pb = new ProgressBar( "computing the neighborhood imputation errors for " + to_deal_with_name, test_length);
-    for (test_row = 0; test_row < test_length; test_row++)
+    ProgressBar* pb = new ProgressBar( "computing the neighborhood imputation errors for " + to_deal_with_name, test_length);
+    for (int test_row = 0; test_row < test_length; test_row++)
     {
         test_samples_set->getRow(test_row, train_input);
-        for (test_col = 0; test_col < train_width; test_col++)
+        for (int test_col = 0; test_col < train_width; test_col++)
         {
             if (test_col == to_deal_with_next) knnf_input[test_col] = covariancePreservationValue(test_col);
             else if (is_missing(train_input[test_col])) knnf_input[test_col] = covariancePreservationValue(test_col);
             else knnf_input[test_col] = train_input[test_col];
         }
         ball_tree->computeOutput(knnf_input, knnf_neighbors);
-        knnf_sum_value = 0.0;
-        knnf_sum_cov = 0.0;
-        knnv_value_count = 0.0;
-        for (knnf_row = 0; knnf_row < knnf_neighbors.size(); knnf_row++)
+        real knnf_sum_value = 0.0;
+        real knnf_sum_cov = 0.0;
+        real knnv_value_count = 0.0;
+        for (int knnf_row = 0; knnf_row < knnf_neighbors.size(); knnf_row++)
         {
-            knnf_value = ref_mis((int) knnf_neighbors[knnf_row], to_deal_with_next);
+            real knnf_value = ref_mis((int) knnf_neighbors[knnf_row], to_deal_with_next);
             if (!is_missing(knnf_value))
             {
                 knnf_sum_value += knnf_value;
@@ -499,20 +519,24 @@
         pb->update( test_row );
     }
     delete pb;
-    for (knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++) knnf_mean_err[knnf_row] = knnf_mean_err[knnf_row] /  (real) test_length;
+    for (int knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++) knnf_mean_err[knnf_row] = knnf_mean_err[knnf_row] /  (real) test_length;
 }
 
 void TestImputations::createHeaderFile()
 { 
     cout << "in createHeaderFile()" << endl;
-    for (train_col = 0; train_col < train_width; train_col++)
+    for (int train_col = 0; train_col < train_width; train_col++)
     {
         train_stats = train_set->getStats(train_col);
         train_total = train_stats.n();
         train_missing = train_stats.nmissing();
         train_present = train_total - train_missing;
         if (train_missing <= 0.0) header_record[train_col] = 0.0;                       // no missing, noting to do.
-        else if (train_present < min_number_of_samples) header_record[train_col] = 0.0; // should not happen
+        else if (train_present < min_number_of_samples){
+            header_record[train_col] = -1.0; // should not happen
+            PLERROR("In TestImputations::createHeaderFile: train_present(%d) < min_number_of_samples (%d)",
+                    train_present,min_number_of_samples);
+        }
         else header_record[train_col] = 1.0;                                            // test imputations
     }
     header_file = new FileVMatrix(header_file_name, 1, train_names);
@@ -534,7 +558,6 @@
 void TestImputations::train()
 {
     // initialize the output file
-    output_file_name = train_metadata + "/TestImputation/output.pmat";
     cout << "initialize the output file: " << output_file_name << endl;
     train_set->lockMetaDataDir();
     output_record.resize(knnf_mean_err.size()+5);
@@ -545,7 +568,7 @@
     output_record[2] = mmmf_mode_err;
     output_record[3] = tcmf_mean_err;
     output_record[4] = cvpf_mean_err;
-    for (knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++)
+    for (int knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++)
     {
        output_record[knnf_row + 5] = knnf_mean_err[knnf_row];
     }
@@ -561,19 +584,20 @@
     output_names[2] = "mode";
     output_names[3] = "tree_cond";
     output_names[4] = "cov_pres";
-    for (knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++)
+    for (int knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++)
     {
        output_names[knnf_row + 5] = "KNN_" + tostring(knnf_row);
     }
     output_record.clear();
     output_file = new FileVMatrix(output_file_name, train_width, output_names);
-    for (train_col = 0; train_col < train_width; train_col++)
+    for (int train_col = 0; train_col < train_width; train_col++)
         output_file->putRow(train_col, output_record);
 }
 
 void TestImputations::getOutputRecord(int var_col)
 { 
     output_file = new FileVMatrix(output_file_name, true);
+    output_record.resize(output_file->width());
     output_file->getRow(var_col, output_record);
 }
 

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-06-21 15:19:54 UTC (rev 7626)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-06-21 15:24:11 UTC (rev 7627)
@@ -40,6 +40,8 @@
 #ifndef TestImputations_INC
 #define TestImputations_INC
 
+#include <cstdarg>
+
 #include <plearn_learners/generic/PLearner.h>
 #include <plearn_learners/testers/PTester.h>
 #include <plearn/vmat/FileVMatrix.h>
@@ -142,12 +144,11 @@
     void createOutputFile();
     void getOutputRecord(int var_col);
     void updateOutputRecord(int var_col);
-
+    void endtestimputation(const char* msg, ...);
 private:
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
-    ProgressBar* pb;
     ExhaustiveNearestNeighbors* ball_tree;
     Mat ref_cov;
     Mat ref_mis;
@@ -160,8 +161,6 @@
     real train_total;
     real train_missing;
     real train_present;
-    int train_row;
-    int train_col;
     PPath header_file_name;
     VMat header_file;
     Vec header_record;
@@ -171,46 +170,21 @@
     real to_deal_with_value;
     VMat test_samples_set;
     TVec<int> indices;
-    int ind_next;
     int test_length;
     int test_width;
-    int test_row;
-    int test_col;
-    VMat mmmf_file;
-    int mmmf_length;
-    int mmmf_width;
-    real mmmf_mean;
-    real mmmf_median;
-    real mmmf_mode;
     real mmmf_mean_err;
     real mmmf_median_err;
     real mmmf_mode_err;
     PPath tcmf_file_name;
     VMat tcmf_file;
-    int tcmf_length;
-    int tcmf_width;
     real tcmf_mean_err;
-    VMat cvpf_file;
-    int cvpf_length;
-    int cvpf_width;
-    int cvpf_row;
-    int cvpf_col;
     Mat cvpf_cov;
     Vec cvpf_mu;
-    real cvpf_sum_cov_xl;
-    real cvpf_sum_xl_square;
-    real cvpf_value;
     real cvpf_mean_err;
     Vec knnf_input;
     Vec knnf_neighbors;
     Vec knnf_mean_err;
-    real knnf_value;
-    real knnf_sum_value;
-    real knnf_sum_cov;
-    real knnv_value_count;
-    int knnf_row;
     Vec weights;
-    int mi_col;
     WeightedDistance* weighted_distance_kernel;
     PPath output_file_name;
     VMat output_file;



From nouiz at mail.berlios.de  Thu Jun 21 17:43:30 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Jun 2007 17:43:30 +0200
Subject: [Plearn-commits] r7628 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706211543.l5LFhUM5017757@sheep.berlios.de>

Author: nouiz
Date: 2007-06-21 17:43:30 +0200 (Thu, 21 Jun 2007)
New Revision: 7628

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc
Log:
-Do all iteration


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc	2007-06-21 15:24:11 UTC (rev 7627)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeFieldStats.cc	2007-06-21 15:43:30 UTC (rev 7628)
@@ -120,7 +120,7 @@
     MODULE_LOG << "build_() called" << endl;
     if (train_set)
     {
-        for (int iteration = 1; iteration <= 50; iteration++)
+        for (int iteration = 1; iteration <= train_set->width(); iteration++)
         {
             cout << "In AnalyzeFieldStats, Iteration # " << iteration << endl;
             analyzeVariableStats();



From nouiz at mail.berlios.de  Thu Jun 21 18:15:44 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Jun 2007 18:15:44 +0200
Subject: [Plearn-commits] r7629 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706211615.l5LGFiRe020271@sheep.berlios.de>

Author: nouiz
Date: 2007-06-21 18:15:44 +0200 (Thu, 21 Jun 2007)
New Revision: 7629

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
Log:
-BUGFIX: net get is working properly


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-21 15:43:30 UTC (rev 7628)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-21 16:15:44 UTC (rev 7629)
@@ -104,7 +104,7 @@
 real ConditionalMeanImputationVMatrix::get(int i, int j) const
 { 
   real variable_value = source->get(i, j);
-  if (!is_missing(variable_value) && condmean_col_ref[j] >= 0) return condmean(i, condmean_col_ref[j]);
+  if (is_missing(variable_value) && condmean_col_ref[j] >= 0) return condmean(i, condmean_col_ref[j]);
   else if (is_missing(variable_value)) PLERROR("In ConditionalMeanImputationVMatrix::getExample(%d,%d) we have a missing value that haven't been assigned a value",i,j);
   return variable_value;
 }



From nouiz at mail.berlios.de  Thu Jun 21 18:22:07 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Jun 2007 18:22:07 +0200
Subject: [Plearn-commits] r7630 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706211622.l5LGM7He020568@sheep.berlios.de>

Author: nouiz
Date: 2007-06-21 18:22:06 +0200 (Thu, 21 Jun 2007)
New Revision: 7630

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
Log:
-More PLERROR


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-21 16:15:44 UTC (rev 7629)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-21 16:22:06 UTC (rev 7630)
@@ -67,7 +67,7 @@
 
 void ConditionalMeanImputationVMatrix::declareOptions(OptionList &ol)
 {
-  declareOption(ol, "source", &ConditionalMeanImputationVMatrix::source, OptionBase::buildoption, 
+  declareOption(ol, "source", &ConditionalMeanImputationVMatrix::source, OptionBase::buildoption,
                 "The source VMatrix with missing values.\n");
   declareOption(ol, "condmean_dir", &ConditionalMeanImputationVMatrix::condmean_dir, OptionBase::buildoption, 
                 "The directory in the source metadatadir housing the variable conditional mean files.\n");
@@ -98,7 +98,7 @@
   source->getExample(i, input, target, weight);
   for (int source_col = 0; source_col < input->length(); source_col++)
     if (is_missing(input[source_col]) && condmean_col_ref[source_col] >= 0) input[source_col] = condmean(i, condmean_col_ref[source_col]);
-    else if (is_missing(input[source_col])) PLERROR("In ConditionalMeanImputationVMatrix::getExample() we have a missing value in column %d that haven't been assigned a value",source_col);
+    else if (is_missing(input[source_col])) PLERROR("In ConditionalMeanImputationVMatrix::getExample(%d,vec,vec,vec) we have a missing value in column %d that haven't been assigned a value",i,source_col);
 }
 
 real ConditionalMeanImputationVMatrix::get(int i, int j) const
@@ -119,7 +119,7 @@
   source->getSubRow(i, j, v);
   for (int source_col = 0; source_col < v->length(); source_col++) 
     if (is_missing(v[source_col])) v[source_col] = condmean(i, condmean_col_ref[source_col + j]);
-    else if (is_missing(v[source_col])) cout << "getSubRow : " << i << " " << source_col + j << endl;
+    else if (is_missing(v[source_col])) PLERROR("In ConditionalMeanImputationVMatrix::getSubRow(%d,%d,vec) we have a missing value in colomn %d that haven't been assigned a value",i,j,source_col);
 }
 
 void ConditionalMeanImputationVMatrix::putSubRow(int i, int j, Vec v)
@@ -142,7 +142,7 @@
   source-> getRow(i, v);
   for (int source_col = 0; source_col < v->length(); source_col++)
     if (is_missing(v[source_col]) && condmean_col_ref[source_col] >= 0) v[source_col] = condmean(i, condmean_col_ref[source_col]);
-    else if (is_missing(v[source_col])) cout << "getRow : " << i << " " << source_col << endl;
+    else if (is_missing(v[source_col])) PLERROR("In ConditionalMeanImputationVMatrix::getRow(%d,vec) we have a missing value in column %d that haven't been assigned a value",i,source_col);
 }
 
 void ConditionalMeanImputationVMatrix::putRow(int i, Vec v)
@@ -155,7 +155,7 @@
   source-> getColumn(i, v);
   for (int source_row = 0; source_row < v->length(); source_row++)
     if (is_missing(v[source_row]) && condmean_col_ref[i] >= 0) v[source_row] = condmean(source_row, condmean_col_ref[i]);
-    else if (is_missing(v[source_row])) cout << "getColumn : " << source_row << " " << i << endl;
+    else if (is_missing(v[source_row])) PLERROR("In ConditionalMeanImputationVMatrix::getColumn(%d,vec) we have a missing value in row %d that haven't been assigned a value",i,source_row);
 }
 
 



From lamblin at mail.berlios.de  Fri Jun 22 02:48:59 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 22 Jun 2007 02:48:59 +0200
Subject: [Plearn-commits] r7631 - trunk/plearn_learners/online
Message-ID: <200706220048.l5M0mx6W023298@sheep.berlios.de>

Author: lamblin
Date: 2007-06-22 02:48:58 +0200 (Fri, 22 Jun 2007)
New Revision: 7631

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
Log:
Implemented batch bpropNLL


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-21 16:22:06 UTC (rev 7630)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-22 00:48:58 UTC (rev 7631)
@@ -552,7 +552,7 @@
 
 void RBMGaussianLayer::update( const Mat& pos_values, const Mat& neg_values )
 {
-    
+
     PLASSERT( pos_values.width() == size );
     PLASSERT( neg_values.width() == size );
 
@@ -572,16 +572,16 @@
             {
                 real *pv_k = pos_values[k];
                 real *nv_k = neg_values[k];
-		real update=0;
+                real update=0;
                 for( int i=0; i<size; i++ )
                 {
                     update += two_lr * a[0] * (nv_k[i]*nv_k[i] - pv_k[i]*pv_k[i]);
                 }
-		a[0] += update/(real)size;
+                a[0] += update/(real)size;
                 if( a[0] < min_quad_coeff )
                     a[0] = min_quad_coeff;
             }
-	else
+        else
             for( int k=0; k<batch_size; k++ )
             {
                 real *pv_k = pos_values[k];
@@ -616,13 +616,13 @@
         real* v = unit_values.data();
         real* a = quad_coeff.data();
         real* b = bias.data();
-	if(share_quad_coeff)
+        if(share_quad_coeff)
             for(register int i=0; i<size; i++)
             {
                 tmp = a[0]*v[i];
                 en += tmp*tmp + b[i]*v[i];
             }
-	else
+        else
             for(register int i=0; i<size; i++)
             {
                 tmp = a[i]*v[i];

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-06-21 16:22:06 UTC (rev 7630)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-06-22 00:48:58 UTC (rev 7631)
@@ -395,6 +395,30 @@
     }
 }
 
+void RBMMixedLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
+                             Mat& bias_gradients)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+
+    for( int i=0 ; i<n_layers ; i++ )
+    {
+        int begin = init_positions[i];
+        int size_i = sub_layers[i]->size;
+
+        Mat sub_targets = targets.subMatColumns(begin, size_i);
+        Mat sub_bias_gradients = bias_gradients.subMatColumns(begin, size_i);
+        // TODO: something else than store mat_nlls...
+        sub_layers[i]->bpropNLL( sub_targets, mat_nlls.column(i),
+                                 sub_bias_gradients );
+    }
+}
+
 void RBMMixedLayer::declareOptions(OptionList& ol)
 {
     declareOption(ol, "sub_layers", &RBMMixedLayer::sub_layers,

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-06-21 16:22:06 UTC (rev 7630)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-06-22 00:48:58 UTC (rev 7631)
@@ -139,6 +139,8 @@
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
+    virtual void bpropNLL(const Mat& targets, const Mat& costs_column,
+                          Mat& bias_gradients);
 
     //! Accumulates positive phase statistics
     virtual void accumulatePosStats( const Vec& pos_values );



From ducharme at mail.berlios.de  Fri Jun 22 14:09:19 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 22 Jun 2007 14:09:19 +0200
Subject: [Plearn-commits] r7632 - trunk/scripts
Message-ID: <200706221209.l5MC9JJH029313@sheep.berlios.de>

Author: ducharme
Date: 2007-06-22 14:09:19 +0200 (Fri, 22 Jun 2007)
New Revision: 7632

Modified:
   trunk/scripts/perlgrep
Log:
Ajout du fichier pytest.config.  Plus facile de trouver des pytest qui plantent...


Modified: trunk/scripts/perlgrep
===================================================================
--- trunk/scripts/perlgrep	2007-06-22 00:48:58 UTC (rev 7631)
+++ trunk/scripts/perlgrep	2007-06-22 12:09:19 UTC (rev 7632)
@@ -102,7 +102,7 @@
             { 
                 my @flist = lsdir($fname);
                 
-                @flist = grep { -d $_ or /Makefile|makefile|
+                @flist = grep { -d $_ or /Makefile|makefile|pytest\.config|
                                     \.c$|\.cc$|\.cpp$|\.CC$|\.h$|\.hpp$
                                    |\.plearn$|\.pyplearn$|\.vmat$|\.py$|\.pymat$
                                    |\.txt$|^readme|^Readme|^README/x } @flist;



From nouiz at mail.berlios.de  Fri Jun 22 21:24:01 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 22 Jun 2007 21:24:01 +0200
Subject: [Plearn-commits] r7633 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706221924.l5MJO1KD008833@sheep.berlios.de>

Author: nouiz
Date: 2007-06-22 21:24:00 +0200 (Fri, 22 Jun 2007)
New Revision: 7633

Added:
   branches/cgi-desjardin/plearn_learners/second_iteration/ImputationVMatrix.cc
Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
Log:
-Made a super VMatrix ImputationVMatrxi and all VMatrix that replace missing value inherit from it
-NeighborhoodImputationVMatrix take an optional imputation_spec that take a specific number of neighbor by column
-MeanMedianModeImputationVMatrix, made 1 global variable local


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.cc	2007-06-22 19:24:00 UTC (rev 7633)
@@ -67,8 +67,6 @@
 
 void ConditionalMeanImputationVMatrix::declareOptions(OptionList &ol)
 {
-  declareOption(ol, "source", &ConditionalMeanImputationVMatrix::source, OptionBase::buildoption,
-                "The source VMatrix with missing values.\n");
   declareOption(ol, "condmean_dir", &ConditionalMeanImputationVMatrix::condmean_dir, OptionBase::buildoption, 
                 "The directory in the source metadatadir housing the variable conditional mean files.\n");
   declareOption(ol, "condmean", &ConditionalMeanImputationVMatrix::condmean, OptionBase::learntoption, 
@@ -86,7 +84,6 @@
 
 void ConditionalMeanImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-  deepCopyField(source, copies);
   deepCopyField(condmean_dir, copies);
   deepCopyField(condmean, copies);
   deepCopyField(condmean_col_ref, copies);
@@ -164,6 +161,7 @@
 {
     if (!source) PLERROR("In ConditionalMeanImputationVMatrix::source vmat must be supplied");
     loadCondMeanMatrix(); 
+    testResultantVMatrix();
 }
 
 void ConditionalMeanImputationVMatrix::loadCondMeanMatrix()
@@ -215,10 +213,11 @@
     {
         source_stats = source->getStats(source_col);
         if (source_stats.nmissing() <= 0) continue;
-        condmean_col = condmean_col_ref[source_col];
-        condmean_variable_file_name = source_metadata + "/" + condmean_dir + "/dir/" + source_names[source_col] + "/Split0/test1_outputs.pmat";
-        if (!isfile(condmean_variable_file_name)) PLERROR("In ConditionalMeanImputationVMatrix::A conditional mean file was not found for variable %s", source_names[source_col].c_str());
-        condmean_variable_file = new FileVMatrix(condmean_variable_file_name, false);
+        int condmean_col = condmean_col_ref[source_col];
+        PPath condmean_variable_file_name = source_metadata + "/" + condmean_dir + "/dir/" + source_names[source_col] + "/Split0/test1_outputs.pmat";
+        if (!isfile(condmean_variable_file_name)) PLERROR("In ConditionalMeanImputationVMatrix::A conditional mean file(%s) was not found for variable %s",
+                                                          condmean_variable_file_name.c_str(),source_names[source_col].c_str());
+        VMat condmean_variable_file = new FileVMatrix(condmean_variable_file_name, false);
         if (condmean_variable_file->length() != source_length)
             PLERROR("In ConditionalMeanImputationVMatrix::Source and conditional mean file length are not equal for variable %s", source_names[source_col].c_str());
         for (source_row = 0; source_row < source_length; source_row++)

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h	2007-06-22 19:24:00 UTC (rev 7633)
@@ -44,21 +44,18 @@
 #ifndef ConditionalMeanImputationVMatrix_INC
 #define ConditionalMeanImputationVMatrix_INC
 
-#include <plearn/vmat/SourceVMatrix.h>
+#include "ImputationVMatrix.h"
 #include <plearn/vmat/FileVMatrix.h>
 
 namespace PLearn {
 using namespace std;
 
-class ConditionalMeanImputationVMatrix: public VMatrix
+class ConditionalMeanImputationVMatrix: public ImputationVMatrix
 {
-  typedef VMatrix inherited;
+  typedef ImputationVMatrix inherited;
   
 public:
 
-  //! The source VMatrix with missing values.
-  VMat                 source;
-
   //! The directory in the source metadatadir housing the variable conditional mean files.
   string               condmean_dir;
 
@@ -100,9 +97,6 @@
   PPath                source_metadata;
   TVec<string>         source_names;
   StatsCollector       source_stats;
-  PPath                condmean_variable_file_name;
-  VMat                 condmean_variable_file;
-  int                  condmean_col;
         
 
           void         build_();

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.cc	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.cc	2007-06-22 19:24:00 UTC (rev 7633)
@@ -64,8 +64,6 @@
 
 void CovariancePreservationImputationVMatrix::declareOptions(OptionList &ol)
 {
-  declareOption(ol, "source", &CovariancePreservationImputationVMatrix::source, OptionBase::buildoption, 
-                "The source VMatrix with missing values.\n");
 
   declareOption(ol, "train_set", &CovariancePreservationImputationVMatrix::train_set, OptionBase::buildoption, 
                 "A referenced train set.\n"
@@ -82,7 +80,6 @@
 
 void CovariancePreservationImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-  deepCopyField(source, copies);
   deepCopyField(train_set, copies);
   inherited::makeDeepCopyFromShallowCopy(copies);
 }

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h	2007-06-22 19:24:00 UTC (rev 7633)
@@ -44,6 +44,7 @@
 #ifndef CovariancePreservationImputationVMatrix_INC
 #define CovariancePreservationImputationVMatrix_INC
 
+#include "ImputationVMatrix.h"
 #include <plearn/vmat/SourceVMatrix.h>
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/io/fileutils.h>                     //!<  For isfile()
@@ -52,14 +53,11 @@
 namespace PLearn {
 using namespace std;
 
-class CovariancePreservationImputationVMatrix: public VMatrix
+class CovariancePreservationImputationVMatrix: public ImputationVMatrix
 {
-  typedef VMatrix inherited;
+  typedef ImputationVMatrix inherited;
   
 public:
-
-  //! The source VMatrix with missing values.
-  VMat                  source;
   
   //! A referenced train set.
   //! The covariance imputation is computed with the observed values in this data set.

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h	2007-06-22 19:24:00 UTC (rev 7633)
@@ -40,7 +40,7 @@
 #ifndef GaussianizeVMatrix_INC
 #define GaussianizeVMatrix_INC
 
-#include <plearn/vmat/SourceVMatrix.h>
+#include "ImputationVMatrix.h"
 
 namespace PLearn {
 

Added: branches/cgi-desjardin/plearn_learners/second_iteration/ImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ImputationVMatrix.cc	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ImputationVMatrix.cc	2007-06-22 19:24:00 UTC (rev 7633)
@@ -0,0 +1,161 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+// Copyright (C) 2007 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************************    
+   * $Id: NeighborhoodImputationVMatrix.cc 3658 2005-07-06 20:30:15  Godbout $
+   ******************************************************************* */
+
+
+#include "ImputationVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+/** ImputationVMatrix **/
+
+PLEARN_IMPLEMENT_ABSTRACT_OBJECT(
+  ImputationVMatrix,
+  "Super-class for VMatrices that replace missing value in another one",
+  ""
+  );
+
+  ImputationVMatrix::ImputationVMatrix():
+    test_level(0)
+{
+}
+
+ImputationVMatrix::~ImputationVMatrix()
+{
+}
+
+void ImputationVMatrix::declareOptions(OptionList &ol)
+{
+  declareOption(ol, "source", &ImputationVMatrix::source, OptionBase::buildoption, 
+                "The source VMatrix with missing values that will be filled.\n");
+  declareOption(ol, "test_level", &ImputationVMatrix::test_level, OptionBase::buildoption, 
+                "The level of test of final matrix. 0 : no test, 1: linear in column or row test, 2: linear in cell\n");
+
+  inherited::declareOptions(ol);
+}
+
+void ImputationVMatrix::build()
+{
+  inherited::build();
+  build_();
+}
+
+void ImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+  deepCopyField(source, copies);
+  inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void ImputationVMatrix::build_()
+{
+}
+
+void ImputationVMatrix::testResultantVMatrix()
+{
+  TVec<string> source_names(source->width());
+  source_names = source->fieldNames();
+  
+  if(test_level>=1){
+    for(int row=0;row<length();row++)
+      for(int col=0;col<width();col++){
+        real data=get(row,col);
+        real sourcedata=source->get(row,col);
+
+        //test if variable not missing are not changed.
+        if(!is_missing(sourcedata))
+          if(data!=sourcedata){
+            PLERROR("ImputationImputations::testResultantVMatrix() data at [%d,%d] are different but the data is not missing",row,col);
+          }
+
+        //test if missing variable are replaced by a value not missing.
+        if(is_missing(data))
+          PLERROR("ImputationImputations::testResultantVMatrix() data at [%d,%d] in the final matrix is missing",row,col);
+      }
+  }
+  if(test_level>=2){ //Must verify if source->getStats is linear
+    getStats();
+    source->getStats();
+    //print the variable that the replacement of missing value change the mean by more then 3 times the stderr
+    int nberr = 0;
+    for(int col=0;col<width();col++)
+      {
+        real mean = field_stats[col].mean();
+        real smean = source->getStats(col).mean();
+        real sstderr = source->getStats(col).stddev()/sqrt(source->getStats(col).nnonmissing());
+        real val=(mean-smean)/sstderr;
+        if(fabs(val)>3){
+          PLWARNING("ImputationImputations::testResultantVMatrix() the variable %d(%s) have a value of %f for abs((mean-sourcemean)/source_stderr)",col,source_names[col].c_str(),val);
+          nberr++;
+        }
+      }
+    if(nberr>0)
+      PLWARNING("ImputationImputations::testResultantVMatrix() There have been %d variables with the mean after imputation outside of sourcemean +- 3*source_stderr",nberr);
+  }
+  //    for(int row=0;row<length();row++)
+  /*    for(int col=0;col<width();col++)
+        {
+        StatsCollector sstats=source->getStats(col);
+        StatsCollector stats=getStats(col);
+        if(sstats.nmissing()!=0){
+        real sum=0;
+        condmean_variable_file_name = source_metadata + "/" + condmean_dir + "/dir/" + source_names[col] + "/Split0/test1_outputs.pmat";
+        
+        if (!isfile(condmean_variable_file_name))
+        PLERROR("In ImputationVMatrix::A conditional mean file was not found for variable %s", source_names[col].c_str());
+        condmean_variable_file = new FileVMatrix(condmean_variable_file_name, false);
+        if (condmean_variable_file->length() != source_length)
+        PLERROR("In ImputationVMatrix::Source and conditional mean file length are not equal for variable %s", source_names[col].c_str());
+        for (int source_row = 0; source_row < source_length; source_row++){
+        real rdata = condmean_variable_file->get(source_row, 0);
+        real data = get(source_row, col);
+        if(!is_missing(rdata))
+        sum+=pow(data - rdata, 2.0);
+        }
+        real mse=sum/stats.nnonmissing();
+        real diff=mse/sstats.variance();
+        if(diff >0.9){
+        perr <<col<<" "<<diff<<endl;
+        PLWARNING("ImputationImputations::testresultantVMatrix() the variable %d(%s) have a MSEtreecondmean(%f)/MSEmean(%f) of %f",col,source_names[col].c_str(),mse,sstats.variance(),diff);
+        }
+        }*/
+}
+} // end of namespcae PLearn

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.cc	2007-06-22 19:24:00 UTC (rev 7633)
@@ -65,9 +65,6 @@
 
 void MeanMedianModeImputationVMatrix::declareOptions(OptionList &ol)
 {
-  declareOption(ol, "source", &MeanMedianModeImputationVMatrix::source, OptionBase::buildoption, 
-                "The source VMatrix with missing values.\n");
-
   declareOption(ol, "train_set", &MeanMedianModeImputationVMatrix::train_set, OptionBase::buildoption, 
                 "A referenced train set.\n"
                 "The mean, median or mode is computed with the observed values in this data set.\n"
@@ -114,7 +111,6 @@
 
 void MeanMedianModeImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-  deepCopyField(source, copies);
   deepCopyField(train_set, copies);
   deepCopyField(number_of_train_samples_to_use, copies);
   deepCopyField(imputation_spec, copies);
@@ -242,7 +238,7 @@
     variable_mode.resize(train_width);
     variable_imputation_instruction.resize(train_width);
     variable_imputation_instruction.clear();
-    for (spec_col = 0; spec_col < imputation_spec.size(); spec_col++)
+    for (int spec_col = 0; spec_col < imputation_spec.size(); spec_col++)
     {
         for (train_col = 0; train_col < train_width; train_col++)
         {

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MeanMedianModeImputationVMatrix.h	2007-06-22 19:24:00 UTC (rev 7633)
@@ -44,7 +44,7 @@
 #ifndef MeanMedianModeImputationVMatrix_INC
 #define MeanMedianModeImputationVMatrix_INC
 
-#include <plearn/vmat/SourceVMatrix.h>
+#include "ImputationVMatrix.h"
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/io/fileutils.h>                     //!<  For isfile()
 #include <plearn/math/BottomNI.h>
@@ -52,14 +52,12 @@
 namespace PLearn {
 using namespace std;
 
-class MeanMedianModeImputationVMatrix: public VMatrix
+class MeanMedianModeImputationVMatrix: public ImputationVMatrix
 {
-  typedef VMatrix inherited;
+  typedef ImputationVMatrix inherited;
   
 public:
 
-  //! The source VMatrix with missing values.
-  VMat                          source;
   
   //! A referenced train set.
   //! The mean, median or mode is computed with the observed values in this data set.
@@ -136,7 +134,6 @@
   int                  source_targetsize;
   int                  source_weightsize;
   Vec                  variable_vec;
-  int                  spec_col;
   int                  current_value_count;
   real                 current_value;
   PPath                mean_median_mode_file_name;

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.cc	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.cc	2007-06-22 19:24:00 UTC (rev 7633)
@@ -55,6 +55,7 @@
   );
 
 NeighborhoodImputationVMatrix::NeighborhoodImputationVMatrix()
+  : count_missing_neighbors(0)
 {
 }
 
@@ -64,12 +65,9 @@
 
 void NeighborhoodImputationVMatrix::declareOptions(OptionList &ol)
 {
-  declareOption(ol, "source_with_missing", &NeighborhoodImputationVMatrix::source_with_missing, OptionBase::buildoption, 
-                "The source VMatrix with missing values.\n");
-
   declareOption(ol, "reference_index", &NeighborhoodImputationVMatrix::reference_index, OptionBase::buildoption, 
                 "The set of pre-computed neighbors index.\n"
-                "his can be done with BallTreeNearestNeighbors.\n");
+                "This can be done with BallTreeNearestNeighbors.\n");
 
   declareOption(ol, "reference_with_missing", &NeighborhoodImputationVMatrix::reference_with_missing, OptionBase::buildoption, 
                 "The reference set corresponding to the pre-computed index with missing values.");
@@ -77,10 +75,18 @@
   declareOption(ol, "reference_with_covariance_preserved", &NeighborhoodImputationVMatrix::reference_with_covariance_preserved, OptionBase::buildoption, 
                 "The reference set corresponding to the pre-computed index with the initial imputations.");
 
-  declareOption(ol, "number_of_neighbors", &NeighborhoodImputationVMatrix::number_of_neighbors, OptionBase::learntoption,
+  declareOption(ol, "number_of_neighbors", &NeighborhoodImputationVMatrix::number_of_neighbors, OptionBase::buildoption,
                 "This is usually called K, the number of neighbors to consider.\n"   
                 "It must be less or equal than the with of the reference index.");
 
+  declareOption(ol, "count_missing_neighbors", &NeighborhoodImputationVMatrix::count_missing_neighbors, OptionBase::buildoption,
+                "0: (default) We do not count a neighbour with a missing value in the number of neighbors.\n"   
+                "1: We count a neighbour with a missing value in the number of neighbors.\n");
+
+  declareOption(ol, "imputation_spec", &NeighborhoodImputationVMatrix::imputation_spec, OptionBase::buildoption,
+                "A vector that give for each variable the number of neighbors to use.\n"
+                "If a variable don't have a value, we use the value of the varialbe: number_of_neighbors.\n"
+                " Ex: [ varname1 : nbneighbors1, varname2 : nbneighbors2 ]\n");
   inherited::declareOptions(ol);
 }
 
@@ -88,21 +94,23 @@
 {
   inherited::build();
   build_();
+  testResultantVMatrix();
 }
 
 void NeighborhoodImputationVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-  deepCopyField(source_with_missing, copies);
   deepCopyField(reference_index, copies);
   deepCopyField(reference_with_missing, copies);
   deepCopyField(reference_with_covariance_preserved, copies);
   deepCopyField(number_of_neighbors, copies);
+  deepCopyField(count_missing_neighbors, copies);
+  deepCopyField(imputation_spec, copies);
   inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
 void NeighborhoodImputationVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
 {
-  source_with_missing->getExample(i, input, target, weight);
+  source->getExample(i, input, target, weight);
   for (int source_col = 0; source_col < input->length(); source_col++)
   {
     if (is_missing(input[source_col])) input[source_col] = impute(i, source_col);
@@ -111,7 +119,7 @@
 
 real NeighborhoodImputationVMatrix::get(int i, int j) const
 { 
-  real variable_value = source_with_missing->get(i, j);
+  real variable_value = source->get(i, j);
   if (!is_missing(variable_value)) return variable_value;
   return impute(i, j);
 }
@@ -123,7 +131,7 @@
 
 void NeighborhoodImputationVMatrix::getSubRow(int i, int j, Vec v) const
 {  
-  source_with_missing->getSubRow(i, j, v);
+  source->getSubRow(i, j, v);
   for (int source_col = 0; source_col < v->length(); source_col++) 
     if (is_missing(v[source_col])) v[source_col] = impute(i, source_col + j);
 }
@@ -145,7 +153,7 @@
 
 void NeighborhoodImputationVMatrix::getRow(int i, Vec v) const
 {  
-  source_with_missing-> getRow(i, v);
+  source-> getRow(i, v);
   for (int source_col = 0; source_col < v->length(); source_col++)
     if (is_missing(v[source_col])) v[source_col] = impute(i, source_col); 
 }
@@ -157,34 +165,33 @@
 
 void NeighborhoodImputationVMatrix::getColumn(int i, Vec v) const
 {  
-  source_with_missing-> getColumn(i, v);
+  source-> getColumn(i, v);
   for (int source_row = 0; source_row < v->length(); source_row++)
     if (is_missing(v[source_row])) v[source_row] = impute(source_row, i);
 }
 
 void NeighborhoodImputationVMatrix::build_()
 {
-    if (!source_with_missing)                 PLERROR("In NeighborhoodImputationVMatrix::source with missing set must be supplied");
+    if (!source)                 PLERROR("In NeighborhoodImputationVMatrix::source with missing set must be supplied");
     if (!reference_index)                     PLERROR("In NeighborhoodImputationVMatrix::reference index set must be supplied");
     if (!reference_with_missing)              PLERROR("In NeighborhoodImputationVMatrix::reference with missing set must be supplied");
     if (!reference_with_covariance_preserved) PLERROR("In NeighborhoodImputationVMatrix::reference with covariance preserved must be supplied");
-    src_length = source_with_missing->length();
+    src_length = source->length();
     if (src_length != reference_index->length())
         PLERROR("In NeighborhoodImputationVMatrix::length of the source and its index must agree, got: %i - %i", src_length, reference_index->length());
     ref_length = reference_with_missing->length();
     if (ref_length != reference_with_covariance_preserved->length())
         PLERROR("In NeighborhoodImputationVMatrix::length of the reference set with missing and with covariance preserved must agree, got: %i - %i",
                 ref_length, reference_with_covariance_preserved->length());
-    src_width = source_with_missing->width();
+    src_width = source->width();
     if (src_width != reference_with_missing->width())
         PLERROR("In NeighborhoodImputationVMatrix::width of the source and the reference with missing must agree, got: %i - %i",
                 src_width, reference_with_missing->width());
     if (src_width != reference_with_covariance_preserved->width())
-        PLERROR("In NeighborhoodImputationVMatrix::width of the source and the reference with missing must agree, got: %i - %i",
+        PLERROR("In NeighborhoodImputationVMatrix::width of the source and the reference with covariance preserved must agree, got: %i - %i",
                 src_width, reference_with_covariance_preserved->width());
     if (number_of_neighbors < 1)
-        PLERROR("In NeighborhoodImputationVMatrix::the index must contains at least as many reference as the specified number of neighbors, got: %i - %i",
-                number_of_neighbors, reference_index->width());
+      PLERROR("In NeighborhoodImputationVMatrix::there must be at least 1 neighbors, got: %d",number_of_neighbors);
     if (number_of_neighbors > reference_index->width())
         PLERROR("In NeighborhoodImputationVMatrix::the index must contains at least as many reference as the specified number of neighbors, got: %i - %i",
                 number_of_neighbors, reference_index->width());
@@ -194,38 +201,57 @@
     ref_mis = reference_with_missing->toMat();
     ref_cov.resize(reference_with_covariance_preserved->length(), reference_with_covariance_preserved->width());
     ref_cov = reference_with_covariance_preserved->toMat();
+
     length_ = src_length;
     width_ = src_width;
-    inputsize_ = source_with_missing->inputsize();
-    targetsize_ = source_with_missing->targetsize();
-    weightsize_ = source_with_missing->weightsize();
-    declareFieldNames(source_with_missing->fieldNames());
+    inputsize_ = source->inputsize();
+    targetsize_ = source->targetsize();
+    weightsize_ = source->weightsize();
+    declareFieldNames(source->fieldNames());
+
+    nb_neighbors.resize(source->inputsize());
+    nb_neighbors.fill(number_of_neighbors);
+    for (int spec_col = 0; spec_col < imputation_spec.size(); spec_col++)
+      {
+        int source_col = fieldIndex(imputation_spec[spec_col].first);
+        if (source_col < 0) 
+          PLERROR("In NeighborhoodImputationVMatrix::build_() no field with this name in source data set: %s", (imputation_spec[spec_col].first).c_str());
+        nb_neighbors[source_col] = imputation_spec[spec_col].second;
+      }
 }
 real NeighborhoodImputationVMatrix::impute(int i, int j) const
 {
-    int ref_idx_col;
     int ref_row;
     real return_value = 0.0;
-    real value_count = 0.0;
-    for (ref_idx_col = 0; ref_idx_col < number_of_neighbors; ref_idx_col++)
+    int value_count = 0;
+    int neighbors_count = 0;
+    for (int ref_idx_col = 0; ref_idx_col < ref_idx.width() &&
+           neighbors_count < nb_neighbors[j]; ref_idx_col++)
     {
         ref_row = (int) ref_idx(i, ref_idx_col);
         if (ref_row < 0 || ref_row >= ref_length)
             PLERROR("In NeighborhoodImputationVMatrix::invalid index reference, got: %i for sample %i", ref_row, i);
-        if (is_missing(ref_mis(ref_row, j))) continue;
+        if (is_missing(ref_mis(ref_row, j))){
+          if(count_missing_neighbors)
+            neighbors_count++;
+          continue;
+        }
         return_value += ref_mis(ref_row, j);
-        value_count += 1.0;
+        value_count++;
+        neighbors_count++;
     }
     if (value_count > 0) return return_value / value_count;
+    //if all neighbors have missing value we use the inputed version
+    //TODO put a warning
     return_value = 0.0;
-    value_count = 0.0;
-    for (ref_idx_col = 0; ref_idx_col < number_of_neighbors; ref_idx_col++)
+    value_count = 0;
+    for (int ref_idx_col = 0; ref_idx_col < number_of_neighbors; ref_idx_col++)
     {
         ref_row = (int) ref_idx(i, ref_idx_col);
         if (is_missing(ref_cov(ref_row, j)))
             PLERROR("In NeighborhoodImputationVMatrix::missing value found in the reference with covariance preserved at: %i , %i", ref_row, j);
         return_value += ref_cov(ref_row, j);
-        value_count += 1.0;
+        value_count += 1;
     }
     return return_value / value_count;
 }

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h	2007-06-22 19:24:00 UTC (rev 7633)
@@ -44,7 +44,7 @@
 #ifndef NeighborhoodImputationVMatrix_INC
 #define NeighborhoodImputationVMatrix_INC
 
-#include <plearn/vmat/SourceVMatrix.h>
+#include "ImputationVMatrix.h"
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/io/fileutils.h>                     //!<  For isfile()
 #include <plearn/math/BottomNI.h>
@@ -52,15 +52,12 @@
 namespace PLearn {
 using namespace std;
 
-class NeighborhoodImputationVMatrix: public VMatrix
+class NeighborhoodImputationVMatrix: public ImputationVMatrix
 {
-  typedef VMatrix inherited;
+  typedef ImputationVMatrix inherited;
   
 public:
 
-  //! The source VMatrix with missing values.
-  VMat                          source_with_missing;
-  
   //! The set of pre-computed neighbors index.
   //! This can be done with BallTreeNearestNeighbors.
   VMat                          reference_index;
@@ -74,8 +71,15 @@
   //! This is usually called K, the number of neighbors to consider.
   //! It must be less or equal than the with of the reference index.
   int                           number_of_neighbors;
-  
 
+  //! A vector that give for each variable the number of neighbors to use
+  //! If a variable is not in the spec, it will use number_of_neighbors
+  TVec< pair<string, int> >  imputation_spec;
+
+  //!0: (default) We do not count a neighbour with a missing value in the number of neighbors
+  //!1: We count a neighbour with a missing value in the number of neighbors
+  int                           count_missing_neighbors;
+
                         NeighborhoodImputationVMatrix();
   virtual               ~NeighborhoodImputationVMatrix();
 
@@ -103,6 +107,7 @@
           Mat          ref_idx;
           Mat          ref_mis;
           Mat          ref_cov;
+          TVec<int>    nb_neighbors;
 
           void         build_();
           real         impute(int i, int j) const;

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-06-22 12:09:19 UTC (rev 7632)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-06-22 19:24:00 UTC (rev 7633)
@@ -399,6 +399,11 @@
     mmmf_mean_err = mmmf_mean_err / (real) test_length;
     mmmf_median_err = mmmf_median_err / (real) test_length;
     mmmf_mode_err = mmmf_mode_err / (real) test_length;
+    //TODO check the formul
+    //mmmf_mean_stddev = sqrt(mmmf_mean_err);
+    //mmmf_median_stddev = sqrt(mmmf_median_err);
+    //mmmf_mode_stddev = sqrt(mmmf_mode_err);
+
 }
 
 void TestImputations::computeTreeCondMeanStats()
@@ -421,6 +426,8 @@
     }
     delete pb;
     tcmf_mean_err = tcmf_mean_err / (real) test_length;
+    //TODO check the formul
+    //tcmf_mean_stddev = sqrt(tcmf_mean_err);
 }
 
 void TestImputations::computeCovPresStats()
@@ -457,6 +464,9 @@
     }
     delete pb;
     cvpf_mean_err = cvpf_mean_err / (real) test_length;
+    //TODO check the formul
+    //cvpf_mean_stddev = sqrt(cvpf_mean_err);
+
 }
 
 real TestImputations::covariancePreservationValue(int col)



From louradou at mail.berlios.de  Sat Jun 23 00:20:57 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 23 Jun 2007 00:20:57 +0200
Subject: [Plearn-commits] r7634 - in
	trunk/python_modules/plearn/learners/modulelearners: . examples
Message-ID: <200706222220.l5MMKvsi019517@sheep.berlios.de>

Author: louradou
Date: 2007-06-23 00:20:56 +0200 (Sat, 23 Jun 2007)
New Revision: 7634

Added:
   trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py
   trunk/python_modules/plearn/learners/modulelearners/examples/sample.py
   trunk/python_modules/plearn/learners/modulelearners/examples/sample_from_hidden.py
Modified:
   trunk/python_modules/plearn/learners/modulelearners/__init__.py
Log:
Some examples to see images generated with Gibbs sampling



Modified: trunk/python_modules/plearn/learners/modulelearners/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-06-22 19:24:00 UTC (rev 7633)
+++ trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-06-22 22:20:56 UTC (rev 7634)
@@ -18,13 +18,14 @@
     learner  = serv.new(learner)
     trainset = serv.new(trainset)
     validset = serv.new(validset)
-    testset  = serv.new(testset)
+    testset  = serv.new(testset)    
 
-
 # examples:
 #   isModule(module,'RBM')
 #   isModule(module,'Split')
 #
+# return True OR False (whether the type of the Module match or not)
+#
 def isModule(module,name):
     return name+'Module' in str(type(module))
 
@@ -90,12 +91,17 @@
        return copy.copy( myObject.modules )
     # Module Learner
     elif 'ModuleLearner' in str(type(myObject)):
-       return copy.copy( myObject.module.modules )
+       return getModules( myObject.module )
+    # Hyper Learner
+    elif 'HyperLearner' in str(type(myObject)):
+       if 'module' not in myObject.learner._optionnames:
+          raise TypeError, "The Hyperlearner must have a learner with module"
+       return getModules( myObject.learner.module )
     # List of modules
     elif type(myObject) == list and 'Module' in str(type(myObject[0])):
          return myObject
     else:
-        raise TypeError, "Please give a ModuleLearner or NetworkModule"
+        raise TypeError, "Please give a ModuleLearner or NetworkModule (not a "+str(type(myObject))+")"
 
 def setModules( myObject , new_connections_list):
     if 'NetworkModule' in str(type(myObject)):
@@ -126,9 +132,9 @@
        return deepcopy( myObject.module.modules[index] )
     # List of modules
     elif type(myObject) == list and 'Module' in str(type(myObject[0])):
-       return myObject[index]
+       return deepcopy( myObject[index] )
     else:
-       raise TypeError, "Please give a ModuleLearner or NetworkModule"
+       raise TypeError, "Please give a ModuleLearner or NetworkModule or a List of modules"
     
 def getConnections( myObject ):
     if 'NetworkModule' in str(type(myObject)):
@@ -162,7 +168,8 @@
     else:
         raise TypeError, "Please give a ModuleLearner or NetworkModule"
 
-def get_last_layer_module_name( learner ):
+
+def getPreviousTopModuleName( learner ):
     last_layer = []
     output_layer = getOutputModuleName(learner)
     connections_list = getConnections(learner)
@@ -173,17 +180,37 @@
        raise TypeError, "find several layers before output layer\n(" + str(last_layer) + ")"
     return last_layer[0]
 
-def get_last_layer_module( learner ):
-    last_module_name = get_last_layer_module_name(learner)
+def getPreviousTopModule( learner ):
+    last_module_name = getPreviousTopModuleName(learner)
     modules_list = getModules( learner )
     for module in modules_list:
         if module.name == last_module_name:
 	   return module
 
+def getTopRBMModule( learner ):
+    topModule = getOutputModule(learner)
+    if isModule( topModule, 'RBM' ):
+       return topModule
+    previous_connections, previous_modulenames = getAllPrevious( learner, topModule.name )
+    for modulename in previous_modulenames:
+        module = getModule(learner,modulename)
+        if isModule( module , 'RBM' ):
+	   return module
+    return None
+
+def getTopRBMModuleName( learner ):
+    module = getTopRBMModule(learner)
+    if module != None:
+       return module.name
+    return None
+
 def getOutputModuleName( learner ):
     outputPort = getOutputPort(learner)
     return port2moduleName(outputPort[1])
 
+def getOutputModule( learner ):
+    return getModule(learner, getOutputModuleName(learner))
+
 def getOutputPort( learner ):
     ports_list = getPorts(learner)
     for port in ports_list:
@@ -593,7 +620,7 @@
 	   learner=learner.learner
         else:
            raise TypeError,  'Sorry, but this code can only be used with ModuleLearner !!!'
-    learner_nickname = 'DBN-2-2-1_'+get_last_layer_module_name( learner ) #os.path.basename(learner_filename)
+    learner_nickname = 'DBN-2-2-1_'+getPreviousTopModuleName( learner ) #os.path.basename(learner_filename)
     
   
     for typeDataSet in ['Train','Valid','Test']:

Added: trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py	2007-06-22 19:24:00 UTC (rev 7633)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py	2007-06-22 22:20:56 UTC (rev 7634)
@@ -0,0 +1,98 @@
+from plearn.learners.modulelearners import *
+
+def basename_withoutExt(name):
+    return '.'.join(os.path.basename(name).split('.')[:-1])
+
+
+####################################
+## Choosing the model to generate ##
+####################################
+
+
+plarg_defaults.gibbs_step                    = 10
+gibbs_step                                   = plargs.gibbs_step # Number of Gibbs step between each sampling
+
+##############################
+# Characteristics of the data
+
+plarg_defaults.Path                          = '/cluster/pauli/data/babyAI/textual_v3'
+plarg_defaults.Encoding                      = '3gram01'
+plarg_defaults.Topics                        = 'shape' #, 'color', location', 'size', 
+plarg_defaults.Size                          = 32
+plarg_defaults.Nobj                          = 1 # 1, 2, 3, 4
+plarg_defaults.ImageType                     = 'gray' # 'gray', 'gray_norot' (without rotation of objects)
+plarg_defaults.trainNsamples                 = 10000
+plarg_defaults.unsupervised_trainNsamples    = 250000 # 10000 250000 1250000
+plarg_defaults.testNsamples                  = 5000
+plarg_defaults.validNsamples                 = plargs.testNsamples
+plarg_defaults.Extension                     = 'vmat'
+
+Path                          = plargs.Path                       # Directory where are the datasets'files
+Encoding                      = plargs.Encoding                   # layerType of Encoding (onehot, 1gram, 2gram, 3gram, ...)
+Topics                        = plargs.Topics                     # Question topic (color, color-size, color-size-location, color-size-location-shape, ...)
+Size                          = plargs.Size                       # Image width/height
+Nobj                          = plargs.Nobj                       # Number of objects in the image
+ImageType                     = plargs.ImageType                  # Color encoding (gray, color) and other features (no rotation...)
+trainNsamples                 = plargs.trainNsamples              # number of training samples
+unsupervised_trainNsamples    = plargs.unsupervised_trainNsamples # number of training samples
+testNsamples                  = plargs.testNsamples               # number of samples to test
+validNsamples                 = plargs.validNsamples              # number of validation samples
+Extension                     = plargs.Extension                  # extension of the input files
+
+
+plarg_defaults.nRBM                     = 3
+plarg_defaults.NH1                      = 500
+plarg_defaults.NH2                      = 500
+plarg_defaults.NH3                      = 500
+plarg_defaults.batchSize                = 50
+plarg_defaults.layerType                = 'gaussian'
+plarg_defaults.unsupervised_nStages     = int(unsupervised_trainNsamples)
+plarg_defaults.supervised_nStages       = 100  # /!\ see after: supervised_nStages   *= trainNsamples
+plarg_defaults.nStagesStep              = 5
+plarg_defaults.LR_CDiv                  = 0.01
+plarg_defaults.LR_GRAD_UNSUP            = 0.003
+plarg_defaults.LR_SUP                   = 0.03
+plarg_defaults.L2wd_SUP                 = 1e-5
+plarg_defaults.nGibbs                   = 1
+plarg_defaults.Tag                      = 'dbn-'+str(plargs.nRBM)+'RBMimage'
+
+# Network structure
+nRBM          = int(plargs.nRBM)
+NH1           = int(plargs.NH1)                        # num units for the image part
+NH2           = int(plargs.NH2)                        # num units, 2nd hid layer (image part)
+NH3           = int(plargs.NH3)                        # num units, 2rd hid layer
+layerType                = plargs.layerType
+#
+# Network learning parameters
+batchSize                 = int(plargs.batchSize)                # num of samples in the minibatch
+unsupervised_nStages      = int(plargs.unsupervised_nStages)                # total number of samples to see (unsupervised phase)
+supervised_nStages        = int(plargs.supervised_nStages)                # total number of samples to see (supervised phase)
+supervised_nStages       *= trainNsamples
+nStagesStep               = int(plargs.nStagesStep)                #
+LR_CDiv                   = float(plargs.LR_CDiv)                # unsup. lr
+LR_GRAD_UNSUP             = float(plargs.LR_GRAD_UNSUP)         # super. lr
+LR_SUP                    = float(plargs.LR_SUP) # super. lr
+L2wd_SUP                  = float(plargs.L2wd_SUP)
+nGibbs                     = int(plargs.nGibbs)
+#
+# Other
+Tag                     = plargs.Tag
+
+
+
+BaseDir =  '/u/louradoj/PRGM/blocksworld/res/'+os.path.basename(Path)
+data_filename = Path+'/BABYAI_'+ImageType+'_'+str(unsupervised_trainNsamples)+'x'+str(Nobj)+'obj_'+str(Size)+'x'+str(Size)+'.'+Topics+'.train.'+Encoding+'.'+Extension
+BaseTag = Tag+'_'+layerType+'_'+basename_withoutExt(data_filename)
+
+unsupervised_expdir = BaseDir + '/greedyDBN/UNSUP_' + BaseTag + '_'+layerType+''.join(['_N'+str(i)+'-'+str(globals()['NH'+str(i)]) for i in range(1,nRBM+1)])+'_LRs'+str(LR_CDiv)+'-'+str(LR_GRAD_UNSUP)+'_ns'+str(unsupervised_nStages)+'_ng'+str(nGibbs) 
+finetuning_expdir = BaseDir + '/greedyDBN/FINETUNE_' + BaseTag + '_'+layerType+''.join(['_N'+str(i)+'-'+str(globals()['NH'+str(i)]) for i in range(1,nRBM+1)])+'_LRs'+str(LR_CDiv)+'-'+str(LR_GRAD_UNSUP)+'-'+str(LR_SUP)+'_WD'+str(L2wd_SUP)+'_ns'+str(unsupervised_nStages)+'-'+str(supervised_nStages)+'_ng'+str(nGibbs)
+
+if LR_SUP==None or LR_SUP == 0:
+   print "UNSUPERVISED-way trained model"
+   learner_filename = unsupervised_expdir+"/init_learner.psave"
+else:
+   print "supervised FINETUNED model"
+   learner_filename = finetuning_expdir+"/Split0/final_learner.psave"
+
+#os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample.py '+' '.join([ learner_filename, str(Size*Size), data_filename, 'gibbs_step='+str(gibbs_step) ]))
+os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_hidden.py '+' '.join([ learner_filename, str(Size*Size), 'gibbs_step='+str(gibbs_step) ]))

Added: trunk/python_modules/plearn/learners/modulelearners/examples/sample.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/sample.py	2007-06-22 19:24:00 UTC (rev 7633)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/sample.py	2007-06-22 22:20:56 UTC (rev 7634)
@@ -0,0 +1,211 @@
+from plearn.learners.modulelearners import *
+import random, sys, os.path
+
+from pygame import *
+from math import *
+
+
+zoom_factor = 5
+
+def init_screen(Nim):
+    init()
+    width = int(sqrt(Nim*1.0))
+    if width**2 != Nim:
+       width = int(sqrt(Nim*1.0))+1
+#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
+    width *= zoom_factor
+    return display.set_mode([width, width])
+
+
+def draw_image(visible,screen):
+    Nim=len(visible)
+    width = int(sqrt(Nim*1.0))
+#    if width**2 != Nim:
+#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
+    width *= zoom_factor
+    surface = Surface((width, width),0,8)
+    surface.set_palette([(i,i,i) for i in range(2**8)])
+    for x in range(width/zoom_factor):
+       for y in range(width/zoom_factor):
+           graycol = max(min(255,int(255.0*visible[x*width/zoom_factor+y])),0)
+           for i in range(zoom_factor):
+               for j in range(zoom_factor):
+                   surface.set_at((x*zoom_factor+i,y*zoom_factor+j),(graycol,graycol,graycol,255))
+    screen.blit(surface, (0,0))
+    display.update()
+    return pause()
+  
+def pause():
+   c = sys.stdin.readline()
+   if c.strip() == 'q' or  c.strip() == 'x':
+      sys.exit(0)
+   if c.strip() == 'n' :
+      return -1
+   try: return int(c.strip())
+   except: return 0
+
+
+if __name__ == "__main__":
+
+  if len(sys.argv) < 2:
+     print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size> <dataSet_filename> [gibbs_step=<gibbs_step>]\n"
+     print "Purpose:\n\tSee consecutive Gibbs sample"
+     print "\twhen input visible units are initalized with real images"
+     print "Tips:\n\tOnce you can see an image type"
+     print "\t:    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
+     print "\t: <an integer> : to change the gibbs step (10 will set the number of gibbs step between each example to 10)"
+     print "\t:      n       : (next) to try with another image as initialization"
+     print "\t:      q       : (quit) to stop the massacre\n"
+     sys.exit()
+
+  learner_filename = sys.argv[1]
+  Nim = int(sys.argv[2])
+  data_filename = sys.argv[3]
+     
+  plarg_defaults.gibbs_step                    = 10
+  gibbs_step                                   = plargs.gibbs_step # Number of Gibbs step between each sampling
+
+     
+  if os.path.isfile(learner_filename) == False:
+     raise TypeError, "Cannot find file "+learner_filename
+  print " loading... "+learner_filename
+  learner = loadObject(learner_filename)
+  if 'HyperLearner' in str(type(learner)):
+     learner=learner.learner
+  
+  if os.path.isfile(data_filename) == False:
+     raise TypeError, "Cannot find file "+data_filename
+  print " loading... "+data_filename
+  dataSet = pl.AutoVMatrix( specification = data_filename )
+
+  #
+  # Getting the RBMmodule which sees the image (looking at size of the down layer)
+  #
+  modules=getModules(learner)
+  for i in range(len(modules)):
+     module = modules[i]
+     if isModule(module,'RBM') and module.connection.down_size == Nim:
+        image_RBM=learner.module.modules[i]
+        break
+  image_RBM_name=image_RBM.name
+  #
+  # Getting the top RBMmodule
+  #
+
+  top_RBM = getTopRBMModule( learner )
+  top_RBM_name = top_RBM.name
+  
+  NH=top_RBM.connection.up_size
+
+
+  init_ports = [ ('input',  image_RBM_name+'.visible'),
+                 ('output', top_RBM_name+'.hidden_sample')
+               ]
+  ports = [ ('input', top_RBM_name+'.hidden_sample' ),
+            ('output', image_RBM_name+'.visible_expectation')
+            ]
+
+  #
+  # Removing useless connections for sampling
+  #
+  old_connections_list = copy.copy(learner.module.connections)
+  conn_toremove=[]
+  connections_list_down=[]
+  connections_list_up=[]
+  for connection in old_connections_list:
+      source_module = getModule( learner, port2moduleName( connection.source ))
+      dest_module   = getModule( learner, port2moduleName( connection.destination ))
+      if isModule( source_module, 'RBM') and isModule( dest_module,'RBM'):
+         connections_list_up.append ( pl.NetworkConnection(source = port2moduleName( connection.source )+'.hidden_sample',
+                                                           destination = port2moduleName( connection.destination )+'.visible_sample',
+                                                           propagate_gradient = 0) )
+         connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_sample',
+                                                    destination = port2moduleName( connection.source )+'.hidden_sample',
+                                                    propagate_gradient = 0) )
+  
+  #
+  # Removing useless modules for sampling
+  #
+  modules_list = getModules(learner)
+  mod_toremove=[]
+  for module in modules_list:
+      if isModule( module, 'RBM') == False:
+         mod_toremove.append(module)
+  for module in mod_toremove:
+      modules_list.remove(module)
+  
+  
+  RBMnetwork = pl.NetworkModule(
+                          modules = modules_list,
+                          connections = connections_list_down,
+                          ports = ports,
+                          # to avoid calling the forget() method in ModuleLearner                          
+                          random_gen = pl.PRandom( seed = 1827 ),
+                          # Hack from Olivier
+                          save_states = 0
+                         )
+  RBMnetworkInit = pl.NetworkModule(
+                          modules = modules_list,
+                          connections = connections_list_up,
+                          ports = init_ports,
+                          # to avoid calling the forget() method in ModuleLearner                          
+                          random_gen = pl.PRandom( seed = 1827 ),
+                          # Hack from Olivier
+                          save_states = 0
+                         )
+
+  
+  RBMmodel = pl.ModuleLearner(
+                              cost_ports = [],
+                              target_ports = [],
+                              module = RBMnetwork
+                           )
+  RBMmodelInit = pl.ModuleLearner(
+                              cost_ports = [],
+                              target_ports = [],
+                              module = RBMnetworkInit
+                           )
+
+
+
+  RBMmodelInit.setTrainingSet(pl.AutoVMatrix(inputsize=Nim, targetsize=0, weightsize=0), False)
+  RBMmodel.setTrainingSet(pl.AutoVMatrix(inputsize=NH, targetsize=0, weightsize=0), False)
+
+
+  screen=init_screen(Nim)
+  random.seed(1969)
+
+  for i in range(len(RBMmodel.module.modules)):
+    module = RBMmodel.module.modules[i]
+    if isModule( module, 'RBM'):
+       RBMmodel.module.modules[i].compute_contrastive_divergence = False
+       if module.name == top_RBM_name:
+          RBMmodel.module.modules[i].n_Gibbs_steps_per_generated_sample = gibbs_step
+          top_RBM = RBMmodel.module.modules[i]
+
+  while True:
+ 
+   random_index=random.randint(0,dataSet.length)
+   init_image=[dataSet.getRow(random_index)[i] for i in range(Nim)]
+   
+   c = draw_image( init_image, screen )
+   if c<0:
+      continue
+   elif c>0:
+      top_RBM.n_Gibbs_steps_per_generated_sample = c
+   
+   init_hidden = RBMmodelInit.computeOutput(init_image)
+   c = draw_image( RBMmodel.computeOutput(init_hidden), screen )
+   if c<0:
+      break
+   elif c>0:
+      top_RBM.n_Gibbs_steps_per_generated_sample = c
+   
+      
+   while True:
+       c = draw_image( RBMmodel.computeOutput([]) , screen)
+       if c<0:
+          break
+       elif c>0:
+          top_RBM.n_Gibbs_steps_per_generated_sample = c
+    

Added: trunk/python_modules/plearn/learners/modulelearners/examples/sample_from_hidden.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/sample_from_hidden.py	2007-06-22 19:24:00 UTC (rev 7633)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/sample_from_hidden.py	2007-06-22 22:20:56 UTC (rev 7634)
@@ -0,0 +1,173 @@
+from plearn.learners.modulelearners import *
+import random, sys, os.path
+
+from pygame import *
+from math import *
+
+
+zoom_factor = 5
+
+def init_screen(Nim):
+    init()
+    width = int(sqrt(Nim*1.0))
+    if width**2 != Nim:
+       width = int(sqrt(Nim*1.0))+1
+#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
+    width *= zoom_factor
+    return display.set_mode([width, width])
+
+
+def draw_image(visible,screen):
+    Nim=len(visible)
+    width = int(sqrt(Nim*1.0))
+#    if width**2 != Nim:
+#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
+    width *= zoom_factor
+    surface = Surface((width, width),0,8)
+    surface.set_palette([(i,i,i) for i in range(2**8)])
+    for x in range(width/zoom_factor):
+       for y in range(width/zoom_factor):
+           graycol = max(min(255,int(255.0*visible[x*width/zoom_factor+y])),0)
+           for i in range(zoom_factor):
+               for j in range(zoom_factor):
+                   surface.set_at((x*zoom_factor+i,y*zoom_factor+j),(graycol,graycol,graycol,255))
+    screen.blit(surface, (0,0))
+    display.update()
+    return pause()
+  
+def pause():
+   c = sys.stdin.readline()
+   if c.strip() == 'q' or  c.strip() == 'x':
+      sys.exit(0)
+   if c.strip() == 'n' :
+      return -1
+   try: return int(c.strip())
+   except: return 0
+
+
+if __name__ == "__main__":
+
+  if len(sys.argv) < 2:
+     print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size>\n"
+     print "Purpose:\n\tSee consecutive Gibbs sample"
+     print "\twhen top hidden units are initalized randomly"
+     print "Tips:\n\tOnce you can see an image type"
+     print "\t:    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
+     print "\t: <an integer> : to change the gibbs step (10 will set the number of gibbs step between each example to 10)"
+     print "\t:      n       : (next) to try another hidden state for initialization"
+     print "\t:      q       : (quit) to stop the massacre\n"
+     sys.exit()
+
+  learner_filename = sys.argv[1]
+  Nim = int(sys.argv[2])
+     
+  plarg_defaults.gibbs_step                    = 10
+  gibbs_step                                   = plargs.gibbs_step # Number of Gibbs step between each sampling
+
+     
+  if os.path.isfile(learner_filename) == False:
+     raise TypeError, "Cannot find file "+learner_filename
+  print " loading... "+learner_filename
+  learner = loadObject(learner_filename)
+  if 'HyperLearner' in str(type(learner)):
+     learner=learner.learner
+
+  #
+  # Getting the RBMmodule which sees the image (looking at size of the down layer)
+  #
+  modules=getModules(learner)
+  for i in range(len(modules)):
+     module = modules[i]
+     if isModule(module,'RBM') and module.connection.down_size == Nim:
+        image_RBM=learner.module.modules[i]
+        break
+  image_RBM_name=image_RBM.name
+  #
+  # Getting the top RBMmodule
+  #
+
+  top_RBM = getTopRBMModule( learner )
+  top_RBM_name = top_RBM.name
+  
+  NH=top_RBM.connection.up_size
+
+
+  ports = [ ('input', top_RBM_name+'.hidden_sample' ),
+            ('output', image_RBM_name+'.visible_expectation')
+            ]
+
+  #
+  # Removing useless connections for sampling
+  #
+  old_connections_list = copy.copy(learner.module.connections)
+  conn_toremove=[]
+  connections_list_down=[]
+  connections_list_up=[]
+  for connection in old_connections_list:
+      source_module = getModule( learner, port2moduleName( connection.source ))
+      dest_module   = getModule( learner, port2moduleName( connection.destination ))
+      if isModule( source_module, 'RBM') and isModule( dest_module,'RBM'):
+         connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_sample',
+                                                    destination = port2moduleName( connection.source )+'.hidden_sample',
+                                                    propagate_gradient = 0) )
+  
+  #
+  # Removing useless modules for sampling
+  #
+  modules_list = getModules(learner)
+  mod_toremove=[]
+  for module in modules_list:
+      if isModule( module, 'RBM') == False:
+         mod_toremove.append(module)
+  for module in mod_toremove:
+      modules_list.remove(module)
+  
+  
+  RBMnetwork = pl.NetworkModule(
+                          modules = modules_list,
+                          connections = connections_list_down,
+                          ports = ports,
+                          # to avoid calling the forget() method in ModuleLearner                          
+                          random_gen = pl.PRandom( seed = 1827 ),
+                          # Hack from Olivier
+                          save_states = 0
+                         )
+
+  
+  RBMmodel = pl.ModuleLearner(
+                              cost_ports = [],
+                              target_ports = [],
+                              module = RBMnetwork
+                           )
+
+
+  RBMmodel.setTrainingSet(pl.AutoVMatrix(inputsize=NH, targetsize=0, weightsize=0), False)
+
+  screen=init_screen(Nim)
+  random.seed(1969)
+
+  for i in range(len(RBMmodel.module.modules)):
+    module = RBMmodel.module.modules[i]
+    if isModule( module, 'RBM'):
+       RBMmodel.module.modules[i].compute_contrastive_divergence = False
+       if module.name == top_RBM_name:
+          RBMmodel.module.modules[i].n_Gibbs_steps_per_generated_sample = gibbs_step
+          top_RBM = RBMmodel.module.modules[i]
+
+  while True:
+ 
+   init_hidden=[random.randint(0,1) for i in range(NH)]
+   
+   c = draw_image( RBMmodel.computeOutput(init_hidden), screen )
+   if c<0:
+      continue
+   elif c>0:
+      top_RBM.n_Gibbs_steps_per_generated_sample = c
+      
+   while True:
+       c = draw_image( RBMmodel.computeOutput([]) , screen)
+       if c<0:
+          break
+       elif c>0:
+          top_RBM.n_Gibbs_steps_per_generated_sample = c
+    



From louradou at mail.berlios.de  Sat Jun 23 00:31:17 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 23 Jun 2007 00:31:17 +0200
Subject: [Plearn-commits] r7635 -
	trunk/python_modules/plearn/learners/modulelearners/examples
Message-ID: <200706222231.l5MMVH0p021777@sheep.berlios.de>

Author: louradou
Date: 2007-06-23 00:31:16 +0200 (Sat, 23 Jun 2007)
New Revision: 7635

Modified:
   trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py
Log:


Modified: trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py	2007-06-22 22:20:56 UTC (rev 7634)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py	2007-06-22 22:31:16 UTC (rev 7635)
@@ -9,7 +9,7 @@
 ####################################
 
 
-plarg_defaults.gibbs_step                    = 10
+plarg_defaults.gibbs_step                    = 1
 gibbs_step                                   = plargs.gibbs_step # Number of Gibbs step between each sampling
 
 ##############################
@@ -51,7 +51,7 @@
 plarg_defaults.nStagesStep              = 5
 plarg_defaults.LR_CDiv                  = 0.01
 plarg_defaults.LR_GRAD_UNSUP            = 0.003
-plarg_defaults.LR_SUP                   = 0.03
+plarg_defaults.LR_SUP                   = 0
 plarg_defaults.L2wd_SUP                 = 1e-5
 plarg_defaults.nGibbs                   = 1
 plarg_defaults.Tag                      = 'dbn-'+str(plargs.nRBM)+'RBMimage'
@@ -94,5 +94,23 @@
    print "supervised FINETUNED model"
    learner_filename = finetuning_expdir+"/Split0/final_learner.psave"
 
-#os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample.py '+' '.join([ learner_filename, str(Size*Size), data_filename, 'gibbs_step='+str(gibbs_step) ]))
-os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_hidden.py '+' '.join([ learner_filename, str(Size*Size), 'gibbs_step='+str(gibbs_step) ]))
+
+def check_choice(c):
+    try:
+       if int(c) in [1,2]:
+          return True
+    except: pass
+    return False
+
+
+c=None
+while check_choice(c)==False:
+   print "Type the number corresponding to your choice :"
+   print "1. Sample after having initialized input visible units with (randomly picked) real image"
+   print "2. Sample after having initialized top hidden units with a random binary vector"
+   c = sys.stdin.readline()
+
+if int(c) == 1:
+   os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample.py '+' '.join([ learner_filename, str(Size*Size), data_filename, 'gibbs_step='+str(gibbs_step) ]))
+elif int(c) == 2:
+   os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_hidden.py '+' '.join([ learner_filename, str(Size*Size), 'gibbs_step='+str(gibbs_step) ]))



From louradou at mail.berlios.de  Sat Jun 23 00:32:06 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 23 Jun 2007 00:32:06 +0200
Subject: [Plearn-commits] r7636 - trunk/plearn_learners/online
Message-ID: <200706222232.l5MMW6OQ022877@sheep.berlios.de>

Author: louradou
Date: 2007-06-23 00:31:54 +0200 (Sat, 23 Jun 2007)
New Revision: 7636

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-22 22:31:16 UTC (rev 7635)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-22 22:31:54 UTC (rev 7636)
@@ -581,10 +581,12 @@
 
 void RBMModule::fprop(const TVec<Mat*>& ports_value)
 {
+
     PLASSERT( ports_value.length() == nPorts() );
     PLASSERT( visible_layer );
     PLASSERT( hidden_layer );
     PLASSERT( connection );
+        
     Mat* visible = ports_value[getPortIndex("visible")]; 
     Mat* hidden = ports_value[getPortIndex("hidden.state")];
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
@@ -787,49 +789,49 @@
     // SAMPLING
     if ((visible_sample && visible_sample->isEmpty())               // is asked to sample visible units (discrete)
         || (visible_expectation && visible_expectation->isEmpty())  //              "                   (continous)
-	|| (hidden_sample && hidden_sample->isEmpty()))             // or to sample hidden units
+        || (hidden_sample && hidden_sample->isEmpty()))             // or to sample hidden units
     {
         if (hidden_sample && !hidden_sample->isEmpty()) // sample visible conditionally on hidden
         {
             sampleVisibleGivenHidden(*hidden_sample);
-	    Gibbs_step=0;
-	    cout << "sampling init (from hidden)" << endl;
+            Gibbs_step=0;
+            cout << "sampling visible from hidden" << endl;
         }
         else if (visible_sample && !visible_sample->isEmpty()) // if an input is provided, sample hidden conditionally
         {
             sampleHiddenGivenVisible(*visible_sample);
-	    Gibbs_step=0;
-	    cout << "sampling init (from visible)" << endl;
-	}
+            Gibbs_step=0;
+            cout << "sampling hidden from (discrete) visible" << endl;
+        }
         else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
         {
             visible_layer->generateSamples();
-	    sampleHiddenGivenVisible(visible_layer->samples);
-	    Gibbs_step=0;
-	    cout << "sampling init (from visible)" << endl;
+            sampleHiddenGivenVisible(visible_layer->samples);
+            Gibbs_step=0;
+            cout << "sampling hidden from visible expectation" << endl;
         }
         else if (visible_expectation && !visible_expectation->isEmpty()) 
         {
-	     PLERROR("In RBMModule::fprop visible_expectation can only be an output port (use visible as input port");
-	}
+             PLERROR("In RBMModule::fprop visible_expectation can only be an output port (use visible as input port");
+        }
         else // sample unconditionally: Gibbs sample after k steps
         {
             // the visible_layer->expectations contain the "state" from which we
             // start or continue the chain
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
-            cout << "Gibbs sampling " << Gibbs_step;
+            cout << "Gibbs sampling " << Gibbs_step+1;
             for (;Gibbs_step<min_n;Gibbs_step++)
             {
                 sampleHiddenGivenVisible(visible_layer->samples);
                 sampleVisibleGivenHidden(hidden_layer->samples);
             }
-  	    cout << " -> " << Gibbs_step-1 << endl;
+              cout << " -> " << Gibbs_step << endl;
         }
 
-	if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
+        if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
         {
-   	      const Mat& hidden_expect = hidden_layer->getExpectations();
+              const Mat& hidden_expect = hidden_layer->getExpectations();
               hidden->resize(hidden_expect.length(), hidden_expect.width());
               *hidden << hidden_expect;
         }
@@ -847,16 +849,14 @@
         }
         if (visible_expectation && visible_expectation->isEmpty()) // provide expectation of the visible units
         {
-	    const Mat& to_store = visible_layer->getExpectations();
+            const Mat& to_store = visible_layer->getExpectations();
             visible_expectation->resize(to_store.length(),
-	                                to_store.width());
+                                        to_store.width());
             *visible_expectation << to_store;
         }
-
-	found_a_valid_configuration = true;
-    } // END SAMPLING
-
-
+        found_a_valid_configuration = true;
+    }
+    
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)
     {
@@ -949,7 +949,11 @@
             PLERROR("RBMModule: unknown configuration to compute contrastive_divergence (currently\n"
                     "only possible if only visible are provided in input).\n");
         found_a_valid_configuration = true;
-    }
+    }// END SAMPLING
+    
+
+    
+    
     // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
     if ( visible && !visible->isEmpty() && 
          ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) || 
@@ -1000,14 +1004,29 @@
     }
     
 
+	
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;
     weights = NULL;
     hidden_activations_are_computed = false;
 
+
+
     if (!found_a_valid_configuration)
-        PLERROR("In RBMModule::fprop - Unknown port configuration");
+    {
+        if (visible)
+        cout << "visible_empty : "<< (bool) visible->isEmpty() << endl;
+        if (hidden)
+        cout << "hidden_empty : "<< (bool) hidden->isEmpty() << endl;
+        if (visible_sample)
+        cout << "visible_sample_empty : "<< (bool) visible_sample->isEmpty() << endl;
+        if (hidden_sample)
+        cout << "hidden_sample_empty : "<< (bool) hidden_sample->isEmpty() << endl;
+        if (visible_expectation)
+        cout << "visible_expectation_empty : "<< (bool) visible_expectation->isEmpty() << endl;
+        PLERROR("In RBMModule::fprop - Unknown port configuration for module %s", name.c_str());
+    }
 
     checkProp(ports_value);
 



From dorionc at mail.berlios.de  Sat Jun 23 00:38:43 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Sat, 23 Jun 2007 00:38:43 +0200
Subject: [Plearn-commits] r7637 - trunk/python_modules/plearn/analysis
Message-ID: <200706222238.l5MMch7k005042@sheep.berlios.de>

Author: dorionc
Date: 2007-06-23 00:38:42 +0200 (Sat, 23 Jun 2007)
New Revision: 7637

Modified:
   trunk/python_modules/plearn/analysis/latex.py
Log:


Modified: trunk/python_modules/plearn/analysis/latex.py
===================================================================
--- trunk/python_modules/plearn/analysis/latex.py	2007-06-22 22:31:54 UTC (rev 7636)
+++ trunk/python_modules/plearn/analysis/latex.py	2007-06-22 22:38:42 UTC (rev 7637)
@@ -80,8 +80,8 @@
         writer("  ")
         formatTableLine(headers, writer)
         lwriter("\\hline\\hline")
-    else:
-        lwriter("\\hline")
+    # else:
+    #     lwriter("\\hline")
  
     for line in table:
         writer("  ")
@@ -89,7 +89,9 @@
             lwriter(line) # Single string is wrote as is...
         else:
             formatTableLine(line, writer)
-    lwriter("\\hline")
+
+    if headers:
+        lwriter("\\hline")
        
     lwriter("\\end{tabular}")
 



From chapados at mail.berlios.de  Mon Jun 25 21:38:47 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 25 Jun 2007 21:38:47 +0200
Subject: [Plearn-commits] r7638 - trunk/plearn/math
Message-ID: <200706251938.l5PJclCY013470@sheep.berlios.de>

Author: chapados
Date: 2007-06-25 21:38:46 +0200 (Mon, 25 Jun 2007)
New Revision: 7638

Modified:
   trunk/plearn/math/ObservationWindow.cc
   trunk/plearn/math/ObservationWindow.h
   trunk/plearn/math/VecStatsCollector.cc
   trunk/plearn/math/VecStatsCollector.h
Log:
Added mechanism allowing StatsCollector to be re-updated from scratch at a certain frequency from the observations in the window, when using a window -- useful for numerical stability and windowed min/max

Modified: trunk/plearn/math/ObservationWindow.cc
===================================================================
--- trunk/plearn/math/ObservationWindow.cc	2007-06-22 22:38:42 UTC (rev 7637)
+++ trunk/plearn/math/ObservationWindow.cc	2007-06-25 19:38:46 UTC (rev 7638)
@@ -41,8 +41,9 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     ObservationWindow,
-    "ONE LINE USER DESCRIPTION",
-    "MULTI LINE\nHELP FOR USERS"
+    "Used by StatsCollector to keep a finite-size window of observations",
+    "This allows StatsCollector to compute finite-horizon moving averages, for\n"
+    "instance."
     );
 
 

Modified: trunk/plearn/math/ObservationWindow.h
===================================================================
--- trunk/plearn/math/ObservationWindow.h	2007-06-22 22:38:42 UTC (rev 7637)
+++ trunk/plearn/math/ObservationWindow.h	2007-06-25 19:38:46 UTC (rev 7638)
@@ -49,6 +49,12 @@
 namespace PLearn {
 using namespace std;
 
+/**
+ *  Used by StatsCollector to keep a finite-size window of observations
+ *
+ *  This allows StatsCollector to compute finite-horizon moving averages, for
+ *  instance.
+ */
 class ObservationWindow: public Object
 {
     typedef Object inherited;

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-06-22 22:38:42 UTC (rev 7637)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-06-25 19:38:46 UTC (rev 7638)
@@ -53,10 +53,12 @@
       compute_covariance(false),
       epsilon(0.0),
       m_window(-1),
+      m_full_update_frequency(-1),
       m_window_nan_code(0),
       no_removal_warnings(false), // Window mechanism
       sum_non_missing_weights(0),
-      sum_non_missing_square_weights(0)
+      sum_non_missing_square_weights(0),
+      m_num_incremental(0)
 { }
 
 PLEARN_IMPLEMENT_OBJECT(
@@ -100,6 +102,26 @@
         "VecStatsCollector::remove_observation mechanism.\n"
         "Default: -1 (all observations are considered);\n"
         " -2 means all observations kept in an ObservationWindow\n");
+
+    declareOption(
+        ol, "full_update_frequency", &VecStatsCollector::m_full_update_frequency,
+        OptionBase::buildoption,
+        "If the window mechanism is used, number of updates at which a full\n"
+        "update of the underlying StatsCollector is performed.  A 'full update'\n"
+        "is defined as:\n"
+        "\n"
+        "- 1. Calling forget()\n"
+        "- 2. Updating the StatsCollector from all observations in the window.\n"
+        "\n"
+        "This is useful for two reasons: 1) when performing a remove-observation\n"
+        "on a StatsCollector that contains a wide range of values, the\n"
+        "accumulators for the fourth power may become negative, yielding\n"
+        "inconsistent estimation.  2) without this option, the statistics\n"
+        "'FIRST', 'LAST', 'MIN', 'MAX' are not updated properly in the presence\n"
+        "of a window.  To get proper estimation of these statistics, you must\n"
+        "use the setting 'full_update_frequency=1'.\n"
+        "\n"
+        "Default value: -1 (never re-update the StatsCollector from scratch).\n");
     
     declareOption(
         ol, "window_nan_code", &VecStatsCollector::m_window_nan_code,
@@ -107,10 +129,10 @@
         "How to deal with update vectors containing NaNs with respect to the\n"
         "window mechanism.\n"
         "\n"
-        " 0 - Do not check for NaNs (all updates are accounted in the window)\n"
-        " 1 - If *all* entries of the update vector are NaNs, do not account for\n"
+        "- 0: Do not check for NaNs (all updates are accounted in the window)\n"
+        "- 1: If *all* entries of the update vector are NaNs, do not account for\n"
         "     that observation in the window.\n"
-        " 2 - If *any* entries of the update vector are NaNs, do not account for\n"
+        "- 2: If *any* entries of the update vector are NaNs, do not account for\n"
         "     that observation in the window.\n"
         "\n"
         " Default: 0" );
@@ -118,12 +140,11 @@
     declareOption(
         ol, "no_removal_warnings", &VecStatsCollector::no_removal_warnings,
         OptionBase::buildoption,
-        "If the remove_observation mecanism is used and the removed\n"
-        "value is equal to one of last_, min_ or max_, the default\n"
-        "behavior is to warn the user.\n"
+        "If the remove_observation mechanism is used (without\n"
+        "'full_update_frequency=1') and the removed value is equal to one of\n"
+        "first_, last_, min_ or max_, the default behavior is to warn the user.\n"
         "\n"
-        "If one want to disable this feature, he may set\n"
-        "no_removal_warnings to true.\n"
+        "To disable this feature, set 'no_removal_warnings' to true.\n"
         "\n"
         "Default: false (0)." );
   
@@ -294,9 +315,31 @@
                 "%d, while size of stats (and most likely previously seen vector) is "
                 "%d", n, stats.size());
 
-    for(int k=0; k<n; k++)
+    // Update the underlying StatsCollectors.  If we use the window mechanism
+    // and we are at a boundary given by m_full_update_frequency, perform a
+    // full re-update of the StatsCollectors from the saved observations in the
+    // window.
+    if ((m_window > 0 || m_window == -2) && m_full_update_frequency > 0 &&
+        ++m_num_incremental >= m_full_update_frequency)
+    {
+        for (int k=0 ; k<n ; ++k)
+            stats[k].forget();
+        // Drop oldest observation in window to make room for new observation:
+        // start at t=1
+        for (int t=1 ; t<m_observation_window->length() ; ++t) {
+            Vec obs = m_observation_window->getObs(t);
+            real w  = m_observation_window->getWeight(t);
+            for (int k=0 ; k<n ; ++k)
+                stats[k].update(obs[k], w);
+        }
+        m_num_incremental = 0;
+    }
+
+    // Incremental update with current observation, as usual
+    for(int k=0; k<n; ++k)
         stats[k].update(x[k], weight);
 
+    // Compute covariance if required
     if(compute_covariance) {
         if (x.hasMissing()) {
             // Slower version to handle missing values.
@@ -333,7 +376,10 @@
         tuple<Vec, real> outdated = m_observation_window->update(x, weight);
         Vec& obs = get<0>(outdated);
         real w = get<1>(outdated);
-        if(obs.isNotEmpty() && m_window > 0)
+
+        // If m_num_incremental==0, we just re-updated the StatsCollectors from
+        // scratch.  In this case, don't call remove_observation.
+        if(obs.isNotEmpty() && m_window > 0 && m_num_incremental > 0)
             remove_observation(obs, w);
     }
 }

Modified: trunk/plearn/math/VecStatsCollector.h
===================================================================
--- trunk/plearn/math/VecStatsCollector.h	2007-06-22 22:38:42 UTC (rev 7637)
+++ trunk/plearn/math/VecStatsCollector.h	2007-06-25 19:38:46 UTC (rev 7638)
@@ -76,37 +76,57 @@
     double epsilon;
 
     /**
-     * If positive, the window restricts the stats computed by this
-     * FinVecStatsCollector to the last 'window' observations. This uses the
-     * VecStatsCollector::remove_observation mechanism.
+     *  If positive, the window restricts the stats computed by this
+     *  FinVecStatsCollector to the last 'window' observations. This uses the
+     *  VecStatsCollector::remove_observation mechanism; but see
+     *  'full_update_frequency' below.
      *
-     * Default: -1 (all observations are considered).
+     *  Default: -1 (all observations are considered).
      */
     int m_window;
 
     /**
-     * How to deal with update vectors containing NaNs with respect to the
-     * window mechanism.
+     *  If the window mechanism is used, number of updates at which a full
+     *  update of the underlying StatsCollector is performed.  A 'full update'
+     *  is defined as:
      *
-     *  0 - Do not check for NaNs (all updates are accounted in the window)
-     *  1 - If *all* entries of the update vector are NaNs, do not account for
-     *      that observation in the window.
-     *  2 - If *any* entries of the update vector are NaNs, do not account for
-     *      that observation in the window.
+     *  - 1. Calling forget()
+     *  - 2. Updating the StatsCollector from all observations in the window.
      *
+     *  This is useful for two reasons: 1) when performing a remove-observation
+     *  on a StatsCollector that contains a wide range of values, the
+     *  accumulators for the fourth power may become negative, yielding
+     *  inconsistent estimation.  2) without this option, the statistics
+     *  'FIRST', 'LAST', 'MIN', 'MAX' are not updated properly in the presence
+     *  of a window.  To get proper estimation of these statistics, you must
+     *  use the setting 'full_update_frequency=1'.
+     *
+     *  Default value: -1 (never re-update the StatsCollector from scratch).
+     */
+    int m_full_update_frequency;
+    
+    /**
+     *  How to deal with update vectors containing NaNs with respect to the
+     *  window mechanism.
+     *
+     *  - 0: Do not check for NaNs (all updates are accounted in the window)
+     *  - 1: If *all* entries of the update vector are NaNs, do not account for
+     *       that observation in the window.
+     *  - 2: If *any* entries of the update vector are NaNs, do not account for
+     *       that observation in the window.
+     *
      *  Default: 0
      */
     int m_window_nan_code;
     
     /**
-     * If the remove_observation mecanism is used and the removed
-     * value is equal to one of first_, last_, min_ or max_, the default
-     * behavior is to warn the user.
+     *  If the remove_observation mechanism is used (without
+     *  'full_update_frequency=1') and the removed value is equal to one of
+     *  first_, last_, min_ or max_, the default behavior is to warn the user.
      * 
-     * If one want to disable this feature, he may set
-     * no_removal_warnings to true.
+     *  To disable this feature, set 'no_removal_warnings' to true.
      *
-     * Default: false (0).
+     *  Default: false (0).
      */
     bool no_removal_warnings;
 
@@ -280,6 +300,10 @@
 
     //! Window mechanism
     PP<ObservationWindow> m_observation_window;
+
+    //! (Window mechanism) Number of incremental updates since the last
+    //! update from scratch of the underlying statscollectors
+    int m_num_incremental;
     
 protected: 
     //! Declares this class' options



From chapados at mail.berlios.de  Mon Jun 25 21:40:02 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 25 Jun 2007 21:40:02 +0200
Subject: [Plearn-commits] r7639 - trunk/python_modules/plearn/plide
Message-ID: <200706251940.l5PJe2bk013737@sheep.berlios.de>

Author: chapados
Date: 2007-06-25 21:39:54 +0200 (Mon, 25 Jun 2007)
New Revision: 7639

Modified:
   trunk/python_modules/plearn/plide/plide_tabs.py
Log:
Do not abort if scintilla is not installed on machine

Modified: trunk/python_modules/plearn/plide/plide_tabs.py
===================================================================
--- trunk/python_modules/plearn/plide/plide_tabs.py	2007-06-25 19:38:46 UTC (rev 7638)
+++ trunk/python_modules/plearn/plide/plide_tabs.py	2007-06-25 19:39:54 UTC (rev 7639)
@@ -38,7 +38,10 @@
 pygtk.require('2.0')
 import gtk
 import gobject
-import scintilla
+try:
+    import scintilla
+except ImportError:
+    pass                                # Ignore scintilla problems
 
 from plearn.pl_pygtk  import MessageBox
 from plearn.vmat.PMat import PMat



From chapados at mail.berlios.de  Mon Jun 25 22:59:31 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 25 Jun 2007 22:59:31 +0200
Subject: [Plearn-commits] r7640 - trunk/plearn/math
Message-ID: <200706252059.l5PKxVTm019219@sheep.berlios.de>

Author: chapados
Date: 2007-06-25 22:59:31 +0200 (Mon, 25 Jun 2007)
New Revision: 7640

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
Bugfix in full_update_frequency mechanism

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-06-25 19:39:54 UTC (rev 7639)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-06-25 20:59:31 UTC (rev 7640)
@@ -379,8 +379,11 @@
 
         // If m_num_incremental==0, we just re-updated the StatsCollectors from
         // scratch.  In this case, don't call remove_observation.
-        if(obs.isNotEmpty() && m_window > 0 && m_num_incremental > 0)
+        if(obs.isNotEmpty() && m_window > 0 &&
+           (m_full_update_frequency <= 0 || m_num_incremental > 0))
+        {
             remove_observation(obs, w);
+        }
     }
 }
 



From chapados at mail.berlios.de  Mon Jun 25 23:02:02 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 25 Jun 2007 23:02:02 +0200
Subject: [Plearn-commits] r7641 - in trunk/plearn/opt/test/.pytest:
	PL_ConjGradientRosenbrock100/expected_results
	PL_ConjGradientRosenbrock2/expected_results
Message-ID: <200706252102.l5PL22Eg019422@sheep.berlios.de>

Author: chapados
Date: 2007-06-25 23:02:02 +0200 (Mon, 25 Jun 2007)
New Revision: 7641

Modified:
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log
Log:
New test results following new option in VecStatsCollector

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log	2007-06-25 20:59:31 UTC (rev 7640)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log	2007-06-25 21:02:02 UTC (rev 7641)
@@ -45,6 +45,7 @@
 compute_covariance = 0;
 epsilon = 0;
 window = -1;
+full_update_frequency = -1;
 window_nan_code = 0;
 no_removal_warnings = 0;
 stats = # samples: 12

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log	2007-06-25 20:59:31 UTC (rev 7640)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log	2007-06-25 21:02:02 UTC (rev 7641)
@@ -45,6 +45,7 @@
 compute_covariance = 0;
 epsilon = 0;
 window = -1;
+full_update_frequency = -1;
 window_nan_code = 0;
 no_removal_warnings = 0;
 stats = # samples: 12



From nouiz at mail.berlios.de  Tue Jun 26 18:48:59 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 26 Jun 2007 18:48:59 +0200
Subject: [Plearn-commits] r7642 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706261648.l5QGmxVw019999@sheep.berlios.de>

Author: nouiz
Date: 2007-06-26 18:48:51 +0200 (Tue, 26 Jun 2007)
New Revision: 7642

Added:
   branches/cgi-desjardin/plearn_learners/second_iteration/ImputationVMatrix.h
Log:
Added Missing header file


Added: branches/cgi-desjardin/plearn_learners/second_iteration/ImputationVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/ImputationVMatrix.h	2007-06-25 21:02:02 UTC (rev 7641)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/ImputationVMatrix.h	2007-06-26 16:48:51 UTC (rev 7642)
@@ -0,0 +1,87 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2001 Pascal Vincent, Yoshua Bengio, Rejean Ducharme and University of Montreal
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux
+// Copyright (C) 2003 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ******************************************************************      
+   * $Id: ImputationVMatrix.h 3658 2005-07-06 20:30:15  Godbout $
+   ****************************************************************** */
+
+/*! \file PLearnLibrary/PLearnCore/VMat.h */
+
+#ifndef ImputationVMatrix_INC
+#define ImputationVMatrix_INC
+
+#include <plearn/vmat/SourceVMatrix.h>
+#include <plearn/vmat/FileVMatrix.h>
+#include <plearn/io/fileutils.h>                     //!<  For isfile()
+#include <plearn/math/BottomNI.h>
+
+namespace PLearn {
+using namespace std;
+
+class ImputationVMatrix: public VMatrix
+{
+  typedef VMatrix inherited;
+  
+public:
+
+  //! The source VMatrix with missing values.
+  VMat                  source;
+  int                   test_level;
+
+                        ImputationVMatrix();
+  virtual               ~ImputationVMatrix();
+
+  virtual void          build();
+  virtual void          makeDeepCopyFromShallowCopy(CopiesMap& copies);
+          void          testResultantVMatrix();
+protected:
+  //! Declares this class' options
+  // (Please implement in .cc)
+  static void           declareOptions(OptionList &ol);
+
+private:
+  
+         void           build_();
+  
+  PLEARN_DECLARE_ABSTRACT_OBJECT(ImputationVMatrix);
+
+};
+
+DECLARE_OBJECT_PTR(ImputationVMatrix);
+
+} // end of namespcae PLearn
+#endif



From nouiz at mail.berlios.de  Tue Jun 26 18:51:14 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 26 Jun 2007 18:51:14 +0200
Subject: [Plearn-commits] r7643 - in branches/cgi-desjardin: commands
	plearn_learners/second_iteration
Message-ID: <200706261651.l5QGpE6s023337@sheep.berlios.de>

Author: nouiz
Date: 2007-06-26 18:51:13 +0200 (Tue, 26 Jun 2007)
New Revision: 7643

Removed:
   branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h
   branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h
Modified:
   branches/cgi-desjardin/commands/plearn_gg_inc.h
   branches/cgi-desjardin/commands/plearn_inc_gg.h
Log:
Removec file that are already in PLearn


Modified: branches/cgi-desjardin/commands/plearn_gg_inc.h
===================================================================
--- branches/cgi-desjardin/commands/plearn_gg_inc.h	2007-06-26 16:48:51 UTC (rev 7642)
+++ branches/cgi-desjardin/commands/plearn_gg_inc.h	2007-06-26 16:51:13 UTC (rev 7643)
@@ -66,7 +66,7 @@
 #include <plearn_learners/second_iteration/ConditionalMeanImputationVMatrix.h>
 #include <plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h>
 #include <plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h>
-#include <plearn_learners/second_iteration/BallTreeNearestNeighbors.h>
+//#include <plearn_learners/second_iteration/BallTreeNearestNeighbors.h>
 #include <plearn_learners/second_iteration/WeightedDistance.h>
 #include <plearn_learners/regressors/RegressionTree.h>
 #include <plearn/vmat/VariableDeletionVMatrix.h>

Modified: branches/cgi-desjardin/commands/plearn_inc_gg.h
===================================================================
--- branches/cgi-desjardin/commands/plearn_inc_gg.h	2007-06-26 16:48:51 UTC (rev 7642)
+++ branches/cgi-desjardin/commands/plearn_inc_gg.h	2007-06-26 16:51:13 UTC (rev 7643)
@@ -185,7 +185,7 @@
 #include <plearn_learners/distributions/UniformDistribution.h>
 
 // Nearest-Neighbors
-//gg#include <plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h>
+#include <plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h>
 //gg#include <plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h>
 //gg#include <plearn_learners/nearest_neighbors/GenericNearestNeighbors.h>
 

Deleted: branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc	2007-06-26 16:48:51 UTC (rev 7642)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.cc	2007-06-26 16:51:13 UTC (rev 7643)
@@ -1,797 +0,0 @@
-// -*- C++ -*-
-
-// BallTreeNearestNeighbors.cc
-//
-// Copyright (C) 2004 Pascal Lamblin & Marius Muja
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************      
- * $Id: BallTreeNearestNeighbors.cc 4911 2006-02-09 22:02:57Z lamblin $ 
- ******************************************************* */
-
-// Authors: Pascal Lamblin & Marius Muja
-
-/*! \file BallTreeNearestNeighbors.cc */
-
-#include "BallTreeNearestNeighbors.h"
-#include <plearn/base/lexical_cast.h>
-
-namespace PLearn {
-using namespace std;
-
-BallTreeNearestNeighbors::BallTreeNearestNeighbors() 
-    : rmin( 1 ),
-      train_method( "anchor" )
-{
-    num_neighbors = 1;
-    expdir = "";
-    stage = 0;
-    nstages = -1;
-    report_progress = 0;
-}
-
-BallTreeNearestNeighbors::BallTreeNearestNeighbors( const VMat& tr_set, const BinBallTree& b_tree )
-    : rmin( 1 ),
-      train_method( "anchor" )
-{
-    num_neighbors = 1;
-    expdir = "";
-    stage = 1;
-    nstages = 1;
-    report_progress = 0;
-
-    setTrainingSet( tr_set );
-    ball_tree = b_tree;
-}
-
-PLEARN_IMPLEMENT_OBJECT( BallTreeNearestNeighbors, 
-                         "Organizes hierarchically a set of points to perform efficient  KNN search", 
-                         "This learner builds a Ball Tree, a hierarchized structure\n"
-                         "allowing to perform efficient KNN search.\n"
-                         "Output is formatted as in GenericNearestNeighbors.\n"
-                         "The square distance to this point can be computed as the error.\n" );
-
-void BallTreeNearestNeighbors::declareOptions( OptionList& ol )
-{
-    // build options
-    declareOption( ol, "point_indices", &BallTreeNearestNeighbors::point_indices, 
-                   OptionBase::buildoption,
-                   "Indices of the points we will consider" );
-
-    declareOption( ol, "rmin", &BallTreeNearestNeighbors::rmin, OptionBase::buildoption,
-                   "Max number of points in a leaf node of the tree" );
-
-    declareOption( ol, "train_method", &BallTreeNearestNeighbors::train_method, 
-                   OptionBase::buildoption,
-                   "Method used to build the tree. Just one is supported:\n"
-                   "  \"anchor\" (middle-out building based on Anchor\'s hierarchy\n"
-        );
-
-    declareOption( ol, "anchor_set", &BallTreeNearestNeighbors::anchor_set, 
-                   OptionBase::learntoption, 
-                   "Set of anchors, hierarchizing the set of points" );
-
-    declareOption( ol, "pivot_indices", &BallTreeNearestNeighbors::pivot_indices, 
-                   OptionBase::learntoption, "Indices of the anchors' centers" );
-
-    // saved options
-    declareOption( ol, "train_set", &BallTreeNearestNeighbors::train_set, 
-                   OptionBase::buildoption,
-                   "Indexed set of points we will be working with" );
-
-    declareOption( ol, "nb_train_points", &BallTreeNearestNeighbors::nb_train_points, 
-                   OptionBase::learntoption, "Number of points in train_set" );
-
-    declareOption( ol, "nb_points", &BallTreeNearestNeighbors::nb_points, 
-                   OptionBase::learntoption, "Number of points in point_indices" );
-
-    declareOption( ol, "ball_tree", &BallTreeNearestNeighbors::ball_tree, 
-                   OptionBase::learntoption, "Built ball-tree" );
-
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions( ol );
-}
-
-void BallTreeNearestNeighbors::build_()
-{
-    if (train_set) {
-        // initialize nb_train_points
-        nb_train_points = train_set.length();
-        
-        // if point_indices isn't specified, we take all the points in train_set
-        if( !point_indices )
-            point_indices = TVec<int>( 0, nb_train_points-1, 1 );
-
-        // initialize nb_points
-        nb_points = point_indices.size();
-    }
-}
-
-
-void BallTreeNearestNeighbors::build()
-{
-    inherited::build();
-    build_();
-}
-
-
-void BallTreeNearestNeighbors::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField( ball_tree, copies );
-    deepCopyField( point_indices, copies );
-    deepCopyField( anchor_set, copies );
-    deepCopyField( pivot_indices, copies );
-}
-
-
-void BallTreeNearestNeighbors::forget()
-{
-    //! (Re-)initialize the PLearner in its fresh state (that state may depend on the 'seed' option)
-    //! And sets 'stage' back to 0   (this is the stage of a fresh learner!)
-
-    anchor_set.resize( 0 );
-    pivot_indices.resize( 0 );
-    ball_tree = new BinaryBallTree;
-    stage = 0;
-    build();
-}
-
-
-
-
-void BallTreeNearestNeighbors::train()
-{
-    // The role of the train method is to bring the learner up to stage==nstages,
-    // updating train_stats with training costs measured on-line in the process.
-
-    if( train_method == "anchor" )
-    {
-        anchorTrain();
-    }
-    else
-        PLERROR( "train_method \"%s\" not implemented", train_method.c_str() );
-}
-
-
-void BallTreeNearestNeighbors::anchorTrain()
-{
-    /*  nstages and stage conventions, for "anchor" train method:
-     *
-     *  nstages == -1
-     *    We will construct ball_tree recursively,
-     *    until, for all leaf, nb_points <= rmin,
-     *    no matter how many iterations it will take.
-     *
-     *  nstages == 0
-     *    We want the PLearner il its fresh, blank state.
-     *
-     *  nstages == 1
-     *    We want ball_tree to be a unique leaf node,
-     *    containing all the point indices, with no children.
-     *
-     *  nstages > 1
-     *    We want to build ball_tree recursively,
-     *    but limiting the levels of recursion.
-     *    This means we will decrement this number at each recursive call,
-     *    the recursion will stop when nstages == 1 or nb_points <= rmin.
-     *
-     *  stage == 0
-     *    The learner is it its fresh, blank state.
-     *
-     *  stage == 1
-     *    The learner has one anchor, that's all.
-     *
-     *  Other values of stage might be used one day or anoter...
-     */
-
-    if( stage == 0 && nstages !=0 )
-    {
-        // That means we weren't provided with any anchor nor node parameter,
-        // or that they were just bullsh!t
-
-        // So, we build a single anchor
-        pivot_indices.resize( 1 );
-        pivot_indices[ 0 ] = 0;
-        Vec pivot = train_set.getSubRow( 0, inputsize() );
-
-        distance_kernel->setDataForKernelMatrix( train_set );
-        distance_kernel->build();
-        Vec distances_from_pivot( nb_train_points );
-        distance_kernel->evaluate_all_i_x( pivot, distances_from_pivot );
-
-        anchor_set.resize( 1 );
-        Mat* p_anchor = &anchor_set[ 0 ];
-        p_anchor->resize( nb_points, 2 );
-        p_anchor->column( 0 ) << Vec( 0, nb_points-1, 1 );
-        p_anchor->column( 1 ) << distances_from_pivot;
-        sortRows( *p_anchor, TVec<int>( 1, 1 ), false );
-
-        // then, we build the corresponding tree
-        ball_tree = leafFromAnchor( 0 );
-
-        ++stage;
-    }
-
-    if( nstages == 0 )
-    {
-        // we want a fresh, blank learner
-        forget();
-    }
-    else if( nstages == 1 )
-    {
-        // We have an anchor, and we want a leaf node
-        ball_tree = leafFromAnchor( 0 );
-    }
-    else
-    {
-        // nstages to be used on children learners
-        int new_nstages = nstages<0 ? -1 : nstages-1;
-
-        // First create sqrt( R )-1 anchors, from the initial anchor_set
-        int nb_anchors = (int) sqrt( (float) nb_points ) + 1 ;
-        nb_anchors = min( nb_anchors, nb_points );
-
-        createAnchors( nb_anchors-1 ); // because we already have one
-
-        // Convert them into leaf nodes
-        TVec< BinBallTree > leaf_set = TVec<BinBallTree>( nb_anchors );
-        for ( int i=0 ; i<nb_anchors ; i++ )
-        {
-            leaf_set[ i ] = leafFromAnchor( i );
-        }
-
-        // Then, group them to form the ball_tree
-        // keep an index of the leaves
-        ball_tree = treeFromLeaves( leaf_set );
-
-        // Now, recurse...
-        for( int i=0 ; i<leaf_set.size() ; i++ )
-        {
-            int rec_nb_points = anchor_set[ i ].length();
-
-            // if the leaf is too small, don't do anything
-            if( rec_nb_points > rmin )
-            {
-                // child learner
-                PP<BallTreeNearestNeighbors> p_rec_learner = new BallTreeNearestNeighbors();
-
-                // initializes child's nstages (see explanation above)
-                stringstream out;
-                out << new_nstages;
-                p_rec_learner->setOption( "nstages" , out.str() );
-
-                // keep the same training set: it give us all the point coordinates !
-                // but we don't want to call forget() after that
-                p_rec_learner->setTrainingSet( train_set, false );
-
-                // however, we only work on the points contained by current leaf
-                p_rec_learner->anchor_set.resize( 1 );
-                p_rec_learner->anchor_set[ 0 ].resize( rec_nb_points, 2 );
-                p_rec_learner->anchor_set[ 0 ] << anchor_set[ i ];
-
-                p_rec_learner->pivot_indices.resize( 1 );
-                p_rec_learner->pivot_indices[ 0 ] = pivot_indices[ i ];
-
-                p_rec_learner->point_indices.resize( rec_nb_points );
-                p_rec_learner->point_indices << 
-                    p_rec_learner->anchor_set[ 0 ].column( 0 );
-
-                p_rec_learner->stage = 1; 
-                // faudra peut-etre faire ?a plus subtilement
-
-                p_rec_learner->rmin = rmin;
-                p_rec_learner->train_method = train_method;
-                p_rec_learner->build();
-                p_rec_learner->train();
-
-                // once the child learner is trained, we can get the sub-tree,
-                // and link it correctly
-                BinBallTree subtree = p_rec_learner->getBallTree();
-                leaf_set[ i ]->pivot = subtree->pivot;
-                leaf_set[ i ]->radius = subtree->radius;
-                leaf_set[ i ]->point_set.resize( subtree->point_set.size() );
-                leaf_set[ i ]->point_set << subtree->point_set;
-                leaf_set[ i ]->setFirstChild( subtree->getFirstChild() );
-                leaf_set[ i ]->setSecondChild( subtree->getSecondChild() );
-
-            }
-        }
-    }
-}
-
-
-void BallTreeNearestNeighbors::createAnchors( int nb_anchors )
-{
-    // This method creates nb_anchors new anchors, and adds them to anchor_set
-
-    // Make room
-    int anchor_set_size = anchor_set.size();
-    anchor_set.resize( anchor_set_size, nb_anchors );
-
-    for( int i=0 ; i<nb_anchors ; i++ )
-    {
-        Mat new_anchor = Mat( 1, 2 );
-        int new_pivot_index;
-
-        // Search for the largest ball.
-        // pivot of the new anchor will be the point of this ball
-        // that is the furthest from the pivot.
-        int largest_index = 0;
-        real largest_radius = 0;
-        for( int j=0 ; j<anchor_set_size ; j++ )
-        {
-            // points are sorted in decreasing order of distance, 
-            // so anchor_set[ j ]( 0, 1 ) is the furthest point from 
-            // pivot_indices[ j ]
-            real current_radius = anchor_set[ j ]( 0, 1 );
-            if( current_radius > largest_radius )
-            {
-                largest_radius = current_radius;
-                largest_index = j;
-            }
-        }
-
-        Mat* p_largest_anchor = &anchor_set[ largest_index ];
-        new_pivot_index = (int) (*p_largest_anchor)( 0, 0 );
-
-        // assign the point to its new anchor
-        new_anchor( 0, 0 ) = new_pivot_index;
-        new_anchor( 0, 1 ) = 0;
-        Vec new_pivot = train_set.getSubRow( new_pivot_index, inputsize() );
-
-        int largest_anchor_length = p_largest_anchor->length();
-
-        // Verify that largest_anchor owns at least 2 points
-        if( largest_anchor_length <= 1 )
-        {
-            PLERROR("In BallTreeNearestNeighbors::createAnchors, more anchors asked than points");
-        }
-
-        // delete this point from its original anchor
-        *p_largest_anchor = p_largest_anchor->
-            subMatRows( 1, largest_anchor_length-1 );
-
-        // now, try to steal points from all the existing anchors
-        for( int j=0 ; j<anchor_set_size ; j++ )
-        {
-            Mat* p_anchor = &anchor_set[ j ];
-            int nb_points = p_anchor->length();
-            int pivot_index = pivot_indices[ j ];
-            Vec pivot = train_set.getSubRow( pivot_index, inputsize() );
-            real pivot_pow_dist = powdistance( new_pivot, pivot, 2 );
-
-            // loop on the anchor's points
-            for( int k=0 ; k<nb_points ; k++ )
-            {
-                int point_index = (int) (*p_anchor)( k, 0 );
-                real point_pow_dist = (*p_anchor)( k, 1 );
-
-                // if this inequality is verified,
-                // then we're sure that all the points closer to the pivot 
-                // belong to the pivot, and we don't need to check
-                if( 4*point_pow_dist < pivot_pow_dist )
-                {
-                    break;
-                }
-
-                Vec point = train_set.getSubRow( point_index, inputsize() );
-                real new_pow_dist = powdistance( new_pivot, point, 2 );
-
-                // if the point is closer to the new pivot, then steal it
-                if( new_pow_dist < point_pow_dist )
-                {
-                    Vec new_row( 2 );
-                    new_row[ 0 ] = point_index;
-                    new_row[ 1 ] = new_pow_dist;
-                    new_anchor.appendRow( new_row );
-
-                    *p_anchor = removeRow( *p_anchor, k );
-                    // bleaah, this is ugly !
-                    --k;
-                    --nb_points;
-                }
-            }
-        }
-
-        // sort the points by decreasing distance
-        sortRows( new_anchor, TVec<int>( 1, 1 ), false );
-
-        // append the new anchor to the anchor_set (and same for pivot)
-        anchor_set.append( new_anchor );
-        pivot_indices.append( new_pivot_index );
-        ++anchor_set_size;
-    }
-}
-
-BinBallTree BallTreeNearestNeighbors::leafFromAnchor( int anchor_index )
-{
-    BinBallTree leaf = new BinaryBallTree();
-
-    int pivot_index = pivot_indices[ anchor_index ];
-    leaf->pivot = train_set.getSubRow( pivot_index, inputsize() );
-
-    leaf->radius = anchor_set[ anchor_index ]( 0, 1 );
-
-    int nb_leaf_points = anchor_set[ anchor_index ].length();
-    leaf->point_set.resize( nb_leaf_points );
-    leaf->point_set << anchor_set[ anchor_index ].column( 0 );
-
-    return leaf;
-}
-
-
-BinBallTree BallTreeNearestNeighbors::treeFromLeaves( const TVec<BinBallTree>& leaves )
-{
-    int nb_nodes = leaves.size();
-    TVec<BinBallTree> nodes = TVec<BinBallTree>( nb_nodes );
-    nodes << leaves;
-
-    // if there is no leaf
-    if( nb_nodes < 1 )
-    {
-        PLERROR( "In BallTreeNearestNeighbors::treeFromLeaves(): no leaf existing" );
-    }
-
-    while( nb_nodes > 1 )
-    {
-        int min_i = 0;
-        int min_j = 0;
-        Vec min_center;
-        real min_radius = -1;
-
-        // we get the most "compatible" pair of nodes :
-        // the ball containing them both is the smallest
-        for( int i=0 ; i<nb_nodes ; i++ )
-        {
-            Vec center_i = nodes[ i ]->pivot;
-            real radius_i = nodes[ i ]->radius;
-
-            // to scan all pairs only once, and avoid i==j
-            for( int j=0 ; j<i ; j++ )
-            {
-                Vec center_j = nodes[ j ]->pivot;
-                real radius_j = nodes[ j ]->radius;
-
-                Vec t_center;
-                real t_radius;
-                smallestContainer( center_i, radius_i, center_j, radius_j, 
-                                   t_center, t_radius );
-
-                if( t_radius < min_radius || min_radius < 0 )
-                {
-                    min_i = i;
-                    min_j = j ;
-                    min_radius = t_radius;
-                    min_center = t_center;
-                }
-            }
-        }
-
-#ifdef DEBUG_CHECK_NAN
-        if (min_center.hasMissing())
-            PLERROR("In BallTreeNearestNeighbors::treeFromLeaves: min_center is NaN");
-#endif
-        
-        // Group these two nodes into a parent_node.
-        // TODO: something more sensible for the radius and center...
-        BinBallTree parent_node = new BinaryBallTree();
-        parent_node->pivot = min_center;
-        parent_node->radius = min_radius;
-        parent_node->setFirstChild( nodes[ min_i ] );
-        parent_node->setSecondChild( nodes[ min_j ] );
-
-        nodes[ min_j ] = parent_node;
-        nodes.remove( min_i );
-
-        --nb_nodes;
-    }
-
-    // then, we have only one anchor
-    BinBallTree root = nodes[ 0 ];
-    return root;
-}
-
-
-BinBallTree BallTreeNearestNeighbors::getBallTree()
-{
-    return ball_tree;
-}
-
-
-void BallTreeNearestNeighbors::computeOutputAndCosts(
-    const Vec& input, const Vec& target, Vec& output, Vec& costs ) const
-{
-    int nout = outputsize();
-    output.resize( nout );
-    costs.resize( num_neighbors );
-
-    // we launch a k-nearest-neighbors query on the root node (ball_tree)
-    priority_queue< pair<real,int> > q;
-    FindBallKNN( q, input, num_neighbors );
-
-    // dequeue the found nearest neighbors, beginning by the farthest away
-    int n_found = q.size();
-    TVec<int> neighbors( n_found );
-    for( int i=n_found-1 ; i>=0 ; i-- )
-    {
-        const pair<real,int>& cur_top = q.top();
-        costs[i] = cur_top.first;
-        neighbors[i] = cur_top.second;
-        q.pop();
-    }
-
-    // fill costs with missing values
-    for( int i= n_found ; i<num_neighbors ; i++ )
-        costs[i] = MISSING_VALUE;
-
-    constructOutputVector( neighbors, output );
-}
-
-void BallTreeNearestNeighbors::computeOutput(
-    const Vec& input, Vec& output ) const
-{
-    // Compute the output from the input.
-    // int nout = outputsize();
-    // output.resize(nout);
-
-    int nout = outputsize();
-    output.resize( nout );
-
-    // we launch a k-nearest-neighbors query on the root node (ball_tree)
-    priority_queue< pair<real,int> > q;
-    FindBallKNN( q, input, num_neighbors );
-
-    // dequeue the found nearest neighbors, beginning by the farthest away
-    int n_found = q.size();
-    TVec<int> neighbors( n_found );
-    for( int i=n_found-1 ; i>=0 ; i-- )
-    {
-        const pair<real,int>& cur_top = q.top();
-        neighbors[i] = cur_top.second;
-        q.pop();
-    }
-
-    constructOutputVector( neighbors, output );
-
-}
-
-
-void BallTreeNearestNeighbors::computeCostsFromOutputs(
-    const Vec& input, const Vec& output, const Vec& target, Vec& costs ) const
-{
-    // Compute the costs from *already* computed output.
-    costs.resize( num_neighbors );
-
-    int inputsize = train_set->inputsize();
-    int targetsize = train_set->targetsize();
-    int weightsize = train_set->weightsize();
-
-    Mat out( num_neighbors, inputsize );
-
-    if( copy_input )
-    {
-        for( int i=0 ; i<num_neighbors ; i++ )
-            out( i ) << output.subVec( i*outputsize(), inputsize );
-    }
-    else if( copy_index )
-    {
-        int offset = 0;
-
-        if( copy_target )
-            offset += targetsize;
-
-        if( copy_weight )
-            offset += weightsize;
-
-        for( int i=0 ; i<num_neighbors ; i++ )
-            out( i ) << train_set( (int) output[ i*outputsize() + offset ] );
-    }
-    else
-    {
-        PLERROR( "computeCostsFromOutput:\n"
-                 "neither indices nor coordinates of output computed\n" );
-    }
-
-    for( int i=0 ; i<num_neighbors ; i++ )
-        costs[ i ] = powdistance( input, out( i ) );
-}
-
-TVec<string> BallTreeNearestNeighbors::getTestCostNames() const
-{
-    return TVec<string>( num_neighbors, "squared_distance" );
-}
-
-TVec<string> BallTreeNearestNeighbors::getTrainCostNames() const
-{
-    return TVec<string>();
-}
-
-bool BallTreeNearestNeighbors::intersect(
-    const Vec& center1, const real& powrad1,
-    const Vec& center2, const real& powrad2 )
-{
-    real radius1 = sqrt( powrad1 );
-    real radius2 = sqrt( powrad2 );
-
-    real pow_dist = powdistance( center1, center2, 2 );
-    real rad_sum = radius1 + radius2;
-    bool result = ( pow_dist <= ( rad_sum * rad_sum ) );
-    return result;
-}
-
-bool BallTreeNearestNeighbors::contain(
-    const Vec& center1, const real& powrad1,
-    const Vec& center2, const real& powrad2 )
-{
-    real radius1 = sqrt( powrad1 );
-    real radius2 = sqrt( powrad2 );
-    real rad_dif = radius1 - radius2;
-
-    if( rad_dif >= 0 )
-    {
-        real pow_dist = powdistance( center1, center2, 2 );
-        bool result = ( pow_dist <= ( rad_dif * rad_dif ) );
-        return result;
-    }
-    else
-    {
-        return false;
-    }
-}
-
-void BallTreeNearestNeighbors::smallestContainer(
-    const Vec& center1, const real& powrad1,
-    const Vec& center2, const real& powrad2,
-    Vec& t_center, real& t_powrad )
-{
-    if( center1 == center2 )
-    {
-        t_center = center1;
-        t_powrad = max( powrad1, powrad2 );
-    }
-    else if( contain( center1, powrad1, center2, powrad2 ) )
-    {
-        t_center = center1;
-        t_powrad = powrad1;
-    }
-    else if( contain( center2, powrad2, center1, powrad1 ) )
-    {
-        t_center = center2;
-        t_powrad = powrad2;
-    }
-    else
-    {
-        real radius1 = sqrt( powrad1 );
-        real radius2 = sqrt( powrad2 );
-        real center_dist = dist( center1, center2, 2 ) ;
-        real coef = ( radius1 - radius2 ) / center_dist ;
-        t_center = real(0.5) * ( ( 1 + coef ) * center1  +  ( 1 - coef ) * center2 ) ;
-        real t_radius = real(0.5) * ( center_dist + radius1 + radius2 ) ;
-        t_powrad = t_radius * t_radius;
-    }
-
-#ifdef DEBUG_CHECK_NAN
-    if (t_center.hasMissing())
-        PLERROR("In BallTreeNearestNeighbors::smallestContainer: t_center is NaN.");
-#endif
-}
-
-
-
-void BallTreeNearestNeighbors::BallKNN(
-     priority_queue< pair<real,int> >& q, BinBallTree node,
-     const Vec& t, real& d2_sofar, real d2_pivot, const int k ) const
-{
-    real d_minp = max( sqrt(d2_pivot) - node->radius, 0.0 );
-#ifdef DEBUG_CHECK_NAN
-    if (isnan(d_minp))
-        PLERROR("BallTreeNearestNeighbors::BallKNN: d_minp is NaN");
-#endif
-
-    if (d_minp*d_minp > d2_sofar)
-    {
-        // no chance of finding anything closer around this node
-        return;
-    }
-    else if (node->point_set.size()!=0) // node is leaf
-    {
-        int n_points = node->point_set.size();
-        for( int i=0 ; i<n_points ; i++ )
-        {
-            int j = node->point_set[i];
-            real dist;
-            // last point is pivot, and we already now the distance
-            if( i==n_points-1 )
-            {
-                dist = d2_pivot;
-            }
-            else
-            {
-                Vec x = train_set.getSubRow(j, inputsize());
-                dist = powdistance(x, t, 2);
-            }
-            if( dist < d2_sofar )
-            {
-                q.push( make_pair(dist, j) );
-                int n_found = q.size();
-                if( n_found > k )
-                    q.pop();
-                if( n_found >= k )
-                    d2_sofar = q.top().first;
-            }
-        }
-    }
-    else if (!node->isEmpty()) // node is not leaf
-    {
-        BinBallTree node1 = node->getFirstChild();
-        BinBallTree node2 = node->getSecondChild();
-
-        real d2_pivot1 = powdistance(t, node1->pivot, 2);
-        real d2_pivot2 = powdistance(t, node2->pivot, 2);
-
-        if( d2_pivot1 > d2_pivot2 ) // node1 is closer to t
-        {
-            pl_swap(node1, node2);
-            pl_swap(d2_pivot1, d2_pivot2);
-        }
-
-        BallKNN(q, node1, t, d2_sofar, d2_pivot1, k);
-        BallKNN(q, node2, t, d2_sofar, d2_pivot2, k); 
-    }
-}
-
-
-void BallTreeNearestNeighbors::FindBallKNN(
-    priority_queue< pair<real,int> >& q, const Vec& point, const int k ) const
-{
-    real d2_sofar;
-    pl_isnumber("+inf", &d2_sofar);
-    real d2_pivot = powdistance(point, ball_tree->pivot, 2);
-//    real d_minp = 0;
-    BallKNN(q, ball_tree, point, d2_sofar, d2_pivot, k);
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h	2007-06-26 16:48:51 UTC (rev 7642)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/BallTreeNearestNeighbors.h	2007-06-26 16:51:13 UTC (rev 7643)
@@ -1,227 +0,0 @@
-// -*- C++ -*-
-
-// BallTreeNearestNeighbors.h
-//
-// Copyright (C) 2004 Pascal Lamblin & Marius Muja
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************      
- * $Id: BallTreeNearestNeighbors.h 4900 2006-02-05 08:42:31Z lamblin $ 
- ******************************************************* */
-
-// Authors: Pascal Lamblin & Marius Muja
-
-/*! \file BallTreeNearestNeighbors.h */
-
-
-#ifndef BallTreeNearestNeighbors_INC
-#define BallTreeNearestNeighbors_INC
-
-#include <queue>
-
-#include <plearn_learners/generic/PLearner.h>
-#include <plearn_learners/nearest_neighbors/GenericNearestNeighbors.h>
-#include <plearn/vmat/SelectRowsVMatrix.h>
-#include <plearn/ker/DistanceKernel.h>
-
-#include "BinaryBallTree.h"
-
-namespace PLearn {
-using namespace std;
-
-class BallTreeNearestNeighbors;
-typedef PP< BallTreeNearestNeighbors > BallTreeNN;
-
-class BallTreeNearestNeighbors: public GenericNearestNeighbors
-{
-
-private:
-
-    typedef GenericNearestNeighbors inherited;
-
-protected:
-
-    // *********************
-    // * protected options *
-    // *********************
-
-    BinBallTree ball_tree;
-    int nb_train_points;
-    int nb_points;
-
-public:
-
-    // ************************
-    // * public build options *
-    // ************************
-
-    TVec<int> point_indices;
-    int rmin;
-    string train_method;
-    TVec<Mat> anchor_set;
-    TVec<int> pivot_indices;
-
-    // ****************
-    // * Constructors *
-    // ****************
-
-    //! Default constructor.
-    BallTreeNearestNeighbors();
-
-    //! Constructor from a TrainSet and a BinBallTree.
-    BallTreeNearestNeighbors( const VMat& tr_set, const BinBallTree& b_tree );
-
-    // ********************
-    // * PLearner methods *
-    // ********************
-
-private: 
-
-    //! This does the actual building. 
-    // (Please implement in .cc)
-    void build_();
-    void anchorTrain();
-
-protected: 
-  
-    //! Declares this class' options.
-    // (Please implement in .cc)
-    static void declareOptions(OptionList& ol);
-
-public:
-
-    // ************************
-    // **** Static methods ****
-    // ************************
-    // Maybe should I put this somewhere else...
-
-    // Returns true if the balls defined by (center1, radius1) and
-    // (center2, radius2) have a common part
-
-    static bool intersect( const Vec& center1, const real& radius1,
-                           const Vec& center2, const real& radius2 );
-
-    // Returns true if the first ball contains the second one
-    static bool contain( const Vec& center1, const real& radius1,
-                         const Vec& center2, const real& radius2 );
-
-    // Returns the smallest ball containing two balls
-    static void smallestContainer( const Vec& center1, const real& radius1,
-                                   const Vec& center2, const real& radius2,
-                                   Vec& t_center, real& t_radius);
-
-    virtual void BallKNN( priority_queue< pair<real,int> >& q,
-                          BinBallTree node, const Vec& t,
-                          real& d_sofar, real d_minp, const int k ) const;
-
-    virtual void FindBallKNN( priority_queue< pair<real,int> >& q,
-                              const Vec& point, int k ) const;
-
-
-    // ************************
-    // **** Object methods ****
-    // ************************
-
-    //! Simply calls inherited::build() then build_().
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy.
-    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
-
-    // Declares other standard object methods.
-    PLEARN_DECLARE_OBJECT(BallTreeNearestNeighbors);
-
-
-    // **************************
-    // **** PLearner methods ****
-    // **************************
-
-    //! (Re-)initializes the PLearner in its fresh state (that state may
-    //! depend on the 'seed' option) And sets 'stage' back to 0 (this is
-    //! the stage of a fresh learner!).
-    virtual void forget();
-
-
-    //! The role of the train method is to bring the learner up to
-    //! stage==nstages, updating the train_stats collector with training
-    //! costs measured on-line in the process.
-    virtual void train();
-
-    void createAnchors( int nb_anchors );
-
-
-    BinBallTree leafFromAnchor( int anchor_index );
-
-    BinBallTree treeFromLeaves( const TVec<BinBallTree>& leaves );
-
-    BinBallTree getBallTree();
-
-
-    //! Computes the output and costs from the input (more effectively)
-    virtual void computeOutputAndCosts( const Vec& input, const Vec& target,
-                                        Vec& output, Vec& costs ) const;
-    //! Computes the output from the input.
-    virtual void computeOutput(const Vec& input, Vec& output) const;
-
-    //! Computes the costs from already computed output. 
-    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output, 
-                                         const Vec& target, Vec& costs) const;
-
-
-    //! Returns the names of the costs computed by computeCostsFromOutpus
-    //! (and thus the test method).
-    virtual TVec<string> getTestCostNames() const;
-
-    //! Returns the names of the objective costs that the train method
-    //computes and ! for which it updates the VecStatsCollector
-    //train_stats.  (PLEASE IMPLEMENT IN .cc)
-    virtual TVec<string> getTrainCostNames() const;
-
-};
-
-// Declares a few other classes and functions related to this class.
-DECLARE_OBJECT_PTR(BallTreeNearestNeighbors);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc	2007-06-26 16:48:51 UTC (rev 7642)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.cc	2007-06-26 16:51:13 UTC (rev 7643)
@@ -1,153 +0,0 @@
-// -*- C++ -*-
-
-// BinaryBallTree.cc
-//
-// Copyright (C) 2004 Pascal Lamblin 
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************      
- * $Id: BinaryBallTree.cc 3994 2005-08-25 13:35:03Z chapados $ 
- ******************************************************* */
-
-// Authors: Pascal Lamblin
-
-/*! \file BinaryBallTree.cc */
-
-
-#include "BinaryBallTree.h"
-
-namespace PLearn {
-using namespace std;
-
-BinaryBallTree::BinaryBallTree() 
-    : pivot( Vec() ),
-      radius( 0 )
-{}
-
-PLEARN_IMPLEMENT_OBJECT( BinaryBallTree,
-                         "Binary Tree, containing a point, a radius, and a set of points", 
-                         "Each node of the tree contains the parameters of a ball :\n"
-                         "a point and a radius.\n"
-                         "Each leaf node contains a list of indices of points,\n"
-                         "each non-leaf node has two children nodes.");
-
-void BinaryBallTree::declareOptions( OptionList& ol )
-{
-    declareOption( ol, "pivot", &BinaryBallTree::pivot, OptionBase::buildoption,
-                   "Center of the ball" );
-
-    declareOption(ol, "radius", &BinaryBallTree::radius, OptionBase::buildoption,
-                  "Radius of the ball" );
-
-    declareOption(ol, "point_set", &BinaryBallTree::point_set, OptionBase::buildoption,
-                  "List of indices of the points owned by this node (leaf only)" );
-
-    declareOption(ol, "child1", &BinaryBallTree::child1, OptionBase::tuningoption,
-                  "Pointer to first child (non-leaf only)" );
-
-    declareOption(ol, "child2", &BinaryBallTree::child2, OptionBase::tuningoption,
-                  "Pointer to second child (non-leaf only)" );
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-void BinaryBallTree::build_()
-{
-    if( child1 )
-    { child1->parent = this; }
-
-    if( child2 )
-    { child2->parent = this; }
-}
-
-void BinaryBallTree::build()
-{
-    inherited::build();
-    build_();
-}
-
-void BinaryBallTree::setFirstChild( const BinBallTree& first_child )
-{
-    this->child1 = first_child;
-    if( first_child )
-    {
-        first_child->parent = this;
-    }
-}
-
-void BinaryBallTree::setSecondChild( const BinBallTree& second_child )
-{
-    this->child2 = second_child;
-    if( second_child )
-    {
-        second_child->parent = this;
-    }
-}
-
-BinBallTree BinaryBallTree::getFirstChild()
-{
-    return this->child1;
-}
-
-BinBallTree BinaryBallTree::getSecondChild()
-{
-    return this->child2;
-}
-
-BinaryBallTree* BinaryBallTree::getParent()
-{
-    return this->parent;
-}
-
-void BinaryBallTree::makeDeepCopyFromShallowCopy(map<const void*, void*>& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField( child1, copies );
-    deepCopyField( child2, copies );
-    deepCopyField( pivot, copies );
-    deepCopyField( point_set, copies );
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h	2007-06-26 16:48:51 UTC (rev 7642)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/BinaryBallTree.h	2007-06-26 16:51:13 UTC (rev 7643)
@@ -1,147 +0,0 @@
-// -*- C++ -*-
-
-// BinaryBallTree.h
-//
-// Copyright (C) 2004 Pascal Lamblin 
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************      
- * $Id: BinaryBallTree.h 3994 2005-08-25 13:35:03Z chapados $ 
- ******************************************************* */
-
-// Authors: Pascal Lamblin
-
-/*! \file BinaryBallTree.h */
-
-
-#ifndef BinaryBallTree_INC
-#define BinaryBallTree_INC
-
-#include <plearn/base/Object.h>
-
-namespace PLearn {
-using namespace std;
-
-class BinaryBallTree;
-typedef PP<BinaryBallTree> BinBallTree;
-
-class BinaryBallTree: public Object
-{
-
-private:
-  
-    typedef Object inherited;
-
-protected:
-    // *********************
-    // * protected options *
-    // *********************
-
-    BinaryBallTree* parent;
-    BinBallTree child1;
-    BinBallTree child2;
-
-public:
-
-    // ************************
-    // * public build options *
-    // ************************
-
-    Vec pivot;
-    real radius;
-    TVec<int> point_set;
-
-    // ****************
-    // * Constructors *
-    // ****************
-
-    //! Default constructor.
-    BinaryBallTree();
-
-
-    // ******************
-    // * Object methods *
-    // ******************
-
-private: 
-    //! This does the actual building. 
-    void build_();
-
-protected: 
-    //! Declares this class' options.
-    static void declareOptions(OptionList& ol);
-
-public:
-    // Declares other standard object methods.
-    PLEARN_DECLARE_OBJECT(BinaryBallTree);
-
-    // simply calls inherited::build() then build_() 
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    virtual void makeDeepCopyFromShallowCopy(map<const void*, void*>& copies);
-
-    virtual void setFirstChild( const BinBallTree& first_child );
-
-    virtual void setSecondChild( const BinBallTree& second_child );
-
-    virtual BinBallTree getFirstChild();
-
-    virtual BinBallTree getSecondChild();
-
-    virtual BinaryBallTree* getParent();
-
-    bool isEmpty() const
-    {
-        bool result = !pivot && !child1 && !child2 ;
-        return result;
-    }
-
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(BinaryBallTree);
-  
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Tue Jun 26 18:52:26 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 26 Jun 2007 18:52:26 +0200
Subject: [Plearn-commits] r7644 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706261652.l5QGqPN5024422@sheep.berlios.de>

Author: nouiz
Date: 2007-06-26 18:52:16 +0200 (Tue, 26 Jun 2007)
New Revision: 7644

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
Log:
Modified the header to take the good version of the deleted file previously


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h	2007-06-26 16:51:13 UTC (rev 7643)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/NeighborhoodConditionalMean.h	2007-06-26 16:52:16 UTC (rev 7644)
@@ -48,9 +48,9 @@
 #include <plearn/io/fileutils.h>              //!<  For isfile()
 #include <plearn/math/random.h>               //!<  For the seed stuff.
 #include <plearn/vmat/ExplicitSplitter.h>     //!<  For the splitter stuff.
+#include <plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h>
 #include <plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h>
 #include <plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h>
-#include <plearn_learners/second_iteration/BallTreeNearestNeighbors.h>
 #include <plearn_learners/second_iteration/Experimentation.h>
 
 namespace PLearn {

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-06-26 16:51:13 UTC (rev 7643)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-06-26 16:52:16 UTC (rev 7644)
@@ -50,11 +50,11 @@
 #include <plearn/io/fileutils.h>              //!<  For isfile()
 #include <plearn/math/random.h>               //!<  For the seed stuff.
 #include <plearn/vmat/ExplicitSplitter.h>     //!<  For the splitter stuff.
+#include <plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h>
+#include <plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h>
 #include <plearn_learners/second_iteration/CovariancePreservationImputationVMatrix.h>
 #include <plearn_learners/second_iteration/NeighborhoodImputationVMatrix.h>
-#include <plearn_learners/second_iteration/BallTreeNearestNeighbors.h>
 #include <plearn_learners/second_iteration/WeightedDistance.h>
-#include <plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h>
 #include <plearn_learners/second_iteration/Experimentation.h>
 
 namespace PLearn {



From nouiz at mail.berlios.de  Tue Jun 26 18:52:49 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 26 Jun 2007 18:52:49 +0200
Subject: [Plearn-commits] r7645 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200706261652.l5QGqnlU025228@sheep.berlios.de>

Author: nouiz
Date: 2007-06-26 18:52:49 +0200 (Tue, 26 Jun 2007)
New Revision: 7645

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc
Log:
Better error message


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc	2007-06-26 16:52:16 UTC (rev 7644)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/Experimentation.cc	2007-06-26 16:52:49 UTC (rev 7645)
@@ -175,7 +175,7 @@
             {
                 if (missing_indicator_field_names[fields_col] == main_names[main_col]) break;
             }
-            if (main_col >= main_width) PLERROR("In Experimentation: no field with this name in input dataset: %", (missing_indicator_field_names[fields_col]).c_str());
+            if (main_col >= main_width) PLERROR("In Experimentation::experimentSetUp() no field with this name in input dataset: %", (missing_indicator_field_names[fields_col]).c_str());
         }
         main_fields_selected_col = 0;
         for (main_col = 0; main_col < main_width; main_col++)
@@ -312,7 +312,7 @@
         pb->update( main_row );
     }
     delete pb;
-    PLERROR("In Experimentation: we are done here");
+    PLERROR("In Experimentation::experimentSetUp() we are done here");
 }
 
 void Experimentation::createHeaderFile()



From tihocan at mail.berlios.de  Tue Jun 26 19:26:17 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Jun 2007 19:26:17 +0200
Subject: [Plearn-commits] r7646 - trunk/plearn/math
Message-ID: <200706261726.l5QHQHh7012205@sheep.berlios.de>

Author: tihocan
Date: 2007-06-26 19:26:16 +0200 (Tue, 26 Jun 2007)
New Revision: 7646

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
- Fixed compilation crash in minabs(TVec)
- Minor optimizations in minabs functions
- Removed handling of a case (0 length vector) in minabs(TVec), that should have triggered a PLERROR in debug mode anyway


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2007-06-26 16:52:49 UTC (rev 7645)
+++ trunk/plearn/math/TMat_maths_impl.h	2007-06-26 17:26:16 UTC (rev 7646)
@@ -858,18 +858,15 @@
     if(vec.length()==0)
         PLERROR("IN T minabs(const TVec<T>& vec) vec has zero length");
 #endif
-    if (vec.size() == 0)
-        return std::numeric_limits<T>::max();
+    int n = vec.length();
+    PLASSERT( n >= 1 );
     T* v = vec.data();
     T minval = fabs(v[0]);
-    for(int i=1; i<vec.length(); i++)
+    for(int i=1; i<n; i++)
     {
         T a=fabs(v[i]);
         if(a<minval)
-        {
-            index = i;
             minval = a;
-        }
     }
 
     return minval;
@@ -890,7 +887,7 @@
     T* pv = vec.data();
     T minval = fabs(*pv++);
     argmin = 0;
-    for (int i=1; i<vec.length(); i++,pv++)
+    for (int i=1; i<n; i++,pv++)
     {
         T a = fabs(*pv);
         if (a<minval)



From lamblin at mail.berlios.de  Tue Jun 26 21:08:13 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 26 Jun 2007 21:08:13 +0200
Subject: [Plearn-commits] r7647 - trunk/plearn/vmat
Message-ID: <200706261908.l5QJ8DgI020094@sheep.berlios.de>

Author: lamblin
Date: 2007-06-26 21:08:13 +0200 (Tue, 26 Jun 2007)
New Revision: 7647

Modified:
   trunk/plearn/vmat/VMat_basic_stats.cc
Log:
Compile fix


Modified: trunk/plearn/vmat/VMat_basic_stats.cc
===================================================================
--- trunk/plearn/vmat/VMat_basic_stats.cc	2007-06-26 17:26:16 UTC (rev 7646)
+++ trunk/plearn/vmat/VMat_basic_stats.cc	2007-06-26 19:08:13 UTC (rev 7647)
@@ -508,7 +508,7 @@
         externalProductScaleAcc(covarmat, input, input, weight);
         weightsum += weight;
     }
-    covarmat *= 1./weightsum;
+    covarmat *= real(1./weightsum);
     addToDiagonal(covarmat, epsilon);
 }
 



From lamblin at mail.berlios.de  Tue Jun 26 21:09:27 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 26 Jun 2007 21:09:27 +0200
Subject: [Plearn-commits] r7648 - trunk/plearn_learners/generic
Message-ID: <200706261909.l5QJ9RMF020176@sheep.berlios.de>

Author: lamblin
Date: 2007-06-26 21:09:27 +0200 (Tue, 26 Jun 2007)
New Revision: 7648

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Declare two more distant methods


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-06-26 19:08:13 UTC (rev 7647)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-06-26 19:09:27 UTC (rev 7648)
@@ -298,6 +298,17 @@
          RetDoc ("Current experiment directory")));
 
     declareMethod(
+        rmm, "setTrainStatsCollector", &PLearner::setTrainStatsCollector,
+        (BodyDoc("Sets the statistics collector whose update() method will be called\n"
+                 "during training.\n."),
+         ArgDoc ("statscol", "The tatistics collector to set")));
+
+    declareMethod(
+        rmm, "getTrainStatsCollector", &PLearner::getTrainStatsCollector,
+        (BodyDoc("Returns the statistics collector that was used during training.\n"),
+         RetDoc ("Current training statistics collector")));
+
+    declareMethod(
         rmm, "forget", &PLearner::forget,
         (BodyDoc("(Re-)initializes the PLearner in its fresh state (that state may depend\n"
                  "on the 'seed' option) and sets 'stage' back to 0 (this is the stage of\n"



From lamblin at mail.berlios.de  Tue Jun 26 21:10:58 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 26 Jun 2007 21:10:58 +0200
Subject: [Plearn-commits] r7649 - trunk/plearn_learners/online
Message-ID: <200706261910.l5QJAwNL020411@sheep.berlios.de>

Author: lamblin
Date: 2007-06-26 21:10:58 +0200 (Tue, 26 Jun 2007)
New Revision: 7649

Modified:
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/ModuleLearner.h
Log:
train() now computes (approximate) training error.


Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2007-06-26 19:09:27 UTC (rev 7648)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2007-06-26 19:10:58 UTC (rev 7649)
@@ -250,6 +250,7 @@
     deepCopyField(network,            copies);
     deepCopyField(null_pointers,      copies);
     deepCopyField(all_ones,           copies);
+    deepCopyField(tmp_costs,          copies);
 }
 
 ////////////////
@@ -306,6 +307,9 @@
     Vec weights;
     PP<ProgressBar> pb = NULL;
 
+    // clear statistics of previous calls
+    train_stats->forget();
+
     int stage_init = stage;
     if (report_progress)
         pb = new ProgressBar( "Training " + classname(), nstages - stage);
@@ -327,13 +331,16 @@
                 "only %d stages (instead of nstages = %d, which is not a "
                 "multiple of batch_size = %d", stage, nstages, batch_size);
     OnlineLearningModule::during_training=false;
+
+    // finalize statistics for this call
+    train_stats->finalize();
 }
 
 //////////////////
 // trainingStep //
 //////////////////
 void ModuleLearner::trainingStep(const Mat& inputs, const Mat& targets,
-                      const Vec& weights)
+                                 const Vec& weights)
 {
     // Fill in the provided batch values (only if they are actually used by the
     // network).
@@ -347,6 +354,24 @@
     // Forward propagation.
     network->fprop(null_pointers);
 
+    // Copy the costs into a single matrix.
+    // First compute total size.
+    int cost_size = 0;
+    for (int i = 0; i < store_costs.length(); i++)
+        cost_size += store_costs[i]->getData().width();
+    // Then resize the 'tmp_costs' matrix and fill it.
+    tmp_costs.resize(inputs.length(), cost_size);
+    int cost_idx = 0;
+    for (int i = 0; i < store_costs.length(); i++) {
+        const Mat& cost_i = store_costs[i]->getData();
+        PLASSERT( cost_i.length() == tmp_costs.length() );
+        tmp_costs.subMatColumns(cost_idx, cost_i.width()) << cost_i;
+        cost_idx += cost_i.width();
+    }
+
+    // Then update the training statistics.
+    train_stats->update(tmp_costs);
+
     // Initialize cost gradients to 1.
     // Note that we may not need to re-do it at every iteration, but this is so
     // cheap it should not impact performance.
@@ -484,8 +509,7 @@
 ///////////////////////
 TVec<string> ModuleLearner::getTrainCostNames() const
 {
-    // No training cost is currently computed.
-    return TVec<string>();
+    return cost_ports;
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/ModuleLearner.h
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.h	2007-06-26 19:09:27 UTC (rev 7648)
+++ trunk/plearn_learners/online/ModuleLearner.h	2007-06-26 19:10:58 UTC (rev 7649)
@@ -187,7 +187,11 @@
 
     //! Matrix that contains only ones (used to fill weights at test time).
     mutable Mat all_ones;
-    
+
+    //! Matrix that stores a copy of the costs 
+    //! (used to update the cost statistics).
+    mutable Mat tmp_costs;
+
     //#####  Private Member Functions  ########################################
 
     //! This does the actual building.



From lamblin at mail.berlios.de  Tue Jun 26 21:47:11 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 26 Jun 2007 21:47:11 +0200
Subject: [Plearn-commits] r7650 - in
	trunk/plearn_learners/online/test/ModuleLearner/.pytest:
	PL_ModuleLearner_Basic/expected_results/expdir-tester
	PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester
Message-ID: <200706261947.l5QJlBVo022860@sheep.berlios.de>

Author: lamblin
Date: 2007-06-26 21:47:11 +0200 (Tue, 26 Jun 2007)
New Revision: 7650

Modified:
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Basic/expected_results/expdir-tester/train_cost_names.txt
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/train_cost_names.txt
Log:
Updates test results


Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Basic/expected_results/expdir-tester/train_cost_names.txt
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Basic/expected_results/expdir-tester/train_cost_names.txt	2007-06-26 19:10:58 UTC (rev 7649)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Basic/expected_results/expdir-tester/train_cost_names.txt	2007-06-26 19:47:11 UTC (rev 7650)
@@ -1 +1 @@
-
+mse

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/train_cost_names.txt
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/train_cost_names.txt	2007-06-26 19:10:58 UTC (rev 7649)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/train_cost_names.txt	2007-06-26 19:47:11 UTC (rev 7650)
@@ -1 +1,3 @@
-
+NLL
+class_error
+mse



From lamblin at mail.berlios.de  Tue Jun 26 22:25:42 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 26 Jun 2007 22:25:42 +0200
Subject: [Plearn-commits] r7651 - trunk/plearn/ker
Message-ID: <200706262025.l5QKPgqI025782@sheep.berlios.de>

Author: lamblin
Date: 2007-06-26 22:25:41 +0200 (Tue, 26 Jun 2007)
New Revision: 7651

Modified:
   trunk/plearn/ker/Kernel.cc
Log:
Fix compilation warning


Modified: trunk/plearn/ker/Kernel.cc
===================================================================
--- trunk/plearn/ker/Kernel.cc	2007-06-26 19:47:11 UTC (rev 7650)
+++ trunk/plearn/ker/Kernel.cc	2007-06-26 20:25:41 UTC (rev 7651)
@@ -488,7 +488,7 @@
 
     // Finalize computation
     KD -= KDminus;
-    KD /= 2.*epsilon;
+    KD /= real(2.*epsilon);
 
     This->changeOption(kernel_param, cur_param_str);
     This->build();                           //!< Temporarily necessary



From lamblin at mail.berlios.de  Tue Jun 26 22:26:07 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 26 Jun 2007 22:26:07 +0200
Subject: [Plearn-commits] r7652 - trunk/plearn_learners/online
Message-ID: <200706262026.l5QKQ72A025817@sheep.berlios.de>

Author: lamblin
Date: 2007-06-26 22:26:07 +0200 (Tue, 26 Jun 2007)
New Revision: 7652

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
Log:
Fix compilation warning and code formatting


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-26 20:25:41 UTC (rev 7651)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-26 20:26:07 UTC (rev 7652)
@@ -54,6 +54,7 @@
     inherited( the_learning_rate ),
     min_quad_coeff( 0. ),
     share_quad_coeff( false ),
+    size_quad_coeff( 0 ),
     sigma_is_up_to_date( false )
 {
 }
@@ -62,6 +63,7 @@
     inherited( the_learning_rate ),
     min_quad_coeff( 0. ),
     share_quad_coeff( false ),
+    size_quad_coeff( 0 ),
     quad_coeff( the_size, 1. ), // or 1./M_SQRT2 ?
     quad_coeff_pos_stats( the_size ),
     quad_coeff_neg_stats( the_size ),
@@ -77,12 +79,13 @@
     bias_neg_stats.resize( the_size );
 }
 
-RBMGaussianLayer::RBMGaussianLayer( int the_size, real the_learning_rate, bool do_share_quad_coeff ) :
+RBMGaussianLayer::RBMGaussianLayer( int the_size, real the_learning_rate,
+                                    bool do_share_quad_coeff ) :
     inherited( the_learning_rate ),
     min_quad_coeff( 0. ),
-    sigma_is_up_to_date( false ),
     quad_coeff_pos_stats( the_size ),
-    quad_coeff_neg_stats( the_size )
+    quad_coeff_neg_stats( the_size ),
+    sigma_is_up_to_date( false )
 {
     size = the_size;
     activation.resize( the_size );
@@ -197,7 +200,7 @@
     if( sigma_is_up_to_date )
         return;
 
-    // sigma = 1 / (sqrt(2) * quad_coeff[i])    
+    // sigma = 1 / (sqrt(2) * quad_coeff[i])
     if(share_quad_coeff)
         sigma[0] = 1 / (M_SQRT2 * quad_coeff[0]);
     else
@@ -337,8 +340,8 @@
     declareOption(ol, "share_quad_coeff", &RBMGaussianLayer::share_quad_coeff,
                   OptionBase::buildoption,
                   "Should all the units share the same quadratic coefficients?\n"
-		  "Suitable to avoid unstability (overfitting)  in cases where\n"
-		  "all the units have the same 'meaning'  (pixels of an image)");
+                  "Suitable to avoid unstability (overfitting)  in cases where\n"
+                  "all the units have the same 'meaning'  (pixels of an image)");
 
 
     // Now call the parent class' declareOptions
@@ -428,17 +431,17 @@
     if( momentum == 0. )
     {
         if(share_quad_coeff)
-	{
-	    real update=0;
+        {
+            real update=0;
             for( int i=0 ; i<size ; i++ )
             {
                 update += pos_factor * aps[i] + neg_factor * ans[i];
             }
-	    a[0] += update/(real)size;
+            a[0] += update/(real)size;
             if( a[0] < min_quad_coeff )
                 a[0] = min_quad_coeff;
         }
-	else
+        else
             for( int i=0 ; i<size ; i++ )
             {
                 a[i] += pos_factor * aps[i] + neg_factor * ans[i];
@@ -449,20 +452,20 @@
     else
     {
         if(share_quad_coeff)
-	{
+        {
             quad_coeff_inc.resize( 1 );
             real* ainc = quad_coeff_inc.data();
             for( int i=0 ; i<size ; i++ )
             {
                 ainc[0] = momentum*ainc[0] + pos_factor*aps[i] + neg_factor*ans[i];
-		ainc[0] /= (real)size;
+                ainc[0] /= (real)size;
                 a[0] += ainc[0];
             }
             if( a[0] < min_quad_coeff )
                 a[0] = min_quad_coeff;
         }
-	else
-	{
+        else
+        {
             quad_coeff_inc.resize( size );
             real* ainc = quad_coeff_inc.data();
             for( int i=0 ; i<size ; i++ )
@@ -494,52 +497,51 @@
     if( momentum == 0. )
     {
         if (share_quad_coeff)
-	{
-	    real update=0;
+        {
+            real update=0;
             for( int i=0 ; i<size ; i++ )
             {
                 update += two_lr * a[0] * (nv[i]*nv[i] - pv[i]*pv[i]);
             }
-	    a[0] += update/(real)size;
+            a[0] += update/(real)size;
             if( a[0] < min_quad_coeff )
                 a[0] = min_quad_coeff;
         }
-	else
+        else
             for( int i=0 ; i<size ; i++ )
             {
                 a[i] += two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
                 if( a[i] < min_quad_coeff )
                     a[i] = min_quad_coeff;
-            }	
+            }
     }
     else
     {
-        
         real* ainc = quad_coeff_inc.data();
         if(share_quad_coeff)
         {
-	   quad_coeff_inc.resize( 1 );
-           for( int i=0 ; i<size ; i++ )
-           {
+            quad_coeff_inc.resize( 1 );
+            for( int i=0 ; i<size ; i++ )
+            {
                 ainc[0] = momentum*ainc[0]
                     + two_lr * a[0] * (nv[i]*nv[i] - pv[i]*pv[i]);
-		ainc[0] /= (real)size;
+                ainc[0] /= (real)size;
                 a[0] += ainc[0];
-           }
-           if( a[0] < min_quad_coeff )
-               a[0] = min_quad_coeff;
+            }
+            if( a[0] < min_quad_coeff )
+                a[0] = min_quad_coeff;
         }
         else
-	{
-	   quad_coeff_inc.resize( size );
-           for( int i=0 ; i<size ; i++ )
-           {
+        {
+            quad_coeff_inc.resize( size );
+            for( int i=0 ; i<size ; i++ )
+            {
                 ainc[i] = momentum*ainc[i]
                     + two_lr * a[i] * (nv[i]*nv[i] - pv[i]*pv[i]);
                 a[i] += ainc[i];
                 if( a[i] < min_quad_coeff )
                     a[i] = min_quad_coeff;
-           }
+            }
         }
     }
 



From lamblin at mail.berlios.de  Tue Jun 26 22:57:31 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 26 Jun 2007 22:57:31 +0200
Subject: [Plearn-commits] r7653 - trunk/commands
Message-ID: <200706262057.l5QKvVFm028566@sheep.berlios.de>

Author: lamblin
Date: 2007-06-26 22:57:30 +0200 (Tue, 26 Jun 2007)
New Revision: 7653

Modified:
   trunk/commands/plearn_lapack_inc.h
   trunk/commands/plearn_python_inc.h
Log:
Do not include the headers if we cannot compile them
(compilation with -noblas or -nopython)


Modified: trunk/commands/plearn_lapack_inc.h
===================================================================
--- trunk/commands/plearn_lapack_inc.h	2007-06-26 20:26:07 UTC (rev 7652)
+++ trunk/commands/plearn_lapack_inc.h	2007-06-26 20:57:30 UTC (rev 7653)
@@ -48,6 +48,10 @@
 #ifndef plearn_lapack_inc_INC
 #define plearn_lapack_inc_INC
 
+// Do not include if USE_BLAS_SPECIALISATIONS is undefined
+// (compiling with -noblas)
+#ifdef USE_BLAS_SPECIALISATIONS
+
 // Regressors
 #include <plearn_learners/regressors/LinearRegressor.h>
 #include <plearn_learners/regressors/PLS.h>
@@ -79,8 +83,10 @@
 #include <plearn_learners/online/RBMMatrixConnectionNatGrad.h>
 
 
-#endif
+#endif // USE_BLAS_SPECIALISATIONS
 
+#endif // plearn_lapack_inc_INC
+
 
 /*
   Local Variables:

Modified: trunk/commands/plearn_python_inc.h
===================================================================
--- trunk/commands/plearn_python_inc.h	2007-06-26 20:26:07 UTC (rev 7652)
+++ trunk/commands/plearn_python_inc.h	2007-06-26 20:57:30 UTC (rev 7653)
@@ -47,6 +47,9 @@
 #ifndef plearn_python_inc_INC
 #define plearn_python_inc_INC
 
+// Do not include if PL_PYTHON_VERSION is undefined (compiling with -nopython)
+#ifdef PL_PYTHON_VERSION
+
 /******************************************************
  * Python includes must come FIRST, as per Python doc *
  ******************************************************/
@@ -57,8 +60,10 @@
 #include <plearn/vmat/DictionaryVMatrix.h>
 #include <commands/PLearnCommands/VMatDictionaryCommand.h>
 
-#endif
+#endif // PL_PYTHON_VERSION
 
+#endif // plearn_python_inc_INC
+
 
 /*
   Local Variables:



From tihocan at mail.berlios.de  Wed Jun 27 18:22:18 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 27 Jun 2007 18:22:18 +0200
Subject: [Plearn-commits] r7654 - trunk/plearn/python
Message-ID: <200706271622.l5RGMIds002517@sheep.berlios.de>

Author: tihocan
Date: 2007-06-27 18:22:18 +0200 (Wed, 27 Jun 2007)
New Revision: 7654

Modified:
   trunk/plearn/python/PythonCodeSnippet.cc
Log:
Fixed compilation under Windows

Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-06-26 20:57:30 UTC (rev 7653)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-06-27 16:22:18 UTC (rev 7654)
@@ -48,7 +48,11 @@
 #include <plearn/io/fileutils.h>
 #include <plearn/base/tostring.h>
 
+#ifdef WIN32
+#include <plearn/base/stringutils.h>   // For 'search_replace'.
+#endif
 
+
 namespace PLearn {
 using namespace std;
 



From tihocan at mail.berlios.de  Wed Jun 27 18:50:51 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 27 Jun 2007 18:50:51 +0200
Subject: [Plearn-commits] r7655 - trunk/plearn/base
Message-ID: <200706271650.l5RGopqn004604@sheep.berlios.de>

Author: tihocan
Date: 2007-06-27 18:50:41 +0200 (Wed, 27 Jun 2007)
New Revision: 7655

Modified:
   trunk/plearn/base/HelpSystem.cc
Log:
Added errors in buggy code so that someone fixes it

Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2007-06-27 16:22:18 UTC (rev 7654)
+++ trunk/plearn/base/HelpSystem.cc	2007-06-27 16:50:41 UTC (rev 7655)
@@ -178,8 +178,13 @@
             + helpOnFunctionHTML(it->first, it->second)
             + "</tr>\n";
 
-    if(index == 0)
+        PLERROR("In HelpSystem::helpFunctionsHTML - Error thrown because of "
+            "commented code (see code for details)");
+    /* TODO The code below was commented because the 'index' variable was not
+     * defined. It should probably be fixed and uncommented.
+     if(index == 0)
         s+= "<tr><td>No Remote-Callable Functions.</td></tr>\n";
+     */
            
     s+= "</table></div>\n";
 
@@ -793,9 +798,14 @@
             + helpOnMethodHTML(definingclass[*it], it->first, it->second)
             + "</tr>\n";
 
+    PLERROR("In HelpSystem::helpMethodsHTML - Error thrown because of "
+            "commented code (see code for details)");
+    /* TODO The code below was commented because the 'index' variable was not
+     * defined. It should probably be fixed and uncommented.
     if(index == 0)
         s+= "<tr><td>This class does not define any remote-callable methods.</td></tr>\n";
            
+        */
     s+= "</table></div>\n";
 
     return s;



From nouiz at mail.berlios.de  Wed Jun 27 21:51:52 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Jun 2007 21:51:52 +0200
Subject: [Plearn-commits] r7656 - trunk/python_modules/plearn/learners
Message-ID: <200706271951.l5RJpqhl030355@sheep.berlios.de>

Author: nouiz
Date: 2007-06-27 21:51:51 +0200 (Wed, 27 Jun 2007)
New Revision: 7656

Modified:
   trunk/python_modules/plearn/learners/discr_power_SVM.py
Log:
Bugfix


Modified: trunk/python_modules/plearn/learners/discr_power_SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-27 16:50:41 UTC (rev 7655)
+++ trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-27 19:51:51 UTC (rev 7656)
@@ -236,7 +236,7 @@
 
       def compute_accuracy(self, samples_target_list):
 	  best_expert = eval( 'self.'+self.best_parameters[0]+'_expert' )
-	  best_parameters = best_expert.best_parameters[1:]
+	  best_parameters = best_expert.best_parameters
 	  param = best_expert.get_svm_parameter( best_parameters )
 	  if len(samples_target_list) == 1: # cross-validation
 	     accuracy = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)



From nouiz at mail.berlios.de  Wed Jun 27 22:25:45 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Jun 2007 22:25:45 +0200
Subject: [Plearn-commits] r7657 - trunk/python_modules/plearn/learners
Message-ID: <200706272025.l5RKPjwQ000037@sheep.berlios.de>

Author: nouiz
Date: 2007-06-27 22:25:45 +0200 (Wed, 27 Jun 2007)
New Revision: 7657

Modified:
   trunk/python_modules/plearn/learners/discr_power_SVM.py
Log:
Added more verification that raise error if they fail


Modified: trunk/python_modules/plearn/learners/discr_power_SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-27 19:51:51 UTC (rev 7656)
+++ trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-27 20:25:45 UTC (rev 7657)
@@ -352,6 +352,10 @@
 	     for samples_target in samples_target_list:
 	         if len(samples_target) != 2:
 	            raise TypeError, "ERROR: samples_target_list has an element with length "+str(len(samples_target))+" (instead of 2)"
+                 if len(samples_target[0]) == 0 or len(samples_target[1]) == 0:
+                    raise ValueError, "ERROR: samples_target_list has an element that has an element with an empty length"
+                 if len(samples_target[0]) != len(samples_target[1]):
+                    raise ValueError, "ERROR: samples_target_list has an element that has an elements with different len. Len are: " + len(samples_target[0])+" and " + len(samples_target[1])
 	  if len(samples_target_list) == 1:
 	     print "cross-validation"
 	     return



From nouiz at mail.berlios.de  Wed Jun 27 22:58:52 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Jun 2007 22:58:52 +0200
Subject: [Plearn-commits] r7658 - trunk/python_modules/plearn/learners
Message-ID: <200706272058.l5RKwqPh002299@sheep.berlios.de>

Author: nouiz
Date: 2007-06-27 22:58:52 +0200 (Wed, 27 Jun 2007)
New Revision: 7658

Modified:
   trunk/python_modules/plearn/learners/discr_power_SVM.py
Log:
Added some verification

Modified: trunk/python_modules/plearn/learners/discr_power_SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-27 20:25:45 UTC (rev 7657)
+++ trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-06-27 20:58:52 UTC (rev 7658)
@@ -235,6 +235,8 @@
 	  #self.clerror  = 100 - self.accuracy
 
       def compute_accuracy(self, samples_target_list):
+	  check_samples_target_list(samples_target_list)
+
 	  best_expert = eval( 'self.'+self.best_parameters[0]+'_expert' )
 	  best_parameters = best_expert.best_parameters
 	  param = best_expert.get_svm_parameter( best_parameters )



From lamblin at mail.berlios.de  Thu Jun 28 00:57:24 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 28 Jun 2007 00:57:24 +0200
Subject: [Plearn-commits] r7659 - trunk/plearn/python
Message-ID: <200706272257.l5RMvOVr014306@sheep.berlios.de>

Author: lamblin
Date: 2007-06-28 00:57:22 +0200 (Thu, 28 Jun 2007)
New Revision: 7659

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Fix conversion problem: we are iterating over rows (not elements!) of the TMat.


Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-06-27 20:58:52 UTC (rev 7658)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-06-27 22:57:22 UTC (rev 7659)
@@ -789,8 +789,8 @@
 template <class T>
 PyObject* ConvertToPyObject<TMat<T> >::newPyObject(const TMat<T>& data)
 {
-    PyObject* newlist = PyList_New(data.size());
-    for (int i=0, n=data.size() ; i<n ; ++i) 
+    PyObject* newlist = PyList_New(data.length());
+    for (int i=0, n=data.length() ; i<n ; ++i) 
     {
         // Since PyList_SET_ITEM steals the reference to the item being set,
         // one does not need to Py_XDECREF the inserted string as was required



From tihocan at mail.berlios.de  Thu Jun 28 21:02:15 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Jun 2007 21:02:15 +0200
Subject: [Plearn-commits] r7660 -
	trunk/plearn_learners/online/test/ModuleLearner
Message-ID: <200706281902.l5SJ2FhR015660@sheep.berlios.de>

Author: tihocan
Date: 2007-06-28 21:02:15 +0200 (Thu, 28 Jun 2007)
New Revision: 7660

Added:
   trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.pyplearn
Removed:
   trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.plearn
Modified:
   trunk/plearn_learners/online/test/ModuleLearner/pytest.config
Log:
Converted test 'PL_ModuleLearner_TwoRBMs' to a .pyplearn script for more flexibility

Deleted: trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.plearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.plearn	2007-06-27 22:57:22 UTC (rev 7659)
+++ trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.plearn	2007-06-28 19:02:15 UTC (rev 7660)
@@ -1,180 +0,0 @@
-# Network architecture consisting in two RBMs stacked together, followed by an
-# affine transformation for classification.
-
-$DEFINE{expdir}{expdir-tester}
-$EVALUATE{expdir}
-
-$DEFINE{rbm_size_1}{10}
-$DEFINE{rbm_size_2}{100}
-
-$DEFINE{inputsize}{5}
-$DEFINE{targetsize}{1}
-$DEFINE{n_classes}{2}
-
-PTester( 
-    expdir = "${expdir}"
-    learner =
-        ModuleLearner(
-            module =
-                NetworkModule(
-
-                    modules = [
-                        RBMModule(
-                            name = "rbm_1"
-                            visible_layer =
-                                RBMBinomialLayer(
-                                    size = ${inputsize}
-                                    use_fast_approximations = 0
-                                )
-                            hidden_layer =
-                                RBMBinomialLayer(
-                                    size = ${rbm_size_1}
-                                    use_fast_approximations = 0
-                                )
-                            connection =
-                                RBMMatrixConnection(
-                                    down_size = ${inputsize}
-                                    up_size = ${rbm_size_1}
-                                )
-                            grad_learning_rate = ${grad_lr}
-                            cd_learning_rate = ${cd_lr}
-                        )
-
-                        RBMModule(
-                            name = "rbm_2"
-                            visible_layer =
-                                RBMBinomialLayer(
-                                    size = ${rbm_size_1}
-                                    use_fast_approximations = 0
-                                )
-                            hidden_layer =
-                                RBMBinomialLayer(
-                                    use_fast_approximations = 0
-                                    size = ${rbm_size_2}
-                                )
-                            connection =
-                                RBMMatrixConnection(
-                                    down_size = ${rbm_size_1}
-                                    up_size = ${rbm_size_2}
-                                )
-                            grad_learning_rate = ${grad_lr}
-                            cd_learning_rate = ${cd_lr}
-                        )
-
-                        GradNNetLayerModule( 
-                            name = "affine_net"
-                            input_size = ${rbm_size_2}
-                            output_size = ${n_classes}
-                            start_learning_rate = ${grad_lr}
-                        )
-
-                        SoftmaxModule(
-                            name = "softmax"
-                            input_size = ${n_classes}
-                            output_size = ${n_classes}
-                        )
-
-                        NLLCostModule( 
-                            name = "nll"
-                            input_size = ${n_classes}
-                        )
-
-                        ClassErrorCostModule(
-                            name = "class_error"
-                            input_size = ${n_classes}
-                        )
-
-                        ArgmaxModule(
-                            name = "argmax_class"
-                        )
-
-                        SquaredErrorCostModule(
-                            name = "mse"
-                            input_size = 1 # The predicted class index.
-                        )
-
-                    ]
-
-                    connections = [
-                        NetworkConnection( 
-                            source = "rbm_1.hidden.state"
-                            destination = "rbm_2.visible"
-                        )
-
-                        NetworkConnection( 
-                            source = "rbm_2.hidden.state"
-                            destination = "affine_net.input"
-                        )
-                       
-                        NetworkConnection( 
-                            source = "affine_net.output"
-                            destination = "softmax.input"
-                        )
-
-                        NetworkConnection( 
-                            source = "softmax.output"
-                            destination = "nll.prediction"
-                        )
-
-                        NetworkConnection( 
-                            source = "softmax.output"
-                            destination = "class_error.prediction"
-                            propagate_gradient = 0
-                        )
-
-                        NetworkConnection( 
-                            source = "affine_net.output"
-                            destination = "argmax_class.input"
-                            propagate_gradient = 0
-                        )
-
-                        NetworkConnection( 
-                            source = "argmax_class.output"
-                            destination = "mse.prediction"
-                            propagate_gradient = 0
-                        )
-                    ]
-
-                    ports = [
-                        "input":"rbm_1.visible"
-                        "":"nll.target"
-                        "":"class_error.target"
-                        "":"mse.target"
-                        "output":"affine_net.output"
-                        "NLL":"nll.cost"
-                        "class_error":"class_error.cost"
-                        "mse":"mse.cost"
-                    ]
-                )
-
-                cost_ports = [ "NLL" "class_error" "mse" ]
-                target_ports = [ "nll.target" "class_error.target" "mse.target" ]
-
-                batch_size = ${batch_size}
-                nstages = ${nstages}
-        )
-
-    report_stats = 1
-    save_initial_tester = 0
-    save_learners = 1
-    save_stat_collectors = 0
-    save_test_outputs = 0
-    splitter =
-        ExplicitSplitter( 
-            splitsets = 1  2  [ 
-                *4374-> AutoVMatrix(
-                    filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y_binary_class.vmat"
-                    inputsize = ${inputsize}
-                    targetsize = ${targetsize}
-                    weightsize = 0
-                )
-                *4374
-            ]
-        );
-    statnames = [
-        "E[test1.E[NLL]]"
-        "E[test1.E[class_error]]"
-        "E[test1.E[mse]]"
-    ]
-
-)

Added: trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.pyplearn	2007-06-27 22:57:22 UTC (rev 7659)
+++ trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.pyplearn	2007-06-28 19:02:15 UTC (rev 7660)
@@ -0,0 +1,171 @@
+# Network architecture consisting in two RBMs stacked together, followed by an
+# affine transformation for classification.
+
+expdir = 'expdir-tester'
+
+rbm_size_1 = 10
+rbm_size_2 = 100
+
+inputsize = 5
+targetsize = 1
+n_classes = 2
+
+grad_lr=1e-2
+cd_lr=1e-3
+batch_size=11
+nstages=1001
+
+
+def rbm(name, size_1, size_2, grad_lr, cd_lr, type):
+    if type == 'binomial':
+        visible_layer = pl.RBMBinomialLayer(
+                size = size_1,
+                use_fast_approximations = 0)
+        hidden_layer = pl.RBMBinomialLayer(
+                size = size_2,
+                use_fast_approximations = 0)
+    elif type == 'gaussian':
+        visible_layer = pl.RBMGaussianLayer(
+                size = size_1,
+                use_fast_approximations = 0)
+        hidden_layer = pl.RBMGaussianLayer(
+                size = size_2,
+                use_fast_approximations = 0)
+    
+    return pl.RBMModule(
+            name = name,
+            visible_layer = visible_layer,
+            hidden_layer = hidden_layer,
+            connection = pl.RBMMatrixConnection(
+                down_size = size_1,
+                up_size = size_2),
+            grad_learning_rate = grad_lr,
+            cd_learning_rate = cd_lr)
+
+data = pl.AutoVMatrix(
+        filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y_binary_class.vmat",
+        inputsize = inputsize,
+        targetsize = targetsize,
+        weightsize = 0)
+
+tester = pl.PTester( 
+    expdir = expdir,
+    learner =
+        pl.ModuleLearner(
+            module =
+                pl.NetworkModule(
+                    modules = [ \
+                        rbm('rbm_1', inputsize, rbm_size_1, grad_lr, cd_lr, 'binomial'),
+                        rbm('rbm_2', rbm_size_1, rbm_size_2, grad_lr, cd_lr, 'binomial'),
+
+                        pl.GradNNetLayerModule( 
+                            name = "affine_net",
+                            input_size = rbm_size_2,
+                            output_size = n_classes,
+                            start_learning_rate = grad_lr
+                        ),
+
+                        pl.SoftmaxModule(
+                            name = "softmax",
+                            input_size = n_classes,
+                            output_size = n_classes
+                        ),
+
+                        pl.NLLCostModule( 
+                            name = "nll",
+                            input_size = n_classes
+                        ),
+
+                        pl.ClassErrorCostModule(
+                            name = "class_error",
+                            input_size = n_classes
+                        ),
+
+                        pl.ArgmaxModule(
+                            name = "argmax_class"
+                        ),
+
+                        pl.SquaredErrorCostModule(
+                            name = "mse",
+                            input_size = 1 # The predicted class index.
+                        )
+                    ],
+
+                    connections = [
+                        pl.NetworkConnection( 
+                            source = "rbm_1.hidden.state",
+                            destination = "rbm_2.visible"
+                        ),
+
+                        pl.NetworkConnection( 
+                            source = "rbm_2.hidden.state",
+                            destination = "affine_net.input"
+                        ),
+                       
+                        pl.NetworkConnection( 
+                            source = "affine_net.output",
+                            destination = "softmax.input"
+                        ),
+
+                        pl.NetworkConnection( 
+                            source = "softmax.output",
+                            destination = "nll.prediction"
+                        ),
+
+                        pl.NetworkConnection( 
+                            source = "softmax.output",
+                            destination = "class_error.prediction",
+                            propagate_gradient = 0
+                        ),
+
+                        pl.NetworkConnection( 
+                            source = "affine_net.output",
+                            destination = "argmax_class.input",
+                            propagate_gradient = 0
+                        ),
+
+                        pl.NetworkConnection( 
+                            source = "argmax_class.output",
+                            destination = "mse.prediction",
+                            propagate_gradient = 0
+                        )
+                    ],
+
+                    ports = [
+                        ("input","rbm_1.visible"),
+                        ("","nll.target"),
+                        ("","class_error.target"),
+                        ("","mse.target"),
+                        ("output","affine_net.output"),
+                        ("NLL","nll.cost"),
+                        ("class_error","class_error.cost"),
+                        ("mse","mse.cost")
+                    ]
+                ),
+
+                cost_ports = [ "NLL", "class_error", "mse" ],
+                target_ports = [ "nll.target", "class_error.target", "mse.target" ],
+
+                batch_size = batch_size,
+                nstages = nstages
+        ),
+
+    report_stats = 1,
+    save_initial_tester = 0,
+    save_learners = 1,
+    save_stat_collectors = 0,
+    save_test_outputs = 0,
+    splitter =
+        pl.ExplicitSplitter( 
+            splitsets = TMat(1, 2, [ data, data ])
+        ),
+    statnames = [ \
+        "E[test1.E[NLL]]",
+        "E[test1.E[class_error]]",
+        "E[test1.E[mse]]"
+    ]
+
+)
+
+def main():
+    return tester

Modified: trunk/plearn_learners/online/test/ModuleLearner/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/pytest.config	2007-06-27 22:57:22 UTC (rev 7659)
+++ trunk/plearn_learners/online/test/ModuleLearner/pytest.config	2007-06-28 19:02:15 UTC (rev 7660)
@@ -115,8 +115,8 @@
         name = "plearn_tests",
         compiler = "pymake"
         ),
-    arguments = "PL_ModuleLearner_TwoRBMs.plearn grad_lr=1e-2 cd_lr=1e-3 batch_size=11 nstages=1001",
-    resources = [ "PL_ModuleLearner_TwoRBMs.plearn" ],
+    arguments = "PL_ModuleLearner_TwoRBMs.pyplearn",
+    resources = [ "PL_ModuleLearner_TwoRBMs.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
     disabled = False



From tihocan at mail.berlios.de  Thu Jun 28 21:31:05 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Jun 2007 21:31:05 +0200
Subject: [Plearn-commits] r7661 - in
	trunk/plearn_learners/online/test/ModuleLearner: .
	.pytest/PL_ModuleLearner_TwoRBMs/expected_results
	.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0
	.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2
	.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0
	.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata
	.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata
Message-ID: <200706281931.l5SJV5bk018300@sheep.berlios.de>

Author: tihocan
Date: 2007-06-28 21:31:03 +0200 (Thu, 28 Jun 2007)
New Revision: 7661

Added:
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/test_cost_names.txt
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/train_cost_names.txt
Modified:
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/RUN.log
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.pyplearn
Log:
Added a variant of the test PL_ModuleLearner_TwoRBMs where the visible layer is a Gaussian layer instead of a binomial layer

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/RUN.log	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/RUN.log	2007-06-28 19:31:03 UTC (rev 7661)
@@ -1,3 +1,6 @@
  WARNING: RBMMatrixConnection: cannot forget() without random_gen
  WARNING: RBMMatrixConnection: cannot forget() without random_gen
  WARNING: GradNNetLayerModule: cannot forget() without random_gen
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+ WARNING: RBMMatrixConnection: cannot forget() without random_gen
+ WARNING: GradNNetLayerModule: cannot forget() without random_gen

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-28 19:31:03 UTC (rev 7661)
@@ -80,6 +80,7 @@
 min_n_Gibbs_steps = 1 ;
 n_Gibbs_steps_per_generated_sample = 1 ;
 compute_log_likelihood = 0 ;
+minimize_log_likelihood = 0 ;
 Gibbs_step = 0 ;
 log_partition_function = 0 ;
 partition_function_is_stale = 1 ;
@@ -257,6 +258,7 @@
 min_n_Gibbs_steps = 1 ;
 n_Gibbs_steps_per_generated_sample = 1 ;
 compute_log_likelihood = 0 ;
+minimize_log_likelihood = 0 ;
 Gibbs_step = 0 ;
 log_partition_function = 0 ;
 partition_function_is_stale = 1 ;

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave	2007-06-28 19:31:03 UTC (rev 7661)
@@ -0,0 +1,403 @@
+*1 ->ModuleLearner(
+module = *2 ->NetworkModule(
+modules = 8 [ *3 ->RBMModule(
+visible_layer = *4 ->RBMGaussianLayer(
+min_quad_coeff = 0 ;
+quad_coeff = 5 [ 1.0408525953828216 1.03890035931453717 1.01010962868336795 1.02713877432309397 1.00593906555968826 ] ;
+share_quad_coeff = 0 ;
+size = 5 ;
+learning_rate = 0.00100000000000000002 ;
+momentum = 0 ;
+gibbs_ma_schedule = []
+;
+gibbs_ma_increment = 0.100000000000000006 ;
+gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+bias = 5 [ -0.0400498787760139099 -0.0449658259678535205 -0.0564253100833261509 -0.0305132626610919895 -0.0275159111271822028 ] ;
+input_size = 5 ;
+output_size = 5 ;
+name = "RBMGaussianLayer" ;
+use_fast_approximations = 0 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5 ->PRandom(
+seed = 1827 ;
+fixed_seed = 0  )
+ )
+;
+hidden_layer = *6 ->RBMBinomialLayer(
+size = 10 ;
+learning_rate = 0.00100000000000000002 ;
+momentum = 0 ;
+gibbs_ma_schedule = []
+;
+gibbs_ma_increment = 0.100000000000000006 ;
+gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+bias = 10 [ -0.000679801686262589784 -0.0011838975221978318 0.00754248353955261526 -0.00424115803739747669 -0.00151023456893898387 0.00161621670652929619 -0.00205204570891677658 -0.0014724833853869447 0.00159008176652559985 -0.00464945593498112663 ] ;
+input_size = 10 ;
+output_size = 10 ;
+name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+;
+connection = *7 ->RBMMatrixConnection(
+weights = 10  5  [ 
+-0.269997045368186084 	0.172285902750972358 	0.260973589274125617 	-0.286217474548966289 	-0.274471886575587298 	
+-0.0195157236992702481 	-0.174128722742926456 	-0.0521487023859824664 	0.0781508512007194484 	0.0814327257664252957 	
+0.247335763680624515 	0.278361454468297631 	-0.094202498288889519 	0.221490162166844667 	0.191463361139713989 	
+-0.140976955210450866 	-0.269577019891866165 	0.109683465708992581 	0.00169154134986601113 	-0.291726512169596275 	
+0.0488686513835587741 	0.022631508551110447 	0.0828411732209624052 	-0.304793471900027702 	-0.244656347873107055 	
+-0.0558133582879227069 	0.12398147682134783 	0.205688949276112282 	-0.0968288719544328352 	-0.276454164328514329 	
+0.0511178615823486907 	0.0541443469138282868 	-0.281168265106303938 	0.0756573910258672433 	-0.0537269160660981959 	
+0.0531084882377516956 	-0.249011225279938075 	0.0630869797850509423 	0.162493248821561631 	-0.276964073046844395 	
+0.0250962869004374839 	0.0904763011697778674 	0.142611569646583586 	-0.322396902434824273 	0.0351703974340924902 	
+-0.249991346660943858 	-0.0395017036945069511 	-0.206338275282021283 	-0.175830785082946722 	0.277034923898056784 	
+]
+;
+gibbs_ma_schedule = []
+;
+gibbs_ma_increment = 0.100000000000000006 ;
+gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+down_size = 5 ;
+up_size = 10 ;
+learning_rate = 0.00100000000000000002 ;
+momentum = 0 ;
+initialization_method = "uniform_sqrt" ;
+input_size = 5 ;
+output_size = 10 ;
+name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+;
+reconstruction_connection = *0 ;
+grad_learning_rate = 0.0100000000000000002 ;
+cd_learning_rate = 0.00100000000000000002 ;
+compute_contrastive_divergence = 0 ;
+standard_cd_grad = 1 ;
+standard_cd_bias_grad = 1 ;
+standard_cd_weights_grad = 1 ;
+n_Gibbs_steps_CD = 1 ;
+min_n_Gibbs_steps = 1 ;
+n_Gibbs_steps_per_generated_sample = 1 ;
+compute_log_likelihood = 0 ;
+minimize_log_likelihood = 0 ;
+Gibbs_step = 0 ;
+log_partition_function = 0 ;
+partition_function_is_stale = 1 ;
+input_size = -1 ;
+output_size = -1 ;
+name = "rbm_1" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+*8 ->RBMModule(
+visible_layer = *9 ->RBMGaussianLayer(
+min_quad_coeff = 0 ;
+quad_coeff = 10 [ 1.03736835214705025 1.05420459942646927 1.06102760255011708 1.0419841472073712 1.05442774302328957 1.05720797570505698 1.05314606216591389 1.03813695144617957 1.06172896566133912 1.05807767866850999 ] ;
+share_quad_coeff = 0 ;
+size = 10 ;
+learning_rate = 0.00100000000000000002 ;
+momentum = 0 ;
+gibbs_ma_schedule = []
+;
+gibbs_ma_increment = 0.100000000000000006 ;
+gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+bias = 10 [ -0.0310275003615368868 -0.0392131972615835864 -0.0283766144589966823 -0.0367870960989302789 -0.0515140657600649254 -0.0428136467889393343 -0.0269753719316090552 -0.0381739656230388053 -0.026637561803439621 -0.0236700222302586097 ] ;
+input_size = 10 ;
+output_size = 10 ;
+name = "RBMGaussianLayer" ;
+use_fast_approximations = 0 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+;
+hidden_layer = *10 ->RBMBinomialLayer(
+size = 100 ;
+learning_rate = 0.00100000000000000002 ;
+momentum = 0 ;
+gibbs_ma_schedule = []
+;
+gibbs_ma_increment = 0.100000000000000006 ;
+gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+bias = 100 [ -0.00353253196714415187 0.00105021011514272369 -0.00123981425195365705 0.000148524069484933544 -0.00182069295190169186 -0.000725092111953462982 0.00127075954286112519 0.00130690271360941503 0.00208726241074961303 -0.000444289267811945852 -0.000734885951036389731 -0.0010466284174252972 -0.0003307441793853783 0.000772987488476326598 0.00101617306644444932 -0.000676301296153658375 -0.000646062399632431404 0.0028779968372038855 0.00113808472886859149 -0.00133906886819176274 0.00320034120593920991 -0.000837753495153736264 0.000122519008195277678 -0.00165717964239016835 0.00404741434099294747 0.0043902806364162451 -0.00254918976809790894 -0.000419852924529751548 0.000207475737633038297 -0.00287067792535434249 -0.00112546272051902273 -0.0009198666626967076 -0.00269291681396742564 0.00128462292022009676 -0.000365287572261410894 -0.00145969407304802367 -0.0031788203227378015 -0.00114608020499133993 0.000449029468350347315 -0.000719793809590452225 -0.00205229263027559544!
  -0.00120675650753899501 0.000613612244685269041 0.00153431839321508832 -0.000365676693302229971 0.000678700038285790408 0.00200561577647605065 -0.0017764852713110053 -0.00329881334357158811 -2.50084185957622889e-05 -0.000361555493158581412 0.00329287154011595126 -0.000750971992712015581 -0.00134442609996333743 0.00136298626541056424 -0.00120274591463599982 1.98312362937778323e-05 6.37674647892915418e-06 -0.00124246613292821229 0.000903722600576549787 0.000880763043512510445 0.00122255628852947621 0.00176636414610012232 -0.00204554583995231621 0.00106449362519541097 -0.00173600372367498459 -0.00133674574506152926 0.000677638094168815804 0.00158417146199775157 0.000288071983736850809 0.00131294473703304714 0.00248626932367626086 -0.00259209983727004805 0.000614532373301187587 -0.0012947486456764436 -0.000711605935694047337 -0.00212758443151684446 -6.14131283728724481e-05 -0.00205049319607211126 0.000912299357397174451 0.0017406235231178231 0.000107118609198131556 -0.00546626!
 837283472038 0.00019796664494023923 0.00120925920299135101 -0.!
 00225438
208353684116 9.55193343922752271e-05 -1.3676766958950453e-05 -0.00338416762965698843 0.0012341180939135798 0.000910766363965857806 0.00107888272015214904 -0.00219135813923306043 0.00258347033499443804 0.0023738988870446549 -0.000193111224550719207 0.00174978845009484221 -0.00161608227941032519 -0.000595770140218897281 2.4678772673427895e-05 ] ;
+input_size = 100 ;
+output_size = 100 ;
+name = "RBMBinomialLayer" ;
+use_fast_approximations = 0 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+;
+connection = *11 ->RBMMatrixConnection(
+weights = 100  10  [ 
+-0.112968997720965206 	-0.107261622322238917 	0.0690843499648503695 	0.0246368644491140008 	-0.120456372285623212 	-0.066612146170254169 	-0.0906744272833491516 	0.0597997655297617461 	-0.105986898342330743 	-0.0411454473463422413 	
+-0.0374976346999811791 	0.0210859367774480881 	0.0709464150658829507 	0.0488643751654367214 	0.0343323691786596788 	0.0279475558666552604 	-0.084333392382157174 	-0.0194080793851280214 	-0.052358429434102037 	-0.0492076924286100892 	
+-0.0259011496309593664 	0.026215622730212447 	-0.0746893287828352404 	0.0130851606114712523 	-0.0420653977313660693 	-0.0717655779777460062 	-0.0518072564752484685 	0.0676300855209071017 	-0.0886449651710874836 	-0.0262112170569699862 	
+-0.00915200527651004528 	-0.0644439747139869262 	0.0514365614610269722 	0.0768722260848204442 	-0.0157775253241844403 	-0.00465242240018836663 	-0.0653460395456518101 	0.00367989894016004836 	-0.0249611973856444548 	-0.0400986040812955774 	
+-0.0270270149563998432 	-0.119229165379597851 	-0.0699416464556981143 	0.0334107016070862295 	-0.0160833734362593947 	-0.0653353282790603423 	-0.0946587899503815444 	0.0566728737912418831 	0.0474862885045988156 	-0.0756514896908607598 	
+-0.0851390082595065156 	0.0431691224195017778 	-0.00891028135812327489 	-0.0967791324840578393 	0.0685304240580123653 	0.0198254518260957587 	-0.079392564026046622 	-0.0978732410671857012 	0.0171628860939631173 	-0.0253132286859064533 	
+0.0196836486892928257 	0.0509494353831387789 	-0.0393004807807989104 	0.031201825876827842 	0.00461940004997505801 	0.0255922844035603378 	-0.0653964519191958943 	0.0531915905307929898 	-0.0594682690238555189 	-0.0363797595837481447 	
+-0.0757350321031600021 	-0.0433603280357398729 	0.0795755312206672949 	0.0547841007048637668 	0.00168063176117762758 	0.0495689859908219829 	0.00967641649701043993 	-0.102649758349485656 	0.070236676589944716 	0.0570849541620821674 	
+-0.0661440773605440069 	0.0637796737211135389 	0.0350691360738870223 	0.0437665316017624983 	-0.065319618977366975 	0.0377506455285399334 	-0.081347412529510027 	0.0594788886687925636 	0.0679125775057212722 	0.0849047037111063801 	
+-0.0838220453377574309 	0.0107215192094131263 	0.0776920833088479978 	0.0493952849585796994 	7.60710340389198869e-05 	-0.0408305196187505534 	-0.0392132654665172317 	-0.0586497903701430207 	-0.0376435012027389396 	-0.042859252730564408 	
+-0.0417528418174105478 	0.0534147679359561686 	0.00605318137651622134 	-0.110518564704705169 	0.0303083444138963834 	-0.0698363012644192538 	-0.0799617955193665797 	0.0743114422269026825 	-0.109716121251313081 	0.0149344985328543588 	
+-0.0882668827874336925 	-0.0708540653680969357 	0.0381793383515727017 	-0.0293551773647720039 	0.0531512103930633154 	-0.0697112506188075631 	0.0408215624827846721 	-0.0951110283124548817 	-0.0171942741452455125 	0.0470348372971189643 	
+-0.0657399362828440387 	-0.053272374316605052 	0.0133975324905930577 	0.0300709451951773898 	-0.0544225333546613579 	-0.00606314640518731702 	0.0233024762167977559 	-0.0204132321559630688 	0.0600914703901505448 	-0.0180978739494220692 	
+0.0267230367355865625 	0.0569189085968382705 	-0.073558081828668076 	0.0191147696594518267 	0.0514129153490664043 	-0.0542074397118775114 	0.0833216436482976802 	-0.117239535025526143 	0.00874818477038309152 	-0.00863655799432195767 	
+-0.0290881305805484568 	0.0766245179130629478 	-0.112431134543697478 	0.0336367047288538973 	0.0536623234208511157 	-0.116043029514711057 	0.0156681828540595515 	0.0794740797586113573 	0.0457990674313311086 	-0.0773972466366339984 	
+0.0277402397527089888 	-0.0657519208753022943 	-0.00144130694569506376 	-0.029245189849984414 	0.0119994188641120905 	-0.0510471681495050164 	-0.0988489514444823997 	-0.0488611448767203443 	0.0728258021114098208 	0.03729545391264176 	
+0.0471196020128240023 	-0.0720067999664676939 	0.0521688194364444552 	-0.0703627478511613114 	0.0695663387564082053 	-0.04122729377567666 	0.0309010979392343142 	-0.0906980135844153573 	-0.0870352070175022391 	-0.0181563431643288094 	
+0.0477398639452905962 	-0.00427757025208319683 	-0.102829011862847444 	0.00477949329307009521 	0.0500701112223516029 	0.0718234941700118812 	0.0361289226321894752 	0.0584854752628645447 	0.0745560372971755952 	-0.0415089319879584639 	
+-0.00388116272985549121 	-0.0687229988042429435 	-0.0429283520106844285 	-0.056744365605824236 	0.0686228270932693063 	0.0290002213028409395 	0.00469927030016458608 	-0.0275339668945225498 	0.0836486630173390033 	0.0570814647420558388 	
+-0.041851861447237966 	-0.0760002934799642593 	-0.0696019499262585245 	0.00445659818187340781 	0.0118975581803188908 	0.00423910870913413087 	0.0419631315949083455 	-0.0787829406173317787 	-0.0925713508144801883 	0.033374993268392085 	
+0.0618580299917116869 	0.0540702187176752003 	0.0351577973258900348 	0.0392531284000064221 	0.0509239760845504666 	0.0636569646157703062 	-0.000402066496232148115 	-0.0701891456028211347 	0.0393362692298720312 	-0.0188510509052676259 	
+0.0828736040104000038 	-0.0865704635880224921 	-0.0766730904587804851 	-0.0857540213367499748 	0.0274860757995049783 	0.0194383725230562604 	-0.105823123394605756 	-0.0709866837150482377 	0.0424011834755511421 	0.0659565473289708548 	
+0.0036130832149628817 	0.0773558655899370951 	-0.0692170225708402947 	0.0301559807407275927 	0.0587332778870047895 	-0.0670568462776033047 	-0.0857525797229270609 	-0.0727256236644798493 	-0.0284197034758939433 	0.0209000573711875318 	
+-0.0158750191354218341 	-0.102443578561385071 	0.0131753780039702405 	0.0295873077267609433 	-0.0182660365762192869 	-0.0828137537390702533 	-0.106082090703428542 	0.0763935018170668473 	-0.0248981141092162184 	-0.0852388161242966652 	
+0.0485228653750592251 	0.0703820693115500134 	-0.0287026291431580587 	0.00620496227657369327 	0.0611070229875667337 	0.0704873601192797516 	-0.0491202307475595429 	0.0464901762658672044 	0.0693784235424054069 	0.0470252760368678815 	
+0.0839435740596762431 	0.0192933727028773462 	0.0762444334897555281 	0.0689146654434652645 	0.0753830899155011752 	0.0334980135036066357 	0.0513369614380649875 	0.0254788521129208703 	0.0435277375312624129 	-0.0940481067652092773 	
+0.0116544327482557449 	-0.0691339191625459371 	-0.10393914858029378 	-0.117372763017342605 	-0.00785033929587877766 	0.0461465561024753113 	-0.0406486539248725673 	-0.0678173742170913169 	-0.0137995380926319907 	-0.074555060660587355 	
+-0.0391532977842081881 	0.00346534487915843093 	-0.0135241067395013466 	0.052012039094271198 	0.029211171046032814 	0.00610286821941561849 	-0.0230423397835572662 	-0.0745839088647388543 	-0.105740298910158625 	-0.0455664055803137041 	
+-0.0951966176446819579 	0.0670348203174379043 	-0.0257014117287915241 	-0.0980667481698469651 	-0.0127935797949776057 	0.005305818047409319 	0.000931322233582784677 	-0.0267574794370888103 	0.0833776602179078646 	0.0575917938888648867 	
+-0.0223711389333252457 	-0.112617353676874102 	-0.0960978243512426167 	0.0198566225218770841 	-0.0824692529421016812 	-0.00417554968339145949 	-0.0186781599251174477 	-0.0551885624161383911 	0.0123685826512496405 	-0.0552826407103401835 	
+-0.0547634357980012684 	-0.0881982851899758835 	0.0388226205520223952 	0.00714647224228950016 	0.0469961939452888389 	-0.0330110650752288987 	0.0548662569532775007 	-0.113713535393649895 	-0.0772877922398471223 	-0.0119519227458886237 	
+-0.00954333337495951763 	0.00121514504966621553 	0.029867855120931236 	-0.0120406526844303117 	-0.0976494139377943049 	0.0401897294570796176 	-0.0892792898667015278 	-0.0804433857883190551 	0.0501179561177237526 	0.00761128836897070286 	
+-0.0981474801680612546 	-0.112011161657356167 	0.0176002337088038523 	-0.0700647637144275887 	-0.0391876675049213016 	0.0209297780293818438 	-0.000728955837822507433 	-0.0340065394616916317 	-0.0924830588449497282 	-0.000552412545458035031 	
+-0.0431068414488170737 	-0.00406823763688253254 	0.0056946038715491136 	-0.0417677533101437487 	0.00927716284748522461 	0.0629743824208994635 	0.00639895672960912997 	0.00764739822664606966 	-0.0300792676582859915 	0.0814167605158566815 	
+0.0360652228161833371 	-0.0690374562389083901 	0.0478249715454292235 	0.0564118965832087205 	-0.0780281613949698472 	0.0420220571571057397 	0.041710896338119037 	-0.0752657598585945858 	-0.0239750093530065669 	-0.0880941915313790819 	
+0.0705359892510492026 	0.0168584696131418819 	-0.0297975899815548888 	-0.0973010730742479291 	-0.00116375961688545645 	-0.101869078845710351 	-0.0521385277408914632 	0.0195184303971443415 	-0.0670736426542261205 	-0.0384189916576083215 	
+0.0783286016833320969 	-0.116532571617073935 	-0.0835827021073091841 	-0.0906192538420639015 	-0.110050588008381592 	-0.0147503831886230067 	-0.0723611309633018368 	-0.0392664734734171211 	0.0720694461597808228 	-0.0282548526483542145 	
+-0.0953002880586378137 	-0.0191968642131555046 	-0.109976748101646274 	-0.0398195150308879586 	0.0445682193164055954 	-0.0372455727526815675 	-0.0809227359144047836 	0.0576819641209402947 	-0.0812956853737853563 	0.0701997509637005829 	
+0.0626810867928122156 	0.00654365135353230602 	-0.0132922583641513718 	-0.0934516957687276245 	0.00336836045661683108 	-0.0146243792582376013 	-0.0850793530754924143 	0.0739612083207051157 	-0.00965526051421400529 	0.0107502715390670276 	
+-0.102807927545290861 	-0.0657537722201639724 	-0.0447395170524635399 	0.0487808710737742401 	-0.0210542380038344738 	0.0324787756184322443 	0.0209833916655619082 	0.0153919180969652285 	-0.0744224166182238661 	-0.0233391217189672373 	
+0.0278434630978708179 	0.0527038337281267102 	-0.100739170867329988 	-0.10934334865271722 	0.0189960566278678487 	-0.112944160507240682 	-0.102923704596979998 	-0.0255168808107219848 	-0.0280575508767641445 	0.0218049855397044953 	
+-0.0590970813266171666 	0.024824247152644207 	0.00156139349797227681 	0.0627526149240287962 	-0.084586370058175181 	-0.00315309081369090013 	0.0586784517428365626 	-0.111090784003396448 	-0.0147070852254384769 	-0.0953363454910192559 	
+-0.0460796996528023486 	0.0438823016264262081 	0.0611429288771634452 	-0.0970707209572797131 	0.071376954135173587 	0.0339343777744265038 	0.054508009444280435 	-0.073311469007171462 	-0.107867380993618325 	-0.0120567825688234643 	
+0.0728407269129164037 	-0.0891651421772195057 	0.0427604046206212229 	-0.0512854911392803259 	0.0501019681682121354 	-0.0369765463348414106 	0.0748187592922426731 	-0.00293026320861910182 	-0.00395549757596779829 	0.0879356739720067865 	
+0.0421973783853101039 	-0.0184521167539976293 	-0.00162561722807323694 	0.00210613800444347382 	-0.119664662703793473 	0.0443381377343637223 	-0.0868806643448104959 	0.0379388675909690248 	-0.0365485868966792318 	0.0288104165270520012 	
+-0.0905315246382656647 	0.0285882762695103161 	0.0538612017366503215 	-0.00489066176729904909 	0.0481024196368253706 	-0.107291727722060012 	0.0493405069153138587 	0.0528152054492457348 	0.0144944316391132765 	-0.0791517983434828071 	
+0.0657470573647559037 	-0.00756404705580436716 	0.0419331011663033604 	-0.0377218693816129269 	0.00708866359936066812 	0.0587389627953765342 	0.0111423409288077743 	-0.00609246862258327072 	0.0558171307519831975 	-0.0480410779381007583 	
+0.0149975606862876344 	-0.0104597228911564068 	-0.0264335096281875029 	0.0617828237206741321 	-0.106649227531296367 	-0.110315418687146907 	-0.0319799239374929864 	-0.0819523008970640954 	0.051364449699607892 	0.0291810974652220514 	
+-0.057929830295404397 	-0.0672853652139217551 	-0.0951946243698622713 	0.0728910297855222961 	-0.104225090075724849 	-0.0368793299557001464 	-0.104844773065315505 	-0.0157109695259885808 	0.00754626690689901101 	-0.0961144689466412461 	
+-0.0331191615471033776 	0.0350010823099160331 	0.00669623969624057961 	0.0770455229616985038 	-0.0203635871837350782 	-0.0703635905067119816 	-0.0377656593251949613 	0.00883789638563593945 	0.0051732630203421533 	-0.0948906412871207972 	
+-0.00820852271610198782 	0.0030608361330911678 	-0.0809251093231248275 	0.000684867385085873156 	-0.0847046551663089287 	-0.0108805835901810751 	0.0386528579381290247 	0.0461222221240333019 	0.0798751626114453939 	-0.10577853201824404 	
+0.0659830224357993572 	0.0472835137427353447 	0.0694036973977961852 	-0.00138011200127536671 	0.0312080234807149935 	0.0164652708121146608 	0.0765334203251932244 	0.0204527091470514816 	-0.0158138395301828071 	-0.0167830964972385463 	
+-0.110945552823942695 	-0.0173933720479128931 	0.0509530800681333926 	0.0759344511279405332 	0.070469533215113872 	-0.120842637821257065 	-0.067055731450413561 	-0.056758193586146713 	-0.0231823146373266384 	-0.0189824187544384952 	
+-0.0244469373936185702 	-0.0500102054966653359 	-0.102521226008294483 	-0.0481296035217862045 	0.0466001275431255496 	-0.0789845511538133094 	-0.0501562718227550539 	0.0556009080154579866 	-0.00676372233178482163 	-0.0463238927245332235 	
+0.0567610778128717214 	-0.0764646873274305861 	0.062711022933166341 	-0.0270848025612393413 	0.0290944663359772558 	0.0245860652598726484 	-0.102280486511207552 	0.0797966556596675092 	0.0417607933127779468 	-0.0549845054585790388 	
+-0.0891063574603898206 	-0.0364545727694119137 	0.0835332251905906015 	-0.0790819643114479554 	-0.104676062557898519 	0.0742572023960191147 	0.0596794217970020233 	0.0308445805765217469 	-0.110264996302738144 	-0.0504269149838760811 	
+0.0395069985661478523 	-0.0344723238477800814 	-0.0578798741947319992 	-0.0337422605822460531 	-0.0674513329203961093 	0.0200048026698817355 	-0.00821614032522092856 	0.0106654981375493355 	0.0123016411242311777 	0.0795155547089083925 	
+-0.0617715433542510781 	0.065479166618212209 	0.00195774921141070963 	0.0770071127700327068 	0.0427958489424534252 	-0.0693411692059071327 	-0.0359698293045637996 	-0.0334113662704650996 	-0.0901659135790251892 	-0.064342042174802952 	
+0.00593590368594166043 	-0.0774346663206200575 	0.0347161136512349963 	-0.00903712342001561537 	-0.069519069727859345 	-0.0476111327623072178 	-0.0613173438225481371 	0.0261492933503423598 	0.0572318334716369614 	-0.0626478836489953866 	
+0.0347243530383698484 	0.0187746969106717815 	0.0683985583550629633 	0.038111018198438408 	-0.00413639840821313637 	-0.0503724557193259495 	0.0265259717962897414 	-0.022856711483214151 	-0.0109258244846265739 	-0.0848539440388113853 	
+0.0600002393133610007 	0.0111924002688265616 	-0.0924575875815014103 	-0.0159846415015863046 	0.0463204861705724252 	-0.0723339870535546908 	-0.0858391717508157487 	0.0639409323032253119 	0.0640047136376664383 	0.00157904530216265129 	
+0.0459364576217070955 	-0.0428783596338899037 	0.0492541372159891605 	-0.0414541902431913925 	-0.0115750387046378196 	0.0463370440455991434 	-0.0375833819480468692 	0.0713106287410141815 	-0.0193994089327903275 	-0.0237151803757713747 	
+-0.0753453310124642534 	0.0470511656806676948 	0.0324626170902225439 	0.0614037178137466777 	-0.061281590712654356 	0.0623676898892027848 	0.0750512255759631791 	0.0190380376989027073 	0.0466127631455927963 	-0.0983929534245903681 	
+-0.0753203082417579078 	-0.0224176903021309284 	0.00471628052934990483 	-0.0855904417885777757 	-0.0421638077286862617 	-0.0658455221012471947 	-0.0806309726221091649 	0.00860571380238822972 	0.0152878589221822903 	0.039018792176756148 	
+-0.0565291369224710169 	0.0158585135887158411 	-0.0442658643538430124 	0.0592908324210727269 	0.0112558633893031952 	0.0287660160009656427 	0.0078460232423851993 	-0.00674314133982322362 	-0.0980240877364823154 	0.0860735417597574964 	
+0.0181616787805686901 	0.0780931131490483488 	0.0384256775627969394 	-0.0590138645669058687 	-0.104693937007087998 	-0.0714499650120191954 	-0.0468880823266030894 	-0.0190346249456964933 	-0.0994299324576733978 	0.00318301500888165971 	
+-0.0702791188779655213 	0.0232283973941731017 	-0.0394057787524467532 	0.0583369595991816842 	0.0285719661683397734 	-0.0660293824518379674 	0.0141841461406632032 	-0.0997941017916422479 	-0.0755195647435984857 	-0.0780061915865791211 	
+-0.0798418475238190095 	0.043414742583256144 	0.0117247535240395453 	0.0112276466632689753 	-0.00787629396390318592 	0.0301382914524404681 	-0.0890409852897879189 	0.0248154954079231535 	0.0382007610670286948 	-0.0444501845513029514 	
+-0.00505573199014942593 	0.0241526588268680918 	-0.0990856819916874076 	0.0470290891320045007 	-0.0391118062172869374 	0.0687789002874952121 	0.0247822567088178645 	0.0554081887662699335 	-0.0331573704821253243 	0.0253307001033349598 	
+0.0201517257681016887 	0.0424133507908756036 	0.0517612350126334941 	-0.105216583665790911 	0.0477340875905697473 	-0.0490337199516985983 	-0.0469924448469433292 	0.0359340144289553159 	-0.0349709810159789941 	-0.0697181465531829248 	
+-0.00373024315202581669 	0.0301549767787843703 	0.0653145428872653061 	0.0517543034267617316 	-0.0846288875011196595 	0.0256938491157166285 	-0.092406857734933151 	0.0761029237061653929 	0.0547017652131371532 	-0.0603374975548693834 	
+0.0307161094129311633 	-0.0153807572336820651 	-0.0297366666359162868 	0.00127567340092943392 	0.0556035621987012244 	0.033339025409315888 	0.0611728416771542965 	0.0491116387705762061 	-0.0565046412502441323 	0.0409294409949754537 	
+0.0812533962147731076 	-0.0941516774508668192 	-0.102366243823632366 	-0.0860446544742183572 	-0.0411605110879607397 	-0.0230437949780598755 	0.0262613656524550813 	-0.0520510814075603942 	-0.079723152873806713 	-0.0086349250381016155 	
+0.0705616517718082592 	-0.0203791360987840564 	0.0107018455941401899 	-0.110833871528211766 	0.00995908866670183746 	0.0151457424113217391 	0.0337119811051480034 	-0.0279546597955270745 	-0.0356902891797534066 	0.0635603586482349076 	
+-0.0796486245604698939 	-0.00983081619719249587 	-0.0189773683249421996 	0.0193339237986886563 	-0.084538490201946731 	0.00642026009175085313 	0.0551327086774688196 	-0.10696646639063434 	0.00971244413446942928 	0.0198357787774446834 	
+0.0448307618176287531 	0.066344698304299371 	-0.0253722974031335528 	-0.112107658203131191 	-0.00556958261647357965 	-0.10994797134525211 	0.0359616760039740482 	-0.0190416857441550839 	-0.0594872400740054444 	0.0394624028793299081 	
+0.0690658534122153278 	-0.0999062150088833173 	0.0324104406735968992 	-0.0978605721595309924 	-0.0754630170597721806 	-0.0426534453468719529 	-0.00782411099958435323 	-0.0774677768754250956 	-0.0253543833180141509 	0.0845468666162700611 	
+0.0398423687402394214 	-0.0573769925457700003 	-0.00784732402103928764 	-0.0245265652164750429 	-0.11027706910147847 	0.0112654570866892369 	0.0401507524320682194 	0.0607132127388824161 	0.0666522189730806014 	-0.0588533154384459981 	
+-0.0383086617403472079 	-0.114883973613259624 	-0.0489867387371351048 	0.00385798122961478586 	0.0419306854226488002 	-0.0214770291324544171 	-0.0164813856498231617 	-0.0634619556600920787 	-0.0970966633803915929 	-0.0276198680889830142 	
+0.0474525399797028971 	0.00663627772342876513 	-0.0472919877615821937 	0.077938206459603393 	-0.0462471586967727541 	0.0579487031144106657 	0.0219817345940784593 	-0.0193762669554546629 	-0.0183893010175783314 	-0.099893734159963804 	
+-0.02658541286723572 	0.04560736548220342 	0.081509923857694136 	-0.0842613175258934144 	0.0285023643408886737 	-0.0423973102204057914 	0.0699295602520995929 	0.0373395933756589735 	-0.0324780631757300153 	0.0632516912133445075 	
+-0.0528291128181526126 	0.0745724130999155382 	-0.109252341300047864 	-0.0412118413136273939 	0.0390441113660137812 	-0.104227722565378184 	0.0675162591384109723 	0.0445242602306092708 	0.0304571690960141403 	-0.0620456448249670015 	
+-0.0976664214191611396 	-0.111064089641682615 	-0.0283842659320335533 	-0.107549656510199113 	-0.051039564985508816 	-0.0922281337448709326 	-0.0425284781241276055 	-0.0504513021219049548 	-0.0731898172230704691 	-0.0955232430862920229 	
+-0.00636443008799744084 	-0.0631390788061653579 	-0.0324780761242052796 	0.0150811640480009064 	0.0163275605569621722 	-0.025847800105606264 	0.0510397886916933441 	-0.0396596568891199805 	-0.0232522754136261223 	0.0651385955343220663 	
+-0.0115372693728243627 	0.0777036948587940257 	-0.0514738542471039895 	-0.00884068246085071771 	0.0517044975901950615 	0.0671036882691528058 	-0.0990420740708328889 	-0.0291615760162107808 	-0.0306045720842398027 	-0.013700357832346674 	
+0.038979011528923968 	-0.00743021666900376028 	-0.0967692004794863947 	0.0415170201455853372 	-0.0768028499520836266 	-0.0929296400082273016 	-0.0335372471530388377 	-0.0854159976525200731 	-0.0694836172339819563 	0.072084458476765742 	
+-0.102940982369903178 	0.00474983390680198264 	0.0389516699387414955 	-0.0536567399902947231 	0.067437544584287662 	0.0182991335344237956 	-0.0525736786723241603 	-0.0245199283407692148 	0.0532927456789939052 	-0.105185987293311953 	
+0.0289570986015648367 	-0.0179862426219850255 	-0.0347952882094319585 	0.0467508681374518495 	-0.00445332369677817804 	-0.0657205313783880002 	0.0667002393041097041 	-0.0769456469784192826 	-0.0429461701962694439 	0.0431691788006636074 	
+-0.0861836073997126056 	-0.0666895583740770709 	-0.069126730597191044 	-0.0779614929705302467 	-0.074722838976120462 	-0.00738192541775280296 	-0.00724688264280800958 	0.02348157920484506 	-0.100370399363810717 	-0.0434823895111814615 	
+0.0648992315743629616 	0.00518799840129952061 	0.0296072399423903572 	-0.043553395974616764 	0.0398729784709352109 	0.06952026026583559 	0.0259812016394117447 	-0.113273167752301951 	0.00398951619577828098 	-0.0557691777463301952 	
+-0.0306715000983116375 	-0.0530179653347295568 	-0.0369861843810302265 	-0.0145518908103485136 	0.0278581306317795263 	0.0774478922361130084 	-0.00300894119328308594 	-0.0552116426581901187 	0.080387320058459652 	0.0115086156060474707 	
+0.00517056593626328588 	0.0412391380788425313 	0.0237618633986983428 	0.0266537386581753889 	-0.0300720997391868868 	-0.0342424916070309532 	-0.0257235135761303482 	0.0727656596101706327 	-0.070357230078374966 	0.0178199394458505404 	
+-0.00653963648713632752 	-0.0251344123448872317 	-0.0897628875633802309 	-0.0271786656230643235 	-0.0622650232627898134 	0.0366814291386488717 	-0.0347457996638991864 	-0.0822490548688505074 	-0.0611579093048781469 	-0.0121232285512562783 	
+-0.0581145581944492035 	0.0542045052725088281 	0.0326961931862242913 	0.0600261361289733952 	0.0468347026540592848 	0.0524454251514683903 	0.0735490572539929804 	0.00397861840757738131 	-0.065682986863011189 	-0.056092442123518875 	
+-0.0163716809113475179 	0.0504557180777146994 	-0.0761281479907330028 	-0.0816112570970967399 	0.0316886669875248508 	0.0530848721544111626 	0.0642335805352825251 	0.0664892469824084342 	-0.000983864507556370723 	0.0810985051009005115 	
+-0.0174021299862850073 	0.027118986517123151 	0.0330927183795870772 	-0.116263538179238096 	0.0496603244956592554 	0.0307525159980376328 	-0.0919703339240501849 	0.0273858413326795481 	-0.0572168787185550962 	-0.0985817004740841968 	
+-0.0200053252010550416 	-0.00453046370932334765 	0.0586412385403922501 	0.0230202850563303689 	-0.013850504556597568 	0.0701250583602681804 	0.0414017325088206073 	-0.088851278764485439 	0.0204634849374224943 	0.0657438278386667241 	
+-0.00984134846166172077 	-0.0103704578992097323 	0.0228153465299076874 	-0.0242430333836236821 	-0.0620582684933687137 	-0.0477976932216939016 	-0.0249725372654928841 	-0.072226958950752207 	-0.000770137235563852986 	-0.0135805723907640383 	
+-0.0915735476092910533 	0.0491403698085967483 	0.0253514135603257439 	-0.00579511106651758173 	-0.103015736809559735 	0.0329857031764390421 	0.083881273902148043 	-0.0567909593545623537 	0.0162733338452598145 	-0.0855496384592240172 	
+-0.027829894704633814 	-0.0114219698142563504 	0.0385334939859549239 	-0.0717620801117500873 	0.0546021330176594544 	-0.0547381223252964913 	0.012425473571005069 	-0.0297097494525451027 	0.0434377308336664283 	-0.0529853310004288827 	
+]
+;
+gibbs_ma_schedule = []
+;
+gibbs_ma_increment = 0.100000000000000006 ;
+gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+down_size = 10 ;
+up_size = 100 ;
+learning_rate = 0.00100000000000000002 ;
+momentum = 0 ;
+initialization_method = "uniform_sqrt" ;
+input_size = 10 ;
+output_size = 100 ;
+name = "RBMMatrixConnection" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+;
+reconstruction_connection = *0 ;
+grad_learning_rate = 0.0100000000000000002 ;
+cd_learning_rate = 0.00100000000000000002 ;
+compute_contrastive_divergence = 0 ;
+standard_cd_grad = 1 ;
+standard_cd_bias_grad = 1 ;
+standard_cd_weights_grad = 1 ;
+n_Gibbs_steps_CD = 1 ;
+min_n_Gibbs_steps = 1 ;
+n_Gibbs_steps_per_generated_sample = 1 ;
+compute_log_likelihood = 0 ;
+minimize_log_likelihood = 0 ;
+Gibbs_step = 0 ;
+log_partition_function = 0 ;
+partition_function_is_stale = 1 ;
+input_size = -1 ;
+output_size = -1 ;
+name = "rbm_2" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+*12 ->GradNNetLayerModule(
+start_learning_rate = 0.0100000000000000002 ;
+decrease_constant = 0 ;
+init_weights = 0  0  [ 
+]
+;
+init_bias = []
+;
+init_weights_random_scale = 1 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
+weights = 2  100  [ 
+-0.00361456086924041289 	-0.0034546392301283888 	0.00892340884822014069 	0.00344600332427484786 	0.00409064634836684045 	0.00330896605896703958 	-0.0071032199901970676 	-0.0077218952474459113 	-0.00354635209487409817 	-0.00981545859166288101 	-0.000813800539720391583 	-0.00966192627622517591 	-0.000917079339102153156 	0.00454432087430920763 	0.00417192126901913193 	-0.00862727928662285551 	0.00209955758356582213 	-0.00702340380695190211 	0.00542175158362248871 	-0.00390224612647109993 	-0.00214279922826613074 	0.000233095447677282835 	0.00804593137043763812 	0.00574502357026333843 	0.000565606677185788963 	-0.00482300706056737992 	0.00739447598229839038 	0.00811872141969053294 	0.00379486357640810429 	0.00658199484371788188 	-0.00653934446117148023 	-0.0107571495767850647 	9.18485893836186841e-06 	-0.000736319356757701975 	0.00568173954498692266 	0.00736199843687386241 	-0.00864132204601229977 	0.00249227451801360645 	0.0031692626664838212 	-0.00096500131406930523 	-0.00238!
 053629351953977 	-0.000507034767643173593 	-0.00369954298604871458 	0.00391972062390184145 	-0.00732055190192157194 	-0.00879860623868512597 	0.009309560879504181 	-0.000515274541135739761 	0.00584820601158001462 	0.000468091146464152128 	-0.00204273692383042911 	-0.00104438579406907492 	0.00676734198102325098 	0.00259775966195790022 	-0.00506898157846626292 	0.00400301500060299201 	0.00462300065256556186 	0.0090185902254103379 	0.00714303775483081556 	-0.00835706779120674696 	0.00954111518838614281 	-0.00779115044297907546 	-0.00236347716601639129 	-0.00118304905244554295 	0.0050209025693952079 	-0.000286016031183988328 	-0.00482798210193771411 	-0.00454637521312786612 	-0.00287093068063447657 	-0.00581445626144608523 	-0.00198314410522661654 	-0.00711353476514379589 	0.0020642326867833221 	-0.00934342665262296972 	-0.00365153936610122668 	-0.00280561527474843149 	0.000543520117863609091 	-0.0040994033838285043 	0.0075005566754033498 	0.00741998824691424922 	0.006035248938!
 87166097 	-0.000161966156904518282 	-0.00299263494129304057 	-!
 0.003555
76391448426087 	-0.0028232877964259784 	-0.00114021105423473251 	-0.00392387421914959424 	-0.00744660364820715071 	-0.00428922564650745589 	0.00642389043866236405 	-0.00220870553974613281 	0.00340434749506830824 	0.00491865372732491583 	0.0003043342327976939 	-0.00347359847246154737 	-0.00805576626600675028 	-0.0101676942138113346 	0.00184513175038662097 	0.00834935392649940357 	-0.00233387137457582083 	
+-0.00428357603894804085 	-0.00560878811569882722 	0.00644610901550797186 	2.85361801116452833e-05 	0.00231843951024641396 	0.0091920061067590781 	-0.00498422481254911457 	-0.00798262593082200815 	-0.00681521017393437074 	0.00229313407708757206 	-0.000639420462873237813 	0.00721425721718700706 	0.00655349518388535258 	0.0018080823175403893 	0.005513546170684544 	0.00470534668525295421 	0.00210720344970842001 	-0.00959463543317737658 	0.00925219698373813855 	-0.00282655751003510394 	-0.00160556445194010283 	0.00670403504636591339 	-0.00689192001286197987 	0.00402486128987972636 	-0.00692580474096167464 	0.00524873955024266186 	0.001330082096385525 	-0.00749040086355653046 	0.00723936863861645349 	-0.00981095865746234655 	-0.00885297577408837577 	0.0105326493621040566 	-0.00686186375766028884 	-0.00186042921694784774 	0.00177016931350605969 	0.00713124303157063651 	0.00216752614868498134 	0.00380119429726093888 	-0.00594750008661806915 	0.00855464479060022258 	0.00024896574849!
 4373008 	-0.00158649273566845205 	0.00953251045551742164 	-0.00540848395533746838 	0.00277396057439592125 	0.000904513165711946896 	-0.000937195773606970301 	-0.000418610167075077926 	0.00755916712339810368 	0.00961871529798706375 	0.00812048427811438121 	0.00186746010322442341 	0.00407341980157822654 	-0.00311260062525448543 	0.00094987641171063997 	0.00530444774939974485 	-0.00473036555390706544 	0.00965177183175397642 	-0.00646798155063100025 	-0.00496175775451397627 	0.00669502184691669021 	0.00346766019491735701 	0.00202831558717488742 	0.0079754747566342446 	0.000698865924041077607 	-0.00304638580417564749 	-0.00679405487759599924 	0.00432431142677767999 	0.00216422450319122151 	-0.00061007203372814389 	-0.00382641334663995011 	-0.00998287979623964025 	0.00705941722703434284 	0.000176218032275878543 	-0.00094440667949118322 	0.00563608474335693544 	0.00410871051462780544 	0.0083555071211082934 	-0.00166986093452475707 	-0.00484421281078854273 	-0.00522019012264732067 !
 	0.00898723094886254292 	0.00862398867535417619 	-0.0096836565!
 19044459
01 	0.000221534183445729537 	0.000664773860125775521 	-0.00484719011819337645 	-0.00387543718905094398 	-0.00210722179689584645 	-0.00500161898954350377 	-0.00233237632557197535 	-0.00976789325837463217 	0.00146967192664533672 	0.00341503265385532462 	0.00298996139752164469 	0.00122907757217042288 	0.00700538259559502739 	0.00589268468676095239 	0.00867987032582888743 	-0.00537698855929221158 	
+]
+;
+bias = 2 [ -0.000484761096494694074 0.000484761096494707409 ] ;
+input_size = 100 ;
+output_size = 2 ;
+name = "affine_net" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+*13 ->SoftmaxModule(
+input_size = 2 ;
+name = "softmax" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+*14 ->NLLCostModule(
+target_size = 1 ;
+input_size = 2 ;
+output_size = 1 ;
+name = "nll" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+*15 ->ClassErrorCostModule(
+target_size = 1 ;
+input_size = 2 ;
+output_size = 1 ;
+name = "class_error" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+*16 ->ArgmaxModule(
+input_size = -1 ;
+output_size = 1 ;
+name = "argmax_class" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+*17 ->SquaredErrorCostModule(
+target_size = 1 ;
+input_size = 1 ;
+output_size = 1 ;
+name = "mse" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+] ;
+connections = 7 [ *18 ->NetworkConnection(
+source = "rbm_1.hidden.state" ;
+destination = "rbm_2.visible" ;
+propagate_gradient = 1  )
+*19 ->NetworkConnection(
+source = "rbm_2.hidden.state" ;
+destination = "affine_net.input" ;
+propagate_gradient = 1  )
+*20 ->NetworkConnection(
+source = "affine_net.output" ;
+destination = "softmax.input" ;
+propagate_gradient = 1  )
+*21 ->NetworkConnection(
+source = "softmax.output" ;
+destination = "nll.prediction" ;
+propagate_gradient = 1  )
+*22 ->NetworkConnection(
+source = "softmax.output" ;
+destination = "class_error.prediction" ;
+propagate_gradient = 0  )
+*23 ->NetworkConnection(
+source = "affine_net.output" ;
+destination = "argmax_class.input" ;
+propagate_gradient = 0  )
+*24 ->NetworkConnection(
+source = "argmax_class.output" ;
+destination = "mse.prediction" ;
+propagate_gradient = 0  )
+] ;
+ports = 8 [ ("input" , "rbm_1.visible" )("" , "nll.target" )("" , "class_error.target" )("" , "mse.target" )("output" , "affine_net.output" )("NLL" , "nll.cost" )("class_error" , "class_error.cost" )("mse" , "mse.cost" )] ;
+save_states = 1 ;
+name = "NetworkModule" ;
+use_fast_approximations = 1 ;
+estimate_simpler_diag_hessian = 0 ;
+expdir = "" ;
+random_gen = *5   )
+;
+batch_size = 11 ;
+reset_seed_upon_train = 0 ;
+cost_ports = 3 [ "NLL" "class_error" "mse" ] ;
+input_ports = 1 [ "input" ] ;
+target_ports = 3 [ "nll.target" "class_error.target" "mse.target" ] ;
+weight_ports = []
+;
+mbatch_size = 11 ;
+seed = 1827 ;
+stage = 1001 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 0 ;
+nstages = 1001 ;
+report_progress = 1 ;
+verbosity = 1 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1000 ;
+use_a_separate_random_generator_for_testing = 1827  )

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/fieldnames	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/fieldnames	2007-06-28 19:31:03 UTC (rev 7661)
@@ -0,0 +1,3 @@
+E[test1.E[NLL]]	0
+E[test1.E[class_error]]	0
+E[test1.E[mse]]	0

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/sizes	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat.metadata/sizes	2007-06-28 19:31:03 UTC (rev 7661)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/fieldnames	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/fieldnames	2007-06-28 19:31:03 UTC (rev 7661)
@@ -0,0 +1,4 @@
+splitnum	0
+test1.E[NLL]	0
+test1.E[class_error]	0
+test1.E[mse]	0

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/sizes	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat.metadata/sizes	2007-06-28 19:31:03 UTC (rev 7661)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/test_cost_names.txt	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/test_cost_names.txt	2007-06-28 19:31:03 UTC (rev 7661)
@@ -0,0 +1,3 @@
+NLL
+class_error
+mse

Added: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/train_cost_names.txt
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/train_cost_names.txt	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/train_cost_names.txt	2007-06-28 19:31:03 UTC (rev 7661)
@@ -0,0 +1,3 @@
+NLL
+class_error
+mse

Modified: trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.pyplearn	2007-06-28 19:02:15 UTC (rev 7660)
+++ trunk/plearn_learners/online/test/ModuleLearner/PL_ModuleLearner_TwoRBMs.pyplearn	2007-06-28 19:31:03 UTC (rev 7661)
@@ -1,7 +1,7 @@
 # Network architecture consisting in two RBMs stacked together, followed by an
 # affine transformation for classification.
 
-expdir = 'expdir-tester'
+expdirs = [ 'expdir-tester', 'expdir-tester2' ]
 
 rbm_size_1 = 10
 rbm_size_2 = 100
@@ -21,17 +21,13 @@
         visible_layer = pl.RBMBinomialLayer(
                 size = size_1,
                 use_fast_approximations = 0)
-        hidden_layer = pl.RBMBinomialLayer(
-                size = size_2,
-                use_fast_approximations = 0)
     elif type == 'gaussian':
         visible_layer = pl.RBMGaussianLayer(
                 size = size_1,
                 use_fast_approximations = 0)
-        hidden_layer = pl.RBMGaussianLayer(
-                size = size_2,
-                use_fast_approximations = 0)
-    
+    hidden_layer = pl.RBMBinomialLayer(
+            size = size_2,
+            use_fast_approximations = 0)
     return pl.RBMModule(
             name = name,
             visible_layer = visible_layer,
@@ -48,15 +44,21 @@
         targetsize = targetsize,
         weightsize = 0)
 
-tester = pl.PTester( 
+testers = []
+
+i = 0
+for type in [ 'binomial', 'gaussian' ]:
+    expdir = expdirs[i]
+    i += 1
+    tester = pl.PTester( 
     expdir = expdir,
     learner =
         pl.ModuleLearner(
             module =
                 pl.NetworkModule(
                     modules = [ \
-                        rbm('rbm_1', inputsize, rbm_size_1, grad_lr, cd_lr, 'binomial'),
-                        rbm('rbm_2', rbm_size_1, rbm_size_2, grad_lr, cd_lr, 'binomial'),
+                        rbm('rbm_1', inputsize, rbm_size_1, grad_lr, cd_lr, type),
+                        rbm('rbm_2', rbm_size_1, rbm_size_2, grad_lr, cd_lr, type),
 
                         pl.GradNNetLayerModule( 
                             name = "affine_net",
@@ -165,7 +167,10 @@
         "E[test1.E[mse]]"
     ]
 
-)
+    )
+    testers.append(tester)
 
 def main():
-    return tester
+    return pl.RunObject(
+            objects = testers,
+            run_objects = True)



From louradou at mail.berlios.de  Thu Jun 28 21:38:57 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 28 Jun 2007 21:38:57 +0200
Subject: [Plearn-commits] r7662 - in trunk/python_modules/plearn/learners: .
	modulelearners modulelearners/examples
Message-ID: <200706281938.l5SJcv60018933@sheep.berlios.de>

Author: louradou
Date: 2007-06-28 21:38:56 +0200 (Thu, 28 Jun 2007)
New Revision: 7662

Added:
   trunk/python_modules/plearn/learners/SVM.py
Modified:
   trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py
   trunk/python_modules/plearn/learners/modulelearners/examples/sample.py
   trunk/python_modules/plearn/learners/modulelearners/network_view.py
Log:
Added SVM (easy module to run SVM without having to care too much about
hyperparameters) 



Added: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2007-06-28 19:31:03 UTC (rev 7661)
+++ trunk/python_modules/plearn/learners/SVM.py	2007-06-28 19:38:56 UTC (rev 7662)
@@ -0,0 +1,500 @@
+import sys, os, time
+#from numarray import *
+from math import *
+from libsvm import *
+
+             
+class SVM_expert(object):
+      __attributes__ = ['kernel_type',
+                        'parameters_names',
+                        'tried_parameters',
+                        'best_parameters',
+                        'error_rate'
+                        ]
+                        
+      def __init__( self ):
+          self.parameters_names = ['C']
+          self.tried_parameters = {}
+          self.best_parameters  = None
+          self.error_rate       = 1.
+
+      def reset(self):
+          self.tried_parameters = {}
+          #if self.best_parameters != None:
+          #   self.add_parameter_to_tried_list(self.getBestValue('C'), self.best_parameters[1:])
+          self.error_rate       = 1.
+          
+      def get_svm_parameter( self, parameters ):
+          s= ', '.join([ self.parameters_names[i]+' = '+str(parameters[i]) for i in range(len(self.parameters_names)) ])
+          return eval('svm_parameter( svm_type = C_SVC, kernel_type = '+self.kernel_type+', '+s+')')
+
+      def add_parameter_to_tried_list(self, C, kernel_parameters):
+          if self.tried_parameters.has_key(kernel_parameters):
+             self.tried_parameters[kernel_parameters]+=[C]
+          else:
+             self.tried_parameters[kernel_parameters] =[C] 
+             
+      def init_C(self, C_value):
+          return [C_value/10., C_value, C_value*10.]
+
+      def init_parameters(self, kernel_parameters_list):
+          C_base = self.init_C(10.)
+          table  = []
+          for C in C_base:
+              for prm in kernel_parameters_list:
+                  table.append( parameters2list(C, prm) )
+                  self.add_parameter_to_tried_list(C, prm)
+          return table
+
+      def choose_new_parameters(self, kernel_parameters):
+          if self.tried_parameters.has_key(kernel_parameters):
+             C_table = choose_new_parameters_geom( self.tried_parameters[kernel_parameters], self.getBestValue('C') )
+          else:
+             C_table = self.init_C( self.best_parameters[0] )
+          table  = []
+          for C in C_table:
+                  table.append( parameters2list(C, kernel_parameters) )
+                  self.add_parameter_to_tried_list(C, kernel_parameters)
+          return table
+          
+      def getBestValue(self, name_prm):
+          return self.best_parameters[ self.parameters_names.index(name_prm) ]
+          
+
+class RBF_expert(SVM_expert):
+      def __init__( self ):
+          SVM_expert.__init__( self )
+          self.kernel_type       = 'RBF'
+          self.parameters_names += ['gamma']
+
+      def init_gamma(self, gamma):
+          return [gamma, gamma/9., gamma*9.]
+          
+      def init_parameters( self, samples ):
+          dim = len(samples[0])
+          std = mean_std(samples)
+          rho=sqrt(dim)*std
+          gamma0 = 1/(2*rho**2)
+          gamma_base = self.init_gamma(gamma0)
+          return SVM_expert.init_parameters( self, gamma_base )
+          
+      def choose_new_parameters( self ):
+          best_gamma = self.getBestValue('gamma') 
+          tried_gamma = self.tried_parameters
+          if tried_gamma.has_key(best_gamma):
+             if self.getBestValue('C') == self.tried_parameters[best_gamma][0] or self.getBestValue('C') == self.tried_parameters[best_gamma][len(self.tried_parameters[best_gamma])-1]:
+                return SVM_expert.choose_new_parameters(self, best_gamma)
+             else:
+                proposed_gammas = choose_new_parameters_geom( tried_gamma, best_gamma)
+          else:
+             proposed_gammas = self.init_gamma(best_gamma)
+          return SVM_expert.init_parameters(self, proposed_gammas)
+
+class POLY_expert(SVM_expert):
+      def __init__( self ):
+          SVM_expert.__init__( self )
+          self.kernel_type       = 'POLY'
+          self.parameters_names += ['degree','coef0']
+
+      def init_degree(self, degree):
+          return [  (degree-1,1), (degree,1), (degree+1,1) ]
+
+      def init_parameters( self, samples ):
+          #return SVM_expert.init_parameters(self, self.init_degree(3) )
+          return SVM_expert.init_parameters(self, [ (2,1), (3,1), (4,1) ] )
+          
+      def choose_new_parameters( self ):
+          best_degree = self.getBestValue('degree')
+          tried_degrees = [prms[0] for prms in self.tried_parameters]
+          if self.tried_parameters.has_key(best_degree):
+             if best_degree == max(tried_degrees):
+                return SVM_expert.init_parameters(self, [(best_degree+1,1)])
+             else:
+                return SVM_expert.choose_new_parameters(self, (self.best_parameters[1], self.best_parameters[2])  )
+          else:
+             return SVM_expert.init_parameters(self, self.init_degree(best_degree) )
+             
+class LINEAR_expert(SVM_expert):
+      def __init__( self ):
+          SVM_expert.__init__( self )
+          self.kernel_type = 'LINEAR'
+
+      def init_parameters( self, samples ):
+          return SVM_expert.init_parameters(self, [None])
+
+      def choose_new_parameters( self ):
+          return SVM_expert.choose_new_parameters(self, None)
+
+
+class SVM(object):
+
+      __attributes__ = ['error_rate',
+                        'valid_error_rate',
+                        'best_parameters',
+                        'tried_parameters',
+                        'save_filename'
+                        ]
+       
+      def __init__( self ):
+      
+          self.error_rate       = 1.
+          self.valid_error_rate = 1.
+          
+          self.LINEAR_expert  = LINEAR_expert()
+          self.RBF_expert     = RBF_expert()
+          self.POLY_expert    = POLY_expert()
+          
+          self.best_parameters      = None  
+          self.best_model           = None
+          self.tried_parameters     = {}
+          
+          self.save_filename        = None
+          
+          # For cross-validation
+          self.nr_fold        = 5
+
+      def reset( self ):
+          self.LINEAR_expert.reset()
+          self.RBF_expert.reset()
+          self.POLY_expert.reset()
+          
+          self.tried_parameters = {}
+          #if self.best_parameters != None:
+          #   self.add_parameter_to_tried_list(self.best_parameters[0], self.best_parameters[1:])
+          self.error_rate       = 1.
+          self.valid_error_rate = 1.
+
+      def add_parameter_to_tried_list(self, kernel, kernel_parameters):
+          if self.tried_parameters.has_key(kernel):
+             self.tried_parameters[kernel]+=[kernel_parameters]
+          else:
+             self.tried_parameters[kernel] =[kernel_parameters] 
+
+      def train_and_test(self, samples_target_list):
+          check_samples_target_list(samples_target_list)
+
+          best_expert = eval( 'self.'+self.best_parameters[0]+'_expert' )
+          best_parameters = best_expert.best_parameters
+          param = best_expert.get_svm_parameter( best_parameters )
+          if len(samples_target_list) == 1: # cross-validation
+             error_rate = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
+          else:
+             error_rate = do_simple_validation(samples_target_list[0][0] , samples_target_list[0][1] , samples_target_list[1][0] , samples_target_list[1][1], param)
+          return error_rate
+
+      def test(self, samples_target_list):
+          check_samples_target_list(samples_target_list)
+          if len(samples_target_list) <> 1:
+             raise TypeError, "in SVM::test(), samples_target_list must be [[samples],[targets]] (list of list)"
+          error_rate = test_model(model, samples_target_list[0][0], samples_target_list[0][1])
+          return error_rate
+
+
+      def train_and_tune(self, kernel_type, samples_target_list):
+          check_samples_target_list(samples_target_list)
+          
+          expert = eval( 'self.'+kernel_type+'_expert' )
+          
+          if len(expert.tried_parameters) == 0:
+             recompute_best = True
+          else:
+             recompute_best = False
+
+          if expert.best_parameters  == None:
+             parameters_to_try = expert.init_parameters( samples_target_list[0][0] )
+          else:
+             parameters_to_try = expert.choose_new_parameters()
+          
+          best_parameters   = expert.best_parameters
+          best_error_rate   = expert.error_rate
+
+          for parameters in parameters_to_try:
+              if parameters != expert.best_parameters or recompute_best:
+              
+                  self.add_parameter_to_tried_list(kernel_type, parameters)
+                  param = expert.get_svm_parameter( parameters )
+                  
+                  if len(samples_target_list) == 1: # cross-validation
+                     error_rate = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
+                  else:
+                     train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
+                     model = svm_model(train_problem, param)
+                     error_rate = test_model(model, samples_target_list[1][0], samples_target_list[1][1])
+                     
+                     if self.save_filename != None:
+                        try:
+                           FID=open(self.save_filename,'a')
+                           FID.write('------------\nTry with '+kernel_type+' kernel :\n')
+                           FID.write('parameters : '+str(parameters)+'\n')
+                           FID.write(' --> Error rate = '+str(error_rate)+'\n')
+                           FID.close()
+                        except:
+                           print "COULD not write in save_filename"   
+                                  
+                  if error_rate < best_error_rate:
+                     best_parameters = parameters
+                     best_error_rate = error_rate
+                     self.best_model = model
+
+          if best_error_rate < expert.error_rate:
+             expert.best_parameters = best_parameters
+             expert.error_rate = best_error_rate
+             
+             if best_error_rate < self.valid_error_rate:
+                self.best_parameters = [kernel_type, best_parameters]
+                self.valid_error_rate = best_error_rate
+                if len(samples_target_list) == 3: # train-valid-test
+                   self.error_rate = test_model(self.best_model, samples_target_list[2][0], samples_target_list[2][1])
+                else:
+                   self.error_rate = self.valid_error_rate
+          
+          return self.error_rate
+          
+
+##
+## Using the cross validation of libSVM
+## (problem: the folds are chosen randomly, so things are not rigorously compared)
+##
+#def do_cross_validation(samples, targets, param, nr_fold):
+#        N = len(targets)
+#        total_correct = 0
+#        prob = svm_problem(targets, samples)
+#            outputs = cross_validation(prob, param, nr_fold)
+#        for i in range(N):
+#            if outputs[i] == targets[i]:
+#               total_correct = total_correct + 1 
+#        return ( (N - total_correct) / N)
+
+##
+## Doing cross validation on samples
+## - divide the set of samples in nr_fold subsets
+## - (nr_fold - 1) subsets serve as training set / 1 subset as test set
+##   for each step.
+## - Then the error rate is simply the average error rate...
+##
+def do_cross_validation(samples, targets, param, nr_fold):
+    samples_subsets=[]
+    targets_subsets=[]
+    N=len(samples)
+    for i in range(nr_fold):
+        samples_subsets.append(samples[i:N:nr_fold])
+        targets_subsets.append(targets[i:N:nr_fold])
+    cum_error_rate=0.
+    for i in range(nr_fold):
+        test_samples = samples_subsets[i]
+        test_targets = targets_subsets[i]
+        train_samples=[]
+        train_targets=[]
+        for j in range(0,i)+range(i+1,nr_fold):
+            train_samples += samples_subsets[j]
+            train_targets += targets_subsets[j]
+        cum_error_rate += do_simple_validation(train_samples, train_targets, test_samples, test_targets, param)
+    return cum_error_rate / nr_fold
+        
+def test_model(model, samples, targets):
+    N = len(samples)
+    total_correct = 0
+    for i in range(N):
+        if model.predict(samples[i]) == targets[i]:
+           total_correct = total_correct + 1
+    return ( ( N - total_correct )*1. / N)
+
+def do_simple_validation(train_samples, train_targets, test_samples, test_targets, param):
+    train_problem = svm_problem( train_targets, train_samples )
+    model = svm_model(train_problem, param)
+    N = len(test_samples)
+    total_correct = 0
+    for i in range(N):
+        if model.predict(test_samples[i]) == test_targets[i]:
+           total_correct = total_correct + 1
+    return ( ( N - total_correct )*1. / N)
+
+
+
+
+
+
+#
+# Some useful functions
+#
+
+def choose_new_parameters_geom( table, best_value ):
+    sorted_table = sorted(table)
+    if best_value == sorted_table[0]:
+       # smallest value
+       proposed_table = [ best_value*1.1*best_value/sorted_table[1],
+                          #best_value,
+                          geom_mean([sorted_table[1],best_value]) ]
+    elif best_value == sorted_table[len(table)-1]:
+       # largest value
+       proposed_table = [ geom_mean([sorted_table[len(table)-2],best_value]), 
+                          #best_value,
+                          best_value*0.9*best_value/sorted_table[len(table)-2] ]
+    else:
+       # middle value (best case: dichotomie)
+       if best_value not in sorted_table:
+          raise TypeError, "in RBF.choose_new_parameters: "+str(best_value)+" not found in tried_parameters"
+       index = sorted_table.index(best_value)
+       proposed_table = [ geom_mean([sorted_table[index-1],best_value]),
+                          #best_value,
+                          geom_mean([sorted_table[index+1],best_value]) ]
+    return proposed_table
+
+def normalize(data,mean,std):
+    if mean == None:
+       mean=[]
+       for i in range(len(data[0])):
+           mean.append( get_mean_cmp(data,i) )
+    if std == None:
+       std=[]
+       for i in range(len(data[0])):
+           std_tmp=get_std_cmp(data,i)
+           if std_tmp == 0.:
+              print "WARNING : standard deviation is 0 on component "+str(i)
+              std.append( 1. )
+           else:
+              std.append( std_tmp )
+    for i in range(len(data[0])):
+        for j in range(len(data)):
+            data[j][i]=(data[j][i]-mean[i])/std[i]
+    return mean, std
+
+def mean_std(data):
+    stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
+    return sum(stds)/len(stds)
+def get_std_cmp(data,i):
+    values=[vec[i] for vec in data]
+    tot = sum(values)
+    avg = tot*1.0/len(values)
+    sdsq = sum([(i-avg)**2 for i in values])
+    return (sdsq*1.0/(len(values)-1 or 1))**.5
+def get_mean_cmp(data,i):
+    values=[vec[i] for vec in data]
+    return  sum(values)/len(values)
+
+def arithm_mean(data):
+    if type(data[0]) == list:
+       return [sum( [data[i][coor] for i in range(len(data))] )*1.0/len(data) for coor in range(len(data[0]))]
+    else:
+       return sum(data)*1.0/len(data)
+def geom_mean(data):
+    if type(data[0]) == list:
+       res=[]
+       for coor in range(len(data[0])):
+           res.append( geom_mean( [data[i][coor] for i in range(len(data))] ) )
+       return res
+    else:
+       prod = 1.0
+       for value in data:
+           prod *= value
+       return prod**(1.0/len(data))
+
+def check_samples_target_list(samples_target_list):
+    if type(samples_target_list) != list or len(samples_target_list) == 0 or type(samples_target_list[0]) != list:
+       raise TypeError, "ERROR: samples_target_list must be a list of list (of arrays)"
+    else:
+       for samples_target in samples_target_list:
+           if len(samples_target) != 2:
+              raise TypeError, "ERROR: samples_target_list has an element with length "+str(len(samples_target))+" (instead of 2)"
+           if len(samples_target[0]) == 0 or len(samples_target[1]) == 0:
+              raise ValueError, "ERROR: samples_target_list has an element that has an element with an empty length"
+           if len(samples_target[0]) != len(samples_target[1]):
+              raise ValueError, "ERROR: samples_target_list has an element that has an elements with different len. Len are: " + len(samples_target[0])+" and " + len(samples_target[1])
+    if len(samples_target_list) == 1:
+       print "cross-validation"
+    elif len(samples_target_list) == 2:
+       print "simple validation"
+    elif len(samples_target_list) == 3:
+       print "validation + test"
+    else:
+       raise TypeError, "ERROR: samples_target_list have length "+str(len(samples_target_list))+" (not in [1,2,3])\n"+"samples_target_list has to be a list of [sample, target] arrays\n"+"for example :\n\t[[TrainSet, TrainLabels]]\n"+"\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels]]\n"+"\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels], [TestSamples, TestLabels]]\n"
+
+def parameters2list(C, kernel_parameters):
+          if kernel_parameters == None:
+             return [C]
+          elif type(kernel_parameters) == list:
+             return [C]+kernel_parameters
+          elif type(kernel_parameters) == tuple:
+             return [C]+[prm for prm in kernel_parameters]
+          else:
+             return [C, kernel_parameters ]
+
+
+
+if __name__ == '__main__':
+
+    # an EXAMPLE to use the class...
+
+#>>># Initialization 
+
+    my_svm=SVM()
+
+#<<<#
+#>>># To save the results (progressively) in a ASCII file
+
+    my_svm.save_filename = 'my_svm_results.txt'
+   
+#<<<#
+#>>># Defining train / valid data
+    # - CROSS-VALIDATION
+    
+    DATA = [ [train_samples , train_targets] ]
+
+    # or...
+    # - SIMPLE VALIDATION
+    
+    DATA = [ [train_samples , train_targets] , [valid_samples , valid_targets] ]
+    
+    # Note:
+    # You can also do
+    #
+    # DATA = [ [train_samples , train_targets] , [valid_samples , valid_targets] , [test_samples , test_targets] ]
+    #
+    # (my_svm.error_rate will be the statistic on test set, and my_svm.valid_error_rate the statistics on the validation set)
+    #
+    # But it is not the most efficient way to do :
+    #     You'd better tune your model with validation,
+    #     and then test when you have the best model
+    
+    
+#<<<#
+#>>># Compute the accuracies (exploring a bit, each time, the space of parameters)
+    # - my_svm.error_rate indicates the current error rates.
+    # - This error rate can only decrease while you run "train_and_tune"
+    #   (as you are tuning parameters so as to improve the results)
+   
+    my_svm.train_and_tune( 'LINEAR' ,  DATA )
+    my_svm.train_and_tune( 'LINEAR' ,  DATA )
+    my_svm.train_and_tune( 'RBF' ,     DATA )
+    my_svm.train_and_tune( 'RBF' ,     DATA )
+    my_svm.train_and_tune( 'POLY' ,    DATA )
+    my_svm.train_and_tune( 'POLY' ,    DATA )
+    #[..]
+
+    valid_error_rate =  my_svm.error_rate
+    print valid_error_rate
+    print my_svm.best_parameters
+    print my_svm.tried_parameters
+    
+#<<<#
+#>>># When training with new data (closed to the previous one)
+    # ones can need to forget the explored tables of parameters and accuracies
+    # while reminding the best set of parameters
+
+    my_svm.reset()
+   
+    NEW_DATA = #[..]
+   
+    # Simple Validation
+    my_svm.train_and_tune( 'RBF' , NEW_DATA )
+    
+    valid_error_rate = my_svm.error_rate
+
+#<<<#
+#>>># To try the best trained model with new data (and obtain "fair" error rates)
+   
+    TEST_DATA=[ [test_samples , test_targets] ]
+    
+    test_error_rate = my_svm.test( TEST_DATA )
+    
+    print test_error_rate

Modified: trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py	2007-06-28 19:31:03 UTC (rev 7661)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py	2007-06-28 19:38:56 UTC (rev 7662)
@@ -50,8 +50,14 @@
 plarg_defaults.supervised_nStages       = 100  # /!\ see after: supervised_nStages   *= trainNsamples
 plarg_defaults.nStagesStep              = 5
 plarg_defaults.LR_CDiv                  = 0.01
+plarg_defaults.LR_CDiv1		        = float(plargs.LR_CDiv)
+plarg_defaults.LR_CDiv2			= float(plargs.LR_CDiv)
+plarg_defaults.LR_CDiv3			= float(plargs.LR_CDiv)
 plarg_defaults.LR_GRAD_UNSUP            = 0.003
-plarg_defaults.LR_SUP                   = 0
+plarg_defaults.LR_GRAD_UNSUP1		= float(plargs.LR_GRAD_UNSUP) 
+plarg_defaults.LR_GRAD_UNSUP2 		= float(plargs.LR_GRAD_UNSUP) 
+plarg_defaults.LR_GRAD_UNSUP3 		= float(plargs.LR_GRAD_UNSUP)	
+plarg_defaults.LR_SUP                   = 0.
 plarg_defaults.L2wd_SUP                 = 1e-5
 plarg_defaults.nGibbs                   = 1
 plarg_defaults.Tag                      = 'dbn-'+str(plargs.nRBM)+'RBMimage'
@@ -70,7 +76,13 @@
 supervised_nStages       *= trainNsamples
 nStagesStep               = int(plargs.nStagesStep)                #
 LR_CDiv                   = float(plargs.LR_CDiv)                # unsup. lr
+LR_CDiv1                   = float(plargs.LR_CDiv1)                # unsup. lr
+LR_CDiv2                   = float(plargs.LR_CDiv2)                # unsup. lr
+LR_CDiv3                   = float(plargs.LR_CDiv3)                # unsup. lr
 LR_GRAD_UNSUP             = float(plargs.LR_GRAD_UNSUP)         # super. lr
+LR_GRAD_UNSUP1             = float(plargs.LR_GRAD_UNSUP1)         # super. lr
+LR_GRAD_UNSUP2             = float(plargs.LR_GRAD_UNSUP2)         # super. lr
+LR_GRAD_UNSUP3             = float(plargs.LR_GRAD_UNSUP3)         # super. lr
 LR_SUP                    = float(plargs.LR_SUP) # super. lr
 L2wd_SUP                  = float(plargs.L2wd_SUP)
 nGibbs                     = int(plargs.nGibbs)
@@ -79,15 +91,32 @@
 Tag                     = plargs.Tag
 
 
-
 BaseDir =  '/u/louradoj/PRGM/blocksworld/res/'+os.path.basename(Path)
 data_filename = Path+'/BABYAI_'+ImageType+'_'+str(unsupervised_trainNsamples)+'x'+str(Nobj)+'obj_'+str(Size)+'x'+str(Size)+'.'+Topics+'.train.'+Encoding+'.'+Extension
 BaseTag = Tag+'_'+layerType+'_'+basename_withoutExt(data_filename)
 
-unsupervised_expdir = BaseDir + '/greedyDBN/UNSUP_' + BaseTag + '_'+layerType+''.join(['_N'+str(i)+'-'+str(globals()['NH'+str(i)]) for i in range(1,nRBM+1)])+'_LRs'+str(LR_CDiv)+'-'+str(LR_GRAD_UNSUP)+'_ns'+str(unsupervised_nStages)+'_ng'+str(nGibbs) 
-finetuning_expdir = BaseDir + '/greedyDBN/FINETUNE_' + BaseTag + '_'+layerType+''.join(['_N'+str(i)+'-'+str(globals()['NH'+str(i)]) for i in range(1,nRBM+1)])+'_LRs'+str(LR_CDiv)+'-'+str(LR_GRAD_UNSUP)+'-'+str(LR_SUP)+'_WD'+str(L2wd_SUP)+'_ns'+str(unsupervised_nStages)+'-'+str(supervised_nStages)+'_ng'+str(nGibbs)
 
-if LR_SUP==None or LR_SUP == 0:
+unsupervised_expdir = BaseDir + '/greedyDBN/UNSUP_' + BaseTag + '_'+layerType+''.join(['_N'+str(i)+'-'+str(globals()['NH'+str(i)]) for i in range(1,nRBM+1)])+'_LRs'+'-'.join([str(globals()['LR_CDiv'+str(i)]) for i in range(1,nRBM+1)])+'_'+'-'.join([str(globals()['LR_GRAD_UNSUP'+str(i)]) for i in range(1,nRBM+1)])+'_ns'+str(unsupervised_nStages)+'_ng'+str(nGibbs) 
+finetuning_expdir = BaseDir + '/greedyDBN/FINETUNE_' + BaseTag + '_'+layerType+''.join(['_N'+str(i)+'-'+str(globals()['NH'+str(i)]) for i in range(1,nRBM+1)])+'_LRs'+'-'.join([str(globals()['LR_CDiv'+str(i)]) for i in range(1,nRBM+1)])+'_'+'-'.join([str(globals()['LR_GRAD_UNSUP'+str(i)]) for i in range(1,nRBM+1)])+'-'+str(LR_SUP)+'_WD'+str(L2wd_SUP)+'_ns'+str(unsupervised_nStages)+'-'+str(supervised_nStages)+'_ng'+str(nGibbs)
+
+
+
+if len(sys.argv) == 1:
+   learner_filename = '/u/louradoj/PRGM/blocksworld/res/textual_v3/greedyDBN/FINETUNE_dbn-3RBMimage_gaussian_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_gaussian_N1-500_N2-500_N3-500_LRs0.01-0.01-0.01_0.003-0.003-0.003-0.03_WD1e-05_ns2500000-1000000_ng1'+"/Split0/final_learner.psave"
+   learner_filename = '/u/louradoj/PRGM/blocksworld/res/textual_v3/greedyDBN/UNSUP_dbn-3RBMimage_gaussian_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_gaussian_N1-500_N2-500_N3-500_LRs0.01-0.01-0.01_0.003-0.003-0.003_ns1250000_ng1'+"/init_learner.psave"
+   learner_filename = '/u/louradoj/PRGM/blocksworld/res/textual_v3/greedyDBN/FINETUNE_dbn-3RBMimage_gaussian_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_gaussian_N1-500_N2-500_N3-500_LRs0.01-0.01-0.01_0.01-0.01-0.01-0.03_WD1e-05_ns5000000-1000000_ng1'+"/Split0/final_learner.psave"
+   learner_filename = '/u/louradoj/PRGM/blocksworld/res/textual_v3/greedyDBN/UNSUP_dbn-3RBMimage_gaussian_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_gaussian_N1-500_N2-500_N3-500_LRs0.01-0.01-0.01_0.01-0.01-0.01_ns5000000_ng1'+"/init_learner.psave"
+elif len(sys.argv) == 2:
+   basename=os.path.basename(sys.argv[1])
+   if basename[0:5] == 'UNSUP':
+      print "UNSUPERVISED-way trained model"
+      learner_filename = sys.argv[1]+"/init_learner.psave"
+   elif basename[0:5] == 'FINET':
+      print "supervised FINETUNED model"
+      learner_filename = sys.argv[1]+"/Split0/final_learner.psave"
+   else:
+      raise TypeError, "Cannot recognize "+basename[0:5]+" in "+sys.argv[1]
+elif LR_SUP==None or LR_SUP == 0:
    print "UNSUPERVISED-way trained model"
    learner_filename = unsupervised_expdir+"/init_learner.psave"
 else:
@@ -97,7 +126,7 @@
 
 def check_choice(c):
     try:
-       if int(c) in [1,2]:
+       if int(c) in [1,2,3,4]:
           return True
     except: pass
     return False
@@ -108,9 +137,15 @@
    print "Type the number corresponding to your choice :"
    print "1. Sample after having initialized input visible units with (randomly picked) real image"
    print "2. Sample after having initialized top hidden units with a random binary vector"
+   print "3. Reconstruct the input image"
+   print "4. Visualize input weights"
    c = sys.stdin.readline()
 
 if int(c) == 1:
    os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample.py '+' '.join([ learner_filename, str(Size*Size), data_filename, 'gibbs_step='+str(gibbs_step) ]))
 elif int(c) == 2:
    os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_hidden.py '+' '.join([ learner_filename, str(Size*Size), 'gibbs_step='+str(gibbs_step) ]))
+elif int(c) == 3:
+   os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/reconstruct.py '+' '.join([ learner_filename, str(Size*Size), data_filename]))
+elif int(c) == 4:
+   os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/inputweights.py '+' '.join([ learner_filename, str(Size*Size)]))

Modified: trunk/python_modules/plearn/learners/modulelearners/examples/sample.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/sample.py	2007-06-28 19:31:03 UTC (rev 7661)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/sample.py	2007-06-28 19:38:56 UTC (rev 7662)
@@ -47,7 +47,7 @@
 
 if __name__ == "__main__":
 
-  if len(sys.argv) < 2:
+  if len(sys.argv) < 4:
      print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size> <dataSet_filename> [gibbs_step=<gibbs_step>]\n"
      print "Purpose:\n\tSee consecutive Gibbs sample"
      print "\twhen input visible units are initalized with real images"

Modified: trunk/python_modules/plearn/learners/modulelearners/network_view.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-06-28 19:31:03 UTC (rev 7661)
+++ trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-06-28 19:38:56 UTC (rev 7662)
@@ -1,4 +1,4 @@
-#!/bin/env python
+#!/usr/bin/env python
 
 import sys
 import os, os.path
@@ -49,7 +49,7 @@
 def countModules(mynetwork):
     n=0
     for module in mynetwork.modules:
-        if is_SplitModule(module) == False:
+        if isModule(module,'Split') == False:
 	   n += 1
     return n
 
@@ -110,9 +110,9 @@
     return formatModulesNames(ModuleName,modules_dict)
 
 
-def is_SplitModule(module):
-    return 'SplitModule' in str(type(module)) # type(module)==type(pl.SplitModule())  #
-       
+def isModule(module,name):
+    return name+'Module' in str(type(module))
+     
 def module_type(module):
     return str(type(module)).upper()
 
@@ -148,20 +148,23 @@
 def get_graph(modules, connections, ports):
     edges=[]
     edges_toAdd={}
-    globals()['modules_dict'] = Modules2dict(modules)
+    tmp_dict = Modules2dict(modules)
+    for key in tmp_dict:
+        if globals()['modules_dict'].has_key(key) == False:
+	   globals()['modules_dict'][key] = tmp_dict[key]
     ports_dict = getPortDict(ports)
     for connection in connections:
         inputModuleName , outputModuleName, inputModulePort = getInputOutput(connection)
 	inputModuleNameToplot = checkName(inputModuleName,ports_dict,modules_dict)
 	outputModuleNameToplot = checkName(outputModuleName,ports_dict,modules_dict)
 	
-	if is_SplitModule(modules_dict[inputModuleName]):
+	if isModule(modules_dict[inputModuleName],'Split'):
 	   if edges_toAdd.has_key(inputModuleName):
 	      edges_toAdd[inputModuleName][1].append(outputModuleNameToplot)
 	   else:
 	      edges_toAdd[inputModuleName]=[[],[]]
 	      edges_toAdd[inputModuleName][1]=[outputModuleNameToplot]
-	elif is_SplitModule(modules_dict[outputModuleName]):
+	elif isModule(modules_dict[outputModuleName],'Split'):
 	   if edges_toAdd.has_key(outputModuleName):
 	      edges_toAdd[outputModuleName][0].append(inputModuleNameToplot)
 	   else:
@@ -188,6 +191,11 @@
         myedge=pydot.Edge(edge[0],edge[1])
 	myedge.set_label(edge[2])
         graph.add_edge(myedge)
+    
+    for module in modules:
+        if isModule(module,'Network'):
+	   for edge in get_graph(module.modules, module.connections, module.ports).get_edge_list():
+	       graph.add_edge(edge)
 
 
     for port in port_list:
@@ -237,7 +245,10 @@
 
 if __name__ == '__main__':
 
-    inputname  = sys.argv[1]
+    if len(sys.argv) == 2:
+       inputname  = sys.argv[1]
+    else:
+       inputname  =  '/u/louradoj/PRGM/babyAI/dbn/convNet.py'
     output_extension = '.network.jpeg'
     output_name = os.path.splitext(inputname)[0]
     input_extension = os.path.splitext(inputname)[1]



From tihocan at mail.berlios.de  Thu Jun 28 21:39:15 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Jun 2007 21:39:15 +0200
Subject: [Plearn-commits] r7663 - trunk/plearn/vmat
Message-ID: <200706281939.l5SJdFWd018997@sheep.berlios.de>

Author: tihocan
Date: 2007-06-28 21:39:15 +0200 (Thu, 28 Jun 2007)
New Revision: 7663

Modified:
   trunk/plearn/vmat/MemoryVMatrix.cc
Log:
Commented out declaration of 'memory_data' as a learnt option, as it may waste a lot of disk space

Modified: trunk/plearn/vmat/MemoryVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MemoryVMatrix.cc	2007-06-28 19:38:56 UTC (rev 7662)
+++ trunk/plearn/vmat/MemoryVMatrix.cc	2007-06-28 19:39:15 UTC (rev 7663)
@@ -109,8 +109,14 @@
                    "If provided, will be used to set this VMatrix's"
                    " fieldnames." );
 
+    /* This field was declared as an option, but the author does not remember
+     * why. The problem is that we do not want it to be a learnt option, since
+     * it may save the whole dataset pointed by 'source', which could waste a
+     * lot of disk space.
+     * As a result, the two lines below are now commented out.
     declareOption(ol, "memory_data", &MemoryVMatrix::memory_data, OptionBase::learntoption,
                   "The underlying Mat with the data.");
+    */
 
     inherited::declareOptions(ol);
 }



From nouiz at mail.berlios.de  Thu Jun 28 21:47:30 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 28 Jun 2007 21:47:30 +0200
Subject: [Plearn-commits] r7664 - trunk/plearn/ker
Message-ID: <200706281947.l5SJlUvR019788@sheep.berlios.de>

Author: nouiz
Date: 2007-06-28 21:47:30 +0200 (Thu, 28 Jun 2007)
New Revision: 7664

Modified:
   trunk/plearn/ker/DistanceKernel.cc
   trunk/plearn/ker/DistanceKernel.h
Log:
Added an option ignore_missing that work only if pow_distance is true


Modified: trunk/plearn/ker/DistanceKernel.cc
===================================================================
--- trunk/plearn/ker/DistanceKernel.cc	2007-06-28 19:39:15 UTC (rev 7663)
+++ trunk/plearn/ker/DistanceKernel.cc	2007-06-28 19:47:30 UTC (rev 7664)
@@ -62,7 +62,8 @@
 DistanceKernel::DistanceKernel(real the_Ln, bool pd)
     : n(the_Ln),
       optimized(false),
-      pow_distance(pd)
+      pow_distance(pd),
+      ignore_missing(false)
 {}
 
 ////////////////////
@@ -81,6 +82,11 @@
                   "If set to 1, the evaluate_i_j method will be faster, at the cost of potential\n"
                   "approximations in the result.");
 
+    declareOption(ol, "ignore_missing", &DistanceKernel::ignore_missing, OptionBase::buildoption, 
+                  "If set to false, nan will be propagated.\n"
+                  "If set to true, if a value is missing in the matrix of some examples, we will ignore this value for the distance\n"
+                  "If set to true, work only if pow_distance is set to 1.");
+
     inherited::declareOptions(ol);
 }
 
@@ -88,8 +94,11 @@
 // evaluate //
 //////////////
 real DistanceKernel::evaluate(const Vec& x1, const Vec& x2) const {
+    if (!ignore_missing && !pow_distance)
+        PLERROR("DistanceKernel::evaluate(int i, int j) ignore_missing implemented only if pow_distance is set");
+
     if (pow_distance) {
-        return powdistance(x1, x2, n);
+        return powdistance(x1, x2, n, ignore_missing);
     } else {
         return dist(x1, x2, n);
     }
@@ -100,6 +109,9 @@
 //////////////////
 real DistanceKernel::evaluate_i_j(int i, int j) const {
     static real d;
+    if (ignore_missing)
+        PLERROR("DistanceKernel::evaluate_i_j(int i, int j) not implemented for ignore_missing");
+
     if (optimized && fast_exact_is_equal(n, 2.0)) {
         if (i == j)
             // The case 'i == j' can cause precision issues because of the optimized
@@ -130,6 +142,9 @@
 ////////////////////////////
 void DistanceKernel::setDataForKernelMatrix(VMat the_data)
 {
+    if (ignore_missing)
+        PLWARNING("DistanceKernel::setDataForKernelMatrix(VMat the_data) not tested for ignore_missing");
+
     inherited::setDataForKernelMatrix(the_data);
     if (fast_exact_is_equal(n, 2.0)) {
         squarednorms.resize(data.length());

Modified: trunk/plearn/ker/DistanceKernel.h
===================================================================
--- trunk/plearn/ker/DistanceKernel.h	2007-06-28 19:39:15 UTC (rev 7663)
+++ trunk/plearn/ker/DistanceKernel.h	2007-06-28 19:47:30 UTC (rev 7664)
@@ -67,6 +67,7 @@
 
     bool optimized;
     bool pow_distance;
+    bool ignore_missing;
 
     DistanceKernel(real the_Ln=2, bool powdist=false);
     



From louradou at mail.berlios.de  Thu Jun 28 21:47:44 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 28 Jun 2007 21:47:44 +0200
Subject: [Plearn-commits] r7665 - trunk/python_modules/plearn/learners
Message-ID: <200706281947.l5SJliKq019808@sheep.berlios.de>

Author: louradou
Date: 2007-06-28 21:47:44 +0200 (Thu, 28 Jun 2007)
New Revision: 7665

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2007-06-28 19:47:30 UTC (rev 7664)
+++ trunk/python_modules/plearn/learners/SVM.py	2007-06-28 19:47:44 UTC (rev 7665)
@@ -483,10 +483,21 @@
 
     my_svm.reset()
    
-    NEW_DATA = #[..]
+    # Cross-validation
+    NEW_DATA =  [ [train_samples , train_targets] ]
+    #
+    # or
+    #
+    # Simple validation
+    NEW_DATA =  [ [train_samples , train_targets], [valid_samples , valid_targets]  ]
    
-    # Simple Validation
+    # If you want to tune again on the new data
     my_svm.train_and_tune( 'RBF' , NEW_DATA )
+    #
+    # or
+    #
+    # If you want to try what give the best parameters (retrain the model on new train data)
+    my_svm.train_and_test( NEW_DATA )
     
     valid_error_rate = my_svm.error_rate
 



From louradou at mail.berlios.de  Thu Jun 28 21:59:01 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 28 Jun 2007 21:59:01 +0200
Subject: [Plearn-commits] r7666 -
	trunk/python_modules/plearn/learners/modulelearners/examples
Message-ID: <200706281959.l5SJx1c3020926@sheep.berlios.de>

Author: louradou
Date: 2007-06-28 21:59:01 +0200 (Thu, 28 Jun 2007)
New Revision: 7666

Modified:
   trunk/python_modules/plearn/learners/modulelearners/examples/plugNetwork2SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/modulelearners/examples/plugNetwork2SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/plugNetwork2SVM.py	2007-06-28 19:47:44 UTC (rev 7665)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/plugNetwork2SVM.py	2007-06-28 19:59:01 UTC (rev 7666)
@@ -1,6 +1,6 @@
 import sys
 from plearn.learners.modulelearners  import *
-from plearn.learners.discr_power_SVM import *
+from plearn.learners.SVM import *
 
 if __name__ == '__main__':
 
@@ -15,22 +15,11 @@
     dataTest_filename = data_filename
     dataValid_filename = data_filename
 
-#    learner_filename = "/u/louradoj/PRGM/blocksworld/res/textual_v2/BESTdbn/final_learner.psave"
-#    dataPath='/cluster/opter/data/babyAI/textual_v2/'    
-#    dataTrain_filename = dataPath+'/BABYAI_gray_10000x2obj_32x32.color-size-location-shape.train.3gram.vmat'
-#    dataValid_filename = dataPath+'/BABYAI_gray_5000x2obj_32x32.color-size-location-shape.valid.3gram.vmat'
-#    dataTest_filename = dataPath+'/BABYAI_gray_5000x2obj_32x32.color-size-location-shape.test.3gram.vmat'
-
-
     if os.path.isfile(learner_filename) == False:
        raise TypeError, "ERROR : Learner file cannot be find\n\tCould not find file "+learner_filename
     learner = loadModuleLearner(learner_filename)
     learner_nickname = 'DBN-2-2-1_'+"_".join(ports_list).replace(".","")
 
-
-
-
-
     result_dir = os.path.dirname(learner_filename)
     output_filename = result_dir+'/SVM_results_'+"_"+learner_nickname+"-"+os.path.basename(dataTrain_filename).replace(".vmat","").replace(".amat","")
 
@@ -60,7 +49,7 @@
 	else:
 	   normalize(globals()[typeDataSet+'_outputs'],mean,std)
 
-    E=discr_power_SVM_eval()
+    E=SVM()
     
     print "Writing results in "+output_filename
     if os.path.isfile(output_filename):
@@ -83,19 +72,19 @@
     FID.write('--------\n')
 
     E.save_filename = output_filename
-    E.valid_and_compute_accuracy( 'LINEAR' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    E.train_and_tune( 'LINEAR' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
     FID = open(output_filename, 'a')
     FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
-    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.write('BEST ERROR RATE: '+str(E.valid_error_rate)+' (valid) - '+str(E.error_rate)+' (test) for '+str(E.best_parameters)+'\n')
     FID.close()
-    E.valid_and_compute_accuracy( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    E.train_and_tune( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
     FID = open(output_filename, 'a')
     FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
-    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.write('BEST ERROR RATE: '+str(E.valid_error_rate)+' (valid) - '+str(E.error_rate)+' (test) for '+str(E.best_parameters)+'\n')
     FID.close()
-    E.valid_and_compute_accuracy( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
+    E.train_and_tune( 'RBF' ,     [[Train_outputs,Train_targets], [Valid_outputs,Valid_targets], [Test_outputs,Test_targets]])
     FID = open(output_filename, 'a')
     FID.write("Tried parameters : "+str(E.tried_parameters)+'\n')
-    FID.write('BEST ACCURACY: '+str(E.valid_accuracy)+' (valid) - '+str(E.accuracy)+' (test) for '+str(E.best_parameters)+'\n')
+    FID.write('BEST ERROR RATE: '+str(E.valid_error_rate)+' (valid) - '+str(E.error_rate)+' (test) for '+str(E.best_parameters)+'\n')
     FID.close()
     print "Results written in "+output_filename



From tihocan at mail.berlios.de  Thu Jun 28 22:03:40 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Jun 2007 22:03:40 +0200
Subject: [Plearn-commits] r7667 - trunk/plearn_learners/online
Message-ID: <200706282003.l5SK3e9N021340@sheep.berlios.de>

Author: tihocan
Date: 2007-06-28 22:03:39 +0200 (Thu, 28 Jun 2007)
New Revision: 7667

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Commented out all cout, that seemed to be undesired debug output

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-28 19:59:01 UTC (rev 7666)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-28 20:03:39 UTC (rev 7667)
@@ -795,20 +795,20 @@
         {
             sampleVisibleGivenHidden(*hidden_sample);
             Gibbs_step=0;
-            cout << "sampling visible from hidden" << endl;
+            //cout << "sampling visible from hidden" << endl;
         }
         else if (visible_sample && !visible_sample->isEmpty()) // if an input is provided, sample hidden conditionally
         {
             sampleHiddenGivenVisible(*visible_sample);
             Gibbs_step=0;
-            cout << "sampling hidden from (discrete) visible" << endl;
+            //cout << "sampling hidden from (discrete) visible" << endl;
         }
         else if (visible && !visible->isEmpty()) // if an input is provided, sample hidden conditionally
         {
             visible_layer->generateSamples();
             sampleHiddenGivenVisible(visible_layer->samples);
             Gibbs_step=0;
-            cout << "sampling hidden from visible expectation" << endl;
+            //cout << "sampling hidden from visible expectation" << endl;
         }
         else if (visible_expectation && !visible_expectation->isEmpty()) 
         {
@@ -820,13 +820,13 @@
             // start or continue the chain
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
-            cout << "Gibbs sampling " << Gibbs_step+1;
+            //cout << "Gibbs sampling " << Gibbs_step+1;
             for (;Gibbs_step<min_n;Gibbs_step++)
             {
                 sampleHiddenGivenVisible(visible_layer->samples);
                 sampleVisibleGivenHidden(hidden_layer->samples);
             }
-              cout << " -> " << Gibbs_step << endl;
+              //cout << " -> " << Gibbs_step << endl;
         }
 
         if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
@@ -1015,6 +1015,7 @@
 
     if (!found_a_valid_configuration)
     {
+        /*
         if (visible)
         cout << "visible_empty : "<< (bool) visible->isEmpty() << endl;
         if (hidden)
@@ -1025,6 +1026,7 @@
         cout << "hidden_sample_empty : "<< (bool) hidden_sample->isEmpty() << endl;
         if (visible_expectation)
         cout << "visible_expectation_empty : "<< (bool) visible_expectation->isEmpty() << endl;
+        */
         PLERROR("In RBMModule::fprop - Unknown port configuration for module %s", name.c_str());
     }
 



From nouiz at mail.berlios.de  Fri Jun 29 17:59:13 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Jun 2007 17:59:13 +0200
Subject: [Plearn-commits] r7668 - trunk/python_modules/plearn/learners
Message-ID: <200706291559.l5TFxDOt008630@sheep.berlios.de>

Author: nouiz
Date: 2007-06-29 17:59:12 +0200 (Fri, 29 Jun 2007)
New Revision: 7668

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
removed some code duplication


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2007-06-28 20:03:39 UTC (rev 7667)
+++ trunk/python_modules/plearn/learners/SVM.py	2007-06-29 15:59:12 UTC (rev 7668)
@@ -302,12 +302,7 @@
 def do_simple_validation(train_samples, train_targets, test_samples, test_targets, param):
     train_problem = svm_problem( train_targets, train_samples )
     model = svm_model(train_problem, param)
-    N = len(test_samples)
-    total_correct = 0
-    for i in range(N):
-        if model.predict(test_samples[i]) == test_targets[i]:
-           total_correct = total_correct + 1
-    return ( ( N - total_correct )*1. / N)
+    test_model(model,test_samples,test_targets)
 
 
 



From nouiz at mail.berlios.de  Fri Jun 29 19:00:43 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Jun 2007 19:00:43 +0200
Subject: [Plearn-commits] r7669 - trunk/plearn/vmat
Message-ID: <200706291700.l5TH0h5v015309@sheep.berlios.de>

Author: nouiz
Date: 2007-06-29 19:00:41 +0200 (Fri, 29 Jun 2007)
New Revision: 7669

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
Added the option train_set that will be used to compute the % of missing value


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2007-06-29 15:59:12 UTC (rev 7668)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2007-06-29 17:00:41 UTC (rev 7669)
@@ -128,6 +128,10 @@
         "DEPRECATED (use 'source' instead) - The data set with all variables\n"
         "to select the columns from.");
 
+    declareOption(ol, "train_set", &VariableDeletionVMatrix::train_set, OptionBase::buildoption,
+                  "The train set in which to compute the percentage of missing values.\n"
+                  "If null, will use the source to compute the percentage of missing values.");
+
     declareOption(ol, "deletion_threshold",
                   &VariableDeletionVMatrix::deletion_threshold,
                   OptionBase::learntoption,
@@ -196,6 +200,7 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(complete_dataset, copies);
+    deepCopyField(train_set, copies);
 }
 
 ////////////
@@ -223,11 +228,14 @@
                      "you should instead use 'max_constant_threshold'");
         max_constant_threshold = remove_columns_with_constant_value == 0 ? 0:1;
     }
-
-    VMat the_train_source = source;
+    if(!source)
+        PLERROR("In VariableDeletionVMatrix::build_ - The source VMat do not exist!");
+    if(!train_set)
+        train_set = source;
+    VMat the_train_source = train_set;
     if (number_of_train_samples > 0 &&
-        number_of_train_samples < source->length())
-        the_train_source = new SubVMatrix(source, 0, 0,
+        number_of_train_samples < train_set->length())
+        the_train_source = new SubVMatrix(train_set, 0, 0,
                                           number_of_train_samples,
                                           source->width());
         



From nouiz at mail.berlios.de  Fri Jun 29 19:08:24 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Jun 2007 19:08:24 +0200
Subject: [Plearn-commits] r7670 - trunk/plearn/vmat
Message-ID: <200706291708.l5TH8Ow3025314@sheep.berlios.de>

Author: nouiz
Date: 2007-06-29 19:08:22 +0200 (Fri, 29 Jun 2007)
New Revision: 7670

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.h
Log:
Add the modified header


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.h
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.h	2007-06-29 17:00:41 UTC (rev 7669)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.h	2007-06-29 17:08:22 UTC (rev 7670)
@@ -59,6 +59,7 @@
     real    min_non_missing_threshold;
     real    max_constant_threshold;
     int     number_of_train_samples;
+    VMat    train_set;
 
     // Deprecated.
     VMat       complete_dataset;



From lamblin at mail.berlios.de  Fri Jun 29 19:23:31 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 29 Jun 2007 19:23:31 +0200
Subject: [Plearn-commits] r7671 - trunk/plearn_learners/online
Message-ID: <200706291723.l5THNVjR012838@sheep.berlios.de>

Author: lamblin
Date: 2007-06-29 19:23:30 +0200 (Fri, 29 Jun 2007)
New Revision: 7671

Modified:
   trunk/plearn_learners/online/RBMConv2DConnection.cc
Log:
Normalize learning rate by batch size in update(...)


Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-29 17:08:22 UTC (rev 7670)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-29 17:23:30 UTC (rev 7671)
@@ -370,6 +370,8 @@
     PLASSERT( neg_down_values.length() == batch_size );
     PLASSERT( neg_up_values.length() == batch_size );
 
+    real norm_lr = learning_rate / batch_size;
+
     /*  for i=0 to up_image_length:
      *   for j=0 to up_image_width:
      *     for l=0 to kernel_length:
@@ -412,7 +414,7 @@
                                                    pdv2+=down_image_width,
                                                    ndv2+=down_image_width )
                         for( int m=0; m<kernel_width; m++ )
-                            k[m] += learning_rate *
+                            k[m] += norm_lr *
                                 (ndv2[m] * nuv_ij - pdv2[m] * puv_ij);
                 }
             }



From tihocan at mail.berlios.de  Fri Jun 29 19:25:26 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 29 Jun 2007 19:25:26 +0200
Subject: [Plearn-commits] r7672 - trunk/plearn/base
Message-ID: <200706291725.l5THPQDq022203@sheep.berlios.de>

Author: tihocan
Date: 2007-06-29 19:25:25 +0200 (Fri, 29 Jun 2007)
New Revision: 7672

Modified:
   trunk/plearn/base/Object.cc
Log:
Changed mechanism to create a dummy object when parsing ignored options: the object is not built anymore, to ensure there is no crash when the build does not work with the default options

Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2007-06-29 17:23:30 UTC (rev 7671)
+++ trunk/plearn/base/Object.cc	2007-06-29 17:25:25 UTC (rev 7672)
@@ -529,10 +529,13 @@
             if (it!=options.end() && (*it)->shouldBeSkipped() ) {
                 // Create a dummy object that will read this option.
                 if (!dummy_obj) {
-                    dummy_obj = new Object();
-                    string dummy_string = this->classname() + "()";
-                    PStream dummy_in = openString(dummy_string, PStream::plearn_ascii);
-                    dummy_in >> dummy_obj;
+                    // Note that we do not call build on 'dummy_obj'. This is
+                    // because some classes may crash when build is called
+                    // before setting options (though this is not a desired
+                    // behaviour, it can be hard to figure out what is going on
+                    // when it crashes here).
+                    dummy_obj =
+                        TypeFactory::instance().newObject(this->classname());
                 }
                 dummy_obj->readOptionVal(in, optionname);
             }



From lamblin at mail.berlios.de  Fri Jun 29 19:49:21 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 29 Jun 2007 19:49:21 +0200
Subject: [Plearn-commits] r7673 - trunk/plearn_learners/online
Message-ID: <200706291749.l5THnLVW029749@sheep.berlios.de>

Author: lamblin
Date: 2007-06-29 19:49:21 +0200 (Fri, 29 Jun 2007)
New Revision: 7673

Modified:
   trunk/plearn_learners/online/RBMConv2DConnection.cc
Log:
Fix weight initialization


Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-29 17:25:25 UTC (rev 7672)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-29 17:49:21 UTC (rev 7673)
@@ -648,7 +648,7 @@
             return;
         }
 
-        real d = 1. / max( down_size, up_size );
+        real d = 1. / max( kernel_length, kernel_width );
         if( initialization_method == "uniform_sqrt" )
             d = sqrt( d );
 



From larocheh at mail.berlios.de  Fri Jun 29 20:03:28 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 29 Jun 2007 20:03:28 +0200
Subject: [Plearn-commits] r7674 - trunk/plearn_learners_experimental
Message-ID: <200706291803.l5TI3S77030612@sheep.berlios.de>

Author: larocheh
Date: 2007-06-29 20:03:27 +0200 (Fri, 29 Jun 2007)
New Revision: 7674

Added:
   trunk/plearn_learners_experimental/StackedSVDNet.cc
   trunk/plearn_learners_experimental/StackedSVDNet.h
Log:
Deep Net using SVDs in parameter space to stack layers...


Added: trunk/plearn_learners_experimental/StackedSVDNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-06-29 17:49:21 UTC (rev 7673)
+++ trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-06-29 18:03:27 UTC (rev 7674)
@@ -0,0 +1,681 @@
+// -*- C++ -*-
+
+// StackedSVDNet.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedSVDNet.cc */
+
+
+#define PL_LOG_MODULE_NAME "StackedSVDNet"
+#include <plearn/io/pl_log.h>
+
+#include "StackedSVDNet.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    StackedSVDNet,
+    "Neural net, initialized with SVDs of logistic auto-regressions.",
+    ""
+    );
+
+StackedSVDNet::StackedSVDNet() :
+    greedy_learning_rate( 0. ),
+    greedy_decrease_ct( 0. ),
+    fine_tuning_learning_rate( 0. ),
+    fine_tuning_decrease_ct( 0. ),
+    batch_size(50),
+    minimum_relative_improvement(1e-3),
+    n_layers( 0 ),
+    currently_trained_layer( 0 )
+{
+    // random_gen will be initialized in PLearner::build_()
+    random_gen = new PRandom();
+    nstages = 0;
+}
+
+void StackedSVDNet::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "greedy_learning_rate", 
+                  &StackedSVDNet::greedy_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the logistic auto-regression "
+                  "gradient descent training"
+        );
+    
+    declareOption(ol, "greedy_decrease_ct", 
+                  &StackedSVDNet::greedy_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during the "
+                  "logistic auto-regression gradient descent training. "
+        );
+
+    declareOption(ol, "fine_tuning_learning_rate", 
+                  &StackedSVDNet::fine_tuning_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the fine tuning gradient descent");
+
+    declareOption(ol, "fine_tuning_decrease_ct", 
+                  &StackedSVDNet::fine_tuning_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "fine tuning\n"
+                  "gradient descent.\n");
+
+    declareOption(ol, "batch_size", 
+                  &StackedSVDNet::batch_size,
+                  OptionBase::buildoption,
+                  "Size of mini-batch for gradient descent");
+
+    declareOption(ol, "minimum_relative_improvement", 
+                  &StackedSVDNet::minimum_relative_improvement,
+                  OptionBase::buildoption,
+                  "Minimum relative improvement convergence criteria \n"
+                  "for the logistic auto-regression.");
+
+    declareOption(ol, "layers", &StackedSVDNet::layers,
+                  OptionBase::buildoption,
+                  "The layers of units in the network. The first element\n"
+                  "of this vector should be the input layer and the\n"
+                  "subsequent elements should be the hidden layers. The\n"
+                  "should not be included in this layer.\n");
+
+    declareOption(ol, "final_module", &StackedSVDNet::final_module,
+                  OptionBase::buildoption,
+                  "Module that takes as input the output of the last layer\n"
+                  "(layers[n_layers-1), and feeds its output to final_cost\n"
+                  "which defines the fine-tuning criteria.\n"
+                 );
+
+    declareOption(ol, "final_cost", &StackedSVDNet::final_cost,
+                  OptionBase::buildoption,
+                  "The cost function to be applied on top of the neural network\n"
+                  "(i.e. at the output of final_module). Its gradients will be \n"
+                  "backpropagated to final_module and then backpropagated to\n"
+                  "the layers.\n"
+                  );
+
+    declareOption(ol, "connections", &StackedSVDNet::connections,
+                  OptionBase::learntoption,
+                  "The weights of the connections between the layers");
+
+    declareOption(ol, "n_layers", &StackedSVDNet::n_layers,
+                  OptionBase::learntoption,
+                  "Number of layers");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void StackedSVDNet::build_()
+{
+
+    MODULE_LOG << "build_() called" << endl;
+
+    if(inputsize_ > 0 && targetsize_ > 0)
+    {
+        // Initialize some learnt variables
+        n_layers = layers.length();
+        
+        if( weightsize_ > 0 )
+            PLERROR("StackedSVDNet::build_() - \n"
+                    "usage of weighted samples (weight size > 0) is not\n"
+                    "implemented yet.\n");
+        
+        if(layers[0]->size != inputsize_)
+            PLERROR("StackedSVDNet::build_layers_and_connections() - \n"
+                    "layers[0] should have a size of %d.\n",
+                    inputsize_);
+    
+        activations.resize( n_layers );
+        expectations.resize( n_layers );
+        activation_gradients.resize( n_layers );
+        expectation_gradients.resize( n_layers );
+
+        for( int i=0 ; i<n_layers ; i++ )
+        {
+            if( !(layers[i]->random_gen) )
+            {
+                layers[i]->random_gen = random_gen;
+                layers[i]->forget();
+            }
+
+            if(i>0 && layers[i]->size > layers[i-1]->size)
+                PLERROR("In StackedSVDNet::build()_: "
+                    "layers must have decreasing sizes from bottom to top.");
+                
+            activations[i].resize( batch_size, layers[i]->size );
+            expectations[i].resize( batch_size, layers[i]->size );
+            activation_gradients[i].resize( batch_size, layers[i]->size );
+            expectation_gradients[i].resize( batch_size, layers[i]->size );
+        }
+
+        if( !final_cost )
+            PLERROR("StackedSVDNet::build_costs() - \n"
+                    "final_cost should be provided.\n");
+
+        final_cost_gradient.resize( final_cost->input_size );
+        final_cost->setLearningRate( fine_tuning_learning_rate );
+
+        if( !(final_cost->random_gen) )
+        {
+            final_cost->random_gen = random_gen;
+            final_cost->forget();
+        }
+
+        if( !final_module )
+            PLERROR("StackedSVDNet::build_costs() - \n"
+                    "final_module should be provided.\n");
+    
+        if( layers[n_layers-1]->size != final_module->input_size )
+            PLERROR("StackedSVDNet::build_costs() - \n"
+                    "final_module should have an input_size of %d.\n", 
+                    layers[n_layers-1]->size);
+    
+        if( final_module->output_size != final_cost->input_size )
+            PLERROR("StackedSVDNet::build_costs() - \n"
+                    "final_module should have an output_size of %d.\n", 
+                    final_module->input_size);
+
+        final_module->setLearningRate( fine_tuning_learning_rate );
+
+        if( !(final_module->random_gen) )
+        {
+            final_module->random_gen = random_gen;
+            final_module->forget();
+        }
+
+
+        if(targetsize_ != 1)
+            PLERROR("StackedSVDNet::build_costs() - \n"
+                    "target size of %d is not supported.\n", targetsize_);    
+    }
+}
+
+// ### Nothing to add here, simply calls build_
+void StackedSVDNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void StackedSVDNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(, copies);
+
+    deepCopyField(training_schedule, copies);
+    deepCopyField(layers, copies);
+    deepCopyField(connections, copies);
+    deepCopyField(reconstruction_connections, copies);
+    deepCopyField(final_module, copies);
+    deepCopyField(final_cost, copies);
+    deepCopyField(partial_costs, copies);
+    deepCopyField(partial_costs_weights, copies);
+    deepCopyField(activations, copies);
+    deepCopyField(expectations, copies);
+    deepCopyField(activation_gradients, copies);
+    deepCopyField(expectation_gradients, copies);
+    deepCopyField(reconstruction_activations, copies);
+    deepCopyField(reconstruction_expectations, copies);
+    deepCopyField(reconstruction_activation_gradients, copies);
+    deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(partial_costs_positions, copies);
+    deepCopyField(partial_cost_value, copies);
+    deepCopyField(final_cost_input, copies);
+    deepCopyField(final_cost_value, copies);
+    deepCopyField(final_cost_gradient, copies);
+    deepCopyField(greedy_stages, copies);
+    
+    PLERROR("In StackedSVDNet::makeDeepCopyFromShallowCopy(): "
+            "not implemented yet.");
+}
+
+
+int StackedSVDNet::outputsize() const
+{
+    return final_module->output_size;
+}
+
+void StackedSVDNet::forget()
+{
+    inherited::forget();
+
+    connections.resize(0);
+    
+    final_module->forget();
+    final_cost->forget();
+
+    stage = 0;
+}
+
+void StackedSVDNet::train()
+{
+    MODULE_LOG << "train() called " << endl;
+    MODULE_LOG << "  training_schedule = " << training_schedule << endl;
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    real weight; // unused
+
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set->length();
+    int sample;
+
+    PP<ProgressBar> pb;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+    real lr = 0;
+    int init_stage;
+
+    /***** initial greedy training *****/
+    if(stage == 0)
+    {
+        connections.resize(n_layers-1);
+        TVec< Vec > biases(n_layers-1);
+        for( int i=0 ; i<n_layers-1 ; i++ )
+        {
+            MODULE_LOG << "Training connection weights between layers " << i
+                       << " and " << i+1 << endl;
+
+            connections[i] = new RBMMatrixConnection();
+            connections[i]->up_size = layers[i]->size;
+            connections[i]->down_size = layers[i]->size;
+            connections[i]->build();
+            for(int j=0; j < layers[i]->size; j++)
+                connections[i]->weights(j,j) = 0;
+
+            lr = greedy_learning_rate;
+            layers[i]->setLearningRate( lr );
+            connections[i]->setLearningRate( lr );
+            layers[i+1]->setLearningRate( lr );
+
+            real cost = 30;
+            real last_cost = 100;
+            int nupdates = 0;
+            int nepochs = 0;
+            while( nepochs < 2 ||
+                   (last_cost - cost) / last_cost >= minimum_relative_improvement )
+            {
+                train_stats->forget();
+                for(int sample = 0; sample < train_set.length(); sample++)
+                {
+                    if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+                    {
+                        lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                                   * nupdates);
+                        layers[i]->setLearningRate( lr );
+                        connections[i]->setLearningRate( lr );
+                        reconstruction_connections[i]->setLearningRate( lr );
+                        layers[i+1]->setLearningRate( lr );                
+                    }
+
+                    train_set->getExample(sample, input, target, weight);
+                    greedyStep( input, target, sample, train_costs );
+                    nupdates++;
+                    train_stats->update( train_costs );
+                }
+                train_stats->finalize();
+                nepochs++;
+                last_cost = cost;
+                cost = train_stats->mean()[0];
+            }
+            Mat A,U,S,Vt;
+            A.resize(layers[i]->size,layers[i]->size+1);
+            A.column(0) << layers[i]->bias;
+            A.subMat(0,1,layers[i]->size,layers[i]->size) << 
+                connections[i]->weights;
+            SVD(connections[i]->weights,U,S,V);
+            connections[i]->up_size = layers[i+1]->size;
+            connections[i]->down_size = layers[i]->size;
+            connections[i]->build();
+            connection[i]->weights << Vt.subRows(0,layers[i+1]->size);
+            biases[i].resize(layers[i+1]->size);
+            biases[i] << Vt.column(0).subVec(0,layers[i+1]->size);
+            for(int j=0; j<connections[i]->up_size; j++)
+            {
+                connections[i]->weights(j) *= S(j,j);
+                biases[i][j] *= S(j,j);
+            }
+        }
+        stage++;
+        for(int i=0; i<biases.length(); i++)
+        {
+            layers[i]->bias << biases[i];
+        }
+    }
+
+    /***** fine-tuning by gradient descent *****/
+    if( stage < nstages )
+    {
+
+        MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  nstages = " << nstages << endl;
+        MODULE_LOG << "  fine_tuning_learning_rate = " << 
+            fine_tuning_learning_rate << endl;
+
+        init_stage = stage;
+        if( report_progress && stage < nstages )
+            pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                                  + classname(),
+                                  nstages - init_stage );
+
+        setLearningRate( fine_tuning_learning_rate );
+        train_costs.fill(MISSING_VALUE);
+        for( ; stage<nstages ; stage++ )
+        {
+            sample = stage % nsamples;
+            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                setLearningRate( fine_tuning_learning_rate
+                                 / (1. + fine_tuning_decrease_ct * stage ) );
+
+            train_set->getExample( sample, input, target, weight );
+            fineTuningStep( input, target, train_costs );
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( stage - init_stage + 1 );
+        }
+    }
+    
+    train_stats->finalize();
+}
+
+void StackedSVDNet::greedyStep( const Vec& input, const Vec& target, int index, Vec train_costs )
+{
+    PLASSERT( index < n_layers );
+
+    expectations[0] << input;
+    for( int i=0 ; i<index + 1; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+
+    reconstruction_connections[ index ]->fprop( expectations[ index + 1],
+                                                reconstruction_activations);
+    layers[ index ]->fprop( reconstruction_activations,
+                            layers[ index ]->expectation);
+    
+    layers[ index ]->expectation_is_up_to_date = true;
+    train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
+
+    layers[ index ]->bpropNLL(expectations[index], train_costs[index],
+                                  reconstruction_activation_gradients);
+
+    layers[ index ]->update(reconstruction_activation_gradients);
+
+    // // This is a bad update! Propagates gradient through sigmoid again!
+    // layers[ index ]->bpropUpdate( reconstruction_activations, 
+    //                                   layers[ index ]->expectation,
+    //                                   reconstruction_activation_gradients,
+    //                                   reconstruction_expectation_gradients);
+
+    reconstruction_connections[ index ]->bpropUpdate( 
+        expectations[ index + 1], 
+        reconstruction_activations, 
+        reconstruction_expectation_gradients, //reused
+        reconstruction_activation_gradients);
+
+    if(!fast_exact_is_equal(l1_neuron_decay,0))
+    {
+        // Compute L1 penalty gradient on neurons
+        real* hid = expectations[ index + 1 ].data();
+        real* grad = reconstruction_expectation_gradients.data();
+        int len = expectations[ index + 1 ].length();
+        for(int i=0; i<len; i++)
+        {
+            if(*hid > l1_neuron_decay_center)
+                *grad -= l1_neuron_decay;
+            else if(*hid < l1_neuron_decay_center)
+                *grad += l1_neuron_decay;
+            hid++;
+            grad++;
+        }
+    }
+
+    // Update hidden layer bias and weights
+    layers[ index+1 ]->bpropUpdate( activations[ index + 1 ],
+                                    expectations[ index + 1 ],
+                                    reconstruction_activation_gradients, // reused
+                                    reconstruction_expectation_gradients);    
+
+    connections[ index ]->bpropUpdate( 
+        expectations[ index ],
+        activations[ index + 1 ],
+        reconstruction_expectation_gradients, //reused
+        reconstruction_activation_gradients);
+
+    // Set diagonal to zero!!!
+}
+
+void StackedSVDNet::fineTuningStep( const Vec& input, const Vec& target,
+                                    Vec& train_costs )
+{
+    // fprop
+    expectations[0] << input;
+    for( int i=0 ; i<n_layers-1; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+
+    final_module->fprop( expectations[ n_layers-1 ],
+                         final_cost_input );
+    final_cost->fprop( final_cost_input, target, final_cost_value );
+
+    train_costs.subVec(train_costs.length()-final_cost_value.length(),
+                       final_cost_value.length()) <<
+        final_cost_value;
+
+    final_cost->bpropUpdate( final_cost_input, target,
+                             final_cost_value[0],
+                             final_cost_gradient );
+    final_module->bpropUpdate( expectations[ n_layers-1 ],
+                               final_cost_input,
+                               expectation_gradients[ n_layers-1 ],
+                               final_cost_gradient );
+
+    for( int i=n_layers-1 ; i>0 ; i-- )
+    {
+        layers[i]->bpropUpdate( activations[i],
+                                expectations[i],
+                                activation_gradients[i],
+                                expectation_gradients[i] );
+
+        connections[i-1]->bpropUpdate( expectations[i-1],
+                                       activations[i],
+                                       expectation_gradients[i-1],
+                                       activation_gradients[i] );
+    }
+}
+
+void StackedSVDNet::computeOutput(const Vec& input, Vec& output) const
+{
+    // fprop
+
+    expectations[0] << input;
+
+    for(int i=0 ; i<currently_trained_layer-1 ; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+
+    if( currently_trained_layer<n_layers )
+    {
+        connections[currently_trained_layer-1]->fprop( 
+            expectations[currently_trained_layer-1], 
+            activations[currently_trained_layer] );
+        layers[currently_trained_layer]->fprop(
+            activations[currently_trained_layer],
+            output);
+    }
+    else        
+        final_module->fprop( expectations[ currently_trained_layer - 1],
+                             output );
+}
+
+void StackedSVDNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    //Assumes that computeOutput has been called
+
+    costs.resize( getTestCostNames().length() );
+    costs.fill( MISSING_VALUE );
+
+    if(compute_all_test_costs)
+    {
+        for(int i=0; i<currently_trained_layer-1; i++)
+        {
+            reconstruction_connections[ i ]->fprop( expectations[ i+1 ],
+                                                    reconstruction_activations);
+            layers[ i ]->fprop( reconstruction_activations,
+                                    layers[ i ]->expectation);
+            
+            layers[ i ]->expectation_is_up_to_date = true;
+            costs[i] = layers[ i ]->fpropNLL(expectations[ i ]);
+            
+            if( partial_costs && partial_costs[i])
+            {
+                partial_costs[ i ]->fprop( expectations[ i + 1],
+                                           target, partial_cost_value );
+                costs.subVec(partial_costs_positions[i],
+                             partial_cost_value.length()) << 
+                    partial_cost_value;
+            }
+        }
+    }
+
+    if( currently_trained_layer<n_layers )
+    {
+        reconstruction_connections[ currently_trained_layer-1 ]->fprop( 
+            output,
+            reconstruction_activations);
+        layers[ currently_trained_layer-1 ]->fprop( 
+            reconstruction_activations,
+            layers[ currently_trained_layer-1 ]->expectation);
+        
+        layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
+        costs[ currently_trained_layer-1 ] = 
+            layers[ currently_trained_layer-1 ]->fpropNLL(
+                expectations[ currently_trained_layer-1 ]);
+
+        if( partial_costs && partial_costs[ currently_trained_layer-1 ] )
+        {
+            partial_costs[ currently_trained_layer-1 ]->fprop( 
+                output,
+                target, partial_cost_value );
+            costs.subVec(partial_costs_positions[currently_trained_layer-1],
+                         partial_cost_value.length()) << partial_cost_value;
+        }
+    }
+    else
+    {
+        final_cost->fprop( output, target, final_cost_value );        
+        costs.subVec(costs.length()-final_cost_value.length(),
+                     final_cost_value.length()) <<
+            final_cost_value;
+    }
+}
+
+TVec<string> StackedSVDNet::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    TVec<string> cost_names(0);
+
+    for( int i=0; i<layers.size()-1; i++)
+        cost_names.push_back("reconstruction_error_" + tostring(i+1));
+    
+    for( int i=0 ; i<partial_costs.size() ; i++ )
+    {
+        TVec<string> cost_names = partial_costs[i]->name();
+        for(int j=0; j<cost_names.length(); j++)
+            cost_names.push_back("partial_cost_" + tostring(i+1) + "_" + 
+                cost_names[j]);
+    }
+
+    cost_names.append( final_cost->name() );
+
+    return cost_names;
+}
+
+TVec<string> StackedSVDNet::getTrainCostNames() const
+{
+    return getTestCostNames() ;    
+}
+
+
+//#####  Helper functions  ##################################################
+
+void StackedSVDNet::setLearningRate( real the_learning_rate )
+{
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        layers[i]->setLearningRate( the_learning_rate );
+        connections[i]->setLearningRate( the_learning_rate );
+    }
+    layers[n_layers-1]->setLearningRate( the_learning_rate );
+
+    final_cost->setLearningRate( fine_tuning_learning_rate );
+    final_module->setLearningRate( fine_tuning_learning_rate );
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/StackedSVDNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.h	2007-06-29 17:49:21 UTC (rev 7673)
+++ trunk/plearn_learners_experimental/StackedSVDNet.h	2007-06-29 18:03:27 UTC (rev 7674)
@@ -0,0 +1,261 @@
+// -*- C++ -*-
+
+// StackedAutoassociatorsNet.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedAutoassociatorsNet.h */
+
+
+#ifndef StackedAutoassociatorsNet_INC
+#define StackedAutoassociatorsNet_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn_learners/online/RBMMatrixConnection.h>
+#include <plearn/misc/PTimer.h>
+
+namespace PLearn {
+
+/**
+ * Neural net, initialized with SVDs of logistic auto-regressions.
+ */
+class StackedAutoassociatorsNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The learning rate used during the logistic auto-regression 
+    //! gradient descent training
+    real greedy_learning_rate;
+
+    //! The decrease constant of the learning rate used during the 
+    //! logistic auto-regression gradient descent training. 
+    real greedy_decrease_ct;
+
+    //! The learning rate used during the fine tuning gradient descent
+    real fine_tuning_learning_rate;
+
+    //! The decrease constant of the learning rate used during fine tuning
+    //! gradient descent
+    real fine_tuning_decrease_ct;
+
+    //! Size of mini-batch for gradient descent
+    int batch_size;
+    
+    //! Minimum relative improvement convergence criteria
+    //! for the logistic auto-regression.
+    real minimum_relative_improvement;
+    
+    //! The layers of units in the network
+    TVec< PP<RBMLayer> > layers;
+
+    //! Module that takes as input the output of the last layer
+    //! (layers[n_layers-1), and feeds its output to final_cost
+    //! which defines the fine-tuning criteria.
+    PP<OnlineLearningModule> final_module;
+
+    //! The cost function to be applied on top of the neural network
+    //! (i.e. at the output of final_module). Its gradients will be 
+    //! backpropagated to final_module and then backpropagated to
+    //! the layers.
+    PP<CostModule> final_cost;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! The weights of the connections between the layers
+    TVec< PP<RBMMatrixConnection> > connections;
+
+    //! Number of layers
+    int n_layers;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    StackedAutoassociatorsNet();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    void greedyStep( const Vec& input, const Vec& target, int index, 
+                     Vec train_costs );
+
+    void fineTuningStep( const Vec& input, const Vec& target,
+                         Vec& train_costs );
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(StackedAutoassociatorsNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    //! Stores the activations of the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Mat> activations;
+
+    //! Stores the expectations of the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Mat> expectations;
+
+    //! Stores the gradient of the cost wrt the activations of 
+    //! the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Mat> activation_gradients;
+
+    //! Stores the gradient of the cost wrt the expectations of 
+    //! the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Mat> expectation_gradients;
+
+    //! Reconstruction activations
+    mutable Mat reconstruction_activations;
+    
+    //! Reconstruction expectations
+    mutable Mat reconstruction_expectations;
+    
+    //! Reconstruction activations
+    mutable Mat reconstruction_activation_gradients;
+    
+    //! Reconstruction expectations
+    mutable Mat reconstruction_expectation_gradients;
+
+    //! Input of the final_cost
+    mutable Vec final_cost_input;
+
+    //! Cost value of final_cost
+    mutable Vec final_cost_value;
+
+    //! Stores the gradient of the cost at the input of final_cost
+    mutable Vec final_cost_gradient;
+
+    //! Currently trained layer (1 means the first hidden layer,
+    //! n_layers means the output layer)
+    int currently_trained_layer;
+
+    //! Indication whether final_module has learning rate
+    bool final_module_has_learning_rate;
+    
+    //! Indication whether final_cost has learning rate
+    bool final_cost_has_learning_rate;
+    
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_classification_cost();
+
+    void build_costs();
+
+    void setLearningRate( real the_learning_rate );
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(StackedAutoassociatorsNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Fri Jun 29 21:50:49 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 29 Jun 2007 21:50:49 +0200
Subject: [Plearn-commits] r7675 - trunk/plearn/ker
Message-ID: <200706291950.l5TJonJA004616@sheep.berlios.de>

Author: tihocan
Date: 2007-06-29 21:50:49 +0200 (Fri, 29 Jun 2007)
New Revision: 7675

Modified:
   trunk/plearn/ker/DistanceKernel.cc
Log:
Fixed wrong error condition introduced recently in DistanceKernel

Modified: trunk/plearn/ker/DistanceKernel.cc
===================================================================
--- trunk/plearn/ker/DistanceKernel.cc	2007-06-29 18:03:27 UTC (rev 7674)
+++ trunk/plearn/ker/DistanceKernel.cc	2007-06-29 19:50:49 UTC (rev 7675)
@@ -94,8 +94,9 @@
 // evaluate //
 //////////////
 real DistanceKernel::evaluate(const Vec& x1, const Vec& x2) const {
-    if (!ignore_missing && !pow_distance)
-        PLERROR("DistanceKernel::evaluate(int i, int j) ignore_missing implemented only if pow_distance is set");
+    if (ignore_missing && !pow_distance)
+        PLERROR("In DistanceKernel::evaluate(int i, int j) - 'ignore_missing' "
+                "implemented only if pow_distance is set");
 
     if (pow_distance) {
         return powdistance(x1, x2, n, ignore_missing);



From tihocan at mail.berlios.de  Fri Jun 29 22:14:45 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 29 Jun 2007 22:14:45 +0200
Subject: [Plearn-commits] r7676 - trunk/plearn/vmat
Message-ID: <200706292014.l5TKEjJp005898@sheep.berlios.de>

Author: tihocan
Date: 2007-06-29 22:14:45 +0200 (Fri, 29 Jun 2007)
New Revision: 7676

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
Fixed potential issue where 'train_set', if initially not provided, would have been set in build and potentially reused in a subsequent build

Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2007-06-29 19:50:49 UTC (rev 7675)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2007-06-29 20:14:45 UTC (rev 7676)
@@ -230,14 +230,12 @@
     }
     if(!source)
         PLERROR("In VariableDeletionVMatrix::build_ - The source VMat do not exist!");
-    if(!train_set)
-        train_set = source;
-    VMat the_train_source = train_set;
+    VMat the_train_source = train_set ? train_set : source;
     if (number_of_train_samples > 0 &&
-        number_of_train_samples < train_set->length())
-        the_train_source = new SubVMatrix(train_set, 0, 0,
+        number_of_train_samples < the_train_source->length())
+        the_train_source = new SubVMatrix(the_train_source, 0, 0,
                                           number_of_train_samples,
-                                          source->width());
+                                          the_train_source->width());
         
     TVec<StatsCollector> stats =
         PLearn::computeStats(the_train_source, -1, false);



From lamblin at mail.berlios.de  Fri Jun 29 23:49:44 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 29 Jun 2007 23:49:44 +0200
Subject: [Plearn-commits] r7677 - tags
Message-ID: <200706292149.l5TLniVU011050@sheep.berlios.de>

Author: lamblin
Date: 2007-06-29 23:49:44 +0200 (Fri, 29 Jun 2007)
New Revision: 7677

Added:
   tags/before_energy_sign_changes/
Log:
Tag before we change sign convention for energy in RBMs


Copied: tags/before_energy_sign_changes (from rev 7676, trunk)



From lamblin at mail.berlios.de  Fri Jun 29 23:59:13 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 29 Jun 2007 23:59:13 +0200
Subject: [Plearn-commits] r7678 - trunk/plearn_learners/online
Message-ID: <200706292159.l5TLxD8m011637@sheep.berlios.de>

Author: lamblin
Date: 2007-06-29 23:59:11 +0200 (Fri, 29 Jun 2007)
New Revision: 7678

Modified:
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMClassificationModule.cc
   trunk/plearn_learners/online/RBMConv2DConnection.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
Log:
Change convention for energy sign in RBMs.
Now, it is:
  E = -h' b - h' W v - c' v


Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -94,7 +94,6 @@
     cost.resize( output_size );
 
     if( input.hasMissing() )
-        // TODO: should we put something else? infinity?
         cost[0] = MISSING_VALUE;
     else
     {
@@ -153,10 +152,7 @@
         for( int i=0; i<batch_size; i++ )
         {
             if( (*prediction)(i).hasMissing() )
-            {
-                // TODO: should we put something else? infinity?
                 (*cost)(i,0) = MISSING_VALUE;
-            }
             else
             {
 #ifdef BOUNDCHECK
@@ -168,8 +164,8 @@
                 if (!is_equal( sum((*prediction)(i)), 1., 1., 1e-5, 1e-5 ))
                     PLERROR("In NLLCostModule::fprop - Elements of"
                             " \"prediction\" should sum to 1"
-                            " (found a sum = %f)",
-                            sum((*prediction)(i)));
+                            " (found a sum = %f at row %d)",
+                            sum((*prediction)(i)), i);
 #endif
                 int target_i = (int) round( (*target)(i,0) );
                 PLASSERT( is_equal( (*target)(i, 0), target_i ) );

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -115,10 +115,10 @@
 
     if (use_fast_approximations)
         for( int i=0 ; i<size ; i++ )
-            expectation[i] = fastsigmoid( -activation[i] );
+            expectation[i] = fastsigmoid( activation[i] );
     else
         for( int i=0 ; i<size ; i++ )
-            expectation[i] = sigmoid( -activation[i] );
+            expectation[i] = sigmoid( activation[i] );
 
     expectation_is_up_to_date = true;
 }
@@ -136,11 +136,11 @@
     if (use_fast_approximations)
         for (int k = 0; k < batch_size; k++)
             for (int i = 0 ; i < size ; i++)
-                expectations(k, i) = fastsigmoid(-activations(k, i));
+                expectations(k, i) = fastsigmoid(activations(k, i));
     else
         for (int k = 0; k < batch_size; k++)
             for (int i = 0 ; i < size ; i++)
-                expectations(k, i) = sigmoid(-activations(k, i));
+                expectations(k, i) = sigmoid(activations(k, i));
 
     expectations_are_up_to_date = true;
 }
@@ -155,10 +155,10 @@
 
     if (use_fast_approximations)
         for( int i=0 ; i<size ; i++ )
-            output[i] = fastsigmoid( -input[i] - bias[i] );
+            output[i] = fastsigmoid( input[i] + bias[i] );
     else
         for( int i=0 ; i<size ; i++ )
-            output[i] = sigmoid( -input[i] - bias[i] );
+            output[i] = sigmoid( input[i] + bias[i] );
 }
 
 void RBMBinomialLayer::fprop( const Mat& inputs, Mat& outputs ) const
@@ -170,11 +170,11 @@
     if (use_fast_approximations)
         for( int k = 0; k < mbatch_size; k++ )
             for( int i = 0; i < size; i++ )
-                outputs(k,i) = fastsigmoid( -inputs(k,i) - bias[i] );
+                outputs(k,i) = fastsigmoid( inputs(k,i) + bias[i] );
     else
         for( int k = 0; k < mbatch_size; k++ )
             for( int i = 0; i < size; i++ )
-                outputs(k,i) = sigmoid( -inputs(k,i) - bias[i] );
+                outputs(k,i) = sigmoid( inputs(k,i) + bias[i] );
 }
 
 void RBMBinomialLayer::fprop( const Vec& input, const Vec& rbm_bias,
@@ -186,10 +186,10 @@
 
     if (use_fast_approximations)
         for( int i=0 ; i<size ; i++ )
-            output[i] = fastsigmoid( -input[i] - rbm_bias[i]);
+            output[i] = fastsigmoid( input[i] + rbm_bias[i]);
     else
         for( int i=0 ; i<size ; i++ )
-            output[i] = sigmoid( -input[i] - rbm_bias[i]);
+            output[i] = sigmoid( input[i] + rbm_bias[i]);
 }
 
 /////////////////
@@ -221,7 +221,7 @@
     for( int i=0 ; i<size ; i++ )
     {
         real output_i = output[i];
-        real in_grad_i = - output_i * (1-output_i) * output_gradient[i];
+        real in_grad_i = output_i * (1-output_i) * output_gradient[i];
         input_gradient[i] += in_grad_i;
 
         if( momentum == 0. )
@@ -278,7 +278,7 @@
         for( int i=0 ; i<size ; i++ )
         {
             real output_i = outputs(j, i);
-            real in_grad_i = -output_i * (1-output_i) * output_gradients(j, i);
+            real in_grad_i = output_i * (1-output_i) * output_gradients(j, i);
             input_gradients(j, i) += in_grad_i;
 
             if( momentum == 0. )
@@ -317,7 +317,7 @@
     for( int i=0 ; i<size ; i++ )
     {
         real output_i = output[i];
-        input_gradient[i] = - output_i * (1-output_i) * output_gradient[i];
+        input_gradient[i] = output_i * (1-output_i) * output_gradient[i];
     }
 
     rbm_bias_gradient << input_gradient;
@@ -338,16 +338,15 @@
                 // nll -= target[i] * pl_log(expectations[i]); 
                 // but it is numerically unstable, so use instead
                 // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                // but note that expectation = sigmoid(-activation)
-                ret += target_i * tabulated_softplus(activation_i);
+                ret += target_i * tabulated_softplus(-activation_i);
             if(!fast_exact_is_equal(target_i,1.0))
                 // ret -= (1-target_i) * pl_log(1-expectation_i);
                 // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
                 //                         = log(1/(1+exp(x)))
                 //                         = -log(1+exp(x)) = -softplus(x)
-                ret += (1-target_i) * tabulated_softplus(-activation_i);
+                ret += (1-target_i) * tabulated_softplus(activation_i);
         }
-    }else{
+    } else {
         for( int i=0 ; i<size ; i++ )
         {
             target_i = target[i];
@@ -356,14 +355,13 @@
                 // nll -= target[i] * pl_log(expectations[i]); 
                 // but it is numerically unstable, so use instead
                 // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                // but note that expectation = sigmoid(-activation)
-                ret += target_i * softplus(activation_i);
+                ret += target_i * softplus(-activation_i);
             if(!fast_exact_is_equal(target_i,1.0))
                 // ret -= (1-target_i) * pl_log(1-expectation_i);
                 // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
                 //                         = log(1/(1+exp(x)))
                 //                         = -log(1+exp(x)) = -softplus(x)
-                ret += (1-target_i) * softplus(-activation_i);
+                ret += (1-target_i) * softplus(activation_i);
         }
     }
     return ret;
@@ -390,33 +388,30 @@
                     // nll -= target[i] * pl_log(expectations[i]); 
                     // but it is numerically unstable, so use instead
                     // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                    // but note that expectation = sigmoid(-activation)
-                    nll += target[i] * tabulated_softplus(activation[i]);
+                    nll += target[i] * tabulated_softplus(-activation[i]);
                 if(!fast_exact_is_equal(target[i],1.0))
                     // nll -= (1-target[i]) * pl_log(1-output[i]);
                     // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
                     //                         = log(1/(1+exp(x)))
                     //                         = -log(1+exp(x))
                     //                         = -softplus(x)
-                    nll += (1-target[i]) * tabulated_softplus(-activation[i]);
+                    nll += (1-target[i]) * tabulated_softplus(activation[i]);
             }
-        }else{
+        } else {
             for( int i=0 ; i<size ; i++ ) // loop over outputs
             {
                 if(!fast_exact_is_equal(target[i],0.0))
                     // nll -= target[i] * pl_log(expectations[i]); 
                     // but it is numerically unstable, so use instead
                     // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                    // but note that expectation = sigmoid(-activation)
-                    nll += target[i] * softplus(activation[i]);
+                    nll += target[i] * softplus(-activation[i]);
                 if(!fast_exact_is_equal(target[i],1.0))
                     // nll -= (1-target[i]) * pl_log(1-output[i]);
                     // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
                     //                         = log(1/(1+exp(x)))
                     //                         = -log(1+exp(x))
                     //                         = -softplus(x)
-                    nll += (1-target[i]) * softplus(-activation[i]);
-                
+                    nll += (1-target[i]) * softplus(activation[i]);
             }
         }
         costs_column(k,0) = nll;
@@ -430,10 +425,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    for( int i=0 ; i<size ; i++ )
-    {
-        bias_gradient[i] = target[i]-expectation[i];
-    }
+    // bias_gradient = target - expectation
+    substract(target, expectation, bias_gradient);
 }
 
 void RBMBinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -447,7 +440,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    substract(targets,expectations,bias_gradients);
+    // bias_gradients = targets - expectations
+    substract(targets, expectations, bias_gradients);
 }
 
 void RBMBinomialLayer::declareOptions(OptionList& ol)
@@ -479,7 +473,7 @@
 
 real RBMBinomialLayer::energy(const Vec& unit_values) const
 {
-    return dot(unit_values,bias);
+    return -dot(unit_values, bias);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-06-29 21:59:11 UTC (rev 7678)
@@ -117,7 +117,7 @@
     virtual void bpropNLL(const Mat& targets, const Mat& costs_column,
                           Mat& bias_gradients);
 
-    //! compute bias' unit_values
+    //! compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
     //#####  PLearn::Object Protocol  #########################################

Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -197,7 +197,7 @@
     last_layer->getAllActivations( previous_to_last );
 
     // target_layer->activation =
-    //      bias - sum_j softplus(-(W_ji + last_layer->activation[j]))
+    //      bias + sum_j softplus(W_ji + last_layer->activation[j])
     Vec target_act = target_layer->activation;
     for( int i=0 ; i<output_size ; i++ )
     {
@@ -210,7 +210,7 @@
         for( int j=0 ; j<last_size ; j++, w+=m )
         {
             // *w = weights(j,i)
-            target_act[i] -= softplus( -(*w + last_act[j]) );
+            target_act[i] += softplus(*w + last_act[j]);
         }
     }
 
@@ -268,7 +268,7 @@
         for( int k=0 ; k<output_size ; k++ )
         {
             // dC/d( w_ik + target_act_i )
-            real d_z = d_target_act[k]*(sigmoid(-w[k] - last_act[i]));
+            real d_z = d_target_act[k]*(sigmoid(w[k] + last_act[i]));
             w[k] -= last_to_target->learning_rate * d_z;
 
             d_last_act[i] += d_z;

Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -218,10 +218,10 @@
 void RBMConv2DConnection::update()
 {
     // updates parameters
-    // kernel -= learning_rate * (kernel_pos_stats/pos_count
+    // kernel += learning_rate * (kernel_pos_stats/pos_count
     //                              - kernel_neg_stats/neg_count)
-    real pos_factor = -learning_rate / pos_count;
-    real neg_factor = learning_rate / neg_count;
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = -learning_rate / neg_count;
 
     real* k_i = kernel.data();
     real* kps_i = kernel_pos_stats.data();
@@ -279,7 +279,7 @@
      *   for j=0 to up_image_width:
      *     for l=0 to kernel_length:
      *       for m=0 to kernel_width:
-     *         kernel_neg_stats(l,m) -= learning_rate *
+     *         kernel_neg_stats(l,m) += learning_rate *
      *           ( pos_down_image(step1*i+l,step2*j+m) * pos_up_image(i,j)
      *             - neg_down_image(step1*i+l,step2*j+m) * neg_up_image(i,j) )
      */
@@ -315,7 +315,7 @@
                                                ndv2+=down_image_width )
                     for( int m=0; m<kernel_width; m++ )
                         k[m] += learning_rate *
-                            (ndv2[m] * nuv_ij - pdv2[m] * puv_ij);
+                            (pdv2[m] * puv_ij - ndv2[m] * nuv_ij);
             }
         }
     }
@@ -348,7 +348,7 @@
                                                pdv2+=down_image_width,
                                                ndv2+=down_image_width )
                     for( int m=0; m<kernel_width; m++ )
-                        kinc[m] += ndv2[m] * nuv_ij - pdv2[m] * puv_ij;
+                        kinc[m] += pdv2[m] * puv_ij - ndv2[m] * nuv_ij;
             }
         }
         multiplyAcc( kernel, kernel_inc, learning_rate );
@@ -376,7 +376,7 @@
      *   for j=0 to up_image_width:
      *     for l=0 to kernel_length:
      *       for m=0 to kernel_width:
-     *         kernel_neg_stats(l,m) -= learning_rate *
+     *         kernel_neg_stats(l,m) += learning_rate *
      *           ( pos_down_image(step1*i+l,step2*j+m) * pos_up_image(i,j)
      *             - neg_down_image(step1*i+l,step2*j+m) * neg_up_image(i,j) )
      */
@@ -415,7 +415,7 @@
                                                    ndv2+=down_image_width )
                         for( int m=0; m<kernel_width; m++ )
                             k[m] += norm_lr *
-                                (ndv2[m] * nuv_ij - pdv2[m] * puv_ij);
+                                (pdv2[m] * puv_ij - ndv2[m] * nuv_ij);
                 }
             }
         }

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -154,14 +154,14 @@
         real a_i = quad_coeff[0];
         for( int i=0 ; i<size ; i++ )
         {
-            expectation[i] = - activation[i] / (2 * a_i * a_i);
+            expectation[i] = activation[i] / (2 * a_i * a_i);
         }
     }
     else
         for( int i=0 ; i<size ; i++ )
         {
             real a_i = quad_coeff[i];
-            expectation[i] = - activation[i] / (2 * a_i * a_i);
+            expectation[i] = activation[i] / (2 * a_i * a_i);
         }
 
     expectation_is_up_to_date = true;
@@ -181,7 +181,7 @@
         for (int k = 0; k < batch_size; k++)
             for (int i = 0 ; i < size ; i++)
                 {
-                    expectations(k, i) = -activations(k, i) / (2 * a_i * a_i) ;
+                    expectations(k, i) = activations(k, i) / (2 * a_i * a_i) ;
                 }
     }
     else
@@ -189,7 +189,7 @@
             for (int i = 0 ; i < size ; i++)
                 {
                     real a_i = quad_coeff[i];
-                    expectations(k, i) = -activations(k, i) / (2 * a_i * a_i) ;
+                    expectations(k, i) = activations(k, i) / (2 * a_i * a_i) ;
                 }
     expectations_are_up_to_date = true;
 }
@@ -220,14 +220,14 @@
         real a_i = quad_coeff[0];
         for( int i=0 ; i<size ; i++ )
         {
-            output[i] = - (input[i] + bias[i]) / (2 * a_i * a_i);
+            output[i] = (input[i] + bias[i]) / (2 * a_i * a_i);
         }
     }
     else
         for( int i=0 ; i<size ; i++ )
         {
             real a_i = quad_coeff[i];
-            output[i] = - (input[i] + bias[i]) / (2 * a_i * a_i);
+            output[i] = (input[i] + bias[i]) / (2 * a_i * a_i);
         }
 }
 
@@ -263,7 +263,7 @@
     {
         if(!share_quad_coeff)
             a_i = quad_coeff[i];
-        real in_grad_i = - output_gradient[i] / (2 * a_i * a_i);
+        real in_grad_i = output_gradient[i] / (2 * a_i * a_i);
         input_gradient[i] += in_grad_i;
 
         if( momentum == 0. )
@@ -275,9 +275,9 @@
                coefficient during the gradient descent phase.
 
             // update the quadratic coefficient:
-            // a_i += learning_rate * out_grad_i * (b_i + input_i) / a_i^3
-            // (or a_i += 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
-            a_i += two_lr * in_grad_i * (bias[i] + input[i])
+            // a_i -= learning_rate * out_grad_i * (b_i + input_i) / a_i^3
+            // (or a_i -= 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
+            a_i -= two_lr * in_grad_i * (bias[i] + input[i])
                                                     / a_i;
             if( a_i < min_quad_coeff )
                 a_i = min_quad_coeff;
@@ -292,11 +292,11 @@
 
             /*
             // The update rule becomes:
-            // a_inc_i = momentum * a_i_inc + learning_rate * out_grad_i
+            // a_inc_i = momentum * a_i_inc - learning_rate * out_grad_i
             //                                  * (b_i + input_i) / a_i^3
             // a_i += a_inc_i
-            quad_coeff_inc[i] += momentum * quad_coeff_inc[i]
-                + two_lr * in_grad_i * (bias[i] + input[i])
+            quad_coeff_inc[i] = momentum * quad_coeff_inc[i]
+                - two_lr * in_grad_i * (bias[i] + input[i])
                                          / a_i;
             a_i += quad_coeff_inc[i];
             if( a_i < min_quad_coeff )
@@ -622,13 +622,13 @@
             for(register int i=0; i<size; i++)
             {
                 tmp = a[0]*v[i];
-                en += tmp*tmp + b[i]*v[i];
+                en += tmp*tmp - b[i]*v[i];
             }
         else
             for(register int i=0; i<size; i++)
             {
                 tmp = a[i]*v[i];
-                en += tmp*tmp + b[i]*v[i];
+                en += tmp*tmp - b[i]*v[i];
             }
     }
     return en;
@@ -713,10 +713,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    for( int i=0 ; i<size ; i++ )
-    {
-        bias_gradient[i] = target[i]-expectation[i];
-    }
+    // bias_gradient = target - expectation
+    substract(target, expectation, bias_gradient);
 }
 
 void RBMGaussianLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -730,7 +728,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    substract(targets,expectations,bias_gradients);
+    // bias_gradients = targets - expectations
+    substract(targets, expectations, bias_gradients);
 }
 
 

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-29 21:59:11 UTC (rev 7678)
@@ -48,7 +48,6 @@
 /**
  * Layer in an RBM formed with Gaussian units
  *
- * @todo: yes
  */
 class RBMGaussianLayer: public RBMLayer
 {
@@ -58,9 +57,9 @@
     //#####  Public Build Options  ############################################
 
     real min_quad_coeff;
-    
+
     bool share_quad_coeff;
-    
+
     //! Number of units when share_quad_coeff is False
     //! or 1 when share_quad_coeff is True
     int size_quad_coeff;
@@ -75,8 +74,10 @@
     //! Constructor from the number of units in the multinomial
     RBMGaussianLayer( int the_size, real the_learning_rate=0. );
 
-    //! Constructor from the number of units in the multinomial, with an aditional option
-    RBMGaussianLayer( int the_size, real the_learning_rate=0., bool do_share_quad_coeff=false );
+    //! Constructor from the number of units in the multinomial,
+    //! with an aditional option
+    RBMGaussianLayer( int the_size, real the_learning_rate=0.,
+                      bool do_share_quad_coeff=false );
 
     //! compute a sample, and update the sample field
     virtual void generateSample() ;

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -284,10 +284,8 @@
     This->activation << input;
     This->activation += bias;
     This->expectation_is_up_to_date = false;
-    This->expectations_are_up_to_date = false;
 
-    PLERROR("In RBMLayer::fprop - The code seems buggy (no expectation seems"
-            " to be computed), someone should check this out");
+    This->computeExpectation();
 
     output << This->expectation;
 }
@@ -363,10 +361,10 @@
 ////////////
 void RBMLayer::update()
 {
-    // bias -= learning_rate * (bias_pos_stats/pos_count
+    // bias += learning_rate * (bias_pos_stats/pos_count
     //                          - bias_neg_stats/neg_count)
-    real pos_factor = -learning_rate / pos_count;
-    real neg_factor = learning_rate / neg_count;
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = -learning_rate / neg_count;
 
     real* b = bias.data();
     real* bps = bias_pos_stats.data();
@@ -385,7 +383,7 @@
 
         // The update rule becomes:
         // bias_inc = momentum * bias_inc
-        //              - learning_rate * (bias_pos_stats/pos_count
+        //              + learning_rate * (bias_pos_stats/pos_count
         //                                  - bias_neg_stats/neg_count)
         // bias += bias_inc
         real* binc = bias_inc.data();
@@ -425,7 +423,7 @@
 
 void RBMLayer::update( const Vec& pos_values, const Vec& neg_values)
 {
-    // bias -= learning_rate * (pos_values - neg_values)
+    // bias += learning_rate * (pos_values - neg_values)
     real* b = bias.data();
     real* pv = pos_values.data();
     real* nv = neg_values.data();
@@ -433,7 +431,7 @@
     if( momentum == 0. )
     {
         for( int i=0 ; i<size ; i++ )
-            b[i] += learning_rate * ( nv[i] - pv[i] );
+            b[i] += learning_rate * ( pv[i] - nv[i] );
     }
     else
     {
@@ -441,7 +439,7 @@
         real* binc = bias_inc.data();
         for( int i=0 ; i<size ; i++ )
         {
-            binc[i] = momentum*binc[i] + learning_rate*( nv[i] - pv[i] );
+            binc[i] = momentum*binc[i] + learning_rate*( pv[i] - nv[i] );
             b[i] += binc[i];
         }
     }
@@ -449,7 +447,7 @@
 
 void RBMLayer::update( const Mat& pos_values, const Mat& neg_values)
 {
-    // bias -= learning_rate * (pos_values - neg_values)
+    // bias += learning_rate * (pos_values - neg_values)
 
     int n = pos_values.length();
     PLASSERT( neg_values.length() == n );
@@ -466,8 +464,8 @@
 
     if( momentum == 0. )
     {
-        transposeProductScaleAcc(bias, pos_values, ones, -avg_lr, real(1));
-        transposeProductScaleAcc(bias, neg_values, ones,  avg_lr, real(1));
+        transposeProductScaleAcc(bias, pos_values, ones,  avg_lr, real(1));
+        transposeProductScaleAcc(bias, neg_values, ones, -avg_lr, real(1));
     }
     else
     {
@@ -477,7 +475,7 @@
         real* binc = bias_inc.data();
         for( int i=0 ; i<size ; i++ )
         {
-            binc[i] = momentum*binc[i] + learning_rate*( nv[i] - pv[i] );
+            binc[i] = momentum*binc[i] + learning_rate*( pv[i] - nv[i] );
             b[i] += binc[i];
         }
         */
@@ -513,15 +511,17 @@
                           bias_neg_stats);
     neg_count++;
 
-    // delta w = -lrate * ( sumoverrows(pos_values)
+    // delta w = lrate * ( sumoverrows(pos_values)
     //                   - ( background_gibbs_update_ratio*neg_stats
     //                      +(1-background_gibbs_update_ratio)
     //                       * sumoverrows(cd_neg_values) ) )
     columnSum(pos_values,tmp);
-    multiplyAcc(bias, tmp, -learning_rate*normalize_factor);
-    multiplyAcc(bias,bias_neg_stats,learning_rate*background_gibbs_update_ratio);
+    multiplyAcc(bias, tmp, learning_rate*normalize_factor);
+    multiplyAcc(bias, bias_neg_stats,
+                -learning_rate*background_gibbs_update_ratio);
     columnSum(cd_neg_values, tmp);
-    multiplyAcc(bias, tmp, learning_rate*(1-background_gibbs_update_ratio)*normalize_factor);
+    multiplyAcc(bias, tmp,
+                -learning_rate*(1-background_gibbs_update_ratio)*normalize_factor);
 }
 
 /////////////////
@@ -541,7 +541,7 @@
     real normalize_factor=1.0/minibatch_size;
     columnSum(gibbs_neg_values,tmp);
     if (neg_count==0)
-        multiply(tmp,normalize_factor,bias_neg_stats);
+        multiply(tmp, normalize_factor, bias_neg_stats);
     else // bias_neg_stats <-- tmp*(1-gibbs_chain_statistics_forgetting_factor)/minibatch_size 
         //                    +gibbs_chain_statistics_forgetting_factor*bias_neg_stats
         multiplyScaledAdd(tmp,gibbs_ma_coefficient,
@@ -560,10 +560,10 @@
         gibbs_ma_coefficient = sigmoid(gibbs_ma_increment + inverse_sigmoid(gibbs_ma_coefficient));
 
 
-    // delta w = -lrate * ( meanoverrows(pos_values) - neg_stats ) 
+    // delta w = lrate * ( meanoverrows(pos_values) - neg_stats ) 
     columnSum(pos_values,tmp);
-    multiplyAcc(bias, tmp, -learning_rate*normalize_factor);
-    multiplyAcc(bias,bias_neg_stats,learning_rate);
+    multiplyAcc(bias, tmp, learning_rate*normalize_factor);
+    multiplyAcc(bias, bias_neg_stats, -learning_rate);
 }
 
 ////////////////
@@ -602,27 +602,27 @@
 /////////////
 void RBMLayer::bpropCD(Vec& bias_gradient)
 {
-    // grad = bias_pos_stats/pos_count - bias_neg_stats/neg_count
+    // grad = -bias_pos_stats/pos_count + bias_neg_stats/neg_count
 
     real* bg = bias_gradient.data();
     real* bps = bias_pos_stats.data();
     real* bns = bias_neg_stats.data();
 
     for( int i=0 ; i<size ; i++ )
-        bg[i] = bps[i]/pos_count - bns[i]/neg_count;
+        bg[i] = -bps[i]/pos_count + bns[i]/neg_count;
 }
 
 void RBMLayer::bpropCD(const Vec& pos_values, const Vec& neg_values,
                        Vec& bias_gradient)
 {
-    // grad = bias_pos_stats/pos_count - bias_neg_stats/neg_count
+    // grad = -bias_pos_stats/pos_count + bias_neg_stats/neg_count
 
     real* bg = bias_gradient.data();
     real* bps = pos_values.data();
     real* bns = neg_values.data();
 
     for( int i=0 ; i<size ; i++ )
-        bg[i] = bps[i] - bns[i];
+        bg[i] = -bps[i] + bns[i];
 }
 
 real RBMLayer::energy(const Vec& unit_values) const

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-06-29 21:59:11 UTC (rev 7678)
@@ -52,7 +52,6 @@
 /**
  * Virtual class for a layer in an RBM.
  *
- * @todo: yes
  */
 class RBMLayer: public OnlineLearningModule
 {
@@ -262,10 +261,10 @@
     //! (or activations, which is equivalent), given the positive and
     //! negative phase values.
     virtual void bpropCD(const Vec& pos_values, const Vec& neg_values,
-                    Vec& bias_gradient);
+                         Vec& bias_gradient);
 
     virtual real energy(const Vec& unit_values) const;
-    
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -173,10 +173,10 @@
 void RBMMatrixConnection::update()
 {
     // updates parameters
-    //weights -= learning_rate * (weights_pos_stats/pos_count
+    //weights += learning_rate * (weights_pos_stats/pos_count
     //                              - weights_neg_stats/neg_count)
-    real pos_factor = -learning_rate / pos_count;
-    real neg_factor = learning_rate / neg_count;
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = -learning_rate / neg_count;
 
     int l = weights.length();
     int w = weights.width();
@@ -227,9 +227,9 @@
                                   const Vec& neg_down_values, // v_1
                                   const Vec& neg_up_values )  // h_1
 {
-    // weights -= learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // weights += learning_rate * ( h_0 v_0' - h_1 v_1' );
     // or:
-    // weights[i][j] += learning_rate * (h_1[i] v_1[j] - h_0[i] v_0[j]);
+    // weights[i][j] += learning_rate * (h_0[i] v_0[j] - h_1[i] v_1[j]);
 
     int l = weights.length();
     int w = weights.width();
@@ -249,7 +249,7 @@
     {
         for( int i=0 ; i<l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
             for( int j=0 ; j<w ; j++ )
-                w_i[j] += learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                w_i[j] += learning_rate * (*puv_i * pdv[j] - *nuv_i * ndv[j]);
     }
     else
     {
@@ -268,7 +268,7 @@
             for( int j=0 ; j<w ; j++ )
             {
                 winc_i[j] = momentum * winc_i[j]
-                    + learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                    + learning_rate * (*puv_i * pdv[j] - *nuv_i * ndv[j]);
                 w_i[j] += winc_i[j];
             }
     }
@@ -279,9 +279,9 @@
                                   const Mat& neg_down_values, // v_1
                                   const Mat& neg_up_values )  // h_1
 {
-    // weights -= learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // weights += learning_rate * ( h_0 v_0' - h_1 v_1' );
     // or:
-    // weights[i][j] += learning_rate * (h_1[i] v_1[j] - h_0[i] v_0[j]);
+    // weights[i][j] += learning_rate * (h_0[i] v_0[j] - h_1[i] v_1[j]);
 
     PLASSERT( pos_up_values.width() == weights.length() );
     PLASSERT( neg_up_values.width() == weights.length() );
@@ -294,10 +294,10 @@
         real avg_lr = learning_rate / pos_down_values.length();
 
         transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
-                                 -avg_lr, real(1));
+                                 avg_lr, real(1));
 
         transposeProductScaleAcc(weights, neg_up_values, neg_down_values,
-                                 avg_lr, real(1));
+                                 -avg_lr, real(1));
     }
     else
     {
@@ -308,7 +308,7 @@
 
         // The update rule becomes:
         // weights_inc = momentum * weights_inc
-        //               - learning_rate * ( h_0 v_0' - h_1 v_1' );
+        //               + learning_rate * ( h_0 v_0' - h_1 v_1' );
         // weights += weights_inc;
 
         real* winc_i = weights_inc.data();
@@ -318,7 +318,7 @@
             for( int j=0 ; j<w ; j++ )
             {
                 winc_i[j] = momentum * winc_i[j]
-                    + learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                    + learning_rate * (*puv_i * pdv[j] - *nuv_i * ndv[j]);
                 w_i[j] += winc_i[j];
             }
          */
@@ -350,16 +350,16 @@
                                  gibbs_ma_coefficient);
     neg_count++;
 
-    // delta w = -lrate * ( pos_up_values'*pos_down_values
+    // delta w = lrate * ( pos_up_values'*pos_down_values
     //                   - ( background_gibbs_update_ratio*neg_stats
     //                      +(1-background_gibbs_update_ratio)
     //                       * cd_neg_up_values'*cd_neg_down_values/minibatch_size))
     transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
-                             -learning_rate*normalize_factor, real(1));
+                             learning_rate*normalize_factor, real(1));
     multiplyAcc(weights, weights_neg_stats,
-                learning_rate*background_gibbs_update_ratio);
+                -learning_rate*background_gibbs_update_ratio);
     transposeProductScaleAcc(weights, cd_neg_up_values, cd_neg_down_values,
-        learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,
+        -learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,
         real(1));
 }
 
@@ -399,10 +399,10 @@
         cout << "new coefficient = " << gibbs_ma_coefficient << " at example " << neg_count*minibatch_size << endl;
     }
 
-    // delta w = -lrate * ( pos_up_values'*pos_down_values/minibatch_size - neg_stats )
+    // delta w = lrate * ( pos_up_values'*pos_down_values/minibatch_size - neg_stats )
     transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
-                             -learning_rate*normalize_factor, real(1));
-    multiplyAcc(weights, weights_neg_stats,learning_rate);
+                             learning_rate*normalize_factor, real(1));
+    multiplyAcc(weights, weights_neg_stats, -learning_rate);
 }
 
 ////////////////
@@ -552,9 +552,9 @@
 }
 
 void RBMMatrixConnection::bpropUpdate(const Mat& inputs, const Mat& outputs,
-                             Mat& input_gradients,
-                             const Mat& output_gradients,
-                             bool accumulate)
+                                      Mat& input_gradients,
+                                      const Mat& output_gradients,
+                                      bool accumulate)
 {
     PLASSERT( inputs.width() == down_size );
     PLASSERT( outputs.width() == up_size );
@@ -641,13 +641,13 @@
     {
         for( int i=0 ; i<l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
             for( int j=0 ; j<w ; j++ )
-                w_i[j] += wps_i[j]/pos_count - wns_i[j]/neg_count;
+                w_i[j] += wns_i[j]/pos_count - wps_i[j]/neg_count;
     }
     else
     {
         for( int i=0 ; i<l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
             for( int j=0 ; j<w ; j++ )
-                w_i[j] = wps_i[j]/pos_count - wns_i[j]/neg_count;
+                w_i[j] = wns_i[j]/pos_count - wps_i[j]/neg_count;
     }
 }
 
@@ -679,13 +679,13 @@
     {
         for( int i=0 ; i<l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
             for( int j=0 ; j<w ; j++ )
-                w_i[j] +=  *puv_i * pdv[j] - *nuv_i * ndv[j] ;
+                w_i[j] +=  *nuv_i * ndv[j] - *puv_i * pdv[j] ;
     }
     else
     {
         for( int i=0 ; i<l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
             for( int j=0 ; j<w ; j++ )
-                w_i[j] =  *puv_i * pdv[j] - *nuv_i * ndv[j] ;
+                w_i[j] =  *nuv_i * ndv[j] - *puv_i * pdv[j] ;
     }
 }
 

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -368,6 +368,12 @@
 ///////////////////
 // computeEnergy //
 ///////////////////
+// FULLY OBSERVED CASE
+// we know x and h:
+// energy(h,x) = -b'x - c'h - h'Wx
+//  = visible_layer->energy(x) + hidden_layer->energy(h)
+//      - dot(h, hidden_layer->activation-c)
+//  = visible_layer->energy(x) - dot(h, hidden_layer->activation)
 void RBMModule::computeEnergy(const Mat& visible, const Mat& hidden,
                               Mat& energy, bool positive_phase)
 {
@@ -383,13 +389,20 @@
     }
     PLASSERT( hidden_activations );
     for (int i=0;i<mbs;i++)
-        energy(i,0) = visible_layer->energy(visible(i)) + 
-            dot(hidden(i), (*hidden_activations)(i));
+        energy(i,0) = visible_layer->energy(visible(i))
+            - dot(hidden(i), (*hidden_activations)(i));
+            // Why not: + hidden_layer->energy(hidden(i)) ?
 }
 
 ///////////////////////////////
 // computeFreeEnergyOfHidden //
 ///////////////////////////////
+// FREE-ENERGY(hidden) CASE
+// we know h:
+// free energy = -log sum_x e^{-energy(h,x)}
+//  = -c'h - sum_i log sigmoid(b_i + W_{.i}'h) .... FOR BINOMIAL INPUT LAYER
+// or more robustly,
+//  = hidden_layer->energy(h) - sum_i softplus(visible_layer->activation[i])
 void RBMModule::computeFreeEnergyOfHidden(const Mat& hidden, Mat& energy)
 {
     int mbs=hidden.length();
@@ -405,16 +418,22 @@
         energy(i,0) = hidden_layer->energy(hidden(i));
         if (use_fast_approximations)
             for (int j=0;j<visible_layer->size;j++)
-                energy(i,0) -= tabulated_softplus(-visible_layer->activations(i,j));
+                energy(i,0) -= tabulated_softplus(visible_layer->activations(i,j));
         else
             for (int j=0;j<visible_layer->size;j++)
-                energy(i,0) -= softplus(-visible_layer->activations(i,j));
+                energy(i,0) -= softplus(visible_layer->activations(i,j));
     }
 }
 
 ////////////////////////////////
 // computeFreeEnergyOfVisible //
 ////////////////////////////////
+// FREE-ENERGY(visible) CASE
+// we know x:
+// free energy = -log sum_h e^{-energy(h,x)}
+//  = -b'x - sum_i log sigmoid(c_i + W_i'x) .... FOR BINOMIAL HIDDEN LAYER
+// or more robustly,
+//  = visible_layer->energy(x) - sum_i softplus(hidden_layer->activation[i])
 void RBMModule::computeFreeEnergyOfVisible(const Mat& visible, Mat& energy,
                                            bool positive_phase)
 {
@@ -441,10 +460,10 @@
         energy(i,0) = visible_layer->energy(visible(i));
         if (use_fast_approximations)
             for (int j=0;j<hidden_layer->size;j++)
-                energy(i,0) -= tabulated_softplus(-(*hidden_activations)(i,j));
+                energy(i,0) -= tabulated_softplus((*hidden_activations)(i,j));
         else
             for (int j=0;j<hidden_layer->size;j++)
-                energy(i,0) -= softplus(-(*hidden_activations)(i,j));
+                energy(i,0) -= softplus((*hidden_activations)(i,j));
     }
 }
 
@@ -586,7 +605,7 @@
     PLASSERT( visible_layer );
     PLASSERT( hidden_layer );
     PLASSERT( connection );
-        
+
     Mat* visible = ports_value[getPortIndex("visible")]; 
     Mat* hidden = ports_value[getPortIndex("hidden.state")];
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
@@ -647,32 +666,23 @@
         PLASSERT_MSG( energy->isEmpty(), 
                       "RBMModule: the energy port can only be an output port\n" );
         if (visible && !visible->isEmpty()
-            && hidden && !hidden->isEmpty()) 
+            && hidden && !hidden->isEmpty())
         {
-            // FULLY OBSERVED CASE
-            // we know x and h: energy(h,x) = b'x + c'h + h'Wx
-            //  = visible_layer->energy(x) + hidden_layer->energy(h) + dot(h,hidden_layer->activation-c)
-            //  = visible_layer->energy(x) + dot(h,hidden_layer->activation)
-            computeEnergy(*visible,*hidden,*energy);
-        } else if (visible && !visible->isEmpty())
+            computeEnergy(*visible, *hidden, *energy);
+        }
+        else if (visible && !visible->isEmpty())
         {
-            // FREE-ENERGY(visible) CASE
-            // we know x: free energy = -log sum_h e^{-energy(h,x)}
-            //                        = b'x + sum_i log sigmoid(c_i + W_i'x) .... FOR BINOMIAL HIDDEN LAYER
-            // or more robustly,      = visible_layer->energy(x) - sum_i softplus(-hidden_layer->activation[i])
             computeFreeEnergyOfVisible(*visible,*energy);
         }
         else if (hidden && !hidden->isEmpty())
-            // FREE-ENERGY(hidden) CASE
-            // we know h: free energy = -log sum_x e^{-energy(h,x)}
-            //                        = c'h + sum_i log sigmoid(b_i + W_{.i}'h) .... FOR BINOMIAL INPUT LAYER
-            // or more robustly,      = hidden_layer->energy(h) - sum_i softplus(-visible_layer->activation[i])
         {
             computeFreeEnergyOfHidden(*hidden,*energy);
         }
-        else 
+        else
+        {
             PLERROR("RBMModule: unknown configuration to compute energy (currently\n"
                     "only possible if at least visible or hidden are provided).\n");
+        }
         found_a_valid_configuration = true;
     }
     if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
@@ -937,12 +947,12 @@
                 (*contrastive_divergence)(i,0) = 
                     // positive phase energy
                     visible_layer->energy((*visible)(i))
-                    + dot((*h)(i),(*h_act)(i))
+                    - dot((*h)(i),(*h_act)(i))
                     // minus
                     - 
                     // negative phase energy
                     (visible_layer->energy(visible_layer->samples(i))
-                     + dot(hidden_expectations(i),hidden_layer->activations(i)));
+                     - dot(hidden_expectations(i),hidden_layer->activations(i)));
             }
         }
         else
@@ -1004,7 +1014,7 @@
     }
     
 
-	
+
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;
@@ -1017,15 +1027,16 @@
     {
         /*
         if (visible)
-        cout << "visible_empty : "<< (bool) visible->isEmpty() << endl;
+            cout << "visible_empty : "<< (bool) visible->isEmpty() << endl;
         if (hidden)
-        cout << "hidden_empty : "<< (bool) hidden->isEmpty() << endl;
+            cout << "hidden_empty : "<< (bool) hidden->isEmpty() << endl;
         if (visible_sample)
-        cout << "visible_sample_empty : "<< (bool) visible_sample->isEmpty() << endl;
+            cout << "visible_sample_empty : "<< (bool) visible_sample->isEmpty() << endl;
         if (hidden_sample)
-        cout << "hidden_sample_empty : "<< (bool) hidden_sample->isEmpty() << endl;
+            cout << "hidden_sample_empty : "<< (bool) hidden_sample->isEmpty() << endl;
         if (visible_expectation)
-        cout << "visible_expectation_empty : "<< (bool) visible_expectation->isEmpty() << endl;
+            cout << "visible_expectation_empty : "<< (bool) visible_expectation->isEmpty() << endl;
+
         */
         PLERROR("In RBMModule::fprop - Unknown port configuration for module %s", name.c_str());
     }
@@ -1314,46 +1325,47 @@
                 store_weights_grad.clear();
                 weights_g = & store_weights_grad;
             }
-                PLASSERT( connection->classname() == "RBMMatrixConnection" &&
-                          visible_layer->classname() == "RBMBinomialLayer" &&
-                          hidden_layer->classname() == "RBMBinomialLayer" );
+            PLASSERT( connection->classname() == "RBMMatrixConnection" &&
+                      visible_layer->classname() == "RBMBinomialLayer" &&
+                      hidden_layer->classname() == "RBMBinomialLayer" );
 
+            for (int k = 0; k < mbs; k++) {
+                int idx = 0;
+                for (int i = 0; i < up; i++) {
+                    real p_i_p = (*hidden)(k, i);
+                    real a_i_p = (*hidden_act)(k, i);
+                    real p_i_n =
+                        (*negative_phase_hidden_expectations)(k, i);
+                    real a_i_n =
+                        (*negative_phase_hidden_activations)(k, i);
+
+                    real scale_p = 1 + (1 - p_i_p) * a_i_p;
+                    real scale_n = 1 + (1 - p_i_n) * a_i_n;
+                    for (int j = 0; j < down; j++, idx++) {
+                        // Weight 'idx' is the (i,j)-th element in the
+                        // 'weights' matrix.
+                        real v_j_p = (*visible)(k, j);
+                        real v_j_n =
+                            (*negative_phase_visible_samples)(k, j);
+                        (*weights_g)(k, idx) +=
+                            p_i_n * v_j_n * scale_n     // Negative phase.
+                            -(p_i_p * v_j_p * scale_p); // Positive phase.
+                    }
+                }
+            }
+            if (!standard_cd_grad) {
+                // Update connection manually.
+                Mat& weights = ((RBMMatrixConnection*)
+                                get_pointer(connection))->weights;
+                real lr = cd_learning_rate / mbs;
                 for (int k = 0; k < mbs; k++) {
                     int idx = 0;
-                    for (int i = 0; i < up; i++) {
-                        real p_i_p = (*hidden)(k, i);
-                        real a_i_p = (*hidden_act)(k, i);
-                        real p_i_n =
-                            (*negative_phase_hidden_expectations)(k, i);
-                        real a_i_n =
-                            (*negative_phase_hidden_activations)(k, i);
-                        real scale_p = 1 - (1 - p_i_p) * a_i_p;
-                        real scale_n = 1 - (1 - p_i_n) * a_i_n;
-                        for (int j = 0; j < down; j++, idx++) {
-                            // Weight 'idx' is the (i,j)-th element in the
-                            // 'weights' matrix.
-                            real v_j_p = (*visible)(k, j);
-                            real v_j_n =
-                                (*negative_phase_visible_samples)(k, j);
-                            (*weights_g)(k, idx) +=
-                                p_i_p * v_j_p * scale_p   // Positive phase.
-                             - (p_i_n * v_j_n * scale_n); // Negative phase.
-                        }
-                    }
+                    for (int i = 0; i < up; i++)
+                        for (int j = 0; j < down; j++, idx++)
+                            weights(i, j) -= lr * (*weights_g)(k, idx);
                 }
-                if (!standard_cd_grad) {
-                    // Update connection manually.
-                    Mat& weights = ((RBMMatrixConnection*)
-                            get_pointer(connection))->weights;
-                    real lr = cd_learning_rate / mbs;
-                    for (int k = 0; k < mbs; k++) {
-                        int idx = 0;
-                        for (int i = 0; i < up; i++)
-                            for (int j = 0; j < down; j++, idx++)
-                                weights(i, j) -= lr * (*weights_g)(k, idx);
-                    }
-                    connection_update_is_done = true;
-                }
+                connection_update_is_done = true;
+            }
         }
         if (!connection_update_is_done)
             // Perform standard update of the connection.
@@ -1388,9 +1400,9 @@
                     real p_i_n = (*negative_phase_hidden_expectations)(k, i);
                     real a_i_n = (*negative_phase_hidden_activations)(k, i);
                     (*hidden_bias_g)(k, i) +=
-                        standard_cd_bias_grad ? p_i_p - p_i_n :
-                        - p_i_p * (1 - p_i_p) * a_i_p + p_i_p    // Pos. phase
-                     -( - p_i_n * (1 - p_i_n) * a_i_n + p_i_n ); // Neg. phase
+                        standard_cd_bias_grad ? p_i_n - p_i_p :
+                        p_i_n * (1 - p_i_n) * a_i_n + p_i_n     // Neg. phase
+                     -( p_i_p * (1 - p_i_p) * a_i_p + p_i_p );  // Pos. phase
 
                 }
             }

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -1,5 +1,5 @@
+// -*- C++ -*-
 
-
 // RBMMultinomialLayer.cc
 //
 // Copyright (C) 2006 Pascal Lamblin & Dan Popovici
@@ -67,25 +67,7 @@
     bias_pos_stats.resize( the_size );
     bias_neg_stats.resize( the_size );
 }
-/*
-//! Uses "rbmp" to obtain the activations of unit "i" of this layer.
-//! This activation vector is computed by the "i+offset"-th unit of "rbmp"
-void RBMMultinomialLayer::getUnitActivations( int i, PP<RBMParameters> rbmp,
-                                              int offset )
-{
-    Vec activation = activations.subVec( i, 1 );
-    rbmp->computeUnitActivations( i+offset, 1, activation );
-    expectation_is_up_to_date = false;
-}
 
-void RBMMultinomialLayer::getAllActivations( PP<RBMParameters> rbmp,
-                                             int offset )
-{
-    rbmp->computeUnitActivations( offset, size, activations );
-    expectation_is_up_to_date = false;
-}
-*/
-
 void RBMMultinomialLayer::generateSample()
 {
     PLASSERT_MSG(random_gen,
@@ -121,7 +103,7 @@
         return;
 
     // expectation = softmax(-activation)
-    softmaxMinus(activation, expectation);
+    softmax(activation, expectation);
     expectation_is_up_to_date = true;
 }
 
@@ -135,7 +117,7 @@
 
     // expectation = softmax(-activation)
     for (int k = 0; k < batch_size; k++)
-        softmaxMinus(activations(k), expectations(k));
+        softmax(activations(k), expectations(k));
 
     expectations_are_up_to_date = true;
 }
@@ -147,7 +129,7 @@
     output.resize( output_size );
 
     // inefficient
-    softmaxMinus( input+bias, output );
+    softmax( input+bias, output );
 }
 
 ///////////
@@ -161,7 +143,7 @@
     output.resize( output_size );
 
     // inefficient
-    softmaxMinus( input+rbm_bias, output );
+    softmax( input+rbm_bias, output );
 }
 
 /////////////////
@@ -191,7 +173,7 @@
         bias_inc.resize( size );
 
     // input_gradient[i] =
-    //      (output_gradient . output - output_gradient[i] ) output[i]
+    //      (output_gradient[i] - output_gradient . output) output[i]
     real outg_dot_out = dot( output_gradient, output );
     real* out = output.data();
     real* outg = output_gradient.data();
@@ -201,7 +183,7 @@
 
     for( int i=0 ; i<size ; i++ )
     {
-        real ing_i = (outg_dot_out - outg[i]) * out[i];
+        real ing_i = (outg[i] - outg_dot_out) * out[i];
         ing[i] += ing_i;
 
         if( momentum == 0. )
@@ -252,7 +234,7 @@
     // TODO see if we can have a speed-up by reorganizing the different steps
 
     // input_gradients[k][i] =
-    //   (output_gradients[k].outputs[k]-output_gradients[k][i]) outputs[k][i]
+    //   (output_gradients[k][i]-output_gradients[k].outputs[k]) outputs[k][i]
     for( int k=0; k<mbatch_size; k++ )
     {
         real outg_dot_out = dot( output_gradients(k), outputs(k) );
@@ -264,7 +246,7 @@
 
         for( int i=0 ; i<size ; i++ )
         {
-            real ing_ki = (outg_dot_out - outg[i]) * out[i];
+            real ing_ki = (outg[i] - outg_dot_out) * out[i];
             ing[i] += ing_ki;
 
             if( momentum == 0. )
@@ -305,7 +287,7 @@
     real* outg = output_gradient.data();
     real* ing = input_gradient.data();
     for( int i=0 ; i<size ; i++ )
-        ing[i] = (outg_dot_out - outg[i]) * out[i];
+        ing[i] = (outg[i] - outg_dot_out) * out[i];
 
     rbm_bias_gradient << input_gradient;
 }
@@ -319,17 +301,31 @@
 
     PLASSERT( target.size() == input_size );
 
-    real ret = 0;
+#ifdef BOUNDCHECK
+    if (!target.hasMissing())
+    {
+        PLASSERT_MSG( min(target) >= 0.,
+                      "Elements of \"target\" should be positive" );
+        // Ensure the distribution probabilities sum to 1. We relax a
+        // bit the default tolerance as probabilities using
+        // exponentials could suffer numerical imprecisions.
+        if (!is_equal( sum(target), 1., 1., 1e-5, 1e-5 ))
+            PLERROR("In RBMMultinomialLayer::fpropNLL - Elements of \"target\""
+                    " should sum to 1 (found a sum = %f)",
+                    sum(target));
+    }
+#endif
 
+    real nll = 0;
     real target_i, expectation_i;
-    for( int i=0 ; i<size ; i++ )
+    for (int i=0; i<size; i++)
     {
         target_i = target[i];
         expectation_i = expectation[i];
-        if(!fast_exact_is_equal(target_i,0.0))
-            ret -= target_i * pl_log(expectation_i);
+        if(!fast_exact_is_equal(target_i, 0.0))
+            nll -= target_i * pl_log(expectation_i);
     }
-    return ret;
+    return nll;
 }
 
 void RBMMultinomialLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
@@ -342,20 +338,34 @@
     PLASSERT( costs_column.length() == batch_size );
 
     real target_i, expectation_i;
-    for (int k=0;k<batch_size;k++) // loop over minibatch
+    for (int k=0; k<batch_size; k++) // loop over minibatch
     {
+#ifdef BOUNDCHECK
+        if (!targets(k).hasMissing())
+        {
+            PLASSERT_MSG( min(targets(k)) >= 0.,
+                          "Elements of \"targets\" should be positive" );
+            // Ensure the distribution probabilities sum to 1. We relax a
+            // bit the default tolerance as probabilities using
+            // exponentials could suffer numerical imprecisions.
+            if (!is_equal( sum(targets(k)), 1., 1., 1e-5, 1e-5 ))
+                PLERROR("In RBMMultinomialLayer::fpropNLL - Elements of"
+                        " \"target\" should sum to 1 (found a sum = %f at row"
+                        " %d)",
+                        sum(targets(k)), k);
+        }
+#endif
         real nll = 0;
-        //real* activation = activations[k];
         real* expectation = expectations[k];
         real* target = targets[k];
-        for( int i=0 ; i<size ; i++ )
+        for(int i=0; i<size; i++)
         {
             target_i = target[i];
             expectation_i = expectation[i];
-            if(!fast_exact_is_equal(target_i,0.0))
+            if(!fast_exact_is_equal(target_i, 0.0))
                 nll -= target_i * pl_log(expectation_i);
         }
-        costs_column(k,0) = nll;
+        costs_column(k, 0) = nll;
     }
 }
 
@@ -367,16 +377,11 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    real sum_tar = sum( target );
-    real* exp = expectation.data();
-    real* tar = target.data();
-    real* biasg = bias_gradient.data();
-    for( int i=0 ; i<size ; i++ )
-        biasg[i] = tar[i] - sum_tar * exp[i];
+    substract(target, expectation, bias_gradient);
 }
 
 void RBMMultinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
-                                Mat& bias_gradients)
+                                   Mat& bias_gradients)
 {
     computeExpectations();
 
@@ -386,15 +391,7 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    for (int k=0;k<batch_size;k++) // loop over minibatch
-    {        
-        real sum_tar = sum( targets(k) );
-        real* exp = expectations[k];
-        real* tar = targets[k];
-        real* biasg = bias_gradients[k];
-        for( int i=0 ; i<size ; i++ )
-            biasg[i] = tar[i] - sum_tar * exp[i];
-    }
+    substract(targets, expectations, bias_gradients);
 }
 
 void RBMMultinomialLayer::declareOptions(OptionList& ol)

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -67,38 +67,18 @@
     bias_neg_stats.resize( the_size );
 }
 
-/*
-//! Uses "rbmp" to obtain the activations of unit "i" of this layer.
-//! This activation vector is computed by the "i+offset"-th unit of "rbmp"
-void RBMTruncExpLayer::getUnitActivations( int i, PP<RBMParameters> rbmp,
-                                           int offset )
-{
-    Vec activation = activations.subVec( i, 1 );
-    rbmp->computeUnitActivations( i+offset, 1, activation );
-    expectation_is_up_to_date = false;
-}
-
-//! Uses "rbmp" to obtain the activations of all units in this layer.
-//! Unit 0 of this layer corresponds to unit "offset" of "rbmp".
-void RBMTruncExpLayer::getAllActivations( PP<RBMParameters> rbmp, int offset )
-{
-    rbmp->computeUnitActivations( offset, size, activations );
-    expectation_is_up_to_date = false;
-}
-*/
-
 void RBMTruncExpLayer::generateSample()
 {
     PLASSERT_MSG(random_gen,
                  "random_gen should be initialized before generating samples");
 
     /* The cumulative is :
-     * C(U) = P(u<U | x) = (1 - exp(-U a)) / (1 - exp(-a)) if 0 < U < 1,
+     * C(U) = P(u<U | x) = (1 - exp(U a)) / (1 - exp(a)) if 0 < U < 1,
      *        0 if U <= 0 and
      *        1 if 1 <= U
      *
      * And the inverse, if 0 <= s <=1:
-     * C^{-1}(s) = - log(1 - s*(1 - exp(-a)) / a
+     * C^{-1}(s) = log(1 - s*(1 - exp(a)) / a
      */
 
     for( int i=0 ; i<size ; i++ )
@@ -107,11 +87,11 @@
         real a_i = activation[i];
 
         // Polynomial approximation to avoid numerical instability if a ~ 0
-        // C^{-1}(s) ~ s + (-s + s^2)/2 * a + O(a^2)
+        // C^{-1}(s) ~ s + (s - s^2)/2 * a + O(a^2)
         if( fabs( a_i ) <= 1e-5 )
-            sample[i] = s + a_i*( s*(-1 + s)/2 );
+            sample[i] = s + a_i*( s*(1 - s)/2 );
         else
-            sample[i] = - logadd( pl_log( 1-s ), pl_log(s) - a_i ) / a_i;
+            sample[i] = logadd( pl_log( 1-s ), pl_log(s) + a_i ) / a_i;
     }
 }
 
@@ -121,7 +101,7 @@
         return;
 
     /* Conditional expectation:
-     * E[u|x] = 1/(1-exp(a)) + 1/a
+     * E[u|x] = 1/(1-exp(-a)) - 1/a
      */
 
     for( int i=0 ; i<size ; i++ )
@@ -129,11 +109,11 @@
         real a_i = activation[i];
 
         // Polynomial approximation to avoid numerical instability
-        // f(a) = 1/2 - a/12 + a^3/720 + O(a^5)
+        // f(a) = 1/2 + a/12 - a^3/720 + O(a^5)
         if( fabs( a_i ) <= 0.01 )
-            expectation[i] = 0.5 - a_i*(1./12. + a_i*a_i/720.);
+            expectation[i] = 0.5 + a_i*(1./12. - a_i*a_i/720.);
         else
-            expectation[i] = 1/(1-exp(a_i)) + 1/a_i;
+            expectation[i] = 1/(1-exp(-a_i)) - 1/a_i;
     }
 
     expectation_is_up_to_date = true;
@@ -150,12 +130,12 @@
         real a_i = input[i] + bias[i];
 
         // Polynomial approximation to avoid numerical instability
-        // f(a) = 1/(1-exp(a) + 1/a
-        // f(a) = 1/2 - a/12 + a^3/720 + O(a^5)
+        // f(a) = 1/(1-exp(-a) - 1/a
+        // f(a) = 1/2 + a/12 - a^3/720 + O(a^5)
         if( fabs( a_i ) <= 0.01 )
-            output[i] = 0.5 - a_i*(1./12. +a_i*a_i/720.);
+            output[i] = 0.5 + a_i*(1./12. - a_i*a_i/720.);
         else
-            output[i] = 1/(1-exp(a_i)) + 1/a_i;
+            output[i] = 1/(1-exp(-a_i)) - 1/a_i;
     }
 }
 



From lamblin at mail.berlios.de  Sat Jun 30 00:19:49 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 30 Jun 2007 00:19:49 +0200
Subject: [Plearn-commits] r7679 - in trunk/plearn_learners/online/test:
 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0
 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0
 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1
 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0
 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0
 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0
 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1
 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0
 DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir
 DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0
 DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir
 ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester
 ModuleLearner/.pytest/PL_ModuleLearn!
 er_Greedy/expected_results/expdir-tester/Split0
 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester
 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0
 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2
 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0
Message-ID: <200706292219.l5TMJnrt012883@sheep.berlios.de>

Author: lamblin
Date: 2007-06-30 00:19:45 +0200 (Sat, 30 Jun 2007)
New Revision: 7679

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/global_stats.pmat
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/split_stats.pmat
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/global_stats.pmat
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/split_stats.pmat
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat
Log:
Regenerated tests after previous commit.


Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -15,7 +15,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ 0.52000000000000024 0.450000000000000233 ] ;
+bias = 2 [ -0.550000000000000266 -0.440000000000000224 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -3.21580575368705368 -3.22364753521804737 0.0657827866426371755 0.0696030904799730815 0.0622029618057324957 0.0657378684765310972 0.0656101265818516688 0.0638544637987858155 0.0668341550566126791 0.0685139935739243339 0.0638885967214564421 0.0682193222523920229 0.0614189353126122703 0.0625664766511173087 0.0644591111231603425 0.0672080258882709475 0.068118922416383898 0.0620144498236062866 0.0658168511287076929 0.0614584698111661498 0.0685006396586318178 0.0670479847182284955 0.0641700406260315298 0.0661248069558832557 0.0671701269075536539 0.0619280966934446925 0.0631002357645635142 0.0616579996278061038 0.067283462359264759 0.0648275672466712555 0.0651764494044027171 0.0700494806732006176 0.0633482123265595443 0.0705131377405389115 0.0622909603789631242 0.062816786792906254 0.0663347352041287891 0.0649449035714201317 0.0699396681514389412 0.0655818667632351998 0.0685066157524031605 0.0638341419302660973 0.0675948606542311481 0.066125477314180614 0.06856318275!
 81078962 0.0618955421308295742 0.0659183468910491538 0.0612163654801688092 0.0654024349135763905 0.0621983451516724475 0.0622920880548175557 0.0612458129415098251 0.0618062259590422894 0.0630664460752235617 0.0669921649854180873 0.0666066772938697094 0.0718850881647959405 0.0635225507551099022 0.0682316342600248621 0.0678732444344292418 0.066056804056856086 0.0640291125303126818 0.0636386992597940543 0.0679831833666539737 0.0699530259034038676 0.0660419388372846944 0.0688321552553833454 0.0649094861976334953 0.0658691841957568108 0.0614068151019241601 0.0690146718658728037 0.0673971388239837654 0.0672023840821927287 0.067165533087422416 0.0664861857760034314 0.0693207953501458057 0.0637773254435999026 0.0642392433002831748 0.0668207490601321202 0.0679989486904433588 0.0652705848529419319 0.0645560824978383824 0.0664835106333893228 0.0702736876926789017 0.0636389201503628765 0.0714316391373439191 0.0665688382090241643 0.0651440873146577892 0.0660134349623082289 0.06810149087!
 29601215 0.066483264784777063 0.0662017489390426123 0.06273735!
 32592977
873 0.064759889243550986 0.0639710264357981478 0.0659921336077071846 0.0622046577630927452 0.0658226101336753244 0.0660693261083197814 0.068692607110742876 ] ;
+bias = 100 [ 3.22297502090243615 3.22850343798757411 -0.0655674198722276391 -0.0620531502143017999 -0.0694639415907335606 -0.0656967869581215891 -0.065802796160687807 -0.0676665281073794145 -0.0646008478691122434 -0.0630437566075335715 -0.0675636254713856965 -0.0633361959019119736 -0.0703945608316206561 -0.0690432124347641341 -0.0669936710720746009 -0.0641822519514770595 -0.0633215475695771413 -0.0696978904345579853 -0.0655365737634271694 -0.0703843254243849137 -0.0629756492516730521 -0.0643907155410799181 -0.0672190493187708044 -0.0652833514948132076 -0.0643953690592243555 -0.0698244386567378611 -0.068417804617699432 -0.0701758716581505348 -0.0640983927904117123 -0.0665726831989223305 -0.066404014349293411 -0.0616877132603564499 -0.0681706276454905474 -0.0612647343384444751 -0.0694090319721886173 -0.0688279871752820321 -0.065124427046754807 -0.0665585108343436371 -0.0617623458616671991 -0.0658212640585509184 -0.0630170514137075588 -0.0675889446130171906 -0.0638338044249902!
 314 -0.0655297688361054925 -0.0629649661551778261 -0.0698847192903202657 -0.0654629364108216816 -0.0706810954132212621 -0.0660285024336473153 -0.0695005153829066358 -0.0693978927744081514 -0.0706556651377230061 -0.069953748785900452 -0.0684365451391091423 -0.064539881485192116 -0.0647616493444652941 -0.0601662010225776159 -0.0679644403313143103 -0.0632144607207602016 -0.0636181859919972004 -0.0652850922111074539 -0.067444192856024418 -0.0678073923733376466 -0.0634914797452603014 -0.0617561876343265051 -0.0655494061977655029 -0.0627296640677947465 -0.0664271156689422576 -0.065479249890638011 -0.0704179681617658898 -0.0626083718349837032 -0.064150644361228526 -0.0642433194221907894 -0.0641793160292923603 -0.0648969996254884218 -0.0623260735216022987 -0.0677062572193665863 -0.0672066873746975935 -0.0647069715718852079 -0.0634935571762970802 -0.0661137956042305575 -0.0668230777159821848 -0.0650723733982817298 -0.061487132750550684 -0.0678324572139960141 -0.060563054793671578 -0!
 .0648627663702872337 -0.0662330568700430083 -0.065336260342353!
 4583 -0.
0634672818945638401 -0.0648919074644516009 -0.0651717128396615553 -0.0688591344930329918 -0.0666241742120582681 -0.0675404276392551489 -0.0654246462891826669 -0.0694808269603974654 -0.0656411525931498013 -0.0653383347553044974 -0.062874898273063487 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
@@ -45,106 +45,106 @@
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
--1.21415711267087589 	-1.29354388830904177 	
--1.24418997827235622 	-1.27717900530481865 	
-0.0690673896090493872 	0.00106171994505329303 	
--0.0516350068945717194 	0.00282024601463587519 	
-0.11576289542965755 	0.0779696628865055724 	
-0.0937738356356867114 	-0.021341106697606714 	
-0.0972995083962690244 	-0.0209936897396545835 	
-0.125729266175554216 	0.0102265283424613773 	
-0.0763061532337257764 	-0.0398982061240517327 	
--0.0404402347009997706 	0.0256125208841107149 	
-0.0915328264772420214 	0.0429378683671718664 	
--0.0570336912737006985 	0.0526007297112317876 	
-0.102969530366209081 	0.119573904600153644 	
-0.0772888280315299631 	0.104255387702343244 	
-0.116394649377435544 	-0.00151413154731653296 	
-0.042691841196573814 	-0.0187388372631490177 	
-0.00309655765104493563 	-0.00765739105447752182 	
-0.122622375403801068 	0.0780030163934584547 	
-0.0483846049070615694 	0.0210722938700518593 	
-0.125103439244352993 	0.0962171519640014627 	
-0.00709479590682871551 	-0.0237722892465301594 	
--0.0227695901334794421 	0.0543427620501140754 	
-0.0687163096186778344 	0.0558421452465020021 	
-0.0873560612567313155 	-0.0280358619475560913 	
--0.0464106966487873446 	0.0764830303301708148 	
-0.104489235992809293 	0.100178142145328428 	
-0.0776732347849002358 	0.0845290052909978551 	
-0.103692129703154029 	0.111410227121787497 	
-0.0430717292444947816 	-0.0217089627191627901 	
-0.0813883488134935357 	0.0210933819137022749 	
-0.129102379174570181 	-0.036355898240278843 	
--0.0286792898569442761 	-0.034418242614459485 	
-0.0835488383450222333 	0.0702781455282296547 	
--0.0580004135006431942 	-0.01856040113472247 	
-0.0855508878825002456 	0.106579346680886644 	
-0.0488784468778965461 	0.125566752615730387 	
--0.023122679386145515 	0.0787930530316436201 	
-0.114978759718911597 	-0.0154694984082932637 	
--0.0386528393637206938 	-0.0209924608991451864 	
-0.0160783249246924942 	0.0627623210970354728 	
-0.0331228024081457323 	-0.0498308212634323566 	
-0.0582388085345984102 	0.0783366438529952969 	
-0.0406519677826411435 	-0.0288869980516132489 	
--0.0597141338098050589 	0.126602072017401146 	
-0.0220616100720658521 	-0.0404334267520467042 	
-0.0907453618703058101 	0.115803257937635892 	
-0.0150504297724695993 	0.0522433262471396417 	
-0.113094839239612333 	0.117470889171381862 	
--0.00690135564924855734 	0.0928396049715910776 	
-0.109649827478102729 	0.0849798591361091565 	
-0.125891761693266213 	0.0650552724959696516 	
-0.11704082920771855 	0.112558749468915059 	
-0.127449941476074358 	0.0808065072573193877 	
-0.0919423334473952608 	0.0707898574066043729 	
-0.0858278120887288193 	-0.0534235543680405545 	
-0.060280040625982266 	-0.0170601239435519865 	
--0.0538103520979965749 	-0.0635244256416778175 	
-0.0499377763793946439 	0.0982902763280064928 	
-0.00693050343376365452 	-0.015191684253758728 	
-0.0331727775095305918 	-0.0296617801682230349 	
-0.0083328011499871589 	0.0539681702117705206 	
-0.0314617608986332009 	0.100064949873964096 	
-0.0855326426254968963 	0.057315017051513345 	
-0.023207493781608253 	-0.0233352076821683845 	
--0.0583191191212871773 	-0.000979764064585384288 	
--0.0479906282582027444 	0.116284453036889113 	
-0.0210591810317722547 	-0.0477397138282999317 	
-0.0434376062540039742 	0.0561817288505665896 	
-0.0452950663181666685 	0.022414002260599572 	
-0.125304222726260239 	0.0973538514926476223 	
-0.0273905390733705199 	-0.0593483422458846235 	
--0.0482520815783283041 	0.0706049670949486785 	
--0.0230148144711252271 	0.0495507483466000181 	
-0.0282514195131074383 	-0.00321472486741668142 	
-0.058562138849964071 	-0.0111312665818238642 	
--0.00555360888280791425 	-0.0355896183418070028 	
-0.0834121315444082828 	0.0554918556198370494 	
-0.103621345570545739 	0.0187585478098261545 	
-0.100422626762839626 	-0.0625169307095753274 	
--0.0307622944804058426 	0.0317687524264961274 	
-0.0373219821352471226 	0.0511030992694271741 	
-0.074764936607399371 	0.0365997076737267432 	
--0.0396251857033068436 	0.0922725594729125048 	
--0.0415625864709825432 	-0.0281025005267726037 	
-0.0938474228099761276 	0.0492258097438833461 	
--0.0465543952870261582 	-0.0573943961338660089 	
-0.0642999004945939145 	-0.0190406742732633638 	
-0.0192566912094867988 	0.0737614039978816 	
-0.0456318762720610838 	0.0172826898145707897 	
--0.0449514150475808227 	0.0440256246457320088 	
-0.00338020415539046389 	0.0453054325727234586 	
-0.0505121972965595842 	0.00636915220948123736 	
-0.0637718544343126165 	0.112154363518923855 	
-0.0496769642818689003 	0.0555319134645490819 	
-0.0129640742770456147 	0.121565282319786111 	
-0.0825645251590735457 	-0.0186625708087929665 	
-0.0912876914719137483 	0.103302329171365018 	
--0.0207879822675519077 	0.0935891931065968435 	
-0.0864228173229119323 	-0.0252615256322490621 	
-0.0219937469394293449 	-0.0441223905892084711 	
+1.29220320314167814 	1.23360776084591173 	
+1.27872961480206149 	1.26508818796771783 	
+0.0069235175860512992 	-0.0596493482467894826 	
+-0.114101462969043332 	-0.0580988569046345937 	
+0.0533565400353935779 	0.0170491494543559552 	
+0.0316429427052850806 	-0.0820287849666461455 	
+0.0351032969514864304 	-0.0817381734261192205 	
+0.0634469142848316514 	-0.0505021675106046772 	
+0.0141372893669295084 	-0.100685202722473471 	
+-0.102704511553851074 	-0.0351291077576920061 	
+0.0293595458726126149 	-0.0177656145449792691 	
+-0.119403330826722831 	-0.00820792272976666841 	
+0.0404489624105316206 	0.0584112377699046725 	
+0.0149953341430634525 	0.0433154538714251511 	
+0.0541231875169777901 	-0.0622654971409482144 	
+-0.0194619452202611058 	-0.0794991574421131941 	
+-0.0591309423094542805 	-0.0684592801496005793 	
+0.0601840005623679591 	0.0170691892439329487 	
+-0.0136891222072263643 	-0.0395756093587932289 	
+0.0626097695542838922 	0.0351989189500988703 	
+-0.0551720135255094613 	-0.0846389760005528818 	
+-0.0849388090255431272 	-0.00635662292228355869 	
+0.00653694458356686176 	-0.00490949374195461322 	
+0.0251749745065769151 	-0.0887952349207536823 	
+-0.108549115284084272 	0.0158403099288679516 	
+0.0421390225675163516 	0.0392398494581973761 	
+0.0154309454120496372 	0.0236842030028510397 	
+0.0413412467827884503 	0.0504401859701714764 	
+-0.0191193899502736173 	-0.0825103594157753778 	
+0.0192847607045263797 	-0.0395532286748599901 	
+0.0669408894051742309 	-0.097013099502201855 	
+-0.0910475086595164268 	-0.0953504375875404292 	
+0.0213870941412512526 	0.00954489994435698576 	
+-0.120632544614303922 	-0.0796475324973987819 	
+0.0232968722877611653 	0.0456841016756075052 	
+-0.0133317017153690587 	0.0646234660009369632 	
+-0.0852584387614009065 	0.0180954559630503303 	
+0.0528352112592731377 	-0.0761141873516854583 	
+-0.101081872518667654 	-0.0819376680973578908 	
+-0.0459535875418154091 	0.00213824328063650596 	
+-0.029106554106003997 	-0.110727468696116979 	
+-0.00395890797306441312 	0.0175274950300004229 	
+-0.0215090092317983955 	-0.0896712774928562911 	
+-0.121964926858960737 	0.065767705001880894 	
+-0.0401380015563763265 	-0.101276123247230621 	
+0.0284324854550349615 	0.0548329660062637989 	
+-0.0470073236675853032 	-0.00839467261894549961 	
+0.050587620206005407 	0.0563509932653329312 	
+-0.0690597481883651748 	0.0320654114562946083 	
+0.0473208662992663856 	0.0241074559380521046 	
+0.0635123940506462092 	0.00420828933025830382 	
+0.0545511436002203351 	0.0514803846774994078 	
+0.0649765752170751026 	0.0198470196543508032 	
+0.02964339785624533 	0.00993493870292420402 	
+0.023742498414394124 	-0.114145802727042472 	
+-0.00187765399950314622 	-0.0778128677930624846 	
+-0.116604292498409964 	-0.124884172554877521 	
+-0.0122602490947308022 	0.0374355868395198008 	
+-0.0553151493711838416 	-0.0760245423841495044 	
+-0.0289047938738029066 	-0.0903658074172669773 	
+-0.0538309537866579063 	-0.00676968804299703329 	
+-0.0306764133596947458 	0.0392628660717239189 	
+0.0233107731838695757 	-0.00345790159285788235 	
+-0.0389139668079438519 	-0.0840691835333684523 	
+-0.120854759137730527 	-0.0619500968665968807 	
+-0.110200228557434984 	0.0554872308335307499 	
+-0.0411706677281680408 	-0.108628478414043314 	
+-0.018724214606506244 	-0.00456879672499676745 	
+-0.0167862916672534661 	-0.0382422711585358038 	
+0.062751256521479129 	0.0362753147015339658 	
+-0.0348078718895850139 	-0.120238587017427415 	
+-0.110455941578583558 	0.00991234463564253168 	
+-0.0851807528351593291 	-0.0111410784080641714 	
+-0.0339580751298678152 	-0.0640107816250453932 	
+-0.00353598286695873962 	-0.0718191036224426066 	
+-0.0677595142334457418 	-0.0964041755975349191 	
+0.0213022386562033324 	-0.00517260018543139275 	
+0.0414241226751148675 	-0.0419398819675565748 	
+0.0382120973673575268 	-0.123359848456372476 	
+-0.0929881295964480187 	-0.028957920723804504 	
+-0.0247109665646048776 	-0.00951714433744917793 	
+0.0126175422127599128 	-0.0241006135509846675 	
+-0.101730186036494891 	0.0316157246083580354 	
+-0.104029401939315827 	-0.0890897101669621966 	
+0.0316467628517260596 	-0.0115046536306472325 	
+-0.109160552150135107 	-0.118572727552158905 	
+0.00226257592330038554 	-0.0796744517052890427 	
+-0.0428683774538702808 	0.0130289732273120747 	
+-0.0164520832841672932 	-0.0433770308053599285 	
+-0.10714684080366399 	-0.0166419341475811111 	
+-0.0587359674711478361 	-0.0153743704526102826 	
+-0.0115441064666440829 	-0.0542664942214843987 	
+0.00150219021435402241 	0.0512020860335165931 	
+-0.0124047942645855273 	-0.00513744872628248399 	
+-0.0492572403497689507 	0.060631313068757027 	
+0.0204587837638680638 	-0.0793406716447456889 	
+0.0289676756495262575 	0.0423600752752206211 	
+-0.0829466709775956157 	0.0328361137204496237 	
+0.0242568225615223537 	-0.0860029078247910583 	
+-0.0401690630785093844 	-0.104937097054671441 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -181,11 +181,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.056905686516290202 	0.0506708704397472037 	-0.00281005636301307664 	-0.00883585699550448571 	0.000968317251690557616 	-0.0093402359268631753 	-0.00511697102021179805 	0.00254076500836063982 	-0.00654591324086042751 	-0.00364155904253430198 	0.00817896291237939568 	-0.0107260314649677869 	-0.00859582448416375787 	-0.00872110586451123153 	-0.00976548188772113045 	-0.000336870451277782902 	-0.00688041685678251157 	-0.00350102615490139463 	0.00751567670499836502 	-0.00318769815895596524 	-0.00901948279713809913 	-0.00105238899124976762 	-0.0110583305687190266 	-0.00333929250749426037 	0.0060623229349569584 	-0.00317578753170242017 	-0.00827366343925372769 	0.00617556755864134051 	-0.00795541585758343002 	0.00698929690863082324 	0.00654178299480514744 	0.00169294246983757121 	-0.00144916085077765396 	-0.00849923651446167841 	0.00130887417565956178 	-0.00089281611779064025 	-0.00850108920620314297 	0.00801754174355075178 	-0.000575280972634687588 	0.000958759987538856137 	-0.00!
 981588084176744473 	-0.00608657171273093572 	-0.00418528552938027959 	0.00562735261577514308 	-0.000518050642663960611 	0.00384599591075757358 	0.00226942270388939653 	0.00182406737587418894 	-0.00745302084548695766 	-0.00275203624784230985 	0.00289670326500193623 	0.00571329210131102903 	-0.0103166826500750639 	-0.0106067278271120887 	0.00349608088964599092 	-0.0103525295822042718 	-0.00969075522674989025 	-0.00199977291888563851 	-0.00244342369869020694 	0.00242178409774834041 	-0.00559882884989377876 	0.00311628278103911289 	0.000164874834633516076 	0.00718502450411627643 	-0.0068113234290610343 	0.000672392566058982845 	0.0063538034110592204 	-0.0105935035307277263 	-0.00137094737061349472 	-0.010079814546680034 	-0.0041041553287970798 	0.00529336414233272726 	0.00643952878798065547 	-0.00912602857564504683 	0.00860528804414868211 	0.00461363049429684268 	0.00539813907327492664 	-0.00692511721474696647 	-0.0107749454556943548 	-0.00112986939806045779 	0.0074615780902936!
 1716 	-0.00855606835453529696 	0.00444295613423474709 	-0.0043!
 67958872
62757948 	0.00603238991047170842 	-0.0025171971459563942 	0.00758185921101216329 	0.000155711332240578772 	-0.000431246983178085804 	0.00890166994818770117 	0.00461909965740889532 	-0.00113556307955706365 	-0.000295616485385759048 	0.000702331257979741893 	-0.0109428013806251619 	0.00538394407367557247 	-0.00903117295401474444 	0.00450712775405929817 	-0.00359892962931220245 	0.00308890717513798105 	
--0.0557374783241876143 	-0.0420545575624722071 	0.00813500400145022833 	0.00229379993579923065 	0.00906162971906280222 	-0.00743593205632606721 	0.00762647643368444868 	0.00869621363055657695 	0.00397056380565017839 	-0.00579797794713223652 	0.00955106062258910364 	0.00215716664630538537 	0.00727422984820777236 	-0.000500762515461652936 	0.00459815118867967342 	0.00915734535344571443 	0.00344464560102859417 	0.00282240875012756531 	0.00817036200424728354 	-0.00208257539611419844 	0.00419592447125884897 	0.00214751070833023739 	0.000255317581120266405 	0.00943034204455690979 	-0.00699278020379851418 	-0.00729840657960003237 	-0.000260528849972028196 	-0.00460316891749462144 	0.00777252327054753483 	0.0056079871755551321 	-0.00274809084372059894 	-0.00372002594684294049 	-0.00651644438953754689 	0.0103209839040837165 	-0.00611001780234204234 	0.000184097771358787942 	-0.00780916915251383265 	0.00192688876696273451 	0.00323210381052388331 	-0.00702904698233882633 	-0.000720446!
 543576197204 	0.00699558581945907473 	0.00324777016255779717 	0.0108918231660932469 	0.00160644625032169033 	-0.00329273878532777625 	-0.0011641046204615515 	0.00455768810001198118 	0.00318682572519702039 	-0.0067666981178593601 	0.00236205422100616838 	0.00399784141973917768 	-0.0050821403727673237 	0.00348082825425537461 	-0.00478393313130366128 	0.00227700635475103566 	0.00605499926312638454 	0.0080935200085025874 	0.01050136892500937 	-0.00838322001458193487 	0.00928850191789566682 	0.0080190087972090273 	0.00977389443603242022 	0.00260957077468325057 	0.00646325893474177654 	0.00487004162247831052 	0.00993040859333413004 	0.00514526110946576711 	0.000832536835531275934 	0.00100537648604621553 	-0.00857486094001503295 	0.0014183364908354893 	0.00779028400638433292 	0.0107715494732272214 	0.0110326988617241439 	-0.00901334626340149873 	-0.00387799214547862245 	-0.00230720040427873562 	0.000256380375565748214 	0.000419927959661204188 	0.000527364086741303703 	-0.001595243!
 91287246608 	-0.0076566933826778278 	-0.00201338956872734688 	!
 0.008916
10717349773998 	-0.00492588019476571104 	-0.00240272388192210636 	0.00899383373568070354 	0.00202094779099536506 	-0.00233203482051662749 	0.00845518840813633092 	-0.00456789830484366499 	0.00927236344592883947 	0.000431382254701682337 	0.00692583736414298292 	0.00586433371064961143 	-0.00744023043615439552 	0.0111956952692525424 	0.00678182668483380286 	-0.00339477021191207651 	
+0.0571179363884330299 	0.0501099146229690012 	-0.00283641365247566321 	-0.00951556532440867887 	0.0016087601981925239 	-0.00935152806483453662 	-0.00510740503617761622 	0.00287368433171549366 	-0.00675304534641369119 	-0.00413678788857376519 	0.00850030537003726959 	-0.0111684868748137111 	-0.00780306127119203066 	-0.00814942063470409928 	-0.00954679294574281231 	-0.000614088438469789929 	-0.00731516352452465587 	-0.00282304865036278347 	0.00748388346081026255 	-0.0023990470393796281 	-0.00951902894901785444 	-0.00129605959454318141 	-0.0107927282259586976 	-0.00342207145289506265 	0.00580774854206700386 	-0.00247776382757305047 	-0.00780547079113386253 	0.0069294054970583099 	-0.00824672112506309209 	0.00713823191991569926 	0.00664356302299650738 	0.000940279142242215755 	-0.00102485514370707453 	-0.00932895795875588775 	0.00193833827130056747 	-0.000361220844477941339 	-0.00861518843955716949 	0.00815415350633161111 	-0.00131097896514859903 	0.000974099005917072251 	-0.01!
 03129990867254634 	-0.00575765436393235394 	-0.00452824694320300234 	0.00556878198151231595 	-0.00102490057129662137 	0.00455297491978201487 	0.00222249556539440719 	0.00266082274728519919 	-0.00740263579388739535 	-0.00210709456730666682 	0.00352362954929475369 	0.00654536123261181563 	-0.00959748701237313206 	-0.0101344825179493572 	0.00326867246220256639 	-0.0105244789083738365 	-0.0107385022345107932 	-0.00160905283676741353 	-0.00289771267188897131 	0.00203421491244993972 	-0.00567387788282249558 	0.00341582289983727175 	0.000530202561715857113 	0.00677671540668820481 	-0.00754837941802427061 	0.0006229809090904973 	0.0058020471328928732 	-0.0104643646725042557 	-0.00141249642388021897 	-0.00928429527767735824 	-0.00468360331918606185 	0.00499672932982297372 	0.00616881440923875823 	-0.00939918330075985298 	0.00845612416447832915 	0.00398191050011537009 	0.00574267253302497387 	-0.00666732884012975237 	-0.0109720985382393051 	-0.00153878814978031719 	0.0075306508608097!
 9482 	-0.00836043286960834464 	0.0043107997290448034 	-0.00515!
 77966711
5612208 	0.00639998094998550388 	-0.00349161079685592329 	0.00742192480911468486 	0.000247062267102248563 	-0.000498515317870227475 	0.00848067040967163954 	0.00447065128420599464 	-0.0012345618400069001 	0.000244823631494026962 	0.000862566839725849993 	-0.0106291315660229577 	0.00532573396950207587 	-0.00838831631330394685 	0.00448531943412492067 	-0.00367181364364815507 	0.00256206525739502856 	
+-0.0559497281963303936 	-0.041493601745693949 	0.00816136129091281533 	0.00297350826470341905 	0.00842118677256085274 	-0.00742463991835468594 	0.00761691044965026598 	0.00836329430720171227 	0.00417769591120346982 	-0.00530274910109274512 	0.00922971816493122627 	0.00259962205615130865 	0.00648146663523604082 	-0.00107244774526877662 	0.00437946224670130412 	0.00943456334063768817 	0.00387939226877071462 	0.00214443124558895458 	0.00820215524843536173 	-0.00287122651569053601 	0.00469547062313856872 	0.00239118131162365617 	-1.02847616400715141e-05 	0.00951312098995770253 	-0.00673820581090856137 	-0.00799643028372943286 	-0.000728721498091911024 	-0.00535700685591159863 	0.00806382853802718388 	0.00545905216427026909 	-0.00284987087191196494 	-0.00296736261924756585 	-0.00694075009660812849 	0.0111507053483779275 	-0.00673948189798301376 	-0.000347497501953908719 	-0.00769506991915977923 	0.00179027700418187322 	0.00396780180303779508 	-0.00704438600071702684 	-0.00022332!
 8298618172169 	0.00666666847066049381 	0.00359073157638053336 	0.0109503938003560793 	0.00211329617895435196 	-0.00399971779435220279 	-0.00111717748196656303 	0.0037209327286009679 	0.0031364406735974815 	-0.00741163979839500921 	0.00173512793671334962 	0.00316577228843840713 	-0.00580133601046927631 	0.00300858294509262405 	-0.00455652470386023415 	0.00244895568092062636 	0.00710274627088726754 	0.00770279992638437435 	0.0109556578982081369 	-0.00799565082928353418 	0.0093635509508243446 	0.00771946867841088492 	0.00940856670895008455 	0.00301787987211131568 	0.00720031492370501198 	0.00491945327944680322 	0.0104821648715004538 	0.00501612225124228785 	0.000874085888797997254 	0.000209857217043509637 	-0.00799541294962607778 	0.00171497130334525151 	0.00806099838512624317 	0.011044704198342024 	0.0111818627413944934 	-0.00838162626922003308 	-0.00422252560522866707 	-0.00256498877889595623 	0.000453533458110704207 	0.000828846711381063701 	0.000458291316225098943 	-0.0017!
 9087939779941862 	-0.00752453697748784248 	-0.0012235517701988!
 1382 	0.
00854851613398391157 	-0.00395146654386617675 	-0.00224278948002464224 	0.00890248280081903931 	0.00208821612568750592 	-0.00191103528200058646 	0.00860363678133929839 	-0.00446889954439381749 	0.00873192332904906832 	0.000271146672955575863 	0.00661216754954073015 	0.00592254381482310802 	-0.00808308707686518357 	0.0112175035891868662 	0.00685471069916975938 	-0.00286792829416912792 	
 ]
 ;
-bias = 2 [ -0.0323078363819910008 0.032307836381991098 ] ;
+bias = 2 [ -0.0322402029767411905 0.0322402029767412598 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -15,7 +15,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ 1.17000000000000082 0.900000000000000577 ] ;
+bias = 2 [ -1.22000000000000086 -0.870000000000000551 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -4.32711526727717022 -4.3372701645486238 0.0884972513670912453 0.0931047158365755634 0.0841804577845747942 0.0884423692492627839 0.0882859838041619854 0.0861706449956605863 0.0897606441010612016 0.0917997185462621629 0.0862172150073365201 0.0914492689853110274 0.0832344574341433024 0.0846255461316875179 0.086897933435698016 0.0902140465230860666 0.0913138521627376021 0.0839526766363259125 0.0885437376916390906 0.0832835086330831892 0.0917704045377616545 0.0900370148671758574 0.0865570415437719592 0.0889061535637738487 0.0901933883949007087 0.083855126464766741 0.0852683942355133007 0.083532078336021498 0.0903033950528041307 0.087349502272501281 0.0877663854611010485 0.0936361074968803525 0.0855698088882395069 0.0941943261400616239 0.0842970734238651792 0.0849357771997706346 0.0891822142770181264 0.0874879941894524393 0.0935054566357249178 0.0882701258129414135 0.0917743293773878727 0.0861543718441722672 0.0906792393721227302 0.0889428130004461664 0.091844875169!
 0769255 0.0838195887654502175 0.088673345702564188 0.0829929657926603698 0.0880565159446428936 0.0841799095124265706 0.0842893022098406414 0.0830294042875595467 0.0837011826816978638 0.0852232267568197449 0.0899532197532659367 0.0894885518089336696 0.0958346480546612284 0.0857821218519718293 0.0914478410021243027 0.0910177787617019729 0.0888371373050060309 0.0863973972481843211 0.0859149352705649016 0.0911501112497726668 0.0935250257635063392 0.0888385233716558198 0.0921673857226444071 0.0874503485910435724 0.0886068150786281233 0.0832175599229860452 0.0923866976451308552 0.0904643653202136522 0.0902225674836392155 0.0901632676756105172 0.089346078747887997 0.0927602015627450316 0.0860875669223204326 0.086636914767091594 0.0897427212560572563 0.0911794384643066502 0.0878912474958726958 0.0870217265658023015 0.089368170197947544 0.0939061363420257006 0.0859158022475398631 0.0952932285977266119 0.0894471842536392059 0.0877395566357450918 0.0887800292092797427 0.09130840845767!
 55023 0.089352658573633309 0.0890067708000333718 0.08483390687!
 49338343
 0.0872732569112319445 0.0863292264483929844 0.0887496467528825772 0.0841889624230460348 0.0885658836328168247 0.088839997890117095 0.0920015433771734364 ] ;
+bias = 100 [ 4.33273938062738928 4.34213993410700283 -0.088204571348629579 -0.0839684393536935664 -0.0928823561132764292 -0.0883649721278776668 -0.0884903050550344544 -0.0907308059625350216 -0.0870444767617748333 -0.0851670367578043069 -0.0906043378833479851 -0.0855157280731373287 -0.093994825582694605 -0.0923762769413334628 -0.0899213889269599026 -0.0865376360683695195 -0.0854988138212786908 -0.0931633295456289895 -0.0881682224466178444 -0.0939859157720970839 -0.0850815970387604481 -0.0867878324914827526 -0.0901873951672764601 -0.0878652643364732922 -0.0867982308213180176 -0.0933152233563941808 -0.0916264905154507009 -0.0937369638746636535 -0.0864351187425157208 -0.0894153298628880683 -0.0892212011575993919 -0.0835338549363071475 -0.0913327598696803272 -0.0830141340166982988 -0.0928174262541913153 -0.0921193265512855047 -0.0876716699574961733 -0.089403450459274475 -0.0836202697787554045 -0.0885111489819176561 -0.0851349794678951111 -0.0906305634336227184 -0.086118935539891!
 0561 -0.0881613358275717129 -0.085073325097997185 -0.0933873403560817328 -0.0880793734547384716 -0.0943401228795518326 -0.0887566532073704034 -0.0929276378631985339 -0.0928055814139878005 -0.0943105206420898068 -0.0934701923530768114 -0.0916486244331431166 -0.0869773547786528434 -0.0872351452710243142 -0.0816886634694239666 -0.0910816766531487854 -0.0853692782416057144 -0.0858641028921361266 -0.0878611086369744665 -0.0904581686504181276 -0.0908946202910138473 -0.0857090460576450386 -0.0836093889250728167 -0.0881840540347646057 -0.0847895277075860904 -0.0892344597301107101 -0.0880987817164715359 -0.0940247149622682188 -0.0846467093966444262 -0.0865013358775244268 -0.0866108410192989037 -0.086530102032007214 -0.0874001930942246985 -0.0843070574656997057 -0.0907767781573203814 -0.0901769535117404458 -0.0871757644291058509 -0.0857078607918897312 -0.088862835014407851 -0.0897134775637553616 -0.0876126284089064483 -0.0832887442018750007 -0.0909265836630956253 -0.08217540835519779!
 32 -0.0873627859717287791 -0.0890024063612101651 -0.0879269268!
 68388072
3 -0.0856800091525585228 -0.087390743888033473 -0.0877310087418665951 -0.0921551835833112065 -0.0894747740662434343 -0.0905720902783772042 -0.0880371305006222432 -0.0929021937092538164 -0.088292093490187698 -0.0879317082348496093 -0.0849675822040164286 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
@@ -45,106 +45,106 @@
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
--1.24792831633213597 	-1.39735089256445932 	
--1.2613956625633862 	-1.38981579762817375 	
-0.0761048699587760225 	0.00801901792621093762 	
--0.0437362423957267515 	0.0100258312296207363 	
-0.122341829544572872 	0.0843382980897796897 	
-0.100681906572919158 	-0.0142979123332143618 	
-0.104194045090286028 	-0.0139423394592549325 	
-0.132369740996299834 	0.0170023995994674736 	
-0.0833785830127285188 	-0.0326773524247471261 	
--0.0326950623956562686 	0.0326269485130347772 	
-0.0983250247890316564 	0.0495618447986045443 	
--0.0492018798408611485 	0.0594823721799443636 	
-0.109567946177656486 	0.125725738863893627 	
-0.0840674972530368464 	0.110534760298992382 	
-0.12312706019800275 	0.00537144577668966644 	
-0.0499502891283985229 	-0.0115941354457338875 	
-0.0106130835455938467 	-0.000502204039447316742 	
-0.129155197592288079 	0.0843559859284681718 	
-0.0555094106422285502 	0.0279252235392405679 	
-0.131579496299484372 	0.102446402857112664 	
-0.0146207819569676885 	-0.0165110880938397521 	
--0.0152098035716024798 	0.0611314970453571208 	
-0.0756480514037247004 	0.0624429547810556249 	
-0.0943313046294627644 	-0.0209162148785304661 	
--0.0387538020308656958 	0.0831676274390826392 	
-0.111079692147122572 	0.106413612787671669 	
-0.0844859003750915022 	0.0909281536163425025 	
-0.110255534107209507 	0.117569964096915211 	
-0.0503402481910374192 	-0.0145363440385552298 	
-0.0882877347052690442 	0.0278747608862448884 	
-0.13579989243262322 	-0.0293047418304829208 	
--0.0208853376616612955 	-0.0270185613474963164 	
-0.0903303247636411538 	0.0767358706059570739 	
--0.04999537211703068 	-0.0111852725638231943 	
-0.0922451774733284852 	0.112807474834139387 	
-0.0557920505124698748 	0.13176204053668622 	
--0.0156166793851998647 	0.085422366288600074 	
-0.121727203196954181 	-0.00852357251789235063 	
--0.0308060743325063273 	-0.013656142939411418 	
-0.0233263322247821842 	0.0694023941057379939 	
-0.0405156370159508103 	-0.0424488523830184525 	
-0.0652001999783928532 	0.0848180390433775772 	
-0.0479443611943429562 	-0.0216705924806779755 	
--0.0520331275937088097 	0.133010909353283802 	
-0.0295060359707903092 	-0.0331018324098504846 	
-0.0973891803189232824 	0.121966636503981624 	
-0.0223348997781607252 	0.058958206082786721 	
-0.119610467943873047 	0.123597553887780973 	
-0.000476921336380206943 	0.0993550689442175561 	
-0.116236242367810105 	0.0912979105807741886 	
-0.132418046816531965 	0.0714706742660448091 	
-0.123533091063945721 	0.118700555327761526 	
-0.133945660716787401 	0.087131676224561902 	
-0.0986966479771140132 	0.0772562696165317725 	
-0.0928489721357925596 	-0.0461591939206654617 	
-0.0674164705057713198 	-0.00995972789837364786 	
--0.0457363065061925228 	-0.0558365841657046599 	
-0.0569120685437913346 	0.104659972014240071 	
-0.0144387081701442682 	-0.00799004058277001834 	
-0.0405014545351480848 	-0.022447051109867406 	
-0.0156885394621569495 	0.0607053011427125325 	
-0.0385488185786774379 	0.10645334041582398 	
-0.0923477123362821395 	0.0638716009610263563 	
-0.0305985475582648929 	-0.0161337302924452024 	
--0.0503582362547266155 	0.0062688895039419313 	
--0.0403763892900047272 	0.122732268245548809 	
-0.0285286154269412187 	-0.0403524371964778528 	
-0.0505460324818351181 	0.0628354760976757576 	
-0.0524402592962677147 	0.0292665783345892605 	
-0.131792667337911978 	0.103591992774134056 	
-0.0348344614551319917 	-0.0519031455810079895 	
--0.0405527523441988058 	0.0773380740705313652 	
--0.0154457459363229466 	0.0563698254393016004 	
-0.0355900832138369844 	0.00386744655745152946 	
-0.0656871478796531016 	-0.00408206902396727328 	
-0.00206405204327912793 	-0.0282454046662434823 	
-0.0902178471958940686 	0.0620365098886469374 	
-0.110386287835553584 	0.0255196613515526888 	
-0.107379950901278345 	-0.0551879644255911656 	
--0.0230972154790073485 	0.0387229438492729464 	
-0.044449321979716315 	0.0577741337594893656 	
-0.0816879272017193014 	0.0433059394724301799 	
--0.0320513493983501443 	0.0988418390166333621 	
--0.033678793373187646 	-0.0207102496453733416 	
-0.100614235785147152 	0.0558074948349463126 	
--0.0385665850321436632 	-0.0497831257878785238 	
-0.0713897633525990005 	-0.0119680204600129698 	
-0.026486493415709144 	0.0803429688797962643 	
-0.0527857984995237159 	0.0241688198442964464 	
--0.0372283475335263153 	0.0509203961830493809 	
-0.0107724740413416079 	0.0520979069590650906 	
-0.057648503478319589 	0.0133097617517652072 	
-0.0706270442107162499 	0.118412798126190016 	
-0.0567206293871870756 	0.0621572219754823949 	
-0.020160473323243909 	0.127871237627920936 	
-0.0895405314619520387 	-0.0116162226188489346 	
-0.0979661989894221458 	0.109552112759567374 	
--0.0133191795274148882 	0.100124728352003506 	
-0.0933963703177931454 	-0.0181627406802816228 	
-0.0294391786966599518 	-0.0367752161076390013 	
+1.29857318855224602 	1.36455047376715077 	
+1.29503014985172449 	1.37722523831473675 	
+-0.000252780665954720218 	-0.0663565453561717616 	
+-0.120492482610137133 	-0.0645854320225070361 	
+0.0456893983539013032 	0.00972867070355152262 	
+0.0243550790860833297 	-0.0886339901078360276 	
+0.0277726720773790944 	-0.0883624560091030575 	
+0.0558589857291091341 	-0.0573831943664615332 	
+0.0069934384091225937 	-0.107146230578152063 	
+-0.109192150211851011 	-0.0417584880666270267 	
+0.0219514948606382432 	-0.024786226207574643 	
+-0.125854973288939742 	-0.0150065264583970626 	
+0.0327746851195726691 	0.0508174379487866673 	
+0.00754614672980853662 	0.0358965596485881916 	
+0.0466149485486741788 	-0.0690559363149078204 	
+-0.0264179334804802504 	-0.0860260390623684701 	
+-0.0658485990481896821 	-0.0749808175081218276 	
+0.052468480768551172 	0.00973441317483856629 	
+-0.0207510229337725359 	-0.0463586678886904191 	
+0.0548390937624918945 	0.0277343286268516252 	
+-0.0618894708438075605 	-0.0910711651982611115 	
+-0.0916013181680967875 	-0.0132164241331492651 	
+-0.000750215746710206619 	-0.0119785452684900116 	
+0.0179277584065802437 	-0.0953548503092788852 	
+-0.115090264040634674 	0.00891037791005744838 	
+0.0345144494722095727 	0.0318002140111754983 	
+0.00802274509847889991 	0.0164028539955009892 	
+0.0337013262646112879 	0.0429289556612899095 	
+-0.0260816807507645611 	-0.0890266673801552166 	
+0.0119986734108657508 	-0.0464038488597266693 	
+0.0594543527503941377 	-0.103589035001864102 	
+-0.0974894342862974717 	-0.101622646678464976 	
+0.013982098258917228 	0.00236285580240335217 	
+-0.126956301839784597 	-0.0860030077220637484 	
+0.0157963719524664842 	0.0382470126871552046 	
+-0.0206234009737232341 	0.0571259148886519713 	
+-0.0919656417300176582 	0.0110777468755206502 	
+0.0453985648840396633 	-0.0827951972378504208 	
+-0.107500047927003572 	-0.0882879545551681216 	
+-0.0528762439290722738 	-0.00484062810201232787 	
+-0.0359387819954649496 	-0.117042364171918434 	
+-0.0112224488366410788 	0.0103250371647676305 	
+-0.0284284782490021072 	-0.0961280713635520173 	
+-0.128538514008866955 	0.0584897876081584833 	
+-0.0469042015390775807 	-0.107618930689975731 	
+0.0208727968646322259 	0.0473177452652355918 	
+-0.0539056974226136409 	-0.0153078294554692268 	
+0.0428540411435972268 	0.0487613042436001592 	
+-0.0759102158970440949 	0.024902608440362517 	
+0.039694277685536572 	0.0167625801937488807 	
+0.0558099007941625438 	-0.00303854988471284138 	
+0.0468036811197067512 	0.0439206048260738849 	
+0.0572203209597936524 	0.0124824390985034117 	
+0.0221576094536766431 	0.00271125886158923206 	
+0.0165930449959407963 	-0.120524172810695052 	
+-0.00896032649170553155 	-0.0843883802869024591 	
+-0.122871642717333862 	-0.130970016326819622 	
+-0.0195060329534726962 	0.0301163330459929372 	
+-0.0620471976943272469 	-0.0825096519951731416 	
+-0.0357446869289266209 	-0.0967808757306874573 	
+-0.0607079842258980429 	-0.0137072352750048469 	
+-0.0377946640426320563 	0.0319781379436551647 	
+0.0159043582945675409 	-0.0105709436057683012 	
+-0.0457106238823807784 	-0.0905114283836155609 	
+-0.127200309513792131 	-0.068404463933075213 	
+-0.116826845402778517 	0.0482588080762488963 	
+-0.047918728355812186 	-0.114926464365377595 	
+-0.0258426034880490547 	-0.0115929898431668954 	
+-0.023832047400995135 	-0.0450295642345298688 	
+0.0549673432422954633 	0.0287941570930679257 	
+-0.0415614860833058153 	-0.126468135852143448 	
+-0.116981703220581795 	0.00300855899836534671 	
+-0.091830196492139235 	-0.0179658301158359505 	
+-0.0408627699492909238 	-0.0706212091823788801 	
+-0.0106033827865716045 	-0.0784176146443308214 	
+-0.0743325188467987286 	-0.102701653534542497 	
+0.0139361477518215471 	-0.0122491885067015586 	
+0.0339784394229312309 	-0.0488284680760181014 	
+0.0309452094450517849 	-0.129728345547099222 	
+-0.0995532082202472968 	-0.0356519795916181367 	
+-0.0317522386420212993 	-0.0164652758150018452 	
+0.00533389366990226748 	-0.0310484595693043437 	
+-0.108348407801560062 	0.0245639666817451807 	
+-0.110414329531041536 	-0.095388678428209514 	
+0.0242067557913066972 	-0.0185742036017027476 	
+-0.115454110673715221 	-0.124678425465277937 	
+-0.00480860338684518072 	-0.0862211112911418082 	
+-0.0498520813047680783 	0.00594508090814193941 	
+-0.023489889666694401 	-0.0501319023232620967 	
+-0.113632484613219023 	-0.0233651478295003982 	
+-0.0655510114657870402 	-0.0222282438482392167 	
+-0.0185844365573900501 	-0.0609538189214826748 	
+-0.00586998784931073558 	0.0437557537188896992 	
+-0.0195469593322523429 	-0.0121502364841579152 	
+-0.0563049888373474486 	0.053216137318378133 	
+0.0132454397771677983 	-0.0859383916298488704 	
+0.0214261997383835695 	0.0349233757161144104 	
+-0.0897044020026717542 	0.0257031979551521177 	
+0.0170144225586621074 	-0.0925760031804471611 	
+-0.0469138127321606577 	-0.111246197882682057 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -181,11 +181,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.053238693449869097 	0.0806262914109139861 	-0.00322832263047528444 	-0.0091034307405599528 	0.000362212364114244175 	-0.00974991141665568217 	-0.00553090895961827218 	0.00203960694798924419 	-0.00690742803008835907 	-0.00394282619132612245 	0.00766613026441769277 	-0.011021549599272245 	-0.00924724742099556664 	-0.00930398259817479673 	-0.0102344772667839687 	-0.000696014066066441171 	-0.00720779799897976366 	-0.00411788273161691094 	0.00709460022217443462 	-0.00383944268079425407 	-0.00933301520711202551 	-0.00140595604819702198 	-0.0115574854661330828 	-0.00373112402975689502 	0.00573110929815871806 	-0.00379954945814822871 	-0.00882869991630134097 	0.00553558683983973367 	-0.00831084406338638421 	0.0065247246929201622 	0.00612490476129661041 	0.00142932511701680202 	-0.00199249906965874666 	-0.00874289066291807949 	0.000708105471669862236 	-0.00145190000566321697 	-0.00887602388839884258 	0.00757538439212778955 	-0.000839859074017534706 	0.000533248145951293207 	-0.010!
 122585017289425 	-0.006600730991961898 	-0.00452815842694289077 	0.00528765362002428258 	-0.000826747184378848416 	0.00322234347196983725 	0.00185752294625764376 	0.00115790562636331872 	-0.0078704505018446589 	-0.00335996956149444293 	0.00229723571371282442 	0.00504812464862783739 	-0.0109458737169870982 	-0.0111640298400931398 	0.00314859000401087989 	-0.0107333863626845761 	-0.00990496283984226497 	-0.00252614321577472297 	-0.00276674250666862197 	0.00208774392453801947 	-0.00600209242006417235 	0.00262090610153620359 	-0.000361770208588622748 	0.00685332269176408115 	-0.00706721837785403203 	0.000315119951773165207 	0.00605543728940713833 	-0.0110544512908535408 	-0.00178970847570509976 	-0.0107338376993417629 	-0.00439294534012347158 	0.00496981016904086883 	0.00609072055761556916 	-0.0094892060786338113 	0.00821754095252104876 	0.00432696950688535569 	0.00487749096412092194 	-0.00741430702567230475 	-0.011118380498022792 	-0.00145075801315798915 	0.0070166105850248761!
 5 	-0.00903557532195551315 	0.00408713070637453744 	-0.0046231!
 64359064
14899 	0.00550622837347921126 	-0.00274284051463609185 	0.00719964435159268709 	-0.000287628302747018974 	-0.000843487492493124051 	0.00859268413927025305 	0.00423301492662385512 	-0.00153895089395540523 	-0.000865210986956573697 	0.000232639542571767957 	-0.0114273613766537781 	0.0049815735378366836 	-0.00963711400233785605 	0.00411475321119588507 	-0.00399444886549325893 	0.00278501374815410966 	
--0.0520704852577664259 	-0.072009978533638816 	0.00855327026891241141 	0.00256137368085467649 	0.00966773460663913377 	-0.00702625656653353865 	0.00804041437309092194 	0.0091973716909279548 	0.00433207859487812468 	-0.00549671079834038699 	0.0100638932705507892 	0.00245268478060985074 	0.007925652785039582 	8.2114218201916571e-05 	0.00506714656774250906 	0.00951648896823437118 	0.00377202674322583281 	0.00343926532684308336 	0.00859143848707118879 	-0.00143083087427590722 	0.00450945688123279009 	0.00250107776527749674 	0.000754472478534290274 	0.00982217356681956526 	-0.00666156656700028771 	-0.00667464465315429755 	0.000294507627075602169 	-0.00396318819869305537 	0.00812795147635048988 	0.00607255939126580702 	-0.00233121261021206711 	-0.00345640859402216458 	-0.00597310617065646351 	0.0105646380525400863 	-0.00550924909835233834 	0.000743181659231372058 	-0.00743423447031809313 	0.00236904611838570497 	0.00349668191190673173 	-0.00660353514075127067 	-0.0004137423680541!
 4525 	0.00750974509869004048 	0.00359064306012042006 	0.0112315221618441846 	0.00191514279203657987 	-0.0026690863465400191 	-0.00075220486282978811 	0.00522384984952284966 	0.00360425538155473811 	-0.00615876480420723439 	0.00296152177229528453 	0.00466300887242236325 	-0.00445294930585530147 	0.00403813026723639189 	-0.00443644224566854591 	0.00265786313523137634 	0.00626920687621874192 	0.00861989030539164758 	0.0108246877329877655 	-0.00804917984137160265 	0.00969176548806604046 	0.00851438547671192966 	0.0103005394792546022 	0.00294127258703543544 	0.00671915388353477427 	0.00522731423676413808 	0.0102287747149861774 	0.00560620886959161801 	0.00125129794062288401 	0.00165939963870793657 	-0.00828607092868861861 	0.00174189046412734513 	0.00813909223674941749 	0.0111347269762160014 	0.011420445953351779 	-0.00872668527598998052 	-0.00335734403632461471 	-0.00181801059335338542 	0.000599815417894180702 	0.000740816574758737609 	0.000972331592010054584 	-0.00111573694545!
 227201 	-0.00730086795481763289 	-0.00175818408229077628 	0.00!
 94422687
1049019638 	-0.0047002368260860147 	-0.00202050902250264187 	0.00943717337066829302 	0.00243318830031040477 	-0.00202304901159921406 	0.00884127313892139541 	-0.00416451049044531257 	0.00984195794749967087 	0.000901073970109655432 	0.00741039736017155316 	0.00626670424648849683 	-0.00683428938783130994 	0.0115880698121159 	0.00717734592101487625 	-0.00309087678492820641 	
+0.0536258697428237258 	0.079771188059310244 	-0.00326805927482090933 	-0.0101229918485100604 	0.00132124513011801247 	-0.00976680035729327739 	-0.00551641246591087937 	0.00254003340756739533 	-0.00721877830476366777 	-0.00468686665758342308 	0.00814809711888342392 	-0.0116876545229839049 	-0.00806173142044939832 	-0.00844777528480624394 	-0.00990555371904450546 	-0.00111239955721024347 	-0.00786032371784966405 	-0.00310281135291619456 	0.0070465220500588446 	-0.00265996475887015802 	-0.0100824644275515612 	-0.00177291933557880885 	-0.0111593304689046741 	-0.00385551710435362398 	0.00534703358449456403 	-0.00275507471051412252 	-0.00812720392968241563 	0.00666305567954580437 	-0.00874838287369643364 	0.00674817119844186696 	0.00627860122963375122 	0.000302175516742306347 	-0.00135664417948326088 	-0.00998550006068867763 	0.00165042782930788939 	-0.000655044087697747527 	-0.00904853174342502495 	0.00778106626316841469 	-0.00194207063149650955 	0.000555605035051326781 	-0.0108!
 686767428995933 	-0.00610774713115255167 	-0.00504316984626982951 	0.00519809660452281823 	-0.00158718442579143288 	0.00428019214136634001 	0.00178641056789957849 	0.00240861182377035982 	-0.00779553954556565058 	-0.00239444393683160275 	0.00323645674539891791 	0.006291858113373139 	-0.00986938945263454714 	-0.0104563344000978657 	0.00280662907268667931 	-0.0109917825537314533 	-0.0114696134856420804 	-0.00194047294777082747 	-0.00344848702059746088 	0.00150594576613708158 	-0.00611549231782085052 	0.00307000904075582694 	0.000185932299171378868 	0.00624048575727767507 	-0.0081723764797312394 	0.000239532188827330924 	0.00522778090410386601 	-0.010861109485718886 	-0.00185245300057644861 	-0.00954410501496315289 	-0.00526218925171827341 	0.00452255102975858587 	0.00568321458944887666 	-0.00989952837609308842 	0.00799337046359746735 	0.00338016544604234854 	0.00539398668242293839 	-0.00702719021479430734 	-0.0114150502039247568 	-0.00206543733759186532 	0.0071197648460417095!
 6 	-0.00874220908263338183 	0.00388714478005785851 	-0.0058058!
 68728593
84026 	0.00605745616534606943 	-0.00419900638732950346 	0.00695931687446142786 	-0.000151112416788917817 	-0.000944819039427680259 	0.00795937629094451218 	0.00400941810416922256 	-0.00168785423660831619 	-5.54994787460903836e-05 	0.000472649845764057538 	-0.0109565853518955626 	0.00489409718203449948 	-0.00867480616208503134 	0.00408101082119373838 	-0.00410396684372572526 	0.00199467788050524493 	
+-0.0524576615507209854 	-0.0711548751820349351 	0.00859300691325806318 	0.00358093478880480492 	0.00870870184063538347 	-0.00700936762589595818 	0.00802591787938353868 	0.00869694523134979976 	0.00464342886955343599 	-0.00475267033208308289 	0.00958192641608508061 	0.00311878970432149377 	0.00674013678449342842 	-0.000774093095166621657 	0.00473822302000300333 	0.0099328744593782034 	0.00442455246209573494 	0.0024241939481423661 	0.00863951665918676059 	-0.00261030879619999091 	0.00525890610167225464 	0.00286804105265928881 	0.000356317481305886948 	0.00994656664141626082 	-0.00627749085333609898 	-0.00771911940078836688 	-0.000406988359543328975 	-0.0050906570383990879 	0.00856549028666053237 	0.00584911288574413782 	-0.00248490907854921313 	-0.00232925899374765866 	-0.00660896106083192804 	0.0118072474503107122 	-0.00645157145599034414 	-5.36742587341041639e-05 	-0.00726172661529194633 	0.002163364247345082 	0.00459889346938568803 	-0.00662589202985131021 	0.0003323493575!
 55962119 	0.0070167612378806898 	0.00410565447944734752 	0.0113210791773455596 	0.00267558003344916129 	-0.00372693501593652967 	-0.000681092484471729014 	0.00397314365211581897 	0.00352934442527573933 	-0.00712429042887002296 	0.00202230074060919798 	0.00341927540767706512 	-0.00552943357020784301 	0.00333043482724112603 	-0.00409448131434433926 	0.00291625932627824144 	0.00783385752201855128 	0.00803422003738779936 	0.0115064322469165814 	-0.00746738168297064828 	0.00980516538582273771 	0.00806528253749231108 	0.00975283697149454537 	0.0035541095215218645 	0.00782431198541197644 	0.00530290199970997762 	0.0110564311002894437 	0.00541286706445689471 	0.00131404246549422983 	0.000469666954329339388 	-0.00741682701709382806 	0.00218914960340962396 	0.0085465982049161178 	0.0115450492736752612 	0.011644616442275357 	-0.00777988121514700849 	-0.00387383975462663507 	-0.00220512740423139822 	0.000896485123796164759 	0.00135549589919261009 	0.000869177330993192872 	-0.0014091031!
 8477438469 	-0.00710088202850090496 	-0.000575479712761079159 !
 	0.00889
104091862338505 	-0.00324407095339257403 	-0.00178018154537139283 	0.00930065748471015433 	0.00253451984724496803 	-0.00138974116327348512 	0.00906486996137605312 	-0.00401560714779240682 	0.00903224643928919081 	0.000661063666917365662 	0.00693962133541331429 	0.00635418060229068181 	-0.00779659722808411382 	0.0116218122021180016 	0.00728686389924735212 	-0.00230054091727934385 	
 ]
 ;
-bias = 2 [ -0.0480886388473655835 0.0480886388473659929 ] ;
+bias = 2 [ -0.0479920603803564855 0.0479920603803568463 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -15,7 +15,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ 0.0566666666666666707 0.0600000000000000047 ] ;
+bias = 2 [ -0.083333333333333301 -0.0600000000000000047 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -3.67929816976774715 -3.6682448752547705 0.0750909232339263683 0.0795841166700318875 0.0708342668804167325 0.0750421938062638405 0.0749057540871664457 0.07281350621284427 0.0763556633275116747 0.0782689083009728281 0.072824643124686117 0.0779183009350939287 0.0699000332593961438 0.0712420215383051392 0.073542444248528932 0.0767792116507820255 0.0778439304287265182 0.0706109057057301009 0.0751034493617696863 0.069940057614123502 0.0783087691931432239 0.0765286316068690409 0.0731593533332815099 0.0755157467085580819 0.0766351376284480079 0.070479449352692139 0.0718806226985668345 0.0701457067916425731 0.0768776599755875978 0.0739384583091209219 0.0743774129171129589 0.0801217793115617943 0.0721628274764351541 0.0806814897703578976 0.0708951835495260224 0.0715097933984782386 0.0756688379895598395 0.0740952778784406407 0.0799916253106679881 0.0747857117152906925 0.0783304419111606476 0.0727520148249376497 0.0772393904668482456 0.0753918317216105416 0.07838219159449!
 77049 0.0704261841624362062 0.0751946302171443287 0.0696449037214908612 0.0745767703531106385 0.070806479283302623 0.0709348311649994623 0.0696759908129132854 0.0703636071845372135 0.0718591719674553375 0.0765280528752296935 0.0760744758082740613 0.0823326087023871778 0.0723685391237849779 0.0779853133866276332 0.0775462066951605189 0.0753792470513583723 0.0729536446087890172 0.0725329279936358651 0.0776784049340394772 0.0800016351796151021 0.0753005795642522346 0.0787062629788652934 0.0740323485729651176 0.0751659610584930055 0.0698937309834748999 0.0789209919201119686 0.0769177427996144136 0.0767115469459718896 0.0767304756154562617 0.0759158170119265813 0.0792541201190776479 0.0726706650970279366 0.0732572878386490989 0.0763562455137006407 0.0776622930565265407 0.0744300500286751709 0.0736199596196006473 0.0758170804080609273 0.0803895298645543877 0.0725294734781802286 0.0817744617003312024 0.0760032369969883548 0.0742856132339138747 0.0753395223184904345 0.0777583837745!
 135925 0.0758727019124877389 0.0755613483759634968 0.071438449!
 98902137
98 0.0738360175084723652 0.0728876094213855713 0.0753375538997586996 0.0708094847711961267 0.0750639894616385306 0.0754455419716839742 0.0785276661645328 ] ;
+bias = 100 [ 3.68880251319452146 3.67455572697473087 -0.0748056477610814785 -0.0707067281518160839 -0.0794316222066286659 -0.0749255137498329588 -0.0750635319871154894 -0.0772618806308057321 -0.073641443280301172 -0.0718607350161090258 -0.0771651258335693119 -0.0722440742688146703 -0.0805696794069616362 -0.0789613112572297854 -0.0764742098994685704 -0.0731698766452147792 -0.0721761964128051908 -0.0797042166243662892 -0.0747733902094199904 -0.0805211583755357507 -0.0717639495149448581 -0.073464206098538723 -0.076784839179855971 -0.0744510158274711425 -0.0734646795027404376 -0.0798614449615559774 -0.0782103179133733784 -0.0802772948589520613 -0.073078045351197371 -0.0759826086790340721 -0.0757239946960654442 -0.0702277123773308726 -0.07789291394744563 -0.0697834112429474712 -0.0793754900503946537 -0.0787186866454939738 -0.0743382352194213447 -0.0759276280251790225 -0.070338746160929097 -0.0751299449899901495 -0.0717849484592146675 -0.0772394307811129083 -0.0727521073155275727!
  -0.0748553547437217776 -0.0717221705041391627 -0.079942813452833078 -0.0747078261285098327 -0.0808907990333310734 -0.0754205994699822385 -0.0794675734812663737 -0.0793329145737262237 -0.0808518798972240882 -0.0800058752450361899 -0.0782272636798783116 -0.073533293451067322 -0.0738515406762122323 -0.0684666617143456502 -0.0776928711958970869 -0.0720488995279365274 -0.072474735612465771 -0.0745251009662933339 -0.0770781602286758905 -0.0774730651132638504 -0.0723408065348730839 -0.0703609060057496655 -0.0748692514306317225 -0.0714440308086841808 -0.0758636992657139575 -0.0747095125039830971 -0.0805721793124508645 -0.0712816233477524819 -0.0731863774792445715 -0.0732856325934941738 -0.0731935352174640369 -0.0740010586698171685 -0.0709587708179019033 -0.0773305918377132778 -0.0767297062261277779 -0.0737460077516916973 -0.0723959745071232341 -0.0754641989578205136 -0.0762994147979306481 -0.0742707440510437106 -0.0700104862165812913 -0.0774873633583698374 -0.0689049443094244818 -!
 0.0739378324707463813 -0.0756424682784822416 -0.07453836879051!
 58132 -0
.0723568603509939928 -0.0740438450018933436 -0.0743291358145748288 -0.0787535633411283881 -0.0760738545048729037 -0.0772240880043779665 -0.0746081796641250433 -0.0794668618979842839 -0.0749625415005695794 -0.0745145065393848505 -0.0716019257964348038 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
@@ -45,106 +45,106 @@
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
--0.52038541925781967 	-0.607667740189380523 	
--0.547335303272597051 	-0.574510722075016633 	
-0.0504029420063143857 	-0.0174374020209189032 	
--0.0726720177154327324 	-0.0166870493884907455 	
-0.0985284220175968173 	0.0611863005277123173 	
-0.0754034367023085489 	-0.0400349156555762642 	
-0.078973421957143769 	-0.0396774616380166784 	
-0.108170790392739913 	-0.00765013072823586982 	
-0.0574395119975671803 	-0.0591070156090361745 	
--0.061007039891299214 	0.0066718476208302898 	
-0.0736287540104566957 	0.0253895592031331274 	
--0.0777417566768631441 	0.033988152696632544 	
-0.085793260023404358 	0.10336889613750673 	
-0.0595732996791215677 	0.0876320649751073655 	
-0.0985639370941749221 	-0.0196922373217131852 	
-0.023366080547435969 	-0.0378252019980260118 	
--0.0169121891224506757 	-0.0268755261362537863 	
-0.105509436301355369 	0.0612729979063185229 	
-0.0295185514598919063 	0.00279503185259742009 	
-0.108172318468795839 	0.0798166168810517801 	
--0.0129827416114516835 	-0.043273012491637404 	
--0.0427436926119571375 	0.0360769453606456306 	
-0.0504779492722270823 	0.0383236706034821642 	
-0.0687895317968882231 	-0.0469326547384686793 	
--0.0666313507642620761 	0.0584517370219266724 	
-0.0872457685108091324 	0.0837080805890952351 	
-0.0598201510048938442 	0.0675784349064649376 	
-0.0865288471252173008 	0.0951316360468640598 	
-0.0237140180718280608 	-0.0408629407609415601 	
-0.0631379942820747275 	0.00308499348420874673 	
-0.111272744450818958 	-0.0550198793991546961 	
--0.0495399255127684165 	-0.0544110084214884313 	
-0.0657230040930184001 	0.0531562468007424349 	
--0.0793895470235106288 	-0.0385651446308253806 	
-0.0680320003763357217 	0.0900858662837026314 	
-0.0308277344266279264 	0.10910237774397652 	
--0.0428935242523314775 	0.0609697782115297127 	
-0.0970561821731235064 	-0.0338666308471151159 	
--0.0596192092162303872 	-0.0408316442859519985 	
--0.00304223746333419452 	0.0449917804586676268 	
-0.0133410948519769729 	-0.0696020236616901877 	
-0.03997782323945865 	0.0611241954582473629 	
-0.0212044378814250263 	-0.0481804311018778769 	
--0.0798320840154110589 	0.109307593668522079 	
-0.00216013628189293694 	-0.0601016310390021027 	
-0.0733821940828805336 	0.099497308314277666 	
--0.00418577457569986339 	0.0342672749731996601 	
-0.0961049167980579794 	0.101328682742033507 	
--0.0262656255829718105 	0.0753752819357317028 	
-0.0923863548467529772 	0.0682949741016277145 	
-0.108756281771888105 	0.0481458386888888659 	
-0.100093215436711402 	0.0963736142319992356 	
-0.11044140819668434 	0.064159753976061315 	
-0.0742272888131287334 	0.0536989130170025461 	
-0.0670662090171207786 	-0.0727680558490813728 	
-0.0412975622692688532 	-0.035973626288952322 	
--0.0755336957322752522 	-0.0843988974337767928 	
-0.031683218853364746 	0.0813611862515050827 	
--0.0130739435676640054 	-0.034526988248239604 	
-0.0136140272169084763 	-0.0489930421672095315 	
--0.011061723879014709 	0.0359297344246217523 	
-0.0128932585850817186 	0.0830364377215482841 	
-0.0676125872825022733 	0.039953970286474183 	
-0.00349451945220510861 	-0.0426454815726958619 	
--0.0795372606084337741 	-0.0206274492374718434 	
--0.0679556269290350762 	0.0989090112668678106 	
-0.00106926327384114202 	-0.0675641733945039902 	
-0.0247260524176569838 	0.0384630055092971754 	
-0.026378225328460677 	0.00413268501827163039 	
-0.108363320390186382 	0.0809527489082843277 	
-0.00744067734895159909 	-0.0793276345356500878 	
--0.0685828648959103254 	0.0524331672735784751 	
--0.0430299179219996233 	0.0311991517220432564 	
-0.00875455007682279102 	-0.0221527377393929761 	
-0.0396190903019198737 	-0.0299243858711141855 	
--0.0259299398002285457 	-0.0553743962254750755 	
-0.0654815998567056939 	0.0381233449386994272 	
-0.0857406226041131925 	0.000868266886166538395 	
-0.0818156967821113412 	-0.0819606571238428866 	
--0.0510867284702254815 	0.0130209282921754724 	
-0.0185073534397708767 	0.0332937538289315785 	
-0.0564972252169523639 	0.0187982842748548977 	
--0.0595800360354162978 	0.0745776702050969659 	
--0.0626526219961449776 	-0.0481068295040314731 	
-0.0760290724928658901 	0.0318015323295316663 	
--0.0680175615269424244 	-0.0780292036804604988 	
-0.045426527393173266 	-0.0379064295013739616 	
-0.000245257330670841593 	0.0561754669591406422 	
-0.0266789063222490352 	-0.00109026801003190752 	
--0.065426210107723759 	0.0254095633358379196 	
--0.0161558737254881811 	0.0270927473390010429 	
-0.0315749936922243776 	-0.0121516449151251385 	
-0.0458707558030447202 	0.0955607640219145738 	
-0.0311126203336938301 	0.0378846159519728373 	
--0.00581538096481791413 	0.104733200172821811 	
-0.0640125283786390359 	-0.0373943635204284364 	
-0.0738255534250779194 	0.0867777875959012679 	
--0.0404066042800288808 	0.0760341306135597778 	
-0.0678668982597476489 	-0.0441082122306330204 	
-0.00207766362528058381 	-0.0638437672563030023 	
+0.60324184452909746 	0.52610861717432944 	
+0.573889414252517605 	0.556757554437076529 	
+0.0261172339319165163 	-0.0417266797118322 	
+-0.0970764807226444371 	-0.0410491724357002485 	
+0.0741131894083618126 	0.0367940492146358877 	
+0.0511143452564637932 	-0.0643270428244155645 	
+0.0546696974742041514 	-0.0639812835063832963 	
+0.0838175544274623319 	-0.0319622112866029748 	
+0.0331478951397085053 	-0.0834267803158253995 	
+-0.085346975633093361 	-0.0176323575944970674 	
+0.0493112301169950987 	0.00108652499183434985 	
+-0.102107014510997435 	0.0096758636761334945 	
+0.0613318419464063264 	0.0788860997150088589 	
+0.0352036300223434601 	0.063239167992612727 	
+0.074227168976902777 	-0.0440016598760048436 	
+-0.00092084284171127951 	-0.0621321696535562032 	
+-0.0412257856745673959 	-0.0511947772496824521 	
+0.0810782785497820846 	0.0368716425058080652 	
+0.00524895074668816318 	-0.0214776774928671747 	
+0.0837077558817662798 	0.0553725060799138274 	
+-0.0373082375788181958 	-0.0676159155161298459 	
+-0.0670440376649158037 	0.0117951035043272791 	
+0.0261694113569656273 	0.0140156385848142825 	
+0.044493486467578329 	-0.0712406715889533082 	
+-0.0909357541752950777 	0.0341783580594380298 	
+0.0628356885182562996 	0.0592957337386767619 	
+0.0354753811298013089 	0.0432252770180205914 	
+0.0621082037461518147 	0.0706981017678859486 	
+-0.000580808082183228145 	-0.065179720660335741 	
+0.038850672569852876 	-0.0211946431624295793 	
+0.0869594178043191363 	-0.0793162883123661039 	
+-0.0739242031243203951 	-0.0787995735400271458 	
+0.0413984147681554018 	0.028833693246709851 	
+-0.103851286532993733 	-0.0629881050587312308 	
+0.0436587846659366521 	0.0656916133076659303 	
+0.00648415962861266255 	0.0847055847217833835 	
+-0.0671858402426120105 	0.0366864364727969849 	
+0.0727509233151331647 	-0.0581538430599003314 	
+-0.0840172959618063486 	-0.0652142892658817697 	
+-0.027306329445590017 	0.0207231266998691432 	
+-0.0109737322845146375 	-0.093963630514310531 	
+0.015661041083923171 	0.0367955952452021021 	
+-0.00308659411959401374 	-0.0724990326043745276 	
+-0.10416475850439072 	0.0849801137766668657 	
+-0.022150463268884802 	-0.084448224751915768 	
+0.0489822036368685182 	0.0750707485878217423 	
+-0.0284536317518466891 	0.00999823954382460071 	
+0.0716328620142713535 	0.0768460562442043332 	
+-0.0505608144640681545 	0.0510695206284420555 	
+0.0679880308741660638 	0.043909685427124992 	
+0.0843441106583406597 	0.0237727328559663408 	
+0.075623920417366583 	0.0719023355961756483 	
+0.0859934587953604807 	0.0397452099532621464 	
+0.0498672742800235422 	0.0293461945737641255 	
+0.0427886162805164172 	-0.0970826872115481271 	
+0.0170111273480947796 	-0.0602761089824957325 	
+-0.10006493915181143 	-0.108939149963149823 	
+0.0073603412153819929 	0.0570115030349874485 	
+-0.0373917485400131339 	-0.0588563328154444176 	
+-0.0106630295782426266 	-0.0732984366739459492 	
+-0.0353513704442752486 	0.0116412035123935557 	
+-0.0114082774919415722 	0.058706100029514062 	
+0.0432824628029821259 	0.0156319939448138767 	
+-0.0207931958773757281 	-0.0669553416286962771 	
+-0.103966467156310513 	-0.0450079640470971015 	
+-0.0922739886998298897 	0.0745934058759385316 	
+-0.0232514676085771553 	-0.0919278547650759642 	
+0.000432885488560150384 	0.0141648235669390096 	
+0.0021074033303418011 	-0.0201413191849692079 	
+0.083885644304006815 	0.0564950500408555253 	
+-0.0168759761212089951 	-0.103701250339013146 	
+-0.0929026382573996112 	0.0281495766550947588 	
+-0.0673304729666608787 	0.0069182639265360571 	
+-0.0155437347969988628 	-0.046461453752145547 	
+0.0153448363653640348 	-0.054211707033286094 	
+-0.0502592553615646143 	-0.0797252286538448762 	
+0.0411763690141464175 	0.0138251725859302194 	
+0.0614203820515334328 	-0.0234296605012813929 	
+0.057509547962485405 	-0.106304249680942875 	
+-0.0754086609777919192 	-0.011273652281573298 	
+-0.00575725713588796581 	0.0090247286111742233 	
+0.0321995055439566608 	-0.00549372924335959478 	
+-0.083872824153877798 	0.0502993195370103341 	
+-0.0870672370416587377 	-0.0725084402829943298 	
+0.0517006246513320791 	0.00748826658549397051 	
+-0.0924935534593258474 	-0.102515504356175402 	
+0.0211636098592593302 	-0.0621858826813804083 	
+-0.0240399595060106254 	0.0318806266452920511 	
+0.00240796449009358622 	-0.0253653007642216863 	
+-0.0897490519257006375 	0.00112661385718494525 	
+-0.040436786628099429 	0.00281631858396141476 	
+0.00730964533356637594 	-0.0364237165347537015 	
+0.0215128174012390073 	0.0711670024008028562 	
+0.00683328492840286957 	0.0136007448724301452 	
+-0.0301366831378556618 	0.0803666391553434917 	
+0.0397323374070795987 	-0.0616827204556885042 	
+0.0494355409814625363 	0.0623732547212860269 	
+-0.0647038813372060456 	0.0517347343407355276 	
+0.0435739884637324215 	-0.0684115705809415775 	
+-0.0222274194688612953 	-0.0881885023254915384 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -181,11 +181,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0113262925564693168 	0.00641492602461605013 	-0.00159847381051862899 	-0.00789459485609227303 	0.00244219358215985814 	-0.0081261550019702377 	-0.00389482222121642152 	0.00389091998488833111 	-0.00541050652884287036 	-0.00262556755390811825 	0.00952868237595928119 	-0.00969494151094163828 	-0.00706508174692422209 	-0.00727612306607582256 	-0.00846038975666718249 	0.000773966054525186621 	-0.00583361570628981342 	-0.00201278590637816437 	0.00872572454424036352 	-0.00165701502752675124 	-0.00799891102918076635 	6.47096589051982863e-05 	-0.00973055555769559294 	-0.00215375660638213173 	0.00716715790392904832 	-0.00168040418721199583 	-0.00686749154366765677 	0.00769175181889198388 	-0.00685037181174525354 	0.00827079361448006777 	0.00779395048534614263 	0.00260805740919305227 	-5.93787847705055239e-05 	-0.00761901303209245716 	0.00277648344971009441 	0.000531124508030605003 	-0.00733566395363501652 	0.00928862170370055307 	0.000345824306290504029 	0.00218347321736047685 	-0.!
 00879584583154094379 	-0.00473551449789763775 	-0.0031015010128094731 	0.00679600317669165795 	0.000498927731410178546 	0.00534312277919853518 	0.00347021641974513528 	0.00337220069515059041 	-0.00622143586125560328 	-0.00127655934433769597 	0.00436484410802257493 	0.00725999145664699228 	-0.00881265446020709255 	-0.00919797829396393728 	0.00462042560691856407 	-0.00919982500002390104 	-0.00889966122589246934 	-0.000627033368822297178 	-0.00140435990570596876 	0.00348752003687191961 	-0.00440996403305462647 	0.00445091226375020181 	0.00153199411182978963 	0.00824276166398772887 	-0.00589422826971325482 	0.00185054823285728731 	0.00735205392812524582 	-0.00932065008082455862 	-0.000164838547741595403 	-0.00854643924540077898 	-0.00311837434025636013 	0.00638201655108370635 	0.00754628751573688424 	-0.00801299059807394388 	0.00976747753260353083 	0.00557945832622598354 	0.00675698139843258164 	-0.00560185246073486524 	-0.00964174761582759464 	-7.80623115802624038e-05 	0.00871!
 036682392008037 	-0.00725561868064580548 	0.005595311930029335!
 18 	-0.0
0346921625696461865 	0.00740028832034525432 	-0.00169509809182802015 	0.00873888335497757102 	0.00140994409075873494 	0.000764624405132729979 	0.00994480655086805825 	0.00577877274202991901 	4.74923655357136959e-05 	0.0011353196069151506 	0.00198775187931363694 	-0.00960880431385865638 	0.00658056776284521566 	-0.00755775334700503273 	0.00570729827446314446 	-0.00240907164842276734 	0.00409745863965042725 	
--0.0101580843643666804 	0.00220138685265901506 	0.00692342144895578045 	0.00135253779638701581 	0.00758775338859351851 	-0.00865001298121899007 	0.00640432763468906565 	0.0073460586540288822 	0.00283515709363264898 	-0.00681396943575839856 	0.00820134115900921293 	0.00112607669227921393 	0.00574348711096823398 	-0.00194574531389705811 	0.00329305905762568817 	0.00804650884764271553 	0.00239784445053587304 	0.00133416850160433245 	0.00696031416500525382 	-0.00361325852754340766 	0.00317535270330150578 	0.00103041205817527436 	-0.00107245742990319179 	0.00824480614344477378 	-0.00809761517277059889 	-0.00879378992409047752 	-0.001666700745558108 	-0.00611935317774526394 	0.00666747922470936009 	0.00432649046970583293 	-0.00400025833426162231 	-0.00463514088619843348 	-0.0079062264555447067 	0.00944076042171451517 	-0.00757762707639256846 	-0.00123984285446245143 	-0.00897459440508197211 	0.000655808806812932781 	0.00231099853159869245 	-0.00825376021216044781 	-0.001740481553!
 80268415 	0.00564452860462577328 	0.00216398564598699068 	0.00972317260517675808 	0.0005894678762475515 	-0.00478986565376872571 	-0.00236489833631729068 	0.00300955478073557537 	0.00195524074096567512 	-0.008242175021364 	0.000893913377985542366 	0.00245114206440323395 	-0.00658616856263531582 	0.00207207872110719412 	-0.00590827784857623703 	0.0011243017725706564 	0.00526390526226897665 	0.00672078045843925789 	0.00946230513202510858 	-0.00944895595370550713 	0.00809963710105650325 	0.00668437931449794229 	0.00840677515883615084 	0.00155183361481183628 	0.00554616377539399619 	0.00369188595568001928 	0.00893215807626812804 	0.0038724076595626016 	-0.00037357198734062414 	-0.000527998815233036468 	-0.00956064192855578081 	0.000329684082084518717 	0.00668352527862810502 	0.00965851149565611841 	0.00987050937326929348 	-0.0099791740953306448 	-0.00523683447063627659 	-0.00363046515829083902 	-0.000876817464300999236 	-0.000631879126818990543 	-0.000721424646885170237 	-0.002!
 8956935867619593 	-0.00880904917847242369 	-0.0029121321843903!
 1248 	0.
00754820876362416286 	-0.00574797924889403674 	-0.00355974802588750845 	0.00773960097716254198 	0.000825076402684549767 	-0.00337517142319696765 	0.00729551532351534106 	-0.0057509537499364459 	0.00784142735362793053 	-0.000854038366632214711 	0.0055918402973764306 	0.00466771002147996824 	-0.00891365004316409248 	0.00999552474884863457 	0.0055919687039443864 	-0.00440332167642452834 	
+0.0112930086105568449 	0.00616351561707523341 	-0.00160124426969843322 	-0.00802923364774296537 	0.00257365332377883053 	-0.00812585784087365738 	-0.00389030633538853428 	0.00396073978712296968 	-0.00544979422319446811 	-0.00272314781273958958 	0.00959605413022264313 	-0.00978206878944272787 	-0.00690313915092731056 	-0.0071583777452128329 	-0.00841361536316625677 	0.000720548967869462094 	-0.00591881034530785961 	-0.00187381153038211645 	0.00872183759475835506 	-0.00149595484940319304 	-0.00809712575578339783 	1.78988179957268726e-05 	-0.00967442763147958844 	-0.00216791085383057818 	0.00711801911543108196 	-0.00153741792568698357 	-0.00677056645157830383 	0.00784590579796891861 	-0.00690663089690580177 	0.00830341046778483062 	0.00781717632155564529 	0.00245902778824725905 	2.87068481194732033e-05 	-0.00778361873442472499 	0.00290579750983127908 	0.000640988181111136176 	-0.00735628493145134513 	0.00931883557940061009 	0.000200120989175870258 	0.00218908285970437081 	-0.0!
 0889358298502484178 	-0.00466660676048613349 	-0.00316817797404978733 	0.006786580520917373 	0.000399249430999985605 	0.00548796099863497165 	0.0034632416153528708 	0.00354287535829364911 	-0.00620872539959907606 	-0.00114420335390142563 	0.00449362450527556086 	0.00742971657202637793 	-0.00866544049941442296 	-0.00910025631504250525 	0.00457702039075809884 	-0.00923199573811228288 	-0.00910743854459208566 	-0.000545625040137308077 	-0.00149347604161448018 	0.0034118547128509924 	-0.00442262911536558251 	0.00451397641136160651 	0.00160821261276277556 	0.00816291825515298546 	-0.00604038605311225853 	0.00184300334127493535 	0.00724334381932651827 	-0.00929205025401912653 	-0.000170697534500130878 	-0.00838400966523670556 	-0.00323266907268441716 	0.00632436577670770703 	0.00749401810403165728 	-0.00806559711156345713 	0.00973990699691918739 	0.00545469430216973059 	0.00682901262664863465 	-0.00554724677438882523 	-0.00967905977762816526 	-0.000158221712962183759 	0.008726836!
 61315692104 	-0.00721359382370552471 	0.00557098704223077303 	!
 -0.00362
573588239716374 	0.00747697002782156915 	-0.0018883244797203299 	0.00870913922116942768 	0.00143092288873057183 	0.000753574583016262782 	0.00986211025418400229 	0.00575127316528340708 	3.00429243871667279e-05 	0.00124685295869076751 	0.00202262793202890537 	-0.00954277131961513321 	0.00657137608518045761 	-0.00742577634385300882 	0.00570537745406464892 	-0.00242122632293401228 	0.00399375881836177768 	
+-0.0101248004184541999 	0.00245279726019983699 	0.0069261919081355771 	0.001487176588037696 	0.00745629364697455478 	-0.00865031014231556865 	0.0063998117488611771 	0.00727623885179423842 	0.0028744447879842489 	-0.00671638917692694761 	0.00813396940474585099 	0.00121320397078031263 	0.00558154451497132244 	-0.00206349063476004646 	0.00324628466412478457 	0.008099925934298478 	0.00248303908955393397 	0.00119519412560828496 	0.00696420111448728483 	-0.00377431870566696803 	0.00327356742990412815 	0.00107722289908474683 	-0.0011285853561191848 	0.00825896039089322499 	-0.00804847638427265075 	-0.0089367761856154887 	-0.00176362583764744707 	-0.00627350715682216311 	0.00672373830986990745 	0.00429387361640116462 	-0.00402348417047111804 	-0.00448611126525261944 	-0.00799431208843467966 	0.00960536612404676825 	-0.00770694113651373362 	-0.00134970652754298629 	-0.00895397342726560359 	0.000625594931112897655 	0.00245670184871332265 	-0.00825936985450433959 	-0.00164274440031878!
 595 	0.00557562086721426989 	0.00223066260722730664 	0.00973259526095102481 	0.000689146176657742002 	-0.00493470387320516218 	-0.00235792353192502361 	0.00283888011759251581 	0.00194253027930914985 	-0.00837453101180023543 	0.000765132980732538765 	0.00228141694902383356 	-0.00673338252342801317 	0.00197435674218579592 	-0.00586487263241577267 	0.001156472510659066 	0.00547168258096855827 	0.00663937212975426391 	0.00955142126793357901 	-0.00937329062968458512 	0.00811230218336748271 	0.00662131516688653672 	0.00833055665790316709 	0.00163167702364654175 	0.00569232155879299991 	0.00369943084726236408 	0.00904086818506683911 	0.00384380783275716604 	-0.000367713000582090021 	-0.000690428395397136674 	-0.00944634719612769992 	0.000387334856460506711 	0.00673579469033333197 	0.00971111800914562819 	0.00989807990895363865 	-0.00985441007127441093 	-0.00530886569885233046 	-0.00368507084463687642 	-0.000839505302500462552 	-0.000551719725437070218 	-0.000737894436122015461 	-0!
 .00293771844370225915 	-0.0087847242906738416 	-0.002755612558!
 95776609
 	0.0074715270561478463 	-0.00555475286100175387 	-0.00353000389207932651 	0.00771862217919071832 	0.000836126224801017831 	-0.00329247512651293078 	0.0073230149002618556 	-0.00573350430878788626 	0.00772989400185233877 	-0.000888914419347477188 	0.00552580730313291003 	0.00467690169914472542 	-0.00904562704631614416 	0.00999744556924713271 	0.00560412337845562093 	-0.00429962185513587704 	
 ]
 ;
-bias = 2 [ -0.0027464009200991687 0.00274640092009921163 ] ;
+bias = 2 [ -0.00270410090954439257 0.00270410090954443074 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -15,7 +15,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ 0.259999999999999731 0.320000000000000118 ] ;
+bias = 2 [ -0.306666666666666698 -0.310000000000000053 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -4.88348781864529524 -4.8837790966619945 0.0998136183975280172 0.105471817501073567 0.0944416603126441573 0.0997535283630023167 0.0995807079469193807 0.0969420280226542391 0.101407798986863112 0.103819719061399696 0.0969561262386221118 0.103379686484071684 0.0932604806258945512 0.0949581076326168755 0.0978612202826870392 0.101940630727569342 0.103281189924958716 0.0941595431112412506 0.0998305935628365643 0.0933123105594513141 0.103865433445130023 0.101629187737637239 0.0973778633969569346 0.100349460306733224 0.101767253426589127 0.0939952628691959613 0.0957642872874144846 0.0935745523905245069 0.102064003256194066 0.098361691863909001 0.0989175053269632093 0.10614746415871347 0.0961217644556267709 0.106849253706285993 0.0945216452223907322 0.0952991047808851438 0.100547639463152719 0.0985605833598848236 0.105983428725311621 0.0994327692125842322 0.103893097444509405 0.0968643024791368545 0.102520198423240241 0.100204431378448769 0.103958775859172015 0.0939290!
 211077105641 0.099947437518405513 0.0929393923414386647 0.0991701323486275238 0.094408220117351227 0.0945697106806672877 0.0929791640592441959 0.0938470820938613198 0.0957359700320323803 0.101626912170330416 0.101052854003951254 0.108921157875693661 0.0963814492151018887 0.103458632943262935 0.10290812806515362 0.100178659212683582 0.0971214500704266626 0.0965871618141980054 0.103073960705580392 0.105996494971855509 0.100087602274505402 0.104366272501995289 0.0984793481047083058 0.0999092875429416077 0.0932523589552438636 0.104637234166777962 0.10212217225891014 0.10185947931508095 0.101878379680090239 0.100853886423779154 0.105057249424197946 0.0967631186351052691 0.0975018372905746139 0.101409956557437189 0.103055880161497679 0.0989832218691384519 0.0979591861849546941 0.100737252815511033 0.106483496226710855 0.096583400725600066 0.108223002610933108 0.100965208043670873 0.0988008819013017348 0.100127932147128867 0.103179296286824845 0.100801354336986573 0.10040789257892!
 4282 0.0952068316396505326 0.0982331855345689731 0.09703896325!
 95778512
 0.10012569038037146 0.0944119786063295219 0.0997856098841084088 0.100261110879061879 0.104142637036895788 ] ;
+bias = 100 [ 4.89451941270932966 4.89197942438928912 -0.0994420803264053077 -0.0942778846704483142 -0.105270493455753844 -0.0995908208512013143 -0.0997637257232806968 -0.102534368163212236 -0.0979702072386830908 -0.0957394442620273095 -0.102417956428504736 -0.0962278870500655609 -0.106706045188527068 -0.104685782833708454 -0.101541489311648997 -0.0973788381890702687 -0.0961283172113815004 -0.105612846178692946 -0.0994058639730708155 -0.106641928652250625 -0.0956053153143950618 -0.0977652900667601854 -0.101941362116462569 -0.0989914507667482013 -0.0977733527136772362 -0.10581571029155 -0.103738546189915584 -0.106340627335140117 -0.0972619991501929154 -0.100927576553259776 -0.100595521994940873 -0.0936679707242785986 -0.10333825612701987 -0.0931072375720367329 -0.105207747035523921 -0.104387571792632389 -0.0988717876544415591 -0.100853778649247536 -0.0938094528930358412 -0.0998641169201297529 -0.0956282507427345801 -0.10251724938560812 -0.0968509892805764633 -0.09953679041182!
 32144 -0.095551223865302265 -0.105921653701933766 -0.0993302563383529485 -0.107109760683654992 -0.100235821197798655 -0.105318301624457877 -0.105145242632932581 -0.107060422627812998 -0.105992055984730604 -0.103756330112340728 -0.0978340861345907836 -0.0982376561275740412 -0.091435906267223685 -0.103091868949142934 -0.0959661474433256001 -0.0965032227289652211 -0.0990990157708882963 -0.102320868045302965 -0.102806740567493088 -0.0963347271999627497 -0.0938405954230747269 -0.0995507244656916312 -0.0951991253302886808 -0.100782999283239624 -0.0993255626263235825 -0.106704954191659707 -0.0949935369946697944 -0.097420725822433607 -0.0975394671206146202 -0.0974100970752363599 -0.0984278880063963885 -0.0945910963829521001 -0.102629268292377993 -0.101866355992808869 -0.0980998123378614001 -0.0964147972248048241 -0.100281744085462435 -0.101327864225343761 -0.0987917945610762155 -0.0933940369808724247 -0.102823910036620558 -0.0919935419983451408 -0.0983483085868706447 -0.10050967981!
 8633318 -0.0991091765591899099 -0.0963699248024051658 -0.09849!
 24766700
019938 -0.0988444425335484739 -0.104426907930808888 -0.101048666937016404 -0.10250921927481367 -0.0991916579055246855 -0.105320663815402643 -0.0996605948828681726 -0.0990719624774636515 -0.0954000170520914598 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMMultinomialLayer" ;
@@ -45,106 +45,106 @@
 ] ;
 connections = 1 [ *5 ->RBMMatrixConnection(
 weights = 100  2  [ 
--0.595479171538772389 	-0.647843265634009846 	
--0.615238337477441055 	-0.626646056674283858 	
-0.0538900294397439625 	-0.0137742833948165666 	
--0.06875053142052448 	-0.0128356405145059228 	
-0.101750482831042349 	0.0645179709928756923 	
-0.0788404196343822639 	-0.0363313352895024075 	
-0.0823984677889268713 	-0.0359772385695514493 	
-0.111455643794722045 	-0.00410482619395231754 	
-0.0609652140478198074 	-0.0553056639846301532 	
--0.0571686153486568316 	0.0104202988290012649 	
-0.0769767542086591816 	0.0288702909576745818 	
--0.0738826113496724585 	0.0376651648655449978 	
-0.0890025291790311757 	0.10658630388745495 	
-0.0628827179229135558 	0.0909309318538381178 	
-0.10189692292911387 	-0.0160894138207842638 	
-0.0269756763337721528 	-0.0340491698234521578 	
--0.0131781351491743713 	-0.0230764112596843068 	
-0.108709363233458603 	0.064594665896358705 	
-0.0330446970922479294 	0.00641644683858688929 	
-0.111338474011847738 	0.0830744128480884469 	
--0.00923663549878875507 	-0.039419952693748761 	
--0.0390168467135288966 	0.039692762930110273 	
-0.053884855284114512 	0.0417960182001957531 	
-0.0722589195882538671 	-0.0431916351119050954 	
--0.0628580553109818196 	0.0620282656863879525 	
-0.0904696456760091527 	0.0869795013810659712 	
-0.063155692471325367 	0.070941165193654579 	
-0.089737672806708732 	0.0983663784623535487 	
-0.0273272832467084739 	-0.0370754352212263952 	
-0.0665515477695941687 	0.00665545528587137454 	
-0.114615148493610861 	-0.0513201825260646144 	
--0.0456422913795766633 	-0.0504568195824679072 	
-0.0690561056867186052 	0.0565546631705502387 	
--0.0754060455133409996 	-0.0346219815131878009 	
-0.0713063685070573078 	0.0933620354169992783 	
-0.0341980957421883225 	0.112370604562321508 	
--0.0392048865688329473 	0.0645005741897170803 	
-0.100413128501990329 	-0.030219248975625241 	
--0.0557068816320919549 	-0.0369117849033234746 	
-0.000529733929197459758 	0.0485162644411036431 	
-0.0170355689113608801 	-0.0656936836103067801 	
-0.0433878525009998087 	0.0645360645558576418 	
-0.0248375275797735953 	-0.0443637944730375169 	
--0.0760831140867710004 	0.112732191490474185 	
-0.00587915934077326736 	-0.0562115972021530394 	
-0.0766272396820571744 	0.102736863234715037 	
--0.000592684940426658319 	0.0378303322767531494 	
-0.0992806567766492953 	0.104535562881183194 	
--0.0226534723321051186 	0.0788333372758215217 	
-0.0956148861294622843 	0.071608409395350639 	
-0.111961983119019282 	0.051503114001920508 	
-0.103261697334856045 	0.0995893663062659279 	
-0.113622132076437066 	0.0674654151128029206 	
-0.0775371261339915241 	0.0570887588626447992 	
-0.0705810984356155524 	-0.0689366435996839055 	
-0.044843023134881671 	-0.0322309100088603862 	
--0.0714869712521966627 	-0.0802860339949844071 	
-0.0350917134068162787 	0.0847180859020596616 	
--0.00934136513864572762 	-0.0307058733146445759 	
-0.017274684402242526 	-0.0451641166704254277 	
--0.00744338758825805347 	0.0394987995010958187 	
-0.01635978807368689 	0.0864131668208693443 	
-0.0709612773109407208 	0.0433967733386337942 	
-0.00718060967436377762 	-0.0388226740324390668 	
--0.0755833033596572668 	-0.0167504198245917793 	
--0.0642340161003093352 	0.102350333481158032 	
-0.0048042487224343542 	-0.0636443844298318767 	
-0.0282189683208145595 	0.0419719166047329909 	
-0.0299132095892969373 	0.00775440451798690283 	
-0.111530306999090348 	0.0842099709862055845 	
-0.0111726585535115806 	-0.0753750532311086435 	
--0.0647904300854395682 	0.0560328728134857693 	
--0.0392948976134838007 	0.0348323569795132501 	
-0.0123915169443690812 	-0.0184093328026520113 	
-0.0431605911084246008 	-0.0262028154072604068 	
--0.022117499304064675 	-0.0514567367068857959 	
-0.068835727405720748 	0.0415705062017272323 	
-0.0890847854970412689 	0.00441555037709988676 	
-0.0852942013840160274 	-0.0781125130895272829 	
--0.0472940655022145579 	0.0167306727323274472 	
-0.0220238553939347895 	0.0368257958383149606 	
-0.0599111578444758569 	0.0223267883631605608 	
--0.055857013383988692 	0.0780883336256121829 	
--0.058716698004954021 	-0.0441544608089798271 	
-0.0793606765007453946 	0.0352579600100396198 	
--0.0640094082428914191 	-0.0739527557808974806 	
-0.0489600429692438541 	-0.0341678748008232419 	
-0.00379375700322669976 	0.0596590822724617845 	
-0.0302207297680062124 	0.00254921219936410327 	
--0.0616037449568939341 	0.0290990160924210796 	
--0.0125089212557680778 	0.0306990708174415226 	
-0.0351162640727813793 	-0.00848184184674327657 	
-0.049213844559273745 	0.0988530120564841547 	
-0.0345815650076597564 	0.0413837339984958544 	
--0.00231189629031251787 	0.1080671288555725 	
-0.0674832518044770763 	-0.0336835252022576748 	
-0.0770889815106832049 	0.0900588562201595916 	
--0.0367474445651055082 	0.0795105696210817031 	
-0.0713351233590624745 	-0.0403765780191503165 	
-0.00580328195072376662 	-0.0599406438638086306 	
+0.662158354424907891 	0.583232854342350615 	
+0.640064466062109427 	0.608798193940926713 	
+0.022527412437393389 	-0.0453103422133988507 	
+-0.100266891588086798 	-0.0444642975036807497 	
+0.0702295556677405702 	0.0328532174664254439 	
+0.047473830353256169 	-0.0678698446488236462 	
+0.0510132237044769107 	-0.0675309405493934312 	
+0.0800098875205152577 	-0.035667553088809055 	
+0.0295945439730681409 	-0.086879820017033399 	
+-0.0886030416950577687 	-0.0211361569633355464 	
+0.0455763521836290778 	-0.00268248005582537178 	
+-0.105350063086555548 	0.00609662927904994385 	
+0.0574254211913950985 	0.0748055798832980706 	
+0.0314187402169011434 	0.0592625199905627703 	
+0.0704709948580114071 	-0.0476495496382620606 	
+-0.0043894888250582036 	-0.0656076840431512459 	
+-0.0445779806701181239 	-0.0546509115189693084 	
+0.0771686825448611585 	0.032918881442087565 	
+0.00170242077002343115 	-0.025099071089440296 	
+0.0797568907209052769 	0.0513446672167540627 	
+-0.0406513951961165743 	-0.0710239575492945113 	
+-0.0704003900367427132 	0.00816312518888440662 	
+0.0224951788845600513 	0.0102353844886529984 	
+0.0408830077684917623 	-0.0747508817475029813 	
+-0.0942471517414461973 	0.030508494763001829 	
+0.0589559210333447867 	0.0552885530688721066 	
+0.0317220950414108041 	0.0393234591481275916 	
+0.0582111484660004647 	0.0666484882870143208 	
+-0.00404796326721662634 	-0.0686465542027541453 	
+0.0351881294664364297 	-0.0248677733188647741 	
+0.0832186421140416327 	-0.082862637938772743 	
+-0.0771320771131728811 	-0.0821174329393306868 	
+0.0376477065629339941 	0.0249766399520880943 	
+-0.106994475045222595 	-0.0663262563819450451 	
+0.0398384755177023236 	0.0616927636838966939 	
+0.00276645173593164078 	0.0806951989051894192 	
+-0.0705778277040368246 	0.0329679151144941368 	
+0.0690270234418436518 	-0.0617505667080445464 	
+-0.0872144610485454802 	-0.0685653858942102123 	
+-0.030805730619030882 	0.0170041856586852745 	
+-0.0143651193546521731 	-0.0973207514133055535 	
+0.0119881909269199168 	0.0329483775113013772 	
+-0.00653292824618773634 	-0.0759367741124321177 	
+-0.107508406524880057 	0.0811397774181080583 	
+-0.0255163862771277013 	-0.0878195693130585237 	
+0.0451263495710342855 	0.0710268329550134275 	
+-0.0319333119034645035 	0.00631793124270262855 	
+0.0676903729911221552 	0.0727559957419445791 	
+-0.0540290149289171032 	0.0472715615216171142 	
+0.0641154119265864852 	0.0399522123719096706 	
+0.0804446851382299571 	0.0198634165623277487 	
+0.071674718730542139 	0.0678245326884632377 	
+0.0820606732950758394 	0.0357732820017380437 	
+0.046084256474110824 	0.0254724581819018511 	
+0.0392284482108255853 	-0.100503223493228919 	
+0.0134788660749131355 	-0.0637837024876457132 	
+-0.103161602985934894 	-0.11213644058335516 	
+0.00368484984043175286 	0.0531028729535656385 	
+-0.0407464407245594559 	-0.0622929861333136395 	
+-0.0140781234809960482 	-0.0767197188767816118 	
+-0.0388121926402475448 	0.0079607752389461682 	
+-0.0150210981163975688 	0.054821689198744257 	
+0.0395452117824861976 	0.0118193186934059564 	
+-0.0241859232165774728 	-0.0703843417427873208 	
+-0.107131075815018045 	-0.0484020032333970576 	
+-0.0956406337548928454 	0.0707739746173131762 	
+-0.0266040933847700861 	-0.0952738525063773767 	
+-0.00315255194014438939 	0.0104225674220301233 	
+-0.00143072069558646312 	-0.023762917767274655 	
+0.0799323620048853906 	0.052462994890691339 	
+-0.0202304107648025172 	-0.107016633036747189 	
+-0.0961993961416405946 	0.024500135955506732 	
+-0.0706787356854953058 	0.00330422045808240845 	
+-0.0189883818151368421 	-0.0499709111451519597 	
+0.0118120532825840736 	-0.0577361019533632544 	
+-0.0535373303811612772 	-0.0830696516560714415 	
+0.0374511855232045401 	0.0100237064219110533 	
+0.0576803633973364677 	-0.0271301803890959849 	
+0.0539051641178651397 	-0.109716565071205918 	
+-0.0787054032790544394 	-0.0148139951541322278 	
+-0.00931162442447889052 	0.0053141875865541895 	
+0.0285347315353834674 	-0.00921285620513620507 	
+-0.0872308162461840758 	0.0465615086994649804 	
+-0.0902451275813747067 	-0.0758316671018854221 	
+0.0479468263188592703 	0.00369223713080175106 	
+-0.0956149663757486501 	-0.10573571181435916 	
+0.0176260572271366329 	-0.065690471987151125 	
+-0.0275682464441893357 	0.0281128525154292763 	
+-0.00112346216923607392 	-0.0289692992359120334 	
+-0.093016498452142049 	-0.00243123932537142986 	
+-0.043866864213928547 	-0.000822958151842882279 	
+0.00377917811659888702 	-0.0399960413215662142 	
+0.0177642960633843106 	0.0671824370848984054 	
+0.00322806621841479919 	0.00985336178814748102 	
+-0.0337181670178254358 	0.076427473507669319 	
+0.0361277275274375434 	-0.0652174804828195148 	
+0.0456000680094742414 	0.0583767919401788626 	
+-0.0681263411873757468 	0.0479566088286199135 	
+0.039963263199487245 	-0.0719298457490680915 	
+-0.0255852854871204613 	-0.0915459023913980374 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -181,11 +181,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
-0.0123420928014191765 	0.00916499543260136662 	-0.00164506058869937728 	-0.0079295871380481147 	0.00236549116291899161 	-0.00816839208649198199 	-0.00393729201208150269 	0.00383571983020708452 	-0.00544728800267464067 	-0.00266053897009083237 	0.00946810364372980412 	-0.00972470140984709602 	-0.00715049251794375867 	-0.00734869875953184874 	-0.00851045478860876746 	0.000733198497111746091 	-0.00587341695528064475 	-0.00209131432022617667 	0.00867743624742248892 	-0.00174242905445374857 	-0.00803777150919455716 	2.68947234888729918e-05 	-0.00978963001083593166 	-0.00219383685419234295 	0.00713629807466046717 	-0.0017607190700940085 	-0.00693551341442329074 	0.00760840838401072306 	-0.00689064362225262714 	0.00821779551345409691 	0.00775615975335418317 	0.00256994077777214936 	-0.000125487652430557821 	-0.00765557075172304622 	0.00270057028444748229 	0.00046445864750938921 	-0.00737340071900495931 	0.00924350722960627473 	0.000308359129016673504 	0.00213576384551587232 	-0.00!
 88316481921647573 	-0.00479667378996826745 	-0.00314050304959243194 	0.00677359813450965909 	0.000461574357387472883 	0.0052630551506867209 	0.00342377191963190138 	0.00328382212079000434 	-0.00626455935271804255 	-0.00135382711424076175 	0.00428995531271315379 	0.00717178746011403606 	-0.00889343281654111917 	-0.00926633853826809052 	0.00458758045120281812 	-0.00924171707330578504 	-0.0089407727361354699 	-0.000689382609061561812 	-0.00144386531203365706 	0.00344874682264902609 	-0.00445501595657440443 	0.00439439909658437614 	0.00146877925949968099 	0.00820337445445518988 	-0.00592893418508044692 	0.00182255472740502824 	0.00731552216505707356 	-0.0093740454197580262 	-0.000212912355459383902 	-0.00863228712512624334 	-0.00315315105065477668 	0.00635133933025240193 	0.00750852663533482702 	-0.00805517934762344477 	0.00972441442848316628 	0.00554144065331621267 	0.00669469475106060277 	-0.00565723229911999689 	-0.00967100372334113445 	-0.000114466692242828325 	0.0086591209!
 0766649163 	-0.00731143980482081273 	0.00556306573368089453 	-!
 0.003507
00272215526306 	0.00733748393075391286 	-0.00173505410219775328 	0.0086972297748011216 	0.00136029609347290475 	0.00071739170531200905 	0.00991167438484492555 	0.00573543123204991317 	1.63578136879163428e-06 	0.00106564480483038434 	0.00193302874506671757 	-0.00966095349810980504 	0.00653802734483209004 	-0.00763470234441813479 	0.00566876259818077134 	-0.0024499179463127619 	0.00406053318954284741 	
--0.0111738846093165402 	-0.000548682555326289933 	0.00697000822713652029 	0.00138753007834285184 	0.00766445580783438156 	-0.00860777589669724057 	0.00644679742555415332 	0.00740125880871012574 	0.00287193856746441843 	-0.00677899801957570006 	0.00826191989123869694 	0.00115583659118469336 	0.00582889788198777056 	-0.00187316962044102651 	0.00334312408956729005 	0.0080872764050561631 	0.00243764569952672171 	0.0014126969154523441 	0.00700860246182310413 	-0.00352784450061643332 	0.00321421318331525842 	0.00106822699359159965 	-0.00101338297676285176 	0.00828488639125498846 	-0.00806675534350202901 	-0.00871347504120848285 	-0.00159867887480246102 	-0.00603600974286399878 	0.00670775103521673542 	0.00437948857073182894 	-0.00396246760226966893 	-0.00459702425477752537 	-0.00784011758788464089 	0.00947731814134509208 	-0.00750171391112994246 	-0.00117317699394124133 	-0.00893685763971203626 	0.000700923280907214804 	0.00234846370887252 	-0.00820605084031584935 	-0.00170467919!
 317889645 	0.00570568789669640558 	0.00220298768276994995 	0.00974557764735873785 	0.000626821250270257813 	-0.00470979802525691143 	-0.00231845383620405462 	0.00309793335509616187 	0.00199836423242812523 	-0.00816490725146091882 	0.000968802173294960032 	0.0025393460609361854 	-0.00650539020630128747 	0.00214043896541134824 	-0.00587543269286049108 	0.00116619384585259374 	0.0053050167725119373 	0.00678312969867851494 	0.00950181053835282008 	-0.00941018273948261708 	0.00814468902457629769 	0.00674089248166376623 	0.00846999001116628507 	0.00159122082434438785 	0.00558086969076118829 	0.00371987946113228485 	0.00896868983933631937 	0.00392580299849605227 	-0.000325498179622835126 	-0.000442150935507603228 	-0.00952586521815736165 	0.000360361302915812836 	0.0067212861590301631 	0.00970070024520561756 	0.00991357247738965976 	-0.0099411564224208826 	-0.00517454782326429424 	-0.00357508531990571995 	-0.00084756135678746181 	-0.000595474746156425842 	-0.000670178730631579441 !
 	-0.00283987246258698067 	-0.00877680298212392754 	-0.00287434!
 57191996
6764 	0.00761101315321550259 	-0.00570802323852433267 	-0.00351809444571104472 	0.00778924897444837998 	0.000872309102505270263 	-0.00334203925717386661 	0.00733885683349534863 	-0.00570509716576950762 	0.00791110215571273062 	-0.000799315232385290142 	0.00564398948162753589 	0.00471025043949309472 	-0.00883670104575096267 	0.0100340604251309938 	0.00563281500183433759 	-0.00436639622631694244 	
+0.0123095540038060513 	0.00883269083283308443 	-0.00164942141170705265 	-0.0081036761942630698 	0.00253566988886377636 	-0.00816932558833289788 	-0.00393274090447697823 	0.00392566856556591116 	-0.00550002870429359188 	-0.00278679617049868052 	0.00955537399950993904 	-0.00983725066694298919 	-0.0069407138091524042 	-0.00719533120339206113 	-0.00845060279724597213 	0.000662886268420245892 	-0.00598429177189082825 	-0.00191155793018608477 	0.00867211703386516763 	-0.00153435583843726888 	-0.00816574163814774712 	-3.32695231458855485e-05 	-0.00971658259858894369 	-0.00221369033864166802 	0.00707341489929369776 	-0.00157542538431900011 	-0.00680930893147756004 	0.00780812098798183262 	-0.00696470478833612853 	0.00825970132911229941 	0.00778474804689965709 	0.0023770416377033492 	-1.09780547725701351e-05 	-0.00786798391658959963 	0.00286871108339486602 	0.000608588343548391634 	-0.00739910533196916527 	0.00928158490075437111 	0.00011991527491460068 	0.00214372758739949081 	-0.00!
 895984715449167982 	-0.00470659240588922664 	-0.00322818379872686061 	0.00676347736322614225 	0.000331232525777035017 	0.00545115252760670216 	0.00341519558746549313 	0.00350449224011963045 	-0.00624663198141399856 	-0.00118238027851743223 	0.00445652370724215503 	0.00739113019304100775 	-0.0087031505534373426 	-0.0091394482304814912 	0.00452906553797543035 	-0.00928457507165773681 	-0.00920788489313654514 	-0.000582642413974866508 	-0.00155995661041069349 	0.0033495029178421923 	-0.00447092224843721915 	0.0044776295812166949 	0.00156774506916903785 	0.00809895877987856436 	-0.00611777613146196383 	0.00181467320458168009 	0.00717340736278910401 	-0.00933651903630436375 	-0.000220749351164449797 	-0.0084224589884120387 	-0.0033028101519856929 	0.00627729257006310031 	0.00744120610164107892 	-0.00812402397033160778 	0.00968766529268026352 	0.00537929524572177895 	0.0067882260588817871 	-0.00558689771081887377 	-0.00972195660418946096 	-0.000218153053328702412 	0.0086808447539!
 9950699 	-0.00725703444283621029 	0.00553278802625390212 	-0.0!
 03709269
62823098184 	0.00743688132580626673 	-0.00198395713324295373 	0.00865745503577009662 	0.00138842646266003973 	0.000702719534444898038 	0.00980483240321440054 	0.00570012771018503332 	-2.15772583666940978e-05 	0.00121137547075392195 	0.00197864458720439351 	-0.00957312438910756965 	0.00652485640508057834 	-0.00746327917414440264 	0.00566770032383096065 	-0.00246710406198358961 	0.0039249367347760104 	
+-0.011141345811703415 	-0.000216377955558022159 	0.00697436905014421453 	0.0015616191345578412 	0.00749427708188957122 	-0.00860684239485632815 	0.00644224631794959503 	0.00731131007335129347 	0.00292467926908334752 	-0.00665274081916784193 	0.00817464953545855855 	0.00126838584828057157 	0.00561911917319641609 	-0.00202653717658083038 	0.00328327209820448301 	0.00815758863374769182 	0.00254852051613689134 	0.0012329405254122535 	0.00701392167538051302 	-0.00373591771663289241 	0.0033421833122684614 	0.00112839124022635754 	-0.00108643038900984191 	0.00830473987570431657 	-0.00800387216813527175 	-0.00889876872698347997 	-0.00172488335774820516 	-0.00623572234683515692 	0.00678181220130023507 	0.00433758275507368802 	-0.00399105589581511769 	-0.00440412511470870266 	-0.00795462718554264606 	0.00968973130621162554 	-0.00766985471007732792 	-0.0013173066899802412 	-0.00891115302674781555 	0.000662845609759145859 	0.00253690756297459416 	-0.00821401458219949213 	-0.00157648023!
 085192601 	0.0056156065126173639 	0.00229066843190437993 	0.00975569841864227898 	0.000757163081880695083 	-0.00489789540217689182 	-0.0023098775040376455 	0.00287726323576653707 	0.00198043686112406303 	-0.0083363540871842258 	0.000802233778765951426 	0.00232000332800923236 	-0.00669567246940506663 	0.00201354865762473851 	-0.00581691777963310244 	0.00120905184420450019 	0.00557212892951302035 	0.00667638950359183709 	0.00961790183672981552 	-0.0093109388346757499 	0.00816059531643908119 	0.0066576619970314466 	0.00837102420149688463 	0.00169563649892095851 	0.00576971163714270433 	0.003727760983955615 	0.009110804641604223 	0.00388827661504239069 	-0.000317661183917771319 	-0.000651979072221808625 	-0.00937620611682643025 	0.000434408063105112883 	0.00678860669272391033 	0.00976954486791378057 	0.00995032161319256252 	-0.00977901101482642546 	-0.00526807913108548118 	-0.0036454199082068422 	-0.000796608475939119902 	-0.000491788385070552323 	-0.000691902576964578534 	-0.0!
 0289427782457156359 	-0.00874652527469698543 	-0.0026720788131!
 2394886 
	0.00751161575816314785 	-0.00545912020747913156 	-0.0034783197066800167 	0.00776111860526124131 	0.00088698127337238301 	-0.00323519727554336415 	0.00737416035536022849 	-0.00568188412603403293 	0.0077653714897891607 	-0.000844931074522962391 	0.00555616037262536555 	0.00472342137924460642 	-0.00900812421602477376 	0.010035122699480847 	0.0056500011175052 	-0.00423079977155010716 	
 ]
 ;
-bias = 2 [ -0.00391578848611695736 0.00391578848611706665 ] ;
+bias = 2 [ -0.00386380145527327095 0.00386380145527336506 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "GradNNetLayerModule" ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -15,7 +15,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.0299999999999999989 0 ] ;
+bias = 2 [ 0.0299999999999999989 0 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -34,7 +34,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ 0.00746183420346146695 0.00401358957022956986 ] ;
+bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -45,8 +45,8 @@
 ] ;
 connections = 1 [ *4 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.247752783231626306 	0.590442976497255634 	
-0.489552741554161064 	-0.317583248355345904 	
+-0.222066209664342068 	0.582925561708350082 	
+0.516883916989072545 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -71,8 +71,8 @@
 last_layer = *3  ;
 last_to_target = *6 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.58309332149676929 	0.362149014836746996 	
-0.117267655399388715 	-0.687182627254732004 	
+-0.603392333131359426 	0.370787384786479657 	
+0.09708625652197031 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -100,7 +100,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ 0.0324754185935069578 -0.032475418593506937 ] ;
+bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -46,7 +46,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.0299999999999999989 0 ] ;
+bias = 2 [ 0.0299999999999999989 0 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -65,7 +65,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ 0.00746183420346146695 0.00401358957022956986 ] ;
+bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -76,8 +76,8 @@
 ] ;
 connections = 1 [ *9 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.247752783231626306 	0.590442976497255634 	
-0.489552741554161064 	-0.317583248355345904 	
+-0.222066209664342068 	0.582925561708350082 	
+0.516883916989072545 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -102,8 +102,8 @@
 last_layer = *8  ;
 last_to_target = *11 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.58309332149676929 	0.362149014836746996 	
-0.117267655399388715 	-0.687182627254732004 	
+-0.603392333131359426 	0.370787384786479657 	
+0.09708625652197031 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -131,7 +131,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ 0.0324754185935069578 -0.032475418593506937 ] ;
+bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 4 [ StatsCollector(
@@ -17,10 +18,10 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.647925652672704411 ;
-max_ = 0.647925652672704411 ;
-first_ = 0.647925652672704411 ;
-last_ = 0.647925652672704411 ;
+min_ = 0.651261219307483818 ;
+max_ = 0.651261219307483818 ;
+first_ = 0.651261219307483818 ;
+last_ = 0.651261219307483818 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -34,10 +35,10 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-first_ = 0 ;
-last_ = 0 ;
+min_ = 0.5 ;
+max_ = 0.5 ;
+first_ = 0.5 ;
+last_ = 0.5 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -51,10 +52,10 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.610120244293007152 ;
-max_ = 0.610120244293007152 ;
-first_ = 0.610120244293007152 ;
-last_ = 0.610120244293007152 ;
+min_ = 0.614096318051678525 ;
+max_ = 0.614096318051678525 ;
+first_ = 0.614096318051678525 ;
+last_ = 0.614096318051678525 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2007-06-29 22:19:45 UTC (rev 7679)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL7547"
+__REVISION__ = "PL7677"
 

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -85,8 +85,8 @@
 ] ;
 connections = 1 [ *10 ->RBMMatrixConnection(
 weights = 2  2  [ 
-0.211616747512905712 	0.215664750468485322 	
-0.59211590021607885 	0.667132771633056509 	
+0.211616747512905684 	0.21566475046848535 	
+0.59211590021607885 	0.667132771633056398 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -111,8 +111,8 @@
 last_layer = *9  ;
 last_to_target = *12 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.166376995329432731 	0.522222782976989097 	
-0.449826313170107572 	-0.266489754613600693 	
+-0.166376995329432703 	0.522222782976989097 	
+0.449826313170107683 	-0.266489754613600693 	
 ]
 ;
 gibbs_ma_schedule = []

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -35,7 +35,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 5 [ -0.00278509775797984452 -0.0165640011469744587 -0.00598010917213076532 0.00176154865174407379 -0.00543761947532792316 ] ;
+bias = 5 [ -0.00741490224202020608 -0.00543599885302569787 -0.00631989082786923572 0.000938451348255942832 -0.000862380524672084797 ] ;
 input_size = 5 ;
 output_size = 5 ;
 name = "RBMBinomialLayer" ;
@@ -55,7 +55,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 15 [ 0.000779997877980971718 -0.00104728075180422309 0.000162596992700449037 0.000132818845402643711 0.00133590769684036287 0.000687589776443822708 -7.76560227385681117e-05 0.00125951472842306968 -0.000202306033338346051 -0.000277744847549938072 0.000862967922845554389 0.00050754326341195062 0.000595454593460812104 0.000212416496440332819 -0.000303482986891311719 ] ;
+bias = 15 [ -0.00019091881638435857 0.000692612344419769725 8.05887038896703814e-05 6.58650757853363334e-05 -0.000462703741505384781 -0.000345319709093550389 -0.000426910327063052602 -0.000332496350013624959 -0.000249832087456332448 -0.000305219800169643238 -0.00010980993375596001 -3.23934346327403339e-05 -0.000659721786812819916 -0.000434905533523833536 -5.51609528667892839e-05 ] ;
 input_size = 15 ;
 output_size = 15 ;
 name = "RBMBinomialLayer" ;
@@ -66,21 +66,21 @@
 ;
 connection = *12 ->RBMMatrixConnection(
 weights = 15  5  [ 
-0.114223035535808898 	0.182442683757086221 	-0.22558965553523691 	-0.232130397907910097 	0.11730116647153796 	
--0.236729823211870999 	-0.238573859120553428 	-0.0174289393146416227 	-0.0352968636593247662 	0.0868521987958738306 	
--0.112385167931701516 	0.109165120043211133 	0.0398879707018945789 	0.213863755913404424 	-0.153776418605648835 	
-0.0472304326758663795 	0.179914195257864562 	-0.241836339155833635 	-0.00219891447934676861 	-0.219975733657415989 	
--0.0801878004779727066 	0.15885585162094773 	0.194765932183775026 	-0.202183990715325973 	0.25294763522081759 	
-0.142088781314686163 	0.167911434953121719 	-0.144817746011810855 	-0.245515785176769086 	-0.00427092943781542164 	
-0.22482713619054745 	-0.190942470533467412 	0.143516397182502198 	-0.0873212311324905038 	0.191847265724084781 	
--0.0443110228904399148 	0.220151981275360675 	0.0383038099022214029 	0.0237199697935066532 	0.256490747789178242 	
-0.148629878649942732 	-0.00573752550059243756 	0.0289256023536940202 	0.0543794629658403209 	-0.247820321229642482 	
-0.169644163328278808 	-0.197948084038232108 	0.146201678801202523 	-0.0586144105923159867 	0.104114336557621506 	
--0.19614775922123337 	0.118052340202558517 	0.175372344118596563 	0.0364296668610980603 	0.191065325213556791 	
--0.225122139804147053 	0.154647360886455448 	0.183380332951252722 	0.072590554739058441 	-0.177866796759333312 	
-0.206857124804643994 	0.0213917710286117141 	0.142039122446603244 	-0.0515882822962070581 	0.0809595650963546021 	
-0.204445705418892149 	0.0531513116531849572 	0.0282427368276513754 	0.17829493078693312 	-0.0993056962247372693 	
-0.0794396692635105095 	0.0164876104390719348 	-0.0339128563784461129 	0.210641398549506947 	-0.212868022370464677 	
+0.111532539174318199 	0.187811788715984557 	-0.22626158765521126 	-0.233129870295510383 	0.119238976328474175 	
+-0.237753965753817409 	-0.231057634349966873 	-0.0160342360440717188 	-0.0348869237801669382 	0.0904224929888394341 	
+-0.11480906531481426 	0.114256570455461448 	0.0395184419057638264 	0.213465119193897074 	-0.151617646776664866 	
+0.0452810620903923225 	0.186138648341168028 	-0.241669850969782773 	-0.00273182989094965852 	-0.217454074499106198 	
+-0.0837472601661387317 	0.162600765517560919 	0.19306474933853307 	-0.203481508797696664 	0.254103925287085419 	
+0.139460994658605147 	0.173337056482309709 	-0.145409675053789461 	-0.246538097386788446 	-0.00236295468943661545 	
+0.221884912969608578 	-0.186265533072615563 	0.142780753905995345 	-0.0878442708884909357 	0.193631993546532849 	
+-0.0480186751024380479 	0.223544608567212366 	0.0364252949429773806 	0.0225722593694714958 	0.257623444853429029 	
+0.146378222626808829 	-0.000158438587164408488 	0.0288614979002441464 	0.0539600251678709272 	-0.245602785443513966 	
+0.167026346551878119 	-0.192829125680318236 	0.145830128272192411 	-0.0589841834439883347 	0.106161183485431715 	
+-0.199347233078686348 	0.122085769377975079 	0.174091772543225415 	0.0355997489339363454 	0.192618740686110151 	
+-0.22758879765717735 	0.159694755162982593 	0.182898714339929436 	0.0719426859730288304 	-0.175849783304905943 	
+0.203433183673328438 	0.0252734783895589288 	0.140629819750046015 	-0.0525607739415335368 	0.0822164419110390232 	
+0.201369706773431967 	0.0573850010944562394 	0.0272266445762920119 	0.177649517290784931 	-0.0977265536520297434 	
+0.0771932230840291916 	0.0219421891209198236 	-0.0339867489356452807 	0.210398374465431665 	-0.210555786860010002 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -111,6 +111,7 @@
 min_n_Gibbs_steps = 1 ;
 n_Gibbs_steps_per_generated_sample = 1 ;
 compute_log_likelihood = 0 ;
+minimize_log_likelihood = 0 ;
 Gibbs_step = 0 ;
 log_partition_function = 0 ;
 partition_function_is_stale = 1 ;
@@ -141,7 +142,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 15 [ -0.000926553686800094109 -0.0108532037578065402 -0.0131440558251403501 -0.00648439658228983432 0.00471989560268569214 -0.0109584775927349561 0.00355569914317291 0.00144157351643795493 9.59724932935818539e-05 -0.00333027077973015477 0.00821548772285247975 0.000423010261683955569 0.00594638451851331809 0.000717568772213690153 0.00993454669422304433 ] ;
+bias = 15 [ -0.00181010305207781903 -0.0139565166488147593 -0.0173911992694234588 -0.011301792170158172 0.00715722737262551763 -0.0118417704523890339 7.84881727217838948e-05 -0.000922491975538373282 0.00176248685259962259 -0.00917947635612968195 0.00659751404013013236 -0.0012431721700721357 -0.000690366017608163066 0.00301805856872872395 0.00991044597535732999 ] ;
 input_size = 15 ;
 output_size = 15 ;
 name = "RBMBinomialLayer" ;
@@ -158,7 +159,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 20 [ -0.000453387009620988659 0.000845531434293066305 0.00162883533683450725 -3.87668695796345645e-05 0.000766236793527613085 0.000932980795007214061 0.000281682865875736458 -0.000730057603079139355 0.000925606672020178045 0.000919221687457388663 0.000712359500870028369 0.00066635569518742313 -0.000240566238039152463 0.00119560783510103694 0.00108325902627077705 0.00111064457759879262 0.00146226644400449402 0.000706339139820732847 0.000266131366869354086 -0.000122875266295231354 ] ;
+bias = 20 [ 0.000809098640355019726 -0.000661246082206968979 -0.00226872094948746923 0.000222388424196158057 -0.00119824300992009631 -0.00124327581110912091 -0.000399884602011445667 0.000756073269680812088 -0.00129997591475091703 -0.000872283779968379119 -0.00117326298168328935 -0.0013574076213712827 0.000459098879022368418 -0.000704366492196004113 -0.00166789963894511625 -0.00104582443465928848 -0.0013668875266807511 -0.000665999997969096377 -0.000470681182910106957 -0.000143409964875425484 ] ;
 input_size = 20 ;
 output_size = 20 ;
 name = "RBMBinomialLayer" ;
@@ -169,26 +170,26 @@
 ;
 connection = *17 ->RBMMatrixConnection(
 weights = 20  15  [ 
--0.199665893027455355 	-0.0465024958801317015 	-0.147568301000631952 	0.143054794116631301 	0.0966066196462358889 	-0.0976258306024285483 	-0.101260036292867034 	-0.177797091014299635 	0.209639908308190293 	-0.174018034822132461 	-0.0259767922673940534 	-0.200856387154690202 	0.0159748160488032212 	0.0511866829977281404 	-0.179770670392769255 	
--0.0394512522337529467 	0.117840023095250546 	0.0406692025192514767 	0.212098869137966661 	0.0151624548892851253 	-0.114980398362988789 	-0.0520374297605031844 	0.0642289531052787493 	0.0422259342848326294 	-0.187772552189459013 	0.0209784630500729034 	0.0512813133491341056 	-0.14641172744390632 	0.0436920843454037117 	-0.127221071086011134 	
-0.0240646352843687829 	0.114480065465243114 	0.142795644595856197 	0.208236887934659359 	-0.208742793563283596 	0.175597887892326027 	0.148280878037553882 	0.185452532449155744 	0.0346077698543971399 	0.122778150266735803 	0.0845210001875947581 	0.198934487485176004 	0.086839913406693961 	-0.00898635970767380256 	-0.0111275482563717148 	
--0.213578110958704687 	0.000783623293485158487 	0.140494085029853583 	0.20977459976397464 	0.220353956831890962 	-0.227977352869991629 	-0.11721922738915512 	-0.0824019551832265162 	-0.0206905471229207605 	-0.0166860237671342983 	-0.0138231679221703201 	-0.06644867570643119 	-0.193966813932034327 	-0.0645985818367366404 	0.170276407643900035 	
--0.127523970842238821 	-0.0851757289646849075 	0.164384819742476457 	0.0140251388982130087 	-0.0732135404671881429 	0.156004639419107971 	-0.127159504142860313 	0.172245706801283233 	-0.0206740136211002734 	0.120672514310639328 	0.10628409805808868 	-0.200853600817347988 	0.223679090521664098 	0.123463793339618613 	-0.0934865472635584699 	
--0.163600932258851389 	-0.0418094868025905772 	0.214154774373236295 	-0.137245088342446725 	-0.172756288292473636 	0.211436999415832744 	0.168846152845123137 	0.115024252628079374 	-0.215975313525567147 	-0.0865929432670289601 	0.127996731837503985 	-0.0336441892227711398 	-0.0950821504813939705 	-0.0344875439759191466 	-0.0890344163684737011 	
-0.0917676243332203323 	0.00647432876973066257 	0.0603689903341120895 	0.0542591111360847464 	0.207053633996143571 	-0.108213485753186317 	0.193655137718294484 	0.0375678836523921159 	0.215362120551664205 	0.153593816507550801 	-0.103225753462595063 	-0.0493100621030968245 	-0.0285065985944886607 	-0.17169186074478282 	-0.113158878902886909 	
-0.0486067978554309346 	-0.134060412435438092 	0.104782818192997548 	0.0190855905819467542 	-0.0954268780990478044 	-0.0627496439144036022 	-0.104877551848392467 	0.103512582834357852 	0.159857418569593096 	-0.114480538155331588 	0.114591425115921075 	0.0849393166543545991 	0.186750914082214892 	0.125692199697273382 	0.0510844829873717612 	
--0.0661431606631400076 	0.0841866145468284505 	-0.0154465516592166962 	0.00164622472970422668 	-0.162502068098615698 	0.163538697437007885 	0.0699601254969599851 	-0.175808159566014699 	0.00442778145254507476 	0.159691474307498776 	-0.111367619281643446 	-0.163415033503308771 	0.188656812819911396 	0.173690145881305186 	0.0342121254537379044 	
-0.136609915040623919 	-0.0580771352834966934 	0.135967651679254259 	-0.0556662900796270135 	0.0328655569514085949 	0.146730425379887258 	-0.0537434771234091488 	0.202439659618439766 	-0.0144895047848336948 	-0.0291653643612580227 	-0.132837722628549398 	0.148101772403925996 	0.106044862035487741 	0.177971943378715719 	-0.0779249685806468195 	
-0.185878517029531509 	0.191768322809052677 	0.0768459022325266583 	0.129586434444902449 	-0.194273450712217605 	-0.136587867028291066 	-0.00207230691257036465 	0.0452998690185411959 	-0.147800010724965097 	-0.0353738952329342923 	-0.093391426851810469 	-0.147904532123091598 	0.0681951546937868969 	0.0675988322767383659 	0.121983215944044099 	
--0.0934389356416226724 	0.0742796580208564261 	-0.0740796687796987652 	0.170508684419131029 	0.0838546408778213515 	0.106760370320955617 	0.0485989308522178573 	0.0271100657127993604 	-0.191458846064656241 	0.217350646628457045 	0.081807296237196736 	0.222329288422829729 	0.122760409777772098 	-0.0880630055038586074 	-0.170762431602553749 	
--0.111209426892105417 	-0.0789584494826100491 	-0.00509111537201256247 	-0.195500594890432339 	0.0387899926972619311 	-0.127353846738302329 	0.100367944943578202 	-0.0541142586180089619 	0.17473497852335329 	0.122616552037058957 	-0.0933993395500215678 	0.064816217682076821 	-0.175539044822730528 	-0.137612966756728627 	-0.141212831687991758 	
--0.144619919922026746 	0.136671857687587411 	0.0523892905598260378 	0.0637515977285305624 	0.0428730694091070397 	0.111030712937232795 	-0.168074062951118458 	0.0994484096369092213 	0.116117466705265751 	-0.0746991941351960037 	0.0257699172506545005 	0.0968915977359569441 	-0.18908693294671805 	0.145331939853380332 	-0.0274078888771996307 	
-0.200473936294629423 	0.0796472950114080896 	0.159829518589416686 	-0.0492859638056888094 	0.083823632353591776 	0.075883690665256423 	0.14163962831817084 	0.148997932669615391 	-0.194198858420308457 	0.164874030797497989 	-0.0576412089875710729 	-0.0744650365686934795 	0.126924490687926189 	-0.0473407687237771616 	-0.125163578650308832 	
-0.0248530862717537583 	0.106211296949465114 	0.172465562053498639 	0.153958502157922977 	-0.132092722841237326 	0.100459233574110601 	-0.177248765737252678 	0.213131782659715552 	0.152143086860893506 	-0.111340194589212529 	0.104235842785752458 	0.00698315906037692141 	-0.0348543672686432993 	0.0410771107041951328 	0.18376298231127941 	
-0.119847647485374964 	0.159723365513106952 	0.143771142846700811 	-0.102994303084915736 	0.11836461517276245 	0.215610544097093876 	-0.162335170778609095 	-0.194796983396369694 	-0.148049540636433108 	-0.0323209485217053685 	0.0041163461249623598 	0.0936775712720108039 	-0.0669931747800228961 	-0.145342144480870988 	0.0160231568615098845 	
-0.192114447823598294 	-0.00771167347709920351 	0.0489591389382780448 	-0.211709068839596143 	0.0816521131270076528 	0.0760189582843147915 	0.106958451458287168 	-0.0202866958504462763 	-0.0509688750422014961 	0.167271624740515823 	-0.138516884904345883 	0.0236545195213502916 	-0.00646597850364279744 	0.0871377827793629739 	-0.126109342802363328 	
-0.0632490108382660066 	0.150703644197975467 	-0.202290834600961367 	0.0500933244563882274 	0.074730197824073169 	0.131017723370825123 	0.195307550896969329 	-0.0242539328047582693 	-0.2096862509337237 	0.0443687107458422964 	-0.194568980241651646 	0.112336943259278441 	0.00360686173938918209 	-0.102790113151382603 	0.120951709749142711 	
-0.19123869786015596 	-0.184565098222969626 	0.0988981591479547034 	-0.180196103199863594 	-0.107604564176469616 	-0.0520615142032379088 	0.0168105346399400335 	-0.128247449073637471 	-0.0249673569200909128 	0.217053863018495719 	0.128025739243165237 	-0.0856297093917833541 	0.0167650115631037093 	-0.0137172613840686568 	-0.185647068414078298 	
+-0.199183283591467586 	-0.0448861405222477916 	-0.145911605089940416 	0.143167128930779708 	0.097264546687425249 	-0.0949925133778040665 	-0.102716450211031304 	-0.178404509085510282 	0.210887183685237151 	-0.174857624749812868 	-0.0275685708748617357 	-0.20085264239421638 	0.0127999445714401579 	0.0525619265455185072 	-0.181114851994662734 	
+-0.0405345646699039885 	0.115713015655211388 	0.0382070774945121316 	0.209081052310465254 	0.0154753678304534268 	-0.115963095563931154 	-0.0545581384739589198 	0.0622760463753706944 	0.0423000400201669555 	-0.19141013712398261 	0.0195581330642528688 	0.0497516461676063906 	-0.150568769029755634 	0.044099340425539868 	-0.128099183304432179 	
+0.0212798441799221354 	0.107092884451473497 	0.134112247501170623 	0.201054824223234785 	-0.207564543339615937 	0.169553880747502411 	0.145202456176182881 	0.182382196206076985 	0.0338098074757868308 	0.115703271531355081 	0.084076361430113114 	0.195966020934331825 	0.0823736521471305078 	-0.00913591333341486067 	-0.00998542292239412363 	
+-0.213808731496335919 	-3.57330316626099779e-05 	0.139752412963511513 	0.208377805756720713 	0.221252008485986112 	-0.227515159385436067 	-0.119083828031646008 	-0.0834267669359637087 	-0.0198051341758010112 	-0.018834976026176755 	-0.0147944479928383658 	-0.0671443164205455106 	-0.197328863181277181 	-0.0636097587615780946 	0.169747111964694303 	
+-0.129095368343955269 	-0.0888443250152665676 	0.15999870979229569 	0.00991180882672867102 	-0.0723159457344011436 	0.1535621811608186 	-0.129795156939259609 	0.170079227523400461 	-0.0207740929889144686 	0.116278017909354006 	0.105182066749371791 	-0.202777591919059585 	0.219466888486365835 	0.12370898938567175 	-0.0936132531499176967 	
+-0.165050353670635891 	-0.0435589319440154235 	0.211862476683310236 	-0.140221785334447402 	-0.173047144269131703 	0.210517225745375763 	0.165803320692554462 	0.112748400313367414 	-0.216285011527125065 	-0.0903413637635619715 	0.125602744201942867 	-0.0355660465583450269 	-0.099736673904979728 	-0.0345107616919458973 	-0.0907558488484283277 	
+0.0907620876195860127 	0.0031112905420243712 	0.056192360136695936 	0.0505260375405189935 	0.208668743045385324 	-0.110248218664548467 	0.191720854009795771 	0.0361553634497484844 	0.215994366414907241 	0.149777468047158163 	-0.103513680983149642 	-0.0505190687185509413 	-0.0317941043051931313 	-0.170769240409502421 	-0.112378271931309684 	
+0.0487690395705152324 	-0.136349650315423954 	0.10166556779473096 	0.0164455374614681654 	-0.0926661213082684426 	-0.063904658788169727 	-0.105535610323310505 	0.103138039735415368 	0.16163997241155223 	-0.117491996772037213 	0.115599170790724362 	0.084912011840722279 	0.184423359707916762 	0.127927366591155778 	0.0530818670446324692 	
+-0.0677287415174576829 	0.0809964567517113543 	-0.0195424402232347166 	-0.00234618811655406103 	-0.162188494160543345 	0.161510693358132701 	0.0672422678381438266 	-0.178178544520118665 	0.00409551312339385964 	0.155293083170179314 	-0.113117763319655687 	-0.165469649900201715 	0.184381214596949949 	0.17371604967408702 	0.0335741236754884265 	
+0.135057664772329028 	-0.0625317430835595905 	0.130353400231434408 	-0.0605017926117314975 	0.0341619687090545382 	0.143589971976766345 	-0.0560524005758504459 	0.200415286788135277 	-0.0143093493141647262 	-0.034064429123071796 	-0.133204077490446743 	0.14635290101959747 	0.102182978734996863 	0.178539241205019183 	-0.077229389893740355 	
+0.18449946751420962 	0.189003724286365554 	0.0733148255564332746 	0.125919553021787811 	-0.19400489683760086 	-0.138330194324075062 	-0.00463023911156397808 	0.0430833112717642283 	-0.147927678399169926 	-0.0395301083992953134 	-0.094807587083368311 	-0.149765119075252223 	0.063964250555166996 	0.06784891506159417 	0.121436033608037355 	
+-0.0951353216285513031 	0.0698845430271437945 	-0.0792594642563902718 	0.165798137588183431 	0.085001790360831761 	0.103741466635519058 	0.0459721074331424159 	0.0248930365946458433 	-0.191523552757090126 	0.212552098030806186 	0.0808342450785385219 	0.220291996514472982 	0.118777171673848436 	-0.0877877443919223843 	-0.170523275914209238 	
+-0.11113273135806466 	-0.0783495570820732234 	-0.00458520115775960368 	-0.196201530322556855 	0.0394905130558005288 	-0.125693748428586577 	0.0987032594647054839 	-0.0549026133969209434 	0.175712916180009476 	0.121127071128284844 	-0.0950606462321347961 	0.0644456081388188606 	-0.178814649109568846 	-0.136472097688931454 	-0.142439315950637746 	
+-0.145975586300460619 	0.133195998967976653 	0.0483913722439270749 	0.0597838240065062185 	0.0435847922746179436 	0.108895270934145541 	-0.170627032747475649 	0.0973387506263579672 	0.116040734082825739 	-0.0791557496692463086 	0.02470237269618486 	0.0950701182149208401 	-0.193197890566248559 	0.145668765551007107 	-0.027749652168303874 	
+0.198398860472742578 	0.0748486204979505604 	0.153835092500548165 	-0.0545505188068773836 	0.0845650568354725796 	0.0723265025689846103 	0.138709224380672597 	0.146492818989972018 	-0.19457948319521895 	0.15964410781348326 	-0.0586848185802863118 	-0.0768467088050233393 	0.122578832502764287 	-0.0474091109791810808 	-0.125035892567181839 	
+0.0231602096473796683 	0.101152310252098712 	0.166538464234946421 	0.148817381944993332 	-0.130712803632445113 	0.0966351039136320172 	-0.179653661653241731 	0.210901483761747555 	0.152086536422386953 	-0.11677447559900532 	0.104005828855715687 	0.0049574435177267423 	-0.0388113984088926806 	0.0416311090585825272 	0.184646906532305449 	
+0.118003014455172445 	0.156810604485115607 	0.140185871814998197 	-0.106793368511750822 	0.118109991102171422 	0.213803585283534131 	-0.165602253030414426 	-0.19740285349086803 	-0.14867722710498707 	-0.036693641308080234 	0.00190101283092516536 	0.0913318284426307275 	-0.0717647299711370312 	-0.145638383254883763 	0.0144819652994947357 	
+0.190898004079688044 	-0.0105865443108898553 	0.045103835106424163 	-0.215370421512423749 	0.0824334362374186669 	0.0743494290457734491 	0.104582470590265431 	-0.0221113870931153519 	-0.0507552873024821119 	0.163349281186003337 	-0.139658142898227727 	0.0220992644453049357 	-0.0103308405592924916 	0.0876248823067597993 	-0.12634008093061902 	
+0.0623093147787751878 	0.148170929224122755 	-0.205778911070684828 	0.0466533311755787314 	0.0757877277385159143 	0.129665202643826916 	0.193334660473715725 	-0.0258395529259260875 	-0.209167429241361169 	0.0407058656450209264 	-0.195600174202360783 	0.111011627274150843 	0.000231292374348730556 	-0.101977810287692483 	0.121069362554991275 	
+0.190842131767268691 	-0.185167598723336546 	0.0978899396526391602 	-0.181864114656352577 	-0.106782088342657663 	-0.0517523444814251277 	0.0147853680011378052 	-0.129438454801838632 	-0.0242626106507723427 	0.214688607370236678 	0.12661801914067547 	-0.0864452182514442863 	0.0131257843063401942 	-0.0126889411988008911 	-0.18644360949281652 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -219,6 +220,7 @@
 min_n_Gibbs_steps = 1 ;
 n_Gibbs_steps_per_generated_sample = 1 ;
 compute_log_likelihood = 0 ;
+minimize_log_likelihood = 0 ;
 Gibbs_step = 0 ;
 log_partition_function = 0 ;
 partition_function_is_stale = 1 ;
@@ -255,11 +257,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  20  [ 
-0.0128971541272204088 	0.0240948720799898826 	0.038090222095950782 	0.037163863684979502 	-0.0270027056800153653 	-0.0143830649287863613 	-0.0504111073661725542 	-0.019953259136972247 	0.00926120484532864593 	0.0322593454740600222 	-0.00196114628952165143 	-0.00379474247586950031 	-0.0253797390653449258 	-0.0444362466155495406 	-0.0103776355182317662 	0.0293393611662794568 	0.0102238931453799246 	-0.0200415901357403579 	0.0456535800127743016 	-0.0141947075442552979 	
-0.0433498675730351613 	0.0205954078154498496 	0.00170036442589811452 	0.000301639148968105189 	-0.0416695674417588355 	-0.00262672591829156985 	0.035342179563095745 	0.0507752571585376108 	-0.031225741964821644 	0.0294932804765521382 	-0.00861177878077906862 	0.0445753457328775735 	0.0315607079692813847 	-0.007536097203666315 	0.0404239425343957959 	-0.0166658338379819908 	0.0503311884596000722 	-0.0449513913381749813 	-0.00878542310096892647 	0.0363060686455251025 	
+0.0140741517422963459 	0.0240834059583218268 	0.0360118846831064751 	0.0374805587163126175 	-0.0276567128699623938 	-0.0141115213235216731 	-0.0512251993073655834 	-0.0208456831823070655 	0.00886820967367440462 	0.031115840858219123 	-0.00226946082449172018 	-0.00473980592688510397 	-0.0245274143902266956 	-0.0449922107970131546 	-0.0114658242515571409 	0.0279812900985528731 	0.0101001167313342841 	-0.0204964532283910331 	0.0452068878780007774 	-0.0138422507158356342 	
+0.0421728699579591929 	0.0206068739371179227 	0.00377870183874261028 	-1.50558823653514107e-05 	-0.0410155602518118417 	-0.00289826952355626884 	0.0361562715042887811 	0.0516676812038726341 	-0.0308327467931672657 	0.0306367850923931415 	-0.00830346424580894089 	0.0455204091838931763 	0.030708383294163151 	-0.00698013302220279203 	0.0415121312677211377 	-0.0153077627702553273 	0.0504549648736455775 	-0.0444965282455243305 	-0.00833873096619561736 	0.0359536118171052757 	
 ]
 ;
-bias = 2 [ -0.00617991517582894886 0.00617991517582895319 ] ;
+bias = 2 [ -0.00643817158042547886 0.00643817158042547973 ] ;
 input_size = 20 ;
 output_size = 2 ;
 name = "affine_net" ;

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -9,7 +9,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 5 [ 0.00289199834010759769 -0.00336838171739850837 -0.00907291517398268327 0.010587459701651555 0.00342034593152005855 ] ;
+bias = 5 [ -0.000346543794653077165 -0.00181343646441971262 -0.00256344846238087606 0.00123072211653035054 0.0105796540684799031 ] ;
 input_size = 5 ;
 output_size = 5 ;
 name = "RBMBinomialLayer" ;
@@ -29,7 +29,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 10 [ 0.0018381573662358409 -0.000532456445105348682 -0.000800829502916901238 0.000329863500327470436 0.0011533134657890653 0.0011050295814163055 -0.000785770690897080195 -0.0003073384783694397 0.00121904564884400845 -0.000115327602579064678 ] ;
+bias = 10 [ -0.00106440430474535582 0.000369301683813727641 0.000513350554420346637 -0.000709235433622423567 -0.00081825368190638749 -0.000936944443114255785 5.21038279847706566e-05 -0.00060723211325813993 -0.00012022306301326933 0.000849342609018322535 ] ;
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMBinomialLayer" ;
@@ -40,16 +40,16 @@
 ;
 connection = *7 ->RBMMatrixConnection(
 weights = 10  5  [ 
--0.247150332667494199 	0.196807708175855495 	0.28953684917501582 	-0.264451459825080504 	-0.258072765501957679 	
-0.00187795759218806137 	-0.154300262559131623 	-0.028743199964827007 	0.0993508076713483651 	0.0976474343171701287 	
-0.264744503372727036 	0.295806683849920427 	-0.0758943197380283763 	0.237120308602678992 	0.202882784321844395 	
--0.116888894760578288 	-0.247282897311932398 	0.137491692967800644 	0.0259224478134961259 	-0.274712571619822266 	
-0.0731813616424504126 	0.0460770916015187312 	0.11003177463826512 	-0.28323145626222973 	-0.228267349714453921 	
--0.0338323783743128062 	0.146290192340879532 	0.232123711886084855 	-0.0763206082503783134 	-0.262086838835151437 	
-0.0732187298233244743 	0.0755528688271968757 	-0.25890796831221341 	0.0971575769522933647 	-0.0377292812748117809 	
-0.0760386495552429903 	-0.228740223026275419 	0.088578909690409241 	0.185091031321392485 	-0.262031437700294634 	
-0.0470757318676603975 	0.112426432682485422 	0.168060702049136201 	-0.303306464411579102 	0.0510034683872520808 	
--0.227981923082971383 	-0.0169124519419365203 	-0.182429778000555542 	-0.153665802607211771 	0.296579964964229836 	
+-0.25033029216361119 	0.196297332269796926 	0.291863216743886378 	-0.271177564771000135 	-0.256443283847534731 	
+0.000750454165465756448 	-0.153051576826741781 	-0.0251140371746953056 	0.0951948005469039904 	0.101690890357688787 	
+0.264014387667070805 	0.296640484199731047 	-0.0733552745166331926 	0.234544519515334449 	0.20878469797271762 	
+-0.119055970902863212 	-0.246763413010821359 	0.140945917211564492 	0.0199301834550586615 	-0.272554977700818657 	
+0.0704522821956235917 	0.0459836760937902683 	0.112858621879697182 	-0.289343817002414683 	-0.226235693833594709 	
+-0.0364476443667945474 	0.145989242583078827 	0.234450892747573192 	-0.0821300666227130632 	-0.25953723478216989 	
+0.0719990749894757337 	0.0767519526074037367 	-0.255160105938427029 	0.0929396172618830452 	-0.0337076755676481918 	
+0.0743735670898155177 	-0.22811485856468966 	0.0918907401537037843 	0.180020360495644433 	-0.258920436145750887 	
+0.0447145136911352367 	0.112516270735609575 	0.170636125028530872 	-0.308567573762140279 	0.0539703490405033862 	
+-0.229320061680709814 	-0.015335473409631175 	-0.178325969033065856 	-0.158212845296357901 	0.300210567187072896 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -100,7 +100,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 10 [ -0.00390966210624125315 -0.00737485251426493738 0.0052263247663137764 -0.00882641574000199609 -0.0188942572354752238 -0.0105180277191638527 -0.00276884805782914335 -0.00527161019628003599 0.000602437690733600029 0.00146710182512755391 ] ;
+bias = 10 [ -0.00497296088693641256 -0.00960776823318316812 0.00300914972033978307 -0.010843134408859461 -0.0184786313956434023 -0.00746522161667526832 0.000908085308330407095 -0.00808384343338955022 -0.000599485986346771807 0.000143083459286945185 ] ;
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMBinomialLayer" ;
@@ -117,7 +117,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -0.000755294392186399454 0.000473141080488761437 0.000121239614396920963 0.000142312863403495796 -1.56529502345341978e-05 0.000278111537572710516 0.00064412400024600725 0.000110686936299085386 0.000141317165094744261 3.30169938608685325e-05 0.000100735445571570949 -9.97157318432540109e-05 -0.000129149334160201469 0.000549170849861042192 0.000694991888923602425 -0.000118002928380585718 0.000112982659172943006 0.00097794307242938909 0.000403994804962673417 0.000197557852185654081 0.000777521019783406777 9.45821131546686016e-05 0.000529147107043928294 -0.000114736377688701068 0.000955717434987283414 0.000963960427828801716 4.88728569854154411e-05 0.000461122549806003738 5.18723317254770645e-06 -0.000237375577569392144 7.11583545776674648e-05 -0.000334074232570689337 -0.000351164675225166634 0.000317756318849510364 -4.5901386659710746e-05 -5.24619801921385131e-05 -0.000609213091931972393 0.000329772565386653876 0.000174020043403563883 0.000265002787981150512 -1.671!
 63178030798124e-05 -7.17824032921054111e-05 0.000407650688616504871 0.000159954541764148971 -0.00017864510034187452 0.000231516808885985181 0.000376357453814627711 -0.000502248183139440306 -0.000254948657813671765 0.000207591708428371384 6.31421070913029746e-05 0.000592860377852076625 0.000146908151782446404 0.000267245055802555549 0.000294985302086573904 -0.000341823538262975692 -4.17131526833635773e-05 0.000498507957504412051 -0.00037039697131602213 0.000202662556804245823 0.000481865148143872544 0.000224362043974340368 0.000355137658429877513 -0.000463092823957273793 0.000544213168569053344 -0.00046510547271210248 0.000321289604371223199 0.000309750340113553173 0.000621923186005091783 0.000218427769189384835 6.84172143699791305e-05 0.000756221596616806586 -0.000156506001561774224 9.70512212965714163e-05 -0.000246565946789161796 -9.50964175788724395e-05 -0.000703435459434254072 -0.00021598136490223296 0.000186520971492648178 0.000531968569886104086 0.000136253635170905291!
  0.000447362861709063564 -0.00072627445057955051 0.00019697310!
 40408915
7 0.000782704822635808383 -0.000236740170959581791 0.000350450107313313869 0.000156572905157562514 -0.000311221867538525915 0.000475448339884174847 0.000405513333648548112 0.000229944767464625589 -2.2734962194866673e-05 0.000846764378459848163 0.000651537443173442056 0.000349652535323760247 0.00021112237912589103 -0.000358778664930493722 -0.000156668169288188717 0.000116730900045225445 ] ;
+bias = 100 [ 0.000597156499075335366 -0.000557823606602718511 -0.000238253337450569153 -0.000235879342121554105 -8.69444764492487197e-05 -0.00022858411903278514 -0.000730159306076523691 -6.1310862865765039e-05 -0.000338108827126749683 -0.000106820660282853225 -0.000240598232693696955 0.000178999751807968119 0.000147132194605513656 -0.000441800759242361532 -0.000802922511599219986 1.2260192346137149e-05 8.16029189217909819e-07 -0.000883220981615029919 -0.000311479120957375928 -1.5260112041313645e-05 -0.000757912731700950931 -6.9395901164534822e-05 -0.000636567710498314146 -5.75710033384960597e-05 -0.0010289751399738911 -0.000961161353018534308 0.000141822862868206463 -0.000403791916958990045 1.02762718962510986e-05 0.000341415377768926841 8.14495862766646253e-05 0.000278342135779378137 0.000494169695295036747 -0.000265473688031311785 0.000154029955903179039 -4.41655417330052039e-05 0.000628093550740835673 -0.00037777589507353884 -0.000296093156850220189 -0.000160696743342817!
 687 -0.000102863595841186077 0.000173761070511071596 -0.000245392814311622891 -0.000107863608452741852 7.33017429951104049e-05 -0.000308803079148418631 -0.000339285603320606949 0.000367751727385619656 0.000184930165812790398 -0.000346475970163319857 -4.0110584105177597e-05 -0.000585039447767461869 -0.000291850089263570893 -0.000299463125227201956 -0.000417868516658002247 0.000480876115391925055 4.64757193019625954e-05 -0.000586106684639309304 0.000257814955044925449 -0.000259232385338512429 -0.000648200046445872573 -0.00026824023145179815 -0.00029401570728265498 0.000353914946215985271 -0.00051662926462639632 0.000329196416397329022 -0.000264261909695937512 -0.000414322501555562227 -0.000574572372497456573 -0.000307518676965848022 -0.000277021766343235902 -0.000685439464607044279 0.000318077902563635047 -1.406624648959265e-05 0.000362849808789448549 6.2624018284797791e-05 0.000728078154506861574 0.00022795360397397457 -5.87026037444542326e-05 -0.000458977441061176196 -0.000!
 160753719307193829 -0.000441783646407283015 0.0007941407743608!
 73529 -0
.000122821327054280883 -0.000799745805908814002 0.000178776017105758064 -0.000329561939590614268 -0.000110755124289750007 0.000417785418861984646 -0.000317521079520178173 -0.000280339867216013899 -0.000381508040821792148 0.000144435280777569891 -0.00075784843857775808 -0.000557582505652309021 -0.000367294182775605195 -0.000128098503839851695 0.000331263477375859752 0.000277292872525089561 -0.000105039647617090483 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMBinomialLayer" ;
@@ -128,106 +128,106 @@
 ;
 connection = *11 ->RBMMatrixConnection(
 weights = 100  10  [ 
--0.0983278853201735165 	-0.090476953427393661 	0.0878270763559228645 	0.0404778761400572118 	-0.103681094452749867 	-0.0493210439705874595 	-0.0773346126048265609 	0.0783310894245029532 	-0.0911586872086200123 	-0.0270551779475636418 	
--0.0240770656034972642 	0.0372975327188816083 	0.0880993813960214561 	0.0633397467092382399 	0.0512573338750677565 	0.044607653193218133 	-0.0727165539271129313 	-0.002955972209571799 	-0.0390562442384003936 	-0.0369834158339325952 	
--0.0115583371156903151 	0.0431130737741324266 	-0.0575219223318406137 	0.0280552970569761216 	-0.0251469125835525505 	-0.0551756733886376305 	-0.0390099071630479188 	0.0853960829320404985 	-0.0746697252753047575 	-0.0129532894709286969 	
-0.0044595361338094679 	-0.0486962915242001096 	0.068574141200721736 	0.0915359179830042141 	0.000751337126967821707 	0.0118066452260008736 	-0.0535164231104842125 	0.0203754617105944558 	-0.011321919222791136 	-0.0277033023117404867 	
--0.0125011399546409614 	-0.103105698434456758 	-0.0525534230612588055 	0.0486104283968393919 	0.00116106625621186735 	-0.0485512787926472636 	-0.0820744450612246185 	0.0746984409189113824 	0.0626413038863968957 	-0.0624909659599040268 	
--0.0709562455236415063 	0.0602146255403163758 	0.0087064653605689931 	-0.0823551091897687382 	0.0864122366090946947 	0.0370387674206175341 	-0.066860340602185292 	-0.080896793781381926 	0.0318217146010210186 	-0.011935867026001603 	
-0.0332704813086629816 	0.0671623488273559027 	-0.0229085599400149741 	0.0453890515930241364 	0.0212654153448908687 	0.0420831357455372507 	-0.0537685317282787026 	0.0698978039270756885 	-0.0463971252642229326 	-0.0242865923049640636 	
--0.0631940446603122818 	-0.0282042681328232119 	0.0961779185675527837 	0.0685300264892744898 	0.0176552129611400321 	0.0655138407554695656 	0.0210617263486490881 	-0.0874249550717382945 	0.0835293615526824101 	0.0692447634262297745 	
--0.053909086361254091 	0.0791192567229923593 	0.0510631114598945265 	0.0570873515058361591 	-0.0500921660267183155 	0.0532795682402603249 	-0.070896146183767661 	0.0751060957133843909 	0.0808167344027823642 	0.0967467565640633409 	
--0.0701835239147086953 	0.0272675374377542717 	0.0953539227826255553 	0.0642853153633272251 	0.0169576909482348492 	-0.0243533262524799066 	-0.0268185938246712219 	-0.041978207129887854 	-0.0237588638138911713 	-0.0300364138331597856 	
--0.0275888683709031857 	0.0703739444629655209 	0.0234373135733819925 	-0.096314634051284792 	0.0476937024316003016 	-0.0532863855488921248 	-0.0675179648343382788 	0.0919040121964791734 	-0.0959515946818641635 	0.028363318554251369 	
--0.0745607014256321476 	-0.0547537438312731412 	0.0556256908793625396 	-0.0149606590785685903 	0.0703849378092315237 	-0.0534903436497037033 	0.0537319383085632304 	-0.0786078219810544265 	-0.00311310591500297803 	0.0604994036507613756 	
--0.0524209366900263429 	-0.0376027809658913414 	0.0302922290755389852 	0.0443606238242897147 	-0.0383321711963647491 	0.0101198350468794229 	0.0355589528484405101 	-0.00396720592750030802 	0.0741141958128512801 	-0.00561775838115452161 	
-0.0402944272959286157 	0.0731953505748937916 	-0.0573769153196316758 	0.0331077644181598171 	0.0682437950186051689 	-0.03847391257012929 	0.0958362139033041849 	-0.101520368482999043 	0.0221235100308348372 	0.00369842533136112081 	
--0.0156608535841887865 	0.0929490251792778055 	-0.0964571007261865732 	0.0477882299576741776 	0.0705837660026292363 	-0.100592801841497007 	0.0277736504352724643 	0.0962985901423360074 	0.0595498375517518608 	-0.0655048511280196949 	
-0.0417365002087489098 	-0.0499368460089651967 	0.015589476338193315 	-0.0151399206113957008 	0.0288501800996455969 	-0.0349011456787701596 	-0.0870877684738774055 	-0.0322696612790026247 	0.0873518879944091969 	0.0504215924988923453 	
-0.0614046957026526674 	-0.0558493268484981237 	0.069561049317357071 	-0.0561528225999334829 	0.0870380590906843221 	-0.024659928923111956 	0.0437812121302702459 	-0.0741130055714870478 	-0.0733746843242180646 	-0.00507259459973898041 	
-0.060622303859138936 	0.0107456374770745437 	-0.0877112542221551023 	0.0178939068968729173 	0.0663851485354365611 	0.0877615635424070645 	0.0473126639372273491 	0.074329085230616071 	0.0874583845595456599 	-0.0304526378252197479 	
-0.00932020210968988704 	-0.0535687410912044845 	-0.0268785361616462569 	-0.0434815860770612211 	0.0853969050551322134 	0.0450502498605219667 	0.0162775609138952858 	-0.0115966876124464109 	0.0973283834198579723 	0.0694642820258329768 	
--0.0275735365627713461 	-0.059627174696372541 	-0.0523705006062654063 	0.0193351810864486259 	0.0291976434131886037 	0.0212850473226258857 	0.0553098775557430214 	-0.0617544134570313444 	-0.078702942175654933 	0.0470142971465319706 	
-0.0745197488085138776 	0.0693913359521724643 	0.0508923750816854745 	0.0524500737217688875 	0.0670534240758872552 	0.0794025930446826678 	0.010404161240022438 	-0.0552951642267308849 	0.0518329831400171498 	-0.007755728119622083 	
-0.0973863507463915007 	-0.0706857106799629042 	-0.0598370209277892251 	-0.071802993444297622 	0.0447350207471328212 	0.0362578977776103969 	-0.0938579653823830018 	-0.0542128964703343513 	0.0569301322639583357 	0.0794507950575790295 	
-0.017642144177710338 	0.0942158975352288403 	-0.0524008000101425231 	0.0447325449675124998 	0.0761182862727339538 	-0.0508414292674252907 	-0.0736717577629257825 	-0.0561709978254562037 	-0.0145710354501133194 	0.0339727149536715178 	
--0.00140529554390188597 	-0.0862090118783880865 	0.0308975871178201154 	0.0447892602117326427 	-0.00110268218834636608 	-0.0661016920605117797 	-0.0935898150736757944 	0.094413857896100975 	-0.0102214340190406022 	-0.07215667521606553 	
-0.060800166667983338 	0.0853500992134728631 	-0.0137083259305561898 	0.0187814742021329383 	0.0770531912406703101 	0.0859164504239076782 	-0.0390682745931406963 	0.0615710805276957623 	0.0816771178526538577 	0.058013112671925951 	
-0.0961254618259466398 	0.0339269885052152984 	0.0915213230559148577 	0.0818078670147190934 	0.0912466255438074691 	0.0486681809513788283 	0.0618332814312244697 	0.0402956574900289258 	0.0554841358599464443 	-0.0840294201155209197 	
-0.0269614104937117745 	-0.0521796121518171771 	-0.0862074703465969261 	-0.102462360708182376 	0.0100124477156839954 	0.0641239830898032021 	-0.0271152447016854807 	-0.0498377328660065538 	0.00141790462791869512 	-0.0607460880204375095 	
--0.0250077324553783717 	0.0202464252302377509 	0.00390031476628415049 	0.0671377576791647307 	0.0466313851230096693 	0.0231880833276212894 	-0.0102147724489738481 	-0.0576455304140673364 	-0.0920814230075747836 	-0.0325461385349084323 	
--0.0820556159462757645 	0.0832474370808195946 	-0.009142642788799114 	-0.0846494993430734477 	0.0035743095260170883 	0.0213644565194758343 	0.0128959304883104094 	-0.0105626982922347387 	0.0973564477964843217 	0.0703965987674467913 	
--0.00749316098147650295 	-0.0961195402981020602 	-0.0784370479866065701 	0.0353004349716579333 	-0.0654845832293780572 	0.0131938368830952866 	-0.00520050634238103295 	-0.0373891767240617304 	0.0275432468028760195 	-0.0415962215084591594 	
--0.0407181205755452261 	-0.0719506805695318691 	0.0564663808900484274 	0.0219920231424191731 	0.0644047128146026937 	-0.0162659181569590576 	0.0681238332189003093 	-0.0970265036843121209 	-0.0634015443117269817 	0.00136076159497540341 	
-0.00433744288270478985 	0.0174819678737437242 	0.0472404564199387228 	0.00231815326885091103 	-0.0815493246678215961 	0.0569510324280310468 	-0.0773430650560512356 	-0.0639536049063257339 	0.0644626057099640937 	0.0206314660411838456 	
--0.0836066096010600612 	-0.0954402715092635551 	0.0358009131168599351 	-0.0550184946126441726 	-0.0219028936087398395 	0.0385544171598716442 	0.0128658083549691073 	-0.0161969249095287905 	-0.0779585416672219228 	0.0135133776847866614 	
--0.030151459350523227 	0.0114781422683107627 	0.0219916416482176004 	-0.0283696501983019232 	0.0255631405262626288 	0.079255915564360277 	0.0180339959679713183 	0.0236487529438588646 	-0.0172269639827540531 	0.093887422514503896 	
-0.0499053250218559064 	-0.0532445924094384279 	0.0649826583038403405 	0.0710338957138486882 	-0.0619513487229561566 	0.0587563649495363716 	0.0542773464037465472 	-0.0589124801090629216 	-0.0103806015895185743 	-0.075879895393659455 	
-0.0853123754768466275 	0.0337373747494842113 	-0.012490029278148464 	-0.0829369150150501833 	0.0160836310248429447 	-0.0854470057716257142 	-0.0393542963131241763 	0.0370483233586837019 	-0.0528552445327559461 	-0.0250989784459685547 	
-0.0935289747697133911 	-0.100240526724298146 	-0.0660413361171134222 	-0.0759536334060253032 	-0.0933551726539701421 	0.00238567952636332221 	-0.059383369196017971 	-0.0215139199215652437 	0.0876078497834278103 	-0.0144582045141890304 	
--0.0810642787298139211 	-0.00247787844109333697 	-0.0928438719987697975 	-0.0250919939974236783 	0.0622739321920611072 	-0.0203270053412506584 	-0.0682269423585313145 	0.0755301848146254784 	-0.0671298128742403993 	0.0841279872743389973 	
-0.0765276255234817598 	0.0224782103037902727 	0.00322022222338522787 	-0.0799534215455117991 	0.0199914962021877606 	0.00157939893958884735 	-0.0735587953796657257 	0.0908107548332792724 	0.00393554654255167877 	0.0232946501139690287 	
--0.0890024840762552172 	-0.0495538904132049179 	-0.0275648584094368987 	0.0637839096325383964 	-0.00414192863785452312 	0.0495847204751576551 	0.0339416428473338708 	0.0327087197825736281 	-0.0606697807853727594 	-0.0103313022199674077 	
-0.0428388315324413765 	0.0700674032801735791 	-0.0833546811564132711 	-0.0947875721998415466 	0.0366937154700035656 	-0.0963802689483590075 	-0.0900708554229863595 	-0.00784572334137589346 	-0.0132219455565878927 	0.0358560999167491998 	
--0.0451477308054970342 	0.0416044115365380163 	0.0190494121430670078 	0.0778867094761605816 	-0.0682015530740904541 	0.0136163490153087558 	0.0719171792854219893 	-0.0944470951923955387 	-0.000606664261586872095 	-0.0826199315030712728 	
--0.0325029256441092193 	0.060411057299010848 	0.0782834545004192084 	-0.0832542298584394713 	0.0886634591454777965 	0.0506957223379732314 	0.0671633057036700315 	-0.0570631010587839826 	-0.0948574929054655375 	0.000634581666020785562 	
-0.0857755834892572622 	-0.0744997702981224086 	0.0586380947437978375 	-0.0384530961647886332 	0.0662065712618262331 	-0.0217737239924924927 	0.0863573689972307185 	0.012493174954816803 	0.00866768976220950137 	0.100030079103112735 	
-0.0560412506369472596 	-0.00253552571936021577 	0.0152369653688978653 	0.0162900949614415598 	-0.103910545390845169 	0.0609848164568339915 	-0.075155674143427581 	0.0547659482421834765 	-0.0230216008361844165 	0.0416008305858422758 	
--0.0774658283394780711 	0.0446673191329392053 	0.0706609197874755179 	0.0091392103620709618 	0.0648532195160397562 	-0.0917724297412287754 	0.0615962298137745015 	0.0694039746175760391 	0.0280746973579828231 	-0.0671877415305803249 	
-0.0787932477921934604 	0.00763544963546061457 	0.0579864429836383019 	-0.0246056916909965853 	0.023126496262486574 	0.0747379293319850452 	0.0223607797924659409 	0.00953537239624350011 	0.0688730492665259597 	-0.0366972767429773264 	
-0.0290512028119888598 	0.00585508124568106646 	-0.00926810537172123756 	0.07656109690999989 	-0.0906915062731083466 	-0.0945540821859799097 	-0.0195007629359466618 	-0.0654068528685156358 	0.0658629352702254506 	0.0424838898619428959 	
--0.0428497783616252001 	-0.0502122777949626023 	-0.0771089991216601656 	0.0890108352682747461 	-0.0870910829205351206 	-0.0193591932075558998 	-0.0914845690913227982 	0.00269604333330901273 	0.0231239751861652583 	-0.0823036249228250721 	
--0.0194362019329465084 	0.0514586301553504322 	0.0237474821937480561 	0.0918693894583151455 	-0.00375521186423493844 	-0.0542597350788369265 	-0.0255748809511426589 	0.0256937511305837321 	0.0191128464610620524 	-0.0826443413012285383 	
-0.00554734701511273767 	0.019121293064866797 	-0.0644677332752886384 	0.0149358527139032554 	-0.0686449848671990409 	0.00539234094701816422 	0.05115577237672949 	0.063112208166000508 	0.0941164323426225968 	-0.0937243928214305383 	
-0.0783926294696053738 	0.0623519002802927283 	0.0849495582755682987 	0.0113930971376347296 	0.0469053182654787515 	0.0316919001036405426 	0.0875785966353781059 	0.0354791236374941213 	-0.00392301608673616218 	-0.00591338932318759107 	
--0.0971858458945512038 	-0.000803127052693033581 	0.0686943560426363925 	0.0911256764064952707 	0.0880199004006167601 	-0.104683156831063287 	-0.0545998035296439144 	-0.0398363625108950348 	-0.00889532127266509307 	-0.00577548645977217241 	
--0.00989455288618178858 	-0.0334707193168130857 	-0.0854176835135032481 	-0.0334144206971058594 	0.0643345385755737242 	-0.0623033162676676078 	-0.0373016784091210057 	0.0735286704328881879 	0.00794625672345554077 	-0.0330196747509667529 	
-0.07018286081551392 	-0.0613311379032248638 	0.0792629242140580059 	-0.0135090765132870674 	0.045644682068675245 	0.0408379360481003206 	-0.0913437752476284615 	0.0963643562413194626 	0.0553375724544897599 	-0.0432523023018455272 	
--0.0753365376278482685 	-0.0201379581163607727 	0.101249467594313539 	-0.0647144412929950286 	-0.088483809828661053 	0.0915711869331539141 	0.0728124826859900542 	0.0481027427578791661 	-0.0967793893299807029 	-0.0375158544299487723 	
-0.0530822766193038212 	-0.0189302872545122139 	-0.0416425524445555167 	-0.0201283968203071865 	-0.0515802535842797213 	0.0361310608923682969 	0.00364487342246621118 	0.0270089257352053476 	0.0257755708015442453 	0.0922968594297266381 	
--0.0479121125847451687 	0.0824668997356646366 	0.0192735970892271499 	0.092123010820163137 	0.0601451644425714388 	-0.0529001038260220255 	-0.0233991134035352877 	-0.0164962464123145323 	-0.0765709259445440105 	-0.0516139593216815781 	
-0.0199639232372231176 	-0.0615709091412481521 	0.0520640588394241932 	0.00543201855353124628 	-0.053197654371009824 	-0.0312724842732881869 	-0.049148987757265436 	0.043317040918352763 	0.0718232041000750054 	-0.0499460314918058479 	
-0.0480927167600447708 	0.0346777380508245109 	0.0851177548099239373 	0.0521898087770936644 	0.01212167860061957 	-0.0346189658860344834 	0.0384892889761907914 	-0.00681297258415391668 	0.00228101663303072969 	-0.0731192285033845785 	
-0.0737206119044683944 	0.0270138821010978275 	-0.0764604153981908174 	-0.00226630066609131856 	0.0631583083171374515 	-0.0566712930930714995 	-0.0744845801696929627 	0.0806291246157731928 	0.0779051512579470307 	0.0138713802585085626 	
-0.0592873134191246459 	-0.0275413285555696184 	0.0657181749168198603 	-0.0279645553471553829 	0.00462399783637966032 	0.0626660905314359584 	-0.026208358815707683 	0.0877587375833814298 	-0.00632619425999898441 	-0.0118173954924016122 	
--0.0627931881153840077 	0.0626892206344245306 	0.0487099499207079706 	0.0752838044507967152 	-0.0457175493944406292 	0.0784480142047764561 	0.0868466736360639091 	0.0349067087009391416 	0.0596160576287692451 	-0.0872582848971534125 	
--0.061147543218998554 	-0.00583737820842567413 	0.0223942497402318426 	-0.071153983346790517 	-0.0253012516175615369 	-0.049312243268452724 	-0.0680667598911656874 	0.0260724197429555958 	0.0300740395368933426 	0.0528345197674364511 	
--0.0433923006167366565 	0.0318148743996462258 	-0.0279253990082873708 	0.0734804124094689953 	0.0277511320453761381 	0.0450738057991846228 	0.0198538133404026207 	0.00945691106703770885 	-0.0853790011223002493 	0.0987929250004806886 	
-0.0325267328903749223 	0.0952401737277347726 	0.0560993265344558664 	-0.0444916841547440892 	-0.0884035391499191081 	-0.0550252211776931233 	-0.034133180586491764 	-0.00192132250469503494 	-0.0856021153653816202 	0.0166607834878846929 	
--0.055896400096288712 	0.0404526487060456602 	-0.0217439598689860639 	0.0738381854603670229 	0.0462103366390317627 	-0.0491691122801568986 	0.0276479894825398531 	-0.082610331856038402 	-0.0612813188112465157 	-0.064736011328146062 	
--0.0665224772780299151 	0.0596625227947199799 	0.0286352102090198686 	0.0254819018884713359 	0.00874635873925168165 	0.0467093871543559774 	-0.0774700244980991715 	0.0415353773488281244 	0.052100625771484152 	-0.0321805657422645913 	
-0.00803060568628212457 	0.0397968803396624576 	-0.0833773277261976725 	0.0608505931295171831 	-0.0231741035300231137 	0.0850574652316489904 	0.0365258469983746198 	0.0716690741502798156 	-0.0204639038296242325 	0.0373054612321424006 	
-0.0340718982900511044 	0.058884465155935671 	0.0688483442858997818 	-0.091417337985972652 	0.0648978609468913448 	-0.0327408272723838656 	-0.0349399389313258049 	0.0528555613398018417 	-0.0212665853140083898 	-0.05729779497111058 	
-0.00923393063813251862 	0.0457497102887895815 	0.0818658261969869849 	0.0656610813865291937 	-0.0691213244308360036 	0.0416929572623182601 	-0.0815117367220334205 	0.0924216398174351056 	0.0680961246171801343 	-0.0488162491678962307 	
-0.0435400029231261809 	-0.000254524623420633806 	-0.0141872675289828973 	0.0144717832723222726 	0.0719176289087174736 	0.0491238153998477881 	0.0726819074867860737 	0.0648766611468602622 	-0.0443322488276437915 	0.052617672201412391 	
-0.0964650626153733121 	-0.0775936379347950261 	-0.0850231948749835542 	-0.0713300987249227136 	-0.0239377480287550938 	-0.00586124503496828972 	0.0399356771801630608 	-0.0344248717886282002 	-0.0652660605438540459 	0.00524069968771229393 	
-0.0841416512431300279 	-0.00477408357296171294 	0.0270603378458687434 	-0.0977092462365141273 	0.0263417590806497605 	0.031227521035244777 	0.0456917006816976515 	-0.0119833773116489774 	-0.0226570693498512367 	0.0761771648904658177 	
--0.0659555777685713607 	0.00652291958179466915 	-0.0017564509536305054 	0.0339608122156557918 	-0.0683537416917689572 	0.0229797972383320448 	0.0681260313703592607 	-0.0905467792143607869 	0.0237903571956571518 	0.03304106662873494 	
-0.0589076909933827428 	0.0830149173516911837 	-0.00859968055546159182 	-0.0984042728618043649 	0.0111066626294483179 	-0.0942126926594988728 	0.0486915835113160625 	-0.00243638710464806228 	-0.045934685634043633 	0.0526514846078448631 	
-0.0834674127768168655 	-0.0840056728371355727 	0.0498417629352895311 	-0.0838586803371844103 	-0.0591693299412577589 	-0.0262906784244881335 	0.00487891794296023504 	-0.0608050737092921331 	-0.0111772313802894156 	0.0983490152725689931 	
-0.0533427822916349922 	-0.0420435985106295471 	0.00853761437418805967 	-0.010812839247998661 	-0.0947908149167195346 	0.0273258270364765375 	0.0521999903395058626 	0.077307331292262832 	0.0804312670549324343 	-0.0469291276035466096 	
--0.0235070151931778722 	-0.0982788052044813315 	-0.0311709329633853699 	0.01924818378745154 	0.0598886196439838009 	-0.00405389722327817795 	-0.00297411358786609263 	-0.0457528130724480647 	-0.0826181050671389861 	-0.013783865915585107 	
-0.0611122065550565446 	0.0225860074577934507 	-0.0309316701616212039 	0.0923737181887799019 	-0.0300375294164095148 	0.0745718428931507332 	0.0341234587110302856 	-0.00301358803787621582 	-0.0051507190276591161 	-0.0881797716914208968 	
--0.0140137062112228507 	0.0611165953552297928 	0.097679618557603709 	-0.071447391910098107 	0.0444986390898739967 	-0.0271814779474569973 	0.0815287758744890206 	0.0529466272427190077 	-0.0200417066799937794 	0.0752697219870388373 	
--0.0391659008443283538 	0.0911737126213487681 	-0.0929557298317114644 	-0.0271579745188694419 	0.0560739240273912462 	-0.0884841374785882984 	0.0802916667454270649 	0.0614788017419554897 	0.0444241360062974378 	-0.0496550765480281292 	
--0.0817549722432899806 	-0.0933097579735990695 	-0.0090996618425296695 	-0.0914330161054382612 	-0.0328129073826540288 	-0.0741914968973021449 	-0.0277215685248717661 	-0.0312378344619301043 	-0.0569910690568706177 	-0.0804762580290174778 	
-0.00707233340408409527 	-0.0475894265941107369 	-0.0160474562311309771 	0.029066483878026305 	0.0328487878307830231 	-0.00988979815711029352 	0.0633634332667313005 	-0.0235093913424807913 	-0.00994445326882900139 	0.0779323277318088037 	
-0.00218426339933425759 	0.0942811272588666843 	-0.0348541464257178454 	0.00532485814561971943 	0.0689520116266645888 	0.0840451853201161364 	-0.0874067209621135627 	-0.0126245799817042478 	-0.0171356010780326917 	-0.00121557161339867444 	
-0.0536518466966218049 	0.00938898951681198593 	-0.0795274843701763989 	0.0566116316176375825 	-0.0601724336869432566 	-0.0765668707897655537 	-0.020469575727949333 	-0.0683694839380735858 	-0.0552817083718727328 	0.0861146117875377509 	
--0.0892562159673176841 	0.0211978711325650944 	0.0563737350082109123 	-0.0393068273517857628 	0.0849825885356143562 	0.0352269407608744728 	-0.0403630068908733611 	-0.00755722683053348856 	0.0677743815947410289 	-0.0927439514516107261 	
-0.0425842031933740262 	-0.00206487780634025337 	-0.018304162287049966 	0.0609838673965466815 	0.0119410742321709364 	-0.0499815768860966542 	0.0792521845090305155 	-0.0609374574582420836 	-0.0297134497326824801 	0.0559357631430829061 	
--0.0711486713036194873 	-0.0495408200160052589 	-0.0510011253452351196 	-0.0625892903068582945 	-0.0573508909516913176 	0.0103905131125873532 	0.00677753253337011995 	0.0420447948197065283 	-0.0854862651808653007 	-0.0292738890658508361 	
-0.0785104298240523607 	0.0210520500348749008 	0.0461853448293126981 	-0.0299216199152313594 	0.0566339115697696363 	0.0861027452217623807 	0.0379434372922779414 	-0.0975870826523519697 	0.0172656530238030556 	-0.0438289270445107668 	
--0.0174013071631024079 	-0.0375898928223683954 	-0.0206377423709534526 	-0.000818465582805396057 	0.0445104499308474483 	0.0939983776931624798 	0.00874119126013329917 	-0.0391747122073122164 	0.0942004980312532159 	0.0238063576985232186 	
-0.018346437742182805 	0.0571399644642307594 	0.0401803101400046159 	0.0405384151668085266 	-0.0140694114507092811 	-0.0184780317204564989 	-0.0141389462461444532 	0.0891615589620289722 	-0.0576213681971627922 	0.0299959523568848058 	
-0.00831481331449872296 	-0.00816533678018874567 	-0.0721813427027074295 	-0.0120732305148561203 	-0.0451318530109676708 	0.0542670991284076309 	-0.0214298188595108997 	-0.0647567141550051545 	-0.0466329959415629769 	0.00169818153598384541 	
--0.0454999413374200523 	0.0699830244255046779 	0.0488542100621626868 	0.0738786423341689508 	0.0632588946393739804 	0.0685889983076646975 	0.0853622624591779955 	0.0197225784444896614 	-0.0534336631456610034 	-0.0447608995319677266 	
--0.00371352438591492258 	0.0658577503410383708 	-0.0608128443071316652 	-0.0689276519025837781 	0.0478158090477341022 	0.0688486398338425537 	0.0757086680660917499 	0.0822802822541476159 	0.0114711784167629751 	0.0930052010885359243 	
--0.00309707417417784577 	0.0439425883806780215 	0.0506333546952207475 	-0.102003028592923817 	0.0673399126573312468 	0.04807948049467866 	-0.0796666215835296898 	0.0448761642195496524 	-0.0431353375026409594 	-0.0858333618009598787 	
--0.00741095610430689104 	0.0106993240545882155 	0.0748824226867455256 	0.0364078310911113492 	0.00189138608753070979 	0.0860495096174477975 	0.0528192357791415074 	-0.0737862474358411541 	0.0331765611005001224 	0.0777348815506801877 	
-0.00434950599295950372 	0.00617028160775128041 	0.0403802031284580254 	-0.00964203679864853933 	-0.0455115207570566116 	-0.0313154228913074886 	-0.0122537392341694387 	-0.0553897218317578044 	0.0136085673696140566 	-0.000295370207961080742 	
--0.0781261736755488462 	0.0656585813008742636 	0.0425508052403904338 	0.00862257464017145842 	-0.0870721390656806898 	0.049616709887092679 	0.0968133237017820419 	-0.0403157816361057741 	0.0301330816745670992 	-0.0731897599790999331 	
--0.0141914108397773638 	0.00465341556321864549 	0.0555460216402348453 	-0.0578963296520183177 	0.0716783129179473788 	-0.0386630053601519502 	0.024721547723120324 	-0.0131978363987982541 	0.0575332763330094268 	-0.040524405550603583 	
+-0.0978471800215257037 	-0.0902053404604457182 	0.0869754571825244738 	0.04089118804206434 	-0.101368086228391774 	-0.0465450149174224462 	-0.0747286765670649306 	0.0781538958897185221 	-0.0910596663974124754 	-0.027060705005204036 	
+-0.0252235703285304709 	0.0354286612789488087 	0.0865639653081493043 	0.0614650337319190138 	0.0504116863076464333 	0.0453555903578909195 	-0.0713761496668450651 	-0.00510598328215907966 	-0.0401508455479649556 	-0.0381547596057954103 	
+-0.012201564296345337 	0.0420692804838235443 	-0.0589592119877042353 	0.0271329090054837352 	-0.0246631886592046899 	-0.0536774953404823632 	-0.0373242762756519977 	0.0839985306538834353 	-0.075511151135593188 	-0.0137996039460813787 	
+0.00368161158062422014 	-0.0501473051925635516 	0.0673034534554419889 	0.090145308971782856 	0.000450396820622549448 	0.0129954022032453178 	-0.051842324345002036 	0.0186563341973783187 	-0.0120690257360572071 	-0.0285276541865736519 	
+-0.0129379800125673291 	-0.103980705380140676 	-0.0539147279523639888 	0.0479517446087257757 	0.002094493937602701 	-0.046696977104081952 	-0.0802431914099717286 	0.0735409085950746444 	0.0620187184289074656 	-0.0632002931674442298 	
+-0.0716343255372051541 	0.0590042959001069733 	0.0072827631311536559 	-0.0834759753498713064 	0.0867722030742086037 	0.0385226901828749912 	-0.0652884327719597674 	-0.0824271906213171618 	0.0309700920772651468 	-0.0129277639998376884 	
+0.0318868724906055737 	0.0650593210786601789 	-0.0245714586022686143 	0.0433299122052953969 	0.0201364126839004114 	0.0425738749199970068 	-0.0526247915666996774 	0.0675331131708467175 	-0.0477044071959432589 	-0.025598622557758275 	
+-0.0640216405987112103 	-0.0299635163224174494 	0.0952592499384477542 	0.0668103062730507208 	0.0166086112657421181 	0.0664500679329425165 	0.0227828956923666094 	-0.0893090662942366553 	0.0829269017431499555 	0.06856507275119339 	
+-0.0550278698599023092 	0.0770251765830581442 	0.0500563329594138628 	0.0550146273205624689 	-0.0516753580715226812 	0.0537945041440263463 	-0.0693359883892173123 	0.0729218325706957604 	0.0799929000213076835 	0.0960063223401683014 	
+-0.0707554591003550087 	0.0261111657093878249 	0.0941372243888994598 	0.0631654536171545877 	0.0170878181536895449 	-0.0229053753818019984 	-0.0250195065968273275 	-0.0434434186710700304 	-0.0244383203596413717 	-0.0307728326241952094 	
+-0.0282726977772407816 	0.0692922496347341127 	0.0220729968096950724 	-0.0973701172734391524 	0.0479943686715152487 	-0.0518425289509872947 	-0.0658329478901666376 	0.0904534376062660767 	-0.0967483311631070902 	0.0274779032809275119 	
+-0.0749180771549134289 	-0.0556314468690856934 	0.0546280338170591212 	-0.015759322709810647 	0.0708181780326413574 	-0.0517166526253960424 	0.0557141215974588169 	-0.0797537802331814011 	-0.00352613999712846592 	0.0599144738690658776 	
+-0.0528670085289950284 	-0.038762170620892461 	0.0293976134619478198 	0.0433346791942989459 	-0.0382805807924307892 	0.0116297052156124018 	0.0375198279873637033 	-0.00532168376490164442 	0.0736722032536451937 	-0.00607366033620099048 	
+0.0391095640230744584 	0.0712773667374083791 	-0.058777370369054463 	0.0312712307707591133 	0.067242349443067187 	-0.0377084412744321251 	0.0971308441427504443 	-0.103652307129281654 	0.0209696696634045464 	0.00253509251293177742 	
+-0.0171588364027313658 	0.0907601551232998655 	-0.0981836322692552915 	0.0457181217897632761 	0.0694491906621885757 	-0.100096807598169274 	0.0288349067131420563 	0.0939563190155330713 	0.0580861665086693096 	-0.0668678678101209123 	
+0.0412420631610641505 	-0.0510501257610846207 	0.014572665601196402 	-0.016143936728842706 	0.02900140314584242 	-0.0332714076777847964 	-0.0851973071178984964 	-0.0336245111848018155 	0.0868618018288649374 	0.0498116353297165179 	
+0.0608481024886332392 	-0.0569116799989186073 	0.068400372248770297 	-0.0571831431037030449 	0.0872489540589142798 	-0.0230842286627061591 	0.0455652187818045007 	-0.0755394545215909846 	-0.073978778720638308 	-0.00589058283666940574 	
+0.0587743374572413171 	0.00789329314258944113 	-0.0893465210301869345 	0.015147978699547579 	0.0641597853200572971 	0.0876528070286558963 	0.0481404816619878523 	0.0714061221447997718 	0.085947483446818268 	-0.0319564492764216473 	
+0.00821089371880088723 	-0.0555051105100034595 	-0.0280933946238387765 	-0.0453126509210930103 	0.0843524901379098524 	0.0458737090022064212 	0.0176972451932225529 	-0.0136543532179953086 	0.0964521208966484661 	0.0684559826177911362 	
+-0.0280958870094338217 	-0.0605968595383250125 	-0.0536939830278911484 	0.0185050456732452039 	0.0297323388736373574 	0.0229160670400564009 	0.0570508108183647791 	-0.0630618711032856771 	-0.0793649365440222132 	0.0461738074819058181 	
+0.0728351420887712597 	0.0666410235313574378 	0.0494792264267038748 	0.0496763266619888941 	0.0646676527061976619 	0.0793576424332215064 	0.0114131708236924114 	-0.0581806821111141154 	0.0504859418107055993 	-0.00911237013743403448 	
+0.0967952710302167896 	-0.0718511758900344244 	-0.0610162385239595706 	-0.0728244178184092733 	0.0449532172677859013 	0.0378468997457577574 	-0.0921341442982845926 	-0.055664063262185752 	0.0563301655537746179 	0.0786459667210254038 	
+0.0164745996068444006 	0.0924700937753852592 	-0.0540731976644756376 	0.0430464584705524192 	0.0755343311208682283 	-0.0499327949708687907 	-0.0724181596807987188 	-0.0582060784221022642 	-0.0158077489368196773 	0.0326834633026971549 	
+-0.0017875468964886228 	-0.0869974968769170326 	0.0295976076183521418 	0.0441334710836344332 	-0.000192463262426864287 	-0.0642391132227323886 	-0.0916698367580487811 	0.0932836870941053803 	-0.0107793497686112395 	-0.0728094587693954576 	
+0.0587479219193328406 	0.0821680843214413342 	-0.0152641196778351643 	0.0155875877114616222 	0.0740472414247421301 	0.0854385475675609879 	-0.038305799066720117 	0.0583463179284819458 	0.0801144536650854194 	0.0564738144694165167 	
+0.0940899965797011434 	0.0307091441747545831 	0.0900328705251852435 	0.0785367782152500832 	0.0881328378019818753 	0.0481656782083180884 	0.0626514637218753279 	0.0370131535481861423 	0.0539419100316362587 	-0.08554070466044425 	
+0.0267770875099327746 	-0.0527028069259232596 	-0.0875121894418438712 	-0.102763886213666097 	0.0115735260855198765 	0.0662990428625958583 	-0.0252398580773533915 	-0.0507800685040700056 	0.000888250710291267577 	-0.0614608956463818801 	
+-0.0258964815038111966 	0.0188390051509506745 	0.00227796022337114394 	0.0657967815509993559 	0.0466204263681099795 	0.0243726024934155967 	-0.00878145291935440893 	-0.059414483397086576 	-0.0931162159601290201 	-0.0336980703159352746 	
+-0.0827055227273208599 	0.0818624414826298469 	-0.0100739312020822012 	-0.0859335293119874305 	0.00327143050697012352 	0.0226369583876831447 	0.0146765130992764989 	-0.0121176597533669203 	0.0967442086187228073 	0.0697890856835364504 	
+-0.00745826725616853883 	-0.0964640904546764932 	-0.0795215372867596054 	0.0352150068892425339 	-0.0638309265380326007 	0.0155327281291228799 	-0.00305800644031185799 	-0.0380918216866338449 	0.0272470536843010362 	-0.0419965098332424625 	
+-0.0411439402374458724 	-0.0728629894578508991 	0.0552819486421851358 	0.0211570654093570822 	0.0649121319561195009 	-0.0145505857125498 	0.0699884843315146804 	-0.0982756424787766397 	-0.063944643334474735 	0.000620209474960204668 	
+0.00416525307999549444 	0.0166465191526073905 	0.0464601937490181777 	0.00159073011554853932 	-0.0810419547018469133 	0.0587695853496177853 	-0.0752120721188494629 	-0.0650844598149195319 	0.0641881737373511813 	0.0203239899258255211 	
+-0.0834353432236865228 	-0.0955822808254698508 	0.0348501126493723015 	-0.0550142504178344066 	-0.0201505792185051914 	0.0409778259330861455 	0.0151601782900929634 	-0.0167503488448184912 	-0.0780623677050969189 	0.0131996537271319124 	
+-0.0311875626213257012 	0.00964766922438716965 	0.0208310649693951716 	-0.030166128094916457 	0.0245147953760221637 	0.0800177054946719785 	0.0195435731221534278 	0.0216283567782093039 	-0.0180527829305334592 	0.0929732723292680113 	
+0.0494884033174168639 	-0.054358742361751422 	0.0640360253134918123 	0.0700181231003522836 	-0.0618263622355974471 	0.0602747299634640507 	0.0562135558304485278 	-0.0603367217069216283 	-0.0108509209337982193 	-0.0764004339137397126 	
+0.0848612067087960997 	0.0329109132943089591 	-0.0136977181999342005 	-0.0836780609100953587 	0.0167781460043432348 	-0.0836734220703956033 	-0.0375079134292391966 	0.0358231923109561187 	-0.0535012528723355779 	-0.02581413891750442 	
+0.093872581974977623 	-0.100288405586654278 	-0.0667369635404056472 	-0.0757432695708722681 	-0.0914286777768788944 	0.00506720166870514282 	-0.0569430514126728557 	-0.0219288156913358609 	0.0876542703845367838 	-0.0145189574390498823 	
+-0.0818798343322028055 	-0.00366424918696527362 	-0.0944773533676381977 	-0.0261456960698514818 	0.0626537530347011096 	-0.0189298493990359799 	-0.0667215622413877274 	0.0740343450983735191 	-0.0680679552072742111 	0.0830434652894488923 	
+0.075616084261812877 	0.0209163601145687973 	0.00200392697592247353 	-0.0814615918998283783 	0.0194881244677043743 	0.00265130477851547794 	-0.0719831298173050638 	0.0889834087611306446 	0.0031138416969896149 	0.0224144322520954016 	
+-0.0896849819534228931 	-0.0507681076651540775 	-0.0289779219096500985 	0.0627111423993589739 	-0.00388153114169776346 	0.050949102919937185 	0.0355886318638906976 	0.0312050420039175977 	-0.0614487171521126557 	-0.0112073945688570542 	
+0.0424085111970063794 	0.0693315307404584608 	-0.0846862732797641765 	-0.095393870981865761 	0.0376548271927241024 	-0.0944778011564852693 	-0.0882780307280190552 	-0.00897148407766692156 	-0.0139322356736678553 	0.0350624563590257315 	
+-0.0454355257807062193 	0.0407335288575404603 	0.0180117722199369633 	0.0771493812157636322 	-0.0675758387109726688 	0.0153502240693769044 	0.0738811919590802307 	-0.0956471899756761029 	-0.00114179025482402099 	-0.083127948494937931 	
+-0.0334213644483165387 	0.058862185098133811 	0.0769761653587529709 	-0.0848236867748237977 	0.0881689584635699936 	0.0517082924243271319 	0.0686564098239453718 	-0.0589395097959882833 	-0.0957627504743620295 	-0.000411925827903007378 	
+0.0847763359119753507 	-0.0763375138329060315 	0.0577498920380896397 	-0.0402810002700549064 	0.0648664324744550047 	-0.0209504144505155525 	0.0880108710566264996 	0.0105080587462940585 	0.008022103259592012 	0.0992584693355362535 	
+0.0555958892386492418 	-0.0036287394435019593 	0.0143069433727433942 	0.0152863546044747795 	-0.103839765995600436 	0.0624549146255257079 	-0.0731947351204865204 	0.0533670424959666531 	-0.0234650870602571907 	0.0411367046365152672 	
+-0.0784377634731182327 	0.043013821241959814 	0.0694041032397587909 	0.00752475691259612504 	0.0642290136577166187 	-0.0907926066714660751 	0.0631589696110833737 	0.0675612503274558018 	0.0271515196662819938 	-0.0680822117213741168 	
+0.0776204240847777893 	0.00551169289000760353 	0.0568811442911565032 	-0.0266963208569938049 	0.0216901896270737318 	0.0753191394404137821 	0.0237798723518230296 	0.00724602707662163215 	0.0679537365137067723 	-0.0376416963609675775 	
+0.0290078560004861583 	0.00526448632767929046 	-0.00998920760775910514 	0.0761213374238426804 	-0.0899273903063939339 	-0.0924918258563098422 	-0.01722691103449785 	-0.0662764783305807709 	0.0656496418263388115 	0.0423025198272805672 	
+-0.0428145482360620735 	-0.0504746153192209557 	-0.0783926234610002326 	0.0890118049976672265 	-0.085156531974798208 	-0.0169602694851252872 	-0.0893771321465304802 	0.00203897817814427873 	0.0227081666325220609 	-0.0827621401647491428 	
+-0.0202939332038845968 	0.0499651065821050816 	0.022370442965727922 	0.0904591859661133257 	-0.00400335804406958765 	-0.0531147537264334418 	-0.0239984404112206819 	0.023937289048371696 	0.0181681047757719302 	-0.083538633665291781 	
+0.00491340188589175272 	0.0177981967637759995 	-0.0655557033661144095 	0.0138100424275088545 	-0.0685934613762886336 	0.0067489785737811208 	0.0529034439817044205 	0.0615799327121482054 	0.0934024260239453841 	-0.0943407425561808011 	
+0.0768151585785116359 	0.0597422989017513673 	0.0837595244610199324 	0.00872565094808127467 	0.0445126667769655329 	0.0316961713127733163 	0.0887695112490788363 	0.032737584807786449 	-0.00510488375715354228 	-0.0070663215006711794 	
+-0.0978956823384336677 	-0.00200908659061244327 	0.0672434044670669057 	0.0899697203124039391 	0.0881844596800903796 	-0.103233207082043713 	-0.052931062714118432 	-0.0413267167620032655 	-0.00972650187846000464 	-0.00671074612941753579 	
+-0.0106292336415997925 	-0.0346021147296487752 	-0.0869695336119137991 	-0.0343722972888396566 	0.0648971622010802579 	-0.0607500675535119355 	-0.0357395000343674676 	0.0720958341470771813 	0.00704618764407910578 	-0.0340237311570772138 	
+0.0690819359301237668 	-0.063247316946823659 	0.0779931352755664081 	-0.0153871641512064901 	0.0446621165647628215 	0.0416411351295535928 	-0.0898780644202687579 	0.0942492828334858812 	0.0544579672023831063 	-0.0442268507056601118 	
+-0.075344362525791797 	-0.0206650076247724343 	0.100501537574655547 	-0.0651641498538071684 	-0.0875231576506865783 	0.0935193382443201687 	0.0750709168043961761 	0.04720028002728794 	-0.0969269081918623654 	-0.0377364874448853707 	
+0.0524635402955774058 	-0.020256222914507991 	-0.0425486874800179993 	-0.0213338359419469603 	-0.0519005485415260231 	0.0374293880400871221 	0.00548471073340413805 	0.0254655463435365555 	0.0252572388840250385 	0.0917372757377646897 	
+-0.0489883465507234758 	0.0808549616726040171 	0.0175690392216576834 	0.0905441074731389295 	0.0597978198485654075 	-0.051931566427641053 	-0.0220680567163850788 	-0.0184359263509704351 	-0.0777786933795720781 	-0.0528490428893146763 	
+0.0197866657554006016 	-0.0623268710318765629 	0.0512216021139901834 	0.00481367540814497692 	-0.0524808786895438056 	-0.0293432470985781277 	-0.0469914955742517237 	0.0422838757133955079 	0.0715508130881882254 	-0.0502512994820883199 	
+0.047162830758310452 	0.0329843168639450332 	0.0839644723490537287 	0.0505133876477681654 	0.0113144966169118639 	-0.0336779467177079767 	0.0400864726177653508 	-0.00875443578863203953 	0.00141773973231703785 	-0.0739639332619120848 	
+0.0724060653433289264 	0.0249891318163628189 	-0.0779804093548025351 	-0.00418599605605514836 	0.0621294941531449138 	-0.0559658941182699793 	-0.0732465016145991549 	0.0784311003474822688 	0.0767134927729501981 	0.0126710224321379319 	
+0.0583069380662354989 	-0.0293102779377430317 	0.0645766828723413205 	-0.0297037758640440359 	0.00373395119793487303 	0.0635226154708643936 	-0.0246390714515497718 	0.0857587717904586821 	-0.0071112261976321537 	-0.0126705462308569199 	
+-0.0638861182897556973 	0.0606482406746605071 	0.0475696350655952713 	0.0733154834629952262 	-0.0469453033611485562 	0.0790303515559192815 	0.0883217850098304696 	0.0327253475504564739 	0.0586621296998571656 	-0.0880996216489109557 	
+-0.0611465984403298801 	-0.00625200849821254921 	0.0215485781279635726 	-0.0714409142747882558 	-0.0240918699485527833 	-0.0471072137986393372 	-0.0658120049173296084 	0.0253410948031073252 	0.0298882899553423141 	0.0525663923173221095 	
+-0.0446255036375375058 	0.0298737678039323225 	-0.0294200842196646398 	0.0715801808220764696 	0.0266416466882419163 	0.0456747000463598937 	0.021156654567761779 	0.00729003319023803845 	-0.08648148970374471 	0.0976206317447985694 	
+0.0325006861144892037 	0.0948047441966916454 	0.0552968812959793374 	-0.0448840539946116918 	-0.0874241780518338757 	-0.0529760302504636918 	-0.0318827349764679649 	-0.00278714771528817268 	-0.0858725369518935633 	0.0163996717366173411 	
+-0.0565635907236660407 	0.0393659838055386499 	-0.0233180830360347723 	0.072863158273558673 	0.0467477920730156662 	-0.0476420517489369796 	0.0292247241584120479 	-0.0840660705971008754 	-0.0622357183100257505 	-0.0657602182571084826 	
+-0.0675117869112619867 	0.0579327661780450823 	0.0272521470153735366 	0.0238255598644825164 	0.00818234658452314696 	0.0476342449350507582 	-0.07600264331210696 	0.0395888239670870098 	0.0511301106927895543 	-0.0331474220748810972 	
+0.00665646873795682197 	0.0375958396472406142 	-0.0848427208591413656 	0.0587497797189621765 	-0.0245756817811631431 	0.0854398162685425372 	0.0377263946322403468 	0.0692954913285604296 	-0.0216598497790273826 	0.0361372816773090424 	
+0.0331965773376135392 	0.0574177242029850141 	0.0675521383088981131 	-0.092874323009645432 	0.0646108581813402 	-0.03158427038835733 	-0.0333754792947546855 	0.0510736519378347034 	-0.0221540054924109185 	-0.0582445210225887994 	
+0.00831261159168455131 	0.043949936149094343 	0.0807988898334478511 	0.0639048827841863926 	-0.0700620858635795374 	0.0425263342773682593 	-0.0798596085912762765 	0.0904361374548426494 	0.0673149697901879313 	-0.0495078811992989906 	
+0.0419252774569496209 	-0.00274780174071750883 	-0.0156616940455278075 	0.0119951715798352794 	0.0699234306356514013 	0.0492385818970535807 	0.0737547335980926339 	0.0622351858607390895 	-0.0456057665774118534 	0.0512802770624419674 	
+0.096390229619075285 	-0.0779849853422766304 	-0.0860802167023280163 	-0.0715296967936188827 	-0.0225309237815867411 	-0.00363272948207470905 	0.0420066209404783564 	-0.0352401961709145825 	-0.0656094816363315542 	0.00473140422529562746 	
+0.0833825990666294814 	-0.00624769795217346418 	0.0261331971787710504 	-0.0991511552554600467 	0.0257142706855030267 	0.0323690044508465405 	0.0474230023001297671 	-0.0137130565796517081 	-0.0232590910323398278 	0.0754590374261445013 	
+-0.066105648662068639 	0.00577369683238790517 	-0.00256740781634764471 	0.0333594230424996696 	-0.0677189469204543815 	0.0248398724313597108 	0.0702538799750030418 	-0.0915703777218647846 	0.0234901769983958085 	0.0327162617841887937 	
+0.0583985241336935634 	0.0820107470728685672 	-0.00955928345111235062 	-0.0993580599362161032 	0.0112385800919470868 	-0.0926553902029668419 	0.0505787953213084895 	-0.00376689350291678059 	-0.0465189557234865769 	0.0520450241696136656 	
+0.0837502091232682905 	-0.0841884966418415814 	0.0494121729707651947 	-0.083936133206873681 	-0.057966470336787583 	-0.0238596826816774993 	0.0074211822514484195 	-0.06134676063250738 	-0.0109940538886606209 	0.0983730670734794393 	
+0.0529123183531036573 	-0.0432345001520824043 	0.00781517721136753586 	-0.0118531469690165252 	-0.0948478666683585103 	0.028792792703982082 	0.0542174633404027967 	0.0759149147840643967 	0.0800598366226032976 	-0.0472517348772329912 	
+-0.0239333137589194597 	-0.0990300630786514896 	-0.032659862078664309 	0.0186467634580876618 	0.0609526977463653793 	-0.00216736699074640852 	-0.00121879808021418306 	-0.0469006585582069785 	-0.0832894957266879232 	-0.0146878193159981473 	
+0.0599722138658623227 	0.020659008771052919 	-0.0323822445617458557 	0.0905535021272905055 	-0.0309009341657960138 	0.0752865968092552157 	0.0354442313417307475 	-0.00519450309841556178 	-0.00626713774979259942 	-0.0892717640885118957 	
+-0.0150246354325077117 	0.0592881779616925772 	0.0967698546253035613 	-0.0733209827353182436 	0.0431741209534788201 	-0.0264526659172714743 	0.0831733342366334505 	0.0509486301014937248 	-0.020779836100568494 	0.0745160348521420601 	
+-0.0402585896396290208 	0.0894841536363436274 	-0.0944303698064100777 	-0.0287095310195410146 	0.0556318572909870884 	-0.0875172338170900205 	0.0816467544554470431 	0.0595895079693248494 	0.0432700294858368237 	-0.0507418226081152654 	
+-0.0809319774806011444 	-0.0924418585648622143 	-0.0100643304933994756 	-0.0903205327462608537 	-0.029226336637553349 	-0.0707156936603000053 	-0.025055311486261115 	-0.0309185316225945514 	-0.0568428656173292218 	-0.0805315476908849553 	
+0.00625181634523529813 	-0.049085613016863601 	-0.0171859723498964945 	0.0276611138545139815 	0.0323232909201209007 	-0.00874766115791745895 	0.0650206249906532391 	-0.0252155741309334318 	-0.0106590562263953643 	0.0771124162221107218 	
+0.000754516090355915889 	0.0921314113864970113 	-0.0366355947914679334 	0.0032200353476968804 	0.0678823463794506893 	0.0845419605618411285 	-0.0863804653901767833 	-0.0150447948971687197 	-0.0185310063059124404 	-0.00267969226841359648 	
+0.0534687822377956853 	0.00883277851451328734 	-0.080587161326575299 	0.0562053915657517791 	-0.0592228707012416797 	-0.0745477080543376497 	-0.0184071952324021577 	-0.0693042091926673737 	-0.0557136555986918192 	0.0856277545351637331 	
+-0.0901180419774364333 	0.0196822318357597545 	0.0549339855542598995 	-0.0407378702141069429 	0.084912674586448314 	0.036434032152185214 	-0.0388734174225119564 	-0.00932332780646582399 	0.066842251031516503 	-0.0937655525763287506 	
+0.0418094611472034847 	-0.0034865495405729109 	-0.0194268913471666443 	0.0596401053066652684 	0.0114756524502698503 	-0.0487885484250375548 	0.0809407284520493181 	-0.0626085378556807542 	-0.0304553560075297089 	0.0551435526152317598 	
+-0.0709662261717747822 	-0.0495529079667091088 	-0.0521124899547957612 	-0.0623922623241493848 	-0.0551999586897789149 	0.0129194853780452319 	0.00901508776135405247 	0.0415840756926816602 	-0.0857215515425445024 	-0.0296371271449199745 	
+0.0774465411392550457 	0.0191608557913295272 	0.0449317265383520487 	-0.0317803820965146741 	0.0556669639242832401 	0.0869109445508808587 	0.0393351692325231983 	-0.0997391157559899683 	0.0163021708533605328 	-0.0448930060002757925 	
+-0.018418132561986135 	-0.0394433166496583887 	-0.0218883833798195933 	-0.00254786229958634935 	0.0436960887846094825 	0.0948916596651195182 	0.0101859201705475338 	-0.0411800986788466786 	0.0933280365166843651 	0.0228310993419338498 	
+0.0172904373863724641 	0.0553657391310228633 	0.0389401209215691282 	0.0387697874927849517 	-0.015063296509071962 	-0.0177175456163194134 	-0.0126091300850640024 	0.0871464199720079491 	-0.0585451638022550791 	0.0290960679291529939 	
+0.00810595506075861491 	-0.00876555606222915486 	-0.0734014035199618275 	-0.0124936078550607604 	-0.0439246418069537473 	0.0562730522301329861 	-0.0195028412347448936 	-0.0657665036585061979 	-0.0471461632622273766 	0.00106772839415366001 	
+-0.0471256240253403111 	0.0674603157880786786 	0.0472513376814104047 	0.0713426610452947141 	0.0613530236129621667 	0.0686458077983143206 	0.0863777940649222004 	0.017018328124105378 	-0.0548410041778817636 	-0.0461669862492033178 	
+-0.00522413158780944513 	0.063453200293711931 	-0.0621297409088511082 	-0.0712824688434225834 	0.0459522273945540544 	0.0690492122492832178 	0.0768613542128165211 	0.079767484699452551 	0.0102854984391889098 	0.0918132944267897738 	
+-0.00393427891828779241 	0.0425840829275945451 	0.0491208377697215023 	-0.103318937985919818 	0.0674843008411907253 	0.0493452973535978209 	-0.0781833223247483111 	0.0431451359384204536 	-0.0440880980351303006 	-0.0869082782702664119 	
+-0.00837582867377460218 	0.00878233975268849151 	0.0739575362477558618 	0.0345049475001776851 	0.000531406817804182881 	0.0867710584392183265 	0.0544403128258240457 	-0.0758545580659029939 	0.03247325983917599 	0.076979697548940379 	
+0.00426838174104775085 	0.00558167442142068308 	0.0395484715143735385 	-0.0101225888321720883 	-0.0446245005720766058 	-0.0292792912149089229 	-0.0100682685054897039 	-0.0563179675782023367 	0.0133393130862614635 	-0.000614661970216939953 	
+-0.0784082845832225622 	0.0646942741948428046 	0.041722169649281271 	0.00777852843327421842 	-0.0866909991926182177 	0.0512377842593555871 	0.0988431344334173151 	-0.0415543275238469539 	0.0296994591925104565 	-0.0735547351960807172 	
+-0.0149061239641139071 	0.00328688974623620508 	0.0544199374629077692 	-0.0591943112961044737 	0.0715185650527177252 	-0.0373254372535212278 	0.0264238647628533899 	-0.0148004898905053327 	0.0568208495103211184 	-0.0413109107758464489 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -281,11 +281,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
--0.0036518323858142957 	-0.00361855059190776922 	0.0088157826227196201 	0.00329435582091404215 	0.00399612176076663723 	0.00319620626019718156 	-0.00728403645489904966 	-0.00792428510918756837 	-0.00377451845635688689 	-0.00994167159938483605 	-0.000926733341182592926 	-0.0097836500687347415 	-0.0010694717333363721 	0.00435924233856639903 	0.00399160269543265885 	-0.00877151894798462381 	0.00197355307296072889 	-0.00727062331247925811 	0.00522284242472082199 	-0.00401282500843031052 	-0.00239435326202826204 	9.25496545061000719e-05 	0.00789449007377343755 	0.00565358649830638091 	0.000283292056387247123 	-0.0051057304708907892 	0.0073271419254243679 	0.00799738660369348799 	0.00362672701461169223 	0.00651005584159845656 	-0.00664963821413064489 	-0.0108928947489404931 	-5.33112865594360702e-05 	-0.000931394450580481123 	0.00553630902573310109 	0.00725725624859564766 	-0.0087199697367579352 	0.00238609317228216482 	0.0030018324624390331 	-0.00108545914606011365 	-0.002470574!
 49166678033 	-0.000622212950647952961 	-0.00385150530911814717 	0.00370010191041333708 	-0.00747330129010255386 	-0.00896020903272062505 	0.0090917501631254307 	-0.000640754789249930925 	0.00579876418228320555 	0.000323996585281896197 	-0.00219359056862478835 	-0.00130090983661071413 	0.00665310730119290754 	0.00249558915869901778 	-0.00525705507946501362 	0.00389697494964724679 	0.00444668333025927381 	0.00888742663396156068 	0.00702122191394294586 	-0.00853493797794211556 	0.00935419372544291257 	-0.00797889342326523095 	-0.00256779078330117471 	-0.00127813893797099716 	0.00483553254291013007 	-0.000389288544983300191 	-0.00492279930517412062 	-0.00470808356997727724 	-0.00308018445661309426 	-0.00595964051326252752 	-0.00217616722941537961 	-0.00734618972569881998 	0.00198183076894415354 	-0.00952674993777776445 	-0.00377798363451271185 	-0.00294619955530591544 	0.00043024461960738654 	-0.00426869460281676001 	0.00742468238003972381 	0.00724013756032714645 	0.00582525457!
 190652841 	-0.000315695917656800976 	-0.00296934631266410239 	!
 -0.00372
753277041127035 	-0.00299667247098370226 	-0.00124319910868674987 	-0.00405563981109149235 	-0.00761454948998514791 	-0.00432897034429482672 	0.00623702729313122938 	-0.00239503991881194608 	0.0032185292965010953 	0.00483282595970415708 	8.87212570550639952e-05 	-0.00370865627249922355 	-0.00817331740100062445 	-0.0103862196264158761 	0.00173431089930873791 	0.00821416249274280544 	-0.00248194403064848747 	
--0.00424630452237416064 	-0.00544487675391943857 	0.00655373524100850025 	0.000180183683472441667 	0.00241296409784662411 	0.00930476590552893959 	-0.00480340834784713684 	-0.00778023606908031119 	-0.00658704381245159461 	0.00241934708480953404 	-0.000526487661411041133 	0.0073359810096966082 	0.00670588757811955635 	0.00199316085328318576 	0.00569386474427098846 	0.00484958634661471644 	0.00223320796031350327 	-0.00934741592765000844 	0.00945110614263983302 	-0.00271597862807588207 	-0.0013540104181779737 	0.00684458083953712847 	-0.00674047871619778451 	0.00411629836183668042 	-0.00664349012016314088 	0.00553146296056607808 	0.00139741615325955203 	-0.00736906604755948724 	0.00740750520041282522 	-0.0097390196553429273 	-0.00874268202112920938 	0.0106683945342594677 	-0.00679936761216249144 	-0.00166535412312507065 	0.0019155998327598598 	0.00723598521984881483 	0.00224617383943055086 	0.00390737564299237835 	-0.00578006988257328148 	0.00867510262259102612 	0.000339003946!
 64161291 	-0.00147131455266367572 	0.00968447277858689413 	-0.00518886524184895143 	0.00292670996257684548 	0.00106611595974746365 	-0.000719385057228231388 	-0.000293129918960888171 	0.00760860895269491969 	0.00976280985916930938 	0.00827133792290869925 	0.00212398414576606782 	0.00418765448140857779 	-0.00301043012199560516 	0.00113794991270941854 	0.00541048780035551218 	-0.00455404823160076785 	0.00978293542320275884 	-0.0063461657097431505 	-0.00478388756777868053 	0.00688194330985992912 	0.00365540317520350686 	0.00223262920445967648 	0.00807056464215963505 	0.000884235950526153155 	-0.00294311329037632435 	-0.0066992376743596066 	0.00448601978362710065 	0.00237347827916982749 	-0.000464887781911728977 	-0.00363339022245116796 	-0.00975022483568457193 	0.00714181914487350793 	0.000359541317430670188 	-0.000817962411079717344 	0.00577666902391442069 	0.0042219860128840216 	0.00852479834009656906 	-0.00159398663916113887 	-0.00466436212420145036 	-0.00501019575568218897!
  	0.00914096070961484958 	0.00860070004672525319 	-0.009511887!
 66311747
251 	0.000394918858003463161 	0.000767761914577790505 	-0.00471542452625148961 	-0.00370749134727296628 	-0.00206747709910847259 	-0.0048147558440123769 	-0.0021460419465061777 	-0.00958207505980744265 	0.00155549969426609482 	0.00363064562959795022 	0.00322501919755932044 	0.00134662870716430573 	0.00722390800819952029 	0.00600350553783882135 	0.00881506175958545607 	-0.00522891590321956142 	
+-0.00352105587679368229 	-0.00357354810371110932 	0.00890488617222215395 	0.00335092598517475757 	0.00410129629474930389 	0.00328131721187711195 	-0.00724348529870416386 	-0.00790721803139651618 	-0.00377540548143182357 	-0.00987397526955148021 	-0.000846853248181811424 	-0.0097106451184314057 	-0.00101484208149652715 	0.00439779577157847074 	0.0040341871863944518 	-0.00870366598845686255 	0.00204737765091421355 	-0.00726639929732078582 	0.00525372337084230659 	-0.00392427964261965641 	-0.0024053662587198133 	0.000170991325917960158 	0.00795705864310706709 	0.0057548868705265847 	0.000255604408781480821 	-0.00514063290295806597 	0.00745414562319659472 	0.00807584802275822591 	0.00366958268156301805 	0.00663127608167605499 	-0.00656721375058644946 	-0.0108239158127894797 	6.34527694779678632e-05 	-0.000906459224718117137 	0.00559722672401867169 	0.00734982591691369855 	-0.00859877079397596 	0.0024783492817755195 	0.00305145382606909375 	-0.00100694414771244781 	-0.0023635426!
 470743166 	-0.000543119707448018138 	-0.00380241953577049874 	0.00370967997352433451 	-0.00741465398244088233 	-0.0089191885763690807 	0.00910377752883012939 	-0.000563199178933415732 	0.00593549611870217413 	0.000384692560666607876 	-0.00213171801523757802 	-0.00132158145799427256 	0.0067312346743975222 	0.00259376405440213918 	-0.00522260806019301822 	0.00397489335352790244 	0.0044919261057543361 	0.00895539647173829167 	0.00710003389824242093 	-0.00850049898750221584 	0.00939741521443935093 	-0.00794720677179726505 	-0.00255343141509417918 	-0.0011832382855273371 	0.00486996100081734089 	-0.000304198523377983954 	-0.00482704882737465535 	-0.00465940526140109709 	-0.00305599630463488812 	-0.0059014634132350946 	-0.00215085851611780026 	-0.00734117331981761715 	0.0020957707352232029 	-0.00949115454420371499 	-0.00370571986074030423 	-0.00288355319652020816 	0.000515109721517998482 	-0.00422227694717533433 	0.00753861566472265737 	0.00728340993210358379 	0.00583098839794525!
 883 	-0.000258446958121499469 	-0.00278610266114094575 	-0.003!
 68208205
091915359 	-0.00294943868297097359 	-0.00114658589511941553 	-0.00398703532093420184 	-0.00756696972411996602 	-0.00419248545804548536 	0.00627166440409154379 	-0.0023563781922079155 	0.00324798868809928962 	0.00494263337252684887 	9.60063237721263532e-05 	-0.00370622637761735358 	-0.00809324784107792045 	-0.0103797355880524203 	0.00181847719525322351 	0.00827524248897480372 	-0.00242511316360745962 	
+-0.00437708103139477535 	-0.00548987924211610454 	0.00646463169150594385 	0.000123613519211731237 	0.00230778956386396483 	0.00921965495384904389 	-0.00484395950404199315 	-0.00779730314687136512 	-0.00658615678737665706 	0.00235165075497618123 	-0.000606367754411816996 	0.00726297605939323857 	0.00665125792627973417 	0.00195460742027110971 	0.00565128025330919118 	0.00478173338708695952 	0.00215938338236002945 	-0.00935163994280847378 	0.00942022519651833194 	-0.00280452399388654962 	-0.00134299742148642071 	0.00676613916812528193 	-0.00680304728553140971 	0.00401499798961649484 	-0.0066158024725573748 	0.00556636539263337046 	0.00127041245548731785 	-0.00744752746662424251 	0.00736464953346151543 	-0.00986023989542055175 	-0.00882510648467334756 	0.0105994155981084578 	-0.00691613166819990237 	-0.00169028934898742336 	0.00185468213447432433 	0.00714341555153076221 	0.00212497489664858 	0.00381511953349902757 	-0.00582969124620332305 	0.00859658762424336809 	0.000231972102!
 049148179 	-0.00155040779586361173 	0.00963538700523923486 	-0.00519844330495995016 	0.00286806265491520128 	0.00102509550339589372 	-0.000731412422932949374 	-0.000370685529277404123 	0.00747187701627593636 	0.00970211388378461673 	0.00820946536952153749 	0.00214465576714964077 	0.00410952710820394838 	-0.00310860501769873003 	0.00110350289343739906 	0.005332569396474824 	-0.00459929100709582754 	0.00971496558542602091 	-0.00642497769404262124 	-0.00481832655821851693 	0.00683872182086346127 	0.0036237165237355401 	0.00221826983625268573 	0.00797566398971597847 	0.0008498074926189432 	-0.00302820331198164075 	-0.00679498815215908575 	0.00443734147505092311 	0.00234929012719161875 	-0.000523064881939133653 	-0.00365869893574876119 	-0.00975524124156581032 	0.00702787917859446897 	0.000323945923856628964 	-0.000890226184852111629 	0.00571402266512871602 	0.00413712091097342277 	0.00847838068445512343 	-0.00170791992384406593 	-0.00470763449597788423 	-0.00501592958172093414 !
 	0.00908371175007952752 	0.0084174563952020779 	-0.00955733838!
 26095693
2 	0.000347685069990730201 	0.000671148701010459417 	-0.00478402901640877318 	-0.0037550711131381408 	-0.00220396198535779833 	-0.00484939295497268785 	-0.0021847036731101944 	-0.00961153445140557755 	0.00144569228144339934 	0.00362336056288088942 	0.00322258930267744831 	0.00126655914724157505 	0.0072174239698360931 	0.00591933924189433879 	0.00875398176335348902 	-0.00528574677026058277 	
 ]
 ;
-bias = 2 [ -0.000768511739526251693 0.000768511739526259391 ] ;
+bias = 2 [ -0.000596536845831870197 0.000596536845831888304 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "affine_net" ;

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave	2007-06-29 21:59:11 UTC (rev 7678)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave	2007-06-29 22:19:45 UTC (rev 7679)
@@ -3,7 +3,7 @@
 modules = 8 [ *3 ->RBMModule(
 visible_layer = *4 ->RBMGaussianLayer(
 min_quad_coeff = 0 ;
-quad_coeff = 5 [ 1.0408525953828216 1.03890035931453717 1.01010962868336795 1.02713877432309397 1.00593906555968826 ] ;
+quad_coeff = 5 [ 1.04126452354610177 1.04051544992275513 1.00843700350871535 1.02648808921970036 0.997683771588443125 ] ;
 share_quad_coeff = 0 ;
 size = 5 ;
 learning_rate = 0.00100000000000000002 ;
@@ -12,7 +12,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 5 [ -0.0400498787760139099 -0.0449658259678535205 -0.0564253100833261509 -0.0305132626610919895 -0.0275159111271822028 ] ;
+bias = 5 [ 0.0434941669190519672 0.0364142277196536426 0.034241237123591145 0.0501800016691351519 0.0578342822039583232 ] ;
 input_size = 5 ;
 output_size = 5 ;
 name = "RBMGaussianLayer" ;
@@ -32,7 +32,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 10 [ -0.000679801686262589784 -0.0011838975221978318 0.00754248353955261526 -0.00424115803739747669 -0.00151023456893898387 0.00161621670652929619 -0.00205204570891677658 -0.0014724833853869447 0.00159008176652559985 -0.00464945593498112663 ] ;
+bias = 10 [ -0.00489387387961851255 0.00160433220966122499 0.0111635940286596343 -0.00544183202598008068 -0.00415357938715449522 -0.00122289797479547581 0.000539999407976528311 -0.00139379294340556182 0.000468276194375018523 -0.00130073698323172155 ] ;
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMBinomialLayer" ;
@@ -43,16 +43,16 @@
 ;
 connection = *7 ->RBMMatrixConnection(
 weights = 10  5  [ 
--0.269997045368186084 	0.172285902750972358 	0.260973589274125617 	-0.286217474548966289 	-0.274471886575587298 	
--0.0195157236992702481 	-0.174128722742926456 	-0.0521487023859824664 	0.0781508512007194484 	0.0814327257664252957 	
-0.247335763680624515 	0.278361454468297631 	-0.094202498288889519 	0.221490162166844667 	0.191463361139713989 	
--0.140976955210450866 	-0.269577019891866165 	0.109683465708992581 	0.00169154134986601113 	-0.291726512169596275 	
-0.0488686513835587741 	0.022631508551110447 	0.0828411732209624052 	-0.304793471900027702 	-0.244656347873107055 	
--0.0558133582879227069 	0.12398147682134783 	0.205688949276112282 	-0.0968288719544328352 	-0.276454164328514329 	
-0.0511178615823486907 	0.0541443469138282868 	-0.281168265106303938 	0.0756573910258672433 	-0.0537269160660981959 	
-0.0531084882377516956 	-0.249011225279938075 	0.0630869797850509423 	0.162493248821561631 	-0.276964073046844395 	
-0.0250962869004374839 	0.0904763011697778674 	0.142611569646583586 	-0.322396902434824273 	0.0351703974340924902 	
--0.249991346660943858 	-0.0395017036945069511 	-0.206338275282021283 	-0.175830785082946722 	0.277034923898056784 	
+-0.227792003702524865 	0.213078987007091047 	0.307229903614284316 	-0.245421671196374314 	-0.231661885947266138 	
+0.0227634339277156816 	-0.132765758861420169 	-0.00598562280103339533 	0.119192935630912156 	0.12501964543711705 	
+0.289494620724608964 	0.319651563456747057 	-0.0485688859221443323 	0.262655021633441677 	0.235511746120987631 	
+-0.0987995665763502784 	-0.22852210103816209 	0.156140865592929973 	0.0423180829303292858 	-0.24883553839637651 	
+0.0910683050232134889 	0.063544779331590695 	0.129116509035014204 	-0.264071188535917767 	-0.201761414116804844 	
+-0.0136575598826177112 	0.16486296546771656 	0.251952474852185526 	-0.0560741977509775641 	-0.233431504053665889 	
+0.0933686583109359231 	0.0955230499882306211 	-0.235117347498481205 	0.116660054365897131 	-0.0100077233218127306 	
+0.0952679704615028017 	-0.207883287442698711 	0.109514671318512447 	0.203153840670997871 	-0.233793901172238527 	
+0.067369071125721755 	0.131496759666859669 	0.188701779988374646 	-0.281404822764224849 	0.078351339709537407 	
+-0.207605280356845362 	0.00197533522814370959 	-0.160360718055488488 	-0.134550885771956424 	0.320699614475532413 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -97,7 +97,7 @@
 *8 ->RBMModule(
 visible_layer = *9 ->RBMGaussianLayer(
 min_quad_coeff = 0 ;
-quad_coeff = 10 [ 1.03736835214705025 1.05420459942646927 1.06102760255011708 1.0419841472073712 1.05442774302328957 1.05720797570505698 1.05314606216591389 1.03813695144617957 1.06172896566133912 1.05807767866850999 ] ;
+quad_coeff = 10 [ 1.05316323046459637 1.06757569854843326 1.03994099141493934 1.07443793317521119 1.10081520919294573 1.07882255745024347 1.05605868349609522 1.05405803109891871 1.05915337093685702 1.06366958190106264 ] ;
 share_quad_coeff = 0 ;
 size = 10 ;
 learning_rate = 0.00100000000000000002 ;
@@ -106,7 +106,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 10 [ -0.0310275003615368868 -0.0392131972615835864 -0.0283766144589966823 -0.0367870960989302789 -0.0515140657600649254 -0.0428136467889393343 -0.0269753719316090552 -0.0381739656230388053 -0.026637561803439621 -0.0236700222302586097 ] ;
+bias = 10 [ 0.0200271393396775077 0.0154680798098186349 0.0258585400742424379 0.0140981283564797152 -0.00134668739887318974 0.0148545929121032746 0.0257687050941674337 0.0193589672997187615 0.0263077974042512375 0.0282150856406524382 ] ;
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMGaussianLayer" ;
@@ -123,7 +123,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -0.00353253196714415187 0.00105021011514272369 -0.00123981425195365705 0.000148524069484933544 -0.00182069295190169186 -0.000725092111953462982 0.00127075954286112519 0.00130690271360941503 0.00208726241074961303 -0.000444289267811945852 -0.000734885951036389731 -0.0010466284174252972 -0.0003307441793853783 0.000772987488476326598 0.00101617306644444932 -0.000676301296153658375 -0.000646062399632431404 0.0028779968372038855 0.00113808472886859149 -0.00133906886819176274 0.00320034120593920991 -0.000837753495153736264 0.000122519008195277678 -0.00165717964239016835 0.00404741434099294747 0.0043902806364162451 -0.00254918976809790894 -0.000419852924529751548 0.000207475737633038297 -0.00287067792535434249 -0.00112546272051902273 -0.0009198666626967076 -0.00269291681396742564 0.00128462292022009676 -0.000365287572261410894 -0.00145969407304802367 -0.0031788203227378015 -0.00114608020499133993 0.000449029468350347315 -0.000719793809590452225 -0.00205229263027559544!
  -0.00120675650753899501 0.000613612244685269041 0.00153431839321508832 -0.000365676693302229971 0.000678700038285790408 0.00200561577647605065 -0.0017764852713110053 -0.00329881334357158811 -2.50084185957622889e-05 -0.000361555493158581412 0.00329287154011595126 -0.000750971992712015581 -0.00134442609996333743 0.00136298626541056424 -0.00120274591463599982 1.98312362937778323e-05 6.37674647892915418e-06 -0.00124246613292821229 0.000903722600576549787 0.000880763043512510445 0.00122255628852947621 0.00176636414610012232 -0.00204554583995231621 0.00106449362519541097 -0.00173600372367498459 -0.00133674574506152926 0.000677638094168815804 0.00158417146199775157 0.000288071983736850809 0.00131294473703304714 0.00248626932367626086 -0.00259209983727004805 0.000614532373301187587 -0.0012947486456764436 -0.000711605935694047337 -0.00212758443151684446 -6.14131283728724481e-05 -0.00205049319607211126 0.000912299357397174451 0.0017406235231178231 0.000107118609198131556 -0.00546626!
 837283472038 0.00019796664494023923 0.00120925920299135101 -0.!
 00225438
208353684116 9.55193343922752271e-05 -1.3676766958950453e-05 -0.00338416762965698843 0.0012341180939135798 0.000910766363965857806 0.00107888272015214904 -0.00219135813923306043 0.00258347033499443804 0.0023738988870446549 -0.000193111224550719207 0.00174978845009484221 -0.00161608227941032519 -0.000595770140218897281 2.4678772673427895e-05 ] ;
+bias = 100 [ -0.000920452577715708052 0.000245701349212379579 -0.000513077974386876761 0.000410352819676094147 -0.000742123590533037424 -0.000675782301077305003 0.000379734490765858514 0.00172813952100234289 0.00220798838919422473 3.99980255300325208e-05 -0.000413952498886994145 0.000157356220871785504 0.000945096442174088555 0.000644914269874834623 0.000283696372582155994 0.000423717536541108576 -9.15917405183856227e-05 0.00145722505219732083 0.00111973030386828621 -0.000430089794572077386 0.00180342228352120396 3.76830315064562149e-05 -0.000277803164695551166 -0.000680519488708591696 0.00218594087690755258 0.00230148729430260709 -0.00132872648160434312 -0.000631343935544002107 0.00109563535685577493 -0.000782682985352260111 -0.000283404115289804788 0.000697331069274859214 -0.000741063872868963662 0.00129202952159453914 0.00070394782503980461 -0.000463997372728362399 -0.000313527430361479006 -0.000862388653056470186 0.000649854775102858147 -0.00022810098612181592 -0.000946!
 814695892115449 0.000105766666845392263 0.000269216623610254546 0.0019636946951001235 0.000904082800554930352 0.000647769756047155516 0.00165906233071529195 0.000637019062785513287 -0.00138369558315339829 0.000162091282283951968 0.00066056979294287744 0.00228451936284440344 -0.000513154038857693362 -0.00090647098336691745 0.000947944692690520329 0.00043419375277817284 0.0012706135131809672 -0.000516609340150207927 0.000419785807383476746 0.000982417941495361026 0.000541407992745716622 0.00117241581446960418 0.00162007915223743811 -6.75347534531582026e-05 0.000733180109199808535 0.00017396165751435428 -0.00102286550012955341 0.000444409454669872491 0.00119017907337350256 0.000148655712771739775 0.00148825598296720152 0.00148321181373795886 -0.000666771763512361113 0.0012626007802255773 0.000589837172197661469 0.000520785344685928951 0.000693809052826012428 0.00146489812392977428 -0.00129024726941498989 0.000635727603563731214 0.00191371662548164975 0.000147950302635302335 -0!
 .00241805910905778859 0.000866529300444627399 -1.5075127391659!
 7944e-05
 -0.000221866166384207525 -0.000263900397151751987 0.000814278940789008639 -0.00129677226049522811 0.000833049390240212851 0.000905089293650155821 0.00111404133163897265 -0.00076339797952878867 0.00112138332929521446 0.00175315541221823627 -0.000634608866670686111 0.00198662592302062922 0.000198270783730450828 0.000785476440785266264 0.000407673491056323477 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMBinomialLayer" ;
@@ -134,106 +134,106 @@
 ;
 connection = *11 ->RBMMatrixConnection(
 weights = 100  10  [ 
--0.112968997720965206 	-0.107261622322238917 	0.0690843499648503695 	0.0246368644491140008 	-0.120456372285623212 	-0.066612146170254169 	-0.0906744272833491516 	0.0597997655297617461 	-0.105986898342330743 	-0.0411454473463422413 	
--0.0374976346999811791 	0.0210859367774480881 	0.0709464150658829507 	0.0488643751654367214 	0.0343323691786596788 	0.0279475558666552604 	-0.084333392382157174 	-0.0194080793851280214 	-0.052358429434102037 	-0.0492076924286100892 	
--0.0259011496309593664 	0.026215622730212447 	-0.0746893287828352404 	0.0130851606114712523 	-0.0420653977313660693 	-0.0717655779777460062 	-0.0518072564752484685 	0.0676300855209071017 	-0.0886449651710874836 	-0.0262112170569699862 	
--0.00915200527651004528 	-0.0644439747139869262 	0.0514365614610269722 	0.0768722260848204442 	-0.0157775253241844403 	-0.00465242240018836663 	-0.0653460395456518101 	0.00367989894016004836 	-0.0249611973856444548 	-0.0400986040812955774 	
--0.0270270149563998432 	-0.119229165379597851 	-0.0699416464556981143 	0.0334107016070862295 	-0.0160833734362593947 	-0.0653353282790603423 	-0.0946587899503815444 	0.0566728737912418831 	0.0474862885045988156 	-0.0756514896908607598 	
--0.0851390082595065156 	0.0431691224195017778 	-0.00891028135812327489 	-0.0967791324840578393 	0.0685304240580123653 	0.0198254518260957587 	-0.079392564026046622 	-0.0978732410671857012 	0.0171628860939631173 	-0.0253132286859064533 	
-0.0196836486892928257 	0.0509494353831387789 	-0.0393004807807989104 	0.031201825876827842 	0.00461940004997505801 	0.0255922844035603378 	-0.0653964519191958943 	0.0531915905307929898 	-0.0594682690238555189 	-0.0363797595837481447 	
--0.0757350321031600021 	-0.0433603280357398729 	0.0795755312206672949 	0.0547841007048637668 	0.00168063176117762758 	0.0495689859908219829 	0.00967641649701043993 	-0.102649758349485656 	0.070236676589944716 	0.0570849541620821674 	
--0.0661440773605440069 	0.0637796737211135389 	0.0350691360738870223 	0.0437665316017624983 	-0.065319618977366975 	0.0377506455285399334 	-0.081347412529510027 	0.0594788886687925636 	0.0679125775057212722 	0.0849047037111063801 	
--0.0838220453377574309 	0.0107215192094131263 	0.0776920833088479978 	0.0493952849585796994 	7.60710340389198869e-05 	-0.0408305196187505534 	-0.0392132654665172317 	-0.0586497903701430207 	-0.0376435012027389396 	-0.042859252730564408 	
--0.0417528418174105478 	0.0534147679359561686 	0.00605318137651622134 	-0.110518564704705169 	0.0303083444138963834 	-0.0698363012644192538 	-0.0799617955193665797 	0.0743114422269026825 	-0.109716121251313081 	0.0149344985328543588 	
--0.0882668827874336925 	-0.0708540653680969357 	0.0381793383515727017 	-0.0293551773647720039 	0.0531512103930633154 	-0.0697112506188075631 	0.0408215624827846721 	-0.0951110283124548817 	-0.0171942741452455125 	0.0470348372971189643 	
--0.0657399362828440387 	-0.053272374316605052 	0.0133975324905930577 	0.0300709451951773898 	-0.0544225333546613579 	-0.00606314640518731702 	0.0233024762167977559 	-0.0204132321559630688 	0.0600914703901505448 	-0.0180978739494220692 	
-0.0267230367355865625 	0.0569189085968382705 	-0.073558081828668076 	0.0191147696594518267 	0.0514129153490664043 	-0.0542074397118775114 	0.0833216436482976802 	-0.117239535025526143 	0.00874818477038309152 	-0.00863655799432195767 	
--0.0290881305805484568 	0.0766245179130629478 	-0.112431134543697478 	0.0336367047288538973 	0.0536623234208511157 	-0.116043029514711057 	0.0156681828540595515 	0.0794740797586113573 	0.0457990674313311086 	-0.0773972466366339984 	
-0.0277402397527089888 	-0.0657519208753022943 	-0.00144130694569506376 	-0.029245189849984414 	0.0119994188641120905 	-0.0510471681495050164 	-0.0988489514444823997 	-0.0488611448767203443 	0.0728258021114098208 	0.03729545391264176 	
-0.0471196020128240023 	-0.0720067999664676939 	0.0521688194364444552 	-0.0703627478511613114 	0.0695663387564082053 	-0.04122729377567666 	0.0309010979392343142 	-0.0906980135844153573 	-0.0870352070175022391 	-0.0181563431643288094 	
-0.0477398639452905962 	-0.00427757025208319683 	-0.102829011862847444 	0.00477949329307009521 	0.0500701112223516029 	0.0718234941700118812 	0.0361289226321894752 	0.0584854752628645447 	0.0745560372971755952 	-0.0415089319879584639 	
--0.00388116272985549121 	-0.0687229988042429435 	-0.0429283520106844285 	-0.056744365605824236 	0.0686228270932693063 	0.0290002213028409395 	0.00469927030016458608 	-0.0275339668945225498 	0.0836486630173390033 	0.0570814647420558388 	
--0.041851861447237966 	-0.0760002934799642593 	-0.0696019499262585245 	0.00445659818187340781 	0.0118975581803188908 	0.00423910870913413087 	0.0419631315949083455 	-0.0787829406173317787 	-0.0925713508144801883 	0.033374993268392085 	
-0.0618580299917116869 	0.0540702187176752003 	0.0351577973258900348 	0.0392531284000064221 	0.0509239760845504666 	0.0636569646157703062 	-0.000402066496232148115 	-0.0701891456028211347 	0.0393362692298720312 	-0.0188510509052676259 	
-0.0828736040104000038 	-0.0865704635880224921 	-0.0766730904587804851 	-0.0857540213367499748 	0.0274860757995049783 	0.0194383725230562604 	-0.105823123394605756 	-0.0709866837150482377 	0.0424011834755511421 	0.0659565473289708548 	
-0.0036130832149628817 	0.0773558655899370951 	-0.0692170225708402947 	0.0301559807407275927 	0.0587332778870047895 	-0.0670568462776033047 	-0.0857525797229270609 	-0.0727256236644798493 	-0.0284197034758939433 	0.0209000573711875318 	
--0.0158750191354218341 	-0.102443578561385071 	0.0131753780039702405 	0.0295873077267609433 	-0.0182660365762192869 	-0.0828137537390702533 	-0.106082090703428542 	0.0763935018170668473 	-0.0248981141092162184 	-0.0852388161242966652 	
-0.0485228653750592251 	0.0703820693115500134 	-0.0287026291431580587 	0.00620496227657369327 	0.0611070229875667337 	0.0704873601192797516 	-0.0491202307475595429 	0.0464901762658672044 	0.0693784235424054069 	0.0470252760368678815 	
-0.0839435740596762431 	0.0192933727028773462 	0.0762444334897555281 	0.0689146654434652645 	0.0753830899155011752 	0.0334980135036066357 	0.0513369614380649875 	0.0254788521129208703 	0.0435277375312624129 	-0.0940481067652092773 	
-0.0116544327482557449 	-0.0691339191625459371 	-0.10393914858029378 	-0.117372763017342605 	-0.00785033929587877766 	0.0461465561024753113 	-0.0406486539248725673 	-0.0678173742170913169 	-0.0137995380926319907 	-0.074555060660587355 	
--0.0391532977842081881 	0.00346534487915843093 	-0.0135241067395013466 	0.052012039094271198 	0.029211171046032814 	0.00610286821941561849 	-0.0230423397835572662 	-0.0745839088647388543 	-0.105740298910158625 	-0.0455664055803137041 	
--0.0951966176446819579 	0.0670348203174379043 	-0.0257014117287915241 	-0.0980667481698469651 	-0.0127935797949776057 	0.005305818047409319 	0.000931322233582784677 	-0.0267574794370888103 	0.0833776602179078646 	0.0575917938888648867 	
--0.0223711389333252457 	-0.112617353676874102 	-0.0960978243512426167 	0.0198566225218770841 	-0.0824692529421016812 	-0.00417554968339145949 	-0.0186781599251174477 	-0.0551885624161383911 	0.0123685826512496405 	-0.0552826407103401835 	
--0.0547634357980012684 	-0.0881982851899758835 	0.0388226205520223952 	0.00714647224228950016 	0.0469961939452888389 	-0.0330110650752288987 	0.0548662569532775007 	-0.113713535393649895 	-0.0772877922398471223 	-0.0119519227458886237 	
--0.00954333337495951763 	0.00121514504966621553 	0.029867855120931236 	-0.0120406526844303117 	-0.0976494139377943049 	0.0401897294570796176 	-0.0892792898667015278 	-0.0804433857883190551 	0.0501179561177237526 	0.00761128836897070286 	
--0.0981474801680612546 	-0.112011161657356167 	0.0176002337088038523 	-0.0700647637144275887 	-0.0391876675049213016 	0.0209297780293818438 	-0.000728955837822507433 	-0.0340065394616916317 	-0.0924830588449497282 	-0.000552412545458035031 	
--0.0431068414488170737 	-0.00406823763688253254 	0.0056946038715491136 	-0.0417677533101437487 	0.00927716284748522461 	0.0629743824208994635 	0.00639895672960912997 	0.00764739822664606966 	-0.0300792676582859915 	0.0814167605158566815 	
-0.0360652228161833371 	-0.0690374562389083901 	0.0478249715454292235 	0.0564118965832087205 	-0.0780281613949698472 	0.0420220571571057397 	0.041710896338119037 	-0.0752657598585945858 	-0.0239750093530065669 	-0.0880941915313790819 	
-0.0705359892510492026 	0.0168584696131418819 	-0.0297975899815548888 	-0.0973010730742479291 	-0.00116375961688545645 	-0.101869078845710351 	-0.0521385277408914632 	0.0195184303971443415 	-0.0670736426542261205 	-0.0384189916576083215 	
-0.0783286016833320969 	-0.116532571617073935 	-0.0835827021073091841 	-0.0906192538420639015 	-0.110050588008381592 	-0.0147503831886230067 	-0.0723611309633018368 	-0.0392664734734171211 	0.0720694461597808228 	-0.0282548526483542145 	
--0.0953002880586378137 	-0.0191968642131555046 	-0.109976748101646274 	-0.0398195150308879586 	0.0445682193164055954 	-0.0372455727526815675 	-0.0809227359144047836 	0.0576819641209402947 	-0.0812956853737853563 	0.0701997509637005829 	
-0.0626810867928122156 	0.00654365135353230602 	-0.0132922583641513718 	-0.0934516957687276245 	0.00336836045661683108 	-0.0146243792582376013 	-0.0850793530754924143 	0.0739612083207051157 	-0.00965526051421400529 	0.0107502715390670276 	
--0.102807927545290861 	-0.0657537722201639724 	-0.0447395170524635399 	0.0487808710737742401 	-0.0210542380038344738 	0.0324787756184322443 	0.0209833916655619082 	0.0153919180969652285 	-0.0744224166182238661 	-0.0233391217189672373 	
-0.0278434630978708179 	0.0527038337281267102 	-0.100739170867329988 	-0.10934334865271722 	0.0189960566278678487 	-0.112944160507240682 	-0.102923704596979998 	-0.0255168808107219848 	-0.0280575508767641445 	0.0218049855397044953 	
--0.0590970813266171666 	0.024824247152644207 	0.00156139349797227681 	0.0627526149240287962 	-0.084586370058175181 	-0.00315309081369090013 	0.0586784517428365626 	-0.111090784003396448 	-0.0147070852254384769 	-0.0953363454910192559 	
--0.0460796996528023486 	0.0438823016264262081 	0.0611429288771634452 	-0.0970707209572797131 	0.071376954135173587 	0.0339343777744265038 	0.054508009444280435 	-0.073311469007171462 	-0.107867380993618325 	-0.0120567825688234643 	
-0.0728407269129164037 	-0.0891651421772195057 	0.0427604046206212229 	-0.0512854911392803259 	0.0501019681682121354 	-0.0369765463348414106 	0.0748187592922426731 	-0.00293026320861910182 	-0.00395549757596779829 	0.0879356739720067865 	
-0.0421973783853101039 	-0.0184521167539976293 	-0.00162561722807323694 	0.00210613800444347382 	-0.119664662703793473 	0.0443381377343637223 	-0.0868806643448104959 	0.0379388675909690248 	-0.0365485868966792318 	0.0288104165270520012 	
--0.0905315246382656647 	0.0285882762695103161 	0.0538612017366503215 	-0.00489066176729904909 	0.0481024196368253706 	-0.107291727722060012 	0.0493405069153138587 	0.0528152054492457348 	0.0144944316391132765 	-0.0791517983434828071 	
-0.0657470573647559037 	-0.00756404705580436716 	0.0419331011663033604 	-0.0377218693816129269 	0.00708866359936066812 	0.0587389627953765342 	0.0111423409288077743 	-0.00609246862258327072 	0.0558171307519831975 	-0.0480410779381007583 	
-0.0149975606862876344 	-0.0104597228911564068 	-0.0264335096281875029 	0.0617828237206741321 	-0.106649227531296367 	-0.110315418687146907 	-0.0319799239374929864 	-0.0819523008970640954 	0.051364449699607892 	0.0291810974652220514 	
--0.057929830295404397 	-0.0672853652139217551 	-0.0951946243698622713 	0.0728910297855222961 	-0.104225090075724849 	-0.0368793299557001464 	-0.104844773065315505 	-0.0157109695259885808 	0.00754626690689901101 	-0.0961144689466412461 	
--0.0331191615471033776 	0.0350010823099160331 	0.00669623969624057961 	0.0770455229616985038 	-0.0203635871837350782 	-0.0703635905067119816 	-0.0377656593251949613 	0.00883789638563593945 	0.0051732630203421533 	-0.0948906412871207972 	
--0.00820852271610198782 	0.0030608361330911678 	-0.0809251093231248275 	0.000684867385085873156 	-0.0847046551663089287 	-0.0108805835901810751 	0.0386528579381290247 	0.0461222221240333019 	0.0798751626114453939 	-0.10577853201824404 	
-0.0659830224357993572 	0.0472835137427353447 	0.0694036973977961852 	-0.00138011200127536671 	0.0312080234807149935 	0.0164652708121146608 	0.0765334203251932244 	0.0204527091470514816 	-0.0158138395301828071 	-0.0167830964972385463 	
--0.110945552823942695 	-0.0173933720479128931 	0.0509530800681333926 	0.0759344511279405332 	0.070469533215113872 	-0.120842637821257065 	-0.067055731450413561 	-0.056758193586146713 	-0.0231823146373266384 	-0.0189824187544384952 	
--0.0244469373936185702 	-0.0500102054966653359 	-0.102521226008294483 	-0.0481296035217862045 	0.0466001275431255496 	-0.0789845511538133094 	-0.0501562718227550539 	0.0556009080154579866 	-0.00676372233178482163 	-0.0463238927245332235 	
-0.0567610778128717214 	-0.0764646873274305861 	0.062711022933166341 	-0.0270848025612393413 	0.0290944663359772558 	0.0245860652598726484 	-0.102280486511207552 	0.0797966556596675092 	0.0417607933127779468 	-0.0549845054585790388 	
--0.0891063574603898206 	-0.0364545727694119137 	0.0835332251905906015 	-0.0790819643114479554 	-0.104676062557898519 	0.0742572023960191147 	0.0596794217970020233 	0.0308445805765217469 	-0.110264996302738144 	-0.0504269149838760811 	
-0.0395069985661478523 	-0.0344723238477800814 	-0.0578798741947319992 	-0.0337422605822460531 	-0.0674513329203961093 	0.0200048026698817355 	-0.00821614032522092856 	0.0106654981375493355 	0.0123016411242311777 	0.0795155547089083925 	
--0.0617715433542510781 	0.065479166618212209 	0.00195774921141070963 	0.0770071127700327068 	0.0427958489424534252 	-0.0693411692059071327 	-0.0359698293045637996 	-0.0334113662704650996 	-0.0901659135790251892 	-0.064342042174802952 	
-0.00593590368594166043 	-0.0774346663206200575 	0.0347161136512349963 	-0.00903712342001561537 	-0.069519069727859345 	-0.0476111327623072178 	-0.0613173438225481371 	0.0261492933503423598 	0.0572318334716369614 	-0.0626478836489953866 	
-0.0347243530383698484 	0.0187746969106717815 	0.0683985583550629633 	0.038111018198438408 	-0.00413639840821313637 	-0.0503724557193259495 	0.0265259717962897414 	-0.022856711483214151 	-0.0109258244846265739 	-0.0848539440388113853 	
-0.0600002393133610007 	0.0111924002688265616 	-0.0924575875815014103 	-0.0159846415015863046 	0.0463204861705724252 	-0.0723339870535546908 	-0.0858391717508157487 	0.0639409323032253119 	0.0640047136376664383 	0.00157904530216265129 	
-0.0459364576217070955 	-0.0428783596338899037 	0.0492541372159891605 	-0.0414541902431913925 	-0.0115750387046378196 	0.0463370440455991434 	-0.0375833819480468692 	0.0713106287410141815 	-0.0193994089327903275 	-0.0237151803757713747 	
--0.0753453310124642534 	0.0470511656806676948 	0.0324626170902225439 	0.0614037178137466777 	-0.061281590712654356 	0.0623676898892027848 	0.0750512255759631791 	0.0190380376989027073 	0.0466127631455927963 	-0.0983929534245903681 	
--0.0753203082417579078 	-0.0224176903021309284 	0.00471628052934990483 	-0.0855904417885777757 	-0.0421638077286862617 	-0.0658455221012471947 	-0.0806309726221091649 	0.00860571380238822972 	0.0152878589221822903 	0.039018792176756148 	
--0.0565291369224710169 	0.0158585135887158411 	-0.0442658643538430124 	0.0592908324210727269 	0.0112558633893031952 	0.0287660160009656427 	0.0078460232423851993 	-0.00674314133982322362 	-0.0980240877364823154 	0.0860735417597574964 	
-0.0181616787805686901 	0.0780931131490483488 	0.0384256775627969394 	-0.0590138645669058687 	-0.104693937007087998 	-0.0714499650120191954 	-0.0468880823266030894 	-0.0190346249456964933 	-0.0994299324576733978 	0.00318301500888165971 	
--0.0702791188779655213 	0.0232283973941731017 	-0.0394057787524467532 	0.0583369595991816842 	0.0285719661683397734 	-0.0660293824518379674 	0.0141841461406632032 	-0.0997941017916422479 	-0.0755195647435984857 	-0.0780061915865791211 	
--0.0798418475238190095 	0.043414742583256144 	0.0117247535240395453 	0.0112276466632689753 	-0.00787629396390318592 	0.0301382914524404681 	-0.0890409852897879189 	0.0248154954079231535 	0.0382007610670286948 	-0.0444501845513029514 	
--0.00505573199014942593 	0.0241526588268680918 	-0.0990856819916874076 	0.0470290891320045007 	-0.0391118062172869374 	0.0687789002874952121 	0.0247822567088178645 	0.0554081887662699335 	-0.0331573704821253243 	0.0253307001033349598 	
-0.0201517257681016887 	0.0424133507908756036 	0.0517612350126334941 	-0.105216583665790911 	0.0477340875905697473 	-0.0490337199516985983 	-0.0469924448469433292 	0.0359340144289553159 	-0.0349709810159789941 	-0.0697181465531829248 	
--0.00373024315202581669 	0.0301549767787843703 	0.0653145428872653061 	0.0517543034267617316 	-0.0846288875011196595 	0.0256938491157166285 	-0.092406857734933151 	0.0761029237061653929 	0.0547017652131371532 	-0.0603374975548693834 	
-0.0307161094129311633 	-0.0153807572336820651 	-0.0297366666359162868 	0.00127567340092943392 	0.0556035621987012244 	0.033339025409315888 	0.0611728416771542965 	0.0491116387705762061 	-0.0565046412502441323 	0.0409294409949754537 	
-0.0812533962147731076 	-0.0941516774508668192 	-0.102366243823632366 	-0.0860446544742183572 	-0.0411605110879607397 	-0.0230437949780598755 	0.0262613656524550813 	-0.0520510814075603942 	-0.079723152873806713 	-0.0086349250381016155 	
-0.0705616517718082592 	-0.0203791360987840564 	0.0107018455941401899 	-0.110833871528211766 	0.00995908866670183746 	0.0151457424113217391 	0.0337119811051480034 	-0.0279546597955270745 	-0.0356902891797534066 	0.0635603586482349076 	
--0.0796486245604698939 	-0.00983081619719249587 	-0.0189773683249421996 	0.0193339237986886563 	-0.084538490201946731 	0.00642026009175085313 	0.0551327086774688196 	-0.10696646639063434 	0.00971244413446942928 	0.0198357787774446834 	
-0.0448307618176287531 	0.066344698304299371 	-0.0253722974031335528 	-0.112107658203131191 	-0.00556958261647357965 	-0.10994797134525211 	0.0359616760039740482 	-0.0190416857441550839 	-0.0594872400740054444 	0.0394624028793299081 	
-0.0690658534122153278 	-0.0999062150088833173 	0.0324104406735968992 	-0.0978605721595309924 	-0.0754630170597721806 	-0.0426534453468719529 	-0.00782411099958435323 	-0.0774677768754250956 	-0.0253543833180141509 	0.0845468666162700611 	
-0.0398423687402394214 	-0.0573769925457700003 	-0.00784732402103928764 	-0.0245265652164750429 	-0.11027706910147847 	0.0112654570866892369 	0.0401507524320682194 	0.0607132127388824161 	0.0666522189730806014 	-0.0588533154384459981 	
--0.0383086617403472079 	-0.114883973613259624 	-0.0489867387371351048 	0.00385798122961478586 	0.0419306854226488002 	-0.0214770291324544171 	-0.0164813856498231617 	-0.0634619556600920787 	-0.0970966633803915929 	-0.0276198680889830142 	
-0.0474525399797028971 	0.00663627772342876513 	-0.0472919877615821937 	0.077938206459603393 	-0.0462471586967727541 	0.0579487031144106657 	0.0219817345940784593 	-0.0193762669554546629 	-0.0183893010175783314 	-0.099893734159963804 	
--0.02658541286723572 	0.04560736548220342 	0.081509923857694136 	-0.0842613175258934144 	0.0285023643408886737 	-0.0423973102204057914 	0.0699295602520995929 	0.0373395933756589735 	-0.0324780631757300153 	0.0632516912133445075 	
--0.0528291128181526126 	0.0745724130999155382 	-0.109252341300047864 	-0.0412118413136273939 	0.0390441113660137812 	-0.104227722565378184 	0.0675162591384109723 	0.0445242602306092708 	0.0304571690960141403 	-0.0620456448249670015 	
--0.0976664214191611396 	-0.111064089641682615 	-0.0283842659320335533 	-0.107549656510199113 	-0.051039564985508816 	-0.0922281337448709326 	-0.0425284781241276055 	-0.0504513021219049548 	-0.0731898172230704691 	-0.0955232430862920229 	
--0.00636443008799744084 	-0.0631390788061653579 	-0.0324780761242052796 	0.0150811640480009064 	0.0163275605569621722 	-0.025847800105606264 	0.0510397886916933441 	-0.0396596568891199805 	-0.0232522754136261223 	0.0651385955343220663 	
--0.0115372693728243627 	0.0777036948587940257 	-0.0514738542471039895 	-0.00884068246085071771 	0.0517044975901950615 	0.0671036882691528058 	-0.0990420740708328889 	-0.0291615760162107808 	-0.0306045720842398027 	-0.013700357832346674 	
-0.038979011528923968 	-0.00743021666900376028 	-0.0967692004794863947 	0.0415170201455853372 	-0.0768028499520836266 	-0.0929296400082273016 	-0.0335372471530388377 	-0.0854159976525200731 	-0.0694836172339819563 	0.072084458476765742 	
--0.102940982369903178 	0.00474983390680198264 	0.0389516699387414955 	-0.0536567399902947231 	0.067437544584287662 	0.0182991335344237956 	-0.0525736786723241603 	-0.0245199283407692148 	0.0532927456789939052 	-0.105185987293311953 	
-0.0289570986015648367 	-0.0179862426219850255 	-0.0347952882094319585 	0.0467508681374518495 	-0.00445332369677817804 	-0.0657205313783880002 	0.0667002393041097041 	-0.0769456469784192826 	-0.0429461701962694439 	0.0431691788006636074 	
--0.0861836073997126056 	-0.0666895583740770709 	-0.069126730597191044 	-0.0779614929705302467 	-0.074722838976120462 	-0.00738192541775280296 	-0.00724688264280800958 	0.02348157920484506 	-0.100370399363810717 	-0.0434823895111814615 	
-0.0648992315743629616 	0.00518799840129952061 	0.0296072399423903572 	-0.043553395974616764 	0.0398729784709352109 	0.06952026026583559 	0.0259812016394117447 	-0.113273167752301951 	0.00398951619577828098 	-0.0557691777463301952 	
--0.0306715000983116375 	-0.0530179653347295568 	-0.0369861843810302265 	-0.0145518908103485136 	0.0278581306317795263 	0.0774478922361130084 	-0.00300894119328308594 	-0.0552116426581901187 	0.080387320058459652 	0.0115086156060474707 	
-0.00517056593626328588 	0.0412391380788425313 	0.0237618633986983428 	0.0266537386581753889 	-0.0300720997391868868 	-0.0342424916070309532 	-0.0257235135761303482 	0.0727656596101706327 	-0.070357230078374966 	0.0178199394458505404 	
--0.00653963648713632752 	-0.0251344123448872317 	-0.0897628875633802309 	-0.0271786656230643235 	-0.0622650232627898134 	0.0366814291386488717 	-0.0347457996638991864 	-0.0822490548688505074 	-0.0611579093048781469 	-0.0121232285512562783 	
--0.0581145581944492035 	0.0542045052725088281 	0.0326961931862242913 	0.0600261361289733952 	0.0468347026540592848 	0.0524454251514683903 	0.0735490572539929804 	0.00397861840757738131 	-0.065682986863011189 	-0.056092442123518875 	
--0.0163716809113475179 	0.0504557180777146994 	-0.0761281479907330028 	-0.0816112570970967399 	0.0316886669875248508 	0.0530848721544111626 	0.0642335805352825251 	0.0664892469824084342 	-0.000983864507556370723 	0.0810985051009005115 	
--0.0174021299862850073 	0.027118986517123151 	0.0330927183795870772 	-0.116263538179238096 	0.0496603244956592554 	0.0307525159980376328 	-0.0919703339240501849 	0.0273858413326795481 	-0.0572168787185550962 	-0.0985817004740841968 	
--0.0200053252010550416 	-0.00453046370932334765 	0.0586412385403922501 	0.0230202850563303689 	-0.013850504556597568 	0.0701250583602681804 	0.0414017325088206073 	-0.088851278764485439 	0.0204634849374224943 	0.0657438278386667241 	
--0.00984134846166172077 	-0.0103704578992097323 	0.0228153465299076874 	-0.0242430333836236821 	-0.0620582684933687137 	-0.0477976932216939016 	-0.0249725372654928841 	-0.072226958950752207 	-0.000770137235563852986 	-0.0135805723907640383 	
--0.0915735476092910533 	0.0491403698085967483 	0.0253514135603257439 	-0.00579511106651758173 	-0.103015736809559735 	0.0329857031764390421 	0.083881273902148043 	-0.0567909593545623537 	0.0162733338452598145 	-0.0855496384592240172 	
--0.027829894704633814 	-0.0114219698142563504 	0.0385334939859549239 	-0.0717620801117500873 	0.0546021330176594544 	-0.0547381223252964913 	0.012425473571005069 	-0.0297097494525451027 	0.0434377308336664283 	-0.0529853310004288827 	
+-0.0861417325165266257 	-0.0786638063827643924 	0.0966714479698655149 	0.0516780664389202213 	-0.0935759860853147829 	-0.0364467826231854516 	-0.0632230535763845664 	0.0899660456888490834 	-0.0785036383028760859 	-0.014121450415743135 	
+-0.0118011407174486611 	0.0485753097209529819 	0.0984039313044046265 	0.0742977643447037817 	0.0592505794545960945 	0.0568950489975254547 	-0.0577722069161301852 	0.00955998174413244008 	-0.0256568308342410671 	-0.0230714865657609472 	
+6.86409515634804434e-05 	0.0539688960990592076 	-0.0474098608737475308 	0.0390513213728072989 	-0.0163310080854958024 	-0.0424903746844489533 	-0.0250371765781936574 	0.0969578142480475441 	-0.0618559765714472437 	1.0038411417479738e-05 	
+0.016710236954564639 	-0.0368186488099817161 	0.0789652499668531543 	0.102548747765204756 	0.00938864343331914479 	0.0244532752127019255 	-0.0386115567242019397 	0.0328043149676868623 	0.00190350816000103471 	-0.0137535446178924624 	
+-0.000954765156340244567 	-0.0913976646034908025 	-0.0427049009465165427 	0.0595171970706733153 	0.00985895543397233981 	-0.0359627788803006804 	-0.067808688020551261 	0.0860914763124255022 	0.0743649106463285248 	-0.0492203084620364828 	
+-0.0590837552483936615 	0.0709852711401236652 	0.0184607515186512583 	-0.0707698079308057976 	0.0942957407422609778 	0.0491742987756700334 	-0.0525511842831454801 	-0.0684996368201336442 	0.0440761316090169386 	0.00102870713363901025 	
+0.0451452796278654783 	0.078206554857198296 	-0.0119609284414650737 	0.0563893684865491851 	0.0293357018721735058 	0.0542868400177525245 	-0.0390224449260943296 	0.0819483695956100405 	-0.0330015865550528018 	-0.0105219668517385918 	
+-0.0499894242243375184 	-0.015894074857273361 	0.107471610197175299 	0.0802425806743711639 	0.0263286127686550957 	0.0784702958989455246 	0.0364745409049370209 	-0.0737217182490697287 	0.0972820759963946152 	0.0835359815717947873 	
+-0.0406043806064697171 	0.0911117414574526413 	0.0630438906076174199 	0.0689966241156475674 	-0.0409675703179725043 	0.0664546808627409225 	-0.0546834086539292955 	0.0882598205419448389 	0.0948128898564022105 	0.111091202043442056 	
+-0.0577282240834442947 	0.0385927859031357928 	0.105268405747193386 	0.0753678978141263112 	0.0255934525326895236 	-0.011463510068290373 	-0.0122885429212349091 	-0.0292666128593720987 	-0.0105928704586070194 	-0.0163378778543190988 	
+-0.0157063878647499891 	0.081256864396226744 	0.0334458860507079458 	-0.084509224140674008 	0.0560778025405891181 	-0.0404645054342887833 	-0.0530869295061847449 	0.103741105865636632 	-0.0828094369476290448 	0.0411698534625893897 	
+-0.0621140454422243193 	-0.0429971206212671972 	0.0657430869007141067 	-0.00325691458029098158 	0.0788406018437792649 	-0.0403026041408256416 	0.0678723377877541029 	-0.0656720797189949967 	0.00997771535482402938 	0.073642271430935477 	
+-0.0396962958142890107 	-0.0254810603405014845 	0.0411679579951165636 	0.0559713605412997736 	-0.0290663479480464323 	0.0231695939106714086 	0.0502866704491430963 	0.00889143825357812528 	0.0872213430530378725 	0.0084964686974578485 	
+0.0522217454229906702 	0.0841518958659751987 	-0.0461661076913289642 	0.0443513476295392325 	0.0761312908644860842 	-0.02549649345602506 	0.10982836038155859 	-0.0884739628090181335 	0.0353683117101483632 	0.0173923230628976883 	
+-0.00374796660707523354 	0.103780497970321003 	-0.0852256331389483202 	0.0587023816493198666 	0.0783518567127009236 	-0.0874692209144963262 	0.0420216669865208156 	0.108197354322956782 	0.0722225678151461464 	-0.0515708969259319749 	
+0.0537420830108407341 	-0.0380244804292810823 	0.026164430920372829 	-0.00329877393710177689 	0.0374834397951652673 	-0.0217635549371616617 	-0.0719117383255620107 	-0.0195808461344031369 	0.0998755627764081472 	0.0637633694993499228 	
+0.0731786080715679521 	-0.0442273706555351767 	0.079611369006817867 	-0.0444354898402060175 	0.0951557701522115357 	-0.0118920966779942437 	0.0578001151286287249 	-0.0613560732694219957 	-0.0600623613275997059 	0.00825944018930453196 	
+0.0727545087991228051 	0.022501010892436852 	-0.0754437600551990523 	0.0293142845246605541 	0.0740018326844740504 	0.0999558764868055533 	0.0622457379065934555 	0.0867616411224109707 	0.100815237584435891 	-0.015878874761582961 	
+0.0216471235775783574 	-0.0414865047469387335 	-0.0153837885406207867 	-0.0314777842825800583 	0.0933348231789881688 	0.0577040457278816324 	0.0312849998979567306 	0.00124736923410225268 	0.110387969295368094 	0.0831866957618924846 	
+-0.0158676964480599543 	-0.0483277394966532062 	-0.0423398434183346467 	0.0304304408489798027 	0.0375953333665299261 	0.0334747761769227381 	0.0687454537092801604 	-0.0495298047499326682 	-0.0657249394513980678 	0.059697367150593178 	
+0.0870486817088142739 	0.0810302056306821533 	0.062823121054105327 	0.0639294557777338041 	0.0747826981844236238 	0.0919973549081927466 	0.0258849172438556235 	-0.0418067047009972326 	0.0658379882835492952 	0.00700521966772083005 	
+0.108773222389732596 	-0.0589819425792947188 	-0.0492586517648492123 	-0.0598701836531344428 	0.0530194755731719092 	0.0486197628755582686 	-0.0790363649123128481 	-0.0418281084210015974 	0.0692595970449976922 	0.0922223344679270612 	
+0.0291780755943490296 	0.104675562718049522 	-0.0420272337621786299 	0.0556023915279756295 	0.0837845408729784163 	-0.0381816439907214089 	-0.0592873049958303963 	-0.0438537476898653023 	-0.00187167168714608796 	0.0468441481761720097 	
+0.010309211388964826 	-0.0744753547431004498 	0.0405072525339581074 	0.0557743386919779677 	0.00771808818288482329 	-0.0533071950011594631 	-0.079142590228909987 	0.105930392230867074 	0.00207570760029600516 	-0.0587360642017585241 	
+0.0733965862472779201 	0.0970373361984766258 	-0.00109849263293838352 	0.0305287505369259728 	0.0845862567563747808 	0.0984948638090170775 	-0.0230498018508680375 	0.0745952559226813611 	0.0956719710427825221 	0.072576700989362572 	
+0.108912322688067401 	0.0460630728303972198 	0.103924077225970704 	0.0931835847016890401 	0.0987757598439102102 	0.0615682600340273778 	0.0774781476388523016 	0.0536684385955828805 	0.0698955075194553771 	-0.0683256484083163279 	
+0.037970379286684057 	-0.0410890660465148549 	-0.0767246133195958191 	-0.090941035139824003 	0.0185846877983473352 	0.0757633836419891599 	-0.0136823190686553454 	-0.0381813239556530112 	0.013141196531555337 	-0.0480829558159745277 	
+-0.0133240826496765757 	0.0310445835666900878 	0.013671194979685131 	0.0777258034048227825 	0.0546127244544156731 	0.0352094466291683963 	0.00355070753493351085 	-0.0454728981109072186 	-0.0790728300959578079 	-0.0194063201819028759 	
+-0.0692634210448788423 	0.0947305025740419332 	0.00211369978218783937 	-0.0722712119269828951 	0.0124626768245360876 	0.0344385224095069831 	0.0278795414320396964 	0.00247073153068135161 	0.110467512958603797 	0.0839991815740364495 	
+0.00398281352443292502 	-0.0845493716388155125 	-0.0687451330406574079 	0.0463477871671479982 	-0.0561305982601386921 	0.0254556612634104333 	0.00837847739756166464 	-0.0255468361799351638 	0.0394466075054662538 	-0.0286202228478918685 	
+-0.0286392430116645287 	-0.0603679364214899927 	0.0662258580202890329 	0.03319494200456162 	0.0727065123806992231 	-0.0036286893406804392 	0.0817950952518442659 	-0.0843198201388197788 	-0.0502615632725388825 	0.0145770024197204619 	
+0.0167402906901718031 	0.0292495317914311763 	0.0577628136799800893 	0.014210206634362019 	-0.0719394045562366247 	0.0697212331777804784 	-0.0621354616761680689 	-0.0509255733917263476 	0.0773937768913134788 	0.0343228431426546923 	
+-0.0715707139718044227 	-0.0837175467046543387 	0.0450978515797024401 	-0.0433621067706970384 	-0.0126450137818251915 	0.0507834202411756586 	0.0265242726550822851 	-0.00413175557298824016 	-0.0651858469772916144 	0.026243031821955129 	
+-0.017471650561774299 	0.0233032621970668506 	0.0333745827459559358 	-0.0163940019396641712 	0.034025271075211902 	0.0917831558468536618 	0.0330526481935554742 	0.0365281333098671501 	-0.00326566027688106374 	0.107551399248148188 	
+0.0621623926449725098 	-0.0411948172956044523 	0.075549634703823712 	0.0823213108380658337 	-0.0526455340069498789 	0.0713145677979969278 	0.0686465958903964973 	-0.0459533969321020141 	0.00308279456446657009 	-0.0615049390081123873 	
+0.0966774367645016619 	0.0447787833120154161 	-0.00240170793913392199 	-0.0711624629889161531 	0.0247741104500982384 	-0.0723894107585482532 	-0.02518444035136682 	0.0490272970660198745 	-0.0401208012571635034 	-0.0120644347924088544 	
+0.104896523558917057 	-0.088247860080853946 	-0.0559507943313718378 	-0.0638668641776518747 	-0.0834804606626358137 	0.0151166208062483327 	-0.0450394086847113462 	-0.00940439760158509955 	0.0993974174228176854 	-0.00141123609896087526 	
+-0.0694705351948282923 	0.00836844618983835845 	-0.082892074690401607 	-0.0139454301750882581 	0.0702778345482266975 	-0.00810862368765842556 	-0.0542774235312317652 	0.0868698888360365656 	-0.0546222244360724252 	0.0962580555048138803 	
+0.088450340284502077 	0.0340920983084579479 	0.0142716655817638326 	-0.0678617045144634945 	0.0285207807267017638 	0.0144018957684261314 	-0.0583579374292024014 	0.103049707452223777 	0.0171435455939946586 	0.0368784764809119009 	
+-0.0768768524549947041 	-0.0380837480406824441 	-0.0174156691185993334 	0.0746250803938452029 	0.00447983531724382555 	0.0616268975946653377 	0.0477100770065501806 	0.0446087823828205249 	-0.0476142831444484466 	0.00296633456802302678 	
+0.0539661493513540272 	0.080582051994979767 	-0.0734843818411942407 	-0.0831010940349215521 	0.0451120231654724702 	-0.0834462882886870122 	-0.0760003488700317364 	0.00398497120614737209 	-0.00114786528677741201 	0.0480962968356828252 	
+-0.0328699639787280729 	0.0528157274592290921 	0.0291982272370953505 	0.0889000913905268336 	-0.058853216410345334 	0.0263002932630747692 	0.0856943415853353013 	-0.0816029069654683609 	0.0124065706959511739 	-0.0686830257735963345 	
+-0.0201810103197048001 	0.0715424384109942246 	0.0886837224332765739 	-0.0714180202611850168 	0.0966127183945824503 	0.0630595050065478752 	0.0812818543367808261 	-0.0441297748579887392 	-0.0809887969486290799 	0.0141899334600852651 	
+0.0984104041724991191 	-0.0618961766005818592 	0.0705361246551845927 	-0.0260645621322587867 	0.0745990092946819811 	-0.00824612130850121798 	0.10153297434354909 	0.0258748887472621154 	0.022930298184031056 	0.11413739464306194 	
+0.0682468308991390216 	0.00935485276277766946 	0.02616049981290108 	0.0280691948412451278 	-0.0942483238529438355 	0.0736216775414032243 	-0.059944088366491767 	0.0672354339298348591 	-0.00950623923805744842 	0.055243903706101799 	
+-0.0647203348909599213 	0.0562174520542019635 	0.0814442678077735988 	0.0206419449310587325 	0.0731732493967917758 	-0.0782596107034229105 	0.0761348876621017329 	0.081988665459976337 	0.0414087385509907119 	-0.0528294048973853153 	
+0.0913119163181111415 	0.0197729192916254425 	0.0697170163802186549 	-0.0125694301862307543 	0.0315630033509623209 	0.087456153811527465 	0.0377483602147873007 	0.0227062430791546699 	0.0825911761127729044 	-0.0218927707463625366 	
+0.0412407958738151836 	0.0175136901900071022 	0.00133792076556665893 	0.0880686142672262029 	-0.0808887210533200118 	-0.0807901445512391458 	-0.00481692937095364732 	-0.0524418046259498005 	0.0786356200848076642 	0.0559111915627236999 	
+-0.0315027394460976468 	-0.0391007973715141177 	-0.0679413975484128158 	0.0995195871613830524 	-0.0776688188655555861 	-0.00712006588752233608 	-0.0777911813284563047 	0.0140438328799954142 	0.0345978502243173766 	-0.0694457428834264895 	
+-0.00726763680335373082 	0.0626607101086971985 	0.0341710974120858074 	0.102718487628999322 	0.00487404164813985165 	-0.0412502466380579996 	-0.0110361818774208642 	0.038010030764682165 	0.0320005157425173187 	-0.0685833582522396845 	
+0.0177226364538976763 	0.0307849915215467167 	-0.0532981292231850534 	0.0264555447889565989 	-0.0593282429474002518 	0.0182389077520079025 	0.0654998964232903214 	0.0753679752185550528 	0.10679371523845245 	-0.0793821108424969529 	
+0.0912604634202468651 	0.0743478794195307768 	0.097220239319326357 	0.0233354719393765765 	0.055085846104544707 	0.0448648089526751045 	0.102967402263711275 	0.0489633798340027501 	0.0108101971229397128 	0.00914972251735276902 	
+-0.0849809687268331809 	0.0103323293428373396 	0.0782606426675547667 	0.101827352843738464 	0.0959969554412966886 	-0.0915578694860124942 	-0.0402509761414983555 	-0.0274663769567490591 	0.00373489115083617965 	0.00741892197366237748 	
+0.00145183272208730695 	-0.0223501542720259266 	-0.07542270502961472 	-0.0222342025127398903 	0.0723968845762090474 	-0.0497782199743885684 	-0.0234495722596958986 	0.0848813022167153458 	0.019943564265201677 	-0.0201503621131671265 	
+0.0824305825874949993 	-0.0490107101764307648 	0.0903297359352691759 	-0.00173321969858411312 	0.0539108063986477315 	0.0534741071723026512 	-0.0756492235374460475 	0.108750788168692672 	0.0685291777136947389 	-0.0288132941942501643 	
+-0.0625880573972465298 	-0.00816163087686855304 	0.111405356815965445 	-0.0526378793987037283 	-0.0786012704988595379 	0.103969529045432982 	0.0869553935788356241 	0.0606432982002185136 	-0.0829028454556515609 	-0.0236224752570268794 	
+0.06533351234519226 	-0.00692765580719425806 	-0.0301493079389002321 	-0.00804374270597679915 	-0.0423309488853819069 	0.0490271871647760329 	0.0186128378780059806 	0.039737222381360876 	0.0392496115688754782 	0.105815769667155196 	
+-0.0360309654593565301 	0.0930131687470760471 	0.0291689127854928357 	0.102586807805481642 	0.0680255079999032702 	-0.0403031830725272822 	-0.00941038612265636398 	-0.00434338082798208479 	-0.0635262511249817946 	-0.0382379076388928221 	
+0.0322038567854431718 	-0.0493921379132445937 	0.0624464956070978558 	0.0171818272514066249 	-0.0437045328382055578 	-0.0180821456846998081 	-0.0341899619374273142 	0.0557210303917616279 	0.0844468450044989771 	-0.0359460161067151132 	
+0.0605110867530842811 	0.046354834151279424 	0.0960814898793639349 	0.0635832203032224985 	0.0207423732778661205 	-0.021370534598248718 	0.0532747561660496735 	0.00620639203468931373 	0.0159574703625993547 	-0.0585339464681958119 	
+0.0854098863273274195 	0.0383749256477820336 	-0.0651336122856546879 	0.00920700994449101529 	0.0710695971931226828 	-0.0436666128536733547 	-0.0594047118799946663 	0.0926846146363629747 	0.0905231559434967192 	0.027458892943277375 	
+0.0716787706281246384 	-0.0153600100611413631 	0.0769642814573454903 	-0.0160163313956490812 	0.0132956568408133472 	0.0752725399462989275 	-0.0108770644375806146 	0.100322111686597923 	0.00743522065119585698 	0.0024934191500020934 	
+-0.0496875782208254027 	0.0745264291523592415 	0.0602964221677511555 	0.0866675334889287635 	-0.0367131105137037464 	0.0911437873522130959 	0.101717666780812219 	0.0479493466383217673 	0.0734592813113824628 	-0.072113114808994086 	
+-0.048889762507005316 	0.00576665220695898612 	0.0323976248819665621 	-0.0590574377889105939 	-0.0159328548762790977 	-0.0361092095674989816 	-0.0533663657161536908 	0.0383742036302115799 	0.0426182679150549359 	0.0657470150822094812 	
+-0.0310641866261481547 	0.0430511510380869322 	-0.0168711037294389903 	0.0845310130118581138 	0.0359149574417389206 	0.0574322911566908026 	0.0342836772440739332 	0.0219694738084665261 	-0.0714463693933729271 	0.112012574006262786 	
+0.0446070865595669663 	0.106324549132881682 	0.0662143505861573084 	-0.032528146893984991 	-0.0785988569875457482 	-0.0416858433603130227 	-0.0196350214180688633 	0.0107285756133524077 	-0.0721250591782636868 	0.0298617383887609909 	
+-0.0442817643996714777 	0.0509814916614788755 	-0.0122512129523514625 	0.0842959321956655572 	0.0543198932839219217 	-0.0367292623995177717 	0.040919558965889731 	-0.0704755587188997251 	-0.0487365131569632501 	-0.0516895887108212715 	
+-0.0540600238664465244 	0.0710029311522224205 	0.0392951512761265387 	0.0368060118481603124 	0.0172171366715894239 	0.0591507848146754708 	-0.0623579837344967886 	0.0538977289035742144 	0.0650159873279133327 	-0.0182209574434030047 	
+0.0202877169313203445 	0.0512503510157464939 	-0.0716152055631475715 	0.0720692571330929649 	-0.0146697715604106223 	0.0972726511001290178 	0.0511389445166192877 	0.0840021391470703732 	-0.00667526833338390104 	0.0511809241151031796 	
+0.0460841657647156067 	0.0701593466453695885 	0.0792930989248262774 	-0.0794914376738072165 	0.0731105960930893833 	-0.0198214222819848677 	-0.0201756113906326577 	0.0652202272548568635 	-0.00808872697198904228 	-0.0434649107228186962 	
+0.0220469976060823913 	0.057760123006089098 	0.0931977532934198521 	0.0772346664469984595 	-0.0598475777233471035 	0.0546670321583845389 	-0.0656482723600904788 	0.105148703985770878 	0.0816324277971470502 	-0.0340082982135949188 	
+0.0559073669022559297 	0.0115499190459667292 	-0.00227637075738397953 	0.0260368330527698498 	0.0797145175003680928 	0.0616751385907746835 	0.0874564372453314753 	0.0775559584151787468 	-0.0300766064678407476 	0.0666863867569315594 	
+0.107547133094365602 	-0.0661597175618631322 	-0.0750312873175343203 	-0.0596465291139990014 	-0.0148864131111699789 	0.00654354685106274189 	0.0532968302730689875 	-0.0224559145729239969 	-0.0527039145134178752 	0.0178677635211209572 	
+0.0963923033747754177 	0.00717344875208557504 	0.038453036435484339 	-0.0852305729109014837 	0.0349974754938469079 	0.0441776047503394248 	0.0605526140911736982 	0.00112664200986311658 	-0.0087323645914181066 	0.0898344769753848943 	
+-0.0534115895805138616 	0.0181249695940979248 	0.00878363003094958204 	0.0455466177148464782 	-0.0588178514773917208 	0.0358653651595036357 	0.082242281840827422 	-0.0774890541600476573 	0.0369472008605820496 	0.0465351635632989744 	
+0.0708749948360742621 	0.0941472365254581306 	0.00226708912281538104 	-0.0861419439496607375 	0.019994648121601552 	-0.0806222482550226216 	0.062960522319972137 	0.0103389001022006783 	-0.0324336760051576259 	0.0658303459011729958 	
+0.0955803387142432764 	-0.0717037539477126395 	0.0603093016230788898 	-0.0712723826989075659 	-0.0493296803767544825 	-0.0128637489358975829 	0.0195715846647744121 	-0.047704226654149251 	0.00212767938104396715 	0.111425073998696722 	
+0.0658839890321762378 	-0.0295628146900774602 	0.0200661450461363106 	0.00131485756584521466 	-0.0849866013677955529 	0.0404686078881808867 	0.0671671998381296737 	0.0900259681327555267 	0.0937799245423497629 	-0.0322888665769429373 	
+-0.012193640755859993 	-0.0870625365848600175 	-0.0219153724537396524 	0.0300269070805505404 	0.0679963006559635985 	0.00794668553911313802 	0.0103240085384890962 	-0.0340401721806524588 	-0.0702681992831467039 	-0.00124906146825614142 	
+0.0730247717053444517 	0.0339815257289185976 	-0.019848855447414427 	0.103214962249267753 	-0.0214835899446404119 	0.0867021686659173174 	0.0484463412604088511 	0.00943655047359577039 	0.00817973246608490884 	-0.0738347947439276647 	
+-0.00089600701321111947 	0.0730742423097295035 	0.109410823441072003 	-0.0589267259135850333 	0.053123321772475543 	-0.0135355590753920615 	0.096744074162400745 	0.0663206959641660238 	-0.00549436838177646061 	0.0894934520821886043 	
+-0.0271782661674656693 	0.102020433000738431 	-0.0819332552441457557 	-0.0157483850508686185 	0.0642100696527682091 	-0.0753445699775463773 	0.0941361076333349367 	0.0735604700520547383 	0.0571258886864093804 	-0.0359806897455684713 	
+-0.0705536054527853007 	-0.0822073384032595555 	-0.00112150171006669837 	-0.0800304259132479656 	-0.0232782089529210978 	-0.0617095838930734122 	-0.0149387116402193675 	-0.0199394783436769515 	-0.0456782844612378669 	-0.0684025863257921724 	
+0.0193421802849989069 	-0.0357321956878843133 	-0.00496072883196821152 	0.0406104609691799387 	0.0413136602383304824 	0.00306079672871352633 	0.0777421250184955753 	-0.0107018642013677195 	0.00358113616845277078 	0.0913720102624297897 	
+0.0139232091273168331 	0.104943154693448962 	-0.0242118497989684961 	0.0163840269683069827 	0.0765070145006150754 	0.0958150456763250669 	-0.0727019189383650705 	-0.000419746250886005902 	-0.00416409232701833373 	0.0121238470826693732 	
+0.0651230092492420531 	0.0204138782025374797 	-0.0693453796121564769 	0.0677844774416461737 	-0.0508879293537423255 	-0.0634646977857465655 	-0.00656173557114147783 	-0.0559936636352839148 	-0.0424628758847008098 	0.0985469407427353034 	
+-0.0769886117618060889 	0.032501820267599818 	0.0663893309678701693 	-0.0278876471174986269 	0.0929045402890513461 	0.0474998247528790329 	-0.0257978670799245066 	0.00476154287973440914 	0.0801698203420577182 	-0.0788382104339286627 	
+0.0547014778198773449 	0.00946833203311331188 	-0.00726462752600475457 	0.0723254716201774117 	0.0205615977056121738 	-0.0367530694409559516 	0.0934301107854380308 	-0.0479505446674512031 	-0.0160957470058741732 	0.0694328484699304055 	
+-0.0595876310745595203 	-0.0383477302239165624 	-0.041792377416207567 	-0.0511609147197889594 	-0.0479087444457946612 	0.0225203087610199339 	0.0199693634725238364 	0.0534311524927292766 	-0.0731815681391047718 	-0.016781129205447963 	
+0.0905792518376024591 	0.0326154103319607688 	0.0572041638604988273 	-0.018205789586669973 	0.0646721485482818076 	0.0983918662842096087 	0.0525943899156008782 	-0.0843735568830600025 	0.0307371132750456598 	-0.0295980138681536178 	
+-0.00504787451744056342 	-0.0256729736995633048 	-0.00942989152080439541 	0.0108273404656922673 	0.052692591304093539 	0.106240139148546284 	0.0235946303473424275 	-0.0263567496424630988 	0.107147791099797343 	0.0376929710309186969 	
+0.0308158526794357242 	0.0686749627074960955 	0.0513940667149026809 	0.0520384058598451771 	-0.00530110123048805823 	-0.00537524554129623772 	0.000916187426836804347 	0.101704735597045612 	-0.0435904190560880214 	0.0439280282078316278 	
+0.0197027542880891977 	0.00282764625979796663 	-0.0624133168796937379 	-0.000838184694977099696 	-0.0361146913489463275 	0.0662115725869421717 	-0.00779921545030481857 	-0.0527288120519812734 	-0.0341886177796775978 	0.0143417420339569616 	
+-0.0328006992308578008 	0.0813174594280948321 	0.060185677611119523 	0.0848767764431867522 	0.0710419417995348801 	0.0809102300225778281 	0.0998703827619521756 	0.0325526540674225956 	-0.0392016732023402273 	-0.0302138858715109253 	
+0.00889465585724647304 	0.077477696001067492 	-0.048517506641452858 	-0.0567270884410531334 	0.0559349667575460979 	0.0814817504417899435 	0.0906402150722598332 	0.0950341870525590049 	0.0255652886676162247 	0.106900329777593742 	
+0.0086226725446124329 	0.0549555002798989348 	0.060457014530452928 	-0.0903770014132604521 	0.0753477814899700254 	0.0600692564159891404 	-0.065193230359046131 	0.0567607535996484738 	-0.030403393263260501 	-0.0723456062223043483 	
+0.0056503701531967921 	0.022848910592854519 	0.0865681102397283242 	0.0483345487266812826 	0.010626938123042785 	0.0989198164244114858 	0.0681344486795087551 	-0.0600214201563913016 	0.0474228192206973403 	0.0920626419468070084 	
+0.0165113020903557041 	0.0177311826089620629 	0.0505398087175398941 	0.00211791774586567866 	-0.0361001501220852769 	-0.0181649815856086451 	0.00221247923582702658 	-0.0425889963615622771 	0.0265007825372027619 	0.0131443579397779817 	
+-0.0653153889137650656 	0.0771867556581843917 	0.053232169674471766 	0.0203190656293838964 	-0.0773921069536342293 	0.0624205572191128474 	0.110997722997582149 	-0.0272642040811898433 	0.0435103308168569708 	-0.0588413621392204211 	
+-0.00187058312404764357 	0.0163102571290564546 	0.0661237839240759739 	-0.0459968476517004579 	0.0799619050368084311 	-0.0255381660134464256 	0.0393167535168238089 	-0.000428726455830320471 	0.0704347503703292022 	-0.0265693858624537725 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -287,11 +287,11 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
--0.00361456086924041289 	-0.0034546392301283888 	0.00892340884822014069 	0.00344600332427484786 	0.00409064634836684045 	0.00330896605896703958 	-0.0071032199901970676 	-0.0077218952474459113 	-0.00354635209487409817 	-0.00981545859166288101 	-0.000813800539720391583 	-0.00966192627622517591 	-0.000917079339102153156 	0.00454432087430920763 	0.00417192126901913193 	-0.00862727928662285551 	0.00209955758356582213 	-0.00702340380695190211 	0.00542175158362248871 	-0.00390224612647109993 	-0.00214279922826613074 	0.000233095447677282835 	0.00804593137043763812 	0.00574502357026333843 	0.000565606677185788963 	-0.00482300706056737992 	0.00739447598229839038 	0.00811872141969053294 	0.00379486357640810429 	0.00658199484371788188 	-0.00653934446117148023 	-0.0107571495767850647 	9.18485893836186841e-06 	-0.000736319356757701975 	0.00568173954498692266 	0.00736199843687386241 	-0.00864132204601229977 	0.00249227451801360645 	0.0031692626664838212 	-0.00096500131406930523 	-0.00238!
 053629351953977 	-0.000507034767643173593 	-0.00369954298604871458 	0.00391972062390184145 	-0.00732055190192157194 	-0.00879860623868512597 	0.009309560879504181 	-0.000515274541135739761 	0.00584820601158001462 	0.000468091146464152128 	-0.00204273692383042911 	-0.00104438579406907492 	0.00676734198102325098 	0.00259775966195790022 	-0.00506898157846626292 	0.00400301500060299201 	0.00462300065256556186 	0.0090185902254103379 	0.00714303775483081556 	-0.00835706779120674696 	0.00954111518838614281 	-0.00779115044297907546 	-0.00236347716601639129 	-0.00118304905244554295 	0.0050209025693952079 	-0.000286016031183988328 	-0.00482798210193771411 	-0.00454637521312786612 	-0.00287093068063447657 	-0.00581445626144608523 	-0.00198314410522661654 	-0.00711353476514379589 	0.0020642326867833221 	-0.00934342665262296972 	-0.00365153936610122668 	-0.00280561527474843149 	0.000543520117863609091 	-0.0040994033838285043 	0.0075005566754033498 	0.00741998824691424922 	0.006035248938!
 87166097 	-0.000161966156904518282 	-0.00299263494129304057 	-!
 0.003555
76391448426087 	-0.0028232877964259784 	-0.00114021105423473251 	-0.00392387421914959424 	-0.00744660364820715071 	-0.00428922564650745589 	0.00642389043866236405 	-0.00220870553974613281 	0.00340434749506830824 	0.00491865372732491583 	0.0003043342327976939 	-0.00347359847246154737 	-0.00805576626600675028 	-0.0101676942138113346 	0.00184513175038662097 	0.00834935392649940357 	-0.00233387137457582083 	
--0.00428357603894804085 	-0.00560878811569882722 	0.00644610901550797186 	2.85361801116452833e-05 	0.00231843951024641396 	0.0091920061067590781 	-0.00498422481254911457 	-0.00798262593082200815 	-0.00681521017393437074 	0.00229313407708757206 	-0.000639420462873237813 	0.00721425721718700706 	0.00655349518388535258 	0.0018080823175403893 	0.005513546170684544 	0.00470534668525295421 	0.00210720344970842001 	-0.00959463543317737658 	0.00925219698373813855 	-0.00282655751003510394 	-0.00160556445194010283 	0.00670403504636591339 	-0.00689192001286197987 	0.00402486128987972636 	-0.00692580474096167464 	0.00524873955024266186 	0.001330082096385525 	-0.00749040086355653046 	0.00723936863861645349 	-0.00981095865746234655 	-0.00885297577408837577 	0.0105326493621040566 	-0.00686186375766028884 	-0.00186042921694784774 	0.00177016931350605969 	0.00713124303157063651 	0.00216752614868498134 	0.00380119429726093888 	-0.00594750008661806915 	0.00855464479060022258 	0.00024896574849!
 4373008 	-0.00158649273566845205 	0.00953251045551742164 	-0.00540848395533746838 	0.00277396057439592125 	0.000904513165711946896 	-0.000937195773606970301 	-0.000418610167075077926 	0.00755916712339810368 	0.00961871529798706375 	0.00812048427811438121 	0.00186746010322442341 	0.00407341980157822654 	-0.00311260062525448543 	0.00094987641171063997 	0.00530444774939974485 	-0.00473036555390706544 	0.00965177183175397642 	-0.00646798155063100025 	-0.00496175775451397627 	0.00669502184691669021 	0.00346766019491735701 	0.00202831558717488742 	0.0079754747566342446 	0.000698865924041077607 	-0.00304638580417564749 	-0.00679405487759599924 	0.00432431142677767999 	0.00216422450319122151 	-0.00061007203372814389 	-0.00382641334663995011 	-0.00998287979623964025 	0.00705941722703434284 	0.000176218032275878543 	-0.00094440667949118322 	0.00563608474335693544 	0.00410871051462780544 	0.0083555071211082934 	-0.00166986093452475707 	-0.00484421281078854273 	-0.00522019012264732067 !
 	0.00898723094886254292 	0.00862398867535417619 	-0.0096836565!
 19044459
01 	0.000221534183445729537 	0.000664773860125775521 	-0.00484719011819337645 	-0.00387543718905094398 	-0.00210722179689584645 	-0.00500161898954350377 	-0.00233237632557197535 	-0.00976789325837463217 	0.00146967192664533672 	0.00341503265385532462 	0.00298996139752164469 	0.00122907757217042288 	0.00700538259559502739 	0.00589268468676095239 	0.00867987032582888743 	-0.00537698855929221158 	
+-0.00328975223759890838 	-0.00346826422071847646 	0.00907581277511097101 	0.00347136775228049088 	0.0042871187908744314 	0.00343990328209323746 	-0.00714654471289515817 	-0.00784111630398876686 	-0.00372907859368766569 	-0.00973366008345864786 	-0.000688643740409981982 	-0.00956428308641964382 	-0.000897958259188700041 	0.00449082326757663242 	0.0041337330985273214 	-0.0085686570969960036 	0.00218937131657726642 	-0.00723381293808018216 	0.0053319848721854789 	-0.00375811070933532903 	-0.00238323519353948309 	0.000316195355204442862 	0.00808973600787776456 	0.00593774438444158621 	0.000253541476231059718 	-0.00515489325378380193 	0.00766421459345257173 	0.00822820508513122384 	0.00377213064554823407 	0.0068385909867962457 	-0.00641018616097005674 	-0.0106864673431768097 	0.000268021179583320559 	-0.000829993432607160496 	0.00571896098541352284 	0.00752085871272469789 	-0.00839566831792106123 	0.00265450762386824175 	0.00316020877807904679 	-0.000855220153014026431 	-0.00216!
 972224943933921 	-0.000391017313711675466 	-0.00369284201163942337 	0.00376187812233940398 	-0.00729078327222553498 	-0.00881831736733464273 	0.00915501757283917507 	-0.00040865373133016226 	0.00616774748917324998 	0.000512820143645118789 	-0.00200929522244623886 	-0.00131110755932426068 	0.00688874424070522705 	0.00277139637559157539 	-0.00513924785431744014 	0.00412479945516167935 	0.00459517600763888635 	0.00909708604687939695 	0.00724915463143394664 	-0.00841163282039994464 	0.00949616967726132563 	-0.00786567154882882556 	-0.00249471971964055489 	-0.00100552161343312255 	0.00496276761900886834 	-0.000137242292421134546 	-0.00464854749180680302 	-0.00454994604220504157 	-0.00298497966000130189 	-0.00578020370355490816 	-0.00207397345344491335 	-0.007298667005837642 	0.00229286690879886923 	-0.00940271360625089155 	-0.00356147796776346481 	-0.00275095748254701586 	0.000675238932615726604 	-0.00412192037523308099 	0.00773887852981238692 	0.00737895208119485405 	0.00588435!
 370720583484 	-0.000137416867987901782 	-0.0024894531471409070!
 8 	-0.00
357732556298006417 	-0.00284391451132938976 	-0.000963057691869287791 	-0.00385410792476033803 	-0.00745729185583782813 	-0.00396037228324378019 	0.00635389739462362095 	-0.00226757216711522295 	0.00333486553739848054 	0.00513589831121753163 	0.000145886258715480888 	-0.00366597445275700399 	-0.00794414559264285634 	-0.0103289536804432973 	0.00197890813580487834 	0.00840111384503721033 	-0.00230698117710193733 	
+-0.00460838467058952757 	-0.0055951631251087517 	0.00629370508861714674 	3.17175210599941552e-06 	0.00212196706773883125 	0.00906106888363287545 	-0.00494090008985100925 	-0.00786340487427916908 	-0.00663248367512082968 	0.00221133556888333457 	-0.000764577262183648932 	0.00711661402738147843 	0.00653437410397188657 	0.00186157992427295866 	0.00555173434117631811 	0.00464672449562609276 	0.00201738971669697094 	-0.00938422630204910173 	0.00934196369517516917 	-0.00297069292717086703 	-0.00136512848666674701 	0.0066209351388387969 	-0.00693572465030210545 	0.00383214047570147771 	-0.00661373954000695971 	0.00558062574345910209 	0.00106034348523132544 	-0.00759988452899723263 	0.00726210156947633412 	-0.0100675548005407529 	-0.00898213407428978192 	0.0104619671284957843 	-0.00712070007830524748 	-0.00176675514109837914 	0.00173294787307944564 	0.00697238275571979323 	0.0019218724205936654 	0.00363896119140629144 	-0.0059384461982132878 	0.00844486362954492351 	3.8151704414179!
 9211e-05 	-0.00170251018959995028 	0.00952580948110814604 	-0.00525064145377502397 	0.00274419194469983918 	0.000924224294361454123 	-0.000782652466941984645 	-0.000525230976880659275 	0.00723962564580487699 	0.00957398630080609145 	0.00808704257673019053 	0.00213418186847960309 	0.00395201754189624006 	-0.00328623733888816104 	0.00102014268756182316 	0.00518266329484105837 	-0.00470254090898037606 	0.0095732760102849191 	-0.00657409842723416082 	-0.00490719272532080981 	0.00673996735804150305 	0.0035421813007671275 	0.00215955814079904712 	0.00779794731762177996 	0.000757000874427418355 	-0.00319515954293849748 	-0.00697348948772691726 	0.00432788225585485978 	0.0022782734825580477 	-0.000644324591619325733 	-0.00373558399842165764 	-0.00979774755554577506 	0.00683078300501879614 	0.000235504985903817014 	-0.00103446807782896352 	0.005581426951155548 	0.00397699169987568608 	0.00837802411251285968 	-0.00190818278893380025 	-0.00480317664506915362 	-0.00506929489098148847 	!
 0.00896268165994594523 	0.00812080688120202232 	-0.00966209487!
 05486674
2 	0.000242160898349140031 	0.00048762049776032853 	-0.00491695641258263656 	-0.00386474898142027783 	-0.00243607516015952041 	-0.00493162594550476414 	-0.00227350969820288651 	-0.00969841130070480013 	0.00125242734275270943 	0.00357348062793753788 	0.00318233737781711649 	0.00111745689880651897 	0.00716664206222697615 	0.00575890830134267572 	0.0086281104072910824 	-0.00540387875676610549 	
 ]
 ;
-bias = 2 [ -0.000484761096494694074 0.000484761096494707409 ] ;
+bias = 2 [ -0.000388256263229395493 0.000388256263229405359 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "affine_net" ;

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat
===================================================================
(Binary files differ)



From lamblin at mail.berlios.de  Sat Jun 30 00:30:06 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 30 Jun 2007 00:30:06 +0200
Subject: [Plearn-commits] r7680 - in
	trunk/plearn_learners/online/test/ModuleLearner/.pytest:
	PL_ModuleLearner_Greedy PL_ModuleLearner_TwoRBMs
Message-ID: <200706292230.l5TMU6E7013649@sheep.berlios.de>

Author: lamblin
Date: 2007-06-30 00:30:06 +0200 (Sat, 30 Jun 2007)
New Revision: 7680

Modified:
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/
Log:
Ignore files generated by the test



Property changes on: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy
___________________________________________________________________
Name: svn:ignore
   - .plearn
run_results

   + .plearn
run_results
PSAVEDIFF



Property changes on: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs
___________________________________________________________________
Name: svn:ignore
   - .plearn
run_results

   + .plearn
run_results
PSAVEDIFF




From lamblin at mail.berlios.de  Sat Jun 30 00:31:22 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 30 Jun 2007 00:31:22 +0200
Subject: [Plearn-commits] r7681 - tags
Message-ID: <200706292231.l5TMVMY9015516@sheep.berlios.de>

Author: lamblin
Date: 2007-06-30 00:31:21 +0200 (Sat, 30 Jun 2007)
New Revision: 7681

Added:
   tags/after_energy_sign_changes/
Log:
Energy sign change should be complete now.


Copied: tags/after_energy_sign_changes (from rev 7680, trunk)



From lamblin at mail.berlios.de  Sat Jun 30 00:39:52 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 30 Jun 2007 00:39:52 +0200
Subject: [Plearn-commits] r7682 -
	trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Basic
Message-ID: <200706292239.l5TMdqsT032575@sheep.berlios.de>

Author: lamblin
Date: 2007-06-30 00:39:50 +0200 (Sat, 30 Jun 2007)
New Revision: 7682

Modified:
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Basic/
Log:
One more.



Property changes on: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Basic
___________________________________________________________________
Name: svn:ignore
   - .plearn
run_results

   + .plearn
run_results
PSAVEDIFF




From louradou at mail.berlios.de  Sat Jun 30 04:18:49 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 30 Jun 2007 04:18:49 +0200
Subject: [Plearn-commits] r7683 - in
	trunk/python_modules/plearn/learners/modulelearners: .
	examples sampler sampler/example sampler/example/data
	sampler/example/data/babyAI-1obj.dmat
	sampler/example/data/babyAI-1obj.dmat.metadata
Message-ID: <200706300218.l5U2InC9011000@sheep.berlios.de>

Author: louradou
Date: 2007-06-30 04:17:03 +0200 (Sat, 30 Jun 2007)
New Revision: 7683

Added:
   trunk/python_modules/plearn/learners/modulelearners/sampler/
   trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.network.jpeg
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/fieldnames
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/sizes
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat/
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat/0.data
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat/indexfile
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/reconstruct.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py
Removed:
   trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py
   trunk/python_modules/plearn/learners/modulelearners/examples/sample.py
   trunk/python_modules/plearn/learners/modulelearners/examples/sample_from_hidden.py
Modified:
   trunk/python_modules/plearn/learners/modulelearners/network_view.py
Log:
[python_modules for NetworkModule]
sampling with RBMs: things cleaned and organized a bit



Deleted: trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/do_sampling.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -1,151 +0,0 @@
-from plearn.learners.modulelearners import *
-
-def basename_withoutExt(name):
-    return '.'.join(os.path.basename(name).split('.')[:-1])
-
-
-####################################
-## Choosing the model to generate ##
-####################################
-
-
-plarg_defaults.gibbs_step                    = 1
-gibbs_step                                   = plargs.gibbs_step # Number of Gibbs step between each sampling
-
-##############################
-# Characteristics of the data
-
-plarg_defaults.Path                          = '/cluster/pauli/data/babyAI/textual_v3'
-plarg_defaults.Encoding                      = '3gram01'
-plarg_defaults.Topics                        = 'shape' #, 'color', location', 'size', 
-plarg_defaults.Size                          = 32
-plarg_defaults.Nobj                          = 1 # 1, 2, 3, 4
-plarg_defaults.ImageType                     = 'gray' # 'gray', 'gray_norot' (without rotation of objects)
-plarg_defaults.trainNsamples                 = 10000
-plarg_defaults.unsupervised_trainNsamples    = 250000 # 10000 250000 1250000
-plarg_defaults.testNsamples                  = 5000
-plarg_defaults.validNsamples                 = plargs.testNsamples
-plarg_defaults.Extension                     = 'vmat'
-
-Path                          = plargs.Path                       # Directory where are the datasets'files
-Encoding                      = plargs.Encoding                   # layerType of Encoding (onehot, 1gram, 2gram, 3gram, ...)
-Topics                        = plargs.Topics                     # Question topic (color, color-size, color-size-location, color-size-location-shape, ...)
-Size                          = plargs.Size                       # Image width/height
-Nobj                          = plargs.Nobj                       # Number of objects in the image
-ImageType                     = plargs.ImageType                  # Color encoding (gray, color) and other features (no rotation...)
-trainNsamples                 = plargs.trainNsamples              # number of training samples
-unsupervised_trainNsamples    = plargs.unsupervised_trainNsamples # number of training samples
-testNsamples                  = plargs.testNsamples               # number of samples to test
-validNsamples                 = plargs.validNsamples              # number of validation samples
-Extension                     = plargs.Extension                  # extension of the input files
-
-
-plarg_defaults.nRBM                     = 3
-plarg_defaults.NH1                      = 500
-plarg_defaults.NH2                      = 500
-plarg_defaults.NH3                      = 500
-plarg_defaults.batchSize                = 50
-plarg_defaults.layerType                = 'gaussian'
-plarg_defaults.unsupervised_nStages     = int(unsupervised_trainNsamples)
-plarg_defaults.supervised_nStages       = 100  # /!\ see after: supervised_nStages   *= trainNsamples
-plarg_defaults.nStagesStep              = 5
-plarg_defaults.LR_CDiv                  = 0.01
-plarg_defaults.LR_CDiv1		        = float(plargs.LR_CDiv)
-plarg_defaults.LR_CDiv2			= float(plargs.LR_CDiv)
-plarg_defaults.LR_CDiv3			= float(plargs.LR_CDiv)
-plarg_defaults.LR_GRAD_UNSUP            = 0.003
-plarg_defaults.LR_GRAD_UNSUP1		= float(plargs.LR_GRAD_UNSUP) 
-plarg_defaults.LR_GRAD_UNSUP2 		= float(plargs.LR_GRAD_UNSUP) 
-plarg_defaults.LR_GRAD_UNSUP3 		= float(plargs.LR_GRAD_UNSUP)	
-plarg_defaults.LR_SUP                   = 0.
-plarg_defaults.L2wd_SUP                 = 1e-5
-plarg_defaults.nGibbs                   = 1
-plarg_defaults.Tag                      = 'dbn-'+str(plargs.nRBM)+'RBMimage'
-
-# Network structure
-nRBM          = int(plargs.nRBM)
-NH1           = int(plargs.NH1)                        # num units for the image part
-NH2           = int(plargs.NH2)                        # num units, 2nd hid layer (image part)
-NH3           = int(plargs.NH3)                        # num units, 2rd hid layer
-layerType                = plargs.layerType
-#
-# Network learning parameters
-batchSize                 = int(plargs.batchSize)                # num of samples in the minibatch
-unsupervised_nStages      = int(plargs.unsupervised_nStages)                # total number of samples to see (unsupervised phase)
-supervised_nStages        = int(plargs.supervised_nStages)                # total number of samples to see (supervised phase)
-supervised_nStages       *= trainNsamples
-nStagesStep               = int(plargs.nStagesStep)                #
-LR_CDiv                   = float(plargs.LR_CDiv)                # unsup. lr
-LR_CDiv1                   = float(plargs.LR_CDiv1)                # unsup. lr
-LR_CDiv2                   = float(plargs.LR_CDiv2)                # unsup. lr
-LR_CDiv3                   = float(plargs.LR_CDiv3)                # unsup. lr
-LR_GRAD_UNSUP             = float(plargs.LR_GRAD_UNSUP)         # super. lr
-LR_GRAD_UNSUP1             = float(plargs.LR_GRAD_UNSUP1)         # super. lr
-LR_GRAD_UNSUP2             = float(plargs.LR_GRAD_UNSUP2)         # super. lr
-LR_GRAD_UNSUP3             = float(plargs.LR_GRAD_UNSUP3)         # super. lr
-LR_SUP                    = float(plargs.LR_SUP) # super. lr
-L2wd_SUP                  = float(plargs.L2wd_SUP)
-nGibbs                     = int(plargs.nGibbs)
-#
-# Other
-Tag                     = plargs.Tag
-
-
-BaseDir =  '/u/louradoj/PRGM/blocksworld/res/'+os.path.basename(Path)
-data_filename = Path+'/BABYAI_'+ImageType+'_'+str(unsupervised_trainNsamples)+'x'+str(Nobj)+'obj_'+str(Size)+'x'+str(Size)+'.'+Topics+'.train.'+Encoding+'.'+Extension
-BaseTag = Tag+'_'+layerType+'_'+basename_withoutExt(data_filename)
-
-
-unsupervised_expdir = BaseDir + '/greedyDBN/UNSUP_' + BaseTag + '_'+layerType+''.join(['_N'+str(i)+'-'+str(globals()['NH'+str(i)]) for i in range(1,nRBM+1)])+'_LRs'+'-'.join([str(globals()['LR_CDiv'+str(i)]) for i in range(1,nRBM+1)])+'_'+'-'.join([str(globals()['LR_GRAD_UNSUP'+str(i)]) for i in range(1,nRBM+1)])+'_ns'+str(unsupervised_nStages)+'_ng'+str(nGibbs) 
-finetuning_expdir = BaseDir + '/greedyDBN/FINETUNE_' + BaseTag + '_'+layerType+''.join(['_N'+str(i)+'-'+str(globals()['NH'+str(i)]) for i in range(1,nRBM+1)])+'_LRs'+'-'.join([str(globals()['LR_CDiv'+str(i)]) for i in range(1,nRBM+1)])+'_'+'-'.join([str(globals()['LR_GRAD_UNSUP'+str(i)]) for i in range(1,nRBM+1)])+'-'+str(LR_SUP)+'_WD'+str(L2wd_SUP)+'_ns'+str(unsupervised_nStages)+'-'+str(supervised_nStages)+'_ng'+str(nGibbs)
-
-
-
-if len(sys.argv) == 1:
-   learner_filename = '/u/louradoj/PRGM/blocksworld/res/textual_v3/greedyDBN/FINETUNE_dbn-3RBMimage_gaussian_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_gaussian_N1-500_N2-500_N3-500_LRs0.01-0.01-0.01_0.003-0.003-0.003-0.03_WD1e-05_ns2500000-1000000_ng1'+"/Split0/final_learner.psave"
-   learner_filename = '/u/louradoj/PRGM/blocksworld/res/textual_v3/greedyDBN/UNSUP_dbn-3RBMimage_gaussian_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_gaussian_N1-500_N2-500_N3-500_LRs0.01-0.01-0.01_0.003-0.003-0.003_ns1250000_ng1'+"/init_learner.psave"
-   learner_filename = '/u/louradoj/PRGM/blocksworld/res/textual_v3/greedyDBN/FINETUNE_dbn-3RBMimage_gaussian_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_gaussian_N1-500_N2-500_N3-500_LRs0.01-0.01-0.01_0.01-0.01-0.01-0.03_WD1e-05_ns5000000-1000000_ng1'+"/Split0/final_learner.psave"
-   learner_filename = '/u/louradoj/PRGM/blocksworld/res/textual_v3/greedyDBN/UNSUP_dbn-3RBMimage_gaussian_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_gaussian_N1-500_N2-500_N3-500_LRs0.01-0.01-0.01_0.01-0.01-0.01_ns5000000_ng1'+"/init_learner.psave"
-elif len(sys.argv) == 2:
-   basename=os.path.basename(sys.argv[1])
-   if basename[0:5] == 'UNSUP':
-      print "UNSUPERVISED-way trained model"
-      learner_filename = sys.argv[1]+"/init_learner.psave"
-   elif basename[0:5] == 'FINET':
-      print "supervised FINETUNED model"
-      learner_filename = sys.argv[1]+"/Split0/final_learner.psave"
-   else:
-      raise TypeError, "Cannot recognize "+basename[0:5]+" in "+sys.argv[1]
-elif LR_SUP==None or LR_SUP == 0:
-   print "UNSUPERVISED-way trained model"
-   learner_filename = unsupervised_expdir+"/init_learner.psave"
-else:
-   print "supervised FINETUNED model"
-   learner_filename = finetuning_expdir+"/Split0/final_learner.psave"
-
-
-def check_choice(c):
-    try:
-       if int(c) in [1,2,3,4]:
-          return True
-    except: pass
-    return False
-
-
-c=None
-while check_choice(c)==False:
-   print "Type the number corresponding to your choice :"
-   print "1. Sample after having initialized input visible units with (randomly picked) real image"
-   print "2. Sample after having initialized top hidden units with a random binary vector"
-   print "3. Reconstruct the input image"
-   print "4. Visualize input weights"
-   c = sys.stdin.readline()
-
-if int(c) == 1:
-   os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample.py '+' '.join([ learner_filename, str(Size*Size), data_filename, 'gibbs_step='+str(gibbs_step) ]))
-elif int(c) == 2:
-   os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_hidden.py '+' '.join([ learner_filename, str(Size*Size), 'gibbs_step='+str(gibbs_step) ]))
-elif int(c) == 3:
-   os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/reconstruct.py '+' '.join([ learner_filename, str(Size*Size), data_filename]))
-elif int(c) == 4:
-   os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/inputweights.py '+' '.join([ learner_filename, str(Size*Size)]))

Deleted: trunk/python_modules/plearn/learners/modulelearners/examples/sample.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/sample.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/sample.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -1,211 +0,0 @@
-from plearn.learners.modulelearners import *
-import random, sys, os.path
-
-from pygame import *
-from math import *
-
-
-zoom_factor = 5
-
-def init_screen(Nim):
-    init()
-    width = int(sqrt(Nim*1.0))
-    if width**2 != Nim:
-       width = int(sqrt(Nim*1.0))+1
-#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
-    width *= zoom_factor
-    return display.set_mode([width, width])
-
-
-def draw_image(visible,screen):
-    Nim=len(visible)
-    width = int(sqrt(Nim*1.0))
-#    if width**2 != Nim:
-#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
-    width *= zoom_factor
-    surface = Surface((width, width),0,8)
-    surface.set_palette([(i,i,i) for i in range(2**8)])
-    for x in range(width/zoom_factor):
-       for y in range(width/zoom_factor):
-           graycol = max(min(255,int(255.0*visible[x*width/zoom_factor+y])),0)
-           for i in range(zoom_factor):
-               for j in range(zoom_factor):
-                   surface.set_at((x*zoom_factor+i,y*zoom_factor+j),(graycol,graycol,graycol,255))
-    screen.blit(surface, (0,0))
-    display.update()
-    return pause()
-  
-def pause():
-   c = sys.stdin.readline()
-   if c.strip() == 'q' or  c.strip() == 'x':
-      sys.exit(0)
-   if c.strip() == 'n' :
-      return -1
-   try: return int(c.strip())
-   except: return 0
-
-
-if __name__ == "__main__":
-
-  if len(sys.argv) < 4:
-     print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size> <dataSet_filename> [gibbs_step=<gibbs_step>]\n"
-     print "Purpose:\n\tSee consecutive Gibbs sample"
-     print "\twhen input visible units are initalized with real images"
-     print "Tips:\n\tOnce you can see an image type"
-     print "\t:    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
-     print "\t: <an integer> : to change the gibbs step (10 will set the number of gibbs step between each example to 10)"
-     print "\t:      n       : (next) to try with another image as initialization"
-     print "\t:      q       : (quit) to stop the massacre\n"
-     sys.exit()
-
-  learner_filename = sys.argv[1]
-  Nim = int(sys.argv[2])
-  data_filename = sys.argv[3]
-     
-  plarg_defaults.gibbs_step                    = 10
-  gibbs_step                                   = plargs.gibbs_step # Number of Gibbs step between each sampling
-
-     
-  if os.path.isfile(learner_filename) == False:
-     raise TypeError, "Cannot find file "+learner_filename
-  print " loading... "+learner_filename
-  learner = loadObject(learner_filename)
-  if 'HyperLearner' in str(type(learner)):
-     learner=learner.learner
-  
-  if os.path.isfile(data_filename) == False:
-     raise TypeError, "Cannot find file "+data_filename
-  print " loading... "+data_filename
-  dataSet = pl.AutoVMatrix( specification = data_filename )
-
-  #
-  # Getting the RBMmodule which sees the image (looking at size of the down layer)
-  #
-  modules=getModules(learner)
-  for i in range(len(modules)):
-     module = modules[i]
-     if isModule(module,'RBM') and module.connection.down_size == Nim:
-        image_RBM=learner.module.modules[i]
-        break
-  image_RBM_name=image_RBM.name
-  #
-  # Getting the top RBMmodule
-  #
-
-  top_RBM = getTopRBMModule( learner )
-  top_RBM_name = top_RBM.name
-  
-  NH=top_RBM.connection.up_size
-
-
-  init_ports = [ ('input',  image_RBM_name+'.visible'),
-                 ('output', top_RBM_name+'.hidden_sample')
-               ]
-  ports = [ ('input', top_RBM_name+'.hidden_sample' ),
-            ('output', image_RBM_name+'.visible_expectation')
-            ]
-
-  #
-  # Removing useless connections for sampling
-  #
-  old_connections_list = copy.copy(learner.module.connections)
-  conn_toremove=[]
-  connections_list_down=[]
-  connections_list_up=[]
-  for connection in old_connections_list:
-      source_module = getModule( learner, port2moduleName( connection.source ))
-      dest_module   = getModule( learner, port2moduleName( connection.destination ))
-      if isModule( source_module, 'RBM') and isModule( dest_module,'RBM'):
-         connections_list_up.append ( pl.NetworkConnection(source = port2moduleName( connection.source )+'.hidden_sample',
-                                                           destination = port2moduleName( connection.destination )+'.visible_sample',
-                                                           propagate_gradient = 0) )
-         connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_sample',
-                                                    destination = port2moduleName( connection.source )+'.hidden_sample',
-                                                    propagate_gradient = 0) )
-  
-  #
-  # Removing useless modules for sampling
-  #
-  modules_list = getModules(learner)
-  mod_toremove=[]
-  for module in modules_list:
-      if isModule( module, 'RBM') == False:
-         mod_toremove.append(module)
-  for module in mod_toremove:
-      modules_list.remove(module)
-  
-  
-  RBMnetwork = pl.NetworkModule(
-                          modules = modules_list,
-                          connections = connections_list_down,
-                          ports = ports,
-                          # to avoid calling the forget() method in ModuleLearner                          
-                          random_gen = pl.PRandom( seed = 1827 ),
-                          # Hack from Olivier
-                          save_states = 0
-                         )
-  RBMnetworkInit = pl.NetworkModule(
-                          modules = modules_list,
-                          connections = connections_list_up,
-                          ports = init_ports,
-                          # to avoid calling the forget() method in ModuleLearner                          
-                          random_gen = pl.PRandom( seed = 1827 ),
-                          # Hack from Olivier
-                          save_states = 0
-                         )
-
-  
-  RBMmodel = pl.ModuleLearner(
-                              cost_ports = [],
-                              target_ports = [],
-                              module = RBMnetwork
-                           )
-  RBMmodelInit = pl.ModuleLearner(
-                              cost_ports = [],
-                              target_ports = [],
-                              module = RBMnetworkInit
-                           )
-
-
-
-  RBMmodelInit.setTrainingSet(pl.AutoVMatrix(inputsize=Nim, targetsize=0, weightsize=0), False)
-  RBMmodel.setTrainingSet(pl.AutoVMatrix(inputsize=NH, targetsize=0, weightsize=0), False)
-
-
-  screen=init_screen(Nim)
-  random.seed(1969)
-
-  for i in range(len(RBMmodel.module.modules)):
-    module = RBMmodel.module.modules[i]
-    if isModule( module, 'RBM'):
-       RBMmodel.module.modules[i].compute_contrastive_divergence = False
-       if module.name == top_RBM_name:
-          RBMmodel.module.modules[i].n_Gibbs_steps_per_generated_sample = gibbs_step
-          top_RBM = RBMmodel.module.modules[i]
-
-  while True:
- 
-   random_index=random.randint(0,dataSet.length)
-   init_image=[dataSet.getRow(random_index)[i] for i in range(Nim)]
-   
-   c = draw_image( init_image, screen )
-   if c<0:
-      continue
-   elif c>0:
-      top_RBM.n_Gibbs_steps_per_generated_sample = c
-   
-   init_hidden = RBMmodelInit.computeOutput(init_image)
-   c = draw_image( RBMmodel.computeOutput(init_hidden), screen )
-   if c<0:
-      break
-   elif c>0:
-      top_RBM.n_Gibbs_steps_per_generated_sample = c
-   
-      
-   while True:
-       c = draw_image( RBMmodel.computeOutput([]) , screen)
-       if c<0:
-          break
-       elif c>0:
-          top_RBM.n_Gibbs_steps_per_generated_sample = c
-    

Deleted: trunk/python_modules/plearn/learners/modulelearners/examples/sample_from_hidden.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/examples/sample_from_hidden.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/examples/sample_from_hidden.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -1,173 +0,0 @@
-from plearn.learners.modulelearners import *
-import random, sys, os.path
-
-from pygame import *
-from math import *
-
-
-zoom_factor = 5
-
-def init_screen(Nim):
-    init()
-    width = int(sqrt(Nim*1.0))
-    if width**2 != Nim:
-       width = int(sqrt(Nim*1.0))+1
-#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
-    width *= zoom_factor
-    return display.set_mode([width, width])
-
-
-def draw_image(visible,screen):
-    Nim=len(visible)
-    width = int(sqrt(Nim*1.0))
-#    if width**2 != Nim:
-#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
-    width *= zoom_factor
-    surface = Surface((width, width),0,8)
-    surface.set_palette([(i,i,i) for i in range(2**8)])
-    for x in range(width/zoom_factor):
-       for y in range(width/zoom_factor):
-           graycol = max(min(255,int(255.0*visible[x*width/zoom_factor+y])),0)
-           for i in range(zoom_factor):
-               for j in range(zoom_factor):
-                   surface.set_at((x*zoom_factor+i,y*zoom_factor+j),(graycol,graycol,graycol,255))
-    screen.blit(surface, (0,0))
-    display.update()
-    return pause()
-  
-def pause():
-   c = sys.stdin.readline()
-   if c.strip() == 'q' or  c.strip() == 'x':
-      sys.exit(0)
-   if c.strip() == 'n' :
-      return -1
-   try: return int(c.strip())
-   except: return 0
-
-
-if __name__ == "__main__":
-
-  if len(sys.argv) < 2:
-     print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size>\n"
-     print "Purpose:\n\tSee consecutive Gibbs sample"
-     print "\twhen top hidden units are initalized randomly"
-     print "Tips:\n\tOnce you can see an image type"
-     print "\t:    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
-     print "\t: <an integer> : to change the gibbs step (10 will set the number of gibbs step between each example to 10)"
-     print "\t:      n       : (next) to try another hidden state for initialization"
-     print "\t:      q       : (quit) to stop the massacre\n"
-     sys.exit()
-
-  learner_filename = sys.argv[1]
-  Nim = int(sys.argv[2])
-     
-  plarg_defaults.gibbs_step                    = 10
-  gibbs_step                                   = plargs.gibbs_step # Number of Gibbs step between each sampling
-
-     
-  if os.path.isfile(learner_filename) == False:
-     raise TypeError, "Cannot find file "+learner_filename
-  print " loading... "+learner_filename
-  learner = loadObject(learner_filename)
-  if 'HyperLearner' in str(type(learner)):
-     learner=learner.learner
-
-  #
-  # Getting the RBMmodule which sees the image (looking at size of the down layer)
-  #
-  modules=getModules(learner)
-  for i in range(len(modules)):
-     module = modules[i]
-     if isModule(module,'RBM') and module.connection.down_size == Nim:
-        image_RBM=learner.module.modules[i]
-        break
-  image_RBM_name=image_RBM.name
-  #
-  # Getting the top RBMmodule
-  #
-
-  top_RBM = getTopRBMModule( learner )
-  top_RBM_name = top_RBM.name
-  
-  NH=top_RBM.connection.up_size
-
-
-  ports = [ ('input', top_RBM_name+'.hidden_sample' ),
-            ('output', image_RBM_name+'.visible_expectation')
-            ]
-
-  #
-  # Removing useless connections for sampling
-  #
-  old_connections_list = copy.copy(learner.module.connections)
-  conn_toremove=[]
-  connections_list_down=[]
-  connections_list_up=[]
-  for connection in old_connections_list:
-      source_module = getModule( learner, port2moduleName( connection.source ))
-      dest_module   = getModule( learner, port2moduleName( connection.destination ))
-      if isModule( source_module, 'RBM') and isModule( dest_module,'RBM'):
-         connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_sample',
-                                                    destination = port2moduleName( connection.source )+'.hidden_sample',
-                                                    propagate_gradient = 0) )
-  
-  #
-  # Removing useless modules for sampling
-  #
-  modules_list = getModules(learner)
-  mod_toremove=[]
-  for module in modules_list:
-      if isModule( module, 'RBM') == False:
-         mod_toremove.append(module)
-  for module in mod_toremove:
-      modules_list.remove(module)
-  
-  
-  RBMnetwork = pl.NetworkModule(
-                          modules = modules_list,
-                          connections = connections_list_down,
-                          ports = ports,
-                          # to avoid calling the forget() method in ModuleLearner                          
-                          random_gen = pl.PRandom( seed = 1827 ),
-                          # Hack from Olivier
-                          save_states = 0
-                         )
-
-  
-  RBMmodel = pl.ModuleLearner(
-                              cost_ports = [],
-                              target_ports = [],
-                              module = RBMnetwork
-                           )
-
-
-  RBMmodel.setTrainingSet(pl.AutoVMatrix(inputsize=NH, targetsize=0, weightsize=0), False)
-
-  screen=init_screen(Nim)
-  random.seed(1969)
-
-  for i in range(len(RBMmodel.module.modules)):
-    module = RBMmodel.module.modules[i]
-    if isModule( module, 'RBM'):
-       RBMmodel.module.modules[i].compute_contrastive_divergence = False
-       if module.name == top_RBM_name:
-          RBMmodel.module.modules[i].n_Gibbs_steps_per_generated_sample = gibbs_step
-          top_RBM = RBMmodel.module.modules[i]
-
-  while True:
- 
-   init_hidden=[random.randint(0,1) for i in range(NH)]
-   
-   c = draw_image( RBMmodel.computeOutput(init_hidden), screen )
-   if c<0:
-      continue
-   elif c>0:
-      top_RBM.n_Gibbs_steps_per_generated_sample = c
-      
-   while True:
-       c = draw_image( RBMmodel.computeOutput([]) , screen)
-       if c<0:
-          break
-       elif c>0:
-          top_RBM.n_Gibbs_steps_per_generated_sample = c
-    

Modified: trunk/python_modules/plearn/learners/modulelearners/network_view.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/network_view.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -1,13 +1,9 @@
 #!/usr/bin/env python
 
-import sys
-import os, os.path
-
-from plearn.pymake.pymake import *
-
 try:
   from plearn.pyext import *
 except:
+  from plearn.pymake.pymake import *
   PLEARNDIR = os.environ.get('PLEARNDIR', os.getcwd())
   PLEARNDIRpyext = os.path.join(PLEARNDIR,'python_modules','plearn','pyext')
   PLEARNDIRpyextOBJ =  os.path.join(PLEARNDIRpyext,'OBJS')
@@ -244,11 +240,16 @@
     os.system('kuickshow '+output_name+' &')
 
 if __name__ == '__main__':
+    import sys
+    import os, os.path
 
-    if len(sys.argv) == 2:
-       inputname  = sys.argv[1]
-    else:
-       inputname  =  '/u/louradoj/PRGM/babyAI/dbn/convNet.py'
+
+    if len(sys.argv) <> 2:
+       print "Usage:\n\tpython "+sys.argv[0]+" mylearner.ext"
+       print "Purpose:\n\tDraw the graph of a network implemented/saved in a file mylearner.ext\n\twith extension (.ext) .py .pyplearn or .psave"
+       sys.exit(0)
+    
+    inputname  = sys.argv[1]
     output_extension = '.network.jpeg'
     output_name = os.path.splitext(inputname)[0]
     input_extension = os.path.splitext(inputname)[1]


Property changes on: trunk/python_modules/plearn/learners/modulelearners/sampler
___________________________________________________________________
Name: svn:ignore
   + *pyc


Added: trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,98 @@
+from pygame import *
+import math
+import sys
+
+EXITCODE = -2
+NEXTCODE = -1
+
+def pause():
+   c = sys.stdin.readline()
+   if c.strip() == 'q' or  c.strip() == 'x' or c.strip() == 'Q' or c.strip() == 'X':
+      return EXITCODE
+#      raise SystemExit
+   if c.strip() == 'n' :
+      return NEXTCODE
+   try: return int(c.strip())
+   except: return 0
+
+def init_screen(Nim,zoom_factor):
+    init()
+    width = int(math.sqrt(Nim*1.0))
+    if width**2 != Nim:
+       width = int(math.sqrt(Nim*1.0))+1
+#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
+    width *= zoom_factor
+    return display.set_mode([width, width])
+
+def draw_image(values_in_01,screen,zoom_factor):
+    """ Draw a 2D image where the gray level corresponds to a value scaled in [0,1]
+        (a warning is given when at least one of the value does not lie in the interval)
+        - values_in_01 : list of values in [0,1]
+        - screen  : output of init_screen()
+	- zoom_factor : int > 0
+    """
+    GiveWarning=True
+    
+    Nim=len(values_in_01)
+    width = int(math.sqrt(Nim*1.0))
+    if width**2 != Nim:
+       width += 1
+#       raise TypeError, "This code only deals with square images\n(and image size "+str(Nim)+" is not the square of an integer)"
+    width *= zoom_factor
+    surface = Surface((width, width),0,8)
+    surface.set_palette([(i,i,i) for i in range(2**8)])
+    for x in range(width/zoom_factor):
+       for y in range(width/zoom_factor):
+           value = values_in_01[x*width/zoom_factor+y]
+	   if value < 0. or value > 1.:
+	      if GiveWarning:
+	         GiveWarning=False
+	         print "Warning: In draw image : value "+str(value)+" is not in [0,1]"
+	      value = min(max(0.,value),1.)
+           graycol = int(255.0*value)
+           for i in range(zoom_factor):
+               for j in range(zoom_factor):
+                   surface.set_at((x*zoom_factor+i,y*zoom_factor+j),(graycol,graycol,graycol,255))
+    screen.blit(surface, (0,0))
+    display.update()
+    return pause()
+
+#
+# Weights will be scaled in [0...255] such that:
+# - the middle gray (127) corresponds to 0
+# - a same variation in the gray level (negative, or positive)
+#   correponds to a same variation
+#
+def draw_normalized_image(weights,screen,zoom_factor):
+    """ Draw a 2D image where the gray level corresponds to a (scaled) weight
+        ( gray represents a null weigth)
+        - weights : list of real numbers, where 0 is a special case
+        - screen  : output of init_screen()
+	- zoom_factor : int > 0
+    """
+
+    MAX=max(weights)
+    MIN=min(weights)
+    MAX=2*max(MAX,-MIN)
+
+    for i in range(len(weights)):
+        weights[i] = weights[i]/MAX+0.5
+    return draw_image(weights,screen,zoom_factor)
+
+def max_matrix(array):
+    print array[0]
+    raise SystemExit
+    MAX = max(array[0])
+    for vec in array:
+        maxtmp=max(vec)
+        if maxtmp > MAX:
+	   MAX = maxtmp
+    return MAX
+def min_matrix(array):
+    MIN = min(array[0])
+    for vec in array:
+        mintmp=min(vec)
+        if mintmp < MIN:
+	   MIN = mintmp
+    return MIN
+

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
===================================================================
(Binary files differ)


Property changes on: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.network.jpeg
===================================================================
(Binary files differ)


Property changes on: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.network.jpeg
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,281 @@
+import os, os.path
+import sys
+import time
+import datetime
+import math
+
+#from pdb import *
+
+#from plearn.pyplearn import *
+#from plearn.learners.autolr import *
+from plearn.learners.modulelearners import *
+   
+
+def basename_withoutExt(name):
+    return '.'.join(os.path.basename(name).split('.')[:-1])
+
+   
+
+##############################
+# Characteristics of the data
+
+# Default parameters values
+# (see below for description)
+
+plarg_defaults.databasePath                  = '/cluster/pauli/data/babyAI/textual_v3'
+plarg_defaults.Encoding                      = '3gram01'
+plarg_defaults.Size                          = 32
+plarg_defaults.Nobj                          = 1 # 1, 2, 3, 4
+plarg_defaults.ImageType                     = 'gray' # 'gray', 'gray_norot' (without rotation of objects)
+plarg_defaults.trainNsamples                 = 10000
+plarg_defaults.unsupervised_trainNsamples    = 1250000 # 10000 250000 1250000
+plarg_defaults.testNsamples                  = 5000
+plarg_defaults.validNsamples                 = plargs.testNsamples
+plarg_defaults.Extension                     = 'vmat'
+
+databasePath                  = plargs.databasePath                       # Directory where are the datasets'files
+Encoding                      = plargs.Encoding                   # layerType of Encoding (onehot, 1gram, 2gram, 3gram, ...)
+Size                          = plargs.Size                       # Image width/height
+Nobj                          = plargs.Nobj                       # Number of objects in the image
+ImageType                     = plargs.ImageType                  # Color encoding (gray, color) and other features (no rotation...)
+trainNsamples                 = plargs.trainNsamples              # number of training samples
+unsupervised_trainNsamples    = plargs.unsupervised_trainNsamples # number of training samples
+testNsamples                  = plargs.testNsamples               # number of samples to test
+validNsamples                 = plargs.validNsamples              # number of validation samples
+Extension                     = plargs.Extension                  # extension of the input files
+
+
+# Datasets filenames
+
+unsupervised_trainFilename = databasePath+'/BABYAI_'+ImageType+'_'+str(unsupervised_trainNsamples)+'x'+str(Nobj)+'obj_'+str(Size)+'x'+str(Size)+'.color-size-location-shape.train.'+Encoding+'.'+Extension   
+if os.path.isfile(unsupervised_trainFilename) == False:
+   raise EOFError, "CANNOT find "+unsupervised_trainFilename
+
+unsupervised_trainSet = pl.AutoVMatrix(
+            specification = unsupervised_trainFilename
+            )
+validSet = unsupervised_trainSet
+
+sys.path.append(databasePath) # the module to read sizes is in the same directory as data...
+from read_sizes import *
+imageSize, textSize, nClasses = read_sizes(unsupervised_trainFilename)
+inputSize = imageSize + textSize
+
+##############################
+
+## default values, see below for descriptions
+
+plarg_defaults.NH1                   = 500
+plarg_defaults.NH2                   = 500
+plarg_defaults.NH3                   = 500
+plarg_defaults.batchSize             = 50
+plarg_defaults.layerType             = 'gaussian'
+plarg_defaults.unsupervised_nStages  = int(plargs.unsupervised_trainNsamples)
+plarg_defaults.supervised_nStages    = 100  # /!\ see after: supervised_nStages   *= trainNsamples
+plarg_defaults.nStagesStep           = 5
+plarg_defaults.MDS                   = 10
+plarg_defaults.LR_CDiv               = 0.01
+plarg_defaults.LR_CDiv1              = float(plargs.LR_CDiv)
+plarg_defaults.LR_CDiv12             = float(plargs.LR_CDiv)
+plarg_defaults.LR_CDiv2              = float(plargs.LR_CDiv)
+plarg_defaults.LR_CDiv3              = float(plargs.LR_CDiv)
+plarg_defaults.LR_GRAD_UNSUP         = 0.003
+plarg_defaults.LR_GRAD_UNSUP1        = float(plargs.LR_GRAD_UNSUP)
+plarg_defaults.LR_GRAD_UNSUP2        = float(plargs.LR_GRAD_UNSUP)
+plarg_defaults.LR_GRAD_UNSUP3        = float(plargs.LR_GRAD_UNSUP)
+plarg_defaults.LR_SUP                = 0.01
+plarg_defaults.L2wd_SUP              = 1e-7
+plarg_defaults.seed                  = 6343
+plarg_defaults.nGibbs                = 1
+
+NH1                     = int(plargs.NH1)                        # num units for the image part
+NH2                     = int(plargs.NH2)                        # num units, 2nd hid layer (image part)
+NH3                     = int(plargs.NH3)                        # num units, 2rd hid layer
+batchSize               = int(plargs.batchSize)                # num of samples in the minibatch
+unsupervised_nStages    = int(plargs.unsupervised_nStages)                # total number of samples to see (unsupervised phase)
+supervised_nStages      = int(plargs.supervised_nStages)                # total number of samples to see (supervised phase)
+nStagesStep             = int(plargs.nStagesStep)                #
+layerType               = plargs.layerType
+LR_CDiv                 = float(plargs.LR_CDiv)                # unsup. lr
+# lotsa optional learning rates that one can specify for each layer
+LR_CDiv1                = float(plargs.LR_CDiv1)        # unsup. lr
+LR_CDiv12               = float(plargs.LR_CDiv12)        # unsup. lr
+LR_CDiv2                = float(plargs.LR_CDiv2)                # unsup. lr
+LR_CDiv3                = float(plargs.LR_CDiv3)                # unsup. lr
+LR_GRAD_UNSUP           = float(plargs.LR_GRAD_UNSUP)         # super. lr
+# same thing with the supervised ones
+LR_GRAD_UNSUP1          = float(plargs.LR_GRAD_UNSUP1) # super. lr
+LR_GRAD_UNSUP2          = float(plargs.LR_GRAD_UNSUP2)         # super. lr
+LR_GRAD_UNSUP3          = float(plargs.LR_GRAD_UNSUP3)        # super. lr
+LR_SUP                  = float(plargs.LR_SUP) # super. lr
+MDS                     = int(plargs.MDS)                        # minim. # of non-decreas. steps
+seed                    = int(plargs.seed)                        
+L2wd_SUP                = float(plargs.L2wd_SUP)
+LR_batchfactor          = math.sqrt(int(plargs.batchSize))
+nGibbs                  = int(plargs.nGibbs)
+supervised_nStages *= trainNsamples
+
+##############################
+
+expdir =  os.environ.get('PLEARNDIR', os.getcwd())+'/python_modules/plearn/learners/modulelearners/sampler/example/data'
+init_DBN_filename = expdir+'/DBN-3RBM.babyAI-1obj.psave'
+
+##############################
+
+def rbm_layer(layer_type,nunits):
+        if layer_type=='gaussian':
+                return pl.RBMGaussianLayer(        
+                                        size = nunits
+                                )
+        else:
+                return pl.RBMBinomialLayer(
+                                             size = nunits
+                                )
+
+def rbm_module(name,vis_size,hid_size,lr_GRAD_UNSUP,lr_CDiv,ng,layer_type,compute_cd):
+        x =  pl.RBMMatrixConnection(
+                                down_size = vis_size,
+                                up_size = hid_size
+                        )
+
+        return pl.RBMModule(
+                        name = name,
+                        visible_layer = rbm_layer(layer_type,vis_size),
+                        hidden_layer = rbm_layer('binomial',hid_size),
+                        connection = x,
+                        reconstruction_connection = pl.RBMMatrixTransposeConnection(rbm_matrix_connection = x),
+                        n_Gibbs_steps_CD = ng,
+                        grad_learning_rate = lr_GRAD_UNSUP,
+                        cd_learning_rate = lr_CDiv,
+                        compute_contrastive_divergence = compute_cd
+                )
+        
+        
+unsupervised_modules = []
+unsupervised_connections = []
+
+##########################
+unsupervised_modules.append(pl.SplitModule(
+                                                        name = 'split',
+                                                        down_port_name = 'input',
+                                                        up_port_names = ['out1', 'out2'],
+                                                        up_port_sizes = [imageSize, textSize]
+                                                ))
+unsupervised_modules.append( rbm_module('rbm_gaussian_inputs', imageSize, NH1, LR_GRAD_UNSUP1 * LR_batchfactor, LR_CDiv1 * LR_batchfactor, nGibbs, layerType, True ) )
+
+unsupervised_connections.append(pl.NetworkConnection(source = 'split.out1',
+                                        destination = 'rbm_gaussian_inputs.visible'))
+
+##########################
+
+unsupervised_modules.append( rbm_module('rbm_binomial',        NH1, NH2,   LR_GRAD_UNSUP2 * LR_batchfactor, LR_CDiv2 * LR_batchfactor, nGibbs, 'binomial', True))
+unsupervised_connections.append(pl.NetworkConnection(source = 'rbm_gaussian_inputs.hidden.state',
+                                        destination = 'rbm_binomial.visible'))
+
+unsupervised_modules.append( rbm_module('rbm_binomial_top',        NH2, NH3,   LR_GRAD_UNSUP3 * LR_batchfactor, LR_CDiv3 * LR_batchfactor, nGibbs, 'binomial', True))
+unsupervised_connections.append(pl.NetworkConnection(source = 'rbm_binomial.hidden.state',
+                                        destination = 'rbm_binomial_top.visible'))
+
+##########################                
+unsupervised_modules.append(pl.LinearCombinationModule(
+                                                        name = 'total_cost',
+                                                        weights = [
+                                                                   1.0 / getModule(unsupervised_modules,'rbm_gaussian_inputs').connection.down_size,
+                                                                   1.0 / getModule(unsupervised_modules,'rbm_binomial').connection.down_size,
+                                                                   1.0 / getModule(unsupervised_modules,'rbm_binomial_top').connection.down_size
+                                                                   ]
+                                                ))
+unsupervised_connections.append(pl.NetworkConnection(source = 'rbm_gaussian_inputs.reconstruction_error.state',
+                                        destination = 'total_cost.in_1',
+                                        propagate_gradient = 1))
+unsupervised_connections.append(pl.NetworkConnection(source = 'rbm_binomial.reconstruction_error.state',
+                                        destination = 'total_cost.in_2',
+                                        propagate_gradient = 1))
+unsupervised_connections.append(pl.NetworkConnection(source = 'rbm_binomial_top.reconstruction_error.state',
+                                        destination = 'total_cost.in_3',
+                                        propagate_gradient = 1))
+
+##########################
+                
+unsupervised_ports = [
+          ('input', 'split.input'),
+          ('output', 'rbm_binomial_top.hidden.state'),
+          ('total_cost', 'total_cost.output'),
+          ('reconstruction_error_1','rbm_gaussian_inputs.reconstruction_error.state'),
+          ('reconstruction_error_2','rbm_binomial.reconstruction_error.state'),
+          ('reconstruction_error_3','rbm_binomial_top.reconstruction_error.state'),
+          ('contrastive_divergence_1','rbm_gaussian_inputs.contrastive_divergence'),
+          ('contrastive_divergence_2','rbm_binomial.contrastive_divergence'),
+          ('contrastive_divergence_3','rbm_binomial_top.contrastive_divergence')
+        ]
+
+
+unsupervised_module = pl.NetworkModule(
+                          modules = unsupervised_modules,
+                          connections = unsupervised_connections,
+                          ports = unsupervised_ports
+                         )
+                         
+unsupervised_learner = pl.ModuleLearner(
+                              module = unsupervised_module,
+                              cost_ports = ['total_cost', 
+                                                   'reconstruction_error_1','reconstruction_error_2', 'reconstruction_error_3',
+                                            'contrastive_divergence_1', 'contrastive_divergence_2', 'contrastive_divergence_3' 
+                                           ],
+                              target_ports = [],
+                              batch_size = batchSize,
+                              nstages = unsupervised_nStages,
+                              expdir = expdir
+                             )
+
+unsupervised_statnames = [
+                        'E[test1.E[reconstruction_error_1]]',
+                        'E[test1.E[contrastive_divergence_1]]',
+                        'E[test1.E[reconstruction_error_2]]',
+                        'E[test1.E[contrastive_divergence_2]]',
+                        'E[test1.E[reconstruction_error_3]]',
+                        'E[test1.E[contrastive_divergence_3]]',
+                        'E[test1.E[total_cost]]'
+           ]
+
+unsupervised_cost = unsupervised_statnames.index('E[test1.E[total_cost]]')
+
+oracle = pl.EarlyStoppingOracle(
+                      option = 'nstages',
+                     values = [ str(unsupervised_nStages)], # range = [ unsupervised_nStages, unsupervised_nStages+1 ],
+)
+                        
+unsupervised_strategy = [
+        pl.HyperOptimize(
+                  which_cost = str(unsupervised_cost),
+                  provide_tester_expdir = 1,
+                  oracle = oracle
+          )
+       ]
+unsupervised_hyperlearner = pl.HyperLearner(
+   tester = pl.PTester(
+       splitter = pl.ExplicitSplitter(splitsets = TMat(1,2,[ unsupervised_trainSet, validSet])),
+       statnames = unsupervised_statnames,
+       save_learners = 0,
+       save_initial_tester = 1,
+       provide_learner_expdir = 1),
+   option_fields = [ 'nstages' ],
+   dont_restart_upon_change = [ 'nstages' ],
+   learner = unsupervised_learner,
+   strategy = unsupervised_strategy,
+   provide_strategy_expdir = 1,
+   save_final_learner = 0,
+   expdir = expdir
+   )
+
+
+
+print "Training UNSUPERVISED hyperlearner... (seeing "+str(int(unsupervised_nStages))+" examples)\n\tin"+init_DBN_filename
+
+if os.path.isfile(init_DBN_filename) == False:
+   unsupervised_hyperlearner.train()
+   print "... OK. Saving in "+init_DBN_filename
+   unsupervised_learner.save(init_DBN_filename,'plearn_binary')
+else:
+   unsupervised_learner=loadObject(init_DBN_filename)
+   print "\nWARNING: unsupervised learner already trained in:\n\t"+expdir+"\n\n"

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat/0.data
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat/0.data	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat/0.data	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,128 @@
+^-??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K?B???>???>
?>???>???>???>???>???>???>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>J???>???>???>???>???>???>???>???>???>???>I???>???>???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>G???>???>???>???>???>???>???>F???>???>???>???>???>???>E???>???>???>???>???>D???>???>???>???>
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??? ?d?????Y????*?D??*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*????*?B??*???*????>?!?,????1?^
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>?!
 ??>??*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J^
??>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>? ?-???>?
???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?E??*???*???*???*???*????*????*????*???>?!?,????1????*????*?B??*???*????*?F??*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*?!
 ??*???*???*????*?H??*???*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?B??*???*??I?>??7??????*&?
?*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????d????_^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@n??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>???<??9????J@???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@ ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^D??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K????
+???????????
+???M??!?u?????^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???!
 *????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>??>??:??????????????
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*?I??*???*???*???*???*???*???*???*???*?????s????^
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*?!
 ??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??F?????*@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@i???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>??>? ?-???>?@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H
?*???*???*?F??*???*???*???*???*???*?F??*???*???*???*???*???*?F??*???*???*???*???*???*?E??*???*???*???*???*?F??*???*???*???*???*???*?E??*???*???*???*???*?C??*???*???*?
?*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4????*?A??*????*?A??*????*?A??*????*?C??*???*???*????*?C??*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?C??*???*???*????*?C??*???*???*????*?C??*???*???*????*?A??*????*?A??*????*?A??*????>??D????4?????>A???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>????>F???>???>???>???>???>???>????>A???>???? ?d?????Y^
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????d????_^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 *???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??7??????++S??*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?D??*???*???*???*?
?*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?C??*???*???*????*?A??*??'??#?f????$?4
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*???>??;???'?@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>A???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^
+???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>? ?-???>??"???>D???>???>???>???>????>H???>???>???>???>???>???>???>???>????>L???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>G???>??!
 ?>???>???>???>???>???>????>C???>???>???>????>??<??9????J?!
 A??*?A
??*?C??*???*???*?E??*???*???*???*???*?E??*???*???*???*???*?E??*???*???*???*???*?F??*???*???*???*???*???*?F??*???*???*???*???*???*?F??*???*???*???*???*???*?F??*???*???*???*???*???*?F??*???*???*???*???*???*?F??*???*???*???*???*???*?E??*???*???*???*???*?E??*???*???*???*???*?E??*???*???*???*???*?D??*???*???*???*?B??*???*?
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^x???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!?u?????^:???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????s????
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??>????%
???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??F?????)^
??*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J^
>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4??????????????W??9??????? ?*^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??:?????^
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4^
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???!
 *???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??? ?d?????Y^
>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????f???*?4@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??F?????,?????????????/??!?u??????.??*?A??*????*?I??*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?A??*???????f???*?4^
??*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J????*????*?A??*????*?A??*????*?B??*???*????*?B??*???*????*?C??*???*???*????*?D??*???*???*???*????*?E??*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?E??*???*???*???*!
 ???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?C??*???*???*????*?B??*???*????*?A??*????*?? ??*????>??D????4???*?I??*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*??!
 ?*???*???*??A?>?!?,????1?????*?D??*???*???*???*????*?H??*??!
 ?*???*??
?*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?C??*???*???*????*?B??*???*????*?B??*???*????*????*??C?>??D????4^(??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???9??N???K^???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?A??*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??0???6?@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!?u??????????>????>B???>???>????>C???>???>???>????>E???>???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>E???>???>???>???>???>????>D???>???>???>???>????>C???>???>???>????>?B?>???!???????"
?>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>J???>???>???>???>???>???>???>???>???>???>L???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????s???????*????*?C??*???*???*????*?F??*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????!
 *?C??*???*???*??z?>?!?,????1?@o???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????!
 >???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????d????_^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>????>^P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???[????Y^
>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*?!
 ??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H?????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>K???>???>???>???>???>???>???>?!
 ??>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>?!
 ???>I???
>???>???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>F???>???>???>???>???>???>????>D???>???>???>???>????>B???>???>????>A???>????>?[?>??????9?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^8??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!?u?????^
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9??????+?n?????????????????>??G???H^F???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^3???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_?L??*????*?A??*????*?B??*???*????*?C??*???*???*????*?D??*???*???*???*????*?E??*???*???*???*???*????*?F??*!
 ???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?H?!
 ?*???*??
?*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?C??*???*???*????*?B??*???*????*?A??*????*?? ??*??z?>??7??????,?R???????????!?u?????^{???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_^-??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H?.?????????????
+?
+?????????????s????^
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*??? ?d?????Y@L???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????s????^
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??? ?d?????Y????*????*?I??*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?C??*???*???*????>??7??????,^
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4^
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
  ?d?????Y?B???>???>M???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>B???>???>
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_?B???>???>
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?B??*???*????*?A??*????>??D????4^
?*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*??? ?d?????Y^+??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H@r??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *????*?S
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????[????YA???>
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>A???>????[???_^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??>????%^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??>????%?????	??????
+?
+?
+?	??????>??G???H.B???>???>
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^4??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???KxA??*?
?*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?I??*???*???*???*???*???*???*???*???*?E??*???*???*???*???*?
>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??7??????,?8????
+?????????Y??!?u?????^
?>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>??>??7??????*aC??*???*???*?
+?	????????? ?V?>??G???H????????????>?????????%?????>????>A???>????>A???>????>B???>???>????>C???>???>???>????>C???>???>???>????>E???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>F???>???>???>???>???>???>????>C???>???>???>???>???!???????"????*????*?A??*????*?C??*???*???*????*?E??*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?C??*???*???*????*?C??*???*???*????*?B??*???*????*?A??*????*?? ??*?????#?f????$?4?????????????????&?>??:???????`???>C???>???>???>????>G???>???>???>???>???>???>???>????>K???>???>???>???>???>???>???>???>???>???>???>????>L???>???>???>???>???>???>???>???>???>???>???>???>????>K???>???>???>???>???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>C???>???>???>???>??(????"?"@w???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ????>Q??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>M???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>O!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ??>@????
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>????????M???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^s???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??F?????,^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???!
 *???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4@???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??0???6?^4???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_??????
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????!
 *?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@B??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K?????>A???>????>C???>???>???>????>E???>???>???>???>???>????>G???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>K???>???>???>???>???>???>???>???>???>???>???>????>M???>???>???>???>???>???>???>???>???>???>???>???>???>????>L???>???>???>???>???>???>???>???>???>???>???>???>????>M???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>????>J???>???>???>???>???>???>???>???>???>???>????>H???>???>!
 ???>???>
???>???>???>???>????>E???>???>???>???>???>????>C???>???>???>????>?y?<??9????J@%??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????s????@)???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???[????Y?H????????????>??F?????*^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>C???>???>???>B???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>O??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>B???>???>C???>???>???>^t???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H?B??*???*?
?*???*???*???*???*???*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?K??*???*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?E??*???*???*???*???*?C??*???*???*?
+??????????s????@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??F?????*,B???>???>E???>???>???>???>???>F???>???>???>???>???>???>B???>???>s??<??9????J?^???>????>J???>???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>I???!
 >???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>?????d????_^
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4?C???>???>???>E???>???>???>???>???>H???>???>???>???>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>E???>???>???>???>???>
C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>A???>?	?<??9??????*
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K???????????????3??9??.???? ?*?i?????????????^?>??G???HN??????????????????
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??!
 ?>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>!
 ???>??*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????d????_^
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>?!
 ??>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????? ?)@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>???7??????@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*?@???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!?u?????^
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9??????+0?????????????
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??? ?d?????Y
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?L??*???*???*???*???*???*???*???*???*???*???*???*?
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>L???>???>???>???>???>???>???>???>???>???>???>???>I???>???>???>???>???>???>???>???>???>G???>???>???>???>???>???>???>E???>???>???>???>???>C???>???>???>9?? ?d?????Y@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???9??.?!
 ???!?)^
????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>????>A???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^<???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_?????>????>C???>???>???>????>E???>???>???>???>?!
 ??>????>E???>???>???>???>???>????>D???>???>???>???>????>E???>?!
 ??>???>?
??>???>????>D???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>D???>???>???>???>????>B???>???>????>A???>??<??9????J^O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K
?>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>???&??????)
+D??*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?D??*???*???*???*???#?f????$?4?$??*?B??*???*????*?D??*???*???*???*????*?E??*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?J??*???!
 *???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?B??*???*????*?A??*????*??}?>??D????4?q????????	?
+???	?? ???>??G???H^
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>??>??-???????O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??.???? ?*2A??*?G??*???*???*???*???*???*???*?A??*?I??!
 *???*???*???*???*???*???*???*???*?I??*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?I??*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?F??*???*???*???*???*???*????>??D????4^0??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??>????%@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^5??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>?????????%@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+^ ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!?u?????@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>E???>???>???>???>???>G???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >????s?
???^*??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*????*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????[????Y^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>?!?,????1?^
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9??????+
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@ ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@D??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>? ?-???>?^
??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>A???>Z???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>A???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H^
*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????f???*?4@/???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??0???6?A??*?D??*???*???*???*?G??*???*???*???*???*???*???*?H??*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?D??*???*???*???*?A??*?*??>???'???????v????????????
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??:??????^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@ ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>???2?????)^
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????d????_?R???>J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????!
 >J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>???? ?d?????Y????*?B??*???*????*?E??*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?B??*???*????*??$?>???)??????*IB???>???>F???>???>???>???>???>???>I???>???>???>???>???>???>???>???>???>J???>???>???>???>???>???>???>???>???>???>F???>???>???>???>???>???>B???>???>????d????_KA???>
???J?'??????;????s????@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??0???6?
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???HK???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>@???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>???7??????^
???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J^
*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????f???*?4^
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>???)???????????
+?
+?	????????????????{?>??G???HL??*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????s?????1???>????>B???>???>????>C???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>E???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>E???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>E???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>E???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>E???>???>???>???>???>????>F???>???>???>??!
 ?>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>!
 ????>D??
?>???>???>???>????>B???>???>????>A???>??????d????_)?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>?!
 ??>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^-??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N?!
 ??K@3??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^5???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???[????Y
???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>L???>???>???>???>???>???>???>???>???>???>???>???>F???>???>???>???>???>???>y??<??9????J????>A???>????>B???>???>????>B???>???>????>C???>???>???>????>B???>???>????>B???>???>????>A???>????>????>A???>????>B???>???>????>???<??9????J??????
+?	?	???????????? ???>??G???HG???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>G???>???>???>???>???>???>???>
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 ?>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K^
*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>???'??????)??????????
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>? ?-???>?@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^f???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>??>??
?3???????*????
+???????????	?????u?>? ?-???>?PJ??*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?G????f???*?4?????>????>A???>????>C???>???>???>????>C???>???>???>????>B???>???>????>A???>????>B???>???>????>A???>????>?@???>??<??9????J
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K^=???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H^
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????f???*?4@L??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???HmC???>???>???>A???>G???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>G???>???>???>???>???>???>???>G???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>G???>???>???>???>???>???>???>A???>???<??9????J^
???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>???+??????*??????????????????
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????[????Y^
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????f???*?4????*????*????*?D??*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?D??*???*???*???*????*????*????>??D????4^
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??7??????)^
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??(????"?"?V???>A???>????>C???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>?L?? ?d?????Y^
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*????>???>???>???>???>??*????>??*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J
??*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?L??*???*???*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?I??*???*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?E??*???*???*???*???*?D??*???*???*???*?B??*???*?
+??????????
+????>? ?,?????%?^F???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>A???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>A???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!?u?????@1??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*????*?^G??*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??>????%?C???>???>???>E???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>G???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>G???>???>???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>E???>???>???>???>???>C???>???>???>
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*??!
 ?*???*??
?*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4@9??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@ ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????s????^n??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??F?????*@B???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??F?????,?D???>A???>????>A???>????>A???>????>A???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>A???>????>A???>????>A???>????>A???>?V????d????_?????>D???>???>???>???>????>H???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>???!
 ?>F???>???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>D???>???>???>???>????>C???>???>???>????>C???>???>???>????>B???>???>????>B???>???>????>B???>???>????>A???>????>A???>????>A???>????>????>????>????>?5?<??9????? ?)^
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4?u???>????>C???>???>???>????>B???>???>????>A???>?<?<??9??????+@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?A??*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?I??*???*???*???*???*???*???*???*???*?????s?????$??*?G??*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????>???+??????*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!?u?????^
??>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4????????|?>??;???'??A???>A???>M???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>P???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>A???>A???>
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????s????GA??*?
???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?K??*???*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?A??*?E??*???*???*???*???*?
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!?u?????^	??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?!
 T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^A??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K^
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*!
 ???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4
*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?K??*???*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?B??*???*?A??*?
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????f???*?4^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????f???*?4?????>A???>????>H???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>L???>???>???>???>???>???>???>???>???>???>???>???>????>L???>???>???>???>???>???>???>???>???>???>???>???>????>I???>???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>A???>????>?f?? ?d?????YX???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@k???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_
??f???*?4A???>
*???*???*????*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??0???6?t???????????????
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???[????Y^
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9??????*%?
*???*???*???*?I??*???*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?E??*???*???*???*???*?C??*???*???*?
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???HA??*?
??*?G??*???*???*???*???*???*???*?F??*???*???*???*???*???*?F??*???*???*???*???*???*?E??*???*???*???*???*?E??*???*???*???*???*?E??*???*???*???*???*?D??*???*???*???*?D??*???*???*???*?C??*???*???*?
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>???&???????^f??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^+??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??.???? ?*?N??*????*????*????*?A??*????*?A??*????*?B??*???*????*?B??*???*????*?C??*???*???*????*?C??*???*???*????*?C??*???*???*????*?D??*???*???*???*????*?D??*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?D!
 ??*???*???*???*????*?B??*???*????>??D????4?????>C???>???>???>????>D???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>F???>???>???>???>???>???>????>C???>???>???>?g?<???#??????*????>A???>????>A???>????>A???>????>B???>???>????>B???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>B???>???>????>B???>???>????>????>???<??9??????,^
??>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???!
 *???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@d??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??0???6?^??*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????s????I???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>? ?,?????&?^
*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??-???????
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*!
 ???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4^M???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@!
 ???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K????*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?D??*???*???*???*????*?B??*???*????*?A??*??[?>??D????4^
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??? ?d?????Y@E???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!?u?????????*????*????*????*????*?A??*????*?A??*????*?B??*???*????*?B??*???*????*?C??*???*???*????*?C??*???*???*????*?C??*???*???*????*?C??*???*???*????*?D??*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????>??D????4
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^%???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H@b??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@|??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!?u?????^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?A??*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?B??*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?B??*???*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H?????>B???>???>????>C???>???>???>????>C???>???>???>??>??(????"?"^
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*!
 ???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@ ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>??>??G???H
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^;???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!?u?????JE???>???>???>???>???>F???>???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>F???>???>???>???>???>???>
?*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?C??*???*???*????*?A??*??p????f???*?4
???*???*?B??*???*?
???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>??>??D????4
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>????[???_^V??*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?!
 W??*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K?A???>
???>???>???>???>???>???>???>???>???>???>J???>???>???>???>???>???>???>???>???>???>F???>???>???>???>???>???>D???>???>???>???>????d????_^-???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H@???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??>????%
?H??*???*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?G??*???*???*???*???*???*???*?E??*???*???*???*???*?D??*???*???*???*?C??*???*???*?C??*???*???*?B??*???*?
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????s????^
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>?!
 ??>???>?
??>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J
*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^!
 	??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*?????s?????P??*????*?B??*???*????*?D??*???*???*???*????*?F??*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?C??*???*???*????*?A??*??g?>?!?,????1?^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^m???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???[????Y^P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*?
+[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@ ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?E??*???*???*???*???*???>??G???H^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>????[???_^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K^
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*!
 ???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4????*?A??*????*?C??!
 *???*???*????*?E??*???*???*???*???*????*?H??*???*???*???*???*?!
 ??*???*?
??*????*?J??*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?K??*???*???*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?B??*???*????*???>?!?,????1?^
?>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4^
??>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???!
 *???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4^
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????f???*?4@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H^
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J^
???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H?????>A???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>A???>???? ?d?????Y^
???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????d????_@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???[????Y@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*?R?
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>? ?-???>?@D??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H??????????????????U?>??G???H
???*???*???*?E??*???*???*???*???*?D??*???*???*???*?
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????[???_@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>?????????%?I?	??????????>??G???H?Q??*????*????*?A??*????*?C??*???*???*????*?C??*???*???*????*?E??*???*???*???*???*????*?F??*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*?!
 ??*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*?!
 ???*?H??
*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?C??*???*???*????*?A??*????*??f?>??7??????*^
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4????*????*?B??*???*????*?C??*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?D??*???*???*???*????*?C??*???*???*????*?B??*???*????*?A??*??????f???*?4?N??*?A??*????*?B??*???*????*?E??*???*???*???*???*????*?C??*???*???*????*?A??*?? ??*???>??:?????@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>W?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>??!?u????????????????????
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K
?>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>O???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>??>??G???H
>???>???>
>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????s????^
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
?>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??? ?d?????YjA???>D???>???>???>???>F???>???>???>???>???>???>D???>???>???>???>A???>????d????_
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ?>??D???
?4L??????????????????????????[???_^
??*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>??>??
:??????????????>??F?????,^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9???????!?)?????>K???>???>???>???>???>???>???>???>???>???>???>????>K???>???>???>???>???>???>???>???>???>???>???>????>K???>???>???>???>???>???>???>???>???>???>???>????>K???>???>???>???>???>???>???>!
 ???>???>???>???>????>K???>???>???>???>???>???>???>???>???>???>!
 ???>????
>K???>???>???>???>???>???>???>???>???>???>???>????>K???>???>???>???>???>???>???>???>???>???>???>????>J???>???>???>???>???>???>???>???>???>???>??????d????_^
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??????9??'???>B???>???>????>G???>???>???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>????>C???>???>???>????>???>??(????"?"^&???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>????>@p???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K
>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>E???>???>???>???>???>A???>???<??9????J@>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^a???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!?u??????N???>????>B???>???>????>C???>???>???>????>E???>???>???>???>???>????>H???>???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>E???>???>???>???>???>????>E???>???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>C???>???>???>????>C???>???>???>????>C???>???>???>????>B???>???>????>B???>???>????>B???>???>????>A???>????>A???>????>? ???>???<??9????J^
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^l???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_DC???>???>???>D???>???>???>???>C???>???>???>D???>???>???>???>D???>???>???>???>D???>???>???>???>
?*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J????*?A??*????*?B??*???*????*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?F?!
 ?*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?F??*???*???*???*???*???*????*?E??*???*???*???*???*????*?E??*???*???*???*???*????*?A??*????*??D??#?f????$?4
>???>F???>???>???>???>???>???>C???>???>???>
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??>????%???????????????[???_?U???>C???>???>???>????>C???>???>???>????>C???>???>???>????>D???>???>???>???>????>E???>???>???>???>???>????>F???>???>???>???>???>???>????>F???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>F???>???>???>???>???>???>????>G???>???>???>???>???>???>???>????>E???>???>???>???>???>????>D???>???>???>???>????>D???>???>???>???>????>C???>???>???>????>C??!
 ?>???>???>????>B???>???>???<??9????J^
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??!
 ?>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???<???!??????*^
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??7??????*@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*????*?@o??*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????[???_
???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??9??N???K
+??
+??G?>???7??????^
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>????f???*?4^g??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^:??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!?u?????^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>^????>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??F?????*^
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???<??9????J
??>???>???>???>???>???>???>???>???>???>J???>???>???>???>???>???>???>???>???>???>I???>???>???>???>???>???>???>???>???>I???>???>???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>F???>???>???>???>???>???>F???>???>???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>D???>???>???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>????[???_?????????????????????n???[????Y?v??????	?	??	?	?	??	?	???	?	??	?	?	?????d?>??G???H@&???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H'A??*?
?*???*???*???*???*????*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??D????4@o???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^'???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????[???_@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^*??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???[????Y??? ?????????F?>??;???'?^
*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*!
 ???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4^
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4^
*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>??*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D!
 ????4?6??*????*?B??*???*????*?C??*???*???*????*?F??*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?J??*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?M??*???*???*???*???*???*???*???*???*???*???*???*???*????*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?L??*???*???*???*???*???*???*???*???*???*???*???*????*?I??*???*???*???*???*???*???*???*???*????*?H??*???*???*???*???*???*???*???*????*?E??*???*???*???*???*????*?C??*???*???*????*?A??*????>?!?,????1?@k???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!?u??????D???>???>???>???>E???>???>???>???>???>D???>???>???>???>D???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>D???>???>???>???>D???>???>???>???>E???>???>???>???>???>E???>???>???>???>???>D???>???>???>???>D???>???>?!
 ??>???>E???>???>???>???>???>E???>???>???>???>???>D???>???>?!
 ??>???>
E???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>? ?,?????#???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>??>??G???H@N??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?!
 Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*!
 ?@??*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????[????YU??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^J??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K?I??*????*?A??*????*?D??*???*???*???*????*?F??*???*???*???*???*???*????*?G??*???*???*???*???*???*???*????*?D??*???*???*???*????*????*?A??*??????f???*?4?E??*???*???*???*???*?I??*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?L??*???*???*???*???*???*???*???*???*???*???*???*?H??*???*???*???*???*???*???*???*?D??*???*???*???*?
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^n??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???9??N???K^Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?A??*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*?[??!
 *???*???
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??G???H^
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>??*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??D????4
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???[????Y?A???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>L???>???>???>???>???>???>???>???>???>???>???>???>J???>???>???>???>???>???>???>???>???>???>H???>???>???>???>???>???>???>???>F???>???>???>???>???>???>D???>???>???>???>B???>???>
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*?A??*!
 ?U??*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Q??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?R??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?S??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@J??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>? ?-???>?^??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?@??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?
+W??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?	X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?^m??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*
???*???*???*???*???*???*???*???*???*???*???*?????s?????????>A???>????>C???>???>???>????>E???>???>???>???>???>????>D???>???>???>???>????>E???>???>???>???>???>????>F???>???>???>???>???>???>????>B???>???>????>??>??(????"?"
+U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>P???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>M???>???>???>???>???>???>???>???>???>???>???>???>???>N???>???>???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>S???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>]???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>	X???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>T???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>U???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?V??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*????*?T??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?U??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Y??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?\??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^I??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*???*???*???*???*???*???*???9??N???K?H??*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?O??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?L??*???*???*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?I??*???*???*???*???*???*???*???*???*?H??*???*???*???*???*???*???*???*?F??*???*???*???*???*???*?E??*???*???*???*???*?C??*???*???*?B??*???*?
*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>??*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>??*???*???*???>??????9?^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>
???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>L???>???>???>???>???>???>???>???>???>???>???>???>K???>???>???>???>???>???>???>???>???>???>???>L???>???>???>???>???>???>???>???>???>???>???>???>Q???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>
+V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>^X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>????s????@???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?X??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?Z??*???*???*???*???*???*???*???*???*?!
 ??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???
?*?[??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?]??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????*?^???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*!
 ???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*??!
 ?*???*??
?*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?????[???_^???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>X???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	W???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	V???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>	Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>\???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>@???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>R???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???HLD??*???*???*???*?H??*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?L??*???*???*???*???*???*???*???*???*???*???*???*?L??*???*???*???*???*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?P??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?N??!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*?L??*???!
 *???*???
*???*???*???*???*???*???*???*???*?L??*???*???*???*???*???*???*???*???*???*???*???*?J??*???*???*???*???*???*???*???*???*???*?H??*???*???*???*???*???*???*???*?D??*???*???*???*?????f???*?4
>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*????>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>!
 ???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>??
?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??#?f????$?4[A??*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?N??*???*???*???*???*???*???*???*???*???*???*???*???*???*?M??*???*???*???*???*???*???*???*???*???*???*???*???*?%??>??:?????^????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>?
??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??!
 ?>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>A???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Y???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>[???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>Z???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>B???>???>@????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>?!
 ??>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???!
 >???>???
>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??>??G???H
?*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>???>???>??*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>???>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*????>???>??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???!
 *???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*?!
 ??*???*?
??*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???*???>??????9?
\ No newline at end of file

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat/indexfile
===================================================================
(Binary files differ)


Property changes on: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat/indexfile
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/fieldnames
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/fieldnames	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/fieldnames	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,1281 @@
+0	0
+1	0
+2	0
+3	0
+4	0
+5	0
+6	0
+7	0
+8	0
+9	0
+10	0
+11	0
+12	0
+13	0
+14	0
+15	0
+16	0
+17	0
+18	0
+19	0
+20	0
+21	0
+22	0
+23	0
+24	0
+25	0
+26	0
+27	0
+28	0
+29	0
+30	0
+31	0
+32	0
+33	0
+34	0
+35	0
+36	0
+37	0
+38	0
+39	0
+40	0
+41	0
+42	0
+43	0
+44	0
+45	0
+46	0
+47	0
+48	0
+49	0
+50	0
+51	0
+52	0
+53	0
+54	0
+55	0
+56	0
+57	0
+58	0
+59	0
+60	0
+61	0
+62	0
+63	0
+64	0
+65	0
+66	0
+67	0
+68	0
+69	0
+70	0
+71	0
+72	0
+73	0
+74	0
+75	0
+76	0
+77	0
+78	0
+79	0
+80	0
+81	0
+82	0
+83	0
+84	0
+85	0
+86	0
+87	0
+88	0
+89	0
+90	0
+91	0
+92	0
+93	0
+94	0
+95	0
+96	0
+97	0
+98	0
+99	0
+100	0
+101	0
+102	0
+103	0
+104	0
+105	0
+106	0
+107	0
+108	0
+109	0
+110	0
+111	0
+112	0
+113	0
+114	0
+115	0
+116	0
+117	0
+118	0
+119	0
+120	0
+121	0
+122	0
+123	0
+124	0
+125	0
+126	0
+127	0
+128	0
+129	0
+130	0
+131	0
+132	0
+133	0
+134	0
+135	0
+136	0
+137	0
+138	0
+139	0
+140	0
+141	0
+142	0
+143	0
+144	0
+145	0
+146	0
+147	0
+148	0
+149	0
+150	0
+151	0
+152	0
+153	0
+154	0
+155	0
+156	0
+157	0
+158	0
+159	0
+160	0
+161	0
+162	0
+163	0
+164	0
+165	0
+166	0
+167	0
+168	0
+169	0
+170	0
+171	0
+172	0
+173	0
+174	0
+175	0
+176	0
+177	0
+178	0
+179	0
+180	0
+181	0
+182	0
+183	0
+184	0
+185	0
+186	0
+187	0
+188	0
+189	0
+190	0
+191	0
+192	0
+193	0
+194	0
+195	0
+196	0
+197	0
+198	0
+199	0
+200	0
+201	0
+202	0
+203	0
+204	0
+205	0
+206	0
+207	0
+208	0
+209	0
+210	0
+211	0
+212	0
+213	0
+214	0
+215	0
+216	0
+217	0
+218	0
+219	0
+220	0
+221	0
+222	0
+223	0
+224	0
+225	0
+226	0
+227	0
+228	0
+229	0
+230	0
+231	0
+232	0
+233	0
+234	0
+235	0
+236	0
+237	0
+238	0
+239	0
+240	0
+241	0
+242	0
+243	0
+244	0
+245	0
+246	0
+247	0
+248	0
+249	0
+250	0
+251	0
+252	0
+253	0
+254	0
+255	0
+256	0
+257	0
+258	0
+259	0
+260	0
+261	0
+262	0
+263	0
+264	0
+265	0
+266	0
+267	0
+268	0
+269	0
+270	0
+271	0
+272	0
+273	0
+274	0
+275	0
+276	0
+277	0
+278	0
+279	0
+280	0
+281	0
+282	0
+283	0
+284	0
+285	0
+286	0
+287	0
+288	0
+289	0
+290	0
+291	0
+292	0
+293	0
+294	0
+295	0
+296	0
+297	0
+298	0
+299	0
+300	0
+301	0
+302	0
+303	0
+304	0
+305	0
+306	0
+307	0
+308	0
+309	0
+310	0
+311	0
+312	0
+313	0
+314	0
+315	0
+316	0
+317	0
+318	0
+319	0
+320	0
+321	0
+322	0
+323	0
+324	0
+325	0
+326	0
+327	0
+328	0
+329	0
+330	0
+331	0
+332	0
+333	0
+334	0
+335	0
+336	0
+337	0
+338	0
+339	0
+340	0
+341	0
+342	0
+343	0
+344	0
+345	0
+346	0
+347	0
+348	0
+349	0
+350	0
+351	0
+352	0
+353	0
+354	0
+355	0
+356	0
+357	0
+358	0
+359	0
+360	0
+361	0
+362	0
+363	0
+364	0
+365	0
+366	0
+367	0
+368	0
+369	0
+370	0
+371	0
+372	0
+373	0
+374	0
+375	0
+376	0
+377	0
+378	0
+379	0
+380	0
+381	0
+382	0
+383	0
+384	0
+385	0
+386	0
+387	0
+388	0
+389	0
+390	0
+391	0
+392	0
+393	0
+394	0
+395	0
+396	0
+397	0
+398	0
+399	0
+400	0
+401	0
+402	0
+403	0
+404	0
+405	0
+406	0
+407	0
+408	0
+409	0
+410	0
+411	0
+412	0
+413	0
+414	0
+415	0
+416	0
+417	0
+418	0
+419	0
+420	0
+421	0
+422	0
+423	0
+424	0
+425	0
+426	0
+427	0
+428	0
+429	0
+430	0
+431	0
+432	0
+433	0
+434	0
+435	0
+436	0
+437	0
+438	0
+439	0
+440	0
+441	0
+442	0
+443	0
+444	0
+445	0
+446	0
+447	0
+448	0
+449	0
+450	0
+451	0
+452	0
+453	0
+454	0
+455	0
+456	0
+457	0
+458	0
+459	0
+460	0
+461	0
+462	0
+463	0
+464	0
+465	0
+466	0
+467	0
+468	0
+469	0
+470	0
+471	0
+472	0
+473	0
+474	0
+475	0
+476	0
+477	0
+478	0
+479	0
+480	0
+481	0
+482	0
+483	0
+484	0
+485	0
+486	0
+487	0
+488	0
+489	0
+490	0
+491	0
+492	0
+493	0
+494	0
+495	0
+496	0
+497	0
+498	0
+499	0
+500	0
+501	0
+502	0
+503	0
+504	0
+505	0
+506	0
+507	0
+508	0
+509	0
+510	0
+511	0
+512	0
+513	0
+514	0
+515	0
+516	0
+517	0
+518	0
+519	0
+520	0
+521	0
+522	0
+523	0
+524	0
+525	0
+526	0
+527	0
+528	0
+529	0
+530	0
+531	0
+532	0
+533	0
+534	0
+535	0
+536	0
+537	0
+538	0
+539	0
+540	0
+541	0
+542	0
+543	0
+544	0
+545	0
+546	0
+547	0
+548	0
+549	0
+550	0
+551	0
+552	0
+553	0
+554	0
+555	0
+556	0
+557	0
+558	0
+559	0
+560	0
+561	0
+562	0
+563	0
+564	0
+565	0
+566	0
+567	0
+568	0
+569	0
+570	0
+571	0
+572	0
+573	0
+574	0
+575	0
+576	0
+577	0
+578	0
+579	0
+580	0
+581	0
+582	0
+583	0
+584	0
+585	0
+586	0
+587	0
+588	0
+589	0
+590	0
+591	0
+592	0
+593	0
+594	0
+595	0
+596	0
+597	0
+598	0
+599	0
+600	0
+601	0
+602	0
+603	0
+604	0
+605	0
+606	0
+607	0
+608	0
+609	0
+610	0
+611	0
+612	0
+613	0
+614	0
+615	0
+616	0
+617	0
+618	0
+619	0
+620	0
+621	0
+622	0
+623	0
+624	0
+625	0
+626	0
+627	0
+628	0
+629	0
+630	0
+631	0
+632	0
+633	0
+634	0
+635	0
+636	0
+637	0
+638	0
+639	0
+640	0
+641	0
+642	0
+643	0
+644	0
+645	0
+646	0
+647	0
+648	0
+649	0
+650	0
+651	0
+652	0
+653	0
+654	0
+655	0
+656	0
+657	0
+658	0
+659	0
+660	0
+661	0
+662	0
+663	0
+664	0
+665	0
+666	0
+667	0
+668	0
+669	0
+670	0
+671	0
+672	0
+673	0
+674	0
+675	0
+676	0
+677	0
+678	0
+679	0
+680	0
+681	0
+682	0
+683	0
+684	0
+685	0
+686	0
+687	0
+688	0
+689	0
+690	0
+691	0
+692	0
+693	0
+694	0
+695	0
+696	0
+697	0
+698	0
+699	0
+700	0
+701	0
+702	0
+703	0
+704	0
+705	0
+706	0
+707	0
+708	0
+709	0
+710	0
+711	0
+712	0
+713	0
+714	0
+715	0
+716	0
+717	0
+718	0
+719	0
+720	0
+721	0
+722	0
+723	0
+724	0
+725	0
+726	0
+727	0
+728	0
+729	0
+730	0
+731	0
+732	0
+733	0
+734	0
+735	0
+736	0
+737	0
+738	0
+739	0
+740	0
+741	0
+742	0
+743	0
+744	0
+745	0
+746	0
+747	0
+748	0
+749	0
+750	0
+751	0
+752	0
+753	0
+754	0
+755	0
+756	0
+757	0
+758	0
+759	0
+760	0
+761	0
+762	0
+763	0
+764	0
+765	0
+766	0
+767	0
+768	0
+769	0
+770	0
+771	0
+772	0
+773	0
+774	0
+775	0
+776	0
+777	0
+778	0
+779	0
+780	0
+781	0
+782	0
+783	0
+784	0
+785	0
+786	0
+787	0
+788	0
+789	0
+790	0
+791	0
+792	0
+793	0
+794	0
+795	0
+796	0
+797	0
+798	0
+799	0
+800	0
+801	0
+802	0
+803	0
+804	0
+805	0
+806	0
+807	0
+808	0
+809	0
+810	0
+811	0
+812	0
+813	0
+814	0
+815	0
+816	0
+817	0
+818	0
+819	0
+820	0
+821	0
+822	0
+823	0
+824	0
+825	0
+826	0
+827	0
+828	0
+829	0
+830	0
+831	0
+832	0
+833	0
+834	0
+835	0
+836	0
+837	0
+838	0
+839	0
+840	0
+841	0
+842	0
+843	0
+844	0
+845	0
+846	0
+847	0
+848	0
+849	0
+850	0
+851	0
+852	0
+853	0
+854	0
+855	0
+856	0
+857	0
+858	0
+859	0
+860	0
+861	0
+862	0
+863	0
+864	0
+865	0
+866	0
+867	0
+868	0
+869	0
+870	0
+871	0
+872	0
+873	0
+874	0
+875	0
+876	0
+877	0
+878	0
+879	0
+880	0
+881	0
+882	0
+883	0
+884	0
+885	0
+886	0
+887	0
+888	0
+889	0
+890	0
+891	0
+892	0
+893	0
+894	0
+895	0
+896	0
+897	0
+898	0
+899	0
+900	0
+901	0
+902	0
+903	0
+904	0
+905	0
+906	0
+907	0
+908	0
+909	0
+910	0
+911	0
+912	0
+913	0
+914	0
+915	0
+916	0
+917	0
+918	0
+919	0
+920	0
+921	0
+922	0
+923	0
+924	0
+925	0
+926	0
+927	0
+928	0
+929	0
+930	0
+931	0
+932	0
+933	0
+934	0
+935	0
+936	0
+937	0
+938	0
+939	0
+940	0
+941	0
+942	0
+943	0
+944	0
+945	0
+946	0
+947	0
+948	0
+949	0
+950	0
+951	0
+952	0
+953	0
+954	0
+955	0
+956	0
+957	0
+958	0
+959	0
+960	0
+961	0
+962	0
+963	0
+964	0
+965	0
+966	0
+967	0
+968	0
+969	0
+970	0
+971	0
+972	0
+973	0
+974	0
+975	0
+976	0
+977	0
+978	0
+979	0
+980	0
+981	0
+982	0
+983	0
+984	0
+985	0
+986	0
+987	0
+988	0
+989	0
+990	0
+991	0
+992	0
+993	0
+994	0
+995	0
+996	0
+997	0
+998	0
+999	0
+1000	0
+1001	0
+1002	0
+1003	0
+1004	0
+1005	0
+1006	0
+1007	0
+1008	0
+1009	0
+1010	0
+1011	0
+1012	0
+1013	0
+1014	0
+1015	0
+1016	0
+1017	0
+1018	0
+1019	0
+1020	0
+1021	0
+1022	0
+1023	0
+1024	0
+1025	0
+1026	0
+1027	0
+1028	0
+1029	0
+1030	0
+1031	0
+1032	0
+1033	0
+1034	0
+1035	0
+1036	0
+1037	0
+1038	0
+1039	0
+1040	0
+1041	0
+1042	0
+1043	0
+1044	0
+1045	0
+1046	0
+1047	0
+1048	0
+1049	0
+1050	0
+1051	0
+1052	0
+1053	0
+1054	0
+1055	0
+1056	0
+1057	0
+1058	0
+1059	0
+1060	0
+1061	0
+1062	0
+1063	0
+1064	0
+1065	0
+1066	0
+1067	0
+1068	0
+1069	0
+1070	0
+1071	0
+1072	0
+1073	0
+1074	0
+1075	0
+1076	0
+1077	0
+1078	0
+1079	0
+1080	0
+1081	0
+1082	0
+1083	0
+1084	0
+1085	0
+1086	0
+1087	0
+1088	0
+1089	0
+1090	0
+1091	0
+1092	0
+1093	0
+1094	0
+1095	0
+1096	0
+1097	0
+1098	0
+1099	0
+1100	0
+1101	0
+1102	0
+1103	0
+1104	0
+1105	0
+1106	0
+1107	0
+1108	0
+1109	0
+1110	0
+1111	0
+1112	0
+1113	0
+1114	0
+1115	0
+1116	0
+1117	0
+1118	0
+1119	0
+1120	0
+1121	0
+1122	0
+1123	0
+1124	0
+1125	0
+1126	0
+1127	0
+1128	0
+1129	0
+1130	0
+1131	0
+1132	0
+1133	0
+1134	0
+1135	0
+1136	0
+1137	0
+1138	0
+1139	0
+1140	0
+1141	0
+1142	0
+1143	0
+1144	0
+1145	0
+1146	0
+1147	0
+1148	0
+1149	0
+1150	0
+1151	0
+1152	0
+1153	0
+1154	0
+1155	0
+1156	0
+1157	0
+1158	0
+1159	0
+1160	0
+1161	0
+1162	0
+1163	0
+1164	0
+1165	0
+1166	0
+1167	0
+1168	0
+1169	0
+1170	0
+1171	0
+1172	0
+1173	0
+1174	0
+1175	0
+1176	0
+1177	0
+1178	0
+1179	0
+1180	0
+1181	0
+1182	0
+1183	0
+1184	0
+1185	0
+1186	0
+1187	0
+1188	0
+1189	0
+1190	0
+1191	0
+1192	0
+1193	0
+1194	0
+1195	0
+1196	0
+1197	0
+1198	0
+1199	0
+1200	0
+1201	0
+1202	0
+1203	0
+1204	0
+1205	0
+1206	0
+1207	0
+1208	0
+1209	0
+1210	0
+1211	0
+1212	0
+1213	0
+1214	0
+1215	0
+1216	0
+1217	0
+1218	0
+1219	0
+1220	0
+1221	0
+1222	0
+1223	0
+1224	0
+1225	0
+1226	0
+1227	0
+1228	0
+1229	0
+1230	0
+1231	0
+1232	0
+1233	0
+1234	0
+1235	0
+1236	0
+1237	0
+1238	0
+1239	0
+1240	0
+1241	0
+1242	0
+1243	0
+1244	0
+1245	0
+1246	0
+1247	0
+1248	0
+1249	0
+1250	0
+1251	0
+1252	0
+1253	0
+1254	0
+1255	0
+1256	0
+1257	0
+1258	0
+1259	0
+1260	0
+1261	0
+1262	0
+1263	0
+1264	0
+1265	0
+1266	0
+1267	0
+1268	0
+1269	0
+1270	0
+1271	0
+1272	0
+1273	0
+1274	0
+1275	0
+1276	0
+1277	0
+1278	0
+1279	0
+1280	0

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/sizes
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/sizes	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/babyAI-1obj.dmat.metadata/sizes	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1 @@
+1280 1 0 0 

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,71 @@
+#!/usr/bin/env python
+
+from plearn.learners.modulelearners import *
+
+from plearn.learners.modulelearners.sampler.inputweights import *
+from plearn.learners.modulelearners.sampler.reconstruct import *
+from plearn.learners.modulelearners.sampler.sample_from_visible import *
+from plearn.learners.modulelearners.sampler.sample_from_hidden import *
+
+learner_filename = os.path.dirname(os.path.abspath(sys.argv[0]))+'/data/DBN-3RBM.babyAI-1obj.psave'
+data_filename=os.path.dirname(os.path.abspath(sys.argv[0]))+'/data/babyAI-1obj.dmat'
+width = 32
+imageSize = width*width
+
+
+if len(sys.argv)>=2:
+  learner_filename = sys.argv[1]
+if len(sys.argv)>=3:
+  data_filename = sys.argv[2]
+if len(sys.argv)>=4:
+  imageSize = int(sys.argv[3])
+
+if os.path.isfile(learner_filename) == False:
+   raise EOFError, "Cannot find file "+learner_filename
+print " loading... "+learner_filename
+learner = loadObject(learner_filename)
+if 'HyperLearner' in str(type(learner)):
+   learner=learner.learner
+
+if os.path.isfile(data_filename) == False and os.path.isdir(data_filename) == False:
+   raise EOFError, "Cannot find file or directory "+data_filename
+print " loading... "+data_filename
+dataSet = pl.AutoVMatrix( specification = data_filename )
+
+
+
+def check_choice(c):
+    try:
+       if int(c) in [1,2,3,4,EXITCODE]:
+          return True
+    except: pass
+    return False
+    
+while True:
+
+   c=None
+   while check_choice(c)==False:
+      print "\n---------------"
+      print "-- MAIN MENU --"
+      print "---------------"
+      print "1. *Sample visible units*: initialization of bottom RBM visible units with (randomly picked) real input image"
+      print "2. *Sample visible units*: initialization of top RBM hidden units (random binary vector)"
+      print "3. *Reconstruct* some input image"
+      print "4. *Visualize weights* of the 1st RBM (input)"
+      print "(to quit, type 'q' or 'Q')\n"
+      c = pause()
+
+
+   if c == 1:
+      view_sample_from_visible(learner, imageSize, dataSet, 1)
+#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_visible.py '+' '.join([ learner_filename, str(Size*Size), data_filename, 'gibbs_step='+str(gibbs_step) ]))
+   elif c == 2:
+      view_sample_from_hidden(learner, imageSize, 1)
+#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_hidden.py '+' '.join([ learner_filename, str(Size*Size), 'gibbs_step='+str(gibbs_step) ]))
+   elif c == 3:
+      view_reconstruct( learner, imageSize , dataSet)
+#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/reconstruct.py '+' '.join([ learner_filename, str(Size*Size), data_filename]))
+   elif c == 4:
+      view_inputweights(learner, imageSize)
+   elif c == EXITCODE:
+      break

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,60 @@
+from plearn.learners.modulelearners import *
+
+zoom_factor = 5
+from plearn.learners.modulelearners.sampler import *
+
+import sys, os.path
+
+
+def view_inputweights(learner, Nim):
+  
+  inputweights_man()
+  #
+  # Getting the RBMmodule which sees the image (looking at size of the down layer)
+  #
+  modules=getModules(learner)
+  for i in range(len(modules)):
+     module = modules[i]
+     if isModule(module,'RBM') and module.connection.down_size == Nim:
+        image_RBM=learner.module.modules[i]
+        break
+  image_RBM_name=image_RBM.name
+
+  screen=init_screen(Nim,zoom_factor)
+  
+  
+#  if 'RBMMatrixConnection' in str(type(image_RBM.connection)):
+
+  for i in range(len(image_RBM.connection.weights)):
+    weights=image_RBM.connection.weights[i]
+    print str(i+1)+"/"+str(len(image_RBM.connection.weights))
+    c = draw_normalized_image( weights, screen, zoom_factor )
+    if c==EXITCODE:
+       return
+       
+#  elif 'RBMMixedConnection' in str(type(image_RBM.connection)):
+
+def inputweights_man():
+     print "\nPlease type:"
+     print ":    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
+     print ":      q       : (quit) when you are fed up\n"
+
+if __name__ == "__main__":
+
+  if len(sys.argv) < 2:
+     print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size>\n"
+     print "Purpose:\n\tSee weights of the RBM which sees image\n\t(i.e. with visible_size=Image_size)"
+     inputweights_man()
+     sys.exit()
+
+  learner_filename = sys.argv[1]
+  Nim = int(sys.argv[2])
+
+  if os.path.isfile(learner_filename) == False:
+     raise TypeError, "Cannot find file "+learner_filename
+  print " loading... "+learner_filename
+  learner = loadObject(learner_filename)
+  if 'HyperLearner' in str(type(learner)):
+     learner=learner.learner
+
+  view_inputweights(learner, Nim)

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/reconstruct.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/reconstruct.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/reconstruct.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,163 @@
+from plearn.learners.modulelearners import *
+
+zoom_factor = 5
+from plearn.learners.modulelearners.sampler import *
+
+import random
+
+
+def view_reconstruct( learner, Nim , dataSet):
+
+  print "analyzing learner..."
+  #
+  # Getting the RBMmodule which sees the image (looking at size of the down layer)
+  #
+  modules=getModules(learner)
+  for i in range(len(modules)):
+     module = modules[i]
+     if isModule(module,'RBM') and module.connection.down_size == Nim:
+        image_RBM=learner.module.modules[i]
+        break
+  image_RBM_name=image_RBM.name
+  #
+  # Getting the top RBMmodule
+  #
+
+  top_RBM = getTopRBMModule( learner )
+  top_RBM_name = top_RBM.name
+  
+  NH=top_RBM.connection.up_size
+
+
+  init_ports = [ ('input',  image_RBM_name+'.visible'),
+                 ('output', top_RBM_name+'.hidden.state')
+               ]
+  ports = [ ('input', top_RBM_name+'.hidden.state' ),
+            ('output', image_RBM_name+'.visible_reconstruction.state')
+            ]
+
+  #
+  # Removing useless connections for sampling
+  #
+  old_connections_list = copy.copy(learner.module.connections)
+  conn_toremove=[]
+  connections_list_down=[]
+  connections_list_up=[]
+  for connection in old_connections_list:
+      source_module = getModule( learner, port2moduleName( connection.source ))
+      dest_module   = getModule( learner, port2moduleName( connection.destination ))
+      if isModule( source_module, 'RBM') and isModule( dest_module,'RBM'):
+         connections_list_up.append ( pl.NetworkConnection(source = port2moduleName( connection.source )+'.hidden.state',
+                                                           destination = port2moduleName( connection.destination )+'.visible',
+                                                           propagate_gradient = 0) )
+         connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_reconstruction.state',
+                                                    destination = port2moduleName( connection.source )+'.hidden.state',
+                                                    propagate_gradient = 0) )
+  
+  #
+  # Removing useless modules for sampling
+  #
+  modules_list = getModules(learner)
+  mod_toremove=[]
+  for module in modules_list:
+      if isModule( module, 'RBM') == False:
+         mod_toremove.append(module)
+  for module in mod_toremove:
+      modules_list.remove(module)
+  
+  RBMnetworkInit = pl.NetworkModule(
+                          modules = modules_list,
+                          connections = connections_list_up,
+                          ports = init_ports,
+                          # to avoid calling the forget() method in ModuleLearner                          
+                          random_gen = pl.PRandom( seed = 1827 ),
+                          # Hack from Olivier
+                          save_states = 0
+                         )
+  RBMnetwork = pl.NetworkModule(
+                          modules = modules_list,
+                          connections = connections_list_down,
+                          ports = ports,
+                          # to avoid calling the forget() method in ModuleLearner                          
+                          random_gen = pl.PRandom( seed = 1827 ),
+                          # Hack from Olivier
+                          save_states = 0
+                         )
+
+  
+  RBMmodel = pl.ModuleLearner(
+                              cost_ports = [],
+                              target_ports = [],
+                              module = RBMnetwork
+                           )
+  RBMmodelInit = pl.ModuleLearner(
+                              cost_ports = [],
+                              target_ports = [],
+                              module = RBMnetworkInit
+                           )
+
+
+
+  RBMmodelInit.setTrainingSet(pl.AutoVMatrix(inputsize=Nim, targetsize=0, weightsize=0), False)
+  RBMmodel.setTrainingSet(pl.AutoVMatrix(inputsize=NH, targetsize=0, weightsize=0), False)
+
+
+  screen=init_screen(Nim,zoom_factor)
+  random.seed(1969)
+
+  for i in range(len(RBMmodel.module.modules)):
+    module = RBMmodel.module.modules[i]
+    if isModule( module, 'RBM'):
+       RBMmodel.module.modules[i].compute_contrastive_divergence = False
+       if module.name == top_RBM_name:
+          top_RBM = RBMmodel.module.modules[i]
+
+  reconstruct_man()
+  while True:
+ 
+   random_index=random.randint(0,dataSet.length)
+   init_image=[dataSet.getRow(random_index)[i] for i in range(Nim)]
+   
+   c = draw_image( init_image, screen, zoom_factor )
+   if c==EXITCODE:
+      return
+   
+   init_hidden = RBMmodelInit.computeOutput(init_image)
+   c = draw_image( RBMmodel.computeOutput(init_hidden), screen, zoom_factor )   
+   if c==EXITCODE:
+      return
+
+def reconstruct_man():
+     print "\nPlease type:"
+     print ":    <ENTER>   : to continue"
+     print ":      q       : (quit) when you are fed up\n"
+
+
+if __name__ == "__main__":
+  import sys, os.path
+
+  if len(sys.argv) < 2:
+     print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size> <dataSet_filename>\n"
+     print "Purpose:\n\tSee the reconstruction of an image by an RBM (autoassiocator mode)\n"
+     reconstruct_man()
+     sys.exit()
+
+  learner_filename = sys.argv[1]
+  Nim = int(sys.argv[2])
+  data_filename = sys.argv[3]
+     
+
+     
+  if os.path.isfile(learner_filename) == False:
+     raise TypeError, "Cannot find file "+learner_filename
+  print " loading... "+learner_filename
+  learner = loadObject(learner_filename)
+  if 'HyperLearner' in str(type(learner)):
+     learner=learner.learner
+  
+  if os.path.isfile(data_filename) == False:
+     raise TypeError, "Cannot find file "+data_filename
+  print " loading... "+data_filename
+  dataSet = pl.AutoVMatrix( specification = data_filename )
+
+  view_reconstruct( learner, Nim , dataSet)

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,175 @@
+from plearn.learners.modulelearners import *
+
+zoom_factor = 5
+from plearn.learners.modulelearners.sampler import *
+
+import random
+
+def view_sample_from_hidden(learner, Nim, init_gibbs_step):
+
+  print "\nChoose betweem these options:"
+  print "1.[default] Gibbs sampling in the top RBM + mean field"
+  print "2.                   ''                   + sample hidden->visible"
+  c = pause()
+  while c not in [0,1,2,EXITCODE]:
+        c = pause()
+  MeanField = False
+  if c==1:
+     MeanField = True
+  elif c==EXITCODE:
+     return
+
+  print "analyzing learner..."
+  #
+  # Getting the RBMmodule which sees the image (looking at size of the down layer)
+  #
+  modules=getModules(learner)
+  for i in range(len(modules)):
+     module = modules[i]
+     if isModule(module,'RBM') and module.connection.down_size == Nim:
+        image_RBM=learner.module.modules[i]
+        break
+  image_RBM_name=image_RBM.name
+  #
+  # Getting the top RBMmodule
+  #
+
+  top_RBM = getTopRBMModule( learner )
+  top_RBM_name = top_RBM.name
+  
+  NH=top_RBM.connection.up_size
+
+
+  if MeanField:
+     ports = [ ('input', top_RBM_name+'.hidden_sample' ),
+               ('output', image_RBM_name+'.visible_reconstruction.state')
+             ]
+
+  else:
+     ports = [ ('input', top_RBM_name+'.hidden_sample' ),
+               ('output', image_RBM_name+'.visible_expectation')
+               ]
+
+  #
+  # Removing useless connections for sampling
+  #
+  old_connections_list = copy.copy(learner.module.connections)
+  conn_toremove=[]
+  connections_list_down=[]
+  connections_list_up=[]
+  for connection in old_connections_list:
+      source_module = getModule( learner, port2moduleName( connection.source ))
+      dest_module   = getModule( learner, port2moduleName( connection.destination ))
+      if isModule( source_module, 'RBM') and isModule( dest_module,'RBM'):
+         if MeanField:
+            if dest_module.name == top_RBM_name:
+               connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_sample',
+                                                                destination = port2moduleName( connection.source )+'.hidden.state',
+                                                                propagate_gradient = 0) )
+            else:
+               connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_reconstruction.state',
+                                                                destination = port2moduleName( connection.source )+'.hidden.state',
+                                                                propagate_gradient = 0) )
+	 else:
+            connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_sample',
+                                                                destination = port2moduleName( connection.source )+'.hidden_sample',
+                                                                propagate_gradient = 0) )
+  
+  #
+  # Removing useless modules for sampling
+  #
+  modules_list = getModules(learner)
+  mod_toremove=[]
+  for module in modules_list:
+      if isModule( module, 'RBM') == False:
+         mod_toremove.append(module)
+  for module in mod_toremove:
+      modules_list.remove(module)
+  
+  
+  RBMnetwork = pl.NetworkModule(
+                          modules = modules_list,
+                          connections = connections_list_down,
+                          ports = ports,
+                          # to avoid calling the forget() method in ModuleLearner                          
+                          random_gen = pl.PRandom( seed = 1827 ),
+                          # Hack from Olivier
+                          save_states = 0
+                         )
+
+  
+  RBMmodel = pl.ModuleLearner(
+                              cost_ports = [],
+                              target_ports = [],
+                              module = RBMnetwork
+                           )
+
+
+  RBMmodel.setTrainingSet(pl.AutoVMatrix(inputsize=NH, targetsize=0, weightsize=0), False)
+
+  screen=init_screen(Nim,zoom_factor)
+  random.seed(1969)
+
+  for i in range(len(RBMmodel.module.modules)):
+    module = RBMmodel.module.modules[i]
+    if isModule( module, 'RBM'):
+       RBMmodel.module.modules[i].compute_contrastive_divergence = False
+       if module.name == top_RBM_name:
+          RBMmodel.module.modules[i].n_Gibbs_steps_per_generated_sample = init_gibbs_step
+          top_RBM = RBMmodel.module.modules[i]
+
+  sample_from_hidden_man()
+  while True:
+ 
+   init_hidden=[random.randint(0,1) for i in range(NH)]
+   
+   c = draw_image( RBMmodel.computeOutput(init_hidden), screen, zoom_factor )
+   if c==NEXTCODE:
+      continue
+   elif c==EXITCODE:
+      return
+   elif c>0:
+      top_RBM.n_Gibbs_steps_per_generated_sample = c
+      
+   while True:
+       c = draw_image( RBMmodel.computeOutput([]) , screen, zoom_factor )
+       if c==NEXTCODE:
+          break
+       elif c==EXITCODE:
+          return
+       elif c>0:
+          top_RBM.n_Gibbs_steps_per_generated_sample = c
+    
+
+def sample_from_hidden_man():
+     print "\nPlease type:"
+     print ":    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
+     print ": <an integer> : to change the gibbs step (ex: 10 will set the number of gibbs step between each example to 10)"
+     print ":      n       : (next) to try another hidden state for initialization"
+     print ":      q       : (quit) when you are fed up\n"
+
+if __name__ == "__main__":
+  import sys, os.path
+
+  if len(sys.argv) < 2:
+     print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size> [init_gibbs_step=<init_gibbs_step>]\n"
+     print "Purpose:\n\tSee consecutive Gibbs sample"
+     print "\twhen top hidden units are initalized randomly"
+     sample_from_hidden_man()
+     sys.exit()
+
+  learner_filename = sys.argv[1]
+  Nim = int(sys.argv[2])
+     
+  plarg_defaults.init_gibbs_step                    = 10
+  init_gibbs_step                                   = plargs.init_gibbs_step # Number of Gibbs step between each sampling
+
+     
+  if os.path.isfile(learner_filename) == False:
+     raise TypeError, "Cannot find file "+learner_filename
+  print " loading... "+learner_filename
+  learner = loadObject(learner_filename)
+  if 'HyperLearner' in str(type(learner)):
+     learner=learner.learner
+
+  view_sample_from_hidden(learner, Nim, init_gibbs_step)

Added: trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py	2007-06-29 22:39:50 UTC (rev 7682)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py	2007-06-30 02:17:03 UTC (rev 7683)
@@ -0,0 +1,219 @@
+from plearn.learners.modulelearners import *
+
+zoom_factor = 5
+from plearn.learners.modulelearners.sampler import *
+
+import random
+
+def view_sample_from_visible(learner, Nim, dataSet, init_gibbs_step):
+
+  print "\nChoose betweem these options:"
+  print "1.[default] Gibbs sampling in the top RBM + mean field"
+  print "2.                   ''                   + sample hidden<->visible"
+  c = pause()
+  while c not in [0,1,2,EXITCODE]:
+        c = pause()
+  MeanField = False
+  if c==1:
+     MeanField = True
+  elif c==EXITCODE:
+     return
+
+  print "analyzing learner..."
+  #
+  # Getting the RBMmodule which sees the image (looking at size of the down layer)
+  #
+  modules=getModules(learner)
+  for i in range(len(modules)):
+     module = modules[i]
+     if isModule(module,'RBM') and module.connection.down_size == Nim:
+        image_RBM=learner.module.modules[i]
+        break
+  image_RBM_name=image_RBM.name
+  #
+  # Getting the top RBMmodule
+  #
+
+  top_RBM = getTopRBMModule( learner )
+  top_RBM_name = top_RBM.name
+  
+  NH=top_RBM.connection.up_size
+
+
+  if MeanField:
+     init_ports = [ ('input',  image_RBM_name+'.visible'),
+                    ('output', top_RBM_name+'.hidden.state')
+                  ]
+     ports = [ ('input', top_RBM_name+'.hidden_sample' ),
+               ('output', image_RBM_name+'.visible_reconstruction.state')
+             ]
+  else:	
+     init_ports = [ ('input',  image_RBM_name+'.visible'),
+                    ('output', top_RBM_name+'.hidden_sample')
+                  ]
+     ports = [ ('input', top_RBM_name+'.hidden_sample' ),
+               ('output', image_RBM_name+'.visible_expectation')
+             ]
+
+  #
+  # Removing useless connections for sampling
+  #
+  old_connections_list = copy.copy(learner.module.connections)
+  conn_toremove=[]
+  connections_list_down=[]
+  connections_list_up=[]
+  for connection in old_connections_list:
+      source_module = getModule( learner, port2moduleName( connection.source ))
+      dest_module   = getModule( learner, port2moduleName( connection.destination ))
+      if isModule( source_module, 'RBM') and isModule( dest_module,'RBM'):
+         if MeanField:
+            connections_list_up.append ( pl.NetworkConnection(source = port2moduleName( connection.source )+'.hidden.state',
+                                                              destination = port2moduleName( connection.destination )+'.visible',
+                                                              propagate_gradient = 0) )
+            if dest_module.name == top_RBM_name:
+               connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_sample',
+                                                                destination = port2moduleName( connection.source )+'.hidden.state',
+                                                                propagate_gradient = 0) )
+            else:
+               connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_reconstruction.state',
+                                                                destination = port2moduleName( connection.source )+'.hidden.state',
+                                                                propagate_gradient = 0) )
+         else:
+            connections_list_up.append ( pl.NetworkConnection(source = port2moduleName( connection.source )+'.hidden_sample',
+                                                              destination = port2moduleName( connection.destination )+'.visible_sample',
+                                                              propagate_gradient = 0) )
+            connections_list_down.append ( pl.NetworkConnection(source = port2moduleName( connection.destination )+'.visible_sample',
+                                                                destination = port2moduleName( connection.source )+'.hidden_sample',
+                                                                propagate_gradient = 0) )
+  
+  #
+  # Removing useless modules for sampling
+  #
+  modules_list = getModules(learner)
+  mod_toremove=[]
+  for module in modules_list:
+      if isModule( module, 'RBM') == False:
+         mod_toremove.append(module)
+  for module in mod_toremove:
+      modules_list.remove(module)
+  
+  
+  RBMnetwork = pl.NetworkModule(
+                          modules = modules_list,
+                          connections = connections_list_down,
+                          ports = ports,
+                          # to avoid calling the forget() method in ModuleLearner                          
+                          random_gen = pl.PRandom( seed = 1827 ),
+                          # Hack from Olivier
+                          save_states = 0
+                         )
+  RBMnetworkInit = pl.NetworkModule(
+                          modules = modules_list,
+                          connections = connections_list_up,
+                          ports = init_ports,
+                          # to avoid calling the forget() method in ModuleLearner                          
+                          random_gen = pl.PRandom( seed = 1827 ),
+                          # Hack from Olivier
+                          save_states = 0
+                         )
+
+  
+  RBMmodel = pl.ModuleLearner(
+                              cost_ports = [],
+                              target_ports = [],
+                              module = RBMnetwork
+                           )
+  RBMmodelInit = pl.ModuleLearner(
+                              cost_ports = [],
+                              target_ports = [],
+                              module = RBMnetworkInit
+                           )
+
+
+
+  RBMmodelInit.setTrainingSet(pl.AutoVMatrix(inputsize=Nim, targetsize=0, weightsize=0), False)
+  RBMmodel.setTrainingSet(pl.AutoVMatrix(inputsize=NH, targetsize=0, weightsize=0), False)
+
+
+  screen=init_screen(Nim,zoom_factor)
+  random.seed(1969)
+
+  for i in range(len(RBMmodel.module.modules)):
+    module = RBMmodel.module.modules[i]
+    if isModule( module, 'RBM'):
+       RBMmodel.module.modules[i].compute_contrastive_divergence = False
+       if module.name == top_RBM_name:
+          RBMmodel.module.modules[i].n_Gibbs_steps_per_generated_sample = init_gibbs_step
+          top_RBM = RBMmodel.module.modules[i]
+
+  sample_from_visible_man()
+  while True:
+
+      random_index=random.randint(0,dataSet.length)
+      init_image=[dataSet.getRow(random_index)[i] for i in range(Nim)]
+      
+      c = draw_image( init_image, screen, zoom_factor )
+      if c==NEXTCODE:
+         continue
+      elif c==EXITCODE:
+         return
+      elif c>0:
+         top_RBM.n_Gibbs_steps_per_generated_sample = c
+      
+      init_hidden = RBMmodelInit.computeOutput(init_image)
+      c = draw_image( RBMmodel.computeOutput(init_hidden), screen, zoom_factor )
+      if c==NEXTCODE:
+         continue
+      elif c==EXITCODE:
+         return
+      elif c>0:
+         top_RBM.n_Gibbs_steps_per_generated_sample = c
+
+
+      while True:
+          c = draw_image( RBMmodel.computeOutput([]) , screen, zoom_factor )
+          if c==NEXTCODE:
+               break
+          elif c==EXITCODE:
+               return
+          elif c>0:
+             top_RBM.n_Gibbs_steps_per_generated_sample = c
+    
+def sample_from_visible_man():
+     print "\nPlease type:"
+     print ":    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
+     print ": <an integer> : to change the gibbs step (ex: 10 will set the number of gibbs step between each example to 10)"
+     print ":      n       : (next) to try with another image as initialization"
+     print ":      q       : (quit) when you are fed up\n"
+
+if __name__ == "__main__":
+  import sys, os.path
+
+  if len(sys.argv) < 4:
+     print "Usage:\n\t" + sys.argv[0] + " <ModuleLearner_filename> <Image_size> <dataSet_filename> [init_gibbs_step=<init_gibbs_step>]\n"
+     print "Purpose:\n\tSee consecutive Gibbs sample"
+     print "\twhen input visible units are initalized with real images"
+     sample_from_visible_man()
+     sys.exit(0)
+
+  learner_filename = sys.argv[1]
+  Nim = int(sys.argv[2])
+  data_filename = sys.argv[3]
+     
+  plarg_defaults.init_gibbs_step                    = 1
+  init_gibbs_step                                   = plargs.init_gibbs_step # Number of Gibbs step between each sampling
+
+     
+  if os.path.isfile(learner_filename) == False:
+     raise TypeError, "Cannot find file "+learner_filename
+  print " loading... "+learner_filename
+  learner = loadObject(learner_filename)
+  if 'HyperLearner' in str(type(learner)):
+     learner=learner.learner
+  
+  if os.path.isfile(data_filename) == False:
+     raise TypeError, "Cannot find file "+data_filename
+  print " loading... "+data_filename
+  dataSet = pl.AutoVMatrix( specification = data_filename )
+
+  view_sample_from_visible(learner, Nim, dataSet)



From louradou at mail.berlios.de  Sat Jun 30 04:32:24 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 30 Jun 2007 04:32:24 +0200
Subject: [Plearn-commits] r7684 - trunk/plearn_learners/online
Message-ID: <200706300232.l5U2WOhF011710@sheep.berlios.de>

Author: louradou
Date: 2007-06-30 04:32:23 +0200 (Sat, 30 Jun 2007)
New Revision: 7684

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-30 02:17:03 UTC (rev 7683)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-30 02:32:23 UTC (rev 7684)
@@ -865,7 +865,7 @@
             *visible_expectation << to_store;
         }
         found_a_valid_configuration = true;
-    }
+    }// END SAMPLING
     
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)
@@ -959,7 +959,7 @@
             PLERROR("RBMModule: unknown configuration to compute contrastive_divergence (currently\n"
                     "only possible if only visible are provided in input).\n");
         found_a_valid_configuration = true;
-    }// END SAMPLING
+    }
     
 
     
@@ -1012,9 +1012,31 @@
         }
         found_a_valid_configuration = true;
     }
-    
+    // COMPUTE VISIBLE GIVEN HIDDEN
+    else if ( visible_reconstruction && visible_reconstruction->isEmpty() 
+         && hidden && !hidden->isEmpty())
+           
+    {        
+        // Don't need to verify if they are asked in a port, this was done previously
+        
+	computeVisibleActivations(*hidden,true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            const Mat& to_store = visible_layer->activations;
+            visible_reconstruction_activations->resize(to_store.length(), 
+                                                       to_store.width());
+            *visible_reconstruction_activations << to_store;
+        }      
+        visible_layer->computeExpectations();
+        PLASSERT( visible_reconstruction->isEmpty() );
+        const Mat& to_store = visible_layer->getExpectations();
+        visible_reconstruction->resize(to_store.length(), 
+                                       to_store.width());
+        *visible_reconstruction << to_store;
+        found_a_valid_configuration = true;
+    }
 
-
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;



