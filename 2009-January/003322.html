<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9882 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2009-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9882%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200901282155.n0SLtfM2019892%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003321.html">
   <LINK REL="Next"  HREF="003323.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9882 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9882%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200901282155.n0SLtfM2019892%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9882 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Wed Jan 28 22:55:41 CET 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="003321.html">[Plearn-commits] r9881 - trunk/python_modules/plearn/pymake
</A></li>
        <LI>Next message: <A HREF="003323.html">[Plearn-commits] r9883 - trunk/plearn_learners_experimental
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3322">[ date ]</a>
              <a href="thread.html#3322">[ thread ]</a>
              <a href="subject.html#3322">[ subject ]</a>
              <a href="author.html#3322">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2009-01-28 22:55:40 +0100 (Wed, 28 Jan 2009)
New Revision: 9882

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Added an option to have target connections during greedy training.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2009-01-28 17:51:31 UTC (rev 9881)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2009-01-28 21:55:40 UTC (rev 9882)
@@ -201,6 +201,14 @@
                   OptionBase::buildoption,
                   &quot;The weights of the connections between the layers&quot;);
 
+    declareOption(ol, &quot;greedy_target_layers&quot;, &amp;DeepBeliefNet::greedy_target_layers,
+                  OptionBase::buildoption,
+                  &quot;Optional target layers for greedy layer-wise pretraining&quot;);
+
+    declareOption(ol, &quot;greedy_target_connections&quot;, &amp;DeepBeliefNet::greedy_target_connections,
+                  OptionBase::buildoption,
+                  &quot;Optional target matrix connections for greedy layer-wise pretraining&quot;);
+
     declareOption(ol, &quot;classification_module&quot;,
                   &amp;DeepBeliefNet::classification_module,
                   OptionBase::learntoption,
@@ -492,7 +500,18 @@
     else
         reconstruction_costs.resize(0);
 
+    if( !greedy_target_layers.isEmpty() )
+    {
+        greedy_target_layer_nlls_index = current_index;
+        target_one_hot.resize(n_classes);
+        for( int i=0; i&lt;n_layers-1; i++ )
+        {
+            cost_names.append(&quot;layer&quot;+tostring(i)+&quot;.nll&quot;);
+            current_index++;
+        }
+    }
 
+
     cost_names.append(&quot;cpu_time&quot;);
     cost_names.append(&quot;cumulative_train_time&quot;);
     cost_names.append(&quot;cumulative_test_time&quot;);
@@ -556,6 +575,78 @@
         activation_gradients[i].resize( layers[i]-&gt;size );
         expectation_gradients[i].resize( layers[i]-&gt;size );
 
+
+        if( greedy_target_layers.length()&gt;i &amp;&amp; greedy_target_layers[i] )
+        {
+            if( use_classification_cost )
+                PLERROR(&quot;DeepBeliefNet::build_layers_and_connections() - \n&quot;
+                        &quot;use_classification_cost not implemented for greedy_target_layers.&quot;);
+
+            if( greedy_target_connections.length()&gt;i &amp;&amp; !greedy_target_connections[i] )
+                PLERROR(&quot;DeepBeliefNet::build_layers_and_connections() - \n&quot;
+                        &quot;some greedy_target_connections are missing.&quot;);
+
+            if( greedy_target_layers[i]-&gt;size != n_classes)
+                PLERROR(&quot;DeepBeliefNet::build_layers_and_connections() - \n&quot;
+                        &quot;greedy_target_layers[%d] should be of size %d.&quot;,i,n_classes);
+
+            if( greedy_target_connections[i]-&gt;down_size != n_classes ||
+                greedy_target_connections[i]-&gt;up_size != layers[i+1]-&gt;size )
+                PLERROR(&quot;DeepBeliefNet::build_layers_and_connections() - \n&quot;
+                        &quot;greedy_target_connections[%d] should be of size (%d,%d).&quot;,
+                        i,layers[i+1]-&gt;size,n_classes);
+                
+            if( partial_costs.length() != 0 )
+                PLERROR(&quot;DeepBeliefNet::build_layers_and_connections() - \n&quot;
+                        &quot;greedy_target_layers can't be used with partial_costs.&quot;);
+                
+            greedy_target_expectations.resize(n_layers-1);
+            greedy_target_activations.resize(n_layers-1);
+            greedy_target_expectation_gradients.resize(n_layers-1);
+            greedy_target_activation_gradients.resize(n_layers-1);
+            greedy_target_probability_gradients.resize(n_layers-1);
+
+            greedy_target_expectations[i].resize(n_classes);
+            greedy_target_activations[i].resize(n_classes);
+            greedy_target_expectation_gradients[i].resize(n_classes);
+            greedy_target_activation_gradients[i].resize(n_classes);
+            greedy_target_probability_gradients[i].resize(n_classes);
+            for( int c=0; c&lt;n_classes; c++) 
+            {
+                greedy_target_expectations[i][c].resize(layers[i+1]-&gt;size);
+                greedy_target_activations[i][c].resize(layers[i+1]-&gt;size);
+                greedy_target_expectation_gradients[i][c].resize(layers[i+1]-&gt;size);
+                greedy_target_activation_gradients[i][c].resize(layers[i+1]-&gt;size);
+            }
+
+            greedy_joint_layers.resize(n_layers-1);
+            PP&lt;RBMMixedLayer&gt; ml = new RBMMixedLayer();
+            ml-&gt;sub_layers.resize(2);
+            ml-&gt;sub_layers[0] = layers[ i ];
+            ml-&gt;sub_layers[1] = greedy_target_layers[ i ];
+            ml-&gt;random_gen = random_gen;
+            ml-&gt;build();
+            greedy_joint_layers[i] = (RBMMixedLayer *)ml;
+
+            greedy_joint_connections.resize(n_layers-1);
+            PP&lt;RBMMixedConnection&gt; mc = new RBMMixedConnection();
+            mc-&gt;sub_connections.resize(1,2);
+            mc-&gt;sub_connections(0,0) = connections[i];
+            mc-&gt;sub_connections(0,1) = greedy_target_connections[i];
+            mc-&gt;build();
+            greedy_joint_connections[i] = (RBMMixedConnection *)mc;
+
+            if( !(greedy_target_connections[i]-&gt;random_gen) )
+            {
+                greedy_target_connections[i]-&gt;random_gen = random_gen;
+                greedy_target_connections[i]-&gt;forget();
+            }
+            if( !(greedy_target_layers[i]-&gt;random_gen) )
+            {
+                greedy_target_layers[i]-&gt;random_gen = random_gen;
+                greedy_target_layers[i]-&gt;forget();
+            }
+        }
     }
     if( !(layers[n_layers-1]-&gt;random_gen) )
     {
@@ -726,6 +817,8 @@
     deepCopyField(training_schedule,        copies);
     deepCopyField(layers,                   copies);
     deepCopyField(connections,              copies);
+    deepCopyField(greedy_target_layers,     copies);
+    deepCopyField(greedy_target_connections,copies);
     deepCopyField(final_module,             copies);
     deepCopyField(final_cost,               copies);
     deepCopyField(partial_costs,            copies);
@@ -738,6 +831,13 @@
     deepCopyField(activations_gradients,    copies);
     deepCopyField(expectation_gradients,    copies);
     deepCopyField(expectations_gradients,   copies);
+    deepCopyField(greedy_target_expectations,copies);
+    deepCopyField(greedy_target_activations, copies);
+    deepCopyField(greedy_target_expectation_gradients,copies);
+    deepCopyField(greedy_target_activation_gradients,copies);
+    deepCopyField(greedy_target_probability_gradients,copies);
+    deepCopyField(greedy_joint_layers   ,   copies);
+    deepCopyField(greedy_joint_connections, copies);
     deepCopyField(final_cost_input,         copies);
     deepCopyField(final_cost_inputs,        copies);
     deepCopyField(final_cost_value,         copies);
@@ -753,14 +853,17 @@
     deepCopyField(save_layer_expectations,  copies);
     deepCopyField(pos_down_val,             copies);
     deepCopyField(pos_up_val,               copies);
+    deepCopyField(pos_down_vals,            copies);
+    deepCopyField(pos_up_vals,              copies);
+    deepCopyField(cd_neg_down_vals,         copies);
     deepCopyField(cd_neg_up_vals,           copies);
-    deepCopyField(cd_neg_down_vals,         copies);
+    deepCopyField(mf_cd_neg_down_vals,      copies);
     deepCopyField(mf_cd_neg_up_vals,        copies);
-    deepCopyField(mf_cd_neg_down_vals,      copies);
+    deepCopyField(mf_cd_neg_down_val,       copies);
     deepCopyField(mf_cd_neg_up_val,         copies);
-    deepCopyField(mf_cd_neg_down_val,       copies);
     deepCopyField(gibbs_down_state,         copies);
     deepCopyField(optimized_costs,          copies);
+    deepCopyField(target_one_hot,           copies);
     deepCopyField(reconstruction_costs,     copies);
     deepCopyField(partial_costs_indices,    copies);
     deepCopyField(cumulative_schedule,      copies);
@@ -820,6 +923,15 @@
             if( partial_costs[i] )
                 partial_costs[i]-&gt;forget();
 
+    for( int i=0 ; i&lt;generative_connections.length() ; i++ )
+        generative_connections[i]-&gt;forget();
+
+    for( int i=0; i&lt;greedy_target_connections.length(); i++ )
+        greedy_target_connections[i]-&gt;forget();
+
+    for( int i=0; i&lt;greedy_target_layers.length(); i++ )
+        greedy_target_layers[i]-&gt;forget();
+
     cumulative_training_time = 0;
     cumulative_testing_time = 0;
     up_down_stage = 0;
@@ -976,6 +1088,15 @@
             connections[i]-&gt;setLearningRate( cd_learning_rate );
             layers[i+1]-&gt;setLearningRate( cd_learning_rate );
 
+            if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[i] )
+                greedy_target_layers[i]-&gt;setLearningRate( cd_learning_rate );
+            if( greedy_target_connections.length() &amp;&amp; greedy_target_connections[i] )
+                greedy_target_connections[i]-&gt;setLearningRate( cd_learning_rate );
+            if( greedy_joint_layers.length() &amp;&amp; greedy_joint_layers[i] )
+                greedy_joint_layers[i]-&gt;setLearningRate( cd_learning_rate );
+            if( greedy_joint_connections.length() &amp;&amp; greedy_joint_connections[i] )
+                greedy_joint_connections[i]-&gt;setLearningRate( cd_learning_rate );
+
             for( ; stage&lt;end_stage ; stage++ )
             {
                 if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
@@ -987,6 +1108,14 @@
                     layers[i]-&gt;setLearningRate( lr );
                     connections[i]-&gt;setLearningRate( lr );
                     layers[i+1]-&gt;setLearningRate( lr );
+                    if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[i] )
+                        greedy_target_layers[i]-&gt;setLearningRate( lr );
+                    if( greedy_target_connections.length() &amp;&amp; greedy_target_connections[i] )
+                        greedy_target_connections[i]-&gt;setLearningRate( lr );
+                    if( greedy_joint_layers.length() &amp;&amp; greedy_joint_layers[i] )
+                        greedy_joint_layers[i]-&gt;setLearningRate( lr );
+                    if( greedy_joint_connections.length() &amp;&amp; greedy_joint_connections[i] )
+                        greedy_joint_connections[i]-&gt;setLearningRate( lr );
                 }
 
                 initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
@@ -1203,6 +1332,10 @@
     real lr;
     PLASSERT(batch_size == 1);
 
+    if( greedy_target_layers.length() )
+        PLERROR(&quot;In DeepBeliefNet::onlineStep(): greedy_target_layers not implemented\n&quot;
+                &quot;for online setting&quot;);
+
     TVec&lt;Vec&gt; cost;
     if (!partial_costs.isEmpty())
         cost.resize(n_layers-1);
@@ -1428,6 +1561,10 @@
         cost.resize(n_layers-1);
     }
 
+    if( greedy_target_layers.length() )
+        PLERROR(&quot;In DeepBeliefNet::onlineStep(): greedy_target_layers not implemented\n&quot;
+                &quot;for online setting&quot;);
+
     layers[0]-&gt;setExpectations(inputs);
     // FORWARD PHASE
     //Vec layer_input;
@@ -1683,9 +1820,56 @@
     layers[0]-&gt;expectation &lt;&lt; input;
     for( int i=0 ; i&lt;=index ; i++ )
     {
-        connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
-        layers[i+1]-&gt;getAllActivations( connections[i] );
-        layers[i+1]-&gt;computeExpectation();
+        if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[i] )
+        {
+            connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+            layers[i+1]-&gt;getAllActivations( connections[i] );
+
+            if( i != index )
+            {
+                greedy_target_layers[i]-&gt;activation.clear();
+                greedy_target_layers[i]-&gt;activation += greedy_target_layers[i]-&gt;bias;
+                for( int c=0; c&lt;n_classes; c++ )
+                {
+                    // Compute class free-energy
+                    layers[i+1]-&gt;activation.toMat(layers[i+1]-&gt;size,1) += 
+                        greedy_target_connections[i]-&gt;weights.column(c);
+                    greedy_target_layers[i]-&gt;activation[c] -= 
+                        layers[i+1]-&gt;freeEnergyContribution(layers[i+1]-&gt;activation);
+                    
+                    // Compute class dependent expectation and store it
+                    layers[i+1]-&gt;expectation_is_not_up_to_date();
+                    layers[i+1]-&gt;computeExpectation();
+                    greedy_target_expectations[i][c] &lt;&lt; layers[i+1]-&gt;expectation;
+                    
+                    // Remove class-dependent energy for next free-energy computations
+                    layers[i+1]-&gt;activation.toMat(layers[i+1]-&gt;size,1) -= greedy_target_connections[i]-&gt;weights.column(c);
+                }
+                greedy_target_layers[i]-&gt;expectation_is_not_up_to_date();
+                greedy_target_layers[i]-&gt;computeExpectation();
+            
+                // Computing next layer representation
+                layers[i+1]-&gt;expectation.clear();
+                Vec expectation = layers[i+1]-&gt;expectation;
+                for( int c=0; c&lt;n_classes; c++ )
+                {
+                    Vec expectation_c = greedy_target_expectations[i][c];
+                    real p_c = greedy_target_layers[i]-&gt;expectation[c];
+                    multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+                }
+            }
+            else
+            {
+                fill_one_hot( greedy_target_layers[i]-&gt;expectation, 
+                              (int) round(target[0]), real(0.), real(1.) );
+            }
+        }
+        else
+        {
+            connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+            layers[i+1]-&gt;getAllActivations( connections[i] );
+            layers[i+1]-&gt;computeExpectation();
+        }
     }
 
     if( !partial_costs.isEmpty() &amp;&amp; partial_costs[ index ] )
@@ -1733,10 +1917,20 @@
         layers[ index+1 ]-&gt;setLearningRate( lr );
     }
 
-    contrastiveDivergenceStep( layers[ index ],
-                               connections[ index ],
-                               layers[ index+1 ],
-                               index, true);
+    if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[index] )
+    {
+        contrastiveDivergenceStep( greedy_joint_layers[ index ],
+                                   greedy_joint_connections[ index ],
+                                   layers[ index+1 ],
+                                   index, false);
+    }
+    else
+    {
+        contrastiveDivergenceStep( layers[ index ],
+                                   connections[ index ],
+                                   layers[ index+1 ],
+                                   index, true);
+    }
 }
 
 /////////////////
@@ -1749,6 +1943,11 @@
     PLASSERT( index &lt; n_layers );
 
     layers[0]-&gt;setExpectations(inputs);
+
+    if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[0] )
+        PLERROR(&quot;In DeepBeliefNet::greedyStep(): greedy_target_layers not implemented\n&quot;
+                &quot;for minibatch setting&quot;);
+
     for( int i=0 ; i&lt;=index ; i++ )
     {
         connections[i]-&gt;setAsDownInputs( layers[i]-&gt;getExpectations() );
@@ -1911,6 +2110,11 @@
 void DeepBeliefNet::upDownStep( const Vec&amp; input, const Vec&amp; target,
                                 Vec&amp; train_costs )
 {
+
+    if( greedy_target_layers.length() )
+        PLERROR(&quot;In DeepBeliefNet::onlineStep(): greedy_target_layers not implemented\n&quot;
+                &quot;for up-down setting&quot;);
+
     // Up pass
     up_sample[0] &lt;&lt; input;
     for( int i=0 ; i&lt;n_layers-2 ; i++ )
@@ -1997,18 +2201,96 @@
     layers[0]-&gt;expectation &lt;&lt; input;
     for( int i=0 ; i&lt;n_layers-2 ; i++ )
     {
-        connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
-        layers[i+1]-&gt;getAllActivations( connections[i] );
-        layers[i+1]-&gt;computeExpectation();
+        if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[i] )
+        {
+            connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+            layers[i+1]-&gt;getAllActivations( connections[i] );
+            
+            greedy_target_layers[i]-&gt;activation.clear();
+            greedy_target_layers[i]-&gt;activation += greedy_target_layers[i]-&gt;bias;
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                // Compute class free-energy
+                layers[i+1]-&gt;activation.toMat(layers[i+1]-&gt;size,1) += greedy_target_connections[i]-&gt;weights.column(c);
+                greedy_target_layers[i]-&gt;activation[c] -= layers[i+1]-&gt;freeEnergyContribution(layers[i+1]-&gt;activation);
+                
+                // Compute class dependent expectation and store it
+                layers[i+1]-&gt;expectation_is_not_up_to_date();
+                layers[i+1]-&gt;computeExpectation();
+                greedy_target_expectations[i][c] &lt;&lt; layers[i+1]-&gt;expectation;
+                
+                // Remove class-dependent energy for next free-energy computations
+                layers[i+1]-&gt;activation.toMat(layers[i+1]-&gt;size,1) -= greedy_target_connections[i]-&gt;weights.column(c);
+            }
+            greedy_target_layers[i]-&gt;expectation_is_not_up_to_date();
+            greedy_target_layers[i]-&gt;computeExpectation();
+            
+            // Computing next layer representation
+            layers[i+1]-&gt;expectation.clear();
+            Vec expectation = layers[i+1]-&gt;expectation;
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                Vec expectation_c = greedy_target_expectations[i][c];
+                real p_c = greedy_target_layers[i]-&gt;expectation[c];
+                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+            }
+        }
+        else
+        {
+            connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+            layers[i+1]-&gt;getAllActivations( connections[i] );
+            layers[i+1]-&gt;computeExpectation();
+        }
     }
 
     if( final_cost )
     {
-        connections[ n_layers-2 ]-&gt;setAsDownInput(
-            layers[ n_layers-2 ]-&gt;expectation );
-        layers[ n_layers-1 ]-&gt;getAllActivations( connections[ n_layers-2 ] );
-        layers[ n_layers-1 ]-&gt;computeExpectation();
-
+        if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[n_layers-2] )
+        {
+            connections[n_layers-2]-&gt;setAsDownInput( layers[n_layers-2]-&gt;expectation );
+            layers[n_layers-1]-&gt;getAllActivations( connections[n_layers-2] );
+            
+            greedy_target_layers[n_layers-2]-&gt;activation.clear();
+            greedy_target_layers[n_layers-2]-&gt;activation += 
+                greedy_target_layers[n_layers-2]-&gt;bias;
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                // Compute class free-energy
+                layers[n_layers-1]-&gt;activation.toMat(layers[n_layers-1]-&gt;size,1) += 
+                    greedy_target_connections[n_layers-2]-&gt;weights.column(c);
+                greedy_target_layers[n_layers-2]-&gt;activation[c] -= 
+                    layers[n_layers-1]-&gt;freeEnergyContribution(layers[n_layers-1]-&gt;activation);
+                
+                // Compute class dependent expectation and store it
+                layers[n_layers-1]-&gt;expectation_is_not_up_to_date();
+                layers[n_layers-1]-&gt;computeExpectation();
+                greedy_target_expectations[n_layers-2][c] &lt;&lt; layers[n_layers-1]-&gt;expectation;
+                
+                // Remove class-dependent energy for next free-energy computations
+                layers[n_layers-1]-&gt;activation.toMat(layers[n_layers-1]-&gt;size,1) -= 
+                    greedy_target_connections[n_layers-2]-&gt;weights.column(c);
+            }
+            greedy_target_layers[n_layers-2]-&gt;expectation_is_not_up_to_date();
+            greedy_target_layers[n_layers-2]-&gt;computeExpectation();
+            
+            // Computing next layer representation
+            layers[n_layers-1]-&gt;expectation.clear();
+            Vec expectation = layers[n_layers-1]-&gt;expectation;
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                Vec expectation_c = greedy_target_expectations[n_layers-2][c];
+                real p_c = greedy_target_layers[n_layers-2]-&gt;expectation[c];
+                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+            }
+        }
+        else
+        {
+            connections[ n_layers-2 ]-&gt;setAsDownInput(
+                layers[ n_layers-2 ]-&gt;expectation );
+            layers[ n_layers-1 ]-&gt;getAllActivations( connections[ n_layers-2 ] );
+            layers[ n_layers-1 ]-&gt;computeExpectation();
+        }
+        
         if( final_module )
         {
             final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
@@ -2036,17 +2318,78 @@
         train_costs.subVec(final_cost_index, final_cost_value.length())
             &lt;&lt; final_cost_value;
 
-        layers[ n_layers-1 ]-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;activation,
-                                           layers[ n_layers-1 ]-&gt;expectation,
-                                           activation_gradients[ n_layers-1 ],
-                                           expectation_gradients[ n_layers-1 ]
-                                         );
+        if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[n_layers-2] )
+        {
+            activation_gradients[n_layers-1].clear();
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                greedy_target_expectation_gradients[n_layers-2][c] &lt;&lt; 
+                    expectation_gradients[ n_layers-1 ];
+                greedy_target_expectation_gradients[n_layers-2][c] *= 
+                    greedy_target_layers[n_layers-2]-&gt;expectation[c];
+                layers[ n_layers-1 ]-&gt;bpropUpdate( 
+                    greedy_target_activations[n_layers-2][c],
+                    greedy_target_expectations[n_layers-2][c],
+                    greedy_target_activation_gradients[n_layers-2][c],
+                    greedy_target_expectation_gradients[n_layers-2][c] );
 
-        connections[ n_layers-2 ]-&gt;bpropUpdate(
-            layers[ n_layers-2 ]-&gt;expectation,
-            layers[ n_layers-1 ]-&gt;activation,
-            expectation_gradients[ n_layers-2 ],
-            activation_gradients[ n_layers-1 ] );
+                activation_gradients[n_layers-1] += 
+                    greedy_target_activation_gradients[n_layers-2][c];
+
+                // Update target connections, with gradient from p(h_l | h_l-1, y)
+                multiplyScaledAdd( greedy_target_activation_gradients[n_layers-2][c].toMat(layers[n_layers-1]-&gt;size,1),
+                                   1., -greedy_target_connections[n_layers-2]-&gt;learning_rate,
+                                   greedy_target_connections[n_layers-2]-&gt;weights.column(c));
+                
+                greedy_target_probability_gradients[n_layers-2][c] = 
+                    dot( expectation_gradients[ n_layers-1 ], 
+                         greedy_target_expectations[ n_layers-2 ][c] );
+            }
+
+            // Update bias
+            greedy_target_layers[n_layers-2]-&gt;bpropUpdate(
+                greedy_target_layers[n_layers-2]-&gt;expectation, // Isn't used
+                greedy_target_layers[n_layers-2]-&gt;expectation,
+                greedy_target_probability_gradients[n_layers-2], 
+                greedy_target_probability_gradients[n_layers-2] );
+
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                layers[n_layers-1]-&gt;freeEnergyContributionGradient(
+                    greedy_target_activations[n_layers-2][c],
+                    greedy_target_activation_gradients[n_layers-2][c], // Overwrite previous activation gradient
+                    -greedy_target_probability_gradients[n_layers-2][c] );
+
+                activation_gradients[n_layers-1] += 
+                    greedy_target_activation_gradients[n_layers-2][c];
+
+                // Update target connections, with gradient from p(y | h_l-1 )
+                multiplyScaledAdd( greedy_target_activation_gradients[n_layers-2][c].toMat(layers[n_layers-1]-&gt;size,1),
+                                   1., -greedy_target_connections[n_layers-2]-&gt;learning_rate,
+                                   greedy_target_connections[n_layers-2]-&gt;weights.column(c));
+            }
+
+            connections[ n_layers-2 ]-&gt;bpropUpdate(
+                layers[ n_layers-2 ]-&gt;expectation,
+                layers[ n_layers-1 ]-&gt;activation, //Not really, but this isn't used for matrix connections
+                expectation_gradients[ n_layers-2 ],
+                activation_gradients[ n_layers-1 ] );
+            
+        }
+        else
+        {
+            layers[ n_layers-1 ]-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;activation,
+                                               layers[ n_layers-1 ]-&gt;expectation,
+                                               activation_gradients[ n_layers-1 ],
+                                               expectation_gradients[ n_layers-1 ]
+                );
+            
+            connections[ n_layers-2 ]-&gt;bpropUpdate(
+                layers[ n_layers-2 ]-&gt;expectation,
+                layers[ n_layers-1 ]-&gt;activation,
+                expectation_gradients[ n_layers-2 ],
+                activation_gradients[ n_layers-1 ] );
+        }
     }
     else  {
         expectation_gradients[ n_layers-2 ].clear();
@@ -2082,21 +2425,85 @@
 
     for( int i=n_layers-2 ; i&gt;0 ; i-- )
     {
-        layers[i]-&gt;bpropUpdate( layers[i]-&gt;activation,
-                                layers[i]-&gt;expectation,
-                                activation_gradients[i],
-                                expectation_gradients[i] );
+        if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[i] )
+        {
+            activation_gradients[i-1].clear();
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                greedy_target_expectation_gradients[i-1][c] &lt;&lt; 
+                    expectation_gradients[ i ];
+                greedy_target_expectation_gradients[i-1][c] *= 
+                    greedy_target_layers[i-1]-&gt;expectation[c];
+                layers[ i ]-&gt;bpropUpdate( 
+                    greedy_target_activations[i-1][c],
+                    greedy_target_expectations[i-1][c],
+                    greedy_target_activation_gradients[i-1][c],
+                    greedy_target_expectation_gradients[i-1][c] );
 
-        connections[i-1]-&gt;bpropUpdate( layers[i-1]-&gt;expectation,
-                                       layers[i]-&gt;activation,
-                                       expectation_gradients[i-1],
-                                       activation_gradients[i] );
+                activation_gradients[i ] += 
+                    greedy_target_activation_gradients[i-1][c];
+
+                // Update target connections, with gradient from p(h_l | h_l-1, y)
+                multiplyScaledAdd( greedy_target_activation_gradients[i-1][c].toMat(layers[i]-&gt;size,1),
+                                   1., -greedy_target_connections[i-1]-&gt;learning_rate,
+                                   greedy_target_connections[i-1]-&gt;weights.column(c));
+                
+                greedy_target_probability_gradients[i-1][c] = 
+                    dot( expectation_gradients[ i ], 
+                         greedy_target_expectations[ i-1 ][c] );
+            }
+
+            // Update bias
+            greedy_target_layers[i-1]-&gt;bpropUpdate(
+                greedy_target_layers[i-1]-&gt;expectation, // Isn't used
+                greedy_target_layers[i-1]-&gt;expectation,
+                greedy_target_probability_gradients[i-1], 
+                greedy_target_probability_gradients[i-1] );
+
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                layers[i]-&gt;freeEnergyContributionGradient(
+                    greedy_target_activations[i-1][c],
+                    greedy_target_activation_gradients[i-1][c], // Overwrite previous activation gradient
+                    -greedy_target_probability_gradients[i-1][c] );
+
+                activation_gradients[i] += 
+                    greedy_target_activation_gradients[i-1][c];
+
+                // Update target connections, with gradient from p(y | h_l-1 )
+                multiplyScaledAdd( greedy_target_activation_gradients[i-1][c].toMat(layers[i]-&gt;size,1),
+                                   1., -greedy_target_connections[i-1]-&gt;learning_rate,
+                                   greedy_target_connections[i-1]-&gt;weights.column(c));
+            }
+
+            connections[ i-1 ]-&gt;bpropUpdate(
+                layers[ i-1 ]-&gt;expectation,
+                layers[ i ]-&gt;activation, //Not really, but this isn't used for matrix connections
+                expectation_gradients[ i-1 ],
+                activation_gradients[ i ] );
+        }
+        else
+        {
+            layers[i]-&gt;bpropUpdate( layers[i]-&gt;activation,
+                                    layers[i]-&gt;expectation,
+                                    activation_gradients[i],
+                                    expectation_gradients[i] );
+            
+            connections[i-1]-&gt;bpropUpdate( layers[i-1]-&gt;expectation,
+                                           layers[i]-&gt;activation,
+                                           expectation_gradients[i-1],
+                                           activation_gradients[i] );
+        }
     }
 }
 
 void DeepBeliefNet::fineTuningStep(const Mat&amp; inputs, const Mat&amp; targets,
                                    Mat&amp; train_costs)
 {
+    if( greedy_target_layers.length() )
+        PLERROR(&quot;In DeepBeliefNet::fineTuningStep(): greedy_target_layers not implemented\n&quot;
+                &quot;for minibatch setting&quot;);
+
     final_cost_values.resize(0, 0);
     // fprop
     layers[0]-&gt;getExpectations() &lt;&lt; inputs;
@@ -2530,10 +2937,46 @@
 
     for( int i=0 ; i&lt;n_layers-2 ; i++ )
     {
-        connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
-        layers[i+1]-&gt;getAllActivations( connections[i] );
-        layers[i+1]-&gt;computeExpectation();
-
+        if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[i] )
+        {
+            connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+            layers[i+1]-&gt;getAllActivations( connections[i] );
+            
+            greedy_target_layers[i]-&gt;activation.clear();
+            greedy_target_layers[i]-&gt;activation += greedy_target_layers[i]-&gt;bias;
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                // Compute class free-energy
+                layers[i+1]-&gt;activation.toMat(layers[i+1]-&gt;size,1) += greedy_target_connections[i]-&gt;weights.column(c);
+                greedy_target_layers[i]-&gt;activation[c] -= layers[i+1]-&gt;freeEnergyContribution(layers[i+1]-&gt;activation);
+                
+                // Compute class dependent expectation and store it
+                layers[i+1]-&gt;expectation_is_not_up_to_date();
+                layers[i+1]-&gt;computeExpectation();
+                greedy_target_expectations[i][c] &lt;&lt; layers[i+1]-&gt;expectation;
+                
+                // Remove class-dependent energy for next free-energy computations
+                layers[i+1]-&gt;activation.toMat(layers[i+1]-&gt;size,1) -= greedy_target_connections[i]-&gt;weights.column(c);
+            }
+            greedy_target_layers[i]-&gt;expectation_is_not_up_to_date();
+            greedy_target_layers[i]-&gt;computeExpectation();
+            
+            // Computing next layer representation
+            layers[i+1]-&gt;expectation.clear();
+            Vec expectation = layers[i+1]-&gt;expectation;
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                Vec expectation_c = greedy_target_expectations[i][c];
+                real p_c = greedy_target_layers[i]-&gt;expectation[c];
+                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+            }
+        }
+        else
+        {
+            connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+            layers[i+1]-&gt;getAllActivations( connections[i] );
+            layers[i+1]-&gt;computeExpectation();
+        }
         if( i_output_layer==i &amp;&amp; (!use_classification_cost &amp;&amp; !final_module))
         {
             output.resize(outputsize());
@@ -2569,10 +3012,51 @@
 
     if( final_cost || (!partial_costs.isEmpty() &amp;&amp; partial_costs[n_layers-2] ))
     {
-        connections[ n_layers-2 ]-&gt;setAsDownInput(
-            layers[ n_layers-2 ]-&gt;expectation );
-        layers[ n_layers-1 ]-&gt;getAllActivations( connections[ n_layers-2 ] );
-        layers[ n_layers-1 ]-&gt;computeExpectation();
+        if( greedy_target_layers.length() &amp;&amp; greedy_target_layers[n_layers-2] )
+        {
+            connections[n_layers-2]-&gt;setAsDownInput( layers[n_layers-2]-&gt;expectation );
+            layers[n_layers-1]-&gt;getAllActivations( connections[n_layers-2] );
+            
+            greedy_target_layers[n_layers-2]-&gt;activation.clear();
+            greedy_target_layers[n_layers-2]-&gt;activation += 
+                greedy_target_layers[n_layers-2]-&gt;bias;
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                // Compute class free-energy
+                layers[n_layers-1]-&gt;activation.toMat(layers[n_layers-1]-&gt;size,1) += 
+                    greedy_target_connections[n_layers-2]-&gt;weights.column(c);
+                greedy_target_layers[n_layers-2]-&gt;activation[c] -= 
+                    layers[n_layers-1]-&gt;freeEnergyContribution(layers[n_layers-1]-&gt;activation);
+                
+                // Compute class dependent expectation and store it
+                layers[n_layers-1]-&gt;expectation_is_not_up_to_date();
+                layers[n_layers-1]-&gt;computeExpectation();
+                greedy_target_expectations[n_layers-2][c] &lt;&lt; layers[n_layers-1]-&gt;expectation;
+                
+                // Remove class-dependent energy for next free-energy computations
+                layers[n_layers-1]-&gt;activation.toMat(layers[n_layers-1]-&gt;size,1) -= 
+                    greedy_target_connections[n_layers-2]-&gt;weights.column(c);
+            }
+            greedy_target_layers[n_layers-2]-&gt;expectation_is_not_up_to_date();
+            greedy_target_layers[n_layers-2]-&gt;computeExpectation();
+            
+            // Computing next layer representation
+            layers[n_layers-1]-&gt;expectation.clear();
+            Vec expectation = layers[n_layers-1]-&gt;expectation;
+            for( int c=0; c&lt;n_classes; c++ )
+            {
+                Vec expectation_c = greedy_target_expectations[n_layers-2][c];
+                real p_c = greedy_target_layers[n_layers-2]-&gt;expectation[c];
+                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+            }
+        }
+        else
+        {
+            connections[ n_layers-2 ]-&gt;setAsDownInput(
+                layers[ n_layers-2 ]-&gt;expectation );
+            layers[ n_layers-1 ]-&gt;getAllActivations( connections[ n_layers-2 ] );
+            layers[ n_layers-1 ]-&gt;computeExpectation();
+        }
 
         if( final_module )
         {
@@ -2656,6 +3140,19 @@
             }
     }
 
+    if( !greedy_target_layers.isEmpty() )
+    {
+        target_one_hot.clear();
+        fill_one_hot( target_one_hot, 
+                      (int) round(target[0]), real(0.), real(1.) );
+        for( int i=0 ; i&lt;n_layers-1 ; i++ )
+            if( greedy_target_layers[i] )
+                costs[greedy_target_layer_nlls_index+i] = 
+                    greedy_target_layers[i]-&gt;fpropNLL(target_one_hot);
+            else
+                costs[greedy_target_layer_nlls_index+i] = MISSING_VALUE;
+    }
+
     if (reconstruct_layerwise)
         costs.subVec(reconstruction_cost_index, reconstruction_costs.length())
             &lt;&lt; reconstruction_costs;
@@ -2831,6 +3328,12 @@
 
     for( int i=0 ; i&lt;generative_connections.length() ; i++ )
         generative_connections[i]-&gt;setLearningRate( the_learning_rate );
+
+    for( int i=0; i&lt;greedy_target_connections.length(); i++ )
+        greedy_target_connections[i]-&gt;setLearningRate( the_learning_rate );
+
+    for( int i=0; i&lt;greedy_target_layers.length(); i++ )
+        greedy_target_layers[i]-&gt;setLearningRate( the_learning_rate );
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2009-01-28 17:51:31 UTC (rev 9881)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2009-01-28 21:55:40 UTC (rev 9882)
@@ -140,6 +140,12 @@
     //! The weights of the connections between the layers
     TVec&lt; PP&lt;RBMConnection&gt; &gt; connections;
 
+    //! Optional target layers for greedy layer-wise pretraining
+    TVec&lt; PP&lt;RBMMultinomialLayer&gt; &gt; greedy_target_layers;
+
+    //! Optional target matrix connections for greedy layer-wise pretraining
+    TVec&lt; PP&lt;RBMMatrixConnection&gt; &gt; greedy_target_connections;
+
     //! Optional module that takes as input the output of the last layer
     //! (layers[n_layers-1), and its output is fed to final_cost, and
     //! concatenated with the one of classification_cost (if present) as output
@@ -339,6 +345,15 @@
     //! (at the output of the layers)
     mutable TVec&lt;Vec&gt; expectation_gradients;
     mutable TVec&lt;Mat&gt; expectations_gradients; //!&lt; For mini-batch.
+    
+    //! For the fprop with greedy_target_layers
+    mutable TVec&lt; TVec&lt;Vec&gt; &gt; greedy_target_expectations;
+    mutable TVec&lt; TVec&lt;Vec&gt; &gt; greedy_target_activations;
+    mutable TVec&lt; TVec&lt;Vec&gt; &gt; greedy_target_expectation_gradients;
+    mutable TVec&lt; TVec&lt;Vec&gt; &gt; greedy_target_activation_gradients;
+    mutable TVec&lt; Vec &gt; greedy_target_probability_gradients;
+    mutable TVec&lt; PP&lt;RBMLayer&gt; &gt; greedy_joint_layers;
+    mutable TVec&lt; PP&lt;RBMConnection&gt; &gt; greedy_joint_connections;
 
     mutable Vec final_cost_input;
     mutable Mat final_cost_inputs; //!&lt; For mini-batch.
@@ -382,6 +397,9 @@
     //! Used to store the costs optimized by the final cost module.
     Vec optimized_costs;
 
+    //! One-hot representation of the target
+    mutable Vec target_one_hot;
+
     //! Stores reconstruction costs
     mutable Vec reconstruction_costs;
 
@@ -400,6 +418,9 @@
     //! Keeps the beginning index of the reconstruction costs in train_costs
     int reconstruction_cost_index;
 
+    //! Keeps the beginning index of the greedy target layer NLLs
+    int greedy_target_layer_nlls_index;
+
     //! Index of the cpu time cost (per each call of train())
     int training_cpu_time_cost_index;
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003321.html">[Plearn-commits] r9881 - trunk/python_modules/plearn/pymake
</A></li>
	<LI>Next message: <A HREF="003323.html">[Plearn-commits] r9883 - trunk/plearn_learners_experimental
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3322">[ date ]</a>
              <a href="thread.html#3322">[ thread ]</a>
              <a href="subject.html#3322">[ subject ]</a>
              <a href="author.html#3322">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
