<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9859 - in	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results:	. expdir expdir/Split0
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2009-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9859%20-%20in%0A%09trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results%3A%0A%09.%20expdir%20expdir/Split0&In-Reply-To=%3C200901201925.n0KJPwBE020838%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003298.html">
   <LINK REL="Next"  HREF="003300.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9859 - in	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results:	. expdir expdir/Split0</H1>
    <B>nouiz at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9859%20-%20in%0A%09trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results%3A%0A%09.%20expdir%20expdir/Split0&In-Reply-To=%3C200901201925.n0KJPwBE020838%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9859 - in	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results:	. expdir expdir/Split0">nouiz at mail.berlios.de
       </A><BR>
    <I>Tue Jan 20 20:25:58 CET 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="003298.html">[Plearn-commits] r9858 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="003300.html">[Plearn-commits] r9860 - in	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir:	. Split0 Split0/LearnerExpdir
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3299">[ date ]</a>
              <a href="thread.html#3299">[ thread ]</a>
              <a href="subject.html#3299">[ subject ]</a>
              <a href="author.html#3299">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: nouiz
Date: 2009-01-20 20:25:57 +0100 (Tue, 20 Jan 2009)
New Revision: 9859

Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
Log:
updated test following commit about RegressionTreeLeave::id default value of 0.


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log	2009-01-20 19:07:13 UTC (rev 9858)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log	2009-01-20 19:25:57 UTC (rev 9859)
@@ -1,121 +1,121 @@
 HyperLearner: starting the optimization
 split_cols: 2 2 2 
 split_values: 0.00125079586853901747 0.000357032461916012567 0.000981625552665510437 
-weak learner at stage 0 has average loss = 0.02
+weak learner at stage 0 has average loss = 0.0200000000000000004
 split_cols: 2 1 2 
 split_values: 0.991025168386145405 0.482293993618237549 0.891579732096156263 
-weak learner at stage 0 has average loss = 0.08
+weak learner at stage 0 has average loss = 0.0800000000000000017
 split_cols: 4 3 2 
 split_values: 3.23307269844974599e-10 0.698651129400676418 0.000537488498421501149 
-weak learner at stage 1 has average loss = 0.0306122
+weak learner at stage 1 has average loss = 0.0306122448979592558
 split_cols: 4 1 1 
 split_values: 1.54709578481515564e-13 0.588552410435003615 0.568316236782665074 
-weak learner at stage 1 has average loss = 0.13587
+weak learner at stage 1 has average loss = 0.135869565217391769
 split_cols: 2 0 3 
 split_values: 0.000528285193333644099 0.624507340564582236 0.406995257960652612 
-weak learner at stage 2 has average loss = 0.136257
+weak learner at stage 2 has average loss = 0.136257309941520577
 split_cols: 1 3 2 
 split_values: 0.526575453102100632 0.924226804347039965 0.995245802370935517 
-weak learner at stage 2 has average loss = 0.204444
+weak learner at stage 2 has average loss = 0.204444444444444567
 split_cols: 2 3 1 
 split_values: 0.000528285193333644099 0.406995257960652612 0.399391565505198165 
-weak learner at stage 3 has average loss = 0.119855
+weak learner at stage 3 has average loss = 0.119854943177360632
 split_cols: 2 0 3 
 split_values: 0.997650553369808346 0.441781515973124872 0.310957185430505323 
-weak learner at stage 3 has average loss = 0.179198
+weak learner at stage 3 has average loss = 0.179197735115788404
 split_cols: 2 1 2 
 split_values: 0.196634593877310471 0.36967671457248541 0.00125079586853901747 
-weak learner at stage 4 has average loss = 0.103659
+weak learner at stage 4 has average loss = 0.103659200908568977
 split_cols: 3 3 2 
 split_values: 0.141930657011306749 0.0564509465831030677 0.997650553369808346 
-weak learner at stage 4 has average loss = 0.27129
+weak learner at stage 4 has average loss = 0.271289700162368308
 split_cols: 0 3 1 
 split_values: 0.624507340564582236 0.698651129400676418 0.370088477642079638 
-weak learner at stage 5 has average loss = 0.0730531
+weak learner at stage 5 has average loss = 0.0730530888838650527
 split_cols: 2 4 4 
 split_values: 0.00363231682035125569 0.999999964757892545 0.999999999538717876 
-weak learner at stage 5 has average loss = 0.217388
+weak learner at stage 5 has average loss = 0.217387633059202834
 split_cols: 2 0 3 
 split_values: 0.00125079586853901747 0.624507340564582236 0.763338630405507312 
-weak learner at stage 6 has average loss = 0.140932
+weak learner at stage 6 has average loss = 0.140931903580067019
 split_cols: 1 1 1 
 split_values: 0.564858493389006289 0.479480044756096346 0.379258691801199865 
-weak learner at stage 6 has average loss = 0.262423
+weak learner at stage 6 has average loss = 0.262422978648221006
 split_cols: 2 3 1 
 split_values: 0.000357032461916012567 0.130712305658957473 0.399391565505198165 
-weak learner at stage 7 has average loss = 0.214419
+weak learner at stage 7 has average loss = 0.214419375868369705
 split_cols: 4 1 0 
 split_values: 0.999999999999999334 0.569140400436275673 0.318872618050356049 
-weak learner at stage 7 has average loss = 0.26615
+weak learner at stage 7 has average loss = 0.266150190899056949
 split_cols: 1 3 1 
 split_values: 0.401581050193795197 0.360834998492078562 0.370088477642079638 
-weak learner at stage 8 has average loss = 0.135994
+weak learner at stage 8 has average loss = 0.135994152283819725
 split_cols: 2 1 1 
 split_values: 0.0696715340410815898 0.502718698860307178 0.526575453102100632 
-weak learner at stage 8 has average loss = 0.270747
+weak learner at stage 8 has average loss = 0.270746640671640226
 split_cols: 2 0 1 
 split_values: 0.00125079586853901747 0.624507340564582236 0.370088477642079638 
-weak learner at stage 9 has average loss = 0.154551
+weak learner at stage 9 has average loss = 0.154550829757939212
 split_cols: 1 1 1 
 split_values: 0.502718698860307178 0.479480044756096346 0.6805672568090122 
-weak learner at stage 9 has average loss = 0.210479
+weak learner at stage 9 has average loss = 0.210479215655181712
 split_cols: 2 2 2 
 split_values: 0.000357032461916012567 0.000981625552665510437 0.00125079586853901747 
-weak learner at stage 10 has average loss = 0.108164
+weak learner at stage 10 has average loss = 0.10816358087998762
 split_cols: 1 1 2 
 split_values: 0.662011169718969006 0.6805672568090122 0.122353510242232788 
-weak learner at stage 10 has average loss = 0.272979
+weak learner at stage 10 has average loss = 0.27297941302980544
 split_cols: 1 1 1 
 split_values: 0.401581050193795197 0.332158208341527428 0.272305619535323673 
-weak learner at stage 11 has average loss = 0.132455
+weak learner at stage 11 has average loss = 0.132455305997067535
 split_cols: 3 0 1 
 split_values: 0.141930657011306749 0.40739065223433224 0.574273867344056166 
-weak learner at stage 11 has average loss = 0.315862
+weak learner at stage 11 has average loss = 0.315861838715847165
 split_cols: 4 1 3 
 split_values: 7.04158953368505536e-14 0.384853138944362905 0.662373884583906003 
-weak learner at stage 12 has average loss = 0.110589
+weak learner at stage 12 has average loss = 0.110588516596623374
 split_cols: 1 1 2 
 split_values: 0.662011169718969006 0.6805672568090122 0.00363231682035125569 
-weak learner at stage 12 has average loss = 0.327692
+weak learner at stage 12 has average loss = 0.327691610360699437
 split_cols: 3 2 0 
 split_values: 0.357489402445920146 0.0015353418386078177 0.624507340564582236 
-weak learner at stage 13 has average loss = 0.181377
+weak learner at stage 13 has average loss = 0.181377011933805232
 split_cols: 1 4 1 
 split_values: 0.379258691801199865 8.80684414283905426e-14 0.479480044756096346 
-weak learner at stage 13 has average loss = 0.350098
+weak learner at stage 13 has average loss = 0.350097661251061343
 split_cols: 2 2 2 
 split_values: 0.000981625552665510437 0.000357032461916012567 0.196634593877310471 
-weak learner at stage 14 has average loss = 0.12825
+weak learner at stage 14 has average loss = 0.128250465944868525
 split_cols: 1 4 4 
 split_values: 0.564858493389006289 0.999999999999999334 0.999999991749624728 
-weak learner at stage 14 has average loss = 0.30729
+weak learner at stage 14 has average loss = 0.307290463636907374
 split_cols: 2 3 0 
 split_values: 0.000528285193333644099 0.406995257960652612 0.624507340564582236 
-weak learner at stage 15 has average loss = 0.207133
+weak learner at stage 15 has average loss = 0.207132868740156156
 split_cols: 1 1 1 
 split_values: 0.526575453102100632 0.544178240629241028 0.502718698860307178 
-weak learner at stage 15 has average loss = 0.329174
+weak learner at stage 15 has average loss = 0.329173823067465954
 split_cols: 2 2 2 
 split_values: 0.00125079586853901747 0.000981625552665510437 0.000528285193333644099 
-weak learner at stage 16 has average loss = 0.151516
+weak learner at stage 16 has average loss = 0.151516393229420981
 split_cols: 2 2 2 
 split_values: 0.995010648391392527 0.991025168386145405 0.977100614750671781 
-weak learner at stage 16 has average loss = 0.306159
+weak learner at stage 16 has average loss = 0.306159378549761108
 split_cols: 1 3 2 
 split_values: 0.36967671457248541 0.406995257960652612 0.000254178900377460826 
-weak learner at stage 17 has average loss = 0.179798
+weak learner at stage 17 has average loss = 0.179797569635924248
 split_cols: 2 2 1 
 split_values: 0.885239426681956321 0.627448174543832948 0.482293993618237549 
-weak learner at stage 17 has average loss = 0.348588
+weak learner at stage 17 has average loss = 0.348588375382964111
 split_cols: 0 3 1 
 split_values: 0.624507340564582236 0.698651129400676418 0.370088477642079638 
-weak learner at stage 18 has average loss = 0.126962
+weak learner at stage 18 has average loss = 0.12696171488450525
 split_cols: 2 4 3 
 split_values: 0.997650553369808346 0.999999999999998335 0.782913532932723921 
-weak learner at stage 18 has average loss = 0.283838
+weak learner at stage 18 has average loss = 0.283838119869411709
 split_cols: 2 2 2 
 split_values: 0.00125079586853901747 0.000981625552665510437 0.000528285193333644099 
-weak learner at stage 19 has average loss = 0.174564
+weak learner at stage 19 has average loss = 0.174564396795390542
 split_cols: 1 1 1 
 split_values: 0.662011169718969006 0.6805672568090122 0.502718698860307178 
-weak learner at stage 19 has average loss = 0.34564
+weak learner at stage 19 has average loss = 0.345640057938873146

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:07:13 UTC (rev 9858)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:25:57 UTC (rev 9859)
@@ -475,7 +475,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *23 -&gt;RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -521,6 +521,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 1 ;
@@ -766,7 +767,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *41 -&gt;RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -812,6 +813,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 1 ;
@@ -849,7 +851,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *44 -&gt;RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -895,6 +897,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
@@ -911,6 +914,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ;
+forward_test = 0 ;
 train_time = 0 ;
 total_train_time = 0 ;
 test_time = 0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-01-20 19:07:13 UTC (rev 9858)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-01-20 19:25:57 UTC (rev 9859)
@@ -1,4 +1,4 @@
-__REVISION__ = &quot;PL9749&quot;
+__REVISION__ = &quot;PL9855&quot;
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-01-20 19:07:13 UTC (rev 9858)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-01-20 19:25:57 UTC (rev 9859)
@@ -75,7 +75,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *9 -&gt;RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -121,6 +121,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
@@ -157,7 +158,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *12 -&gt;RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -203,6 +204,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
@@ -240,7 +242,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *15 -&gt;RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -286,6 +288,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
@@ -302,6 +305,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ;
+forward_test = 0 ;
 train_time = 0 ;
 total_train_time = 0 ;
 test_time = 0 ;


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003298.html">[Plearn-commits] r9858 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="003300.html">[Plearn-commits] r9860 - in	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir:	. Split0 Split0/LearnerExpdir
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3299">[ date ]</a>
              <a href="thread.html#3299">[ thread ]</a>
              <a href="subject.html#3299">[ subject ]</a>
              <a href="author.html#3299">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
