<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9845 - in trunk: commands plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2009-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9845%20-%20in%20trunk%3A%20commands%20plearn_learners/online&In-Reply-To=%3C200901162147.n0GLlVHj003883%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003284.html">
   <LINK REL="Next"  HREF="003286.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9845 - in trunk: commands plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9845%20-%20in%20trunk%3A%20commands%20plearn_learners/online&In-Reply-To=%3C200901162147.n0GLlVHj003883%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9845 - in trunk: commands plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Fri Jan 16 22:47:31 CET 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="003284.html">[Plearn-commits] r9844 - trunk/plearn/vmat
</A></li>
        <LI>Next message: <A HREF="003286.html">[Plearn-commits] r9846 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3285">[ date ]</a>
              <a href="thread.html#3285">[ thread ]</a>
              <a href="subject.html#3285">[ subject ]</a>
              <a href="author.html#3285">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2009-01-16 22:47:31 +0100 (Fri, 16 Jan 2009)
New Revision: 9845

Added:
   trunk/plearn_learners/online/SoftmaxNLLCostModule.cc
   trunk/plearn_learners/online/SoftmaxNLLCostModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Softmax and NLL in the same module, to avoid precision loss in the gradient
computation.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2009-01-16 21:45:30 UTC (rev 9844)
+++ trunk/commands/plearn_noblas_inc.h	2009-01-16 21:47:31 UTC (rev 9845)
@@ -252,6 +252,7 @@
 #include &lt;plearn_learners/online/ScaleGradientModule.h&gt;
 #include &lt;plearn_learners/online/ShuntingNNetLayerModule.h&gt;
 #include &lt;plearn_learners/online/SoftmaxModule.h&gt;
+#include &lt;plearn_learners/online/SoftmaxNLLCostModule.h&gt;
 #include &lt;plearn_learners/online/SplitModule.h&gt;
 #include &lt;plearn_learners/online/SquaredErrorCostModule.h&gt;
 #include &lt;plearn_learners/online/BinarizeModule.h&gt;

Added: trunk/plearn_learners/online/SoftmaxNLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SoftmaxNLLCostModule.cc	2009-01-16 21:45:30 UTC (rev 9844)
+++ trunk/plearn_learners/online/SoftmaxNLLCostModule.cc	2009-01-16 21:47:31 UTC (rev 9845)
@@ -0,0 +1,325 @@
+// -*- C++ -*-
+
+// SoftmaxNLLCostModule.cc
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file SoftmaxNLLCostModule.cc */
+
+
+
+#include &quot;SoftmaxNLLCostModule.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    SoftmaxNLLCostModule,
+    &quot;Like SoftmaxModule and NLLCostModule, with more precision.&quot;,
+    &quot;If target is the index of the true class, this module computes\n&quot;
+    &quot;    cost = -log( softmax(input)[target] ),\n&quot;
+    &quot;and back-propagates the gradient and diagonal of Hessian.\n&quot;);
+
+SoftmaxNLLCostModule::SoftmaxNLLCostModule()
+{
+    output_size = 1;
+    target_size = 1;
+}
+
+void SoftmaxNLLCostModule::declareOptions(OptionList&amp; ol)
+{
+    // declareOption(ol, &quot;myoption&quot;, &amp;SoftmaxNLLCostModule::myoption,
+    //               OptionBase::buildoption,
+    //               &quot;Help text describing this option&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void SoftmaxNLLCostModule::build_()
+{
+}
+
+// ### Nothing to add here, simply calls build_
+void SoftmaxNLLCostModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void SoftmaxNLLCostModule::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+
+///////////
+// fprop //
+///////////
+void SoftmaxNLLCostModule::fprop(const Vec&amp; input, const Vec&amp; target, Vec&amp; cost) const
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
+
+    tmp_vec.resize(input_size);
+    cost.resize(output_size);
+
+    if (input.hasMissing())
+        cost[0] = MISSING_VALUE;
+    else
+    {
+        int the_target = (int) round( target[0] );
+        log_softmax(input, tmp_vec);
+        cost[0] = - tmp_vec[the_target];
+    }
+}
+
+void SoftmaxNLLCostModule::fprop(const Mat&amp; inputs, const Mat&amp; targets, Mat&amp; costs)
+    const
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+
+    int batch_size = inputs.length();
+    PLASSERT( inputs.length() == batch_size );
+    PLASSERT( targets.length() == batch_size );
+
+    tmp_vec.resize(input_size);
+    costs.resize(batch_size, output_size);
+
+    for( int k=0; k&lt;batch_size; k++ )
+    {
+        if (inputs(k).hasMissing())
+            costs(k, 0) = MISSING_VALUE;
+        else
+        {
+            int target_k = (int) round( targets(k, 0) );
+            log_softmax(inputs(k), tmp_vec);
+            costs(k, 0) = - tmp_vec[target_k];
+        }
+    }
+}
+
+void SoftmaxNLLCostModule::fprop(const TVec&lt;Mat*&gt;&amp; ports_value)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+
+    Mat* prediction = ports_value[0];
+    Mat* target = ports_value[1];
+    Mat* cost = ports_value[2];
+
+    // If we have prediction and target, and we want cost
+    if( prediction &amp;&amp; !prediction-&gt;isEmpty()
+        &amp;&amp; target &amp;&amp; !target-&gt;isEmpty()
+        &amp;&amp; cost &amp;&amp; cost-&gt;isEmpty() )
+
+    {
+        PLASSERT( prediction-&gt;width() == port_sizes(0, 1) );
+        PLASSERT( target-&gt;width() == port_sizes(1, 1) );
+
+        int batch_size = prediction-&gt;length();
+        PLASSERT( target-&gt;length() == batch_size );
+
+        cost-&gt;resize(batch_size, port_sizes(2, 1));
+
+
+        for( int i=0; i&lt;batch_size; i++ )
+        {
+            if( (*prediction)(i).hasMissing() || is_missing((*target)(i,0)) )
+                (*cost)(i,0) = MISSING_VALUE;
+            else
+            {
+                int target_i = (int) round( (*target)(i,0) );
+                PLASSERT( is_equal( (*target)(i, 0), target_i ) );
+                log_softmax( (*prediction)(i), tmp_vec );
+                (*cost)(i,0) = - tmp_vec[target_i];
+            }
+        }
+    }
+    else if( !prediction &amp;&amp; !target &amp;&amp; !cost )
+        return;
+    else
+        PLCHECK_MSG( false, &quot;Unknown port configuration&quot; );
+
+    checkProp(ports_value);
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void SoftmaxNLLCostModule::bpropUpdate(
+        const Vec&amp; input, const Vec&amp; target, real cost,
+        Vec&amp; input_gradient, bool accumulate)
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
+    int the_target = (int) round( target[0] );
+
+    // input_gradient[ i ] = softmax(x)[i] if i != t,
+    // input_gradient[ t ] = softmax(x)[t] - 1.
+    softmax(input, input_gradient);
+    input_gradient[ the_target ] -= 1.;
+}
+
+void SoftmaxNLLCostModule::bpropUpdate(
+        const Mat&amp; inputs, const Mat&amp; targets, const Vec&amp; costs,
+        Mat&amp; input_gradients, bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size &amp;&amp;
+                input_gradients.length() == inputs.length(),
+                &quot;Cannot resize input_gradients and accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), input_size );
+        input_gradients.clear();
+    }
+
+    // input_gradient[ i ] = softmax(x)[i] if i != t,
+    // input_gradient[ t ] = softmax(x)[t] - 1.
+    for (int i = 0; i &lt; inputs.length(); i++) {
+        int the_target = (int) round( targets(i, 0) );
+        softmax(inputs(i), input_gradients(i));
+        input_gradients(i, the_target) -= 1.;
+    }
+}
+
+void SoftmaxNLLCostModule::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                                          const TVec&lt;Mat*&gt;&amp; ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( ports_gradient.length() == nPorts() );
+
+    Mat* prediction = ports_value[0];
+    Mat* target = ports_value[1];
+#ifdef BOUNDCHECK
+    Mat* cost = ports_value[2];
+#endif
+    Mat* prediction_grad = ports_gradient[0];
+    Mat* target_grad = ports_gradient[1];
+    Mat* cost_grad = ports_gradient[2];
+
+    // If we have cost_grad and we want prediction_grad
+    if( prediction_grad &amp;&amp; prediction_grad-&gt;isEmpty()
+        &amp;&amp; cost_grad &amp;&amp; !cost_grad-&gt;isEmpty() )
+    {
+        PLASSERT( prediction );
+        PLASSERT( target );
+        PLASSERT( cost );
+        PLASSERT( !target_grad );
+
+        PLASSERT( prediction-&gt;width() == port_sizes(0,1) );
+        PLASSERT( target-&gt;width() == port_sizes(1,1) );
+        PLASSERT( cost-&gt;width() == port_sizes(2,1) );
+        PLASSERT( prediction_grad-&gt;width() == port_sizes(0,1) );
+        PLASSERT( cost_grad-&gt;width() == port_sizes(2,1) );
+        PLASSERT( cost_grad-&gt;width() == 1 );
+
+        int batch_size = prediction-&gt;length();
+        PLASSERT( target-&gt;length() == batch_size );
+        PLASSERT( cost-&gt;length() == batch_size );
+        PLASSERT( cost_grad-&gt;length() == batch_size );
+
+        prediction_grad-&gt;resize(batch_size, port_sizes(0,1));
+
+        for( int k=0; k&lt;batch_size; k++ )
+        {
+            // input_gradient[ i ] = softmax(x)[i] if i != t,
+            // input_gradient[ t ] = softmax(x)[t] - 1.
+            int target_k = (int) round((*target)(k, 0));
+            softmax((*prediction)(k), (*prediction_grad)(k));
+            (*prediction_grad)(k, target_k) -= 1.;
+        }
+    }
+    else if( !prediction_grad &amp;&amp; !target_grad &amp;&amp; !cost_grad )
+        return;
+    else if( !cost_grad &amp;&amp; prediction_grad &amp;&amp; prediction_grad-&gt;isEmpty() )
+        PLERROR(&quot;In SoftmaxNLLCostModule::bpropAccUpdate - cost gradient is NULL,\n&quot;
+                &quot;cannot compute prediction gradient. Maybe you should set\n&quot;
+                &quot;\&quot;propagate_gradient = 0\&quot; on the incoming connection.\n&quot;);
+    else
+        PLERROR(&quot;In OnlineLearningModule::bpropAccUpdate - Port configuration &quot;
+                &quot;not implemented for class '%s'&quot;, classname().c_str());
+
+    checkProp(ports_value);
+    checkProp(ports_gradient);
+}
+
+void SoftmaxNLLCostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target,
+                                 real cost,
+                                 Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
+                                 bool accumulate)
+{
+    PLCHECK(false);
+}
+
+TVec&lt;string&gt; SoftmaxNLLCostModule::costNames()
+{
+    if (name == &quot;&quot; || name == classname())
+        return TVec&lt;string&gt;(1, &quot;NLL&quot;);
+    else
+        return TVec&lt;string&gt;(1, name + &quot;.NLL&quot;);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/SoftmaxNLLCostModule.h
===================================================================
--- trunk/plearn_learners/online/SoftmaxNLLCostModule.h	2009-01-16 21:45:30 UTC (rev 9844)
+++ trunk/plearn_learners/online/SoftmaxNLLCostModule.h	2009-01-16 21:47:31 UTC (rev 9845)
@@ -0,0 +1,164 @@
+// -*- C++ -*-
+
+// SoftmaxNLLCostModule.h
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file SoftmaxNLLCostModule.h */
+
+
+#ifndef SoftmaxNLLCostModule_INC
+#define SoftmaxNLLCostModule_INC
+
+#include &lt;plearn_learners/online/CostModule.h&gt;
+
+namespace PLearn {
+
+/**
+ * Computes the NLL, given a probability vector and the true class.
+ * If input is the probability vector, and target the index of the true class,
+ * this module computes cost = -log( input[target] ), and back-propagates the
+ * gradient and diagonal of Hessian.
+ */
+class SoftmaxNLLCostModule : public CostModule
+{
+    typedef CostModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    SoftmaxNLLCostModule();
+
+    // Your other public member functions go here
+
+    //! given the input and target, compute the cost
+    virtual void fprop(const Vec&amp; input, const Vec&amp; target, Vec&amp; cost) const;
+
+    //! batch version
+    virtual void fprop(const Mat&amp; inputs, const Mat&amp; targets, Mat&amp; costs)
+        const;
+
+    //! new version
+    virtual void fprop(const TVec&lt;Mat*&gt;&amp; ports_value);
+
+    //! Adapt based on the output gradient: this method should only
+    //! be called just after a corresponding fprop.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
+                             Vec&amp; input_gradient, bool accumulate=false);
+
+    //! Overridden.
+    virtual void bpropUpdate(const Mat&amp; inputs, const Mat&amp; targets,
+            const Vec&amp; costs, Mat&amp; input_gradients, bool accumulate = false);
+
+    //! Does nothing
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost)
+    {}
+
+    //! New version of backpropagation
+    virtual void bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                                const TVec&lt;Mat*&gt;&amp; ports_gradient);
+
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this back.
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
+                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
+                              bool accumulate=false);
+
+    //! Does nothing
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost)
+    {}
+
+    //! Overridden to do nothing (in particular, no warning).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
+    //! Indicates the name of the computed costs
+    virtual TVec&lt;string&gt; costNames();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(SoftmaxNLLCostModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    mutable Vec tmp_vec;
+    mutable Mat tmp_mat;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SoftmaxNLLCostModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003284.html">[Plearn-commits] r9844 - trunk/plearn/vmat
</A></li>
	<LI>Next message: <A HREF="003286.html">[Plearn-commits] r9846 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3285">[ date ]</a>
              <a href="thread.html#3285">[ thread ]</a>
              <a href="subject.html#3285">[ subject ]</a>
              <a href="author.html#3285">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
