From plearner at mail.berlios.de  Fri Mar 12 17:47:32 2010
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 12 Mar 2010 17:47:32 +0100
Subject: [Plearn-commits] r10323 - trunk/python_modules/plearn/table
Message-ID: <201003121647.o2CGlW0A029540@sheep.berlios.de>

Author: plearner
Date: 2010-03-12 17:47:31 +0100 (Fri, 12 Mar 2010)
New Revision: 10323

Modified:
   trunk/python_modules/plearn/table/viewtable.py
Log:
added minor new functionality to pytable view

Modified: trunk/python_modules/plearn/table/viewtable.py
===================================================================
--- trunk/python_modules/plearn/table/viewtable.py	2010-02-17 16:36:23 UTC (rev 10322)
+++ trunk/python_modules/plearn/table/viewtable.py	2010-03-12 16:47:31 UTC (rev 10323)
@@ -276,6 +276,7 @@
  N            : search next
  P            : search previous
  !            : filter rows satisfying python expression (or current selected fields values)
+ @            : filter rows keeping only the first for each combination of current selected field values.
  o            : revert to the original table (all fields and rows in orignal order)
  h            : display this help screen
  *            : mark main conditioning field (Also used for subplot rows)
@@ -855,6 +856,32 @@
         self.selected_fields = [ self.table.fieldnames[j] for j in constant_cols ]
         self.redraw()
 
+    def keep_first_of_group(self):
+        selection = []
+        keys = {}
+
+        for i in xrange(self.length()):
+            if self.selected_rows is None:
+                orig_i = i
+            else:
+                orig_i = self.selected_rows[i]
+            row = self.table[orig_i]
+
+            key = tuple([ row[fname] for fname in self.selected_fields ])
+            if key not in keys:
+                # self.display_fullscreen(str(key))
+                selection.append(orig_i)
+                keys[key] = 1
+
+        self.selected_rows = selection
+        # self.selected_fields = []
+        self.i0 = 0
+        #self.j0 = 0
+        self.current_i = 0
+        #self.current_j = 0
+        self.redraw()
+
+        
     def filter(self):
         if self.selected_fields == []:
             filter_expression = self.input('Filter expression ex: float(AGE)>3 : ').strip()
@@ -1442,6 +1469,8 @@
                 self.search_next()
         elif c == ord('!'):
             self.filter()
+        elif c == ord('@'):
+            self.keep_first_of_group()
         elif c == ord('N'):
             self.search_next()
         elif c == ord('P'):



From nouiz at mail.berlios.de  Thu Mar 18 14:59:49 2010
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Mar 2010 14:59:49 +0100
Subject: [Plearn-commits] r10324 - trunk/python_modules/plearn/parallel
Message-ID: <201003181359.o2IDxnZD019932@sheep.berlios.de>

Author: nouiz
Date: 2010-03-18 14:59:49 +0100 (Thu, 18 Mar 2010)
New Revision: 10324

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
give sensible error msg when condor is not installed.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2010-03-12 16:47:31 UTC (rev 10323)
+++ trunk/python_modules/plearn/parallel/dbi.py	2010-03-18 13:59:49 UTC (rev 10324)
@@ -840,9 +840,13 @@
 
         if not self.os:
             #if their is not required os, condor launch on the same os.
-            p=Popen( "condor_config_val OpSyS", shell=True,stdout=PIPE)
+            p=Popen( "condor_config_val OpSyS", shell=True, stdout=PIPE, stderr=PIPE)
             p.wait()
-            self.os=p.stdout.readlines()[0].strip()
+            out=p.stdout.readlines()
+            err=p.stderr.readlines()
+            if len(err)!=0 or p.returncode!=0:
+                raise Exception("Can't find the os code used by condor on this computer.\n Is condor installed on this computer?\n return code=%d, \n%s"%(p.returncode,"\n".join(err)))
+            self.os=out[0].strip()
         else: self.os = self.os.upper()
         
         if not os.path.exists(self.log_dir):



From lamblin at mail.berlios.de  Tue Mar 23 00:22:19 2010
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 23 Mar 2010 00:22:19 +0100
Subject: [Plearn-commits] r10325 - trunk/scripts
Message-ID: <201003222322.o2MNMJ2M002505@sheep.berlios.de>

Author: lamblin
Date: 2010-03-23 00:22:18 +0100 (Tue, 23 Mar 2010)
New Revision: 10325

Modified:
   trunk/scripts/dbidispatch
Log:
Add (experimental) dbidispatch support for SGE (Sun's Grid Engine),
used on CLUMEQ's colosse cluster.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2010-03-18 13:59:49 UTC (rev 10324)
+++ trunk/scripts/dbidispatch	2010-03-22 23:22:18 UTC (rev 10325)
@@ -5,23 +5,36 @@
 from subprocess import Popen,PIPE
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [*--[no_]exec_in_exp_dir] [--only_n_first=N] [--repeat_jobs=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart condor_jobs_number... }
+ShortHelp='''Usage: dbidispatch <common options> <back-end parameters> {--file=FILEPATH | <command-template>|--[*no_]restart condor_jobs_number... }
 
+<common options>:
+        [--help|-h]
+        [--[*no_]dbilog]
+        [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]|--sge[=N]]
+        [--[*no_]test]
+        [--[*no_]testdbi]
+        [--[*no_]nb_proc=N]
+        [--exp_dir=dir]
+        [*--[no_]exec_in_exp_dir]
+        [--only_n_first=N]
+        [--repeat_jobs=N]
+
 <back-end parameter>:
     all                      :[--tasks_filename={compact,explicit,nb0,nb1,
                                                  sh(condor only),
                                                  clusterid(condor only),
                                                  processid(condor only))}+]
-    bqtools, cluster, condor option  : [--mem=N]
-                              [--cpu=nb_cpu_per_node]
-    bqtools, cluster option  :[--duree=X]
+    bqtools, cluster, condor, sge option:   [--cpu=nb_cpu_per_node]
+    bqtools, cluster, condor option:        [--mem=N]
+    bqtools, cluster, sge option:           [--duree=X]
     bqtools, condor options  :[--raw=STRING[\nSTRING]]
                               [*--[no_]set_special_env]
                               [--env=VAR=VALUE[;VAR2=VALUE2]]
     cluster, condor options  :[--32|--64|--3264] [--os=X]
+    bqtools, sge options:     [--queue=X] [--jobs_name=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
-                              [--queue=X] [--nano=X] [--submit_options=X]
-                              [*--[no_]clean_up] [--jobs_name=X] [*--[no_]m32G]
+                              [--nano=X] [--submit_options=X]
+                              [*--[no_]clean_up] [*--[no_]m32G]
     cluster option           :[*--[no_]cwait]  [--[*no_]force]
                               [--[*no_]interruptible]
     condor option            :[--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
@@ -37,21 +50,26 @@
                               [--max_file_size=N][--[no_]debug]
                               [--[no_]local_log_file][--next_job_start_delay=N]
                               [--fast]
+    sge options              :[--project=STRING]
+        (Note: SGE support is experimental)
 
 An * after '[', '{' or ',' signals the default value.
 An + tell that we can put one or more separeted by a comma
 '''
-LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster,
-local and ssh. If no system is selected on the command line, we try them in the
-previous order. ssh is never automaticaly selected. Currently we have a common interface for parallel jobs that run on SMP node. We
-we don't have a common interface for parallel job that run on multiple nodes.
 
+LongHelp="""Dispatches jobs with dbi.py. DBI can dispatch jobs on condor,
+bqtools, cluster, local, ssh and sge (experimental). If no system is
+selected on the command line, we try them in the previous order.
+ssh is never automaticaly selected. Currently we have a common interface for
+parallel jobs that run on SMP node. We don't have a common interface for
+parallel job that run on multiple nodes.
+
 %(ShortHelp)s
 
 common options:
   The -h, --help print the long help(this)
-  The --condor, --bqtools, --cluster, --local or --ssh option specify on which 
-    system the jobs will be sent. If not present, we will use the first 
+  The --condor, --bqtools, --cluster, --local, --ssh or --sge option specify on
+    which system the jobs will be sent. If not present, we will use the first
     available in the previously given order. ssh is never automaticaly selected.
   The '--[no_]dbilog' tells dbi to generate (or not) an additional log
   The '--[no_]test' option makes dbidispatch generate the file %(ScriptName)s,
@@ -102,15 +120,17 @@
       - none    : remove all preceding pattern
   The '*--[no_]exec_in_exp_dir' option remove the executable from the log dir.
 
-bqtools, cluster and condor option:
-  The '--mem=X' speficify the number of ram in meg the program need to execute.
+bqtools, cluster, condor and sge option:
   The '--cpu=nb_cpu_per_node' option determine the number of cpu(cores) that 
     will be reserved for each job.(default=1, -1 won't set it)
 
-bqtools and cluster option:
+bqtools, cluster and condor option:
+  The '--mem=X' speficify the number of ram in meg the program need to execute.
+
+bqtools, cluster and sge option:
   The '--duree' option specifies the maximum duration of the jobs. The syntax 
     depends on the back-end. For the cluster syntax, see 'cluster --help'. 
-    For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 
+    For bqtools and sge, the syntax is '--duree=12:13:15', giving 12 hours, 
     13 minutes and 15 seconds.
 
 bqtools and condor options:
@@ -131,6 +151,11 @@
     Condor default to the same as the submit host and --os=FC7,FC9 
     tell to use FC7 or FC9 hosts.
 
+bqtools and sge options:
+  The '--queue=X' tell on witch queue the jobs will be launched.
+  The '--jobs_name=X' option give the X as the jobs name to bqtools or sge.
+    Default, random.
+
 bqtools only options:
   If the --long option is not set, the maximum duration of each job will be 
     120 hours (5 days).
@@ -144,10 +169,8 @@
     set, and 1200:00:00 (50 days) otherwise. Since long jobs are launched on a
     different queue with few nodes, please make sure you are not using too many
     nodes at once with the --nb_proc option.
-  The '--queue=X' tell on witch queue the jobs will be launched.
   The '--nano=X' add nanoJobs=X in the submit file.
   The '--submit_options=X' X is appended to the submitOptions in the submit file.
-  The '--jobs_name=X' option give the X as the jobs name to bqtools. Default, random.
   The '--[no_]m32G' option tell to use node with 32G of RAM. Usefull on mammouth-serie2 only.
   
 cluster only options:
@@ -217,6 +240,9 @@
       add some hardcoded(maggie, brams and zappa) computers to the list 
       generated by --machine=...
 
+sge only option:
+  The '--project' specifies the project to which this  job  is  assigned.
+
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
 The arguments may contain one or many segments of the form {{a,b,c,d}},
@@ -296,7 +322,7 @@
     elif argv == "--dbilog":
         dbi_param["dolog"]=True
     elif argv.split('=')[0] in ["--bqtools","--cluster","--local","--condor",
-                                "--ssh"]:
+                                "--ssh", "--sge"]:
         launch_cmd = argv[2].upper()+argv.split('=')[0][3:]
         if len(argv.split('='))>1:
             dbi_param["nb_proc"]=argv.split('=')[1]
@@ -413,6 +439,8 @@
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
                        "nano", "queue", "raw", "submit_options", "jobs_name",
                        "set_special_env", "env" ]
+elif launch_cmd=="Sge":
+    valid_dbi_param += ["duree", "jobs_name", "queue"]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.
@@ -436,7 +464,7 @@
         if source_with_kerb:
             dbi_param['copy_local_source_file']=True
 
-    
+
 print "\n\nThe jobs will be launched on the system:", launch_cmd
 print "With options: ",dbi_param
 for i in dbi_param:
@@ -608,7 +636,7 @@
     else:
         l=MAX_FILE_NAME_SIZE-len(date_str)-1 #-1 for the '_' before date_str
         tmp=tmp[:l]
-    tmp+='_'+date_str.replace(' ','_')
+    tmp+='_'+date_str.replace(' ','_').replace(':','-')
     dbi_param["log_dir"]=os.path.join(LOGDIR,tmp)
 else:
     dbi_param["log_dir"]=os.path.join(LOGDIR,dbi_param["file"])



From lamblin at mail.berlios.de  Tue Mar 23 00:24:21 2010
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 23 Mar 2010 00:24:21 +0100
Subject: [Plearn-commits] r10326 - trunk/python_modules/plearn/parallel
Message-ID: <201003222324.o2MNOLhG002568@sheep.berlios.de>

Author: lamblin
Date: 2010-03-23 00:24:21 +0100 (Tue, 23 Mar 2010)
New Revision: 10326

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Following of previous commit


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2010-03-22 23:22:18 UTC (rev 10325)
+++ trunk/python_modules/plearn/parallel/dbi.py	2010-03-22 23:24:21 UTC (rev 10326)
@@ -342,7 +342,7 @@
             self.unique_id = get_new_sid('')#compation intense
             self.log_file = truncate( os.path.join(log_dir, self.unique_id +'_'+ formatted_command), 200) + ".log"
         else:
-            self.unique_id = formatted_command[:200]+'_'+str(datetime.datetime.now()).replace(' ','_')
+            self.unique_id = formatted_command[:200]+'_'+str(datetime.datetime.now()).replace(' ','_').replace(':','-')
             self.log_file = os.path.join(log_dir, self.unique_id) + ".log"
 
         if self.add_unique_id:
@@ -771,6 +771,171 @@
         print "[DBI] WARNING cannot wait until all jobs are done for bqtools, use bqwatch or bqstatus"
 
 
+###############################
+# Sun Grid Engine
+# (used on CLUMEQ's colosse
+###############################
+
+class DBISge(DBIBase):
+    def __init__(self, commands, **args):
+        self.jobs_name = ''
+        self.queue = ''
+        self.duree = '23:59:59'
+        self.project = 'jvb-000-aa'
+        DBIBase.__init__(self, commands, **args)
+
+        self.tmp_dir = os.path.abspath(self.tmp_dir)
+        self.log_dir = os.path.abspath(self.log_dir)
+        if not self.jobs_name:
+            #self.jobs_name = os.path.split(self.log_dir)[1]
+            self.jobs_name = 'dbi_'+self.unique_id[1:12]
+        ## No TMP_DIR needed for the moment
+        ##if not os.path.exists(self.tmp_dir):
+        ##    os.makedirs(self.tmp_dir)
+        ##print "[DBI] All SGE file will be in ", self.tmp_dir
+        ##os.chdir(self.tmp_dir)
+        self.args = args
+        self.add_commands(commands)
+
+        # Warn for not implemented features
+        if getattr(self, 'nb_proc', -1) != -1:
+            print "[DBI] WARNING: DBISge does not support nb_proc != -1", self.nb_proc
+
+    def add_commands(self,commands):
+        if not isinstance(commands, list):
+            commands=[commands]
+
+        # create the information about the tasks
+        for command in commands:
+            id=len(self.tasks)+1
+            self.tasks.append(Task(
+                command = command,
+                tmp_dir = self.tmp_dir,
+                log_dir = self.log_dir,
+                time_format = self.time_format,
+                pre_tasks = self.pre_tasks,
+                post_tasks = self.post_tasks,
+                dolog = self.dolog,
+                id = id,
+                gen_unique_id = False,
+                args = self.args))
+            id+=1
+
+    def run(self):
+        pre_batch_command = ';'.join( self.pre_batch )
+        post_batch_command = ';'.join( self.post_batch )
+
+        launcher = open(os.path.join(self.log_dir, 'launcher'), 'w')
+        launcher.write(dedent('''\
+                #!/bin/bash -l
+                # Bash is needed because we use its "array" data structure
+                # the -l flag means it will act like a login shell,
+                # and source the .profile, .bashrc, and so on
+
+                # List of all tasks to execute
+                tasks=(
+                '''))
+        for task in self.tasks:
+            launcher.write("'" + ';'.join(task.commands) + "'\n")
+        launcher.write(dedent('''\
+                )
+
+                # The index in 'tasks' array starts at 0,
+                # but SGE_TASK_ID starts at 1...
+                ID=$(($SGE_TASK_ID - 1))
+
+                # Execute the task
+                ${tasks[$ID]}
+                '''))
+
+        submit_sh_template = '''
+                #!/bin/bash
+
+                ## Reasonable default values
+                # Execute the job from the current working directory.
+                #$ -cwd
+                # Send "warning" signals to a running job prior to sending the signals themselves. 
+                #$ -notify
+
+                ## Mandatory arguments
+                #Specifies  the  project (RAPI number from CCDB) to  which this job is assigned.
+                #$ -P %(project)s
+                #All jobs must be submitted with an estimated run time
+                #$ -l h_rt=%(duree)s
+
+                ## Job name
+                #$ -N %(name)s
+
+                ## log out/err files
+                #$ -o %(log_dir)s/$JOB_NAME.$JOB_ID.$TASK_ID.log.out
+                #$ -e %(log_dir)s/$JOB_NAME.$JOB_ID.$TASK_ID.log.err
+
+                ## Execute as many jobs as needed
+                #$ -t 1-%(n_tasks)i:1
+                '''
+        if self.cpu > 0:
+            submit_sh_template += '''
+                ## Number of CPU (on the same node) per job
+                #$ -pe smp %(cpu)i
+                '''
+        if self.queue:
+            submit_sh_template += '''
+                ## Queue name
+                #$ -q %(queue)s
+                '''
+        submit_sh_template += '''
+                ## Execute the 'launcher' script in bash
+                # Bash is needed because we use its "array" data structure
+                # the -l flag means it will act like a login shell,
+                # and source the .profile, .bashrc, and so on
+                /bin/bash -l -e %(log_dir)s/launcher
+                '''
+
+        submit_sh = open(os.path.join(self.log_dir, 'submit.sh'), 'w')
+        submit_sh.write(dedent(
+            submit_sh_template % dict(
+                project = self.project,
+                duree = self.duree,
+                name = self.jobs_name,
+                log_dir = self.log_dir,
+                n_tasks = len(self.tasks),
+                cpu = self.cpu,
+                queue = self.queue,
+            )))
+
+        submit_sh.close()
+
+        # Execute pre-batch
+        self.exec_pre_batch()
+
+        print "[DBI] All logs will be in the directory: ", self.log_dir
+        print "[DBI] WARNING: the log formatting specified by --task_names will be ignored,"
+        print "     the following format will be used: $JOB_NAME.$JOB_ID.$TASK_ID.log.{err,out}"
+        # Launch qsub
+        if not self.test:
+            for t in self.tasks:
+                t.set_scheduled_time()
+
+            submit_command = 'qsub ' + os.path.join(self.log_dir, 'submit.sh')
+            self.p = Popen(submit_command, shell=True)
+            self.p.wait()
+
+            if self.p.returncode!=0:
+                raise DBIError("[DBI] ERROR: qsub returned an error code of"+str(self.p.returncode))
+        else:
+            print "[DBI] Test mode, we generated all files, but will not execute bqsubmit"
+            if self.dolog:
+                print "[DBI] The scheduling time will not be logged when you submit the generated file"
+
+        # Execute post-batchs
+        self.exec_post_batch()
+
+    def clean(self):
+        pass
+
+    def wait(self):
+        print "[DBI] WARNING cannot wait until all jobs are done for SGE, use qstat"
+
 # Transfor a string so that it is treated by Condor as a single argument
 def condor_escape_argument(argstring):
     # Double every single quote and double quote character,



From lamblin at mail.berlios.de  Tue Mar 23 21:11:31 2010
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 23 Mar 2010 21:11:31 +0100
Subject: [Plearn-commits] r10327 - trunk/plearn_learners/online
Message-ID: <201003232011.o2NKBVoE020591@sheep.berlios.de>

Author: lamblin
Date: 2010-03-23 21:11:30 +0100 (Tue, 23 Mar 2010)
New Revision: 10327

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Fix compilation of StackedAutoassociatorsNet.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2010-03-22 23:24:21 UTC (rev 10326)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2010-03-23 20:11:30 UTC (rev 10327)
@@ -1683,7 +1683,12 @@
                 {
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
                     {
-                        corrupted_input[ j ] = random_gen->bounded_sample(prob_salt_noise,pepVal,saltVal);
+                        // Sample saltVal with probability prob_salt_noise,
+                        // else pepVal
+                        corrupted_input[ j ] =
+                            random_gen->binomial_sample(prob_salt_noise) == 1 ?
+                                saltVal:
+                                pepVal;
                         reconstruction_weights[j] = corrupted_data_weight;
                     }
                     else
@@ -1692,13 +1697,13 @@
                         reconstruction_weights[j] = data_weight;
                     }
                 }
-            }       
+            }
             else if( mask_with_mean )
             {
                 for( int j=0 ; j <input.length() ; j++)
                 {
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
-                    {                    
+                    {
                         corrupted_input[ j ] = expectation_means[layer][ j ];
                         reconstruction_weights[j] = corrupted_data_weight;
                         binary_mask[ j ] = 0;
@@ -1716,7 +1721,7 @@
                 {
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
                     {
-                        corrupted_input[ j ] = 0;   
+                        corrupted_input[ j ] = 0;
                         reconstruction_weights[j] = corrupted_data_weight;
                         binary_mask[ j ] = 0;
                     }
@@ -1746,7 +1751,12 @@
                     }
                     for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
                     {
-                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] = random_gen->bounded_sample(prob_salt_noise,pepVal,saltVal);
+                        // Sample saltVal with probability prob_salt_noise,
+                        // else pepVal
+                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] =
+                            random_gen->binomial_sample(prob_salt_noise) == 1?
+                                saltVal:
+                                pepVal;
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                     }
                 }   



From lamblin at mail.berlios.de  Tue Mar 23 21:21:58 2010
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 23 Mar 2010 21:21:58 +0100
Subject: [Plearn-commits] r10328 - in trunk: plearn/base/test
	plearn/base/test/ObjectGraphIterator plearn/base/test/PP
	plearn/io/test plearn/math/test/PentadiagonalSolveInPlace
	plearn/math/test/TMat plearn/math/test/VecStatsCollector
	plearn/math/test/pl_math plearn/misc/test plearn/opt/test
	plearn/python/test plearn/var/test plearn/vmat/test
	plearn_learners/generic/test/SimpleNet
	plearn_learners/online/test/MaxSubsampling2DModule
	plearn_learners/unsupervised/test
Message-ID: <201003232021.o2NKLwBd021337@sheep.berlios.de>

Author: lamblin
Date: 2010-03-23 21:21:57 +0100 (Tue, 23 Mar 2010)
New Revision: 10328

Modified:
   trunk/plearn/base/test/
   trunk/plearn/base/test/ObjectGraphIterator/
   trunk/plearn/base/test/PP/
   trunk/plearn/io/test/
   trunk/plearn/math/test/PentadiagonalSolveInPlace/
   trunk/plearn/math/test/TMat/
   trunk/plearn/math/test/VecStatsCollector/
   trunk/plearn/math/test/pl_math/
   trunk/plearn/misc/test/
   trunk/plearn/opt/test/
   trunk/plearn/python/test/
   trunk/plearn/var/test/
   trunk/plearn/vmat/test/
   trunk/plearn_learners/generic/test/SimpleNet/
   trunk/plearn_learners/online/test/MaxSubsampling2DModule/
   trunk/plearn_learners/unsupervised/test/
Log:
Ignore more OBJS directories



Property changes on: trunk/plearn/base/test
___________________________________________________________________
Name: svn:ignore
   - assertions

   + assertions
OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn/base/test/ObjectGraphIterator
___________________________________________________________________
Name: svn:ignore
   - *.lnk
object_graph_iterator

   + *.lnk
object_graph_iterator
OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn/base/test/PP
___________________________________________________________________
Name: svn:ignore
   - *.lnk
test_PP

   + *.lnk
test_PP
OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn/io/test
___________________________________________________________________
Name: svn:ignore
   - 

   + OBJS
OBJS.NOBACKUP




Property changes on: trunk/plearn/math/test/PentadiagonalSolveInPlace
___________________________________________________________________
Name: svn:ignore
   - *.lnk
penta_test

   + *.lnk
penta_test
OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn/math/test/TMat
___________________________________________________________________
Name: svn:ignore
   - 

   + OBJS
OBJS.NOBACKUP




Property changes on: trunk/plearn/math/test/VecStatsCollector
___________________________________________________________________
Name: svn:ignore
   + OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn/math/test/pl_math
___________________________________________________________________
Name: svn:ignore
   + OBJS
OBJS.NOBACKUP




Property changes on: trunk/plearn/misc/test
___________________________________________________________________
Name: svn:ignore
   - *.lnk
test_heap

   + *.lnk
test_heap
OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn/opt/test
___________________________________________________________________
Name: svn:ignore
   + OBJS
OBJS.NOBACKUP




Property changes on: trunk/plearn/python/test
___________________________________________________________________
Name: svn:ignore
   - *.lnk
interfunction_xchg
test_injection
basic_identity_calls
test_trampoline
memory_stress_tests

   + *.lnk
interfunction_xchg
test_injection
basic_identity_calls
test_trampoline
memory_stress_tests
OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn/var/test
___________________________________________________________________
Name: svn:ignore
   + OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn/vmat/test
___________________________________________________________________
Name: svn:ignore
   + OBJS
OBJS.NOBACKUP




Property changes on: trunk/plearn_learners/generic/test/SimpleNet
___________________________________________________________________
Name: svn:ignore
   - *.lnk
simplenet

   + *.lnk
simplenet
OBJS
OBJS.NOBACKUP



Property changes on: trunk/plearn_learners/online/test/MaxSubsampling2DModule
___________________________________________________________________
Name: svn:ignore
   + OBJS
OBJS.NOBACKUP




Property changes on: trunk/plearn_learners/unsupervised/test
___________________________________________________________________
Name: svn:ignore
   - *.lnk
incremental_pca

   + *.lnk
incremental_pca
OBJS
OBJS.NOBACKUP




From lamblin at mail.berlios.de  Wed Mar 24 23:42:20 2010
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Mar 2010 23:42:20 +0100
Subject: [Plearn-commits] r10329 - trunk
Message-ID: <201003242242.o2OMgKJk002246@sheep.berlios.de>

Author: lamblin
Date: 2010-03-24 23:42:20 +0100 (Wed, 24 Mar 2010)
New Revision: 10329

Modified:
   trunk/pymake.config.model
Log:
Add colosseblas option for compilation on CLUMEQ's colosse


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2010-03-23 20:21:57 UTC (rev 10328)
+++ trunk/pymake.config.model	2010-03-24 22:42:20 UTC (rev 10329)
@@ -260,19 +260,20 @@
   [ 'g++', 'g++3', 'g++no-cygwin', 'g++-4', 'icc',
     'icc8', 'icc9', 'icc10', 'mpi',
     'purify', 'quantify', 'vc++', 'condor' ],
-  
+
   [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbg', 'optdbggprof', 'safegprof', 
     'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg', 'vecgcc' ],
-  
+
   [ 'double', 'float' ],
-  
+
   # [ 'throwerrors', 'exiterrors' ],
   [ 'autopython', 'python23', 'python24', 'python25', 'python26','nopython' ],
-  
+
   [ 'blas', 'noblas' ],
-  [ 'defblas', 'nolibblas', 'p3blas','p4blas','athlonblas','pentiumblas',
-    'mammouthblas', 'apintelblas', 'veclib', 'scs', 'goto', 'lisa' ],
-  
+  [ 'defblas', 'nolibblas', 'p3blas', 'p4blas', 'athlonblas', 'pentiumblas',
+    'mammouthblas','apintelblas', 'veclib', 'scs', 'goto', 'lisa',
+    'colosseblas' ],
+
   [ 'logging=dbg', 'logging=mand', 'logging=imp', 'logging=normal',
     'logging=extreme', 'logging=dbg-profile' ],
 
@@ -284,7 +285,7 @@
   [ '', 'cygwin-fgets-bugfix'],
   [ '', 'openmpgcc'],
   [ '', 'no-miss'],
-  
+
   [ '', 'pass']
 ]
 
@@ -896,6 +897,10 @@
               linkeroptions = '-L' + libdir +'goto -lgoto'
               )
 
+pymakeLinkOption( name = 'colosseblas',
+              description = 'linking BLAS for CLUMEQ colosse cluster',
+              linkeroptions = '-lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lmkl_lapack -openmp -lglib-2.0' )
+
 tmp = getOptions(options_choices, optionargs[:])
 if 'goto' in tmp:
     optionalLibrary( name = 'gotolapack',



