From nouiz at mail.berlios.de  Fri May  1 19:12:57 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 1 May 2009 19:12:57 +0200
Subject: [Plearn-commits] r10161 - trunk/plearn_learners/meta
Message-ID: <200905011712.n41HCvve007564@sheep.berlios.de>

Author: nouiz
Date: 2009-05-01 19:12:56 +0200 (Fri, 01 May 2009)
New Revision: 10161

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
should create a timer before we reset it.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-04-29 23:02:00 UTC (rev 10160)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-05-01 17:12:56 UTC (rev 10161)
@@ -229,6 +229,8 @@
     }
 
     timer->newTimer("MultiClassAdaBoost::test()", true);
+    timer->newTimer("MultiClassAdaBoost::test() current",true);
+
 }
 
 // ### Nothing to add here, simply calls build_



From nouiz at mail.berlios.de  Fri May  1 21:46:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 1 May 2009 21:46:39 +0200
Subject: [Plearn-commits] r10162 - trunk/plearn/vmat
Message-ID: <200905011946.n41Jkdp0006945@sheep.berlios.de>

Author: nouiz
Date: 2009-05-01 21:46:38 +0200 (Fri, 01 May 2009)
New Revision: 10162

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
don't save some learn option.


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2009-05-01 17:12:56 UTC (rev 10161)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2009-05-01 19:46:38 UTC (rev 10162)
@@ -156,7 +156,7 @@
 
     declareOption(ol, "values",
                   &GaussianizeVMatrix::values,
-                  OptionBase::learntoption,
+                  OptionBase::learntoption|OptionBase::nosave,
                   "The values used to gaussinaze.");
 
     // Now call the parent class' declareOptions



From laulysta at mail.berlios.de  Fri May  1 23:20:58 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 1 May 2009 23:20:58 +0200
Subject: [Plearn-commits] r10163 - trunk/plearn_learners_experimental
Message-ID: <200905012120.n41LKwVk019451@sheep.berlios.de>

Author: laulysta
Date: 2009-05-01 23:20:57 +0200 (Fri, 01 May 2009)
New Revision: 10163

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
generation update


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-05-01 19:46:38 UTC (rev 10162)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-05-01 21:20:57 UTC (rev 10163)
@@ -2737,8 +2737,328 @@
 
 }
 
+void DenoisingRecurrentNet::generateArtificial()
+{
+    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data = new AutoVMatrix();
+    //data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/listData/target_tm12_input_t_tm12_tp12/scoreGen_tar_tm12__in_tm12_tp12.amat";
+    //data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/create_data/scoreGenSuitePerf.amat";
+    //data->filename = "/home/stan/cvs/Gamme/expressive_data/dataGen.amat";
+    data->filename = "/home/stan/Documents/recherche_maitrise/artificialData/generate/dataGen.amat";
+    data->defineSizes(1,1,0);
+    //data->defineSizes(163,16,0);
+    //data->inputsize = 21;
+    //data->targetsize = 0;
+    //data->weightsize = 0;
+    data->build();
 
+    
+    
+   
+   
 
+    int len = data->length();
+    int tarSize = outputsize();
+    int partTarSize;
+    Vec input;
+    Vec target;
+    real weight;
+    int targsize;
+
+    Vec output(outputsize());
+    output.clear();
+//     Vec costs(nTestCosts());
+//     costs.clear();
+//     Vec n_items(nTestCosts());
+//     n_items.clear();
+
+    int r,r2;
+    use_target_layers_masks = false;
+
+    int ith_sample_in_sequence = 0;
+    int inputsize_without_masks = inputsize() 
+        - ( use_target_layers_masks ? targetsize() : 0 );
+    int sum_target_elements = 0;
+    for (int i = 0; i < len; i++)
+    {
+        data->getExample(i, input, target, weight);
+        /*if(i>n)
+        {
+            for (int k = 1; k <= t; k++)
+            {
+                if(k<=i){
+                    partTarSize = outputsize();
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                    {
+                        
+                        input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar](ith_sample_in_sequence-k);
+                        partTarSize -= target_layers[tar]->size;
+                        
+                        
+                    }
+                }
+            }       
+            }*/
+    
+
+//         for (int k = 1; k <= t; k++)
+//         {
+//             partTarSize = outputsize();
+//             for( int tar=0; tar < target_layers.length(); tar++ )
+//             {
+//                 if(i>=t){
+//                     input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar](ith_sample_in_sequence-k);
+//                     partTarSize -= target_layers[tar]->size;
+//                 }
+//             }
+//         }
+
+        if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
+        {
+//             ith_sample_in_sequence = 0;
+//             hidden_list.resize(0);
+//             hidden_act_no_bias_list.resize(0);
+//             hidden2_list.resize(0);
+//             hidden2_act_no_bias_list.resize(0);
+//             target_prediction_list.resize(0);
+//             target_prediction_act_no_bias_list.resize(0);
+//             input_list.resize(0);
+//             targets_list.resize(0);
+//             nll_list.resize(0,0);
+//             masks_list.resize(0);
+
+            
+
+            continue;
+        }
+
+        // Resize internal variables
+        hidden_list.resize(ith_sample_in_sequence+1, hidden_layer->size);
+        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1, hidden_layer->size);
+        if( hidden_layer2 )
+        {
+            hidden2_list.resize(ith_sample_in_sequence+1, hidden_layer2->size);
+            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1, hidden_layer2->size);
+        }
+                 
+        input_list.resize(ith_sample_in_sequence+1);
+        input_list[ith_sample_in_sequence].resize(input_layer->size);
+
+        targets_list.resize( target_layers.length() );
+        target_prediction_list.resize( target_layers.length() );
+        target_prediction_act_no_bias_list.resize( target_layers.length() );
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                targsize = target_layers[tar]->size;
+                targets_list[tar].resize( ith_sample_in_sequence+1, targsize);
+                //targets_list[tar][ith_sample_in_sequence].resize( target_layers[tar]->size);
+                target_prediction_list[tar].resize(
+                    ith_sample_in_sequence+1, targsize);
+                target_prediction_act_no_bias_list[tar].resize(
+                    ith_sample_in_sequence+1, targsize);
+            }
+        }
+        nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
+        if( use_target_layers_masks )
+        {
+            masks_list.resize( target_layers.length() );
+            for( int tar=0; tar < target_layers.length(); tar++ )
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    masks_list[tar].resize( ith_sample_in_sequence+1, target_layers[tar]->size );
+        }
+
+        // Forward propagation
+
+        // Fetch right representation for input
+        clamp_units(input.subVec(0,inputsize_without_masks),
+                    input_layer,
+                    input_symbol_sizes);                
+        input_list[ith_sample_in_sequence] << input_layer->expectation;
+
+        // Fetch right representation for target
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                if( use_target_layers_masks )
+                {
+                    Vec masks_list_tar_i = masks_list[tar](ith_sample_in_sequence);
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar],
+                                input.subVec(
+                                    inputsize_without_masks 
+                                    + sum_target_elements, 
+                                    target_layers_n_of_target_elements[tar]),
+                                masks_list_tar_i
+                        );
+                    
+                }
+                else
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar]);
+                }
+                targets_list[tar](ith_sample_in_sequence) << 
+                    target_layers[tar]->expectation;
+            }
+            sum_target_elements += target_layers_n_of_target_elements[tar];
+        }
+        
+        Vec hidden_act_no_bias_i = hidden_act_no_bias_list(ith_sample_in_sequence);
+        input_connections->fprop( input_list[ith_sample_in_sequence], 
+                                  hidden_act_no_bias_i);
+                
+        if( ith_sample_in_sequence > 0 && dynamic_connections )
+        {
+            dynamic_connections->fprop( 
+                hidden_list(ith_sample_in_sequence-1),
+                dynamic_act_no_bias_contribution );
+
+            hidden_act_no_bias_list(ith_sample_in_sequence) += 
+                dynamic_act_no_bias_contribution;
+        }
+        
+        Vec hidden_i = hidden_list(ith_sample_in_sequence);
+        hidden_layer->fprop( hidden_act_no_bias_i, 
+                             hidden_i );
+
+        Vec last_hidden = hidden_i;
+                 
+        if( hidden_layer2 )
+        {
+            Vec hidden2_i = hidden2_list(ith_sample_in_sequence); 
+            Vec hidden2_act_no_bias_i = hidden2_act_no_bias_list(ith_sample_in_sequence);
+
+            hidden_connections->fprop( 
+                hidden2_i,
+                hidden2_act_no_bias_i);
+
+            hidden_layer2->fprop( 
+                hidden2_act_no_bias_i,
+                hidden2_i 
+                );
+
+            last_hidden = hidden2_i; // last hidden layer vec 
+        }
+           
+       
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                Vec target_prediction_i = target_prediction_list[tar](i);
+                Vec target_prediction_act_no_bias_i = target_prediction_act_no_bias_list[tar](i);
+                target_connections[tar]->fprop(
+                    last_hidden,
+                    target_prediction_act_no_bias_i
+                    );
+                target_layers[tar]->fprop(
+                    target_prediction_act_no_bias_i,
+                    target_prediction_i );
+                if( use_target_layers_masks )
+                    target_prediction_i *= masks_list[tar](ith_sample_in_sequence);
+            }
+        }
+        
+
+        
+
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                target_layers[tar]->activation << 
+                    target_prediction_act_no_bias_list[tar](
+                        ith_sample_in_sequence);
+                target_layers[tar]->activation += target_layers[tar]->bias;
+                target_layers[tar]->setExpectation(
+                    target_prediction_list[tar](
+                        ith_sample_in_sequence));
+                nll_list(ith_sample_in_sequence,tar) = 
+                    target_layers[tar]->fpropNLL( 
+                        targets_list[tar](ith_sample_in_sequence) ); 
+//                 costs[tar] += nll_list(ith_sample_in_sequence,tar);
+                
+//                 // Normalize by the number of things to predict
+//                 if( use_target_layers_masks )
+//                 {
+//                     n_items[tar] += sum(
+//                         input.subVec( inputsize_without_masks 
+//                                       + sum_target_elements, 
+//                                       target_layers_n_of_target_elements[tar]) );
+//                 }
+//                 else
+//                 n_items[tar]++;
+            }
+            if( use_target_layers_masks )
+                sum_target_elements += 
+                    target_layers_n_of_target_elements[tar];
+        }
+        ith_sample_in_sequence++;
+
+        
+
+    }
+
+//     ith_sample_in_sequence = 0;
+//     hidden_list.resize(0);
+//     hidden_act_no_bias_list.resize(0);
+//     hidden2_list.resize(0);
+//     hidden2_act_no_bias_list.resize(0);
+//     target_prediction_list.resize(0);
+//     target_prediction_act_no_bias_list.resize(0);
+//     input_list.resize(0);
+//     targets_list.resize(0);
+//     nll_list.resize(0,0);
+//     masks_list.resize(0);   
+
+    
+    //Vec tempo;
+    //TVec<real> tempo;
+    //tempo.resize(visible_layer->size);
+    ofstream myfile;
+    myfile.open ("/home/stan/Documents/recherche_maitrise/artificialData/generate/generationResult.txt");
+    
+    for (int i = 0; i < target_prediction_list[0].length() ; i++ ){
+       
+       
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            for (int j = 0; j < target_prediction_list[tar](i).length() ; j++ ){
+                
+                //if(i>n){
+                    myfile << target_prediction_list[tar](i)[j] << " ";
+                    myfile << targets_list[tar](i)[j] << " ";
+                    // }
+                    //else{
+                    //    myfile << targets_list[tar](i)[j] << " ";
+                    // }
+                       
+           
+            }
+        }
+        myfile << "\n";
+    }
+     
+
+     myfile.close();
+
+}
+
+
+
+
+
 /*
 void DenoisingRecurrentNet::gen()
 {

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-05-01 19:46:38 UTC (rev 10162)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-05-01 21:20:57 UTC (rev 10163)
@@ -265,6 +265,9 @@
     //! Generate music in a folder
     void generate(int t, int n);
 
+    //! Generate music in a folder
+    void generateArtificial();
+
 //    //! Generate a part of the data in a folder
 //    void gen();
 



From chapados at mail.berlios.de  Fri May  1 23:30:06 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 1 May 2009 23:30:06 +0200
Subject: [Plearn-commits] r10164 - trunk/plearn_learners/regressors
Message-ID: <200905012130.n41LU6uf020641@sheep.berlios.de>

Author: chapados
Date: 2009-05-01 23:30:06 +0200 (Fri, 01 May 2009)
New Revision: 10164

Modified:
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Robustness check

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-05-01 21:20:57 UTC (rev 10163)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-05-01 21:30:06 UTC (rev 10164)
@@ -876,6 +876,8 @@
     // Obtain an estimate of the EFFECTIVE sampling noise from the
     // difference between self_cov and the diagonal of gram
     Vec sigma_sq = self_cov - diag(gram);
+    for (int i=0, n=sigma_sq.size() ; i<n ; ++i) // ensure does not get negative
+        sigma_sq[i] = max(m_weight_decay, sigma_sq[i]);
     double sigma_sq_est = mean(sigma_sq);
     // DBG_MODULE_LOG << "Sigma^2 estimate = " << sigma_sq_est << endl;
 



From nouiz at mail.berlios.de  Mon May  4 17:49:32 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 May 2009 17:49:32 +0200
Subject: [Plearn-commits] r10165 - trunk/plearn_learners/regressors
Message-ID: <200905041549.n44FnWF8004296@sheep.berlios.de>

Author: nouiz
Date: 2009-05-04 17:49:32 +0200 (Mon, 04 May 2009)
New Revision: 10165

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
small speed up.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-05-01 21:30:06 UTC (rev 10164)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-05-04 15:49:32 UTC (rev 10165)
@@ -321,7 +321,7 @@
         real val=p[row];
         PLASSERT(target_weight.size()>row && row>=0);
         PLASSERT(is_equal(p[row],tsource(col,row)));
-            
+        
         RTR_target_t target = ptw[row].first;
         RTR_weight_t weight = ptw[row].second;
         if (RTR_HAVE_MISSING && is_missing(val)){
@@ -411,7 +411,10 @@
     RTR_type* preg = reg.data();
     RTR_type* ptsorted_row = tsorted_row[col];
     RTR_type_id* pleave_register = leave_register.data();
-    if(compact_reg.size()==0){
+    if(reg.size()==length()){
+        //get the full row
+        reg<<tsorted_row(col);
+    }else if(compact_reg.size()==0){
         for(int i=0;i<length() && n> idx;i++){
             PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
             RTR_type srow = ptsorted_row[i];
@@ -544,7 +547,7 @@
         return make_tuple(best_feature_value, best_split_error, best_balance);
 
     int iter=reg.size()-right_leave->length()-1;
-    RTR_type row=reg[iter];
+    RTR_type row=preg[iter];
     real first_value=p[preg[0]];
     real next_feature=p[row];
 



From nouiz at mail.berlios.de  Mon May  4 19:22:30 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 May 2009 19:22:30 +0200
Subject: [Plearn-commits] r10166 - trunk/plearn_learners/regressors
Message-ID: <200905041722.n44HMUXI029725@sheep.berlios.de>

Author: nouiz
Date: 2009-05-04 19:22:27 +0200 (Mon, 04 May 2009)
New Revision: 10166

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
fix test.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-05-04 15:49:32 UTC (rev 10165)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-05-04 17:22:27 UTC (rev 10166)
@@ -414,6 +414,7 @@
     if(reg.size()==length()){
         //get the full row
         reg<<tsorted_row(col);
+        idx=length();
     }else if(compact_reg.size()==0){
         for(int i=0;i<length() && n> idx;i++){
             PLASSERT(ptsorted_row[i]==tsorted_row(col, i));



From nouiz at mail.berlios.de  Mon May  4 22:45:53 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 May 2009 22:45:53 +0200
Subject: [Plearn-commits] r10167 - trunk/plearn/vmat
Message-ID: <200905042045.n44KjrQf031791@sheep.berlios.de>

Author: nouiz
Date: 2009-05-04 22:45:52 +0200 (Mon, 04 May 2009)
New Revision: 10167

Modified:
   trunk/plearn/vmat/OneVsAllVMatrix.cc
   trunk/plearn/vmat/OneVsAllVMatrix.h
Log:
make it compatible with extra field.
added option inverse_target.


Modified: trunk/plearn/vmat/OneVsAllVMatrix.cc
===================================================================
--- trunk/plearn/vmat/OneVsAllVMatrix.cc	2009-05-04 17:22:27 UTC (rev 10166)
+++ trunk/plearn/vmat/OneVsAllVMatrix.cc	2009-05-04 20:45:52 UTC (rev 10167)
@@ -56,9 +56,11 @@
 
 }
 
-OneVsAllVMatrix::OneVsAllVMatrix(VMat the_source,int the_target_class)
+OneVsAllVMatrix::OneVsAllVMatrix(VMat the_source, int the_target_class,
+                                 bool inverse_target)
     : inherited(the_source),
-      target_class(the_target_class)
+      target_class(the_target_class),
+      inverse_target(inverse_target)
 {
     build();
 }
@@ -98,9 +100,10 @@
         inputsize_ = source->inputsize();
         targetsize_ = source->targetsize();
         weightsize_ = source->weightsize();
+        extrasize_ = source->extrasize();
         //fieldinfos = source->fieldinfos;
         length_ = source.length();
-        width_ = inputsize_+targetsize_+weightsize_;
+        width_ = inputsize_+targetsize_+weightsize_+extrasize_;
         sourcerow.resize(source->width());
         if(targetsize_ != 1)
             PLERROR("OneVsAllVMatrix::build_(): targetsize_ should be 1");
@@ -113,7 +116,8 @@
 void OneVsAllVMatrix::getNewRow(int i, const Vec& v) const
 {
     source->getRow(i,v);
-    v[inputsize_] = int(v[inputsize_]) == target_class;
+    bool t = int(v[inputsize_]) == target_class;
+    v[inputsize_] = inverse_target? !t : t;
 }
 
 /////////////////////////////////

Modified: trunk/plearn/vmat/OneVsAllVMatrix.h
===================================================================
--- trunk/plearn/vmat/OneVsAllVMatrix.h	2009-05-04 17:22:27 UTC (rev 10166)
+++ trunk/plearn/vmat/OneVsAllVMatrix.h	2009-05-04 20:45:52 UTC (rev 10167)
@@ -68,7 +68,7 @@
     // ************************
 
     int target_class;
-
+    bool inverse_target;
     // ****************
     // * Constructors *
     // ****************
@@ -76,7 +76,8 @@
     //! Default constructor.
     OneVsAllVMatrix();
 
-    OneVsAllVMatrix(VMat the_source,int the_target_class);
+    OneVsAllVMatrix(VMat the_source, int the_target_class,
+                    bool inverse_target=false);
 
     // ******************
     // * Object methods *



From nouiz at mail.berlios.de  Mon May  4 23:15:49 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 May 2009 23:15:49 +0200
Subject: [Plearn-commits] r10168 - trunk/plearn/vmat
Message-ID: <200905042115.n44LFnnO001897@sheep.berlios.de>

Author: nouiz
Date: 2009-05-04 23:15:49 +0200 (Mon, 04 May 2009)
New Revision: 10168

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
allow to use with Metadatadir, better errormsg.


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2009-05-04 20:45:52 UTC (rev 10167)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2009-05-04 21:15:49 UTC (rev 10168)
@@ -219,7 +219,7 @@
 
     // Obtain meta information from source.
     setMetaInfoFromSource();
-    if((hasMetaDataDir()||!stats_file_to_use.empty()) && values.size()==0)
+    if((hasMetaDataDir()||!stats_file_to_use.empty()||!save_and_reuse_stats) && values.size()==0)
         setMetaDataDir(getMetaDataDir());
 }
 
@@ -260,9 +260,9 @@
 
     VMat the_source = train_source ? train_source : source;
     
-    if(!the_source->hasMetaDataDir() && stats_file_to_use.empty() )
+    if((!the_source->hasMetaDataDir() && stats_file_to_use.empty()) && save_and_reuse_stats)
         PLERROR("In GaussianizeVMatrix::setMetaDataDir() - the "
-                " train_source, source or this VMatrix should have a metadata directory!");
+                " train_source, source or this VMatrix should have a metadata directory or save_and_reuse_stats must be false");
 
     //to save the stats their must be a metadatadir
     if(!the_source->hasMetaDataDir() && hasMetaDataDir()){
@@ -286,11 +286,15 @@
         stats = PLearn::computeStats(the_source, -1, true);
 
     if(fields_to_gaussianize.size()>0){
+        if(fields_to_gaussianize.size()>width())
+           PLERROR("In GaussianizeVMatrix::setMetaDataDir() - "
+                   "More fields in fields_to_gaussianize then the weidth()");
         for(int i=0;i<fields_to_gaussianize.size();i++){
             int field=fields_to_gaussianize[i];
             if(field>=width() || field<0)
                 PLERROR("In GaussianizeVMatrix::setMetaDataDir() - "
-                        "bad fields number to gaussianize(%d)!",field);
+                        "bad fields number (%d) in fields_to_gaussianize!",
+                        field);
         }
         features_to_gaussianize.resize(0,fields_to_gaussianize.length());
 



From nouiz at mail.berlios.de  Mon May  4 23:55:59 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 May 2009 23:55:59 +0200
Subject: [Plearn-commits] r10169 - trunk/plearn/vmat
Message-ID: <200905042155.n44LtxU9005035@sheep.berlios.de>

Author: nouiz
Date: 2009-05-04 23:55:59 +0200 (Mon, 04 May 2009)
New Revision: 10169

Modified:
   trunk/plearn/vmat/OneVsAllVMatrix.cc
Log:
get meta info with setMetaInfoFromSource to have all of them.


Modified: trunk/plearn/vmat/OneVsAllVMatrix.cc
===================================================================
--- trunk/plearn/vmat/OneVsAllVMatrix.cc	2009-05-04 21:15:49 UTC (rev 10168)
+++ trunk/plearn/vmat/OneVsAllVMatrix.cc	2009-05-04 21:55:59 UTC (rev 10169)
@@ -96,17 +96,14 @@
 {
     if(source)
     {
+        if(source->targetsize() != 1)
+            PLERROR("OneVsAllVMatrix::build_(): targetsize_ should be 1");
+
         updateMtime(source);
-        inputsize_ = source->inputsize();
-        targetsize_ = source->targetsize();
-        weightsize_ = source->weightsize();
-        extrasize_ = source->extrasize();
-        //fieldinfos = source->fieldinfos;
-        length_ = source.length();
-        width_ = inputsize_+targetsize_+weightsize_+extrasize_;
+        setMetaInfoFromSource();
+
+        PLCHECK(width_ == inputsize_+targetsize_+weightsize_+extrasize_);
         sourcerow.resize(source->width());
-        if(targetsize_ != 1)
-            PLERROR("OneVsAllVMatrix::build_(): targetsize_ should be 1");
     }
 }
 



From nouiz at mail.berlios.de  Tue May  5 15:19:53 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 May 2009 15:19:53 +0200
Subject: [Plearn-commits] r10170 - in trunk/plearn_learners/meta: .
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
Message-ID: <200905051319.n45DJr9w016862@sheep.berlios.de>

Author: nouiz
Date: 2009-05-05 15:19:52 +0200 (Tue, 05 May 2009)
New Revision: 10170

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
Log:
speed up by using a faster VMatrix for our processing.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-05-05 13:19:52 UTC (rev 10170)
@@ -38,7 +38,7 @@
 
 
 #include "MultiClassAdaBoost.h"
-#include <plearn/vmat/ProcessingVMatrix.h>
+#include <plearn/vmat/OneVsAllVMatrix.h>
 #include <plearn/vmat/SubVMatrix.h>
 #include <plearn/vmat/MemoryVMatrix.h>
 #include <plearn_learners/regressors/RegressionTreeRegisters.h>
@@ -619,28 +619,17 @@
     bool training_set_has_changed = !train_set || !(train_set->looksTheSameAs(training_set));
 
     targetname = training_set->fieldName(training_set->inputsize());
-    input_prg  = "[%0:%"+tostring(training_set->inputsize()-1)+"]";
-    target_prg1= "@"+targetname+" 1 0 ifelse :"+targetname;
-    target_prg2= "@"+targetname+" 2 - 0 1 ifelse :"+targetname;
 
-    if(training_set->weightsize()>0){
-        int index = training_set->inputsize()+training_set->targetsize();
-        weight_prg = "[%"+tostring(index)+"]";
-    }else
-        weight_prg = "1 :weights";
-    
     //We don't give it if the script give them one explicitly.
     //This can be usefull for optimization
     if(training_set_has_changed || !learner1->getTrainingSet()){
-        VMat vmat1 = new ProcessingVMatrix(training_set, input_prg,
-                                           target_prg1,  weight_prg);
+        VMat vmat1 = new OneVsAllVMatrix(training_set,0,true);
         if(training_set->hasMetaDataDir())
             vmat1->setMetaDataDir(training_set->getMetaDataDir()/"0vsOther");
         learner1->setTrainingSet(vmat1, call_forget);
     }
     if(training_set_has_changed || !learner2->getTrainingSet()){
-        VMat vmat2 = new ProcessingVMatrix(training_set, input_prg,
-                                           target_prg2,  weight_prg);
+        VMat vmat2 = new OneVsAllVMatrix(training_set,2);
         PP<RegressionTreeRegisters> t1 =
             (PP<RegressionTreeRegisters>)learner1->getTrainingSet();
         if(t1->classname()=="RegressionTreeRegisters"){
@@ -733,10 +722,9 @@
                                             learner2->nTestCosts()));
     }
     if(index<0){
-        testset1 = new ProcessingVMatrix(testset, input_prg,
-                                         target_prg1,  weight_prg);
-        testset2 = new ProcessingVMatrix(testset, input_prg,
-                                         target_prg2,  weight_prg);
+        testset1 = new OneVsAllVMatrix(testset,0,true);
+        testset2 = new OneVsAllVMatrix(testset,2);
+
         saved_testset.append(testset);
         saved_testset1.append(testset1);
         saved_testset2.append(testset2);
@@ -745,8 +733,8 @@
         //the same dataset to reuse their test results
         testset1=saved_testset1[index];
         testset2=saved_testset2[index];
-        PLCHECK(((PP<ProcessingVMatrix>)testset1)->source==testset);
-        PLCHECK(((PP<ProcessingVMatrix>)testset2)->source==testset);
+        PLCHECK(((PP<OneVsAllVMatrix>)testset1)->source==testset);
+        PLCHECK(((PP<OneVsAllVMatrix>)testset2)->source==testset);
     }
 
     //Profiler::pl_profile_end("MultiClassAdaBoost::test() part1");//cheap

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-05-05 13:19:52 UTC (rev 10170)
@@ -214,10 +214,6 @@
     mutable TVec<Vec> sub_target_tmp;
 
     string targetname;
-    string input_prg;
-    string target_prg1;
-    string target_prg2;
-    string weight_prg;
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-05-05 13:19:52 UTC (rev 10170)
@@ -250,6 +250,7 @@
 learner1 = *6 ->AdaBoost(
 weak_learners = 1 [ *7 ->RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
 compute_train_stats = 0 ;
@@ -273,11 +274,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.378311111111112819 0 0.378311111111112819 ] ;
+leave_error = 3 [ 0.378311111111107712 0 0.378311111111107712 ] ;
 split_col = 2 ;
 split_balance = 70 ;
 split_feature_value = 0.00125079586853901747 ;
-after_split_error = 0.074181818181818418 ;
+after_split_error = 0.0741818181818175992 ;
 missing_node = *0 ;
 missing_leave = *10 ->RegressionTreeLeave(
 id = 2 ;
@@ -296,11 +297,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.048000000000000001 0 0.048000000000000001 ] ;
+leave_error = 3 [ 0.04799999999999989 0 0.04799999999999989 ] ;
 split_col = 2 ;
 split_balance = 24 ;
 split_feature_value = 0.000357032461916012567 ;
-after_split_error = 0.0266666666666666684 ;
+after_split_error = 0.0266666666666666094 ;
 missing_node = *0 ;
 missing_leave = *12 ->RegressionTreeLeave(
 id = 5 ;
@@ -348,11 +349,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0266666666666666684 0 0.0266666666666666684 ] ;
+leave_error = 3 [ 0.0266666666666666094 0 0.0266666666666666094 ] ;
 split_col = 2 ;
 split_balance = 2 ;
 split_feature_value = 0.000981625552665510437 ;
-after_split_error = 0.0106666666666666646 ;
+after_split_error = 0.0106666666666666438 ;
 missing_node = *0 ;
 missing_leave = *16 ->RegressionTreeLeave(
 id = 14 ;
@@ -371,11 +372,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
+leave_error = 3 [ 0.0106666666666666438 0 0.0106666666666666438 ] ;
 split_col = 2 ;
 split_balance = 1 ;
 split_feature_value = 0.000528285193333644099 ;
-after_split_error = 0.00666666666666666449 ;
+after_split_error = 0.00666666666666665235 ;
 missing_node = *0 ;
 missing_leave = *18 ->RegressionTreeLeave(
 id = 17 ;
@@ -433,11 +434,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.026181818181818306 0 0.026181818181818306 ] ;
+leave_error = 3 [ 0.0261818181818178619 0 0.0261818181818178619 ] ;
 split_col = 4 ;
 split_balance = 88 ;
 split_feature_value = 1.54709578481515564e-13 ;
-after_split_error = 0.0218181818181818199 ;
+after_split_error = 0.0218181818181815007 ;
 missing_node = *0 ;
 missing_leave = *22 ->RegressionTreeLeave(
 id = 8 ;
@@ -482,13 +483,14 @@
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
 ] ;
-voting_weights = 1 [ 1.94591014905531323 ] ;
-sum_voting_weights = 1.94591014905531323 ;
-initial_sum_weights = 150 ;
-example_weights = 150 [ 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.0034013605442176956!
 2 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562!
  0.00340136054421769562 0.00340136054421769562 0.0034013605442!
 1769562 
0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00!
 340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 ] ;
-learners_error = 1 [ 0.0200000000000000004 ] ;
+voting_weights = 1 [ 1.94591014905531456 ] ;
+sum_voting_weights = 1.94591014905531456 ;
+initial_sum_weights = 1.00000000000000244 ;
+example_weights = 150 [ 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.0034013605442176869!
 5 0.00340136054421768695 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695!
  0.00340136054421768695 0.00340136054421768695 0.0034013605442!
 1768695 
0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00!
 340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 ] ;
+learners_error = 1 [ 0.0199999999999999518 ] ;
 weak_learner_template = *23 ->RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -553,6 +555,7 @@
 learner2 = *24 ->AdaBoost(
 weak_learners = 1 [ *25 ->RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
 compute_train_stats = 0 ;
@@ -564,11 +567,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.499911111111112305 0 0.499911111111112305 ] ;
+leave_error = 3 [ 0.499911111111108142 0 0.499911111111108142 ] ;
 split_col = 2 ;
 split_balance = 24 ;
 split_feature_value = 0.991025168386145405 ;
-after_split_error = 0.173253056011676648 ;
+after_split_error = 0.173253056011676232 ;
 missing_node = *0 ;
 missing_leave = *27 ->RegressionTreeLeave(
 id = 2 ;
@@ -587,11 +590,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.147432950191570877 0 0.147432950191570877 ] ;
+leave_error = 3 [ 0.147432950191570544 0 0.147432950191570544 ] ;
 split_col = 1 ;
 split_balance = 31 ;
 split_feature_value = 0.482293993618237549 ;
-after_split_error = 0.104535916061339731 ;
+after_split_error = 0.104535916061339579 ;
 missing_node = *0 ;
 missing_leave = *29 ->RegressionTreeLeave(
 id = 5 ;
@@ -610,11 +613,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.013107344632768362 0 0.013107344632768362 ] ;
+leave_error = 3 [ 0.0131073446327683307 0 0.0131073446327683307 ] ;
 split_col = 3 ;
 split_balance = 53 ;
 split_feature_value = 0.924226804347039965 ;
-after_split_error = 0.00888888888888889062 ;
+after_split_error = 0.00888888888888886806 ;
 missing_node = *0 ;
 missing_leave = *31 ->RegressionTreeLeave(
 id = 11 ;
@@ -639,11 +642,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0914285714285713869 0 0.0914285714285713869 ] ;
+leave_error = 3 [ 0.0914285714285712481 0 0.0914285714285712481 ] ;
 split_col = 2 ;
 split_balance = 18 ;
 split_feature_value = 0.891579732096156263 ;
-after_split_error = 0.0802318840579709924 ;
+after_split_error = 0.0802318840579708398 ;
 missing_node = *0 ;
 missing_leave = *33 ->RegressionTreeLeave(
 id = 14 ;
@@ -662,11 +665,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0695652173913043348 0 0.0695652173913043348 ] ;
+leave_error = 3 [ 0.069565217391304196 0 0.069565217391304196 ] ;
 split_col = 2 ;
 split_balance = 15 ;
 split_feature_value = 0.808283414109232878 ;
-after_split_error = 0.0617543859649122839 ;
+after_split_error = 0.0617543859649121521 ;
 missing_node = *0 ;
 missing_leave = *35 ->RegressionTreeLeave(
 id = 17 ;
@@ -691,11 +694,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
+leave_error = 3 [ 0.0106666666666666438 0 0.0106666666666666438 ] ;
 split_col = 2 ;
 split_balance = 1 ;
 split_feature_value = 0.982696507149771858 ;
-after_split_error = 0.00666666666666666709 ;
+after_split_error = 0.00666666666666665061 ;
 missing_node = *0 ;
 missing_leave = *37 ->RegressionTreeLeave(
 id = 20 ;
@@ -728,7 +731,7 @@
 split_col = 2 ;
 split_balance = 47 ;
 split_feature_value = 0.997650553369808346 ;
-after_split_error = 0.0200000000000000039 ;
+after_split_error = 0.0199999999999999553 ;
 missing_node = *0 ;
 missing_leave = *39 ->RegressionTreeLeave(
 id = 8 ;
@@ -773,13 +776,14 @@
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
 ] ;
-voting_weights = 1 [ 1.22117351768460214 ] ;
-sum_voting_weights = 1.22117351768460214 ;
-initial_sum_weights = 150 ;
-example_weights = 150 [ 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971!
 132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579!
 71132 0.0036231884057971132 0.0036231884057971132 0.0036231884!
 05797113
2 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579711!
 32 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 ] ;
-learners_error = 1 [ 0.0800000000000000017 ] ;
+voting_weights = 1 [ 1.22117351768460325 ] ;
+sum_voting_weights = 1.22117351768460325 ;
+initial_sum_weights = 1.00000000000000244 ;
+example_weights = 150 [ 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.!
 00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.0!
 0362318840579710496 0.00362318840579710496 0.00362318840579710!
 496 0.00
362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.003623!
 18840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 ] ;
+learners_error = 1 [ 0.0799999999999998351 ] ;
 weak_learner_template = *40 ->RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -855,6 +859,7 @@
 ;
 weak_learner_template = *42 ->RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -926,6 +931,7 @@
 save_stat_collectors = 1 ;
 save_split_stats = 1 ;
 save_learners = 0 ;
+save_learners_cond = "" ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
 save_test_outputs = 0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-05-05 13:19:52 UTC (rev 10170)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL10066"
+__REVISION__ = "PL10143"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-05-05 13:19:52 UTC (rev 10170)
@@ -69,6 +69,7 @@
 ;
 weak_learner_template = *8 ->RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -155,6 +156,7 @@
 ;
 weak_learner_template = *11 ->RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -230,6 +232,7 @@
 ;
 weak_learner_template = *13 ->RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;



From nouiz at mail.berlios.de  Tue May  5 15:22:06 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 May 2009 15:22:06 +0200
Subject: [Plearn-commits] r10171 - trunk/plearn_learners/hyper
Message-ID: <200905051322.n45DM6bn017155@sheep.berlios.de>

Author: nouiz
Date: 2009-05-05 15:22:05 +0200 (Tue, 05 May 2009)
New Revision: 10171

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
Log:
remove more field when we finalise.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-05 13:19:52 UTC (rev 10170)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-05 13:22:05 UTC (rev 10171)
@@ -279,6 +279,7 @@
 {
     inherited::finalize();
     learner_->finalize();
+    if(tester)tester->dataset=NULL;
 }
 
 ////////////



From nouiz at mail.berlios.de  Tue May  5 15:38:50 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 May 2009 15:38:50 +0200
Subject: [Plearn-commits] r10172 - in
	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir:
	. Split0
Message-ID: <200905051338.n45DcoCc019779@sheep.berlios.de>

Author: nouiz
Date: 2009-05-05 15:38:49 +0200 (Tue, 05 May 2009)
New Revision: 10172

Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
Log:
remove uncommited option.


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-05-05 13:22:05 UTC (rev 10171)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-05-05 13:38:49 UTC (rev 10172)
@@ -250,7 +250,6 @@
 learner1 = *6 ->AdaBoost(
 weak_learners = 1 [ *7 ->RegressionTree(
 missing_is_valid = 0 ;
-will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
 compute_train_stats = 0 ;
@@ -490,7 +489,6 @@
 learners_error = 1 [ 0.0199999999999999518 ] ;
 weak_learner_template = *23 ->RegressionTree(
 missing_is_valid = 0 ;
-will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -555,7 +553,6 @@
 learner2 = *24 ->AdaBoost(
 weak_learners = 1 [ *25 ->RegressionTree(
 missing_is_valid = 0 ;
-will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
 compute_train_stats = 0 ;
@@ -783,7 +780,6 @@
 learners_error = 1 [ 0.0799999999999998351 ] ;
 weak_learner_template = *40 ->RegressionTree(
 missing_is_valid = 0 ;
-will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -859,7 +855,6 @@
 ;
 weak_learner_template = *42 ->RegressionTree(
 missing_is_valid = 0 ;
-will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-05-05 13:22:05 UTC (rev 10171)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-05-05 13:38:49 UTC (rev 10172)
@@ -69,7 +69,6 @@
 ;
 weak_learner_template = *8 ->RegressionTree(
 missing_is_valid = 0 ;
-will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -156,7 +155,6 @@
 ;
 weak_learner_template = *11 ->RegressionTree(
 missing_is_valid = 0 ;
-will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -232,7 +230,6 @@
 ;
 weak_learner_template = *13 ->RegressionTree(
 missing_is_valid = 0 ;
-will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;



From nouiz at mail.berlios.de  Tue May  5 15:49:46 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 May 2009 15:49:46 +0200
Subject: [Plearn-commits] r10173 -
	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
Message-ID: <200905051349.n45DnkMA023255@sheep.berlios.de>

Author: nouiz
Date: 2009-05-05 15:49:46 +0200 (Tue, 05 May 2009)
New Revision: 10173

Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
Log:
remove uncommited new options


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-05-05 13:38:49 UTC (rev 10172)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-05-05 13:49:46 UTC (rev 10173)
@@ -926,7 +926,6 @@
 save_stat_collectors = 1 ;
 save_split_stats = 1 ;
 save_learners = 0 ;
-save_learners_cond = "" ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
 save_test_outputs = 0 ;



From nouiz at mail.berlios.de  Tue May  5 17:23:38 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 May 2009 17:23:38 +0200
Subject: [Plearn-commits] r10174 - trunk/plearn/vmat
Message-ID: <200905051523.n45FNcYg010222@sheep.berlios.de>

Author: nouiz
Date: 2009-05-05 17:23:37 +0200 (Tue, 05 May 2009)
New Revision: 10174

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
warn if not completly build. Build completly when it don't have a metadatadir but the source have one.


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2009-05-05 13:49:46 UTC (rev 10173)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2009-05-05 15:23:37 UTC (rev 10174)
@@ -219,7 +219,8 @@
 
     // Obtain meta information from source.
     setMetaInfoFromSource();
-    if((hasMetaDataDir()||!stats_file_to_use.empty()||!save_and_reuse_stats) && values.size()==0)
+
+    if((the_source->hasMetaDataDir()||hasMetaDataDir()||!stats_file_to_use.empty()||!save_and_reuse_stats) && values.size()==0)
         setMetaDataDir(getMetaDataDir());
 }
 
@@ -349,6 +350,8 @@
 ///////////////
 void GaussianizeVMatrix::getNewRow(int i, const Vec& v) const
 {
+    if(values.size()==0 && features_to_gaussianize.size()>0)
+        PLERROR("In GaussianizeVMatrix::getNewRow() - We don't have been build correctly. Try to set a metadatadir or set save_and_reuse_stats=0.");
     PLASSERT( source );
     source->getRow(i, v);
     for (int k = 0; k < features_to_gaussianize.length(); k++) {



From nouiz at mail.berlios.de  Tue May  5 17:25:16 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 May 2009 17:25:16 +0200
Subject: [Plearn-commits] r10175 - trunk/plearn/vmat
Message-ID: <200905051525.n45FPGw6010441@sheep.berlios.de>

Author: nouiz
Date: 2009-05-05 17:25:15 +0200 (Tue, 05 May 2009)
New Revision: 10175

Modified:
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.h
Log:
added the option ShiftAndRescaleVMatrix::fields that force restric the fields to be modified.


Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2009-05-05 15:23:37 UTC (rev 10174)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2009-05-05 15:25:15 UTC (rev 10175)
@@ -213,6 +213,11 @@
                   "If set to 1, no scaling will be performed (only a shift"
                   " will be applied).");
 
+    declareOption(ol, "fields", &ShiftAndRescaleVMatrix::fields,
+                  OptionBase::buildoption,
+                  "The fields to shift and rescale. Automatic must be true and"
+                  " if empty we use instead n_inputs.");
+
     declareOption(ol, "verbosity", &ShiftAndRescaleVMatrix::verbosity,
                   OptionBase::buildoption,
                   "Controls the amount of output.");
@@ -234,16 +239,18 @@
 void ShiftAndRescaleVMatrix::build_()
 {
     PLASSERT( n_inputs >= 0 || n_inputs == -1 || n_inputs == -2 );
+    PLCHECK(fields.size()<=0||automatic);
     if( source )
     {
         if (automatic && min_max.isEmpty())
         {
             if (n_inputs<0)
             {
+                PLCHECK( source->inputsize() >= 0 );
                 if (n_inputs == -1)
                     n_inputs = source->inputsize();
                 else if (n_inputs == -2) {
-                    PLASSERT( source->targetsize() >= 0 );
+                    PLCHECK( source->targetsize() >= 0 );
                     n_inputs = source->inputsize() + source->targetsize();
                 } else
                     PLERROR("In ShiftAndRescaleVMatrix::build_ - Wrong value "
@@ -274,6 +281,28 @@
                 scale.resize(source->width());
                 scale.subVec(n_inputs, scale.length()-n_inputs).fill(1);
             }
+            
+            if(fields.size()>0){
+                //we shift and scale only the fields
+                TVec<int> idx(fields.size());
+                for(int i=0;i<fields.size();i++){
+                    idx[i]=source->getFieldIndex(fields[i]);
+                    if(idx[i]>n_inputs)
+                        PLERROR("In ShiftAndRescaleVMatrix::build_() - The "
+                                "fields index must be lower or equal to n_inputs");
+                }
+                for(int i=0;i<n_inputs;i++){
+                    bool found=false;
+                    for(int j=0;j<idx.size();j++){
+                        if(i==idx[j])
+                            found=true;
+                    }
+                    if(!found){
+                        shift[i]=0;
+                        scale[i]=1;
+                    }
+                }
+            }
             shift.resize(source->width());
             shift.subVec(n_inputs, shift.length()-n_inputs).fill(0);
         }
@@ -289,12 +318,15 @@
                             "min_max[1]") ; 
             if (n_inputs<0)
             {
-                n_inputs = source->inputsize();
-                if (n_inputs<0)
-                    PLERROR("ShiftAndRescaleVMatrix: either n_inputs should be"
-                            " provided explicitly\n"
-                            "or the source VMatrix should have a set value of"
-                            " inputsize.\n");
+                PLCHECK(source->inputsize()>=0);
+                if(n_inputs==-1)
+                    n_inputs = source->inputsize();
+                else if (n_inputs == -2) {
+                    PLCHECK( source->targetsize() >= 0 );
+                    n_inputs = source->inputsize() + source->targetsize();
+                } else
+                    PLERROR("In ShiftAndRescaleVMatrix::build_ - Wrong value "
+                            "for 'n_inputs'");
             }
                 
                 Vec min_col(n_inputs) , max_col(n_inputs) ; 

Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.h
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.h	2009-05-05 15:23:37 UTC (rev 10174)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.h	2009-05-05 15:25:15 UTC (rev 10175)
@@ -80,6 +80,7 @@
     int n_inputs; // when automatic,
     bool negate_shift;
     bool no_scale;
+    TVec<string> fields;
     int verbosity;
 
     //! For all constructors, the original VMFields are copied upon construction



From nouiz at mail.berlios.de  Tue May  5 22:20:44 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 May 2009 22:20:44 +0200
Subject: [Plearn-commits] r10176 - trunk/plearn/vmat
Message-ID: <200905052020.n45KKi8v029895@sheep.berlios.de>

Author: nouiz
Date: 2009-05-05 22:20:44 +0200 (Tue, 05 May 2009)
New Revision: 10176

Modified:
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.h
Log:
moved some computation from build_ to setMetaDataDir so we don't force to expliciltly give the metadatadir.


Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2009-05-05 15:25:15 UTC (rev 10175)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2009-05-05 20:20:44 UTC (rev 10176)
@@ -261,50 +261,6 @@
                             "or the source VMatrix should have a set value of"
                             " inputsize.\n");
             }
-            if (n_train>0)
-                computeMeanAndStddev(source.subMatRows(0,n_train),
-                        shift, scale);
-            else
-                computeMeanAndStddev(source, shift, scale);
-            if (!negate_shift)
-                negateElements(shift);
-            if (!no_scale) {
-                for (int i=0;i<scale.length();i++)
-                    if (fast_exact_is_equal(scale[i], 0))
-                    {
-                        if (verbosity >= 1)
-                            PLWARNING("ShiftAndRescale: data column number %d"
-                                      " is constant",i);
-                        scale[i]=1;
-                    }
-                invertElements(scale);
-                scale.resize(source->width());
-                scale.subVec(n_inputs, scale.length()-n_inputs).fill(1);
-            }
-            
-            if(fields.size()>0){
-                //we shift and scale only the fields
-                TVec<int> idx(fields.size());
-                for(int i=0;i<fields.size();i++){
-                    idx[i]=source->getFieldIndex(fields[i]);
-                    if(idx[i]>n_inputs)
-                        PLERROR("In ShiftAndRescaleVMatrix::build_() - The "
-                                "fields index must be lower or equal to n_inputs");
-                }
-                for(int i=0;i<n_inputs;i++){
-                    bool found=false;
-                    for(int j=0;j<idx.size();j++){
-                        if(i==idx[j])
-                            found=true;
-                    }
-                    if(!found){
-                        shift[i]=0;
-                        scale[i]=1;
-                    }
-                }
-            }
-            shift.resize(source->width());
-            shift.subVec(n_inputs, shift.length()-n_inputs).fill(0);
         }
         else {
             if (!min_max.isEmpty()) {
@@ -389,11 +345,76 @@
     }
 }
 
+////////////////////
+// setMetaDataDir //
+////////////////////
+void ShiftAndRescaleVMatrix::setMetaDataDir(const PPath& the_metadatadir)
+{
+    if(!source->hasMetaDataDir())
+        source->setMetaDataDir(the_metadatadir/"Source");
+
+    inherited::setMetaDataDir(the_metadatadir);
+
+    if( source )
+    {
+        if (automatic && min_max.isEmpty())
+        {
+            if (n_train>0)
+                computeMeanAndStddev(source.subMatRows(0,n_train),
+                        shift, scale);
+            else
+                computeMeanAndStddev(source, shift, scale);
+            if (!negate_shift)
+                negateElements(shift);
+            if (!no_scale) {
+                for (int i=0;i<scale.length();i++)
+                    if (fast_exact_is_equal(scale[i], 0))
+                    {
+                        if (verbosity >= 1)
+                            PLWARNING("ShiftAndRescale: data column number %d"
+                                      " is constant",i);
+                        scale[i]=1;
+                    }
+                invertElements(scale);
+                scale.resize(source->width());
+                scale.subVec(n_inputs, scale.length()-n_inputs).fill(1);
+            }
+            
+            if(fields.size()>0){
+                //we shift and scale only the fields
+                TVec<int> idx(fields.size());
+                for(int i=0;i<fields.size();i++){
+                    idx[i]=source->getFieldIndex(fields[i]);
+                    if(idx[i]>n_inputs)
+                        PLERROR("In ShiftAndRescaleVMatrix::build_() - The "
+                                "fields index must be lower or equal to n_inputs");
+                }
+                for(int i=0;i<n_inputs;i++){
+                    bool found=false;
+                    for(int j=0;j<idx.size();j++){
+                        if(i==idx[j])
+                            found=true;
+                    }
+                    if(!found){
+                        shift[i]=0;
+                        scale[i]=1;
+                    }
+                }
+            }
+            shift.resize(source->width());
+            shift.subVec(n_inputs, shift.length()-n_inputs).fill(0);
+        }
+    }
+}
+
 ///////////////
 // getNewRow //
 ///////////////
 void ShiftAndRescaleVMatrix::getNewRow(int i, const Vec& v) const
 {
+    if(source && automatic && min_max.isEmpty())
+        PLCHECK(hasMetaDataDir());
+
     source->getRow(i, v);
 
     if( negate_shift )

Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.h
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.h	2009-05-05 15:25:15 UTC (rev 10175)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.h	2009-05-05 20:20:44 UTC (rev 10176)
@@ -126,6 +126,7 @@
 public:
     // simply calls inherited::build() then build_()
     virtual void build();
+    virtual void setMetaDataDir(const PPath& the_metadatadir);
 
 };
 



From nouiz at mail.berlios.de  Tue May  5 22:25:48 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 May 2009 22:25:48 +0200
Subject: [Plearn-commits] r10177 - trunk/plearn_learners/generic
Message-ID: <200905052025.n45KPmug030324@sheep.berlios.de>

Author: nouiz
Date: 2009-05-05 22:25:48 +0200 (Tue, 05 May 2009)
New Revision: 10177

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
use test_minibatch_size in PLearner::use to use the optimized computeOutputs()


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2009-05-05 20:20:44 UTC (rev 10176)
+++ trunk/plearn_learners/generic/PLearner.cc	2009-05-05 20:25:48 UTC (rev 10177)
@@ -841,14 +841,53 @@
         if(report_progress)
             pb = new ProgressBar("Using learner",l);
 
-        for(int i=0; i<l; i++)
+        if (test_minibatch_size==1)
         {
-            testset.getExample(i, input, target, weight);
-            computeOutput(input, output);
-            outputs->putOrAppendRow(i,output);
-            if(pb)
-                pb->update(i);
+            for(int i=0; i<l; i++)
+            {
+                testset.getExample(i, input, target, weight);
+                computeOutput(input, output);
+                outputs->putOrAppendRow(i,output);
+                if(pb)
+                    pb->update(i);
+            }
+        } else
+        {
+            int out_size = outputsize() >= 0 ? outputsize() : 0;
+            int n_batches = l/test_minibatch_size, i=0;
+            b_inputs.resize(test_minibatch_size,inputsize());
+            b_outputs.resize(test_minibatch_size, out_size);
+            b_costs.resize(test_minibatch_size,nTestCosts());
+            b_targets.resize(test_minibatch_size,targetsize());
+            b_weights.resize(test_minibatch_size);
+            for (int b=0;b<n_batches;b++,i+=test_minibatch_size)
+            {
+                testset->getExamples(i,test_minibatch_size,b_inputs,b_targets,b_weights);
+                computeOutputs(b_inputs,b_outputs);
+                for (int j=0;j<test_minibatch_size;j++)
+                {
+                    outputs->putOrAppendRow(i+j, b_outputs(j));
+                }
+                if (pb) pb->update(i+test_minibatch_size);
+            }
+            if (i<l)
+            {
+                b_inputs.resize(l-i,inputsize());
+                b_outputs.resize(l-i, out_size);
+                b_costs.resize(l-i,nTestCosts());
+                b_targets.resize(l-i,targetsize());
+                b_weights.resize(l-i);
+                testset->getExamples(i,l-i,b_inputs,b_targets,b_weights);
+                computeOutputs(b_inputs,b_outputs);
+                for (int j=0;j<l-i;j++)
+                {
+                    outputs->putOrAppendRow(i+j, b_outputs(j));
+                }
+                if (pb) pb->update(l);
+            }
         }
+
+
     }
     else // parallel code
     {



From nouiz at mail.berlios.de  Wed May  6 17:21:17 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 May 2009 17:21:17 +0200
Subject: [Plearn-commits] r10178 - trunk/plearn/vmat
Message-ID: <200905061521.n46FLH2p011701@sheep.berlios.de>

Author: nouiz
Date: 2009-05-06 17:21:16 +0200 (Wed, 06 May 2009)
New Revision: 10178

Modified:
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.h
Log:
revert the split of the build with setMetaDataDir as this cause test to fail.


Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2009-05-05 20:25:48 UTC (rev 10177)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2009-05-06 15:21:16 UTC (rev 10178)
@@ -261,6 +261,50 @@
                             "or the source VMatrix should have a set value of"
                             " inputsize.\n");
             }
+            if (n_train>0)
+                computeMeanAndStddev(source.subMatRows(0,n_train),
+                        shift, scale);
+            else
+                computeMeanAndStddev(source, shift, scale);
+            if (!negate_shift)
+                negateElements(shift);
+            if (!no_scale) {
+                for (int i=0;i<scale.length();i++)
+                    if (fast_exact_is_equal(scale[i], 0))
+                    {
+                        if (verbosity >= 1)
+                            PLWARNING("ShiftAndRescale: data column number %d"
+                                      " is constant",i);
+                        scale[i]=1;
+                    }
+                invertElements(scale);
+                scale.resize(source->width());
+                scale.subVec(n_inputs, scale.length()-n_inputs).fill(1);
+            }
+            
+            if(fields.size()>0){
+                //we shift and scale only the fields
+                TVec<int> idx(fields.size());
+                for(int i=0;i<fields.size();i++){
+                    idx[i]=source->getFieldIndex(fields[i]);
+                    if(idx[i]>n_inputs)
+                        PLERROR("In ShiftAndRescaleVMatrix::build_() - The "
+                                "fields index must be lower or equal to n_inputs");
+                }
+                for(int i=0;i<n_inputs;i++){
+                    bool found=false;
+                    for(int j=0;j<idx.size();j++){
+                        if(i==idx[j])
+                            found=true;
+                    }
+                    if(!found){
+                        shift[i]=0;
+                        scale[i]=1;
+                    }
+                }
+            }
+            shift.resize(source->width());
+            shift.subVec(n_inputs, shift.length()-n_inputs).fill(0);
         }
         else {
             if (!min_max.isEmpty()) {
@@ -345,76 +389,11 @@
     }
 }
 
-////////////////////
-// setMetaDataDir //
-////////////////////
-void ShiftAndRescaleVMatrix::setMetaDataDir(const PPath& the_metadatadir)
-{
-    if(!source->hasMetaDataDir())
-        source->setMetaDataDir(the_metadatadir/"Source");
-
-    inherited::setMetaDataDir(the_metadatadir);
-
-    if( source )
-    {
-        if (automatic && min_max.isEmpty())
-        {
-            if (n_train>0)
-                computeMeanAndStddev(source.subMatRows(0,n_train),
-                        shift, scale);
-            else
-                computeMeanAndStddev(source, shift, scale);
-            if (!negate_shift)
-                negateElements(shift);
-            if (!no_scale) {
-                for (int i=0;i<scale.length();i++)
-                    if (fast_exact_is_equal(scale[i], 0))
-                    {
-                        if (verbosity >= 1)
-                            PLWARNING("ShiftAndRescale: data column number %d"
-                                      " is constant",i);
-                        scale[i]=1;
-                    }
-                invertElements(scale);
-                scale.resize(source->width());
-                scale.subVec(n_inputs, scale.length()-n_inputs).fill(1);
-            }
-            
-            if(fields.size()>0){
-                //we shift and scale only the fields
-                TVec<int> idx(fields.size());
-                for(int i=0;i<fields.size();i++){
-                    idx[i]=source->getFieldIndex(fields[i]);
-                    if(idx[i]>n_inputs)
-                        PLERROR("In ShiftAndRescaleVMatrix::build_() - The "
-                                "fields index must be lower or equal to n_inputs");
-                }
-                for(int i=0;i<n_inputs;i++){
-                    bool found=false;
-                    for(int j=0;j<idx.size();j++){
-                        if(i==idx[j])
-                            found=true;
-                    }
-                    if(!found){
-                        shift[i]=0;
-                        scale[i]=1;
-                    }
-                }
-            }
-            shift.resize(source->width());
-            shift.subVec(n_inputs, shift.length()-n_inputs).fill(0);
-        }
-    }
-}
-
 ///////////////
 // getNewRow //
 ///////////////
 void ShiftAndRescaleVMatrix::getNewRow(int i, const Vec& v) const
 {
-    if(source && automatic && min_max.isEmpty())
-        PLCHECK(hasMetaDataDir());
-
     source->getRow(i, v);
 
     if( negate_shift )

Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.h
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.h	2009-05-05 20:25:48 UTC (rev 10177)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.h	2009-05-06 15:21:16 UTC (rev 10178)
@@ -126,7 +126,6 @@
 public:
     // simply calls inherited::build() then build_()
     virtual void build();
-    virtual void setMetaDataDir(const PPath& the_metadatadir);
 
 };
 



From nouiz at mail.berlios.de  Wed May  6 17:22:08 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 May 2009 17:22:08 +0200
Subject: [Plearn-commits] r10179 - trunk/plearn/vmat
Message-ID: <200905061522.n46FM8YZ011764@sheep.berlios.de>

Author: nouiz
Date: 2009-05-06 17:22:07 +0200 (Wed, 06 May 2009)
New Revision: 10179

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
call the parent build before we build our self to build correctly the metadatadir of the source.


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2009-05-06 15:21:16 UTC (rev 10178)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2009-05-06 15:22:07 UTC (rev 10179)
@@ -214,7 +214,12 @@
 ///////////
 void VariableDeletionVMatrix::build()
 {
-//    inherited::build();
+    //must be done even if we will call it later to have the
+    //source metadatadir set correctly.
+    bool saved_warn_non_selected_field=warn_non_selected_field;
+    warn_non_selected_field=false;
+    inherited::build();
+    warn_non_selected_field=saved_warn_non_selected_field;
     build_();
 }
 



From nouiz at mail.berlios.de  Wed May  6 17:27:15 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 May 2009 17:27:15 +0200
Subject: [Plearn-commits] r10180 - trunk/plearn_learners/meta
Message-ID: <200905061527.n46FRFKX012788@sheep.berlios.de>

Author: nouiz
Date: 2009-05-06 17:27:14 +0200 (Wed, 06 May 2009)
New Revision: 10180

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
don't parallelize the test of MultiClassAdaBoost as currently this is not thread safe.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-05-06 15:22:07 UTC (rev 10179)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-05-06 15:27:14 UTC (rev 10180)
@@ -740,7 +740,7 @@
     //Profiler::pl_profile_end("MultiClassAdaBoost::test() part1");//cheap
     Profiler::pl_profile_start("MultiClassAdaBoost::test() subtest");
 #ifdef _OPENMP
-#pragma omp parallel sections
+#pragma omp parallel sections if(false)//false as this is not thread safe right now.
 {
 #pragma omp section 
     learner1->test(testset1,test_stats1,testoutputs1,testcosts1);



From nouiz at mail.berlios.de  Wed May  6 20:30:14 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 May 2009 20:30:14 +0200
Subject: [Plearn-commits] r10181 - trunk/plearn_learners/hyper
Message-ID: <200905061830.n46IUEdV011112@sheep.berlios.de>

Author: nouiz
Date: 2009-05-06 20:30:14 +0200 (Wed, 06 May 2009)
New Revision: 10181

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperLearner.h
Log:
added option HyperLearner::save_strategy_learner that allow to save the learner after each strategy.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-06 15:27:14 UTC (rev 10180)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-06 18:30:14 UTC (rev 10181)
@@ -77,6 +77,7 @@
 HyperLearner::HyperLearner()
     : provide_strategy_expdir(true),
       save_final_learner(true),
+      save_strategy_learner(false),
       reloaded(false),
       finalize_learner(false)
 {
@@ -130,6 +131,9 @@
     declareOption(ol, "save_final_learner", &HyperLearner::save_final_learner, OptionBase::buildoption,
                   "should final learner be saved in expdir/final_learner.psave");
 
+    declareOption(ol, "save_strategy_learner", &HyperLearner::save_strategy_learner, OptionBase::buildoption,
+                  "should final learner be saved in expdir/Strat#/final_learner.psave");
+
     declareOption(
         ol, "finalize_learner", &HyperLearner::finalize_learner,
         OptionBase::buildoption,
@@ -239,6 +243,16 @@
                 perr<<"HyperLearner: starting the optimization"<<endl;
 
             results = strategy[commandnum]->optimize();
+
+            if(save_strategy_learner)
+            {
+                PPath strat_expdir=strategy[commandnum]->getExperimentDirectory();
+                if(strat_expdir.isEmpty())
+                    PLERROR("Cannot save the strategy model: no experiment directory has been set");
+                if( getLearner().isNull() )
+                    PLERROR("Cannot save final model: no final learner available");
+                PLearn::save(strat_expdir+"final_learner.psave",*getLearner());
+            }
         }
 
         train_stats->update(results);

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2009-05-06 15:27:14 UTC (rev 10180)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2009-05-06 18:30:14 UTC (rev 10181)
@@ -75,6 +75,7 @@
 
     bool provide_strategy_expdir; //!< should each strategy step be provided a directory expdir/Step#
     bool save_final_learner; //!< should final learner be saved in expdir/final_learner.psave
+    bool save_strategy_learner; //!< should final learner be saved in expdir/Strat#/final_learner.psave
     bool reloaded; //!< needed for a warning
     // HyperLearner methods
 



From nouiz at mail.berlios.de  Wed May  6 22:55:35 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 May 2009 22:55:35 +0200
Subject: [Plearn-commits] r10182 - trunk/plearn/vmat
Message-ID: <200905062055.n46KtZum028676@sheep.berlios.de>

Author: nouiz
Date: 2009-05-06 22:55:34 +0200 (Wed, 06 May 2009)
New Revision: 10182

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
use more generic fct.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2009-05-06 18:30:14 UTC (rev 10181)
+++ trunk/plearn/vmat/VMatrix.cc	2009-05-06 20:55:34 UTC (rev 10182)
@@ -1632,7 +1632,7 @@
     PPath metadatadir = getMetaDataDir();
     PPath statsfile;
     bool uptodate = false;
-    if (!metadatadir.isEmpty()) {
+    if (hasMetaDataDir()) {
         lockMetaDataDir();
         statsfile =  metadatadir / filename;
         uptodate = isUpToDate(statsfile);
@@ -1651,7 +1651,7 @@
         if(!uptodate){
             VMat vm = const_cast<VMatrix*>(this);
             stats = PLearn::computeStats(vm, maxnvalues, progress_bar);
-            if(!metadatadir.isEmpty())
+            if(hasMetaDataDir())
                 PLearn::save(statsfile, stats);
         }
     }catch(const PLearnError& e){



From nouiz at mail.berlios.de  Thu May  7 16:29:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 16:29:36 +0200
Subject: [Plearn-commits] r10183 - trunk/plearn/io
Message-ID: <200905071429.n47ETaX4005476@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 16:29:36 +0200 (Thu, 07 May 2009)
New Revision: 10183

Modified:
   trunk/plearn/io/PyPLearnScript.cc
   trunk/plearn/io/PyPLearnScript.h
Log:
moved an include from the header to the code file.


Modified: trunk/plearn/io/PyPLearnScript.cc
===================================================================
--- trunk/plearn/io/PyPLearnScript.cc	2009-05-06 20:55:34 UTC (rev 10182)
+++ trunk/plearn/io/PyPLearnScript.cc	2009-05-07 14:29:36 UTC (rev 10183)
@@ -49,6 +49,7 @@
 #include <plearn/base/stringutils.h>
 #include <plearn/base/pl_repository_revision.h>
 #include <plearn/vmat/VMatrix.h>
+#include <plearn/io/pl_log.h>
 
 
 namespace PLearn {

Modified: trunk/plearn/io/PyPLearnScript.h
===================================================================
--- trunk/plearn/io/PyPLearnScript.h	2009-05-06 20:55:34 UTC (rev 10182)
+++ trunk/plearn/io/PyPLearnScript.h	2009-05-07 14:29:36 UTC (rev 10183)
@@ -47,7 +47,6 @@
 #include <plearn/base/Object.h>
 #include <plearn/io/PPath.h>
 #include <plearn/io/openString.h>
-#include <plearn/io/pl_log.h>
 #include <vector>
 
 namespace PLearn {



From nouiz at mail.berlios.de  Thu May  7 16:55:54 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 16:55:54 +0200
Subject: [Plearn-commits] r10184 - trunk/commands/PLearnCommands
Message-ID: <200905071455.n47EtsmW007964@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 16:55:53 +0200 (Thu, 07 May 2009)
New Revision: 10184

Modified:
   trunk/commands/PLearnCommands/RunCommand.cc
Log:
added missing include.


Modified: trunk/commands/PLearnCommands/RunCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/RunCommand.cc	2009-05-07 14:29:36 UTC (rev 10183)
+++ trunk/commands/PLearnCommands/RunCommand.cc	2009-05-07 14:55:53 UTC (rev 10184)
@@ -47,9 +47,9 @@
 #include <plearn/sys/Popen.h>
 
 #include <plearn/io/PyPLearnScript.h>
-
 #include <plearn/io/openString.h>
 #include <plearn/io/openFile.h>
+#include <plearn/io/pl_log.h>
 
 #include <algorithm>
 



From nouiz at mail.berlios.de  Thu May  7 17:07:11 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 17:07:11 +0200
Subject: [Plearn-commits] r10185 - trunk/commands/PLearnCommands
Message-ID: <200905071507.n47F7BRf009236@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 17:07:11 +0200 (Thu, 07 May 2009)
New Revision: 10185

Modified:
   trunk/commands/PLearnCommands/DiffCommand.cc
Log:
allow to load more type of file.


Modified: trunk/commands/PLearnCommands/DiffCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/DiffCommand.cc	2009-05-07 14:55:53 UTC (rev 10184)
+++ trunk/commands/PLearnCommands/DiffCommand.cc	2009-05-07 15:07:11 UTC (rev 10185)
@@ -44,11 +44,8 @@
 #include "DiffCommand.h"
 #include <plearn/base/Object.h>
 #include <plearn/base/PLearnDiff.h>
-#include <plearn/io/fileutils.h>
-#include <plearn/io/openFile.h>
-#include <plearn/io/openString.h>
+#include <plearn/io/PyPLearnScript.h>
 #include <plearn/math/TVec_decl.h>
-//#include <plearn/math/TMat_impl.h>
 
 namespace PLearn {
 using namespace std;
@@ -104,10 +101,8 @@
     // Load objects.
     TVec< PP<Object> > obj;
     for (int i = 0; i < n; i++) {
-        PP<Object> new_object;
-        string object_spec = readFileAndMacroProcess(obj_spec[i]);
-        PStream in = openString(object_spec, PStream::plearn_ascii);
-        in >> new_object;
+        PP<Object> new_object = smartLoadObject(obj_spec[i]);
+
         if (!new_object)
             PLERROR("In DiffCommand::run - Unable to serialize file %s as an Object",
                     obj_spec[i].absolute().c_str());



From nouiz at mail.berlios.de  Thu May  7 17:43:04 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 17:43:04 +0200
Subject: [Plearn-commits] r10186 - trunk/plearn/io
Message-ID: <200905071543.n47Fh4GB011837@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 17:43:03 +0200 (Thu, 07 May 2009)
New Revision: 10186

Modified:
   trunk/plearn/io/PyPLearnScript.cc
   trunk/plearn/io/PyPLearnScript.h
Log:
bugfix smartLoadObject and parse the args in the filename test.plearn::var=X::...


Modified: trunk/plearn/io/PyPLearnScript.cc
===================================================================
--- trunk/plearn/io/PyPLearnScript.cc	2009-05-07 15:07:11 UTC (rev 10185)
+++ trunk/plearn/io/PyPLearnScript.cc	2009-05-07 15:43:03 UTC (rev 10186)
@@ -323,11 +323,28 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
-Object* smartLoadObject(PPath filepath, const vector<string>& args, time_t& return_date)
+Object* smartLoadObject(PPath filepath, const vector<string>& args_, time_t& return_date)
 {
-    if (!isfile(filepath))
-        PLERROR("Non-existent script file: %s\n",filepath.c_str());
-    
+    vector<string> args = args_;
+    vector<string> args_augmented;
+
+    if (!isfile(filepath)) {
+        // There is no file with this exact name. Maybe there are parameters
+        // appended to the name?
+        string base;
+        map<string, string> params;
+        parseBaseAndParameters(filepath, base, params);
+        if (!isfile(base))
+            PLERROR("Non-existent script file: %s\n", filepath.c_str());
+        // Add new arguments.
+        args_augmented = args;
+        map<string, string>::const_iterator it = params.begin();
+        for (; it != params.end(); it++)
+            args_augmented.push_back(it->first + "=" + it->second);
+        args = args_augmented;
+        filepath = base;
+    }
+
     const string extension = extract_extension(filepath);
     string script;
     time_t date = 0;

Modified: trunk/plearn/io/PyPLearnScript.h
===================================================================
--- trunk/plearn/io/PyPLearnScript.h	2009-05-07 15:07:11 UTC (rev 10185)
+++ trunk/plearn/io/PyPLearnScript.h	2009-05-07 15:43:03 UTC (rev 10186)
@@ -199,7 +199,7 @@
 { time_t d=0; return smartLoadObject(filepath, args, d); }
 //! Same as smartLoadObject(PPath, vector<string>, time_t) but passing an empty vector<string>
 inline Object* smartLoadObject(PPath filepath, time_t& return_date)
-{ vector<string> args; return smartLoadObject(filepath, args,return_date); }
+{ vector<string> args; args.push_back("");/* empty script filename*/ return smartLoadObject(filepath, args,return_date); }
 //! Same as smartLoadObject(PPath, vector<string>, time_t) but passing an empty vector<string> 
 //! and an empty return_date
 inline Object* smartLoadObject(PPath filepath)



From nouiz at mail.berlios.de  Thu May  7 17:49:01 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 17:49:01 +0200
Subject: [Plearn-commits] r10187 - trunk/commands/PLearnCommands
Message-ID: <200905071549.n47Fn1rG012422@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 17:48:59 +0200 (Thu, 07 May 2009)
New Revision: 10187

Modified:
   trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
Log:
to the read_and_write command, it now use smartLoadObject() to load more type offile and have a new option --mode={plearn_ascii,plearn_binary} to test the different format.


Modified: trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-05-07 15:43:03 UTC (rev 10186)
+++ trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-05-07 15:48:59 UTC (rev 10187)
@@ -40,10 +40,9 @@
 #include "ReadAndWriteCommand.h"
 #include <plearn/base/Object.h>
 #include <plearn/base/stringutils.h>      //!< For extract_extension.
-#include <plearn/io/fileutils.h>        //!< For readFileAndMacroProcess.
-#include <plearn/io/load_and_save.h>
+#include <plearn/io/fileutils.h>        //!< For mtime.
+#include <plearn/io/PyPLearnScript.h>   //!< For smartLoadObject
 #include <plearn/io/openFile.h>
-#include <plearn/io/openString.h>
 
 namespace PLearn {
 using namespace std;
@@ -56,7 +55,7 @@
                 
                   "Used to check (debug) the serialization system",
                 
-                  "read_and_write <sourcefile> <destfile> [--updaet] [modification string] ...\n"
+                  "read_and_write <sourcefile> <destfile> [--update] [modification string] ...\n"
                   "Reads an Object (in PLearn serialization format) from the <sourcefile> and writes it to the <destfile>\n"
                   "If the sourcefile ends with a .psave file, then it will not be subjected to macro preprosessing \n"
                   "Otherwise (ex: .plearn .vmat) it will. \n"
@@ -69,7 +68,7 @@
 void ReadAndWriteCommand::run(const vector<string>& args)
 {
     if(args.size()<2)
-        PLERROR("read_and_write takes 2 or more arguments: <sourcefile> <destfile> [--update] [modification string] ...");
+        PLERROR("read_and_write takes 2 or more arguments: <sourcefile> <destfile> [--update] [--mode={plearn_ascii,plearn_binary}][modification string] ...");
     string source = args[0];
     string dest = args[1];
 
@@ -78,27 +77,25 @@
     time_t date_src=0;
 
     //read the file
-    if(ext==".psave") // may be binay. Don't macro-process
-    {
-        PLearn::load(source,o);
-        date_src=mtime(source);
-    }
-    else
-    {
-        map<string, string> vars;
-        string script = readFileAndMacroProcess(source, vars, date_src);
-        PStream in = openString(script,PStream::plearn_ascii);
-        o = readObject(in);
-    }
-    int idx_start=2;
-    if(args.size()>2 && args[2]=="--update"){
-        PLCHECK(date_src>0);
+    o=smartLoadObject(source, date_src);
+
+    uint idx_start=2;
+    PStream::mode_t mode = PStream::plearn_ascii;
+    for(;idx_start<args.size();){
+        if(args[idx_start]=="--update"){
+            PLCHECK(date_src>0);
+            time_t date_dst=mtime(dest);
+            if((date_dst>date_src) && (date_src>0)){
+                pout << "The file is up to date. We don't regenerate it."<<endl;
+                return;
+            }
+        } else if(args[idx_start]=="--mode=plearn_ascii"){
+            mode=PStream::plearn_ascii;
+        } else if(args[idx_start]=="--mode=plearn_binary"){
+            mode=PStream::plearn_binary;
+        } else
+            break;//the rest are modification string
         idx_start++;
-        time_t date_dst=mtime(dest);
-        if((date_dst>date_src) && (date_src>0)){
-            pout << "The file is up to date. We do not regenerate it."<<endl;
-            return;
-        }
     }
 
     //modif the object
@@ -110,7 +107,7 @@
     }
 
     //write the file
-    PStream out = openFile(dest,PStream::plearn_ascii,"w");
+    PStream out = openFile(dest, mode, "w");
     if(!out)
         PLERROR("Could not open file %s for writing",dest.c_str());
     out << *o;



From lamblin at mail.berlios.de  Thu May  7 17:54:32 2009
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 7 May 2009 17:54:32 +0200
Subject: [Plearn-commits] r10188 - trunk/plearn_learners/online
Message-ID: <200905071554.n47FsWBJ013015@sheep.berlios.de>

Author: lamblin
Date: 2009-05-07 17:54:31 +0200 (Thu, 07 May 2009)
New Revision: 10188

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
Correct check for target size


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2009-05-07 15:48:59 UTC (rev 10187)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2009-05-07 15:54:31 UTC (rev 10188)
@@ -759,7 +759,7 @@
         // check target size and final_cost->input_size
         if( n_classes == 0 ) // regression
         {
-            if( final_cost->input_size != targetsize() )
+            if( targetsize_ >= 0 && final_cost->input_size != targetsize() )
                 PLERROR("DeepBeliefNet::build_final_cost() - "
                     "final_cost->input_size (%d) != targetsize() (%d), "
                     "although we are doing regression (n_classes == 0).\n",



From nouiz at mail.berlios.de  Thu May  7 18:05:13 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 18:05:13 +0200
Subject: [Plearn-commits] r10189 - trunk/commands/PLearnCommands
Message-ID: <200905071605.n47G5Dj3014215@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 18:05:13 +0200 (Thu, 07 May 2009)
New Revision: 10189

Modified:
   trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
Log:
better doc.


Modified: trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-05-07 15:54:31 UTC (rev 10188)
+++ trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-05-07 16:05:13 UTC (rev 10189)
@@ -55,7 +55,7 @@
                 
                   "Used to check (debug) the serialization system",
                 
-                  "read_and_write <sourcefile> <destfile> [--update] [modification string] ...\n"
+                  "read_and_write <sourcefile> <destfile> [--update] [--mode={plearn_ascii,plearn_binary}] [modification string] ...\n"
                   "Reads an Object (in PLearn serialization format) from the <sourcefile> and writes it to the <destfile>\n"
                   "If the sourcefile ends with a .psave file, then it will not be subjected to macro preprosessing \n"
                   "Otherwise (ex: .plearn .vmat) it will. \n"



From nouiz at mail.berlios.de  Thu May  7 18:12:37 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 18:12:37 +0200
Subject: [Plearn-commits] r10190 - trunk/plearn_learners/hyper
Message-ID: <200905071612.n47GCbET014673@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 18:12:36 +0200 (Thu, 07 May 2009)
New Revision: 10190

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperLearner.h
Log:
added option HyperLearner::save_mode that can take plearn_ascii or plearn_binary to change the mode in witch we save the file.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-07 16:05:13 UTC (rev 10189)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-07 16:12:36 UTC (rev 10190)
@@ -75,7 +75,8 @@
 }
 
 HyperLearner::HyperLearner()
-    : provide_strategy_expdir(true),
+    : save_mode_(PStream::plearn_ascii),
+      provide_strategy_expdir(true),
       save_final_learner(true),
       save_strategy_learner(false),
       reloaded(false),
@@ -132,8 +133,11 @@
                   "should final learner be saved in expdir/final_learner.psave");
 
     declareOption(ol, "save_strategy_learner", &HyperLearner::save_strategy_learner, OptionBase::buildoption,
-                  "should final learner be saved in expdir/Strat#/final_learner.psave");
+                  "should final learner be saved in expdir/Strat#final_learner.psave");
 
+    declareOption(ol, "save_mode", &HyperLearner::save_mode, OptionBase::buildoption,
+                  "The mode to use to save the file. Default plearn_ascii.");
+
     declareOption(
         ol, "finalize_learner", &HyperLearner::finalize_learner,
         OptionBase::buildoption,
@@ -251,7 +255,7 @@
                     PLERROR("Cannot save the strategy model: no experiment directory has been set");
                 if( getLearner().isNull() )
                     PLERROR("Cannot save final model: no final learner available");
-                PLearn::save(strat_expdir+"final_learner.psave",*getLearner());
+                PLearn::save(strat_expdir+"final_learner.psave",*getLearner(), save_mode_);
             }
         }
 
@@ -266,7 +270,7 @@
                 PLERROR("Cannot save final model: no experiment directory has been set");
             if( getLearner().isNull() )
                 PLERROR("Cannot save final model: no final learner available");
-            PLearn::save(expdir+"final_learner.psave",*getLearner());
+            PLearn::save(expdir+"final_learner.psave",*getLearner(), save_mode_);
         }
 
         stage = 1;
@@ -315,7 +319,14 @@
 
     for(int commandnum=0; commandnum<strategy.length(); commandnum++)
         strategy[commandnum]->setHyperLearner(this);
-
+    if(save_mode.empty());
+    else if(save_mode=="plearn_ascii")
+        save_mode_ = PStream::plearn_ascii;
+    else if(save_mode=="plearn_binary")
+        save_mode_ = PStream::plearn_binary;
+    else
+        PLERROR("In HyperLearner::build_(): invalid save_mode %s",
+                save_mode.c_str());
 }
 
 /////////
@@ -380,7 +391,7 @@
     if(verbosity>0)
         perr << "In HyperLearner::auto_save() - We save the hlearner"
              << endl;
-    PLearn::save(tmp, this);
+    PLearn::save(tmp, this, save_mode_);
 
 #ifdef BOUNDCHECK
     HyperLearner *n = new HyperLearner();

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2009-05-07 16:05:13 UTC (rev 10189)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2009-05-07 16:12:36 UTC (rev 10190)
@@ -73,9 +73,11 @@
 
     TVec< PP<HyperCommand> > strategy; //!< The strategy to follow to optimize hyper parameters
 
+    string save_mode;
+    PStream::mode_t save_mode_;
     bool provide_strategy_expdir; //!< should each strategy step be provided a directory expdir/Step#
     bool save_final_learner; //!< should final learner be saved in expdir/final_learner.psave
-    bool save_strategy_learner; //!< should final learner be saved in expdir/Strat#/final_learner.psave
+    bool save_strategy_learner; //!< should each Strat# learner be saved in expdir/Strat#final_learner.psave
     bool reloaded; //!< needed for a warning
     // HyperLearner methods
 



From nouiz at mail.berlios.de  Thu May  7 19:08:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 19:08:26 +0200
Subject: [Plearn-commits] r10191 - trunk/plearn/io
Message-ID: <200905071708.n47H8Qb4013253@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 19:08:25 +0200 (Thu, 07 May 2009)
New Revision: 10191

Modified:
   trunk/plearn/io/PyPLearnScript.h
Log:
added comment.


Modified: trunk/plearn/io/PyPLearnScript.h
===================================================================
--- trunk/plearn/io/PyPLearnScript.h	2009-05-07 16:12:36 UTC (rev 10190)
+++ trunk/plearn/io/PyPLearnScript.h	2009-05-07 17:08:25 UTC (rev 10191)
@@ -190,6 +190,7 @@
 //! .plearn .vmat : perform simple plearn macro processing
 //! .pyplearn .pymat: use python preprocessor
 //! The given args vector can be used to pass string arguments of the form argname=value.
+//!   The first arguments is ignored as we consider that it is the script name.
 //! The return_date is set to the lastest date of dependence of file. 
 //!   Otherwise return (time_t)0. Work for .vmat, .psave and .plearn file.
 Object* smartLoadObject(PPath filepath, const vector<string>& args, time_t& return_date);



From nouiz at mail.berlios.de  Thu May  7 21:18:24 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 May 2009 21:18:24 +0200
Subject: [Plearn-commits] r10192 - trunk/python_modules/plearn/pymake
Message-ID: <200905071918.n47JIOtD010921@sheep.berlios.de>

Author: nouiz
Date: 2009-05-07 21:18:23 +0200 (Thu, 07 May 2009)
New Revision: 10192

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
fixed a bug with the -link-target option. If it was the same value as the default link name, it was bugged.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-05-07 17:08:25 UTC (rev 10191)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-05-07 19:18:23 UTC (rev 10192)
@@ -952,6 +952,14 @@
                 if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll:
                     # Refresh symbolic link.
                     info.make_symbolic_link(linkname, None, info.corresponding_output)
+
+                    if link_target_override and os.path.islink(info.corresponding_output):
+                        src=os.path.realpath(info.corresponding_output)
+                        os.remove(info.corresponding_output)
+                        shutil.copyfile(src, info.corresponding_output)
+                        shutil.copymode(src, info.corresponding_output)
+                        print "The link target was a symlink. We replaced it with a binary."
+
                     print 'Target', info.filebase, 'is up to date.'
                 else:
                     executables_to_link[info] = 1
@@ -2021,6 +2029,10 @@
 
         # In the following, we create the link 'symlink_from' -> 'symlink_to'.
 
+        #if we overrided the link-target, we should link to it.
+        if link_target_override and not symlink_to:
+            symlink_to=os.path.abspath(link_target_override)
+
         # First, we change to the directory of the source file. This is to
         # ensure that if the target is given by a relative path (in OBJS/...)
         # then this relative path is taken relative to the source file path.
@@ -2055,9 +2067,13 @@
         else:
             symlink_from = linkname
 
-        # Create symbolic link.
-        toolkit.symlink(symlink_to, symlink_from, True, True)
+        #we don't create a symlink to itself.
+        #otherwise their is a bug if link_target_override if the same as the destination of 
 
+        if not link_target_override or os.path.abspath(link_target_override)!=os.path.abspath(symlink_from):
+            # Create symbolic link.
+            toolkit.symlink(symlink_to, symlink_from, True, True)
+
         # Restore original working directory.
         os.chdir(backup_cwd)
 



From lamblin at mail.berlios.de  Thu May  7 23:19:47 2009
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 7 May 2009 23:19:47 +0200
Subject: [Plearn-commits] r10193 - trunk/plearn_learners/online
Message-ID: <200905072119.n47LJlEF025324@sheep.berlios.de>

Author: lamblin
Date: 2009-05-07 23:19:46 +0200 (Thu, 07 May 2009)
New Revision: 10193

Modified:
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
   trunk/plearn_learners/online/CrossEntropyCostModule.h
   trunk/plearn_learners/online/SquaredErrorCostModule.cc
Log:
Add standard (non-port) version of backprop to CrossEntropyCostModule.


Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2009-05-07 19:18:23 UTC (rev 10192)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2009-05-07 21:19:46 UTC (rev 10193)
@@ -110,7 +110,6 @@
             //                         = -log(1+exp(x)) = -softplus(x)
             cost += (1-target_i) * softplus(-activation_i);
     }
-
 }
 
 void CrossEntropyCostModule::fprop(const Vec& input, const Vec& target, Vec& cost) const
@@ -128,6 +127,59 @@
 
 }
 
+
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void CrossEntropyCostModule::bpropUpdate(const Vec& input, const Vec& target,
+                                         real cost, Vec& input_gradient,
+                                         bool accumulate)
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
+
+    if (accumulate)
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize(input_size);
+        input_gradient.clear();
+    }
+
+    for (int i=0; i < target_size; i++)
+        input_gradient[i] += target[i] - sigmoid(-input[i]);
+}
+
+void CrossEntropyCostModule::bpropUpdate(const Mat& inputs, const Mat& targets,
+                                         const Vec& costs,
+                                         Mat& input_gradients, bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+
+    int batch_size = inputs.length();
+
+    if (accumulate)
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size &&
+                      input_gradients.length() == batch_size,
+                      "Cannot resize input_gradients AND accumulate into it" );
+    }
+    else
+    {
+        input_gradients.resize(batch_size, input_size);
+        input_gradients.clear();
+    }
+
+    for (int i=0; i < batch_size; i++)
+        for (int j=0; j < target_size; j++)
+            input_gradients(i, j) += targets(i, j) - sigmoid(-inputs(i, j));
+}
+
 void CrossEntropyCostModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
                                    const TVec<Mat*>& ports_gradient)
 {

Modified: trunk/plearn_learners/online/CrossEntropyCostModule.h
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.h	2009-05-07 19:18:23 UTC (rev 10192)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.h	2009-05-07 21:19:46 UTC (rev 10193)
@@ -68,6 +68,15 @@
     virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
     virtual void fprop(const Mat& input, const Mat& target, Mat& cost) const;
 
+    //! Standard backpropagation
+    virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
+                             Vec& input_gradient, bool accumulate=false);
+
+    //! Minibatch backpropagation
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+                             const Vec& costs, Mat& input_gradients,
+                             bool accumulate=false);
+
     //! New version of backpropagation
     virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
                                 const TVec<Mat*>& ports_gradient);

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.cc	2009-05-07 19:18:23 UTC (rev 10192)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.cc	2009-05-07 21:19:46 UTC (rev 10193)
@@ -160,12 +160,12 @@
     {
         PLASSERT_MSG( input_gradients.width() == input_size &&
                       input_gradients.length() == inputs.length(),
-                      "Cannot resize input_gradient AND accumulate into it" );
+                      "Cannot resize input_gradients AND accumulate into it" );
     }
     else
     {
         input_gradients.resize( inputs.length(), input_size );
-        input_gradients.fill(0);
+        input_gradients.clear();
     }
 
     // input_gradient = 2*(input - target)



From nouiz at mail.berlios.de  Fri May  8 18:47:27 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 May 2009 18:47:27 +0200
Subject: [Plearn-commits] r10194 - in trunk: plearn/io plearn_learners/hyper
	plearn_learners/testers
Message-ID: <200905081647.n48GlR3J022417@sheep.berlios.de>

Author: nouiz
Date: 2009-05-08 18:47:23 +0200 (Fri, 08 May 2009)
New Revision: 10194

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
added option PTester::save_mode as HyperLearner::save_mode to allow saving in the more compact file format plearn_binary.


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2009-05-07 21:19:46 UTC (rev 10193)
+++ trunk/plearn/io/PStream.cc	2009-05-08 16:47:23 UTC (rev 10194)
@@ -264,7 +264,19 @@
     return oldmode;
 }
 
-
+PStream::mode_t PStream::parseModeT(const string& str_mode){
+    PStream::mode_t mode;
+    if(str_mode.empty())
+        mode = PStream::plearn_ascii;
+    else if(str_mode=="plearn_ascii")
+        mode = PStream::plearn_ascii;
+    else if(str_mode=="plearn_binary")
+        mode = PStream::plearn_binary;
+    else
+        PLERROR("In PStream::parseModeT(%s): invalid mode",
+                str_mode.c_str());
+    return mode;
+}
 /////////////
 // readAll //
 /////////////

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2009-05-07 21:19:46 UTC (rev 10193)
+++ trunk/plearn/io/PStream.h	2009-05-08 16:47:23 UTC (rev 10194)
@@ -269,6 +269,10 @@
         return *this;
     }
 
+    //!parse a string and return the equivalent PStream::mode_t.
+    //!if empty string, we return plearn_ascii
+    static PStream::mode_t parseModeT(const string& str);
+
 public:
     //op()'s: re-init with different underlying stream(s)
 

Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-07 21:19:46 UTC (rev 10193)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-08 16:47:23 UTC (rev 10194)
@@ -319,14 +319,8 @@
 
     for(int commandnum=0; commandnum<strategy.length(); commandnum++)
         strategy[commandnum]->setHyperLearner(this);
-    if(save_mode.empty());
-    else if(save_mode=="plearn_ascii")
-        save_mode_ = PStream::plearn_ascii;
-    else if(save_mode=="plearn_binary")
-        save_mode_ = PStream::plearn_binary;
-    else
-        PLERROR("In HyperLearner::build_(): invalid save_mode %s",
-                save_mode.c_str());
+
+    save_mode_ = PStream::parseModeT(save_mode);
 }
 
 /////////

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2009-05-07 21:19:46 UTC (rev 10193)
+++ trunk/plearn_learners/testers/PTester.cc	2009-05-08 16:47:23 UTC (rev 10194)
@@ -88,6 +88,7 @@
 PTester::PTester():
        reloaded(false),
        need_to_save_test_names(false),
+       save_mode_(PStream::plearn_ascii),
        provide_learner_expdir(false),
        report_stats(true),
        save_data_sets(false),
@@ -201,8 +202,12 @@
 
     declareOption(
         ol, "save_learners", &PTester::save_learners, OptionBase::buildoption,
-        "If true, the final trained learner for split#k will be saved in Split#k/final_learner.psave");
+        "If true, the final trained learner for split#k will be saved in Split#k/final_learner.psave."
+        "The format is defined by save_mode");
 
+    declareOption(ol, "save_mode", &PTester::save_mode, OptionBase::buildoption,
+                  "The mode to use to save the file. Default plearn_ascii.");
+
     declareOption(
         ol, "save_initial_learners", &PTester::save_initial_learners, OptionBase::buildoption,
         "If true, the initial untrained learner for split#k (just after forget() has been called) will be saved in Split#k/initial_learner.psave");
@@ -464,6 +469,8 @@
                           c,nb_testset);
         }
     }
+
+    save_mode_ = PStream::parseModeT(save_mode);
 }
 
 // ### Nothing to add here, simply calls build_
@@ -585,7 +592,7 @@
             if (save_stat_collectors)
                 PLearn::save(splitdir / "train_stats.psave", train_stats);
             if (save_learners)
-                PLearn::save(splitdir / "final_learner.psave", learner);
+                PLearn::save(splitdir / "final_learner.psave", learner, save_mode_);
         }
     }
     else

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2009-05-07 21:19:46 UTC (rev 10193)
+++ trunk/plearn_learners/testers/PTester.h	2009-05-08 16:47:23 UTC (rev 10194)
@@ -87,6 +87,9 @@
     TVec<string> final_commands;
     PP<VecStatsCollector> global_template_stats_collector;
     PP<PLearner> learner;
+    string save_mode;
+    PStream::mode_t save_mode_;
+
     bool provide_learner_expdir;
     bool report_stats;
     bool save_data_sets;



From lamblin at mail.berlios.de  Fri May  8 19:59:57 2009
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 8 May 2009 19:59:57 +0200
Subject: [Plearn-commits] r10195 - trunk/plearn_learners/online
Message-ID: <200905081759.n48Hxv2P026355@sheep.berlios.de>

Author: lamblin
Date: 2009-05-08 19:59:57 +0200 (Fri, 08 May 2009)
New Revision: 10195

Modified:
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
Log:
Fix computation of fprop


Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2009-05-08 16:47:23 UTC (rev 10194)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2009-05-08 17:59:57 UTC (rev 10195)
@@ -95,20 +95,14 @@
     real target_i, activation_i;
     for( int i=0 ; i < target_size ; i++ )
     {
+        // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+        // but it is numerically unstable, so use instead the following:
+        //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+        //     = act + softplus(-act) - target*act
+        //     = softplus(act) - target*act
         target_i = target[i];
         activation_i = input[i];
-        if(!fast_exact_is_equal(target_i,0.0))
-            // nll -= target[i] * pl_log(expectations[i]);
-            // but it is numerically unstable, so use instead
-            // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-            // but note that expectation = sigmoid(-activation)
-            cost += target_i * softplus(activation_i);
-        if(!fast_exact_is_equal(target_i,1.0))
-            // ret -= (1-target_i) * pl_log(1-expectation_i);
-            // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
-            //                         = log(1/(1+exp(x)))
-            //                         = -log(1+exp(x)) = -softplus(x)
-            cost += (1-target_i) * softplus(-activation_i);
+        cost += softplus(activation_i) - target_i * activation_i;
     }
 }
 



From nouiz at mail.berlios.de  Fri May  8 20:55:56 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 May 2009 20:55:56 +0200
Subject: [Plearn-commits] r10196 - trunk/python_modules/plearn/parallel
Message-ID: <200905081855.n48ItuwG030133@sheep.berlios.de>

Author: nouiz
Date: 2009-05-08 20:55:55 +0200 (Fri, 08 May 2009)
New Revision: 10196

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
check that their is some computer with the requested requirement.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-05-08 17:59:57 UTC (rev 10195)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-05-08 18:55:55 UTC (rev 10196)
@@ -840,8 +840,13 @@
 
         #transform from meg to kilo
         self.mem=self.mem*1024
-
-        self.os = self.os.upper()
+        if not self.os:
+            #if their is not required os, condor launch on the same os.
+            p=Popen( "condor_config_val OpSyS", shell=True,stdout=PIPE)
+            p.wait()
+            self.os=p.stdout.readlines()[0].strip()
+        else: self.os = self.os.upper()
+        
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
 
@@ -1135,7 +1140,7 @@
                     pwd 1>&2
                     echo "nb args: $#" 1>&2
                     echo "Running: command: \\"$@\\"" 1>&2
-                    #[ -x "$1" ];echo $?
+                    [ -x "$1" ];echo $?
                     %s
                     ret=$?
                     rm -f echo ${KRB5CCNAME:5}
@@ -1163,6 +1168,7 @@
                 echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
                 echo "OMP_NUM_THREADS: $OMP_NUM_THREADS"
                 echo "CONDOR_JOB_LOGDIR: $CONDOR_JOB_LOGDIR"
+                echo "HOME: $HOME"
                 pwd
                 echo "Running command: $argv"
                 $argv
@@ -1177,7 +1183,17 @@
 
             os.chmod(launch_tmp_file, 0755)
             os.rename(launch_tmp_file, self.launch_file)
+
     def print_common_condor_submit(self, fd, output, error, arguments=None):
+        #check that their is some host with those requirement
+        cmd="""condor_status -const '%s' -tot |wc"""%self.req
+        p=Popen( cmd, shell=True,stdout=PIPE)
+        p.wait()
+        lines=p.stdout.readlines()
+        if p.returncode != 0 or lines==['      1       0       1\n']:
+            raise DBIError("Their is no compute node with those requirement: %s."%self.req)
+
+
         fd.write( dedent('''\
                 executable     = %s
                 universe       = %s



From lamblin at mail.berlios.de  Fri May  8 21:58:54 2009
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 8 May 2009 21:58:54 +0200
Subject: [Plearn-commits] r10197 - trunk/plearn_learners/online
Message-ID: <200905081958.n48Jwse6001784@sheep.berlios.de>

Author: lamblin
Date: 2009-05-08 21:58:54 +0200 (Fri, 08 May 2009)
New Revision: 10197

Modified:
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
Log:
Now fix bprop according to fprop...


Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2009-05-08 18:55:55 UTC (rev 10196)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2009-05-08 19:58:54 UTC (rev 10197)
@@ -145,7 +145,7 @@
     }
 
     for (int i=0; i < target_size; i++)
-        input_gradient[i] += target[i] - sigmoid(-input[i]);
+        input_gradient[i] += sigmoid(input[i]) - target[i];
 }
 
 void CrossEntropyCostModule::bpropUpdate(const Mat& inputs, const Mat& targets,
@@ -171,7 +171,7 @@
 
     for (int i=0; i < batch_size; i++)
         for (int j=0; j < target_size; j++)
-            input_gradients(i, j) += targets(i, j) - sigmoid(-inputs(i, j));
+            input_gradients(i, j) += sigmoid(inputs(i, j)) - targets(i, j);
 }
 
 void CrossEntropyCostModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
@@ -214,7 +214,7 @@
         for( int i=0; i < batch_size; i++ )
             for ( int j=0; j < target->width(); j++ )
                 (*prediction_grad)(i, j) +=
-                (*cost_grad)(i,0)*((*target)(i,j) - sigmoid(-(*prediction)(i,j) ));
+                (*cost_grad)(i,0)*(sigmoid((*prediction)(i,j)) - (*target)(i,j));
     }
 
     else if( !prediction_grad && !target_grad &&



From nouiz at mail.berlios.de  Tue May 12 14:46:20 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 12 May 2009 14:46:20 +0200
Subject: [Plearn-commits] r10198 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200905121246.n4CCkKR4008236@sheep.berlios.de>

Author: nouiz
Date: 2009-05-12 14:46:20 +0200 (Tue, 12 May 2009)
New Revision: 10198

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
changed default. Now by default, we will force to have 1 cores for each jobs. -1 don't force it(will take the default the back end use).


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-05-08 19:58:54 UTC (rev 10197)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-05-12 12:46:20 UTC (rev 10198)
@@ -183,7 +183,7 @@
         self.stdouts = ''
         self.stderrs = ''
         self.raw = ''
-        self.cpu = 0
+        self.cpu = 1
         self.mem = 0
 
         for key in args.keys():
@@ -604,13 +604,11 @@
         self.m32G = False
         self.set_special_env = True
         self.env = ""
-        self.cpu = 0
         DBIBase.__init__(self, commands, **args)
         
         self.nb_proc = int(self.nb_proc)
         self.micro = int(self.micro)
         self.nano = int(self.nano)
-        self.cpu = int(self.cpu)
 
         if self.set_special_env and self.cpu>0:
             self.env+=' OMP_NUM_THREADS=%d'%self.cpu

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-05-08 19:58:54 UTC (rev 10197)
+++ trunk/scripts/dbidispatch	2009-05-12 12:46:20 UTC (rev 10198)
@@ -104,7 +104,7 @@
 bqtools, cluster and condor option:
   The '--mem=X' speficify the number of ram in meg the program need to execute.
   The '--cpu=nb_cpu_per_node' option determine the number of cpu(cores) that 
-    will be reserved for each job.
+    will be reserved for each job.(default=1, -1 won't set it)
 
 bqtools and cluster option:
   The '--duree' option specifies the maximum duration of the jobs. The syntax 



From nouiz at mail.berlios.de  Tue May 12 17:01:07 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 12 May 2009 17:01:07 +0200
Subject: [Plearn-commits] r10199 - trunk/scripts
Message-ID: <200905121501.n4CF17wd027575@sheep.berlios.de>

Author: nouiz
Date: 2009-05-12 17:01:06 +0200 (Tue, 12 May 2009)
New Revision: 10199

Modified:
   trunk/scripts/dbidispatch
Log:
set /data/ to be not under kerberos.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-05-12 12:46:20 UTC (rev 10198)
+++ trunk/scripts/dbidispatch	2009-05-12 15:01:06 UTC (rev 10199)
@@ -410,7 +410,7 @@
         dbi_param["pkdilly"]=True
 
     p = os.path.abspath(os.path.curdir)
-    nokerb_path=["/home/fringant1/","/home/fringant2/","/cluster/"]
+    nokerb_path=["/home/fringant1/","/home/fringant2/","/cluster/", "/data/"]
     pkdilly=False
     dir_with_kerb=not any([p.startswith(x) for x in nokerb_path])
     if not dir_with_kerb or dbi_param.get('files'):



From tihocan at mail.berlios.de  Tue May 12 19:17:50 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 12 May 2009 19:17:50 +0200
Subject: [Plearn-commits] r10200 - trunk/plearn/vmat
Message-ID: <200905121717.n4CHHoUj022087@sheep.berlios.de>

Author: tihocan
Date: 2009-05-12 19:17:48 +0200 (Tue, 12 May 2009)
New Revision: 10200

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
Minor typo fix in help

Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2009-05-12 15:01:06 UTC (rev 10199)
+++ trunk/plearn/vmat/VMatLanguage.cc	2009-05-12 17:17:48 UTC (rev 10200)
@@ -93,7 +93,7 @@
                         "for a 0.5 thresholding: 0.5 < 0 1 ifelse. To copy a single field, use [field].\n"
                         "There is also a special feature available only for single field copies: if you\n"
                         "use the syntax [field?], then VPL will not produce an error if the field cannot\n"
-                        "be found. END-N(N an integer) will select a field conting from the END.\n"
+                        "be found. END-N (N an integer) will select a field counting from the END.\n"
                         "\n"
                         "Here's a real-life example of a VPL program:\n"
                         "\n"



From plearner at mail.berlios.de  Tue May 12 19:57:00 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 12 May 2009 19:57:00 +0200
Subject: [Plearn-commits] r10201 - trunk/plearn/var
Message-ID: <200905121757.n4CHv0nZ001635@sheep.berlios.de>

Author: plearner
Date: 2009-05-12 19:57:00 +0200 (Tue, 12 May 2009)
New Revision: 10201

Modified:
   trunk/plearn/var/NaryVariable.cc
   trunk/plearn/var/NaryVariable.h
Log:
Added setParents method


Modified: trunk/plearn/var/NaryVariable.cc
===================================================================
--- trunk/plearn/var/NaryVariable.cc	2009-05-12 17:17:48 UTC (rev 10200)
+++ trunk/plearn/var/NaryVariable.cc	2009-05-12 17:57:00 UTC (rev 10201)
@@ -101,6 +101,11 @@
     deepCopyField(varray, copies);
 }
 
+void NaryVariable::setParents(const VarArray& parents)
+{
+    varray = parents;
+    sizeprop();
+}
 
 bool NaryVariable::markPath()
 {

Modified: trunk/plearn/var/NaryVariable.h
===================================================================
--- trunk/plearn/var/NaryVariable.h	2009-05-12 17:17:48 UTC (rev 10200)
+++ trunk/plearn/var/NaryVariable.h	2009-05-12 17:57:00 UTC (rev 10201)
@@ -79,6 +79,8 @@
     
     //#####  PLearn::Variable Interface  ######################################
 
+    virtual void setParents(const VarArray& parents);
+
     virtual bool markPath();
     virtual void buildPath(VarArray& proppath);
     virtual VarArray sources();



From nouiz at mail.berlios.de  Tue May 12 23:01:41 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 12 May 2009 23:01:41 +0200
Subject: [Plearn-commits] r10202 - trunk/python_modules/plearn/parallel
Message-ID: <200905122101.n4CL1fBJ026024@sheep.berlios.de>

Author: nouiz
Date: 2009-05-12 23:01:40 +0200 (Tue, 12 May 2009)
New Revision: 10202

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
-small bugfix with a cast to str
-Now try to get kerberos ticket 3 times to help solve a bug that we renew a the kerb script with 0 ticket
-Don't renew the ticket if we didn't got new ticket
-print more debut info in we don't got new ticket


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-05-12 17:57:00 UTC (rev 10201)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-05-12 21:01:40 UTC (rev 10202)
@@ -957,12 +957,13 @@
             id+=1
             #keeps a list of the temporary files created, so that they can be deleted at will
 
-    def get_pkdilly_var(self):
+    def get_pkdilly_var(self, out):
 
 #the ssh is to have a renewed and cleaned kerberos ticket
 #the +P is to have only the KRV* var, 
 #the +P don't need a condor_submit_file
 #ssh HOSTNAME pkdilly +P
+
         cmd="pkdilly -S "+self.condor_submit_file
         self.p = Popen( cmd, shell=True, stdout=PIPE, stderr=PIPE)
         self.p.wait()
@@ -1003,6 +1004,10 @@
                 get.append(line.strip()[:-1])
         get=[x for x in get if x.startswith("KRV")]
         get=[x for x in get if not x.startswith("KRVEXECUTE=")]
+        if out and len(lines)==0:
+            out.write("We didnt found kerberos ticket!")
+        if out:
+            out.write(lines)
         return get
 
     def renew_launch_file(self, renew_out_file,
@@ -1038,13 +1043,17 @@
                     out.flush()
                     launch_tmp_file=self.launch_file+".tmp"
                     fd=open(launch_tmp_file,'w')
-                    kerb_vars=self.make_kerb_script(fd,self.second_lauch_file)
+                    kerb_vars=self.make_kerb_script(fd, self.second_lauch_file, 3, out)
                     fd.close()
                     os.chmod(launch_tmp_file, 0755)
                     os.rename(launch_tmp_file, self.launch_file)
                     s=os.stat(self.launch_file)[os.path.stat.ST_SIZE]
-                    out.write(line_header()+
-                              "generated "+str(len(kerb_vars))+" kerberos variables. The file size is "+str(s)+"\n")
+                    if len(kerb_vars)>0:
+                        out.write(line_header()+
+                                  "generated "+str(len(kerb_vars))+" kerberos variables. The file size is "+str(s)+".\n")
+                    else:
+                        out.write("We have not been able to renew kerberos ticket! Their is 0 kerberos variables!")
+                        
                 out.flush()
                 #we do this as in some case(with dagman) the log file can 
                 #take a few second to be created. So we don't loop too fast
@@ -1058,19 +1067,27 @@
             
             os.system("pkboost +d "+str(pid))
 
-    def make_kerb_script(self, fd, second_lauch_file):
+    def make_kerb_script(self, fd, second_lauch_file, nb_try=3, out=None):
+        for i in range(nb_try):
+            ##we try 3 times to get the keys as sometimes this fail.                                                    
+            vars=self.get_pkdilly_var(out)
+            if len(vars)>0:
+                break
+        if len(vars)==0:
+            print "We didn't got any kerberos ticket after %d try! We don't redo the kerberos script."%(nb_try)
+            return vars
+
         fd.write(dedent('''\
                     #!/bin/sh
                     '''))
-        get=self.get_pkdilly_var()
             
-        for g in get:
+        for g in vars:
             fd.write("export "+g+"\n")
         fd.write(dedent('''
                 export KRVEXECUTE=%s
                 /usr/sbin/circus "$@"
                 '''%(os.path.abspath(second_lauch_file))))
-        return get
+        return vars
 
     def make_launch_script(self, bash_exec):
             
@@ -1107,8 +1124,9 @@
             if self.pkdilly:
                 self.second_lauch_file = self.launch_file+"2.sh"
                 kerb_vars=self.make_kerb_script(fd, self.second_lauch_file)
-                assert(len(kerb_vars)>0)
                 fd.close()
+                if len(kerb_vars)==0:
+                    DBIError("We didn't got kerberos ticket!")
 
                 fd = open(self.second_lauch_file,'w')
 
@@ -1382,7 +1400,7 @@
         else :
             self.req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
         if self.cpu>0:
-            self.req+='&&(target.CPUS=='+self.cpu+')'
+            self.req+='&&(target.CPUS=='+str(self.cpu)+')'
 
         if self.os:
             self.req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',



From nouiz at mail.berlios.de  Thu May 14 22:13:08 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 14 May 2009 22:13:08 +0200
Subject: [Plearn-commits] r10203 - trunk/plearn/vmat
Message-ID: <200905142013.n4EKD8hL027249@sheep.berlios.de>

Author: nouiz
Date: 2009-05-14 22:13:07 +0200 (Thu, 14 May 2009)
New Revision: 10203

Modified:
   trunk/plearn/vmat/CompactFileVMatrix.cc
   trunk/plearn/vmat/CompactFileVMatrix.h
Log:
added comment and cast to real instead of float.


Modified: trunk/plearn/vmat/CompactFileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.cc	2009-05-12 21:01:40 UTC (rev 10202)
+++ trunk/plearn/vmat/CompactFileVMatrix.cc	2009-05-14 20:13:07 UTC (rev 10203)
@@ -466,7 +466,7 @@
                 value_b >>= overflow;
                 remainder = 8 - nbits + remainder;
                 if (type == 'i')
-                    v[current_v] = (float)val / max;
+                    v[current_v] = (real)val / max;
                 else if (type == 'u')
                     v[current_v] = val;
                 else if (type == 'o' && val <= max)
@@ -475,7 +475,7 @@
             else { // the value is whole in the current byte
                 int val = (value_b & ((1<<nbits) - 1));
                 if (type == 'i')
-                    v[current_v] = (float)val / max;
+                    v[current_v] = (real)val / max;
                 else if (type == 'u')
                     v[current_v] = val;
                 else if (type == 'o' && val <= max) {

Modified: trunk/plearn/vmat/CompactFileVMatrix.h
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.h	2009-05-12 21:01:40 UTC (rev 10202)
+++ trunk/plearn/vmat/CompactFileVMatrix.h	2009-05-14 20:13:07 UTC (rev 10203)
@@ -73,9 +73,10 @@
         compact_length(-1)
     {}
 
-    char type; //!< type of data (i for int, o for onehot)
+    char type; //!< type of data (i for int, o for onehot, u for unsigned integer)
     int length; //!< number of fields
     int max; //!< maximal value of a field (entries will be normalized by that, there is no boundcheck)
+    //!we could change the type of bits_per_value if we want to minimize the size of GroupInfo
     int bits_per_value; //!< amount of bits used to encode each field (must be <= 8) (8 yields fastest conversion)
     bool active; //!< true if this field group is active
     int compact_length; //!< length of the group in the file



From nouiz at mail.berlios.de  Tue May 19 19:47:07 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 May 2009 19:47:07 +0200
Subject: [Plearn-commits] r10204 - trunk/python_modules/plearn/parallel
Message-ID: <200905191747.n4JHl6r1030588@sheep.berlios.de>

Author: nouiz
Date: 2009-05-19 19:47:06 +0200 (Tue, 19 May 2009)
New Revision: 10204

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
-bugfix the printing of a list.
-try to fix file don't exist error by the nfs server.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-05-14 20:13:07 UTC (rev 10203)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-05-19 17:47:06 UTC (rev 10204)
@@ -1007,7 +1007,7 @@
         if out and len(lines)==0:
             out.write("We didnt found kerberos ticket!")
         if out:
-            out.write(lines)
+            out.write(str(lines))
         return get
 
     def renew_launch_file(self, renew_out_file,
@@ -1141,6 +1141,9 @@
                     cd %s
                     '''%(os.path.abspath("."))))
                 if self.source_file:
+                    #we do the next line in hope to remove transiant error to
+                    #access this file by the nfs server.
+                    fd.write('[ -r "%s" ];echo "Can read the source file? " $? 1>&2 \n'%self.source_file)
                     fd.write('source ' + self.source_file + '\n')
 
                 fd.write(dedent('''\
@@ -1156,7 +1159,7 @@
                     pwd 1>&2
                     echo "nb args: $#" 1>&2
                     echo "Running: command: \\"$@\\"" 1>&2
-                    [ -x "$1" ];echo $?
+                    [ -x "$1" ];echo "Can execute the cmd? " $? 1>&2 
                     %s
                     ret=$?
                     rm -f echo ${KRB5CCNAME:5}



From nouiz at mail.berlios.de  Wed May 20 16:59:27 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 20 May 2009 16:59:27 +0200
Subject: [Plearn-commits] r10205 - trunk/python_modules/plearn/parallel
Message-ID: <200905201459.n4KExR0F013481@sheep.berlios.de>

Author: nouiz
Date: 2009-05-20 16:59:26 +0200 (Wed, 20 May 2009)
New Revision: 10205

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
set the new kerberos script file only if succed in creating a new one!


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-05-19 17:47:06 UTC (rev 10204)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-05-20 14:59:26 UTC (rev 10205)
@@ -1045,10 +1045,10 @@
                     fd=open(launch_tmp_file,'w')
                     kerb_vars=self.make_kerb_script(fd, self.second_lauch_file, 3, out)
                     fd.close()
-                    os.chmod(launch_tmp_file, 0755)
-                    os.rename(launch_tmp_file, self.launch_file)
-                    s=os.stat(self.launch_file)[os.path.stat.ST_SIZE]
                     if len(kerb_vars)>0:
+                        os.chmod(launch_tmp_file, 0755)
+                        os.rename(launch_tmp_file, self.launch_file)
+                        s=os.stat(self.launch_file)[os.path.stat.ST_SIZE]
                         out.write(line_header()+
                                   "generated "+str(len(kerb_vars))+" kerberos variables. The file size is "+str(s)+".\n")
                     else:



From tihocan at mail.berlios.de  Wed May 20 20:50:32 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 May 2009 20:50:32 +0200
Subject: [Plearn-commits] r10206 - trunk/scripts
Message-ID: <200905201850.n4KIoWNt008647@sheep.berlios.de>

Author: tihocan
Date: 2009-05-20 20:50:31 +0200 (Wed, 20 May 2009)
New Revision: 10206

Modified:
   trunk/scripts/appendresults
   trunk/scripts/makeresults
Log:
Fixed syntax for lockfile sleeptime option (just -60 instead of -n60)

Modified: trunk/scripts/appendresults
===================================================================
--- trunk/scripts/appendresults	2009-05-20 14:59:26 UTC (rev 10205)
+++ trunk/scripts/appendresults	2009-05-20 18:50:31 UTC (rev 10206)
@@ -31,9 +31,9 @@
 done
 
 # Wait until nobody else is messing with the output file.
-# we wait 60s in case the filesystem is too busy.
-# in that case retrying too often worsen the trouble.
-lockfile -n60 $OUTPUT.lock
+# We wait 60s in case the filesystem is too busy.
+# In that case retrying too often would make it worse.
+lockfile -60 $OUTPUT.lock
 
 if [ -d $DIR ];then
     echo -n "$PARAMS" >> $OUTPUT

Modified: trunk/scripts/makeresults
===================================================================
--- trunk/scripts/makeresults	2009-05-20 14:59:26 UTC (rev 10205)
+++ trunk/scripts/makeresults	2009-05-20 18:50:31 UTC (rev 10206)
@@ -25,9 +25,9 @@
 shift
 
 # Wait until nobody else is messing with the output file.
-# we wait 60s in case the filesystem is too busy.
-# in that case retrying too often worsen the trouble.
-lockfile -n60 $MATNAME.amat.lock
+# We wait 60s in case the filesystem is too busy.
+# In that case retrying too often would make it worse.
+lockfile -60 $MATNAME.amat.lock
 
 # Don't do anything if files already exist.
 if [ -f $MATNAME.amat ] && [ -f $MATNAME.vmat ]; then



From plearner at mail.berlios.de  Thu May 21 01:03:59 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 21 May 2009 01:03:59 +0200
Subject: [Plearn-commits] r10207 - in trunk: python_modules/plearn/plotting
	scripts/EXPERIMENTAL
Message-ID: <200905202303.n4KN3x99001408@sheep.berlios.de>

Author: plearner
Date: 2009-05-21 01:03:58 +0200 (Thu, 21 May 2009)
New Revision: 10207

Modified:
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
   trunk/scripts/EXPERIMENTAL/linearfilters.py
Log:
Small fixes to plotting utilities


Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2009-05-20 18:50:31 UTC (rev 10206)
+++ trunk/python_modules/plearn/plotting/netplot.py	2009-05-20 23:03:58 UTC (rev 10207)
@@ -511,7 +511,7 @@
         show()
 
     def draw(self):
-        print "Start plotting..."
+        # print "Start plotting..."
         clf()
         endidx = min(self.startidx+self.nrows*self.ncols, len(self.X))        
         title = self.figtitle+" ("+str(self.startidx)+" ... "+str(endidx-1)+")"
@@ -529,9 +529,9 @@
                          vmax = self.vmax,
                          transpose_img = self.transpose_img
                          )
-        print "Plotted,"
+        # print "Plotted,"
         draw()
-        print "Drawn."
+        # print "Drawn."
         
 
     def plotNext(self):

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-05-20 18:50:31 UTC (rev 10206)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-05-20 23:03:58 UTC (rev 10207)
@@ -1,6 +1,8 @@
 #!/usr/bin/env python
 
 import sys
+import matplotlib.pyplot as plt
+
 from pylab import *
 from plearn.io.server import *
 from plearn.pyplearn import *
@@ -15,6 +17,12 @@
 ### methods ###
 ################
 
+def myion():
+    pass
+
+def myioff():
+    pass
+
 def print_usage_and_exit():
     print "Usage :"  
     print "deepnetplot.py plotSingleMatrix x.psave "
@@ -502,10 +510,10 @@
 
                     
                     figure(3)
-                    ioff()
+                    myioff()
                     clf()                  
                     plotMatrices([matW,matM,produit], [nameW, nameM,'term-to-term product'])
-                    ion()
+                    myion()
                     draw()
 
 #                     if nameWr in listNames and nameMr in listNames:
@@ -525,10 +533,10 @@
 #                         produit2 = matWr2*matMr2
 
 #                         figure(4)
-#                         ioff()
+#                         myioff()
 #                         clf()
 #                         plotMatrices([matWr1, matMr1, matWr2, matMr2, produit, produit2], [nameWr + "-", nameMr + "-", nameWr + "+", nameMr + "+", 't.-t.-t. product (-)', 't.-t.-t. product (+)'])
-#                         ion()
+#                         myion()
 #                         draw()
 
                 #BIAS
@@ -568,10 +576,10 @@
                     print learner.getParameterValue(nameW).shape
                    
                     figure(3)
-                    ioff()                    
+                    myioff()                    
                     clf()
                     plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.05, self.from1568to784functions[self.from1568to784function], [], names, self.same_scale)
-                    ion()
+                    myion()
                     draw()
 
                 if nameW in listNames and nameM in listNames:
@@ -585,10 +593,10 @@
                         M[a] = m[y]*w[a]                    
 
                     figure(3)
-                    ioff()
+                    myioff()
                     clf()
                     plotLayer1(M, 28, .056,0,M.shape[0],.05, self.from1568to784functions[self.from1568to784function], [], names, self.same_scale)
-                    ion()
+                    myion()
                     draw()
 
             # like 'w' on the max of each row -- 'C'
@@ -625,16 +633,16 @@
                 if nameW in listNames and nameM not in listNames:
                    
                     figure(3)
-                    ioff()                    
+                    myioff()                    
                     clf()
                     plotLayer1(learner.getParameterValue(nameW), 28, .056, 0,0,.05, self.from1568to784functions[self.from1568to784function], indexes,names, self.same_scale)
-                    ion()
+                    myion()
                     draw()
                     
                 if nameW in listNames and nameM in listNames:
                     
                     figure(3)
-                    ioff()
+                    myioff()
                     clf()
 
                     w = learner.getParameterValue(nameW)
@@ -647,23 +655,23 @@
                     print M
                     
                     plotLayer1(M, 28, .056,0,M.shape[0],.05,self.from1568to784functions[self.from1568to784function],[],names, self.same_scale)
-                    ion()
+                    myion()
                     draw()
 
                     figure(4)
-                    ioff()
+                    myioff()
                     clf()                    
                     plotLayer1(M, 28, .056,0,M.shape[0],.05,self.from1568to784functions[self.from1568to784function],[],names, self.same_scale)
-                    ion()
+                    myion()
                     draw()
 
                 if nameWr in listNames and nameMr not in listNames:
 
                     figure(5)
-                    ioff()
+                    myioff()
                     clf()
                     plotLayer1(learner.getParameterValue(nameWr), 28, .056,0,0,.05,self.from1568to784functions[self.from1568to784function],indexes,names, self.same_scale)
-                    ion()
+                    myion()
                     draw()
                     
                     
@@ -757,14 +765,15 @@
 
     def __linkEvents(self):
 
-        figure(self.fig_rep)
+        fig = figure(self.fig_rep)
         
         connect('key_press_event', self.__changeChar)
         connect('button_press_event', self.__clicked)
         connect('key_press_event', self.__repCommands)
 
-        figure(self.fig_rec)
+        fig = figure(self.fig_rec)
         
+        # fig.canvas.mpl_connect('key_press_event', self.__changeChar)        
         connect('key_press_event', self.__changeChar)        
         #connect('button_press_event', self.__clicked)        
         
@@ -775,16 +784,16 @@
     def __plotReconstructions(self):
         print 'plotting reconstructions...'        
         figure(self.fig_rec)
-        ioff()
+        myioff()
         clf()
         plotMatrices(self.reconstructions)
-        ion()
+        myion()
         draw()
         print '...done.'
 
     def __plotRepresentations(self):
         print 'plotting representations...'
-        ioff()
+        myioff()
         figure(self.fig_rep)
         clf()
         temp = []
@@ -793,7 +802,7 @@
         #draw()
         self.rep_axes = plotMatrices(temp)        
         draw()
-        ion()
+        myion()
         print '...done.'
 
     def __computeAndPlot(self):        
@@ -937,7 +946,7 @@
 ### main ###
 ############
 
-server_command = "plearn_exp server"
+server_command = "myplearn server"
 serv = launch_plearn_server(command = server_command)
 
 #print "Press Enter to continue"
@@ -964,9 +973,9 @@
     matrices = learner.listParameter() 
     names = learner.listParameterNames()
     
-    #doToRow = None
-    doToRow = toMinusRow
+    doToRow = None
     #doToRow = toMinusRow
+    #doToRow = toMinusRow
 
     matrixName = ''
     while matrixName != 'exit':
@@ -975,16 +984,26 @@
         print names
 
         print
-        matrixName = raw_input('Choose a matrix to be plotted (or \'exit\')>>>')
+        matrixName = raw_input('Choose a matrix to be plotted (or \'exit\') >>> ')
         
         if matrixName in names:
 
+            #matrix = rand(500,28*28*2)
             matrix = learner.getParameterValue(matrixName)
-            #matrix = rand(500,28*28*2)
-            plotter = EachRowPlotter(matrix, 28, .1, .01, doToRow)
-            plotter.plot()
-            show()          
+            print "shape: ",matrix.shape
+
+            # guess image dimensions
+            n = matrix.shape[1]
+            imgheight = int(math.sqrt(n))+1
+            while n%imgheight != 0:
+                imgheight = imgheight-1
+            imgwidth = n/imgheight
             
+            showRowsAsImages(matrix, img_height=imgheight, img_width=imgwidth, nrows=5, ncols=7, figtitle=matrixName)
+            #plotter = EachRowPlotter(matrix, 28, .1, .01, doToRow)
+            #plotter.plot()
+            #show()          
+            
         elif matrixName != 'exit':
             print
             print 'This matrix does not exist !'
@@ -1004,7 +1023,7 @@
     vmat = openVMat(datafname)
     
     matrix_plot = InteractiveRepRecPlotter(learner, vmat)
-    
+
     show()
 
     

Modified: trunk/scripts/EXPERIMENTAL/linearfilters.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/linearfilters.py	2009-05-20 18:50:31 UTC (rev 10206)
+++ trunk/scripts/EXPERIMENTAL/linearfilters.py	2009-05-20 23:03:58 UTC (rev 10207)
@@ -9,21 +9,29 @@
 #server_command = 'plearn_exp server'
 #serv = launch_plearn_server(command = server_command)
 
-def computeAndShowFilters(datapmatfile, img_height, img_width, filtertype='PCA', lambd=1e-6, nu=0):
+def computeAndShowFilters(datapmatfile, img_height, img_width, filtertype='PCA', lambd=1e-6, nu=0, centered=True, displaytranspose=False):
     """
     Input is considered to be the first img_height x img_width columns of datapmatfile.
-    Filtertype can be 'PCA' or 'denoising'        
-    Covariance matrix will get lambd*I added to its diagonal, and its off-diagonal terms multiplied by (1-nu).
+    centered indicates whether we compte centered covariance (default) or uncentered covariance.
+    Covariance matrix will get lambd*I added to its diagonal, and its off-diagonal terms multiplied by (1-nu).    
+    Filtertype can be 'PCA' or 'denoising' or 'denoising_eig'.
+
+    Original version of linear denoising with zeroing noise (with probability of zeroing equal to nu)
+    is obtained for centered=False and lambda=0 
     """
     data = load_pmat_as_array(datapmatfile)
     inputs = data[:,0:img_height*img_width]
-    C = cov(inputs, rowvar=0, bias=1)
+    C = mycov(inputs, centered)
     if(filtertype=="PCA"):
-        filters = computePCAFilters(C, lambd, nu)
+        filters = computePCAFiltersFromCovariance(C, lambd, nu)
     elif(filtertype=="denoising"):
-        filters = computeDenoisingFilters(C, lambd, nu)
+        filters = computeDenoisingFiltersFromCovariance(C, lambd, nu)
+    elif(filtertype=="denoising_eig"):
+        filters = computeDenoisingEigenFiltersFromCovariance(C, lambd, nu)
     else:
-        raise ValueError("Invalid filtertype "+filtertype)    
+        raise ValueError("Invalid filtertype "+filtertype)
+    if displaytranspose:
+        filters = filters.T
     showRowsAsImages(filters, img_height, img_width, figtitle="Filters")
 
 def mycov(inputs, centered=True):
@@ -34,7 +42,7 @@
         C *= 1.0/len(inputs)        
     return C
 
-def computePCAFilters(C, lambd=1e-6, nu=0):
+def computePCAFiltersFromCovariance(C, lambd=1e-6, nu=0):
     C = C+diag(len(C)*[lambd])
     Cd = C.diagonal()
     C2 = C*(1.0-nu)
@@ -44,7 +52,7 @@
     eigvals, eigvecs = eig(C2)
     return real(eigvecs.T)
 
-def computeDenoisingFilters(C, lambd=1e-6, nu=0.10):
+def computeDenoisingFiltersFromCovariance(C, lambd=1e-6, nu=0.10):
     C = C+diag(len(C)*[lambd])
     Cd = C.diagonal()
     C2 = C*(1.0-nu)
@@ -52,8 +60,13 @@
     for i in range(len(Cd)):
         C2[i,i] = Cd[i]
     WW = dot(inv(C2),C)
-    return WW
+    return WW.T
 
+def computeDenoisingEigenFiltersFromCovariance(C, lambd=1e-6, nu=0.10):
+    WW = computeDenoisingFiltersFromCovariance(C, lambd, nu).T
+    eigvals, eigvecs = eig(WW)
+    return real(eigvecs.T)
+    # return real(inv(eigvecs).T)
 
 ####################
 ### main program ###



From dorionc at mail.berlios.de  Thu May 21 01:59:51 2009
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 21 May 2009 01:59:51 +0200
Subject: [Plearn-commits] r10208 - trunk/python_modules/plearn/parallel
Message-ID: <200905202359.n4KNxpas024210@sheep.berlios.de>

Author: dorionc
Date: 2009-05-21 01:59:51 +0200 (Thu, 21 May 2009)
New Revision: 10208

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
Fixed a bug that had been introduced a while ago


Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2009-05-20 23:03:58 UTC (rev 10207)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2009-05-20 23:59:51 UTC (rev 10208)
@@ -37,7 +37,7 @@
 #
 TASK_TYPE_MAP    = { 'apstat.com':            'SshTask',
                      'iro.umontreal.ca':      'ClusterTask',
-                     '## UNKNOWN DOMAINE ##': 'OnHostTask'
+                     '## UNKNOWN DOMAIN ##': 'OnHostTask'
                      }
 
 # Figure out if we are running on a 32bit or 64 bit machine.
@@ -66,7 +66,7 @@
                                             'fermi',   'plank',   'einstein'
                                             ],
 
-                     '## UNKNOWN DOMAINE ##': [ 'host' ]
+                     '## UNKNOWN DOMAIN ##': [ 'host' ]
                      }
 
 # To override the default of 1
@@ -86,8 +86,10 @@
 LOGDIR        = None  # May be set by set_logdir()
 try:
     DOMAIN_NAME = get_domain_name()
+    if DOMAIN_NAME not in SSH_MACHINES_MAP.keys():
+        DOMAIN_NAME = "## UNKNOWN DOMAIN ##"
 except Exception, e:
-    DOMAIN_NAME = "## UNKNOWN DOMAINE ##"
+    DOMAIN_NAME = "## UNKNOWN DOMAIN ##"
 
 # Configurables
 NICE          = 'nice'
@@ -357,7 +359,7 @@
             #print "Saved %f at %s (now %s)"%(loadavg, t, cur_t)
             if cur_t < t+LOADAVG_DELAY:
                 return loadavg
-        return cls.getMachineLoad(machine)
+        return cls.getMachineLoad(machine, command)
     getLoadAvg = classmethod(getLoadAvg)
 
     def getMachineLoad(cls, machine, command = lambda host: 'ssh -x %s cat /proc/loadavg' % host):



From laulysta at mail.berlios.de  Fri May 22 01:58:52 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 22 May 2009 01:58:52 +0200
Subject: [Plearn-commits] r10209 - trunk/plearn_learners_experimental
Message-ID: <200905212358.n4LNwqvR021589@sheep.berlios.de>

Author: laulysta
Date: 2009-05-22 01:58:52 +0200 (Fri, 22 May 2009)
New Revision: 10209

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
for artificial data


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-05-20 23:59:51 UTC (rev 10208)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-05-21 23:58:52 UTC (rev 10209)
@@ -773,7 +773,7 @@
 
                     // greedy phase hidden
                     if(hidden_reconstruction_lr!=0){
-                        setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
+                        setLearningRate( hidden_reconstruction_lr);
                         recurrentFprop(train_costs, train_n_items, true);
                         //recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
                         recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
@@ -797,7 +797,7 @@
                         if (noise)
                             encodeSequenceAndPopulateLists(seq, false);
                         //recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0,1, train_costs, train_n_items );
-                        recurrentUpdate(input_reconstruction_cost_weight, 0, 0, 0,1, train_costs, train_n_items );
+                        recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0,1, train_costs, train_n_items );
                     }
                     
                     
@@ -1422,8 +1422,18 @@
 }
 
 
-double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec theInput, Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec& reconstruction_bias2, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
-                                                                 Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec theInput, 
+                                                                      Vec hidden, 
+                                                                      Mat reconstruction_weights, 
+                                                                      Mat& acc_weights_gr, 
+                                                                      Vec& reconstruction_bias, 
+                                                                      Vec& reconstruction_bias2, 
+                                                                      Vec hidden_reconstruction_activation_grad, 
+                                                                      Vec& reconstruction_prob, 
+                                                                      Vec hidden_target, 
+                                                                      Vec hidden_gradient, 
+                                                                      double hidden_reconstruction_cost_weight, 
+                                                                      double lr)
 {
     // set appropriate sizes
     int fullhiddenlength = hidden_target.length();
@@ -1431,6 +1441,7 @@
     Vec hidden_input_noise;
     Vec hidden_fprop_noise;
     Vec hidden_act_no_bias;
+    Vec hidden_exp;
     Vec dynamic_act_no_bias_contribution;
     if(reconstruction_bias.length()==0)
     {
@@ -1448,22 +1459,22 @@
     hidden_fprop_noise.resize(fullhiddenlength);
     hidden_input_noise.resize(fullhiddenlength);
     hidden_act_no_bias.resize(fullhiddenlength);
+    hidden_exp.resize(fullhiddenlength);
     dynamic_act_no_bias_contribution.resize(fullhiddenlength);
 
-    //input_connections->fprop( theInput, hidden_act_no_bias);
+    input_connections->fprop( theInput, hidden_act_no_bias);
     hidden_input_noise << hidden_target;
     inject_zero_forcing_noise(hidden_input_noise, input_noise_prob);
     dynamic_connections->fprop(hidden_input_noise, dynamic_act_no_bias_contribution );
     hidden_act_no_bias += dynamic_act_no_bias_contribution;
-    //hidden_layer->fprop( hidden_act_no_bias, hidden_fprop_noise);
-    hidden_act_no_bias += reconstruction_bias2;
-    for( int j=0 ; j<fullhiddenlength ; j++ )
-        hidden_fprop_noise[j] = fastsigmoid(hidden_act_no_bias[j] );
+    hidden_layer->fprop( hidden_act_no_bias, hidden_exp);
+    //hidden_act_no_bias += reconstruction_bias2;
+    //for( int j=0 ; j<fullhiddenlength ; j++ )
+    //    hidden_fprop_noise[j] = fastsigmoid(hidden_act_no_bias[j] );
 
     // predict (denoised) input_reconstruction 
-    transposeProduct(reconstruction_activation, reconstruction_weights, hidden_fprop_noise); //dynamic matrice tied
-    //transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
-    //product(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice not tied
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden_exp); //dynamic matrice tied
+    //product(reconstruction_activation, reconstruction_weights, hidden_exp); //dynamic matrice not tied
     reconstruction_activation += reconstruction_bias;
 
     for( int j=0 ; j<fullhiddenlength ; j++ )
@@ -1484,11 +1495,11 @@
     //update bias
     multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
     // update weight
-    //externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
+    externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
     //externalProductScaleAcc(acc_weights_gr, hidden_reconstruction_activation_grad, hidden, -lr); //dynamic matrice not tied
                 
     //update bias2
-    multiplyAcc(reconstruction_bias2, hidden_gradient, -lr);
+    //multiplyAcc(reconstruction_bias2, hidden_gradient, -lr);
     /********************************************************************************/
     // Vec hidden_reconstruction_activation_grad;
     /*hidden_reconstruction_activation_grad.clear();
@@ -1815,7 +1826,18 @@
                     
                     //truc stan
                     //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                    train_costs[train_costs.length()-1] += fpropHiddenReconstructionFromLastHidden(input_list[i], hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_bias2, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                    train_costs[train_costs.length()-1] += fpropHiddenReconstructionFromLastHidden(input_list[i], 
+                                                                                                   hidden_list(i), 
+                                                                                                   dynamicWeights, //reconsWeights, //dynamicWeights, 
+                                                                                                   acc_dynamic_connections_gr, //acc_reconstruction_dynamic_connections_gr, //acc_dynamic_connections_gr, 
+                                                                                                   hidden_reconstruction_bias, 
+                                                                                                   hidden_reconstruction_bias2, 
+                                                                                                   hidden_reconstruction_activation_grad, 
+                                                                                                   hidden_reconstruction_prob, 
+                                                                                                   hidden_list(i-1), 
+                                                                                                   hidden_gradient, 
+                                                                                                   hidden_reconstruction_weight, 
+                                                                                                   current_learning_rate);
                     //fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconsWeights, acc_reconstruction_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
                     train_n_items[train_costs.length()-1]++;
                 }
@@ -1842,26 +1864,26 @@
                 
                 
                 //input
-                if(hidden_reconstruction_weight==0)
-                {
+                //if(hidden_reconstruction_weight==0)
+                //{
                    
                     
-                    bpropUpdateConnection(input_list[i],
-                                          hidden_act_no_bias_list(i), 
-                                          visi_bias_gradient, 
-                                          hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
-                                          inputWeights,
-                                          acc_input_connections_gr,
-                                          input_connections->down_size,
-                                          input_connections->up_size,
-                                          input_connections->learning_rate,
-                                          false,
-                                          true);
-                }
+                bpropUpdateConnection(input_list[i],
+                                      hidden_act_no_bias_list(i), 
+                                      visi_bias_gradient, 
+                                      hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                      inputWeights,
+                                      acc_input_connections_gr,
+                                      input_connections->down_size,
+                                      input_connections->up_size,
+                                      input_connections->learning_rate,
+                                      false,
+                                      true);
+                    //}
                 
                 //Dynamic
-                if(input_reconstruction_weight==0)
-                {
+                //if(input_reconstruction_weight==0)
+                //{
                     /*bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
                                        hidden_list(i),
                                        hidden_temporal_gradient, 
@@ -1869,18 +1891,18 @@
                                        hidden_layer->bias, 
                                        hidden_layer->learning_rate );*/
 
-                    bpropUpdateConnection(hidden_list(i-1),
-                                          hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
-                                          hidden_gradient, 
-                                          hidden_temporal_gradient, 
-                                          dynamicWeights,
-                                          acc_dynamic_connections_gr,
-                                          dynamic_connections->down_size,
-                                          dynamic_connections->up_size,
-                                          dynamic_connections->learning_rate,
-                                          false,
-                                          false);
-                }
+                bpropUpdateConnection(hidden_list(i-1),
+                                      hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
+                                      hidden_gradient, 
+                                      hidden_temporal_gradient, 
+                                      dynamicWeights,
+                                      acc_dynamic_connections_gr,
+                                      dynamic_connections->down_size,
+                                      dynamic_connections->up_size,
+                                      dynamic_connections->learning_rate,
+                                      false,
+                                      false);
+                    //}
                 
                 hidden_temporal_gradient << hidden_gradient; 
                 //if(hidden_reconstruction_weight!=0)
@@ -1922,7 +1944,7 @@
     if(dynamic_connections )
     {
         multiplyAcc(dynamicWeights, acc_dynamic_connections_gr, 1);
-        multiplyAcc(reconsWeights, acc_reconstruction_dynamic_connections_gr, 1);
+        //multiplyAcc(reconsWeights, acc_reconstruction_dynamic_connections_gr, 1);
     }
 }
 



From plearner at mail.berlios.de  Fri May 22 21:03:35 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 22 May 2009 21:03:35 +0200
Subject: [Plearn-commits] r10210 - in trunk: plearn/var
	python_modules/plearn/plotting python_modules/plearn/var
Message-ID: <200905221903.n4MJ3ZG3010222@sheep.berlios.de>

Author: plearner
Date: 2009-05-22 21:03:34 +0200 (Fri, 22 May 2009)
New Revision: 10210

Modified:
   trunk/plearn/var/SumSquareVariable.cc
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/var/Var.py
Log:
Minor additions


Modified: trunk/plearn/var/SumSquareVariable.cc
===================================================================
--- trunk/plearn/var/SumSquareVariable.cc	2009-05-21 23:58:52 UTC (rev 10209)
+++ trunk/plearn/var/SumSquareVariable.cc	2009-05-22 19:03:34 UTC (rev 10210)
@@ -51,8 +51,8 @@
 /** SumSquareVariable **/
 
 PLEARN_IMPLEMENT_OBJECT(SumSquareVariable,
-                        "ONE LINE DESCR",
-                        "NO HELP");
+                        "Computes the sum of the squares of the values of all elements of its input",
+                        "");
 
 SumSquareVariable::SumSquareVariable(Variable* input)
     : inherited(input, 1, 1) {}
@@ -63,9 +63,14 @@
 void SumSquareVariable::fprop()
 {
     int n=input->nelems();
-    *valuedata= 0;
+    const real* inputdata = input->valuedata;
+    real sumsq = 0;
     for(int i=0; i<n; i++)
-        *valuedata+= input->valuedata[i]*input->valuedata[i];
+    {
+        real v = inputdata[i];
+        sumsq += v*v;
+    }
+    *valuedata = sumsq;
 }
 
 

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2009-05-21 23:58:52 UTC (rev 10209)
+++ trunk/python_modules/plearn/plotting/netplot.py	2009-05-22 19:03:34 UTC (rev 10210)
@@ -183,8 +183,8 @@
     if len(inputs[0])>img_height*img_width:
         inputs = inputs[:,0:(img_height*img_width)]
         
+        print 'luminanca_scale_mode = ',luminance_scale_mode
     if vmin is None and luminance_scale_mode!=0:
-        print 'luminanca_scale_mode = ',luminance_scale_mode
         vmin = inputs.min()
         vmax = inputs.max()
         print 'filter value range: ',vmin,',',vmax

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2009-05-21 23:58:52 UTC (rev 10209)
+++ trunk/python_modules/plearn/var/Var.py	2009-05-22 19:03:34 UTC (rev 10210)
@@ -162,6 +162,9 @@
     def transposeDoubleProduct(self, W, M):
         return Var(pl.TransposedDoubleProductVariable(varray=[self.v, W, M]))
 
+    def sumsquare(self):
+        return Var(pl.SumSquareVariable(input=self.v))
+
     def square(self):
         return Var(pl.SquareVariable(input=self.v))
 



From plearner at mail.berlios.de  Sat May 23 06:11:36 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 23 May 2009 06:11:36 +0200
Subject: [Plearn-commits] r10211 - in trunk: commands/EXPERIMENTAL
	plearn_learners/unsupervised/EXPERIMENTAL scripts/EXPERIMENTAL
Message-ID: <200905230411.n4N4BavF025728@sheep.berlios.de>

Author: plearner
Date: 2009-05-23 06:11:33 +0200 (Sat, 23 May 2009)
New Revision: 10211

Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc
   trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h
   trunk/scripts/EXPERIMENTAL/dcaexperiment.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
experimental stuff


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2009-05-22 19:03:34 UTC (rev 10210)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2009-05-23 04:11:33 UTC (rev 10211)
@@ -373,6 +373,7 @@
 #include <plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.h>
 #include <plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.h>
 #include <plearn/var/EXPERIMENTAL/LogSoftSoftMaxVariable.h>
+#include <plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.h>
 #include <plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h>
 #include <plearn/var/SourceVariable.h>
 #include <plearn/var/ConcatColumnsVariable.h>
@@ -401,6 +402,8 @@
 // Stuff used for DiverseComponentAnalysis
 // #include <plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h>
 
+// Stuff used for DenoisingRecurrentNet
+#include <plearn_learners_experimental/DenoisingRecurrentNet.h>
 
 using namespace PLearn;
 

Modified: trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc
===================================================================
--- trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc	2009-05-22 19:03:34 UTC (rev 10210)
+++ trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc	2009-05-23 04:11:33 UTC (rev 10211)
@@ -77,6 +77,7 @@
     :ncomponents(2),
      nonlinearity("none"),
      cov_transformation_type("cov"),
+     diag_add(0.),
      diag_premul(1.0),
      offdiag_premul(1.0),
      diag_nonlinearity("square"),
@@ -113,7 +114,7 @@
 
     declareOption(
         ol, "nonlinearity", &DiverseComponentAnalysis::nonlinearity, OptionBase::buildoption,
-        "The nonlinearity to apply after linear transformation of the inpuits t ield the representation.");
+        "The nonlinearity to apply after linear transformation of the inputs to obtain the representation.");
 
     declareOption(
         ol, "force_zero_mean", &DiverseComponentAnalysis::force_zero_mean, OptionBase::buildoption,
@@ -150,6 +151,10 @@
         "        this is computed as sqrt((1-<u.v>^2) * <u,u>^2 * <v,v>^2) where <u,v> is given by the covariance matrix\n");
 
     declareOption(
+        ol, "diag_add", &DiverseComponentAnalysis::diag_add, OptionBase::buildoption,
+        "This value will be added to the diagonal (before premultiplying and applying non-linearity)");
+
+    declareOption(
         ol, "diag_premul", &DiverseComponentAnalysis::diag_premul, OptionBase::buildoption,
         "diagonal elements of Cy will be pre-multiplied by diag_premul (before applying non-linearity)");
 
@@ -352,9 +357,14 @@
             PLERROR("Invalid cov_transformation_type");
 
         if(diag_weight!=0)
-            L += diag_weight*sum(nonlinear_transform(diag(Cyt*diag_premul),diag_nonlinearity));
+        {
+            Var diagelems = diag(Cyt);
+            if(diag_add!=0)
+                diagelems = diagelems+diag_add;
+            L += diag_weight*sum(nonlinear_transform(diagelems*diag_premul,diag_nonlinearity));
+        }
         if(offdiag_weight!=0)
-            L += offdiag_weight*sum(nonlinear_transform(nondiag(Cyt*offdiag_premul),offdiag_nonlinearity));
+            L += offdiag_weight*sum(nonlinear_transform(nondiag(Cyt)*offdiag_premul,offdiag_nonlinearity));
             
         if(constrain_norm_type>0)
             L += L+constrain_norm_type*exp(sumsquare(W));

Modified: trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h
===================================================================
--- trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h	2009-05-22 19:03:34 UTC (rev 10210)
+++ trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h	2009-05-23 04:11:33 UTC (rev 10211)
@@ -72,6 +72,7 @@
 
     string cov_transformation_type;
 
+    double diag_add;
     double diag_premul;
     double offdiag_premul;
 

Modified: trunk/scripts/EXPERIMENTAL/dcaexperiment.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2009-05-22 19:03:34 UTC (rev 10210)
+++ trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2009-05-23 04:11:33 UTC (rev 10211)
@@ -36,7 +36,8 @@
                  nonlinearity="none",
                  constrain_norm_type=-1,
                  cov_transformation_type="cov",
-                 diag_weight = -1.0,
+                 diag_add = 0.,
+                 diag_weight = -1.0,                 
                  diag_nonlinearity="square", 
                  diag_premul = 1.0,
                  offdiag_weight=1.0,
@@ -100,6 +101,7 @@
             nonlinearity=nonlinearity,
             constrain_norm_type=constrain_norm_type,
             cov_transformation_type=cov_transformation_type,
+            diag_add = diag_add,
             diag_weight = diag_weight,
             diag_nonlinearity = diag_nonlinearity, 
             diag_premul = diag_premul,

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-05-22 19:03:34 UTC (rev 10210)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-05-23 04:11:33 UTC (rev 10211)
@@ -1,7 +1,7 @@
 #!/usr/bin/env python
 
 import sys
-import matplotlib.pyplot as plt
+# import matplotlib.pyplot as plt
 
 from pylab import *
 from plearn.io.server import *



From plearner at mail.berlios.de  Sat May 23 19:30:34 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 23 May 2009 19:30:34 +0200
Subject: [Plearn-commits] r10212 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200905231730.n4NHUYFX028802@sheep.berlios.de>

Author: plearner
Date: 2009-05-23 19:30:31 +0200 (Sat, 23 May 2009)
New Revision: 10212

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
Changed to save in plearn_binary format



Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2009-05-23 04:11:33 UTC (rev 10211)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2009-05-23 17:30:31 UTC (rev 10212)
@@ -374,7 +374,7 @@
             for(int k=0; k<nreconstructions; k++)
             {
                 trainHiddenLayer(k, dset);
-                PLearn::save(expdir/"learner.psave", *this);
+                PLearn::save(expdir/"learner.psave", *this, PStream::plearn_binary, false);
                 // 'if' is a hack to avoid precomputing last hidden layer if not needed
                 // if(k<nreconstructions-1 ||  must_train_supervised_layer) 
                 { 



From plearner at mail.berlios.de  Sat May 23 19:58:15 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 23 May 2009 19:58:15 +0200
Subject: [Plearn-commits] r10213 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200905231758.n4NHwF79017316@sheep.berlios.de>

Author: plearner
Date: 2009-05-23 19:58:14 +0200 (Sat, 23 May 2009)
New Revision: 10213

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
not saving large matrix uselessly


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2009-05-23 17:30:31 UTC (rev 10212)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2009-05-23 17:58:14 UTC (rev 10213)
@@ -376,7 +376,7 @@
                 trainHiddenLayer(k, dset);
                 PLearn::save(expdir/"learner.psave", *this, PStream::plearn_binary, false);
                 // 'if' is a hack to avoid precomputing last hidden layer if not needed
-                // if(k<nreconstructions-1 ||  must_train_supervised_layer) 
+                if(k<nreconstructions-1 ||  must_train_supervised_layer) 
                 { 
                     int width = layers[k+1].width();
                     outmat[k] = new FileVMatrix(outmatfname+tostring(k+1)+".pmat",0,width);



From plearner at mail.berlios.de  Sun May 24 01:30:56 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sun, 24 May 2009 01:30:56 +0200
Subject: [Plearn-commits] r10214 - trunk/python_modules/plearn/plotting
Message-ID: <200905232330.n4NNUuJp008407@sheep.berlios.de>

Author: plearner
Date: 2009-05-24 01:30:55 +0200 (Sun, 24 May 2009)
New Revision: 10214

Modified:
   trunk/python_modules/plearn/plotting/numpy_utils.py
Log:
minor fix for recognition of nan


Modified: trunk/python_modules/plearn/plotting/numpy_utils.py
===================================================================
--- trunk/python_modules/plearn/plotting/numpy_utils.py	2009-05-23 17:58:14 UTC (rev 10213)
+++ trunk/python_modules/plearn/plotting/numpy_utils.py	2009-05-23 23:30:55 UTC (rev 10214)
@@ -43,7 +43,7 @@
 threshold = 0
 
 def default_is_missing(x):
-    return x is None or x=='-' or (type(x) is str and x.strip()=='')
+    return x is None or x=='-' or x=='nan' or (type(x) is str and x.strip()=='')
     
 def to_numpy_float_array(values_list,
                          missing_value = ValueError("Found value interpreted as missing value and no missing_value was specified"),



From larocheh at mail.berlios.de  Tue May 26 16:32:41 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 26 May 2009 16:32:41 +0200
Subject: [Plearn-commits] r10215 - trunk/plearn_learners_experimental
Message-ID: <200905261432.n4QEWfKN000368@sheep.berlios.de>

Author: larocheh
Date: 2009-05-26 16:32:40 +0200 (Tue, 26 May 2009)
New Revision: 10215

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added support for weighted examples and semi-supervised learning.


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2009-05-23 23:30:55 UTC (rev 10214)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2009-05-26 14:32:40 UTC (rev 10215)
@@ -83,6 +83,7 @@
     pseudolikelihood_context_type( "uniform_random" ),
     k_most_correlated( -1 ),
     generative_learning_weight( 0 ),
+    semi_sup_learning_weight( 0. ),
     nll_cost_index( -1 ),
     log_Z_cost_index( -1 ),
     log_Z_ais_cost_index( -1 ),
@@ -296,6 +297,13 @@
                   "Weight of generative learning.\n"
                   );
 
+    declareOption(ol, "semi_sup_learning_weight", 
+                  &PseudolikelihoodRBM::semi_sup_learning_weight,
+                  OptionBase::buildoption,
+                  "Weight on unlabeled examples update during unsupervised learning.\n"
+                  "In other words, it's the same thing at generaitve_learning_weight,\n"
+                  "but for the unlabeled examples.\n");
+
     declareOption(ol, "input_layer", &PseudolikelihoodRBM::input_layer,
                   OptionBase::buildoption,
                   "The binomial input layer of the RBM.\n");
@@ -810,6 +818,7 @@
     int target_index;
     real weight; // unused
     real lr;
+    int weightsize = train_set->weightsize();
 
     //real mean_pseudolikelihood = 0;
 
@@ -878,9 +887,12 @@
         else 
             lr += denoising_learning_rate;
 
+        if( weightsize > 0 )
+            lr *= weight;
+
         setLearningRate(lr);
 
-        if( targetsize() == 1 )
+        if( targetsize() == 1 && !is_missing(target[0]) )
         {
             Vec target_act = target_layer->activation;
             Vec hidden_act = hidden_layer->activation;
@@ -1188,9 +1200,16 @@
 
             if( targetsize() > 0 )
                 lr *= generative_learning_weight;
+            
+            if( weightsize > 0 )
+                lr *= weight;
 
             setLearningRate(lr);
 
+            if( is_missing(target[0]) )
+                PLERROR("In PseudolikelihoodRBM::train(): generative training with "
+                        "unlabeled examples not supported for pseudolikehood training.");
+
             if( pseudolikelihood_context_size == 0 )
             {
                 // Compute input_probs
@@ -2871,6 +2890,11 @@
         {
             if( input_is_sparse )
             {
+                if( is_missing(target[0]) )
+                    PLERROR("In PseudolikelihoodRBM::train(): generative training with "
+                            "unlabeled examples not supported for CD training with "
+                            "sparse inputs.");
+
                 // Randomly select inputs
                 if( n_selected_inputs_cd > inputsize() ||
                     n_selected_inputs_cd <= 0 )
@@ -2921,6 +2945,9 @@
                 if( targetsize() > 0 )
                     lr *= generative_learning_weight;
 
+                if( weightsize > 0 )
+                    lr *= weight;
+
                 setLearningRate(lr);
 
                 // Positive phase
@@ -3067,12 +3094,50 @@
                     
                     lr *= (1-persistent_cd_weight);
 
+                    if( weightsize > 0 )
+                        lr *= weight;
+
                     setLearningRate(lr);
 
                     // Positive phase
                     pos_input = input;
-                    if( targetsize() > 0 )
+                    if( targetsize() > 0)
+                    {
+                        if( is_missing(target[0]) )
+                        {
+                            // Sample from p(y|x)
+                            lr *= semi_sup_learning_weight/generative_learning_weight;
+                            // Get output probabilities
+                            connection->setAsDownInput( input );
+                            hidden_layer->getAllActivations( 
+                                (RBMMatrixConnection*) connection );
+                            
+                            Vec target_act = target_layer->activation;
+                            Vec hidden_act = hidden_layer->activation;
+                            for( int i=0 ; i<target_layer->size ; i++ )
+                            {
+                                target_act[i] = target_layer->bias[i];
+                                // LATERAL CONNECTIONS CODE HERE!!
+                                real *w = &(target_connection->weights(0,i));
+                                // step from one row to the next in weights matrix
+                                int m = target_connection->weights.mod();                
+                                
+                                for( int j=0 ; j<hidden_layer->size ; j++, w+=m )
+                                {
+                                    // *w = weights(j,i)
+                                    hidden_activation_pos_i[j] = hidden_act[j] + *w;
+                                }
+                                target_act[i] -= hidden_layer->freeEnergyContribution(
+                                    hidden_activation_pos_i);
+                            }
+                            
+                            target_layer->expectation_is_up_to_date = false;
+                            target_layer->computeExpectation();
+                            target_layer->generateSample();
+                            target_one_hot << target_layer->sample;
+                        }
                         pos_target = target_one_hot;
+                    }
                     connection->setAsDownInput( input );
                     hidden_layer->getAllActivations( 
                         (RBMMatrixConnection*) connection );
@@ -3186,6 +3251,9 @@
 
                     lr *= persistent_cd_weight;
 
+                    if( weightsize > 0 )
+                        lr *= weight;
+
                     setLearningRate(lr);
 
                     int chain_i = stage % n_gibbs_chains;
@@ -3288,6 +3356,9 @@
             if( targetsize() > 0 )
                 lr *= generative_learning_weight;
 
+            if( weightsize > 0 )
+                lr *= weight;
+
             setLearningRate(lr);
             if( targetsize() > 0 )
                 PLERROR("In PseudolikelihoodRBM::train(): denoising "
@@ -3592,9 +3663,12 @@
 
     if( targetsize() == 1 )
     {
-        costs[class_cost_index] =
-            (argmax(output) == (int) round(target[0]))? 0 : 1;
-        costs[nll_cost_index] = -pl_log(output[(int) round(target[0])]);
+        if( !is_missing(target[0]) )
+        {
+            costs[class_cost_index] =
+                (argmax(output) == (int) round(target[0]))? 0 : 1;
+            costs[nll_cost_index] = -pl_log(output[(int) round(target[0])]);
+        }
     }
     else if( targetsize() > 1 )
     {

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2009-05-23 23:30:55 UTC (rev 10214)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2009-05-26 14:32:40 UTC (rev 10215)
@@ -182,6 +182,11 @@
     //! Weight of generative learning
     real generative_learning_weight;
 
+    //! Weight on unlabeled examples update during unsupervised learning.
+    //! In other words, it's the same thing at generaitve_learning_weight,
+    //! but for the unlabeled examples.
+    real semi_sup_learning_weight;
+
     //! The binomial input layer of the RBM
     PP<RBMLayer> input_layer;
 



From lamblin at mail.berlios.de  Tue May 26 21:35:22 2009
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 26 May 2009 21:35:22 +0200
Subject: [Plearn-commits] r10216 - trunk/plearn_learners/online
Message-ID: <200905261935.n4QJZMxV023143@sheep.berlios.de>

Author: lamblin
Date: 2009-05-26 21:35:22 +0200 (Tue, 26 May 2009)
New Revision: 10216

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
Log:
Fix build when fixed_std_deviation>0


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2009-05-26 14:32:40 UTC (rev 10215)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2009-05-26 19:35:22 UTC (rev 10216)
@@ -402,11 +402,12 @@
         needs_forget = true;
     }
 
-    if ( fixed_std_deviation > 0 && share_quad_coeff )
+    if (fixed_std_deviation > 0)
     {
-        if( share_quad_coeff )
+        if (share_quad_coeff)
             PLERROR("In RBMGaussianLayer::build_(): fixed_std_deviation should not "
                     "be > 0 when share_quad_coeff is true.");
+
         quad_coeff.fill( 1 / ( M_SQRT2 * fixed_std_deviation ) );
     }
 



From plearner at mail.berlios.de  Tue May 26 23:18:47 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 26 May 2009 23:18:47 +0200
Subject: [Plearn-commits] r10217 - trunk/python_modules/plearn/plotting
Message-ID: <200905262118.n4QLIllC004435@sheep.berlios.de>

Author: plearner
Date: 2009-05-26 23:18:47 +0200 (Tue, 26 May 2009)
New Revision: 10217

Modified:
   trunk/python_modules/plearn/plotting/numpy_utils.py
Log:
more gracefully handling case when MA package is missing


Modified: trunk/python_modules/plearn/plotting/numpy_utils.py
===================================================================
--- trunk/python_modules/plearn/plotting/numpy_utils.py	2009-05-26 19:35:22 UTC (rev 10216)
+++ trunk/python_modules/plearn/plotting/numpy_utils.py	2009-05-26 21:18:47 UTC (rev 10217)
@@ -36,7 +36,10 @@
 
 # from array import *
 import numpy
-import MA
+try:
+    import MA
+except ImportError:
+    print "WARNING: Import of MA (masked array) failed. Skipping this import." 
 import numpy.numarray as numarray
 from numpy.numarray import *
 



From larocheh at mail.berlios.de  Wed May 27 18:27:29 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 27 May 2009 18:27:29 +0200
Subject: [Plearn-commits] r10218 - trunk/plearn_learners_experimental
Message-ID: <200905271627.n4RGRTZM005125@sheep.berlios.de>

Author: larocheh
Date: 2009-05-27 18:27:28 +0200 (Wed, 27 May 2009)
New Revision: 10218

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added a sparsity option and corrected a bug in the sparse inputs case.


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2009-05-26 21:18:47 UTC (rev 10217)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2009-05-27 16:27:28 UTC (rev 10218)
@@ -83,6 +83,7 @@
     pseudolikelihood_context_type( "uniform_random" ),
     k_most_correlated( -1 ),
     generative_learning_weight( 0 ),
+    sparsity_bias_decay( 0 ),
     semi_sup_learning_weight( 0. ),
     nll_cost_index( -1 ),
     log_Z_cost_index( -1 ),
@@ -297,6 +298,13 @@
                   "Weight of generative learning.\n"
                   );
 
+    declareOption(ol, "sparsity_bias_decay", 
+                  &PseudolikelihoodRBM::sparsity_bias_decay,
+                  OptionBase::buildoption,
+                  "Constant to subtract (times the learning rate) to the hidden "
+                  "layer bias at each iteration.\n"
+                  );
+
     declareOption(ol, "semi_sup_learning_weight", 
                   &PseudolikelihoodRBM::semi_sup_learning_weight,
                   OptionBase::buildoption,
@@ -1190,6 +1198,13 @@
             PLERROR("NNNNNNNNNNOOOOOOOOOOOOOOOOOOOOOO!!!!!!!!!!!!!!");
         }
 
+        if( !fast_exact_is_equal(sparsity_bias_decay, 0.) )
+        {
+            Vec b = hidden_layer->bias;
+            for( int i=0 ; i<hidden_layer->size ; i++ )
+                b[i] -= lr * sparsity_bias_decay;
+        }
+
         if( !fast_exact_is_equal(learning_rate, 0.) &&
             (targetsize() == 0 || generative_learning_weight > 0) )
         {
@@ -3074,7 +3089,7 @@
                 hidden_activation_gradient *= -lr;
                 for( int i=0; i<extra.length(); i++ )
                 {
-                    if( input_is_selected[i] = true )
+                    if( input_is_selected[i] == true )
                         pos_input_sparse[(int)extra[i]] = 0;
                     else
                         V((int)extra[i]) += hidden_activation_gradient;

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2009-05-26 21:18:47 UTC (rev 10217)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2009-05-27 16:27:28 UTC (rev 10218)
@@ -182,6 +182,9 @@
     //! Weight of generative learning
     real generative_learning_weight;
 
+    //! Constant to subtract (times the learning rate) to the hidden layer bias at each iteration
+    real sparsity_bias_decay;
+
     //! Weight on unlabeled examples update during unsupervised learning.
     //! In other words, it's the same thing at generaitve_learning_weight,
     //! but for the unlabeled examples.



From tihocan at mail.berlios.de  Wed May 27 22:38:47 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 27 May 2009 22:38:47 +0200
Subject: [Plearn-commits] r10219 - trunk/plearn/base
Message-ID: <200905272038.n4RKclpd014288@sheep.berlios.de>

Author: tihocan
Date: 2009-05-27 22:38:46 +0200 (Wed, 27 May 2009)
New Revision: 10219

Modified:
   trunk/plearn/base/Storage.h
Log:
Fixed typo in error message

Modified: trunk/plearn/base/Storage.h
===================================================================
--- trunk/plearn/base/Storage.h	2009-05-27 16:27:28 UTC (rev 10218)
+++ trunk/plearn/base/Storage.h	2009-05-27 20:38:46 UTC (rev 10219)
@@ -149,7 +149,7 @@
         int l = length();
 #ifdef BOUNDCHECK
         if(l<0)
-            PLERROR("new Storage called with a length() <0; lenght = %d", l);
+            PLERROR("new Storage called with a length() < 0: length = %d", l);
 #endif
         if (l>0) 
         {



From tihocan at mail.berlios.de  Wed May 27 22:40:21 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 27 May 2009 22:40:21 +0200
Subject: [Plearn-commits] r10220 - in trunk: plearn/io plearn_learners/hyper
	plearn_learners/testers
Message-ID: <200905272040.n4RKeL9n014428@sheep.berlios.de>

Author: tihocan
Date: 2009-05-27 22:40:20 +0200 (Wed, 27 May 2009)
New Revision: 10220

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperLearner.h
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
Changed default value for option save_mode of PTester and HyperLearner: instead of being an empty string, it is now explicitely 'plearn_ascii', to avoid any confusion.


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2009-05-27 20:38:46 UTC (rev 10219)
+++ trunk/plearn/io/PStream.cc	2009-05-27 20:40:20 UTC (rev 10220)
@@ -242,10 +242,15 @@
      remote_plearn_comm(false)
 {}
 
-//! dtor.
+//////////////
+// ~PStream //
+//////////////
 PStream::~PStream()
 { }
 
+///////////////////////////
+// switchToPLearnOutMode //
+///////////////////////////
 PStream::mode_t PStream::switchToPLearnOutMode()
 {
     mode_t oldmode = outmode;
@@ -264,11 +269,17 @@
     return oldmode;
 }
 
+////////////////
+// parseModeT //
+////////////////
 PStream::mode_t PStream::parseModeT(const string& str_mode){
     PStream::mode_t mode;
-    if(str_mode.empty())
+    if(str_mode.empty()) {
+        PLDEPRECATED("The use of an empty string as PStream mode to default to"
+                " 'plearn_ascii' is deprecated, please use 'plearn_ascii' "
+                "directly");
         mode = PStream::plearn_ascii;
-    else if(str_mode=="plearn_ascii")
+    } else if(str_mode=="plearn_ascii")
         mode = PStream::plearn_ascii;
     else if(str_mode=="plearn_binary")
         mode = PStream::plearn_binary;

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2009-05-27 20:38:46 UTC (rev 10219)
+++ trunk/plearn/io/PStream.h	2009-05-27 20:40:20 UTC (rev 10220)
@@ -269,8 +269,7 @@
         return *this;
     }
 
-    //!parse a string and return the equivalent PStream::mode_t.
-    //!if empty string, we return plearn_ascii
+    //! Parse a string and return the equivalent PStream::mode_t.
     static PStream::mode_t parseModeT(const string& str);
 
 public:

Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-27 20:38:46 UTC (rev 10219)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2009-05-27 20:40:20 UTC (rev 10220)
@@ -76,6 +76,7 @@
 
 HyperLearner::HyperLearner()
     : save_mode_(PStream::plearn_ascii),
+      save_mode("plearn_ascii"),
       provide_strategy_expdir(true),
       save_final_learner(true),
       save_strategy_learner(false),
@@ -135,8 +136,9 @@
     declareOption(ol, "save_strategy_learner", &HyperLearner::save_strategy_learner, OptionBase::buildoption,
                   "should final learner be saved in expdir/Strat#final_learner.psave");
 
-    declareOption(ol, "save_mode", &HyperLearner::save_mode, OptionBase::buildoption,
-                  "The mode to use to save the file. Default plearn_ascii.");
+    declareOption(ol, "save_mode", &HyperLearner::save_mode,
+                  OptionBase::buildoption,
+                  "The mode to use to save the file.");
 
     declareOption(
         ol, "finalize_learner", &HyperLearner::finalize_learner,

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2009-05-27 20:38:46 UTC (rev 10219)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2009-05-27 20:40:20 UTC (rev 10220)
@@ -62,6 +62,7 @@
 
 protected:
 
+    PStream::mode_t save_mode_;
     static void declareOptions(OptionList &ol);
 
 public:
@@ -74,7 +75,6 @@
     TVec< PP<HyperCommand> > strategy; //!< The strategy to follow to optimize hyper parameters
 
     string save_mode;
-    PStream::mode_t save_mode_;
     bool provide_strategy_expdir; //!< should each strategy step be provided a directory expdir/Step#
     bool save_final_learner; //!< should final learner be saved in expdir/final_learner.psave
     bool save_strategy_learner; //!< should each Strat# learner be saved in expdir/Strat#final_learner.psave

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2009-05-27 20:38:46 UTC (rev 10219)
+++ trunk/plearn_learners/testers/PTester.cc	2009-05-27 20:40:20 UTC (rev 10220)
@@ -89,6 +89,7 @@
        reloaded(false),
        need_to_save_test_names(false),
        save_mode_(PStream::plearn_ascii),
+       save_mode("plearn_ascii"),
        provide_learner_expdir(false),
        report_stats(true),
        save_data_sets(false),
@@ -205,8 +206,9 @@
         "If true, the final trained learner for split#k will be saved in Split#k/final_learner.psave."
         "The format is defined by save_mode");
 
-    declareOption(ol, "save_mode", &PTester::save_mode, OptionBase::buildoption,
-                  "The mode to use to save the file. Default plearn_ascii.");
+    declareOption(
+        ol, "save_mode", &PTester::save_mode, OptionBase::buildoption,
+        "The mode to use to save the file.");
 
     declareOption(
         ol, "save_initial_learners", &PTester::save_initial_learners, OptionBase::buildoption,

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2009-05-27 20:38:46 UTC (rev 10219)
+++ trunk/plearn_learners/testers/PTester.h	2009-05-27 20:40:20 UTC (rev 10220)
@@ -73,6 +73,9 @@
     //! are provided with a training set).
     bool need_to_save_test_names;
 
+    //! Obtained automatically from the 'save_mode' option.
+    PStream::mode_t save_mode_;
+
 public:
 
     // ************************
@@ -88,7 +91,6 @@
     PP<VecStatsCollector> global_template_stats_collector;
     PP<PLearner> learner;
     string save_mode;
-    PStream::mode_t save_mode_;
 
     bool provide_learner_expdir;
     bool report_stats;



From nouiz at mail.berlios.de  Thu May 28 15:56:23 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 28 May 2009 15:56:23 +0200
Subject: [Plearn-commits] r10221 - trunk/python_modules/plearn/parallel
Message-ID: <200905281356.n4SDuNRa025434@sheep.berlios.de>

Author: nouiz
Date: 2009-05-28 15:56:23 +0200 (Thu, 28 May 2009)
New Revision: 10221

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
if condor_submit failed, we don't renew the kerberos ticket.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-05-27 20:40:20 UTC (rev 10220)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-05-28 13:56:23 UTC (rev 10221)
@@ -1021,6 +1021,19 @@
             out=open(renew_out_file,"w")
             out.write(line_header()+"will renew the lauch file "+self.launch_file+" each "+str(seconds)+"s\n")
             out.flush()
+            found=False
+            for i in range(5):
+                if os.path.isfile(self.log_file):
+                    found=True
+                    break
+                #we do this as in some case(with dagman) the log file can 
+                #take a few seconds to be created. So we let it enought time to create it.
+                time.sleep(15)
+            if not found:
+                out.write("Could not found the log file "+self.log_file+"."
+                          +"Probably that condor_submit failed.\n")
+                out.close()
+                sys.exit()
             while True:
                 p = Popen( cmd, shell=True, stdout=out, stderr=STDOUT)
                 ret = p.wait()
@@ -1056,9 +1069,9 @@
                         
                 out.flush()
                 #we do this as in some case(with dagman) the log file can 
-                #take a few second to be created. So we don't loop too fast
+                #take a few seconds to be created. So we don't loop too fast
                 #for no good reason.
-                time.sleep(5)
+                time.sleep(60)
             out.close()
             sys.exit()
         else:



From plearner at mail.berlios.de  Fri May 29 00:03:04 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 29 May 2009 00:03:04 +0200
Subject: [Plearn-commits] r10222 - in trunk: python_modules/plearn/plotting
	scripts scripts/EXPERIMENTAL
Message-ID: <200905282203.n4SM34mq027349@sheep.berlios.de>

Author: plearner
Date: 2009-05-29 00:03:03 +0200 (Fri, 29 May 2009)
New Revision: 10222

Modified:
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
   trunk/scripts/show_rows_as_images.py
Log:
Added support for any vmat through python extension bridge


Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2009-05-28 13:56:23 UTC (rev 10221)
+++ trunk/python_modules/plearn/plotting/netplot.py	2009-05-28 22:03:03 UTC (rev 10222)
@@ -183,7 +183,7 @@
     if len(inputs[0])>img_height*img_width:
         inputs = inputs[:,0:(img_height*img_width)]
         
-        print 'luminanca_scale_mode = ',luminance_scale_mode
+        print 'luminance_scale_mode = ',luminance_scale_mode
     if vmin is None and luminance_scale_mode!=0:
         vmin = inputs.min()
         vmax = inputs.max()

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-05-28 13:56:23 UTC (rev 10221)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-05-28 22:03:03 UTC (rev 10222)
@@ -999,7 +999,7 @@
                 imgheight = imgheight-1
             imgwidth = n/imgheight
             
-            showRowsAsImages(matrix, img_height=imgheight, img_width=imgwidth, nrows=5, ncols=7, figtitle=matrixName)
+            showRowsAsImages(matrix, img_height=imgheight, img_width=imgwidth, nrows=10, ncols=20, figtitle=matrixName)
             #plotter = EachRowPlotter(matrix, 28, .1, .01, doToRow)
             #plotter.plot()
             #show()          

Modified: trunk/scripts/show_rows_as_images.py
===================================================================
--- trunk/scripts/show_rows_as_images.py	2009-05-28 13:56:23 UTC (rev 10221)
+++ trunk/scripts/show_rows_as_images.py	2009-05-28 22:03:03 UTC (rev 10222)
@@ -34,16 +34,22 @@
 # Author: Pascal Vincent
 
 
-from plearn.vmat.PMat import PMat
 from plearn.plotting.netplot import showRowsAsImages
 
-
-def showPMatRowsAsImages(pmatfile, imgheight, imgwidth, nrows=5, ncols=7, figtitle=""):
-    """Will open a .pmat file and consider the beginning of each row a imgheight x imgwidth imagette.
+def show_rows_as_images(matfile, imgheight, imgwidth, nrows=10, ncols=10, figtitle=""):
+    """Will open a .pmat .dmat .amat or .vmat file and consider the beginning of each row a imgheight x imgwidth imagette.
     These images will be interactively displayed in a nrows x ncols grid of imagettes."""
-    data = PMat(pmatfile)
+    data = None
     if figtitle=="":
-        figtitle = pmatfile 
+        figtitle = matfile 
+    if matfile.endswith(".pmat"):
+        # Use pure python implementation of pmat (faster loading)
+        from plearn.vmat.PMat import PMat
+        data = PMat(matfile)
+    else:
+        # Use of VMat through the Python-bridge
+        from plearn.pyext import AutoVMatrix
+        data = AutoVMatrix(filename=matfile)
     showRowsAsImages(data, img_height=imgheight, img_width=imgwidth, nrows=nrows, ncols=ncols, figtitle=figtitle)
 
 ####################
@@ -51,5 +57,5 @@
 
 if __name__ == '__main__':
     from plearn.utilities.autoscript import autoscript
-    autoscript(showPMatRowsAsImages, True)
+    autoscript(show_rows_as_images, True)
 



From plearner at mail.berlios.de  Sat May 30 04:46:17 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 30 May 2009 04:46:17 +0200
Subject: [Plearn-commits] r10223 - trunk/python_modules/plearn/var
Message-ID: <200905300246.n4U2kHX0013021@sheep.berlios.de>

Author: plearner
Date: 2009-05-30 04:46:16 +0200 (Sat, 30 May 2009)
New Revision: 10223

Modified:
   trunk/python_modules/plearn/var/Var.py
Log:
Minor additions to Var.py


Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2009-05-28 22:03:03 UTC (rev 10222)
+++ trunk/python_modules/plearn/var/Var.py	2009-05-30 02:46:16 UTC (rev 10223)
@@ -41,6 +41,8 @@
         if isinstance(l, Var):
             self.v = l.v
         elif isinstance(l,int):
+            self.l = l
+            self.w = w
             if min_value == None and max_value == None:
                 self.v = pl.SourceVariable(build_length=l,
                                            build_width=w,
@@ -74,6 +76,9 @@
     def exp(self):
         return Var(pl.ExpVariable(input=self.v))
 
+    def log(self):
+        return Var(pl.LogVariable(input=self.v))
+
     def sigmoid(self):
         return Var(pl.SigmoidVariable(input=self.v))
 
@@ -186,8 +191,10 @@
     def __mul__(self, other):
         if type(other) in (int, float):
             return Var(pl.TimesConstantVariable(input=self.v, cst=other))
+        elif isinstance(other,Var) and other.l==1 and other.w==1:
+            return Var(pl.TimesScalarVariable(input1=self.v, input2=other.v))
         else:
-            raise NotImplementedError
+            raise NotImplementedError("type(other)=="+str(type(other)))
 
     def neg(self):
         return Var(pl.NegateElementsVariable(input=self.v))



From plearner at mail.berlios.de  Sat May 30 17:51:51 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 30 May 2009 17:51:51 +0200
Subject: [Plearn-commits] r10224 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200905301551.n4UFppUt029423@sheep.berlios.de>

Author: plearner
Date: 2009-05-30 17:51:50 +0200 (Sat, 30 May 2009)
New Revision: 10224

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
minor debug update

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2009-05-30 02:46:16 UTC (rev 10223)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2009-05-30 15:51:50 UTC (rev 10224)
@@ -737,6 +737,9 @@
 
         prev_mean = m;
 
+        // save_learner_after_each_pretraining_epoch
+        PLearn::save(expdir/"learner.psave", *this, PStream::plearn_binary, false);
+
         /*
         if(n==0)
         {



