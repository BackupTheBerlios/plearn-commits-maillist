<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7799 -	trunk/plearn_learners/distributions/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7799%20-%0A%09trunk/plearn_learners/distributions/EXPERIMENTAL&In-Reply-To=%3C200707191515.l6JFFn3f024917%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001246.html">
   <LINK REL="Next"  HREF="001248.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7799 -	trunk/plearn_learners/distributions/EXPERIMENTAL</H1>
    <B>lysiane at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7799%20-%0A%09trunk/plearn_learners/distributions/EXPERIMENTAL&In-Reply-To=%3C200707191515.l6JFFn3f024917%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7799 -	trunk/plearn_learners/distributions/EXPERIMENTAL">lysiane at mail.berlios.de
       </A><BR>
    <I>Thu Jul 19 17:15:49 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001246.html">[Plearn-commits] r7798 - in trunk/plearn: io python
</A></li>
        <LI>Next message: <A HREF="001248.html">[Plearn-commits] r7800 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1247">[ date ]</a>
              <a href="thread.html#1247">[ thread ]</a>
              <a href="subject.html#1247">[ subject ]</a>
              <a href="author.html#1247">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lysiane
Date: 2007-07-19 17:15:44 +0200 (Thu, 19 Jul 2007)
New Revision: 7799

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
Transformation Learner, PDistribution version, for digit experiences


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-18 22:30:31 UTC (rev 7798)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-19 15:15:44 UTC (rev 7799)
@@ -1,8 +1,7 @@
-// -*- C++ -*-
+    // -*- C++ -*-
 
 // TransformationLearner.cc
 //
-//version 5
 // Copyright (C) 2007 Lysiane Bouchard
 //
 // Redistribution and use in source and binary forms, with or without
@@ -40,51 +39,52 @@
 
 #include &quot;TransformationLearner.h&quot;
 
-//C++ 
-#include &lt;math.h&gt;
-
-
-//Plearn
-#include &lt;plearn/math/TMat_maths.h&gt;
-#include &lt;plearn/math/PRandom.h&gt;
-#include &lt;plearn/math/plapack.h&gt;
-
-
 namespace PLearn {
 using namespace std;
 
 PLEARN_IMPLEMENT_OBJECT(
     TransformationLearner,
-    &quot;ONE LINE DESCRIPTION&quot;,
-    &quot;MULTI-LINE \nHELP&quot;);
+    &quot;ONE LINE DESCR&quot;,
+    &quot;NO HELP&quot;
+);
 
-//TO TEST : OK
+//////////////////
+// TransformationLearner //
+//////////////////
 TransformationLearner::TransformationLearner():
-/* ### Initialize all fields to their default value here */
-    seed(1827),
-    transformFamily(TRANSFORM_FAMILY_LINEAR),
-    noiseVariance(1.0),
-    transformsVariance(0.5),
-    nbTransforms(5),
-    nbNeighbors(5),
-    epsilonInitWeight(0.01)
+    behavior(BEHAVIOR_LEARNER),
+    minimumProba(0.0001),
+    transformFamily(TRANSFORM_FAMILY_LINEAR_INCREMENT),
+    withBias(false),
+    learnNoiseVariance(false),
+    regOnNoiseVariance(false),
+    learnTransformDistribution(false),
+    regOnTransformDistribution(false),
+    initializationMode(INIT_MODE_DEFAULT),
+    largeEStepAPeriod(UNDEFINED),
+    largeEStepAOffset(UNDEFINED),
+    largeEStepBPeriod(UNDEFINED),
+    largeEStepBOffset(UNDEFINED),
+    noiseVariancePeriod(UNDEFINED),
+    noiseVarianceOffset(UNDEFINED),
+    noiseAlpha(NOISE_ALPHA_NO_REG),
+    noiseBeta(NOISE_BETA_NO_REG),
+    transformDistributionPeriod(UNDEFINED),
+    transformDistributionOffset(UNDEFINED),
+    transformDistributionAlpha(TRANSFORM_DISTRIBUTION_ALPHA_NO_REG),
+    transformsPeriod(1),
+    transformsOffset(0),
+    noiseVariance(UNDEFINED),
+    transformsVariance(1.0),
+    nbTransforms(2),
+    nbNeighbors(2)
 {
-    // ...
-    //pout &lt;&lt; &quot;TransformationLearner()&quot; &lt;&lt; endl;
-    random_gen = new PRandom();
-    
-
-    // ### You may (or not) want to call build_() to finish building the object
-    // ### (doing so assumes the parent classes' build_() have been called too
-    // ### in the parent classes' constructors, something that you must ensure)
-
-    // ### If this learner needs to generate random numbers, uncomment the
-    // ### line below to enable the use of the inherited PRandom object.
-    // random_gen = new PRandom();
+    pout &lt;&lt; &quot;hello\n&quot;;
 }
 
-
-//TO TEST
+////////////////////
+// declareOptions //
+////////////////////
 void TransformationLearner::declareOptions(OptionList&amp; ol)
 {
     // ### Declare all of this object's options here.
@@ -100,323 +100,577 @@
     //               OptionBase::buildoption,
     //               &quot;Help text describing this option&quot;);
     // ...
-    
-    declareOption(ol, 
-                  &quot;seed&quot;, 
-                  &amp;TransformationLearner::seed, 
+
+
+    //buildoption
+    pout &lt;&lt; &quot;declare options\n&quot;;
+
+    declareOption(ol,
+                  &quot;behavior&quot;,
+                  &amp;TransformationLearner::behavior,
                   OptionBase::buildoption,
-                  &quot;seed of the random generator&quot;);
-    declareOption(ol, 
-                  &quot;transformFamily&quot;, 
-                  &amp;TransformationLearner::transformFamily, 
+                  &quot;a transformationLearner might behave as a learner or as a generator&quot;);
+    declareOption(ol,
+                  &quot;minimumProba&quot;,
+                  &amp;TransformationLearner::minimumProba,
                   OptionBase::buildoption,
-                  &quot;transformation function Family&quot;);
-    declareOption(ol, 
-                  &quot;noiseVariance&quot;, 
-                  &amp;TransformationLearner::noiseVariance, 
+                  &quot;initial weight that will be needed sometimes&quot;);
+    declareOption(ol,
+                  &quot;transformFamily&quot;,
+                  &amp;TransformationLearner::transformFamily,
                   OptionBase::buildoption,
-                  &quot;variance on the noise r.v. (normaly distributed with mean 0)&quot;);
+                  &quot;global form of the transformation functions&quot;);
     declareOption(ol,
-                  &quot;transformsVariance&quot;, 
-                  &amp;TransformationLearner::transformsVariance, 
+                  &quot;withBias&quot;,
+                  &amp;TransformationLearner::withBias,
                   OptionBase::buildoption,
-                  &quot;variance on the transformation parameters r.vs&quot;
-                  &quot;(normaly distributed with mean 0)&quot;);
+                  &quot;yes/no: add a bias to the transformation function ?&quot;);
     declareOption(ol,
-                  &quot;nbTransforms&quot;, 
-                  &amp;TransformationLearner::nbTransforms, 
+                  &quot;learnNoiseVariance&quot;,
+                  &amp;TransformationLearner::learnNoiseVariance,
                   OptionBase::buildoption,
-                  &quot;number of transformations&quot;);
+                  &quot;the noise variance is ...fixed/learned ?&quot;);
     declareOption(ol,
-                  &quot;nbNeighbors&quot;, 
-                  &amp;TransformationLearner::nbNeighbors, 
+                  &quot;regOnNoiseVariance&quot;,
+                  &amp;TransformationLearner::regOnNoiseVariance,
                   OptionBase::buildoption,
-                  &quot;number of neighbors&quot;);
+                  &quot;yes/no: prior assumptions on the noise variance?&quot;);
     declareOption(ol,
-                  &quot;epsilonInitWeight&quot;, 
-                  &amp;TransformationLearner::epsilonInitWeight, 
+                  &quot;learnTransformDistribution&quot;,
+                  &amp;TransformationLearner::learnTransformDistribution,
                   OptionBase::buildoption,
-                  &quot;smallest amount of weight we can give to a choosen \n&quot;
-                  &quot;generation candidate at initialization of the \n &quot;
-                  &quot;generation set&quot;);
-
+                  &quot;the transformation distribution is ... fixed/learned ?&quot;);
     declareOption(ol,
-                  &quot;transformDistribution&quot;, 
-                  &amp;TransformationLearner::transformDistribution, 
+                  &quot;regOnTransformDistribution&quot;,
+                  &amp;TransformationLearner::regOnTransformDistribution,
                   OptionBase::buildoption,
-                  &quot;a multinomial distribution for the transformations\n&quot;
-                  &quot;i.e. p(kth transformation) = transformDistribution[k] \n &quot;);
-
+                  &quot;yes/no: prior assumptions on the transformation distribution ?&quot;);
+    
     declareOption(ol,
-                  &quot;inputSpaceDim&quot;, 
-                  &amp;TransformationLearner::inputSpaceDim, 
-                  OptionBase::learntoption,
-                  &quot;dimension of the training set input space&quot;);
+                  &quot;initializationMode&quot;,
+                  &amp;TransformationLearner::initializationMode,
+                  OptionBase::buildoption,
+                  &quot;how the initial values of the parameters to learn are choosen?&quot;);
+    
     declareOption(ol,
-                  &quot;nbGenerationCandidatesPerTarget&quot;, 
-                  &amp;TransformationLearner::nbGenerationCandidatesPerTarget, 
-                  OptionBase::learntoption,
-                  &quot;number of generation candidates per target&quot;); 
+                  &quot;largeEStepAPeriod&quot;,
+                  &amp;TransformationLearner::largeEStepAPeriod,
+                  OptionBase::buildoption,
+                  &quot;time interval between two updates of the reconstruction set\n&quot;
+                  &quot;(version A, method largeEStepA())&quot;);
     declareOption(ol,
-                  &quot;nbGenerationCandidates&quot;, 
-                  &amp;TransformationLearner::nbGenerationCandidates, 
-                  OptionBase::learntoption,
-                  &quot;number of generation candidates in the generation set&quot;);
+                  &quot;largeEStepAOffset&quot;,
+                  &amp;TransformationLearner::largeEStepAOffset,
+                  OptionBase::buildoption,
+                  &quot;time of the first update of the reconstruction set&quot;
+                  &quot;(version A, method largeEStepA())&quot;);
     declareOption(ol,
-                  &quot;nbTrainingInput&quot;, 
-                  &amp;TransformationLearner::nbTrainingInput, 
-                  OptionBase::learntoption,
-                  &quot;number of input given in the training set&quot;);  
+                  &quot;largeEStepBPeriod&quot;,
+                  &amp;TransformationLearner::largeEStepBPeriod,
+                  OptionBase::buildoption,
+                  &quot;time interval between two updates of the reconstruction set\n&quot;
+                  &quot;(version  B, method largeEStepB())&quot;); 
     declareOption(ol,
-                  &quot;trainsformsSet&quot;, 
-                  &amp;TransformationLearner::transformsSet, 
-                  OptionBase::learntoption,
-                  &quot;set of transformations&quot;);
+                  &quot;noiseVariancePeriod&quot;,
+                  &amp;TransformationLearner::noiseVariancePeriod,
+                  OptionBase::buildoption,
+                  &quot;time interval between two updates of the noise variance&quot;);
     declareOption(ol,
-                  &quot;transforms&quot;, 
-                  &amp;TransformationLearner::transforms, 
-                  OptionBase::learntoption,
-                  &quot;views on the transformation set&quot;);
-    
+                  &quot;noiseVarianceOffset&quot;,
+                  &amp;TransformationLearner::noiseVarianceOffset,
+                  OptionBase::buildoption,
+                  &quot;time of the first update of the noise variance&quot;);
     declareOption(ol,
-                  &quot;generationSet&quot;, 
-                  &amp;TransformationLearner::generationSet, 
-                  OptionBase::learntoption,
-                  &quot;set of generation candidates&quot;
-                  &quot;i.e. triples (target, neighbor, transformation)&quot;);
-    declareOption(ol,
-                  &quot;lambda&quot;, 
-                  &amp;TransformationLearner::lambda, 
-                  OptionBase::learntoption,
-                  &quot;weight decay&quot;);
+                  &quot;noiseAlpha&quot;,
+                  &amp;TransformationLearner::noiseAlpha,
+                  OptionBase::buildoption,
+                  &quot;parameter of the prior distribution of the noise variance&quot;);
+   declareOption(ol,
+                 &quot;noiseBeta&quot;,
+                 &amp;TransformationLearner::noiseBeta,
+                 OptionBase::buildoption,
+                 &quot;parameter of the prior distribution of the noise variance&quot;);
+   declareOption(ol,
+                 &quot;transformDistributionPeriod&quot;,
+                 &amp;TransformationLearner::transformDistributionPeriod,
+                 OptionBase::buildoption,
+                 &quot;time interval between two updates of the transformation distribution&quot;);
+   declareOption(ol, 
+                 &quot;transformDistributionOffset&quot;,
+                 &amp;TransformationLearner::transformDistributionOffset,
+                 OptionBase::buildoption,
+                 &quot;time of the first update of the transformation distribution&quot;);
+   declareOption(ol, 
+                 &quot;transformDistributionAlpha&quot;,
+                 &amp;TransformationLearner::transformDistributionAlpha,
+                 OptionBase::buildoption,
+                 &quot;parameter of the prior distribution of the transformation distribution&quot;);
+   declareOption(ol,
+                 &quot;transformsPeriod&quot;,
+                 &amp;TransformationLearner::transformsPeriod,
+                 OptionBase::buildoption,
+                 &quot;time interval between two updates of the transformations parameters&quot;);
+   declareOption(ol,
+                 &quot;transformsOffset&quot;,
+                 &amp;TransformationLearner::transformsOffset,
+                 OptionBase::buildoption,
+                 &quot;time of the first update of the transformations parameters&quot;);
 
-    declareOption(ol,
-                  &quot;noiseVarianceFactor&quot;, 
-                  &amp;TransformationLearner::noiseVarianceFactor, 
-                  OptionBase::learntoption,
-                  &quot;factor used in computation of generation weights&quot;
-                  &quot; 1/(2*noise variance)&quot;);
+   declareOption(ol, 
+                 &quot;noiseVariance&quot;,
+                 &amp;TransformationLearner::noiseVariance,
+                 OptionBase::buildoption,
+                 &quot;noise variance (noise = random variable normally distributed)&quot;);
+   declareOption(ol, 
+                 &quot;transformsVariance&quot;,
+                 &amp;TransformationLearner::transformsVariance,
+                 OptionBase::buildoption,
+                 &quot;variance on the transformation parameters (normally distributed)&quot;);
+   declareOption(ol, 
+                 &quot;nbTransforms&quot;,
+                 &amp;TransformationLearner::nbTransforms,
+                 OptionBase::buildoption,
+                 &quot;how many transformations?&quot;);
+   declareOption(ol, 
+                 &quot;nbNeighbors&quot;,
+                 &amp;TransformationLearner::nbNeighbors,
+                 OptionBase::buildoption,
+                 &quot;how many neighbors?&quot;);
+   declareOption(ol, 
+                 &quot;transformDistribution&quot;,
+                 &amp;TransformationLearner::transformDistribution,
+                 OptionBase::buildoption,
+                 &quot;transformation distribution&quot;);
+   
+   //learntoption
+   declareOption(ol,
+                 &quot;transformsSet&quot;,
+                 &amp;TransformationLearner::transformsSet,
+                 OptionBase::learntoption,
+                 &quot;set of transformations \n)&quot;
+                 &quot;implemented as a mdXd matrix,\n&quot;
+                 &quot;     where m is the number of transformations\n&quot;
+                 &quot;           and d is dimensionality of the input space&quot;);
+   declareOption(ol,
+                 &quot;transforms&quot;,
+                 &amp;TransformationLearner::transforms,
+                 OptionBase::learntoption,
+                 &quot;set of transformations\n&quot;
+                 &quot;vector form of the previous set:\n)&quot;
+                 &quot;    kth element of the vector = view on the kth sub-matrix&quot;);
+   declareOption(ol,
+                 &quot;biasSet&quot;,
+                 &amp;TransformationLearner::biasSet,
+                 OptionBase::learntoption,
+                 &quot;set of bias (one by transformation)&quot;);
+   declareOption(ol,
+                 &quot;inputSpaceDim&quot;,
+                 &amp;TransformationLearner::inputSpaceDim,
+                 OptionBase::learntoption,
+                 &quot;dimensionality of the input space&quot;);
+   declareOption(ol,
+                 &quot;nbTargetReconstructions&quot;,
+                 &amp;TransformationLearner::nbTargetReconstructions,
+                 OptionBase::learntoption,
+                 &quot;number of reconstructions of the same target&quot;);
+   declareOption(ol,
+                 &quot;nbReconstructions&quot;,
+                 &amp;TransformationLearner::nbReconstructions,
+                 OptionBase::learntoption,
+                 &quot;total number of reconstructions&quot;);
+   declareOption(ol,
+                 &quot;trainingSetLength&quot;,
+                 &amp;TransformationLearner::trainingSetLength,
+                 OptionBase::learntoption,
+                 &quot;number of samples in the training&quot; );
+   declareOption(ol,
+                 &quot;transformsSD&quot;,
+                 &amp;TransformationLearner::transformsSD,
+                 OptionBase::learntoption,
+                 &quot;standard deviation of the transformations parameters&quot;);
+   declareOption(ol,
+                 &quot;targetReconstructionSet&quot;,
+                 &amp;TransformationLearner::targetReconstructionSet,
+                 OptionBase::learntoption,
+                 &quot;will be used to store a view on the reconstructions of a same target&quot;);
+   declareOption(ol,
+                 &quot;B_C&quot;,
+                 &amp;TransformationLearner::B_C,
+                 OptionBase::learntoption,
+                 &quot;storage space needed in the maximization step (to update transformations parameters)\n&quot;
+                 &quot; - 2mdXd matrix, m=number of transformations, d = dimensionality of the input space&quot;);
+   declareOption(ol,
+                 &quot;B&quot;,
+                 &amp;TransformationLearner::B,
+                 OptionBase::learntoption,
+                 &quot;views on m first dxd submatrices of B_C \n&quot;
+                 &quot;(vector form)&quot;);
+   declareOption(ol,
+                 &quot;C&quot;,
+                 &amp;TransformationLearner::C,
+                 OptionBase::learntoption,
+                 &quot;views on m last dxd submatrices of B_C \n&quot;
+                 &quot;(vector form)&quot;);
+   declareOption(ol,
+                 &quot;target&quot;,
+                 &amp;TransformationLearner::target,
+                 OptionBase::learntoption,
+                 &quot;to store a view on a training sample&quot;);
+   declareOption(ol,
+                 &quot;neighbor&quot;,
+                 &amp;TransformationLearner::neighbor,
+                 OptionBase::learntoption,
+                 &quot;to store a view on a training sample&quot;);
 
-    declareOption(ol,
-                  &quot;noiseStDev&quot;, 
-                  &amp;TransformationLearner::noiseStDev, 
-                  OptionBase::learntoption,
-                  &quot;standard deviation on noise distribution&quot;);
-    declareOption(ol,
-                  &quot;transformsStDev&quot;, 
-                  &amp;TransformationLearner::transformsStDev, 
-                  OptionBase::learntoption,
-                  &quot;standard deviation on transformation parameters&quot;);
+   // Now call the parent class' declareOptions().
+   inherited::declareOptions(ol);
+}
 
-    // Now call the parent class' declareOptions
-    
+void TransformationLearner::declareMethods(RemoteMethodMap&amp; rmm){
 
 
-    inherited::declareOptions(ol);
-}
-
-
-//TO TEST
-void TransformationLearner::declareMethods(RemoteMethodMap&amp; rmm)
-{
-    declareMethod(rmm, &quot;largeEStepA&quot;, &amp;TransformationLearner::largeEStepA,
-                  (BodyDoc(&quot;Performs a large update of the generation set (expectation step)&quot;  
-                           &quot;For each target, we take the best generation candidates among all the possibilities \n&quot;)));
+    pout &lt;&lt; &quot;declare methods\n&quot;;
+    rmm.inherited(inherited::_getRemoteMethodMap_());
     
-    declareMethod(rmm, &quot;initEStep&quot;, &amp;TransformationLearner::initEStep,
-                  (BodyDoc(&quot;Initialization of the generation set (expectation step)\n&quot; 
-                           &quot;For each possible couple (target,neighbor), we take the best transformations to form \n&quot; 
-                           &quot;the generation candidates&quot;)));
+    declareMethod(rmm, 
+                  &quot;initTransformsParameters&quot;,
+                  &amp;TransformationLearner::initTransformsParameters,
+                  (BodyDoc(&quot;initializes the transformation parameters randomly \n&quot;
+                           &quot;  (all parameters are a priori independent and normally distributed)&quot;)));
+   
+    declareMethod(rmm, 
+                  &quot;setTransformsParameters&quot;,
+                  &amp;TransformationLearner::setTransformsParameters,
+                  (BodyDoc(&quot;initializes the transformation parameters with the given values&quot;),
+                   ArgDoc(&quot;TVec&lt;Mat&gt; transforms&quot;, &quot;initial transformation matrices&quot;),
+                   ArgDoc(&quot;Mat  biasSet&quot;,&quot;initial bias (one by transformation) (optional)&quot;)));
+    declareMethod(rmm, 
+                  &quot;initNoiseVariance&quot;,
+                  &amp;TransformationLearner::initNoiseVariance,
+                  (BodyDoc(&quot;initializes the noise variance randomly (gamma distribution)&quot;)));
+    declareMethod(rmm, 
+                  &quot;setNoiseVariance&quot;,
+                  &amp;TransformationLearner::setNoiseVariance,
+                  (BodyDoc(&quot;initializes the noise variance to the given value&quot;),
+                   ArgDoc(&quot;real nv&quot;,&quot;noise variance&quot;)));
+    declareMethod(rmm, 
+                  &quot;initTransformDistribution&quot;,
+                  &amp;TransformationLearner::initTransformDistribution,
+                  (BodyDoc(&quot;initializes the transformation distribution randomly \n&quot;
+                           &quot;-we use a dirichlet distribution \n&quot;
+                           &quot;-we store log-probabilities instead probabilities&quot;)));
+    declareMethod(rmm, 
+                  &quot;setTransformDistribution&quot;,
+                  &amp;TransformationLearner::setTransformDistribution,
+                  (BodyDoc(&quot;initializes the transformation distribution with the given values \n&quot;
+                           &quot; -the given values might represent log-probabilities&quot;),
+                   ArgDoc(&quot;Vec td&quot;,&quot;initial values of the transformation distribution&quot;)));
     
-    declareMethod(rmm, &quot;smallEStep&quot;,&amp;TransformationLearner::smallEStep,
-                  (BodyDoc(&quot;Update of the generation set (expectation step) \n&quot;
-                           &quot;we update the weights of the generation candidates while keeping them fixed&quot;)));
-    
-    declareMethod(rmm, &quot;MStep&quot;, &amp;TransformationLearner::MStep,
-                  (BodyDoc(&quot;Updating the transformation parameters (maximization step)\n&quot;)));
+    declareMethod(rmm,
+                  &quot;returnPredictedFrom&quot;,
+                  &amp;TransformationLearner::returnPredictedFrom,
+                  (BodyDoc(&quot;generates a sample data point from a source data point and returns it \n&quot;
+                           &quot; - a specific transformation is used&quot;),
+                   ArgDoc(&quot;const Vec source&quot;,&quot;source data point&quot;),
+                   ArgDoc(&quot;int transformIdx&quot;,&quot;index of the transformation (optional)&quot;),
+                   RetDoc(&quot;Vec&quot;)));
 
-    declareMethod(rmm, &quot;largeEStepB&quot;, &amp;TransformationLearner::largeEStepB,
-                  (BodyDoc(&quot;Performs a large update of the&quot;)));
- 
-    declareMethod(rmm, &quot;returnReproductionSources&quot;, &amp;TransformationLearner::returnReproductionSources,
-                  (BodyDoc(&quot;Returns the generation candidates associated to a specific target &quot;),
-                   ArgDoc (&quot;targetIdx&quot;, &quot;Index of the target data point in the training set&quot;),
-                   RetDoc (&quot;A vector of tuples (target index, neighbor index, transformation index, weight )&quot;)));
-
-    declareMethod(rmm, &quot;returnReproductions&quot;, &amp;TransformationLearner::returnReproductions,
-                  (BodyDoc(&quot;Computes the reproductions of the target from his generation candidates &quot;),
-                   ArgDoc(&quot;targetIdx&quot;,&quot;Index of the target data point in the training set&quot;),
-                   RetDoc(&quot;A matrix of data points (reproductions of the target)&quot;)));
-
-    declareMethod(rmm, &quot;returnTransform&quot;, &amp;TransformationLearner::returnTransform,
-                  (BodyDoc(&quot;Returns the parameters of a transformation&quot;),
-                   ArgDoc(&quot;transformIdx&quot;,&quot; Index of the transformation&quot;),
-                   RetDoc(&quot;a dXd matrix, d = dimension of input space&quot;)));
-    declareMethod(rmm, &quot;returnAllTransforms&quot;,&amp;TransformationLearner::returnAllTransforms,
-                  (BodyDoc(&quot;Returns all the transformation parameters&quot;),
-                   RetDoc(&quot;a kdXd matrix,  k = nb transformations \n&quot; 
-                          &quot;                d = dimension of input space&quot;)));
-    declareMethod(rmm, 
-                  &quot;returnGeneratedSamplesFrom&quot;, 
+    declareMethod(rmm,
+                  &quot;returnGeneratedSamplesFrom&quot;,
                   &amp;TransformationLearner::returnGeneratedSamplesFrom,
-                  (BodyDoc(&quot;returns samples data point generated from\n&quot;
-                           &quot;a center data point&quot;),
-                   ArgDoc(&quot;Vec center, int n&quot;,&quot; center data point&quot;),
-                   ArgDoc(&quot;int n&quot;, &quot; number of sample data points to generate&quot;),
-                   RetDoc(&quot;a nXd matrix, the center is included in the dataset&quot;)));
+                  (BodyDoc(&quot;generates samples data points form a source data point and return them \n&quot;
+                           &quot;    -we use a specific transformation&quot;),
+                   ArgDoc(&quot;Vec source&quot;,&quot;source data point&quot;),
+                   ArgDoc(&quot;int n&quot;,&quot;number of samples&quot;),
+                   ArgDoc(&quot;int transformIdx&quot;, &quot;index of the transformation (optional)&quot;),
+                   RetDoc(&quot;nXd matrix (one row = one sample)&quot;)));
     declareMethod(rmm,
-                  &quot;returnGeneratedDataSet&quot;,
-                  &amp;TransformationLearner::returnGeneratedDataSet,
-                  (BodyDoc(&quot;returns a data set with respect to the current\n&quot;
-                           &quot;distribution paramaters\n&quot;
-                           &quot;We use a tree generation process (see createDataSet)\n&quot;
-                           &quot;i.e.: each new data point is used to generate a fix number of data points &quot;),
-                   ArgDoc(&quot;Vec root&quot;,&quot;initial data point&quot;),
-                   ArgDoc(&quot;int nbGenerations&quot;,&quot;deepness of the tree&quot;),
-                   ArgDoc(&quot;int GenerationLen&quot;,&quot;number of child for interior nodes&quot;),
-                   RetDoc(&quot;a nXd matrix, where n = number of nodes in the tree (root = part of the dataSet)&quot;)));
-    
+                  &quot;pickTransformIdx&quot;,
+                  &amp;TransformationLearner::pickTransformIdx,
+                  (BodyDoc(&quot;select a transformation ramdomly&quot;),
+                   RetDoc(&quot;int (index of the choosen transformation)&quot;)));
+               
     declareMethod(rmm,
-                  &quot;returnSequentiallyGeneratedDataSet&quot;,
-                  &amp;TransformationLearner::returnSequentiallyGeneratedDataSet,
-                  (BodyDoc(&quot;returns a data set with respect to the current\n&quot;
-                           &quot;distribution parameters\n&quot;
-                           &quot;We use a sequential generation process\n&quot;
-                           &quot;i.e: each new data point is used to generate the next data point&quot;),
-                   ArgDoc(&quot;Vec root&quot;,&quot;initial data point&quot;),
-                   ArgDoc(&quot;int n&quot;,&quot;number of data points to generate&quot;),  
-                   RetDoc(&quot;a nXd matrix (the root is included in the data set)&quot;)));
-    
-    inherited::declareMethods(rmm);
+                  &quot;pickNeighborIdx&quot;,
+                  &amp;TransformationLearner::pickNeighborIdx,
+                  (BodyDoc(&quot;select a neighbor among the data points in the training set&quot;),
+                   RetDoc(&quot;int (index of the data point in the training set)&quot;)));
+    declareMethod(rmm,
+                  &quot;returnTreeDataSet&quot;,
+                  &amp;TransformationLearner::returnTreeDataSet,
+                  (BodyDoc(&quot;creates and returns a data set using a 'tree generation process'\n&quot;
+                           &quot; see 'treeDataSet()' implantation for more details&quot;),
+                   ArgDoc(&quot;Vec root&quot;,&quot;data point from which all the other data points will derive (directly or indirectly)&quot;),
+                   ArgDoc(&quot;int deepness&quot;,&quot;deepness of the tree reprenting the samples created&quot;),
+                   ArgDoc(&quot;int branchingFactor&quot;,&quot;branching factor of the tree representing the samples created&quot;),
+                   RetDoc(&quot;Mat (one row = one sample)&quot;)));
+    declareMethod(rmm,
+                  &quot;returnSequenceDataSet&quot;,
+                  &amp;TransformationLearner::returnSequenceDataSet,
+                  (BodyDoc(&quot;creates and returns a data set using a 'sequential procedure' \n&quot;
+                           &quot;see 'sequenceDataSet()' implantation for more details&quot;),
+                   ArgDoc(&quot;const Vec start&quot;,&quot;data point from which all the other data points will derice (directly or indirectly)&quot;),
+                   ArgDoc(&quot;int n&quot;,&quot;number of sample data points to generate&quot;),
+                   RetDoc(&quot;nXd matrix (one row = one sample)&quot;)));
+    declareMethod(rmm,
+                  &quot;returnTrainingPoint&quot;,
+                  &amp;TransformationLearner::returnTrainingPoint,
+                  (BodyDoc(&quot;returns the 'idx'th data point in the training set&quot;),
+                   ArgDoc(&quot;int idx&quot;,&quot;index of the data point in the training set&quot;),
+                   RetDoc(&quot;Vec&quot;)));
+    declareMethod(rmm,
+                  &quot;returnReconstructionCandidates&quot;,
+                  &amp;TransformationLearner::returnReconstructionCandidates,
+                  (BodyDoc(&quot;return all the reconstructions candidates associated to a given target&quot;),
+                   ArgDoc(&quot;int targetIdx&quot;,&quot;index of the target data point in the training set&quot;),
+                   RetDoc(&quot;TVec&lt;ReconstructionCandidate&gt;&quot;)));
+    declareMethod(rmm,
+                  &quot;returnReconstructions&quot;,
+                  &amp;TransformationLearner::returnReconstructions,
+                  (BodyDoc(&quot;returns the reconstructions of the 'targetIdx'th data point in the training set \n&quot;
+                           &quot;(one reconstruction per reconstruction candidate)&quot;),
+                   ArgDoc(&quot;int targetIdx&quot;,&quot;index of the target data point in the training set&quot;),
+                   RetDoc(&quot;Mat (ith row = reconstruction associated to the ith reconstruction candidate)&quot;)));
+    declareMethod(rmm,
+                  &quot;returnNeighbors&quot;,
+                  &amp;TransformationLearner::returnNeighbors,
+                  (BodyDoc(&quot;returns the choosen neighbors of the target\n&quot;
+                           &quot;  (one neighbor per reconstruction candidate)&quot;),
+                   ArgDoc(&quot;int targetIdx&quot;,&quot;index of the target in the training set&quot;),
+                   RetDoc(&quot;Mat (ith row = neighbor associated to the ith reconstruction candidate)&quot;)));
+    declareMethod(rmm,
+                  &quot;returnTransform&quot;,
+                  &amp;TransformationLearner::returnTransform,
+                  (BodyDoc(&quot;returns the parameters of the 'transformIdx'th transformation&quot;),
+                   ArgDoc(&quot;int transformIdx&quot;,&quot;index of the transformation&quot;),
+                   RetDoc(&quot;Mat&quot;)));
+    declareMethod(rmm,
+                  &quot;returnAllTransforms&quot;,
+                  &amp;TransformationLearner::returnAllTransforms,
+                  (BodyDoc(&quot;returns the parameters of each transformation&quot;),
+                   RetDoc(&quot;mdXd matrix, m = number of transformations \n&quot;
+                          &quot;             d = dimensionality of the input space&quot;)));
+    declareMethod(rmm,
+                  &quot;trainBuild&quot;,
+                  &amp;TransformationLearner::trainBuild,
+                  (BodyDoc(&quot;training specific initialization operations&quot;)));
+    declareMethod(rmm,
+                  &quot;generatorBuild&quot;,
+                  &amp;TransformationLearner::generatorBuild,
+                  (BodyDoc(&quot;generator specific initialization operations&quot;),
+                   ArgDoc(&quot;int inputSpaceDim&quot;,&quot;dimensionality of the input space&quot;),
+                   ArgDoc(&quot;TVec&lt;Mat&gt; transforms_&quot;, &quot;transformations matrices&quot;),
+                   ArgDoc(&quot;Mat biasSet_&quot;,&quot;transformations bias&quot;),
+                   ArgDoc(&quot;real noiseVariance_&quot;,&quot;noise variance&quot;),
+                   ArgDoc(&quot;transformDistribution_&quot;, &quot;transformation distribution&quot;)));
+    declareMethod(rmm,
+                  &quot;gamma_sample&quot;,
+                  &amp;TransformationLearner::gamma_sample,
+                  (BodyDoc(&quot;returns a pseudo-random positive real value using the distribution p(x)=Gamma(x |alpha,beta)&quot;),
+                   ArgDoc(&quot;real alpha&quot;,&quot;&gt;=1&quot;),
+                   ArgDoc(&quot;real beta&quot;,&quot;&gt;= 0 (optional: default value==1)&quot;),
+                   RetDoc(&quot;real &gt;=0&quot;)));
+    declareMethod(rmm,
+                  &quot;return_dirichlet_sample&quot;,
+                  &amp;TransformationLearner::return_dirichlet_sample,
+                  (BodyDoc(&quot;returns a pseudo-random positive real vector using the distribution p(x)=Dirichlet(x|alpha)&quot;),
+                   ArgDoc(&quot;real alpha&quot;,&quot;all the parameters of the distribution are equal to 'alpha'&quot;),
+                   RetDoc(&quot;Vec (each element is between 0 and 1 , the elements sum to one)&quot;)));
+/* declareMethod(rmm,
+   &quot;return_dirichlet_sample&quot;,
+   &amp;TransformationLearner::return_dirichlet_sample,
+   (BodyDoc(&quot;returns a pseudo-random positive real vector using the distribution p(x)=Dirichlet(x|alphas)&quot;),
+   ArgDoc(&quot;Vec alphas&quot;,&quot;parameters of the distribution&quot;),
+   RetDoc(&quot;Vec (each element is between 0 and 1, the elements sum to one )&quot;))); */
+    declareMethod(rmm,
+                  &quot;initEStep&quot;,
+                  &amp;TransformationLearner::initEStep,
+                  (BodyDoc(&quot;initial expectation step&quot;)));
+    declareMethod(rmm,
+                  &quot;EStep&quot;,
+                  &amp;TransformationLearner::EStep,
+                  (BodyDoc(&quot;coordination of the different kinds of expectation steps&quot;)));
+    declareMethod(rmm,
+                  &quot;largeEStepA&quot;,
+                  &amp;TransformationLearner::largeEStepA,
+                  (BodyDoc(&quot;update the reconstruction set \n&quot;
+                           &quot;for each target, keeps the most probable &lt;neighbor, transformation&gt; pairs&quot;)));
+    declareMethod(rmm,
+                  &quot;largeEStepB&quot;,
+                  &amp;TransformationLearner::largeEStepB,
+                  (BodyDoc(&quot;update the reconstruction set \n&quot;
+                           &quot;for each &lt;target,transformation&gt; pairs,choose the most probable neighbors &quot;)));
+    declareMethod(rmm,
+                  &quot;smallEStep&quot;,
+                  &amp;TransformationLearner::smallEStep,
+                  (BodyDoc(&quot;update the weights of the reconstruction candidates&quot;)));
+    declareMethod(rmm,
+                  &quot;MStep&quot;,
+                  &amp;TransformationLearner::MStep,
+                  (BodyDoc(&quot;coordination of the different kinds of maximization step&quot;)));
+    declareMethod(rmm,
+                  &quot;MStepTransformDistribution&quot;,
+                  &amp;TransformationLearner::MStepTransformDistribution,
+                  (BodyDoc(&quot;maximization step with respect to transformation distribution parameters&quot;)));
+    declareMethod(rmm,
+                  &quot;MStepTransformations&quot;,
+                  &amp;TransformationLearner::MStepTransformations,
+                  (BodyDoc(&quot;maximization step with respect to transformation parameters (MAP version)&quot;)));
+    declareMethod(rmm,
+                  &quot;MStepNoiseVariance&quot;,
+                  &amp;TransformationLearner::MStepNoiseVariance,
+                  (BodyDoc(&quot;maximization step with respect to noise variance&quot;)));
+    declareMethod(rmm,
+                  &quot;stoppingCriterionReached&quot;,
+                  &amp;TransformationLearner::stoppingCriterionReached,
+                  (BodyDoc(&quot;stages == nstages?&quot;)));
+    declareMethod(rmm,
+                  &quot;nextStage&quot;,
+                  &amp;TransformationLearner::nextStage,
+                  (BodyDoc(&quot;increment 'stage' by one&quot;)));
+
 }
 
 
-
-//TO TEST
-//do the building operations related to the generation process
-//warning: we suppose the transformation parameters are set 
-void TransformationLearner::generatorInit(){
-    
-    inputSpaceDim = transformsSet.width();
-    
-
+///////////
+// build //
+///////////
+void TransformationLearner::build()
+{
+    pout &lt;&lt; &quot;build\n&quot;;
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
 }
 
+////////////
+// build_ //
+////////////
+void TransformationLearner::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
 
-//TO TEST
-//do the building operations related to training
-//warning: we suppose the training set has been transmitted
-//         before calling the method
-void TransformationLearner::trainInit(){
-   
-    //DIMENSION VARIABLES
-    
-    //dimension of the input space
-    inputSpaceDim = train_set-&gt;inputsize();
-      
-    //number of samples given in the training set
-    nbTrainingInput = train_set-&gt;length();
+    // ### In general, you will want to call this class' specific methods for
+    // ### conditional distributions.
+    // TransformationLearner::setPredictorPredictedSizes(predictor_size,
+    //                                          predicted_size,
+    //                                          false);
+    // TransformationLearner::setPredictor(predictor_part, false);
 
-    
-    //number of generation candidates related to a specific target in the 
-    //generation set.   
-    nbGenerationCandidatesPerTarget = nbNeighbors * nbTransforms;
-
-   //total number of generation candidates in the generation set
-    nbGenerationCandidates = nbTrainingInput * nbGenerationCandidatesPerTarget;
-
-    
-    //LEARNED MODEL PARAMETERS
-    
-    //set of transformations (represented as a single matrix)
-    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
-    
-    //view on the set of transformations (vector)
-    //    each transformation = one matrix 
-    transforms.resize(nbTransforms);
-    for(int k = 0; k&lt; nbTransforms; k++){
-        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    pout &lt;&lt; &quot;build_\n&quot;;
+    if(behavior == BEHAVIOR_LEARNER){
+        trainBuild();
     }
-    
-     //generation set and weights of the entries in the generation set
-    generationSet.resize(nbGenerationCandidates);
-
-    //OTHER VARIABLES
-    
-    //weight decay
-    lambda = noiseVariance/transformsVariance;
    
+    else{
+        generatorBuild(); //initialization of the parameters with all the default values
+    }
     
+ 
+}
 
-    //factor used in the computation of the generation weights
-    noiseVarianceFactor = 1/(2*noiseVariance);
-    
+/////////
+// cdf //
+/////////
+real TransformationLearner::cdf(const Vec&amp; y) const
+{
+    PLERROR(&quot;cdf not implemented for TransformationLearner&quot;); return 0;
+}
 
-    //to store a view on the generation set 
-    //   (entries related to a specific target)
-    targetGenerationSet.resize(nbGenerationCandidatesPerTarget);
+/////////////////
+// expectation //
+/////////////////
+void TransformationLearner::expectation(Vec&amp; mu) const
+{
+    PLERROR(&quot;expectation not implemented for TransformationLearner&quot;);
+}
 
-    //Storage space used in the update of the transformation parameters
-    B_C = Mat(2 * nbTransforms * inputSpaceDim , inputSpaceDim);
-
-    B.resize(nbTransforms);
-    C.resize(nbTransforms);
-    for(int k=0; k&lt;nbTransforms; k++){
-        B[k]= B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
-    }
-    for(int k= nbTransforms ; k&lt;2*nbTransforms ; k++){
-        C[(k % nbTransforms)] = B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
-    }
+// ### Remove this method if your distribution does not implement it.
+////////////
+// forget //
+////////////
+void TransformationLearner::forget()
+{
     
+    /*!
+      A typical forget() method should do the following:
+      - initialize a random number generator with the seed option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */    
+    //PLERROR(&quot;forget method not implemented for TransformationLearner&quot;);
     
-    target.resize(inputSpaceDim);
-    neighbor.resize(inputSpaceDim);
-
+    inherited::forget();
+    stage = 0;
+    trainBuild();
+    
 }
 
+//////////////
+// generate //
+//////////////
 
-//TO TEST 
-void TransformationLearner::build_(){
+//!generate a point using the training set: 
+//! - choose ramdomly a neighbor among data points in the training set
+//! - choose randomly a transformation 
+//! - apply the transformation on the choosen neighbor
+//! - add some noise 
+void TransformationLearner::generate(Vec &amp; y) //const
+{
+    //PLERROR(&quot;generate not implemented for TransformationLearner&quot;);
+    PLASSERT(y.length() == inputSpaceDim);
+    int neighborIdx ;
+    neighborIdx=pickNeighborIdx();
+    seeNeighbor(neighborIdx);
+    generatePredictedFrom(neighbor, y);
+}
 
-  
-    if(transformDistribution.length() == 0){
-        transformDistribution.resize(nbTransforms);
-        transformDistribution.fill(1.0/nbTransforms);
-    }
-    else{
-        PLASSERT(transformDistribution.length() == nbTransforms);
-        real sum =0;
-        for(int i=0; i&lt;nbTransforms; i++){
-            sum += transformDistribution[i];
-        }
-        PLASSERT(sum == 1);  
-    }
-    
-   
-
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
-    // ###    all serialised options.
-    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
-    // ###    options have been modified.
-    // ### You should assume that the parent class' build_() has already been
-    // ### called.
+// ### Default version of inputsize returns learner-&gt;inputsize()
+// ### If this is not appropriate, you should uncomment this and define
+// ### it properly here:
+int TransformationLearner::inputsize() const {
+    return inputSpaceDim;
 }
 
-//TO TEST
-// ### Nothing to add here, simply calls build_
-void TransformationLearner::build()
+/////////////////
+// log_density //
+/////////////////
+real TransformationLearner::log_density(const Vec&amp; y) //const
 {
-
-    // pout &lt;&lt; &quot;build()&quot; &lt;&lt; endl;
-    inherited::build();
-    build_(); 
+    PLASSERT(y.length() == inputSpaceDim);
+    real weight;
+    real totalWeight = INIT_weight(0);
+    real scalingFactor = -1*(pl_log(pow(2*Pi*noiseVariance, inputSpaceDim/2.0)) 
+                             +
+                             pl_log(trainingSetLength));
+    for(int neighborIdx=0; neighborIdx&lt;trainingSetLength; neighborIdx++){
+        for(int transformIdx=0 ; transformIdx&lt;nbTransforms ; transformIdx++){
+            weight = computeReconstructionWeight(y,
+                                                 neighborIdx,
+                                                 transformIdx);
+            weight = MULT_weights(weight,
+                                  transformDistribution[transformIdx]);
+            totalWeight = SUM_weights(weight,totalWeight);
+        }  
+    }
+    totalWeight = MULT_weights(totalWeight, scalingFactor);
+    return totalWeight;
+    
+    /*PLERROR(&quot;density not implemented for TransformationLearner&quot;); return 0;*/
 }
 
-
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void TransformationLearner::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -431,40 +685,60 @@
     PLERROR(&quot;TransformationLearner::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
 }
 
-/******** LEARNING MODULE ******************************************************/
+////////////////////
+// resetGenerator //
+////////////////////
+/*void TransformationLearner::resetGenerator(long g_seed) const
+{
+    PLERROR(&quot;resetGenerator not implemented for TransformationLearner&quot;);
+}
+*/
 
+//////////////////
+// setPredictor //
+//////////////////
+void TransformationLearner::setPredictor(const Vec&amp; predictor, bool call_parent) const
+{
+    if (call_parent)
+        inherited::setPredictor(predictor, true);
+    // ### Add here any specific code required by your subclass.
+}
 
-//TO DO
-int TransformationLearner::outputsize() const
+////////////////////////////////
+// setPredictorPredictedSizes //
+////////////////////////////////
+bool TransformationLearner::setPredictorPredictedSizes(int the_predictor_size,
+                                               int the_predicted_size,
+                                               bool call_parent)
 {
-    return 0;
-    // Compute and return the size of this learner's output (which typically
-    // may depend on its inputsize(), targetsize() and set options).
+    bool sizes_have_changed = false;
+    if (call_parent)
+        sizes_have_changed = inherited::setPredictorPredictedSizes(
+                the_predictor_size, the_predicted_size, true);
+
+    // ### Add here any specific code required by your subclass.
+
+    // Returned value.
+    return sizes_have_changed;
 }
 
-
-//TO DO
-void TransformationLearner::forget()
+/////////////////
+// survival_fn //
+/////////////////
+real TransformationLearner::survival_fn(const Vec&amp; y) const
 {
-    //! (Re-)initialize the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!)
-    /*!
-      A typical forget() method should do the following:
-      - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
-      - initialize the learner's parameters, using this random generator
-      - stage = 0
-    */
-    inherited::forget();
+    PLERROR(&quot;survival_fn not implemented for TransformationLearner&quot;); return 0;
 }
 
-//TO DO
+// ### Remove this method, if your distribution does not implement it.
+///////////
+// train //
+///////////
 void TransformationLearner::train()
 {
-
-    trainInit();
-
+    
+  
+    //PLERROR(&quot;train method not implemented for TransformationLearner&quot;);
     // The role of the train method is to bring the learner up to
     // stage==nstages, updating train_stats with training costs measured
     // on-line in the process.
@@ -496,446 +770,922 @@
         train_stats-&gt;finalize(); // finalize statistics for this epoch
     }
     */
-}
 
-//TO DO
-void TransformationLearner::computeOutput(const Vec&amp; input, Vec&amp; output) const
-{
-    // Compute the output from the input.
-    // int nout = outputsize();
-    // output.resize(nout);
-    // ...
+    initEStep();
+    while(!stoppingCriterionReached()){
+        MStep();
+        EStep();
+        stage ++;
+    }
+    
 }
 
-//TO DO
-void TransformationLearner::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
-                                           const Vec&amp; target, Vec&amp; costs) const
+//////////////
+// variance //
+//////////////
+void TransformationLearner::variance(Mat&amp; covar) const
 {
-// Compute the costs from *already* computed output.
-// ...
+    PLERROR(&quot;variance not implemented for TransformationLearner&quot;);
 }
 
-//TO DO
-TVec&lt;string&gt; TransformationLearner::getTestCostNames() const
-{
-    // Return the names of the costs computed by computeCostsFromOutputs
-    // (these may or may not be exactly the same as what's returned by
-    // getTrainCostNames).
-    // ..
+
+
+//INITIALIZATION METHODS 
+
+
+//! initialization operations that have to be done before the training
+//!WARNING: the trainset (&quot;train_set&quot;) must be given
+void TransformationLearner::trainBuild(){
     
-    return TVec&lt;string&gt;();
-}
+    
+    transformsSD = sqrt(transformsVariance);
+    
+    //DIMENSION VARIABLES
+    
+    //dimension of the input space
+    inputSpaceDim = train_set-&gt;inputsize();
+      
+    //number of samples given in the training set
+    trainingSetLength = train_set-&gt;length();
+    
+    
+    //number of reconstruction candidates related to a specific target in the 
+    //reconstruction set.   
+    nbTargetReconstructions = nbNeighbors * nbTransforms;
 
-//TO DO
-TVec&lt;string&gt; TransformationLearner::getTrainCostNames() const
-{
-    // Return the names of the objective costs that the train method computes
-    // and for which it updates the VecStatsCollector train_stats
-    // (these may or may not be exactly the same as what's returned by
-    // getTestCostNames).
-    // ...
-    return TVec&lt;string&gt;();
+    //total number of reconstruction candidates in the reconstruction set
+    nbReconstructions = trainingSetLength * nbTargetReconstructions;
+    
+    
+    //LEARNED MODEL PARAMETERS
+    
+    //set of transformations (represented as a single matrix)
+    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
+    
+    //view on the set of transformations (vector)
+    //    each transformation = one matrix 
+    transforms.resize(nbTransforms);
+    for(int k = 0; k&lt; nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    
+    if(withBias){
+        biasSet = Mat(nbTransforms,inputSpaceDim);
+    }
+    
+    initTransformsParameters();
+
+   
+    if(transformsPeriod == UNDEFINED || transformsOffset == UNDEFINED){
+        if(learnNoiseVariance){
+            transformsPeriod = 2;
+            transformsOffset = 1;
+        }
+        else{
+            transformsPeriod = 1;
+            transformsOffset = 0;
+        }
+    }
+
+    //training parameters for noise variance
+    if(learnNoiseVariance){
+        if(noiseVariancePeriod == UNDEFINED || noiseVarianceOffset == UNDEFINED){
+            noiseVariancePeriod = 2;
+            noiseVarianceOffset = 1;
+        }
+        if(regOnNoiseVariance){
+            if(noiseAlpha &lt; 1)
+                noiseAlpha = 1;
+            if(noiseBeta &lt;= 0){
+                noiseBeta = 1;
+            }
+        }
+        else{
+            noiseAlpha = NOISE_ALPHA_NO_REG;
+            noiseBeta = NOISE_BETA_NO_REG;
+        }
+    }
+    
+    //initialize the noise variance
+     if(noiseVariance == UNDEFINED){
+        if(learnNoiseVariance &amp;&amp; regOnNoiseVariance){
+            initNoiseVariance();
+        }
+        else{
+            noiseVariance = 1.0;
+        }
+     }
+    
+     //training parameters for transformation distribution
+     if(learnTransformDistribution){
+         if(transformDistributionPeriod == UNDEFINED || transformDistributionOffset == UNDEFINED){
+             transformDistributionPeriod = 1;
+             transformDistributionOffset = 0;
+         }
+         if(regOnTransformDistribution){
+             if(transformDistributionAlpha&lt;=0){
+                 transformDistributionAlpha =10;
+             }
+             else{
+                 transformDistributionAlpha = TRANSFORM_DISTRIBUTION_ALPHA_NO_REG;
+             }
+         }
+     }
+
+
+    //transformDistribution
+    if(transformDistribution.length() == 0){
+        if(learnTransformDistribution &amp;&amp; regOnTransformDistribution)
+            initTransformDistribution();
+        else{
+            transformDistribution.resize(nbTransforms);
+            real w = INIT_weight(1.0/nbTransforms);
+            for(int k=0; k&lt;nbTransforms ; k++){
+                transformDistribution[k] = w;
+            }
+        }       
+    }
+    else{
+        PLASSERT(transformDistribution.length() == nbTransforms);
+        PLASSERT(isWellDefined(transformDistribution));
+    }
+
+    //reconstruction set 
+    reconstructionSet.resize(nbReconstructions);
+    
+    
+
+    //OTHER VARIABLES
+    
+    
+    //to store a view on the generation set 
+    //   (entries related to a specific target)
+    targetReconstructionSet.resize(nbTargetReconstructions);
+    
+    //Storage space used in the update of the transformation parameters
+    B_C = Mat(2 * nbTransforms * inputSpaceDim , inputSpaceDim);
+    
+    B.resize(nbTransforms);
+    C.resize(nbTransforms);
+    for(int k=0; k&lt;nbTransforms; k++){
+        B[k]= B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
+    }
+    for(int k= nbTransforms ; k&lt;2*nbTransforms ; k++){
+        C[(k % nbTransforms)] = B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
+    }
+    
+    
+    target.resize(inputSpaceDim);
+    neighbor.resize(inputSpaceDim);
 }
 
- /*********GENERATION MODULE *****************************************/
 
-    //The object is the representation of a learned distribution
-    //Are are methods to ensure the &quot;generative behavior&quot; of the object
-    //(Once the distribution is learned, we might be able to generate
-    // samples from it)
+//! initialization operations that have to be done before a generation process
+//! (all the undefined parameters will be initialized  randomly)
+void TransformationLearner::generatorBuild( int inputSpaceDim_,
+                                            TVec&lt;Mat&gt; transforms_,
+                                            Mat biasSet_,
+                                            real noiseVariance_,
+                                            Vec transformDistribution_){
+    
+    inputSpaceDim = inputSpaceDim_;
+    transformsSD = sqrt(transformsVariance);
+    
 
+    //transformations parameters
 
+    
+    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
+    transforms.resize(nbTransforms);
+    for(int k = 0; k&lt; nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    
+    if(withBias){
+        biasSet = Mat(nbTransforms,inputSpaceDim);
+    }
 
-// TO TEST
-//Chooses the transformation parameters using a 
-//normal distribution with mean 0 and variance &quot;transformsVariance&quot;
-//(call generatorBuild() after)
-void TransformationLearner::buildTransformationParametersNormal(){
+    if(transforms_.length() == 0){
+        initTransformsParameters();
+    }
+    else{
+        setTransformsParameters(transforms_,biasSet_);
+    }
+    
+
+    //noise variance
+    if(noiseAlpha &lt; 1){
+            noiseAlpha = 1;
+        }
+    if(noiseBeta &lt;= 0){
+        noiseBeta = 1;
+    }
+    if(noiseVariance_ == UNDEFINED){
+        initNoiseVariance();
+    }
+    else{
+        setNoiseVariance(noiseVariance_);
+    }
+    //transformation distribution
+    if(transformDistributionAlpha &lt;=0)
+        transformDistributionAlpha = 10;
+    if(transformDistribution_.length()==0){
+        initTransformDistribution();
+    }
+    else{
+        setTransformDistribution(transformDistribution_);
+    }
+}
+
+
+//!initializes the transformation parameters randomly 
+//!(prior distribution= Normal(0,transformsVariance))
+void TransformationLearner::initTransformsParameters()
+{
+    
     transformsSet.resize(nbTransforms*inputSpaceDim, inputSpaceDim);
     transforms.resize(nbTransforms);
+    for(int k = 0; k&lt; nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    int idx = 0;
     for(int t=0; t&lt;nbTransforms ; t++){
-        random_gen-&gt;fill_random_normal(transforms[t], 0 , transformsStDev);
+        transforms[t] = transformsSet.subMatRows(idx,inputSpaceDim);
+        idx += inputSpaceDim;
+        random_gen-&gt;fill_random_normal(transforms[t], 0 , transformsSD);
     }
+    if(withBias){
+        biasSet = Mat(nbTransforms,inputSpaceDim);
+        random_gen-&gt;fill_random_normal(biasSet, 0,transformsSD);
+    }
+    if(transformFamily == TRANSFORM_FAMILY_LINEAR){
+        for(int t=0; t&lt;nbTransforms;t++){
+            addToDiagonal(transforms[t],1.0);
+        }
+    }
 }
 
-
-//TO TEST
-//set the transformation parameters to the specified values
-//(call generatorBuild() after)
-void TransformationLearner::setTransformationParameters(TVec&lt;Mat&gt; &amp; transforms_){
-     
+//!initializes the transformation parameters to the given values
+//!(bias are set to 0)
+void TransformationLearner::setTransformsParameters(TVec&lt;Mat&gt; transforms_,
+                                                    Mat biasSet_)
+{
     PLASSERT(transforms_.length() == nbTransforms);
-    inputSpaceDim = transforms_[0].width();
     
     int nbRows = inputSpaceDim*nbTransforms;
-    transformsSet = Mat(nbRows,inputSpaceDim);
+    transformsSet.resize(nbRows,inputSpaceDim);
     transforms.resize(nbTransforms);
-  
+    for(int k = 0; k&lt; nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+
+
     int rowIdx = 0;
     for(int t=0; t&lt;nbTransforms; t++){
+        PLASSERT(transforms_[t].width() == inputSpaceDim);
+        PLASSERT(transforms_[t].length() == inputSpaceDim);
         transformsSet.subMatRows(rowIdx,inputSpaceDim) &lt;&lt; transforms_[t];
         transforms[t]= transformsSet.subMatRows(rowIdx,inputSpaceDim);
         rowIdx += inputSpaceDim;
     }
-}
-
-//TO TEST
-//creates a data set
-//
-//     Consists in building a tree of deepness d = &quot;nbGenerations&quot; and
-//     constant branch factor n = &quot;generationLength&quot;
-//
-//            0      1        2     ...         
-//  
-//            r - child1  - child1  ...       
-//                        - child2  ...
-//                            ...   ...
-//                        - childn  ...
-//
-//              - child2  - child1  ...
-//                        - child2  ...
-//                            ...   ...
-//                        - childn  ...
-//                     ...
-//             - childn   - child1  ...
-//                        - child2  ...
-//                            ...   ...
-//                        - childn  ... 
-//
-
-// all the childs are choosen following the same process:
-// 1) choose a transformation  
-// 2) apply the transformation to the parent
-// 3) add noise to the result 
-void TransformationLearner::createDataSet(Vec &amp; root,
-                                          int nbGenerations,
-                                          int generationLength,
-                                          Mat &amp; dataPoints){
-   
-    PLASSERT(root.length() == inputSpaceDim);
- 
-    //we look at the length of the given matrix dataPoint ;.  
-    int nbDataPoints = int(pow(1.0*nbGenerations,1.0*generationLength)) + 1;
-    dataPoints.resize(nbDataPoints,inputSpaceDim);
+    if(withBias){    
+        PLASSERT(biasSet_.length() == nbTransforms);
+        PLASSERT(biasSet_.width() == inputSpaceDim);
+        biasSet = Mat(nbTransforms, inputSpaceDim);
+        biasSet &lt;&lt; biasSet_;
+    }
     
-    //root = first element in the matrix dataPoints
-    dataPoints(0) &lt;&lt; root;
-  
-    //generate the other data points 
-    int centerIdx=0 ;
-    for(int dataIdx=1; dataIdx &lt; nbDataPoints ; dataIdx+=generationLength){
 
-        Vec v = dataPoints(centerIdx);
-        Mat m = dataPoints.subMatRows(dataIdx, generationLength);
-        batchGenerateFrom(v,m); 
-        centerIdx ++ ;
-    }
 }
 
+//!initializes the noise variance randomly
+//!(gamma distribution)
+void TransformationLearner::initNoiseVariance()
+{
+    real noisePrecision = gamma_sample(noiseAlpha, noiseBeta);
+    PLASSERT(noisePrecision != 0);
+    noiseVariance = 1.0/noisePrecision;
+}
 
-//TO TEST
-//create a dataset using the same tree generation process as
-//createDataSet, except the number of child per parent is fixed to 1,
-//   root -&gt; 1st point -&gt; 2nd point ... -&gt; nth point 
-void TransformationLearner::createDataSetSequentially(Vec &amp; root,
-                                                      int n,
-                                                      Mat &amp; dataPoints){
-    createDataSet(root, n-1, 1, dataPoints);
+//!initializes the noise variance with the given value
+void TransformationLearner::setNoiseVariance(real nv)
+{
+    PLASSERT(nv &gt; 0);
+    noiseVariance = nv;
 }
 
 
-//TO TEST
-//Select a transformation randomly (with respect ot our transformation
-//distribution)
-int TransformationLearner::pickTransformIdx(){
-     return random_gen-&gt;multinomial_sample(transformDistribution);
+//!initializes the transformation distribution randomly
+//!(dirichlet distribution)
+void TransformationLearner::initTransformDistribution()
+{
+    
+    transformDistribution.resize(nbTransforms);
+    dirichlet_sample(transformDistributionAlpha, transformDistribution);
+    for(int i=0; i&lt;nbTransforms ;i++){
+        transformDistribution[i] = INIT_weight(transformDistribution[i]);
+    } 
 }
 
-  
-//here is the generation process for a given center data point 
-//  1) choose a transformation
-//  2) apply it on the center data point
-//  3) add noise
+//!initializes the transformation distribution with the given values
+void TransformationLearner::setTransformDistribution(Vec td)
+{
+    PLASSERT(td.length() == nbTransforms);
+    PLASSERT(isWellDefined(td));
+    transformDistribution.resize(nbTransforms);
+    transformDistribution &lt;&lt; td;
+}
 
-//TO TEST
-//generates a sample data point  from a  given center data point 
-void  TransformationLearner::generateFrom(Vec &amp; center, Vec &amp; sample){
+
+//GENERATION
+
+//!generates a sample data point from a source data point
+void TransformationLearner::generatePredictedFrom(const Vec &amp; source,
+                                                  Vec &amp; sample)const
+{
+    
     int transformIdx = pickTransformIdx();
-    generateFrom(center, sample, transformIdx);
+    generatePredictedFrom(source, sample, transformIdx);
 }
 
-//TO TEST
-//generates a sample data point from a given center data point
-void TransformationLearner::generateFrom(Vec &amp; center,
-                                         Vec &amp; sample, 
-                                         int transformIdx){
-    int d = center.length();
+//!generates a sample data point from a source data point with a specific transformation
+void TransformationLearner::generatePredictedFrom(const Vec &amp; source,
+                                                  Vec &amp; sample,
+                                                  int transformIdx)const
+{
+    //TODO
+    real noiseSD = pow(noiseVariance,0.5);
+    int d = source.length();
     PLASSERT(d == inputSpaceDim);
+    PLASSERT(sample.length() == inputSpaceDim);
+    PLASSERT(0&lt;=transformIdx&lt;nbTransforms);
     
-    sample.resize(inputSpaceDim);
-    
     //apply the transformation
-    applyTransformationOn(transformIdx,center,sample);
+    applyTransformationOn(transformIdx,source,sample);
     
     //add noise
     for(int i=0; i&lt;d; i++){
-        sample[i] += random_gen-&gt;gaussian_mu_sigma(0, noiseStDev);
+        sample[i] += random_gen-&gt;gaussian_mu_sigma(0, noiseSD);
     } 
 }
 
-//TO TEST
-//fill the matrix &quot;samples&quot; with sample data points obtained from
-// a given center data point.
-void TransformationLearner::batchGenerateFrom(Vec &amp; center, Mat &amp; samples){
+//!generates a sample data point from a source data point and returns it
+//! (if transformIdx &gt;= 0 , we use the corresponding transformation )
+Vec TransformationLearner::returnPredictedFrom(Vec source,
+                                               int transformIdx)
+{
+    Vec sample;
+    sample.resize(inputSpaceDim);
+    if(transformIdx &lt;0)
+        generatePredictedFrom(source,sample);
+    else
+        generatePredictedFrom(source,sample,transformIdx);
+    return sample;
+}
 
+//!fill the matrix &quot;samples&quot; with data points obtained from a given center data point
+void TransformationLearner::batchGeneratePredictedFrom(const Vec &amp; center,
+                                                        Mat &amp; samples)const
+{
     PLASSERT(center.length() ==inputSpaceDim);
-    PLASSERT(samples.width()==inputSpaceDim);
+    PLASSERT(samples.width() ==inputSpaceDim);
     int l = samples.length();
     for(int i=0; i&lt;l; i++)
     {
         Vec v = samples(i);
-        generateFrom(center, v);
+        generatePredictedFrom(center, v);
     }
 }
 
+//!fill the matrix &quot;samples&quot; with data points obtained form a given center data point
+//!    - we use a specific transformation
+void TransformationLearner::batchGeneratePredictedFrom(const Vec &amp; center,
+                                                        Mat &amp; samples,
+                                                        int transformIdx)const
+{
+    PLASSERT(center.length() ==inputSpaceDim);
+    PLASSERT(samples.width() ==inputSpaceDim);
+    int l = samples.length();
+    for(int i=0; i&lt;l; i++)
+    {
+        Vec v = samples(i);
+        generatePredictedFrom(center, v,transformIdx);
+    }  
+}
 
+//Generates n samples from center and returns them stored in a matrix
+//    (generation process = 1) choose a transformation (*),
+//                          2) apply it on center
+//                          3) add noise)
+// - (*) if transformIdx&gt;=0, we always use the corresponding transformation
+Mat TransformationLearner::returnGeneratedSamplesFrom(Vec center,
+                                                      int n,
+                                                      int transformIdx)
+{
+    Mat samples = Mat(n,inputSpaceDim);
+    if(transformIdx&lt;0)
+        batchGeneratePredictedFrom(center,samples);
+    else
+        batchGeneratePredictedFrom(center,samples,transformIdx);
+    return samples;
+}
 
-//-------------EXTERNAL ACCESS ---------------------------
+//!select a transformation randomly (with respect to our multinomial distribution)
+int TransformationLearner::pickTransformIdx() const
+{
+    
+    Vec probaTransformDistribution ;
+    probaTransformDistribution.resize(nbTransforms);
+    for(int i=0; i&lt;nbTransforms; i++){
+        probaTransformDistribution[i]=PROBA_weight(transformDistribution[i]);
+    }
+    return random_gen-&gt;multinomial_sample(probaTransformDistribution);
+}
 
-//TO TEST
-//Returns a copy of the generation candidates associated to a given target
-TVec&lt;GenerationCandidate&gt; TransformationLearner::returnReproductionSources
-(int targetIdx){
-    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
-    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
-    return generationSet.subVec(startIdx, endIdx).copy();
+//!Select a neighbor in the training set randomly
+//!(return his index in the training set)
+//!We suppose all data points in the training set are equiprobables
+int TransformationLearner::pickNeighborIdx() const
+{
+    
+    return random_gen-&gt;uniform_multinomial_sample(trainingSetLength);
 }
 
-//TO TEST
-//Returns the parameters of a given transformation
-Mat TransformationLearner::returnTransform(int transformIdx){
-    return transforms[transformIdx].copy();   
+
+ //!creates a data set:
+//!     equivalent in building a tree with fixed deepness and constant branching factor
+//!
+//!            0      1        2     ...         
+//!  
+//!            r -&gt; child1  -&gt; child1  ...       
+//!                         -&gt; child2  ...
+//!                             ...    ...
+//!                         -&gt; childn  ...
+//!
+//!              -&gt; child2  -&gt; child1  ...
+//!                         -&gt; child2  ...
+//!                              ...   ...
+//!                         -&gt; childn  ...
+//!                      ...
+//!              -&gt; childn  -&gt; child1  ...
+//!                         -&gt; child2  ...
+//!                              ...   ...
+//!                         -&gt; childn  ... 
+//!
+//!(where &quot;a -&gt; b&quot; stands for &quot;a generate b&quot;)
+//!all the child are generated by the same following process:
+//! 1) choose a transformation  
+//! 2) apply the transformation to the parent
+//! 3) add noise to the result 
+void TransformationLearner::treeDataSet(const Vec &amp; root,
+                                        int deepness,
+                                        int branchingFactor,
+                                        Mat &amp; dataPoints)
+{
+
+    PLASSERT(root.length() == inputSpaceDim);
+
+    //we look at the length of the given matrix dataPoint ;
+    int nbDataPoints;
+    if(branchingFactor == 1)
+        nbDataPoints = deepness + 1;  
+    else nbDataPoints = int((1- pow(1.0*branchingFactor,deepness + 1.0))
+                            /
+                            (1 - branchingFactor));
+    dataPoints.resize(nbDataPoints,inputSpaceDim);
+    
+    //root = first element in the matrix dataPoints
+    dataPoints(0) &lt;&lt; root;
+  
+    //generate the other data points 
+    int centerIdx=0 ;
+    for(int dataIdx=1; dataIdx &lt; nbDataPoints ; dataIdx+=branchingFactor){
+        
+        Vec v = dataPoints(centerIdx);
+        Mat m = dataPoints.subMatRows(dataIdx, branchingFactor);
+        batchGeneratePredictedFrom(v,m); 
+        centerIdx ++ ;
+    }  
 }
 
-//TO TEST
-//Returns all the transformation parameters
-Mat TransformationLearner::returnAllTransforms(){
-    return transformsSet;
+Mat TransformationLearner::returnTreeDataSet(Vec root,
+                                             int deepness,
+                                             int branchingFactor)
+{
+    Mat dataPoints;
+    treeDataSet(root,deepness,branchingFactor, dataPoints);
+    return dataPoints;
 }
 
 
-//TO TEST
-//From the subset ofgeneration candidate associated to the target,
-//builds and returns the corresponding subset of generated data points . 
-Mat TransformationLearner::returnReproductions(int targetIdx){
-    Mat reproductions = Mat(nbGenerationCandidatesPerTarget,inputSpaceDim);
-    int candidateIdx = targetIdx*nbGenerationCandidatesPerTarget;
-    int neighborIdx, transformIdx;
-    for(int i=0; i&lt;nbGenerationCandidatesPerTarget; i++){
-        neighborIdx = generationSet[candidateIdx].neighborIdx;
-        transformIdx= generationSet[candidateIdx].transformIdx;
-        getNeighborFromTrainingSet(neighborIdx);
-        Vec v = reproductions(i);
-        applyTransformationOn(transformIdx, neighbor, v);
-        candidateIdx ++;
-    }
-    return reproductions;
+//!create a &quot;sequential&quot; dataset:
+//!  start -&gt; second point -&gt; third point ... -&gt;nth point
+//! (where &quot;-&gt;&quot; stands for : &quot;generate the&quot;)
+void TransformationLearner::sequenceDataSet(const Vec &amp; start,
+                                            int n,
+                                            Mat &amp; dataPoints)
+{
+    treeDataSet(start,n-1,1,dataPoints);
 }
 
-
-//TO TEST
-//Generates n samples from center and returns them
-//    (generation process = 1) choose a transformation,
-//                          2) apply it on center
-//                          3) add noise)
-Mat TransformationLearner::returnGeneratedSamplesFrom(Vec center, int n){
-    int d = center.length();
-    PLASSERT(d == inputSpaceDim);
-    Mat m = Mat(n,d);
-    batchGenerateFrom(center, m);
-    return m;
+Mat TransformationLearner::returnSequenceDataSet(Vec start,
+                                                 int n)
+{
+    Mat dataPoints;
+    sequenceDataSet(start,n,dataPoints);
+    return dataPoints;
 }
 
-//TO TEST
-//Generates a data set and returns it
-//(tree generation process: see createDataSet for more details)
-Mat TransformationLearner::returnGeneratedDataSet(Vec root,
-                                                  int nbGenerations,
-                                                  int generationLength){
- 
-    int n = int(pow(1.0*nbGenerations, 1.0*generationLength)) + 1;
-    int d = root.length();
-    PLASSERT(d == inputSpaceDim);
 
-    Mat dataSet = Mat(n,d);
-    createDataSet(root,nbGenerations,generationLength,dataSet);
-    return dataSet;
+//! COPIES OF THE STRUCTURES
+
+
+//!returns the &quot;idx&quot;th data point in the training set
+Vec TransformationLearner::returnTrainingPoint(int idx)
+{
+    
+    Vec v,temp;
+    real w;
+    v.resize(inputSpaceDim);
+    train_set-&gt;getExample(idx, v, temp, w);
+    return v;
+    
 }
+ 
 
-//TO TEST
-//Generates a data set and returns it
-//(sequential generation process: see createDataSetSequentially for more details)
-Mat TransformationLearner::returnSequentiallyGeneratedDataSet(Vec root,int n){ 
-    return returnGeneratedDataSet(root, n-1,1);
+//!returns all the reconstructions candidates associated to a given target
+TVec&lt;ReconstructionCandidate&gt; TransformationLearner::returnReconstructionCandidates(int targetIdx)
+{
+   
+    int startIdx = targetIdx * nbTargetReconstructions;  
+    return reconstructionSet.subVec(startIdx, 
+                                    nbTargetReconstructions).copy();
 }
 
 
-// ----------GENERAL USE--------------------------------------------------
+//!returns the reconstructions of the &quot;targetIdx&quot;th data point value in the training set
+//!(one reconstruction for each reconstruction candidate)
+Mat TransformationLearner::returnReconstructions(int targetIdx)
+{
+    Mat reconstructions = Mat(nbTargetReconstructions,inputSpaceDim);
+    int candidateIdx = targetIdx*nbTargetReconstructions;
+    int neighborIdx, transformIdx;
+    for(int i=0; i&lt;nbTargetReconstructions; i++){
+        neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
+        transformIdx= reconstructionSet[candidateIdx].transformIdx;
+        seeNeighbor(neighborIdx);
+        Vec v = reconstructions(i);
+        applyTransformationOn(transformIdx, neighbor, v);
+        candidateIdx ++;
+    }
+    return reconstructions; 
+}
 
+//!returns the neighbors choosen to reconstruct the target
+//!(one choosen neighbor for each reconstruction candidate associated to the target)
+Mat TransformationLearner::returnNeighbors(int targetIdx)
+{
+    int candidateIdx = targetIdx*nbTargetReconstructions;
+    int neighborIdx;
+    Mat neighbors = Mat(nbTargetReconstructions, inputSpaceDim);
+    for(int i=0; i&lt;nbTargetReconstructions; i++){
+        neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
+        seeNeighbor(neighborIdx);
+        neighbors(i) &lt;&lt; neighbor;
+        candidateIdx++;
+    }
+    return neighbors;
+}
 
 
-//REFERENCE OPERATIONS ON GENERATION SET AND TRAINING SET  
+//!returns the parameters of the &quot;transformIdx&quot;th transformation
+Mat TransformationLearner::returnTransform(int transformIdx)
+{
+    return transforms[transformIdx].copy();    
+}
 
-
-//TO TEST
-// stores a view on the subset of generation set related to the specified
-// target (into the variable &quot;targetGenerationSet&quot; )
-void TransformationLearner::getViewOnTargetGenerationCandidates(int targetIdx){
-    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
-    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
-    targetGenerationSet = generationSet.subVec(startIdx, 
-                                               endIdx);
-    
+//!returns the parameters of each transformation
+//!(as an KdXd matrix, K = number of transformations,
+//!                    d = dimension of input space)
+Mat TransformationLearner::returnAllTransforms()
+{
+    return transformsSet.copy();    
 }
 
 
+//! VIEWS ON RECONSTRUCTION SET AND TRAINING SET
 
 
 
-// stores the &quot;targetIdx&quot;th input in the training set into the variable
+//! stores a VIEW on the reconstruction candidates related to the specified
+//! target (into the variable &quot;targetReconstructionSet&quot; )
+void TransformationLearner::seeTargetReconstructionSet(int targetIdx)
+{
+    int startIdx = targetIdx *nbTargetReconstructions;
+    targetReconstructionSet = reconstructionSet.subVec(startIdx, 
+                                                       nbTargetReconstructions); 
+}
+
+// stores the &quot;targetIdx&quot;th point in the training set into the variable
 // &quot;target&quot;
-void TransformationLearner::getTargetFromTrainingSet(int targetIdx){
+void TransformationLearner::seeTarget(const int targetIdx)
+{
     Vec v;
     real w;
     train_set-&gt;getExample(targetIdx,target,v,w);
-
-    //TO TEST : OK
+    
 }
 
 // stores the &quot;neighborIdx&quot;th input in the training set into the variable
 // &quot;neighbor&quot; 
-void TransformationLearner::getNeighborFromTrainingSet(int neighborIdx){
+void TransformationLearner::seeNeighbor(const int neighborIdx)
+{
     Vec v;
     real w;
-    train_set-&gt;getExample(neighborIdx,neighbor,v,w);
+    train_set-&gt;getExample(neighborIdx, neighbor,v,w);
+}
+
+
+//! GENERATE GAMMA RANDOM VARIABLES
+
+//!source of the algorithm: <A HREF="http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf">http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf</A>
     
-    //TO TEST : OK
+
+//!returns a pseudo-random positive real number x  
+//!using the distribution p(x)=Gamma(alpha,beta)
+real TransformationLearner::gamma_sample(real alpha, real beta)
+{
+  real c,x,u,d,v;
+  do{
+      c = 1.0/3.0;
+      x = random_gen-&gt;gaussian_01();
+      u = random_gen-&gt;uniform_sample();
+      d = alpha - c ;
+      v = pow((1 + x/(pow(9*d , 0.5)))  ,3.0);
+  }
+  while(pl_log(u) &lt; 0.5*pow(x,2) + d - d*v + d*pl_log(v));
+  return d*v/beta;   
 }
 
 
-//OPERATIONS RELATED TO GENERATION WEIGHTS
 
-//normalizes the generation weights related to a given target. 
-void TransformationLearner::normalizeTargetGenerationWeights(int targetIdx, 
-                                                             real totalWeight){
+
+//! GENERATE DIRICHLET RANDOM VARIABLES
+
+
+ //!source of the algorithm: WIKIPEDIA
+    
+
+//!returns a pseudo-random positive real vector x 
+//!using the distribution p(x) = Dirichlet(x| all the parameters = alpha)
+//!-all the element of the vector are between 0 and 1,
+//!-the elements of the vector sum to 1
+void TransformationLearner::dirichlet_sample(real alpha, Vec &amp; sample){
+    int d = sample.length();
+    real sum = 0;
+    for(int i=0;i&lt;d;i++){
+        sample[i]=gamma_sample(alpha);
+        sum += sample[i];
+    }
+    for(int i=0;i&lt;d;i++){
+        sample[i]/=sum;
+    }
+}
+Vec TransformationLearner::return_dirichlet_sample(real alpha)
+{
+    Vec sample ;
+    sample.resize(inputSpaceDim);
+    dirichlet_sample(alpha, sample);
+    return sample;
+}
+
+
+
+/*void TransformationLearner::dirichlet_sample(const Vec &amp; alphas,
+                                        Vec &amp; samples)
+{
+    //TODO
+}
+Vec TransformationLearner::return_dirichlet_sample(Vec alphas)
+{
+    //TODO
+    return Vec();
+}
+*/
+
+
+
+//! OPERATIONS ON WEIGHTS
+
+
+//!normalizes the reconstruction weights related to a given target.
+void TransformationLearner::normalizeTargetWeights(int targetIdx,
+                                                   real totalWeight)const
+{
     real w;
-    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
-    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
+    int startIdx = targetIdx * nbTargetReconstructions;
+    int endIdx = startIdx + nbTargetReconstructions;
     for(int candidateIdx =startIdx; candidateIdx&lt;endIdx; candidateIdx++){
-        w = generationSet[candidateIdx].weight;
-        generationSet[candidateIdx].weight =  DIV_weights(w,totalWeight);
+        w = reconstructionSet[candidateIdx].weight;
+        reconstructionSet[candidateIdx].weight =  DIV_weights(w,totalWeight);
     }
+}
 
-    //TO TEST : OK
+//!returns a random weight 
+real TransformationLearner::randomWeight()const
+{  
+    real w = random_gen-&gt;uniform_sample();
+    return INIT_weight((w + minimumProba)/(1.0 + minimumProba));
 }
-    
-//returns a random positive weight 
-real TransformationLearner::randomPositiveGenerationWeight(){
-    return  random_gen-&gt;uniform_sample() + epsilonInitWeight;
 
-    //TO TEST : OK
+//!arithmetic operations on  reconstruction weights : CONSTRUCTOR
+//!proba-&gt;weight
+real TransformationLearner::INIT_weight(real initValue)const
+{
+    return pl_log(initValue);
 }
-  
 
-//arithmetic operations on  generation weights
-real TransformationLearner::DIV_weights(real numWeight,    //DIVISION
-                                        real denomWeight){ 
-    return numWeight/denomWeight;
+//!arithmetic operations on  reconstruction weights :GET CORRESPONDING PROBABILITY 
+//! weight-&gt;proba
+real TransformationLearner::PROBA_weight(real weight)const
+{
+    return exp(weight); 
+}
 
-    //TO TEST : OK
+//!arithmetic operations on  reconstruction weights : DIVISION
+//! In our particular case:
+//!  numWeight = log(w1)
+//!  denomWeight = log(w2)
+//! and we want weight = log(w1/w2) = log(w1) - log(w2) 
+//!                             = numweight - denomWeight
+real TransformationLearner::DIV_weights(real numWeight,
+                                        real denomWeight)const
+{
+    return numWeight - denomWeight;
 }
-real TransformationLearner::MULT_INVERSE_weight(real weight){//MULTIPLICATIVE INVERSE
-    return -weight;
 
-    //TO TEST : OK
+
+//!arithmetic operations on  reconstruction weights :MULTIPLICATIVE INVERSE
+//! weight = log(p)
+//!we want : weight' = log(1/p) = log(1) - log(p)
+//!                             =     0 -  log(p)
+//!                             = -weight
+real TransformationLearner::MULT_INVERSE_weight(real weight)const
+{
+    
+    return -1*weight;
 }
-real TransformationLearner::MULT_weights(real weight1, real weight2){ //MULTIPLICATION
-    return weight1*weight2;
+
+//!arithmetic operations on  reconstruction weights: MULTIPLICATION
+//! weight1 = log(p1)
+//! weight2 = log(p2)
+//! we want weight3 = log(p1*p2) = log(p1) + log(p2)
+//!                              = weight1 + weight2
+real TransformationLearner::MULT_weights(real weight1,real weight2)const
+{
     
-    //TO TEST : OK
+    return weight1 + weight2 ;
 }
-real TransformationLearner::SUM_weights(real weight1, real weight2){ //SUM
-    return weight1 + weight2;
 
-    //TO TEST : OK
-} 
-
-//TO TEST
-//update/compute the weight of a generation candidate with
-//the actual transformation parameters
-real TransformationLearner::updateGenerationWeight(int candidateIdx){
+//!arithmetic operations on  reconstruction weights : SUM 
+//! weight1 = log(p1)
+//! weight2 = log(p2)
+//! we want : weight3 = log(p1 + p2) = logAdd(weight1, weight2)
+real TransformationLearner::SUM_weights(real weight1,
+                                        real weight2)const
+{
     
-    GenerationCandidate * gc = &amp; generationSet[candidateIdx];
-    
-    real w = computeGenerationWeight(gc-&gt;targetIdx,
-                                     gc-&gt;neighborIdx,
-                                     gc-&gt;transformIdx);
-    pout &lt;&lt; &quot;weigth:&quot;&lt;&lt; w &lt;&lt;endl;
-    gc-&gt;weight = w;
-    return w;
+    return logadd(weight1,weight2);
 }
 
-//TO TEST
-real TransformationLearner::computeGenerationWeight(GenerationCandidate &amp; gc){
-    return computeGenerationWeight(gc.targetIdx,
-                                   gc.neighborIdx,
-                                   gc.transformIdx);
 
-   
+
+//!update/compute the weight of a reconstruction candidate with
+//!the actual transformation parameters
+real TransformationLearner::updateReconstructionWeight(int candidateIdx)
+{
+    int targetIdx = reconstructionSet[candidateIdx].targetIdx;
+    int neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
+    int transformIdx = reconstructionSet[candidateIdx].transformIdx;
+    
+    real w = computeReconstructionWeight(targetIdx,
+                                         neighborIdx,
+                                         transformIdx);
+    reconstructionSet[candidateIdx].weight = w;
+    return w; 
 }
-
-//TO TEST
-real TransformationLearner::computeGenerationWeight(int targetIdx, 
-                                                    int neighborIdx, 
-                                                    int transformIdx){
-  
-
-    getTargetFromTrainingSet(targetIdx);
-    getNeighborFromTrainingSet(neighborIdx);
+real TransformationLearner::computeReconstructionWeight(const ReconstructionCandidate &amp; gc)
+{
+    return computeReconstructionWeight(gc.targetIdx,
+                                       gc.neighborIdx,
+                                       gc.transformIdx);
+}
+real TransformationLearner::computeReconstructionWeight(int targetIdx,
+                                                        int neighborIdx,
+                                                        int transformIdx)
+{
+    seeTarget(targetIdx);
+    return computeReconstructionWeight(target,
+                                       neighborIdx,
+                                       transformIdx);
+}
+real TransformationLearner::computeReconstructionWeight(const Vec &amp; target_,
+                                                        int neighborIdx,
+                                                        int transformIdx)
+{
+    seeNeighbor(neighborIdx);
     Vec predictedTarget ;
     predictedTarget.resize(inputSpaceDim);
     applyTransformationOn(transformIdx, neighbor, predictedTarget);
-    return exp(noiseVarianceFactor * powdistance(target, predictedTarget));
-
-    
+    real factor = -1/(2*noiseVariance);
+    real w = factor*powdistance(target_, predictedTarget);
+    return MULT_weights(w, transformDistribution[transformIdx]); 
 }
 
-
-//applies &quot;transformIdx&quot;th transformation on data point &quot;src&quot;
-void TransformationLearner::applyTransformationOn(int transformIdx, Vec &amp; src, Vec &amp;  dst){
-    if(transformFamily == TRANSFORM_FAMILY_LINEAR){
+//!applies &quot;transformIdx&quot;th transformation on data point &quot;src&quot;
+void TransformationLearner::applyTransformationOn(int transformIdx,
+                                                 const Vec &amp; src,
+                                                 Vec &amp; dst)const
+{
+    if(transformFamily==TRANSFORM_FAMILY_LINEAR){
         Mat m  = transforms[transformIdx];
-        product(dst,m,src);
+        transposeProduct(dst,m,src); 
+        if(withBias){
+            dst += biasSet(transformIdx);
+        }
     }
-    else{
+    else{ //transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT
         Mat m = transforms[transformIdx];
-        product(dst, m, src);
+        transposeProduct(dst,m,src);
         dst += src;
+        if(withBias){
+            dst += biasSet(transformIdx);
+        }
     }
 }
 
-//-------- INITIAL E STEP --------------------------------------------
+//! verify if the multinomial distribution given is well-defined
+//! i.e. verify that the weights represent probabilities, and that 
+//! those probabilities sum to 1 . 
+//!(typical case: the distribution is represented as a set of weights, which are typically
+//! log-probabilities)
+bool  TransformationLearner::isWellDefined(Vec &amp; distribution)
+{  
+    if(nbTransforms != distribution.length()){
+        return false;
+    }
+    real sum = 0;
+    real proba;
+    for(int i=0; i&lt;nbTransforms;i++){
+        proba = PROBA_weight(distribution[i]);
+        if(proba &lt; 0 || proba &gt; 1){
+            return false;
+        }
+        sum += proba;
+    }
+    return sum == 1;    
+}
 
-//initialization of the generation set 
+
+//! INITIAL E STEP
+
+//!initialization of the reconstruction set
 void TransformationLearner::initEStep(){
+    if(initializationMode == INIT_MODE_DEFAULT){
+        initEStepA();
+    }
+    else
+        initEStepB();
+}
+
+//!initialization of the reconstruction set, version B
+//!we suppose that all the parameters to learn are already initialized to some value
+//1)for each target,
+//  a)find the neighbors
+//  b)for each neighbor, consider all the possible transformations
+//2)compute the weights of all the reconstruction candidates using 
+//  the current value of the parameters to learn 
+void TransformationLearner::initEStepB(){
+    initEStepA();
+    smallEStep();    
+}
+
+
+//!initialization of the reconstruction set, version A
+//for each target:
+//1)find the neighbors (we use euclidean distance as an heuristic)
+//2)for each neighbor, assign a random weight to each possible transformation
+void TransformationLearner::initEStepA()
+{
+   
     priority_queue&lt; pair&lt; real,int &gt; &gt; pq = priority_queue&lt; pair&lt; real,int &gt; &gt;();
     
     real totalWeight;
     int candidateIdx=0,targetStartIdx, neighborIdx;
     
     //for each point in the training set i.e. for each target point,
-    for(int targetIdx = 0; targetIdx &lt; nbTrainingInput ;targetIdx++){
-    
+    for(int targetIdx = 0; targetIdx &lt; trainingSetLength ;targetIdx++){
+        
         //finds the nearest neighbors and keep them in a priority queue 
         findNearestNeighbors(targetIdx, pq);
         
@@ -943,67 +1693,70 @@
         //(i.e. for each neighbor, creates one entry per transformation and 
         //assignsit a positive random weight)
         
-        totalWeight =0;
+        totalWeight = INIT_weight(0);
         targetStartIdx = candidateIdx;
         for(int k = 0; k &lt; nbNeighbors; k++){
             neighborIdx = pq.top().second;
             pq.pop();
-            totalWeight =SUM_weights(expandTargetNeighborPairInGenerationSet(targetIdx, 
-                                                                             neighborIdx,
-                                                                             candidateIdx),
-                                     totalWeight);
+            totalWeight =
+                SUM_weights(totalWeight,
+                            expandTargetNeighborPairInReconstructionSet(targetIdx, 
+                                                                        neighborIdx,
+                                                                        candidateIdx));
             candidateIdx += nbTransforms;
         }
         //normalizes the  weights of all the entries created for the target 
         //point
-        normalizeTargetGenerationWeights(targetIdx,totalWeight);
+        normalizeTargetWeights(targetIdx,totalWeight);
     }
 
-    //TO TEST
 }
-    
-//auxialiary function of &quot;initEStep&quot; . 
-//    for a given pair (target, neighbor), creates all the associated 
-//    generation candidates (entries) in the data set. 
-//returns the total weight of the generation candidates created
-real TransformationLearner::expandTargetNeighborPairInGenerationSet(int targetIdx,
-                                                                    int neighborIdx,
-                                                                    int candidateIdx){
-    real weight, totalWeight = 0;  
+
+
+//!auxialiary function of &quot;initEStep&quot; . 
+//!    for a given pair (target, neighbor), creates all the  
+//!    possible reconstruction candidates. 
+//!returns the total weight of the reconstruction candidates created
+real TransformationLearner::expandTargetNeighborPairInReconstructionSet(int targetIdx,
+                                                                        int neighborIdx,
+                                                                        int candidateStartIdx)
+{
+    int candidateIdx = candidateStartIdx;
+    real weight, totalWeight = INIT_weight(0);  
     for(int transformIdx=0; transformIdx&lt;nbTransforms; transformIdx ++){
-        //choose a random positive weight
-        weight = randomPositiveGenerationWeight(); 
-        totalWeight = SUM_weights(weight,totalWeight);
-        generationSet[candidateIdx] = GenerationCandidate(targetIdx, 
-                                                          neighborIdx,
-                                                          transformIdx,
-                                                          weight);
+       
+        weight = randomWeight(); 
+        totalWeight = SUM_weights(totalWeight,weight);
+        reconstructionSet[candidateIdx] = ReconstructionCandidate(targetIdx, 
+                                                                  neighborIdx,
+                                                                  transformIdx,
+                                                                  weight);
     
         candidateIdx ++;
     }
     return totalWeight;    
-
-    //TO TEST
 }
 
-//auxiliary function of initEStep
-//    keeps the nearest neighbors for a given target point in a priority
-//    queue.
-void TransformationLearner::findNearestNeighbors (int targetIdx,
-                                                  priority_queue&lt; pair&lt; real, int &gt; &gt; &amp; pq){
+
+//!auxiliary function of initEStep
+//!    stores the nearest neighbors for a given target point in a priority
+//!    queue.
+void TransformationLearner::findNearestNeighbors(int targetIdx,
+                                                 priority_queue&lt; pair&lt; real, int &gt; &gt; &amp; pq)
+{
     
     //we want an empty queue
     PLASSERT(pq.empty()); 
   
     //capture the target from his index in the training set
-    getTargetFromTrainingSet(targetIdx);
+    seeTarget(targetIdx);
      
     //for each potential neighbor,
     real dist;    
-    for(int i=0; i&lt;nbTrainingInput; i++){
+    for(int i=0; i&lt;trainingSetLength; i++){
         if(i != targetIdx){ //(the target cannot be his own neighbor)
             //computes the distance to the target
-            getNeighborFromTrainingSet(i);
+            seeNeighbor(i);
             dist = powdistance(target, neighbor); 
             //if the distance is among &quot;nbNeighbors&quot; smallest distances seen,
             //keep it until to see a closer neighbor. 
@@ -1014,34 +1767,53 @@
                 pq.pop();
                 pq.push(pair&lt;real,int&gt;(dist,i));
             }
+            else if(dist == pq.top().first){
+                if(random_gen-&gt;uniform_sample() &gt;0.5){
+                    pq.pop();
+                    pq.push(pair&lt;real,int&gt;(dist,i));
+                }
+            }
         }
     }    
+}
 
-    //TO TEST
+//! ESTEP 
+
+//!coordination of the different kinds of expectation steps
+//!  -which are : largeEStepA, largeEStepB, smallEStep
+void TransformationLearner::EStep()
+{
+    if(largeEStepAPeriod &gt; 0  &amp;&amp; stage % largeEStepAPeriod == largeEStepAOffset){
+        largeEStepA();
+    }
+    if(largeEStepBPeriod&gt;0 &amp;&amp; stage % largeEStepBPeriod == largeEStepBOffset){
+        largeEStepB();
+    }
+    smallEStep(); 
 }
 
 
-//-------- LARGE E STEP : VERSION A --------------------------------
+//! LARGE E STEP : VERSION A (expectation step)
 
-//full update of the generation set
-//for each target, keeps the top km most probable &lt;neighbor, transformation&gt; 
-//pairs (k = nb neighbors, m= nb transformations)
-void TransformationLearner::largeEStepA(){
-    
-    priority_queue&lt; GenerationCandidate &gt; pq =  priority_queue&lt; GenerationCandidate &gt;();
-    real totalWeight=0;
+//!full update of the reconstruction set
+//!for each target, keeps the km most probable &lt;neighbor, transformation&gt; 
+//!pairs (k = nb neighbors, m= nb transformations)
+void TransformationLearner::largeEStepA()
+{
+    priority_queue&lt; ReconstructionCandidate &gt; pq =  
+        priority_queue&lt; ReconstructionCandidate &gt;();
+    real totalWeight= INIT_weight(0);
     int candidateIdx=0;
     
     //for each point in the training set i.e. for each target point,
-    for(int targetIdx = 0; targetIdx &lt; nbTrainingInput ; targetIdx++){
+    for(int targetIdx = 0; targetIdx &lt; trainingSetLength ; targetIdx++){
         
         //finds the best weighted triples and keep them in a priority queue 
-        findBestTargetCandidates(targetIdx, pq);
-        
+        findBestTargetReconstructionCandidates(targetIdx, pq);
         //store those triples in the dataset:
-        totalWeight = 0;
-        for(int k=0; k &lt; nbGenerationCandidatesPerTarget; k++){
-            generationSet[candidateIdx] = pq.top(); 
+        totalWeight = INIT_weight(0);
+        for(int k=0; k &lt; nbTargetReconstructions; k++){
+            reconstructionSet[candidateIdx] = pq.top(); 
             totalWeight = SUM_weights(pq.top().weight, totalWeight);
             pq.pop();         
             candidateIdx ++;
@@ -1049,203 +1821,328 @@
         
         //normalizes the  weights of all the entries created for the 
         //target point;
-        normalizeTargetGenerationWeights(targetIdx,totalWeight);
-    }
-
-    //TO TEST
+        normalizeTargetWeights(targetIdx,totalWeight);
+    } 
 }
 
-//auxiliary function of largeEStepA()
-//   for a given target, keeps the top km most probable neighbors,
-//   transformation pairs in a priority queue 
-//   (k = nb neighbors, m = nb transformations)
-void  TransformationLearner::findBestTargetCandidates
-(int targetIdx,
- priority_queue&lt; GenerationCandidate &gt; &amp; pq){
-    
+
+//!auxiliary function of largeEStepA()
+//!   for a given target, stores the km most probable (neighbors,
+//!   transformation) pairs in a priority queue 
+//!   (k = nb neighbors, m = nb transformations)
+void TransformationLearner::findBestTargetReconstructionCandidates(int targetIdx,
+                                                                   priority_queue&lt; ReconstructionCandidate &gt; &amp; pq)
+{
     //we want an empty queue
     PLASSERT(pq.empty()); 
+    
     real weight;
 
     //for each potential neighbor
-    for(int neighborIdx=0; neighborIdx&lt;nbTrainingInput; neighborIdx++){
+    for(int neighborIdx=0; neighborIdx&lt;trainingSetLength; neighborIdx++){
         if(neighborIdx != targetIdx){
             for(int transformIdx=0; transformIdx&lt;nbTransforms; transformIdx++){
-                weight = computeGenerationWeight(targetIdx, 
-                                                 neighborIdx, 
-                                                 transformIdx);
+                weight = computeReconstructionWeight(targetIdx, 
+                                                     neighborIdx, 
+                                                     transformIdx);
                 
                 //if the weight is among &quot;nbEntries&quot; biggest weight seen,
                 //keep it until to see a bigger neighbor. 
-                if(int(pq.size()) &lt; nbGenerationCandidatesPerTarget){
-                    pq.push(GenerationCandidate(targetIdx,
-                                                neighborIdx,
-                                                transformIdx,
-                                                weight));  
+                if(int(pq.size()) &lt; nbTargetReconstructions){
+                    pq.push(ReconstructionCandidate(targetIdx,
+                                                    neighborIdx,
+                                                    transformIdx,
+                                                    weight));  
                 }
-                else if (weight &gt; pq.top().weight){
+                else if (weight &gt; pq.top().weight){ 
                     pq.pop();
-                    pq.push(GenerationCandidate(targetIdx,
-                                                neighborIdx,
-                                                transformIdx,
-                                                weight));
+                    pq.push(ReconstructionCandidate(targetIdx,
+                                                    neighborIdx,
+                                                    transformIdx,
+                                                    weight));
                 }
+                else if (weight == pq.top().weight){
+                    if(random_gen-&gt;uniform_sample()&gt;0.5){
+                        pq.pop();
+                        pq.push(ReconstructionCandidate(targetIdx,
+                                                        neighborIdx,
+                                                        transformIdx,
+                                                        weight));
+                    }
+                }
             }
         }     
     }
-
-    //TO TEST
 }
 
 
-//-------- LARGE E STEP : VERSION B --------------------------------
 
-//full update of the generation set
-//   for each given pair (target, transformation), find the best
-//   weighted neighbors  
-void  TransformationLearner::largeEStepB(){
-    priority_queue&lt; GenerationCandidate &gt; pq;
+//! LARGE E STEP : VERSION B (expectation step)
+
+
+//!full update of the reconstruction set
+//!   for each given pair (target, transformation), find the best
+//!   weighted neighbors  
+void TransformationLearner::largeEStepB()
+{
+    priority_queue&lt; ReconstructionCandidate &gt; pq;
     
-  real totalWeight=0 , weight;
-  int candidateIdx=0 ;
-  
-  //for each point in the training set i.e. for each target point,
-  for(int targetIdx =0; targetIdx&lt;nbTrainingInput ;targetIdx++){
-  
-      totalWeight = 0;
-      for(int transformIdx=0; transformIdx &lt; nbTransforms; transformIdx ++){
-          //finds the best weighted triples   them in a priority queue 
-          findBestWeightedNeighbors(targetIdx,transformIdx, pq);
-          
-          //store those neighbors in the dataset
-          for(int k=0; k&lt;nbNeighbors; k++){
-              generationSet[candidateIdx] = pq.top();
-              weight = pq.top().weight;
-              totalWeight = SUM_weights( weight, totalWeight);
-              pq.pop();
-              candidateIdx ++;
-          }
-      }
+    real totalWeight , weight;
+    int candidateIdx=0 ;
+    
+    //for each point in the training set i.e. for each target point,
+    for(int targetIdx =0; targetIdx&lt;trainingSetLength ;targetIdx++){
+        
+        totalWeight = INIT_weight(0);
+        for(int transformIdx=0; transformIdx &lt; nbTransforms; transformIdx ++){
+            //finds the best weighted triples   them in a priority queue 
+            findBestWeightedNeighbors(targetIdx,transformIdx, pq);
+            //store those neighbors in the dataset
+            for(int k=0; k&lt;nbNeighbors; k++){
+                reconstructionSet[candidateIdx] = pq.top();
+                weight = pq.top().weight;
+                totalWeight = SUM_weights( weight, totalWeight);
+                pq.pop();
+                candidateIdx ++;
+            }
+        }
       //normalizes the  weights of all the entries created for the target 
       //point;
-      normalizeTargetGenerationWeights(targetIdx,totalWeight);
-  }
-
-  // TO TEST
+        normalizeTargetWeights(targetIdx,totalWeight);
+    }
 }
+   
 
-    
-//auxiliary function of largeEStepB()
-//   for a given target x and a given transformationt , keeps the best
-//   weighted triples (x, neighbor, t) in a priority queue .
-void  TransformationLearner::findBestWeightedNeighbors
-(int targetIdx,
- int transformIdx,
- priority_queue&lt; GenerationCandidate &gt; &amp; pq){
- 
+//!auxiliary function of largeEStepB()
+//!   for a given target x and a given transformation t , stores the best
+//!    weighted triples (x, neighbor, t) in a priority queue .
+void TransformationLearner::findBestWeightedNeighbors(int targetIdx,
+                                                      int transformIdx,
+                                                      priority_queue&lt; ReconstructionCandidate &gt; &amp; pq)
+{
     //we want an empty queue
     PLASSERT(pq.empty()); 
-
+    
     real weight; 
     
     //for each potential neighbor
-    for(int neighborIdx=0; neighborIdx&lt;nbTrainingInput; neighborIdx++){
+    for(int neighborIdx=0; neighborIdx&lt;trainingSetLength; neighborIdx++){
         if(neighborIdx != targetIdx){ //(the target cannot be his own neighbor)
-            
-	  weight=  computeGenerationWeight(targetIdx, 
-                                           neighborIdx, 
-                                           transformIdx);
-	  //if the weight of the triple is among the &quot;nbNeighbors&quot; biggest 
-	  //seen,keep it until see a bigger weight. 
+          
+            weight = computeReconstructionWeight(targetIdx, 
+                                                 neighborIdx, 
+                                                 transformIdx);
+            //if the weight of the triple is among the &quot;nbNeighbors&quot; biggest 
+            //seen,keep it until see a bigger weight. 
             if(int(pq.size()) &lt; nbNeighbors){
-                pq.push(GenerationCandidate(targetIdx,
-                                            neighborIdx, 
-                                            transformIdx,
-                                            weight));
+                pq.push(ReconstructionCandidate(targetIdx,
+                                                neighborIdx, 
+                                                transformIdx,
+                                                weight));
             }
-            else if (weight &gt;  pq.top().weight){
+            else if (weight &gt; pq.top().weight){
                 pq.pop();
-                pq.push(GenerationCandidate(targetIdx,
-                                            neighborIdx,
-                                            transformIdx,
-                                            weight));
+                pq.push(ReconstructionCandidate(targetIdx,
+                                                neighborIdx,
+                                                transformIdx,
+                                                weight));
             }
+            else if (weight == pq.top().weight){
+                if(random_gen-&gt;uniform_sample() &gt; 0.5){
+                    pq.pop();
+                    pq.push(ReconstructionCandidate(targetIdx,
+                                                    neighborIdx,
+                                                    transformIdx,
+                                                    weight));
+                }
+            }
         }
-    } 
-
-    //TO TEST
+    }   
 }
 
 
-//-------- SMALL E STEP --------------------------------------------- 
 
+//! SMALL E STEP (expectation step)
 
-//updating the weights while keeping the candidate neighbor set fixed
-void TransformationLearner::smallEStep(){
+
+//!updating the weights while keeping the candidate neighbor set fixed
+void TransformationLearner::smallEStep()
+{
+    int candidateIdx =0;
+    int  targetIdx = reconstructionSet[candidateIdx].targetIdx;
+    real totalWeight = INIT_weight(0);
     
-    int candidateIdx =0, startCandidateIdx=0;
-    int startTargetIdx = generationSet[startCandidateIdx].targetIdx;
-    int  targetIdx;
-    real totalWeight = 0;
-  
-    while(candidateIdx &lt; nbGenerationCandidates){
-    
-        totalWeight = SUM_weights(updateGenerationWeight(candidateIdx),
-                                  totalWeight);
+    while(candidateIdx &lt; nbReconstructions){
+        
+        totalWeight = SUM_weights(totalWeight,
+                                  updateReconstructionWeight(candidateIdx));
         candidateIdx ++;
     
-        targetIdx = generationSet[candidateIdx].targetIdx; 
-    
-        if(candidateIdx &gt; nbGenerationCandidates || targetIdx != startTargetIdx){
-            normalizeTargetGenerationWeights(startTargetIdx, totalWeight);
-            totalWeight = 0;
-            startTargetIdx = targetIdx;
+        if(candidateIdx == nbReconstructions)
+            normalizeTargetWeights(targetIdx,totalWeight);
+        else if(targetIdx != reconstructionSet[candidateIdx].targetIdx){
+            normalizeTargetWeights(targetIdx, totalWeight);
+            totalWeight = INIT_weight(0);
+            targetIdx = reconstructionSet[candidateIdx].targetIdx;
         }
     }    
+}
 
-    //TO TEST
+// M STEP
+
+
+//!coordination of the different kinds of maximization step
+//!(i.e.: we optimize with respect to which parameter?)
+void TransformationLearner::MStep()
+{
+    if(noiseVariancePeriod &gt; 0 &amp;&amp; stage%noiseVariancePeriod == noiseVarianceOffset)
+        MStepNoiseVariance();
+    if(transformDistributionPeriod &gt; 0 &amp;&amp; 
+       stage % transformDistributionPeriod == transformDistributionOffset)
+        MStepTransformDistribution();
+    if(stage % transformsPeriod == transformsOffset)
+        MStepTransformations();
+    
 }
+
+//!maximization step  with respect to  transformation distribution
+//!parameters
+void TransformationLearner::MStepTransformDistribution()
+{
+    MStepTransformDistributionMAP(transformDistributionAlpha);
+}
+
+//!maximization step  with respect to transformation distribution
+//!parameters
+//!(MAP version, alpha = dirichlet prior distribution parameter)
+//!NOTE :  alpha =1 -&gt;  no regularization
+void TransformationLearner::MStepTransformDistributionMAP(real alpha)
+{
+   
+    Vec newDistribution ;
+    newDistribution.resize(nbTransforms);
     
+    for(int k=0; k&lt;nbTransforms ; k++){
+        newDistribution[k] = INIT_weight(0);
+    }
+    
+    int transformIdx;
+    real weight;
+    for(int idx =0 ;idx &lt; nbReconstructions ; idx ++){
+        transformIdx = reconstructionSet[idx].transformIdx;
+        weight = reconstructionSet[idx].weight;
+        newDistribution[transformIdx] = 
+            SUM_weights(newDistribution[transformIdx],
+                        weight);
+    }
 
-//-------- M STEP ---------------------------------------------   
+    real addFactor = INIT_weight(alpha - 1);
+    real divisionFactor = INIT_weight(nbTransforms*(alpha - 1) + trainingSetLength); 
+
+    for(int k=0; k&lt;nbTransforms ; k++){
+        newDistribution[k]= DIV_weights(SUM_weights(addFactor,
+                                                    newDistribution[k]),
+                                        divisionFactor);
+    }
+    transformDistribution &lt;&lt; newDistribution ;
+}
+
+//!maximization step with respect to transformation parameters
+//!(MAP version)
+void TransformationLearner::MStepTransformations()
+{
     
-
-//updating the transformation parameters
-void TransformationLearner::MStep(){
     //set the m dXd matrices Ck and Bk , k in{1, ...,m} to 0.
     B_C.clear();
-  
-    for(int idx=0 ; idx&lt;nbGenerationCandidates ; idx++){
     
+    real lambda = 1.0*noiseVariance/transformsVariance;
+    Vec v;
+    for(int idx=0 ; idx&lt;nbReconstructions ; idx++){
+        
         //catch a view on the next entry of our dataset, that is, a  triple:
         //(target_idx, neighbor_idx, transformation_idx)
-        GenerationCandidate * gc = &amp;generationSet[idx];
         
-        real w = gc-&gt;weight;
+        real p = PROBA_weight(reconstructionSet[idx].weight);
   
         //catch the target and neighbor points from the training set
-        getTargetFromTrainingSet(gc-&gt;targetIdx);
-        getNeighborFromTrainingSet(gc-&gt;neighborIdx);
+        seeTarget(reconstructionSet[idx].targetIdx);
+        seeNeighbor(reconstructionSet[idx].neighborIdx);
         
-        int t = gc-&gt;transformIdx;
+        int t = reconstructionSet[idx].transformIdx;
         
-        externalProductScaleAcc(C[t], target, target, w);
-        if(transformFamily == TRANSFORM_FAMILY_LINEAR){
-            externalProductScaleAcc(B[t], target, neighbor,w);
+        v.resize(inputSpaceDim);
+        v &lt;&lt; target;
+        if(transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT){
+            v = v - neighbor;
         }
-        else
-            externalProductScaleAcc(B[t], target, (target - neighbor), w); 
+        if(withBias){
+            v = v - biasSet(t);
+        }
+        externalProductScaleAcc(C[t], neighbor, neighbor, p);
+        
+        externalProductScaleAcc(B[t], neighbor, v,p); 
     }
     for(int t=0; t&lt;nbTransforms; t++){
         addToDiagonal(C[t],lambda);
-        transforms[t] &lt;&lt; solveLinearSystem(C[t], B[t]);
+        transforms[t] &lt;&lt; solveLinearSystem(C[t], B[t]);  
+    }  
+}
+ 
+
+//!maximization step with respect to noise variance
+void TransformationLearner::MStepNoiseVariance()
+{
+    MStepNoiseVarianceMAP(noiseAlpha,noiseBeta);
+}
+
+//!maximization step with respect to noise variance
+//!(MAP version, alpha and beta = gamma prior distribution parameters)
+//!NOTE : alpha=1, beta=0 -&gt; no regularization
+void TransformationLearner::MStepNoiseVarianceMAP(real alpha, real beta)
+{
+    
+    Vec total_k;
+    total_k.resize(nbTransforms);
+    int transformIdx;
+    real proba;
+    for(int idx=0; idx &lt; nbReconstructions; idx++){
+        transformIdx = reconstructionSet[idx].transformIdx;
+        proba = PROBA_weight(reconstructionSet[idx].weight);
+        total_k[transformIdx]+=(proba * reconstructionEuclideanDistance(idx));
     }
+    noiseVariance = (2*beta + sum(total_k))/(2*alpha - 2 + trainingSetLength*inputSpaceDim);  
+}
+ 
+//!returns the distance between the reconstruction and the target
+//!for the 'candidateIdx'th reconstruction candidate
+real TransformationLearner::reconstructionEuclideanDistance(int candidateIdx){
+    seeTarget(reconstructionSet[candidateIdx].targetIdx);
+    seeNeighbor(reconstructionSet[candidateIdx].neighborIdx);
+    Vec reconstruction;
+    reconstruction.resize(inputSpaceDim);
+    applyTransformationOn(reconstructionSet[candidateIdx].transformIdx,
+                          neighbor,
+                          reconstruction);
+    return powdistance(target, reconstruction);
+}
 
-    //TO TEST
 
+//!STOPPING CRITERION
+
+
+//!stages == nstages?
+bool TransformationLearner::stoppingCriterionReached()
+{
+   
+    return stage==nstages;
 }
 
+//!increments the variable 'stage' of 1 
+void TransformationLearner::nextStage(){
+    stage ++;
+}
 
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-18 22:30:31 UTC (rev 7798)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-19 15:15:44 UTC (rev 7799)
@@ -2,8 +2,6 @@
 
 // TransformationLearner.h
 //
-//version 5 
-//
 // Copyright (C) 2007 Lysiane Bouchard
 //
 // Redistribution and use in source and binary forms, with or without
@@ -34,7 +32,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Lysiane Bouchard
+// Authors: Lysiane Bouchard, Pascal Vincent
 
 /*! \file TransformationLearner.h */
 
@@ -42,123 +40,259 @@
 #ifndef TransformationLearner_INC
 #define TransformationLearner_INC
 
-//C++
-#include &lt;utility&gt;
-#include &lt;queue&gt;
-
-//Plearn
-#include &lt;plearn_learners/generic/PLearner.h&gt;
+//PLEARN
+#include &lt;plearn_learners/distributions/PDistribution.h&gt; 
+#include &lt;plearn/math/plapack.h&gt;
+#include &lt;plearn/math/pl_math.h&gt;
+#include &lt;plearn_learners/distributions/PDistribution.h&gt;
 #include &lt;plearn/math/TMat_maths.h&gt;
 #include &lt;plearn/math/PRandom.h&gt;
 #include &lt;plearn/base/tuple.h&gt;
 
+//C++
+#include &lt;utility&gt;
+#include &lt;queue&gt;
+#include &lt;math.h&gt;
 
 #define TRANSFORM_FAMILY_LINEAR 0
 #define TRANSFORM_FAMILY_LINEAR_INCREMENT 1
+#define UNDEFINED -1
+#define INIT_MODE_DEFAULT 0
+#define INIT_MODE_RANDOM 1
+#define NOISE_ALPHA_NO_REG 1
+#define NOISE_BETA_NO_REG 0
+#define TRANSFORM_DISTRIBUTION_ALPHA_NO_REG 1
+#define BEHAVIOR_LEARNER 0
+#define BEHAVIOR_GENERATOR 1
 
+namespace PLearn {
 
 
-namespace PLearn {
 
 /**
- * The first sentence should be a BRIEF DESCRIPTION of what the class does.
- * Place the rest of the class programmer documentation here.  Doxygen supports
- * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
- *
- * @todo Write class to-do's here if there are any.
- *
- * @deprecated Write deprecated stuff here if there is any.  Indicate what else
- * should be used instead.
+description of the main class: TransformationLearner
+
+GENERATION PROCESS
+
+We suppose a new point v is obtained from a point x by :
+1) choosing a transformation t among a set of transformations T
+  (with probability p(t))
+2) applying the choosen transformation t on x
+3) add some noise in all the directions (noise = normally distributed random variable) 
+
+UNDERLYING PROBABILITY SPACE
+
+variables of the distribution : X : real vector
+                                V : real vector, neighbor of X
+                                T : transformation 
+P(x,v,t) = P(v is obtained by applying transformation t on x) 
+         = P(x,v|t)P(t)
+         = N( T(t)(x)|v,sigma)P(t)
+
+LEARNING BEHAVIOR
+ 
+The parameters of the distributions are learned using a variant
+of E.M.algorithm
+- learns  a finite set of transformations 
+- possibly:  learns the parameter sigma describing the noise distribution,
+             learns p(t), the transformation distribution.   
+
  */
 
-/***** GENERATION CANDIDATE ***********************************************/
 
 
-//Generate Candidate objects are basically 4-tuples with the following format: 
-//         nKm x4 matrix 
-//    ------C1----------|---- C2------------|-- C3-----------|-- C4-----------|
-//    index i  in the   | index j in the    | index t of a   | positive weight
-//    training set of a | training set of a | transformation | 
-//    target point      | a neighbour       |                |
-//                      | candidate         |                | 
 
-
-class GenerationCandidate
+/****************************************************************************
+ *AUXILIARY CLASS RECONSTRUCTION CANDIDATE : 
+ *
+ *
+ *
+ *Reconstruction Candidate objects are basically 4-tuples with the following format: 
+ *         nKm x4 matrix 
+ *    ------C1----------|---- C2------------|-- C3-----------|-- C4-----------|
+ *    index i  in the   | index j in the    | index t of a   | positive weight
+ *    training set of a | training set of a | transformation | 
+ *    target point      | a neighbour       |                |
+ *                      | candidate         |                | 
+ *
+ ***************************************************************************/ 
+class ReconstructionCandidate
 {
 public:
     int targetIdx, neighborIdx, transformIdx;
     real weight;
-
-    GenerationCandidate(int targetIdx_=-1, int neighborIdx_=-1, int transformIdx_=-1, real weight_=0){
+    
+    ReconstructionCandidate(int targetIdx_=-1, int neighborIdx_=-1, int transformIdx_=-1, real weight_=0){
         targetIdx =  targetIdx_;
         neighborIdx =  neighborIdx_;
         transformIdx =  transformIdx_ ;
         weight =  weight_;            
     }
 };
-inline bool operator&lt;(const GenerationCandidate&amp; o1 , const GenerationCandidate&amp; o2)
+
+//Comparisons between ReconstructionCandidate objects 
+inline bool operator&lt;(const ReconstructionCandidate&amp; o1 ,
+                      const ReconstructionCandidate&amp; o2)
 {
-    // we need the inverse comparison for the priority queue
-    return o2.weight&gt;o1.weight;
+    //  Will be used in storage process, in a priority queue.
+    //  With the following  definitions, priority measure increases when weight
+    //  field decreases.
+    //  That is, we want to keep ReconstructionCandidate objects with lower 
+    //  weights on top of the priority queue 
+    return o2.weight&lt;o1.weight;
 }
-
-inline bool operator==(const GenerationCandidate&amp; o1, const GenerationCandidate&amp; o2)
+inline bool operator==(const ReconstructionCandidate&amp; o1,
+                       const ReconstructionCandidate&amp; o2)
 {
-        return o1.weight==o2.weight;
+    return o1.weight==o2.weight;
 }
 
 
+//print/read ReconstructionCandidate objects
+inline PStream&amp; operator&lt;&lt;(PStream&amp; out, 
+                           const ReconstructionCandidate&amp; x)
+{
+    out &lt;&lt; tuple&lt;int, int, int, real&gt;(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight);
+    return out;
+}
+inline PStream&amp; operator&gt;&gt;(PStream&amp; in, ReconstructionCandidate&amp; x)
+{
+    tuple&lt;int, int, int, real&gt; t;
+    in &gt;&gt; t;
+    tie(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight) = t;
+    return in;
+}
 
-inline PStream&amp; operator&lt;&lt;(PStream&amp; out, const GenerationCandidate&amp; x)
-    {
-        out &lt;&lt; tuple&lt;int, int, int, real&gt;(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight);
-        return out;
-    }
 
-inline PStream&amp; operator&gt;&gt;(PStream&amp; in, GenerationCandidate&amp; x)
-    {
-        tuple&lt;int, int, int, real&gt; t;
-        in &gt;&gt; t;
-        tie(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight) = t;
-        return in;
-    }
 
-/********* END , GENERATION CANDIDATE *************************************/
-
-
-class TransformationLearner : public PLearner
+/***************************************************************************
+ * main class: TRANSFORMATION LEARNER
+ *
+ *
+ *
+ * Learns a finite set of linear transformations. That is, learns how to move from 
+ * one point to another.  
+ */
+class TransformationLearner : public PDistribution
 {
-    typedef PLearner inherited;
+    typedef PDistribution inherited;
 
 public:
     //#####  Public Build Options  ############################################
 
-    //! for random generators
-    long seed; 
+    //!A transformation learner might behave as a learner,as well as a generator
+    int behavior;
+
+
+    //!The following variable will be used to ensure p(x,v,t )&gt;0 
+    //!at the beginning (see implantation of randomReconstuctionWeight()
+    //!for more details) 
+    real minimumProba;
     
-    //! set the family of transformation functions we are interested in
-    int transformFamily ;
     
-    //HYPER-PARAMETERS OF THE ALGORITHM
+    //WHICH KIND OF TRANSFORMATION FUNCTIONS ... 
     
-    //!variance of the NOISE, considered here as a random variable normaly 
-    //!distributed, with mean 0
+    //! what is the global form of the transformation functions used?
+    int transformFamily;
+    //! add a bias to the transformation function ?
+    bool withBias;
+    
+    //LEARNING MODE ...
+
+    //!is the variance(precision) of the noise random variable learned or fixed ? 
+    //!(recall that the precision = 1/variance) 
+    bool learnNoiseVariance;
+   
+    //!if we learn the noise variance, do we use the MAP estimator ? 
+    bool regOnNoiseVariance;
+    
+    //!is the transformation distribution learned or fixed?
+    bool learnTransformDistribution;
+    
+    //!if we learn the transformation distribution, do we use the MAP estimator ?
+    bool regOnTransformDistribution;
+
+    //!how the initial values of the parameters to learn are choosen?
+    int initializationMode;
+
+    //!For a given training point, we do not consider all the possibilities for the hidden variables.
+    //!We approximate EM by using only the hidden variables with higher probability.
+    //!That is, for each point in the training set, we keep a fixed number of
+    //!hidden variables combinations, the most probable ones.
+    //!We call that selection &quot;large expection step&quot;.
+    //!There are 2 versions, A and B. The following variables tells us when to
+    //!perform each one. (see EStep() for more details)
+    int largeEStepAPeriod;
+    int largeEStepAOffset;
+    int largeEStepBPeriod;
+    int largeEStepBOffset;
+    
+    //!If the noise variance (precision) is learned, the following variables
+    //!tells us when to update the noise variance in the maximization steps:
+    //!(see MStep() for more details)
+    int noiseVariancePeriod;
+    int noiseVarianceOffset;                    
+    
+    //!These 2 parameters have to be defined if the noise variance is learned
+    //!using a MAP procedure.
+    //!We suppose that the prior distribution for the noise variance is a gamma
+    //!distribution with parameters alpha and beta:
+    //! p(x|alpha,beta)= x^(alpha-1)beta^(alpha)exp(-beta*x)/gamma(alpha)
+    //!Note : if alpha = 1, beta=0, all the possibilities are equiprobable 
+    //!      (no regularization effect)      
+    real noiseAlpha;
+    real noiseBeta;
+    
+    //!If the transformation distribution is learned, the following variables
+    //!tells us when to update it in the maximization steps:
+    //!(see MStep() for more details)
+    int transformDistributionPeriod;
+    int transformDistributionOffset;
+    
+    //!This parameter have to be defined if the transformation distribution
+    //!is learned using a MAP procedure. We suppose that this distribution have a a multinomial form
+    //(u1,u2,...,uK) with dirichlet prior probability : 
+    // p(u1,...,uK) = NormalisationCoeff(alpha)*u1^(alpha -1)*u2^(alpha -1)...*uK^(alpha  - 1)
+    //Note: if alpha = 1, it means all possibilities are equiprobable
+    //      (no regularization effect)  
+    real transformDistributionAlpha;
+
+
+    //!tells us when to update the transformation parameters
+    int transformsPeriod;
+    int transformsOffset;
+
+
+    //PARAMETERS OF THE DISTRIBUTION
+
+    //! variance of the NOISE random variable. 
+    //!(recall that this r.v. is normally distributed with mean 0).
+    //! -if it is a learned parameter, will be considered as the initial value 
+    //!  of the noise variance parameter.
+    //! -if it is not well defined (&lt;=0), it will be redefined using its
+    //!  prior distribution (Gamma).
     real noiseVariance;
     
-    //!variance on the transformation parameters. (prior distribution = normal with
-    //!mean 0).
+    //! variance on the transformation parameters (prior distribution = normal with mean 0)
     real transformsVariance;
-
-    //number of transformations
+    
+    //!number of transformations
     int nbTransforms;
     
-    //number of neighbors
+    //!number of neighbors
     int nbNeighbors;
+    
+    //!multinomial distribution for the transformation:
+    //!(i.e. probabilit of kth transformation = transformDistriibution[k])
+    //!(might be learned or fixed)
+    //!-if it is a learned parameter, will be considered as the initial value
+    //! or the transformation distribution
+    //!-if it is not well defined (size, positivity, sum to 1), it will be 
+    //! redefined using its prior distribution (Dirichlet).
+    Vec transformDistribution; 
+   
+    
 
-    //minimum random weight to give to a  chosen generation candidate at
-    //initialization step
-    real epsilonInitWeight;
-
 public:
     //#####  Public Member Functions  #########################################
 
@@ -168,135 +302,105 @@
     TransformationLearner();
 
 
-    //#####  PLearner Member Functions  #######################################
+    //#####  PDistribution Member Functions  ##################################
 
-    //! Returns the size of this learner's output, (which typically
-    //! may depend on its inputsize(), targetsize() and set options).
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual int outputsize() const;
+    //! Return log of probability density log(p(y | x)).
+    virtual real log_density(const Vec&amp; y) ;//const;
 
-    //! (Re-)initializes the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!).
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void forget();
+    //! Return survival function: P(Y&gt;y | x).
+    virtual real survival_fn(const Vec&amp; y) const;
 
-    //! The role of the train method is to bring the learner up to
-    //! stage==nstages, updating the train_stats collector with training costs
-    //! measured on-line in the process.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void train();
+    //! Return cdf: P(Y&lt;y | x).
+    virtual real cdf(const Vec&amp; y) const;
 
-    //! Computes the output from the input.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+    //! Return E[Y | x].
+    virtual void expectation(Vec&amp; mu) const;
 
-    //! Computes the costs from already computed output.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
-                                         const Vec&amp; target, Vec&amp; costs) const;
+    //! Return Var[Y | x].
+    virtual void variance(Mat&amp; cov) const;
 
-    //! Returns the names of the costs computed by computeCostsFromOutpus (and
-    //! thus the test method).
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+    //! Return a pseudo-random sample generated from the conditional
+    //! distribution, of density p(y | x).
+    virtual void generate(Vec&amp; y); //const ;
 
-    //! Returns the names of the objective costs that the train method computes
-    //! and  for which it updates the VecStatsCollector train_stats.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+    //### Override this method if you need it (and if your distribution can
+    //### handle it. Default version calls PLERROR.
+    //! Generates a pseudo-random sample x from the reversed conditional
+    //! distribution, of density p(x | y) (and NOT p(y | x)).
+    //! i.e., generates a &quot;predictor&quot; part given a &quot;predicted&quot; part, regardless
+    //! of any previously set predictor.
+    // virtual void generatePredictorGivenPredicted(Vec&amp; x, const Vec&amp; y);
 
+    //### Override this method if you need it. Default version calls
+    //### random_gen-&gt;manual_seed(g_seed) if g_seed !=0
+    //! Reset the random number generator used by generate() using the
+    //! given seed.
+    // virtual void resetGenerator(long g_seed) const;
 
-    // *** SUBCLASS WRITING: ***
-    // While in general not necessary, in case of particular needs
-    // (efficiency concerns for ex) you may also want to overload
-    // some of the following methods:
-    // virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
-    //                                    Vec&amp; output, Vec&amp; costs) const;
-    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
-    //                               Vec&amp; costs) const;
-    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
-    //                   VMat testoutputs=0, VMat testcosts=0) const;
-    // virtual int nTestCosts() const;
-    // virtual int nTrainCosts() const;
-    // virtual void resetInternalState();
-    // virtual bool isStatefulLearner() const;
+    //! Set the 'predictor' and 'predicted' sizes for this distribution.
+    //### See help in PDistribution.h.
+    virtual bool setPredictorPredictedSizes(int the_predictor_size,
+                                            int the_predicted_size,
+                                            bool call_parent = true);
 
+    //! Set the value for the predictor part of a conditional probability.
+    //### See help in PDistribution.h.
+    virtual void setPredictor(const Vec&amp; predictor, bool call_parent = true)
+                              const;
 
-    /*********GENERATION MODULE *****************************************/
+    // ### These methods may be overridden for efficiency purpose:
+    /*
+    //### Default version calls exp(log_density(y))
+    //! Return probability density p(y | x)
+    virtual real density(const Vec&amp; y) const;
 
-    //The object is the representation of a learned distribution
-    //Are are methods to ensure the &quot;generative behavior&quot; of the object
-    //(Once the distribution is learned, we might be able to generate
-    // samples from it)
+    //### Default version calls setPredictorPredictedSises(0,-1) and generate
+    //! Generates a pseudo-random sample (x,y) from the JOINT distribution,
+    //! of density p(x, y)
+    //! i.e., generates a predictor and a predicted part, regardless of any
+    //! previously set predictor.
+    virtual void generateJoint(Vec&amp; xy);
 
-    //Chooses the transformation parameters using a 
-    //normal distribution with mean 0 and variance &quot;transformsVariance&quot;
-    //(call generatorBuild() after)
-    void buildTransformationParametersNormal();
-   
-    //set the transformation parameters to the specified values
-    //(call generatorBuild() after)
-    void setTransformationParameters(TVec&lt;Mat&gt; &amp; transforms);
+    //### Default version calls generateJoint and discards y
+    //! Generates a pseudo-random sample x from the marginal distribution of
+    //! predictors, of density p(x),
+    //! i.e., generates a predictor part, regardless of any previously set
+    //! predictor.
+    virtual void generatePredictor(Vec&amp; x);
 
-    //creates a data set
-    //
-    //     Consists in building a tree of deepness d = &quot;nbGenerations&quot; and
-    //     constant branch factor n = &quot;generationLength&quot;
-    //
-    //            0      1        2     ...         
-    //  
-    //            r - child1  - child1  ...       
-    //                        - child2  ...
-    //                            ...   ...
-    //                        - childn  ...
-    //
-    //              - child2  - child1  ...
-    //                        - child2  ...
-    //                            ...   ...
-    //                        - childn  ...
-    //                     ...
-    //             - childn   - child1  ...
-    //                        - child2  ...
-    //                            ...   ...
-    //                        - childn  ... 
-    //
+    //### Default version calls generateJoint and discards x
+    //! Generates a pseudo-random sample y from the marginal distribution of
+    //! predicted parts, of density p(y) (and NOT p(y | x)).
+    //! i.e., generates a predicted part, regardless of any previously set
+    //! predictor.
+    virtual void generatePredicted(Vec&amp; y);
+    */
 
-   // all the childs are choosen following the same process:
-   // 1) choose a transformation  
-   // 2) apply the transformation to the parent
-   // 3) add noise to the result 
-    void createDataSet(Vec &amp; root,
-                       int nbGenerations,
-                       int generationLength,
-                       Mat &amp; dataPoints);
-    
-    //create a dataset using the same tree generation process as
-    //createDataSet, except the number of child per parent is fixed to 1,
-    //   root -&gt; 1st point -&gt; 2nd point ... -&gt; nth point 
-    void createDataSetSequentially(Vec &amp; root,
-                                   int n,
-                                   Mat &amp; dataPoints);
 
-    //Select a transformation randomly (with respect ot our transformation
-    //distribution)
-    int pickTransformIdx();
-    
-    
-    //here is the generation process for a given center data point 
-    //  1) choose a transformation
-    //  2) apply it on the center data point
-    //  3) add noise
+    //#####  PLearner Member Functions  #######################################
 
-    //generates a sample data point  from a  given center data point 
-    void generateFrom(Vec &amp; center, Vec &amp; sample);
-    //generates a sample data point from a given center data point
-    void generateFrom(Vec &amp; center, Vec &amp; sample, int transformIdx);
-    //fill the matrix &quot;samples&quot; with sample data points obtained from
-    // a given center data point.
-    void batchGenerateFrom( Vec &amp; center, Mat &amp; samples); 
-  
-   
+    // ### Default version of inputsize returns learner-&gt;inputsize()
+    // ### If this is not appropriate, you should uncomment this and define
+    // ### it properly in the .cc
+    virtual int inputsize() const;
+
+    /**
+     * (Re-)initializes the PDistribution in its fresh state (that state may
+     * depend on the 'seed' option).  And sets 'stage' back to 0 (this is the
+     * stage of a fresh learner!).
+     * ### You may remove this method if your distribution does not
+     * ### implement it.
+     */
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage == nstages, updating the train_stats collector with training
+    //! costs measured on-line in the process.
+    // ### You may remove this method if your distribution does not
+    // ### implement it.
+    virtual void train();
+
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -304,275 +408,452 @@
     // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
     PLEARN_DECLARE_OBJECT(TransformationLearner);
 
-    // Simply calls inherited::build() then build_()
-    virtual void build();
 
-    //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+    //!INITIAL VALUES OF  THE PARAMETERS TO LEARN
+    
+    //!initializes the transformation parameters randomly 
+    //!(prior distribution= Normal(0,transformsVariance))
+    void initTransformsParameters();
 
+    //!initializes the transformation parameters to the given values
+    //!(bias are set to 0)
+    void setTransformsParameters(TVec&lt;Mat&gt;  transforms, Mat bias=Mat());
+    
+    
+    //!initializes the noise variance randomly
+    //!(gamma distribution)
+    void initNoiseVariance();
+    
+    //!initializes the noise variance with the given value
+    void setNoiseVariance(real nv);
+    
+    //!initializes the transformation distribution randomly
+    //!(dirichlet distribution)
+    void initTransformDistribution();
+    
+    //!initializes the transformation distribution with the given values
+    void setTransformDistribution(Vec td);
+    
+    
+    //!GENERATION FUNCTIONS
+    
+    //!generates a sample data point from a source data point
+    void generatePredictedFrom(const Vec &amp; source, Vec &amp; sample)const;
+    
+    //!generates a sample data point from a source data point with a specific transformation
+    void generatePredictedFrom(const Vec &amp; source, Vec &amp; sample, int transformIdx)const;
 
-    //---EXTERIOR ACCESS ON LEARNED VARIABLES -------------------------------
+    //!generates a sample data point from a source data point and returns it
+    //! (if transformIdx &gt;= 0 , we use the corresponding transformation )
+    Vec returnPredictedFrom(Vec source, int transformIdx=-1);
     
-    //returns all the entries in the generation set with the 
-    //TARGET_IDX field fixed to &quot;targetIdx&quot;
-    //those entries represents ways to reproduct the 
-    //&quot;targetIdx&quot;th data point in the training set
-    TVec&lt;GenerationCandidate&gt; returnReproductionSources(int targetIdx); 
-  
-    //returns the reproductions of the &quot;targetIdx&quot;th data point in the
-    //training set
-    //(one reproduction by reproduction source)
-    Mat returnReproductions(int targetIdx);
 
-    //returns the parameter of the &quot;transformIdx&quot;th transformation
-    Mat returnTransform(int transformIdx);
+    //!fill the matrix &quot;samples&quot; with data points obtained from a given center data point
+    void batchGeneratePredictedFrom(const Vec &amp; center,
+                                     Mat &amp; samples)const;
+    
+    //!fill the matrix &quot;samples&quot; with data points obtained form a given center data point
+    //!    - we use a specific transformation
+    void batchGeneratePredictedFrom(const Vec &amp; center,
+                                     Mat &amp; samples,
+                                     int transformIdx)const ;
 
-    
-    //returns the paramter of each transformation
-    //(as an tdXd matrix, t = number of transformation,
-    //                    d = dimension of input space)
-    Mat returnAllTransforms();
-    
-    //Generates n samples from center and returns them
-    //    (generation process = 1) choose a transformation,
+    //Generates n samples from center and returns them stored in a matrix
+    //    (generation process = 1) choose a transformation (*),
     //                          2) apply it on center
     //                          3) add noise)
-    Mat returnGeneratedSamplesFrom(Vec center, int n);
+    // - (*) if transformIdx&gt;=0, we always use the corresponding transformation
+    Mat returnGeneratedSamplesFrom(Vec center, int n, int transformIdx=-1);
     
-    //Creates a data set and returns it
-    //(see createDataSet for more details on the generation process)
-    Mat returnGeneratedDataSet(Vec root,
-                               int nbGenerations,
-                               int generationLength);
+    
+    //!select a transformation randomly (with respect to our multinomial distribution)
+    int pickTransformIdx() const;
 
-    //Generates a data set and returns it
-    //(sequential generation process: see createDataSetSequentially for more details)
-    Mat returnSequentiallyGeneratedDataSet(Vec root,int n);
+    //!Select a neighbor in the training set randomly
+    //!(return his index in the training set)
+    //!We suppose all data points in the training set are equiprobables
+    int pickNeighborIdx() const;
 
+    //!creates a data set:
+    //!     equivalent in building a tree with fixed deepness and constant branching factor
+    //!
+    //!            0      1        2     ...         
+    //!  
+    //!            r -&gt; child1  -&gt; child1  ...       
+    //!                         -&gt; child2  ...
+    //!                             ...    ...
+    //!                         -&gt; childn  ...
+    //!
+    //!              -&gt; child2  -&gt; child1  ...
+    //!                         -&gt; child2  ...
+    //!                              ...   ...
+    //!                         -&gt; childn  ...
+    //!                      ...
+    //!              -&gt; childn  -&gt; child1  ...
+    //!                         -&gt; child2  ...
+    //!                              ...   ...
+    //!                         -&gt; childn  ... 
+    //!
+    //!(where &quot;a -&gt; b&quot; stands for &quot;a generate b&quot;)
+    //!all the child are generated by the same following process:
+    //! 1) choose a transformation  
+    //! 2) apply the transformation to the parent
+    //! 3) add noise to the result 
+    void treeDataSet(const Vec &amp;root,
+                     int deepness,
+                     int branchingFactor,
+                     Mat &amp; dataPoints);
+    Mat returnTreeDataSet(Vec root,
+                          int deepness,
+                          int branchingFactor);
+    
 
-protected:
-    //#####  Protected Options  ###############################################
+    //!create a &quot;sequential&quot; dataset:
+    //!  start -&gt; first point -&gt; second point ... -&gt;nth point
+    //! (where &quot;-&gt;&quot; stands for : &quot;generate the&quot;)
+    void sequenceDataSet(const Vec &amp; start,
+                         int n,
+                         Mat &amp; dataPoints);
 
-    // ### Declare protected option fields (such as learned parameters) here
-    // ...
+    Mat returnSequenceDataSet(Vec start,int n);
 
-public:    
-    
-    //DIMENSION VARIABLES 
   
-    //dimension of the input space;
-    int inputSpaceDim;
+
     
-    //number of generation candidates related to a specific target in the 
-    //generation set. 
-    int nbGenerationCandidatesPerTarget;
 
-    //total number of generation candidates in the generation set
-    int nbGenerationCandidates;
 
-    //number of samples given in the training set
-    int nbTrainingInput;
 
-    //multinomial probability ditribution for the transformations
-    //(i.e. probability of kth transformation = transformDistribution[k])
-    Vec transformDistribution;
-    
+    //!COPIES OF THE STRUCTURES
 
-    //LEARNED MODEL PARAMETERS
+    //!returns the &quot;idx&quot;th data point in the training set
+    Vec returnTrainingPoint(int idx);
 
-    //set of transformations:
-    //mdxd matrix :  -where m = number of transformation,
-    //                      d = dimensionality of the input space
-    //               -rows kd to kd + d (exclusively) = sub-matrix = parameters of the
-    //                                                               kth transformation
-    //                                                               (0&lt;=k&lt;m)
-    Mat transformsSet ; 
-    TVec&lt; Mat &gt; transforms; //views on sub-matrices of the matrix transformsSet 
+    //!returns all the reconstructions candidates associated to a given target
+    TVec&lt;ReconstructionCandidate&gt; returnReconstructionCandidates(int targetIdx);
+
+    //!returns the reconstructions of the &quot;targetIdx&quot;th data point value in the training set
+    //!(one reconstruction for each reconstruction candidate)
+    Mat returnReconstructions(int targetIdx);
+
+    //!returns the neighbors choosen to reconstruct the target
+    //!(one choosen neighbor for each reconstruction candidate associated to the target)
+    Mat returnNeighbors(int targetIdx);
+
+    //!returns the parameters of the &quot;transformIdx&quot;th transformation
+    Mat returnTransform(int transformIdx);
+
+    //!returns the parameters of each transformation
+    //!(as an KdXd matrix, K = number of transformations,
+    //!                    d = dimension of input space)
+    Mat returnAllTransforms();
+
+
+    //OTHER BUILDING/INITIALIZATION METHODS 
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
     
-    //a generationSet D : 
-    //implemented as a vector of &quot;GenerationCandidate&quot; objects.
-    TVec&lt; GenerationCandidate &gt; generationSet; 
+    //! initialization operations that have to be done before the training
+    void trainBuild();
+    
+    //! initialization operations that have to be done before a generation process
+    //! (all the undefined parameters will be initialized  randomly)
+    void generatorBuild(int inputSpaceDim_=2,
+                        TVec&lt;Mat&gt; transforms_ =TVec&lt;Mat&gt;(),  
+                        Mat biasSet_ =Mat(),  
+                        real noiseVariance_ =-1.0,
+                        Vec transformDistribution_ =Vec());
+    
+    
 
 
-    //OTHER VARIABLES
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
 
-    //the weight decay
-    real lambda;
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
     
 
-    //factor used in the computation of generation weights :
-    // 1/2(noiseVariance)y
-    real noiseVarianceFactor;
+    //TRANSFORMATIONS
+    
+    //!set of transformations:
+    //!mdxd matrix :  -m = number of transformation,
+    //!               -d = dimensionality of the input space
+    //!               -rows kd to kd + d (exclusively) = sub-matrix = parameters of the
+    //!                                                               kth transformation
+    //!                                                               (0&lt;=k&lt;m)
+    Mat transformsSet;
+    TVec&lt;Mat&gt; transforms; //!views on sub-matrices of the matrix transformsSet
+    
+    //!set of bias (one by transformation)
+    //!-might be used only if the flag &quot;withBias&quot; is turned on
+    Mat biasSet;
+    
+    //SELECTED HIDDEN VARIABLES COMBINATIONS
 
-    //standard deviation for the noise distribution, and transformation
-    //parameters distributions:
-    real noiseStDev,transformsStDev;
+    //!a reconstruction set:
+    //!-choosen hidden variables combinations for each point in the training set 
+    //!-implemented as a vector of &quot;ReconstructionCandidate&quot; objects.
+    TVec&lt; ReconstructionCandidate &gt; reconstructionSet; 
+    
+    
+    //DIMENSION VARIABLES
 
-    //will be used to store a view on the generation set:
-    //that is, all the entries related to a specific target . 
-    TVec&lt;GenerationCandidate&gt;  targetGenerationSet; 
+    //!dimension of the input space
+    int inputSpaceDim;
+
+    //!number of hidden variables combinations keeped for a specific target 
+    //!in the reconstruction set.
+    //!(Those combinations might be seen like reconstructions of the target)
+    int nbTargetReconstructions;
     
-    //Storage space that will be used to update the transformation
-    //parameters. It represents a set of sub-matrices. There are exactly 2 
-    //sub-matrices by transformation.   
-    Mat B_C ;
+    //!total number of combinations (x,v,t) keeped in the reconstruction set
+    int nbReconstructions;
+    
+    //!number of samples given in the training set
+    int trainingSetLength;
+    
+    //USEFUL CONSTANTS
+    
+    
+    //!standard deviations for the transformation parameters:
+    real transformsSD;
+    
 
-    //Vectors of matrices that will be used to update the transformation 
-    //parameters. Each matrix is a view on a sub-matrix in B_C. 
-    TVec&lt;Mat&gt; C , B ;
+    //OTHERS
 
-    //to retrieve easily an input point from the training set 
+    
+    //! Will be used to store a view on the reconstructionSet.
+    //! The view will consist in all the entries related to a specific target
+    TVec&lt;ReconstructionCandidate&gt; targetReconstructionSet;
+    
+    //!Storage space that will be used in the maximization step, in transformation parameters
+    //! updating process.
+    //!It represents a set of sub-matrices.There are exactly 2 sub-matrices by transformation.
+    Mat B_C;
+    //!Vectors of matrices that will be used in transformations parameters updating process.
+    //!Each matrix is a view on a sub-matrix in th bigger matrix &quot;B_C&quot; described above.
+    TVec&lt;Mat&gt; B,C;
+    
+    //!To get easily a view on an input point from the training set
     Vec target, neighbor;
-
     
-
+    
 protected:
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.
     // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList&amp; ol);
-
-
-    //! Declare the methods that are remote-callable
+    //! Declares the methods that are remote-callable
     static void declareMethods(RemoteMethodMap&amp; rmm);
 
-    //general building operations 
-    void build_();
-    
-    //do the building operations related to training
-    //warning: we suppose the training set has been transmitted
-    //         before calling the method
-    void trainInit();
-    //do the building operations related to the generation process
-    //warning: we suppose the transformation parameters are set 
-    void generatorInit();
 
 private:
     //#####  Private Member Functions  ########################################
 
     //! This does the actual building.
     // (PLEASE IMPLEMENT IN .cc)
- 
+    void build_();
 
-public:
 
-    // ----------GENERAL USE--------------------------------------------------
+    //!VIEWS ON RECONSTRUCTION SET AND TRAINING SET
     
-
-    //REFERENCE OPERATIONS ON GENERATION SET AND TRAINING SET  
-
-    // stores a view on the subset of generation set related to the specified
-    // target (into the variable &quot;targetGenerationSet&quot; )
-    void getViewOnTargetGenerationCandidates(int targetIdx);
-    // stores the &quot;targetIdx&quot;th input in the training set into the variable
+    //! stores a VIEW on the reconstruction candidates related to the specified
+    //! target (into the variable &quot;targetReconstructionSet&quot; )
+    void seeTargetReconstructionSet(int targetIdx) ;
+    // stores the &quot;targetIdx&quot;th point in the training set into the variable
     // &quot;target&quot;
-    void getTargetFromTrainingSet(int targetIdx);
+    void seeTarget(const int targetIdx) ;
     // stores the &quot;neighborIdx&quot;th input in the training set into the variable
     // &quot;neighbor&quot; 
-    void getNeighborFromTrainingSet(int neighborIdx);
+    void seeNeighbor(const int neighborIdx);
+
+
+    //!GENERATE GAMMA RANDOM VARIABLES
     
+    //!source of the algorithm: <A HREF="http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf">http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf</A>
     
-    //OPERATIONS RELATED TO GENERATION WEIGHTS
+    //!returns a pseudo-random positive real number x  
+    //!using the distribution p(x)=Gamma(alpha,beta)
+    real gamma_sample(real alpha,real beta=1);
     
-    //normalizes the generation weights related to a given target. 
-    void normalizeTargetGenerationWeights(int targetIdx, real totalWeight);
     
-    //returns a random positive weight 
-    real randomPositiveGenerationWeight();
-  
-    //arithmetic operations on  generation weights
-    real DIV_weights(real numWeight, real denomWeight); //DIVISION
-    real MULT_INVERSE_weight(real weight);//MULTIPLICATIVE INVERSE
-    real MULT_weights(real weight1, real weight2); //MULTIPLICATION
-    real SUM_weights(real weight1, real weight2); //SUM 
+    //!GENERATE DIRICHLET RANDOM VARIABLES
+    //!source of the algorithm: WIKIPEDIA
     
-    //update/compute the weight of a generation candidate with
-    //the actual transformation parameters
-    real updateGenerationWeight(int candidateIdx);
-    real computeGenerationWeight(GenerationCandidate &amp; gc);
-    real computeGenerationWeight(int targetIdx, 
-                                 int neighborIdx, 
-                                 int transformIdx);
+    //!returns a pseudo-random positive real vector x 
+    //!using the distribution p(x) = Dirichlet(x| all the parameters = alpha)
+    //!-all the element of the vector are between 0 and 1,
+    //!-the elements of the vector sum to 1
+    void dirichlet_sample(real alpha, Vec &amp; sample);
+    Vec return_dirichlet_sample(real alpha);
+
     
-    //applies &quot;transformIdx&quot;th transformation on data point &quot;src&quot;
-    void applyTransformationOn(int transformIdx, Vec &amp; src , Vec &amp; dst);
   
+    
 
-   //-------- INITIAL E STEP --------------------------------------------
+    //!OPERATIONS ON WEIGHTS 
+    
+     //!normalizes the reconstruction weights related to a given target. 
+    void normalizeTargetWeights(int targetIdx, real totalWeight) const;
+    
+    //!returns a random weight 
+    real randomWeight() const;
+    
+    //!arithmetic operations on  reconstruction weights
+    real INIT_weight(real initValue) const; //!CONSTRUCTOR
+    real PROBA_weight(real weight) const; //!GET CORRESPONDING PROBABILITY 
+    real DIV_weights(real numWeight, real denomWeight) const; //!DIVISION
+    real MULT_INVERSE_weight(real weight) const ;//!MULTIPLICATIVE INVERSE
+    real MULT_weights(real weight1, real weight2) const ; //!MULTIPLICATION
+    real SUM_weights(real weight1, real weight2) const ; //!SUM 
+    
+    //!update/compute the weight of a reconstruction candidate with
+    //!the actual transformation parameters
+    real updateReconstructionWeight(int candidateIdx);
+    real computeReconstructionWeight(const ReconstructionCandidate &amp; gc);
+    real computeReconstructionWeight(int targetIdx, 
+                                     int neighborIdx, 
+                                     int transformIdx);
+    real computeReconstructionWeight(const Vec &amp; target,
+                                     int neighborIdx,
+                                     int transformIdx);
 
-    //initialization of the generation set 
+    //!applies &quot;transformIdx&quot;th transformation on data point &quot;src&quot;
+    void applyTransformationOn(int transformIdx, const Vec &amp; src , Vec &amp; dst) const ;
+
+    
+    //! verify if the multinomial distribution given is well-defined
+    //! i.e. verify that the weights represent probabilities, and that 
+    //! those probabilities sum to 1 . 
+    //!(the distribution is represented as a set of weights, which are typically
+    //! log-probabilities)
+    bool isWellDefined(Vec &amp; distribution);
+
+    //!INITIAL E STEP 
+    
+    //!initialization of the reconstruction set
     void initEStep();
+
+    //!initialization of the reconstruction set, version A
+    //for each target:
+    // 1)find the neighbors (we use euclidean distance as an heuristic)
+    // 2)for each neighbor, assign a random weight to each possible transformation
+    void initEStepA();
+
+    //!initialization of the reconstruction set, version B
+
+    void initEStepB();
     
-    //auxialiary function of &quot;initEStep&quot; . 
-    //    for a given pair (target, neighbor), creates all the associated 
-    //    generation candidates (entries) in the data set. 
-    //returns the total weight of the generation candidates created
-    real expandTargetNeighborPairInGenerationSet(int targetIdx,
-                                                 int neighborIdx,
-                                                 int candidateStartIdx);
+
     
-    //auxiliary function of initEStep
-    //    keeps the nearest neighbors for a given target point in a priority
-    //    queue.
+    //!auxialiary function of &quot;initEStep&quot; . 
+    //!    for a given pair (target, neighbor), creates all the  
+    //!    possible reconstruction candidates. 
+    //!returns the total weight of the reconstruction candidates created
+    real expandTargetNeighborPairInReconstructionSet(int targetIdx,
+                                                     int neighborIdx,
+                                                     int candidateStartIdx);
+    
+    //!auxiliary function of initEStep
+    //!    stores the nearest neighbors for a given target point in a priority
+    //!    queue.
     void findNearestNeighbors(int targetIdx,
                               priority_queue&lt; pair&lt; real, int &gt; &gt; &amp; pq);
     
+    
+    //!E STEP
 
-    //-------- LARGE E STEP : VERSION A --------------------------------
+    //!coordination of the different kinds of expectation steps
+    //!  -which are : largeEStepA, largeEStepB, smallEStep
+    void EStep();
+    
+    //!LARGE E STEP : VERSION A (expectation step)
 
-    //full update of the generation set
-    //for each target, keeps the top km most probable &lt;neighbor, transformation&gt; 
-    //pairs (k = nb neighbors, m= nb transformations)
+    //!full update of the reconstruction set
+    //!for each target, keeps the km most probable &lt;neighbor, transformation&gt; 
+    //!pairs (k = nb neighbors, m= nb transformations)
     void largeEStepA();
 
-    //auxiliary function of largeEStepA()
-    //   for a given target, keeps the top km most probable neighbors,
-    //   transformation pairs in a priority queue 
-    //   (k = nb neighbors, m = nb transformations)
-    void findBestTargetCandidates
+    //!auxiliary function of largeEStepA()
+    //!   for a given target, stores the km most probable (neighbors,
+    //!   transformation) pairs in a priority queue 
+    //!   (k = nb neighbors, m = nb transformations)
+    void findBestTargetReconstructionCandidates
     (int targetIdx,
-     priority_queue&lt; GenerationCandidate &gt; &amp; pq);
+     priority_queue&lt; ReconstructionCandidate &gt; &amp; pq);
     
-
-    //-------- LARGE E STEP : VERSION B --------------------------------
-
-    //full update of the generation set
-    //   for each given pair (target, transformation), find the best
-    //   weighted neighbors  
+    
+    //!LARGE E STEP : VERSION B (expectation step)
+    
+    //!full update of the reconstruction set
+    //!   for each given pair (target, transformation), find the best
+    //!   weighted neighbors  
     void largeEStepB();
-
     
-    //auxiliary function of largeEStepB()
-    //   for a given target x and a given transformationt , keeps the best
-    //   weighted triples (x, neighbor, t) in a priority queue .
+    
+    //!auxiliary function of largeEStepB()
+    //!   for a given target x and a given transformation t , stores the best
+    //!    weighted triples (x, neighbor, t) in a priority queue .
     void findBestWeightedNeighbors
     (int targetIdx,
      int transformIdx,
-     priority_queue&lt; GenerationCandidate &gt; &amp; pq);
+     priority_queue&lt; ReconstructionCandidate &gt; &amp; pq);
 
+    //!SMALL E STEP (expectation step)
 
+    //!updating the weights while keeping the candidate neighbor set fixed
+    void smallEStep();
+   
+    //!M STEP
+    
+    //!coordination of the different kinds of maximization step
+    //!(i.e.: we optimize with respect to which parameter?)
+    void MStep();
 
-    //-------- SMALL E STEP --------------------------------------------- 
+    //!maximization step  with respect to  transformation distribution
+    //!parameters
+    void MStepTransformDistribution();
+    
+    //!maximization step  with respect to transformation distribution
+    //!parameters
+    //!(MAP version, alpha = dirichlet prior distribution parameter)
+    //!NOTE :  alpha =1 -&gt;  no regularization
+    void MStepTransformDistributionMAP(real alpha);
 
+    //!maximization step with respect to transformation parameters
+    //!(MAP version)
+    void MStepTransformations();
     
-    //updating the weights while keeping the candidate neighbor set fixed
-    void smallEStep();
+    //!maximization step with respect to noise variance
+    void MStepNoiseVariance();
     
-
-    //-------- M STEP ---------------------------------------------   
+    //!maximization step with respect to noise variance
+    //!(MAP version, alpha and beta = gamma prior distribution parameters)
+    //!NOTE : alpha=1, beta=0 -&gt; no regularization   
+    void MStepNoiseVarianceMAP(real alpha, real beta);    
     
+    //!returns the distance between the reconstruction and the target
+    //!for the 'candidateIdx'th reconstruction candidate
+    real reconstructionEuclideanDistance(int candidateIdx);
+    
+    
+    //STOPPING CRITERION
+    //stage == nstages?
+    bool stoppingCriterionReached();
+    
+    //increment the variable 'stage' of 1
+    void nextStage();
 
-    //updating the transformation parameters
-    void MStep();
-
-
-
 private:
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
+   
 };
 
 // Declares a few other classes and functions related to this class


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001246.html">[Plearn-commits] r7798 - in trunk/plearn: io python
</A></li>
	<LI>Next message: <A HREF="001248.html">[Plearn-commits] r7800 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1247">[ date ]</a>
              <a href="thread.html#1247">[ thread ]</a>
              <a href="subject.html#1247">[ subject ]</a>
              <a href="author.html#1247">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
