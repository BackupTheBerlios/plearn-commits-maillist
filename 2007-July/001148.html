<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7700 - trunk/plearn_learners/online/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7700%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200707052118.l65LIKSI030972%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001147.html">
   <LINK REL="Next"  HREF="001149.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7700 - trunk/plearn_learners/online/EXPERIMENTAL</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7700%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200707052118.l65LIKSI030972%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7700 - trunk/plearn_learners/online/EXPERIMENTAL">yoshua at mail.berlios.de
       </A><BR>
    <I>Thu Jul  5 23:18:20 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001147.html">[Plearn-commits] r7699 -	branches/cgi-desjardin/plearn_learners/second_iteration
</A></li>
        <LI>Next message: <A HREF="001149.html">[Plearn-commits] r7701 -	trunk/python_modules/plearn/learners/modulelearners/sampler/example/data
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1148">[ date ]</a>
              <a href="thread.html#1148">[ thread ]</a>
              <a href="subject.html#1148">[ subject ]</a>
              <a href="author.html#1148">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-07-05 23:18:19 +0200 (Thu, 05 Jul 2007)
New Revision: 7700

Added:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h
Log:
Clone of RBMModule which also computes and optimizes the KL(p0||p1) criterion


Added: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-05 20:56:06 UTC (rev 7699)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-05 21:18:19 UTC (rev 7700)
@@ -0,0 +1,1780 @@
+// -*- C++ -*-
+
+// KLp0p1RBMModule.cc
+//
+// Copyright (C) 2007 Olivier Delalleau, Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau, Yoshua Bengio
+
+/*! \file KLp0p1RBMModule.cc */
+
+
+
+#include &quot;KLp0p1RBMModule.h&quot;
+#include &lt;plearn/vmat/VMat.h&gt;
+#include &lt;plearn_learners/online/RBMMatrixConnection.h&gt;
+
+#define PL_LOG_MODULE_NAME &quot;KLp0p1RBMModule&quot;
+#include &lt;plearn/io/pl_log.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    KLp0p1RBMModule,
+    &quot;Implement KL(p0||p1) criterion for RBMs&quot;,
+    &quot;This criterion is described and justified in the paper by Le Roux and Bengio entitled&quot;
+    &quot;'Representational Power of Restricted Boltzmann Machines and Deep Belief Networks'.&quot;
+    &quot;The exact and very inefficient implementation of this criterion is done here.&quot;
+    &quot;For an example x the cost is:&quot;
+    &quot;  C(x) = - log P1(x) = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k)&quot;
+    &quot;where {x^1, ... x^n} is the training set of examples x^k, h is a hidden layer bit vector,&quot;
+    &quot;P(x|h) is the hidden-to-visible conditional distribution and P(h|x) is the&quot;
+    &quot;input-to-hidden conditional distribution. Both are the usual found in Binomial&quot;
+    &quot;layer RBMs here.&quot;
+    &quot;The gradient on the weight Wij is&quot;
+    &quot;  dC(x)/dWij = (1/(n P1(x))) &quot;
+    &quot;       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j|h)) + x_j^k(h_i - P(h_i|x^k)))&quot;
+    &quot;Apart from the KLp0p1 output port, and the fact that CD learning is replaced by minimization&quot;
+    &quot;of KLp0p1, this module acts like a regular RBMModule.&quot;
+);
+
+///////////////
+// KLp0p1RBMModule //
+///////////////
+KLp0p1RBMModule::KLp0p1RBMModule():
+    cd_learning_rate(0),
+    grad_learning_rate(0),
+    klp0p1_learning_rate(0),
+    compute_contrastive_divergence(false),
+    n_Gibbs_steps_CD(1),
+    min_n_Gibbs_steps(1),
+    n_Gibbs_steps_per_generated_sample(-1),
+    compute_log_likelihood(false),
+    minimize_log_likelihood(false),
+    Gibbs_step(0),
+    log_partition_function(0),
+    partition_function_is_stale(true),
+    standard_cd_grad(true),
+    standard_cd_bias_grad(true),
+    standard_cd_weights_grad(true),
+    hidden_bias(NULL),
+    weights(NULL),
+    hidden_act(NULL),
+    hidden_activations_are_computed(false)
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void KLp0p1RBMModule::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;training_set&quot;, &amp;KLp0p1RBMModule::training_set,
+                  OptionBase::buildoption,
+                  &quot;VMatrix with one input example per row, the training set.&quot;);
+
+    declareOption(ol, &quot;visible_layer&quot;, &amp;KLp0p1RBMModule::visible_layer,
+                  OptionBase::buildoption,
+        &quot;Visible layer of the RBM.&quot;);
+
+    declareOption(ol, &quot;hidden_layer&quot;, &amp;KLp0p1RBMModule::hidden_layer,
+                  OptionBase::buildoption,
+        &quot;Hidden layer of the RBM.&quot;);
+
+    declareOption(ol, &quot;connection&quot;, &amp;KLp0p1RBMModule::connection,
+                  OptionBase::buildoption,
+        &quot;Connection between the visible and hidden layers.&quot;);
+
+    declareOption(ol, &quot;reconstruction_connection&quot;, 
+                  &amp;KLp0p1RBMModule::reconstruction_connection,
+                  OptionBase::buildoption,
+        &quot;Reconstuction connection between the hidden and visible layers.&quot;);
+
+    declareOption(ol, &quot;grad_learning_rate&quot;, &amp;KLp0p1RBMModule::grad_learning_rate,
+                  OptionBase::buildoption,
+        &quot;Learning rate for the gradient descent step.&quot;);
+
+    declareOption(ol, &quot;cd_learning_rate&quot;, &amp;KLp0p1RBMModule::cd_learning_rate,
+                  OptionBase::buildoption,
+        &quot;Learning rate for the constrastive divergence step. Note that when\n&quot;
+        &quot;set to 0, the gradient of the contrastive divergence will not be\n&quot;
+        &quot;computed at all.&quot;);
+
+    declareOption(ol, &quot;klp0p1_learning_rate&quot;, &amp;KLp0p1RBMModule::klp0p1_learning_rate,
+                  OptionBase::buildoption,
+        &quot;Learning rate for the KLp0p1 criterion update. If\n&quot;
+        &quot;set to 0, the gradient of KLp0p1 (and corresponding update) will not be\n&quot;
+        &quot;computed at all.&quot;);
+
+    declareOption(ol, &quot;compute_contrastive_divergence&quot;, &amp;KLp0p1RBMModule::compute_contrastive_divergence,
+                  OptionBase::buildoption,
+        &quot;Compute the constrastive divergence in an output port.&quot;);
+
+    declareOption(ol, &quot;standard_cd_grad&quot;,
+                  &amp;KLp0p1RBMModule::standard_cd_grad,
+                  OptionBase::buildoption,
+        &quot;Whether to use the standard contrastive divergence gradient for\n&quot;
+        &quot;updates, or the true gradient of the contrastive divergence. This\n&quot;
+        &quot;affects only the gradient w.r.t. internal parameters of the layers\n&quot;
+        &quot;and connections. Currently, this option works only with layers of\n&quot;
+        &quot;the type 'RBMBinomialLayer', connected by a 'RBMMatrixConnection'.&quot;);
+
+    declareOption(ol, &quot;standard_cd_bias_grad&quot;,
+                  &amp;KLp0p1RBMModule::standard_cd_bias_grad,
+                  OptionBase::buildoption,
+        &quot;This option is only used when biases of the hidden layer are given\n&quot;
+        &quot;through the 'hidden_bias' port. When this is the case, the gradient\n&quot;
+        &quot;of contrastive divergence w.r.t. these biases is either computed:\n&quot;
+        &quot;- by the usual formula if 'standard_cd_bias_grad' is true\n&quot;
+        &quot;- by the true gradient if 'standard_cd_bias_grad' is false.&quot;);
+
+    declareOption(ol, &quot;standard_cd_weights_grad&quot;,
+                  &amp;KLp0p1RBMModule::standard_cd_weights_grad,
+                  OptionBase::buildoption,
+        &quot;This option is only used when weights of the connection are given\n&quot;
+        &quot;through the 'weights' port. When this is the case, the gradient of\n&quot;
+        &quot;contrastive divergence w.r.t. weights is either computed:\n&quot;
+        &quot;- by the usual formula if 'standard_cd_weights_grad' is true\n&quot;
+        &quot;- by the true gradient if 'standard_cd_weights_grad' is false.&quot;);
+
+    declareOption(ol, &quot;n_Gibbs_steps_CD&quot;, 
+                  &amp;KLp0p1RBMModule::n_Gibbs_steps_CD,
+                  OptionBase::buildoption,
+                  &quot;Number of Gibbs sampling steps in negative phase of &quot;
+                  &quot;contrastive divergence.&quot;);
+
+    declareOption(ol, &quot;min_n_Gibbs_steps&quot;, &amp;KLp0p1RBMModule::min_n_Gibbs_steps,
+                  OptionBase::buildoption,
+                  &quot;Used in generative mode (when visible_sample or hidden_sample is requested)\n&quot;
+                  &quot;when one has to sample from the joint or a marginal of visible and hidden,\n&quot;
+                  &quot;and thus a Gibbs chain has to be run. This option gives the minimum number\n&quot;
+                  &quot;of Gibbs steps to perform in the chain before outputting a sample.\n&quot;);
+
+    declareOption(ol, &quot;n_Gibbs_steps_per_generated_sample&quot;, 
+                  &amp;KLp0p1RBMModule::n_Gibbs_steps_per_generated_sample,
+                  OptionBase::buildoption,
+                  &quot;Used in generative mode (when visible_sample or hidden_sample is requested)\n&quot;
+                  &quot;when one has to sample from the joint or a marginal of visible and hidden,\n&quot;
+                  &quot;This option gives the number of steps to run in the Gibbs chain between\n&quot;
+                  &quot;consecutive generated samples that are produced in output of the fprop method.\n&quot;
+                  &quot;By default this is equal to min_n_Gibbs_steps.\n&quot;);
+
+    declareOption(ol, &quot;compute_log_likelihood&quot;,
+                  &amp;KLp0p1RBMModule::compute_log_likelihood,
+                  OptionBase::buildoption,
+                  &quot;Whether to compute the exact RBM generative model's log-likelihood\n&quot;
+                  &quot;(on the neg_log_likelihood port). If false then the neg_log_likelihood\n&quot;
+                  &quot;port just computes the input visible's free energy.\n&quot;);
+    
+    declareOption(ol, &quot;minimize_log_likelihood&quot;,
+                  &amp;KLp0p1RBMModule::minimize_log_likelihood,
+                  OptionBase::buildoption,
+                  &quot;Whether to minimize the exact RBM generative model's log-likelihood\n&quot;
+                  &quot;i.e. take stochastic gradient steps w.r.t. the log-likelihood instead\n&quot;
+                  &quot;of w.r.t. the contrastive divergence.\n&quot;);
+
+    declareOption(ol, &quot;Gibbs_step&quot;, 
+                  &amp;KLp0p1RBMModule::Gibbs_step,
+                  OptionBase::learntoption,
+                  &quot;Used in generative mode (when visible_sample or hidden_sample is requested)\n&quot;
+                  &quot;when one has to sample from the joint or a marginal of visible and hidden,\n&quot;
+                  &quot;Keeps track of the number of steps that have been run since the beginning\n&quot;
+                  &quot;of the chain.\n&quot;);
+
+    declareOption(ol, &quot;log_partition_function&quot;, 
+                  &amp;KLp0p1RBMModule::log_partition_function,
+                  OptionBase::learntoption,
+                  &quot;log(Z) = log(sum_{h,x} exp(-energy(h,x))\n&quot;
+                  &quot;only computed if compute_log_likelihood is true and\n&quot;
+                  &quot;the neg_log_likelihood port is requested.\n&quot;);
+
+    declareOption(ol, &quot;partition_function_is_stale&quot;, 
+                  &amp;KLp0p1RBMModule::partition_function_is_stale,
+                  OptionBase::learntoption,
+                  &quot;Whether parameters have changed since the last computation\n&quot;
+                  &quot;of the log_partition_function (to know if it should be recomputed\n&quot;
+                  &quot;when the neg_log_likelihood port is requested.\n&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void KLp0p1RBMModule::build_()
+{
+    PLASSERT( cd_learning_rate &gt;= 0 &amp;&amp; grad_learning_rate &gt;= 0 );
+    if(visible_layer)
+        visible_bias_grad.resize(visible_layer-&gt;size);
+
+    conf_visible_layer = PLearn::deepCopy(visible_layer);
+    conf_hidden_layer = PLearn::deepCopy(hidden_layer);
+
+    // Forward random generator to underlying modules.
+    if (random_gen) {
+        if (hidden_layer &amp;&amp; !hidden_layer-&gt;random_gen) {
+            hidden_layer-&gt;random_gen = random_gen;
+            hidden_layer-&gt;build();
+            hidden_layer-&gt;forget();
+        }
+        if (visible_layer &amp;&amp; !visible_layer-&gt;random_gen) {
+            visible_layer-&gt;random_gen = random_gen;
+            visible_layer-&gt;build();
+            visible_layer-&gt;forget();
+        }
+        if (connection &amp;&amp; !connection-&gt;random_gen) {
+            connection-&gt;random_gen = random_gen;
+            connection-&gt;build();
+            connection-&gt;forget();
+        }
+        if (reconstruction_connection &amp;&amp;
+                !reconstruction_connection-&gt;random_gen) {
+            reconstruction_connection-&gt;random_gen = random_gen;
+            reconstruction_connection-&gt;build();
+            reconstruction_connection-&gt;forget();
+        }
+    }
+
+    // buid ports and port_sizes
+
+    ports.resize(0);
+    portname_to_index.clear();
+    addPortName(&quot;visible&quot;);
+    addPortName(&quot;hidden.state&quot;);
+    addPortName(&quot;hidden_activations.state&quot;);
+    addPortName(&quot;visible_sample&quot;);
+    addPortName(&quot;visible_expectation&quot;);
+    addPortName(&quot;hidden_sample&quot;);
+    addPortName(&quot;energy&quot;);
+    addPortName(&quot;hidden_bias&quot;); 
+    addPortName(&quot;weights&quot;); 
+    addPortName(&quot;neg_log_likelihood&quot;);
+    addPortName(&quot;KLp0p1&quot;); 
+    if(reconstruction_connection)
+    {
+        addPortName(&quot;visible_reconstruction.state&quot;);
+        addPortName(&quot;visible_reconstruction_activations.state&quot;);
+        addPortName(&quot;reconstruction_error.state&quot;);
+    }
+    if (compute_contrastive_divergence)
+    {
+        addPortName(&quot;contrastive_divergence&quot;);
+        addPortName(&quot;negative_phase_visible_samples.state&quot;);
+        addPortName(&quot;negative_phase_hidden_expectations.state&quot;);
+        addPortName(&quot;negative_phase_hidden_activations.state&quot;);
+    }
+
+    port_sizes.resize(nPorts(), 2);
+    port_sizes.fill(-1);
+    if (visible_layer) {
+        port_sizes(getPortIndex(&quot;visible&quot;), 1) = visible_layer-&gt;size;
+        port_sizes(getPortIndex(&quot;visible_sample&quot;), 1) = visible_layer-&gt;size;
+        port_sizes(getPortIndex(&quot;visible_expectation&quot;), 1) = visible_layer-&gt;size;
+    }
+    if (hidden_layer) {
+        port_sizes(getPortIndex(&quot;hidden.state&quot;), 1) = hidden_layer-&gt;size;
+        port_sizes(getPortIndex(&quot;hidden_activations.state&quot;), 1) = hidden_layer-&gt;size; 
+        port_sizes(getPortIndex(&quot;hidden_sample&quot;), 1) = hidden_layer-&gt;size; 
+        port_sizes(getPortIndex(&quot;hidden_bias&quot;),1) = hidden_layer-&gt;size;
+        if(visible_layer)
+            port_sizes(getPortIndex(&quot;weights&quot;),1) = hidden_layer-&gt;size * visible_layer-&gt;size;
+    }
+    port_sizes(getPortIndex(&quot;energy&quot;),1) = 1;
+    port_sizes(getPortIndex(&quot;neg_log_likelihood&quot;),1) = 1;
+    port_sizes(getPortIndex(&quot;KLp0p1&quot;),1) = 1;
+    if(reconstruction_connection)
+    {
+        if (visible_layer) {
+            port_sizes(getPortIndex(&quot;visible_reconstruction.state&quot;),1) = 
+                visible_layer-&gt;size; 
+            port_sizes(getPortIndex(&quot;visible_reconstruction_activations.state&quot;),1) = 
+                       visible_layer-&gt;size; 
+        }
+        port_sizes(getPortIndex(&quot;reconstruction_error.state&quot;),1) = 1; 
+    }
+    if (compute_contrastive_divergence)
+    {
+        port_sizes(getPortIndex(&quot;contrastive_divergence&quot;),1) = 1; 
+        if (visible_layer) 
+            port_sizes(getPortIndex(&quot;negative_phase_visible_samples.state&quot;),1) = visible_layer-&gt;size; 
+        if (hidden_layer)
+            port_sizes(getPortIndex(&quot;negative_phase_hidden_expectations.state&quot;),1) = hidden_layer-&gt;size; 
+        if (fast_exact_is_equal(cd_learning_rate, 0))
+            PLWARNING(&quot;In KLp0p1RBMModule::build_ - Contrastive divergence is &quot;
+                    &quot;computed but 'cd_learning_rate' is set to 0: no internal &quot;
+                    &quot;update will be performed AND no contrastive divergence &quot;
+                    &quot;gradient will be propagated.&quot;);
+    }
+
+    PLCHECK_MSG(!(!standard_cd_grad &amp;&amp; standard_cd_bias_grad), &quot;You cannot &quot;
+            &quot;compute the standard CD gradient w.r.t. external hidden bias and &quot;
+            &quot;use the 'true' CD gradient w.r.t. internal hidden bias&quot;);
+
+    if (n_Gibbs_steps_per_generated_sample&lt;0)
+        n_Gibbs_steps_per_generated_sample = min_n_Gibbs_steps;
+
+}
+
+///////////
+// build //
+///////////
+void KLp0p1RBMModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////
+// addPortName //
+/////////////////
+void KLp0p1RBMModule::addPortName(const string&amp; name)
+{
+    PLASSERT( portname_to_index.find(name) == portname_to_index.end() );
+    portname_to_index[name] = ports.length();
+    ports.append(name);
+}
+
+///////////////////
+// computeEnergy //
+///////////////////
+// FULLY OBSERVED CASE
+// we know x and h:
+// energy(h,x) = -b'x - c'h - h'Wx
+//  = visible_layer-&gt;energy(x) + hidden_layer-&gt;energy(h)
+//      - dot(h, hidden_layer-&gt;activation-c)
+//  = visible_layer-&gt;energy(x) - dot(h, hidden_layer-&gt;activation)
+void KLp0p1RBMModule::computeEnergy(const Mat&amp; visible, const Mat&amp; hidden,
+                              Mat&amp; energy, bool positive_phase)
+{
+    int mbs=hidden.length();
+    energy.resize(mbs, 1);
+    Mat* hidden_activations = NULL;
+    if (positive_phase) {
+        computePositivePhaseHiddenActivations(visible);
+        hidden_activations = hidden_act;
+    } else {
+        computeHiddenActivations(visible);
+        hidden_activations = &amp; hidden_layer-&gt;activations;
+    }
+    PLASSERT( hidden_activations );
+    for (int i=0;i&lt;mbs;i++)
+        energy(i,0) = visible_layer-&gt;energy(visible(i))
+            - dot(hidden(i), (*hidden_activations)(i));
+            // Why not: + hidden_layer-&gt;energy(hidden(i)) ?
+}
+
+///////////////////////////////
+// computeFreeEnergyOfHidden //
+///////////////////////////////
+// FREE-ENERGY(hidden) CASE
+// we know h:
+// free energy = -log sum_x e^{-energy(h,x)}
+//  = -c'h - sum_i log sigmoid(b_i + W_{.i}'h) .... FOR BINOMIAL INPUT LAYER
+// or more robustly,
+//  = hidden_layer-&gt;energy(h) - sum_i softplus(visible_layer-&gt;activation[i])
+void KLp0p1RBMModule::computeFreeEnergyOfHidden(const Mat&amp; hidden, Mat&amp; energy)
+{
+    int mbs=hidden.length();
+    if (energy.isEmpty())
+        energy.resize(mbs,1);
+    else {
+        PLASSERT( energy.length() == mbs &amp;&amp; energy.width() == 1 );
+    }
+    PLASSERT(visible_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+    computeVisibleActivations(hidden, false);
+    for (int i=0;i&lt;mbs;i++)
+    {
+        energy(i,0) = hidden_layer-&gt;energy(hidden(i));
+        if (use_fast_approximations)
+            for (int j=0;j&lt;visible_layer-&gt;size;j++)
+                energy(i,0) -= tabulated_softplus(visible_layer-&gt;activations(i,j));
+        else
+            for (int j=0;j&lt;visible_layer-&gt;size;j++)
+                energy(i,0) -= softplus(visible_layer-&gt;activations(i,j));
+    }
+}
+
+////////////////////////////////
+// computeFreeEnergyOfVisible //
+////////////////////////////////
+// FREE-ENERGY(visible) CASE
+// we know x:
+// free energy = -log sum_h e^{-energy(h,x)}
+//  = -b'x - sum_i log sigmoid(c_i + W_i'x) .... FOR BINOMIAL HIDDEN LAYER
+// or more robustly,
+//  = visible_layer-&gt;energy(x) - sum_i softplus(hidden_layer-&gt;activation[i])
+void KLp0p1RBMModule::computeFreeEnergyOfVisible(const Mat&amp; visible, Mat&amp; energy,
+                                           bool positive_phase)
+{
+    int mbs=visible.length();
+    if (energy.isEmpty())
+        energy.resize(mbs,1);
+    else {
+        PLASSERT( energy.length() == mbs &amp;&amp; energy.width() == 1 );
+    }
+    PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+    Mat* hidden_activations = NULL;
+    if (positive_phase) {
+        computePositivePhaseHiddenActivations(visible);
+        hidden_activations = hidden_act;
+    }
+    else {
+        computeHiddenActivations(visible);
+        hidden_activations = &amp; hidden_layer-&gt;activations;
+    }
+    PLASSERT( hidden_activations &amp;&amp; hidden_activations-&gt;length() == mbs
+            &amp;&amp; hidden_activations-&gt;width() == hidden_layer-&gt;size );
+    for (int i=0;i&lt;mbs;i++)
+    {
+        energy(i,0) = visible_layer-&gt;energy(visible(i));
+        if (use_fast_approximations)
+            for (int j=0;j&lt;hidden_layer-&gt;size;j++)
+                energy(i,0) -= tabulated_softplus((*hidden_activations)(i,j));
+        else
+            for (int j=0;j&lt;hidden_layer-&gt;size;j++)
+                energy(i,0) -= softplus((*hidden_activations)(i,j));
+    }
+}
+
+//////////////////////////////
+// computeHiddenActivations //
+//////////////////////////////
+void KLp0p1RBMModule::computeHiddenActivations(const Mat&amp; visible)
+{
+    if(weights &amp;&amp; !weights-&gt;isEmpty())
+    {
+        Mat old_weights;
+        Vec old_activation;
+        connection-&gt;getAllWeights(old_weights);
+        old_activation = hidden_layer-&gt;activation;
+        int up = connection-&gt;up_size;
+        int down = connection-&gt;down_size;
+        PLASSERT( weights-&gt;width() == up * down  );
+        hidden_layer-&gt;setBatchSize( visible.length() );
+        for(int i=0; i&lt;visible.length(); i++)
+        {
+            connection-&gt;setAllWeights(Mat(up, down, (*weights)(i)));
+            connection-&gt;setAsDownInput(visible(i));
+            hidden_layer-&gt;activation = hidden_layer-&gt;activations(i);
+            hidden_layer-&gt;getAllActivations(connection, 0, false);
+            if (hidden_bias &amp;&amp; !hidden_bias-&gt;isEmpty())
+                hidden_layer-&gt;activation += (*hidden_bias)(i);
+        }
+        connection-&gt;setAllWeights(old_weights);
+        hidden_layer-&gt;activation = old_activation;
+    }
+    else
+    {
+        connection-&gt;setAsDownInputs(visible);
+        hidden_layer-&gt;getAllActivations(connection, 0, true);
+        if (hidden_bias &amp;&amp; !hidden_bias-&gt;isEmpty())
+            hidden_layer-&gt;activations += *hidden_bias;
+    }
+}
+
+///////////////////////////////////////////
+// computePositivePhaseHiddenActivations //
+///////////////////////////////////////////
+void KLp0p1RBMModule::computePositivePhaseHiddenActivations(const Mat&amp; visible)
+{
+    if (hidden_activations_are_computed) {
+        // Nothing to do.
+        PLASSERT( !hidden_act || !hidden_act-&gt;isEmpty() );
+        return;
+    }
+    computeHiddenActivations(visible);
+    if (hidden_act &amp;&amp; hidden_act-&gt;isEmpty())
+    {
+        hidden_act-&gt;resize(visible.length(),hidden_layer-&gt;size);
+        *hidden_act &lt;&lt; hidden_layer-&gt;activations;
+    }
+    hidden_activations_are_computed = true;
+}
+
+///////////////////////////////
+// computeVisibleActivations //
+///////////////////////////////
+void KLp0p1RBMModule::computeVisibleActivations(const Mat&amp; hidden,
+                                          bool using_reconstruction_connection)
+{
+    if (using_reconstruction_connection)
+    {
+        PLASSERT( reconstruction_connection );
+        reconstruction_connection-&gt;setAsDownInputs(hidden);
+        visible_layer-&gt;getAllActivations(reconstruction_connection, 0, true);
+    }
+    else
+    {
+        if(weights &amp;&amp; !weights-&gt;isEmpty())
+        {
+            Mat old_weights;
+            Vec old_activation;
+            connection-&gt;getAllWeights(old_weights);
+            old_activation = visible_layer-&gt;activation;
+            int up = connection-&gt;up_size;
+            int down = connection-&gt;down_size;
+            PLASSERT( weights-&gt;width() == up * down  );
+            visible_layer-&gt;setBatchSize( hidden.length() );
+            for(int i=0; i&lt;hidden.length(); i++)
+            {
+                connection-&gt;setAllWeights(Mat(up,down,(*weights)(i)));
+                connection-&gt;setAsUpInput(hidden(i));
+                visible_layer-&gt;activation = visible_layer-&gt;activations(i);
+                visible_layer-&gt;getAllActivations(connection, 0, false);
+            }
+            connection-&gt;setAllWeights(old_weights);
+            visible_layer-&gt;activation = old_activation;
+        }
+        else
+        {
+            connection-&gt;setAsUpInputs(hidden);
+            visible_layer-&gt;getAllActivations(connection, 0, true);
+        }
+    }
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void KLp0p1RBMModule::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(hidden_layer,     copies);
+    deepCopyField(visible_layer,    copies);
+    deepCopyField(connection,       copies);
+    deepCopyField(reconstruction_connection, copies);
+
+    deepCopyField(hidden_exp_grad, copies);
+    deepCopyField(hidden_act_grad, copies);
+    deepCopyField(store_weights_grad, copies);
+    deepCopyField(store_hidden_bias_grad, copies);
+    deepCopyField(visible_exp_grad, copies);
+    deepCopyField(visible_act_grad, copies);
+    deepCopyField(visible_bias_grad, copies);
+    deepCopyField(hidden_exp_store, copies);
+    deepCopyField(hidden_act_store, copies);
+
+    deepCopyField(ports, copies);
+    deepCopyField(energy_inputs, copies);
+}
+
+///////////
+// fprop //
+///////////
+void KLp0p1RBMModule::fprop(const Vec&amp; input, Vec&amp; output) const
+{
+    PLERROR(&quot;In KLp0p1RBMModule::fprop - Not implemented&quot;);
+}
+
+void KLp0p1RBMModule::fprop(const TVec&lt;Mat*&gt;&amp; ports_value)
+{
+
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( visible_layer );
+    PLASSERT( hidden_layer );
+    PLASSERT( connection );
+
+    Mat* visible = ports_value[getPortIndex(&quot;visible&quot;)]; 
+    Mat* hidden = ports_value[getPortIndex(&quot;hidden.state&quot;)];
+    hidden_act = ports_value[getPortIndex(&quot;hidden_activations.state&quot;)];
+    Mat* visible_sample = ports_value[getPortIndex(&quot;visible_sample&quot;)];
+    Mat* visible_expectation = ports_value[getPortIndex(&quot;visible_expectation&quot;)];
+    Mat* hidden_sample = ports_value[getPortIndex(&quot;hidden_sample&quot;)];
+    Mat* energy = ports_value[getPortIndex(&quot;energy&quot;)];
+    Mat* neg_log_likelihood = ports_value[getPortIndex(&quot;neg_log_likelihood&quot;)];
+    Mat* KLp0p1 = ports_value[getPortIndex(&quot;KLp0p1&quot;)];
+    hidden_bias = ports_value[getPortIndex(&quot;hidden_bias&quot;)];
+    weights = ports_value[getPortIndex(&quot;weights&quot;)];
+    Mat* visible_reconstruction = 0;
+    Mat* visible_reconstruction_activations = 0;
+    Mat* reconstruction_error = 0;
+    if(reconstruction_connection)
+    {
+        visible_reconstruction = 
+            ports_value[getPortIndex(&quot;visible_reconstruction.state&quot;)]; 
+        visible_reconstruction_activations = 
+            ports_value[getPortIndex(&quot;visible_reconstruction_activations.state&quot;)];
+        reconstruction_error = 
+            ports_value[getPortIndex(&quot;reconstruction_error.state&quot;)];
+    }
+    Mat* contrastive_divergence = 0;
+    Mat* negative_phase_visible_samples = 0;
+    Mat* negative_phase_hidden_expectations = 0;
+    Mat* negative_phase_hidden_activations = NULL;
+    if (compute_contrastive_divergence)
+    {
+        contrastive_divergence = ports_value[getPortIndex(&quot;contrastive_divergence&quot;)]; 
+        if (!contrastive_divergence || !contrastive_divergence-&gt;isEmpty())
+            PLERROR(&quot;In KLp0p1RBMModule::fprop - When option &quot;
+                    &quot;'compute_contrastive_divergence' is 'true', the &quot;
+                    &quot;'contrastive_divergence' port should be provided, as an &quot;
+                    &quot;output.&quot;);
+        negative_phase_visible_samples = 
+            ports_value[getPortIndex(&quot;negative_phase_visible_samples.state&quot;)];
+        negative_phase_hidden_expectations = 
+            ports_value[getPortIndex(&quot;negative_phase_hidden_expectations.state&quot;)];
+        negative_phase_hidden_activations =
+            ports_value[getPortIndex(&quot;negative_phase_hidden_activations.state&quot;)];
+    }
+
+    bool hidden_expectations_are_computed = false;
+    hidden_activations_are_computed = false;
+    bool found_a_valid_configuration = false;
+
+    if (visible &amp;&amp; !visible-&gt;isEmpty())
+    {
+        // When an input is provided, that would restart the chain for
+        // unconditional sampling, from that example.
+        Gibbs_step = 0; 
+        visible_layer-&gt;setExpectations(*visible);
+    }
+
+    // COMPUTE ENERGY
+    if (energy) 
+    {
+        PLASSERT_MSG( energy-&gt;isEmpty(), 
+                      &quot;KLp0p1RBMModule: the energy port can only be an output port\n&quot; );
+        if (visible &amp;&amp; !visible-&gt;isEmpty()
+            &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
+        {
+            computeEnergy(*visible, *hidden, *energy);
+        }
+        else if (visible &amp;&amp; !visible-&gt;isEmpty())
+        {
+            computeFreeEnergyOfVisible(*visible,*energy);
+        }
+        else if (hidden &amp;&amp; !hidden-&gt;isEmpty())
+        {
+            computeFreeEnergyOfHidden(*hidden,*energy);
+        }
+        else
+        {
+            PLERROR(&quot;KLp0p1RBMModule: unknown configuration to compute energy (currently\n&quot;
+                    &quot;only possible if at least visible or hidden are provided).\n&quot;);
+        }
+        found_a_valid_configuration = true;
+    }
+    if (neg_log_likelihood &amp;&amp; neg_log_likelihood-&gt;isEmpty() &amp;&amp; compute_log_likelihood)
+    {
+        if (partition_function_is_stale &amp;&amp; !during_training)
+        {
+            PLASSERT_MSG(hidden_layer-&gt;size&lt;32 || visible_layer-&gt;size&lt;32,
+                         &quot;To compute exact log-likelihood of an RBM, hidden_layer-&gt;size &quot;
+                         &quot;or visible_layer-&gt;size must be &lt;32&quot;);
+            // recompute partition function
+            if (hidden_layer-&gt;size &gt; visible_layer-&gt;size)
+                // do it by log-summing minus-free-energy of visible configurations
+            {
+                PLASSERT(visible_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+                // assuming a binary input we sum over all bit configurations
+                int n_configurations = 1 &lt;&lt; visible_layer-&gt;size; // = 2^{visible_layer-&gt;size}
+                energy_inputs.resize(1, visible_layer-&gt;size);
+                Vec input = energy_inputs(0);
+                // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+                // AT ONCE IN A 'MINIBATCH'
+                Mat free_energy(1, 1);
+                log_partition_function = 0;
+                for (int c=0;c&lt;n_configurations;c++)
+                {
+                    // convert integer c into a bit-wise visible representation
+                    int x=c;
+                    for (int i=0;i&lt;visible_layer-&gt;size;i++)
+                    {
+                        input[i]= x &amp; 1; // take least significant bit
+                        x &gt;&gt;= 1; // and shift right (divide by 2)
+                    }
+                    computeFreeEnergyOfVisible(energy_inputs,free_energy,false);
+                    if (c==0)
+                        log_partition_function = -free_energy(0,0);
+                    else
+                        log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+                }
+            }
+            else
+                // do it by summing free-energy of hidden configurations
+            {
+                PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+                // assuming a binary hidden we sum over all bit configurations
+                int n_configurations = 1 &lt;&lt; hidden_layer-&gt;size; // = 2^{hidden_layer-&gt;size}
+                energy_inputs.resize(1, hidden_layer-&gt;size);
+                Vec input = energy_inputs(0);
+                // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+                // AT ONCE IN A 'MINIBATCH'
+                Mat free_energy(1,1);
+                log_partition_function = 0;
+                for (int c=0;c&lt;n_configurations;c++)
+                {
+                    // convert integer c into a bit-wise hidden representation
+                    int x=c;
+                    for (int i=0;i&lt;hidden_layer-&gt;size;i++)
+                    {
+                        input[i]= x &amp; 1; // take least significant bit
+                        x &gt;&gt;= 1; // and shift right (divide by 2)
+                    }
+                    computeFreeEnergyOfHidden(energy_inputs, free_energy);
+                    if (c==0)
+                        log_partition_function = -free_energy(0,0);
+                    else
+                        log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+                }
+            }
+            partition_function_is_stale=false;
+        }
+        if (visible &amp;&amp; !visible-&gt;isEmpty()
+            &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
+        {
+            // neg-log-likelihood(visible,hidden) = energy(visible,visible) + log(partition_function)
+            computeEnergy(*visible,*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (visible &amp;&amp; !visible-&gt;isEmpty()) 
+        {
+            // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
+            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (hidden &amp;&amp; !hidden-&gt;isEmpty())
+        {
+            // neg-log-likelihood(hidden) = free_energy(hidden) + log(partition_function)
+            computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else PLERROR(&quot;KLp0p1RBMModule: neg_log_likelihood currently computable only of the visible as inputs&quot;);
+    }
+    
+    // REGULAR FPROP
+    // we are given the visible units and we want to compute the hidden
+    // activation and/or the hidden expectation
+    if ( visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp;
+         ((hidden &amp;&amp; hidden-&gt;isEmpty() ) ||
+          (hidden_act &amp;&amp; hidden_act-&gt;isEmpty())) )
+    {
+        computePositivePhaseHiddenActivations(*visible);
+        if (hidden) {
+            PLASSERT( hidden-&gt;isEmpty() );
+            PLCHECK_MSG( !hidden_layer-&gt;expectations_are_up_to_date, &quot;Safety &quot;
+                    &quot;check: how were expectations computed previously?&quot; );
+            hidden_layer-&gt;computeExpectations();
+            hidden_expectations_are_computed=true;
+            const Mat&amp; hidden_out = hidden_layer-&gt;getExpectations();
+            hidden-&gt;resize(hidden_out.length(), hidden_out.width());
+            *hidden &lt;&lt; hidden_out;
+        }
+        // Since we return below, the other ports must be unused.
+        //PLASSERT( !visible_sample &amp;&amp; !hidden_sample );
+        found_a_valid_configuration = true;
+    } 
+
+    // compute KLp0p1 cost, given visible input
+    if (KLp0p1 &amp;&amp; KLp0p1-&gt;isEmpty() &amp;&amp; visible &amp;&amp; !visible-&gt;isEmpty())
+    {
+        int mbs=visible-&gt;length();
+        KLp0p1-&gt;resize(mbs,1);
+        KLp0p1-&gt;clear();
+        PLASSERT_MSG(training_set,&quot;KLp0p1RBMModule: training_set must be provided&quot;);
+        int n=training_set.length();
+        PLASSERT_MSG(n&gt;0,&quot;KLp0p1RBMModule: training_set must have n&gt;0 rows&quot;);
+
+        // compute all P(hidden_i=1|x^k) for all x^k in training set
+        const Mat&amp; ph=hidden_layer-&gt;getExpectations();
+        training_set-&gt;getMat(0,0,visible_layer-&gt;getExpectations());
+        hidden_layer-&gt;getAllActivations(connection,0,true);
+        hidden_layer-&gt;computeExpectations();
+
+        PLASSERT_MSG(hidden_layer-&gt;size&lt;32,&quot;To compute KLp0p1 of an RBM, hidden_layer-&gt;size must be &lt;32&quot;);
+        PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+        real logn=safelog(n);
+        // assuming a binary hidden we sum over all bit configurations
+        int n_configurations = 1 &lt;&lt; hidden_layer-&gt;size; // = 2^{hidden_layer-&gt;size}
+        // put all h configurations in the hidden_layer-&gt;samples
+        conf_hidden_layer-&gt;setBatchSize(n_configurations);
+        conf_visible_layer-&gt;setBatchSize(n_configurations);
+        for (int c=0;c&lt;n_configurations;c++)
+        {
+            // convert integer c into a bit-wise hidden representation
+            int N=c;
+            for (int i=0;i&lt;hidden_layer-&gt;size;i++)
+            {
+                conf_hidden_layer-&gt;samples(c,i)= N &amp; 1; // take least significant bit
+                N &gt;&gt;= 1; // and shift right (divide by 2)
+            }
+        }
+        // compute all P(visible_i=1|h) for each h configuration
+        visible_layer-&gt;getAllActivations(connection,0,true);
+        visible_layer-&gt;computeExpectations();
+
+        for (int c=0;c&lt;n_configurations;c++)
+        {
+            //  C(x) = - log P1(x) = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k)
+            //                     = - log sum_h P(x|h) (sum_k P(h|x^k))/n
+            real log_sum_ph_given_xk = 0;
+            Vec h = conf_hidden_layer-&gt;samples(c);
+            for (int k=0;k&lt;n;k++)
+            {
+                real lp=h[0]==1?safelog(ph(k,0)):safelog(1-ph(k,0));
+                for (int i=1;i&lt;hidden_layer-&gt;size;i++)
+                {
+                    real p_hi_given_xk = h[i]==1?safelog(ph(k,i)):safelog(1-ph(k,i)); 
+                    lp = logadd(lp,p_hi_given_xk);
+                }
+                // now lp = log P(h|x^k)
+                if (k==0)
+                    log_sum_ph_given_xk = lp;
+                else
+                    log_sum_ph_given_xk = logadd(log_sum_ph_given_xk,lp);
+            }
+            log_sum_ph_given_xk -= logn;
+            // now log_sum_ph_given_xk = log (1/n) sum_k P(h|x^k)
+            for (int t=0;t&lt;mbs;t++)
+                if (c==0)
+                    (*KLp0p1)(t,0) = conf_visible_layer-&gt;fpropNLL((*visible)(t)) + log_sum_ph_given_xk;
+                else
+                    (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), conf_visible_layer-&gt;fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
+        }
+        *KLp0p1 *= -1;
+    }
+
+    // SAMPLING
+    if ((visible_sample &amp;&amp; visible_sample-&gt;isEmpty())               // is asked to sample visible units (discrete)
+        || (visible_expectation &amp;&amp; visible_expectation-&gt;isEmpty())  //              &quot;                   (continous)
+        || (hidden_sample &amp;&amp; hidden_sample-&gt;isEmpty()))             // or to sample hidden units
+    {
+        if (hidden_sample &amp;&amp; !hidden_sample-&gt;isEmpty()) // sample visible conditionally on hidden
+        {
+            sampleVisibleGivenHidden(*hidden_sample);
+            Gibbs_step=0;
+            //cout &lt;&lt; &quot;sampling visible from hidden&quot; &lt;&lt; endl;
+        }
+        else if (visible_sample &amp;&amp; !visible_sample-&gt;isEmpty()) // if an input is provided, sample hidden conditionally
+        {
+            sampleHiddenGivenVisible(*visible_sample);
+            Gibbs_step=0;
+            //cout &lt;&lt; &quot;sampling hidden from (discrete) visible&quot; &lt;&lt; endl;
+        }
+        else if (visible &amp;&amp; !visible-&gt;isEmpty()) // if an input is provided, sample hidden conditionally
+        {
+            visible_layer-&gt;generateSamples(); // WHY THIS LINE????
+            sampleHiddenGivenVisible(visible_layer-&gt;samples);
+            Gibbs_step=0;
+            //cout &lt;&lt; &quot;sampling hidden from visible expectation&quot; &lt;&lt; endl;
+        }
+        else if (visible_expectation &amp;&amp; !visible_expectation-&gt;isEmpty()) 
+        {
+             PLERROR(&quot;In KLp0p1RBMModule::fprop visible_expectation can only be an output port (use visible as input port&quot;);
+        }
+        else // sample unconditionally: Gibbs sample after k steps
+        {
+            // the visible_layer-&gt;expectations contain the &quot;state&quot; from which we
+            // start or continue the chain
+            int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
+                            min_n_Gibbs_steps);
+            //cout &lt;&lt; &quot;Gibbs sampling &quot; &lt;&lt; Gibbs_step+1;
+            for (;Gibbs_step&lt;min_n;Gibbs_step++)
+            {
+                sampleHiddenGivenVisible(visible_layer-&gt;samples);
+                sampleVisibleGivenHidden(hidden_layer-&gt;samples);
+            }
+              //cout &lt;&lt; &quot; -&gt; &quot; &lt;&lt; Gibbs_step &lt;&lt; endl;
+        }
+
+        if ( hidden &amp;&amp; hidden-&gt;isEmpty())   // fill hidden.state with expectations
+        {
+              const Mat&amp; hidden_expect = hidden_layer-&gt;getExpectations();
+              hidden-&gt;resize(hidden_expect.length(), hidden_expect.width());
+              *hidden &lt;&lt; hidden_expect;
+        }
+        if (visible_sample &amp;&amp; visible_sample-&gt;isEmpty()) // provide sample of the visible units
+        {
+            visible_sample-&gt;resize(visible_layer-&gt;samples.length(),
+                                   visible_layer-&gt;samples.width());
+            *visible_sample &lt;&lt; visible_layer-&gt;samples;
+        }
+        if (hidden_sample &amp;&amp; hidden_sample-&gt;isEmpty()) // provide sample of the hidden units
+        {
+            hidden_sample-&gt;resize(hidden_layer-&gt;samples.length(),
+                                  hidden_layer-&gt;samples.width());
+            *hidden_sample &lt;&lt; hidden_layer-&gt;samples;
+        }
+        if (visible_expectation &amp;&amp; visible_expectation-&gt;isEmpty()) // provide expectation of the visible units
+        {
+            const Mat&amp; to_store = visible_layer-&gt;getExpectations();
+            visible_expectation-&gt;resize(to_store.length(),
+                                        to_store.width());
+            *visible_expectation &lt;&lt; to_store;
+        }
+        found_a_valid_configuration = true;
+    }// END SAMPLING
+    
+    // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
+    if (contrastive_divergence)
+    {
+        PLASSERT_MSG( contrastive_divergence-&gt;isEmpty(), 
+                      &quot;KLp0p1RBMModule: the contrastive_divergence port can only be an output port\n&quot; );
+        if (visible &amp;&amp; !visible-&gt;isEmpty())
+        {
+            int mbs = visible-&gt;length();
+            const Mat&amp; hidden_expectations = hidden_layer-&gt;getExpectations();
+            Mat* h=0;
+            Mat* h_act=0;
+            if (!hidden_activations_are_computed) // it must be because neither hidden nor hidden_act were asked
+            {
+                PLASSERT(!hidden_act);
+                computePositivePhaseHiddenActivations(*visible);
+                
+                // we need to save the hidden activations somewhere
+                hidden_act_store.resize(mbs,hidden_layer-&gt;size);
+                hidden_act_store &lt;&lt; hidden_layer-&gt;activations;
+                h_act = &amp;hidden_act_store;
+            } else 
+            {
+                // hidden_act must have been computed above if they were requested on port
+                PLASSERT(hidden_act &amp;&amp; !hidden_act-&gt;isEmpty()); 
+                h_act = hidden_act;
+            }
+            if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
+            {
+                PLASSERT(!hidden);
+                hidden_layer-&gt;computeExpectations();
+                hidden_expectations_are_computed=true;
+                // we need to save the hidden expectations somewhere
+                hidden_exp_store.resize(mbs,hidden_layer-&gt;size);
+                hidden_exp_store &lt;&lt; hidden_expectations;
+                h = &amp;hidden_exp_store;
+            } else
+            {
+                // hidden exp. must have been computed above if they were requested on port
+                PLASSERT(hidden &amp;&amp; !hidden-&gt;isEmpty());
+                h = hidden;
+            }
+            // perform negative phase
+            for( int i=0; i&lt;n_Gibbs_steps_CD; i++)
+            {
+                hidden_layer-&gt;generateSamples();
+                // (Negative phase) Generate visible samples.
+                sampleVisibleGivenHidden(hidden_layer-&gt;samples);
+                // compute corresponding hidden expectations.
+                computeHiddenActivations(visible_layer-&gt;samples);
+                hidden_layer-&gt;computeExpectations();
+            }
+            PLASSERT(negative_phase_visible_samples);
+            PLASSERT(negative_phase_hidden_expectations &amp;&amp;
+                     negative_phase_hidden_expectations-&gt;isEmpty());
+            PLASSERT(negative_phase_hidden_activations &amp;&amp;
+                     negative_phase_hidden_activations-&gt;isEmpty());
+            negative_phase_visible_samples-&gt;resize(mbs,visible_layer-&gt;size);
+            *negative_phase_visible_samples &lt;&lt; visible_layer-&gt;samples;
+            negative_phase_hidden_expectations-&gt;resize(hidden_expectations.length(),
+                                                       hidden_expectations.width());
+            *negative_phase_hidden_expectations &lt;&lt; hidden_expectations;
+            const Mat&amp; neg_hidden_act = hidden_layer-&gt;activations;
+            negative_phase_hidden_activations-&gt;resize(neg_hidden_act.length(),
+                                                      neg_hidden_act.width());
+            *negative_phase_hidden_activations &lt;&lt; neg_hidden_act;
+
+            // compute the energy (again for now only in the binomial case)
+            PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+
+            // note that h_act and h may point to hidden_act_store and hidden_exp_store
+            PLASSERT(h_act &amp;&amp; !h_act-&gt;isEmpty()); 
+            PLASSERT(h &amp;&amp; !h-&gt;isEmpty());
+
+            contrastive_divergence-&gt;resize(hidden_expectations.length(),1);
+            // compute contrastive divergence itself
+            for (int i=0;i&lt;mbs;i++)
+            {
+                (*contrastive_divergence)(i,0) = 
+                    // positive phase energy
+                    visible_layer-&gt;energy((*visible)(i))
+                    - dot((*h)(i),(*h_act)(i))
+                    // minus
+                    - 
+                    // negative phase energy
+                    (visible_layer-&gt;energy(visible_layer-&gt;samples(i))
+                     - dot(hidden_expectations(i),hidden_layer-&gt;activations(i)));
+            }
+        }
+        else
+            PLERROR(&quot;KLp0p1RBMModule: unknown configuration to compute contrastive_divergence (currently\n&quot;
+                    &quot;only possible if only visible are provided in input).\n&quot;);
+        found_a_valid_configuration = true;
+    }
+    
+
+    
+    
+    // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
+    if ( visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp; 
+         ( ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() ) || 
+           ( visible_reconstruction_activations &amp;&amp; 
+             visible_reconstruction_activations-&gt;isEmpty() ) ||
+           ( reconstruction_error &amp;&amp; reconstruction_error-&gt;isEmpty() ) ) ) 
+    {        
+        // Autoassociator reconstruction cost
+        PLASSERT( ports_value.length() == nPorts() );
+        computePositivePhaseHiddenActivations(*visible); 
+        if(!hidden_expectations_are_computed)
+        {
+            hidden_layer-&gt;computeExpectations();
+            hidden_expectations_are_computed=true;
+        }
+
+        // Don't need to verify if they are asked in a port, this was done previously
+        
+        computeVisibleActivations(hidden_layer-&gt;getExpectations(),true);
+        if(visible_reconstruction_activations) 
+        {
+            PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
+            const Mat&amp; to_store = visible_layer-&gt;activations;
+            visible_reconstruction_activations-&gt;resize(to_store.length(), 
+                                                       to_store.width());
+            *visible_reconstruction_activations &lt;&lt; to_store;
+        }
+        if (visible_reconstruction || reconstruction_error)
+        {        
+            visible_layer-&gt;computeExpectations();
+            if(visible_reconstruction)
+            {
+                PLASSERT( visible_reconstruction-&gt;isEmpty() );
+                const Mat&amp; to_store = visible_layer-&gt;getExpectations();
+                visible_reconstruction-&gt;resize(to_store.length(), 
+                                                           to_store.width());
+                *visible_reconstruction &lt;&lt; to_store;
+            }
+            if(reconstruction_error)
+            {
+                PLASSERT( reconstruction_error-&gt;isEmpty() );
+                reconstruction_error-&gt;resize(visible-&gt;length(),1);
+                visible_layer-&gt;fpropNLL(*visible,
+                                        *reconstruction_error);
+            }
+        }
+        found_a_valid_configuration = true;
+    }
+    // COMPUTE VISIBLE GIVEN HIDDEN
+    else if ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() 
+         &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
+           
+    {        
+        // Don't need to verify if they are asked in a port, this was done previously
+        
+	computeVisibleActivations(*hidden,true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
+            const Mat&amp; to_store = visible_layer-&gt;activations;
+            visible_reconstruction_activations-&gt;resize(to_store.length(), 
+                                                       to_store.width());
+            *visible_reconstruction_activations &lt;&lt; to_store;
+        }      
+        visible_layer-&gt;computeExpectations();
+        PLASSERT( visible_reconstruction-&gt;isEmpty() );
+        const Mat&amp; to_store = visible_layer-&gt;getExpectations();
+        visible_reconstruction-&gt;resize(to_store.length(), 
+                                       to_store.width());
+        *visible_reconstruction &lt;&lt; to_store;
+        found_a_valid_configuration = true;
+    }
+
+    // Reset some class fields to ensure they are not reused by mistake.
+    hidden_act = NULL;
+    hidden_bias = NULL;
+    weights = NULL;
+    hidden_activations_are_computed = false;
+
+
+
+    if (!found_a_valid_configuration)
+    {
+        /*
+        if (visible)
+            cout &lt;&lt; &quot;visible_empty : &quot;&lt;&lt; (bool) visible-&gt;isEmpty() &lt;&lt; endl;
+        if (hidden)
+            cout &lt;&lt; &quot;hidden_empty : &quot;&lt;&lt; (bool) hidden-&gt;isEmpty() &lt;&lt; endl;
+        if (visible_sample)
+            cout &lt;&lt; &quot;visible_sample_empty : &quot;&lt;&lt; (bool) visible_sample-&gt;isEmpty() &lt;&lt; endl;
+        if (hidden_sample)
+            cout &lt;&lt; &quot;hidden_sample_empty : &quot;&lt;&lt; (bool) hidden_sample-&gt;isEmpty() &lt;&lt; endl;
+        if (visible_expectation)
+            cout &lt;&lt; &quot;visible_expectation_empty : &quot;&lt;&lt; (bool) visible_expectation-&gt;isEmpty() &lt;&lt; endl;
+
+        */
+        PLERROR(&quot;In KLp0p1RBMModule::fprop - Unknown port configuration for module %s&quot;, name.c_str());
+    }
+
+    checkProp(ports_value);
+
+}
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void KLp0p1RBMModule::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                               const TVec&lt;Mat*&gt;&amp; ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( ports_gradient.length() == nPorts() );
+    Mat* visible_grad = ports_gradient[getPortIndex(&quot;visible&quot;)];
+    Mat* hidden_grad = ports_gradient[getPortIndex(&quot;hidden.state&quot;)];
+    Mat* visible = ports_value[getPortIndex(&quot;visible&quot;)];
+    Mat* hidden = ports_value[getPortIndex(&quot;hidden.state&quot;)];
+    hidden_act = ports_value[getPortIndex(&quot;hidden_activations.state&quot;)];
+    Mat* reconstruction_error_grad = 0;
+    Mat* hidden_bias_grad = ports_gradient[getPortIndex(&quot;hidden_bias&quot;)];
+    weights = ports_value[getPortIndex(&quot;weights&quot;)]; 
+    Mat* weights_grad = ports_gradient[getPortIndex(&quot;weights&quot;)];    
+    hidden_bias = ports_value[getPortIndex(&quot;hidden_bias&quot;)];
+    Mat* contrastive_divergence_grad = NULL;
+    Mat* KLp0p1 = ports_value[getPortIndex(&quot;KLp0p1&quot;)];
+
+    // Ensure the gradient w.r.t. contrastive divergence is 1 (if provided).
+    if (compute_contrastive_divergence) {
+        contrastive_divergence_grad =
+            ports_gradient[getPortIndex(&quot;contrastive_divergence&quot;)];
+        if (contrastive_divergence_grad) {
+            PLASSERT( !contrastive_divergence_grad-&gt;isEmpty() );
+            PLASSERT( min(*contrastive_divergence_grad) &gt;= 1 );
+            PLASSERT( max(*contrastive_divergence_grad) &lt;= 1 );
+        }
+    }
+
+    if(reconstruction_connection)
+        reconstruction_error_grad = 
+            ports_gradient[getPortIndex(&quot;reconstruction_error.state&quot;)];
+
+    // Ensure the visible gradient is not provided as input. This is because we
+    // accumulate more than once in 'visible_grad'.
+    PLASSERT_MSG( !visible_grad || visible_grad-&gt;isEmpty(), &quot;Cannot provide &quot;
+            &quot;an input gradient w.r.t. visible units&quot; );
+
+    bool compute_visible_grad = visible_grad &amp;&amp; visible_grad-&gt;isEmpty();
+    bool compute_weights_grad = weights_grad &amp;&amp; weights_grad-&gt;isEmpty();
+    
+    int mbs = (visible &amp;&amp; !visible-&gt;isEmpty()) ? visible-&gt;length() : -1;
+
+    if (hidden_grad &amp;&amp; !hidden_grad-&gt;isEmpty())
+    {
+        // Note: the assert below is for behavior compatibility with previous
+        // code. It might not be necessary, or might need to be modified.
+        PLASSERT( visible &amp;&amp; !visible-&gt;isEmpty() );
+
+        // Note: we need to perform the following steps even if the gradient
+        // learning rate is equal to 0. This is because we must propagate the
+        // gradient to the visible layer, even though no update is required.
+            setAllLearningRates(grad_learning_rate);
+            PLASSERT( hidden &amp;&amp; hidden_act );
+            // Compute gradient w.r.t. activations of the hidden layer.
+            hidden_layer-&gt;bpropUpdate(
+                    *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
+                    false);
+            if (hidden_bias_grad)
+            {
+                PLASSERT( hidden_bias_grad-&gt;isEmpty() &amp;&amp;
+                          hidden_bias_grad-&gt;width() == hidden_layer-&gt;size );
+                hidden_bias_grad-&gt;resize(mbs,hidden_layer-&gt;size);
+                *hidden_bias_grad += hidden_act_grad;
+            }
+            // Compute gradient w.r.t. expectations of the visible layer (=
+            // inputs).
+            Mat* store_visible_grad = NULL;
+            if (compute_visible_grad) {
+                PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
+                store_visible_grad = visible_grad;
+            } else {
+                // We do not actually need to store the gradient, but since it
+                // is required in bpropUpdate, we provide a dummy matrix to
+                // store it.
+                store_visible_grad = &amp;visible_exp_grad;
+            }
+            store_visible_grad-&gt;resize(mbs,visible_layer-&gt;size);
+            
+            if (weights)
+            {
+                int up = connection-&gt;up_size;
+                int down = connection-&gt;down_size;
+                PLASSERT( !weights-&gt;isEmpty() &amp;&amp;
+                          weights_grad &amp;&amp; weights_grad-&gt;isEmpty() &amp;&amp;
+                          weights_grad-&gt;width() == up * down );
+                weights_grad-&gt;resize(mbs, up * down);
+                Mat w, wg;
+                Vec v,h,vg,hg;
+                for(int i=0; i&lt;mbs; i++)
+                {
+                    w = Mat(up, down,(*weights)(i));
+                    wg = Mat(up, down,(*weights_grad)(i));
+                    v = (*visible)(i);
+                    h = (*hidden_act)(i);
+                    vg = (*store_visible_grad)(i);
+                    hg = hidden_act_grad(i);
+                    connection-&gt;petiteCulotteOlivierUpdate(
+                        v,
+                        w,
+                        h,
+                        vg,
+                        wg,
+                        hg,true);
+                }
+            }
+            else
+            {
+                connection-&gt;bpropUpdate(
+                    *visible, *hidden_act, *store_visible_grad,
+                    hidden_act_grad, true);
+            }
+            partition_function_is_stale = true;
+    }
+
+    if (cd_learning_rate &gt; 0 &amp;&amp; minimize_log_likelihood) {
+        PLASSERT( visible &amp;&amp; !visible-&gt;isEmpty() );
+        PLASSERT( hidden &amp;&amp; !hidden-&gt;isEmpty() );
+        setAllLearningRates(cd_learning_rate);
+
+        // positive phase
+        visible_layer-&gt;accumulatePosStats(*visible);
+        hidden_layer-&gt;accumulatePosStats(*hidden);
+        connection-&gt;accumulatePosStats(*visible,*hidden);
+
+        // negative phase
+        PLASSERT_MSG(hidden_layer-&gt;size&lt;32 || visible_layer-&gt;size&lt;32,
+                     &quot;To minimize exact log-likelihood of an RBM, hidden_layer-&gt;size &quot;
+                     &quot;or visible_layer-&gt;size must be &lt;32&quot;);
+        // gradient of partition function
+        if (hidden_layer-&gt;size &gt; visible_layer-&gt;size)
+            // do it by summing over visible configurations
+        {
+            PLASSERT(visible_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+            // assuming a binary input we sum over all bit configurations
+            int n_configurations = 1 &lt;&lt; visible_layer-&gt;size; // = 2^{visible_layer-&gt;size}
+            energy_inputs.resize(1, visible_layer-&gt;size);
+            Vec input = energy_inputs(0);
+            // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+            // AT ONCE IN A 'MINIBATCH'
+            for (int c=0;c&lt;n_configurations;c++)
+            {
+                // convert integer c into a bit-wise visible representation
+                int x=c;
+                for (int i=0;i&lt;visible_layer-&gt;size;i++)
+                {
+                    input[i]= x &amp; 1; // take least significant bit
+                    x &gt;&gt;= 1; // and shift right (divide by 2)
+                }
+                connection-&gt;setAsDownInput(input);
+                hidden_layer-&gt;getAllActivations(connection,0,false);
+                hidden_layer-&gt;computeExpectation();
+                visible_layer-&gt;accumulateNegStats(input);
+                hidden_layer-&gt;accumulateNegStats(hidden_layer-&gt;expectation);
+                connection-&gt;accumulateNegStats(input,hidden_layer-&gt;expectation);
+            }
+        }
+        else
+        {
+            PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+            // assuming a binary hidden we sum over all bit configurations
+            int n_configurations = 1 &lt;&lt; hidden_layer-&gt;size; // = 2^{hidden_layer-&gt;size}
+            energy_inputs.resize(1, hidden_layer-&gt;size);
+            Vec h = energy_inputs(0);
+            for (int c=0;c&lt;n_configurations;c++)
+            {
+                // convert integer c into a bit-wise hidden representation
+                int x=c;
+                for (int i=0;i&lt;hidden_layer-&gt;size;i++)
+                {
+                    h[i]= x &amp; 1; // take least significant bit
+                    x &gt;&gt;= 1; // and shift right (divide by 2)
+                }
+                connection-&gt;setAsUpInput(h);
+                visible_layer-&gt;getAllActivations(connection,0,false);
+                visible_layer-&gt;computeExpectation();
+                visible_layer-&gt;accumulateNegStats(visible_layer-&gt;expectation);
+                hidden_layer-&gt;accumulateNegStats(h);
+                connection-&gt;accumulateNegStats(visible_layer-&gt;expectation,h);
+            }
+        }
+        // update
+        visible_layer-&gt;update();
+        hidden_layer-&gt;update();
+        connection-&gt;update();
+    }
+    if (cd_learning_rate &gt; 0 &amp;&amp; !minimize_log_likelihood) {
+        EXTREME_MODULE_LOG &lt;&lt; &quot;Performing contrastive divergence step in RBM '&quot;
+                           &lt;&lt; name &lt;&lt; &quot;'&quot; &lt;&lt; endl;
+        // Perform a step of contrastive divergence.
+        PLASSERT( visible &amp;&amp; !visible-&gt;isEmpty() );
+        setAllLearningRates(cd_learning_rate);
+        Mat* negative_phase_visible_samples = 
+            compute_contrastive_divergence?ports_value[getPortIndex(&quot;negative_phase_visible_samples.state&quot;)]:0;
+        const Mat* negative_phase_hidden_expectations =
+            compute_contrastive_divergence ?
+                ports_value[getPortIndex(&quot;negative_phase_hidden_expectations.state&quot;)]
+                : NULL;
+        Mat* negative_phase_hidden_activations =
+            compute_contrastive_divergence ?
+                ports_value[getPortIndex(&quot;negative_phase_hidden_activations.state&quot;)]
+                : NULL;
+        
+        PLASSERT( visible &amp;&amp; hidden );
+        PLASSERT( !negative_phase_visible_samples ||
+                  !negative_phase_visible_samples-&gt;isEmpty() );
+        if (!negative_phase_visible_samples)
+        {
+            // Generate hidden samples.
+            hidden_layer-&gt;setExpectations(*hidden);
+            for( int i=0; i&lt;n_Gibbs_steps_CD; i++)
+            {
+                hidden_layer-&gt;generateSamples();
+                // (Negative phase) Generate visible samples.
+                sampleVisibleGivenHidden(hidden_layer-&gt;samples);
+                // compute corresponding hidden expectations.
+                computeHiddenActivations(visible_layer-&gt;samples);
+                hidden_layer-&gt;computeExpectations();
+            }
+            PLASSERT( !compute_contrastive_divergence );
+            PLASSERT( !negative_phase_hidden_expectations );
+            PLASSERT( !negative_phase_hidden_activations );
+            negative_phase_hidden_expectations = &amp;(hidden_layer-&gt;getExpectations());
+            negative_phase_visible_samples = &amp;(visible_layer-&gt;samples);
+            negative_phase_hidden_activations = &amp;(hidden_layer-&gt;activations);
+        }
+        PLASSERT( negative_phase_hidden_expectations &amp;&amp;
+                  !negative_phase_hidden_expectations-&gt;isEmpty() );
+        PLASSERT( negative_phase_hidden_activations &amp;&amp;
+                  !negative_phase_hidden_activations-&gt;isEmpty() );
+
+        // Perform update.
+        visible_layer-&gt;update(*visible, *negative_phase_visible_samples);
+
+        bool connection_update_is_done = false;
+        if (compute_weights_grad) {
+            // First resize the 'weights_grad' matrix.
+            int up = connection-&gt;up_size;
+            int down = connection-&gt;down_size;
+            PLASSERT( weights &amp;&amp; !weights-&gt;isEmpty() &amp;&amp;
+                      weights_grad-&gt;width() == up * down );
+            weights_grad-&gt;resize(mbs, up * down);
+
+            if (standard_cd_weights_grad)
+            {
+                // Perform both computation of weights gradient and do update
+                // at the same time.
+                Mat wg;
+                Vec vp, hp, vn, hn;
+                for(int i=0; i&lt;mbs; i++)
+                {
+                    vp = (*visible)(i);
+                    hp = (*hidden)(i);
+                    vn = (*negative_phase_visible_samples)(i);
+                    hn = (*negative_phase_hidden_expectations)(i);
+                    wg = Mat(up, down,(*weights_grad)(i));
+                    connection-&gt;petiteCulotteOlivierCD(
+                            vp, hp,
+                            vn,
+                            hn,
+                            wg,
+                            true);
+                    connection_update_is_done = true;
+                }
+            }
+        }
+        if (!standard_cd_weights_grad || !standard_cd_grad) {
+            // Compute 'true' gradient of contrastive divergence w.r.t.
+            // the weights matrix.
+            int up = connection-&gt;up_size;
+            int down = connection-&gt;down_size;
+            Mat* weights_g = weights_grad;
+            if (!weights_g) {
+                // We need to store the gradient in another matrix.
+                store_weights_grad.resize(mbs, up * down);
+                store_weights_grad.clear();
+                weights_g = &amp; store_weights_grad;
+            }
+            PLASSERT( connection-&gt;classname() == &quot;RBMMatrixConnection&quot; &amp;&amp;
+                      visible_layer-&gt;classname() == &quot;RBMBinomialLayer&quot; &amp;&amp;
+                      hidden_layer-&gt;classname() == &quot;RBMBinomialLayer&quot; );
+
+            for (int k = 0; k &lt; mbs; k++) {
+                int idx = 0;
+                for (int i = 0; i &lt; up; i++) {
+                    real p_i_p = (*hidden)(k, i);
+                    real a_i_p = (*hidden_act)(k, i);
+                    real p_i_n =
+                        (*negative_phase_hidden_expectations)(k, i);
+                    real a_i_n =
+                        (*negative_phase_hidden_activations)(k, i);
+
+                    real scale_p = 1 + (1 - p_i_p) * a_i_p;
+                    real scale_n = 1 + (1 - p_i_n) * a_i_n;
+                    for (int j = 0; j &lt; down; j++, idx++) {
+                        // Weight 'idx' is the (i,j)-th element in the
+                        // 'weights' matrix.
+                        real v_j_p = (*visible)(k, j);
+                        real v_j_n =
+                            (*negative_phase_visible_samples)(k, j);
+                        (*weights_g)(k, idx) +=
+                            p_i_n * v_j_n * scale_n     // Negative phase.
+                            -(p_i_p * v_j_p * scale_p); // Positive phase.
+                    }
+                }
+            }
+            if (!standard_cd_grad) {
+                // Update connection manually.
+                Mat&amp; weights = ((RBMMatrixConnection*)
+                                get_pointer(connection))-&gt;weights;
+                real lr = cd_learning_rate / mbs;
+                for (int k = 0; k &lt; mbs; k++) {
+                    int idx = 0;
+                    for (int i = 0; i &lt; up; i++)
+                        for (int j = 0; j &lt; down; j++, idx++)
+                            weights(i, j) -= lr * (*weights_g)(k, idx);
+                }
+                connection_update_is_done = true;
+            }
+        }
+        if (!connection_update_is_done)
+            // Perform standard update of the connection.
+            connection-&gt;update(*visible, *hidden,
+                    *negative_phase_visible_samples,
+                    *negative_phase_hidden_expectations);
+
+        Mat* hidden_bias_g = hidden_bias_grad;
+        if (!standard_cd_grad &amp;&amp; !hidden_bias_grad) {
+            // We need to compute the CD gradient w.r.t. bias of hidden layer,
+            // but there is no bias coming from the outside. Thus we need
+            // another matrix to store this gradient.
+            store_hidden_bias_grad.resize(mbs, hidden_layer-&gt;size);
+            store_hidden_bias_grad.clear();
+            hidden_bias_g = &amp; store_hidden_bias_grad;
+        }
+
+        if (hidden_bias_g)
+        {
+            if (hidden_bias_g-&gt;isEmpty()) {
+                PLASSERT(hidden_bias_g-&gt;width() == hidden_layer-&gt;size);
+                hidden_bias_g-&gt;resize(mbs,hidden_layer-&gt;size);
+            }
+            PLASSERT_MSG( hidden_layer-&gt;classname() == &quot;RBMBinomialLayer&quot; &amp;&amp;
+                          visible_layer-&gt;classname() == &quot;RBMBinomialLayer&quot;,
+                          &quot;Only implemented for binomial layers&quot; );
+            // d(contrastive_divergence)/dhidden_bias
+            for (int k = 0; k &lt; hidden_bias_g-&gt;length(); k++) {
+                for (int i = 0; i &lt; hidden_bias_g-&gt;width(); i++) {
+                    real p_i_p = (*hidden)(k, i);
+                    real a_i_p = (*hidden_act)(k, i);
+                    real p_i_n = (*negative_phase_hidden_expectations)(k, i);
+                    real a_i_n = (*negative_phase_hidden_activations)(k, i);
+                    (*hidden_bias_g)(k, i) +=
+                        standard_cd_bias_grad ? p_i_n - p_i_p :
+                        p_i_n * (1 - p_i_n) * a_i_n + p_i_n     // Neg. phase
+                     -( p_i_p * (1 - p_i_p) * a_i_p + p_i_p );  // Pos. phase
+
+                }
+            }
+        }
+
+        if (standard_cd_grad) {
+            hidden_layer-&gt;update(*hidden, *negative_phase_hidden_expectations);
+        } else {
+            PLASSERT( hidden_layer-&gt;classname() == &quot;RBMBinomialLayer&quot; );
+            // Update hidden layer by hand.
+            Vec&amp; bias = hidden_layer-&gt;bias;
+            real lr = cd_learning_rate / mbs;
+            for (int i = 0; i &lt; mbs; i++)
+                bias -= lr * (*hidden_bias_g)(i);
+        }
+
+
+        partition_function_is_stale = true;
+    } else {
+        PLCHECK_MSG( !contrastive_divergence_grad ||
+                     (!hidden_bias_grad &amp;&amp; !weights_grad),
+                &quot;You currently cannot compute the &quot;
+                &quot;gradient of contrastive divergence w.r.t. external ports &quot;
+                &quot;when 'cd_learning_rate' is set to 0&quot; );
+    }
+
+    if (reconstruction_error_grad &amp;&amp; !reconstruction_error_grad-&gt;isEmpty()) {
+        setAllLearningRates(grad_learning_rate);
+        PLASSERT( reconstruction_connection != 0 );
+        // Perform gradient descent on Autoassociator reconstruction cost
+        Mat* visible_reconstruction = ports_value[getPortIndex(&quot;visible_reconstruction.state&quot;)];
+        Mat* visible_reconstruction_activations = ports_value[getPortIndex(&quot;visible_reconstruction_activations.state&quot;)];
+        Mat* reconstruction_error = ports_value[getPortIndex(&quot;reconstruction_error.state&quot;)];
+        PLASSERT( hidden != 0 );
+        PLASSERT( visible  &amp;&amp; hidden_act &amp;&amp;
+                  visible_reconstruction &amp;&amp; visible_reconstruction_activations &amp;&amp;
+                  reconstruction_error);
+        //int mbs = reconstruction_error_grad-&gt;length();
+
+        PLCHECK_MSG( !weights, &quot;In KLp0p1RBMModule::bpropAccUpdate(): reconstruction cost &quot;
+                     &quot;for conditional weights is not implemented&quot;);
+
+        // Backprop reconstruction gradient
+
+        // Must change visible_layer's expectation
+        visible_layer-&gt;getExpectations() &lt;&lt; *visible_reconstruction;
+        visible_layer-&gt;bpropNLL(*visible,*reconstruction_error,
+                                visible_act_grad);
+
+        // Combine with incoming gradient
+        PLASSERT( (*reconstruction_error_grad).width() == 1 );
+        for (int t=0;t&lt;mbs;t++)
+            visible_act_grad(t) *= (*reconstruction_error_grad)(t,0);
+
+        // Visible bias update
+        columnSum(visible_act_grad,visible_bias_grad);
+        visible_layer-&gt;update(visible_bias_grad);
+
+        // Reconstruction connection update
+        reconstruction_connection-&gt;bpropUpdate(
+            *hidden, *visible_reconstruction_activations,
+            hidden_exp_grad, visible_act_grad, false);
+        
+        // Hidden layer bias update
+        hidden_layer-&gt;bpropUpdate(*hidden_act,
+                                  *hidden, hidden_act_grad,
+                                  hidden_exp_grad, false);
+        if (hidden_bias_grad)
+        {
+            if (hidden_bias_grad-&gt;isEmpty()) {
+                PLASSERT( hidden_bias_grad-&gt;width() == hidden_layer-&gt;size );
+                hidden_bias_grad-&gt;resize(mbs,hidden_layer-&gt;size);
+            }
+            *hidden_bias_grad += hidden_act_grad;
+        }
+        // Connection update
+        if(compute_visible_grad)
+        {
+            // The length of 'visible_grad' must be either 0 (if not computed
+            // previously) or the size of the mini-batches (otherwise).
+            PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size &amp;&amp;
+                      visible_grad-&gt;length() == 0 ||
+                      visible_grad-&gt;length() == mbs );
+            visible_grad-&gt;resize(mbs, visible_grad-&gt;width());
+            connection-&gt;bpropUpdate(
+                *visible, *hidden_act,
+                *visible_grad, hidden_act_grad, true);
+        }
+        else
+        {
+            visible_exp_grad.resize(mbs,visible_layer-&gt;size);        
+            connection-&gt;bpropUpdate(
+                *visible, *hidden_act,
+                visible_exp_grad, hidden_act_grad, true);
+        }
+        partition_function_is_stale = true;
+    }
+
+    // compute KLp0p1 cost, given visible input
+    if (klp0p1_learning_rate&gt;0 &amp;&amp; visible &amp;&amp; !visible-&gt;isEmpty())
+    {
+        // WE ASSUME THAT THIS BPROP IS CALLED JUST AFTER THE CORRESPONDING FPROP!!!
+        // consequentely, we have
+        //   * P(h_i=1|x^k) for each x^k in the training set, in hidden_layer-&gt;expectations
+        //   * every h configuration in conf_hidden_layer-&gt;samples
+        //   * P(visible_j=1|h) for each h configuration, in conf_visible_layer-&gt;expectations
+        //   * x^t for every t in the input visible, in *visible
+        //   * -log P1(x^t) for each input visible(t) in KLp0p1(t,0)
+        //
+        // We want to compute
+        //   dC(x)/dWij = (1/(n P1(x))) 
+        //       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j|h)) + x_j^k(h_i - P(h_i|x^k)))
+        //
+        PLASSERT_MSG(KLp0p1 &amp;&amp; !KLp0p1-&gt;isEmpty(), &quot;Must compute KLp0p1 in order to compute its gradient, connect that port!&quot;);
+        int mbs=visible-&gt;length();
+        int n=training_set.length();
+        PLASSERT(connection-&gt;classname()==&quot;RBMMatrixConnection&quot;);
+        PP&lt;RBMMatrixConnection&gt; matrix_connection = PP&lt;RBMMatrixConnection&gt;(connection);
+        Mat&amp; W = matrix_connection-&gt;weights;
+        Vec&amp; hidden_bias = hidden_layer-&gt;bias;
+        Vec&amp; visible_bias = visible_layer-&gt;bias;
+        Vec pxtj_given_h(visible_layer-&gt;size);
+        Vec phi_given_xk(hidden_layer-&gt;size);
+        const Mat&amp; ph_given_Xk=hidden_layer-&gt;getExpectations();
+        const Mat&amp; pvisible_given_H=conf_visible_layer-&gt;getExpectations();
+        int n_configurations = 1 &lt;&lt; hidden_layer-&gt;size; // = 2^{hidden_layer-&gt;size}
+        real logn=safelog(n);
+        for (int t=0;t&lt;mbs;t++)
+        {
+            Vec xt = (*visible)(t);
+            for (int k=0;k&lt;n;k++)
+            {
+                Vec ph_given_xk = ph_given_Xk(k);
+                Vec xk = (*visible)(k);
+                for (int c=0;c&lt;n_configurations;c++)
+                {
+                    Vec h = conf_hidden_layer-&gt;samples(c);
+                    Vec pvisible_given_h=pvisible_given_H(c);
+                    real lp = (*KLp0p1)(t,0) - logn; // lp = log (1/(n P1(x^t)))
+                    // compute and multiply by P(h|x^k)
+                    for (int i=0;i&lt;hidden_layer-&gt;size;i++)
+                        lp = logadd(lp,phi_given_xk[i]=(h[i]==1?safelog(ph_given_xk[i]):safelog(1-ph_given_xk[i]))); 
+                    // now lp = log ( (1/(n P1(x^t))) P(h|x^k) )
+
+                    // compute and multiply by P(x^t|h)
+                    for (int j=0;j&lt;visible_layer-&gt;size;j++)
+                        lp = logadd(lp,pxtj_given_h[j]=(xt[j]*safelog(pvisible_given_h[j])+(1-xt[j])*safelog(1-pvisible_given_h[j])));
+                    // now lp = log ( (1/(n P1(x^t))) P(h|x^k)  P(x^t|h) )
+                    real coeff = exp(lp);
+                    for (int j=0;j&lt;visible_layer-&gt;size;j++)
+                        visible_bias[j] -= klp0p1_learning_rate*coeff*(xt[j]-pxtj_given_h[j]);
+                    for (int i=0;i&lt;hidden_layer-&gt;size;i++)
+                    {
+                        hidden_bias[i] -= klp0p1_learning_rate*coeff*(h[i]-phi_given_xk[i]);
+                        for (int j=0;j&lt;visible_layer-&gt;size;j++)
+                            W(i,j) -= klp0p1_learning_rate*coeff*(h[i]*(xt[j]-pxtj_given_h[j])-xk[j]*(h[i]-phi_given_xk[i]));
+                    }
+                }
+            }
+        }
+    }
+
+    // Explicit error message in the case of the 'visible' port.
+    if (compute_visible_grad &amp;&amp; visible_grad-&gt;isEmpty())
+        PLERROR(&quot;In KLp0p1RBMModule::bpropAccUpdate - The gradient with respect &quot;
+                &quot;to the 'visible' port was asked, but not computed&quot;);
+
+    checkProp(ports_gradient);
+
+    // Reset pointers to ensure we do not reuse them by mistake.
+    hidden_act = NULL;
+    weights = NULL;
+    hidden_bias = NULL;
+}
+
+////////////
+// forget //
+////////////
+void KLp0p1RBMModule::forget()
+{
+    DBG_MODULE_LOG &lt;&lt; &quot;Forgetting KLp0p1RBMModule '&quot; &lt;&lt; name &lt;&lt; &quot;'&quot; &lt;&lt; endl;
+    PLASSERT( hidden_layer &amp;&amp; visible_layer &amp;&amp; connection );
+    hidden_layer-&gt;forget();
+    visible_layer-&gt;forget();
+    connection-&gt;forget();
+    if (reconstruction_connection)
+        reconstruction_connection-&gt;forget();
+}
+
+//////////////////
+// getPortIndex //
+//////////////////
+int KLp0p1RBMModule::getPortIndex(const string&amp; port)
+{
+    map&lt;string, int&gt;::const_iterator it = portname_to_index.find(port);
+    if (it == portname_to_index.end())
+        return -1;
+    else
+        return it-&gt;second;
+}
+
+//////////////
+// getPorts //
+//////////////
+const TVec&lt;string&gt;&amp; KLp0p1RBMModule::getPorts()
+{
+    return ports;
+}
+
+///////////////////
+// getPortsSizes //
+///////////////////
+const TMat&lt;int&gt;&amp; KLp0p1RBMModule::getPortSizes()
+{
+    return port_sizes;
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+bool KLp0p1RBMModule::bpropDoesNothing()
+{
+}
+*/
+
+/////////////////////////
+// setAllLearningRates //
+/////////////////////////
+void KLp0p1RBMModule::setAllLearningRates(real lr)
+{
+    hidden_layer-&gt;setLearningRate(lr);
+    visible_layer-&gt;setLearningRate(lr);
+    connection-&gt;setLearningRate(lr);
+    if(reconstruction_connection)
+        reconstruction_connection-&gt;setLearningRate(lr);
+}
+
+//////////////////////////////
+// sampleHiddenGivenVisible //
+//////////////////////////////
+void KLp0p1RBMModule::sampleHiddenGivenVisible(const Mat&amp; visible)
+{
+    computeHiddenActivations(visible);
+    hidden_layer-&gt;computeExpectations();
+    hidden_layer-&gt;generateSamples();
+}
+
+//////////////////////////////
+// sampleVisibleGivenHidden //
+//////////////////////////////
+void KLp0p1RBMModule::sampleVisibleGivenHidden(const Mat&amp; hidden)
+{
+    computeVisibleActivations(hidden);
+    visible_layer-&gt;computeExpectations();
+    visible_layer-&gt;generateSamples();
+}
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+void KLp0p1RBMModule::setLearningRate(real dynamic_learning_rate)
+{
+    // Out of safety, force the user to go through the two different learning
+    // rate. May need to be removed if it causes unwanted crashes.
+    PLERROR(&quot;In KLp0p1RBMModule::setLearningRate - Do not use this method, instead &quot;
+            &quot;explicitely use 'cd_learning_rate' and 'grad_learning_rate'&quot;);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h	2007-07-05 20:56:06 UTC (rev 7699)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h	2007-07-05 21:18:19 UTC (rev 7700)
@@ -0,0 +1,355 @@
+// -*- C++ -*-
+
+// KLp0p1RBMModule.h
+//
+// Copyright (C) 2007 Olivier Delalleau, Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file KLp0p1RBMModule.h */
+
+
+#ifndef KLp0p1RBMModule_INC
+#define KLp0p1RBMModule_INC
+
+#include &lt;map&gt;
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn_learners/online/RBMConnection.h&gt;
+#include &lt;plearn_learners/online/RBMLayer.h&gt;
+#include &lt;plearn/vmat/VMat.h&gt;
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class KLp0p1RBMModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    VMat training_set;
+
+    PP&lt;RBMLayer&gt; hidden_layer;
+    PP&lt;RBMLayer&gt; visible_layer;
+    PP&lt;RBMConnection&gt; connection;
+    PP&lt;RBMConnection&gt; reconstruction_connection;
+
+    real cd_learning_rate;
+    real grad_learning_rate;
+    real klp0p1_learning_rate;
+
+    bool compute_contrastive_divergence;
+
+    //! Number of Gibbs sampling steps in negative phase 
+    //! of contrastive divergence.
+    int n_Gibbs_steps_CD;
+
+    //! used to generate samples from the RBM
+    int min_n_Gibbs_steps; 
+    int n_Gibbs_steps_per_generated_sample;
+
+    bool compute_log_likelihood;
+    bool minimize_log_likelihood;
+
+    //#####  Public Learnt Options  ############################################
+    //! used to generate samples from the RBM
+    int Gibbs_step;
+    real log_partition_function;
+    bool partition_function_is_stale;
+
+    bool standard_cd_grad;
+    bool standard_cd_bias_grad;
+    bool standard_cd_weights_grad;
+    
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    KLp0p1RBMModule();
+
+    // Your other public member functions go here
+
+    //! given the input, compute the output (possibly resize it appropriately)
+    virtual void fprop(const Vec&amp; input, Vec&amp; output) const;
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient,
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
+    */
+
+    /* Optional
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             const Vec&amp; output_gradient);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                              Vec&amp; input_gradient,
+                              const Vec&amp; output_gradient,
+                              Vec&amp; input_diag_hessian,
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                              const Vec&amp; output_gradient,
+                              const Vec&amp; output_diag_hessian);
+    */
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    //! Throws an error (please use explicitely the two different kinds of
+    //! learning rates available here).
+    virtual void setLearningRate(real dynamic_learning_rate);
+
+    //! Overridden.
+    virtual void fprop(const TVec&lt;Mat*&gt;&amp; ports_value);
+
+    //! Overridden.
+    virtual void bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                                const TVec&lt;Mat*&gt;&amp; ports_gradient);
+
+    //! Returns all ports in a KLp0p1RBMModule.
+    virtual const TVec&lt;string&gt;&amp; getPorts();
+
+    //! The ports' sizes are given by the corresponding RBM layers.
+    virtual const TMat&lt;int&gt;&amp; getPortSizes();
+
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    virtual int getPortIndex(const string&amp; port);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(KLp0p1RBMModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+
+protected:
+
+    PP&lt;RBMLayer&gt; conf_hidden_layer;
+    PP&lt;RBMLayer&gt; conf_visible_layer;
+
+    Mat* hidden_bias;
+    Mat* weights;
+
+    //! Used to store gradient w.r.t. expectations of the hidden layer.
+    Mat hidden_exp_grad;
+
+    //! Used to store gradient w.r.t. activations of the hidden layer.
+    Mat hidden_act_grad;
+
+    //! Used to store gradient w.r.t. expectations of the visible layer.
+    Mat visible_exp_grad;
+
+    //! Used to store gradient w.r.t. activations of the visible layer.
+    Mat visible_act_grad;
+
+    //! Used to store gradient w.r.t. bias of visible layer
+    Vec visible_bias_grad;
+
+    //! Used to cache the hidden layer expectations and activations
+    Mat hidden_exp_store;
+    Mat hidden_act_store;
+    Mat* hidden_act;
+    bool hidden_activations_are_computed;    
+
+    //! Used to store the contrastive divergence gradient w.r.t. weights.
+    Mat store_weights_grad;
+
+    //! Used to store the contrastive divergence gradient w.r.t. hidden bias.
+    Mat store_hidden_bias_grad;
+
+    //! List of port names.
+    TVec&lt;string&gt; ports;
+
+    //! Map from a port name to its index in the 'ports' vector.
+    map&lt;string, int&gt; portname_to_index;
+
+    //! Used to store inputs generated to compute the free energy.
+    Mat energy_inputs;
+
+    //#####  Protected Member Functions  ######################################
+
+    //! Add a new port to the 'portname_to_index' map and 'ports' vector.
+    void addPortName(const string&amp; name);
+
+    //! Forward the given learning rate to all elements of this module.
+    void setAllLearningRates(real lr);
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+    //! Compute activations on the hidden layer based on the provided
+    //! visible input.
+    //! If 'hidden_bias' is not null nor empty, then it is used as an
+    //! additional bias for hidden activations.
+    void computeHiddenActivations(const Mat&amp; visible);
+
+    //! Compute activations on the visible layer.
+    //! If 'using_reconstruction_connection' is true, then we use the
+    //! reconstruction connection to compute these activations. Otherwise, we
+    //! use the normal connection, in a 'top-&gt;down' fashion.
+    void computeVisibleActivations(const Mat&amp; hidden,
+                                   bool using_reconstruction_connection=false);
+
+    //! Compute activations on the hidden layer based on the provided visible
+    //! input during positive phase. This method is called to ensure hidden
+    //! hidden activations are computed only once, and during a fprop it should
+    //! always be called with the same 'visible' input.
+    //! If 'hidden_act' is not null, it is filled with the computed hidden
+    //! activations.
+    void computePositivePhaseHiddenActivations(const Mat&amp; visible);
+
+    //! Sample hidden layer data based on the provided 'visible' inputs.
+    void sampleHiddenGivenVisible(const Mat&amp; visible);
+
+    //! Sample visible layer data based on the provided 'hidden' inputs.
+    void sampleVisibleGivenHidden(const Mat&amp; hidden);
+
+    //! Compute free energy on the visible layer and store it in the 'energy'
+    //! matrix.
+    //! The 'positive_phase' boolean is used to save computations when we know
+    //! we are in the positive phase of fprop.
+    void computeFreeEnergyOfVisible(const Mat&amp; visible, Mat&amp; energy,
+                                    bool positive_phase = true);
+
+    //! Compute free energy on the hidden layer and store it in the 'energy'
+    //! matrix.
+    void computeFreeEnergyOfHidden(const Mat&amp; hidden, Mat&amp; energy);
+
+    //! Compute energy of the joint (visible, hidden) configuration and store
+    //! it in the 'energy' matrix.
+    //! The 'positive_phase' boolean is used to save computations when we know
+    //! we are in the positive phase of fprop.
+    void computeEnergy(const Mat&amp; visible, const Mat&amp; hidden, Mat&amp; energy, 
+                       bool positive_phase = true);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(KLp0p1RBMModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001147.html">[Plearn-commits] r7699 -	branches/cgi-desjardin/plearn_learners/second_iteration
</A></li>
	<LI>Next message: <A HREF="001149.html">[Plearn-commits] r7701 -	trunk/python_modules/plearn/learners/modulelearners/sampler/example/data
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1148">[ date ]</a>
              <a href="thread.html#1148">[ thread ]</a>
              <a href="subject.html#1148">[ subject ]</a>
              <a href="author.html#1148">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
