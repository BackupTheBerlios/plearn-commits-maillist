<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7776 - trunk/plearn_learners/online/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7776%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200707151430.l6FEUDSD002004%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001223.html">
   <LINK REL="Next"  HREF="001225.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7776 - trunk/plearn_learners/online/EXPERIMENTAL</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7776%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200707151430.l6FEUDSD002004%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7776 - trunk/plearn_learners/online/EXPERIMENTAL">yoshua at mail.berlios.de
       </A><BR>
    <I>Sun Jul 15 16:30:13 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001223.html">[Plearn-commits] r7775 - trunk/python_modules/plearn/pytest
</A></li>
        <LI>Next message: <A HREF="001225.html">[Plearn-commits] r7777 - trunk/plearn_learners/online/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1224">[ date ]</a>
              <a href="thread.html#1224">[ thread ]</a>
              <a href="subject.html#1224">[ subject ]</a>
              <a href="author.html#1224">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-07-15 16:30:13 +0200 (Sun, 15 Jul 2007)
New Revision: 7776

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
Log:
Corrections to scaling of gradient and KLp0p1


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 05:34:40 UTC (rev 7775)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-15 14:30:13 UTC (rev 7776)
@@ -56,7 +56,7 @@
     &quot;The exact and very inefficient implementation of this criterion is done here.&quot;
     &quot;  KL(p0||p1) = sum_x p0(x) log p0(x)/p1(x) = - sum_i (1/n) log p1(x_i) + sum_i (1/n) log(1/n)&quot;
     &quot;For an example x the cost is:&quot;
-    &quot;  C(x) = - log p1(x) -(1/n) log n = - log (1/n)sum_{k=1}^n sum_h P(x|h) P(h|x^k) -(1/n)log n&quot;
+    &quot;  C(x) = - log p1(x) - log n = - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)&quot;
     &quot;where {x^1, ... x^n} is the training set of examples x^k, h is a hidden layer bit vector,&quot;
     &quot;P(x|h) is the hidden-to-visible conditional distribution and P(h|x) is the&quot;
     &quot;input-to-hidden conditional distribution. Both are the usual found in Binomial&quot;
@@ -915,9 +915,11 @@
 
         for (int c=0;c&lt;n_configurations;c++)
         {
-            //  C(x) =  -log p1(x) -(1/n)logn
-            //       =  (1-1/n)log n - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)
-            //       =  (1-1/n)log n - log sum_h P(x|h) (sum_k P(h|x^k))/n
+            // KL(p0|p1) = sum_t (1/n) log ((1/n) / p1(x_t)) = (1/n) sum_t C(x_t)
+            //  p1(x) = sum_k (1/n) sum_h P(x|h) P(h|x_k)
+            //  C(x) =  -log p1(x) - log n
+            //       =  log n - log sum_{k=1}^n sum_h P(x|h) P(h|x^k)  - log n
+            //       =  - log sum_h P(x|h) sum_k P(h|x^k)
 
             real log_sum_ph_given_xk = 0;
             Vec h = conf_hidden_layer-&gt;samples(c);
@@ -947,8 +949,6 @@
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), -conf_visible_layer-&gt;fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
-        *KLp0p1 += logn*((real)1.0 - (real)1.0/(real)n);
-        // going in the other direction just for the fun of it:
         hidden_layer-&gt;setBatchSize(mbs);
         visible_layer-&gt;setBatchSize(mbs);
     }
@@ -1648,8 +1648,9 @@
         //   * x^t for every t in the input visible, in *visible
         //   * -log P1(x^t) for each input visible(t) in KLp0p1(t,0)
         //
+        // Since C(x) = - log sum_h P(x|h) sum_k P(h|x^k), dC/dsum = -1/sum = -1/exp(-C)=-exp(C)
         // We want to compute
-        //   dC(x)/dWij = (-1/(n P1(x))) 
+        //   dC(x)/dWij = (-exp(C(x)))
         //       sum_{k=1}^n sum_h P(x|h) P(h|x^k) (h_i(x_j - P(x_j=1|h)) + x_j^k(h_i - P(h_i=1|x^k)))
         //
         PLASSERT_MSG(KLp0p1 &amp;&amp; !KLp0p1-&gt;isEmpty(), &quot;Must compute KLp0p1 in order to compute its gradient, connect that port!&quot;);
@@ -1681,9 +1682,8 @@
                 {
                     Vec h = conf_hidden_layer-&gt;samples(c);
                     Vec avisible_given_h=conf_visible_layer-&gt;activations(c);
-                    // KLp0p1(x) = -log p1(x) -(1/n)logn
-                    real lp = -(*KLp0p1)(t,0) - (1+1/(real)n)*logn; // lp = log (1/(n P1(x^t)))
-                    // compute and multiply by P(h|x^k)
+                    real lp = (*KLp0p1)(t,0); // lp = log (exp(C(x^t)))
+                    // compute and multiply exp(lp) by P(h|x^k)
                     for (int i=0;i&lt;hidden_layer-&gt;size;i++)
                     {
                         real act=ah_given_xk[i];
@@ -1692,7 +1692,7 @@
                         // so h*log(sigmoid(act))+(1-h)*log(sigmoid(act)) = act*h-softplus(act)
                         lp += h[i]*act-softplus(act);
                     }
-                    // now lp = log ( (1/(n P1(x^t))) P(h|x^k) )
+                    // now lp = log ( exp(C(x^t)) P(h|x^k) )
 
                     // compute and multiply by P(x^t|h)
                     for (int j=0;j&lt;visible_layer-&gt;size;j++)
@@ -1700,7 +1700,7 @@
                         real act=avisible_given_h[j];
                         lp += act*xt[j] - softplus(act);
                     }
-                    // now lp = log ( (1/(n P1(x^t))) P(h|x^k)  P(x^t|h) )
+                    // now lp = log ( exp(C(x^t)) P(h|x^k)  P(x^t|h) )
                     real coeff = exp(lp);
                     Vec pvisible_given_h=pvisible_given_H(c);
                     for (int j=0;j&lt;visible_layer-&gt;size;j++)


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001223.html">[Plearn-commits] r7775 - trunk/python_modules/plearn/pytest
</A></li>
	<LI>Next message: <A HREF="001225.html">[Plearn-commits] r7777 - trunk/plearn_learners/online/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1224">[ date ]</a>
              <a href="thread.html#1224">[ thread ]</a>
              <a href="subject.html#1224">[ subject ]</a>
              <a href="author.html#1224">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
