<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7875 -	trunk/plearn_learners/distributions/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7875%20-%0A%09trunk/plearn_learners/distributions/EXPERIMENTAL&In-Reply-To=%3C200707310348.l6V3mlWn022083%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001322.html">
   <LINK REL="Next"  HREF="001324.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7875 -	trunk/plearn_learners/distributions/EXPERIMENTAL</H1>
    <B>lysiane at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7875%20-%0A%09trunk/plearn_learners/distributions/EXPERIMENTAL&In-Reply-To=%3C200707310348.l6V3mlWn022083%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7875 -	trunk/plearn_learners/distributions/EXPERIMENTAL">lysiane at mail.berlios.de
       </A><BR>
    <I>Tue Jul 31 05:48:47 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001322.html">[Plearn-commits] r7874 - trunk/plearn_learners/generic
</A></li>
        <LI>Next message: <A HREF="001324.html">[Plearn-commits] r7876 -	branches/cgi-desjardin/plearn_learners/second_iteration
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1323">[ date ]</a>
              <a href="thread.html#1323">[ thread ]</a>
              <a href="subject.html#1323">[ subject ]</a>
              <a href="author.html#1323">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lysiane
Date: 2007-07-31 05:48:46 +0200 (Tue, 31 Jul 2007)
New Revision: 7875

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
30 aout 2007, 
modification de log_density


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-31 00:18:20 UTC (rev 7874)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-07-31 03:48:46 UTC (rev 7875)
@@ -1,5 +1,6 @@
     // -*- C++ -*-
 
+
 // TransformationLearner.cc
 //
 // Copyright (C) 2007 Lysiane Bouchard
@@ -81,9 +82,12 @@
     nbTransforms(2),
     nbNeighbors(2)
 {
-    pout &lt;&lt; &quot;hello\n&quot;;
+
+    pout &lt;&lt; &quot;constructor called&quot; &lt;&lt;endl;
+
 }
 
+
 ////////////////////
 // declareOptions //
 ////////////////////
@@ -105,7 +109,7 @@
 
 
     //buildoption
-    pout &lt;&lt; &quot;declare options\n&quot;;
+  
 
     declareOption(ol,
                   &quot;behavior&quot;,
@@ -217,17 +221,17 @@
                  &amp;TransformationLearner::transformsOffset,
                  OptionBase::buildoption,
                  &quot;time of the first update of the transformations matrices&quot;);
+
    declareOption(ol,
                  &quot;biasPeriod&quot;,
                  &amp;TransformationLearner::biasPeriod,
                  OptionBase::buildoption,
-                 &quot;time interval between 2 updates of the transformations bias (if any)&quot;);
-
+                 &quot;time interval between two updates of the transformations bias&quot;);
    declareOption(ol,
                  &quot;biasOffset&quot;,
                  &amp;TransformationLearner::biasOffset,
                  OptionBase::buildoption,
-                 &quot;time of the first update of the transformations bias (if any)&quot;);
+                 &quot;time of the first update of the transformations bias&quot;);
 
    declareOption(ol, 
                  &quot;noiseVariance&quot;,
@@ -257,6 +261,11 @@
    
    //learntoption
    declareOption(ol,
+                 &quot;train_set&quot;,
+                 &amp;TransformationLearner::train_set,
+                 OptionBase::learntoption,
+                 &quot;We remember the training set, as this is a memory-based distribution.&quot; );
+   declareOption(ol,
                  &quot;transformsSet&quot;,
                  &amp;TransformationLearner::transformsSet,
                  OptionBase::learntoption,
@@ -281,60 +290,13 @@
                  &amp;TransformationLearner::inputSpaceDim,
                  OptionBase::learntoption,
                  &quot;dimensionality of the input space&quot;);
+   
    declareOption(ol,
-                 &quot;nbTargetReconstructions&quot;,
-                 &amp;TransformationLearner::nbTargetReconstructions,
+                 &quot;reconstructionSet&quot;,
+                 &amp;TransformationLearner::reconstructionSet,
                  OptionBase::learntoption,
-                 &quot;number of reconstructions of the same target&quot;);
-   declareOption(ol,
-                 &quot;nbReconstructions&quot;,
-                 &amp;TransformationLearner::nbReconstructions,
-                 OptionBase::learntoption,
-                 &quot;total number of reconstructions&quot;);
-   declareOption(ol,
-                 &quot;trainingSetLength&quot;,
-                 &amp;TransformationLearner::trainingSetLength,
-                 OptionBase::learntoption,
-                 &quot;number of samples in the training&quot; );
-   declareOption(ol,
-                 &quot;transformsSD&quot;,
-                 &amp;TransformationLearner::transformsSD,
-                 OptionBase::learntoption,
-                 &quot;standard deviation of the transformations parameters&quot;);
-   declareOption(ol,
-                 &quot;targetReconstructionSet&quot;,
-                 &amp;TransformationLearner::targetReconstructionSet,
-                 OptionBase::learntoption,
-                 &quot;will be used to store a view on the reconstructions of a same target&quot;);
-   declareOption(ol,
-                 &quot;B_C&quot;,
-                 &amp;TransformationLearner::B_C,
-                 OptionBase::learntoption,
-                 &quot;storage space needed in the maximization step (to update transformations parameters)\n&quot;
-                 &quot; - 2mdXd matrix, m=number of transformations, d = dimensionality of the input space&quot;);
-   declareOption(ol,
-                 &quot;B&quot;,
-                 &amp;TransformationLearner::B,
-                 OptionBase::learntoption,
-                 &quot;views on m first dxd submatrices of B_C \n&quot;
-                 &quot;(vector form)&quot;);
-   declareOption(ol,
-                 &quot;C&quot;,
-                 &amp;TransformationLearner::C,
-                 OptionBase::learntoption,
-                 &quot;views on m last dxd submatrices of B_C \n&quot;
-                 &quot;(vector form)&quot;);
-   declareOption(ol,
-                 &quot;target&quot;,
-                 &amp;TransformationLearner::target,
-                 OptionBase::learntoption,
-                 &quot;to store a view on a training sample&quot;);
-   declareOption(ol,
-                 &quot;neighbor&quot;,
-                 &amp;TransformationLearner::neighbor,
-                 OptionBase::learntoption,
-                 &quot;to store a view on a training sample&quot;);
-
+                 &quot;set of weighted reconstruction candidates&quot;);
+ 
    // Now call the parent class' declareOptions().
    inherited::declareOptions(ol);
 }
@@ -342,7 +304,7 @@
 void TransformationLearner::declareMethods(RemoteMethodMap&amp; rmm){
 
 
-    pout &lt;&lt; &quot;declare methods\n&quot;;
+
     rmm.inherited(inherited::_getRemoteMethodMap_());
     
     declareMethod(rmm, 
@@ -416,6 +378,7 @@
                    ArgDoc(&quot;Vec root&quot;,&quot;data point from which all the other data points will derive (directly or indirectly)&quot;),
                    ArgDoc(&quot;int deepness&quot;,&quot;deepness of the tree reprenting the samples created&quot;),
                    ArgDoc(&quot;int branchingFactor&quot;,&quot;branching factor of the tree representing the samples created&quot;),
+                   ArgDoc(&quot;int transformIdx&quot;, &quot;index of the transformation to use (optional)&quot;),
                    RetDoc(&quot;Mat (one row = one sample)&quot;)));
     declareMethod(rmm,
                   &quot;returnSequenceDataSet&quot;,
@@ -424,6 +387,7 @@
                            &quot;see 'sequenceDataSet()' implantation for more details&quot;),
                    ArgDoc(&quot;const Vec start&quot;,&quot;data point from which all the other data points will derice (directly or indirectly)&quot;),
                    ArgDoc(&quot;int n&quot;,&quot;number of sample data points to generate&quot;),
+                   ArgDoc(&quot;int transformIdx&quot;,&quot;index of the transformation to use (optional)&quot;),
                    RetDoc(&quot;nXd matrix (one row = one sample)&quot;)));
     declareMethod(rmm,
                   &quot;returnTrainingPoint&quot;,
@@ -463,11 +427,8 @@
                   (BodyDoc(&quot;returns the parameters of each transformation&quot;),
                    RetDoc(&quot;mdXd matrix, m = number of transformations \n&quot;
                           &quot;             d = dimensionality of the input space&quot;)));
+    
     declareMethod(rmm,
-                  &quot;trainBuild&quot;,
-                  &amp;TransformationLearner::trainBuild,
-                  (BodyDoc(&quot;training specific initialization operations&quot;)));
-    declareMethod(rmm,
                   &quot;generatorBuild&quot;,
                   &amp;TransformationLearner::generatorBuild,
                   (BodyDoc(&quot;generator specific initialization operations&quot;),
@@ -538,10 +499,6 @@
                   &amp;TransformationLearner::MStepNoiseVariance,
                   (BodyDoc(&quot;maximization step with respect to noise variance&quot;)));
     declareMethod(rmm,
-                  &quot;stoppingCriterionReached&quot;,
-                  &amp;TransformationLearner::stoppingCriterionReached,
-                  (BodyDoc(&quot;stages == nstages?&quot;)));
-    declareMethod(rmm,
                   &quot;nextStage&quot;,
                   &amp;TransformationLearner::nextStage,
                   (BodyDoc(&quot;increment 'stage' by one&quot;)));
@@ -554,7 +511,7 @@
 ///////////
 void TransformationLearner::build()
 {
-    pout &lt;&lt; &quot;build\n&quot;;
+
     // ### Nothing to add here, simply calls build_().
     inherited::build();
     build_();
@@ -583,34 +540,23 @@
     //                                          false);
     // TransformationLearner::setPredictor(predictor_part, false);
 
-    pout &lt;&lt; &quot;build_\n&quot;;
-    if(behavior == BEHAVIOR_LEARNER){
-        trainBuild();
+ 
+
+    if(behavior == BEHAVIOR_LEARNER)
+    {
+        if(train_set.isNotNull())
+        {
+            mainLearnerBuild();
+        }
+     
     }
    
     else{
         generatorBuild(); //initialization of the parameters with all the default values
     }
-    
- 
+        
 }
 
-/////////
-// cdf //
-/////////
-real TransformationLearner::cdf(const Vec&amp; y) const
-{
-    PLERROR(&quot;cdf not implemented for TransformationLearner&quot;); return 0;
-}
-
-/////////////////
-// expectation //
-/////////////////
-void TransformationLearner::expectation(Vec&amp; mu) const
-{
-    PLERROR(&quot;expectation not implemented for TransformationLearner&quot;);
-}
-
 // ### Remove this method if your distribution does not implement it.
 ////////////
 // forget //
@@ -618,6 +564,7 @@
 void TransformationLearner::forget()
 {
     
+    
     /*!
       A typical forget() method should do the following:
       - initialize a random number generator with the seed option
@@ -628,7 +575,8 @@
     
     inherited::forget();
     stage = 0;
-    trainBuild();
+    build();
+   
     
 }
 
@@ -641,13 +589,15 @@
 //! - choose randomly a transformation 
 //! - apply the transformation on the choosen neighbor
 //! - add some noise 
-void TransformationLearner::generate(Vec &amp; y) //const
+void TransformationLearner::generate(Vec &amp; y) const
 {
     //PLERROR(&quot;generate not implemented for TransformationLearner&quot;);
     PLASSERT(y.length() == inputSpaceDim);
     int neighborIdx ;
     neighborIdx=pickNeighborIdx();
-    seeNeighbor(neighborIdx);
+    Vec neighbor;
+    neighbor.resize(inputSpaceDim);
+    seeNeighbor(neighborIdx, neighbor);
     generatePredictedFrom(neighbor, y);
 }
 
@@ -661,8 +611,10 @@
 /////////////////
 // log_density //
 /////////////////
-real TransformationLearner::log_density(const Vec&amp; y) //const
+real TransformationLearner::log_density(const Vec&amp; y) const
 {
+ 
+    pout &lt;&lt; &quot;in TransformationLearner :: log_density&quot; &lt;&lt; endl;
     PLASSERT(y.length() == inputSpaceDim);
     real weight;
     real totalWeight = INIT_weight(0);
@@ -681,10 +633,10 @@
     }
     totalWeight = MULT_weights(totalWeight, scalingFactor);
     return totalWeight;
-    
-    /*PLERROR(&quot;density not implemented for TransformationLearner&quot;); return 0;*/
 }
 
+
+
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////
@@ -711,42 +663,6 @@
 }
 */
 
-//////////////////
-// setPredictor //
-//////////////////
-void TransformationLearner::setPredictor(const Vec&amp; predictor, bool call_parent) const
-{
-    if (call_parent)
-        inherited::setPredictor(predictor, true);
-    // ### Add here any specific code required by your subclass.
-}
-
-////////////////////////////////
-// setPredictorPredictedSizes //
-////////////////////////////////
-bool TransformationLearner::setPredictorPredictedSizes(int the_predictor_size,
-                                               int the_predicted_size,
-                                               bool call_parent)
-{
-    bool sizes_have_changed = false;
-    if (call_parent)
-        sizes_have_changed = inherited::setPredictorPredictedSizes(
-                the_predictor_size, the_predicted_size, true);
-
-    // ### Add here any specific code required by your subclass.
-
-    // Returned value.
-    return sizes_have_changed;
-}
-
-/////////////////
-// survival_fn //
-/////////////////
-real TransformationLearner::survival_fn(const Vec&amp; y) const
-{
-    PLERROR(&quot;survival_fn not implemented for TransformationLearner&quot;); return 0;
-}
-
 // ### Remove this method, if your distribution does not implement it.
 ///////////
 // train //
@@ -788,8 +704,11 @@
     }
     */
 
-    initEStep();
-    while(!stoppingCriterionReached()){
+    if(stage==0)
+        buildLearnedParameters();
+        initEStep();
+    while(stage&lt;nstages)
+    {
         MStep();
         EStep();
         stage ++;
@@ -797,41 +716,97 @@
     
 }
 
-//////////////
-// variance //
-//////////////
-void TransformationLearner::variance(Mat&amp; covar) const
-{
-    PLERROR(&quot;variance not implemented for TransformationLearner&quot;);
-}
 
 
+void TransformationLearner::buildLearnedParameters(){
+    
+    //LEARNED PARAMETERS
 
+
+    //set of transformations matrices
+    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
+    
+    //view on the set of transformations (vector)
+    //    each transformation = one matrix 
+    transforms.resize(nbTransforms);
+    for(int k = 0; k&lt; nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    
+    //set of transformations bias (optional)
+    if(withBias){
+        biasSet = Mat(nbTransforms,inputSpaceDim);       
+    }
+    else{
+        biasSet = Mat(0,0);   
+    }
+
+    //choose an initial value for each transformation parameter  (normal distribution) 
+    initTransformsParameters();
+
+    //initialize the noise variance
+    if(noiseVariance == UNDEFINED){
+        if(learnNoiseVariance &amp;&amp; regOnNoiseVariance){
+            initNoiseVariance();
+        }
+        else{
+            noiseVariance = 1.0;
+        }
+    }
+
+    //transformDistribution
+    if(transformDistribution.length() == 0){
+        if(learnTransformDistribution &amp;&amp; regOnTransformDistribution)
+            initTransformDistribution();
+        else{
+            transformDistribution.resize(nbTransforms);
+            real w = INIT_weight(1.0/nbTransforms);
+            for(int k=0; k&lt;nbTransforms ; k++){
+                transformDistribution[k] = w;
+            }
+        }       
+    }
+    else{
+        PLASSERT(transformDistribution.length() == nbTransforms);
+        PLASSERT(isWellDefined(transformDistribution));
+    }
+
+
+     //reconstruction set 
+    reconstructionSet.resize(nbReconstructions);
+
+
+}
+
+
 //INITIALIZATION METHODS 
 
 
 //! initialization operations that have to be done before the training
 //!WARNING: the trainset (&quot;train_set&quot;) must be given
-void TransformationLearner::trainBuild(){
+void TransformationLearner::mainLearnerBuild(){
+    int defaultPeriod = 1;
+    int defaultTransformsOffset;
+    int defaultBiasOffset;
+    int defaultNoiseVarianceOffset;
+    int defaultTransformDistributionOffset;
+
+    defaultTransformsOffset = 0;
     
-    int nbOptimizations =1;
-    int defaultTransformsOffset =0;
-    int defaultBiasOffset ;
-    int defaultNoiseVarianceOffset ;
-    int defaultTransformDistributionOffset ;
     if(withBias){
-        defaultBiasOffset = nbOptimizations ;
-        nbOptimizations ++;
+        defaultBiasOffset = defaultPeriod ;
+        defaultPeriod++;
     }
     if(learnNoiseVariance){
-        defaultNoiseVarianceOffset = nbOptimizations;
-        nbOptimizations ++;
+        defaultNoiseVarianceOffset = defaultPeriod;
+        defaultPeriod++;
     }
     if(learnTransformDistribution){
-        defaultTransformDistributionOffset = nbOptimizations;
-        nbOptimizations ++;
+        defaultTransformDistributionOffset = defaultPeriod;
+        defaultPeriod ++;
     }
-
+    
+    
     transformsSD = sqrt(transformsVariance);
     
     //DIMENSION VARIABLES
@@ -851,43 +826,32 @@
     nbReconstructions = trainingSetLength * nbTargetReconstructions;
     
     
-    //LEARNED MODEL PARAMETERS
     
-    //set of transformations (represented as a single matrix)
-    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
-    
-    //view on the set of transformations (vector)
-    //    each transformation = one matrix 
-    transforms.resize(nbTransforms);
-    for(int k = 0; k&lt; nbTransforms; k++){
-        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
-    }
-    
+
     if(withBias){
-        biasSet = Mat(nbTransforms,inputSpaceDim);
         if(biasPeriod == UNDEFINED || biasOffset == UNDEFINED){
-            biasPeriod = nbOptimizations;
+            biasPeriod = defaultPeriod;
             biasOffset = defaultBiasOffset;
         }
     }
+
     else{
-        biasPeriod = UNDEFINED;
+        biasPeriod = UNDEFINED ;
         biasOffset = UNDEFINED;
     }
 
-    initTransformsParameters();
+ 
 
-  
    
     if(transformsPeriod == UNDEFINED || transformsOffset == UNDEFINED){
-        transformsPeriod = nbOptimizations;
+        transformsPeriod = defaultPeriod;
         transformsOffset = defaultTransformsOffset;
     }
 
     //training parameters for noise variance
     if(learnNoiseVariance){
         if(noiseVariancePeriod == UNDEFINED || noiseVarianceOffset == UNDEFINED){
-            noiseVariancePeriod = nbOptimizations;
+            noiseVariancePeriod = defaultPeriod;
             noiseVarianceOffset = defaultNoiseVarianceOffset;
         }
         if(regOnNoiseVariance){
@@ -907,20 +871,12 @@
         noiseVarianceOffset = UNDEFINED;
     }
     
-    //initialize the noise variance
-     if(noiseVariance == UNDEFINED){
-        if(learnNoiseVariance &amp;&amp; regOnNoiseVariance){
-            initNoiseVariance();
-        }
-        else{
-            noiseVariance = 1.0;
-        }
-     }
+ 
     
      //training parameters for transformation distribution
      if(learnTransformDistribution){
          if(transformDistributionPeriod == UNDEFINED || transformDistributionOffset == UNDEFINED){
-             transformDistributionPeriod = nbOptimizations;
+             transformDistributionPeriod = defaultPeriod;
              transformDistributionOffset = defaultTransformDistributionOffset;
          }
          if(regOnTransformDistribution){
@@ -935,39 +891,18 @@
      else{
          transformDistributionPeriod = UNDEFINED;
          transformDistributionOffset = UNDEFINED;
-         
      }
 
 
-    //transformDistribution
-    if(transformDistribution.length() == 0){
-        if(learnTransformDistribution &amp;&amp; regOnTransformDistribution)
-            initTransformDistribution();
-        else{
-            transformDistribution.resize(nbTransforms);
-            real w = INIT_weight(1.0/nbTransforms);
-            for(int k=0; k&lt;nbTransforms ; k++){
-                transformDistribution[k] = w;
-            }
-        }       
-    }
-    else{
-        PLASSERT(transformDistribution.length() == nbTransforms);
-        PLASSERT(isWellDefined(transformDistribution));
-    }
-
-    //reconstruction set 
-    reconstructionSet.resize(nbReconstructions);
+ 
+   
     
     
 
     //OTHER VARIABLES
     
     
-    //to store a view on the generation set 
-    //   (entries related to a specific target)
-    targetReconstructionSet.resize(nbTargetReconstructions);
-    
+     
     //Storage space used in the update of the transformation parameters
     B_C = Mat(2 * nbTransforms * inputSpaceDim , inputSpaceDim);
     
@@ -981,8 +916,6 @@
     }
     
     
-    target.resize(inputSpaceDim);
-    neighbor.resize(inputSpaceDim);
 }
 
 
@@ -1010,7 +943,9 @@
     if(withBias){
         biasSet = Mat(nbTransforms,inputSpaceDim);
     }
-
+    else{
+        biasSet = Mat(0,0);
+    }
     if(transforms_.length() == 0){
         initTransformsParameters();
     }
@@ -1026,7 +961,7 @@
     if(noiseBeta &lt;= 0){
         noiseBeta = 1;
     }
-    if(noiseVariance_ &lt; 0){
+    if(noiseVariance_ &lt;= 0){
         initNoiseVariance();
     }
     else{
@@ -1049,21 +984,21 @@
 void TransformationLearner::initTransformsParameters()
 {
     
-    transformsSet.resize(nbTransforms*inputSpaceDim, inputSpaceDim);
+    transformsSet .resize(nbTransforms*inputSpaceDim, inputSpaceDim);
     transforms.resize(nbTransforms);
     for(int k = 0; k&lt; nbTransforms; k++){
         transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
     }
-    int idx = 0;
     for(int t=0; t&lt;nbTransforms ; t++){
-        transforms[t] = transformsSet.subMatRows(idx,inputSpaceDim);
-        idx += inputSpaceDim;
         random_gen-&gt;fill_random_normal(transforms[t], 0 , transformsSD);
     }
     if(withBias){
         biasSet = Mat(nbTransforms,inputSpaceDim);
         random_gen-&gt;fill_random_normal(biasSet, 0,transformsSD);
     }
+    else{
+        biasSet = Mat(0,0);
+    }
     if(transformFamily == TRANSFORM_FAMILY_LINEAR){
         for(int t=0; t&lt;nbTransforms;t++){
             addToDiagonal(transforms[t],1.0);
@@ -1100,6 +1035,9 @@
         biasSet = Mat(nbTransforms, inputSpaceDim);
         biasSet &lt;&lt; biasSet_;
     }
+    else{
+        biasSet = Mat(0,0);
+    }
     
 
 }
@@ -1164,7 +1102,7 @@
     int d = source.length();
     PLASSERT(d == inputSpaceDim);
     PLASSERT(sample.length() == inputSpaceDim);
-    PLASSERT(0&lt;=transformIdx&lt;nbTransforms);
+    PLASSERT(0&lt;= transformIdx &amp;&amp; transformIdx&lt;nbTransforms);
     
     //apply the transformation
     applyTransformationOn(transformIdx,source,sample);
@@ -1178,7 +1116,7 @@
 //!generates a sample data point from a source data point and returns it
 //! (if transformIdx &gt;= 0 , we use the corresponding transformation )
 Vec TransformationLearner::returnPredictedFrom(Vec source,
-                                               int transformIdx)
+                                               int transformIdx)const
 {
     Vec sample;
     sample.resize(inputSpaceDim);
@@ -1226,7 +1164,7 @@
 // - (*) if transformIdx&gt;=0, we always use the corresponding transformation
 Mat TransformationLearner::returnGeneratedSamplesFrom(Vec center,
                                                       int n,
-                                                      int transformIdx)
+                                                      int transformIdx)const
 {
     Mat samples = Mat(n,inputSpaceDim);
     if(transformIdx&lt;0)
@@ -1245,7 +1183,8 @@
     for(int i=0; i&lt;nbTransforms; i++){
         probaTransformDistribution[i]=PROBA_weight(transformDistribution[i]);
     }
-    return random_gen-&gt;multinomial_sample(probaTransformDistribution);
+    int w= random_gen-&gt;multinomial_sample(probaTransformDistribution);
+    return w;
 }
 
 //!Select a neighbor in the training set randomly
@@ -1286,7 +1225,8 @@
 void TransformationLearner::treeDataSet(const Vec &amp; root,
                                         int deepness,
                                         int branchingFactor,
-                                        Mat &amp; dataPoints)
+                                        Mat &amp; dataPoints,
+                                        int transformIdx)const
 {
 
     PLASSERT(root.length() == inputSpaceDim);
@@ -1309,14 +1249,20 @@
         
         Vec v = dataPoints(centerIdx);
         Mat m = dataPoints.subMatRows(dataIdx, branchingFactor);
-        batchGeneratePredictedFrom(v,m); 
+        if(transformIdx&gt;=0){
+            batchGeneratePredictedFrom(v,m,transformIdx);
+        }
+        else{
+            batchGeneratePredictedFrom(v,m);
+        } 
         centerIdx ++ ;
     }  
 }
 
 Mat TransformationLearner::returnTreeDataSet(Vec root,
                                              int deepness,
-                                             int branchingFactor)
+                                             int branchingFactor,
+                                             int transformIdx)const
 {
     Mat dataPoints;
     treeDataSet(root,deepness,branchingFactor, dataPoints);
@@ -1329,25 +1275,29 @@
 //! (where &quot;-&gt;&quot; stands for : &quot;generate the&quot;)
 void TransformationLearner::sequenceDataSet(const Vec &amp; start,
                                             int n,
-                                            Mat &amp; dataPoints)
+                                            Mat &amp; dataPoints,
+                                            int transformIdx)const
 {
-    treeDataSet(start,n-1,1,dataPoints);
+    treeDataSet(start,n-1,1,dataPoints , transformIdx);
 }
 
 Mat TransformationLearner::returnSequenceDataSet(Vec start,
-                                                 int n)
+                                                 int n,
+                                                 int transformIdx)const
 {
     Mat dataPoints;
-    sequenceDataSet(start,n,dataPoints);
+    sequenceDataSet(start,n,dataPoints,transformIdx);
     return dataPoints;
 }
 
 
+
+
 //! COPIES OF THE STRUCTURES
 
 
 //!returns the &quot;idx&quot;th data point in the training set
-Vec TransformationLearner::returnTrainingPoint(int idx)
+Vec TransformationLearner::returnTrainingPoint(int idx)const
 {
     
     Vec v,temp;
@@ -1360,7 +1310,7 @@
  
 
 //!returns all the reconstructions candidates associated to a given target
-TVec&lt;ReconstructionCandidate&gt; TransformationLearner::returnReconstructionCandidates(int targetIdx)
+TVec&lt;ReconstructionCandidate&gt; TransformationLearner::returnReconstructionCandidates(int targetIdx)const
 {
    
     int startIdx = targetIdx * nbTargetReconstructions;  
@@ -1371,7 +1321,7 @@
 
 //!returns the reconstructions of the &quot;targetIdx&quot;th data point value in the training set
 //!(one reconstruction for each reconstruction candidate)
-Mat TransformationLearner::returnReconstructions(int targetIdx)
+Mat TransformationLearner::returnReconstructions(int targetIdx)const
 {
     Mat reconstructions = Mat(nbTargetReconstructions,inputSpaceDim);
     int candidateIdx = targetIdx*nbTargetReconstructions;
@@ -1379,7 +1329,9 @@
     for(int i=0; i&lt;nbTargetReconstructions; i++){
         neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
         transformIdx= reconstructionSet[candidateIdx].transformIdx;
-        seeNeighbor(neighborIdx);
+        Vec neighbor;
+        neighbor.resize(inputSpaceDim);
+        seeNeighbor(neighborIdx, neighbor);
         Vec v = reconstructions(i);
         applyTransformationOn(transformIdx, neighbor, v);
         candidateIdx ++;
@@ -1389,14 +1341,16 @@
 
 //!returns the neighbors choosen to reconstruct the target
 //!(one choosen neighbor for each reconstruction candidate associated to the target)
-Mat TransformationLearner::returnNeighbors(int targetIdx)
+Mat TransformationLearner::returnNeighbors(int targetIdx)const
 {
     int candidateIdx = targetIdx*nbTargetReconstructions;
     int neighborIdx;
     Mat neighbors = Mat(nbTargetReconstructions, inputSpaceDim);
     for(int i=0; i&lt;nbTargetReconstructions; i++){
         neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
-        seeNeighbor(neighborIdx);
+        Vec neighbor;
+        neighbor.resize(inputSpaceDim);
+        seeNeighbor(neighborIdx, neighbor);
         neighbors(i) &lt;&lt; neighbor;
         candidateIdx++;
     }
@@ -1405,7 +1359,7 @@
 
 
 //!returns the parameters of the &quot;transformIdx&quot;th transformation
-Mat TransformationLearner::returnTransform(int transformIdx)
+Mat TransformationLearner::returnTransform(int transformIdx)const
 {
     return transforms[transformIdx].copy();    
 }
@@ -1413,7 +1367,7 @@
 //!returns the parameters of each transformation
 //!(as an KdXd matrix, K = number of transformations,
 //!                    d = dimension of input space)
-Mat TransformationLearner::returnAllTransforms()
+Mat TransformationLearner::returnAllTransforms()const
 {
     return transformsSet.copy();    
 }
@@ -1425,7 +1379,8 @@
 
 //! stores a VIEW on the reconstruction candidates related to the specified
 //! target (into the variable &quot;targetReconstructionSet&quot; )
-void TransformationLearner::seeTargetReconstructionSet(int targetIdx)
+void TransformationLearner::seeTargetReconstructionSet(int targetIdx, 
+                                                       TVec&lt;ReconstructionCandidate&gt; &amp; targetReconstructionSet)const
 {
     int startIdx = targetIdx *nbTargetReconstructions;
     targetReconstructionSet = reconstructionSet.subVec(startIdx, 
@@ -1434,17 +1389,17 @@
 
 // stores the &quot;targetIdx&quot;th point in the training set into the variable
 // &quot;target&quot;
-void TransformationLearner::seeTarget(const int targetIdx)
+void TransformationLearner::seeTarget(const int targetIdx, Vec &amp; storage)const
 {
     Vec v;
     real w;
-    train_set-&gt;getExample(targetIdx,target,v,w);
+    train_set-&gt;getExample(targetIdx,storage,v,w);
     
 }
 
 // stores the &quot;neighborIdx&quot;th input in the training set into the variable
 // &quot;neighbor&quot; 
-void TransformationLearner::seeNeighbor(const int neighborIdx)
+void TransformationLearner::seeNeighbor(const int neighborIdx, Vec &amp; neighbor)const
 {
     Vec v;
     real w;
@@ -1459,14 +1414,14 @@
 
 //!returns a pseudo-random positive real number x  
 //!using the distribution p(x)=Gamma(alpha,beta)
-real TransformationLearner::gamma_sample(real alpha, real beta)
+real TransformationLearner::gamma_sample(real alpha, real beta)const
 {
   real c,x,u,d,v;
+  c = 1.0/3.0;
+  d = alpha - c ;
   do{
-      c = 1.0/3.0;
       x = random_gen-&gt;gaussian_01();
-      u = random_gen-&gt;uniform_sample();
-      d = alpha - c ;
+      u = random_gen-&gt;uniform_sample();    
       v = pow((1 + x/(pow(9*d , 0.5)))  ,3.0);
   }
   while(pl_log(u) &lt; 0.5*pow(x,2) + d - d*v + d*pl_log(v));
@@ -1486,7 +1441,7 @@
 //!using the distribution p(x) = Dirichlet(x| all the parameters = alpha)
 //!-all the element of the vector are between 0 and 1,
 //!-the elements of the vector sum to 1
-void TransformationLearner::dirichlet_sample(real alpha, Vec &amp; sample){
+void TransformationLearner::dirichlet_sample(real alpha, Vec &amp; sample)const{
     int d = sample.length();
     real sum = 0;
     for(int i=0;i&lt;d;i++){
@@ -1497,7 +1452,8 @@
         sample[i]/=sum;
     }
 }
-Vec TransformationLearner::return_dirichlet_sample(real alpha)
+
+Vec TransformationLearner::return_dirichlet_sample(real alpha)const
 {
     Vec sample ;
     sample.resize(inputSpaceDim);
@@ -1526,7 +1482,7 @@
 
 //!normalizes the reconstruction weights related to a given target.
 void TransformationLearner::normalizeTargetWeights(int targetIdx,
-                                                   real totalWeight)const
+                                                   real totalWeight)
 {
     real w;
     int startIdx = targetIdx * nbTargetReconstructions;
@@ -1608,7 +1564,7 @@
 
 //!update/compute the weight of a reconstruction candidate with
 //!the actual transformation parameters
-real TransformationLearner::updateReconstructionWeight(int candidateIdx)
+real TransformationLearner::updateReconstructionWeight(int candidateIdx) 
 {
     int targetIdx = reconstructionSet[candidateIdx].targetIdx;
     int neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
@@ -1620,7 +1576,7 @@
     reconstructionSet[candidateIdx].weight = w;
     return w; 
 }
-real TransformationLearner::computeReconstructionWeight(const ReconstructionCandidate &amp; gc)
+real TransformationLearner::computeReconstructionWeight(const ReconstructionCandidate &amp; gc)const
 {
     return computeReconstructionWeight(gc.targetIdx,
                                        gc.neighborIdx,
@@ -1628,18 +1584,23 @@
 }
 real TransformationLearner::computeReconstructionWeight(int targetIdx,
                                                         int neighborIdx,
-                                                        int transformIdx)
+                                                        int transformIdx)const
 {
-    seeTarget(targetIdx);
+
+    Vec target;
+    target.resize(inputSpaceDim);
+    seeTarget(targetIdx,target);
     return computeReconstructionWeight(target,
                                        neighborIdx,
                                        transformIdx);
 }
 real TransformationLearner::computeReconstructionWeight(const Vec &amp; target_,
                                                         int neighborIdx,
-                                                        int transformIdx)
+                                                        int transformIdx)const
 {
-    seeNeighbor(neighborIdx);
+    Vec neighbor;
+    neighbor.resize(inputSpaceDim);
+    seeNeighbor(neighborIdx, neighbor);
     Vec predictedTarget ;
     predictedTarget.resize(inputSpaceDim);
     applyTransformationOn(transformIdx, neighbor, predictedTarget);
@@ -1675,7 +1636,7 @@
 //! those probabilities sum to 1 . 
 //!(typical case: the distribution is represented as a set of weights, which are typically
 //! log-probabilities)
-bool  TransformationLearner::isWellDefined(Vec &amp; distribution)
+bool  TransformationLearner::isWellDefined(Vec &amp; distribution)const
 {  
     if(nbTransforms != distribution.length()){
         return false;
@@ -1684,7 +1645,7 @@
     real proba;
     for(int i=0; i&lt;nbTransforms;i++){
         proba = PROBA_weight(distribution[i]);
-        if(proba &lt; 0 || proba &gt; 1){
+        if(proba &lt; 0 || proba &gt;1){
             return false;
         }
         sum += proba;
@@ -1795,14 +1756,18 @@
     PLASSERT(pq.empty()); 
   
     //capture the target from his index in the training set
-    seeTarget(targetIdx);
-     
+    Vec target;
+    target.resize(inputSpaceDim);
+    seeTarget(targetIdx, target);
+    
     //for each potential neighbor,
     real dist;    
     for(int i=0; i&lt;trainingSetLength; i++){
         if(i != targetIdx){ //(the target cannot be his own neighbor)
             //computes the distance to the target
-            seeNeighbor(i);
+            Vec neighbor;
+            neighbor.resize(inputSpaceDim);
+            seeNeighbor(i, neighbor);
             dist = powdistance(target, neighbor); 
             //if the distance is among &quot;nbNeighbors&quot; smallest distances seen,
             //keep it until to see a closer neighbor. 
@@ -2046,10 +2011,10 @@
     if(transformDistributionPeriod &gt; 0 &amp;&amp; 
        stage % transformDistributionPeriod == transformDistributionOffset)
         MStepTransformDistribution();
+    if(biasPeriod &gt; 0 &amp;&amp; stage % biasPeriod == biasOffset)
+        MStepBias();
     if(stage % transformsPeriod == transformsOffset)
         MStepTransformations();
-    if(stage % biasPeriod == biasOffset)
-        MStepBias();
     
 }
 
@@ -2095,7 +2060,7 @@
     transformDistribution &lt;&lt; newDistribution ;
 }
 
-//!maximization step with respect to transformation matrices
+//!maximization step with respect to transformation parameters
 //!(MAP version)
 void TransformationLearner::MStepTransformations()
 {
@@ -2113,8 +2078,12 @@
         real p = PROBA_weight(reconstructionSet[idx].weight);
   
         //catch the target and neighbor points from the training set
-        seeTarget(reconstructionSet[idx].targetIdx);
-        seeNeighbor(reconstructionSet[idx].neighborIdx);
+        Vec target;
+        target.resize(inputSpaceDim);
+        seeTarget(reconstructionSet[idx].targetIdx, target);
+        Vec neighbor;
+        neighbor.resize(inputSpaceDim);
+        seeNeighbor(reconstructionSet[idx].neighborIdx, neighbor);
         
         int t = reconstructionSet[idx].transformIdx;
         
@@ -2136,40 +2105,10 @@
     }  
 }
  
-//!maximization step with respect to transformation bias
-//!(MAP version)
+
+//TODO
 void TransformationLearner::MStepBias(){
-    Mat  newBiasSet;
-    newBiasSet.resize(nbTransforms,inputSpaceDim);
-    for(int i=0; i&lt;nbTransforms; i++){
-        for(int j =0; j&lt;inputSpaceDim; j++){
-            newBiasSet[i][j]= 0; 
-        }
-    }
-    int transformIdx;
-    real proba;
-    real w;
-    real sum = INIT_weight(0);
-    Vec reconstruction;
-    reconstruction.resize(inputSpaceDim);
-    for(int idx=0; idx&lt;nbReconstructions ; idx++){
-        transformIdx = reconstructionSet[idx].transformIdx;
-        w = reconstructionSet[idx].weight;
-        proba = PROBA_weight(w);
-        sum = SUM_weights(sum, w);
-        seeNeighbor(reconstructionSet[idx].neighborIdx);
-        if(transformFamily == TRANSFORM_FAMILY_LINEAR){
-            transposeProduct(reconstruction,transforms[transformIdx],neighbor);
-        }
-        else{
-            transposeProduct(reconstruction,transforms[transformIdx],neighbor);
-            reconstruction += neighbor;
-        }
-        seeTarget(reconstructionSet[idx].targetIdx);
-        newBiasSet += proba*(target - reconstruction);
-    }
-    newBiasSet = newBiasSet/( noiseVariance/transformsVariance + PROBA_weight(sum) );
-    biasSet &lt;&lt; newBiasSet;
+    
 }
 
 
@@ -2200,8 +2139,13 @@
 //!returns the distance between the reconstruction and the target
 //!for the 'candidateIdx'th reconstruction candidate
 real TransformationLearner::reconstructionEuclideanDistance(int candidateIdx){
-    seeTarget(reconstructionSet[candidateIdx].targetIdx);
-    seeNeighbor(reconstructionSet[candidateIdx].neighborIdx);
+    Vec target;
+    target.resize(inputSpaceDim);
+    seeTarget(reconstructionSet[candidateIdx].targetIdx, target);
+    Vec neighbor;
+    neighbor.resize(inputSpaceDim);
+    seeNeighbor(reconstructionSet[candidateIdx].neighborIdx,
+                neighbor);
     Vec reconstruction;
     reconstruction.resize(inputSpaceDim);
     applyTransformationOn(reconstructionSet[candidateIdx].transformIdx,
@@ -2211,16 +2155,6 @@
 }
 
 
-//!STOPPING CRITERION
-
-
-//!stages == nstages?
-bool TransformationLearner::stoppingCriterionReached()
-{
-   
-    return stage==nstages;
-}
-
 //!increments the variable 'stage' of 1 
 void TransformationLearner::nextStage(){
     stage ++;

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-31 00:18:20 UTC (rev 7874)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-07-31 03:48:46 UTC (rev 7875)
@@ -248,7 +248,7 @@
     //!(see MStep() for more details)
     int transformDistributionPeriod;
     int transformDistributionOffset;
-
+    
     //!This parameter have to be defined if the transformation distribution
     //!is learned using a MAP procedure. We suppose that this distribution have a a multinomial form
     //(u1,u2,...,uK) with dirichlet prior probability : 
@@ -258,13 +258,12 @@
     real transformDistributionAlpha;
 
 
-    //!tells us when to update the transformation matrices and bias
+    //!tells us when to update the transformation parameters
     int transformsPeriod;
     int transformsOffset;
     int biasPeriod;
     int biasOffset;
 
-
     //PARAMETERS OF THE DISTRIBUTION
 
     //! variance of the NOISE random variable. 
@@ -307,23 +306,11 @@
     //#####  PDistribution Member Functions  ##################################
 
     //! Return log of probability density log(p(y | x)).
-    virtual real log_density(const Vec&amp; y) ;//const;
+    virtual real log_density(const Vec&amp; y) const;
 
-    //! Return survival function: P(Y&gt;y | x).
-    virtual real survival_fn(const Vec&amp; y) const;
-
-    //! Return cdf: P(Y&lt;y | x).
-    virtual real cdf(const Vec&amp; y) const;
-
-    //! Return E[Y | x].
-    virtual void expectation(Vec&amp; mu) const;
-
-    //! Return Var[Y | x].
-    virtual void variance(Mat&amp; cov) const;
-
     //! Return a pseudo-random sample generated from the conditional
     //! distribution, of density p(y | x).
-    virtual void generate(Vec&amp; y); //const ;
+    virtual void generate(Vec&amp; y) const ;
 
     //### Override this method if you need it (and if your distribution can
     //### handle it. Default version calls PLERROR.
@@ -339,17 +326,6 @@
     //! given seed.
     // virtual void resetGenerator(long g_seed) const;
 
-    //! Set the 'predictor' and 'predicted' sizes for this distribution.
-    //### See help in PDistribution.h.
-    virtual bool setPredictorPredictedSizes(int the_predictor_size,
-                                            int the_predicted_size,
-                                            bool call_parent = true);
-
-    //! Set the value for the predictor part of a conditional probability.
-    //### See help in PDistribution.h.
-    virtual void setPredictor(const Vec&amp; predictor, bool call_parent = true)
-                              const;
-
     // ### These methods may be overridden for efficiency purpose:
     /*
     //### Default version calls exp(log_density(y))
@@ -421,7 +397,9 @@
     //!(bias are set to 0)
     void setTransformsParameters(TVec&lt;Mat&gt;  transforms, Mat bias=Mat());
     
-    
+   
+
+
     //!initializes the noise variance randomly
     //!(gamma distribution)
     void initNoiseVariance();
@@ -447,7 +425,7 @@
 
     //!generates a sample data point from a source data point and returns it
     //! (if transformIdx &gt;= 0 , we use the corresponding transformation )
-    Vec returnPredictedFrom(Vec source, int transformIdx=-1);
+    Vec returnPredictedFrom(Vec source, int transformIdx=-1)const ;
     
 
     //!fill the matrix &quot;samples&quot; with data points obtained from a given center data point
@@ -465,7 +443,7 @@
     //                          2) apply it on center
     //                          3) add noise)
     // - (*) if transformIdx&gt;=0, we always use the corresponding transformation
-    Mat returnGeneratedSamplesFrom(Vec center, int n, int transformIdx=-1);
+    Mat returnGeneratedSamplesFrom(Vec center, int n, int transformIdx=-1)const;
     
     
     //!select a transformation randomly (with respect to our multinomial distribution)
@@ -504,10 +482,12 @@
     void treeDataSet(const Vec &amp;root,
                      int deepness,
                      int branchingFactor,
-                     Mat &amp; dataPoints);
+                     Mat &amp; dataPoints,
+                     int transformIdx = -1)const;
     Mat returnTreeDataSet(Vec root,
                           int deepness,
-                          int branchingFactor);
+                          int branchingFactor,
+                          int transformIdx =-1)const;
     
 
     //!create a &quot;sequential&quot; dataset:
@@ -515,9 +495,10 @@
     //! (where &quot;-&gt;&quot; stands for : &quot;generate the&quot;)
     void sequenceDataSet(const Vec &amp; start,
                          int n,
-                         Mat &amp; dataPoints);
+                         Mat &amp; dataPoints,
+                         int transformIdx=-1)const;
 
-    Mat returnSequenceDataSet(Vec start,int n);
+    Mat returnSequenceDataSet(Vec start,int n, int transformIdx=-1)const;
 
   
 
@@ -528,26 +509,26 @@
     //!COPIES OF THE STRUCTURES
 
     //!returns the &quot;idx&quot;th data point in the training set
-    Vec returnTrainingPoint(int idx);
+    Vec returnTrainingPoint(int idx)const;
 
     //!returns all the reconstructions candidates associated to a given target
-    TVec&lt;ReconstructionCandidate&gt; returnReconstructionCandidates(int targetIdx);
+    TVec&lt;ReconstructionCandidate&gt; returnReconstructionCandidates(int targetIdx)const;
 
     //!returns the reconstructions of the &quot;targetIdx&quot;th data point value in the training set
     //!(one reconstruction for each reconstruction candidate)
-    Mat returnReconstructions(int targetIdx);
+    Mat returnReconstructions(int targetIdx)const;
 
     //!returns the neighbors choosen to reconstruct the target
     //!(one choosen neighbor for each reconstruction candidate associated to the target)
-    Mat returnNeighbors(int targetIdx);
+    Mat returnNeighbors(int targetIdx)const;
 
     //!returns the parameters of the &quot;transformIdx&quot;th transformation
-    Mat returnTransform(int transformIdx);
+    Mat returnTransform(int transformIdx)const;
 
     //!returns the parameters of each transformation
     //!(as an KdXd matrix, K = number of transformations,
     //!                    d = dimension of input space)
-    Mat returnAllTransforms();
+    Mat returnAllTransforms()const;
 
 
     //OTHER BUILDING/INITIALIZATION METHODS 
@@ -555,9 +536,12 @@
     // Simply calls inherited::build() then build_()
     virtual void build();
     
-    //! initialization operations that have to be done before the training
-    void trainBuild();
+    //! main initialization operations that have to be done before any training phase
+    void mainLearnerBuild();
     
+    void buildLearnedParameters();
+    
+
     //! initialization operations that have to be done before a generation process
     //! (all the undefined parameters will be initialized  randomly)
     void generatorBuild(int inputSpaceDim_=2,
@@ -640,10 +624,8 @@
     //!Each matrix is a view on a sub-matrix in th bigger matrix &quot;B_C&quot; described above.
     TVec&lt;Mat&gt; B,C;
     
-    //!To get easily a view on an input point from the training set
-    Vec target, neighbor;
+   
     
-    
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -666,13 +648,14 @@
     
     //! stores a VIEW on the reconstruction candidates related to the specified
     //! target (into the variable &quot;targetReconstructionSet&quot; )
-    void seeTargetReconstructionSet(int targetIdx) ;
+    void seeTargetReconstructionSet(int targetIdx,
+                                    TVec&lt;ReconstructionCandidate&gt; &amp; targetReconstructionSet)const ;
     // stores the &quot;targetIdx&quot;th point in the training set into the variable
     // &quot;target&quot;
-    void seeTarget(const int targetIdx) ;
+    void seeTarget(const int targetIdx, Vec &amp; target) const ;
     // stores the &quot;neighborIdx&quot;th input in the training set into the variable
     // &quot;neighbor&quot; 
-    void seeNeighbor(const int neighborIdx);
+    void seeNeighbor(const int neighborIdx, Vec &amp; neighbor)const;
 
 
     //!GENERATE GAMMA RANDOM VARIABLES
@@ -681,7 +664,7 @@
     
     //!returns a pseudo-random positive real number x  
     //!using the distribution p(x)=Gamma(alpha,beta)
-    real gamma_sample(real alpha,real beta=1);
+    real gamma_sample(real alpha,real beta=1)const;
     
     
     //!GENERATE DIRICHLET RANDOM VARIABLES
@@ -691,8 +674,8 @@
     //!using the distribution p(x) = Dirichlet(x| all the parameters = alpha)
     //!-all the element of the vector are between 0 and 1,
     //!-the elements of the vector sum to 1
-    void dirichlet_sample(real alpha, Vec &amp; sample);
-    Vec return_dirichlet_sample(real alpha);
+    void dirichlet_sample(real alpha, Vec &amp; sample)const;
+    Vec return_dirichlet_sample(real alpha)const;
 
     
   
@@ -701,7 +684,7 @@
     //!OPERATIONS ON WEIGHTS 
     
      //!normalizes the reconstruction weights related to a given target. 
-    void normalizeTargetWeights(int targetIdx, real totalWeight) const;
+    void normalizeTargetWeights(int targetIdx, real totalWeight);
     
     //!returns a random weight 
     real randomWeight() const;
@@ -717,13 +700,13 @@
     //!update/compute the weight of a reconstruction candidate with
     //!the actual transformation parameters
     real updateReconstructionWeight(int candidateIdx);
-    real computeReconstructionWeight(const ReconstructionCandidate &amp; gc);
+    real computeReconstructionWeight(const ReconstructionCandidate &amp; gc) const;
     real computeReconstructionWeight(int targetIdx, 
                                      int neighborIdx, 
-                                     int transformIdx);
+                                     int transformIdx) const;
     real computeReconstructionWeight(const Vec &amp; target,
                                      int neighborIdx,
-                                     int transformIdx);
+                                     int transformIdx) const;
 
     //!applies &quot;transformIdx&quot;th transformation on data point &quot;src&quot;
     void applyTransformationOn(int transformIdx, const Vec &amp; src , Vec &amp; dst) const ;
@@ -734,7 +717,7 @@
     //! those probabilities sum to 1 . 
     //!(the distribution is represented as a set of weights, which are typically
     //! log-probabilities)
-    bool isWellDefined(Vec &amp; distribution);
+    bool isWellDefined(Vec &amp; distribution)const;
 
     //!INITIAL E STEP 
     
@@ -827,18 +810,14 @@
     //!NOTE :  alpha =1 -&gt;  no regularization
     void MStepTransformDistributionMAP(real alpha);
 
-
-
     //!maximization step with respect to transformation matrices
     //!(MAP version)
     void MStepTransformations();
-    
-
+  
     //!maximization step with respect to transformation bias
     //!(MAP version)
     void MStepBias();
-    
-    
+
     //!maximization step with respect to noise variance
     void MStepNoiseVariance();
     
@@ -851,11 +830,6 @@
     //!for the 'candidateIdx'th reconstruction candidate
     real reconstructionEuclideanDistance(int candidateIdx);
     
-    
-    //STOPPING CRITERION
-    //stage == nstages?
-    bool stoppingCriterionReached();
-    
     //increment the variable 'stage' of 1
     void nextStage();
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001322.html">[Plearn-commits] r7874 - trunk/plearn_learners/generic
</A></li>
	<LI>Next message: <A HREF="001324.html">[Plearn-commits] r7876 -	branches/cgi-desjardin/plearn_learners/second_iteration
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1323">[ date ]</a>
              <a href="thread.html#1323">[ thread ]</a>
              <a href="subject.html#1323">[ subject ]</a>
              <a href="author.html#1323">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
