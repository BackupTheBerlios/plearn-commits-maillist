<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7774 - trunk/python_modules/plearn/learners/autolr
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7774%20-%20trunk/python_modules/plearn/learners/autolr&In-Reply-To=%3C200707140840.l6E8eFTQ008544%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001221.html">
   <LINK REL="Next"  HREF="001223.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7774 - trunk/python_modules/plearn/learners/autolr</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7774%20-%20trunk/python_modules/plearn/learners/autolr&In-Reply-To=%3C200707140840.l6E8eFTQ008544%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7774 - trunk/python_modules/plearn/learners/autolr">lamblin at mail.berlios.de
       </A><BR>
    <I>Sat Jul 14 10:40:15 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001221.html">[Plearn-commits] r7773 - in trunk/plearn_learners: generic	regressors
</A></li>
        <LI>Next message: <A HREF="001223.html">[Plearn-commits] r7775 - trunk/python_modules/plearn/pytest
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1222">[ date ]</a>
              <a href="thread.html#1222">[ thread ]</a>
              <a href="subject.html#1222">[ subject ]</a>
              <a href="author.html#1222">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-07-14 10:40:15 +0200 (Sat, 14 Jul 2007)
New Revision: 7774

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Also report approximate training error


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-14 08:37:22 UTC (rev 7773)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-14 08:40:15 UTC (rev 7774)
@@ -8,18 +8,23 @@
 
 
 if plearn.bridgemode.useserver:
-    servers_lock = Lock() # Lock on the servers list so we don't have race conditions
-    servers = [[serv, 0, Lock()]] # List of [server, amount_of_jobs_server_is_running, lock_on_the_server] lists
-    servers_max = 1e10 # Default maximal number of servers we are willing to run
+    # Lock on the servers list so we don't have race conditions
+    servers_lock = Lock()
+    # List of [server, amount_of_jobs_server_is_running, lock_on_the_server]
+    # lists
+    servers = [[serv, 0, Lock()]]
+    # Default maximal number of servers we are willing to run
+    servers_max = 1e10
 
 def execute(object, tasks, use_threads = False):
     def job(object):
         lock = [servinfo[2] for servinfo in servers if servinfo[0] is object.server][0]
-        lock.acquire() # we will only send our job to the server if nothing is already running
+        # we will only send our job to the server if nothing is already running
+        lock.acquire()
         for method, args in tasks: # do each task sequentially
             getattr(object, method)(*args)
         lock.release()
-        
+
     if plearn.bridgemode.useserver and use_threads:
         t = Thread(target = job, args = (object,))
         return t
@@ -47,7 +52,9 @@
     if nservers &lt; servers_max:
         command = plearn.bridgemode.server_exe + ' server'
         server = launch_plearn_server(command = command)
-        time.sleep(0.5) # give some time to the server to get born and well alive (taken from bridge.py)
+        # give some time to the server to get born and well alive (taken from
+        # bridge.py)
+        time.sleep(0.5)
         lock = Lock()
         servers.append([server, 1, lock])
     else:
@@ -64,7 +71,8 @@
         for servinfo in servers:
             server_, njobs, lock = servinfo
             if server_ is server: # == doesn't work
-                servinfo[1] -= 1 # this server is done so we reduce its job count
+                # this server is done so we reduce its job count
+                servinfo[1] -= 1
                 break
         servers_lock.release()
 
@@ -84,15 +92,17 @@
     with one column per schedule but a unified sequence of stages.
     Each schedule is a Nx2 array, with the first column containing number of
     stages (examples) and the second column containing corresponding learning
-    rates. N may vary across schedules. If we have a schedule first row with [10000 0.01]
-    and a second row with [30000 0.001], it means that from the learner's current stage
-    to stage 10000 (excluded) we should use a learning rate of 0.01, and from stage 10000
-    to 30000 (excluded) we should use a learning rate of 0.001. The different
-    schedules do not have to have the same N nor the same maximum stage. They
-    will be merged in one big schedule ranging from the mininum to the maximum
-    of the stages found in all the schedules.
+    rates. N may vary across schedules. If we have a schedule first row with
+    [10000 0.01] and a second row with [30000 0.001], it means that from the
+    learner's current stage to stage 10000 (excluded) we should use a learning
+    rate of 0.01, and from stage 10000 to 30000 (excluded) we should use a
+    learning rate of 0.001. The different schedules do not have to have the
+    same N nor the same maximum stage. They will be merged in one big schedule
+    ranging from the mininum to the maximum of the stages found in all the
+    schedules.
     The result is an array with the stages in the first column and one
-    additional column (with a sequence of learning rates) for each input schedule.
+    additional column (with a sequence of learning rates) for each input
+    schedule.
     &quot;&quot;&quot;
     n_schedules = len(schedules)
     stages = []
@@ -121,14 +131,14 @@
     res[:,0]=stages
     res[:,1:]=learning_rates
     return res
-    
 
+
 def train_with_schedule(learner,
                         lr_options,
                         schedules,
-                        trainset,testsets,expdir,
+                        trainset, testsets, expdir,
                         cost_to_select_best=0,
-                        selected_costnames = False,
+                        selected_costnames = None,
                         logfile=None):
     &quot;&quot;&quot;Train a learner with one or more schedules of learning rates.
 lr_options is a list of list of option strings. Each list in lr_options
@@ -140,19 +150,26 @@
 column and sequences of learning rates (one sequence per group) in
 each of the other columns (just like the result of the call to merge_schedules).
 &quot;&quot;&quot;
-    costnames = learner.getTestCostNames()
-    if not selected_costnames:
-        # use all cost names if not user-provided
-        selected_costnames=costnames
-    cost_indices = [costnames.index(name) for name in selected_costnames]
+    train_costnames = learner.getTrainCostNames()
+    test_costnames = learner.getTestCostNames()
+
+    # Filter out unwanted costnames
+    if selected_costnames is not None:
+        train_costnames = [ name for name in train_costnames
+                            if name in selected_costnames ]
+        test_costnames = [ name for name in test_costnames
+                           if name in selected_costnames ]
+
+    n_train_costs = len(train_costnames)
+    n_test_costs = len(test_costnames)
+
     learner.setTrainingSet(trainset,False)
     stages = schedules[:,0]
     learning_rates = schedules[:,1:]
     n_train = len(stages)
     n_schedules = len(lr_options)
     n_tests = len(testsets)
-    n_costs = len(costnames)
-    results = zeros([n_train,2+n_tests*n_costs],Float)
+    results = zeros([n_train, 2 + n_train_costs + n_tests*n_test_costs], Float)
     best_err = 1e10
     if plearn.bridgemode.interactive:
         clf()
@@ -169,32 +186,60 @@
         results[i,0] = learner.stage
         for s in range(n_schedules):
             results[i,1+s] = learning_rates[i][s]
-        for j in range(0,n_tests):
+
+        # Report approximate training error
+        train_vsc = learner.getTrainStatsCollector()
+        if train_vsc.fieldnames == []:
+            # learner did not set train_stats fieldnames
+            train_vsc.fieldnames = learner.getTrainCostNames()
+        if logfile:
+            print &gt;&gt;logfile, &quot;At stage &quot;, learner.stage, &quot; train :&quot;,
+        for k, costname in zip(range(n_train_costs), train_costnames):
+            err = train_vsc.getStat('E['+costname+']')
+            results[i, k+1+n_schedules] = err
+            if logfile:
+                print &gt;&gt;logfile, costname, '=', err,
+            if plearn.bridgemode.interactive:
+                plot(   results[0:i+1, 0],
+                        results[0:i+1, 1+n_schedules+k],
+                        colors[k%7]+styles[0],
+                        label='train:'+costname)
+        if logfile:
+            print &gt;&gt;logfile
+            logfile.flush()
+
+        # Report error on test sets
+        for j in range(n_tests):
             ts = pl.VecStatsCollector()
             if plearn.bridgemode.useserver:
                 ts=serv.new(ts)
             learner.test(testsets[j],ts,0,0)
             if logfile:
-                print &gt;&gt;logfile, &quot;At stage &quot;,learner.stage,&quot; test&quot; + str(j+1),&quot;: &quot;,
-            for k in range(0,n_costs):
-                err = ts.getStat(&quot;E[&quot;+str(k)+&quot;]&quot;)
-                results[i,j*n_costs+k+1+n_schedules]=err
-                costname = costnames[cost_indices[k]]
+                print &gt;&gt;logfile, &quot;At stage &quot;, learner.stage, &quot; test&quot; + str(j+1),&quot;: &quot;,
+            for k, costname in zip(range(n_test_costs), test_costnames):
+                err = ts.getStat(&quot;E[&quot;+costname+&quot;]&quot;)
+                results[i, 1+n_schedules+n_train_costs+(j*n_test_costs)+k] = err
                 if logfile:
                     print &gt;&gt;logfile, costname, &quot;=&quot;, err,
                 if k==cost_to_select_best and j==0 and err &lt; best_err:
                     best_err = err
                     learner.save(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;,&quot;plearn_ascii&quot;)
                 if plearn.bridgemode.interactive:
-                    plot(results[0:i+1,0],results[0:i+1,
-                         j*n_costs+k+1+n_schedules],colors[k%7]+styles[j%15],
-                         label='test'+str(j+1)+':'+costname)
+                    plot(results[0:i+1,0],
+                            results[0:i+1, 1+n_schedules+n_train_costs+(j*n_test_costs)+k],
+                            colors[k%7]+styles[(j+1)%15],
+                            label='test'+str(j+1)+':'+costname)
             if logfile:
                 print &gt;&gt;logfile
                 logfile.flush()
         if plearn.bridgemode.interactive and i==0:
             legend()
-    return (['stage']+selected_costnames,results)
+    # Return headers for the result matrix, and the results themselves
+    return (['stage', 'learning_rate']
+            + ['train.' + costname for costname in train_costnames]
+            + ['test'+str(j+1)+'.'+costname for j in range(n_tests)
+                                            for costname in test_costnames],
+            results)
 
 
 def choose_initial_lr(initial_learner,trainset,testset,lr_options,
@@ -205,10 +250,11 @@
                       lr_steps=exp(log(10)/2),
                       logfile=None):
     &quot;&quot;&quot;
-Optimize initial learning rate by exploring greedily from a given initial learning rate
-If call_forget then the provided initial_learner is changed (and not necessarily the
-optimal one) upon return. But if not call_forget then the initial_learner is unchanged
-(we make deep copies internally).
+Optimize initial learning rate by exploring greedily from a given initial
+learning rate.
+If call_forget then the provided initial_learner is changed (and not
+necessarily the optimal one) upon return. But if not call_forget then the
+initial_learner is unchanged (we make deep copies internally).
 &quot;&quot;&quot;
     log_initial_lr=log(initial_lr)
     log_steps=log(lr_steps)
@@ -225,7 +271,7 @@
     def perf(i):
         global best_err
         global best_learner
-        
+
         if call_forget:
             learner = initial_learner
         else:
@@ -259,7 +305,7 @@
         best=bottom
     else:
         best=top
-    
+
     while top-bottom&lt;2 or (best==top or best==bottom):
         if best==top:
             top+=1
@@ -275,40 +321,59 @@
 
 def train_adapting_lr(learner,
                       trainset,testsets,expdir,
-                      lr_options, # List of lists of options (one list/group of options with a given schedule)
-                      optimized_group=0, # Group of options that is actually optimized (if -1, then no optimization is performed)
-                      schedules=None, # Matrix of schedules (number of columns = 1 (stage) + number of groups)
-                      nstages=None, # Used to construct default schedule if 'schedules' is None
+                      # List of lists of options (one list/group of options
+                      # with a given schedule)
+                      lr_options,
+                      # Group of options that is actually optimized
+                      # (if -1, then no optimization is performed)
+                      optimized_group=0,
+                      # Matrix of schedules
+                      # (number of columns = 1 (stage) + number of groups)
+                      schedules=None,
+                      # Used to construct default schedule
+                      # if 'schedules' is None
+                      nstages=None,
                       epoch=None,   # &quot;&quot;
-                      initial_lr=0.1, # Starting value for group being optimized
-                      nskip=2,   # Number of epochs after which we add/remove candidates
-                      cost_to_select_best=0, # Index of cost being optimized
-                      return_best_model=False, # o/w return final model
-                      save_best=False, # for paranoids: save best model every save_best epochs, or not at all (if = False)
-                      selected_costnames = False,
+                      # Starting value for group being optimized
+                      initial_lr=0.1,
+                      # Number of epochs after which we add/remove candidates
+                      nskip=2,
+                      # Index of cost being optimized
+                      cost_to_select_best=0,
+                      # o/w return final model
+                      return_best_model=False,
+                      # for paranoids: save best model every save_best epochs,
+                      # or not at all (if = False)
+                      save_best=False,
+                      selected_costnames = None,
                       min_epochs_to_delete = 2,
-                      lr_steps=exp(log(10)/2), # Scaling coefficient when modifying learning rates
+                      # Scaling coefficient when modifying learning rates
+                      lr_steps=exp(log(10)/2),
                       logfile=False,
-                      min_lr=1e-6, # do not try to go below this learning rate
-                      keep_lr=2, # Learning rate interval for heuristic
+                      # do not try to go below this learning rate
+                      min_lr=1e-6,
+                      # Learning rate interval for heuristic
+                      keep_lr=2,
                       use_threads=False):
 
     if plearn.bridgemode.useserver:
         servers[0][1] = 1 # learner
 
-    min_epochs_to_delete = max(1,min_epochs_to_delete) # although 1 is probably too small
+    # although 1 is probably too small
+    min_epochs_to_delete = max(1, min_epochs_to_delete)
 
     def error_curve(active,start_t,current_t):
         delta_t = current_t+1-start_t
-        # in all_results, column 0 is stage, column 1 is option value (learning rate)
-        # and column 2+test*n_costs+cost is the cost value for cost number 'cost'
-        # (index in the selected_costnames list), in testset 'test'.
+        # in all_results, column 0 is stage, column 1 is option value
+        # (learning rate) and column 2+test*n_costs+cost is the cost value for
+        # cost number 'cost' (index in the selected_costnames list),
+        # in testset 'test'.
         # And testset 0 is the one used for selection.
         return all_results[active][start_t:current_t+1,2+cost_to_select_best]
-        
+
     def error_curve_dominates(c1,c2,t):
-        &quot;&quot;&quot;curve1 has a lower last error than curve2, but
-           will curve2 eventually cross curve1? if yes return False o/w return True&quot;&quot;&quot;
+        &quot;&quot;&quot;curve1 has a lower last error than curve2, but will curve2
+        eventually cross curve1? if yes return False o/w return True&quot;&quot;&quot;
 
         start_t = max(all_start[c1],all_start[c2])
         curve1 = error_curve(c1,start_t,t)
@@ -319,29 +384,42 @@
         slope2=curve2[-1]-curve2[-2]
         if  slope1 &gt;= slope2:
             return False
-        #check to see if c2 is alone with its learning rate (or nearby); if yes keep it
+
+        # check to see if c2 is alone with its learning rate (or nearby);
+        # if yes keep it
         alone=True
         c2lr=all_lr[c2]
         c2err=all_last_err[c2]
         for a in actives:
             if all_lr[a]==c2lr and all_last_err[a]&lt;=c2err:
-                return True # throw it away if worse than other actives of same lr
-            # say that it is alone if there are no other actives with nearby and greater lr
+                # throw it away if worse than other actives of same lr
+                return True
+
+            # say that it is alone if there are no other actives with nearby
+            # and greater lr
             if a!=c2 and all_lr[a]&gt;c2lr and abs(log(all_lr[a]/c2lr))&lt;keep_lr*log(lr_steps):
                 alone=False
         c1lr=all_lr[c1]
-        if alone and c2lr&gt;c1lr: # and slope2&lt;0: 
+        if alone and c2lr&gt;c1lr: # and slope2&lt;0:
             # keep if alone and a larger learning rate and improving
             return False
         return True
-    
-    costnames = learner.getTestCostNames()
-    if not selected_costnames:
-        # use all cost names if not user-provided
-        selected_costnames=costnames
-    cost_indices = [costnames.index(name) for name in selected_costnames]
+
+    train_costnames = learner.getTestCostNames()
+    test_costnames = learner.getTestCostNames()
+    if selected_costnames is not None:
+        # Filter out unwanted costnames
+        train_costnames = [ name for name in train_costnames
+                            if name in selected_costnames ]
+        test_costnames = [ name for name in test_costnames
+                           if name in selected_costnames ]
+
+    #cost_indices = [costnames.index(name) for name in selected_costnames]
     n_tests = len(testsets)
-    n_costs = len(cost_indices)
+    #n_costs = len(cost_indices)
+    n_train_costs = len(train_costnames)
+    n_test_costs = len(test_costnames)
+
     if schedules:
         stages = schedules[:,0]
         learning_rates = schedules[:,1:]
@@ -358,13 +436,14 @@
             raise Error
         if len(lr_options)!=1:
             lr_options=[lr_options[0]]
+
     n_train = len(stages)
     n_schedules = len(lr_options)
     assert n_schedules==learning_rates.shape[1]
     best_err = 1e10
     previous_best_err = best_err
     best_active = -1
-    all_results = [1e10*ones([n_train,2+n_tests*n_costs],Float)]
+    all_results = [1e10*ones([n_train,2+n_train_costs+n_tests*n_test_costs],Float)]
     all_candidates = [learner]
     all_last_err = [best_err]
     all_lr = [initial_lr]
@@ -398,12 +477,17 @@
                         options[lr_option]=str(learning_rates[t,s])
             candidate.changeOptions(options)
             candidate.setTrainingSet(trainset,False)
+            train_vsc = pl.VecStatsCollector();
+            if plearn.bridgemode.useserver:
+                train_vsc = candidate.server.new(train_vsc)
+            candidate.setTrainStatsCollector(train_vsc)
             tasks = [('train', ())]
             stats = []
             for j in range(0,n_tests):
                 ts = pl.VecStatsCollector()
                 if plearn.bridgemode.useserver:
-                    ts = candidate.server.new(ts) # no threads are running so we don't need to lock here
+                    # no threads are running so we don't need to lock here
+                    ts = candidate.server.new(ts)
                 stats.append(ts)
                 tasks.append(('test', (testsets[j], ts, 0, 0)))
             active_stats.append(stats)
@@ -424,18 +508,28 @@
         for active, stats in zip(actives, active_stats):
             candidate = all_candidates[active]
             results = all_results[active]
-        
+
             results[t,0] = candidate.stage
             results[t,1] = all_lr[active]
             if logfile:
                 print &gt;&gt;logfile, &quot;candidate &quot;,active,&quot;:&quot;,
+
+            # Report approximate training statistics
+            if logfile:
+                print &gt;&gt;logfile, 'train :',
+            for k, costname in zip(range(n_train_costs), train_costnames):
+                err = ts.getStat('E['+costname+']')
+                results[t, 2+k] = err
+                if logfile:
+                    print &gt;&gt;logfile, costname, '=', err,
+
+            # Report testing statistics
             for j, ts in zip(range(0,n_tests), stats):
                 if logfile:
                     print &gt;&gt;logfile, &quot; test&quot; + str(j+1),&quot;: &quot;,
-                for k in range(0,n_costs):
-                    err = ts.getStat(&quot;E[&quot;+str(cost_indices[k])+&quot;]&quot;)
-                    results[t,j*n_costs+k+2]=err
-                    costname = costnames[cost_indices[k]]
+                for k, costname in zip(range(n_test_costs), test_costnames):
+                    err = ts.getStat(&quot;E[&quot;+costname+&quot;]&quot;)
+                    results[t, 2+n_train_costs+(j*n_test_costs)+k] = err
                     if logfile:
                         print &gt;&gt;logfile, costname, &quot;=&quot;, err,
                     if k==cost_to_select_best and j==0:
@@ -443,12 +537,14 @@
                             start = all_start[active]
                             if start==t:
                                 plot(results[start:t+1,0],
-                                     results[start:t+1,j*n_costs+k+2],colors[active%7]+styles[j%15],
+                                     results[start:t+1, 2+n_train_costs+(j*n_test_costs)+k],
+                                     colors[active%7]+styles[j%15],
                                      label='candidate'+str(active)+':'+costname)
-                                legend() 
+                                legend()
                             else:
                                 plot(results[start:t+1,0],
-                                     results[start:t+1,j*n_costs+k+2],colors[active%7]+styles[j%15])
+                                     results[start:t+1, 2+n_train_costs+(j*n_test_costs)+k],
+                                     colors[active%7]+styles[j%15])
 
                         all_last_err[active]=err
                         if err &lt; best_err:
@@ -456,7 +552,8 @@
                             best_active = active
                             best_early_stop = stage
                             if return_best_model:
-                                best_candidate = deepcopy(candidate, use_threads)
+                                best_candidate = deepcopy(candidate,
+                                                          use_threads)
             if logfile:
                 print &gt;&gt;logfile
                 logfile.flush()
@@ -467,16 +564,18 @@
             if logfile:
                 print &gt;&gt;logfile,&quot;BEST to now is candidate &quot;,best_active,&quot; with err=&quot;,best_err
                 print &gt;&gt;logfile, &quot;stage\tl.rate\t&quot;,
-                for cost_index in cost_indices:
-                    costname = costnames[cost_index]
-                    print &gt;&gt;logfile, costname+&quot;\t&quot;,
+                for costname in train_costnames:
+                    print &gt;&gt;logfile, 'train.'+costname+&quot;\t&quot;,
                 print &gt;&gt;logfile
+                for costname in test_costnames:
+                    print &gt;&gt;logfile, 'test.'+costname+'\t',
+                print &gt;&gt;logfile
                 for row in all_results[best_active][0:t+1,:]:
                     for val in row:
                         print &gt;&gt;logfile,val,&quot;\t&quot;,
                     print &gt;&gt;logfile
                 print &gt;&gt;logfile
-                
+
         else:
             if logfile:
                 print &gt;&gt;logfile, &quot;THE BEST ACTIVE HAS GOTTEN WORSE!!!!&quot;
@@ -490,14 +589,18 @@
                     if logfile:
                         print &gt;&gt;logfile,&quot;REMOVE candidate &quot;,a
                     release_server(all_candidates[a], use_threads)
-                    all_candidates[a]=None # hopefully this destroys the candidate
+                    # hopefully this destroys the candidate
+                    all_candidates[a]=None
                     del actives[j-ndeleted]
                     ndeleted+=1
-            # add a candidate with slightly lower learning rate than best_active, starting from it
-            new_lr=all_lr[best_active]/lr_steps # only try a smaller learning rate
+            # add a candidate with slightly lower learning rate than
+            # best_active, starting from it
+            # only try a smaller learning rate
+            new_lr=all_lr[best_active]/lr_steps
             if new_lr&gt;=min_lr:
                 all_lr.append(new_lr)
-                new_candidate = deepcopy(all_candidates[best_active], use_threads)
+                new_candidate = deepcopy(all_candidates[best_active],
+                                         use_threads)
                 new_a = len(all_candidates)
                 actives.append(new_a)
                 all_candidates.append(new_candidate)
@@ -515,11 +618,21 @@
         schedules[:,1+optimized_group]=all_results[best_active][:,1]
     if logfile and best_err &lt; all_last_err[best_active]:
         print &gt;&gt;logfile, &quot;WARNING: best performing model would have stopped early at stage &quot;,best_early_stop
-    return (final_model,   # Learner
-            schedules,     # Matrix of schedules (including the one that was optimized)
-            all_results[best_active], # Error curve (matrix) for best model
-            all_results, # List of all error curve matrices
-            all_last_err, # List of all last errors recorded for each candidate (not necessarily at last epoch)
-            all_start,  # Epoch index where each candidate was created (not necessarily the first epoch)
-            best_early_stop) # Timestep (in stages) at which early-stopping should have happened (i.e. stage of best error found)
+    return (# Learner
+            final_model,
+            # Matrix of schedules (including the one that was optimized)
+            schedules,
+            # Error curve (matrix) for best model
+            all_results[best_active],
+            # List of all error curve matrices
+            all_results,
+            # List of all last errors recorded for each candidate
+            # (not necessarily at last epoch)
+            all_last_err,
+            # Epoch index where each candidate was created (not necessarily
+            # the first epoch)
+            all_start,
+            # Timestep (in stages) at which early-stopping should have happened
+            # (i.e. stage of best error found)
+            best_early_stop)
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001221.html">[Plearn-commits] r7773 - in trunk/plearn_learners: generic	regressors
</A></li>
	<LI>Next message: <A HREF="001223.html">[Plearn-commits] r7775 - trunk/python_modules/plearn/pytest
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1222">[ date ]</a>
              <a href="thread.html#1222">[ thread ]</a>
              <a href="subject.html#1222">[ subject ]</a>
              <a href="author.html#1222">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
