<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7691 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7691%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200707040331.l643VIcj020731%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001138.html">
   <LINK REL="Next"  HREF="001140.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7691 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7691%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200707040331.l643VIcj020731%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7691 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Wed Jul  4 05:31:18 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001138.html">[Plearn-commits] r7690 - in trunk: plearn_learners/hyper	plearn_learners_experimental/SurfaceTemplate
</A></li>
        <LI>Next message: <A HREF="001140.html">[Plearn-commits] r7692 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1139">[ date ]</a>
              <a href="thread.html#1139">[ thread ]</a>
              <a href="subject.html#1139">[ subject ]</a>
              <a href="author.html#1139">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-07-04 05:31:17 +0200 (Wed, 04 Jul 2007)
New Revision: 7691

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Cosmetic changes


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-04 02:55:38 UTC (rev 7690)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-07-04 03:31:17 UTC (rev 7691)
@@ -130,7 +130,7 @@
                   OptionBase::buildoption,
         &quot;Connection between the visible and hidden layers.&quot;);
 
-    declareOption(ol, &quot;reconstruction_connection&quot;, 
+    declareOption(ol, &quot;reconstruction_connection&quot;,
                   &amp;RBMModule::reconstruction_connection,
                   OptionBase::buildoption,
         &quot;Reconstuction connection between the hidden and visible layers.&quot;);
@@ -176,7 +176,7 @@
         &quot;- by the usual formula if 'standard_cd_weights_grad' is true\n&quot;
         &quot;- by the true gradient if 'standard_cd_weights_grad' is false.&quot;);
 
-    declareOption(ol, &quot;n_Gibbs_steps_CD&quot;, 
+    declareOption(ol, &quot;n_Gibbs_steps_CD&quot;,
                   &amp;RBMModule::n_Gibbs_steps_CD,
                   OptionBase::buildoption,
                   &quot;Number of Gibbs sampling steps in negative phase of &quot;
@@ -189,7 +189,7 @@
                   &quot;and thus a Gibbs chain has to be run. This option gives the minimum number\n&quot;
                   &quot;of Gibbs steps to perform in the chain before outputting a sample.\n&quot;);
 
-    declareOption(ol, &quot;n_Gibbs_steps_per_generated_sample&quot;, 
+    declareOption(ol, &quot;n_Gibbs_steps_per_generated_sample&quot;,
                   &amp;RBMModule::n_Gibbs_steps_per_generated_sample,
                   OptionBase::buildoption,
                   &quot;Used in generative mode (when visible_sample or hidden_sample is requested)\n&quot;
@@ -204,7 +204,7 @@
                   &quot;Whether to compute the exact RBM generative model's log-likelihood\n&quot;
                   &quot;(on the neg_log_likelihood port). If false then the neg_log_likelihood\n&quot;
                   &quot;port just computes the input visible's free energy.\n&quot;);
-    
+
     declareOption(ol, &quot;minimize_log_likelihood&quot;,
                   &amp;RBMModule::minimize_log_likelihood,
                   OptionBase::buildoption,
@@ -212,7 +212,7 @@
                   &quot;i.e. take stochastic gradient steps w.r.t. the log-likelihood instead\n&quot;
                   &quot;of w.r.t. the contrastive divergence.\n&quot;);
 
-    declareOption(ol, &quot;Gibbs_step&quot;, 
+    declareOption(ol, &quot;Gibbs_step&quot;,
                   &amp;RBMModule::Gibbs_step,
                   OptionBase::learntoption,
                   &quot;Used in generative mode (when visible_sample or hidden_sample is requested)\n&quot;
@@ -220,14 +220,14 @@
                   &quot;Keeps track of the number of steps that have been run since the beginning\n&quot;
                   &quot;of the chain.\n&quot;);
 
-    declareOption(ol, &quot;log_partition_function&quot;, 
+    declareOption(ol, &quot;log_partition_function&quot;,
                   &amp;RBMModule::log_partition_function,
                   OptionBase::learntoption,
                   &quot;log(Z) = log(sum_{h,x} exp(-energy(h,x))\n&quot;
                   &quot;only computed if compute_log_likelihood is true and\n&quot;
                   &quot;the neg_log_likelihood port is requested.\n&quot;);
 
-    declareOption(ol, &quot;partition_function_is_stale&quot;, 
+    declareOption(ol, &quot;partition_function_is_stale&quot;,
                   &amp;RBMModule::partition_function_is_stale,
                   OptionBase::learntoption,
                   &quot;Whether parameters have changed since the last computation\n&quot;
@@ -283,8 +283,8 @@
     addPortName(&quot;visible_expectation&quot;);
     addPortName(&quot;hidden_sample&quot;);
     addPortName(&quot;energy&quot;);
-    addPortName(&quot;hidden_bias&quot;); 
-    addPortName(&quot;weights&quot;); 
+    addPortName(&quot;hidden_bias&quot;);
+    addPortName(&quot;weights&quot;);
     addPortName(&quot;neg_log_likelihood&quot;);
     if(reconstruction_connection)
     {
@@ -309,8 +309,8 @@
     }
     if (hidden_layer) {
         port_sizes(getPortIndex(&quot;hidden.state&quot;), 1) = hidden_layer-&gt;size;
-        port_sizes(getPortIndex(&quot;hidden_activations.state&quot;), 1) = hidden_layer-&gt;size; 
-        port_sizes(getPortIndex(&quot;hidden_sample&quot;), 1) = hidden_layer-&gt;size; 
+        port_sizes(getPortIndex(&quot;hidden_activations.state&quot;), 1) = hidden_layer-&gt;size;
+        port_sizes(getPortIndex(&quot;hidden_sample&quot;), 1) = hidden_layer-&gt;size;
         port_sizes(getPortIndex(&quot;hidden_bias&quot;),1) = hidden_layer-&gt;size;
         if(visible_layer)
             port_sizes(getPortIndex(&quot;weights&quot;),1) = hidden_layer-&gt;size * visible_layer-&gt;size;
@@ -320,20 +320,20 @@
     if(reconstruction_connection)
     {
         if (visible_layer) {
-            port_sizes(getPortIndex(&quot;visible_reconstruction.state&quot;),1) = 
-                visible_layer-&gt;size; 
-            port_sizes(getPortIndex(&quot;visible_reconstruction_activations.state&quot;),1) = 
-                       visible_layer-&gt;size; 
+            port_sizes(getPortIndex(&quot;visible_reconstruction.state&quot;),1) =
+                visible_layer-&gt;size;
+            port_sizes(getPortIndex(&quot;visible_reconstruction_activations.state&quot;),1) =
+                       visible_layer-&gt;size;
         }
-        port_sizes(getPortIndex(&quot;reconstruction_error.state&quot;),1) = 1; 
+        port_sizes(getPortIndex(&quot;reconstruction_error.state&quot;),1) = 1;
     }
     if (compute_contrastive_divergence)
     {
-        port_sizes(getPortIndex(&quot;contrastive_divergence&quot;),1) = 1; 
-        if (visible_layer) 
-            port_sizes(getPortIndex(&quot;negative_phase_visible_samples.state&quot;),1) = visible_layer-&gt;size; 
+        port_sizes(getPortIndex(&quot;contrastive_divergence&quot;),1) = 1;
+        if (visible_layer)
+            port_sizes(getPortIndex(&quot;negative_phase_visible_samples.state&quot;),1) = visible_layer-&gt;size;
         if (hidden_layer)
-            port_sizes(getPortIndex(&quot;negative_phase_hidden_expectations.state&quot;),1) = hidden_layer-&gt;size; 
+            port_sizes(getPortIndex(&quot;negative_phase_hidden_expectations.state&quot;),1) = hidden_layer-&gt;size;
         if (fast_exact_is_equal(cd_learning_rate, 0))
             PLWARNING(&quot;In RBMModule::build_ - Contrastive divergence is &quot;
                     &quot;computed but 'cd_learning_rate' is set to 0: no internal &quot;
@@ -606,7 +606,7 @@
     PLASSERT( hidden_layer );
     PLASSERT( connection );
 
-    Mat* visible = ports_value[getPortIndex(&quot;visible&quot;)]; 
+    Mat* visible = ports_value[getPortIndex(&quot;visible&quot;)];
     Mat* hidden = ports_value[getPortIndex(&quot;hidden.state&quot;)];
     hidden_act = ports_value[getPortIndex(&quot;hidden_activations.state&quot;)];
     Mat* visible_sample = ports_value[getPortIndex(&quot;visible_sample&quot;)];
@@ -621,11 +621,11 @@
     Mat* reconstruction_error = 0;
     if(reconstruction_connection)
     {
-        visible_reconstruction = 
-            ports_value[getPortIndex(&quot;visible_reconstruction.state&quot;)]; 
-        visible_reconstruction_activations = 
+        visible_reconstruction =
+            ports_value[getPortIndex(&quot;visible_reconstruction.state&quot;)];
+        visible_reconstruction_activations =
             ports_value[getPortIndex(&quot;visible_reconstruction_activations.state&quot;)];
-        reconstruction_error = 
+        reconstruction_error =
             ports_value[getPortIndex(&quot;reconstruction_error.state&quot;)];
     }
     Mat* contrastive_divergence = 0;
@@ -634,15 +634,15 @@
     Mat* negative_phase_hidden_activations = NULL;
     if (compute_contrastive_divergence)
     {
-        contrastive_divergence = ports_value[getPortIndex(&quot;contrastive_divergence&quot;)]; 
+        contrastive_divergence = ports_value[getPortIndex(&quot;contrastive_divergence&quot;)];
         if (!contrastive_divergence || !contrastive_divergence-&gt;isEmpty())
             PLERROR(&quot;In RBMModule::fprop - When option &quot;
                     &quot;'compute_contrastive_divergence' is 'true', the &quot;
                     &quot;'contrastive_divergence' port should be provided, as an &quot;
                     &quot;output.&quot;);
-        negative_phase_visible_samples = 
+        negative_phase_visible_samples =
             ports_value[getPortIndex(&quot;negative_phase_visible_samples.state&quot;)];
-        negative_phase_hidden_expectations = 
+        negative_phase_hidden_expectations =
             ports_value[getPortIndex(&quot;negative_phase_hidden_expectations.state&quot;)];
         negative_phase_hidden_activations =
             ports_value[getPortIndex(&quot;negative_phase_hidden_activations.state&quot;)];
@@ -656,14 +656,14 @@
     {
         // When an input is provided, that would restart the chain for
         // unconditional sampling, from that example.
-        Gibbs_step = 0; 
+        Gibbs_step = 0;
         visible_layer-&gt;setExpectations(*visible);
     }
 
     // COMPUTE ENERGY
-    if (energy) 
+    if (energy)
     {
-        PLASSERT_MSG( energy-&gt;isEmpty(), 
+        PLASSERT_MSG( energy-&gt;isEmpty(),
                       &quot;RBMModule: the energy port can only be an output port\n&quot; );
         if (visible &amp;&amp; !visible-&gt;isEmpty()
             &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
@@ -685,6 +685,7 @@
         }
         found_a_valid_configuration = true;
     }
+    // COMPUTE NLL
     if (neg_log_likelihood &amp;&amp; neg_log_likelihood-&gt;isEmpty() &amp;&amp; compute_log_likelihood)
     {
         if (partition_function_is_stale &amp;&amp; !during_training)
@@ -758,7 +759,7 @@
             computeEnergy(*visible,*hidden,*neg_log_likelihood);
             *neg_log_likelihood += log_partition_function;
         }
-        else if (visible &amp;&amp; !visible-&gt;isEmpty()) 
+        else if (visible &amp;&amp; !visible-&gt;isEmpty())
         {
             // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
             computeFreeEnergyOfVisible(*visible,*neg_log_likelihood);
@@ -772,7 +773,7 @@
         }
         else PLERROR(&quot;RBMModule: neg_log_likelihood currently computable only of the visible as inputs&quot;);
     }
-    
+
     // REGULAR FPROP
     // we are given the visible units and we want to compute the hidden
     // activation and/or the hidden expectation
@@ -794,8 +795,8 @@
         // Since we return below, the other ports must be unused.
         //PLASSERT( !visible_sample &amp;&amp; !hidden_sample );
         found_a_valid_configuration = true;
-    } 
-    
+    }
+
     // SAMPLING
     if ((visible_sample &amp;&amp; visible_sample-&gt;isEmpty())               // is asked to sample visible units (discrete)
         || (visible_expectation &amp;&amp; visible_expectation-&gt;isEmpty())  //              &quot;                   (continous)
@@ -820,7 +821,7 @@
             Gibbs_step=0;
             //cout &lt;&lt; &quot;sampling hidden from visible expectation&quot; &lt;&lt; endl;
         }
-        else if (visible_expectation &amp;&amp; !visible_expectation-&gt;isEmpty()) 
+        else if (visible_expectation &amp;&amp; !visible_expectation-&gt;isEmpty())
         {
              PLERROR(&quot;In RBMModule::fprop visible_expectation can only be an output port (use visible as input port&quot;);
         }
@@ -866,11 +867,11 @@
         }
         found_a_valid_configuration = true;
     }// END SAMPLING
-    
+
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)
     {
-        PLASSERT_MSG( contrastive_divergence-&gt;isEmpty(), 
+        PLASSERT_MSG( contrastive_divergence-&gt;isEmpty(),
                       &quot;RBMModule: the contrastive_divergence port can only be an output port\n&quot; );
         if (visible &amp;&amp; !visible-&gt;isEmpty())
         {
@@ -882,15 +883,16 @@
             {
                 PLASSERT(!hidden_act);
                 computePositivePhaseHiddenActivations(*visible);
-                
+
                 // we need to save the hidden activations somewhere
                 hidden_act_store.resize(mbs,hidden_layer-&gt;size);
                 hidden_act_store &lt;&lt; hidden_layer-&gt;activations;
                 h_act = &amp;hidden_act_store;
-            } else 
+            }
+            else
             {
                 // hidden_act must have been computed above if they were requested on port
-                PLASSERT(hidden_act &amp;&amp; !hidden_act-&gt;isEmpty()); 
+                PLASSERT(hidden_act &amp;&amp; !hidden_act-&gt;isEmpty());
                 h_act = hidden_act;
             }
             if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
@@ -902,7 +904,8 @@
                 hidden_exp_store.resize(mbs,hidden_layer-&gt;size);
                 hidden_exp_store &lt;&lt; hidden_expectations;
                 h = &amp;hidden_exp_store;
-            } else
+            }
+            else
             {
                 // hidden exp. must have been computed above if they were requested on port
                 PLASSERT(hidden &amp;&amp; !hidden-&gt;isEmpty());
@@ -937,19 +940,19 @@
             PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
 
             // note that h_act and h may point to hidden_act_store and hidden_exp_store
-            PLASSERT(h_act &amp;&amp; !h_act-&gt;isEmpty()); 
+            PLASSERT(h_act &amp;&amp; !h_act-&gt;isEmpty());
             PLASSERT(h &amp;&amp; !h-&gt;isEmpty());
 
             contrastive_divergence-&gt;resize(hidden_expectations.length(),1);
             // compute contrastive divergence itself
             for (int i=0;i&lt;mbs;i++)
             {
-                (*contrastive_divergence)(i,0) = 
+                (*contrastive_divergence)(i,0) =
                     // positive phase energy
                     visible_layer-&gt;energy((*visible)(i))
                     - dot((*h)(i),(*h_act)(i))
                     // minus
-                    - 
+                    -
                     // negative phase energy
                     (visible_layer-&gt;energy(visible_layer-&gt;samples(i))
                      - dot(hidden_expectations(i),hidden_layer-&gt;activations(i)));
@@ -960,20 +963,18 @@
                     &quot;only possible if only visible are provided in input).\n&quot;);
         found_a_valid_configuration = true;
     }
-    
 
-    
-    
+
     // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
-    if ( visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp; 
-         ( ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() ) || 
-           ( visible_reconstruction_activations &amp;&amp; 
+    if ( visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp;
+         ( ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() ) ||
+           ( visible_reconstruction_activations &amp;&amp;
              visible_reconstruction_activations-&gt;isEmpty() ) ||
-           ( reconstruction_error &amp;&amp; reconstruction_error-&gt;isEmpty() ) ) ) 
-    {        
+           ( reconstruction_error &amp;&amp; reconstruction_error-&gt;isEmpty() ) ) )
+    {
         // Autoassociator reconstruction cost
         PLASSERT( ports_value.length() == nPorts() );
-        computePositivePhaseHiddenActivations(*visible); 
+        computePositivePhaseHiddenActivations(*visible);
         if(!hidden_expectations_are_computed)
         {
             hidden_layer-&gt;computeExpectations();
@@ -981,24 +982,24 @@
         }
 
         // Don't need to verify if they are asked in a port, this was done previously
-        
+
         computeVisibleActivations(hidden_layer-&gt;getExpectations(),true);
-        if(visible_reconstruction_activations) 
+        if(visible_reconstruction_activations)
         {
             PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
             const Mat&amp; to_store = visible_layer-&gt;activations;
-            visible_reconstruction_activations-&gt;resize(to_store.length(), 
+            visible_reconstruction_activations-&gt;resize(to_store.length(),
                                                        to_store.width());
             *visible_reconstruction_activations &lt;&lt; to_store;
         }
         if (visible_reconstruction || reconstruction_error)
-        {        
+        {
             visible_layer-&gt;computeExpectations();
             if(visible_reconstruction)
             {
                 PLASSERT( visible_reconstruction-&gt;isEmpty() );
                 const Mat&amp; to_store = visible_layer-&gt;getExpectations();
-                visible_reconstruction-&gt;resize(to_store.length(), 
+                visible_reconstruction-&gt;resize(to_store.length(),
                                                            to_store.width());
                 *visible_reconstruction &lt;&lt; to_store;
             }
@@ -1013,25 +1014,23 @@
         found_a_valid_configuration = true;
     }
     // COMPUTE VISIBLE GIVEN HIDDEN
-    else if ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() 
+    else if ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty()
          &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
-           
-    {        
+    {
         // Don't need to verify if they are asked in a port, this was done previously
-        
-	computeVisibleActivations(*hidden,true);
+        computeVisibleActivations(*hidden,true);
         if(visible_reconstruction_activations)
         {
             PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
             const Mat&amp; to_store = visible_layer-&gt;activations;
-            visible_reconstruction_activations-&gt;resize(to_store.length(), 
+            visible_reconstruction_activations-&gt;resize(to_store.length(),
                                                        to_store.width());
             *visible_reconstruction_activations &lt;&lt; to_store;
-        }      
+        }
         visible_layer-&gt;computeExpectations();
         PLASSERT( visible_reconstruction-&gt;isEmpty() );
         const Mat&amp; to_store = visible_layer-&gt;getExpectations();
-        visible_reconstruction-&gt;resize(to_store.length(), 
+        visible_reconstruction-&gt;resize(to_store.length(),
                                        to_store.width());
         *visible_reconstruction &lt;&lt; to_store;
         found_a_valid_configuration = true;
@@ -1082,8 +1081,8 @@
     hidden_act = ports_value[getPortIndex(&quot;hidden_activations.state&quot;)];
     Mat* reconstruction_error_grad = 0;
     Mat* hidden_bias_grad = ports_gradient[getPortIndex(&quot;hidden_bias&quot;)];
-    weights = ports_value[getPortIndex(&quot;weights&quot;)]; 
-    Mat* weights_grad = ports_gradient[getPortIndex(&quot;weights&quot;)];    
+    weights = ports_value[getPortIndex(&quot;weights&quot;)];
+    Mat* weights_grad = ports_gradient[getPortIndex(&quot;weights&quot;)];
     hidden_bias = ports_value[getPortIndex(&quot;hidden_bias&quot;)];
     Mat* contrastive_divergence_grad = NULL;
 
@@ -1099,7 +1098,7 @@
     }
 
     if(reconstruction_connection)
-        reconstruction_error_grad = 
+        reconstruction_error_grad =
             ports_gradient[getPortIndex(&quot;reconstruction_error.state&quot;)];
 
     // Ensure the visible gradient is not provided as input. This is because we
@@ -1109,7 +1108,7 @@
 
     bool compute_visible_grad = visible_grad &amp;&amp; visible_grad-&gt;isEmpty();
     bool compute_weights_grad = weights_grad &amp;&amp; weights_grad-&gt;isEmpty();
-    
+
     int mbs = (visible &amp;&amp; !visible-&gt;isEmpty()) ? visible-&gt;length() : -1;
 
     if (hidden_grad &amp;&amp; !hidden_grad-&gt;isEmpty())
@@ -1147,7 +1146,7 @@
                 store_visible_grad = &amp;visible_exp_grad;
             }
             store_visible_grad-&gt;resize(mbs,visible_layer-&gt;size);
-            
+
             if (weights)
             {
                 int up = connection-&gt;up_size;
@@ -1261,7 +1260,7 @@
         // Perform a step of contrastive divergence.
         PLASSERT( visible &amp;&amp; !visible-&gt;isEmpty() );
         setAllLearningRates(cd_learning_rate);
-        Mat* negative_phase_visible_samples = 
+        Mat* negative_phase_visible_samples =
             compute_contrastive_divergence?ports_value[getPortIndex(&quot;negative_phase_visible_samples.state&quot;)]:0;
         const Mat* negative_phase_hidden_expectations =
             compute_contrastive_divergence ?
@@ -1271,7 +1270,7 @@
             compute_contrastive_divergence ?
                 ports_value[getPortIndex(&quot;negative_phase_hidden_activations.state&quot;)]
                 : NULL;
-        
+
         PLASSERT( visible &amp;&amp; hidden );
         PLASSERT( !negative_phase_visible_samples ||
                   !negative_phase_visible_samples-&gt;isEmpty() );
@@ -1487,7 +1486,7 @@
         reconstruction_connection-&gt;bpropUpdate(
             *hidden, *visible_reconstruction_activations,
             hidden_exp_grad, visible_act_grad, false);
-        
+
         // Hidden layer bias update
         hidden_layer-&gt;bpropUpdate(*hidden_act,
                                   *hidden, hidden_act_grad,
@@ -1515,7 +1514,7 @@
         }
         else
         {
-            visible_exp_grad.resize(mbs,visible_layer-&gt;size);        
+            visible_exp_grad.resize(mbs,visible_layer-&gt;size);
             connection-&gt;bpropUpdate(
                 *visible, *hidden_act,
                 visible_exp_grad, hidden_act_grad, true);

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-07-04 02:55:38 UTC (rev 7690)
+++ trunk/plearn_learners/online/RBMModule.h	2007-07-04 03:31:17 UTC (rev 7691)
@@ -74,12 +74,12 @@
 
     bool compute_contrastive_divergence;
 
-    //! Number of Gibbs sampling steps in negative phase 
+    //! Number of Gibbs sampling steps in negative phase
     //! of contrastive divergence.
     int n_Gibbs_steps_CD;
 
     //! used to generate samples from the RBM
-    int min_n_Gibbs_steps; 
+    int min_n_Gibbs_steps;
     int n_Gibbs_steps_per_generated_sample;
 
     bool compute_log_likelihood;
@@ -94,8 +94,8 @@
     bool standard_cd_grad;
     bool standard_cd_bias_grad;
     bool standard_cd_weights_grad;
-    
 
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -242,7 +242,7 @@
     Mat hidden_exp_store;
     Mat hidden_act_store;
     Mat* hidden_act;
-    bool hidden_activations_are_computed;    
+    bool hidden_activations_are_computed;
 
     //! Used to store the contrastive divergence gradient w.r.t. weights.
     Mat store_weights_grad;
@@ -312,7 +312,7 @@
     //! it in the 'energy' matrix.
     //! The 'positive_phase' boolean is used to save computations when we know
     //! we are in the positive phase of fprop.
-    void computeEnergy(const Mat&amp; visible, const Mat&amp; hidden, Mat&amp; energy, 
+    void computeEnergy(const Mat&amp; visible, const Mat&amp; hidden, Mat&amp; energy,
                        bool positive_phase = true);
 
 private:


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001138.html">[Plearn-commits] r7690 - in trunk: plearn_learners/hyper	plearn_learners_experimental/SurfaceTemplate
</A></li>
	<LI>Next message: <A HREF="001140.html">[Plearn-commits] r7692 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1139">[ date ]</a>
              <a href="thread.html#1139">[ thread ]</a>
              <a href="subject.html#1139">[ subject ]</a>
              <a href="author.html#1139">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
