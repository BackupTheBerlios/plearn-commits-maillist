<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7744 - trunk/plearn_learners/online/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7744%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200707111717.l6BHHEAL012725%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001191.html">
   <LINK REL="Next"  HREF="001193.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7744 - trunk/plearn_learners/online/EXPERIMENTAL</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7744%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200707111717.l6BHHEAL012725%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7744 - trunk/plearn_learners/online/EXPERIMENTAL">yoshua at mail.berlios.de
       </A><BR>
    <I>Wed Jul 11 19:17:14 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001191.html">[Plearn-commits] r7743 - trunk/python_modules/plearn/parallel
</A></li>
        <LI>Next message: <A HREF="001193.html">[Plearn-commits] r7745 -	tags/after_energy_sign_changes/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1192">[ date ]</a>
              <a href="thread.html#1192">[ thread ]</a>
              <a href="subject.html#1192">[ subject ]</a>
              <a href="author.html#1192">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-07-11 19:17:13 +0200 (Wed, 11 Jul 2007)
New Revision: 7744

Modified:
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h
Log:


Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-11 14:26:41 UTC (rev 7743)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.cc	2007-07-11 17:17:13 UTC (rev 7744)
@@ -530,13 +530,14 @@
     if (using_reconstruction_connection)
     {
         PLASSERT( reconstruction_connection );
-        reconstruction_connection-&gt;setAsDownInputs(hidden);
+        reconstruction_connection-&gt;setAsUpInputs(hidden);
         visible_layer-&gt;getAllActivations(reconstruction_connection, 0, true);
     }
     else
     {
         if(weights &amp;&amp; !weights-&gt;isEmpty())
         {
+            PLASSERT( connection-&gt;classname() == &quot;RBMMatrixConnection&quot; );
             Mat old_weights;
             Vec old_activation;
             connection-&gt;getAllWeights(old_weights);
@@ -621,11 +622,11 @@
     Mat* reconstruction_error = 0;
     if(reconstruction_connection)
     {
-        visible_reconstruction = 
-            ports_value[getPortIndex(&quot;visible_reconstruction.state&quot;)]; 
-        visible_reconstruction_activations = 
+        visible_reconstruction =
+            ports_value[getPortIndex(&quot;visible_reconstruction.state&quot;)];
+        visible_reconstruction_activations =
             ports_value[getPortIndex(&quot;visible_reconstruction_activations.state&quot;)];
-        reconstruction_error = 
+        reconstruction_error =
             ports_value[getPortIndex(&quot;reconstruction_error.state&quot;)];
     }
     Mat* contrastive_divergence = 0;
@@ -634,15 +635,15 @@
     Mat* negative_phase_hidden_activations = NULL;
     if (compute_contrastive_divergence)
     {
-        contrastive_divergence = ports_value[getPortIndex(&quot;contrastive_divergence&quot;)]; 
+        contrastive_divergence = ports_value[getPortIndex(&quot;contrastive_divergence&quot;)];
         if (!contrastive_divergence || !contrastive_divergence-&gt;isEmpty())
             PLERROR(&quot;In KLp0p1RBMModule::fprop - When option &quot;
                     &quot;'compute_contrastive_divergence' is 'true', the &quot;
                     &quot;'contrastive_divergence' port should be provided, as an &quot;
                     &quot;output.&quot;);
-        negative_phase_visible_samples = 
+        negative_phase_visible_samples =
             ports_value[getPortIndex(&quot;negative_phase_visible_samples.state&quot;)];
-        negative_phase_hidden_expectations = 
+        negative_phase_hidden_expectations =
             ports_value[getPortIndex(&quot;negative_phase_hidden_expectations.state&quot;)];
         negative_phase_hidden_activations =
             ports_value[getPortIndex(&quot;negative_phase_hidden_activations.state&quot;)];
@@ -656,12 +657,13 @@
     {
         // When an input is provided, that would restart the chain for
         // unconditional sampling, from that example.
-        Gibbs_step = 0; 
-        visible_layer-&gt;setExpectations(*visible);
+        Gibbs_step = 0;
+        visible_layer-&gt;samples.resize(visible-&gt;length(),visible-&gt;width());
+        visible_layer-&gt;samples &lt;&lt; *visible;
     }
 
     // COMPUTE ENERGY
-    if (energy) 
+    if (energy)
     {
         PLASSERT_MSG( energy-&gt;isEmpty(), 
                       &quot;KLp0p1RBMModule: the energy port can only be an output port\n&quot; );
@@ -685,6 +687,7 @@
         }
         found_a_valid_configuration = true;
     }
+    // COMPUTE NLL
     if (neg_log_likelihood &amp;&amp; neg_log_likelihood-&gt;isEmpty() &amp;&amp; compute_log_likelihood)
     {
         if (partition_function_is_stale &amp;&amp; !during_training)
@@ -758,7 +761,7 @@
             computeEnergy(*visible,*hidden,*neg_log_likelihood);
             *neg_log_likelihood += log_partition_function;
         }
-        else if (visible &amp;&amp; !visible-&gt;isEmpty()) 
+        else if (visible &amp;&amp; !visible-&gt;isEmpty())
         {
             // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
             computeFreeEnergyOfVisible(*visible,*neg_log_likelihood);
@@ -770,9 +773,9 @@
             computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
             *neg_log_likelihood += log_partition_function;
         }
-        else PLERROR(&quot;KLp0p1RBMModule: neg_log_likelihood currently computable only of the visible as inputs&quot;);
+        else PLERROR(&quot;RBMModule: neg_log_likelihood currently computable only of the visible as inputs&quot;);
     }
-    
+
     // REGULAR FPROP
     // we are given the visible units and we want to compute the hidden
     // activation and/or the hidden expectation
@@ -794,8 +797,79 @@
         // Since we return below, the other ports must be unused.
         //PLASSERT( !visible_sample &amp;&amp; !hidden_sample );
         found_a_valid_configuration = true;
-    } 
+    }
 
+    // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
+    if ( visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp;
+         ( ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() ) ||
+           ( visible_reconstruction_activations &amp;&amp;
+             visible_reconstruction_activations-&gt;isEmpty() ) ||
+           ( reconstruction_error &amp;&amp; reconstruction_error-&gt;isEmpty() ) ) )
+    {
+        // Autoassociator reconstruction cost
+        PLASSERT( ports_value.length() == nPorts() );
+        if(!hidden_expectations_are_computed)
+        {
+            computePositivePhaseHiddenActivations(*visible);
+            hidden_layer-&gt;computeExpectations();
+            hidden_expectations_are_computed=true;
+        }
+
+        // Don't need to verify if they are asked in a port, this was done previously
+
+        computeVisibleActivations(hidden_layer-&gt;getExpectations(), true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
+            const Mat&amp; to_store = visible_layer-&gt;activations;
+            visible_reconstruction_activations-&gt;resize(to_store.length(),
+                                                       to_store.width());
+            *visible_reconstruction_activations &lt;&lt; to_store;
+        }
+        if (visible_reconstruction || reconstruction_error)
+        {
+            visible_layer-&gt;computeExpectations();
+            if(visible_reconstruction)
+            {
+                PLASSERT( visible_reconstruction-&gt;isEmpty() );
+                const Mat&amp; to_store = visible_layer-&gt;getExpectations();
+                visible_reconstruction-&gt;resize(to_store.length(),
+                                               to_store.width());
+                *visible_reconstruction &lt;&lt; to_store;
+            }
+            if(reconstruction_error)
+            {
+                PLASSERT( reconstruction_error-&gt;isEmpty() );
+                reconstruction_error-&gt;resize(visible-&gt;length(),1);
+                visible_layer-&gt;fpropNLL(*visible,
+                                        *reconstruction_error);
+            }
+        }
+        found_a_valid_configuration = true;
+    }
+    // COMPUTE VISIBLE GIVEN HIDDEN
+    else if ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty()
+         &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
+    {
+        // Don't need to verify if they are asked in a port, this was done previously
+        computeVisibleActivations(*hidden,true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
+            const Mat&amp; to_store = visible_layer-&gt;activations;
+            visible_reconstruction_activations-&gt;resize(to_store.length(),
+                                                       to_store.width());
+            *visible_reconstruction_activations &lt;&lt; to_store;
+        }
+        visible_layer-&gt;computeExpectations();
+        PLASSERT( visible_reconstruction-&gt;isEmpty() );
+        const Mat&amp; to_store = visible_layer-&gt;getExpectations();
+        visible_reconstruction-&gt;resize(to_store.length(),
+                                       to_store.width());
+        *visible_reconstruction &lt;&lt; to_store;
+        found_a_valid_configuration = true;
+    }
+
     // compute KLp0p1 cost, given visible input
     if (KLp0p1 &amp;&amp; KLp0p1-&gt;isEmpty() &amp;&amp; visible &amp;&amp; !visible-&gt;isEmpty())
     {
@@ -807,8 +881,11 @@
         PLASSERT_MSG(n&gt;0,&quot;KLp0p1RBMModule: training_set must have n&gt;0 rows&quot;);
 
         // compute all P(hidden_i=1|x^k) for all x^k in training set
+        hidden_layer-&gt;setBatchSize(n);
+        visible_layer-&gt;setBatchSize(n);
         const Mat&amp; ph=hidden_layer-&gt;getExpectations();
         training_set-&gt;getMat(0,0,visible_layer-&gt;getExpectations());
+        connection-&gt;setAsDownInputs(visible_layer-&gt;getExpectations());
         hidden_layer-&gt;getAllActivations(connection,0,true);
         hidden_layer-&gt;computeExpectations();
 
@@ -831,8 +908,9 @@
             }
         }
         // compute all P(visible_i=1|h) for each h configuration
-        visible_layer-&gt;getAllActivations(connection,0,true);
-        visible_layer-&gt;computeExpectations();
+        connection-&gt;setAsUpInputs(conf_hidden_layer-&gt;samples);
+        conf_visible_layer-&gt;getAllActivations(connection,0,true);
+        conf_visible_layer-&gt;computeExpectations();
 
         for (int c=0;c&lt;n_configurations;c++)
         {
@@ -863,6 +941,9 @@
                     (*KLp0p1)(t,0) = logadd((*KLp0p1)(t,0), conf_visible_layer-&gt;fpropNLL((*visible)(t)) + log_sum_ph_given_xk);
         }
         *KLp0p1 *= -1;
+        // reset sizes as before
+        hidden_layer-&gt;setBatchSize(mbs);
+        visible_layer-&gt;setBatchSize(mbs);
     }
 
     // SAMPLING
@@ -879,24 +960,29 @@
         else if (visible_sample &amp;&amp; !visible_sample-&gt;isEmpty()) // if an input is provided, sample hidden conditionally
         {
             sampleHiddenGivenVisible(*visible_sample);
+            hidden_activations_are_computed = false;
             Gibbs_step=0;
-            //cout &lt;&lt; &quot;sampling hidden from (discrete) visible&quot; &lt;&lt; endl;
+            //cout &lt;&lt; &quot;sampling hidden from visible&quot; &lt;&lt; endl;
         }
-        else if (visible &amp;&amp; !visible-&gt;isEmpty()) // if an input is provided, sample hidden conditionally
+        else if (visible_expectation &amp;&amp; !visible_expectation-&gt;isEmpty())
         {
-            visible_layer-&gt;generateSamples(); // WHY THIS LINE????
-            sampleHiddenGivenVisible(visible_layer-&gt;samples);
-            Gibbs_step=0;
-            //cout &lt;&lt; &quot;sampling hidden from visible expectation&quot; &lt;&lt; endl;
-        }
-        else if (visible_expectation &amp;&amp; !visible_expectation-&gt;isEmpty()) 
-        {
              PLERROR(&quot;In KLp0p1RBMModule::fprop visible_expectation can only be an output port (use visible as input port&quot;);
         }
         else // sample unconditionally: Gibbs sample after k steps
         {
             // the visible_layer-&gt;expectations contain the &quot;state&quot; from which we
             // start or continue the chain
+            if (visible_layer-&gt;samples.isEmpty())
+            {
+                if (visible &amp;&amp; !visible-&gt;isEmpty())
+                    visible_layer-&gt;samples &lt;&lt; *visible;
+                else if (!visible_layer-&gt;getExpectations().isEmpty())
+                    visible_layer-&gt;samples &lt;&lt; visible_layer-&gt;getExpectations();
+                else if (!hidden_layer-&gt;samples.isEmpty())
+                    sampleVisibleGivenHidden(hidden_layer-&gt;samples);    
+                else if (!hidden_layer-&gt;getExpectations().isEmpty())
+                    sampleVisibleGivenHidden(hidden_layer-&gt;getExpectations());    
+            }
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
             //cout &lt;&lt; &quot;Gibbs sampling &quot; &lt;&lt; Gibbs_step+1;
@@ -905,7 +991,8 @@
                 sampleHiddenGivenVisible(visible_layer-&gt;samples);
                 sampleVisibleGivenHidden(hidden_layer-&gt;samples);
             }
-              //cout &lt;&lt; &quot; -&gt; &quot; &lt;&lt; Gibbs_step &lt;&lt; endl;
+            hidden_activations_are_computed = false;
+            //cout &lt;&lt; &quot; -&gt; &quot; &lt;&lt; Gibbs_step &lt;&lt; endl;
         }
 
         if ( hidden &amp;&amp; hidden-&gt;isEmpty())   // fill hidden.state with expectations
@@ -933,9 +1020,21 @@
                                         to_store.width());
             *visible_expectation &lt;&lt; to_store;
         }
+        if (hidden &amp;&amp; hidden-&gt;isEmpty())
+        {
+            hidden-&gt;resize(hidden_layer-&gt;samples.length(),
+                           hidden_layer-&gt;samples.width());
+            *hidden &lt;&lt; hidden_layer-&gt;samples;
+        }
+        if (hidden_act &amp;&amp; hidden_act-&gt;isEmpty())
+        {
+            hidden_act-&gt;resize(hidden_layer-&gt;samples.length(),
+                               hidden_layer-&gt;samples.width());
+            *hidden_act &lt;&lt; hidden_layer-&gt;getExpectations();
+        }
         found_a_valid_configuration = true;
     }// END SAMPLING
-    
+
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)
     {
@@ -951,15 +1050,16 @@
             {
                 PLASSERT(!hidden_act);
                 computePositivePhaseHiddenActivations(*visible);
-                
+
                 // we need to save the hidden activations somewhere
                 hidden_act_store.resize(mbs,hidden_layer-&gt;size);
                 hidden_act_store &lt;&lt; hidden_layer-&gt;activations;
                 h_act = &amp;hidden_act_store;
-            } else 
+            }
+            else
             {
                 // hidden_act must have been computed above if they were requested on port
-                PLASSERT(hidden_act &amp;&amp; !hidden_act-&gt;isEmpty()); 
+                PLASSERT(hidden_act &amp;&amp; !hidden_act-&gt;isEmpty());
                 h_act = hidden_act;
             }
             if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
@@ -971,7 +1071,8 @@
                 hidden_exp_store.resize(mbs,hidden_layer-&gt;size);
                 hidden_exp_store &lt;&lt; hidden_expectations;
                 h = &amp;hidden_exp_store;
-            } else
+            }
+            else
             {
                 // hidden exp. must have been computed above if they were requested on port
                 PLASSERT(hidden &amp;&amp; !hidden-&gt;isEmpty());
@@ -985,6 +1086,7 @@
                 sampleVisibleGivenHidden(hidden_layer-&gt;samples);
                 // compute corresponding hidden expectations.
                 computeHiddenActivations(visible_layer-&gt;samples);
+                hidden_activations_are_computed = false;
                 hidden_layer-&gt;computeExpectations();
             }
             PLASSERT(negative_phase_visible_samples);
@@ -1006,19 +1108,19 @@
             PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
 
             // note that h_act and h may point to hidden_act_store and hidden_exp_store
-            PLASSERT(h_act &amp;&amp; !h_act-&gt;isEmpty()); 
+            PLASSERT(h_act &amp;&amp; !h_act-&gt;isEmpty());
             PLASSERT(h &amp;&amp; !h-&gt;isEmpty());
 
             contrastive_divergence-&gt;resize(hidden_expectations.length(),1);
             // compute contrastive divergence itself
             for (int i=0;i&lt;mbs;i++)
             {
-                (*contrastive_divergence)(i,0) = 
+                (*contrastive_divergence)(i,0) =
                     // positive phase energy
                     visible_layer-&gt;energy((*visible)(i))
                     - dot((*h)(i),(*h_act)(i))
                     // minus
-                    - 
+                    -
                     // negative phase energy
                     (visible_layer-&gt;energy(visible_layer-&gt;samples(i))
                      - dot(hidden_expectations(i),hidden_layer-&gt;activations(i)));
@@ -1029,83 +1131,8 @@
                     &quot;only possible if only visible are provided in input).\n&quot;);
         found_a_valid_configuration = true;
     }
-    
 
-    
-    
-    // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
-    if ( visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp; 
-         ( ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() ) || 
-           ( visible_reconstruction_activations &amp;&amp; 
-             visible_reconstruction_activations-&gt;isEmpty() ) ||
-           ( reconstruction_error &amp;&amp; reconstruction_error-&gt;isEmpty() ) ) ) 
-    {        
-        // Autoassociator reconstruction cost
-        PLASSERT( ports_value.length() == nPorts() );
-        computePositivePhaseHiddenActivations(*visible); 
-        if(!hidden_expectations_are_computed)
-        {
-            hidden_layer-&gt;computeExpectations();
-            hidden_expectations_are_computed=true;
-        }
 
-        // Don't need to verify if they are asked in a port, this was done previously
-        
-        computeVisibleActivations(hidden_layer-&gt;getExpectations(),true);
-        if(visible_reconstruction_activations) 
-        {
-            PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
-            const Mat&amp; to_store = visible_layer-&gt;activations;
-            visible_reconstruction_activations-&gt;resize(to_store.length(), 
-                                                       to_store.width());
-            *visible_reconstruction_activations &lt;&lt; to_store;
-        }
-        if (visible_reconstruction || reconstruction_error)
-        {        
-            visible_layer-&gt;computeExpectations();
-            if(visible_reconstruction)
-            {
-                PLASSERT( visible_reconstruction-&gt;isEmpty() );
-                const Mat&amp; to_store = visible_layer-&gt;getExpectations();
-                visible_reconstruction-&gt;resize(to_store.length(), 
-                                                           to_store.width());
-                *visible_reconstruction &lt;&lt; to_store;
-            }
-            if(reconstruction_error)
-            {
-                PLASSERT( reconstruction_error-&gt;isEmpty() );
-                reconstruction_error-&gt;resize(visible-&gt;length(),1);
-                visible_layer-&gt;fpropNLL(*visible,
-                                        *reconstruction_error);
-            }
-        }
-        found_a_valid_configuration = true;
-    }
-    // COMPUTE VISIBLE GIVEN HIDDEN
-    else if ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() 
-         &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
-           
-    {        
-        // Don't need to verify if they are asked in a port, this was done previously
-        
-	computeVisibleActivations(*hidden,true);
-        if(visible_reconstruction_activations)
-        {
-            PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
-            const Mat&amp; to_store = visible_layer-&gt;activations;
-            visible_reconstruction_activations-&gt;resize(to_store.length(), 
-                                                       to_store.width());
-            *visible_reconstruction_activations &lt;&lt; to_store;
-        }      
-        visible_layer-&gt;computeExpectations();
-        PLASSERT( visible_reconstruction-&gt;isEmpty() );
-        const Mat&amp; to_store = visible_layer-&gt;getExpectations();
-        visible_reconstruction-&gt;resize(to_store.length(), 
-                                       to_store.width());
-        *visible_reconstruction &lt;&lt; to_store;
-        found_a_valid_configuration = true;
-    }
-
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;
@@ -1129,7 +1156,7 @@
             cout &lt;&lt; &quot;visible_expectation_empty : &quot;&lt;&lt; (bool) visible_expectation-&gt;isEmpty() &lt;&lt; endl;
 
         */
-        PLERROR(&quot;In KLp0p1RBMModule::fprop - Unknown port configuration for module %s&quot;, name.c_str());
+        PLERROR(&quot;In RBMModule::fprop - Unknown port configuration for module %s&quot;, name.c_str());
     }
 
     checkProp(ports_value);
@@ -1151,8 +1178,8 @@
     hidden_act = ports_value[getPortIndex(&quot;hidden_activations.state&quot;)];
     Mat* reconstruction_error_grad = 0;
     Mat* hidden_bias_grad = ports_gradient[getPortIndex(&quot;hidden_bias&quot;)];
-    weights = ports_value[getPortIndex(&quot;weights&quot;)]; 
-    Mat* weights_grad = ports_gradient[getPortIndex(&quot;weights&quot;)];    
+    weights = ports_value[getPortIndex(&quot;weights&quot;)];
+    Mat* weights_grad = ports_gradient[getPortIndex(&quot;weights&quot;)];
     hidden_bias = ports_value[getPortIndex(&quot;hidden_bias&quot;)];
     Mat* contrastive_divergence_grad = NULL;
     Mat* KLp0p1 = ports_value[getPortIndex(&quot;KLp0p1&quot;)];
@@ -1169,7 +1196,7 @@
     }
 
     if(reconstruction_connection)
-        reconstruction_error_grad = 
+        reconstruction_error_grad =
             ports_gradient[getPortIndex(&quot;reconstruction_error.state&quot;)];
 
     // Ensure the visible gradient is not provided as input. This is because we
@@ -1179,7 +1206,7 @@
 
     bool compute_visible_grad = visible_grad &amp;&amp; visible_grad-&gt;isEmpty();
     bool compute_weights_grad = weights_grad &amp;&amp; weights_grad-&gt;isEmpty();
-    
+
     int mbs = (visible &amp;&amp; !visible-&gt;isEmpty()) ? visible-&gt;length() : -1;
 
     if (hidden_grad &amp;&amp; !hidden_grad-&gt;isEmpty())
@@ -1191,67 +1218,67 @@
         // Note: we need to perform the following steps even if the gradient
         // learning rate is equal to 0. This is because we must propagate the
         // gradient to the visible layer, even though no update is required.
-            setAllLearningRates(grad_learning_rate);
-            PLASSERT( hidden &amp;&amp; hidden_act );
-            // Compute gradient w.r.t. activations of the hidden layer.
-            hidden_layer-&gt;bpropUpdate(
-                    *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
-                    false);
-            if (hidden_bias_grad)
+        setAllLearningRates(grad_learning_rate);
+        PLASSERT( hidden &amp;&amp; hidden_act );
+        // Compute gradient w.r.t. activations of the hidden layer.
+        hidden_layer-&gt;bpropUpdate(
+                *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
+                false);
+        if (hidden_bias_grad)
+        {
+            PLASSERT( hidden_bias_grad-&gt;isEmpty() &amp;&amp;
+                      hidden_bias_grad-&gt;width() == hidden_layer-&gt;size );
+            hidden_bias_grad-&gt;resize(mbs,hidden_layer-&gt;size);
+            *hidden_bias_grad += hidden_act_grad;
+        }
+        // Compute gradient w.r.t. expectations of the visible layer (=
+        // inputs).
+        Mat* store_visible_grad = NULL;
+        if (compute_visible_grad) {
+            PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
+            store_visible_grad = visible_grad;
+        } else {
+            // We do not actually need to store the gradient, but since it
+            // is required in bpropUpdate, we provide a dummy matrix to
+            // store it.
+            store_visible_grad = &amp;visible_exp_grad;
+        }
+        store_visible_grad-&gt;resize(mbs,visible_layer-&gt;size);
+
+        if (weights)
+        {
+            int up = connection-&gt;up_size;
+            int down = connection-&gt;down_size;
+            PLASSERT( !weights-&gt;isEmpty() &amp;&amp;
+                      weights_grad &amp;&amp; weights_grad-&gt;isEmpty() &amp;&amp;
+                      weights_grad-&gt;width() == up * down );
+            weights_grad-&gt;resize(mbs, up * down);
+            Mat w, wg;
+            Vec v,h,vg,hg;
+            for(int i=0; i&lt;mbs; i++)
             {
-                PLASSERT( hidden_bias_grad-&gt;isEmpty() &amp;&amp;
-                          hidden_bias_grad-&gt;width() == hidden_layer-&gt;size );
-                hidden_bias_grad-&gt;resize(mbs,hidden_layer-&gt;size);
-                *hidden_bias_grad += hidden_act_grad;
+                w = Mat(up, down,(*weights)(i));
+                wg = Mat(up, down,(*weights_grad)(i));
+                v = (*visible)(i);
+                h = (*hidden_act)(i);
+                vg = (*store_visible_grad)(i);
+                hg = hidden_act_grad(i);
+                connection-&gt;petiteCulotteOlivierUpdate(
+                    v,
+                    w,
+                    h,
+                    vg,
+                    wg,
+                    hg,true);
             }
-            // Compute gradient w.r.t. expectations of the visible layer (=
-            // inputs).
-            Mat* store_visible_grad = NULL;
-            if (compute_visible_grad) {
-                PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
-                store_visible_grad = visible_grad;
-            } else {
-                // We do not actually need to store the gradient, but since it
-                // is required in bpropUpdate, we provide a dummy matrix to
-                // store it.
-                store_visible_grad = &amp;visible_exp_grad;
-            }
-            store_visible_grad-&gt;resize(mbs,visible_layer-&gt;size);
-            
-            if (weights)
-            {
-                int up = connection-&gt;up_size;
-                int down = connection-&gt;down_size;
-                PLASSERT( !weights-&gt;isEmpty() &amp;&amp;
-                          weights_grad &amp;&amp; weights_grad-&gt;isEmpty() &amp;&amp;
-                          weights_grad-&gt;width() == up * down );
-                weights_grad-&gt;resize(mbs, up * down);
-                Mat w, wg;
-                Vec v,h,vg,hg;
-                for(int i=0; i&lt;mbs; i++)
-                {
-                    w = Mat(up, down,(*weights)(i));
-                    wg = Mat(up, down,(*weights_grad)(i));
-                    v = (*visible)(i);
-                    h = (*hidden_act)(i);
-                    vg = (*store_visible_grad)(i);
-                    hg = hidden_act_grad(i);
-                    connection-&gt;petiteCulotteOlivierUpdate(
-                        v,
-                        w,
-                        h,
-                        vg,
-                        wg,
-                        hg,true);
-                }
-            }
-            else
-            {
-                connection-&gt;bpropUpdate(
-                    *visible, *hidden_act, *store_visible_grad,
-                    hidden_act_grad, true);
-            }
-            partition_function_is_stale = true;
+        }
+        else
+        {
+            connection-&gt;bpropUpdate(
+                *visible, *hidden_act, *store_visible_grad,
+                hidden_act_grad, true);
+        }
+        partition_function_is_stale = true;
     }
 
     if (cd_learning_rate &gt; 0 &amp;&amp; minimize_log_likelihood) {
@@ -1331,7 +1358,7 @@
         // Perform a step of contrastive divergence.
         PLASSERT( visible &amp;&amp; !visible-&gt;isEmpty() );
         setAllLearningRates(cd_learning_rate);
-        Mat* negative_phase_visible_samples = 
+        Mat* negative_phase_visible_samples =
             compute_contrastive_divergence?ports_value[getPortIndex(&quot;negative_phase_visible_samples.state&quot;)]:0;
         const Mat* negative_phase_hidden_expectations =
             compute_contrastive_divergence ?
@@ -1341,7 +1368,7 @@
             compute_contrastive_divergence ?
                 ports_value[getPortIndex(&quot;negative_phase_hidden_activations.state&quot;)]
                 : NULL;
-        
+
         PLASSERT( visible &amp;&amp; hidden );
         PLASSERT( !negative_phase_visible_samples ||
                   !negative_phase_visible_samples-&gt;isEmpty() );
@@ -1550,14 +1577,24 @@
             visible_act_grad(t) *= (*reconstruction_error_grad)(t,0);
 
         // Visible bias update
-        columnSum(visible_act_grad,visible_bias_grad);
+        columnMean(visible_act_grad, visible_bias_grad);
         visible_layer-&gt;update(visible_bias_grad);
 
         // Reconstruction connection update
-        reconstruction_connection-&gt;bpropUpdate(
-            *hidden, *visible_reconstruction_activations,
-            hidden_exp_grad, visible_act_grad, false);
-        
+        hidden_exp_grad.resize(mbs, hidden_layer-&gt;size);
+        hidden_exp_grad.clear();
+        hidden_exp_grad.resize(0, hidden_layer-&gt;size);
+
+        TVec&lt;Mat*&gt; rec_ports_value(2);
+        rec_ports_value[0] = visible_reconstruction_activations;
+        rec_ports_value[1] = hidden;
+        TVec&lt;Mat*&gt; rec_ports_gradient(2);
+        rec_ports_gradient[0] = &amp;visible_act_grad;
+        rec_ports_gradient[1] = &amp;hidden_exp_grad;
+
+        reconstruction_connection-&gt;bpropAccUpdate( rec_ports_value,
+                                                   rec_ports_gradient );
+
         // Hidden layer bias update
         hidden_layer-&gt;bpropUpdate(*hidden_act,
                                   *hidden, hidden_act_grad,
@@ -1585,7 +1622,7 @@
         }
         else
         {
-            visible_exp_grad.resize(mbs,visible_layer-&gt;size);        
+            visible_exp_grad.resize(mbs,visible_layer-&gt;size);
             connection-&gt;bpropUpdate(
                 *visible, *hidden_act,
                 visible_exp_grad, hidden_act_grad, true);

Modified: trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h	2007-07-11 14:26:41 UTC (rev 7743)
+++ trunk/plearn_learners/online/EXPERIMENTAL/KLp0p1RBMModule.h	2007-07-11 17:17:13 UTC (rev 7744)
@@ -78,12 +78,12 @@
 
     bool compute_contrastive_divergence;
 
-    //! Number of Gibbs sampling steps in negative phase 
+    //! Number of Gibbs sampling steps in negative phase
     //! of contrastive divergence.
     int n_Gibbs_steps_CD;
 
     //! used to generate samples from the RBM
-    int min_n_Gibbs_steps; 
+    int min_n_Gibbs_steps;
     int n_Gibbs_steps_per_generated_sample;
 
     bool compute_log_likelihood;
@@ -98,8 +98,8 @@
     bool standard_cd_grad;
     bool standard_cd_bias_grad;
     bool standard_cd_weights_grad;
-    
 
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -249,7 +249,7 @@
     Mat hidden_exp_store;
     Mat hidden_act_store;
     Mat* hidden_act;
-    bool hidden_activations_are_computed;    
+    bool hidden_activations_are_computed;
 
     //! Used to store the contrastive divergence gradient w.r.t. weights.
     Mat store_weights_grad;
@@ -319,7 +319,7 @@
     //! it in the 'energy' matrix.
     //! The 'positive_phase' boolean is used to save computations when we know
     //! we are in the positive phase of fprop.
-    void computeEnergy(const Mat&amp; visible, const Mat&amp; hidden, Mat&amp; energy, 
+    void computeEnergy(const Mat&amp; visible, const Mat&amp; hidden, Mat&amp; energy,
                        bool positive_phase = true);
 
 private:


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001191.html">[Plearn-commits] r7743 - trunk/python_modules/plearn/parallel
</A></li>
	<LI>Next message: <A HREF="001193.html">[Plearn-commits] r7745 -	tags/after_energy_sign_changes/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1192">[ date ]</a>
              <a href="thread.html#1192">[ thread ]</a>
              <a href="subject.html#1192">[ subject ]</a>
              <a href="author.html#1192">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
