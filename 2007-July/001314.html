<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7859 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7859%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200707301646.l6UGkcMY007885%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001306.html">
   <LINK REL="Next"  HREF="001307.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7859 - trunk/plearn_learners_experimental</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7859%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200707301646.l6UGkcMY007885%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7859 - trunk/plearn_learners_experimental">larocheh at mail.berlios.de
       </A><BR>
    <I>Mon Jul 30 18:46:38 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001306.html">[Plearn-commits] r7858 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="001307.html">[Plearn-commits] r7860 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1314">[ date ]</a>
              <a href="thread.html#1314">[ thread ]</a>
              <a href="subject.html#1314">[ subject ]</a>
              <a href="author.html#1314">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-07-30 18:46:28 +0200 (Mon, 30 Jul 2007)
New Revision: 7859

Modified:
   trunk/plearn_learners_experimental/StackedSVDNet.cc
   trunk/plearn_learners_experimental/StackedSVDNet.h
Log:
blu bla


Modified: trunk/plearn_learners_experimental/StackedSVDNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-07-30 16:44:50 UTC (rev 7858)
+++ trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-07-30 16:46:28 UTC (rev 7859)
@@ -36,12 +36,12 @@
 
 /*! \file StackedSVDNet.cc */
 
+#include &quot;StackedSVDNet.h&quot;
 
 #define PL_LOG_MODULE_NAME &quot;StackedSVDNet&quot;
 #include &lt;plearn/io/pl_log.h&gt;
 #include &lt;plearn/math/plapack.h&gt;
 
-#include &quot;StackedSVDNet.h&quot;
 
 namespace PLearn {
 using namespace std;
@@ -58,7 +58,8 @@
     fine_tuning_learning_rate( 0. ),
     fine_tuning_decrease_ct( 0. ),
     batch_size(50),
-    minimum_relative_improvement(1e-3),
+    global_output_layer(false),
+    relative_min_improvement(1e-3),
     n_layers( 0 )
 {
     // random_gen will be initialized in PLearner::build_()
@@ -99,9 +100,16 @@
                   OptionBase::buildoption,
                   &quot;Size of mini-batch for gradient descent&quot;);
 
-    declareOption(ol, &quot;minimum_relative_improvement&quot;, 
-                  &amp;StackedSVDNet::minimum_relative_improvement,
+    declareOption(ol, &quot;global_output_layer&quot;, 
+                  &amp;StackedSVDNet::global_output_layer,
                   OptionBase::buildoption,
+                  &quot;Indication that the output layer (given by the final module)\n&quot;
+                  &quot;should have as input all units of the network (including the&quot;
+                  &quot;input units).\n&quot;);
+
+    declareOption(ol, &quot;relative_min_improvement&quot;, 
+                  &amp;StackedSVDNet::relative_min_improvement,
+                  OptionBase::buildoption,
                   &quot;Minimum relative improvement convergence criteria \n&quot;
                   &quot;for the logistic auto-regression.&quot;);
 
@@ -159,7 +167,7 @@
                     &quot;layers[0] should have a size of %d.\n&quot;,
                     inputsize_);
 
-        reconstruction_costs(batch_size,1);    
+        reconstruction_costs.resize(batch_size,1);    
 
         activation_gradients.resize( n_layers );
         expectation_gradients.resize( n_layers );
@@ -200,15 +208,36 @@
             PLERROR(&quot;StackedSVDNet::build_costs() - \n&quot;
                     &quot;final_module should be provided.\n&quot;);
     
-        if( layers[n_layers-1]-&gt;size != final_module-&gt;input_size )
-            PLERROR(&quot;StackedSVDNet::build_costs() - \n&quot;
-                    &quot;final_module should have an input_size of %d.\n&quot;, 
-                    layers[n_layers-1]-&gt;size);
-    
+        if(global_output_layer)
+        {
+            int sum = 0;
+            for(int i=0; i&lt;layers.length(); i++)
+                sum += layers[i]-&gt;size;
+            if( sum != final_module-&gt;input_size )
+                PLERROR(&quot;StackedSVDNet::build_costs() - \n&quot;
+                        &quot;final_module should have an input_size of %d.\n&quot;, 
+                        sum);
+
+            global_output_layer_input.resize(sum);
+            global_output_layer_inputs.resize(batch_size,sum);
+            global_output_layer_input_gradients.resize(batch_size,sum);
+            expectation_gradients[n_layers-1] = 
+                global_output_layer_input_gradients.subMat(
+                    0, sum-layers[n_layers-1]-&gt;size, 
+                    batch_size, layers[n_layers-1]-&gt;size);
+        }
+        else
+        {
+            if( layers[n_layers-1]-&gt;size != final_module-&gt;input_size )
+                PLERROR(&quot;StackedSVDNet::build_costs() - \n&quot;
+                        &quot;final_module should have an input_size of %d.\n&quot;, 
+                        layers[n_layers-1]-&gt;size);
+        }
+
         if( final_module-&gt;output_size != final_cost-&gt;input_size )
             PLERROR(&quot;StackedSVDNet::build_costs() - \n&quot;
                     &quot;final_module should have an output_size of %d.\n&quot;, 
-                    final_module-&gt;input_size);
+                    final_cost-&gt;input_size);
 
         final_module-&gt;setLearningRate( fine_tuning_learning_rate );
 
@@ -243,6 +272,7 @@
     deepCopyField(final_module, copies);
     deepCopyField(final_cost, copies);
     deepCopyField(connections, copies);
+    deepCopyField(rbm_connections, copies);
     deepCopyField(activation_gradients, copies);
     deepCopyField(expectation_gradients, copies);
     deepCopyField(reconstruction_layer, copies);
@@ -251,6 +281,9 @@
     deepCopyField(reconstruction_activation_gradient, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
     deepCopyField(reconstruction_input_gradients, copies);
+    deepCopyField(global_output_layer_input, copies);
+    deepCopyField(global_output_layer_inputs, copies);
+    deepCopyField(global_output_layer_input_gradients, copies);
     deepCopyField(final_cost_inputs, copies);
     deepCopyField(final_cost_value, copies);
     deepCopyField(final_cost_values, copies);
@@ -273,6 +306,9 @@
     connections.resize(0);
     rbm_connections.resize(0);
     
+    for(int i=0; i&lt;layers.length(); i++)
+        layers[i]-&gt;forget();
+
     final_module-&gt;forget();
     final_cost-&gt;forget();
 
@@ -315,6 +351,7 @@
             connections[i] = new RBMMatrixConnection();
             connections[i]-&gt;up_size = layers[i]-&gt;size;
             connections[i]-&gt;down_size = layers[i]-&gt;size;
+            connections[i]-&gt;random_gen = random_gen;
             connections[i]-&gt;build();
             for(int j=0; j &lt; layers[i]-&gt;size; j++)
                 connections[i]-&gt;weights(j,j) = 0;
@@ -339,10 +376,11 @@
             int nupdates = 0;
             int nepochs = 0;
             while( nepochs &lt; 2 ||
-                   (last_cost - cost) / last_cost &gt;= minimum_relative_improvement )
+                   (last_cost - cost) / last_cost &gt;= relative_min_improvement )
             {
                 train_stats-&gt;forget();
-                for(int sample = 0; sample &lt; train_set.length()/batch_size; 
+                for(int sample = 0; 
+                    sample &lt; train_set.length()/batch_size; 
                     sample++)
                 {
                     if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
@@ -367,29 +405,50 @@
                 train_stats-&gt;finalize();
                 nepochs++;
                 last_cost = cost;
-                cost = train_stats-&gt;getMean()[0];
+                cost = train_stats-&gt;getMean()[i];
+                if(verbosity &gt; 2)
+                    cout &lt;&lt; &quot;reconstruction error at iteration &quot; &lt;&lt; nepochs &lt;&lt; 
+                        &quot;: &quot; &lt;&lt; 
+                        cost &lt;&lt; &quot; or &quot; &lt;&lt; cost/layers[i]-&gt;size &lt;&lt; &quot; (rel)&quot; &lt;&lt; endl;
             }
-            Mat A,U,Vt;
-            Vec S;
-            A.resize( reconstruction_layer-&gt;size, reconstruction_layer-&gt;size+1);
-            A.column( 0 ) &lt;&lt; reconstruction_layer-&gt;bias;
-            A.subMat( 0, 1, reconstruction_layer-&gt;size, 
-                      reconstruction_layer-&gt;size ) &lt;&lt; 
-                connections[i]-&gt;weights;
-            SVD( A, U, S, Vt );
-            connections[ i ]-&gt;up_size = layers[ i+1 ]-&gt;size;
-            connections[ i ]-&gt;down_size = layers[ i ]-&gt;size;
-            connections[ i ]-&gt;build();
-            connections[ i ]-&gt;weights &lt;&lt; Vt.subMat( 
-                0, 0, layers[i+1]-&gt;size, Vt.width() );
-            biases[ i ].resize( layers[i+1]-&gt;size );
-            biases[ i ] &lt;&lt; Vt.column( 0 ).toVec().subVec( 
-                0, layers[i+1]-&gt;size );
-            for(int j=0; j&lt;connections[ i ]-&gt;up_size; j++)
+
+            // Fill in the empty diagonal
+            for(int j=0; j&lt;layers[i]-&gt;size; j++)
             {
-                connections[ i ]-&gt;weights( j ) *= S[ j ];
-                biases[ i ][ j ] *= S[ j ];
+                connections[i]-&gt;weights(j,j) = maxabs(connections[i]-&gt;weights(j));
             }
+
+            if(layers[i]-&gt;size != layers[i+1]-&gt;size)
+            {
+                Mat A,U,Vt;
+                Vec S;
+                A.resize( reconstruction_layer-&gt;size, 
+                          reconstruction_layer-&gt;size+1);
+                A.column( 0 ) &lt;&lt; reconstruction_layer-&gt;bias;
+                A.subMat( 0, 1, reconstruction_layer-&gt;size, 
+                          reconstruction_layer-&gt;size ) &lt;&lt; 
+                    connections[i]-&gt;weights;
+                SVD( A, U, S, Vt );
+                connections[ i ]-&gt;up_size = layers[ i+1 ]-&gt;size;
+                connections[ i ]-&gt;down_size = layers[ i ]-&gt;size;
+                connections[ i ]-&gt;build();
+                connections[ i ]-&gt;weights &lt;&lt; Vt.subMat( 
+                    0, 1, layers[ i+1 ]-&gt;size, Vt.width()-1 );
+                biases[ i ].resize( layers[i+1]-&gt;size );
+                for(int j=0; j&lt;biases[ i ].length(); j++)
+                    biases[ i ][ j ] = Vt(j,0);
+
+                for(int j=0; j&lt;connections[ i ]-&gt;up_size; j++)
+                {
+                    connections[ i ]-&gt;weights( j ) *= S[ j ];
+                    biases[ i ][ j ] *= S[ j ];
+                }
+            }
+            else
+            {
+                biases[ i ].resize( layers[ i+1 ]-&gt;size );
+                biases[ i ] &lt;&lt; reconstruction_layer-&gt;bias;
+            }
         }
         stage++;
         for(int i=0; i&lt;biases.length(); i++)
@@ -419,7 +478,9 @@
 
         for( ; stage&lt;nstages ; stage++ )
         {
-            for( int sample = 0; sample&lt;train_set-&gt;length()/batch_size; sample++)
+            for( int sample = 0; 
+                 sample&lt;train_set-&gt;length()/batch_size; 
+                 sample++)
             {
                 if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
                     setLearningRate( fine_tuning_learning_rate
@@ -438,6 +499,10 @@
                 if( pb )
                     pb-&gt;update( stage - init_stage + 1 );
             }
+            if(verbosity &gt; 2)
+                cout &lt;&lt; &quot;error at stage &quot; &lt;&lt; stage &lt;&lt; &quot;: &quot; &lt;&lt; 
+                    train_stats-&gt;getMean() &lt;&lt; endl;
+
         }
     }
     
@@ -498,25 +563,62 @@
         layers[ i+1 ]-&gt;computeExpectations();
     }
 
-    final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;getExpectations(),
-                         final_cost_inputs );
+    if( global_output_layer )
+    {
+        int offset = 0;
+        for(int i=0; i&lt;layers.length(); i++)
+        {
+            global_output_layer_inputs.subMat(0, offset, 
+                                              batch_size, layers[i]-&gt;size)
+                &lt;&lt; layers[i]-&gt;getExpectations();
+            offset += layers[i]-&gt;size;
+        }
+        final_module-&gt;fprop( global_output_layer_inputs, final_cost_inputs );
+    }
+    else
+    {
+        final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;getExpectations(),
+                             final_cost_inputs );
+    }
     final_cost-&gt;fprop( final_cost_inputs, targets, final_cost_values );
 
     columnMean( final_cost_values, 
                 final_cost_value );
-    train_costs.subVec(train_costs.length()-final_cost_value.length()) &lt;&lt; 
-        final_cost_value;
+    train_costs.subVec(train_costs.length()-final_cost_value.length(),
+                       final_cost_value.length()) &lt;&lt; final_cost_value;
 
     final_cost-&gt;bpropUpdate( final_cost_inputs, targets,
-                             final_cost_values,
+                             final_cost_value,
                              final_cost_gradients );
-    final_module-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;getExpectations(),
-                               final_cost_inputs,
-                               expectation_gradients[ n_layers-1 ],
-                               final_cost_gradients );
+    
+    if( global_output_layer )
+    {
+        final_module-&gt;bpropUpdate( global_output_layer_inputs,
+                                   final_cost_inputs,
+                                   global_output_layer_input_gradients,
+                                   final_cost_gradients );     
+    }
+    else
+    {
+        final_module-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;getExpectations(),
+                                   final_cost_inputs,
+                                   expectation_gradients[ n_layers-1 ],
+                                   final_cost_gradients );
+    }
 
+    int sum = final_module-&gt;input_size - layers[ n_layers-1 ]-&gt;size;
     for( int i=n_layers-1 ; i&gt;0 ; i-- )
     {
+        if( global_output_layer &amp;&amp; i != n_layers-1 )
+        {
+            expectation_gradients[ i ] +=  
+                global_output_layer_input_gradients.subMat(
+                    0, sum - layers[i]-&gt;size,
+                    batch_size, layers[i]-&gt;size);
+            sum -= layers[i]-&gt;size;
+        }
+                
+
         layers[ i ]-&gt;bpropUpdate( layers[ i ]-&gt;activations,
                                   layers[ i ]-&gt;getExpectations(),
                                   activation_gradients[ i ],
@@ -538,12 +640,26 @@
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
         connections[ i ]-&gt;setAsDownInput( layers[i]-&gt;expectation );
-        layers[ i+1 ]-&gt;getAllActivations( rbm_connections[i], 0, true );
+        layers[ i+1 ]-&gt;getAllActivations( rbm_connections[i], 0, false );
         layers[ i+1 ]-&gt;computeExpectation();
     }
 
-    final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
-                         output );
+    if(global_output_layer)
+    {
+        int offset = 0;
+        for(int i=0; i&lt;layers.length(); i++)
+        {
+            global_output_layer_input.subVec(offset, layers[i]-&gt;size)
+                &lt;&lt; layers[i]-&gt;expectation;
+            offset += layers[i]-&gt;size;
+        }
+        final_module-&gt;fprop( global_output_layer_input, output );
+    }
+    else
+    {
+        final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
+                             output );
+    }
 }
 
 void StackedSVDNet::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,

Modified: trunk/plearn_learners_experimental/StackedSVDNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.h	2007-07-30 16:44:50 UTC (rev 7858)
+++ trunk/plearn_learners_experimental/StackedSVDNet.h	2007-07-30 16:46:28 UTC (rev 7859)
@@ -77,10 +77,14 @@
 
     //! Size of mini-batch for gradient descent
     int batch_size;
+
+    //! Indication that the output layer (given by the final module)
+    //! should have as input all units of the network (including the input units)
+    bool global_output_layer;
     
     //! Minimum relative improvement convergence criteria
     //! for the logistic auto-regression.
-    real minimum_relative_improvement;
+    real relative_min_improvement;
     
     //! The layers of units in the network
     TVec&lt; PP&lt;RBMLayer&gt; &gt; layers;
@@ -198,6 +202,15 @@
     //! Reconstruction activations
     mutable Mat reconstruction_input_gradients;
 
+    //! Global output layer input
+    mutable Vec global_output_layer_input;
+
+    //! Global output layer inputs
+    mutable Mat global_output_layer_inputs;
+
+    //! Global output layer input gradients
+    mutable Mat global_output_layer_input_gradients;
+
     //! Inputs of the final_cost
     mutable Mat final_cost_inputs;
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001306.html">[Plearn-commits] r7858 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="001307.html">[Plearn-commits] r7860 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1314">[ date ]</a>
              <a href="thread.html#1314">[ thread ]</a>
              <a href="subject.html#1314">[ subject ]</a>
              <a href="author.html#1314">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
