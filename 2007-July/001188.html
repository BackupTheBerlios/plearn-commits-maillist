<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7740 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7740%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200707101640.l6AGeWH3010730%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001187.html">
   <LINK REL="Next"  HREF="001189.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7740 - trunk/plearn_learners_experimental</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7740%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200707101640.l6AGeWH3010730%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7740 - trunk/plearn_learners_experimental">larocheh at mail.berlios.de
       </A><BR>
    <I>Tue Jul 10 18:40:32 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001187.html">[Plearn-commits] r7739 - trunk
</A></li>
        <LI>Next message: <A HREF="001189.html">[Plearn-commits] r7741 - trunk/plearn/io
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1188">[ date ]</a>
              <a href="thread.html#1188">[ thread ]</a>
              <a href="subject.html#1188">[ subject ]</a>
              <a href="author.html#1188">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-07-10 18:40:25 +0200 (Tue, 10 Jul 2007)
New Revision: 7740

Modified:
   trunk/plearn_learners_experimental/StackedSVDNet.cc
   trunk/plearn_learners_experimental/StackedSVDNet.h
Log:
blu


Modified: trunk/plearn_learners_experimental/StackedSVDNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-07-10 15:27:48 UTC (rev 7739)
+++ trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-07-10 16:40:25 UTC (rev 7740)
@@ -32,13 +32,14 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Pascal Lamblin
+// Authors: Hugo Larochelle
 
 /*! \file StackedSVDNet.cc */
 
 
 #define PL_LOG_MODULE_NAME &quot;StackedSVDNet&quot;
 #include &lt;plearn/io/pl_log.h&gt;
+#include &lt;plearn/math/plapack.h&gt;
 
 #include &quot;StackedSVDNet.h&quot;
 
@@ -58,8 +59,7 @@
     fine_tuning_decrease_ct( 0. ),
     batch_size(50),
     minimum_relative_improvement(1e-3),
-    n_layers( 0 ),
-    currently_trained_layer( 0 )
+    n_layers( 0 )
 {
     // random_gen will be initialized in PLearner::build_()
     random_gen = new PRandom();
@@ -158,9 +158,9 @@
             PLERROR(&quot;StackedSVDNet::build_layers_and_connections() - \n&quot;
                     &quot;layers[0] should have a size of %d.\n&quot;,
                     inputsize_);
-    
-        activations.resize( n_layers );
-        expectations.resize( n_layers );
+
+        reconstruction_costs(batch_size,1);    
+
         activation_gradients.resize( n_layers );
         expectation_gradients.resize( n_layers );
 
@@ -176,8 +176,6 @@
                 PLERROR(&quot;In StackedSVDNet::build()_: &quot;
                     &quot;layers must have decreasing sizes from bottom to top.&quot;);
                 
-            activations[i].resize( batch_size, layers[i]-&gt;size );
-            expectations[i].resize( batch_size, layers[i]-&gt;size );
             activation_gradients[i].resize( batch_size, layers[i]-&gt;size );
             expectation_gradients[i].resize( batch_size, layers[i]-&gt;size );
         }
@@ -186,7 +184,10 @@
             PLERROR(&quot;StackedSVDNet::build_costs() - \n&quot;
                     &quot;final_cost should be provided.\n&quot;);
 
-        final_cost_gradient.resize( final_cost-&gt;input_size );
+        final_cost_inputs.resize( batch_size, final_cost-&gt;input_size );
+        final_cost_value.resize( final_cost-&gt;output_size );
+        final_cost_values.resize( batch_size, final_cost-&gt;output_size );
+        final_cost_gradients.resize( batch_size, final_cost-&gt;input_size );
         final_cost-&gt;setLearningRate( fine_tuning_learning_rate );
 
         if( !(final_cost-&gt;random_gen) )
@@ -238,31 +239,25 @@
 
     // deepCopyField(, copies);
 
-    deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
-    deepCopyField(connections, copies);
-    deepCopyField(reconstruction_connections, copies);
     deepCopyField(final_module, copies);
     deepCopyField(final_cost, copies);
-    deepCopyField(partial_costs, copies);
-    deepCopyField(partial_costs_weights, copies);
-    deepCopyField(activations, copies);
-    deepCopyField(expectations, copies);
+    deepCopyField(connections, copies);
     deepCopyField(activation_gradients, copies);
     deepCopyField(expectation_gradients, copies);
-    deepCopyField(reconstruction_activations, copies);
-    deepCopyField(reconstruction_expectations, copies);
+    deepCopyField(reconstruction_layer, copies);
+    deepCopyField(reconstruction_targets, copies);
+    deepCopyField(reconstruction_costs, copies);
+    deepCopyField(reconstruction_activation_gradient, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
-    deepCopyField(reconstruction_expectation_gradients, copies);
-    deepCopyField(partial_costs_positions, copies);
-    deepCopyField(partial_cost_value, copies);
-    deepCopyField(final_cost_input, copies);
+    deepCopyField(reconstruction_input_gradients, copies);
+    deepCopyField(final_cost_inputs, copies);
     deepCopyField(final_cost_value, copies);
-    deepCopyField(final_cost_gradient, copies);
-    deepCopyField(greedy_stages, copies);
+    deepCopyField(final_cost_values, copies);
+    deepCopyField(final_cost_gradients, copies);
     
-    PLERROR(&quot;In StackedSVDNet::makeDeepCopyFromShallowCopy(): &quot;
-            &quot;not implemented yet.&quot;);
+    //PLERROR(&quot;In StackedSVDNet::makeDeepCopyFromShallowCopy(): &quot;
+    //        &quot;not implemented yet.&quot;);
 }
 
 
@@ -276,6 +271,7 @@
     inherited::forget();
 
     connections.resize(0);
+    rbm_connections.resize(0);
     
     final_module-&gt;forget();
     final_cost-&gt;forget();
@@ -286,19 +282,17 @@
 void StackedSVDNet::train()
 {
     MODULE_LOG &lt;&lt; &quot;train() called &quot; &lt;&lt; endl;
-    MODULE_LOG &lt;&lt; &quot;  training_schedule = &quot; &lt;&lt; training_schedule &lt;&lt; endl;
 
     Vec input( inputsize() );
     Vec target( targetsize() );
     real weight; // unused
+    Mat inputs( batch_size, inputsize() );
+    Mat targets( batch_size, targetsize() );
 
     TVec&lt;string&gt; train_cost_names = getTrainCostNames() ;
     Vec train_costs( train_cost_names.length() );
     train_costs.fill(MISSING_VALUE) ;
 
-    int nsamples = train_set-&gt;length();
-    int sample;
-
     PP&lt;ProgressBar&gt; pb;
 
     // clear stats of previous epoch
@@ -311,6 +305,7 @@
     if(stage == 0)
     {
         connections.resize(n_layers-1);
+        rbm_connections.resize(n_layers-1);
         TVec&lt; Vec &gt; biases(n_layers-1);
         for( int i=0 ; i&lt;n_layers-1 ; i++ )
         {
@@ -324,63 +319,82 @@
             for(int j=0; j &lt; layers[i]-&gt;size; j++)
                 connections[i]-&gt;weights(j,j) = 0;
 
+            rbm_connections[i] = (RBMMatrixConnection *) connections[i];
+
+            CopiesMap map;
+            reconstruction_layer = layers[ i ]-&gt;deepCopy( map );
+            reconstruction_targets.resize( batch_size, layers[ i ]-&gt;size );
+            reconstruction_activation_gradient.resize( layers[ i ]-&gt;size );
+            reconstruction_activation_gradients.resize( 
+                batch_size, layers[ i ]-&gt;size );
+            reconstruction_input_gradients.resize( 
+                batch_size, layers[ i ]-&gt;size );
+
             lr = greedy_learning_rate;
-            layers[i]-&gt;setLearningRate( lr );
             connections[i]-&gt;setLearningRate( lr );
-            layers[i+1]-&gt;setLearningRate( lr );
+            reconstruction_layer-&gt;setLearningRate( lr );
 
-            real cost = 30;
-            real last_cost = 100;
+            real cost = 0;
+            real last_cost = 0;
             int nupdates = 0;
             int nepochs = 0;
             while( nepochs &lt; 2 ||
                    (last_cost - cost) / last_cost &gt;= minimum_relative_improvement )
             {
                 train_stats-&gt;forget();
-                for(int sample = 0; sample &lt; train_set.length(); sample++)
+                for(int sample = 0; sample &lt; train_set.length()/batch_size; 
+                    sample++)
                 {
                     if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
                     {
                         lr = greedy_learning_rate/(1 + greedy_decrease_ct 
                                                    * nupdates);
-                        layers[i]-&gt;setLearningRate( lr );
                         connections[i]-&gt;setLearningRate( lr );
-                        reconstruction_connections[i]-&gt;setLearningRate( lr );
-                        layers[i+1]-&gt;setLearningRate( lr );                
+                        reconstruction_layer-&gt;setLearningRate( lr );                
                     }
-
-                    train_set-&gt;getExample(sample, input, target, weight);
-                    greedyStep( input, target, sample, train_costs );
+                    
+                    for(int j=0; j&lt;batch_size; j++)
+                    {
+                        train_set-&gt;getExample(sample*batch_size + j, 
+                                              input, target, weight);
+                        inputs(j) &lt;&lt; input;
+                        targets(j) &lt;&lt; target;
+                    }
+                    greedyStep( inputs, targets, i, train_costs );
                     nupdates++;
                     train_stats-&gt;update( train_costs );
                 }
                 train_stats-&gt;finalize();
                 nepochs++;
                 last_cost = cost;
-                cost = train_stats-&gt;mean()[0];
+                cost = train_stats-&gt;getMean()[0];
             }
-            Mat A,U,S,Vt;
-            A.resize(layers[i]-&gt;size,layers[i]-&gt;size+1);
-            A.column(0) &lt;&lt; layers[i]-&gt;bias;
-            A.subMat(0,1,layers[i]-&gt;size,layers[i]-&gt;size) &lt;&lt; 
+            Mat A,U,Vt;
+            Vec S;
+            A.resize( reconstruction_layer-&gt;size, reconstruction_layer-&gt;size+1);
+            A.column( 0 ) &lt;&lt; reconstruction_layer-&gt;bias;
+            A.subMat( 0, 1, reconstruction_layer-&gt;size, 
+                      reconstruction_layer-&gt;size ) &lt;&lt; 
                 connections[i]-&gt;weights;
-            SVD(connections[i]-&gt;weights,U,S,V);
-            connections[i]-&gt;up_size = layers[i+1]-&gt;size;
-            connections[i]-&gt;down_size = layers[i]-&gt;size;
-            connections[i]-&gt;build();
-            connection[i]-&gt;weights &lt;&lt; Vt.subRows(0,layers[i+1]-&gt;size);
-            biases[i].resize(layers[i+1]-&gt;size);
-            biases[i] &lt;&lt; Vt.column(0).subVec(0,layers[i+1]-&gt;size);
-            for(int j=0; j&lt;connections[i]-&gt;up_size; j++)
+            SVD( A, U, S, Vt );
+            connections[ i ]-&gt;up_size = layers[ i+1 ]-&gt;size;
+            connections[ i ]-&gt;down_size = layers[ i ]-&gt;size;
+            connections[ i ]-&gt;build();
+            connections[ i ]-&gt;weights &lt;&lt; Vt.subMat( 
+                0, 0, layers[i+1]-&gt;size, Vt.width() );
+            biases[ i ].resize( layers[i+1]-&gt;size );
+            biases[ i ] &lt;&lt; Vt.column( 0 ).toVec().subVec( 
+                0, layers[i+1]-&gt;size );
+            for(int j=0; j&lt;connections[ i ]-&gt;up_size; j++)
             {
-                connections[i]-&gt;weights(j) *= S(j,j);
-                biases[i][j] *= S(j,j);
+                connections[ i ]-&gt;weights( j ) *= S[ j ];
+                biases[ i ][ j ] *= S[ j ];
             }
         }
         stage++;
         for(int i=0; i&lt;biases.length(); i++)
         {
-            layers[i]-&gt;bias &lt;&lt; biases[i];
+            layers[ i+1 ]-&gt;bias &lt;&lt; biases[ i ];
         }
     }
 
@@ -402,158 +416,134 @@
 
         setLearningRate( fine_tuning_learning_rate );
         train_costs.fill(MISSING_VALUE);
+
         for( ; stage&lt;nstages ; stage++ )
         {
-            sample = stage % nsamples;
-            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
-                setLearningRate( fine_tuning_learning_rate
-                                 / (1. + fine_tuning_decrease_ct * stage ) );
+            for( int sample = 0; sample&lt;train_set-&gt;length()/batch_size; sample++)
+            {
+                if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                    setLearningRate( fine_tuning_learning_rate
+                                     / (1. + fine_tuning_decrease_ct * stage ) );
 
-            train_set-&gt;getExample( sample, input, target, weight );
-            fineTuningStep( input, target, train_costs );
-            train_stats-&gt;update( train_costs );
-
-            if( pb )
-                pb-&gt;update( stage - init_stage + 1 );
+                for(int j=0; j&lt;batch_size; j++)
+                {
+                    train_set-&gt;getExample(sample*batch_size + j, 
+                                          input, target, weight);
+                    inputs(j) &lt;&lt; input;
+                    targets(j) &lt;&lt; target;
+                }
+                fineTuningStep( inputs, targets, train_costs );
+                train_stats-&gt;update( train_costs );
+                
+                if( pb )
+                    pb-&gt;update( stage - init_stage + 1 );
+            }
         }
     }
     
     train_stats-&gt;finalize();
 }
 
-void StackedSVDNet::greedyStep( const Vec&amp; input, const Vec&amp; target, int index, Vec train_costs )
+void StackedSVDNet::greedyStep( const Mat&amp; inputs, const Mat&amp; targets, int index, Vec train_costs )
 {
     PLASSERT( index &lt; n_layers );
 
-    expectations[0] &lt;&lt; input;
-    for( int i=0 ; i&lt;index + 1; i++ )
+    layers[ 0 ]-&gt;setExpectations( inputs );
+    
+    for( int i=0 ; i&lt;index ; i++ )
     {
-        connections[i]-&gt;fprop( expectations[i], activations[i+1] );
-        layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+        connections[ i ]-&gt;setAsDownInputs( layers[i]-&gt;getExpectations() );
+        layers[ i+1 ]-&gt;getAllActivations( rbm_connections[i], 0, true );
+        layers[ i+1 ]-&gt;computeExpectations();
     }
-
-    reconstruction_connections[ index ]-&gt;fprop( expectations[ index + 1],
-                                                reconstruction_activations);
-    layers[ index ]-&gt;fprop( reconstruction_activations,
-                            layers[ index ]-&gt;expectation);
+    reconstruction_targets &lt;&lt; layers[ index ]-&gt;getExpectations();
     
-    layers[ index ]-&gt;expectation_is_up_to_date = true;
-    train_costs[index] = layers[ index ]-&gt;fpropNLL(expectations[index]);
+    connections[ index ]-&gt;setAsDownInputs( layers[ index ]-&gt;getExpectations() );
+    reconstruction_layer-&gt;getAllActivations( rbm_connections[ index ], 0, true );
+    reconstruction_layer-&gt;computeExpectations();
+    
+    reconstruction_layer-&gt;fpropNLL( layers[ index ]-&gt;getExpectations(), 
+                                    reconstruction_costs);
+    train_costs[index] = sum( reconstruction_costs )/batch_size;
 
-    layers[ index ]-&gt;bpropNLL(expectations[index], train_costs[index],
-                                  reconstruction_activation_gradients);
+    reconstruction_layer-&gt;bpropNLL( 
+        layers[ index ]-&gt;getExpectations(), reconstruction_costs,
+        reconstruction_activation_gradients );
 
-    layers[ index ]-&gt;update(reconstruction_activation_gradients);
+    columnMean( reconstruction_activation_gradients, 
+                reconstruction_activation_gradient );
+    reconstruction_layer-&gt;update( reconstruction_activation_gradient );
 
-    // // This is a bad update! Propagates gradient through sigmoid again!
-    // layers[ index ]-&gt;bpropUpdate( reconstruction_activations, 
-    //                                   layers[ index ]-&gt;expectation,
-    //                                   reconstruction_activation_gradients,
-    //                                   reconstruction_expectation_gradients);
-
-    reconstruction_connections[ index ]-&gt;bpropUpdate( 
-        expectations[ index + 1], 
-        reconstruction_activations, 
-        reconstruction_expectation_gradients, //reused
-        reconstruction_activation_gradients);
-
-    if(!fast_exact_is_equal(l1_neuron_decay,0))
-    {
-        // Compute L1 penalty gradient on neurons
-        real* hid = expectations[ index + 1 ].data();
-        real* grad = reconstruction_expectation_gradients.data();
-        int len = expectations[ index + 1 ].length();
-        for(int i=0; i&lt;len; i++)
-        {
-            if(*hid &gt; l1_neuron_decay_center)
-                *grad -= l1_neuron_decay;
-            else if(*hid &lt; l1_neuron_decay_center)
-                *grad += l1_neuron_decay;
-            hid++;
-            grad++;
-        }
-    }
-
-    // Update hidden layer bias and weights
-    layers[ index+1 ]-&gt;bpropUpdate( activations[ index + 1 ],
-                                    expectations[ index + 1 ],
-                                    reconstruction_activation_gradients, // reused
-                                    reconstruction_expectation_gradients);    
-
     connections[ index ]-&gt;bpropUpdate( 
-        expectations[ index ],
-        activations[ index + 1 ],
-        reconstruction_expectation_gradients, //reused
+        layers[ index ]-&gt;getExpectations(), 
+        layers[ index ]-&gt;activations, 
+        reconstruction_input_gradients, 
         reconstruction_activation_gradients);
 
-    // Set diagonal to zero!!!
+    // Set diagonal to zero
+    for(int i=0; i&lt;connections[ index ]-&gt;up_size; i++)
+        connections[ index ]-&gt;weights(i,i) = 0;
 }
 
-void StackedSVDNet::fineTuningStep( const Vec&amp; input, const Vec&amp; target,
+void StackedSVDNet::fineTuningStep( const Mat&amp; inputs, const Mat&amp; targets,
                                     Vec&amp; train_costs )
 {
     // fprop
-    expectations[0] &lt;&lt; input;
-    for( int i=0 ; i&lt;n_layers-1; i++ )
+    layers[ 0 ]-&gt;setExpectations( inputs );
+    
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
-        connections[i]-&gt;fprop( expectations[i], activations[i+1] );
-        layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+        connections[ i ]-&gt;setAsDownInputs( layers[i]-&gt;getExpectations() );
+        layers[ i+1 ]-&gt;getAllActivations( rbm_connections[i], 0, true );
+        layers[ i+1 ]-&gt;computeExpectations();
     }
 
-    final_module-&gt;fprop( expectations[ n_layers-1 ],
-                         final_cost_input );
-    final_cost-&gt;fprop( final_cost_input, target, final_cost_value );
+    final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;getExpectations(),
+                         final_cost_inputs );
+    final_cost-&gt;fprop( final_cost_inputs, targets, final_cost_values );
 
-    train_costs.subVec(train_costs.length()-final_cost_value.length(),
-                       final_cost_value.length()) &lt;&lt;
+    columnMean( final_cost_values, 
+                final_cost_value );
+    train_costs.subVec(train_costs.length()-final_cost_value.length()) &lt;&lt; 
         final_cost_value;
 
-    final_cost-&gt;bpropUpdate( final_cost_input, target,
-                             final_cost_value[0],
-                             final_cost_gradient );
-    final_module-&gt;bpropUpdate( expectations[ n_layers-1 ],
-                               final_cost_input,
+    final_cost-&gt;bpropUpdate( final_cost_inputs, targets,
+                             final_cost_values,
+                             final_cost_gradients );
+    final_module-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;getExpectations(),
+                               final_cost_inputs,
                                expectation_gradients[ n_layers-1 ],
-                               final_cost_gradient );
+                               final_cost_gradients );
 
     for( int i=n_layers-1 ; i&gt;0 ; i-- )
     {
-        layers[i]-&gt;bpropUpdate( activations[i],
-                                expectations[i],
-                                activation_gradients[i],
-                                expectation_gradients[i] );
+        layers[ i ]-&gt;bpropUpdate( layers[ i ]-&gt;activations,
+                                  layers[ i ]-&gt;getExpectations(),
+                                  activation_gradients[ i ],
+                                  expectation_gradients[ i ] );
 
-        connections[i-1]-&gt;bpropUpdate( expectations[i-1],
-                                       activations[i],
-                                       expectation_gradients[i-1],
-                                       activation_gradients[i] );
+        connections[ i-1 ]-&gt;bpropUpdate( layers[ i-1 ]-&gt;getExpectations(),
+                                         layers[ i ]-&gt;activations,
+                                         expectation_gradients[ i-1 ],
+                                         activation_gradients[ i ] );
     }
 }
 
 void StackedSVDNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
 {
     // fprop
-
-    expectations[0] &lt;&lt; input;
-
-    for(int i=0 ; i&lt;currently_trained_layer-1 ; i++ )
+    layers[ 0 ]-&gt;expectation &lt;&lt;  input ;
+    layers[ 0 ]-&gt;expectation_is_up_to_date = true;
+    
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
-        connections[i]-&gt;fprop( expectations[i], activations[i+1] );
-        layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+        connections[ i ]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+        layers[ i+1 ]-&gt;getAllActivations( rbm_connections[i], 0, true );
+        layers[ i+1 ]-&gt;computeExpectation();
     }
 
-    if( currently_trained_layer&lt;n_layers )
-    {
-        connections[currently_trained_layer-1]-&gt;fprop( 
-            expectations[currently_trained_layer-1], 
-            activations[currently_trained_layer] );
-        layers[currently_trained_layer]-&gt;fprop(
-            activations[currently_trained_layer],
-            output);
-    }
-    else        
-        final_module-&gt;fprop( expectations[ currently_trained_layer - 1],
-                             output );
+    final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
+                         output );
 }
 
 void StackedSVDNet::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
@@ -563,60 +553,11 @@
 
     costs.resize( getTestCostNames().length() );
     costs.fill( MISSING_VALUE );
-
-    if(compute_all_test_costs)
-    {
-        for(int i=0; i&lt;currently_trained_layer-1; i++)
-        {
-            reconstruction_connections[ i ]-&gt;fprop( expectations[ i+1 ],
-                                                    reconstruction_activations);
-            layers[ i ]-&gt;fprop( reconstruction_activations,
-                                    layers[ i ]-&gt;expectation);
-            
-            layers[ i ]-&gt;expectation_is_up_to_date = true;
-            costs[i] = layers[ i ]-&gt;fpropNLL(expectations[ i ]);
-            
-            if( partial_costs &amp;&amp; partial_costs[i])
-            {
-                partial_costs[ i ]-&gt;fprop( expectations[ i + 1],
-                                           target, partial_cost_value );
-                costs.subVec(partial_costs_positions[i],
-                             partial_cost_value.length()) &lt;&lt; 
-                    partial_cost_value;
-            }
-        }
-    }
-
-    if( currently_trained_layer&lt;n_layers )
-    {
-        reconstruction_connections[ currently_trained_layer-1 ]-&gt;fprop( 
-            output,
-            reconstruction_activations);
-        layers[ currently_trained_layer-1 ]-&gt;fprop( 
-            reconstruction_activations,
-            layers[ currently_trained_layer-1 ]-&gt;expectation);
-        
-        layers[ currently_trained_layer-1 ]-&gt;expectation_is_up_to_date = true;
-        costs[ currently_trained_layer-1 ] = 
-            layers[ currently_trained_layer-1 ]-&gt;fpropNLL(
-                expectations[ currently_trained_layer-1 ]);
-
-        if( partial_costs &amp;&amp; partial_costs[ currently_trained_layer-1 ] )
-        {
-            partial_costs[ currently_trained_layer-1 ]-&gt;fprop( 
-                output,
-                target, partial_cost_value );
-            costs.subVec(partial_costs_positions[currently_trained_layer-1],
-                         partial_cost_value.length()) &lt;&lt; partial_cost_value;
-        }
-    }
-    else
-    {
-        final_cost-&gt;fprop( output, target, final_cost_value );        
-        costs.subVec(costs.length()-final_cost_value.length(),
-                     final_cost_value.length()) &lt;&lt;
-            final_cost_value;
-    }
+    
+    final_cost-&gt;fprop( output, target, final_cost_value );
+    costs.subVec(costs.length()-final_cost_value.length(),
+                 final_cost_value.length()) &lt;&lt;
+        final_cost_value;
 }
 
 TVec&lt;string&gt; StackedSVDNet::getTestCostNames() const
@@ -630,14 +571,6 @@
     for( int i=0; i&lt;layers.size()-1; i++)
         cost_names.push_back(&quot;reconstruction_error_&quot; + tostring(i+1));
     
-    for( int i=0 ; i&lt;partial_costs.size() ; i++ )
-    {
-        TVec&lt;string&gt; cost_names = partial_costs[i]-&gt;name();
-        for(int j=0; j&lt;cost_names.length(); j++)
-            cost_names.push_back(&quot;partial_cost_&quot; + tostring(i+1) + &quot;_&quot; + 
-                cost_names[j]);
-    }
-
     cost_names.append( final_cost-&gt;name() );
 
     return cost_names;

Modified: trunk/plearn_learners_experimental/StackedSVDNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.h	2007-07-10 15:27:48 UTC (rev 7739)
+++ trunk/plearn_learners_experimental/StackedSVDNet.h	2007-07-10 16:40:25 UTC (rev 7740)
@@ -1,6 +1,6 @@
 // -*- C++ -*-
 
-// StackedAutoassociatorsNet.h
+// StackedSVDNet.h
 //
 // Copyright (C) 2007 Hugo Larochelle
 //
@@ -32,16 +32,17 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Pascal Lamblin
+// Authors: Hugo Larochelle
 
-/*! \file StackedAutoassociatorsNet.h */
+/*! \file StackedSVDNet.h */
 
 
-#ifndef StackedAutoassociatorsNet_INC
-#define StackedAutoassociatorsNet_INC
+#ifndef StackedSVDNet_INC
+#define StackedSVDNet_INC
 
 #include &lt;plearn_learners/generic/PLearner.h&gt;
 #include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn_learners/online/CostModule.h&gt;
 #include &lt;plearn_learners/online/RBMLayer.h&gt;
 #include &lt;plearn_learners/online/RBMConnection.h&gt;
 #include &lt;plearn_learners/online/RBMMatrixConnection.h&gt;
@@ -52,7 +53,7 @@
 /**
  * Neural net, initialized with SVDs of logistic auto-regressions.
  */
-class StackedAutoassociatorsNet : public PLearner
+class StackedSVDNet : public PLearner
 {
     typedef PLearner inherited;
 
@@ -100,6 +101,10 @@
     //! The weights of the connections between the layers
     TVec&lt; PP&lt;RBMMatrixConnection&gt; &gt; connections;
 
+    //! View of connections as RBMConnection pointers (for compatibility
+    //! with RBM function calls)
+    TVec&lt; PP&lt;RBMConnection&gt; &gt; rbm_connections;
+
     //! Number of layers
     int n_layers;
 
@@ -107,7 +112,7 @@
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
-    StackedAutoassociatorsNet();
+    StackedSVDNet();
 
 
     //#####  PLearner Member Functions  #######################################
@@ -142,10 +147,10 @@
     virtual TVec&lt;std::string&gt; getTrainCostNames() const;
 
 
-    void greedyStep( const Vec&amp; input, const Vec&amp; target, int index, 
+    void greedyStep( const Mat&amp; inputs, const Mat&amp; targets, int index, 
                      Vec train_costs );
 
-    void fineTuningStep( const Vec&amp; input, const Vec&amp; target,
+    void fineTuningStep( const Mat&amp; inputs, const Mat&amp; targets,
                          Vec&amp; train_costs );
 
     //#####  PLearn::Object Protocol  #########################################
@@ -153,7 +158,7 @@
     // Declares other standard object methods.
     // ### If your class is not instantiatable (it has pure virtual methods)
     // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
-    PLEARN_DECLARE_OBJECT(StackedAutoassociatorsNet);
+    PLEARN_DECLARE_OBJECT(StackedSVDNet);
 
     // Simply calls inherited::build() then build_()
     virtual void build();
@@ -165,14 +170,6 @@
 protected:
     //#####  Not Options  #####################################################
 
-    //! Stores the activations of the input and hidden layers
-    //! (at the input of the layers)
-    mutable TVec&lt;Mat&gt; activations;
-
-    //! Stores the expectations of the input and hidden layers
-    //! (at the output of the layers)
-    mutable TVec&lt;Mat&gt; expectations;
-
     //! Stores the gradient of the cost wrt the activations of 
     //! the input and hidden layers
     //! (at the input of the layers)
@@ -183,37 +180,36 @@
     //! (at the output of the layers)
     mutable TVec&lt;Mat&gt; expectation_gradients;
 
-    //! Reconstruction activations
-    mutable Mat reconstruction_activations;
+    //! Reconstruction layer
+    mutable PP&lt;RBMLayer&gt; reconstruction_layer;
     
-    //! Reconstruction expectations
-    mutable Mat reconstruction_expectations;
-    
-    //! Reconstruction activations
+    //! Reconstruction target
+    mutable Mat reconstruction_targets;
+
+    //! Reconstruction costs
+    mutable Mat reconstruction_costs;
+
+    //! Reconstruction activation gradient
+    mutable Vec reconstruction_activation_gradient;
+
+    //! Reconstruction activation gradients
     mutable Mat reconstruction_activation_gradients;
     
-    //! Reconstruction expectations
-    mutable Mat reconstruction_expectation_gradients;
+    //! Reconstruction activations
+    mutable Mat reconstruction_input_gradients;
 
-    //! Input of the final_cost
-    mutable Vec final_cost_input;
+    //! Inputs of the final_cost
+    mutable Mat final_cost_inputs;
 
     //! Cost value of final_cost
     mutable Vec final_cost_value;
 
-    //! Stores the gradient of the cost at the input of final_cost
-    mutable Vec final_cost_gradient;
+    //! Cost values of final_cost
+    mutable Mat final_cost_values;
 
-    //! Currently trained layer (1 means the first hidden layer,
-    //! n_layers means the output layer)
-    int currently_trained_layer;
+    //! Stores the gradients of the cost at the inputs of final_cost
+    mutable Mat final_cost_gradients;
 
-    //! Indication whether final_module has learning rate
-    bool final_module_has_learning_rate;
-    
-    //! Indication whether final_cost has learning rate
-    bool final_cost_has_learning_rate;
-    
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -241,7 +237,7 @@
 };
 
 // Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(StackedAutoassociatorsNet);
+DECLARE_OBJECT_PTR(StackedSVDNet);
 
 } // end of namespace PLearn
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001187.html">[Plearn-commits] r7739 - trunk
</A></li>
	<LI>Next message: <A HREF="001189.html">[Plearn-commits] r7741 - trunk/plearn/io
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1188">[ date ]</a>
              <a href="thread.html#1188">[ thread ]</a>
              <a href="subject.html#1188">[ subject ]</a>
              <a href="author.html#1188">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
