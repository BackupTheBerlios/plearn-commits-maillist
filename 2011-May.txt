From plearner at mail.berlios.de  Thu May 26 20:41:49 2011
From: plearner at mail.berlios.de (plearner at mail.berlios.de)
Date: Thu, 26 May 2011 20:41:49 +0200
Subject: [Plearn-commits] r10366 - in trunk: plearn/var/EXPERIMENTAL
	plearn_learners/distributions python_modules/plearn/plotting
Message-ID: <20110526184149.9EEC8481357@sheep.berlios.de>

Author: plearner
Date: 2011-05-26 20:41:49 +0200 (Thu, 26 May 2011)
New Revision: 10366

Added:
   trunk/plearn/var/EXPERIMENTAL/AdditiveGaussianNoiseVariable.cc
   trunk/plearn/var/EXPERIMENTAL/AdditiveGaussianNoiseVariable.h
Modified:
   trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.cc
   trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.h
   trunk/plearn_learners/distributions/GaussMix.cc
   trunk/python_modules/plearn/plotting/netplot.py
Log:
experimental softsoftmax

Added: trunk/plearn/var/EXPERIMENTAL/AdditiveGaussianNoiseVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/AdditiveGaussianNoiseVariable.cc	                        (rev 0)
+++ trunk/plearn/var/EXPERIMENTAL/AdditiveGaussianNoiseVariable.cc	2011-05-26 18:41:49 UTC (rev 10366)
@@ -0,0 +1,179 @@
+// -*- C++ -*-
+
+// AdditiveGaussianNoiseVariable.cc
+//
+// Copyright (C) 2010 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file AdditiveGaussianNoiseVariable.cc */
+
+
+#include "AdditiveGaussianNoiseVariable.h"
+
+namespace PLearn {
+using namespace std;
+
+/** AdditiveGaussianNoiseVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    AdditiveGaussianNoiseVariable,
+    "Adds an isotropic Gaussian noise of given standard deviation",
+    ""
+    );
+
+AdditiveGaussianNoiseVariable::AdditiveGaussianNoiseVariable()
+    :sigma(1)
+{
+    // ### You may (or not) want to call build_() to finish building the object
+    // ### (doing so assumes the parent classes' build_() have been called too
+    // ### in the parent classes' constructors, something that you must ensure)
+}
+
+// constructor from input variable and parameters
+// AdditiveGaussianNoiseVariable::AdditiveGaussianNoiseVariable(Variable* input, param_type the_parameter,...)
+// ### replace with actual parameters
+//  : inherited(input, this_variable_length, this_variable_width),
+//    parameter(the_parameter),
+//    ...
+//{
+//    // ### You may (or not) want to call build_() to finish building the
+//    // ### object
+//}
+
+void AdditiveGaussianNoiseVariable::recomputeSize(int& l, int& w) const
+{
+    if (input) 
+    {
+        l = input.length();
+        w = input.width(); // the computed width
+    } 
+    else
+        l = w = 0;
+}
+
+// ### computes value from input's value
+void AdditiveGaussianNoiseVariable::fprop()
+{
+    checkContiguity();
+
+    if(random_gen.isNull())
+        random_gen = PRandom::common(false);
+
+    int n = value.length();
+    for(int i=0; i<n; i++)
+    {
+        valuedata[i] = random_gen->gaussian_mu_sigma(input->valuedata[i], sigma);
+    }
+}
+
+// ### computes input's gradient from gradient
+void AdditiveGaussianNoiseVariable::bprop()
+{
+}
+
+// ### You can implement these methods:
+// void AdditiveGaussianNoiseVariable::bbprop() {}
+// void AdditiveGaussianNoiseVariable::symbolicBprop() {}
+// void AdditiveGaussianNoiseVariable::rfprop() {}
+
+
+// ### Nothing to add here, simply calls build_
+void AdditiveGaussianNoiseVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+void AdditiveGaussianNoiseVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    deepCopyField(random_gen, copies);
+
+    // ### If you want to deepCopy a Var field:
+    // varDeepCopyField(somevariable, copies);
+}
+
+void AdditiveGaussianNoiseVariable::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "sigma", &AdditiveGaussianNoiseVariable::sigma,
+                  OptionBase::buildoption,
+                  "The standard deviation of the isotropic Gaussian noise to be added to the input");
+    declareOption(ol, "random_gen", &AdditiveGaussianNoiseVariable::random_gen,
+                  OptionBase::buildoption,
+                  "Random number generator. If null, the PRandom::common(false) generator will be used.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void AdditiveGaussianNoiseVariable::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/AdditiveGaussianNoiseVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/AdditiveGaussianNoiseVariable.h	                        (rev 0)
+++ trunk/plearn/var/EXPERIMENTAL/AdditiveGaussianNoiseVariable.h	2011-05-26 18:41:49 UTC (rev 10366)
@@ -0,0 +1,163 @@
+// -*- C++ -*-
+
+// AdditiveGaussianNoiseVariable.h
+//
+// Copyright (C) 2010 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file AdditiveGaussianNoiseVariable.h */
+
+
+#ifndef AdditiveGaussianNoiseVariable_INC
+#define AdditiveGaussianNoiseVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+#include <plearn/math/PRandom.h>
+
+namespace PLearn {
+using namespace std;
+
+/*! * AdditiveGaussianNoiseVariable * */
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class AdditiveGaussianNoiseVariable : public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    PP<PRandom> random_gen;
+    real sigma;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor, usually does nothing
+    AdditiveGaussianNoiseVariable();
+
+    // ### If your class has parameters, you probably want a constructor that
+    // ### initializes them
+    // AdditiveGaussianNoiseVariable(Variable* input, param_type the_parameter, ...);
+
+    // Your other public member functions go here
+
+    //#####  PLearn::Variable methods #########################################
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+
+    // ### These ones are not always implemented
+    // virtual void bbprop();
+    // virtual void symbolicBprop();
+    // virtual void rfprop();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(AdditiveGaussianNoiseVariable);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+    
+    // will contain true where features have been corrupted 
+    TVec<bool> corrupted;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(AdditiveGaussianNoiseVariable);
+
+// ### Put here a convenient method for building your variable.
+// ### e.g., if your class is TotoVariable, with two parameters foo_type foo
+// ### and bar_type bar, you could write:
+// inline Var toto(Var v, foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v, foo, bar); }
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.cc	2011-04-08 22:26:19 UTC (rev 10365)
+++ trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.cc	2011-05-26 18:41:49 UTC (rev 10366)
@@ -51,6 +51,188 @@
     );
 
 
+/*
+All matrices must be contiguous space storage.
+X is a (n,d) matrix
+U is a (d,d) matrix
+out is a (n,d) matrix
+
+Beware: You must ensure that U_kk = 0 prior to calling these functions, as they assume this is true
+
+*/
+
+// Provided are two versions: a twopass version and asinglepass version. 
+// The twopass version does a first pass to find the max (no transcendental involved), and a second pass where it calls a single transcendental (exp), 
+// The singlepass version repeatedly calls scalar logadd which means two transcendentals exp and log (possibly yields a numerically more accurate result).
+// I don't know which is faster.
+
+
+#define SOFTSOFTMAX_SAFELOG safelog
+#define SOFTSOFTMAX_EXP exp
+#define SOFTSOFTMAX_SAFEEXP safeexp
+#define SOFTSOFTMAX_LOGADD(a,b) ( ((a)>(b)) ? (a)+log1p(exp((b)-(a))) : (b)+log1p(exp((a)-(b))) )
+// #define SOFTSOFTMAX_LOGADD(a,b) ( ((a)>(b)) ? (a)+softplus((b)-(a)) : (b)+softplus((a)-(b)) )
+// #define SOFTSOFTMAX_LOGADD(a,b) logadd(a,b)
+
+
+// Singlepass version does a stable logadd computation by repeatedly calling scalar logadd (as in normal reduction)
+void softsoftmax_fprop_singlepass_version(int n, int d, 
+                                          const real* __restrict__ const X, 
+                                          const real* __restrict__ const U, 
+                                          real* __restrict__ const H)
+{
+  int Hpos = 0;
+  int xistart = 0;
+  for(int i=0; i<n; i++, xistart+=d)
+    {
+
+      int upos  = 0;
+      for(int j=0; j<d; j++)
+        {
+          real Xij = X[xistart+j];
+
+          real res = X[xistart] + U[upos++] - Xij;
+          for(int xpos=xistart+1; xpos<xistart+d; xpos++, upos++)
+            {
+              real newelem = X[xpos] + U[upos] - Xij;
+              res = SOFTSOFTMAX_LOGADD(res,newelem);
+            }
+
+          H[Hpos++] = SOFTSOFTMAX_SAFEEXP(-res);
+        }
+    }
+}
+
+// Twopass version does a stable logadd computation by first finding the max
+void softsoftmax_fprop_twopass_version(int n, int d, 
+                                       const real* __restrict__ const X, 
+                                       const real* __restrict__ const U, 
+                                       real* __restrict__ const H)
+{
+  int Hpos = 0;
+  int xistart = 0;
+  for(int i=0; i<n; i++, xistart+=d)
+    {
+      int uposstart  = 0;
+      for(int j=0; j<d; j++, uposstart+=d)
+        {
+          real maxelem = X[xistart] + U[uposstart];
+          for(int xpos=xistart+1, upos=uposstart+1; xpos<xistart+d; xpos++, upos++)
+            {
+              real elem = X[xpos] + U[upos];
+              if(elem>maxelem)
+                maxelem = elem;
+            }
+          real res = 0;
+          for(int xpos=xistart, upos=uposstart; xpos<xistart+d; xpos++, upos++)
+            res += SOFTSOFTMAX_EXP(X[xpos] + U[upos] - maxelem);
+          res = maxelem + SOFTSOFTMAX_SAFELOG(res) - X[xistart+j];
+
+          H[Hpos++] = SOFTSOFTMAX_SAFEEXP(-res);
+        }
+    }
+}
+
+// Twopass version does a stable logadd computation by first finding the max
+void softsoftmax_with_log_twopass_version(int n, int d, 
+                                          const real* __restrict__ const X, 
+                                          const real* __restrict__ const U, 
+                                          real* __restrict__ const logH,
+                                          real* __restrict__ const H)
+{
+  int Hpos = 0;
+  int xistart = 0;
+  for(int i=0; i<n; i++, xistart+=d)
+    {
+      int uposstart  = 0;
+      for(int j=0; j<d; j++, uposstart+=d)
+        {
+          real maxelem = X[xistart] + U[uposstart];
+          for(int xpos=xistart+1, upos=uposstart+1; xpos<xistart+d; xpos++, upos++)
+            {
+              real elem = X[xpos] + U[upos];
+              if(elem>maxelem)
+                maxelem = elem;
+            }
+          real res = 0;
+          for(int xpos=xistart, upos=uposstart; xpos<xistart+d; xpos++, upos++)
+            res += SOFTSOFTMAX_EXP(X[xpos] + U[upos] - maxelem);
+          res = -(maxelem + SOFTSOFTMAX_SAFELOG(res) - X[xistart+j]);
+
+          logH[Hpos] = res;
+          H[Hpos] = SOFTSOFTMAX_SAFEEXP(res);
+          Hpos++;
+        }
+    }
+}
+
+
+// Hardapprox version uses only the max of the denominator terms
+void softsoftmax_fprop_hardapprox_version(int n, int d, 
+                                          const real* __restrict__ const X, 
+                                          const real* __restrict__ const U, 
+                                          real* __restrict__ const H)
+{
+  int Hpos = 0;
+  int xistart = 0;
+  for(int i=0; i<n; i++, xistart+=d)
+    {
+      int uposstart  = 0;
+      for(int j=0; j<d; j++, uposstart+=d)
+        {
+          real maxelem = X[xistart] + U[uposstart];
+          for(int xpos=xistart+1, upos=uposstart+1; xpos<xistart+d; xpos++, upos++)
+            {
+              real elem = X[xpos] + U[upos];
+              if(elem>maxelem)
+                maxelem = elem;
+            }
+          H[Hpos++] = SOFTSOFTMAX_SAFEEXP(X[xistart+j]-maxelem);
+        }
+    }
+}
+
+
+void softsoftmax_bprop(int n, int d, 
+                       const real* __restrict__ const X, 
+                       const real* __restrict__ const U, 
+                       const real* __restrict__ const logH,
+                       const real* __restrict__ const H_gr,
+                       real* __restrict__ const X_gr,
+                       real* __restrict__ const U_gr)
+{
+    // Beware: must be passed logH and H_gr, where H_gr is the gradient on H, not on logH. 
+
+    // note: X, logH, H_gr, X_gr  all have the same shape (n,d) 
+    // Offset positions will be the same for these matrices, so we wont prefix the variable holding offset positions for these.
+    // However, variable indicating offset positions kj in U and U_gr (which are (d,d) matrices) will be called Ukj_pos.
+
+
+    for(int i=0, row_i_pos=0; i<n; i++, row_i_pos+=d)
+    {
+        for(int j=0; j<d; j++)
+        {          
+            int ij = row_i_pos+j; // ij index offset
+            real l_ij = logH[ij];
+            real h_ij = SOFTSOFTMAX_SAFEEXP(l_ij);
+            real sumk = 0;
+            for(int k=0, Ukj_pos=j; k<d; k++, Ukj_pos+=d)
+            {
+                // Ukj_pos = k*d+j;
+                int ik = row_i_pos+k; // ik index offset
+                real l_ik = logH[ik];
+                real val_k = -H_gr[ik]*SOFTSOFTMAX_SAFEEXP(l_ij+l_ik+U[Ukj_pos]);
+                if(k!=j)
+                    U_gr[Ukj_pos] += val_k;
+                sumk += val_k;
+            }
+            X_gr[ij] += H_gr[ij]*h_ij + sumk; 
+        }
+    }
+}
+
+
+
 // constructor from input variables.
 SoftSoftMaxVariable::SoftSoftMaxVariable(Variable* input1, Variable* input2)
     : inherited(input1, input2, input1->length(), input1->width())
@@ -73,24 +255,52 @@
 // ### computes value from input1 and input2 values
 void SoftSoftMaxVariable::fprop()
 {
-    Mat X = input1->matValue,
-        A = input2->matValue;
+    if(input1->matValue.isNotContiguous() || input2->matValue.isNotContiguous())
+        PLERROR("SoftSoftMaxVariable input matrices must be contiguous.");
 
-    real sum;
+    int n = input1->matValue.length();
+    int d = input1->matValue.width();
+    
+    if(input2->matValue.length()!=d || input2->matValue.width()!=d)
+        PLERROR("SoftSoftMaxVariable second input matriuix (U) must be a square matrix of width and length matching the width of first input matrix");
+        
+    // make sure U's diagonal is 0
+    Mat Umat = input2->matValue;
+    for(int i=0; i<d; i++)
+        Umat(i,i) = 0;
 
-    for (int n=0; n<X.length(); n++)    
-        for (int k=0; k<X.width(); k++)
-        {
-            sum=0;
-            for (int i=0; i<X.width(); i++)
-                sum += safeexp(X(n,i) + A(k,i));
-            matValue(n,k) = safeexp(X(n,k))/sum;
-        }    
+    const real* const X = input1->matValue.data();
+    const real* const U = input2->matValue.data();
+    real* const H = matValue.data();
+    logH_mat.resize(n,d);
+    real* const logH = logH_mat.data();
+    
+    softsoftmax_with_log_twopass_version(n, d, X, U, logH, H);
+    // perr << "Twopass version: " << endl << matValue << endl;
+    // softsoftmax_fprop_singlepass_version(n, d, X, U, H);
+    // perr << "Singlepass version: " << endl << matValue << endl;
+    // perr << "--------------------------------------" << endl;
 }
 
 // ### computes input1 and input2 gradients from gradient
 void SoftSoftMaxVariable::bprop()
 {
+    int n = input1->matValue.length();
+    int d = input1->matValue.width();
+    const real* const X = input1->matValue.data();
+    const real* const U = input2->matValue.data();
+    // const real* const H = matValue.data();
+    // For numerical reasons we use logH that has been computed during fprop and stored, rather than H that is in output->matValue. 
+    const real* const logH = logH_mat.data(); 
+    
+    const real* const H_gr = matGradient.data();
+    real* const X_gr = input1->matGradient.data();
+    real* const U_gr = input2->matGradient.data();
+
+    softsoftmax_bprop(n, d, X, U, logH, H_gr,  
+                      X_gr, U_gr);
+
+    /*
     Mat X = input1->matValue,
         A = input2->matValue,
         grad_X = input1->matGradient,
@@ -114,6 +324,7 @@
                                                                                             
                 grad_A(j,k) -= temp;
             }
+    */
 }
 
 // ### You can implement these methods:
@@ -178,9 +389,6 @@
     // ###    options have been modified.
     // ### You should assume that the parent class' build_() has already been
     // ### called.
-
-    if (input2.length() != input1.width() || input2.width() != input1.width())
-        PLERROR("Length and width of input2 must be equal to width of input1 in SoftSoftMaxVariable");
 }
 
 

Modified: trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.h	2011-04-08 22:26:19 UTC (rev 10365)
+++ trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.h	2011-05-26 18:41:49 UTC (rev 10366)
@@ -126,6 +126,7 @@
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
+    Mat logH_mat; // storage for keeping log of hidden
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/distributions/GaussMix.cc
===================================================================
--- trunk/plearn_learners/distributions/GaussMix.cc	2011-04-08 22:26:19 UTC (rev 10365)
+++ trunk/plearn_learners/distributions/GaussMix.cc	2011-05-26 18:41:49 UTC (rev 10366)
@@ -2724,8 +2724,8 @@
     eigenvalues.resize(0, 0);
     error_covariance.resize(0);
     initial_weights.resize(nsamples);
-    posteriors.resize(nsamples, L);
-    updated_weights.resize(L, nsamples);
+    //posteriors.resize(nsamples, L);
+    //updated_weights.resize(L, nsamples);
     stage_replaced.resize(L);
     stage_replaced.fill(-1);
 

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2011-04-08 22:26:19 UTC (rev 10365)
+++ trunk/python_modules/plearn/plotting/netplot.py	2011-05-26 18:41:49 UTC (rev 10366)
@@ -206,6 +206,8 @@
         if transpose_img:
             img = transpose(img)        
         imshow(img, interpolation="nearest", cmap = colormap, vmin = vmin, vmax = vmax)
+        if show_colorbar and vmin is None:
+            colorbar()
             
         # if show_colorbar and vmin is None:
         #    colorbar()



From plearner at mail.berlios.de  Thu May 26 23:50:11 2011
From: plearner at mail.berlios.de (plearner at mail.berlios.de)
Date: Thu, 26 May 2011 23:50:11 +0200
Subject: [Plearn-commits] r10367 - trunk/plearn/var/EXPERIMENTAL
Message-ID: <20110526215011.E8276481357@sheep.berlios.de>

Author: plearner
Date: 2011-05-26 23:50:11 +0200 (Thu, 26 May 2011)
New Revision: 10367

Modified:
   trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.cc
Log:
experimental softsoftmax

Modified: trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.cc	2011-05-26 18:41:49 UTC (rev 10366)
+++ trunk/plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.cc	2011-05-26 21:50:11 UTC (rev 10367)
@@ -213,19 +213,18 @@
         for(int j=0; j<d; j++)
         {          
             int ij = row_i_pos+j; // ij index offset
-            real l_ij = logH[ij];
-            real h_ij = SOFTSOFTMAX_SAFEEXP(l_ij);
             real sumk = 0;
             for(int k=0, Ukj_pos=j; k<d; k++, Ukj_pos+=d)
             {
                 // Ukj_pos = k*d+j;
                 int ik = row_i_pos+k; // ik index offset
                 real l_ik = logH[ik];
-                real val_k = -H_gr[ik]*SOFTSOFTMAX_SAFEEXP(l_ij+l_ik+U[Ukj_pos]);
+                real val_k = -H_gr[ik]*SOFTSOFTMAX_SAFEEXP(U[Ukj_pos] + l_ik+l_ik - X[ik] + X[ij]);
                 if(k!=j)
                     U_gr[Ukj_pos] += val_k;
                 sumk += val_k;
             }
+            real h_ij = SOFTSOFTMAX_SAFEEXP(logH[ij]);
             X_gr[ij] += H_gr[ij]*h_ij + sumk; 
         }
     }



