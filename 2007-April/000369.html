<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6920 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-April/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6920%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200704181720.l3IHKepR030454%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000368.html">
   <LINK REL="Next"  HREF="000370.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6920 - trunk/plearn_learners/online</H1>
    <B>tihocan at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6920%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200704181720.l3IHKepR030454%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6920 - trunk/plearn_learners/online">tihocan at mail.berlios.de
       </A><BR>
    <I>Wed Apr 18 19:20:40 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000368.html">[Plearn-commits] r6919 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000370.html">[Plearn-commits] r6921 - trunk
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#369">[ date ]</a>
              <a href="thread.html#369">[ thread ]</a>
              <a href="subject.html#369">[ subject ]</a>
              <a href="author.html#369">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: tihocan
Date: 2007-04-18 19:20:39 +0200 (Wed, 18 Apr 2007)
New Revision: 6920

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Starting work to implement a mini-batch version (not done yet)

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-18 17:10:20 UTC (rev 6919)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-04-18 17:20:39 UTC (rev 6920)
@@ -51,9 +51,13 @@
     &quot;This version supports different unit types, different connection types,\n&quot;
     &quot;and different cost functions, including the NLL in classification.\n&quot;);
 
+///////////////////
+// DeepBeliefNet //
+///////////////////
 DeepBeliefNet::DeepBeliefNet() :
     cd_learning_rate( 0. ),
     grad_learning_rate( 0. ),
+    batch_size(1),
     grad_decrease_ct( 0. ),
     // grad_weight_decay( 0. ),
     n_classes(-1),
@@ -61,16 +65,19 @@
     reconstruct_layerwise( false ),
     n_layers( 0 ),
     online ( false ),
+    minibatch_size(0),
     final_module_has_learning_rate( false ),
     final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
     class_cost_index( -1 ),
     recons_cost_index( -1 )
-
 {
     random_gen = new PRandom( seed_ );
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void DeepBeliefNet::declareOptions(OptionList&amp; ol)
 {
     declareOption(ol, &quot;cd_learning_rate&quot;, &amp;DeepBeliefNet::cd_learning_rate,
@@ -83,6 +90,10 @@
                   &quot;The decrease constant of the learning rate used during&quot;
                   &quot; gradient descent&quot;);
 
+    declareOption(ol, &quot;batch_size&quot;, &amp;DeepBeliefNet::batch_size,
+                  OptionBase::buildoption,
+        &quot;Training batch size (1=stochastic learning, 0=full batch learning).&quot;);
+
     /* NOT IMPLEMENTED YET
     declareOption(ol, &quot;grad_weight_decay&quot;, &amp;DeepBeliefNet::grad_weight_decay,
                   OptionBase::buildoption,
@@ -205,6 +216,10 @@
                   OptionBase::learntoption,
                   &quot;Number of layers&quot;);
 
+    declareOption(ol, &quot;minibatch_size&quot;, &amp;DeepBeliefNet::minibatch_size,
+                  OptionBase::learntoption,
+                  &quot;Size of a mini-batch.&quot;);
+
     /*
     declareOption(ol, &quot;n_final_costs&quot;, &amp;DeepBeliefNet::n_final_costs,
                   OptionBase::learntoption,
@@ -221,18 +236,12 @@
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void DeepBeliefNet::build_()
 {
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
-    // ###    all serialised options.
-    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
-    // ###    options have been modified.
-    // ### You should assume that the parent class' build_() has already been
-    // ### called.
+    PLASSERT( batch_size &gt;= 0 );
 
     MODULE_LOG &lt;&lt; &quot;build_() called&quot; &lt;&lt; endl;
 
@@ -266,6 +275,9 @@
     // build_costs(); /* ? */
 }
 
+//////////////////////////////////
+// build_layers_and_connections //
+//////////////////////////////////
 void DeepBeliefNet::build_layers_and_connections()
 {
     MODULE_LOG &lt;&lt; &quot;build_layers_and_connections() called&quot; &lt;&lt; endl;
@@ -279,7 +291,9 @@
         PLASSERT( layers[0]-&gt;size == inputsize() );
 
     activation_gradients.resize( n_layers );
+    activations_gradients.resize( n_layers );
     expectation_gradients.resize( n_layers );
+    expectations_gradients.resize( n_layers );
 
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
@@ -310,10 +324,17 @@
     expectation_gradients[n_layers-1].resize(last_layer_size);
 }
 
+///////////////////////////////
+// build_classification_cost //
+///////////////////////////////
 void DeepBeliefNet::build_classification_cost()
 {
     MODULE_LOG &lt;&lt; &quot;build_classification_cost() called&quot; &lt;&lt; endl;
 
+    PLASSERT_MSG(batch_size == 1, &quot;DeepBeliefNet::build_classification_cost - &quot;
+            &quot;This method has not been verified yet for minibatch &quot;
+            &quot;compatibility&quot;);
+
     PP&lt;RBMMatrixConnection&gt; last_to_target = new RBMMatrixConnection();
     last_to_target-&gt;up_size = layers[n_layers-1]-&gt;size;
     last_to_target-&gt;down_size = n_classes;
@@ -350,6 +371,9 @@
     joint_layer-&gt;build();
 }
 
+//////////////////////
+// build_final_cost //
+//////////////////////
 void DeepBeliefNet::build_final_cost()
 {
     MODULE_LOG &lt;&lt; &quot;build_final_cost() called&quot; &lt;&lt; endl;
@@ -425,6 +449,8 @@
 /////////////////////////////////
 void DeepBeliefNet::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
 {
+    // TODO Add missing fields.
+
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(training_schedule,        copies);
@@ -446,6 +472,7 @@
     deepCopyField(class_gradient,           copies);
     deepCopyField(class_input_gradient,     copies);
     deepCopyField(final_cost_gradient,      copies);
+    deepCopyField(final_cost_gradients,     copies);
     deepCopyField(save_layer_activation,    copies);
     deepCopyField(save_layer_expectation,   copies);
     deepCopyField(pos_down_values,          copies);
@@ -472,18 +499,11 @@
     return out_size;
 }
 
+////////////
+// forget //
+////////////
 void DeepBeliefNet::forget()
 {
-    //! (Re-)initialize the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!)
-    /*!
-      A typical forget() method should do the following:
-      - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
-      - initialize the learner's parameters, using this random generator
-      - stage = 0
-    */
     inherited::forget();
 
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
@@ -505,55 +525,48 @@
         for( int i=0 ; i&lt;n_layers-1 ; i++ )
             if( partial_costs[i] )
                 partial_costs[i]-&gt;forget();
-
-    stage = 0;
 }
 
+///////////
+// train //
+///////////
 void DeepBeliefNet::train()
 {
     MODULE_LOG &lt;&lt; &quot;train() called &quot; &lt;&lt; endl;
     MODULE_LOG &lt;&lt; &quot;  training_schedule = &quot; &lt;&lt; training_schedule &lt;&lt; endl;
     MODULE_LOG &lt;&lt; &quot;stage = &quot; &lt;&lt; stage &lt;&lt; &quot;, target nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
 
-    // The role of the train method is to bring the learner up to
-    // stage==nstages, updating train_stats with training costs measured
-    // on-line in the process.
+    PLASSERT( train_set );
+    if (stage == 0) {
+        // Training set-dependent initialization.
+        minibatch_size = batch_size &gt; 0 ? batch_size : train_set-&gt;length();
+        for (int i = 0 ; i &lt; n_layers; i++) {
+            activations_gradients[i].resize(minibatch_size, layers[i]-&gt;size);
+            expectation_gradients[i].resize(minibatch_size, layers[i]-&gt;size);
+        }
+        if (final_cost)
+            final_cost_gradients.resize(minibatch_size, final_cost-&gt;input_size);
+    }
 
-    /* TYPICAL CODE:
+    layers[n_layers-1]-&gt;random_gen = random_gen;
+    int last_layer_size = layers[n_layers-1]-&gt;size;
+    PLASSERT_MSG(last_layer_size &gt;= 0,
+                 &quot;Size of last layer must be non-negative&quot;);
+    activation_gradients[n_layers-1].resize(last_layer_size);
+    expectation_gradients[n_layers-1].resize(last_layer_size);
 
-    static Vec input;  // static so we don't reallocate memory each time...
-    static Vec target; // (but be careful that static means shared!)
-    input.resize(inputsize());    // the train_set's inputsize()
-    target.resize(targetsize());  // the train_set's targetsize()
-    real weight;
-
-    // This generic PLearner method does a number of standard stuff useful for
-    // (almost) any learner, and return 'false' if no training should take
-    // place. See PLearner.h for more details.
-    if (!initTrain())
-        return;
-
-    while(stage&lt;nstages)
-    {
-        // clear statistics of previous epoch
-        train_stats-&gt;forget();
-
-        //... train for 1 stage, and update train_stats,
-        // using train_set-&gt;getExample(input, target, weight)
-        // and train_stats-&gt;update(train_costs)
-
-        ++stage;
-        train_stats-&gt;finalize(); // finalize statistics for this epoch
-    }
-    */
-
     Vec input( inputsize() );
     Vec target( targetsize() );
     real weight; // unused
+    Mat inputs(minibatch_size, inputsize());
+    Mat targets(minibatch_size, targetsize());
+    Vec weights;
 
     TVec&lt;string&gt; train_cost_names = getTrainCostNames() ;
     Vec train_costs( train_cost_names.length() );
+    Mat train_costs_m(minibatch_size, train_cost_names.length());
     train_costs.fill(MISSING_VALUE) ;
+    train_costs_m.fill(MISSING_VALUE);
 
     //give the indexes the right values
 
@@ -603,6 +616,9 @@
     if (online)
         // train all layers simultaneously AND fine-tuning as well!
     {
+        PLASSERT_MSG(batch_size == 1, &quot;Online mode not implemented for &quot;
+                &quot;mini-batch learning yet&quot;);
+
         if( report_progress &amp;&amp; stage &lt; nstages )
             pb = new ProgressBar( &quot;Training &quot;+classname(),
                                   nstages - stage );
@@ -645,9 +661,15 @@
 
             for( ; stage&lt;end_stage ; stage++ )
             {
-                int sample = stage % nsamples;
-                train_set-&gt;getExample(sample, input, target, weight);
-                greedyStep( input, target, i );
+                int sample_start = (stage * minibatch_size) % nsamples;
+                if (batch_size &gt; 1) {
+                    train_set-&gt;getExamples(sample_start, minibatch_size,
+                            inputs, targets, weights);
+                    greedyStep( inputs, targets, i );
+                } else {
+                    train_set-&gt;getExample(sample_start, input, target, weight);
+                    greedyStep( input, target, i );
+                }
 
                 if( pb )
                     if( i == 0 )
@@ -660,6 +682,9 @@
         // possible supervised part
         if( use_classification_cost )
         {
+            PLASSERT_MSG(batch_size == 1, &quot;'use_classification_cost' code not &quot;
+                    &quot;verified with mini-batch learning yet&quot;);
+
             MODULE_LOG &lt;&lt; &quot;Training the classification module&quot; &lt;&lt; endl;
 
             int end_stage = min( training_schedule[n_layers-2], nstages );
@@ -692,10 +717,12 @@
         }
 
         /**** compute reconstruction error*****/
-        RBMLayer * down_layer = get_pointer(layers[0]) ;
-        RBMLayer * up_layer =  get_pointer(layers[1]) ;
-        RBMConnection * parameters = get_pointer(connections[0]);
+        PP&lt;RBMLayer&gt; down_layer = get_pointer(layers[0]) ;
+        PP&lt;RBMLayer&gt; up_layer =  get_pointer(layers[1]) ;
+        PP&lt;RBMConnection&gt; parameters = get_pointer(connections[0]);
 
+        // TODO Do we really want to systematically compute this reconstruction
+        // error?
         for(int train_index = 0 ; train_index &lt; nsamples ; train_index++)
         {
 
@@ -743,19 +770,30 @@
 
         setLearningRate( grad_learning_rate );
 
-        int begin_sample = stage % nsamples;
+        train_stats-&gt;forget();
+        bool update_stats = false;
         for( ; stage&lt;nstages ; stage++ )
         {
-            int sample = stage % nsamples;
-            if( sample == begin_sample )
-                train_stats-&gt;forget();
+            int sample_start = (stage * minibatch_size) % nsamples;
+            // Only update train statistics for the last 'epoch', i.e. last
+            // 'nsamples' seen.
+            update_stats = update_stats ||
+                stage &gt;= (nstages - nsamples / minibatch_size);
+
             if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
                 setLearningRate( grad_learning_rate
                                  / (1. + grad_decrease_ct * (stage - init_stage) ) );
 
-            train_set-&gt;getExample( sample, input, target, weight );
-            fineTuningStep( input, target, train_costs );
-            train_stats-&gt;update( train_costs );
+            if (minibatch_size &gt; 1) {
+                train_set-&gt;getExamples(sample_start, minibatch_size, inputs,
+                        targets, weights);
+                fineTuningStep(inputs, targets, train_costs);
+            } else {
+                train_set-&gt;getExample( sample_start, input, target, weight );
+                fineTuningStep( input, target, train_costs );
+            }
+            if (update_stats)
+                train_stats-&gt;update( train_costs );
 
             if( pb )
                 pb-&gt;update( stage - init_stage + 1 );
@@ -768,8 +806,13 @@
     train_stats-&gt;update( train_costs ) ;
 
     train_stats-&gt;finalize();
+
+    // TODO Conversion to mini-batch: CONTINUE HERE
 }
 
+////////////////
+// onlineStep //
+////////////////
 void DeepBeliefNet::onlineStep( const Vec&amp; input, const Vec&amp; target,
                                 Vec&amp; train_costs)
 {
@@ -831,29 +874,57 @@
     {
         if( final_module )
         {
-            final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
-                                 final_cost_input );
-            final_cost-&gt;fprop( final_cost_input, target,
-                               final_cost_value );
-            final_cost-&gt;bpropUpdate( final_cost_input, target,
-                                     final_cost_value[0],
-                                     final_cost_gradient );
+            /*
+            if (minibatch_size &gt; 1) {
+                final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;expectations,
+                        final_cost_inputs );
+                final_cost-&gt;fprop( final_cost_inputs, targets,
+                        final_cost_values );
+                final_cost-&gt;bpropUpdate(final_cost_inputs, targets,
+                        final_cost_values.column(0),
+                        final_cost_gradients );
 
-            final_module-&gt;bpropUpdate(
-                                      layers[ n_layers-1 ]-&gt;expectation,
-                                      final_cost_input,
-                                      expectation_gradients[ n_layers-1 ],
-                                      final_cost_gradient, true );
+                final_module-&gt;bpropUpdate(
+                        layers[ n_layers-1 ]-&gt;expectations,
+                        final_cost_inputs,
+                        expectations_gradients[ n_layers-1 ],
+                        final_cost_gradients, true );
+            } else {*/
+                final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
+                        final_cost_input );
+                final_cost-&gt;fprop( final_cost_input, target,
+                        final_cost_value );
+                final_cost-&gt;bpropUpdate( final_cost_input, target,
+                        final_cost_value[0],
+                        final_cost_gradient );
+
+                final_module-&gt;bpropUpdate(
+                        layers[ n_layers-1 ]-&gt;expectation,
+                        final_cost_input,
+                        expectation_gradients[ n_layers-1 ],
+                        final_cost_gradient, true );
+            //}
         }
         else
         {
-            final_cost-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
-                               target,
-                               final_cost_value );
-            final_cost-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;expectation,
-                                     target, final_cost_value[0],
-                                     expectation_gradients[n_layers-1],
-                                     true);
+            /*
+            if (minibatch_size &gt; 1) {
+                final_cost-&gt;fprop( layers[ n_layers-1 ]-&gt;expectations,
+                        targets,
+                        final_cost_values );
+                final_cost-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;expectations,
+                        targets, final_cost_values.column(0),
+                        expectations_gradients[n_layers-1],
+                        true);
+            } else {*/
+                final_cost-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
+                        target,
+                        final_cost_value );
+                final_cost-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;expectation,
+                        target, final_cost_value[0],
+                        expectation_gradients[n_layers-1],
+                        true);
+            //}
         }
 
         for (int j=0;j&lt;final_cost_indices.length();j++)

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-18 17:10:20 UTC (rev 6919)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-04-18 17:20:39 UTC (rev 6920)
@@ -70,6 +70,8 @@
     //! The learning rate used during the gradient descent
     real grad_learning_rate;
 
+    int batch_size;
+
     //! The decrease constant of the learning rate used during gradient descent
     real grad_decrease_ct;
 
@@ -205,11 +207,22 @@
 
     void greedyStep( const Vec&amp; input, const Vec&amp; target, int index );
 
+    //! TODO Document and implement.
+    void greedyStep( const Mat&amp; inputs, const Mat&amp; targets, int index ) {
+        PLASSERT(false);
+    }
+
     void jointGreedyStep( const Vec&amp; input, const Vec&amp; target );
 
     void fineTuningStep( const Vec&amp; input, const Vec&amp; target,
                          Vec&amp; train_costs );
 
+    //! TODO Document and implement.
+    void fineTuningStep( const Mat&amp; inputs, const Mat&amp; targets,
+                         Vec&amp; train_costs ) {
+        PLASSERT(false);
+    }
+
     void contrastiveDivergenceStep( const PP&lt;RBMLayer&gt;&amp; down_layer,
                                     const PP&lt;RBMConnection&gt;&amp; connection,
                                     const PP&lt;RBMLayer&gt;&amp; up_layer,
@@ -247,23 +260,29 @@
     virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
 
 protected:
+
+    int minibatch_size;
+    
     //#####  Not Options  #####################################################
 
     //! Stores the gradient of the cost wrt the activations
     //! (at the input of the layers)
     mutable TVec&lt;Vec&gt; activation_gradients;
+    mutable TVec&lt;Mat&gt; activations_gradients; //!&lt; For mini-batch.
 
     //! Stores the gradient of the cost wrt the expectations
     //! (at the output of the layers)
     mutable TVec&lt;Vec&gt; expectation_gradients;
+    mutable TVec&lt;Mat&gt; expectations_gradients; //!&lt; For mini-batch.
 
-    //!
     mutable Vec final_cost_input;
+    mutable Mat final_cost_inputs; //!&lt; For mini-batch.
 
     mutable Vec final_cost_value;
+    mutable Vec final_cost_values; //!&lt; For mini-batch.
 
     mutable Vec final_cost_output;
-    //!
+
     mutable Vec class_output;
 
     mutable Vec class_gradient;
@@ -272,6 +291,7 @@
 
     //! Stores the gradient of the cost at the input of final_cost
     mutable Vec final_cost_gradient;
+    mutable Mat final_cost_gradients; //!&lt; For mini-batch.
 
     //! buffers bottom layer activation during onlineStep 
     mutable Vec save_layer_activation;


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000368.html">[Plearn-commits] r6919 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000370.html">[Plearn-commits] r6921 - trunk
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#369">[ date ]</a>
              <a href="thread.html#369">[ thread ]</a>
              <a href="subject.html#369">[ subject ]</a>
              <a href="author.html#369">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
