<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6856 - in trunk: commands plearn/ker
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-April/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6856%20-%20in%20trunk%3A%20commands%20plearn/ker&In-Reply-To=%3C200704091528.l39FSYro019832%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000304.html">
   <LINK REL="Next"  HREF="000306.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6856 - in trunk: commands plearn/ker</H1>
    <B>chapados at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6856%20-%20in%20trunk%3A%20commands%20plearn/ker&In-Reply-To=%3C200704091528.l39FSYro019832%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6856 - in trunk: commands plearn/ker">chapados at mail.berlios.de
       </A><BR>
    <I>Mon Apr  9 17:28:34 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000304.html">[Plearn-commits] r6855 - trunk/plearn/ker
</A></li>
        <LI>Next message: <A HREF="000306.html">[Plearn-commits] r6857 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#305">[ date ]</a>
              <a href="thread.html#305">[ thread ]</a>
              <a href="subject.html#305">[ subject ]</a>
              <a href="author.html#305">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: chapados
Date: 2007-04-09 17:28:33 +0200 (Mon, 09 Apr 2007)
New Revision: 6856

Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn/ker/SquaredExponentialARDKernel.cc
   trunk/plearn/ker/SquaredExponentialARDKernel.h
Log:
Updated SquaredExponential to work in the inverse softplus domain; efficient derivatives not implemented yet

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-04-09 15:27:08 UTC (rev 6855)
+++ trunk/commands/plearn_noblas_inc.h	2007-04-09 15:28:33 UTC (rev 6856)
@@ -111,7 +111,7 @@
 #include &lt;plearn/ker/NegOutputCostFunction.h&gt;
 #include &lt;plearn/ker/PolynomialKernel.h&gt;
 #include &lt;plearn/ker/RationalQuadraticARDKernel.h&gt;
-// #include &lt;plearn/ker/SquaredExponentialARDKernel.h&gt;
+#include &lt;plearn/ker/SquaredExponentialARDKernel.h&gt;
 #include &lt;plearn/ker/ThresholdedKernel.h&gt;
 #include &lt;plearn/ker/VMatKernel.h&gt;
 

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.cc
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-09 15:27:08 UTC (rev 6855)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.cc	2007-04-09 15:28:33 UTC (rev 6856)
@@ -2,7 +2,7 @@
 
 // SquaredExponentialARDKernel.cc
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -55,15 +55,16 @@
     &quot;Similar to C.E. Rasmussen's GPML code (see <A HREF="http://www.gaussianprocess.org">http://www.gaussianprocess.org</A>),\n&quot;
     &quot;this kernel function is specified as:\n&quot;
     &quot;\n&quot;
-    &quot;  k(x,y) = sf2 * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + delta_x,y*sn2\n&quot;
+    &quot;  k(x,y) = sf * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + k_iid(x,y)\n&quot;
     &quot;\n&quot;
-    &quot;where sf2 is the exp of the 'log_signal_sigma' option, sn2 is the exp of\n&quot;
-    &quot;the 'log_noise_sigma' option (added only if x==y), and w_i is\n&quot;
-    &quot;exp(log_global_sigma + log_input_sigma[i]).\n&quot;
+    &quot;where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +\n&quot;
+    &quot;isp_input_sigma[i]), and k_iid(x,y) is the result of the IIDNoiseKernel\n&quot;
+    &quot;kernel evaluation.\n&quot;
     &quot;\n&quot;
     &quot;Note that to make its operations more robust when used with unconstrained\n&quot;
     &quot;optimization of hyperparameters, all hyperparameters of this kernel are\n&quot;
-    &quot;specified in the log-domain.\n&quot;
+    &quot;specified in the inverse softplus domain.  See IIDNoiseKernel for more\n&quot;
+    &quot;explanations.\n&quot;
     );
 
 
@@ -102,92 +103,116 @@
 real SquaredExponentialARDKernel::evaluate(const Vec&amp; x1, const Vec&amp; x2) const
 {
     PLASSERT( x1.size() == x2.size() );
-    PLASSERT( !m_log_input_sigma.size() || x1.size() == m_log_input_sigma.size() );
+    PLASSERT( !m_isp_input_sigma.size() || x1.size() == m_isp_input_sigma.size() );
 
     if (x1.size() == 0)
-        return exp(2*m_log_signal_sigma) + exp(2*m_log_noise_sigma);
+        return softplus(m_isp_signal_sigma) + inherited::evaluate(x1,x2);
     
     const real* px1 = x1.data();
     const real* px2 = x2.data();
-    real expval = 0.0;
-    real sum_sqdiff = 0.0;
+    real sf         = softplus(m_isp_signal_sigma);
+    real expval     = 0.0;
     
-    if (m_log_input_sigma.size() &gt; 0) {
-        const real* pinpsig = m_log_input_sigma.data();
+    if (m_isp_input_sigma.size() &gt; 0) {
+        const real* pinpsig = m_isp_input_sigma.data();
         for (int i=0, n=x1.size() ; i&lt;n ; ++i) {
             real diff   = *px1++ - *px2++;
             real sqdiff = diff * diff;
-            sum_sqdiff += sqdiff;
-            expval     += sqdiff / exp(2*(m_log_global_sigma + *pinpsig++));
+            expval     += sqdiff / softplus(m_isp_global_sigma + *pinpsig++);
         }
     }
     else {
-        real global_sigma = exp(2*m_log_global_sigma);
+        real global_sigma = softplus(m_isp_global_sigma);
         for (int i=0, n=x1.size() ; i&lt;n ; ++i) {
             real diff   = *px1++ - *px2++;
             real sqdiff = diff * diff;
-            sum_sqdiff += sqdiff;
             expval     += sqdiff / global_sigma;
         }
     }
 
-    // We add a noise variance only if x and y are equal (within machine tolerance)
-    real noise_cov = 0.0;
-    if (is_equal(sum_sqdiff, 0))
-        noise_cov = exp(2*m_log_noise_sigma);
-    return exp(2*m_log_signal_sigma + -0.5 * expval) + noise_cov;
+    // EXPERIMENTAL: Multiply noise_cov by kernel value if we have kronecker
+    // terms, otherwise disregard noise_cov
+    // real noise_cov = ( m_kronecker_indexes.size() &gt; 0?
+    //                    inherited::evaluate(x1,x2) : 1.0 );
+    // return sf * exp(-0.5 * expval) * noise_cov;
+
+    real noise_cov = inherited::evaluate(x1,x2);
+    return sf * exp(-0.5 * expval) + noise_cov;
 }
 
 
-//#####  computeGramMatrixDerivative  #########################################
+//#####  computeGramMatrix  ###################################################
 
-/**
-void SquaredExponentialARDKernel::computeGramMatrixDerivative(
-    Mat&amp; KD, const string&amp; kernel_param, real epsilon) const
+void SquaredExponentialARDKernel::computeGramMatrix(Mat K) const
 {
-    static const string LSV = &quot;log_signal_sigma&quot;;
-    static const string LNV = &quot;log_noise_sigma&quot;;
-    static const string LGS = &quot;log_global_sigma&quot;;
-    static const string LIS = &quot;log_input_sigma[&quot;;
+    PLASSERT( !m_isp_input_sigma.size() || dataInputsize() == m_isp_input_sigma.size() );
+    PLASSERT( K.size() == 0 || m_data_cache.size() &gt; 0 );  // Ensure data cached OK
 
-    const int W = nExamples();
-    KD.resize(W,W);
-    const int mod = KD.mod();
-    real* KDij;
-    real* KDji;
+    // Compute IID noise gram matrix
+    inherited::computeGramMatrix(K);
 
-    // log_signal_sigma
-    if (kernel_param == LSV) {
+    // Precompute some terms
+    real sf    = softplus(m_isp_signal_sigma);
+    m_input_sigma.resize(dataInputsize());
+    m_input_sigma.fill(m_isp_global_sigma);
+    if (m_isp_input_sigma.size() &gt; 0)
+        m_input_sigma += m_isp_input_sigma;
+    for (int i=0, n=m_input_sigma.size() ; i&lt;n ; ++i)
+        m_input_sigma[i] = softplus(m_input_sigma[i]);
 
-    }
+    // Compute Gram Matrix
+    int  l = data-&gt;length();
+    int  m = K.mod();
+    int  n = dataInputsize();
+    int  cache_mod = m_data_cache.mod();
 
-    // log_noise_sigma
-    else if (kernel_param == LNV) {
-        for (int i=0 ; i&lt;W ; ++i) {
-            KDij = KD[i];
-            KDji = &amp;KD[0][i];
-            for (int j=0 ; j&lt;i ; ++j, Kji += m) {
-                // Below main diagonal, we have to check if x_i == x_j
-                
+    real *data_start = &amp;m_data_cache(0,0);
+    real *Ki = K[0];                         // Start of current row
+    real *Kij;                               // Current element along row
+    real *input_sigma_data = m_input_sigma.data();
+    real *xi = data_start;
+    
+    for (int i=0 ; i&lt;l ; ++i, xi += cache_mod, Ki+=m)
+    {
+        Kij = Ki;
+        real *xj = data_start;
+
+        for (int j=0; j&lt;=i; ++j, xj += cache_mod) {
+            // Kernel evaluation per se
+            real *x1 = xi;
+            real *x2 = xj;
+            real *p_inpsigma = input_sigma_data;
+            real sum_wt = 0.0;
+            int  k = n;
+
+            // Use Duff's device to unroll the following loop:
+            //     while (k--) {
+            //         real diff = *x1++ - *x2++;
+            //         sum_wt += (diff * diff) / *p_inpsigma++;
+            //     }
+            real diff;
+            switch (k % 8) {
+            case 0: do { diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 7:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 6:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 5:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 4:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 3:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 2:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+            case 1:      diff = *x1++ - *x2++; sum_wt += (diff*diff) / *p_inpsigma++;
+                       } while((k -= 8) &gt; 0);
             }
+
+            // Update kernel matrix (already pre-filled with IID noise terms)
+            *Kij++ += sf * exp(-0.5 * sum_wt);
         }
-        
     }
-
-    // log_global_sigma
-    else if (kernel_param == LGS) {
-
+    if (cache_gram_matrix) {
+        gram_matrix.resize(l,l);
+        gram_matrix &lt;&lt; K;
+        gram_matrix_is_cached = true;
     }
-
-    // log_input_sigma
-    else if (kernel_param.substr(0,16) == LIS) {
-
-    }
-    else
-        PLERROR(&quot;SquaredExponentialARDKernel::computeGramMatrixDerivative: &quot;
-                &quot;unknown hyperparameter '%s'&quot;, kernel_param.c_str());
 }
-*/
 
 
 //#####  makeDeepCopyFromShallowCopy  #########################################

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.h
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-09 15:27:08 UTC (rev 6855)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.h	2007-04-09 15:28:33 UTC (rev 6856)
@@ -2,7 +2,7 @@
 
 // SquaredExponentialARDKernel.h
 //
-// Copyright (C) 2006 Nicolas Chapados
+// Copyright (C) 2006-2007 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -58,15 +58,16 @@
  *  Similar to C.E. Rasmussen's GPML code (see <A HREF="http://www.gaussianprocess.org">http://www.gaussianprocess.org</A>),
  *  this kernel function is specified as:
  *
- *    k(x,y) = sf2 * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + delta_x,y*sn2
+ *    k(x,y) = sf * exp(- 0.5 * (sum_i (x_i - y_i)^2 / w_i)) + k_iid(x,y)
  *
- *  where sf2 is the exp of the 'log_signal_sigma' option, sn2 is the exp of
- *  the 'log_noise_sigma' option (added only if x==y), and w_i is
- *  exp(log_global_sigma + log_input_sigma[i]).
+ *  where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +
+ *  isp_input_sigma[i]), and k_iid(x,y) is the result of the IIDNoiseKernel
+ *  kernel evaluation.
  *
  *  Note that to make its operations more robust when used with unconstrained
  *  optimization of hyperparameters, all hyperparameters of this kernel are
- *  specified in the log-domain.
+ *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
+ *  explanations.
  */
 class SquaredExponentialARDKernel : public ARDBaseKernel
 {
@@ -89,6 +90,9 @@
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec&amp; x1, const Vec&amp; x2) const;
 
+    //! Compute the Gram Matrix.
+    virtual void computeGramMatrix(Mat K) const;
+    
     //! Directly compute the derivative with respect to hyperparameters
     //! (Faster than finite differences...)
     // virtual void computeGramMatrixDerivative(Mat&amp; KD, const string&amp; kernel_param,


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000304.html">[Plearn-commits] r6855 - trunk/plearn/ker
</A></li>
	<LI>Next message: <A HREF="000306.html">[Plearn-commits] r6857 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#305">[ date ]</a>
              <a href="thread.html#305">[ thread ]</a>
              <a href="subject.html#305">[ subject ]</a>
              <a href="author.html#305">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
