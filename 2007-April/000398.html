<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6949 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-April/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6949%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200704272109.l3RL9sZ9006788%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000397.html">
   <LINK REL="Next"  HREF="000399.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6949 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6949%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200704272109.l3RL9sZ9006788%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6949 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Fri Apr 27 23:09:54 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000397.html">[Plearn-commits] r6948 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000399.html">[Plearn-commits] r6950 - trunk/commands
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#398">[ date ]</a>
              <a href="thread.html#398">[ thread ]</a>
              <a href="subject.html#398">[ subject ]</a>
              <a href="author.html#398">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-04-27 23:09:52 +0200 (Fri, 27 Apr 2007)
New Revision: 6949

Added:
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Code for Stacked Autoassociators neural network...


Added: trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2007-04-27 21:02:20 UTC (rev 6948)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2007-04-27 21:09:52 UTC (rev 6949)
@@ -0,0 +1,398 @@
+// -*- C++ -*-
+
+// RBMMatrixTransposeConnection.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMMatrixTransposeConnection.cc */
+
+#include &quot;RBMMatrixTransposeConnection.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMMatrixTransposeConnection,
+    &quot;RBMConnection which uses the tranpose of some other &quot;
+    &quot;RBMMatrixConnection's weights&quot;,
+    &quot;&quot;);
+
+RBMMatrixTransposeConnection::RBMMatrixTransposeConnection( 
+    PP&lt;RBMMatrixConnection&gt; the_rbm_matrix_connection,
+    real the_learning_rate ) :
+    inherited(the_learning_rate), 
+    rbm_matrix_connection(the_rbm_matrix_connection)
+{}
+
+void RBMMatrixTransposeConnection::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;rbm_matrix_connection&quot;, 
+                  &amp;RBMMatrixTransposeConnection::rbm_matrix_connection,
+                  OptionBase::buildoption,
+                  &quot;RBMMatrixConnection from which the weights are taken&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, &quot;up_size&quot;, &amp;RBMConnection::up_size,
+                    OptionBase::learntoption,
+                    &quot;Is set to rbm_matrix_connection-&gt;down_size.&quot;);
+    redeclareOption(ol, &quot;down_size&quot;, &amp;RBMConnection::down_size,
+                    OptionBase::learntoption,
+                    &quot;Is set to rbm_matrix_connection-&gt;up_size.&quot;);
+}
+
+void RBMMatrixTransposeConnection::build_()
+{
+    
+    if( !rbm_matrix_connection )
+        PLERROR(&quot;In RBMMatrixTransposeConnection::build_(): &quot;
+            &quot;rbm_matrix_connection needs to be provided.&quot;);
+
+
+    rbm_matrix_connection-&gt;build();
+    weights = rbm_matrix_connection-&gt;weights;
+    down_size = rbm_matrix_connection-&gt;up_size;
+    up_size = rbm_matrix_connection-&gt;down_size;
+
+    weights_pos_stats.resize( down_size, up_size );
+    weights_neg_stats.resize( down_size, up_size );
+
+    if( momentum != 0. )
+        weights_inc.resize( down_size, up_size );
+
+    clearStats();
+}
+
+void RBMMatrixTransposeConnection::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMMatrixTransposeConnection::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(weights, copies);
+    deepCopyField(rbm_matrix_connection, copies);
+    deepCopyField(weights_pos_stats, copies);
+    deepCopyField(weights_neg_stats, copies);
+    deepCopyField(weights_inc, copies);
+}
+
+void RBMMatrixTransposeConnection::accumulatePosStats( const Vec&amp; down_values,
+                                              const Vec&amp; up_values )
+{
+    // weights_pos_stats += down_values * up_values'
+    externalProductAcc( weights_pos_stats, down_values, up_values );
+
+    pos_count++;
+}
+
+void RBMMatrixTransposeConnection::accumulateNegStats( const Vec&amp; down_values,
+                                              const Vec&amp; up_values )
+{
+    // weights_neg_stats += down_values * up_values'
+    externalProductAcc( weights_neg_stats, down_values, up_values );
+
+    neg_count++;
+}
+
+void RBMMatrixTransposeConnection::update()
+{
+    // updates parameters
+    //weights -= learning_rate * (weights_pos_stats/pos_count
+    //                              - weights_neg_stats/neg_count)
+    real pos_factor = -learning_rate / pos_count;
+    real neg_factor = learning_rate / neg_count;
+
+    int l = weights.length();
+    int w = weights.width();
+
+    real* w_i = weights.data();
+    real* wps_i = weights_pos_stats.data();
+    real* wns_i = weights_neg_stats.data();
+    int w_mod = weights.mod();
+    int wps_mod = weights_pos_stats.mod();
+    int wns_mod = weights_neg_stats.mod();
+
+    if( momentum == 0. )
+    {
+        // no need to use weights_inc
+        for( int i=0 ; i&lt;l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
+            for( int j=0 ; j&lt;w ; j++ )
+                w_i[j] += pos_factor * wps_i[j] + neg_factor * wns_i[j];
+    }
+    else
+    {
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l, w );
+
+        // The update rule becomes:
+        // weights_inc = momentum * weights_inc
+        //               - learning_rate * (weights_pos_stats/pos_count
+        //                                  - weights_neg_stats/neg_count);
+        // weights += weights_inc;
+        real* winc_i = weights_inc.data();
+        int winc_mod = weights_inc.mod();
+        for( int i=0 ; i&lt;l ; i++, w_i += w_mod, wps_i += wps_mod,
+                             wns_i += wns_mod, winc_i += winc_mod )
+            for( int j=0 ; j&lt;w ; j++ )
+            {
+                winc_i[j] = momentum * winc_i[j]
+                    + pos_factor * wps_i[j] + neg_factor * wns_i[j];
+                w_i[j] += winc_i[j];
+            }
+    }
+
+    clearStats();
+}
+
+// Instead of using the statistics, we assume we have only one markov chain
+// runned and we update the parameters from the first 4 values of the chain
+void RBMMatrixTransposeConnection::update( const Vec&amp; pos_down_values, // v_0
+                                  const Vec&amp; pos_up_values,   // h_0
+                                  const Vec&amp; neg_down_values, // v_1
+                                  const Vec&amp; neg_up_values )  // h_1
+{
+    // weights -= learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // or:
+    // weights[i][j] += learning_rate * (h_1[i] v_1[j] - h_0[i] v_0[j]);
+
+    int l = weights.length();
+    int w = weights.width();
+    PLASSERT( pos_up_values.length() == l );
+    PLASSERT( neg_up_values.length() == l );
+    PLASSERT( pos_down_values.length() == w );
+    PLASSERT( neg_down_values.length() == w );
+
+    real* w_i = weights.data();
+    real* puv_i = pos_up_values.data();
+    real* nuv_i = neg_up_values.data();
+    real* pdv = pos_down_values.data();
+    real* ndv = neg_down_values.data();
+    int w_mod = weights.mod();
+
+    if( momentum == 0. )
+    {
+        for( int i=0 ; i&lt;l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
+            for( int j=0 ; j&lt;w ; j++ )
+                w_i[j] += learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+    }
+    else
+    {
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l, w );
+
+        // The update rule becomes:
+        // weights_inc = momentum * weights_inc
+        //               - learning_rate * ( h_0 v_0' - h_1 v_1' );
+        // weights += weights_inc;
+
+        real* winc_i = weights_inc.data();
+        int winc_mod = weights_inc.mod();
+        for( int i=0 ; i&lt;l ; i++, w_i += w_mod, winc_i += winc_mod,
+                             puv_i++, nuv_i++ )
+            for( int j=0 ; j&lt;w ; j++ )
+            {
+                winc_i[j] = momentum * winc_i[j]
+                    + learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                w_i[j] += winc_i[j];
+            }
+    }
+}
+
+void RBMMatrixTransposeConnection::clearStats()
+{
+    weights_pos_stats.clear();
+    weights_neg_stats.clear();
+
+    pos_count = 0;
+    neg_count = 0;
+}
+
+void RBMMatrixTransposeConnection::computeProduct( int start, int length,
+                                          const Vec&amp; activations,
+                                          bool accumulate ) const
+{
+    PLASSERT( activations.length() == length );
+    if( going_up )
+    {
+        PLASSERT( start+length &lt;= up_size );
+        // activations[i-start] += sum_j weights(i,j) input_vec[j]
+
+        if( accumulate )
+            transposeProductAcc( activations,
+                                 weights.subMatColumns(start,length),
+                                 input_vec );
+        else
+            transposeProduct( activations,
+                              weights.subMatColumns(start,length),
+                              input_vec );
+    }
+    else
+    {
+        PLASSERT( start+length &lt;= down_size );
+        // activations[i-start] += sum_j weights(j,i) input_vec[j]
+        if( accumulate )
+            productAcc( activations,
+                        weights.subMatRows(start,length),
+                        input_vec );
+        else
+            product( activations,
+                     weights.subMatRows(start,length),
+                     input_vec );
+    }
+}
+
+void RBMMatrixTransposeConnection::computeProducts(int start, int length,
+                                          Mat&amp; activations,
+                                          bool accumulate ) const
+{
+    activations.resize(inputs_mat.length(), length);
+    if( going_up )
+    {
+        PLASSERT( start+length &lt;= up_size );
+        // activations(k, i-start) += sum_j weights(i,j) inputs_mat(k, j)
+
+        if( accumulate )
+            productAcc(activations,
+                    inputs_mat,
+                    weights.subMatColumns(start,length));
+        else
+            product(activations,
+                    inputs_mat,
+                    weights.subMatColumns(start,length));
+    }
+    else
+    {
+        PLASSERT( start+length &lt;= down_size );
+        // activations(k, i-start) += sum_j weights(j,i) inputs_mat(k, j)
+        if( accumulate )
+            productTransposeAcc(activations,
+                    inputs_mat,
+                    weights.subMatRows(start,length) );
+        else
+            productTranspose(activations,
+                    inputs_mat,
+                    weights.subMatRows(start,length) );
+    }
+}
+
+//! this version allows to obtain the input gradient as well
+void RBMMatrixTransposeConnection::bpropUpdate(const Vec&amp; input, 
+                                               const Vec&amp; output,
+                                               Vec&amp; input_gradient,
+                                               const Vec&amp; output_gradient,
+                                               bool accumulate)
+{
+    PLASSERT( input.size() == down_size );
+    PLASSERT( output.size() == up_size );
+    PLASSERT( output_gradient.size() == up_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+
+        // input_gradient += weights' * output_gradient
+        productAcc( input_gradient, weights, output_gradient );
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+        
+        // input_gradient = weights' * output_gradient
+        product( input_gradient, weights, output_gradient );
+    }
+    
+    // weights -= learning_rate * output_gradient * input'
+    externalProductScaleAcc( weights, input, output_gradient, -learning_rate );
+}
+
+//! reset the parameters to the state they would be BEFORE starting training.
+//! Note that this method is necessarily called from build().
+void RBMMatrixTransposeConnection::forget()
+{
+    rbm_matrix_connection-&gt;forget();
+    clearStats();
+}
+
+
+/* THIS METHOD IS OPTIONAL
+//! reset the parameters to the state they would be BEFORE starting training.
+//! Note that this method is necessarily called from build().
+//! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT DO
+//! ANYTHING.
+void RBMMatrixTransposeConnection::finalize()
+{
+}
+*/
+
+//! return the number of parameters
+int RBMMatrixTransposeConnection::nParameters() const
+{
+    return weights.size();
+}
+
+//! Make the parameters data be sub-vectors of the given global_parameters.
+//! The argument should have size &gt;= nParameters. The result is a Vec
+//! that starts just after this object's parameters end, i.e.
+//!    result = global_parameters.subVec(nParameters(),global_parameters.size()-nParameters());
+//! This allows to easily chain calls of this method on multiple RBMParameters.
+Vec RBMMatrixTransposeConnection::makeParametersPointHere(const Vec&amp; global_parameters)
+{
+    Vec ret = rbm_matrix_connection-&gt;makeParametersPointHere(global_parameters);
+    weights = rbm_matrix_connection-&gt;weights;
+    return ret;
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2007-04-27 21:02:20 UTC (rev 6948)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2007-04-27 21:09:52 UTC (rev 6949)
@@ -0,0 +1,221 @@
+// -*- C++ -*-
+
+// RBMMatrixTransposeConnection.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMMatrixTransposeConnection.h */
+
+
+#ifndef RBMMatrixTransposeConnection_INC
+#define RBMMatrixTransposeConnection_INC
+
+#include &lt;plearn_learners/online/RBMConnection.h&gt;
+#include &lt;plearn_learners/online/RBMMatrixConnection.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+
+/**
+ * RBMConnection which uses the tranpose of some other RBMMatrixConnection's weights
+ */
+class RBMMatrixTransposeConnection: public RBMConnection
+{
+    typedef RBMConnection inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //#####  Learned Options  #################################################
+
+    //! Matrix containing unit-to-unit weights (output_size \times input_size)
+    Mat weights;
+
+    //! RBMMatrixConnection from which the weights are taken
+    PP&lt;RBMMatrixConnection&gt; rbm_matrix_connection;
+
+    //#####  Not Options  #####################################################
+
+    //! Accumulates positive contribution to the weights' gradient
+    Mat weights_pos_stats;
+
+    //! Accumulates negative contribution to the weights' gradient
+    Mat weights_neg_stats;
+
+    //! Used if momentum != 0.
+    Mat weights_inc;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMMatrixTransposeConnection( 
+        PP&lt;RBMMatrixConnection&gt; the_rbm_matrix_connection = 0,
+        real the_learning_rate=0 );
+
+    // Your other public member functions go here
+
+    //! Accumulates positive phase statistics to *_pos_stats
+    virtual void accumulatePosStats( const Vec&amp; down_values,
+                                     const Vec&amp; up_values );
+
+    virtual void accumulatePosStats( const Mat&amp; down_values,
+                                     const Mat&amp; up_values )
+    {
+        PLASSERT_MSG( false, &quot;Not implemented&quot; );
+    }
+
+    //! Accumulates negative phase statistics to *_neg_stats
+    virtual void accumulateNegStats( const Vec&amp; down_values,
+                                     const Vec&amp; up_values );
+
+    virtual void accumulateNegStats( const Mat&amp; down_values,
+                                     const Mat&amp; up_values )
+    {
+        PLASSERT_MSG( false, &quot;Not implemented&quot; );
+    }
+
+    //! Updates parameters according to contrastive divergence gradient
+    virtual void update();
+
+    //! Updates parameters according to contrastive divergence gradient,
+    //! not using the statistics but the explicit values passed
+    virtual void update( const Vec&amp; pos_down_values,
+                         const Vec&amp; pos_up_values,
+                         const Vec&amp; neg_down_values,
+                         const Vec&amp; neg_up_values );
+
+    //! Clear all information accumulated during stats
+    virtual void clearStats();
+
+    //! Computes the vectors of activation of &quot;length&quot; units,
+    //! starting from &quot;start&quot;, and stores (or add) them into &quot;activations&quot;.
+    //! &quot;start&quot; indexes an up unit if &quot;going_up&quot;, else a down unit.
+    virtual void computeProduct( int start, int length,
+                                 const Vec&amp; activations,
+                                 bool accumulate=false ) const;
+
+    //! Same as 'computeProduct' but for mini-batches.
+    virtual void computeProducts(int start, int length,
+                                 Mat&amp; activations,
+                                 bool accumulate=false ) const;
+
+    //! Adapt based on the output gradient: this method should only
+    //! be called just after a corresponding fprop; it should be
+    //! called with the same arguments as fprop for the first two arguments
+    //! (and output should not have been modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+    //! JUST CALLS
+    //!     bpropUpdate(input, output, input_gradient, output_gradient)
+    //! AND IGNORES INPUT GRADIENT.
+    // virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+    //                          const Vec&amp; output_gradient);
+
+    //! this version allows to obtain the input gradient as well
+    //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient,
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
+
+    //! reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+    //! optionally perform some processing after training, or after a
+    //! series of fprop/bpropUpdate calls to prepare the model for truly
+    //! out-of-sample operation.  THE DEFAULT IMPLEMENTATION PROVIDED IN
+    //! THE SUPER-CLASS DOES NOT DO ANYTHING.
+    // virtual void finalize();
+
+    //! return the number of parameters
+    virtual int nParameters() const;
+
+    //! Make the parameters data be sub-vectors of the given global_parameters.
+    //! The argument should have size &gt;= nParameters. The result is a Vec
+    //! that starts just after this object's parameters end, i.e.
+    //!    result = global_parameters.subVec(nParameters(),global_parameters.size()-nParameters());
+    //! This allows to easily chain calls of this method on multiple RBMParameters.
+    virtual Vec makeParametersPointHere(const Vec&amp; global_parameters);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMMatrixTransposeConnection);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Member Functions  ######################################
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMMatrixTransposeConnection);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-04-27 21:02:20 UTC (rev 6948)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-04-27 21:09:52 UTC (rev 6949)
@@ -0,0 +1,854 @@
+// -*- C++ -*-
+
+// StackedAutoassociatorsNet.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedAutoassociatorsNet.cc */
+
+
+#define PL_LOG_MODULE_NAME &quot;StackedAutoassociatorsNet&quot;
+#include &lt;plearn/io/pl_log.h&gt;
+
+#include &quot;StackedAutoassociatorsNet.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    StackedAutoassociatorsNet,
+    &quot;Neural net, trained layer-wise in a greedy fashion using autoassociators&quot;,
+    &quot;It is highly inspired by the DeepBeliefNet class, and can use the\n&quot;
+    &quot;same RBMLayer and RBMConnection components.\n&quot;
+    );
+
+StackedAutoassociatorsNet::StackedAutoassociatorsNet() :
+    greedy_learning_rate( 0. ),
+    greedy_decrease_ct( 0. ),
+    fine_tuning_learning_rate( 0. ),
+    fine_tuning_decrease_ct( 0. ),
+    l1_neuron_decay( 0. ),
+    compute_all_test_costs( false ),
+    n_layers( 0 ),
+    currently_trained_layer( 0 ),
+    final_module_has_learning_rate( false ),
+    final_cost_has_learning_rate( false )
+{
+    random_gen = new PRandom( seed_ );
+    nstages = 0;
+}
+
+void StackedAutoassociatorsNet::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;greedy_learning_rate&quot;, 
+                  &amp;StackedAutoassociatorsNet::greedy_learning_rate,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used during the autoassociator &quot;
+                  &quot;gradient descent training&quot;);
+
+    declareOption(ol, &quot;greedy_decrease_ct&quot;, 
+                  &amp;StackedAutoassociatorsNet::greedy_decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;The decrease constant of the learning rate used during &quot;
+                  &quot;the autoassociator\n&quot;
+                  &quot;gradient descent training. When a hidden layer has finished &quot;
+                  &quot;its training,\n&quot;
+                  &quot;the learning rate is reset to it's initial value.\n&quot;);
+
+    declareOption(ol, &quot;fine_tuning_learning_rate&quot;, 
+                  &amp;StackedAutoassociatorsNet::fine_tuning_learning_rate,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used during the fine tuning gradient descent&quot;);
+
+    declareOption(ol, &quot;fine_tuning_decrease_ct&quot;, 
+                  &amp;StackedAutoassociatorsNet::fine_tuning_decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;The decrease constant of the learning rate used during &quot;
+                  &quot;fine tuning\n&quot;
+                  &quot;gradient descent.\n&quot;);
+
+    declareOption(ol, &quot;l1_neuron_decay&quot;, 
+                  &amp;StackedAutoassociatorsNet::l1_neuron_decay,
+                  OptionBase::buildoption,
+                  &quot; L1 penalty weight on the hidden layers, to encourage &quot;
+                  &quot;sparsity during\n&quot;
+                  &quot;the greedy unsupervised phases.\n&quot;);
+
+    declareOption(ol, &quot;training_schedule&quot;, 
+                  &amp;StackedAutoassociatorsNet::training_schedule,
+                  OptionBase::buildoption,
+                  &quot;Number of examples to use during each phase of learning:\n&quot;
+                  &quot;first the greedy phases, and then the gradient descent.\n&quot;
+                  &quot;Unlike for DeepBeliefNet, these numbers should not be\n&quot;
+                  &quot;cumulative. They correspond to the number of seen training\n&quot;
+                  &quot;examples for each phase.\n&quot;
+        );
+
+    declareOption(ol, &quot;layers&quot;, &amp;StackedAutoassociatorsNet::layers,
+                  OptionBase::buildoption,
+                  &quot;The layers of units in the network. The first element\n&quot;
+                  &quot;of this vector should be the input layer and the\n&quot;
+                  &quot;subsequent elements should be the hidden layers. The\n&quot;
+                  &quot;should not be included in this layer.\n&quot;);
+
+    declareOption(ol, &quot;connections&quot;, &amp;StackedAutoassociatorsNet::connections,
+                  OptionBase::buildoption,
+                  &quot;The weights of the connections between the layers&quot;);
+
+    declareOption(ol, &quot;reconstruction_connections&quot;, 
+                  &amp;StackedAutoassociatorsNet::reconstruction_connections,
+                  OptionBase::buildoption,
+                  &quot;The weights of the reconstruction connections between the &quot;
+                  &quot;layers&quot;);
+
+    declareOption(ol, &quot;final_module&quot;, &amp;StackedAutoassociatorsNet::final_module,
+                  OptionBase::buildoption,
+                  &quot;Module that takes as input the output of the last layer\n&quot;
+                  &quot;(layers[n_layers-1), and feeds its output to final_cost\n&quot;
+                  &quot;which defines the fine-tuning criteria.\n&quot;
+                 );
+
+    declareOption(ol, &quot;final_cost&quot;, &amp;StackedAutoassociatorsNet::final_cost,
+                  OptionBase::buildoption,
+                  &quot;The cost function to be applied on top of the neural network\n&quot;
+                  &quot;(i.e. at the output of final_module). Its gradients will be \n&quot;
+                  &quot;backpropagated to final_module and then backpropagated to\n&quot;
+                  &quot;the layers.\n&quot;
+                  );
+
+    declareOption(ol, &quot;partial_costs&quot;, &amp;StackedAutoassociatorsNet::partial_costs,
+                  OptionBase::buildoption,
+                  &quot;Corresponding additional supervised cost function to be &quot;
+                  &quot;applied on \n&quot;
+                  &quot;top of each hidden layer during the autoassociator &quot;
+                  &quot;training stages. \n&quot;
+                  &quot;The gradient for these costs are not backpropagated to &quot;
+                  &quot;previous layers.\n&quot;
+        );
+
+    declareOption(ol, &quot;partial_costs_weights&quot;, 
+                  &amp;StackedAutoassociatorsNet::partial_costs_weights,
+                  OptionBase::buildoption,
+                  &quot;Relative weights of the partial costs. If not defined,\n&quot;
+                  &quot;weights of 1 will be assumed for all partial costs.\n&quot;
+        );
+
+    declareOption(ol, &quot;compute_all_test_costs&quot;, 
+                  &amp;StackedAutoassociatorsNet::compute_all_test_costs,
+                  OptionBase::buildoption,
+                  &quot;Indication that, at test time, all costs for all layers \n&quot;
+                  &quot;(up to the currently trained layer) should be computed.\n&quot;
+        );
+
+    declareOption(ol, &quot;greedy_stages&quot;, 
+                  &amp;StackedAutoassociatorsNet::greedy_stages,
+                  OptionBase::learntoption,
+                  &quot;Number of training samples seen in the different greedy &quot;
+                  &quot;phases.\n&quot;
+        );
+
+    declareOption(ol, &quot;n_layers&quot;, &amp;StackedAutoassociatorsNet::n_layers,
+                  OptionBase::learntoption,
+                  &quot;Number of layers&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void StackedAutoassociatorsNet::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    MODULE_LOG &lt;&lt; &quot;build_() called&quot; &lt;&lt; endl;
+
+    if(train_set)
+    {
+        // Initialize some learnt variables
+        n_layers = layers.length();
+        
+        if( weightsize_ &gt; 0 )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_() - \n&quot;
+                    &quot;usage of weighted samples (weight size &gt; 0) is not\n&quot;
+                    &quot;implemented yet.\n&quot;);
+
+        if( training_schedule.length() != n_layers-1 )        
+            PLERROR(&quot;StackedAutoassociatorsNet::build_() - \n&quot;
+                    &quot;training_schedule should have %d elements.\n&quot;,
+                    n_layers-1);
+        
+        if( partial_costs &amp;&amp; partial_costs.length() != n_layers-1 )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_() - \n&quot;
+                    &quot;partial_costs should have %d elements.\n&quot;,
+                    n_layers-1);
+
+        if( partial_costs &amp;&amp; partial_costs_weights &amp;&amp;
+            partial_costs_weights.length() != n_layers-1 )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_() - \n&quot;
+                    &quot;partial_costs_weights should have %d elements.\n&quot;,
+                    n_layers-1);
+
+        if(greedy_stages.length() == 0)
+        {
+            greedy_stages.resize(n_layers-1);
+            greedy_stages.clear();
+        }
+
+        if(stage &gt; 0)
+            currently_trained_layer = n_layers;
+        else
+        {            
+            currently_trained_layer = n_layers-1;
+            while(currently_trained_layer&gt;1
+                  &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
+                currently_trained_layer--;
+        }
+
+        build_layers_and_connections();
+        build_costs();
+    }
+}
+
+void StackedAutoassociatorsNet::build_layers_and_connections()
+{
+    MODULE_LOG &lt;&lt; &quot;build_layers_and_connections() called&quot; &lt;&lt; endl;
+
+    if( connections.length() != n_layers-1 )
+        PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;there should be %d connections.\n&quot;,
+                n_layers-1);
+
+    if(layers[0]-&gt;size != inputsize_)
+        PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;layers[0] should have a size of %d.\n&quot;,
+                inputsize_);
+    
+    activations.resize( n_layers );
+    expectations.resize( n_layers );
+    activation_gradients.resize( n_layers );
+    expectation_gradients.resize( n_layers );
+
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        if( layers[i]-&gt;size != connections[i]-&gt;down_size )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() &quot;
+                    &quot;- \n&quot;
+                    &quot;connections[%i] should have a down_size of %d.\n&quot;,
+                    i, layers[i]-&gt;size);
+
+        if( connections[i]-&gt;up_size != layers[i+1]-&gt;size )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() &quot;
+                    &quot;- \n&quot;
+                    &quot;connections[%i] should have a up_size of %d.\n&quot;,
+                    i, layers[i+1]-&gt;size);
+
+        if( layers[i+1]-&gt;size != reconstruction_connections[i]-&gt;down_size )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() &quot;
+                    &quot;- \n&quot;
+                    &quot;recontruction_connections[%i] should have a down_size of &quot;
+                    &quot;%d.\n&quot;,
+                    i, layers[i+1]-&gt;size);
+
+        if( reconstruction_connections[i]-&gt;up_size != layers[i]-&gt;size )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() &quot;
+                    &quot;- \n&quot;
+                    &quot;recontruction_connections[%i] should have a up_size of &quot;
+                    &quot;%d.\n&quot;,
+                    i, layers[i]-&gt;size);
+
+        layers[i]-&gt;random_gen = random_gen;
+        layers[i]-&gt;build();
+
+        connections[i]-&gt;random_gen = random_gen;
+        connections[i]-&gt;build();
+
+        reconstruction_connections[i]-&gt;random_gen = random_gen;
+        reconstruction_connections[i]-&gt;build();
+
+        activations[i].resize( layers[i]-&gt;size );
+        expectations[i].resize( layers[i]-&gt;size );
+        activation_gradients[i].resize( layers[i]-&gt;size );
+        expectation_gradients[i].resize( layers[i]-&gt;size );
+    }
+    layers[n_layers-1]-&gt;random_gen = random_gen;
+    activations[n_layers-1].resize( layers[n_layers-1]-&gt;size );
+    expectations[n_layers-1].resize( layers[n_layers-1]-&gt;size );
+    activation_gradients[n_layers-1].resize( layers[n_layers-1]-&gt;size );
+    expectation_gradients[n_layers-1].resize( layers[n_layers-1]-&gt;size );
+}
+
+void StackedAutoassociatorsNet::build_costs()
+{
+    MODULE_LOG &lt;&lt; &quot;build_final_cost() called&quot; &lt;&lt; endl;
+
+    final_cost_gradient.resize( final_cost-&gt;input_size );
+    final_cost-&gt;setLearningRate( fine_tuning_learning_rate );
+
+    if( !final_module )
+        PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
+                &quot;final_module should be provided.\n&quot;);
+    
+    if( layers[n_layers-1]-&gt;size != final_module-&gt;input_size )
+        PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
+                &quot;final_module should have an input_size of %d.\n&quot;, 
+                layers[n_layers-1]-&gt;size);
+    
+    if( final_module-&gt;output_size != final_cost-&gt;input_size )
+        PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
+                &quot;final_module should have an output_size of %d.\n&quot;, 
+                final_module-&gt;input_size);
+
+    final_module-&gt;setLearningRate( fine_tuning_learning_rate );
+
+    if(targetsize_ != 1)
+        PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
+                &quot;target size of %d is not supported.\n&quot;, targetsize_);
+    
+    if(partial_costs)
+    {
+        partial_costs_positions.resize(partial_costs.length());
+        partial_costs_positions.clear();
+        for(int i=0; i&lt;partial_costs.length(); i++)
+        {
+            if(!partial_costs[i])
+                PLERROR(&quot;StackedAutoassociatorsNet::build_final_cost() - \n&quot;
+                        &quot;partial_costs[%i] should be provided.\n&quot;,i);
+            if( layers[i+1]-&gt;size != partial_costs[i]-&gt;input_size )
+                PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
+                        &quot;partial_costs[%i] should have an input_size of %d.\n&quot;, 
+                        i,layers[i+1]-&gt;size);
+            if(i==0)
+                partial_costs_positions[i] = n_layers-1;
+            else
+                partial_costs_positions[i] = partial_costs_positions[i-1]
+                    + partial_costs[i-1]-&gt;name().length();
+        }
+    }
+}
+
+// ### Nothing to add here, simply calls build_
+void StackedAutoassociatorsNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void StackedAutoassociatorsNet::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(, copies);
+
+    deepCopyField(training_schedule, copies);
+    deepCopyField(layers, copies);
+    deepCopyField(connections, copies);
+    deepCopyField(reconstruction_connections, copies);
+    deepCopyField(final_module, copies);
+    deepCopyField(final_cost, copies);
+    deepCopyField(partial_costs, copies);
+    deepCopyField(partial_costs_weights, copies);
+    deepCopyField(activations, copies);
+    deepCopyField(expectations, copies);
+    deepCopyField(activation_gradients, copies);
+    deepCopyField(expectation_gradients, copies);
+    deepCopyField(reconstruction_activations, copies);
+    deepCopyField(reconstruction_expectations, copies);
+    deepCopyField(reconstruction_activation_gradients, copies);
+    deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(partial_costs_positions, copies);
+    deepCopyField(partial_cost_value, copies);
+    deepCopyField(final_cost_input, copies);
+    deepCopyField(final_cost_value, copies);
+    deepCopyField(final_cost_gradient, copies);
+    deepCopyField(greedy_stages, copies);
+}
+
+
+int StackedAutoassociatorsNet::outputsize() const
+{
+    if(currently_trained_layer &lt; n_layers)
+        return layers[currently_trained_layer]-&gt;size;
+    return final_module-&gt;output_size;
+}
+
+void StackedAutoassociatorsNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        connections[i]-&gt;forget();
+        reconstruction_connections[i]-&gt;forget();
+    }
+    
+    final_module-&gt;forget();
+    final_cost-&gt;forget();
+
+    for( int i=0 ; i&lt;partial_costs.length() ; i++ )
+        if( partial_costs[i] )
+            partial_costs[i]-&gt;forget();
+
+    stage = 0;
+    greedy_stages.clear();
+}
+
+void StackedAutoassociatorsNet::train()
+{
+    MODULE_LOG &lt;&lt; &quot;train() called &quot; &lt;&lt; endl;
+    MODULE_LOG &lt;&lt; &quot;  training_schedule = &quot; &lt;&lt; training_schedule &lt;&lt; endl;
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    real weight; // unused
+
+    TVec&lt;string&gt; train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set-&gt;length();
+    int sample;
+
+    PP&lt;ProgressBar&gt; pb;
+
+    // clear stats of previous epoch
+    train_stats-&gt;forget();
+
+    real lr = 0;
+    int init_stage;
+
+    /***** initial greedy training *****/
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        MODULE_LOG &lt;&lt; &quot;Training connection weights between layers &quot; &lt;&lt; i
+            &lt;&lt; &quot; and &quot; &lt;&lt; i+1 &lt;&lt; endl;
+
+        int end_stage = training_schedule[i];
+        int* this_stage = greedy_stages.subVec(i,1).data();
+        init_stage = *this_stage;
+
+        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; *this_stage &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  greedy_learning_rate = &quot; &lt;&lt; greedy_learning_rate &lt;&lt; endl;
+
+        if( report_progress &amp;&amp; *this_stage &lt; end_stage )
+            pb = new ProgressBar( &quot;Training layer &quot;+tostring(i)
+                                  +&quot; of &quot;+classname(),
+                                  end_stage - init_stage );
+
+        train_costs.fill(MISSING_VALUE);
+        lr = greedy_learning_rate;
+        layers[i]-&gt;setLearningRate( lr );
+        connections[i]-&gt;setLearningRate( lr );
+        reconstruction_connections[i]-&gt;setLearningRate( lr );
+        layers[i+1]-&gt;setLearningRate( lr );
+
+        reconstruction_activations.resize(layers[i+1]-&gt;size);
+        reconstruction_expectations.resize(layers[i+1]-&gt;size);
+        reconstruction_activation_gradients.resize(layers[i+1]-&gt;size);
+        reconstruction_expectation_gradients.resize(layers[i+1]-&gt;size);
+
+        for( ; *this_stage&lt;end_stage ; (*this_stage)++ )
+        {
+            if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+            {
+                lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                           * (*this_stage)); 
+                layers[i]-&gt;setLearningRate( lr );
+                connections[i]-&gt;setLearningRate( lr );
+                reconstruction_connections[i]-&gt;setLearningRate( lr );
+                layers[i+1]-&gt;setLearningRate( lr );                
+            }
+
+            sample = *this_stage % nsamples;
+            train_set-&gt;getExample(sample, input, target, weight);
+            greedyStep( input, target, i, train_costs );
+            train_stats-&gt;update( train_costs );
+
+            if( pb )
+                pb-&gt;update( *this_stage - init_stage + 1 );
+        }
+    }
+
+    /***** fine-tuning by gradient descent *****/
+    if( stage &lt; nstages )
+    {
+
+        MODULE_LOG &lt;&lt; &quot;Fine-tuning all parameters, by gradient descent&quot; &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot; &lt;&lt; 
+            fine_tuning_learning_rate &lt;&lt; endl;
+
+        init_stage = stage;
+        if( report_progress &amp;&amp; stage &lt; nstages )
+            pb = new ProgressBar( &quot;Fine-tuning parameters of all layers of &quot;
+                                  + classname(),
+                                  nstages - init_stage );
+
+        setLearningRate( fine_tuning_learning_rate );
+        train_costs.fill(MISSING_VALUE);
+        for( ; stage&lt;nstages ; stage++ )
+        {
+            sample = stage % nsamples;
+            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                setLearningRate( fine_tuning_learning_rate
+                                 / (1. + fine_tuning_decrease_ct * stage ) );
+
+            train_set-&gt;getExample( sample, input, target, weight );
+            fineTuningStep( input, target, train_costs );
+            train_stats-&gt;update( train_costs );
+
+            if( pb )
+                pb-&gt;update( stage - init_stage + 1 );
+        }
+    }
+    
+    train_stats-&gt;finalize();
+        
+    // Update currently_trained_layer
+    if(stage &gt; 0)
+        currently_trained_layer = n_layers;
+    else
+    {            
+        currently_trained_layer = n_layers-1;
+        while(currently_trained_layer&gt;1 
+              &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
+            currently_trained_layer--;
+    }
+}
+
+void StackedAutoassociatorsNet::greedyStep( const Vec&amp; input, const Vec&amp; target, int index, Vec train_costs )
+{
+    PLASSERT( index &lt; n_layers );
+
+    expectations[0] &lt;&lt; input;
+    for( int i=0 ; i&lt;index + 1; i++ )
+    {
+        connections[i]-&gt;fprop( expectations[i], activations[i+1] );
+        layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+    }
+
+    if( partial_costs &amp;&amp; partial_costs[ index ] )
+    {
+        partial_costs[ index ]-&gt;fprop( expectations[ index + 1],
+                                       target, partial_cost_value );
+
+        // Update partial cost (might contain some weights for example)
+        partial_costs[ index ]-&gt;bpropUpdate( expectations[ index + 1 ],
+                                             target, partial_cost_value,
+                                             expectation_gradients[ index + 1 ]
+                                             );
+
+        train_costs.subVec(partial_costs_positions[index],
+                           partial_cost_value.length()) &lt;&lt; partial_cost_value;
+
+        if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
+            expectation_gradients[ index + 1 ] *= partial_costs_weights[index];
+
+        // Update hidden layer bias and weights
+        layers[ index+1 ]-&gt;bpropUpdate( activations[ index + 1 ],
+                                        expectations[ index + 1 ],
+                                        activation_gradients[ index + 1 ],
+                                        expectation_gradients[ index + 1 ] );
+
+        connections[ index ]-&gt;bpropUpdate( expectations[ index ],
+                                           activations[ index + 1 ],
+                                           expectation_gradients[ index ],
+                                           activation_gradients[ index + 1 ] );
+    }
+
+    reconstruction_connections[ index ]-&gt;fprop( expectations[ index + 1],
+                                                reconstruction_activations);
+    layers[ index ]-&gt;fprop( reconstruction_activations,
+                            layers[ index ]-&gt;expectation);
+    
+    layers[ index ]-&gt;expectation_is_up_to_date = true;
+    train_costs[index] = layers[ index ]-&gt;fpropNLL(expectations[index]);
+
+    layers[ index ]-&gt;bpropNLL(expectations[index], train_costs[index],
+                                  reconstruction_activation_gradients);
+
+    layers[ index ]-&gt;update(reconstruction_activation_gradients);
+
+    // // This is a bad update! Propagates gradient through sigmoid again!
+    // layers[ index ]-&gt;bpropUpdate( reconstruction_activations, 
+    //                                   layers[ index ]-&gt;expectation,
+    //                                   reconstruction_activation_gradients,
+    //                                   reconstruction_expectation_gradients);
+
+    reconstruction_connections[ index ]-&gt;bpropUpdate( 
+        expectations[ index + 1], 
+        reconstruction_activations, 
+        reconstruction_expectation_gradients, //reused
+        reconstruction_activation_gradients);
+
+    if(!fast_exact_is_equal(l1_neuron_decay,0))
+    {
+        // Compute L1 penalty gradient on neurons
+        real* hid = expectations[ index + 1 ].data();
+        real* grad = reconstruction_expectation_gradients.data();
+        int len = expectations[ index + 1 ].length();
+        for(int i=0; i&lt;len; i++)
+        {
+            if(*hid &gt; 0)
+                *grad -= l1_neuron_decay;
+            else if(*hid &lt; 0)
+                *grad += l1_neuron_decay;
+            hid++;
+            grad++;
+        }
+    }
+
+    // Update hidden layer bias and weights
+    layers[ index+1 ]-&gt;bpropUpdate( activations[ index + 1 ],
+                                    expectations[ index + 1 ],
+                                    reconstruction_activation_gradients, // reused
+                                    reconstruction_expectation_gradients);    
+
+    connections[ index ]-&gt;bpropUpdate( 
+        expectations[ index ],
+        activations[ index + 1 ],
+        reconstruction_expectation_gradients, //reused
+        reconstruction_activation_gradients);
+
+}
+
+void StackedAutoassociatorsNet::fineTuningStep( const Vec&amp; input, const Vec&amp; target,
+                                    Vec&amp; train_costs )
+{
+    // fprop
+    expectations[0] &lt;&lt; input;
+    for( int i=0 ; i&lt;n_layers-1; i++ )
+    {
+        connections[i]-&gt;fprop( expectations[i], activations[i+1] );
+        layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+    }
+
+    final_module-&gt;fprop( expectations[ n_layers-1 ],
+                         final_cost_input );
+    final_cost-&gt;fprop( final_cost_input, target, final_cost_value );
+
+    train_costs.subVec(train_costs.length()-final_cost_value.length(),
+                       final_cost_value.length()) &lt;&lt;
+        final_cost_value;
+
+    final_cost-&gt;bpropUpdate( final_cost_input, target,
+                             final_cost_value[0],
+                             final_cost_gradient );
+    final_module-&gt;bpropUpdate( expectations[ n_layers-1 ],
+                               final_cost_input,
+                               expectation_gradients[ n_layers-1 ],
+                               final_cost_gradient );
+
+    for( int i=n_layers-1 ; i&gt;0 ; i-- )
+    {
+        layers[i]-&gt;bpropUpdate( activations[i],
+                                expectations[i],
+                                activation_gradients[i],
+                                expectation_gradients[i] );
+
+        connections[i-1]-&gt;bpropUpdate( expectations[i-1],
+                                       activations[i],
+                                       expectation_gradients[i-1],
+                                       activation_gradients[i] );
+    }
+}
+
+void StackedAutoassociatorsNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    // fprop
+
+    expectations[0] &lt;&lt; input;
+
+    for(int i=0 ; i&lt;currently_trained_layer-1 ; i++ )
+    {
+        connections[i]-&gt;fprop( expectations[i], activations[i+1] );
+        layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+    }
+
+    if( currently_trained_layer&lt;n_layers )
+    {
+        connections[currently_trained_layer-1]-&gt;fprop( 
+            expectations[currently_trained_layer-1], 
+            activations[currently_trained_layer] );
+        layers[currently_trained_layer]-&gt;fprop(
+            activations[currently_trained_layer],
+            output);
+    }
+    else        
+        final_module-&gt;fprop( expectations[ currently_trained_layer - 1],
+                             output );
+}
+
+void StackedAutoassociatorsNet::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+    //Assumes that computeOutput has been called
+
+    costs.resize( getTestCostNames().length() );
+    costs.fill( MISSING_VALUE );
+
+    if(compute_all_test_costs)
+    {
+        for(int i=0; i&lt;currently_trained_layer-1; i++)
+        {
+            reconstruction_connections[ i ]-&gt;fprop( expectations[ i+1 ],
+                                                    reconstruction_activations);
+            layers[ i ]-&gt;fprop( reconstruction_activations,
+                                    layers[ i ]-&gt;expectation);
+            
+            layers[ i ]-&gt;expectation_is_up_to_date = true;
+            costs[i] = layers[ i ]-&gt;fpropNLL(expectations[ i ]);
+            
+            if( partial_costs &amp;&amp; partial_costs[i])
+            {
+                partial_costs[ i ]-&gt;fprop( expectations[ i + 1],
+                                           target, partial_cost_value );
+                costs.subVec(partial_costs_positions[i],
+                             partial_cost_value.length()) &lt;&lt; 
+                    partial_cost_value;
+            }
+        }
+    }
+
+    if( currently_trained_layer&lt;n_layers )
+    {
+        reconstruction_connections[ currently_trained_layer-1 ]-&gt;fprop( 
+            output,
+            reconstruction_activations);
+        layers[ currently_trained_layer-1 ]-&gt;fprop( 
+            reconstruction_activations,
+            layers[ currently_trained_layer-1 ]-&gt;expectation);
+        
+        layers[ currently_trained_layer-1 ]-&gt;expectation_is_up_to_date = true;
+        costs[ currently_trained_layer-1 ] = 
+            layers[ currently_trained_layer-1 ]-&gt;fpropNLL(
+                expectations[ currently_trained_layer-1 ]);
+
+        if( partial_costs &amp;&amp; partial_costs[ currently_trained_layer-1 ] )
+        {
+            partial_costs[ currently_trained_layer-1 ]-&gt;fprop( 
+                output,
+                target, partial_cost_value );
+            costs.subVec(partial_costs_positions[currently_trained_layer-1],
+                         partial_cost_value.length()) &lt;&lt; partial_cost_value;
+        }
+    }
+    else
+    {
+        final_cost-&gt;fprop( output, target, final_cost_value );        
+        costs.subVec(costs.length()-final_cost_value.length(),
+                     final_cost_value.length()) &lt;&lt;
+            final_cost_value;
+    }
+}
+
+TVec&lt;string&gt; StackedAutoassociatorsNet::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    TVec&lt;string&gt; cost_names(0);
+
+    for( int i=0; i&lt;layers.size()-1; i++)
+        cost_names.push_back(&quot;reconstruction_error_&quot; + tostring(i+1));
+    
+    for( int i=0 ; i&lt;partial_costs.size() ; i++ )
+    {
+        TVec&lt;string&gt; cost_names = partial_costs[i]-&gt;name();
+        for(int j=0; j&lt;cost_names.length(); j++)
+            cost_names.push_back(&quot;partial_cost_&quot; + tostring(i+1) + &quot;_&quot; + 
+                cost_names[j]);
+    }
+
+    cost_names.append( final_cost-&gt;name() );
+
+    return cost_names;
+}
+
+TVec&lt;string&gt; StackedAutoassociatorsNet::getTrainCostNames() const
+{
+    return getTestCostNames() ;    
+}
+
+
+//#####  Helper functions  ##################################################
+
+void StackedAutoassociatorsNet::setLearningRate( real the_learning_rate )
+{
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        layers[i]-&gt;setLearningRate( the_learning_rate );
+        connections[i]-&gt;setLearningRate( the_learning_rate );
+    }
+    layers[n_layers-1]-&gt;setLearningRate( the_learning_rate );
+
+    final_cost-&gt;setLearningRate( fine_tuning_learning_rate );
+    final_module-&gt;setLearningRate( fine_tuning_learning_rate );
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-04-27 21:02:20 UTC (rev 6948)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-04-27 21:09:52 UTC (rev 6949)
@@ -0,0 +1,299 @@
+// -*- C++ -*-
+
+// StackedAutoassociatorsNet.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedAutoassociatorsNet.h */
+
+
+#ifndef StackedAutoassociatorsNet_INC
+#define StackedAutoassociatorsNet_INC
+
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn_learners/online/CostModule.h&gt;
+#include &lt;plearn_learners/online/NLLCostModule.h&gt;
+#include &lt;plearn_learners/online/RBMClassificationModule.h&gt;
+#include &lt;plearn_learners/online/RBMLayer.h&gt;
+#include &lt;plearn_learners/online/RBMMixedLayer.h&gt;
+#include &lt;plearn_learners/online/RBMConnection.h&gt;
+#include &lt;plearn/misc/PTimer.h&gt;
+
+namespace PLearn {
+
+/**
+ * Neural net, trained layer-wise in a greedy fashion using autoassociators.
+ * It is highly inspired by the DeepBeliefNet class, and can use use the
+ * same RBMLayer and RBMConnection components.
+ *
+ * TODO: - code globally online version (can't use hyperlearner, 
+ *         because of copies in earlystopping oracle and testing after change...)
+ *       - make sure fpropNLL only uses the expectation field in RBMLayer objects
+ */
+class StackedAutoassociatorsNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The learning rate used during the autoassociator gradient descent training
+    real greedy_learning_rate;
+
+    //! The decrease constant of the learning rate used during the autoassociator
+    //! gradient descent training. When a hidden layer has finished its training,
+    //! the learning rate is reset to it's initial value.
+    real greedy_decrease_ct;
+
+    //! The learning rate used during the fine tuning gradient descent
+    real fine_tuning_learning_rate;
+
+    //! The decrease constant of the learning rate used during fine tuning
+    //! gradient descent
+    real fine_tuning_decrease_ct;
+
+    //! L1 penalty weight on the hidden layers, to encourage sparsity during
+    //! the greedy unsupervised phases
+    real l1_neuron_decay;
+
+    //! Number of examples to use during each phase of learning:
+    //! first the greedy phases, and then the gradient descent.
+    //! Unlike for DeepBeliefNet, these numbers should not be
+    //! cumulative. They correspond to the number of seen training
+    //! examples for each phase.
+    TVec&lt;int&gt; training_schedule;
+
+    //! The layers of units in the network
+    TVec&lt; PP&lt;RBMLayer&gt; &gt; layers;
+
+    //! The weights of the connections between the layers
+    TVec&lt; PP&lt;RBMConnection&gt; &gt; connections;
+
+    //! The weights of the reconstruction connections between the layers
+    TVec&lt; PP&lt;RBMConnection&gt; &gt; reconstruction_connections;
+
+    //! Module that takes as input the output of the last layer
+    //! (layers[n_layers-1), and feeds its output to final_cost
+    //! which defines the fine-tuning criteria.
+    PP&lt;OnlineLearningModule&gt; final_module;
+
+    //! The cost function to be applied on top of the neural network
+    //! (i.e. at the output of final_module). Its gradients will be 
+    //! backpropagated to final_module and then backpropagated to
+    //! the layers.
+    PP&lt;CostModule&gt; final_cost;
+
+    //! Corresponding additional supervised cost function to be applied on 
+    //! top of each hidden layer during the autoassociator training stages. 
+    //! The gradient for these costs are not backpropagated to previous layers.
+    TVec&lt; PP&lt;CostModule&gt; &gt; partial_costs;
+
+    //! Relative weights of the partial costs. If not defined,
+    //! weights of 1 will be assumed for all partial costs.
+    Vec partial_costs_weights;
+
+    //! Indication that, at test time, all costs for all
+    //! layers (up to the currently trained layer) should be computed.
+    bool compute_all_test_costs;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Number of layers
+    int n_layers;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    StackedAutoassociatorsNet();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+
+    void greedyStep( const Vec&amp; input, const Vec&amp; target, int index, 
+                     Vec train_costs );
+
+    void fineTuningStep( const Vec&amp; input, const Vec&amp; target,
+                         Vec&amp; train_costs );
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(StackedAutoassociatorsNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    //! Stores the activations of the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec&lt;Vec&gt; activations;
+
+    //! Stores the expectations of the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec&lt;Vec&gt; expectations;
+
+    //! Stores the gradient of the cost wrt the activations of 
+    //! the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec&lt;Vec&gt; activation_gradients;
+
+    //! Stores the gradient of the cost wrt the expectations of 
+    //! the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec&lt;Vec&gt; expectation_gradients;
+
+    //! Reconstruction activations
+    mutable Vec reconstruction_activations;
+    
+    //! Reconstruction expectations
+    mutable Vec reconstruction_expectations;
+    
+    //! Reconstruction activations
+    mutable Vec reconstruction_activation_gradients;
+    
+    //! Reconstruction expectations
+    mutable Vec reconstruction_expectation_gradients;
+
+    //! Position in the total cost vector of the different partial costs
+    mutable TVec&lt;int&gt; partial_costs_positions;
+    
+    //! Cost value of partial_costs
+    mutable Vec partial_cost_value;
+
+    //! Input of the final_cost
+    mutable Vec final_cost_input;
+
+    //! Cost value of final_cost
+    mutable Vec final_cost_value;
+
+    //! Stores the gradient of the cost at the input of final_cost
+    mutable Vec final_cost_gradient;
+
+    //! Stages of the different greedy phases
+    TVec&lt;int&gt; greedy_stages;
+
+    //! Currently trained layer (1 means the first hidden layer,
+    //! n_layers means the output layer)
+    int currently_trained_layer;
+
+    //! Indication whether final_module has learning rate
+    bool final_module_has_learning_rate;
+    
+    //! Indication whether final_cost has learning rate
+    bool final_cost_has_learning_rate;
+    
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_classification_cost();
+
+    void build_costs();
+
+    void setLearningRate( real the_learning_rate );
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(StackedAutoassociatorsNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000397.html">[Plearn-commits] r6948 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000399.html">[Plearn-commits] r6950 - trunk/commands
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#398">[ date ]</a>
              <a href="thread.html#398">[ thread ]</a>
              <a href="subject.html#398">[ subject ]</a>
              <a href="author.html#398">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
