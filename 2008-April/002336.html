<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8888 - trunk/python_modules/plearn/learners
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-April/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8888%20-%20trunk/python_modules/plearn/learners&In-Reply-To=%3C200804241906.m3OJ6aWD003897%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002335.html">
   <LINK REL="Next"  HREF="002337.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8888 - trunk/python_modules/plearn/learners</H1>
    <B>louradou at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8888%20-%20trunk/python_modules/plearn/learners&In-Reply-To=%3C200804241906.m3OJ6aWD003897%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8888 - trunk/python_modules/plearn/learners">louradou at mail.berlios.de
       </A><BR>
    <I>Thu Apr 24 21:06:36 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002335.html">[Plearn-commits] r8887 - trunk/plearn_learners/hyper
</A></li>
        <LI>Next message: <A HREF="002337.html">[Plearn-commits] r8889 - in	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir:	. Split0 Split0/LearnerExpdir
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2336">[ date ]</a>
              <a href="thread.html#2336">[ thread ]</a>
              <a href="subject.html#2336">[ subject ]</a>
              <a href="author.html#2336">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: louradou
Date: 2008-04-24 21:06:36 +0200 (Thu, 24 Apr 2008)
New Revision: 8888

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
* added an option testlevel to control at which
  frequency we want to test the models in the run()
* little change to compute input stats when balance_classes=True
* when test_on_train is False, no more 'None' column is written in the resultfile.amat



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-24 19:04:13 UTC (rev 8887)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-24 19:06:36 UTC (rev 8888)
@@ -24,14 +24,14 @@
         List of attributes:    
         -------------------    
 
-    public: buildoption
+    user options:
 
 
         'C_initvalue': &lt;float&gt; Default value for 'C'.
 
         'verbosity': &lt;int&gt; Level of verbosity.
 
-    public: learntoption
+    learnt options:
 
         'best_param': &lt;dict&gt; of hyperparameters'values      
                       which gave the best performance by    
@@ -52,7 +52,7 @@
 
         'input_stds': &lt;list&gt; of standard deviation values for each input component.
 
-    protected:
+    internally managed options:
 
         'kernel_type': &lt;str&gt; corresponding to the kernel.    
                        in ['gaussian','linear','poly'].
@@ -562,7 +562,7 @@
         List of attributes:    
         -------------------    
 
-    public: buildoption
+    user options:
 
         'costnames': &lt;list&gt; of cost names. By default, ['class_error'].
                  All cost names implemented are:
@@ -630,6 +630,11 @@
 
         'test_on_train': &lt;bool&gt; Should we test best models on {test, train} (1)
 
+        'testlevel': &lt;int&gt; Frequency of test:
+                     - 0: write results only at the end of the run()
+                     - 1: write results each time a better validation cost is found in run()
+                     - 2: write all intermediate results
+
         'results_filename': &lt;string&gt; Path to an output file for results
         
         'preproc_optionnames': &lt;string&gt; or &lt;list of strings&gt; indicating the names of the
@@ -641,7 +646,7 @@
 
         'verbosity': &lt;int&gt; Level of verbosity.
 
-    public: learntoption
+    learnt options:
 
         'best_model':
 
@@ -670,9 +675,8 @@
         'input_avgstd': &lt;float&gt; input std
 
         'validtype': &lt;str&gt; type of validation: 'simple' or 'cross'.
-        
-       
-    protected:
+               
+    internally managed options:
 
         'param_names': &lt;list&gt; of all hyperparameter names. It    
                        always includes at first the positive     
@@ -698,6 +702,7 @@
                         'max_ntrials',
                         'retrain_on_valid',
                         'test_on_train',
+                        'testlevel',
                         'verbosity',
                         'results_filename',
                         'preproc_optionnames',
@@ -774,9 +779,10 @@
         self.retrain_on_valid = True
         self.retrain_until_local_optimum_is_found = True
         self.max_ntrials = 50
-        self.test_on_train = True
+        self.test_on_train = False
         
         self.verbosity = 0
+        self.testlevel = 1
         
         self.trainset_key = 'trainset'
         self.validset_key = 'validset'
@@ -819,7 +825,8 @@
 
     def additional_preproc(self, input_vmat, isTrain=False):
         if self.balance_classes and isTrain:
-            return ReplicateSamplesVMatrix(source = input_vmat, operate_on_bags = True)
+            return ReplicateSamplesVMatrix(source = input_vmat,
+                                           operate_on_bags = (input_vmat.targetsize &gt; 1 ))
         return input_vmat
 
     ## specific to libsvm
@@ -881,7 +888,7 @@
                      self.inputsize, self.input_avgstd )
         assert vmat &lt;&gt; None
 
-        samples, targets = self.get_svminputlist( vmat )
+        samples, targets = self.get_svminputlist( vmat, True )
 
         self.all_experts[0].verbosity = self.verbosity
         self.inputsize, self.input_avgstd = self.all_experts[0].get_input_stats(samples)
@@ -983,7 +990,8 @@
     def write_results(  self, param,
                         valid_stats,
                         test_stats = None,
-                        train_stats= None ):
+                        train_stats= None,
+                        only_stdout=False ):
         if valid_stats==None and param == self.best_param:
                 valid_stats = self.valid_stats
         if test_stats==None and param == self.best_param:
@@ -996,16 +1004,16 @@
         
         # If no file specified, print on stdout in a readable format
         if self.results_filename == None or self.verbosity &gt; 0:
-            print &quot;\n -- Trial with parameters&quot;
-            for pn in param:
-                print &quot;    &quot;,pn,&quot; = &quot;,param[pn]
+            #print &quot;\n -- Trial with parameters&quot;
+            #for pn in param:
+            #    print &quot;    &quot;,pn,&quot; = &quot;,param[pn]
             if train_costs &lt;&gt; None:
                 print &quot; -- train costs: &quot;, train_costs
             if valid_costs &lt;&gt; None:
                 print &quot; -- valid costs: &quot;, valid_costs
             if test_costs &lt;&gt; None:
                 print &quot; -- test costs: &quot;,  test_costs
-            if self.results_filename == None:
+            if self.results_filename == None or only_stdout:
                 return
 
 
@@ -1041,8 +1049,13 @@
         
         costnames_string = &quot;&quot;
         costvalues_string = &quot;&quot;
+        
+        all_set_names = ['valid','test']
+        if self.test_on_train:
+            all_set_names = ['train']+all_set_names
+        
         for cn in self.costnames:
-            for dataset in ['train','valid','test']:
+            for dataset in all_set_names:
                 costs = eval(dataset+'_costs')
                 
                 # Special processing for the confusion matrix
@@ -1357,6 +1370,7 @@
                dataspec,
                param= None):
         if self.validset_key in dataspec:
+            self.validtype = 'simple'
             return self.simplevalid(dataspec, param)
         else:
             return self.crossvalid(dataspec, param)
@@ -1366,11 +1380,9 @@
     &quot;&quot;&quot;
     def simplevalid( self,
                      dataspec,
-                     param= None,
+                     param,
                      validstats = None,
                      verbosity = True ):
-        if not param:
-            param = self.best_param
         if self.verbosity &gt; 0 and verbosity:
             print &quot;\n** Simple Validation&quot;
             print &quot;   with param %s&quot; % param
@@ -1385,9 +1397,7 @@
     &quot;&quot;&quot;
     def crossvalid( self,
                     dataspec,
-                    param = None ):
-        if not param:
-            param = self.best_param
+                    param ):
         nclasses = self.nclasses
         n_fold = self.n_fold
         self.validtype = '%s-fold' % n_fold
@@ -1439,6 +1449,63 @@
         return validstats
 
 
+    def retrain_and_writeresults(self, dataspec):
+        trainset = self.train_inputspec(dataspec)
+        validset = self.valid_inputspec(dataspec)
+        testset  = self.test_inputspec(dataspec)
+        # Cross Validation
+        if 'fold' in self.validtype:
+            self.train( dataspec )
+
+        # Simple Validation
+        else:
+            self.validtype = 'simple'
+            # CAUTION: in the case of simple validation without retraining on {train+valid},
+            #          self.best_model is supposed to be updated
+            if self.retrain_on_valid:
+                &quot;&quot;&quot; Uncomment following lines if you want to check that
+                    retraining on {train + valid} sets does not degrade.
+                &quot;&quot;&quot;
+                #train_stats = None
+                #test_stats = None
+                #if self.test_on_train:
+                #    if self.verbosity &gt; 2:
+                #        print &quot;\n** (testing simple valid on &quot;+self.trainset_key+&quot;)&quot;
+                #    train_stats = self.test( trainset )
+                #if testset &lt;&gt; None:
+                #    if self.verbosity &gt; 2:
+                #        print &quot;\n** (testing simple valid on &quot;+self.testset_key+&quot;)&quot;
+                #test_stats = self.test( testset  )
+                #
+                #self.write_results( self.best_param,
+                #                    valid_stats, test_stats, train_stats )
+                self.validtype = 'simple+retrain'
+                tv_set = ConcatRowsVMatrix(
+                            sources = [ trainset,
+                                        validset
+                                        ],
+                            fully_check_mappings = False,
+                        )
+                if self.verbosity &gt; 0:
+                    print &quot;\n** re-training model on { train + valid } &quot;
+                self.train( {self.trainset_key: tv_set} )
+
+        train_stats = None
+        test_stats = None
+        if self.test_on_train:
+            if self.verbosity &gt; 2:
+               print &quot;\n** (testing on &quot;+self.trainset_key+&quot;)&quot;
+            train_stats = self.test( trainset )
+        if testset &lt;&gt; None:
+            if self.verbosity &gt; 2:
+                print &quot;\n** (testing on &quot;+self.testset_key+&quot;)&quot;
+            test_stats = self.test( testset )
+        self.update_trials( self.best_param,
+                            None, test_stats, train_stats )
+        self.write_results( self.best_param,
+                            self.valid_stats, self.test_stats, self.train_stats )
+
+
     &quot;&quot;&quot; THE interesting function of the class.
         See __main__ below for usage.
         dataspec is a dictionary which specifies train, valid, test sets.
@@ -1446,9 +1513,6 @@
         cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     &quot;&quot;&quot;
     def run(self, dataspec):
-        if self.verbosity &gt; 3:
-            print &quot;SVM::run() called &quot;, dataspec.keys()
-
         trainset = self.train_inputspec(dataspec)
         validset = self.valid_inputspec(dataspec)
         testset  = self.test_inputspec(dataspec)
@@ -1476,73 +1540,30 @@
 
                 # We reject the model (to avoid testing on it)
                 self.model = None
-                
-                self.write_results( param, valid_stats )
+                self.write_results( param, valid_stats, None, None, self.testlevel &lt; 2  )
 
             # Better valid cost is obtained!
             else:
-
-                # Cross Validation
-                if 'fold' in self.validtype:
-                    self.train( dataspec )
-
+                print &quot;better cost was found!&quot;
                 # Simple Validation
+                if 'fold' not in self.validtype:
+                    self.best_model = self.model
+                    
+                if self.testlevel &gt; 0:
+                    self.retrain_and_writeresults(dataspec)
                 else:
-                    self.validtype = 'simple'
-                    self.best_model = self.model
-                    if self.retrain_on_valid:
+                    self.write_results( param, valid_stats, None, None, True  )
 
-                        &quot;&quot;&quot; Uncomment following lines if you want to check that
-                            retraining on {train + valid} sets does not degrade.
-                        &quot;&quot;&quot;
-                        #train_stats = None
-                        #test_stats = None
-                        #if self.test_on_train:
-                        #    if self.verbosity &gt; 2:
-                        #        print &quot;\n** (testing simple valid on &quot;+self.trainset_key+&quot;)&quot;
-                        #    train_stats = self.test( trainset )
-                        #if testset &lt;&gt; None:
-                        #    if self.verbosity &gt; 2:
-                        #        print &quot;\n** (testing simple valid on &quot;+self.testset_key+&quot;)&quot;
-                        #test_stats = self.test( testset  )
-                        #
-                        #self.write_results( self.best_param,
-                        #                    valid_stats, test_stats, train_stats )
-
-                        self.validtype = 'simple+retrain'
-
-                        tv_set = ConcatRowsVMatrix(
-                                    sources = [ trainset,
-                                                validset
-                                                ],
-                                    fully_check_mappings = False,
-                                )
-                        if self.verbosity &gt; 0:
-                            print &quot;\n** re-training model on { train + valid } &quot;
-                        self.train( {self.trainset_key: tv_set} )
-
-                train_stats = None
-                test_stats = None
-                if self.test_on_train:
-                    if self.verbosity &gt; 2:
-                        print &quot;\n** (testing on &quot;+self.trainset_key+&quot;)&quot;
-                    train_stats = self.test( trainset )
-                if testset &lt;&gt; None:
-                    if self.verbosity &gt; 2:
-                        print &quot;\n** (testing on &quot;+self.testset_key+&quot;)&quot;
-                    test_stats = self.test( testset )
-                self.update_trials( self.best_param,
-                                    None, test_stats, train_stats )
-                self.write_results( self.best_param,
-                                    self.valid_stats, self.test_stats, self.train_stats )
-
             if len(expert.trials_param_list)-L0 &gt;= self.max_ntrials:
                 return dataspec
 
         if( self.retrain_until_local_optimum_is_found
         and expert.should_be_tuned_again() ):
-           self.run( dataspec )
+           return self.run( dataspec )
 
+        if self.testlevel == 0:
+             self.retrain_and_writeresults(dataspec)
+
         return dataspec
 
 &quot;&quot;&quot; Some AUXILIARY FUNCTIONS, to have access or modify 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002335.html">[Plearn-commits] r8887 - trunk/plearn_learners/hyper
</A></li>
	<LI>Next message: <A HREF="002337.html">[Plearn-commits] r8889 - in	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir:	. Split0 Split0/LearnerExpdir
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2336">[ date ]</a>
              <a href="thread.html#2336">[ thread ]</a>
              <a href="subject.html#2336">[ subject ]</a>
              <a href="author.html#2336">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
