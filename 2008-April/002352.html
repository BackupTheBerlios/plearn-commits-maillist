<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8904 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-April/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8904%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200804262037.m3QKbwVC017832%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002351.html">
   <LINK REL="Next"  HREF="002353.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8904 - trunk/plearn_learners_experimental</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8904%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200804262037.m3QKbwVC017832%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8904 - trunk/plearn_learners_experimental">larocheh at mail.berlios.de
       </A><BR>
    <I>Sat Apr 26 22:37:58 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002351.html">[Plearn-commits] r8903 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="002353.html">[Plearn-commits] r8905 - in	trunk/plearn_learners/generic/test/NNet: . .pytest	.pytest/PL_NNet_1_hidden_bug	.pytest/PL_NNet_1_hidden_bug/expected_results	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2352">[ date ]</a>
              <a href="thread.html#2352">[ thread ]</a>
              <a href="subject.html#2352">[ subject ]</a>
              <a href="author.html#2352">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2008-04-26 22:37:58 +0200 (Sat, 26 Apr 2008)
New Revision: 8904

Added:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Attempt at training RBMs with pseudolikelihood...


Added: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-04-26 20:37:19 UTC (rev 8903)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-04-26 20:37:58 UTC (rev 8904)
@@ -0,0 +1,958 @@
+// -*- C++ -*-
+
+// PseudolikelihoodRBM.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file PseudolikelihoodRBM.cc */
+
+
+#define PL_LOG_MODULE_NAME &quot;PseudolikelihoodRBM&quot;
+#include &quot;PseudolikelihoodRBM.h&quot;
+#include &lt;plearn_learners/online/RBMLayer.h&gt;
+#include &lt;plearn/io/pl_log.h&gt;
+
+#define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    PseudolikelihoodRBM,
+    &quot;Restricted Boltzmann Machine trained by (generalized) pseudolikelihood.&quot;,
+    &quot;&quot;);
+
+///////////////////
+// PseudolikelihoodRBM //
+///////////////////
+PseudolikelihoodRBM::PseudolikelihoodRBM() :
+    learning_rate( 0. ),
+    decrease_ct( 0. ),
+    cd_learning_rate( 0. ),
+    cd_decrease_ct( 0. ),
+    cd_n_gibbs( 1 ),
+    n_classes( -1 ),
+    compute_input_space_nll( false ),
+    pseudolikelihood_context_size ( 0 ),
+    log_Z( MISSING_VALUE ),
+    Z_is_up_to_date( false )
+{
+    random_gen = new PRandom();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void PseudolikelihoodRBM::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;learning_rate&quot;, &amp;PseudolikelihoodRBM::learning_rate,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used for pseudolikelihood training.\n&quot;);
+
+    declareOption(ol, &quot;decrease_ct&quot;, &amp;PseudolikelihoodRBM::decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;The decrease constant of the learning rate.\n&quot;);
+
+    declareOption(ol, &quot;cd_learning_rate&quot;, &amp;PseudolikelihoodRBM::cd_learning_rate,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used for contrastive divergence learning.\n&quot;);
+
+    declareOption(ol, &quot;cd_decrease_ct&quot;, &amp;PseudolikelihoodRBM::cd_decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;The decrease constant of the contrastive divergence &quot;
+                  &quot;learning rate.\n&quot;);
+
+    declareOption(ol, &quot;cd_n_gibbs&quot;, &amp;PseudolikelihoodRBM::cd_n_gibbs,
+                  OptionBase::buildoption,
+                  &quot;Number of negative phase gibbs sampling steps.\n&quot;);
+
+    declareOption(ol, &quot;n_classes&quot;, &amp;PseudolikelihoodRBM::n_classes,
+                  OptionBase::buildoption,
+                  &quot;Number of classes in the training set (for supervised learning).\n&quot;
+                  );
+
+    declareOption(ol, &quot;compute_input_space_nll&quot;, 
+                  &amp;PseudolikelihoodRBM::compute_input_space_nll,
+                  OptionBase::buildoption,
+                  &quot;Indication that the input space NLL should be &quot;
+                  &quot;computed during test.\n&quot;
+                  );
+
+    declareOption(ol, &quot;pseudolikelihood_context_size&quot;, 
+                  &amp;PseudolikelihoodRBM::pseudolikelihood_context_size,
+                  OptionBase::buildoption,
+                  &quot;Number of additional input variables chosen to form the joint\n&quot;
+                  &quot;condition likelihoods in generalized pseudolikelihood\n&quot;
+                  &quot;(default = 0, which corresponds to standard pseudolikelihood).\n&quot;
+                  );
+
+    declareOption(ol, &quot;input_layer&quot;, &amp;PseudolikelihoodRBM::input_layer,
+                  OptionBase::buildoption,
+                  &quot;The binomial input layer of the RBM.\n&quot;);
+
+    declareOption(ol, &quot;hidden_layer&quot;, &amp;PseudolikelihoodRBM::hidden_layer,
+                  OptionBase::buildoption,
+                  &quot;The hidden layer of the RBM.\n&quot;);
+
+    declareOption(ol, &quot;connection&quot;, &amp;PseudolikelihoodRBM::connection,
+                  OptionBase::buildoption,
+                  &quot;The connection weights between the input and hidden layer.\n&quot;);
+
+    declareOption(ol, &quot;log_Z&quot;, &amp;PseudolikelihoodRBM::log_Z,
+                  OptionBase::learntoption,
+                  &quot;Normalisation constant (on log scale).\n&quot;);
+
+    declareOption(ol, &quot;Z_is_up_to_date&quot;, &amp;PseudolikelihoodRBM::Z_is_up_to_date,
+                  OptionBase::learntoption,
+                  &quot;Indication that the normalisation constant Z is up to date.\n&quot;);
+
+//    declareOption(ol, &quot;target_weights_L1_penalty_factor&quot;, 
+//                  &amp;PseudolikelihoodRBM::target_weights_L1_penalty_factor,
+//                  OptionBase::buildoption,
+//                  &quot;Target weights' L1_penalty_factor.\n&quot;);
+//
+//    declareOption(ol, &quot;target_weights_L2_penalty_factor&quot;, 
+//                  &amp;PseudolikelihoodRBM::target_weights_L2_penalty_factor,
+//                  OptionBase::buildoption,
+//                  &quot;Target weights' L2_penalty_factor.\n&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void PseudolikelihoodRBM::build_()
+{
+    MODULE_LOG &lt;&lt; &quot;build_() called&quot; &lt;&lt; endl;
+
+    if( inputsize_ &gt; 0 &amp;&amp; targetsize_ &gt;= 0)
+    {
+        if( n_classes &gt; 1 &amp;&amp; targetsize_ != 1 )
+            PLERROR(&quot;In PseudolikelihoodRBM::build_(): can't use supervised &quot;
+                &quot;learning (n_classes &gt; 1) if there is no target field &quot;
+                &quot;(targetsize() != 1)&quot;);
+        
+        if( compute_input_space_nll &amp;&amp; n_classes &gt; 1 )
+            PLERROR(&quot;In PseudolikelihoodRBM::build_(): compute_input_space_nll &quot;
+                    &quot;is not compatible with n_classes &gt; 1&quot;);
+
+        if( pseudolikelihood_context_size &lt; 0 )
+            PLERROR(&quot;In PseudolikelihoodRBM::build_(): &quot;
+                    &quot;pseudolikelihood_context_size should be &gt;= 0.&quot;);
+
+        build_layers_and_connections();
+        build_costs();
+    }
+}
+
+/////////////////
+// build_costs //
+/////////////////
+void PseudolikelihoodRBM::build_costs()
+{
+    cost_names.resize(0);
+    
+    int current_index = 0;
+    if( compute_input_space_nll || n_classes &gt; 1 )
+    {
+        cost_names.append(&quot;NLL&quot;);
+        nll_cost_index = current_index;
+        current_index++;
+    }
+    
+    if( n_classes &gt; 1 )
+    {
+        cost_names.append(&quot;class_error&quot;);
+        class_cost_index = current_index;
+        current_index++;
+    }
+
+    PLASSERT( current_index == cost_names.length() );
+}
+
+//////////////////////////////////
+// build_layers_and_connections //
+//////////////////////////////////
+void PseudolikelihoodRBM::build_layers_and_connections()
+{
+    MODULE_LOG &lt;&lt; &quot;build_layers_and_connections() called&quot; &lt;&lt; endl;
+
+    if( !input_layer )
+        PLERROR(&quot;In PseudolikelihoodRBM::build_layers_and_connections(): &quot;
+                &quot;input_layer must be provided&quot;);
+    if( !hidden_layer )
+        PLERROR(&quot;In PseudolikelihoodRBM::build_layers_and_connections(): &quot;
+                &quot;hidden_layer must be provided&quot;);
+
+    if( !connection )
+        PLERROR(&quot;PseudolikelihoodRBM::build_layers_and_connections(): \n&quot;
+                &quot;connection must be provided&quot;);
+
+    if( connection-&gt;up_size != hidden_layer-&gt;size ||
+        connection-&gt;down_size != input_layer-&gt;size )
+        PLERROR(&quot;PseudolikelihoodRBM::build_layers_and_connections(): \n&quot;
+                &quot;connection's size (%d x %d) should be %d x %d&quot;,
+                connection-&gt;up_size, connection-&gt;down_size,
+                hidden_layer-&gt;size, input_layer-&gt;size);
+
+    hidden_activation_pos_i.resize( hidden_layer-&gt;size );
+    hidden_activation_neg_i.resize( hidden_layer-&gt;size );
+    hidden_activation_gradient.resize( hidden_layer-&gt;size );
+    hidden_activation_pos_i_gradient.resize( hidden_layer-&gt;size );
+    hidden_activation_neg_i_gradient.resize( hidden_layer-&gt;size );
+    connection_gradient.resize( connection-&gt;up_size, connection-&gt;down_size );
+
+    if( inputsize_ &gt;= 0 )
+        PLASSERT( input_layer-&gt;size == inputsize() );
+
+    if( n_classes &gt; 1 )
+    {
+        class_output.resize( n_classes );
+        before_class_output.resize( n_classes );
+        class_gradient.resize( n_classes );
+        target_one_hot.resize( n_classes );
+    }
+
+    if( !input_layer-&gt;random_gen )
+    {
+        input_layer-&gt;random_gen = random_gen;
+        input_layer-&gt;forget();
+    }
+
+    if( !hidden_layer-&gt;random_gen )
+    {
+        hidden_layer-&gt;random_gen = random_gen;
+        hidden_layer-&gt;forget();
+    }
+
+    if( !connection-&gt;random_gen )
+    {
+        connection-&gt;random_gen = random_gen;
+        connection-&gt;forget();
+    }
+}
+
+///////////
+// build //
+///////////
+void PseudolikelihoodRBM::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void PseudolikelihoodRBM::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(input_layer, copies);
+    deepCopyField(hidden_layer, copies);
+    deepCopyField(connection, copies);
+    deepCopyField(cost_names, copies);
+
+    deepCopyField(target_one_hot, copies);
+    deepCopyField(input_gradient, copies);
+    deepCopyField(class_output, copies);
+    deepCopyField(before_class_output, copies);
+    deepCopyField(class_gradient, copies);
+    deepCopyField(hidden_activation_pos_i, copies);
+    deepCopyField(hidden_activation_neg_i, copies);
+    deepCopyField(hidden_activation_gradient, copies);
+    deepCopyField(hidden_activation_pos_i_gradient, copies);
+    deepCopyField(hidden_activation_neg_i_gradient, copies);
+    deepCopyField(connection_gradient, copies);
+    deepCopyField(context_indices, copies);
+    deepCopyField(context_indices_per_i, copies);
+    deepCopyField(hidden_activations_context, copies);
+    deepCopyField(hidden_activations_context_k_gradient, copies);
+    deepCopyField(nums, copies);
+    deepCopyField(nums_act, copies);
+    deepCopyField(context_probs, copies);
+    deepCopyField(gnums_act, copies);
+    deepCopyField(conf, copies);
+    deepCopyField(pos_input, copies);
+    deepCopyField(pos_hidden, copies);
+    deepCopyField(neg_input, copies);
+    deepCopyField(neg_hidden, copies);
+}
+
+
+////////////////
+// outputsize //
+////////////////
+int PseudolikelihoodRBM::outputsize() const
+{
+    return n_classes &gt; 1 ? n_classes : hidden_layer-&gt;size;
+}
+
+////////////
+// forget //
+////////////
+void PseudolikelihoodRBM::forget()
+{
+    inherited::forget();
+
+    input_layer-&gt;forget();
+    hidden_layer-&gt;forget();
+    connection-&gt;forget();
+    Z_is_up_to_date = false;
+}
+
+///////////
+// train //
+///////////
+void PseudolikelihoodRBM::train()
+{
+    MODULE_LOG &lt;&lt; &quot;train() called &quot; &lt;&lt; endl;
+
+    MODULE_LOG &lt;&lt; &quot;stage = &quot; &lt;&lt; stage
+        &lt;&lt; &quot;, target nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
+
+    PLASSERT( train_set );
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    int target_index;
+    real weight; // unused
+    real lr;
+
+    TVec&lt;string&gt; train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set-&gt;length();
+    int init_stage = stage;
+    if( !initTrain() )
+    {
+        MODULE_LOG &lt;&lt; &quot;train() aborted&quot; &lt;&lt; endl;
+        return;
+    }
+
+    PP&lt;ProgressBar&gt; pb;
+
+    // clear stats of previous epoch
+    train_stats-&gt;forget();
+
+    if( report_progress )
+        pb = new ProgressBar( &quot;Training &quot;
+                              + classname(),
+                              nstages - stage );
+
+    for( ; stage&lt;nstages ; stage++ )
+    {
+        Z_is_up_to_date = false;
+        train_set-&gt;getExample(stage%nsamples, input, target, weight);
+
+        if( pb )
+            pb-&gt;update( stage - init_stage + 1 );
+
+        if( targetsize() == 1 )
+        {
+            target_one_hot.clear();
+            if( !is_missing(target[0]) )
+            {
+                target_index = (int)round( target[0] );
+                target_one_hot[ target_index ] = 1;
+            }
+            PLERROR(&quot;In PseudolikelihoodRBM::train(): supervised learning &quot;
+                    &quot;not implemented yet.&quot;);
+
+            if( decrease_ct != 0 )
+                lr = learning_rate / (1.0 + stage * decrease_ct );
+            else
+                lr = learning_rate;
+
+            setLearningRate(lr);
+        }
+        else
+        {
+            if( !fast_is_equal(learning_rate, 0.) )
+            {
+                if( decrease_ct != 0 )
+                    lr = learning_rate / (1.0 + stage * decrease_ct );
+                else
+                    lr = learning_rate;
+
+                setLearningRate(lr);
+
+                if( pseudolikelihood_context_size == 0 )
+                {
+                    // Compute input_probs
+                    //
+                    //a = W x + c
+                    //  for i in 1...d
+                    //      num_pos = b_i
+                    //      num_neg = 0
+                    //      for j in 1...h
+                    //          num_pos += softplus( a_j - W_ji x_i + W_ji)
+                    //          num_neg += softplus( a_j - W_ji x_i)
+                    //      p_i = exp(num_pos) / (exp(num_pos) + exp(num_neg))
+
+                    connection-&gt;setAsDownInput( input );
+                    hidden_layer-&gt;getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+
+                    real num_pos_act;
+                    real num_neg_act;
+                    real num_pos;
+                    real num_neg;
+                    real* a = hidden_layer-&gt;activation.data();
+                    real* a_pos_i = hidden_activation_pos_i.data();
+                    real* a_neg_i = hidden_activation_neg_i.data();
+                    real* w, *gw;
+                    int m = connection-&gt;weights.mod();
+                    real input_i, input_probs_i;
+                    real pseudolikelihood = 0;
+                    real* ga_pos_i = hidden_activation_pos_i_gradient.data();
+                    real* ga_neg_i = hidden_activation_neg_i_gradient.data();
+                    hidden_activation_gradient.clear();
+                    connection_gradient.clear();
+                    for( int i=0; i&lt;input_layer-&gt;size ; i++ )
+                    {
+                        num_pos_act = input_layer-&gt;bias[i];
+                        num_neg_act = 0;
+                        w = &amp;(connection-&gt;weights(0,i));
+                        input_i = input[i];
+                        for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                        {
+                            a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                            a_neg_i[j] = a[j] - *w * input_i;
+                        }
+                        num_pos_act -= hidden_layer-&gt;freeEnergyContribution(
+                            hidden_activation_pos_i);
+                        num_neg_act -= hidden_layer-&gt;freeEnergyContribution(
+                            hidden_activation_neg_i);
+                        num_pos = safeexp(num_pos_act);
+                        num_neg = safeexp(num_neg_act);
+                        input_probs_i = num_pos / (num_pos + num_neg);
+
+                        // Compute input_prob gradient
+                        if( input_layer-&gt;use_fast_approximations )
+                            pseudolikelihood += tabulated_softplus( 
+                                num_pos_act - num_neg_act ) 
+                                - input_i * (num_pos_act - num_neg_act);
+                        else
+                            pseudolikelihood += softplus( 
+                                num_pos_act - num_neg_act ) 
+                                - input_i * (num_pos_act - num_neg_act);;
+                        input_gradient[i] = input_probs_i - input_i;
+
+                        hidden_layer-&gt;freeEnergyContributionGradient(
+                            hidden_activation_pos_i,
+                            hidden_activation_pos_i_gradient,
+                            -input_gradient[i],
+                            false);
+                        hidden_activation_gradient += hidden_activation_pos_i_gradient;
+
+                        hidden_layer-&gt;freeEnergyContributionGradient(
+                            hidden_activation_neg_i,
+                            hidden_activation_neg_i_gradient,
+                            input_gradient[i],
+                            false);
+                        hidden_activation_gradient += hidden_activation_neg_i_gradient;
+
+                        gw = &amp;(connection_gradient(0,i));
+                        for( int j=0; j&lt;hidden_layer-&gt;size; j++,gw+=m )
+                        {
+                            *gw -= ga_pos_i[j] * ( input_i - 1 );
+                            *gw -= ga_neg_i[j] * input_i;
+                        }
+                    }
+
+                    externalProductAcc( connection_gradient, hidden_activation_gradient,
+                                        input );
+
+                    // Hidden bias update
+                    multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
+                                      hidden_layer-&gt;bias);
+                    // Connection weights update
+                    multiplyScaledAdd( connection_gradient, 1.0, -lr,
+                                       connection-&gt;weights );
+                    // Input bias update
+                    multiplyScaledAdd(input_gradient, 1.0, -lr,
+                                      input_layer-&gt;bias);
+
+                    // N.B.: train costs contains pseudolikelihood
+                    //       or pseudoNLL, not NLL
+                    train_costs[nll_cost_index] = pseudolikelihood;
+                }
+                else
+                {
+                    // Generate contexts
+                    context_indices.resize( input_layer-&gt;size - 1);
+                    context_indices_per_i.resize( input_layer-&gt;size, 
+                                                  pseudolikelihood_context_size );
+                    for( int i=0; i&lt;context_indices.length(); i++)
+                        context_indices[i] = i;
+                    int tmp,k;
+                    int n = input_layer-&gt;size-1;
+                    int* c = context_indices.data();
+                    int* ci;
+                    for( int i=0; i&lt;context_indices_per_i.length(); i++)
+                    {
+                        ci = context_indices_per_i[i];
+                        for (int j = 0; j &lt; context_indices_per_i.width(); j++) {
+                            k = j + random_gen-&gt;uniform_multinomial_sample(n - j);
+                            tmp = c[j];
+                            c[j] = c[k];
+                            c[k] = tmp;
+                            if( c[j] &gt;= i )
+                                ci[j] = c[j]+1;
+                            else
+                                ci[j] = c[j];
+                        }
+                    }
+
+                    connection-&gt;setAsDownInput( input );
+                    hidden_layer-&gt;getAllActivations( 
+                        (RBMMatrixConnection *) connection );
+
+                    int n_conf = ipow(2, pseudolikelihood_context_size);
+                    nums_act.resize( 2 * n_conf );
+                    gnums_act.resize( 2 * n_conf );
+                    context_probs.resize( 2 * n_conf );
+                    hidden_activations_context.resize( 2*n_conf, hidden_layer-&gt;size );
+                    hidden_activations_context_k_gradient.resize( hidden_layer-&gt;size );
+                    real* nums_data;
+                    real* gnums_data;
+                    real* cp_data;
+                    real* a = hidden_layer-&gt;activation.data();
+                    real* w, *gw, *gi, *ac, *gac;
+                    int* context_i;
+                    int m = connection-&gt;weights.mod();
+                    int conf_index;
+                    real input_i, input_j, bi, Zi, log_Zi;
+                    real pseudolikelihood = 0;
+
+                    input_gradient.clear();
+                    hidden_activation_gradient.clear();
+                    connection_gradient.clear();
+                    gi = input_gradient.data();
+                    for( int i=0; i&lt;input_layer-&gt;size ; i++ )
+                    {
+                        nums_data = nums_act.data();
+                        cp_data = context_probs.data();
+                        input_i = input[i];
+                        bi = input_layer-&gt;bias[i];
+
+                        // input_i = 1
+                        for( int k=0; k&lt;n_conf; k++)
+                        {
+                            *nums_data = bi;
+                            *cp_data = input_i;
+                            conf_index = k;
+                            ac = hidden_activations_context[k];
+
+                            w = &amp;(connection-&gt;weights(0,i));
+                            for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                                ac[j] = a[j] - *w * ( input_i - 1 );
+
+                            context_i = context_indices_per_i[i];
+                            for( int l=0; l&lt;pseudolikelihood_context_size; l++ )
+                            {
+                                w = &amp;(connection-&gt;weights(0,*context_i));
+                                input_j = input[*context_i];
+                                for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                                {
+                                    if( conf_index &amp; 1)
+                                    {
+                                        ac[j] -=  *w * ( input_j - 1 );
+                                        *cp_data *= input_j;
+                                    }
+                                    else
+                                    {
+                                        ac[j] -=  *w * input_j;
+                                        *cp_data *= (1-input_j);
+                                    }
+                                }
+                                conf_index &gt;&gt;= 1;
+                                context_i++;
+                            }
+                            *nums_data -= hidden_layer-&gt;freeEnergyContribution(
+                                hidden_activations_context(k));
+                            nums_data++;
+                            cp_data++;
+                        }
+
+                        // input_i = 0
+                        for( int k=0; k&lt;n_conf; k++)
+                        {
+                            *nums_data = 0;
+                            *cp_data = (1-input_i);
+                            conf_index = k;
+                            ac = hidden_activations_context[n_conf + k];
+                        
+                            w = &amp;(connection-&gt;weights(0,i));
+                            for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                                ac[j] = a[j] - *w * input_i;
+
+                            context_i = context_indices_per_i[i];
+                            for( int l=0; l&lt;pseudolikelihood_context_size; l++ )
+                            {
+                                w = &amp;(connection-&gt;weights(0,*context_i));
+                                input_j = input[*context_i];
+                                for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                                {
+                                    if( conf_index &amp; 1)
+                                    {
+                                        ac[j] -=  *w * ( input_j - 1 );
+                                        *cp_data *= input_j;
+                                    }
+                                    else
+                                    {
+                                        ac[j] -=  *w * input_j;
+                                        *cp_data *= (1-input_j);
+                                    }
+                                }
+                                conf_index &gt;&gt;= 1;
+                                context_i++;
+                            }
+                            *nums_data -= hidden_layer-&gt;freeEnergyContribution(
+                                hidden_activations_context(n_conf + k));
+                            nums_data++;
+                            cp_data++;
+                        }
+                    
+
+                        // Gradient computation
+                        exp( nums_act, nums);
+                        Zi = sum(nums);
+                        log_Zi = pl_log(Zi);
+
+                        nums_data = nums_act.data();
+                        gnums_data = gnums_act.data();
+                        cp_data = context_probs.data();
+
+                        // Compute input_prob gradient
+
+                        // input_i = 1                    
+                        for( int k=0; k&lt;n_conf; k++)
+                        {
+                            pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
+                            *gnums_data = *nums_data/Zi - *cp_data;
+                            *gi += *gnums_data;
+                        
+                            hidden_layer-&gt;freeEnergyContributionGradient(
+                                hidden_activations_context(k),
+                                hidden_activations_context_k_gradient,
+                                -*gnums_data,
+                                false);
+                            hidden_activation_gradient += 
+                                hidden_activations_context_k_gradient;
+                        
+                            gac = hidden_activations_context_k_gradient.data();
+                            gw = &amp;(connection_gradient(0,i));
+                            for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                                *gw -= gac[j] * ( input_i - 1 );
+
+                            context_i = context_indices_per_i[i];
+                            for( int l=0; l&lt;pseudolikelihood_context_size; l++ )
+                            {
+                                gw = &amp;(connection_gradient(0,*context_i));
+                                input_j = input[*context_i];
+                                for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                                {
+                                    if( conf_index &amp; 1)
+                                        *gw -= gac[j] * ( input_j - 1 );
+                                    else
+                                        *gw -= gac[j] * input_j;
+                                }
+                                conf_index &gt;&gt;= 1;
+                                context_i++;
+                            }
+
+                            nums_data++;
+                            gnums_data++;
+                            cp_data++;
+                        }
+
+                        // input_i = 0
+                        for( int k=0; k&lt;n_conf; k++)
+                        {
+                            pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
+                            *gnums_data = *nums_data/Zi - *cp_data;
+                            *gi += *gnums_data;
+                        
+                            hidden_layer-&gt;freeEnergyContributionGradient(
+                                hidden_activations_context(n_conf + k),
+                                hidden_activations_context_k_gradient,
+                                -*gnums_data,
+                                false);
+                            hidden_activation_gradient += 
+                                hidden_activations_context_k_gradient;
+                        
+                            gac = hidden_activations_context_k_gradient.data();
+                            gw = &amp;(connection_gradient(0,i));
+                            for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                                *gw -= gac[j] *input_i;
+
+                            context_i = context_indices_per_i[i];
+                            for( int l=0; l&lt;pseudolikelihood_context_size; l++ )
+                            {
+                                gw = &amp;(connection_gradient(0,*context_i));
+                                input_j = input[*context_i];
+                                for( int j=0; j&lt;hidden_layer-&gt;size; j++,w+=m )
+                                {
+                                    if( conf_index &amp; 1)
+                                        *gw -= gac[j] * ( input_j - 1 );
+                                    else
+                                        *gw -= gac[j] * input_j;
+                                }
+                                conf_index &gt;&gt;= 1;
+                                context_i++;
+                            }
+
+                            nums_data++;
+                            gnums_data++;
+                            cp_data++;
+                        }
+                        gi++;
+                    }
+
+                    externalProductAcc( connection_gradient, hidden_activation_gradient,
+                                        input );
+
+                    // Hidden bias update
+                    multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
+                                      hidden_layer-&gt;bias);
+                    // Connection weights update
+                    multiplyScaledAdd( connection_gradient, 1.0, -lr,
+                                       connection-&gt;weights );
+                    // Input bias update
+                    multiplyScaledAdd(input_gradient, 1.0, -lr,
+                                      input_layer-&gt;bias);
+
+                    // N.B.: train costs contains pseudolikelihood
+                    //       or pseudoNLL, not NLL
+                    train_costs[nll_cost_index] = pseudolikelihood;
+                }
+            }
+            
+            // CD learning
+            if( !fast_is_equal(cd_learning_rate, 0.) )
+            {
+                if( cd_decrease_ct != 0 )
+                    lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                else
+                    lr = cd_learning_rate;
+
+                setLearningRate(lr);
+
+                // Positive phase
+                pos_input = input;
+                connection-&gt;setAsDownInput( input );
+                hidden_layer-&gt;getAllActivations( 
+                    (RBMMatrixConnection*) connection );
+                hidden_layer-&gt;computeExpectation();
+                pos_hidden.resize( hidden_layer-&gt;size );
+                pos_hidden &lt;&lt; hidden_layer-&gt;expectation;
+
+                // Negative phase
+                for(int i=0; i&lt;cd_n_gibbs; i++)
+                {
+                    hidden_layer-&gt;generateSample();
+                    connection-&gt;setAsUpInput( hidden_layer-&gt;sample );
+                    input_layer-&gt;getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    input_layer-&gt;computeExpectation();
+                    input_layer-&gt;generateSample();
+                    connection-&gt;setAsDownInput( input_layer-&gt;sample );
+                    hidden_layer-&gt;getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    hidden_layer-&gt;computeExpectation();
+                }
+                
+                neg_input = input_layer-&gt;sample;
+                neg_hidden = hidden_layer-&gt;expectation;
+
+                input_layer-&gt;update(pos_input,neg_input);
+                hidden_layer-&gt;update(pos_hidden,neg_hidden);
+                connection-&gt;update(pos_input,pos_hidden,
+                                   neg_input,neg_hidden);
+            }
+            
+        }
+        train_stats-&gt;update( train_costs );
+        
+    }
+    
+    train_stats-&gt;finalize();
+}
+
+
+///////////////////
+// computeOutput //
+///////////////////
+void PseudolikelihoodRBM::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    // Compute the output from the input.
+    output.resize(0);
+    if( n_classes &gt; 1 )
+    {
+        // Get output probabilities
+        PLERROR(&quot;n_classes &gt; 1 not implemented yet&quot;);
+    }
+    else
+    {
+        // Get hidden layer representation
+        connection-&gt;setAsDownInput( input );
+        hidden_layer-&gt;getAllActivations( (RBMMatrixConnection *) connection );
+        hidden_layer-&gt;computeExpectation();
+        output &lt;&lt; hidden_layer-&gt;expectation;
+    }
+}
+
+
+void PseudolikelihoodRBM::computeCostsFromOutputs(const Vec&amp; input, 
+                                                  const Vec&amp; output,
+                                                  const Vec&amp; target, 
+                                                  Vec&amp; costs) const
+{
+
+    // Compute the costs from *already* computed output.
+    costs.resize( cost_names.length() );
+    costs.fill( MISSING_VALUE );
+
+    if( n_classes &gt; 1 )
+    {
+        costs[class_cost_index] =
+            (argmax(output) == (int) round(target[0]))? 0 : 1;
+        costs[nll_cost_index] = -pl_log(output[(int) round(target[0])]);
+    }
+    else
+    {        
+        compute_Z();
+        connection-&gt;setAsDownInput( input );
+        hidden_layer-&gt;getAllActivations( (RBMMatrixConnection *) connection );
+        costs[nll_cost_index] = hidden_layer-&gt;freeEnergyContribution(
+            hidden_layer-&gt;activation) + log_Z;
+
+    }
+}
+
+TVec&lt;string&gt; PseudolikelihoodRBM::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    return cost_names;
+}
+
+TVec&lt;string&gt; PseudolikelihoodRBM::getTrainCostNames() const
+{
+    return cost_names;
+}
+
+
+//#####  Helper functions  ##################################################
+
+void PseudolikelihoodRBM::setLearningRate( real the_learning_rate )
+{
+    input_layer-&gt;setLearningRate( the_learning_rate );
+    hidden_layer-&gt;setLearningRate( the_learning_rate );
+    connection-&gt;setLearningRate( the_learning_rate );
+    //target_layer-&gt;setLearningRate( the_learning_rate );
+    //last_to_target-&gt;setLearningRate( the_learning_rate );
+}
+
+void PseudolikelihoodRBM::compute_Z() const
+{
+    if( Z_is_up_to_date ) return;
+
+    int input_n_conf = input_layer-&gt;getConfigurationCount(); 
+    int hidden_n_conf = hidden_layer-&gt;getConfigurationCount();
+    if( input_n_conf == RBMLayer::INFINITE_CONFIGURATIONS &amp;&amp; 
+        hidden_n_conf == RBMLayer::INFINITE_CONFIGURATIONS )
+        PLERROR(&quot;In PseudolikelihoodRBM::computeCostsFromOutputs: &quot;
+                &quot;RBM's input and hidden layers are too big &quot;
+                &quot;for NLL computations.&quot;);
+
+    log_Z = 0;
+    if( input_n_conf &lt; hidden_n_conf )
+    {
+        conf.resize( input_layer-&gt;size );
+        for(int i=0; i&lt;input_n_conf; i++)
+        {
+            input_layer-&gt;getConfiguration(i,conf);
+            connection-&gt;setAsDownInput( conf );
+            hidden_layer-&gt;getAllActivations( (RBMMatrixConnection *) connection );
+            if( i == 0 )
+                log_Z = -hidden_layer-&gt;freeEnergyContribution(
+                    hidden_layer-&gt;activation);
+            else
+                log_Z = logadd(-hidden_layer-&gt;freeEnergyContribution(
+                                   hidden_layer-&gt;activation),
+                               log_Z);
+        }
+    }
+    else
+    {
+        conf.resize( hidden_layer-&gt;size );
+        for(int i=0; i&lt;hidden_n_conf; i++)
+        {
+            hidden_layer-&gt;getConfiguration(i,conf);
+            connection-&gt;setAsUpInput( conf );
+            input_layer-&gt;getAllActivations( (RBMMatrixConnection *) connection );
+            if( i == 0 )
+                log_Z = -input_layer-&gt;freeEnergyContribution(
+                    input_layer-&gt;activation);
+            else
+                log_Z = logadd(-input_layer-&gt;freeEnergyContribution(
+                                   hidden_layer-&gt;activation),
+                               log_Z);
+        }        
+    }
+    
+    Z_is_up_to_date = true;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-04-26 20:37:19 UTC (rev 8903)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-04-26 20:37:58 UTC (rev 8904)
@@ -0,0 +1,292 @@
+// -*- C++ -*-
+
+// PseudolikelihoodRBM.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file PseudolikelihoodRBM.h */
+
+#ifndef PseudolikelihoodRBM_INC
+#define PseudolikelihoodRBM_INC
+
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn_learners/online/CostModule.h&gt;
+#include &lt;plearn_learners/online/CrossEntropyCostModule.h&gt;
+#include &lt;plearn_learners/online/NLLCostModule.h&gt;
+#include &lt;plearn_learners/online/RBMClassificationModule.h&gt;
+#include &lt;plearn_learners/online/RBMMultitaskClassificationModule.h&gt;
+#include &lt;plearn_learners/online/RBMLayer.h&gt;
+#include &lt;plearn_learners/online/RBMMixedLayer.h&gt;
+#include &lt;plearn_learners/online/RBMConnection.h&gt;
+#include &lt;plearn/misc/PTimer.h&gt;
+#include &lt;plearn/sys/Profiler.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Restricted Boltzmann Machine trained by (generalized) pseudolikelihood
+ */
+class PseudolikelihoodRBM : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The learning rate used for pseudolikelihood training
+    real learning_rate;
+
+    //! The decrease constant of the learning rate
+    real decrease_ct;
+
+    //! The learning rate used for contrastive divergence learning
+    real cd_learning_rate;
+
+    //! The decrease constant of the contrastive divergence learning rate
+    real cd_decrease_ct;
+
+    //! Number of negative phase gibbs sampling steps
+    int cd_n_gibbs;
+
+    //! Number of classes in the training set (for supervised learning)
+    int n_classes;
+
+    //! Indication that the input space NLL should be computed
+    //! during test
+    bool compute_input_space_nll;
+
+    //! Number of additional input variables chosen to form the joint
+    //! condition likelihoods in generalized pseudolikelihood
+    //! (default = 0, which corresponds to standard pseudolikelihood)
+    int pseudolikelihood_context_size;
+
+    //! The binomial input layer of the RBM
+    PP&lt;RBMBinomialLayer&gt; input_layer;
+
+    //! The hidden layer of the RBM
+    PP&lt;RBMLayer&gt; hidden_layer;
+
+    //! The connection weights between the input and hidden layer
+    PP&lt;RBMMatrixConnection&gt; connection;
+
+    ////! Target weights' L1_penalty_factor
+    //real target_weights_L1_penalty_factor;
+    //
+    ////! Target weights' L2_penalty_factor
+    //real target_weights_L2_penalty_factor;
+
+    //#####  Public Learnt Options  ###########################################
+    //! The computed cost names
+    TVec&lt;string&gt; cost_names;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    PseudolikelihoodRBM();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+    //                                    Vec&amp; output, Vec&amp; costs) const;
+    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
+    //                               Vec&amp; costs) const;
+    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(PseudolikelihoodRBM);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+
+    //#####  Not Options  #####################################################
+
+    ////! Matrix connection weights between the hidden layer and the target layer
+    ////! (pointer to classification_module-&gt;last_to_target)
+    //PP&lt;RBMMatrixConnection&gt; last_to_target;
+    //
+    ////! Connection weights between the hidden layer and the target layer
+    ////! (pointer to classification_module-&gt;last_to_target)
+    //PP&lt;RBMConnection&gt; last_to_target_connection;
+    //
+    ////! Connection weights between the hidden layer and the visible layer
+    ////! (pointer to classification_module-&gt;joint_connection)
+    //PP&lt;RBMConnection&gt; joint_connection;
+    //
+    ////! Part of the RBM visible layer corresponding to the target
+    ////! (pointer to classification_module-&gt;target_layer)
+    //PP&lt;RBMLayer&gt; target_layer;
+
+    //! Temporary variables for Contrastive Divergence
+    mutable Vec target_one_hot;
+
+    //! Temporary variables for RBM computations
+    mutable Vec input_gradient;
+    mutable Vec class_output;
+    mutable Vec before_class_output;
+    mutable Vec class_gradient;
+    mutable Vec hidden_activation_pos_i;
+    mutable Vec hidden_activation_neg_i;
+    mutable Vec hidden_activation_gradient;
+    mutable Vec hidden_activation_pos_i_gradient;
+    mutable Vec hidden_activation_neg_i_gradient;
+    mutable Mat connection_gradient;
+    mutable TVec&lt;int&gt; context_indices;
+    mutable TMat&lt;int&gt; context_indices_per_i;
+    mutable Mat hidden_activations_context;
+    mutable Vec hidden_activations_context_k_gradient;
+    mutable Vec nums;
+    mutable Vec nums_act;
+    mutable Vec context_probs;
+    mutable Vec gnums_act;
+    mutable Vec conf;
+    mutable Vec pos_input;
+    mutable Vec pos_hidden;
+    mutable Vec neg_input;
+    mutable Vec neg_hidden;
+
+    //! Keeps the index of the NLL cost in train_costs
+    int nll_cost_index;
+
+    //! Keeps the index of the class_error cost in train_costs
+    int class_cost_index;
+
+    //! Normalisation constant (on log scale)
+    mutable real log_Z;
+
+    // Indication that the normalisation constant Z is up to date
+    mutable bool Z_is_up_to_date;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_costs();
+
+    void setLearningRate( real the_learning_rate );
+
+    void compute_Z() const;
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PseudolikelihoodRBM);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002351.html">[Plearn-commits] r8903 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="002353.html">[Plearn-commits] r8905 - in	trunk/plearn_learners/generic/test/NNet: . .pytest	.pytest/PL_NNet_1_hidden_bug	.pytest/PL_NNet_1_hidden_bug/expected_results	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/Split0	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/global_stats.pmat.metadata	.pytest/PL_NNet_1_hidden_bug/expected_results/expdir-nnet/split_stats.pmat.metadata
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2352">[ date ]</a>
              <a href="thread.html#2352">[ thread ]</a>
              <a href="subject.html#2352">[ subject ]</a>
              <a href="author.html#2352">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
