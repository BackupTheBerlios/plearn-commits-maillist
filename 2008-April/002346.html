<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8898 - trunk/python_modules/plearn/learners
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-April/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8898%20-%20trunk/python_modules/plearn/learners&In-Reply-To=%3C200804252016.m3PKGYVm029283%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002345.html">
   <LINK REL="Next"  HREF="002347.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8898 - trunk/python_modules/plearn/learners</H1>
    <B>louradou at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8898%20-%20trunk/python_modules/plearn/learners&In-Reply-To=%3C200804252016.m3PKGYVm029283%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8898 - trunk/python_modules/plearn/learners">louradou at mail.berlios.de
       </A><BR>
    <I>Fri Apr 25 22:16:34 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002345.html">[Plearn-commits] r8897 - trunk/plearn_learners/hyper
</A></li>
        <LI>Next message: <A HREF="002347.html">[Plearn-commits] r8899 - trunk/plearn_learners/hyper
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2346">[ date ]</a>
              <a href="thread.html#2346">[ thread ]</a>
              <a href="subject.html#2346">[ subject ]</a>
              <a href="author.html#2346">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: louradou
Date: 2008-04-25 22:16:34 +0200 (Fri, 25 Apr 2008)
New Revision: 8898

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
* added options min_cost and max_cost to possibly
  end the hyper-optimization prematurely given the
  measured costs.
* a little clean up of the code.



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-04-25 18:18:23 UTC (rev 8897)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-04-25 20:16:34 UTC (rev 8898)
@@ -620,21 +620,30 @@
                              (valid_samples = None), were the model is re-trained
                              in any case.
 
+        'test_on_train': &lt;bool&gt; Should we test best models on the train set.
+
         'retrain_until_local_optimum_is_found': &lt;bool&gt; when calling run(), whether or
                          not to continue to tune hyperparameters until a local optimum
                          is found. If False, you can re-run run() several times. If True,
                          you can also re-run to possibly find better performance.
 
-        'max_ntrials': &lt;int&gt; when calling run(), maximum number of hyperparameters to try
-                      since the last forget().
-
-        'test_on_train': &lt;bool&gt; Should we test best models on {test, train} (1)
-
         'testlevel': &lt;int&gt; Frequency of test:
                      - 0: write results only at the end of the run()
                      - 1: write results each time a better validation cost is found in run()
                      - 2: write all intermediate results
 
+        'max_ntrials': &lt;int&gt; when calling run(), maximum number of hyperparameters to try
+                      since the last forget(). If set to 1, then no hyperoptimization will be performed
+                      (first arbitrary hyperaparameters will be chosen, this is recommended for debugging only)
+                      
+        'min_cost': &lt;float&gt; minimum sufficient cost value. If this value is reached then the hyperoptimization
+                    will stop immediately. Recommended for speed-up when the optimization is easy.
+                    
+        'max_cost': &lt;float&gt; maximum admissible cost value. If this value is not reached during the first set
+                    of hyperoptimization (e.g. 9 trials with a RBF kernel), then the hyperoptimization will stop.
+                    Recommended when you test SVM with different front-end and you suspect some front-ends to be
+                    simply bad ones (you do not want to waste time with them).
+
         'results_filename': &lt;string&gt; Path to an output file for results
         
         'preproc_optionnames': &lt;string&gt; or &lt;list of strings&gt; indicating the names of the
@@ -700,6 +709,8 @@
                         'outputs_type',
                         'retrain_until_local_optimum_is_found',
                         'max_ntrials',
+                        'min_cost',
+                        'max_cost',
                         'retrain_on_valid',
                         'test_on_train',
                         'testlevel',
@@ -778,8 +789,11 @@
 
         self.retrain_on_valid = True
         self.retrain_until_local_optimum_is_found = True
+
+        self.test_on_train = False
         self.max_ntrials = 50
-        self.test_on_train = False
+        self.min_cost = None
+        self.max_cost = None
         
         self.verbosity = 0
         self.testlevel = 1
@@ -823,12 +837,6 @@
             return None
         return dataspec[ self.testset_key ]
 
-    def additional_preproc(self, input_vmat, isTrain=False):
-        if self.balance_classes and isTrain:
-            return ReplicateSamplesVMatrix(source = input_vmat,
-                                           operate_on_bags = (input_vmat.targetsize &gt; 1 ))
-        return input_vmat
-
     ## specific to libsvm
     &quot;&quot;&quot; Return samples and targets in the format required
         by libsvm, i.e. lists of float.
@@ -855,7 +863,9 @@
         return samples, targets
 
     def get_svminputlist(self, input_vmat, isTrain=False):
-        input_vmat = self.additional_preproc( input_vmat, isTrain )
+        if self.balance_classes and isTrain:
+            input_vmat = ReplicateSamplesVMatrix(source = input_vmat,
+                                                 operate_on_bags = (input_vmat.targetsize &gt; 1 ) )
         samples, targets = self.get_datalist( input_vmat )
         if self.normalize_inputs:
             if self.input_means == None:
@@ -906,8 +916,8 @@
         self.class_priors = class_priors
 
         if self.verbosity &gt; 0:
-            print &quot;  ( class priors: %s  -- %d samples)&quot; % \
-                     ( class_priors, len(targets) )
+            print &quot;  ( class priors: %s  -- %d samples of dim %d )&quot; % \
+                     ( class_priors, len(targets), len(samples[0]) )
 
         if self.balanceC:
             weight = [ 1./p for p in class_priors.values() ]
@@ -1147,15 +1157,18 @@
              ):
         if self.verbosity &gt; 3:
             print &quot;SVM::train() called &quot;, dataspec.keys()
-        if self.verbosity &gt; 1:
-            print &quot;launching libsvm with param %s &quot; % param    
         
         if param == None:
             if not self.best_param:
+                if self.verbosity &gt; 0:
+                    print &quot;WARNING: train() called without hyper-parameters &quot;+\
+                          &quot;and no best hyperparameters found =&gt; run() called&quot;
                 return self.run(dataspec)
             param = self.best_param.copy()
         elif 'kernel_type' not in param:
             param['kernel_type'] = self.kernel_type
+        if self.verbosity &gt; 1:
+            print &quot;launching libsvm with param %s &quot; % param    
         
         trainset = self.train_inputspec(dataspec)
         self.get_data_stats( trainset )
@@ -1454,17 +1467,16 @@
         validset = self.valid_inputspec(dataspec)
         testset  = self.test_inputspec(dataspec)
         # Cross Validation
-        if 'fold' in self.validtype:
+        if self.validset_key not in dataspec:
+            if self.verbosity &gt; 0:
+                print &quot;\n** training model on entire train&quot;
             self.train( dataspec )
 
         # Simple Validation
         else:
-            self.validtype = 'simple'
-            # CAUTION: in the case of simple validation without retraining on {train+valid},
-            #          self.best_model is supposed to be updated
             if self.retrain_on_valid:
                 &quot;&quot;&quot; Uncomment following lines if you want to check that
-                    retraining on {train + valid} sets does not degrade.
+                    retraining on {train + valid} sets does not degrade test performance.
                 &quot;&quot;&quot;
                 #train_stats = None
                 #test_stats = None
@@ -1490,6 +1502,12 @@
                     print &quot;\n** re-training model on { train + valid } &quot;
                 self.train( {self.trainset_key: tv_set} )
 
+            # CAUTION: in the case of simple validation without retraining on {train+valid},
+            #          self.best_model is supposed to be up-to-date or None
+            elif self.best_model == None:
+                self.validtype = 'simple'
+                self.train( dataspec )
+
         train_stats = None
         test_stats = None
         if self.test_on_train:
@@ -1504,8 +1522,8 @@
                             None, test_stats, train_stats )
         self.write_results( self.best_param,
                             self.valid_stats, self.test_stats, self.train_stats )
+        return dataspec
 
-
     &quot;&quot;&quot; THE interesting function of the class.
         See __main__ below for usage.
         dataspec is a dictionary which specifies train, valid, test sets.
@@ -1513,6 +1531,8 @@
         cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     &quot;&quot;&quot;
     def run(self, dataspec):
+        assert self.testlevel &gt;= 0
+        assert self.max_ntrials &gt; 0
         trainset = self.train_inputspec(dataspec)
         validset = self.valid_inputspec(dataspec)
         testset  = self.test_inputspec(dataspec)
@@ -1531,8 +1551,14 @@
         else:
             param_to_try = expert.choose_new_param()
         
+        local_retrain_until_local_optimum_is_found = True
         for param in param_to_try:
 
+            if self.max_ntrials == 1: # No hyper-optimization (debug)
+                self.best_param = param
+                self.validset = None
+                return self.retrain_and_writeresults(dataspec)
+            
             valid_stats = self.valid(dataspec, param)
 
             # No improvement measured
@@ -1544,9 +1570,8 @@
 
             # Better valid cost is obtained!
             else:
-                print &quot;better cost was found!&quot;
                 # Simple Validation
-                if 'fold' not in self.validtype:
+                if self.validset_key in dataspec:
                     self.best_model = self.model
                     
                 if self.testlevel &gt; 0:
@@ -1554,15 +1579,19 @@
                 else:
                     self.write_results( param, valid_stats, None, None, True  )
 
-            if len(expert.trials_param_list)-L0 &gt;= self.max_ntrials:
-                return dataspec
+            if( len(expert.trials_param_list)-L0 &gt;= self.max_ntrials
+            or  ( self.min_cost &lt;&gt; None and expert.best_cost &lt;= self.min_cost ) ):
+                local_retrain_until_local_optimum_is_found = False
+                break
 
         if( self.retrain_until_local_optimum_is_found
-        and expert.should_be_tuned_again() ):
+        and local_retrain_until_local_optimum_is_found
+        and expert.should_be_tuned_again()
+        and ( self.max_cost == None or expert.best_cost &lt;= self.max_cost ) ):
            return self.run( dataspec )
 
         if self.testlevel == 0:
-             self.retrain_and_writeresults(dataspec)
+             return self.retrain_and_writeresults(dataspec)
 
         return dataspec
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002345.html">[Plearn-commits] r8897 - trunk/plearn_learners/hyper
</A></li>
	<LI>Next message: <A HREF="002347.html">[Plearn-commits] r8899 - trunk/plearn_learners/hyper
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2346">[ date ]</a>
              <a href="thread.html#2346">[ thread ]</a>
              <a href="subject.html#2346">[ subject ]</a>
              <a href="author.html#2346">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
