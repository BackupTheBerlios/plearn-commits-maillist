<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6570 - in tags: . OPAL-3.0.3/plearn/ker	OPAL-3.0.3/plearn_learners/generic	OPAL-3.0.3/plearn_learners/regressors
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6570%20-%20in%20tags%3A%20.%20OPAL-3.0.3/plearn/ker%0A%09OPAL-3.0.3/plearn_learners/generic%0A%09OPAL-3.0.3/plearn_learners/regressors&In-Reply-To=%3C200701121544.l0CFi98C010145%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000018.html">
   <LINK REL="Next"  HREF="000020.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6570 - in tags: . OPAL-3.0.3/plearn/ker	OPAL-3.0.3/plearn_learners/generic	OPAL-3.0.3/plearn_learners/regressors</H1>
    <B>ducharme at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6570%20-%20in%20tags%3A%20.%20OPAL-3.0.3/plearn/ker%0A%09OPAL-3.0.3/plearn_learners/generic%0A%09OPAL-3.0.3/plearn_learners/regressors&In-Reply-To=%3C200701121544.l0CFi98C010145%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6570 - in tags: . OPAL-3.0.3/plearn/ker	OPAL-3.0.3/plearn_learners/generic	OPAL-3.0.3/plearn_learners/regressors">ducharme at mail.berlios.de
       </A><BR>
    <I>Fri Jan 12 16:44:09 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000018.html">[Plearn-commits] r6569 - in trunk: plearn/ker	plearn_learners/generic plearn_learners/regressors
</A></li>
        <LI>Next message: <A HREF="000020.html">[Plearn-commits] r6571 - in tags: . OPAL-3.0.3.1/plearn/ker	OPAL-3.0.3.1/plearn_learners/generic	OPAL-3.0.3.1/plearn_learners/regressors
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#19">[ date ]</a>
              <a href="thread.html#19">[ thread ]</a>
              <a href="subject.html#19">[ subject ]</a>
              <a href="author.html#19">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: ducharme
Date: 2007-01-12 16:44:07 +0100 (Fri, 12 Jan 2007)
New Revision: 6570

Added:
   tags/OPAL-3.0.3/
   tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc
   tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc
   tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc
Removed:
   tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc
   tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc
   tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Tag pour release OPAL 3.0.3

Copied: tags/OPAL-3.0.3 (from rev 6568, trunk)

Deleted: tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc	2007-01-12 15:44:07 UTC (rev 6570)
@@ -1,169 +0,0 @@
-// -*- C++ -*-
-
-// SummationKernel.cc
-//
-// Copyright (C) 2007 Nicolas Chapados
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Nicolas Chapados
-
-/*! \file SummationKernel.cc */
-
-
-#include &quot;SummationKernel.h&quot;
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    SummationKernel,
-    &quot;Kernel computing the sum of other kernels&quot;,
-    &quot;This kernel computes the summation of several subkernel objects.  It can\n&quot;
-    &quot;also chop up parts of its input vector and send it to each kernel (so that\n&quot;
-    &quot;each kernel can operate on a subset of the variables).\n&quot;
-    );
-
-
-//#####  Constructor  #########################################################
-
-SummationKernel::SummationKernel()
-{ }
-
-
-//#####  declareOptions  ######################################################
-
-void SummationKernel::declareOptions(OptionList&amp; ol)
-{
-    declareOption(
-        ol, &quot;terms&quot;, &amp;SummationKernel::m_terms, OptionBase::buildoption,
-        &quot;Individual kernels to add to produce the final result.  The\n&quot;
-        &quot;hyperparameters of kernel i can be accesed under the option names\n&quot;
-        &quot;'terms[i].hyperparam' for, e.g. GaussianProcessRegressor.\n&quot;);
-
-    declareOption(
-        ol, &quot;input_indexes&quot;, &amp;SummationKernel::m_input_indexes,
-        OptionBase::buildoption,
-        &quot;Optionally, one can specify which of individual input variables should\n&quot;
-        &quot;be routed to each kernel.  The format is as a vector of vectors: for\n&quot;
-        &quot;each kernel in 'terms', one must list the INDEXES in the original input\n&quot;
-        &quot;vector(zero-based) that should be passed to that kernel.  If a list of\n&quot;
-        &quot;indexes is empty for a given kernel, it means that the COMPLETE input\n&quot;
-        &quot;vector should be passed to the kernel.\n&quot;);
-    
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-
-//#####  build  ###############################################################
-
-void SummationKernel::build()
-{
-    // ### Nothing to add here, simply calls build_
-    inherited::build();
-    build_();
-}
-
-
-//#####  build_  ##############################################################
-
-void SummationKernel::build_()
-{
-    // Preallocate buffers for kernel evaluation
-    const int N = m_input_indexes.size();
-    m_input_buf1.resize(N);
-    m_input_buf2.resize(N);
-    for (int i=0 ; i&lt;N ; ++i) {
-        const int M = m_input_indexes[i].size();
-        m_input_buf1[i].resize(M);
-        m_input_buf2[i].resize(M);
-    }
-
-    // Kernel is symmetric only if all terms are
-    is_symmetric = true;
-    for (int i=0, n=m_terms.size() ; i&lt;n ; ++i) {
-        if (! m_terms[i])
-            PLERROR(&quot;SummationKernel::build_: kernel for term[%d] is not specified&quot;,i);
-        is_symmetric = is_symmetric &amp;&amp; m_terms[i]-&gt;is_symmetric
-    }
-
-    if (m_input_indexes.size() &gt; 0 &amp;&amp; m_terms.size() != m_input_indexes.size())
-        PLERROR(&quot;SummationKernel::build_: if 'input_indexes' is specified &quot;
-                &quot;it must have the same size (%d) as 'terms'; found %d elements&quot;,
-                m_terms.size(), m_input_indexes.size());
-}
-
-
-//#####  evaluate  ############################################################
-
-real SummationKernel::evaluate(const Vec&amp; x1, const Vec&amp; x2) const
-{
-    real kernel_value = 0.0;
-    bool split_inputs = m_input_indexes.size() &gt; 0;
-    for (int i=0, n=m_terms.size() ; i&lt;n ; ++i) {
-        if (split_inputs &amp;&amp; m_input_indexes[i].size() &gt; 0) {
-            selectElements(x1, m_input_indexes[i], m_input_buf1[i]);
-            selectElements(x2, m_input_indexes[i], m_input_buf2[i]);
-            kernel_value += m_terms[i]-&gt;evaluate(m_input_buf1[i],
-                                                 m_input_buf2[i]);
-        }
-        else
-            kernel_value += m_terms[i]-&gt;evaluate(x1,x2);
-    }
-    return kernel_value;
-}
-
-
-//#####  makeDeepCopyFromShallowCopy  #########################################
-
-void SummationKernel::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(m_terms,          copies);
-    deepCopyField(m_input_indexes,  copies);
-    deepCopyField(m_input_buf1,     copies);
-    deepCopyField(m_input_buf2,     copies);
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:&quot;stroustrup&quot;
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc (from rev 6569, trunk/plearn/ker/SummationKernel.cc)

Deleted: tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc	2007-01-12 15:44:07 UTC (rev 6570)
@@ -1,1016 +0,0 @@
-// -*- C++ -*-
-
-// PLearner.cc
-//
-// Copyright (C) 1998-2002 Pascal Vincent
-// Copyright (C) 1999-2002 Yoshua Bengio, Nicolas Chapados, Charles Dugas, Rejean Ducharme, Universite de Montreal
-// Copyright (C) 2001,2002 Francis Pieraut, Jean-Sebastien Senecal
-// Copyright (C) 2002 Frederic Morin, Xavier Saint-Mleux, Julien Keable
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-
- 
-
-/* *******************************************************      
- * $Id$
- ******************************************************* */
-
-#include &quot;PLearner.h&quot;
-#include &lt;plearn/base/stringutils.h&gt;
-#include &lt;plearn/io/fileutils.h&gt;
-#include &lt;plearn/io/pl_log.h&gt;
-#include &lt;plearn/math/pl_erf.h&gt;
-#include &lt;plearn/vmat/FileVMatrix.h&gt;
-#include &lt;plearn/misc/PLearnService.h&gt;
-#include &lt;plearn/misc/RemotePLearnServer.h&gt;
-#include &lt;plearn/vmat/PLearnerOutputVMatrix.h&gt;
-
-namespace PLearn {
-using namespace std;
-
-PLearner::PLearner()
-    : n_train_costs_(-1),
-      n_test_costs_(-1),
-      seed_(1827),                           //!&lt; R.I.P. L.v.B.
-      stage(0),
-      nstages(1),
-      report_progress(true),
-      verbosity(1),
-      nservers(0),
-      save_trainingset_prefix(&quot;&quot;),
-      inputsize_(-1),
-      targetsize_(-1),
-      weightsize_(-1),
-      n_examples(-1),
-      forget_when_training_set_changes(false)  
-{}
-
-PLEARN_IMPLEMENT_ABSTRACT_OBJECT(
-    PLearner,
-    &quot;The base class for all PLearn learning algorithms&quot;,
-    &quot;PLearner provides a base class for all learning algorithms within PLearn.\n&quot;
-    &quot;It presents an abstraction of learning that centers around a \&quot;train-test\&quot;\n&quot;
-    &quot;paradigm:\n&quot;
-    &quot;\n&quot;
-    &quot;- Phase 1: TRAINING.  In this phase, one must first establish an experiment\n&quot;
-    &quot;  directory (usually done by an enclosing PTester) to store any temporary\n&quot;
-    &quot;  files that the learner might seek to create.  Then, one sets a training\n&quot;
-    &quot;  set VMat (also done by the enclosing PTester), which contains the set of\n&quot;
-    &quot;  input-target pairs that the learner should attempt to represent.  Finally\n&quot;
-    &quot;  one calls the train() virtual member function to carry out the actual\n&quot;
-    &quot;  action of training the model.\n&quot;
-    &quot;\n&quot;
-    &quot;- Phase 2: TESTING.  In this phase (to be done after training), one\n&quot;
-    &quot;  repeatedly calls functions from the computeOutput() family to evaluate\n&quot;
-    &quot;  the trained model on new input vectors.\n&quot;
-    &quot;\n&quot;
-    &quot;Note that the PTester class is the usual \&quot;driver\&quot; for a PLearner (and\n&quot;
-    &quot;automatically calls the above functions in the appropriate order), in the\n&quot;
-    &quot;usual scenario wherein one wants to evaluate the generalization performance\n&quot;
-    &quot;on a dataset.\n&quot;
-    );
-
-void PLearner::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(tmp_output, copies);
-    // TODO What's wrong with this?
-    deepCopyField(train_set, copies);
-    deepCopyField(validation_set, copies);
-    deepCopyField(train_stats, copies);
-    deepCopyField(random_gen, copies);
-}
-
-void PLearner::declareOptions(OptionList&amp; ol)
-{
-    declareOption(
-        ol, &quot;expdir&quot;, &amp;PLearner::expdir, OptionBase::buildoption | OptionBase::nosave, 
-        &quot;Path of the directory associated with this learner, in which\n&quot;
-        &quot;it should save any file it wishes to create. \n&quot;
-        &quot;The directory will be created if it does not already exist.\n&quot;
-        &quot;If expdir is the empty string (the default), then the learner \n&quot;
-        &quot;should not create *any* file. Note that, anyway, most file creation and \n&quot;
-        &quot;reporting are handled at the level of the PTester class rather than \n&quot;
-        &quot;at the learner's. \n&quot;);
-
-    declareOption(
-        ol, &quot;seed&quot;, &amp;PLearner::seed_, OptionBase::buildoption, 
-        &quot;The initial seed for the random number generator used in this\n&quot;
-        &quot;learner, for instance for parameter initialization.\n&quot;
-        &quot;If -1 is provided, then a 'random' seed is chosen based on time\n&quot;
-        &quot;of day, ensuring that different experiments run differently.\n&quot;
-        &quot;If 0 is provided, no (re)initialization of the random number\n&quot;
-        &quot;generator is performed.\n&quot;
-        &quot;With a given positive seed, build() and forget() should always\n&quot;
-        &quot;initialize the parameters to the same values.&quot;);
-
-    declareOption(
-        ol, &quot;stage&quot;, &amp;PLearner::stage, OptionBase::learntoption, 
-        &quot;The current training stage, since last fresh initialization (forget()): \n&quot;
-        &quot;0 means untrained, n often means after n epochs or optimization steps, etc...\n&quot;
-        &quot;The true meaning is learner-dependant.&quot;
-        &quot;You should never modify this option directly!&quot;
-        &quot;It is the role of forget() to bring it back to 0,\n&quot;
-        &quot;and the role of train() to bring it up to 'nstages'...&quot;);
-
-    declareOption(
-        ol, &quot;n_examples&quot;, &amp;PLearner::n_examples, OptionBase::learntoption, 
-        &quot;The number of samples in the training set.\n&quot;
-        &quot;Obtained from training set with setTrainingSet.&quot;);
-
-    declareOption(
-        ol, &quot;inputsize&quot;, &amp;PLearner::inputsize_, OptionBase::learntoption, 
-        &quot;The number of input columns in the data sets.&quot;
-        &quot;Obtained from training set with setTrainingSet.&quot;);
-
-    declareOption(
-        ol, &quot;targetsize&quot;, &amp;PLearner::targetsize_, OptionBase::learntoption, 
-        &quot;The number of target columns in the data sets.&quot;
-        &quot;Obtained from training set with setTrainingSet.&quot;);
-
-    declareOption(
-        ol, &quot;weightsize&quot;, &amp;PLearner::weightsize_, OptionBase::learntoption, 
-        &quot;The number of cost weight columns in the data sets.&quot;
-        &quot;Obtained from training set with setTrainingSet.&quot;);
-
-    declareOption(
-        ol, &quot;forget_when_training_set_changes&quot;,
-        &amp;PLearner::forget_when_training_set_changes, OptionBase::buildoption, 
-        &quot;Whether or not to call the forget() method (re-initialize model \n&quot;
-        &quot;as before training) in setTrainingSet when the\n&quot;
-        &quot;training set changes (e.g. of dimension).&quot;);
-
-    declareOption(
-        ol, &quot;nstages&quot;, &amp;PLearner::nstages, OptionBase::buildoption, 
-        &quot;The stage until which train() should train this learner and return.\n&quot;
-        &quot;The meaning of 'stage' is learner-dependent, but for learners whose \n&quot;
-        &quot;training is incremental (such as involving incremental optimization), \n&quot;
-        &quot;it is typically synonym with the number of 'epochs', i.e. the number \n&quot;
-        &quot;of passages of the optimization process through the whole training set, \n&quot;
-        &quot;since the last fresh initialisation.&quot;);
-
-    declareOption(
-        ol, &quot;report_progress&quot;, &amp;PLearner::report_progress, OptionBase::buildoption, 
-        &quot;should progress in learning and testing be reported in a ProgressBar.\n&quot;);
-
-    declareOption(
-        ol, &quot;verbosity&quot;, &amp;PLearner::verbosity, OptionBase::buildoption, 
-        &quot;Level of verbosity. If 0 should not write anything on perr. \n&quot;
-        &quot;If &gt;0 may write some info on the steps performed along the way.\n&quot;
-        &quot;The level of details written should depend on this value.&quot;);
-
-    declareOption(
-        ol, &quot;nservers&quot;, &amp;PLearner::nservers, OptionBase::buildoption, 
-        &quot;Max number of computation servers to use in parallel with the main process.\n&quot;
-        &quot;If &lt;=0 no parallelization will occur at this level.\n&quot;);
-
-    declareOption(
-        ol, &quot;save_trainingset_prefix&quot;, &amp;PLearner::save_trainingset_prefix,
-        OptionBase::buildoption,
-        &quot;Whether the training set should be saved upon a call to\n&quot;
-        &quot;setTrainingSet().  The saved file is put in the learner's expdir\n&quot;
-        &quot;(assuming there is one) and has the form \&quot;&lt;prefix&gt;_trainset_XXX.pmat\&quot;\n&quot;
-        &quot;The prefix is what this option specifies.  'XXX' is a unique\n&quot;
-        &quot;serial number that is globally incremented with each saved\n&quot;
-        &quot;setTrainingSet.  This option is useful when manipulating very\n&quot;
-        &quot;complex nested learner structures, and you want to ensure that\n&quot;
-        &quot;the inner learner is getting the correct results.  (Default=&quot;&quot;,\n&quot;
-        &quot;i.e. don't save anything.)\n&quot;);
-  
-    inherited::declareOptions(ol);
-}
-
-void PLearner::declareMethods(RemoteMethodMap&amp; rmm)
-{
-    // Insert a backpointer to remote methods; note that this
-    // different than for declareOptions()
-    rmm.inherited(inherited::_getRemoteMethodMap_());
-
-    declareMethod(
-        rmm, &quot;setTrainingSet&quot;, &amp;PLearner::setTrainingSet,
-        (BodyDoc(&quot;Declares the training set.  Then calls build() and forget() if\n&quot;
-                 &quot;necessary.\n&quot;),
-         ArgDoc (&quot;training_set&quot;, &quot;The training set VMatrix to set; should have\n&quot;
-                 &quot;its inputsize, targetsize and weightsize fields set properly.\n&quot;),
-         ArgDoc (&quot;call_forget&quot;, &quot;Whether the forget() function should be called\n&quot;
-                 &quot;upon setting the training set\n&quot;)));
-
-    declareMethod(
-        rmm, &quot;setExperimentDirectory&quot;, &amp;PLearner::setExperimentDirectory,
-        (BodyDoc(&quot;The experiment directory is the directory in which files related to\n&quot;
-                 &quot;this model are to be saved.  If it is an empty string, it is understood\n&quot;
-                 &quot;to mean that the user doesn't want any file created by this learner.\n&quot;),
-         ArgDoc (&quot;expdir&quot;, &quot;Experiment directory to set&quot;)));
-
-    declareMethod(
-        rmm, &quot;getExperimentDirectory&quot;, &amp;PLearner::getExperimentDirectory,
-        (BodyDoc(&quot;This returns the currently set experiment directory\n&quot;
-                 &quot;(see setExperimentDirectory)\n&quot;),
-         RetDoc (&quot;Current experiment directory&quot;)));
-
-    declareMethod(
-        rmm, &quot;forget&quot;, &amp;PLearner::forget,
-        (BodyDoc(&quot;(Re-)initializes the PLearner in its fresh state (that state may depend\n&quot;
-                 &quot;on the 'seed' option) and sets 'stage' back to 0 (this is the stage of\n&quot;
-                 &quot;a fresh learner!)\n&quot;
-                 &quot;\n&quot;
-                 &quot;A typical forget() method should do the following:\n&quot;
-                 &quot;\n&quot;
-                 &quot;- call inherited::forget() to initialize the random number generator\n&quot;
-                 &quot;  with the 'seed' option\n&quot;
-                 &quot;\n&quot;
-                 &quot;- initialize the learner's parameters, using this random generator\n&quot;
-                 &quot;\n&quot;
-                 &quot;- stage = 0;\n&quot;
-                 &quot;\n&quot;
-                 &quot;This method is typically called by the build_() method, after it has\n&quot;
-                 &quot;finished setting up the parameters, and if it deemed useful to set or\n&quot;
-                 &quot;reset the learner in its fresh state.  (remember build may be called\n&quot;
-                 &quot;after modifying options that do not necessarily require the learner to\n&quot;
-                 &quot;restart from a fresh state...)  forget is also called by the\n&quot;
-                 &quot;setTrainingSet method, after calling build(), so it will generally be\n&quot;
-                 &quot;called TWICE during setTrainingSet!\n&quot;)));
-
-    declareMethod(
-        rmm, &quot;train&quot;, &amp;PLearner::train,
-        (BodyDoc(&quot;The role of the train method is to bring the learner up to\n&quot;
-                 &quot;stage==nstages, updating the stats with training costs measured on-line\n&quot;
-                 &quot;in the process.\n&quot;)));
-
-    declareMethod(
-        rmm, &quot;resetInternalState&quot;, &amp;PLearner::resetInternalState,
-        (BodyDoc(&quot;If the learner is a stateful one (inherits from StatefulLearner),\n&quot;
-                 &quot;this resets the internal state to its initial value; by default,\n&quot;
-                 &quot;this function does nothing.&quot;)));
-
-    declareMethod(
-        rmm, &quot;computeOutput&quot;, &amp;PLearner::remote_computeOutput,
-        (BodyDoc(&quot;On a trained learner, this computes the output from the input&quot;),
-         ArgDoc (&quot;input&quot;, &quot;Input vector (should have width inputsize)&quot;),
-         RetDoc (&quot;Computed output (will have width outputsize)&quot;)));
-
-    declareMethod(
-        rmm, &quot;use&quot;, &amp;PLearner::remote_use,
-        (BodyDoc(&quot;Compute the output of a trained learner on every row of an\n&quot;
-                 &quot;input VMatrix.  The outputs are stored in a .pmat matrix\n&quot;
-                 &quot;under the specified filename.&quot;),
-         ArgDoc (&quot;input_vmat&quot;, &quot;VMatrix containing the inputs&quot;),
-         ArgDoc (&quot;output_pmat_fname&quot;, &quot;Name of the .pmat to store the computed outputs&quot;)));
-
-    declareMethod(
-        rmm, &quot;use2&quot;, &amp;PLearner::remote_use2,
-        (BodyDoc(&quot;Compute the output of a trained learner on every row of an\n&quot;
-                 &quot;input VMatrix.  The outputs are returned as a matrix.\n&quot;),
-         ArgDoc (&quot;input_vmat&quot;, &quot;VMatrix containing the inputs&quot;),
-         RetDoc (&quot;Matrix holding the computed outputs&quot;)));
-
-    declareMethod(
-        rmm, &quot;computeInputOutputMat&quot;, &amp;PLearner::computeInputOutputMat,
-        (BodyDoc(&quot;Returns a matrix which is a (horizontal) concatenation\n&quot;
-                 &quot;and the computed outputs.\n&quot;),
-         ArgDoc (&quot;inputs&quot;, &quot;VMatrix containing the inputs&quot;),
-         RetDoc (&quot;Matrix holding the inputs+computed_outputs&quot;)));
-
-    declareMethod(
-        rmm, &quot;computeInputOutputConfMat&quot;, &amp;PLearner::computeInputOutputConfMat,
-        (BodyDoc(&quot;Return a Mat that is the contatenation of inputs, outputs, lower\n&quot;
-                 &quot;confidence bound, and upper confidence bound.  If confidence intervals\n&quot;
-                 &quot;cannot be computed for the learner, they are filled with MISSING_VALUE.\n&quot;),
-         ArgDoc (&quot;inputs&quot;, &quot;VMatrix containing the inputs&quot;),
-         ArgDoc (&quot;probability&quot;, &quot;Level at which the confidence intervals should be computed, &quot;
-                                &quot;e.g. 0.95.&quot;),
-         RetDoc (&quot;Matrix holding the inputs+outputs+confidence-low+confidence-high&quot;)));
-
-    declareMethod(
-        rmm, &quot;computeOutputAndCosts&quot;, &amp;PLearner::remote_computeOutputAndCosts,
-        (BodyDoc(&quot;Compute both the output from the input, and the costs associated\n&quot;
-                 &quot;with the desired target.  The computed costs\n&quot;
-                 &quot;are returned in the order given by getTestCostNames()\n&quot;),
-         ArgDoc (&quot;input&quot;,  &quot;Input vector (should have width inputsize)&quot;),
-         ArgDoc (&quot;target&quot;, &quot;Target vector (for cost computation)&quot;),
-         RetDoc (&quot;- Vec containing output \n&quot;
-                 &quot;- Vec containing cost&quot;)));
-
-    declareMethod(
-        rmm, &quot;computeCostsFromOutputs&quot;, &amp;PLearner::remote_computeCostsFromOutputs,
-        (BodyDoc(&quot;Compute the costs from already-computed output.  The computed costs\n&quot;
-                 &quot;are returned in the order given by getTestCostNames()&quot;),
-         ArgDoc (&quot;input&quot;,  &quot;Input vector (should have width inputsize)&quot;),
-         ArgDoc (&quot;output&quot;, &quot;Output vector computed by previous call to computeOutput()&quot;),
-         ArgDoc (&quot;target&quot;, &quot;Target vector&quot;),
-         RetDoc (&quot;The computed costs vector&quot;)));
-
-    declareMethod(
-        rmm, &quot;computeCostsOnly&quot;, &amp;PLearner::remote_computeCostsOnly,
-        (BodyDoc(&quot;Compute the costs only, without the outputs; for some learners, this\n&quot;
-                 &quot;may be more efficient than calling computeOutputAndCosts() if the\n&quot;
-                 &quot;outputs are not needed.  (The default implementation simply calls\n&quot;
-                 &quot;computeOutputAndCosts() and discards the output.)\n&quot;),
-         ArgDoc (&quot;input&quot;,  &quot;Input vector (should have width inputsize)&quot;),
-         ArgDoc (&quot;target&quot;, &quot;Target vector&quot;),
-         RetDoc (&quot;The computed costs vector&quot;)));
-
-    declareMethod(
-        rmm, &quot;computeConfidenceFromOutput&quot;, &amp;PLearner::remote_computeConfidenceFromOutput,
-        (BodyDoc(&quot;Compute a confidence intervals for the output, given the input and the\n&quot;
-                 &quot;pre-computed output (resulting from computeOutput or similar).  The\n&quot;
-                 &quot;probability level of the confidence interval must be specified.\n&quot;
-                 &quot;(e.g. 0.95).  Result is stored in a TVec of pairs low:high for each\n&quot;
-                 &quot;output variable (this is a \&quot;box\&quot; interval; it does not account for\n&quot;
-                 &quot;correlations among the output variables).\n&quot;),
-         ArgDoc (&quot;input&quot;,       &quot;Input vector (should have width inputsize)&quot;),
-         ArgDoc (&quot;output&quot;,      &quot;Output vector computed by previous call to computeOutput()&quot;),
-         ArgDoc (&quot;probability&quot;, &quot;Level at which the confidence interval must be computed,\n&quot;
-                                &quot;e.g. 0.95\n&quot;),
-         RetDoc (&quot;Vector of pairs low:high giving, respectively, the lower-bound confidence\n&quot;
-                 &quot;and upper-bound confidence for each dimension of the output vector.  If this\n&quot;
-                 &quot;vector is empty, then confidence intervals could not be computed for the\n&quot;
-                 &quot;given learner.  Note that this is the PLearner default (not to compute\n&quot;
-                 &quot;any confidence intervals), but some learners such as LinearRegressor\n&quot;
-                 &quot;know how to compute them.&quot;)));
-
-    declareMethod(
-        rmm, &quot;computeOutputCovMat&quot;, &amp;PLearner::remote_computeOutputCovMat,
-        (BodyDoc(&quot;Version of computeOutput that is capable of returning an output matrix\n&quot;
-                 &quot;given an input matrix (set of output vectors), as well as the complete\n&quot;
-                 &quot;covariance matrix between the outputs.\n&quot;
-                 &quot;\n&quot;
-                 &quot;A separate covariance matrix is returned for each output dimension, but\n&quot;
-                 &quot;these matrices are allowed to share the same storage.  This would be\n&quot;
-                 &quot;the case in situations where the output covariance really depends only\n&quot;
-                 &quot;on the location of the training inputs, as in, e.g.,\n&quot;
-                 &quot;GaussianProcessRegressor.\n&quot;
-                 &quot;\n&quot;
-                 &quot;The default implementation is to repeatedly call computeOutput,\n&quot;
-                 &quot;followed by computeConfidenceFromOutput (sampled with probability\n&quot;
-                 &quot;Erf[1/(2*Sqrt(2))], to extract 1*stddev given by subtraction of the two\n&quot;
-                 &quot;intervals, then squaring the stddev to obtain the variance), thereby\n&quot;
-                 &quot;filling a diagonal output covariance matrix.  If\n&quot;
-                 &quot;computeConfidenceFromOutput returns 'false' (confidence intervals not\n&quot;
-                 &quot;supported), the returned covariance matrix is filled with\n&quot;
-                 &quot;MISSING_VALUE.\n&quot;),
-         ArgDoc (&quot;inputs&quot;, &quot;Matrix containing the set of test points&quot;),
-         RetDoc (&quot;Two quantities are returned:\n&quot;
-                 &quot;- The matrix containing the expected output (as rows) for each input row.\n&quot;
-                 &quot;- A vector of covariance matrices between the outputs (one covariance\n&quot;
-                 &quot;  matrix per output dimension).\n&quot;)));
-    
-    declareMethod(
-        rmm, &quot;batchComputeOutputAndConfidencePMat&quot;,
-        &amp;PLearner::remote_batchComputeOutputAndConfidence,
-        (BodyDoc(&quot;Repeatedly calls computeOutput and computeConfidenceFromOutput with the\n&quot;
-                 &quot;rows of inputs.  Writes outputs_and_confidence rows (as a series of\n&quot;
-                 &quot;triples (output, low, high), one for each output).  The results are\n&quot;
-                 &quot;stored in a .pmat whose filename is passed as argument.\n&quot;),
-         ArgDoc (&quot;input_vmat&quot;,  &quot;VMatrix containing the input rows&quot;),
-         ArgDoc (&quot;probability&quot;, &quot;Level at which the confidence interval must be computed,\n&quot;
-                                &quot;e.g. 0.95\n&quot;),
-         ArgDoc (&quot;result_pmat_filename&quot;, &quot;Filename where to store the results&quot;)));
-
-    declareMethod(
-        rmm, &quot;getTestCostNames&quot;, &amp;PLearner::getTestCostNames,
-        (BodyDoc(&quot;Return the name of the costs computed by computeCostsFromOutputs()\n&quot;
-                 &quot;and computeOutputAndCosts()&quot;),
-         RetDoc (&quot;List of test cost names&quot;)));
-
-    declareMethod(
-        rmm, &quot;getTrainCostNames&quot;, &amp;PLearner::getTrainCostNames,
-        (BodyDoc(&quot;Return the names of the objective costs that the train\n&quot;
-                 &quot;method computes and for which it updates the VecStatsCollector\n&quot;
-                 &quot;train_stats.&quot;),
-         RetDoc (&quot;List of train cost names&quot;)));
-}
-
-////////////////////////////
-// setExperimentDirectory //
-////////////////////////////
-void PLearner::setExperimentDirectory(const PPath&amp; the_expdir) 
-{ 
-    if(the_expdir==&quot;&quot;)
-        expdir = &quot;&quot;;
-    else
-    {
-        if(!force_mkdir(the_expdir))
-            PLERROR(&quot;In PLearner::setExperimentDirectory Could not create experiment directory %s&quot;,
-                    the_expdir.absolute().c_str());
-        expdir = the_expdir / &quot;&quot;;
-    }
-}
-
-void PLearner::setTrainingSet(VMat training_set, bool call_forget)
-{ 
-    // YB: je ne suis pas sur qu'il soit necessaire de faire un build si la
-    // LONGUEUR du train_set a change?  les methodes non-parametriques qui
-    // utilisent la longueur devrait faire leur &quot;resize&quot; dans train, pas dans
-    // build.
-    bool training_set_has_changed = !train_set || !(train_set-&gt;looksTheSameAs(training_set));
-    train_set = training_set;
-    if (training_set_has_changed)
-    {
-        inputsize_ = train_set-&gt;inputsize();
-        targetsize_ = train_set-&gt;targetsize();
-        weightsize_ = train_set-&gt;weightsize();
-        if (forget_when_training_set_changes)
-            call_forget=true;
-    }
-    n_examples = train_set-&gt;length();
-    if (training_set_has_changed || call_forget)
-        build(); // MODIF FAITE PAR YOSHUA: sinon apres un setTrainingSet le build n'est pas complete dans un NNet train_set = training_set;
-    if (call_forget)
-        forget();
-
-    // Save the new training set if desired
-    if (save_trainingset_prefix != &quot;&quot; &amp;&amp; expdir != &quot;&quot;) {
-        static int trainingset_serial = 1;
-        PPath fname = expdir / (save_trainingset_prefix + &quot;_trainset_&quot; +
-                                tostring(trainingset_serial++) + &quot;.pmat&quot;);
-        train_set-&gt;savePMAT(fname);
-    }
-}
-
-void PLearner::setValidationSet(VMat validset)
-{ validation_set = validset; }
-
-
-void PLearner::setTrainStatsCollector(PP&lt;VecStatsCollector&gt; statscol)
-{ train_stats = statscol; }
-
-
-int PLearner::inputsize() const
-{ 
-    if (inputsize_&lt;0)
-        PLERROR(&quot;Must specify a training set before calling PLearner::inputsize()&quot;); 
-    return inputsize_; 
-}
-
-int PLearner::targetsize() const 
-{ 
-    if(targetsize_ == -1) 
-        PLERROR(&quot;In PLearner::targetsize - 'targetsize_' is -1, either no training set has beeen specified or its sizes were not set properly&quot;);
-    return targetsize_; 
-}
-
-int PLearner::weightsize() const 
-{ 
-    if(weightsize_ == -1) 
-        PLERROR(&quot;In PLearner::weightsize - 'weightsize_' is -1, either no training set has beeen specified or its sizes were not set properly&quot;);
-    return weightsize_; 
-}
-
-////////////
-// build_ //
-////////////
-void PLearner::build_()
-{
-    if(expdir!=&quot;&quot;)
-    {
-        if(!force_mkdir(expdir))
-            PLWARNING(&quot;In PLearner Could not create experiment directory %s&quot;,expdir.c_str());
-        else
-            expdir = expdir.absolute() / &quot;&quot;;
-    }
-    if (random_gen &amp;&amp; seed_ != 0)
-        random_gen-&gt;manual_seed(seed_);
-}
-
-///////////
-// build //
-///////////
-void PLearner::build()
-{
-    inherited::build();
-    build_();
-}
-
-////////////
-// forget //
-////////////
-void PLearner::forget()
-{
-    if (random_gen &amp;&amp; seed_ != 0)
-        random_gen-&gt;manual_seed(seed_);
-}
-
-int PLearner::nTestCosts() const 
-{ 
-    if(n_test_costs_&lt;0)
-        n_test_costs_ = getTestCostNames().size(); 
-    return n_test_costs_;
-}
-
-int PLearner::nTrainCosts() const 
-{ 
-    if(n_train_costs_&lt;0)
-        n_train_costs_ = getTrainCostNames().size();
-    return n_train_costs_; 
-}
-
-int PLearner::getTestCostIndex(const string&amp; costname) const
-{
-    TVec&lt;string&gt; costnames = getTestCostNames();
-    for(int i=0; i&lt;costnames.length(); i++)
-        if(costnames[i]==costname)
-            return i;
-    PLERROR(&quot;In PLearner::getTestCostIndex, No test cost named %s in this learner.\n&quot;
-            &quot;Available test costs are: %s&quot;, costname.c_str(),
-            tostring(costnames).c_str());
-    return -1;
-}
-
-int PLearner::getTrainCostIndex(const string&amp; costname) const
-{
-    TVec&lt;string&gt; costnames = getTrainCostNames();
-    for(int i=0; i&lt;costnames.length(); i++)
-        if(costnames[i]==costname)
-            return i;
-    PLERROR(&quot;In PLearner::getTrainCostIndex, No train cost named %s in this learner.\n&quot;
-            &quot;Available train costs are: %s&quot;, costname.c_str(), tostring(costnames).c_str());
-    return -1;
-}
-                                
-void PLearner::computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target, 
-                                     Vec&amp; output, Vec&amp; costs) const
-{
-    computeOutput(input, output);
-    computeCostsFromOutputs(input, output, target, costs);
-}
-
-void PLearner::computeCostsOnly(const Vec&amp; input, const Vec&amp; target,  
-                                Vec&amp; costs) const
-{
-    tmp_output.resize(outputsize());
-    computeOutputAndCosts(input, target, tmp_output, costs);
-}
-
-bool PLearner::computeConfidenceFromOutput(
-    const Vec&amp; input, const Vec&amp; output,
-    real probability,
-    TVec&lt; pair&lt;real,real&gt; &gt;&amp; intervals) const
-{
-    // Default version does not know how to compute confidence intervals
-    intervals.resize(output.size());
-    intervals.fill(std::make_pair(MISSING_VALUE,MISSING_VALUE));  
-    return false;
-}
-
-void PLearner::computeOutputCovMat(const Mat&amp; inputs, Mat&amp; outputs,
-                                   TVec&lt;Mat&gt;&amp; covariance_matrices) const
-{
-    PLASSERT( inputs.width() == inputsize() &amp;&amp; outputsize() &gt; 0 );
-    const int N = inputs.length();
-    const int M = outputsize();
-    outputs.resize(N, M);
-    covariance_matrices.resize(M);
-
-    bool has_confidence  = true;
-    bool init_covariance = 0;
-    Vec cur_input, cur_output;
-    TVec&lt; pair&lt;real,real&gt; &gt; intervals;
-    for (int i=0 ; i&lt;N ; ++i) {
-        cur_input  = inputs(i);
-        cur_output = outputs(i);
-        computeOutput(cur_input, cur_output);
-        if (has_confidence) {
-            static const real probability = pl_erf(1. / (2*sqrt(2)));
-            has_confidence = computeConfidenceFromOutput(cur_input, cur_output,
-                                                         probability, intervals);
-            if (has_confidence) {
-                // Create the covariance matrices only once; filled with zeros
-                if (! init_covariance) {
-                    for (int j=0 ; j&lt;M ; ++j)
-                        covariance_matrices[j] = Mat(N, N, 0.0);
-                    init_covariance = true;
-                }
-                
-                // Compute the variance for each output j, and set it on
-                // element i,i of the j-th covariance matrix
-                for (int j=0 ; j&lt;M ; ++j) {
-                    float stddev = intervals[j].second - intervals[j].first;
-                    float var = stddev*stddev;
-                    covariance_matrices[j](i,i) = var;
-                }
-            }
-        }
-    }
-
-    // If confidence intervals are not supported, fill the covariance matrices
-    // with missing values
-    for (int j=0 ; j&lt;M ; ++j)
-        covariance_matrices[j] = Mat(N, N, MISSING_VALUE);
-}
-
-void PLearner::batchComputeOutputAndConfidence(VMat inputs, real probability, VMat outputs_and_confidence) const
-{
-    Vec input(inputsize());
-    Vec output(outputsize());
-    int outsize = outputsize();
-    Vec output_and_confidence(3*outsize);
-    TVec&lt; pair&lt;real,real&gt; &gt; intervals;
-    int l = inputs.length();
-    for(int i=0; i&lt;l; i++)
-    {
-        inputs-&gt;getRow(i,input);
-        computeOutput(input,output);
-        computeConfidenceFromOutput(input,output,probability,intervals);
-        for(int j=0; j&lt;outsize; j++)
-        {
-            output_and_confidence[3*j] = output[j];
-            output_and_confidence[3*j+1] = intervals[j].first;
-            output_and_confidence[3*j+2] = intervals[j].second;
-        }
-        outputs_and_confidence-&gt;putOrAppendRow(i,output_and_confidence);
-    }
-}
-
-/////////
-// use //
-/////////
-void PLearner::use(VMat testset, VMat outputs) const
-{
-    int l = testset.length();
-    int w = testset.width();
-
-    TVec&lt; PP&lt;RemotePLearnServer&gt; &gt; servers;
-    if(nservers&gt;0)
-        servers = PLearnService::instance().reserveServers(nservers);
-
-    if(servers.length()==0) 
-    { // sequential code      
-        Vec input;
-        Vec target;
-        real weight;
-        Vec output(outputsize());
-
-        PP&lt;ProgressBar&gt; pb;
-        if(report_progress)
-            pb = new ProgressBar(&quot;Using learner&quot;,l);
-
-        for(int i=0; i&lt;l; i++)
-        {
-            testset.getExample(i, input, target, weight);
-            computeOutput(input, output);
-            outputs-&gt;putOrAppendRow(i,output);
-            if(pb)
-                pb-&gt;update(i);
-        }
-    }
-    else // parallel code
-    {
-        int n = servers.length(); // number of allocated servers
-        DBG_LOG &lt;&lt; &quot;PLearner::use parallel code using &quot; &lt;&lt; n &lt;&lt; &quot; servers&quot; &lt;&lt; endl;
-        for(int k=0; k&lt;n; k++)  // send this object with objid 0
-            servers[k]-&gt;newObject(0, *this);
-        int chunksize = l/n;
-        if(chunksize*n&lt;l)
-            ++chunksize;
-        if(chunksize*w&gt;1000000) // max 1 Mega elements
-            chunksize = max(1,1000000/w);
-        Mat chunk(chunksize,w);
-        int send_i=0;
-        Mat outmat;
-        int receive_i = 0;
-        while(send_i&lt;l)
-        {
-            for(int k=0; k&lt;n &amp;&amp; send_i&lt;l; k++)
-            {
-                int actualchunksize = chunksize;
-                if(send_i+actualchunksize&gt;l)
-                    actualchunksize = l-send_i;
-                chunk.resize(actualchunksize,w);
-                testset-&gt;getMat(send_i, 0, chunk);
-                VMat inputs(chunk);
-                inputs-&gt;copySizesFrom(testset);
-                DBG_LOG &lt;&lt; &quot;PLearner::use calling use2 remote method with chunk starting at &quot; 
-                        &lt;&lt; send_i &lt;&lt; &quot; of length &quot; &lt;&lt; actualchunksize &lt;&lt; &quot;:&quot; &lt;&lt; inputs &lt;&lt; endl;
-                servers[k]-&gt;callMethod(0,&quot;use2&quot;,inputs);
-                send_i += actualchunksize;
-            }
-            for(int k=0; k&lt;n &amp;&amp; receive_i&lt;l; k++)
-            {
-                outmat.resize(0,0);
-                servers[k]-&gt;getResults(outmat);
-                for(int ii=0; ii&lt;outmat.length(); ii++)
-                    outputs-&gt;putOrAppendRow(receive_i++,outmat(ii));
-            }
-        }
-        if(send_i!=l || receive_i!=l)
-            PLERROR(&quot;In PLearn::use parallel execution failed to complete successfully.&quot;);
-    }
-}
-
-VMat PLearner::processDataSet(VMat dataset) const
-{
-    // PLearnerOutputVMatrix does exactly this.
-    return new PLearnerOutputVMatrix(dataset, this);
-}
-
-
-TVec&lt;string&gt; PLearner::getOutputNames() const
-{
-    int n = outputsize();
-    TVec&lt;string&gt; outnames(n);
-    for(int k=0; k&lt;n; k++)
-        outnames[k] = &quot;out&quot; + tostring(k);
-    return outnames;
-}
-
-////////////////
-// useOnTrain //
-////////////////
-void PLearner::useOnTrain(Mat&amp; outputs) const {
-    VMat train_output(outputs);
-    use(train_set, train_output);
-}
-
-//////////
-// test //
-//////////
-void PLearner::test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats, 
-                    VMat testoutputs, VMat testcosts) const
-{
-    int len = testset.length();
-    Vec input;
-    Vec target;
-    real weight;
-
-    Vec output(outputsize());
-    Vec costs(nTestCosts());
-
-    PP&lt;ProgressBar&gt; pb;
-    if (report_progress) 
-        pb = new ProgressBar(&quot;Testing learner&quot;, len);
-
-    if (len == 0) {
-        // Empty test set: we give -1 cost arbitrarily.
-        costs.fill(-1);
-        test_stats-&gt;update(costs);
-    }
-
-    for (int i = 0; i &lt; len; i++)
-    {
-        testset.getExample(i, input, target, weight);
-      
-        // Always call computeOutputAndCosts, since this is better
-        // behaved with stateful learners
-        computeOutputAndCosts(input,target,output,costs);
-      
-        if (testoutputs)
-            testoutputs-&gt;putOrAppendRow(i, output);
-
-        if (testcosts)
-            testcosts-&gt;putOrAppendRow(i, costs);
-
-        if (test_stats)
-            test_stats-&gt;update(costs, weight);
-
-        if (report_progress)
-            pb-&gt;update(i);
-    }
-
-}
-
-///////////////
-// initTrain //
-///////////////
-bool PLearner::initTrain()
-{
-    string warn_msg = &quot;In PLearner::trainingCheck (called by '&quot; +
-        this-&gt;classname() + &quot;') - &quot;;
-    
-    // Check 'nstages' is valid.
-    if (nstages &lt; 0) {
-        PLWARNING((warn_msg + &quot;Option nstages (set to &quot; + tostring(nstages)
-                    + &quot;) must be non-negative&quot;).c_str());
-        return false;
-    }
-
-    // Check we actually need to train.
-    if (stage == nstages) {
-        if (verbosity &gt;= 1)
-            PLWARNING((warn_msg + &quot;The learner is already trained&quot;).c_str());
-        return false;
-    }
-
-    if (stage &gt; nstages) {
-        if (verbosity &gt;= 1) {
-            string msg = warn_msg + &quot;Learner was already trained up to stage &quot;
-                + tostring(stage) + &quot;, but asked to train up to nstages=&quot;
-                + tostring(nstages) + &quot;: it will be reverted to stage 0 and &quot;
-                                      &quot;trained again&quot;;
-            PLWARNING(msg.c_str());
-        }
-        forget();
-    }
-
-    // Check there is a training set.
-    if (!train_set) {
-        if (verbosity &gt;= 1)
-            PLWARNING((warn_msg + &quot;No training set specified&quot;).c_str());
-        return false;
-    }
-
-    // Initialize train_stats if needed.
-    if (!train_stats)
-        train_stats = new VecStatsCollector();
-
-    // Everything is fine.
-    return true;
-}
-
-////////////////////////
-// resetInternalState //
-////////////////////////
-void PLearner::resetInternalState()
-{ }
-
-bool PLearner::isStatefulLearner() const
-{
-    return false;
-}
-
-
-//#####  computeInputOutputMat  ###############################################
-
-Mat PLearner::computeInputOutputMat(VMat inputs) const
-{
-    int l = inputs.length();
-    int nin = inputsize();
-    int nout = outputsize();
-    Mat m(l, nin+nout);
-    for(int i=0; i&lt;l; i++)
-    {
-        Vec v = m(i);
-        Vec invec = v.subVec(0,nin);
-        Vec outvec = v.subVec(nin,nout);
-        inputs-&gt;getRow(i, invec);
-        computeOutput(invec, outvec);
-    }
-    return m;
-}
-
-
-//#####  computeInputOutputConfMat  ###########################################
-
-Mat PLearner::computeInputOutputConfMat(VMat inputs, real probability) const
-{
-    int l = inputs.length();
-    int nin = inputsize();
-    int nout = outputsize();
-    Mat m(l, nin+3*nout);
-    TVec&lt; pair&lt;real,real&gt; &gt; intervals;
-    for(int i=0; i&lt;l; i++)
-    {
-        Vec v = m(i);
-        Vec invec   = v.subVec(0,nin);
-        Vec outvec  = v.subVec(nin,nout);
-        Vec lowconf = v.subVec(nin+nout, nout);
-        Vec hiconf  = v.subVec(nin+2*nout, nout);
-        inputs-&gt;getRow(i, invec);
-        computeOutput(invec, outvec);
-        bool conf_avail = computeConfidenceFromOutput(invec, outvec, probability,
-                                                      intervals);
-        if (conf_avail) {
-            for (int i=0, n=intervals.size() ; i&lt;n ; ++i) {
-                lowconf[i] = intervals[i].first;
-                hiconf[i]  = intervals[i].second;
-            }
-        }
-        else {
-            lowconf &lt;&lt; MISSING_VALUE;
-            hiconf  &lt;&lt; MISSING_VALUE;
-        }
-    }
-    return m;
-}
-
-
-//! Version of computeOutput that returns a result by value
-Vec PLearner::remote_computeOutput(const Vec&amp; input) const
-{
-    tmp_output.resize(outputsize());
-    computeOutput(input, tmp_output);
-    return tmp_output;
-}
-
-//! Version of use that's called by RMI
-void PLearner::remote_use(VMat inputs, string output_fname) const
-{
-    VMat outputs = new FileVMatrix(output_fname, inputs.length(), outputsize());
-    use(inputs,outputs);
-}
-
-//! Version of use2 that's called by RMI
-Mat PLearner::remote_use2(VMat inputs) const
-{
-    Mat outputs(inputs.length(), outputsize());
-    use(inputs,outputs);
-    return outputs;
-}
-    
-//! Version of computeOutputAndCosts that's called by RMI
-
-tuple&lt;Vec,Vec&gt; PLearner::remote_computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target) const
-{
-    tmp_output.resize(outputsize());
-    Vec costs(nTestCosts());
-    computeOutputAndCosts(input,target,tmp_output,costs);
-    return make_tuple(tmp_output, costs);
-}
-
-//! Version of computeCostsFromOutputs that's called by RMI
-Vec PLearner::remote_computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
-                                             const Vec&amp; target) const
-{
-    Vec costs(nTestCosts());
-    computeCostsFromOutputs(input,output,target,costs);
-    return costs;
-}
-
-//! Version of computeCostsOnly that's called by RMI
-Vec PLearner::remote_computeCostsOnly(const Vec&amp; input, const Vec&amp; target) const
-{
-    Vec costs(nTestCosts());
-    computeCostsOnly(input,target,costs);
-    return costs;
-}
-
-//! Version of computeConfidenceFromOutput that's called by RMI
-TVec&lt; pair&lt;real,real&gt; &gt;
-PLearner::remote_computeConfidenceFromOutput(const Vec&amp; input, const Vec&amp; output,
-                                             real probability) const
-{
-    TVec&lt; pair&lt;real,real&gt; &gt; intervals(output.length());
-    bool ok = computeConfidenceFromOutput(input, output, probability, intervals);
-    if (ok)
-        return intervals;
-    else
-        return TVec&lt; pair&lt;real,real&gt; &gt;();
-}
-
-//! Version of computeOutputCovMat that's called by RMI
-tuple&lt;Mat, TVec&lt;Mat&gt; &gt;
-PLearner::remote_computeOutputCovMat(const Mat&amp; inputs) const
-{
-    Mat outputs;
-    TVec&lt;Mat&gt; covmat;
-    computeOutputCovMat(inputs, outputs, covmat);
-    return make_tuple(outputs, covmat);
-}
-
-//! Version of batchComputeOutputAndConfidence that's called by RMI
-void PLearner::remote_batchComputeOutputAndConfidence(VMat inputs, real probability,
-                                                      string pmat_fname) const
-{
-    TVec&lt;string&gt; fieldnames;
-    for(int j=0; j&lt;outputsize(); j++)
-    {
-        fieldnames.append(&quot;output_&quot;+tostring(j));
-        fieldnames.append(&quot;low_&quot;+tostring(j));
-        fieldnames.append(&quot;high_&quot;+tostring(j));
-    }
-    VMat out_and_conf = new FileVMatrix(pmat_fname,inputs.length(),fieldnames);
-    batchComputeOutputAndConfidence(inputs, probability, out_and_conf);
-}
-
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:&quot;stroustrup&quot;
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc (from rev 6569, trunk/plearn_learners/generic/PLearner.cc)

Deleted: tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-12 15:44:07 UTC (rev 6570)
@@ -1,664 +0,0 @@
-// -*- C++ -*-
-
-// GaussianProcessRegressor.cc
-//
-// Copyright (C) 2006 Nicolas Chapados 
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************      
-   * $Id: .pyskeleton_header 544 2003-09-01 00:05:31Z plearner $ 
-   ******************************************************* */
-
-// Authors: Nicolas Chapados
-
-/*! \file GaussianProcessRegressor.cc */
-
-#define PL_LOG_MODULE_NAME &quot;GaussianProcessRegressor&quot;
-
-// From PLearn
-#include &lt;plearn/base/stringutils.h&gt;
-#include &lt;plearn/io/pl_log.h&gt;
-#include &lt;plearn/vmat/ExtendedVMatrix.h&gt;
-#include &lt;plearn/math/pl_erf.h&gt;
-#include &lt;plearn/var/GaussianProcessNLLVariable.h&gt;
-#include &lt;plearn/var/ObjectOptionVariable.h&gt;
-#include &lt;plearn/opt/Optimizer.h&gt;
-
-#include &quot;GaussianProcessRegressor.h&quot;
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    GaussianProcessRegressor,
-    &quot;Kernelized version of linear ridge regression.&quot;,
-    &quot;Given a kernel K(x,y) = phi(x)'phi(y), where phi(x) is the projection of a\n&quot;
-    &quot;vector x into feature space, this class implements a version of the ridge\n&quot;
-    &quot;estimator, giving the prediction at x as\n&quot;
-    &quot;\n&quot;
-    &quot;    f(x) = k(x)'(M + lambda I)^-1 y,\n&quot;
-    &quot;\n&quot;
-    &quot;where x is the test vector where to estimate the response, k(x) is the\n&quot;
-    &quot;vector of kernel evaluations between the test vector and the elements of\n&quot;
-    &quot;the training set, namely\n&quot;
-    &quot;\n&quot;
-    &quot;    k(x) = (K(x,x1), K(x,x2), ..., K(x,xN))',\n&quot;
-    &quot;\n&quot;
-    &quot;M is the Gram Matrix on the elements of the training set, i.e. the matrix\n&quot;
-    &quot;where the element (i,j) is equal to K(xi, xj), lambda is the VARIANCE of\n&quot;
-    &quot;the observation noise (and can be interpreted as a weight decay\n&quot;
-    &quot;coefficient), and y is the vector of training-set targets.\n&quot;
-    &quot;\n&quot;
-    &quot;The uncertainty in a prediction can be computed by calling\n&quot;
-    &quot;computeConfidenceFromOutput.  Furthermore, if desired, this learner allows\n&quot;
-    &quot;optimization of the kernel hyperparameters by direct optimization of the\n&quot;
-    &quot;marginal likelihood w.r.t. the hyperparameters.  This mechanism relies on a\n&quot;
-    &quot;user-provided Optimizer (see the 'optimizer' option) and does not rely on\n&quot;
-    &quot;the PLearn HyperLearner system.\n&quot;
-    &quot;\n&quot;
-    &quot;GaussianProcessRegressor produces the following train costs:\n&quot;
-    &quot;\n&quot;
-    &quot;- \&quot;nmll\&quot; : the negative marginal log-likelihood on the training set.\n&quot;
-    &quot;- \&quot;mse\&quot;  : the mean-squared error on the training set (by convention,\n&quot;
-    &quot;           divided by two)\n&quot;
-    &quot;\n&quot;
-    &quot;and the following test costs:\n&quot;
-    &quot;\n&quot;
-    &quot;- \&quot;nll\&quot; : the negative log-likelihood of the test example under the\n&quot;
-    &quot;          predictive distribution.  Available only if the option\n&quot;
-    &quot;          'compute_confidence' is true.\n&quot;
-    &quot;- \&quot;mse\&quot; : the squared error of the test example with respect to the\n&quot;
-    &quot;          predictive mean (by convention, divided by two).\n&quot;
-    &quot;\n&quot;
-    &quot;The disadvantage of this learner is that its training time is O(N^3) in the\n&quot;
-    &quot;number of training examples (due to the matrix inversion).  When saving the\n&quot;
-    &quot;learner, the training set inputs must be saved, along with an additional\n&quot;
-    &quot;matrix of length number-of-training-examples, and width number-of-targets.\n&quot;
-    );
-
-GaussianProcessRegressor::GaussianProcessRegressor() 
-    : m_weight_decay(0.0),
-      m_include_bias(true),
-      m_compute_confidence(false),
-      m_confidence_epsilon(1e-8)
-{ }
-
-
-void GaussianProcessRegressor::declareOptions(OptionList&amp; ol)
-{
-    //#####  Build Options  ###################################################
-    
-    declareOption(
-        ol, &quot;kernel&quot;, &amp;GaussianProcessRegressor::m_kernel,
-        OptionBase::buildoption,
-        &quot;Kernel to use for the computation.  This must be a similarity kernel\n&quot;
-        &quot;(i.e. closer vectors give higher kernel evaluations).&quot;);
-
-    declareOption(
-        ol, &quot;weight_decay&quot;, &amp;GaussianProcessRegressor::m_weight_decay,
-        OptionBase::buildoption,
-        &quot;Weight decay coefficient (default = 0)&quot;);
-
-    declareOption(
-        ol, &quot;include_bias&quot;, &amp;GaussianProcessRegressor::m_include_bias,
-        OptionBase::buildoption,
-        &quot;Whether to include a bias term in the regression (true by default)\n&quot;
-        &quot;The effect of this option is NOT to prepend a column of 1 to the inputs\n&quot;
-        &quot;(which has often no effect for GP regression), but to estimate a\n&quot;
-        &quot;separate mean of the targets, perform the GP regression on the\n&quot;
-        &quot;zero-mean targets, and add it back when computing the outputs.\n&quot;);
-
-    declareOption(
-        ol, &quot;compute_confidence&quot;, &amp;GaussianProcessRegressor::m_compute_confidence,
-        OptionBase::buildoption,
-        &quot;Whether to perform the additional train-time computations required\n&quot;
-        &quot;to compute confidence intervals.  This includes computing a separate\n&quot;
-        &quot;inverse of the Gram matrix.  Specification of this option is necessary\n&quot;
-        &quot;for calling both computeConfidenceFromOutput and computeOutputCovMat.\n&quot;);
-
-    declareOption(
-        ol, &quot;confidence_epsilon&quot;, &amp;GaussianProcessRegressor::m_confidence_epsilon,
-        OptionBase::buildoption,
-        &quot;Small regularization to be added post-hoc to the computed output\n&quot;
-        &quot;covariance matrix and confidence intervals; this is mostly used as a\n&quot;
-        &quot;disaster prevention device, to avoid negative predictive variance\n&quot;);
-    
-    declareOption(
-        ol, &quot;hyperparameters&quot;, &amp;GaussianProcessRegressor::m_hyperparameters,
-        OptionBase::buildoption,
-        &quot;List of hyperparameters to optimize.  They must be specified in the\n&quot;
-        &quot;form \&quot;option-name\&quot;:initial-value, where 'option-name' is the name\n&quot;
-        &quot;of an option to set within the Kernel object (the array-index form\n&quot;
-        &quot;'option[i]' is supported), and 'initial-value' is the\n&quot;
-        &quot;(PLearn-serialization string representation) for starting point for the\n&quot;
-        &quot;optimization.  Currently, the hyperparameters are constrained to be\n&quot;
-        &quot;scalars.\n&quot;);
-
-    declareOption(
-        ol, &quot;ARD_hyperprefix_initval&quot;,
-        &amp;GaussianProcessRegressor::m_ARD_hyperprefix_initval,
-        OptionBase::buildoption,
-        &quot;If the kernel support automatic relevance determination (ARD; e.g.\n&quot;
-        &quot;SquaredExponentialARDKernel), the list of hyperparameters corresponding\n&quot;
-        &quot;to each input can be created automatically by giving an option prefix\n&quot;
-        &quot;and an initial value.  The ARD options are created to have the form\n&quot;
-        &quot;\n&quot;
-        &quot;   'prefix[0]', 'prefix[1]', 'prefix[N-1]'\n&quot;
-        &quot;\n&quot;
-        &quot;where N is the number of inputs.  This option is useful when the\n&quot;
-        &quot;dataset inputsize is not (easily) known ahead of time. \n&quot;);
-    
-    declareOption(
-        ol, &quot;optimizer&quot;, &amp;GaussianProcessRegressor::m_optimizer,
-        OptionBase::buildoption,
-        &quot;Specification of the optimizer to use for train-time hyperparameter\n&quot;
-        &quot;optimization.  A ConjGradientOptimizer should be an adequate choice.\n&quot;);
-
-
-    //#####  Learnt Options  ##################################################
-
-    declareOption(
-        ol, &quot;alpha&quot;, &amp;GaussianProcessRegressor::m_alpha,
-        OptionBase::learntoption,
-        &quot;Vector of learned parameters, determined from the equation\n&quot;
-        &quot;    (M + lambda I)^-1 y&quot;);
-
-    declareOption(
-        ol, &quot;gram_inverse&quot;, &amp;GaussianProcessRegressor::m_gram_inverse,
-        OptionBase::learntoption,
-        &quot;Inverse of the Gram matrix, used to compute confidence intervals (must\n&quot;
-        &quot;be saved since the confidence intervals are obtained from the equation\n&quot;
-        &quot;\n&quot;
-        &quot;  sigma^2 = k(x,x) - k(x)'(M + lambda I)^-1 k(x)\n&quot;);
-
-    declareOption(
-        ol, &quot;target_mean&quot;, &amp;GaussianProcessRegressor::m_target_mean,
-        OptionBase::learntoption,
-        &quot;Mean of the targets, if the option 'include_bias' is true&quot;);
-    
-    declareOption(
-        ol, &quot;training_inputs&quot;, &amp;GaussianProcessRegressor::m_training_inputs,
-        OptionBase::learntoption,
-        &quot;Saved version of the training set, which must be kept along for\n&quot;
-        &quot;carrying out kernel evaluations with the test point&quot;);
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-void GaussianProcessRegressor::build_()
-{
-    if (! m_kernel)
-        PLERROR(&quot;GaussianProcessRegressor::build_: 'kernel' option must be specified&quot;);
-
-    if (! m_kernel-&gt;is_symmetric)
-        PLERROR(&quot;GaussianProcessRegressor::build_: the kernel (%s) must be symmetric&quot;,
-                m_kernel-&gt;classname().c_str());
-    
-    // If we are reloading the model, set the training inputs into the kernel
-    if (m_training_inputs)
-        m_kernel-&gt;setDataForKernelMatrix(m_training_inputs);
-
-    // If we specified hyperparameters without an optimizer, complain.
-    // (It is mildly legal to specify an optimizer without hyperparameters;
-    // this does nothing).
-    if (m_hyperparameters.size() &gt; 0 &amp;&amp; ! m_optimizer)
-        PLERROR(&quot;GaussianProcessRegressor::build_: 'hyperparameters' are specified &quot;
-                &quot;but no 'optimizer'; an optimizer is required in order to carry out &quot;
-                &quot;hyperparameter optimization&quot;);
-
-    if (m_confidence_epsilon &lt; 0)
-        PLERROR(&quot;GaussianProcessRegressor::build_: 'confidence_epsilon' must be non-negative&quot;);
-}
-
-// ### Nothing to add here, simply calls build_
-void GaussianProcessRegressor::build()
-{
-    inherited::build();
-    build_();
-}
-
-
-void GaussianProcessRegressor::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(m_kernel,                     copies);
-    deepCopyField(m_hyperparameters,            copies);
-    deepCopyField(m_optimizer,                  copies);
-    deepCopyField(m_alpha,                      copies);
-    deepCopyField(m_gram_inverse,               copies);
-    deepCopyField(m_target_mean,                copies);
-    deepCopyField(m_training_inputs,            copies);
-    deepCopyField(m_kernel_evaluations,         copies);
-    deepCopyField(m_gram_inverse_product,       copies);
-    deepCopyField(m_intervals,                  copies);
-    deepCopyField(m_gram_traintest_inputs,      copies);
-    deepCopyField(m_gram_inv_traintest_product, copies);
-    deepCopyField(m_sigma_reductor,             copies);
-}
-
-
-//#####  setTrainingSet  ######################################################
-
-void GaussianProcessRegressor::setTrainingSet(VMat training_set, bool call_forget)
-{
-    PLASSERT( training_set );
-    int inputsize = training_set-&gt;inputsize() ;
-    if (inputsize &lt; 0)
-        PLERROR(&quot;GaussianProcessRegressor::setTrainingSet: the training set inputsize &quot;
-                &quot;must be specified (current value = %d)&quot;, inputsize);
-
-    // Convert to a real matrix in order to make saving it saner
-    m_training_inputs = training_set.subMatColumns(0, inputsize).toMat();
-    inherited::setTrainingSet(training_set, call_forget);
-}
-
-
-//#####  outputsize  ##########################################################
-
-int GaussianProcessRegressor::outputsize() const
-{
-    return targetsize();
-}
-
-
-//#####  forget  ##############################################################
-
-void GaussianProcessRegressor::forget()
-{
-    inherited::forget();
-    if (m_optimizer)
-        m_optimizer-&gt;reset();
-    m_alpha.resize(0,0);
-    m_target_mean.resize(0);
-    m_gram_inverse.resize(0,0);
-    stage = 0;
-}
-    
-
-//#####  train  ###############################################################
-
-void GaussianProcessRegressor::train()
-{
-    // This generic PLearner method does a number of standard stuff useful for
-    // (almost) any learner, and return 'false' if no training should take
-    // place. See PLearner.h for more details.
-    if (!initTrain())
-        return;
-
-    PLASSERT( m_kernel );
-    if (! train_set || ! m_training_inputs)
-        PLERROR(&quot;GaussianProcessRegressor::train: the training set must be specified&quot;);
-    int trainlength = train_set-&gt;length();
-    int inputsize   = train_set-&gt;inputsize() ;
-    int targetsize  = train_set-&gt;targetsize();
-    int weightsize  = train_set-&gt;weightsize();
-    if (inputsize  &lt; 0 || targetsize &lt; 0 || weightsize &lt; 0)
-        PLERROR(&quot;GaussianProcessRegressor::train: inconsistent inputsize/targetsize/weightsize &quot;
-                &quot;(%d/%d/%d) in training set&quot;, inputsize, targetsize, weightsize);
-    if (weightsize &gt; 0)
-        PLERROR(&quot;GaussianProcessRegressor::train: observations weights are not currently supported&quot;);
-
-    // Subtract the mean if we require it
-    Mat targets(trainlength, targetsize);
-    train_set.subMatColumns(inputsize, targetsize)-&gt;getMat(0,0,targets);
-    if (m_include_bias) {
-        m_target_mean.resize(targets.width());
-        columnMean(targets, m_target_mean);
-        targets -= m_target_mean;
-    }
-
-    // Optimize hyperparameters
-    PP&lt;GaussianProcessNLLVariable&gt; nll = hyperOptimize(m_training_inputs, targets);
-    PLASSERT( nll );
-    
-    // Compute parameters
-    nll-&gt;fprop();
-    m_alpha = nll-&gt;alpha();
-    m_gram_inverse = nll-&gt;gramInverse();
-
-    // Compute train MSE, as 1/(2N) * dot(z,z), with z=K*alpha - y
-    Mat residuals(m_alpha.length(), m_alpha.width());
-    product(residuals, nll-&gt;gram(), m_alpha);
-    residuals -= targets;
-    real mse = dot(residuals, residuals) / (2 * trainlength);
-    
-    // And accumulate some statistics
-    Vec costs(2);
-    costs[0] = nll-&gt;value[0];
-    costs[1] = mse;
-    getTrainStatsCollector()-&gt;update(costs);
-    MODULE_LOG &lt;&lt; &quot;Train NLL: &quot; &lt;&lt; costs[0] &lt;&lt; endl;
-}
-
-
-//#####  computeOutput  #######################################################
-
-void GaussianProcessRegressor::computeOutput(const Vec&amp; input, Vec&amp; output) const
-{
-    PLASSERT( m_kernel &amp;&amp; m_alpha.isNotNull() &amp;&amp; m_training_inputs );
-    PLASSERT( m_alpha.width()  == output.size() );
-    PLASSERT( m_alpha.length() == m_training_inputs.length() );
-    PLASSERT( input.size()     == m_training_inputs.width()  );
-
-    m_kernel_evaluations.resize(m_alpha.length());
-    computeOutputAux(input, output, m_kernel_evaluations);
-}
-
-
-void GaussianProcessRegressor::computeOutputAux(
-    const Vec&amp; input, Vec&amp; output, Vec&amp; kernel_evaluations) const
-{
-    m_kernel-&gt;evaluate_all_x_i(input, kernel_evaluations);
-
-    // Finally compute k(x,x_i) * (M + \lambda I)^-1 y
-    product(Mat(1, output.size(), output),
-            Mat(1, kernel_evaluations.size(), kernel_evaluations),
-            m_alpha);
-
-    if (m_include_bias)
-        output += m_target_mean;
-}
-
-
-//#####  computeCostsFromOutputs  #############################################
-
-void GaussianProcessRegressor::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output, 
-                                                       const Vec&amp; target, Vec&amp; costs) const
-{
-    costs.resize(2);
-
-    // NLL cost is the NLL of the target under the predictive distribution
-    // (centered at predictive mean, with variance obtainable from the
-    // confidence bounds).  HOWEVER, to obain it, we have to be able to compute
-    // the confidence bounds.  If impossible, simply set missing-value for the
-    // NLL cost.
-    if (m_compute_confidence) {
-        static const float PROBABILITY = pl_erf(1. / (2*sqrt(2)));  // 0.5 stddev
-        bool confavail = computeConfidenceFromOutput(input, output, PROBABILITY,
-                                                     m_intervals);
-        assert( confavail &amp;&amp; m_intervals.size() == output.size() &amp;&amp;
-                output.size() == target.size() );
-        static const real LN_2PI_OVER_2 = pl_log(2*M_PI) / 2.0;
-        real nll = 0;
-        for (int i=0, n=output.size() ; i&lt;n ; ++i) {
-            real sigma = m_intervals[i].second - m_intervals[i].first;
-            sigma = max(sigma, 1e-15);        // Very minor regularization
-            real diff = target[i] - output[i];
-            nll += diff*diff / (2.*sigma*sigma) + pl_log(sigma) + LN_2PI_OVER_2;
-        }
-        costs[0] = nll;
-    }
-    else
-        costs[0] = MISSING_VALUE;
-    
-    real squared_loss = 0.5*powdistance(output,target);
-    costs[1] = squared_loss;
-}     
-
-
-//#####  computeConfidenceFromOutput  #########################################
-
-bool GaussianProcessRegressor::computeConfidenceFromOutput(
-    const Vec&amp; input, const Vec&amp; output, real probability,
-    TVec&lt; pair&lt;real,real&gt; &gt;&amp; intervals) const
-{
-    if (! m_compute_confidence) {
-        PLWARNING(&quot;GaussianProcessRegressor::computeConfidenceFromOutput: the option\n&quot;
-                  &quot;'compute_confidence' must be true in order to compute valid\n&quot;
-                  &quot;condidence intervals&quot;);
-        return false;
-    }
-
-    // BIG-BIG assumption: assume that computeOutput has just been called and
-    // that m_kernel_evaluations contains the right stuff.
-    PLASSERT( m_kernel &amp;&amp; m_gram_inverse.isNotNull() );
-    real base_sigma_sq = m_kernel(input, input);
-    m_gram_inverse_product.resize(m_kernel_evaluations.size());
-    product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
-    real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
-    real sigma = sqrt(max(0., base_sigma_sq - sigma_reductor + m_confidence_epsilon));
-
-    // two-tailed
-    const real multiplier = gauss_01_quantile((1+probability)/2);
-    real half_width = multiplier * sigma;
-    intervals.resize(output.size());
-    for (int i=0, n=output.size() ; i&lt;n ; ++i)
-        intervals[i] = std::make_pair(output[i] - half_width,
-                                      output[i] + half_width);
-    return true;
-}
-
-
-//#####  computeOutputCovMat  #################################################
-
-void GaussianProcessRegressor::computeOutputCovMat(
-    const Mat&amp; inputs, Mat&amp; outputs, TVec&lt;Mat&gt;&amp; covariance_matrices) const
-{
-    PLASSERT( m_kernel &amp;&amp; m_alpha.isNotNull() &amp;&amp; m_training_inputs );
-    PLASSERT( m_alpha.width()  == outputsize() );
-    PLASSERT( m_alpha.length() == m_training_inputs.length() );
-    PLASSERT( inputs.width()   == m_training_inputs.width()  );
-    PLASSERT( inputs.width()   == inputsize() );
-    const int N = inputs.length();
-    const int M = outputsize();
-    const int T = m_training_inputs.length();
-    outputs.resize(N, M);
-    covariance_matrices.resize(M);
-
-    // Preallocate space for the covariance matrix, and since all outputs share
-    // the same matrix, copy it into the remaining elements of
-    // covariance_matrices
-    Mat&amp; covmat = covariance_matrices[0];
-    covmat.resize(N, N);
-    for (int j=1 ; j&lt;M ; ++j)
-        covariance_matrices[j] = covmat;
-
-    // Start by computing the matrix of kernel evaluations between the train
-    // and test outputs, and compute the output
-    m_gram_traintest_inputs.resize(N, T);
-    for (int i=0 ; i&lt;N ; ++i) {
-        Vec cur_traintest_kereval = m_gram_traintest_inputs(i);
-        Vec cur_output = outputs(i);
-        computeOutputAux(inputs(i), cur_output, cur_traintest_kereval);
-    }
-
-    // Next compute the kernel evaluations between the test inputs; more or
-    // less lifted from Kernel.cc ==&gt; must see with Olivier how to better
-    // factor this code
-    Mat&amp; K = covmat;
-    K.resize(N,N);
-    const int mod = K.mod();
-    real Kij;
-    real* Ki;
-    real* Kji;
-    for (int i=0 ; i&lt;N ; ++i) {
-        Ki  = K[i];
-        Kji = &amp;K[0][i];
-        const Vec&amp; cur_input_i = inputs(i);
-        for (int j=0 ; j&lt;=i ; ++j, Kji += mod) {
-            Kij = m_kernel-&gt;evaluate(cur_input_i, inputs(j));
-            *Ki++ = Kij;
-            if (j&lt;i)
-                *Kji = Kij;    // Assume symmetry, checked at build
-        }
-    }
-
-    // The predictive covariance matrix is (c.f. Rasmussen and Williams):
-    //
-    //    cov(f*) = K(X*,X*) - K(X*,X) [K(X,X) + sigma*I]^-1 K(X,X*)
-    //
-    // where X are the training inputs, and X* are the test inputs.
-    m_gram_inv_traintest_product.resize(T,N);
-    m_sigma_reductor.resize(N,N);
-    productTranspose(m_gram_inv_traintest_product, m_gram_inverse,
-                     m_gram_traintest_inputs);
-    product(m_sigma_reductor, m_gram_traintest_inputs,
-            m_gram_inv_traintest_product);
-    covmat -= m_sigma_reductor;
-
-    // As a preventive measure, never output negative variance, even though
-    // this does not garantee the non-negative-definiteness of the matrix
-    for (int i=0 ; i&lt;N ; ++i)
-        covmat(i,i) = max(0.0, covmat(i,i) + m_confidence_epsilon);
-}
-
-
-//#####  get*CostNames  #######################################################
-
-TVec&lt;string&gt; GaussianProcessRegressor::getTestCostNames() const
-{
-    TVec&lt;string&gt; c(2);
-    c[0] = &quot;nll&quot;;
-    c[1] = &quot;mse&quot;;
-    return c;
-}
-
-
-TVec&lt;string&gt; GaussianProcessRegressor::getTrainCostNames() const
-{
-    TVec&lt;string&gt; c(2);
-    c[0] = &quot;nmll&quot;;
-    c[1] = &quot;mse&quot;;
-    return c;
-}
-
-
-//#####  hyperOptimize  #######################################################
-
-PP&lt;GaussianProcessNLLVariable&gt;
-GaussianProcessRegressor::hyperOptimize(const Mat&amp; inputs, const Mat&amp; targets)
-{
-    // If there are no hyperparameters or optimizer, just create a simple
-    // variable and return it right away.
-    if (! m_optimizer || (m_hyperparameters.size() == 0 &amp;&amp;
-                          m_ARD_hyperprefix_initval.first.empty()) )
-    {
-        return new GaussianProcessNLLVariable(
-            m_kernel, m_weight_decay, inputs, targets,
-            TVec&lt;string&gt;(), VarArray(), m_compute_confidence);
-    }
-
-    // Otherwise create Vars that wrap each hyperparameter
-    const int numhyper  = m_hyperparameters.size();
-    const int numinputs = ( ! m_ARD_hyperprefix_initval.first.empty() ?
-                            inputsize() : 0 );
-    VarArray     hyperparam_vars (numhyper + numinputs);
-    TVec&lt;string&gt; hyperparam_names(numhyper + numinputs);
-    int i;
-    for (i=0 ; i&lt;numhyper ; ++i) {
-        hyperparam_names[i] = m_hyperparameters[i].first;
-        hyperparam_vars [i] = new ObjectOptionVariable(
-            (Kernel*)m_kernel, m_hyperparameters[i].first, m_hyperparameters[i].second);
-        hyperparam_vars[i]-&gt;setName(m_hyperparameters[i].first);
-    }
-
-    // If specified, create the Vars for automatic relevance determination
-    string&amp; ARD_name = m_ARD_hyperprefix_initval.first;
-    string&amp; ARD_init = m_ARD_hyperprefix_initval.second;
-    if (! ARD_name.empty()) {
-        // Small hack to ensure the ARD vector in the kernel has proper size
-        Vec init(numinputs, lexical_cast&lt;double&gt;(ARD_init));
-        m_kernel-&gt;changeOption(ARD_name, tostring(init, PStream::plearn_ascii));
-        
-        for (int j=0 ; j&lt;numinputs ; ++j, ++i) {
-            hyperparam_names[i] = ARD_name + '[' + tostring(j) + ']';
-            hyperparam_vars [i] = new ObjectOptionVariable(
-                (Kernel*)m_kernel, hyperparam_names[i], ARD_init);
-            hyperparam_vars [i]-&gt;setName(hyperparam_names[i]);
-        }
-    }
-
-    // Create the cost-function variable
-    PP&lt;GaussianProcessNLLVariable&gt; nll = new GaussianProcessNLLVariable(
-        m_kernel, m_weight_decay, inputs, targets, hyperparam_names,
-        hyperparam_vars, true);
-    nll-&gt;setName(&quot;GaussianProcessNLLVariable&quot;);
-
-    // Some logging about the initial values
-    logVarray(hyperparam_vars, &quot;Hyperparameter initial values:&quot;);
-    
-    // And optimize for nstages
-    m_optimizer-&gt;setToOptimize(hyperparam_vars, (Variable*)nll);
-    m_optimizer-&gt;build();
-    PP&lt;ProgressBar&gt; pb(
-        report_progress? new ProgressBar(&quot;Training GaussianProcessRegressor &quot;
-                                         &quot;from stage &quot; + tostring(stage) + &quot; to stage &quot; +
-                                         tostring(nstages), nstages-stage)
-        : 0);
-    bool early_stopping = false;
-    PP&lt;VecStatsCollector&gt; statscol = new VecStatsCollector;
-    for (const int initial_stage = stage ; !early_stopping &amp;&amp; stage &lt; nstages
-             ; ++stage)
-    {
-        if (pb)
-            pb-&gt;update(stage - initial_stage);
-
-        statscol-&gt;forget();
-        early_stopping = m_optimizer-&gt;optimizeN(*statscol);
-        statscol-&gt;finalize();
-    }
-    pb = 0;                                  // Finish progress bar right now
-
-    // Some logging about the final values
-    logVarray(hyperparam_vars, &quot;Hyperparameter final values:&quot;);
-    return nll;
-}
-
-
-//#####  logVarray  ###########################################################
-
-void GaussianProcessRegressor::logVarray(const VarArray&amp; varr,
-                                         const string&amp; title)
-{
-    string entry = title + '\n';
-    for (int i=0, n=varr.size() ; i&lt;n ; ++i) {
-        entry += right(varr[i]-&gt;getName(), 35) + &quot;: &quot; + tostring(varr[i]-&gt;value[0]);
-        if (i &lt; n-1)
-            entry += '\n';
-    }
-    MODULE_LOG &lt;&lt; entry &lt;&lt; endl; 
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:&quot;stroustrup&quot;
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc (from rev 6569, trunk/plearn_learners/regressors/GaussianProcessRegressor.cc)


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000018.html">[Plearn-commits] r6569 - in trunk: plearn/ker	plearn_learners/generic plearn_learners/regressors
</A></li>
	<LI>Next message: <A HREF="000020.html">[Plearn-commits] r6571 - in tags: . OPAL-3.0.3.1/plearn/ker	OPAL-3.0.3.1/plearn_learners/generic	OPAL-3.0.3.1/plearn_learners/regressors
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#19">[ date ]</a>
              <a href="thread.html#19">[ thread ]</a>
              <a href="subject.html#19">[ subject ]</a>
              <a href="author.html#19">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
