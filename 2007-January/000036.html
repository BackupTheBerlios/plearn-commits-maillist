<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6587 - trunk/python_modules/plearn/math
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6587%20-%20trunk/python_modules/plearn/math&In-Reply-To=%3C200701172224.l0HMOrdj030961%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000035.html">
   <LINK REL="Next"  HREF="000037.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6587 - trunk/python_modules/plearn/math</H1>
    <B>dorionc at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6587%20-%20trunk/python_modules/plearn/math&In-Reply-To=%3C200701172224.l0HMOrdj030961%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6587 - trunk/python_modules/plearn/math">dorionc at mail.berlios.de
       </A><BR>
    <I>Wed Jan 17 23:24:53 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000035.html">[Plearn-commits] r6586 - in trunk/python_modules/plearn/math: .	stats stats/.pytest stats/.pytest/PL_cvx_numpy_matrix_conversions	stats/.pytest/PL_cvx_numpy_matrix_conversions/expected_results
</A></li>
        <LI>Next message: <A HREF="000037.html">[Plearn-commits] r6588 - trunk/python_modules/plearn/parallel
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#36">[ date ]</a>
              <a href="thread.html#36">[ thread ]</a>
              <a href="subject.html#36">[ subject ]</a>
              <a href="author.html#36">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: dorionc
Date: 2007-01-17 23:24:53 +0100 (Wed, 17 Jan 2007)
New Revision: 6587

Modified:
   trunk/python_modules/plearn/math/__init__.py
   trunk/python_modules/plearn/math/arrays.py
   trunk/python_modules/plearn/math/statistical_tools.py
Log:
Minor modifications


Modified: trunk/python_modules/plearn/math/__init__.py
===================================================================
--- trunk/python_modules/plearn/math/__init__.py	2007-01-17 22:23:25 UTC (rev 6586)
+++ trunk/python_modules/plearn/math/__init__.py	2007-01-17 22:24:53 UTC (rev 6587)
@@ -31,11 +31,11 @@
 
 # Author: Christian Dorion
 
-# Most of the content of this __init__ file has been moved to 'arrays.py'. For
-# backward comptibility, the functions defined there are forwarded
-# here. However, the 'arrays' module requires numarray... this 'try'
-# statement should make sure other stuff defined here still work even when
-# numarray is not available.
+# Most of the content of this __init__ file has been moved to
+# 'arrays.py'. For backward comptibility, the functions defined there are
+# forwarded here. However, the 'arrays' module requires numarray... this
+# 'try' statement should make sure other stuff defined here still work even
+# when numarray is not available.
 try:
     from arrays import *
 except ImportError:

Modified: trunk/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2007-01-17 22:23:25 UTC (rev 6586)
+++ trunk/python_modules/plearn/math/arrays.py	2007-01-17 22:24:53 UTC (rev 6587)
@@ -105,6 +105,14 @@
 def matrix_distance(m1, m2):
     return maximum.reduce( fabs(ravel(m1-m2)) )
 
+def matrix2vector(mat):
+    shp = shape(mat)
+    assert len(shp)==2
+    length = shp[0]*shp[1]
+
+    mat.setshape((length,))
+    return shp
+
 def mmult(*args):
     &quot;&quot;&quot;Shorthand for matrix multiplication
 
@@ -113,8 +121,15 @@
     function accepts as many matrices as one wants, processing multiplication
     'from left to right'.
     &quot;&quot;&quot;
+    #mmult_shapes(*args)
     return reduce(matrixmultiply, args)
 
+def mmult_shapes(*args):
+    &quot;&quot;&quot;For debugging purposes; print shapes.&quot;&quot;&quot;
+    for a in args:
+        print shape(a),
+    print
+
 def rrange(start, stop, step, include_stop=False):
     &quot;&quot;&quot;Behaves like the builtin range() function but with real arguments.
 
@@ -137,6 +152,35 @@
     m = asarray(m)
     return zeros(shape(m))-ufunc.less(m,0)+ufunc.greater(m,0)
 
+def symmetric(m, precision=1e-6, rel_precision=1e-6):
+    return (asymmetry(m, precision, rel_precision) is None)
+    
+def asymmetry(m, precision=1e-6, rel_precision=1e-6):
+    n, nn = shape(m)
+    if n != nn:
+        return n, nn
+    for i in range(n):
+        for j in range(i+1, n):
+            if fabs(m[i,j]-m[j,i]) &gt; max(precision, rel_precision*m[i,j]):
+                return i, j
+    return None
+
+def assert_symmetric(m, callback=None, precision=1e-6, rel_precision=1e-6):
+    asym = asymmetry(m, precision, rel_precision)
+    if asym is not None:
+        i, j = asym
+        if callback: callback()
+        raise AssertionError(&quot;Matrix is not symmetric (%d, %d):\n%s&quot;%(i,j,m))
+
+def to_diagonal(a):
+    &quot;&quot;&quot;Returns a diagonal matrix with elements in 'a' on the diagonal.&quot;&quot;&quot;
+    assert len(a.shape)==1
+    n = len(a)
+    A = zeros(shape=(n,n), type=a.type())
+    for i in range(n):
+        A[i,i] = a[i]
+    return A
+
 def fast_softmax( x ):
     s = 0
     res = []
@@ -148,10 +192,9 @@
     return [ r/s for r in res ]
 
 def hasNaN(f):
-    has = sum(isNaN(f))
-    while not isinstance(has, int):
-        has = sum(has)
-    return has
+    f = ravel(f)
+    f = choose(isNaN(f), (0, 1))
+    return sum(f)
     
 def isNaN(f):
     &quot;&quot;&quot;Return 1 where f contains NaN values, 0 elsewhere.&quot;&quot;&quot;
@@ -192,6 +235,7 @@
     
     a = [1.0, float('NaN'), 3.0, float('NaN')]
     print a
+    print hasNaN(a)
     print replace_nans(a)
 
     print 
@@ -199,3 +243,15 @@
     print kronecker(array([1, 0]), identity(3))
     print
     print kronecker(array([[1, 2],[3, 4]]), identity(3))
+
+    print
+    print &quot;matrix2vector&quot;
+    a = array([[1,2,3], [4,5,6]])
+    print a
+    a_shape = matrix2vector(a)
+    print a
+    a.setshape(a_shape)
+    print a
+    
+    
+    

Modified: trunk/python_modules/plearn/math/statistical_tools.py
===================================================================
--- trunk/python_modules/plearn/math/statistical_tools.py	2007-01-17 22:23:25 UTC (rev 6586)
+++ trunk/python_modules/plearn/math/statistical_tools.py	2007-01-17 22:24:53 UTC (rev 6587)
@@ -333,7 +333,7 @@
         prediction = mmult(_X_, aug_beta)
         epsilon[ycol][where(isNotNaN(Ycol))] = Y - prediction
 
-    # Is sigma really supposed to be NxN?
+    # The estimate of the covariance matrix of the errors
     sigma = mmult(epsilon, transpose(epsilon)) / T    
 
     # if hccm is not None:
@@ -354,99 +354,10 @@
         
     return alpha, beta, epsilon, sigma
 
-#####  Very First Sketch...  ################################################
 
-class LinearGMM(object):
-    &quot;&quot;&quot;A GMM implementation assuming a linear model and a predetermined 'X'.
+#####  Builtin Tests  #######################################################
 
-    NOTE:
-    This thing is a *very first* sketch toward a more general GMM class...
-    
-    Notation:
-    
-    X (T x k), \beta \in \RSet[k], y,u \in \RSet[T]
-       y = X \beta + u
-         where E[ u u\Tr ] = \Omega (T x T)
-    
-    Instruments W (T x l), T &gt; l, l &gt;= k, 
-       E[ u_t | W_t ] = 0;  E[ u_t u_s | W_t W_s ] = \omega_{ts}
-    
-    The above implies that:
-       E[ W_t\Tr ( y_t - X_t \beta ) ] = 0 \forall t
-    
-    Suppose J (l x k) full rank is s.t.
-       J\Tr W\Tr (y- X\beta) = 0
-    =&gt; Orthogonality conditions!!
-    =&gt; J = (W\Tr \Omega_0 W)\inv W\Tr X
-     where the 0 subscript denotes the TRUE value of \Omega
-    
-    GMM criterion:
-    
-     Q(\beta, y)
-      = (y - X \beta)\Tr W (W\Tr \Omega_0 W)\inv W\Tr (y - X\beta)
-      = T^{-0.5} (y - X \beta)\Tr W (T\inv W\Tr \Omega_0 W)\inv W\Tr (y - X\beta) T^{-0.5}
-    
-     =&gt; Q(\beta_0, y_0) \simASY \Chi^2(l)
-    
-    \betahat_{GMM}
-        = \BETA(\Omega_0)
-        = (X\Tr W (W\Tr \Omega_0 W)\inv W\Tr X)\inv X\Tr W (W\Tr \Omega_0 W)\inv W\Tr y
-    
-    Inefficient
-      \betahat_{iGMM} = (X\Tr W \Lambda W\Tr X)\inv X\Tr W \Lambda W\Tr y
-    
-    IV with W:  \uhat
-      =&gt; Let \Omegahat = Diag(\uhat)
-      =&gt; \betahat_{FGMM} = \BETA(\Omegahat)
-    
-    \Varhat( \betahat_{FGMM} ) = (X\Tr W (W\Tr \Omegahat W)\inv W\Tr X)\inv
-    &quot;&quot;&quot;
-
-    def __init__(self, y, X, W):
-        self.y = y
-        self.X = X
-        self.W = W
-
-        T = len(y)
-        self.T = T
-        assert X.shape[0] == T
-        assert W.shape[0] == T
-        assert W.shape[1] &gt;= X.shape[1]
-
-        # Performing IV with W as instruments
-        Xt       = transpose(X)
-        Wt       = transpose(W)
-        WtW      = mmult(Wt, W)
-        WtW_inv  = inverse(WtW)
-        ProjW    = mmult(W, WtW_inv, Wt) 
-        XtProjW  = mmult(Xt, ProjW)
-
-        self.beta_iv = mmult(inverse(mmult(XtProjW, X)), XtProjW, y)
-
-        # The IV residuals
-        self.uhat_iv = y - mmult(X, self.beta_iv)
-        
-        # Consistent estimator of the covariance matrix of the error terms
-        Omegahat = zeros((T, T), type=Float64)
-        for t in range(T):
-            Omegahat[t,t] = self.uhat_iv[t]**2
-
-        # Feasible GMM
-        XtW         = mmult(Xt, W)
-        WtX         = transpose(XtW)
-        Lambda      = inverse( mmult(Wt, Omegahat, W) )
-        XtWLambda   = mmult(XtW, Lambda)
-        XtWLambdaWt = mmult(XtWLambda, Wt)
-
-        # This is the (feasible) GMM beta!
-        self.beta = mmult(inverse(mmult(XtWLambdaWt, X)), XtWLambdaWt, y)
-
-        # And these are the residuals
-        self.uhat = y - mmult(X, self.beta)
-
-
-
-def test_ols_regression(T, K, alpha=0.0, scale=10.0, plot=False):
+def _test_ols_regression(T, K, alpha=0.0, scale=10.0, plot=False):
     u = normal(0, 1, (T,))
     beta = range(1, K+1)
     
@@ -491,35 +402,29 @@
 
     return X, Y, olsR, ols, scipy
 
+class StatsHolder(dict):
+    def __init__(self, **kwargs):
+        dict.__init__(self, kwargs)
+
+    __getattr__ = dict.__getitem__
+    __setattr__ = dict.__setitem__
+    __delattr__ = dict.__delitem__
+
 if __name__ == &quot;__main__&quot;:        
     from numarray.random_array import seed, random, normal
     seed(02, 25)
     
     # Setting the problem
     print &quot;T=10, K=1, (a, b)=(50, 1)&quot;
-    test_ols_regression(10, 1, 50.0, plot=True)
+    _test_ols_regression(10, 1, 50.0, plot=True)
 
     print
     print &quot;T=100, K=1, (a, b)=(25, 1)&quot;
-    test_ols_regression(100, 1, 25.0, plot=True)
+    _test_ols_regression(100, 1, 25.0, plot=True)
 
     print
     print &quot;T=100, K=3 (a, b)=(25, 1)&quot;
     try:
-        test_ols_regression(100, 3, 25.0)
+        _test_ols_regression(100, 3, 25.0)
     except:
         print &quot;SciPy FAILS!&quot;
-
-    # # Performing OLS
-    # Xt = transpose(X)
-    # XtX = mmult(Xt, X)
-    # XtY = mmult(Xt, y)
-    # beta_ols = mmult( inverse(XtX), XtY)
-    # print &quot;Distance beta_0 and beta_ols:&quot;, matrix_distance(beta, beta_ols)
-    # 
-    # gmm = LinearGMM(y, X, X)
-    # print &quot;Distance beta_ols and beta_IV:&quot;, matrix_distance(beta_ols, gmm.beta_iv)
-    # print &quot;Distance beta_ols and beta_GMM:&quot;, matrix_distance(beta_ols, gmm.beta)
-    # 
-    # print &quot;GMM residuals:&quot;
-    # print gmm.uhat


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000035.html">[Plearn-commits] r6586 - in trunk/python_modules/plearn/math: .	stats stats/.pytest stats/.pytest/PL_cvx_numpy_matrix_conversions	stats/.pytest/PL_cvx_numpy_matrix_conversions/expected_results
</A></li>
	<LI>Next message: <A HREF="000037.html">[Plearn-commits] r6588 - trunk/python_modules/plearn/parallel
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#36">[ date ]</a>
              <a href="thread.html#36">[ thread ]</a>
              <a href="subject.html#36">[ subject ]</a>
              <a href="author.html#36">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
