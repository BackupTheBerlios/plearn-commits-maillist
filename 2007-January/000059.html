<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6610 - trunk/plearn_learners/testers
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6610%20-%20trunk/plearn_learners/testers&In-Reply-To=%3C200701241921.l0OJLgTw023820%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000058.html">
   <LINK REL="Next"  HREF="000060.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6610 - trunk/plearn_learners/testers</H1>
    <B>chrish at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6610%20-%20trunk/plearn_learners/testers&In-Reply-To=%3C200701241921.l0OJLgTw023820%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6610 - trunk/plearn_learners/testers">chrish at mail.berlios.de
       </A><BR>
    <I>Wed Jan 24 20:21:42 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000058.html">[Plearn-commits] r6609 - in trunk: commands plearn_learners/online	plearn_learners/online/DEPRECATED
</A></li>
        <LI>Next message: <A HREF="000060.html">[Plearn-commits] r6611 - trunk/python_modules/plearn/utilities
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#59">[ date ]</a>
              <a href="thread.html#59">[ thread ]</a>
              <a href="subject.html#59">[ subject ]</a>
              <a href="author.html#59">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: chrish
Date: 2007-01-24 20:21:41 +0100 (Wed, 24 Jan 2007)
New Revision: 6610

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
Cleanup in PTester:perform in preparation for bug fixes:
* Use const when declaring an int or bool variable that is constant.
* Indentation and whitespace cleanup.
Change should have no impact on generated object code.


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-01-24 06:33:33 UTC (rev 6609)
+++ trunk/plearn_learners/testers/PTester.cc	2007-01-24 19:21:41 UTC (rev 6610)
@@ -407,44 +407,43 @@
 /////////////
 Vec PTester::perform(bool call_forget)
 {
-    if(!learner)
+    if (!learner)
         PLERROR(&quot;No learner specified for PTester.&quot;);
-    if(!splitter)
+    if (!splitter)
         PLERROR(&quot;No splitter specified for PTester&quot;);
 
-    int nstats;
-    nstats = statnames_processed.length();
+    const int nstats = statnames_processed.length();
     Vec global_result(nstats);
 
     {
-        if(expdir!=&quot;&quot;)
+        if (expdir != &quot;&quot;)
         {
-            if(pathexists(expdir) &amp;&amp; enforce_clean_expdir)
+            if (pathexists(expdir) &amp;&amp; enforce_clean_expdir)
                 PLERROR(&quot;Directory (or file) %s already exists.\n&quot;
                         &quot;First move it out of the way.&quot;, expdir.c_str());
-            if(!force_mkdir(expdir))
+            if (!force_mkdir(expdir))
                 PLERROR(&quot;In PTester Could not create experiment directory %s&quot;,expdir.c_str());
             expdir = expdir.absolute() / &quot;&quot;;
 
             // Save this tester description in the expdir
-            if(save_initial_tester)
-                PLearn::save( expdir / &quot;tester.psave&quot;, *this);
+            if (save_initial_tester)
+                PLearn::save(expdir / &quot;tester.psave&quot;, *this);
         }
 
         splitter-&gt;setDataSet(dataset);
 
-        int nsplits = splitter-&gt;nsplits();
-        if(nsplits&gt;1)
+        const int nsplits = splitter-&gt;nsplits();
+        if (nsplits &gt; 1)
             call_forget = true;
 
         TVec&lt;string&gt; testcostnames = learner-&gt;getTestCostNames();
         TVec&lt;string&gt; traincostnames = learner-&gt;getTrainCostNames();
 
-        int nsets = splitter-&gt;nSetsPerSplit();
+        const int nsets = splitter-&gt;nSetsPerSplit();
 
         // Stats collectors for individual sets of a split:
         TVec&lt; PP&lt;VecStatsCollector&gt; &gt; stcol(nsets);
-        for(int setnum=0; setnum&lt;nsets; setnum++)
+        for (int setnum = 0; setnum &lt; nsets; setnum++)
         {
             if (template_stats_collector)
             {
@@ -454,7 +453,7 @@
             else
                 stcol[setnum] = new VecStatsCollector();
 
-            if(setnum==0)
+            if (setnum == 0)
                 stcol[setnum]-&gt;setFieldNames(traincostnames);
             else
                 stcol[setnum]-&gt;setFieldNames(testcostnames);
@@ -480,9 +479,11 @@
 
         // Stat specs
         TVec&lt;StatSpec&gt; statspecs(nstats);
-        for(int k=0; k&lt;nstats; k++) {
+        for(int k = 0; k &lt; nstats; k++)
+        {
             statspecs[k].init(statnames_processed[k]);
         }
+        
         // Hack to accumulate statistics over splits. We store in 'acc' the sets
         // which need to accumulate statistics.
         TVec&lt;int&gt; acc;
@@ -497,69 +498,73 @@
             else if (acc.find(statspecs[k].setnum) != -1)
                 PLERROR(&quot;In PTester::perform - You can't have stats with and without 'ACC' for set %d&quot;, statspecs[k].setnum);
 
-        // int traincostsize = traincostnames.size();
-        // int testcostsize = testcostnames.size();
-
-        VMat global_stats_vm;    // the vmat in which to save global result stats specified in statnames
-        VMat split_stats_vm;   // the vmat in which to save per split result stats
-        if(expdir!=&quot;&quot; &amp;&amp; report_stats)
+        // The vmat in which to save global result stats specified in statnames
+        VMat global_stats_vm;
+        // The vmat in which to save per split result stats
+        VMat split_stats_vm;
+        
+        if (expdir != &quot;&quot; &amp;&amp; report_stats)
         {
-            saveStringInFile(expdir/&quot;train_cost_names.txt&quot;, join(traincostnames,&quot;\n&quot;)+&quot;\n&quot;);
-            saveStringInFile(expdir/&quot;test_cost_names.txt&quot;, join(testcostnames,&quot;\n&quot;)+&quot;\n&quot;);
+            saveStringInFile(expdir / &quot;train_cost_names.txt&quot;, join(traincostnames, &quot;\n&quot;) + &quot;\n&quot;);
+            saveStringInFile(expdir / &quot;test_cost_names.txt&quot;, join(testcostnames, &quot;\n&quot;) + &quot;\n&quot;);
 
             global_stats_vm = new FileVMatrix(expdir/&quot;global_stats.pmat&quot;, 1, nstats);
-            for(int k=0; k&lt;nstats; k++)
-                global_stats_vm-&gt;declareField(k,statspecs[k].statName());
+            for (int k = 0; k &lt; nstats; k++)
+                global_stats_vm-&gt;declareField(k, statspecs[k].statName());
             global_stats_vm-&gt;saveFieldInfos();
 
-            split_stats_vm = new FileVMatrix(expdir/&quot;split_stats.pmat&quot;, 0, 1+nstats);
-            split_stats_vm-&gt;declareField(0,&quot;splitnum&quot;);
-            for(int k=0; k&lt;nstats; k++)
-                split_stats_vm-&gt;declareField(k+1,statspecs[k].setname + &quot;.&quot; + statspecs[k].intstatname);
+            split_stats_vm = new FileVMatrix(expdir / &quot;split_stats.pmat&quot;, 0, 1+nstats);
+            split_stats_vm-&gt;declareField(0, &quot;splitnum&quot;);
+            for (int k = 0; k &lt; nstats; k++)
+                split_stats_vm-&gt;declareField(k+1, statspecs[k].setname + &quot;.&quot; + statspecs[k].intstatname);
             split_stats_vm-&gt;saveFieldInfos();
         }
 
-        for(int splitnum=0; splitnum&lt;nsplits; splitnum++)
+        for (int splitnum = 0; splitnum &lt; nsplits; splitnum++)
         {
             PPath splitdir;
             bool is_splitdir = false;
-            if(!expdir.isEmpty()) {
-                splitdir = expdir / (&quot;Split&quot;+tostring(splitnum));
+            if(!expdir.isEmpty())
+            {
+                splitdir = expdir / (&quot;Split&quot; + tostring(splitnum));
                 is_splitdir = true;
             }
 
             TVec&lt;VMat&gt; dsets = splitter-&gt;getSplit(splitnum);
             VMat trainset = dsets[0];
-            if(is_splitdir &amp;&amp; save_data_sets)
-                PLearn::save(splitdir/&quot;training_set.psave&quot;,trainset);
+            if (is_splitdir &amp;&amp; save_data_sets)
+                PLearn::save(splitdir / &quot;training_set.psave&quot;, trainset);
 
-            if(should_train &amp;&amp; provide_learner_expdir)
+            if (should_train &amp;&amp; provide_learner_expdir)
             {
-                if(is_splitdir)
-                    learner-&gt;setExperimentDirectory( splitdir/&quot;LearnerExpdir/&quot; );
+                if (is_splitdir)
+                    learner-&gt;setExperimentDirectory(splitdir / &quot;LearnerExpdir/&quot;);
                 else
                     learner-&gt;setExperimentDirectory(&quot;&quot;);
             }
 
             learner-&gt;setTrainingSet(trainset, call_forget &amp;&amp; should_train);
-            if(dsets.size()&gt;1)
+            if (dsets.size() &gt; 1)
                 learner-&gt;setValidationSet(dsets[1]);
 
-            int outputsize = learner-&gt;outputsize();
+            const int outputsize = learner-&gt;outputsize();
 
-
             if (should_train)
             {
-                if(is_splitdir &amp;&amp; save_initial_learners)
-                    PLearn::save(splitdir/&quot;initial_learner.psave&quot;,learner);
+                if (is_splitdir &amp;&amp; save_initial_learners)
+                    PLearn::save(splitdir / &quot;initial_learner.psave&quot;, learner);
 
                 train_stats-&gt;forget();
                 learner-&gt;train();
                 train_stats-&gt;finalize();
-                if(is_splitdir &amp;&amp; save_stat_collectors)
-                    PLearn::save(splitdir/&quot;train_stats.psave&quot;,train_stats);
-                if(is_splitdir &amp;&amp; save_learners)
-                    PLearn::save(splitdir/&quot;final_learner.psave&quot;,learner);
+
+                if (is_splitdir)
+                {
+                    if (save_stat_collectors)
+                        PLearn::save(splitdir / &quot;train_stats.psave&quot;, train_stats);
+                    if (save_learners)
+                        PLearn::save(splitdir / &quot;final_learner.psave&quot;, learner);
+                }
             }
             else
                 learner-&gt;build();
@@ -569,46 +574,52 @@
             TVec&lt; map&lt;string, map&lt;string, real&gt; &gt; &gt; perf_eval_costs(dsets.length());
 
             // Perform the test if required
-            if (should_test) {
-                for(int setnum=1; setnum&lt;dsets.length(); setnum++)
+            if (should_test)
+            {
+                for (int setnum = 1; setnum &lt; dsets.length(); setnum++)
                 {
                     VMat testset = dsets[setnum];
                     PP&lt;VecStatsCollector&gt; test_stats = stcol[setnum];
-                    string setname = &quot;test&quot;+tostring(setnum);
-                    if(is_splitdir &amp;&amp; save_data_sets)
-                        PLearn::save(splitdir/(setname+&quot;_set.psave&quot;),testset);
+                    const string setname = &quot;test&quot; + tostring(setnum);
+                    if (is_splitdir &amp;&amp; save_data_sets)
+                        PLearn::save(splitdir / (setname + &quot;_set.psave&quot;), testset);
                     VMat test_outputs;
                     VMat test_costs;
                     VMat test_confidence;
                     if (is_splitdir)
                         force_mkdir(splitdir); // TODO Why is this done so late?
 
-                    if(is_splitdir &amp;&amp; save_test_outputs)
-                        test_outputs = new FileVMatrix(splitdir/(setname+&quot;_outputs.pmat&quot;),0,learner-&gt;getOutputNames());
-                    //test_outputs = new FileVMatrix(splitdir/(setname+&quot;_outputs.pmat&quot;),0,outputsize);
-                    else if(!perf_evaluators.empty()) // we don't want to save test outputs to disk, but we need them for pef_evaluators
-                    { // So let's store them in a MemoryVMatrix
+                    if (is_splitdir &amp;&amp; save_test_outputs)
+                        test_outputs = new FileVMatrix(splitdir / (setname + &quot;_outputs.pmat&quot;),
+                                                       0, learner-&gt;getOutputNames());
+                    else if (!perf_evaluators.empty())
+                    {
+                        // We don't want to save test outputs to disk, but we
+                        // need them for pef_evaluators. So let's store them in
+                        // a MemoryVMatrix
                         Mat data(testset.length(),outputsize);
                         data.resize(0,outputsize);
                         test_outputs = new MemoryVMatrix(data);
                         test_outputs-&gt;declareFieldNames(learner-&gt;getOutputNames());
                     }
 
-                    if(is_splitdir &amp;&amp; save_test_costs)
-                        test_costs = new FileVMatrix(splitdir/(setname+&quot;_costs.pmat&quot;),0,learner-&gt;getTestCostNames());
-                    //test_costs = new FileVMatrix(splitdir/(setname+&quot;_costs.pmat&quot;),0,testcostsize);
-                    if(is_splitdir &amp;&amp; save_test_confidence)
-                        test_confidence = new FileVMatrix(splitdir/(setname+&quot;_confidence.pmat&quot;),
-                                                          0,2*outputsize);
+                    if (is_splitdir)
+                    {
+                        if (save_test_costs)
+                            test_costs = new FileVMatrix(splitdir / (setname + &quot;_costs.pmat&quot;),
+                                                         0, learner-&gt;getTestCostNames());
+                        if (save_test_confidence)
+                            test_confidence = new FileVMatrix(splitdir / (setname + &quot;_confidence.pmat&quot;),
+                                                              0, 2 * outputsize);
+                    }
 
-                    bool reset_stats = (acc.find(setnum) == -1);
-
-                    //perr &lt;&lt; &quot;reset_stats= &quot; &lt;&lt; reset_stats &lt;&lt; endl;
-
+                    const bool reset_stats = (acc.find(setnum) == -1);
                     if (reset_stats)
                         test_stats-&gt;forget();
-                    if (testset-&gt;length()==0)
-                        PLWARNING(&quot;PTester:: test set %s is of length 0, costs will be set to -1&quot;,setname.c_str());
+                    
+                    if (testset-&gt;length() == 0)
+                        PLWARNING(&quot;PTester:: test set %s is of length 0, costs will be set to -1&quot;,
+                                  setname.c_str());
 
                     // Before each test set, reset the internal state of the learner
                     learner-&gt;resetInternalState();
@@ -616,22 +627,22 @@
                     learner-&gt;test(testset, test_stats, test_outputs, test_costs);
                     if (reset_stats)
                         test_stats-&gt;finalize();
-                    if(is_splitdir &amp;&amp; save_stat_collectors)
-                        PLearn::save(splitdir/(setname+&quot;_stats.psave&quot;),test_stats);
+                    if (is_splitdir &amp;&amp; save_stat_collectors)
+                        PLearn::save(splitdir / (setname + &quot;_stats.psave&quot;), test_stats);
 
                     perf_evaluators_t::iterator it = perf_evaluators.begin();
-                    perf_evaluators_t::iterator itend = perf_evaluators.end();
-                    while(it!=itend)
+                    const perf_evaluators_t::iterator itend = perf_evaluators.end();
+                    while(it != itend)
                     {
                         PPath perf_eval_dir;
-                        if(is_splitdir)
-                            perf_eval_dir = splitdir/setname/(&quot;perfeval_&quot;+it-&gt;first);
+                        if (is_splitdir)
+                            perf_eval_dir = splitdir / setname / (&quot;perfeval_&quot; + it-&gt;first);
                         Vec perf_costvals = it-&gt;second-&gt;evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
                         TVec&lt;string&gt; perf_costnames = it-&gt;second-&gt;getCostNames();
-                        if(perf_costvals.length()!=perf_costnames.length())
+                        if (perf_costvals.length()!=perf_costnames.length())
                             PLERROR(&quot;vector of costs returned by performance evaluator differ in size with its vector of costnames&quot;);
                         map&lt;string, real&gt;&amp; costmap = perf_eval_costs[setnum][it-&gt;first];
-                        for(int costi = 0; costi&lt;perf_costnames.length(); costi++)
+                        for (int costi = 0; costi &lt; perf_costnames.length(); costi++)
                             costmap[perf_costnames[costi]] = perf_costvals[costi];
                         ++it;
                     }
@@ -639,10 +650,10 @@
                 }
             }
 
-            Vec splitres(1+nstats);
+            Vec splitres(1 + nstats);
             splitres[0] = splitnum;
 
-            for(int k=0; k&lt;nstats; k++)
+            for (int k = 0; k &lt; nstats; k++)
             {
                 // If we ask for a test-set that's beyond what's currently
                 // available, OR we are asking for test-statistics in
@@ -659,14 +670,17 @@
                     {
                         string left, right;
                         split_on_first(sp.intstatname, &quot;.&quot;,left,right);
-                        if(right!=&quot;&quot; &amp;&amp; perf_evaluators.find(left)!=perf_evaluators.end())
-                        { // looks like a cost from a performance evaluator
+                        if (right != &quot;&quot; &amp;&amp; perf_evaluators.find(left) != perf_evaluators.end())
+                        {
+                            // looks like a cost from a performance evaluator
                             map&lt;string, real&gt;&amp; costmap = perf_eval_costs[sp.setnum][left];
-                            if(costmap.find(right)==costmap.end())
-                                PLERROR(&quot;No cost named %s appears to be returned by evaluator %s&quot;,right.c_str(),left.c_str());
+                            if (costmap.find(right) == costmap.end())
+                                PLERROR(&quot;No cost named %s appears to be returned by evaluator %s&quot;,
+                                        right.c_str(), left.c_str());
                             splitres[k+1] = costmap[right];
                         }
-                        else // must be a cost from a stats collector
+                        else
+                            // must be a cost from a stats collector
                             splitres[k+1] = stcol[sp.setnum]-&gt;getStat(sp.intstatname);
                     }
                     else
@@ -674,37 +688,39 @@
                 }
             }
 
-            if(split_stats_vm)
+            if (split_stats_vm)
             {
                 split_stats_vm-&gt;appendRow(splitres);
                 split_stats_vm-&gt;flush();
             }
 
-            global_statscol-&gt;update(splitres.subVec(1,nstats));
+            global_statscol-&gt;update(splitres.subVec(1, nstats));
         }
 
 
         global_statscol-&gt;finalize();
-        for(int k=0; k&lt;nstats; k++) {
+        for (int k = 0; k &lt; nstats; k++)
+        {
             if (acc.find(statspecs[k].setnum) == -1)
                 global_result[k] = global_statscol-&gt;getStats(k).getStat(statspecs[k].extstat);
-            else {
-                int j = statspecs[k].setnum;
+            else
+            {
+                const int j = statspecs[k].setnum;
                 stcol[j]-&gt;finalize();
                 global_result[k] = stcol[j]-&gt;getStat(statspecs[k].intstatname);
             }
         }
 
-        if(global_stats_vm)
+        if (global_stats_vm)
             global_stats_vm-&gt;appendRow(global_result);
-
     }
 
 #if USING_MPI
-    if (PLMPI::rank==0)
+    if (PLMPI::rank == 0)
 #endif
     // Perform the final commands provided in final_commands.
-    for (int i = 0; i &lt; final_commands.length(); i++) {
+    for (int i = 0; i &lt; final_commands.length(); i++)
+    {
         system(final_commands[i].c_str());
     }
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000058.html">[Plearn-commits] r6609 - in trunk: commands plearn_learners/online	plearn_learners/online/DEPRECATED
</A></li>
	<LI>Next message: <A HREF="000060.html">[Plearn-commits] r6611 - trunk/python_modules/plearn/utilities
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#59">[ date ]</a>
              <a href="thread.html#59">[ thread ]</a>
              <a href="subject.html#59">[ subject ]</a>
              <a href="author.html#59">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
