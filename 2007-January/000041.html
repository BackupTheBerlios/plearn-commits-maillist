<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6592 - trunk/plearn_learners/testers
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6592%20-%20trunk/plearn_learners/testers&In-Reply-To=%3C200701222111.l0MLB7v4005026%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000040.html">
   <LINK REL="Next"  HREF="000042.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6592 - trunk/plearn_learners/testers</H1>
    <B>chrish at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6592%20-%20trunk/plearn_learners/testers&In-Reply-To=%3C200701222111.l0MLB7v4005026%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6592 - trunk/plearn_learners/testers">chrish at mail.berlios.de
       </A><BR>
    <I>Mon Jan 22 22:11:07 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000040.html">[Plearn-commits] r6591 - in trunk/python_modules/plearn/math/stats:	. .pytest .pytest/PL_cvx_numarray_matrix_conversions	.pytest/PL_cvx_numarray_matrix_conversions/expected_results
</A></li>
        <LI>Next message: <A HREF="000042.html">[Plearn-commits] r6593 - trunk/plearn/vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#41">[ date ]</a>
              <a href="thread.html#41">[ thread ]</a>
              <a href="subject.html#41">[ subject ]</a>
              <a href="author.html#41">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: chrish
Date: 2007-01-22 22:11:06 +0100 (Mon, 22 Jan 2007)
New Revision: 6592

Modified:
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
* Add should_test option to PTester, so we can use the PTester to do just
  the training phase of the experiment, and do the testing later.
* Remove some dead, commented-out code.


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-01-22 03:50:16 UTC (rev 6591)
+++ trunk/plearn_learners/testers/PTester.cc	2007-01-22 21:11:06 UTC (rev 6592)
@@ -90,6 +90,7 @@
        call_forget_in_run(true),
        save_test_confidence(false),
        train(true),
+       should_test(true),
        enforce_clean_expdir(true)
 {}
 
@@ -213,6 +214,13 @@
         &quot;to load an already trained learner in the 'learner' field)&quot;);
 
     declareOption(
+        ol, &quot;should_test&quot;, &amp;PTester::should_test, OptionBase::buildoption,
+        &quot;Whether to carry out the test at all. This can be used, for instance,\n&quot;
+        &quot;to train only (without testing) and save the learners, and test later. \n&quot;
+        &quot;Any test statistics that are required to be computed if 'should_test'\n&quot;
+        &quot;is false yield MISSING_VALUE.\n&quot;);
+    
+    declareOption(
         ol, &quot;template_stats_collector&quot;, &amp;PTester::template_stats_collector, OptionBase::buildoption,
         &quot;If provided, this instance of a subclass of VecStatsCollector will be used as a template\n&quot;
         &quot;to build all the stats collector used during training and testing of the learner&quot;);
@@ -286,52 +294,7 @@
          RetDoc (&quot;Current expdir.&quot;)));
 }
 
-// The following is no longer necessary with the declareMethod mechanism
-// (to be deleted)
 
-/**
- * void PTester::call(const string&amp; methodname, int nargs, PStream&amp; io)
- * {
- *     if(methodname==&quot;perform&quot;)
- *     {
- *         if(nargs!=1) PLERROR(&quot;PTester remote method perform takes 1 argument&quot;);
- *         bool call_forget;
- *         io &gt;&gt; call_forget;
- *         Vec result = perform(call_forget);
- *         prepareToSendResults(io, 1);
- *         io &lt;&lt; result;
- *         io.flush();
- *     }
- *     else if(methodname==&quot;getStatNames&quot;)
- *     {
- *         if(nargs!=0) PLERROR(&quot;PTester remote method getStatNames takes 0 argument&quot;);
- *         TVec&lt;string&gt; result = getStatNames();
- *         prepareToSendResults(io, 1);
- *         io &lt;&lt; result;
- *         io.flush();
- *     }
- *     else if(methodname==&quot;setExperimentDirectory&quot;)
- *     {
- *         if(nargs!=1) PLERROR(&quot;PTester remote method setExperimentDirectory takes 1 argument&quot;);
- *         PPath the_expdir;
- *         io &gt;&gt; the_expdir;
- *         setExperimentDirectory(the_expdir);
- *         prepareToSendResults(io, 0);
- *         io.flush();
- *     }
- *     else if(methodname==&quot;getExperimentDirectory&quot;)
- *     {
- *         if(nargs!=0) PLERROR(&quot;PTester remote method getExperimentDirectory takes 0 arguments&quot;);
- *         PPath result = getExperimentDirectory();
- *         prepareToSendResults(io, 1);
- *         io &lt;&lt; result;
- *         io.flush();
- *     }
- *     else
- *         inherited::call(methodname, nargs, io);
- * }
- */
-
 void PTester::build_()
 {
 
@@ -598,72 +561,76 @@
             // perf_eval_costs[setnum][perf_evaluator_name][costname] will contain value
             // of the given cost returned by the given perf_evaluator on the given setnum
             TVec&lt; map&lt;string, map&lt;string, real&gt; &gt; &gt; perf_eval_costs(dsets.length());
-            for(int setnum=1; setnum&lt;dsets.length(); setnum++)
-            {
-                VMat testset = dsets[setnum];
-                PP&lt;VecStatsCollector&gt; test_stats = stcol[setnum];
-                string setname = &quot;test&quot;+tostring(setnum);
-                if(is_splitdir &amp;&amp; save_data_sets)
-                    PLearn::save(splitdir/(setname+&quot;_set.psave&quot;),testset);
-                VMat test_outputs;
-                VMat test_costs;
-                VMat test_confidence;
-                if (is_splitdir)
-                    force_mkdir(splitdir); // TODO Why is this done so late?
 
-                if(is_splitdir &amp;&amp; save_test_outputs)
-                    test_outputs = new FileVMatrix(splitdir/(setname+&quot;_outputs.pmat&quot;),0,learner-&gt;getOutputNames());
+            // Perform the test if required
+            if (should_test) {
+                for(int setnum=1; setnum&lt;dsets.length(); setnum++)
+                {
+                    VMat testset = dsets[setnum];
+                    PP&lt;VecStatsCollector&gt; test_stats = stcol[setnum];
+                    string setname = &quot;test&quot;+tostring(setnum);
+                    if(is_splitdir &amp;&amp; save_data_sets)
+                        PLearn::save(splitdir/(setname+&quot;_set.psave&quot;),testset);
+                    VMat test_outputs;
+                    VMat test_costs;
+                    VMat test_confidence;
+                    if (is_splitdir)
+                        force_mkdir(splitdir); // TODO Why is this done so late?
+
+                    if(is_splitdir &amp;&amp; save_test_outputs)
+                        test_outputs = new FileVMatrix(splitdir/(setname+&quot;_outputs.pmat&quot;),0,learner-&gt;getOutputNames());
                     //test_outputs = new FileVMatrix(splitdir/(setname+&quot;_outputs.pmat&quot;),0,outputsize);
-                else if(!perf_evaluators.empty()) // we don't want to save test outputs to disk, but we need them for pef_evaluators
-                { // So let's store them in a MemoryVMatrix
-                    Mat data(testset.length(),outputsize);
-                    data.resize(0,outputsize);
-                    test_outputs = new MemoryVMatrix(data);
-                    test_outputs-&gt;declareFieldNames(learner-&gt;getOutputNames());
-                }
+                    else if(!perf_evaluators.empty()) // we don't want to save test outputs to disk, but we need them for pef_evaluators
+                    { // So let's store them in a MemoryVMatrix
+                        Mat data(testset.length(),outputsize);
+                        data.resize(0,outputsize);
+                        test_outputs = new MemoryVMatrix(data);
+                        test_outputs-&gt;declareFieldNames(learner-&gt;getOutputNames());
+                    }
 
-                if(is_splitdir &amp;&amp; save_test_costs)
-                    test_costs = new FileVMatrix(splitdir/(setname+&quot;_costs.pmat&quot;),0,learner-&gt;getTestCostNames());
+                    if(is_splitdir &amp;&amp; save_test_costs)
+                        test_costs = new FileVMatrix(splitdir/(setname+&quot;_costs.pmat&quot;),0,learner-&gt;getTestCostNames());
                     //test_costs = new FileVMatrix(splitdir/(setname+&quot;_costs.pmat&quot;),0,testcostsize);
-                if(is_splitdir &amp;&amp; save_test_confidence)
-                    test_confidence = new FileVMatrix(splitdir/(setname+&quot;_confidence.pmat&quot;),
-                                                      0,2*outputsize);
+                    if(is_splitdir &amp;&amp; save_test_confidence)
+                        test_confidence = new FileVMatrix(splitdir/(setname+&quot;_confidence.pmat&quot;),
+                                                          0,2*outputsize);
 
-                bool reset_stats = (acc.find(setnum) == -1);
+                    bool reset_stats = (acc.find(setnum) == -1);
 
-                //perr &lt;&lt; &quot;reset_stats= &quot; &lt;&lt; reset_stats &lt;&lt; endl;
+                    //perr &lt;&lt; &quot;reset_stats= &quot; &lt;&lt; reset_stats &lt;&lt; endl;
 
-                if (reset_stats)
-                    test_stats-&gt;forget();
-                if (testset-&gt;length()==0)
-                    PLWARNING(&quot;PTester:: test set %s is of length 0, costs will be set to -1&quot;,setname.c_str());
+                    if (reset_stats)
+                        test_stats-&gt;forget();
+                    if (testset-&gt;length()==0)
+                        PLWARNING(&quot;PTester:: test set %s is of length 0, costs will be set to -1&quot;,setname.c_str());
 
-                // Before each test set, reset the internal state of the learner
-                learner-&gt;resetInternalState();
+                    // Before each test set, reset the internal state of the learner
+                    learner-&gt;resetInternalState();
 
-                learner-&gt;test(testset, test_stats, test_outputs, test_costs);
-                if (reset_stats)
-                    test_stats-&gt;finalize();
-                if(is_splitdir &amp;&amp; save_stat_collectors)
-                    PLearn::save(splitdir/(setname+&quot;_stats.psave&quot;),test_stats);
+                    learner-&gt;test(testset, test_stats, test_outputs, test_costs);
+                    if (reset_stats)
+                        test_stats-&gt;finalize();
+                    if(is_splitdir &amp;&amp; save_stat_collectors)
+                        PLearn::save(splitdir/(setname+&quot;_stats.psave&quot;),test_stats);
 
-                perf_evaluators_t::iterator it = perf_evaluators.begin();
-                perf_evaluators_t::iterator itend = perf_evaluators.end();
-                while(it!=itend)
-                {
-                    PPath perf_eval_dir;
-                    if(is_splitdir)
-                        perf_eval_dir = splitdir/setname/(&quot;perfeval_&quot;+it-&gt;first);
-                    Vec perf_costvals = it-&gt;second-&gt;evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
-                    TVec&lt;string&gt; perf_costnames = it-&gt;second-&gt;getCostNames();
-                    if(perf_costvals.length()!=perf_costnames.length())
-                        PLERROR(&quot;vector of costs returned by performance evaluator differ in size with its vector of costnames&quot;);
-                    map&lt;string, real&gt;&amp; costmap = perf_eval_costs[setnum][it-&gt;first];
-                    for(int costi = 0; costi&lt;perf_costnames.length(); costi++)
-                        costmap[perf_costnames[costi]] = perf_costvals[costi];
-                    ++it;
+                    perf_evaluators_t::iterator it = perf_evaluators.begin();
+                    perf_evaluators_t::iterator itend = perf_evaluators.end();
+                    while(it!=itend)
+                    {
+                        PPath perf_eval_dir;
+                        if(is_splitdir)
+                            perf_eval_dir = splitdir/setname/(&quot;perfeval_&quot;+it-&gt;first);
+                        Vec perf_costvals = it-&gt;second-&gt;evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
+                        TVec&lt;string&gt; perf_costnames = it-&gt;second-&gt;getCostNames();
+                        if(perf_costvals.length()!=perf_costnames.length())
+                            PLERROR(&quot;vector of costs returned by performance evaluator differ in size with its vector of costnames&quot;);
+                        map&lt;string, real&gt;&amp; costmap = perf_eval_costs[setnum][it-&gt;first];
+                        for(int costi = 0; costi&lt;perf_costnames.length(); costi++)
+                            costmap[perf_costnames[costi]] = perf_costvals[costi];
+                        ++it;
+                    }
+                    computeConfidence(testset, test_confidence);
                 }
-                computeConfidence(testset, test_confidence);
             }
 
             Vec splitres(1+nstats);
@@ -671,11 +638,15 @@
 
             for(int k=0; k&lt;nstats; k++)
             {
+                // If we ask for a test-set that's beyond what's currently
+                // available, OR we are asking for test-statistics in
+                // train-only mode, then the statistic is MISSING_VALUE.
                 StatSpec&amp; sp = statspecs[k];
-                if (sp.setnum&gt;=stcol.length())
+                if (sp.setnum&gt;=stcol.length() ||
+                    (! should_test &amp;&amp; sp.setnum &gt; 0))
+                {
                     splitres[k+1] = MISSING_VALUE;
-//            PLERROR(&quot;PTester::perform, trying to access a test set (test%d) beyond the last one (test%d)&quot;,
-//                    sp.setnum, stcol.length()-1);
+                }
                 else
                 {
                     if (acc.find(sp.setnum) == -1)

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2007-01-22 03:50:16 UTC (rev 6591)
+++ trunk/plearn_learners/testers/PTester.h	2007-01-22 21:11:06 UTC (rev 6592)
@@ -102,9 +102,15 @@
     /// intervals are saved in a file SETNAME_confidence.pmat (default=false)
     bool save_test_confidence;
   
-    /// whether or not to train or just test
+    /// whether or not to train or just test (see 'should_test', below)
     bool train;
 
+    /// Whether to carry out the test at all. This can be used, for instance,
+    /// to train only (without testing) and save the learners, and test later. 
+    /// Any test statistics that are required to be computed if 'should_test'
+    /// is false yield MISSING_VALUE.
+    bool should_test;
+
     /**
      *  If this option is true, the PTester ensures that the expdir does not
      *  already exist when the experiment is started, and gives a PLerror


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000040.html">[Plearn-commits] r6591 - in trunk/python_modules/plearn/math/stats:	. .pytest .pytest/PL_cvx_numarray_matrix_conversions	.pytest/PL_cvx_numarray_matrix_conversions/expected_results
</A></li>
	<LI>Next message: <A HREF="000042.html">[Plearn-commits] r6593 - trunk/plearn/vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#41">[ date ]</a>
              <a href="thread.html#41">[ thread ]</a>
              <a href="subject.html#41">[ subject ]</a>
              <a href="author.html#41">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
