From nouiz at mail.berlios.de  Tue Sep  2 16:29:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 16:29:07 +0200
Subject: [Plearn-commits] r9417 - trunk/scripts
Message-ID: <200809021429.m82ET7jC013000@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 16:29:05 +0200 (Tue, 02 Sep 2008)
New Revision: 9417

Modified:
   trunk/scripts/dbidispatch
Log:
don't add empty line to the list of jobs to exec


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-08-29 18:43:21 UTC (rev 9416)
+++ trunk/scripts/dbidispatch	2008-09-02 14:29:05 UTC (rev 9417)
@@ -343,6 +343,8 @@
     choise_args = []
     for line in FD.readlines():
         line = line.rstrip()
+        if not line:
+            continue
         sp = line.split(" ")
         (t1,t2)=generate_commands(sp)
         commands+=t1



From nouiz at mail.berlios.de  Tue Sep  2 17:33:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 17:33:49 +0200
Subject: [Plearn-commits] r9418 - in trunk/plearn_learners: distributions
	distributions/DEPRECATED online/DEPRECATED
Message-ID: <200809021533.m82FXnqx019762@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 17:33:46 +0200 (Tue, 02 Sep 2008)
New Revision: 9418

Modified:
   trunk/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.cc
   trunk/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.h
   trunk/plearn_learners/distributions/DEPRECATED/Distribution.cc
   trunk/plearn_learners/distributions/DEPRECATED/Distribution.h
   trunk/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.cc
   trunk/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.h
   trunk/plearn_learners/distributions/DEPRECATED/GaussianProcessRegressor.h
   trunk/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.cc
   trunk/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.h
   trunk/plearn_learners/distributions/GaussianDistribution.cc
   trunk/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.cc
   trunk/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.h
   trunk/plearn_learners/online/DEPRECATED/GaussianDBNClassification.cc
   trunk/plearn_learners/online/DEPRECATED/GaussianDBNClassification.h
   trunk/plearn_learners/online/DEPRECATED/GaussianDBNRegression.cc
   trunk/plearn_learners/online/DEPRECATED/GaussianDBNRegression.h
   trunk/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.cc
   trunk/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.h
   trunk/plearn_learners/online/DEPRECATED/NLLErrModule.cc
   trunk/plearn_learners/online/DEPRECATED/NLLErrModule.h
   trunk/plearn_learners/online/DEPRECATED/PartSupervisedDBN.cc
   trunk/plearn_learners/online/DEPRECATED/PartSupervisedDBN.h
   trunk/plearn_learners/online/DEPRECATED/RBMBinomialLayer.cc
   trunk/plearn_learners/online/DEPRECATED/RBMBinomialLayer.h
   trunk/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.cc
   trunk/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.h
   trunk/plearn_learners/online/DEPRECATED/RBMGaussianLayer.cc
   trunk/plearn_learners/online/DEPRECATED/RBMGaussianLayer.h
   trunk/plearn_learners/online/DEPRECATED/RBMGenericParameters.cc
   trunk/plearn_learners/online/DEPRECATED/RBMGenericParameters.h
   trunk/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.cc
   trunk/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.h
   trunk/plearn_learners/online/DEPRECATED/RBMJointLLParameters.cc
   trunk/plearn_learners/online/DEPRECATED/RBMJointLLParameters.h
   trunk/plearn_learners/online/DEPRECATED/RBMLLParameters.cc
   trunk/plearn_learners/online/DEPRECATED/RBMLLParameters.h
   trunk/plearn_learners/online/DEPRECATED/RBMLQParameters.cc
   trunk/plearn_learners/online/DEPRECATED/RBMLQParameters.h
   trunk/plearn_learners/online/DEPRECATED/RBMLayer.cc
   trunk/plearn_learners/online/DEPRECATED/RBMLayer.h
   trunk/plearn_learners/online/DEPRECATED/RBMMixedLayer.cc
   trunk/plearn_learners/online/DEPRECATED/RBMMixedLayer.h
   trunk/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.h
   trunk/plearn_learners/online/DEPRECATED/RBMParameters.cc
   trunk/plearn_learners/online/DEPRECATED/RBMParameters.h
   trunk/plearn_learners/online/DEPRECATED/RBMQLParameters.cc
   trunk/plearn_learners/online/DEPRECATED/RBMQLParameters.h
   trunk/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.cc
   trunk/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.h
   trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.cc
   trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.h
   trunk/plearn_learners/online/DEPRECATED/StackedModulesLearner.cc
   trunk/plearn_learners/online/DEPRECATED/StackedModulesLearner.h
   trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.cc
   trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.h
   trunk/plearn_learners/online/DEPRECATED/SupervisedDBN.cc
   trunk/plearn_learners/online/DEPRECATED/SupervisedDBN.h
   trunk/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.cc
   trunk/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.h
   trunk/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.cc
   trunk/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.h
Log:
removed doxymen warning caused by bad doxygen directive


Modified: trunk/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.cc
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Yoshua Bengio
 
-/*! \file ConditionalDensityNet.cc */
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.cc */
 
 #include <plearn/var/AffineTransformVariable.h>
 #include <plearn/var/AffineTransformWeightPenalty.h>

Modified: trunk/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.h
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Yoshua Bengio
 
-/*! \file ConditionalDensityNet.h */
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/ConditionalDensityNet.h */
 
 
 #ifndef ConditionalDensityNet_INC

Modified: trunk/plearn_learners/distributions/DEPRECATED/Distribution.cc
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/Distribution.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/Distribution.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -40,7 +40,7 @@
  * $Id$ 
  ******************************************************* */
 
-/*! \file Distribution.cc */
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/Distribution.cc */
 #include "Distribution.h"
 #include <plearn/ker/NegOutputCostFunction.h>
 

Modified: trunk/plearn_learners/distributions/DEPRECATED/Distribution.h
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/Distribution.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/Distribution.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -40,7 +40,7 @@
  * $Id$ 
  ******************************************************* */
 
-/*! \file Distribution.h */
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/Distribution.h */
 #ifndef Distribution_INC
 #define Distribution_INC
 

Modified: trunk/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Yoshua Bengio & Martin Monperrus
 
-/*! \file GaussianContinuumDistribution.cc */
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.cc */
 
 
 #include "GaussianContinuumDistribution.h"

Modified: trunk/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Yoshua Bengio & Martin Monperrus
 
-/*! \file GaussianContinuumDistribution.h */
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/GaussianContinuumDistribution.h */
 
 
 #ifndef GaussianContinuumDistribution_INC

Modified: trunk/plearn_learners/distributions/DEPRECATED/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/GaussianProcessRegressor.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/GaussianProcessRegressor.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -39,8 +39,8 @@
  * $Id$
  ******************************************************* */
 
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/GaussianProcessRegressor.h */
 
-/*! \file PLearn/plearn_learners/distributions/GaussianProcessRegressor.h */
 
 #ifndef GaussianProcessRegressor_INC
 #define GaussianProcessRegressor_INC

Modified: trunk/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -36,7 +36,8 @@
  * $Id$ 
  ******************************************************* */
 
-/*! \file LocallyWeightedDistribution.cc */
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.cc */
+
 #include "LocallyWeightedDistribution.h"
 #include <plearn/vmat/ConcatColumnsVMatrix.h>
 

Modified: trunk/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -36,7 +36,7 @@
  * $Id$ 
  ******************************************************* */
 
-/*! \file LocallyWeightedDistribution.h */
+/*! \file PLearn/plearn_learners/distributions/DEPRECATED/LocallyWeightedDistribution.h */
 #ifndef LocallyWeightedDistribution_INC
 #define LocallyWeightedDistribution_INC
 

Modified: trunk/plearn_learners/distributions/GaussianDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/GaussianDistribution.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/distributions/GaussianDistribution.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -39,9 +39,8 @@
  * This file is part of the PLearn library.
  ******************************************************* */
 
+/*! \file PLearn/plearn_learners/distributions/GaussianDistribution.cc */
 
-/*! \file PLearnLibrary/PLearnAlgo/GaussianDistribution.cc */
-
 #include "GaussianDistribution.h"
 #include <plearn/vmat/VMat_basic_stats.h>
 #include <plearn/math/plapack.h>

Modified: trunk/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file GaussPartSupervisedDBN.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.cc */
 
 #define PL_LOG_MODULE_NAME "GaussPartSupervisedDBN"
 #include <plearn/io/pl_log.h>

Modified: trunk/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file GaussPartSupervisedDBN.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/GaussPartSupervisedDBN.h */
 
 
 #ifndef GaussPartSupervisedDBN_INC

Modified: trunk/plearn_learners/online/DEPRECATED/GaussianDBNClassification.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/GaussianDBNClassification.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/GaussianDBNClassification.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file GaussianDBNClassification.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/GaussianDBNClassification.cc */
 
 #define PL_LOG_MODULE_NAME "GaussianDBNClassification"
 #include <plearn/io/pl_log.h>

Modified: trunk/plearn_learners/online/DEPRECATED/GaussianDBNClassification.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/GaussianDBNClassification.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/GaussianDBNClassification.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file GaussianDBNClassification.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/GaussianDBNClassification.h */
 
 
 #ifndef GaussianDBNClassification_INC

Modified: trunk/plearn_learners/online/DEPRECATED/GaussianDBNRegression.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/GaussianDBNRegression.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/GaussianDBNRegression.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file GaussianDBNRegression.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/GaussianDBNRegression.cc */
 
 #define PL_LOG_MODULE_NAME "GaussianDBNRegression"
 #include <plearn/io/pl_log.h>

Modified: trunk/plearn_learners/online/DEPRECATED/GaussianDBNRegression.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/GaussianDBNRegression.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/GaussianDBNRegression.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file GaussianDBNRegression.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/GaussianDBNRegression.h */
 
 
 #ifndef GaussianDBNRegression_INC

Modified: trunk/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file HintonDeepBeliefNet.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.cc */
 
 #define PL_LOG_MODULE_NAME "HintonDeepBeliefNet"
 #include <plearn/io/pl_log.h>

Modified: trunk/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file HintonDeepBeliefNet.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/HintonDeepBeliefNet.h */
 
 
 #ifndef HintonDeepBeliefNet_INC

Modified: trunk/plearn_learners/online/DEPRECATED/NLLErrModule.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/NLLErrModule.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/NLLErrModule.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file NLLErrModule.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/NLLErrModule.cc */
 
 
 #include "NLLErrModule.h"

Modified: trunk/plearn_learners/online/DEPRECATED/NLLErrModule.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/NLLErrModule.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/NLLErrModule.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file NLLErrModule.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/NLLErrModule.h */
 
 
 #ifndef NLLErrModule_INC

Modified: trunk/plearn_learners/online/DEPRECATED/PartSupervisedDBN.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/PartSupervisedDBN.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/PartSupervisedDBN.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file PartSupervisedDBN.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/PartSupervisedDBN.cc */
 
 #define PL_LOG_MODULE_NAME "PartSupervisedDBN"
 #include <plearn/io/pl_log.h>

Modified: trunk/plearn_learners/online/DEPRECATED/PartSupervisedDBN.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/PartSupervisedDBN.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/PartSupervisedDBN.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file PartSupervisedDBN.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/PartSupervisedDBN.h */
 
 
 #ifndef PartSupervisedDBN_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMBinomialLayer.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMBinomialLayer.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,10 +34,8 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMBinomialLayer.cc */
 
-
-
 #include "RBMBinomialLayer.h"
 #include <plearn/math/TMat_maths.h>
 #include "RBMParameters.h"

Modified: trunk/plearn_learners/online/DEPRECATED/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMBinomialLayer.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMBinomialLayer.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,9 +34,8 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMBinomialLayer.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMBinomialLayer.h */
 
-
 #ifndef RBMBinomialLayer_INC
 #define RBMBinomialLayer_INC
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMConv2DLLParameters.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.cc */
 
 #define PL_LOG_MODULE_NAME "RBMConv2DLLParameters"
 #include <plearn/io/pl_log.h>

Modified: trunk/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMConv2DLLParameters.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMConv2DLLParameters.h */
 
 
 #ifndef RBMConv2DLLParameters_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMGaussianLayer.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMGaussianLayer.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,10 +34,8 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMGaussianLayer.cc */
 
-
-
 #include "RBMGaussianLayer.h"
 #include <plearn/math/TMat_maths.h>
 #include "RBMParameters.h"

Modified: trunk/plearn_learners/online/DEPRECATED/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMGaussianLayer.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMGaussianLayer.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,9 +34,8 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMGaussianLayer.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMGaussianLayer.h */
 
-
 #ifndef RBMGaussianLayer_INC
 #define RBMGaussianLayer_INC
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMGenericParameters.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMGenericParameters.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMGenericParameters.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMGenericParameters.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMGenericParameters.cc */
 
 
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMGenericParameters.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMGenericParameters.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMGenericParameters.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMGenericParameters.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMGenericParameters.h */
 
 
 #ifndef RBMGenericParameters_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMJointGenericParameters.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.cc */
 
 
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMJointGenericParameters.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMJointGenericParameters.h */
 
 
 #ifndef RBMJointGenericParameters_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMJointLLParameters.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMJointLLParameters.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMJointLLParameters.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMJointLLParameters.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMJointLLParameters.cc */
 
 
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMJointLLParameters.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMJointLLParameters.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMJointLLParameters.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMJointLLParameters.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMJointLLParameters.h */
 
 
 #ifndef RBMJointLLParameters_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMLLParameters.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMLLParameters.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMLLParameters.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMLLParameters.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMLLParameters.cc */
 
 
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMLLParameters.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMLLParameters.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMLLParameters.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMLLParameters.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMLLParameters.h */
 
 
 #ifndef RBMLLParameters_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMLQParameters.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMLQParameters.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMLQParameters.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file RBMLQParameters.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMLQParameters.cc */
 
 
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMLQParameters.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMLQParameters.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMLQParameters.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file RBMLQParameters.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMLQParameters.h */
 
 
 #ifndef RBMLQParameters_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMLayer.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMLayer.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,10 +34,8 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMLayer.cc */
 
-
-
 #include "RBMLayer.h"
 #include <plearn/math/TMat_maths.h>
 #include <plearn/math/PRandom.h>

Modified: trunk/plearn_learners/online/DEPRECATED/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMLayer.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMLayer.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,9 +34,8 @@
 
 // Authors: Dan Popovici
 
-/*! \file RBMLayer.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMLayer.h */
 
-
 #ifndef RBMLayer_INC
 #define RBMLayer_INC
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMMixedLayer.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMMixedLayer.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,10 +34,8 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMMixedLayer.cc */
 
-
-
 #include "RBMMixedLayer.h"
 #include <plearn/math/TMat_maths.h>
 #include "RBMParameters.h"

Modified: trunk/plearn_learners/online/DEPRECATED/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMMixedLayer.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMMixedLayer.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMMixedLayer.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMMixedLayer.h */
 
 
 #ifndef RBMMixedLayer_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,10 +34,8 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.cc */
 
-
-
 #include "RBMMultinomialLayer.h"
 #include <plearn/math/TMat_maths.h>
 #include "RBMParameters.h"

Modified: trunk/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMMultinomialLayer.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMMultinomialLayer.h */
 
 
 #ifndef RBMMultinomialLayer_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMParameters.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMParameters.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMParameters.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMParameters.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMParameters.cc */
 
 
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMParameters.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMParameters.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMParameters.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file RBMParameters.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMParameters.h */
 
 
 #ifndef RBMParameters_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMQLParameters.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMQLParameters.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMQLParameters.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file RBMQLParameters.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMQLParameters.cc */
 
 
 

Modified: trunk/plearn_learners/online/DEPRECATED/RBMQLParameters.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMQLParameters.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMQLParameters.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file RBMQLParameters.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMQLParameters.h */
 
 
 #ifndef RBMQLParameters_INC

Modified: trunk/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,10 +34,8 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.cc */
 
-
-
 #include "RBMTruncExpLayer.h"
 #include <plearn/math/TMat_maths.h>
 #include "RBMParameters.h"

Modified: trunk/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMTruncExpLayer.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/RBMTruncExpLayer.h */
 
 
 #ifndef RBMTruncExpLayer_INC

Modified: trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file SquaredErrModule.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/SquaredErrModule.cc */
 
 
 #include "SquaredErrModule.h"

Modified: trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file SquaredErrModule.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/SquaredErrModule.h */
 
 
 #ifndef SquaredErrModule_INC

Modified: trunk/plearn_learners/online/DEPRECATED/StackedModulesLearner.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/StackedModulesLearner.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/StackedModulesLearner.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file StackedModulesLearner.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/StackedModulesLearner.cc */
 
 
 #include "StackedModulesLearner.h"

Modified: trunk/plearn_learners/online/DEPRECATED/StackedModulesLearner.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/StackedModulesLearner.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/StackedModulesLearner.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file StackedModulesLearner.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/StackedModulesLearner.h */
 
 
 #ifndef StackedModulesLearner_INC

Modified: trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file StackedModulesModule.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/StackedModulesModule.cc */
 
 
 

Modified: trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file StackedModulesModule.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/StackedModulesModule.h */
 
 
 #ifndef StackedModulesModule_INC

Modified: trunk/plearn_learners/online/DEPRECATED/SupervisedDBN.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/SupervisedDBN.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/SupervisedDBN.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file SupervisedDBN.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/SupervisedDBN.cc */
 
 #define PL_LOG_MODULE_NAME "SupervisedDBN"
 #include <plearn/io/pl_log.h>

Modified: trunk/plearn_learners/online/DEPRECATED/SupervisedDBN.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/SupervisedDBN.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/SupervisedDBN.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file SupervisedDBN.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/SupervisedDBN.h */
 
 
 #ifndef SupervisedDBN_INC

Modified: trunk/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Yoshua Bengio
 
-/*! \file UndirectedSoftmaxModule.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.cc */
 
 
 #include "UndirectedSoftmaxModule.h"

Modified: trunk/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -38,7 +38,7 @@
 
 // Authors: Yoshua Bengio
 
-/*! \file UndirectedSoftmaxModule.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/UndirectedSoftmaxModule.h */
 
 
 #ifndef UndirectedSoftmaxModule_INC

Modified: trunk/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.cc	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.cc	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file UnfrozenDeepBeliefNet.cc */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.cc */
 
 #define PL_LOG_MODULE_NAME "UnfrozenDeepBeliefNet"
 #include <plearn/io/pl_log.h>

Modified: trunk/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.h	2008-09-02 14:29:05 UTC (rev 9417)
+++ trunk/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.h	2008-09-02 15:33:46 UTC (rev 9418)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file UnfrozenDeepBeliefNet.h */
+/*! \file PLearn/plearn_learners/online/DEPRECATED/UnfrozenDeepBeliefNet.h */
 
 
 #ifndef UnfrozenDeepBeliefNet_INC



From nouiz at mail.berlios.de  Tue Sep  2 18:48:10 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 18:48:10 +0200
Subject: [Plearn-commits] r9419 - in trunk/plearn_learners: cgi generic hyper
Message-ID: <200809021648.m82GmA4d016018@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 18:48:08 +0200 (Tue, 02 Sep 2008)
New Revision: 9419

Modified:
   trunk/plearn_learners/cgi/NeighborhoodImputationVMatrix.h
   trunk/plearn_learners/generic/NeighborhoodSmoothnessNNet.h
   trunk/plearn_learners/hyper/HyperLearner.h
Log:
remove Doxygen warning


Modified: trunk/plearn_learners/cgi/NeighborhoodImputationVMatrix.h
===================================================================
--- trunk/plearn_learners/cgi/NeighborhoodImputationVMatrix.h	2008-09-02 15:33:46 UTC (rev 9418)
+++ trunk/plearn_learners/cgi/NeighborhoodImputationVMatrix.h	2008-09-02 16:48:08 UTC (rev 9419)
@@ -39,7 +39,7 @@
    * $Id: NeighborhoodImputationVMatrix.h 3658 2005-07-06 20:30:15  Godbout $
    ****************************************************************** */
 
-/*! \file NeignborhoodImputationVMatrix.h */
+/*! \file NeighborhoodImputationVMatrix.h */
 
 #ifndef NeighborhoodImputationVMatrix_INC
 #define NeighborhoodImputationVMatrix_INC

Modified: trunk/plearn_learners/generic/NeighborhoodSmoothnessNNet.h
===================================================================
--- trunk/plearn_learners/generic/NeighborhoodSmoothnessNNet.h	2008-09-02 15:33:46 UTC (rev 9418)
+++ trunk/plearn_learners/generic/NeighborhoodSmoothnessNNet.h	2008-09-02 16:48:08 UTC (rev 9419)
@@ -38,7 +38,7 @@
  * $Id$
  ******************************************************* */
 
-/*! \file PLearn/plearn_learners/classifiers/NeighborhoodSmoothnessNNet.h */
+/*! \file PLearn/plearn_learners/generic/NeighborhoodSmoothnessNNet.cc */
 
 #ifndef NeighborhoodSmoothnessNNet_INC
 #define NeighborhoodSmoothnessNNet_INC

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2008-09-02 15:33:46 UTC (rev 9418)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2008-09-02 16:48:08 UTC (rev 9419)
@@ -38,9 +38,8 @@
  ******************************************************* */
 // Author: Pascal Vincent
 
+/*! \file PLearn/plearn_learners/hyper/HyperLearner.h */
 
-/*! \file PLearn/plearn_learners/generic/HyperLearner.h */
-
 #ifndef HyperLearner_INC
 #define HyperLearner_INC
 



From nouiz at mail.berlios.de  Tue Sep  2 19:15:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 19:15:07 +0200
Subject: [Plearn-commits] r9420 - trunk/plearn_learners/regressors
Message-ID: <200809021715.m82HF7cP017732@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 19:15:06 +0200 (Tue, 02 Sep 2008)
New Revision: 9420

Modified:
   trunk/plearn_learners/regressors/BaseRegressorWrapper.h
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.h
Log:
removed doxygen warning


Modified: trunk/plearn_learners/regressors/BaseRegressorWrapper.h
===================================================================
--- trunk/plearn_learners/regressors/BaseRegressorWrapper.h	2008-09-02 16:48:08 UTC (rev 9419)
+++ trunk/plearn_learners/regressors/BaseRegressorWrapper.h	2008-09-02 17:15:06 UTC (rev 9420)
@@ -39,7 +39,7 @@
  * This file is part of the PLearn library.                                     *
  ******************************************************************************** */
 
-/*! \file PLearnLibrary/PLearnAlgo/BaseRegressorWrapper.h */
+/*! \file PLearn/plearn_learners/regressors/BaseRegressorWrapper.h */
 
 #ifndef BaseRegressorWrapper_INC
 #define BaseRegressorWrapper_INC

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2008-09-02 16:48:08 UTC (rev 9419)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2008-09-02 17:15:06 UTC (rev 9420)
@@ -38,7 +38,7 @@
 
 // Authors: Nicolas Chapados
 
-/*! \file GaussianProcessRegressor.cc */
+/*! \file PLearn/plearn_learners/regressors/GaussianProcessRegressor.cc */
 
 #define PL_LOG_MODULE_NAME "GaussianProcessRegressor"
 

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2008-09-02 16:48:08 UTC (rev 9419)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2008-09-02 17:15:06 UTC (rev 9420)
@@ -38,7 +38,7 @@
 
 // Authors: Nicolas Chapados
 
-/*! \file GaussianProcessRegressor.h */
+/*! \file PLearn/plearn_learners/regressors/GaussianProcessRegressor.h */
 
 
 #ifndef GaussianProcessRegressor_INC



From nouiz at mail.berlios.de  Tue Sep  2 19:24:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 19:24:55 +0200
Subject: [Plearn-commits] r9421 - in trunk/plearn_learners: online
	testers/DEPRECATED
Message-ID: <200809021724.m82HOtEp001914@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 19:24:54 +0200 (Tue, 02 Sep 2008)
New Revision: 9421

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.h
   trunk/plearn_learners/testers/DEPRECATED/TestMethod.h
Log:
removed doxygen warning


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMBinomialLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMBinomialLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMBinomialLayer.h */
+/*! \file PLearn/plearn_learners/online/RBMBinomialLayer.h */
 
 
 #ifndef RBMBinomialLayer_INC

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMPLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMGaussianLayer.h */
+/*! \file PLearn/plearn_learners/online/RBMGaussianLayer.h */
 
 
 #ifndef RBMGaussianLayer_INC

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMPLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici
 
-/*! \file RBMLayer.h */
+/*! \file PLearn/plearn_learners/online/RBMLayer.h */
 
 
 #ifndef RBMLayer_INC

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMPLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMMixedLayer.h */
+/*! \file PLearn/plearn_learners/online/RBMMixedLayer.h */
 
 
 #ifndef RBMMixedLayer_INC

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMPLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMMultinomialLayer.h */
+/*! \file PLearn/plearn_learners/online/RBMMultinomialLayer.h */
 
 
 #ifndef RBMMultinomialLayer_INC

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMTruncExpLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.h	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.h	2008-09-02 17:24:54 UTC (rev 9421)
@@ -34,7 +34,7 @@
 
 // Authors: Dan Popovici & Pascal Lamblin
 
-/*! \file RBMTruncExpLayer.h */
+/*! \file PLearn/plearn_learners/online/RBMTruncExpLayer.h */
 
 
 #ifndef RBMTruncExpLayer_INC

Modified: trunk/plearn_learners/testers/DEPRECATED/TestMethod.h
===================================================================
--- trunk/plearn_learners/testers/DEPRECATED/TestMethod.h	2008-09-02 17:15:06 UTC (rev 9420)
+++ trunk/plearn_learners/testers/DEPRECATED/TestMethod.h	2008-09-02 17:24:54 UTC (rev 9421)
@@ -39,7 +39,7 @@
  ******************************************************* */
 
 
-/*! \file PLearn/plearn/opt/TestMethod.h */
+/*! \file PLearn/plearn_learners/testers/DEPRECATED/TestMethod.h */
 
 #ifndef TestMethod_INC
 #define TestMethod_INC



From nouiz at mail.berlios.de  Tue Sep  2 19:26:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 19:26:07 +0200
Subject: [Plearn-commits] r9422 - in trunk/plearn_learners_experimental: .
	SurfaceTemplate
Message-ID: <200809021726.m82HQ7g3004332@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 19:26:06 +0200 (Tue, 02 Sep 2008)
New Revision: 9422

Modified:
   trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc
   trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h
   trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.cc
   trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.h
   trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc
   trunk/plearn_learners_experimental/SurfaceTemplate/geometry.h
   trunk/plearn_learners_experimental/geometry.cc
   trunk/plearn_learners_experimental/geometry.h
Log:
removed doxygen warning


Modified: trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc
===================================================================
--- trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc	2008-09-02 17:24:54 UTC (rev 9421)
+++ trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc	2008-09-02 17:26:06 UTC (rev 9422)
@@ -33,7 +33,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-/*! \file PLearnLibrary/PLearnAlgo/NeuralProbabilisticLanguageModel.h */
+/*! \file PLearn/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h */
 
 
 #include "NeuralProbabilisticLanguageModel.h"

Modified: trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h
===================================================================
--- trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h	2008-09-02 17:24:54 UTC (rev 9421)
+++ trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h	2008-09-02 17:26:06 UTC (rev 9422)
@@ -33,7 +33,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-/*! \file PLearnLibrary/PLearnAlgo/NeuralProbabilisticLanguageModel.h */
+/*! \file PLearn/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h */
 
 #ifndef NeuralProbabilisticLanguageModel_INC
 #define NeuralProbabilisticLanguageModel_INC

Modified: trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.cc
===================================================================
--- trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.cc	2008-09-02 17:24:54 UTC (rev 9421)
+++ trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.cc	2008-09-02 17:26:06 UTC (rev 9422)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file Molecule.cc */
+/*! \file PLearn/plearn_learners_experimental/SurfaceTemplate/Molecule.cc */
 
 
 #include "Molecule.h"

Modified: trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.h
===================================================================
--- trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.h	2008-09-02 17:24:54 UTC (rev 9421)
+++ trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.h	2008-09-02 17:26:06 UTC (rev 9422)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file Molecule.h */
+/*! \file PLearn/plearn_learners_experimental/SurfaceTemplate/Molecule.h */
 
 
 #ifndef Molecule_INC

Modified: trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc
===================================================================
--- trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc	2008-09-02 17:24:54 UTC (rev 9421)
+++ trunk/plearn_learners_experimental/SurfaceTemplate/geometry.cc	2008-09-02 17:26:06 UTC (rev 9422)
@@ -35,7 +35,7 @@
 
 // These functions come from ....
 
-/*! \file geometry.cc */
+/*! \file PLearn/plearn_learners_experimental/SurfaceTemplate/geometry.cc */
 
 
 #include "geometry.h"

Modified: trunk/plearn_learners_experimental/SurfaceTemplate/geometry.h
===================================================================
--- trunk/plearn_learners_experimental/SurfaceTemplate/geometry.h	2008-09-02 17:24:54 UTC (rev 9421)
+++ trunk/plearn_learners_experimental/SurfaceTemplate/geometry.h	2008-09-02 17:26:06 UTC (rev 9422)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file geometry.h */
+/*! \file PLearn/plearn_learners_experimental/geometry.h */
 
 
 #ifndef geometry_INC

Modified: trunk/plearn_learners_experimental/geometry.cc
===================================================================
--- trunk/plearn_learners_experimental/geometry.cc	2008-09-02 17:24:54 UTC (rev 9421)
+++ trunk/plearn_learners_experimental/geometry.cc	2008-09-02 17:26:06 UTC (rev 9422)
@@ -38,7 +38,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file geometry.cc */
+/*! \file PLearn/plearn_learners_experimental/geometry.cc */
 
 
 #include "geometry.h"

Modified: trunk/plearn_learners_experimental/geometry.h
===================================================================
--- trunk/plearn_learners_experimental/geometry.h	2008-09-02 17:24:54 UTC (rev 9421)
+++ trunk/plearn_learners_experimental/geometry.h	2008-09-02 17:26:06 UTC (rev 9422)
@@ -38,7 +38,7 @@
 
 // Authors: Pascal Lamblin
 
-/*! \file geometry.h */
+/*! \file PLearn/plearn_learners_experimental/geometry.h */
 
 
 #ifndef geometry_INC



From nouiz at mail.berlios.de  Tue Sep  2 19:39:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 19:39:39 +0200
Subject: [Plearn-commits] r9423 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200809021739.m82Hddfh021113@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 19:39:38 +0200 (Tue, 02 Sep 2008)
New Revision: 9423

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added a new condor option


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-02 17:26:06 UTC (rev 9422)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-02 17:39:38 UTC (rev 9423)
@@ -687,6 +687,8 @@
         self.redirect_stderr_to_stdout = False
         self.env = ''
         self.os = ''
+        self.stdouts = ''
+        self.stderrs = ''
         self.base_tasks_log_file = []
         self.set_special_env = True
 
@@ -902,6 +904,19 @@
                     condor_dat.write("arguments    = %s \n" %argstring)
                     condor_dat.write("output       = %s \n" %stdout_file)
                     condor_dat.write("error        = %s \nqueue\n" %stderr_file)
+            elif self.stdouts and self.stderrs:
+                assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
+                for (task,stdout_file,stderr_file) in zip(self.tasks,self.stdouts,self.stderrs):
+                    if stdout_file==stderr_file:
+                        print "Condor can't redirect the stdout and stderr to the same file!"
+                        sys.exit(1)
+                    argstring =condor_escape_argument(' ; '.join(task.commands))
+                    condor_dat.write("arguments    = %s \n" %argstring)
+                    condor_dat.write("output       = %s \n" %stdout_file)
+                    condor_dat.write("error        = %s \nqueue\n" %stderr_file)
+            elif self.stdouts or self.stderrs:
+                print "DBICondor should have stdouts and stderrs or none of them"
+                sys.exit(1)
             else:
                 for task in self.tasks:
                     argstring =condor_escape_argument(' ; '.join(task.commands))

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-02 17:26:06 UTC (rev 9422)
+++ trunk/scripts/dbidispatch	2008-09-02 17:39:38 UTC (rev 9423)
@@ -15,7 +15,7 @@
                                [--rank=RANK_EXPRESSION] 
                                [--files=file1[,file2...]]
                                [--env=VAR=VALUE[;VAR2=VALUE2]]
-                               [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,*nb0,nb1}]
+                               [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,*nb0,nb1,sh}]
                                [*--[no_]set_special_env]
     cluster option           : [*--[no_]cwait]  [--[*no_]force]
                                [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
@@ -95,12 +95,13 @@
   The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
   If the CONDOR_HOME environment variable is set, then the HOME variable will
      be set to this value for jobs submitted to condor.
-  The '--tasks_filename={compact,explicit,nb0,nb1}' option will change the filename where the stdout, stderr are redirected. They have this pattern condor.X.{out,error} where X=:
+  The '--tasks_filename={compact,explicit,nb0,nb1,sh}' option will change the filename where the stdout, stderr are redirected. They have this pattern condor.X.{out,error} where X=:
       - default : same as nb0
       - compact : will be a unic string with parameter that change of value between jobs
       - explicit: will be a unic string that represent the full command to execute
       - nb0     : a number from 0 to nb job -1.
       - nb1     : a number from 1 to nb job.
+      - sh      : parse the command for > and 2> redirection command. If one or both of them are missing, they are redirected to /dev/null
   The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
 
 where <command-template> is interpreted as follows: the first argument
@@ -199,7 +200,7 @@
             dbi_param["micro"]=argv[8:]
     elif argv.startswith("--tasks_filename="):
         part = argv.split('=',1)
-        accepted_value=["compact","explicit","nb0","nb1"]
+        accepted_value=["compact","explicit","nb0","nb1","sh"]
         if part[1] not in accepted_value:
             print "The option '"+argv+"' have an invalid value. possible value are:", accepted_value
             sys.exit(2)
@@ -364,24 +365,53 @@
     tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
     dbi_param["log_dir"]=os.path.join(LOGDIR,tmp)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
-    if tasks_filename == "explicit":
-        dbi_param["base_tasks_log_file"]=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands]
-    elif tasks_filename == "compact":
-        dbi_param["base_tasks_log_file"]=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in choise_args]
-    elif tasks_filename == "nb0":
-        dbi_param["base_tasks_log_file"]=map(str,range(len(commands)))
-    elif tasks_filename == "nb1":
-        dbi_param["base_tasks_log_file"]=map(str,range(1,len(commands)+1))
-    elif tasks_filename == "":
-        pass
-    else:
-        print "internal ERROR!"
-        sys.exit(2)
 else:
     dbi_param["log_dir"]=os.path.join(LOGDIR,FILE)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
+if tasks_filename == "explicit":
+    dbi_param["base_tasks_log_file"]=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands]
+elif tasks_filename == "compact":
+    dbi_param["base_tasks_log_file"]=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in choise_args]
+elif tasks_filename == "nb0":
+    dbi_param["base_tasks_log_file"]=map(str,range(len(commands)))
+elif tasks_filename == "nb1":
+    dbi_param["base_tasks_log_file"]=map(str,range(1,len(commands)+1))
+elif tasks_filename == "":
+    pass
+elif tasks_filename == "sh":
+    stdouts=[]
+    stderrs=[]
+    for x in range(len(commands)):
+        sp=commands[x].split()
+        i=0
+        output=""
+        error=""
+        while i < len(sp):
+            if sp[i]=="2>":
+                del sp[i]
+                error=sp[i]
+                del sp[i]
+            elif sp[i]==">":
+                del sp[i]
+                output=sp[i]
+                del sp[i]
+            else:
+                i+=1
+        if stdout_file==stderr_file:
+            print "Condor can't redirect the stdout and stderr to the same file!"
+            sys.exit(1)
+        stdouts.append(output)
+        stderrs.append(error)
+        commands[x]=' '.join(sp)
 
+    dbi_param["stdouts"]=stdouts
+    dbi_param["stderrs"]=stderrs
+else:
+    print "internal error!"
+    sys.exit(2)
+    
+
 SCRIPT=open(os.getenv("HOME")+"/.dbidispatch.launched",'a');
 SCRIPT.write("["+time.ctime()+"] "+str(sys.argv)+"\n")
 SCRIPT.close()



From nouiz at mail.berlios.de  Tue Sep  2 20:11:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 20:11:20 +0200
Subject: [Plearn-commits] r9424 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200809021811.m82IBKuX027726@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 20:11:20 +0200 (Tue, 02 Sep 2008)
New Revision: 9424

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
allow to put many value to the option --tasks_filename


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-02 17:39:38 UTC (rev 9423)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-02 18:11:20 UTC (rev 9424)
@@ -898,8 +898,8 @@
             if self.base_tasks_log_file:
                 for (task,task_log) in zip(self.tasks,self.base_tasks_log_file):
                     argstring =condor_escape_argument(' ; '.join(task.commands))
-                    stdout_file=self.log_dir+"/condor."+task_log+".out"
-                    stderr_file=self.log_dir+"/condor."+task_log+".err"
+                    stdout_file=self.log_dir+"/condor"+task_log+".out"
+                    stderr_file=self.log_dir+"/condor"+task_log+".err"
 
                     condor_dat.write("arguments    = %s \n" %argstring)
                     condor_dat.write("output       = %s \n" %stdout_file)

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-02 17:39:38 UTC (rev 9423)
+++ trunk/scripts/dbidispatch	2008-09-02 18:11:20 UTC (rev 9424)
@@ -15,11 +15,12 @@
                                [--rank=RANK_EXPRESSION] 
                                [--files=file1[,file2...]]
                                [--env=VAR=VALUE[;VAR2=VALUE2]]
-                               [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,*nb0,nb1,sh}]
+                               [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,*nb0,nb1,sh}+]
                                [*--[no_]set_special_env]
     cluster option           : [*--[no_]cwait]  [--[*no_]force]
                                [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
 An * after '[', '{' or ',' signals the default value.
+An + after } tell that we can put one or more of the choise separeted by a comma
 '''
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
@@ -95,7 +96,7 @@
   The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
   If the CONDOR_HOME environment variable is set, then the HOME variable will
      be set to this value for jobs submitted to condor.
-  The '--tasks_filename={compact,explicit,nb0,nb1,sh}' option will change the filename where the stdout, stderr are redirected. They have this pattern condor.X.{out,error} where X=:
+  The '--tasks_filename={compact,explicit,nb0,nb1,sh}+' option will change the filename where the stdout, stderr are redirected. We can put many option separated by comma. They will be appended in the filename with a dot. For all format except sh, they have this pattern condor.X.{out,error} where X=:
       - default : same as nb0
       - compact : will be a unic string with parameter that change of value between jobs
       - explicit: will be a unic string that represent the full command to execute
@@ -201,10 +202,12 @@
     elif argv.startswith("--tasks_filename="):
         part = argv.split('=',1)
         accepted_value=["compact","explicit","nb0","nb1","sh"]
-        if part[1] not in accepted_value:
-            print "The option '"+argv+"' have an invalid value. possible value are:", accepted_value
-            sys.exit(2)
-        tasks_filename = part[1]
+        val=part[1].split(",") 
+        for v in val:
+            if v not in accepted_value:
+                print "The option '"+argv+"' have an invalid value. possible value are:", accepted_value
+                sys.exit(2)
+        tasks_filename = val
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
                    "--set_special_env"]:
@@ -369,49 +372,63 @@
     dbi_param["log_dir"]=os.path.join(LOGDIR,FILE)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
-if tasks_filename == "explicit":
-    dbi_param["base_tasks_log_file"]=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands]
-elif tasks_filename == "compact":
-    dbi_param["base_tasks_log_file"]=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in choise_args]
-elif tasks_filename == "nb0":
-    dbi_param["base_tasks_log_file"]=map(str,range(len(commands)))
-elif tasks_filename == "nb1":
-    dbi_param["base_tasks_log_file"]=map(str,range(1,len(commands)+1))
-elif tasks_filename == "":
-    pass
-elif tasks_filename == "sh":
-    stdouts=[]
-    stderrs=[]
-    for x in range(len(commands)):
-        sp=commands[x].split()
-        i=0
-        output=""
-        error=""
-        while i < len(sp):
-            if sp[i]=="2>":
-                del sp[i]
-                error=sp[i]
-                del sp[i]
-            elif sp[i]==">":
-                del sp[i]
-                output=sp[i]
-                del sp[i]
-            else:
-                i+=1
-        if stdout_file==stderr_file:
-            print "Condor can't redirect the stdout and stderr to the same file!"
-            sys.exit(1)
-        stdouts.append(output)
-        stderrs.append(error)
-        commands[x]=' '.join(sp)
+n="base_tasks_log_file"
+dbi_param[n]=[""]*len(commands)
+print "dbi_param",dbi_param[n]
+print "tasks_filename",tasks_filename
 
-    dbi_param["stdouts"]=stdouts
-    dbi_param["stderrs"]=stderrs
-else:
-    print "internal error!"
-    sys.exit(2)
-    
+def merge_pattern(new_list):
+    return [x+'.'+y for (x,y) in  zip(dbi_param[n], new_list)]
 
+for pattern in tasks_filename:
+    if pattern == "explicit":
+        dbi_param[n]=merge_pattern([re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands])
+    elif pattern == "compact":
+        dbi_param[n]=merge_pattern([re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in choise_args])
+       
+    elif pattern == "nb0":
+        dbi_param[n]=merge_pattern(map(str,range(len(commands))))
+    elif pattern == "nb1":
+        dbi_param[n]=merge_pattern(map(str,range(1,len(commands)+1)))
+    elif pattern == "":
+        pass
+    elif pattern == "sh":
+        stdouts=[]
+        stderrs=[]
+        for x in range(len(commands)):
+            sp=commands[x].split()
+            i=0
+            output=""
+            error=""
+            while i < len(sp):
+                if sp[i]=="2>":
+                    del sp[i]
+                    error=sp[i]
+                    del sp[i]
+                elif sp[i]==">":
+                    del sp[i]
+                    output=sp[i]
+                    del sp[i]
+                else:
+                    i+=1
+            if stdout_file==stderr_file:
+                print "Condor can't redirect the stdout and stderr to the same file!"
+                sys.exit(1)
+            stdouts.append(output)
+            stderrs.append(error)
+            commands[x]=' '.join(sp)
+            
+        dbi_param["stdouts"]=stdouts
+        dbi_param["stderrs"]=stderrs
+    else:
+        print "internal error!"
+        sys.exit(2)
+    assert(not (dbi_param.has_key("stdouts") and (dbi_param[n])==0))
+
+#undef merge_pattern
+print "dbi_param",dbi_param[n]
+print "tasks_filename",tasks_filename
+
 SCRIPT=open(os.getenv("HOME")+"/.dbidispatch.launched",'a');
 SCRIPT.write("["+time.ctime()+"] "+str(sys.argv)+"\n")
 SCRIPT.close()



From nouiz at mail.berlios.de  Tue Sep  2 20:14:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 20:14:50 +0200
Subject: [Plearn-commits] r9425 - trunk/plearn_learners/online
Message-ID: <200809021814.m82IEoOx027921@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 20:14:50 +0200 (Tue, 02 Sep 2008)
New Revision: 9425

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
Log:
removed doxygen warning


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-09-02 18:11:20 UTC (rev 9424)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-09-02 18:14:50 UTC (rev 9425)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file PLearn/plearn_learners/online/RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMGaussianLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-09-02 18:11:20 UTC (rev 9424)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-09-02 18:14:50 UTC (rev 9425)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file PLearn/plearn_learners/online/RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2008-09-02 18:11:20 UTC (rev 9424)
+++ trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2008-09-02 18:14:50 UTC (rev 9425)
@@ -34,7 +34,7 @@
 
 // Author: Pascal Lamblin
 
-/*! \file RBMPLayer.cc */
+/*! \file RBMLocalMultinomialLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2008-09-02 18:11:20 UTC (rev 9424)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2008-09-02 18:14:50 UTC (rev 9425)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file PLearn/plearn_learners/online/RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMMixedLayer.cc */
 
 
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-09-02 18:11:20 UTC (rev 9424)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-09-02 18:14:50 UTC (rev 9425)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin & Dan Popovici
 
-/*! \file PLearn/plearn_learners/online/RBMPLayer.cc */
+/*! \file PLearn/plearn_learners/online/RBMMultinomialLayer.cc */
 
 
 



From nouiz at mail.berlios.de  Tue Sep  2 22:15:28 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 22:15:28 +0200
Subject: [Plearn-commits] r9426 - in trunk: plearn/base plearn/io
	plearn/math plearn_learners/online
	plearn_learners_experimental/onlineNNLM
Message-ID: <200809022015.m82KFSN1007694@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 22:15:23 +0200 (Tue, 02 Sep 2008)
New Revision: 9426

Modified:
   trunk/plearn/base/stringutils.h
   trunk/plearn/io/PStream.h
   trunk/plearn/math/Hash.h
   trunk/plearn/math/ProbSparseMatrix.cc
   trunk/plearn/math/pl_erf.h
   trunk/plearn/math/pl_math.h
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMLocalMultinomialLayer.h
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.h
   trunk/plearn_learners/online/RBMRateLayer.h
   trunk/plearn_learners/online/RBMWoodsLayer.h
   trunk/plearn_learners_experimental/onlineNNLM/NnlmOutputLayer.h
Log:
better doxygen comment:
- \n, \t, \0 must have \ escapted(\\n, \\t, \\0)
- latex formula must be between \f$ formula \f$


Modified: trunk/plearn/base/stringutils.h
===================================================================
--- trunk/plearn/base/stringutils.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn/base/stringutils.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -80,13 +80,13 @@
 string right(const string& s,  size_t width, char padding=' ');
 string center(const string& s, size_t width, char padding=' ');
     
-//!  removes starting and ending blanks '\n','\r',' ','\t'
+//!  removes starting and ending blanks '\\n','\\r',' ','\\t'
 string removeblanks(const string& s);
 
-//!  removes all blanks '\n','\r',' ','\t'
+//!  removes all blanks '\\n','\\r',' ','\\t'
 string removeallblanks(const string& s);
 
-//!  removes any trailing '\n' and/or '\r'
+//!  removes any trailing '\\n' and/or '\\r'
 string removenewline(const string& s);
 
 //!  remove exactly one pair of matching leading and trailing '\'' and '"';
@@ -103,7 +103,7 @@
 string upperstring(const string& s);
     
 //!  returns the next line read from the stream,
-//!  after removing any trailing '\r' and/or '\n'
+//!  after removing any trailing '\\r' and/or '\\n'
 string pgetline(istream& in=cin);
 
 //! returns true if s is a blank line (containing only space, tab, until end of line or a # comment-character is reached

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn/io/PStream.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -736,7 +736,7 @@
 using std::ws;
 
 //!  returns the next line read from the stream,
-//!  after removing any trailing '\r' and/or '\n'
+//!  after removing any trailing '\\r' and/or '\\n'
 string pgetline(PStream& in);
 
 

Modified: trunk/plearn/math/Hash.h
===================================================================
--- trunk/plearn/math/Hash.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn/math/Hash.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -94,7 +94,7 @@
     unsigned int pace;                          //!<  used by the secondary hash function
     unsigned int jumps;                         //!<  number of probes in last operation
 
-    unsigned int hashKey(const KeyType &) const; //!<  computes the primary hash function on the string (length=0 says to look for \0 in bytes to determine length)
+    unsigned int hashKey(const KeyType &) const; //!<  computes the primary hash function on the string (length=0 says to look for \\0 in bytes to determine length)
     void computePace();                         //!<  computes the magic number used by the secondary hash function
     unsigned int pgcd(unsigned int, unsigned int) const;
 

Modified: trunk/plearn/math/ProbSparseMatrix.cc
===================================================================
--- trunk/plearn/math/ProbSparseMatrix.cc	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn/math/ProbSparseMatrix.cc	2008-09-02 20:15:23 UTC (rev 9426)
@@ -172,7 +172,8 @@
     }
 }
 
-//! Normalize the matrix nXY as a joint probability matrix (\sum_i \sum_j x_{ij} =1)  
+//! Normalize the matrix nXY as a joint probability matrix 
+//! /f$ \sum_i \sum_j x_{ij} =1 /f$
 void ProbSparseMatrix::normalizeJoint(ProbSparseMatrix& nXY, bool clear_nXY)
 {
     clear();

Modified: trunk/plearn/math/pl_erf.h
===================================================================
--- trunk/plearn/math/pl_erf.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn/math/pl_erf.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -44,7 +44,7 @@
 using namespace std;
 
 //! function gamma returns log(Gamma(z)), where
-//!  Gamma(z) = \int_0^infty t^{z-1}*e^{-t} dt
+//!  \f$ Gamma(z) = \int_0^infty t^{z-1}*e^{-t} dt \f$
 real pl_gammln(real z);
 
 //! d(pl_gammln(z))/dz 

Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn/math/pl_math.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -577,8 +577,8 @@
 //!  compute log(exp(log_a)-exp(log_b)) without losing too much precision
 real logsub(real log_a, real log_b);
 
-//! return the dilogarithm function dilogarithm(x)
-//!   = sum_{i=1}^{\infty} x^i/i^2 = int_{z=x}^0 log(1-z)/z dz
+//! return the dilogarithm function
+//! \f$ dilogarithm(x) = sum_{i=1}^{\infty} x^i/i^2 = int_{z=x}^0 log(1-z)/z dz \f$
 //! It is also useful because -dilogarithm(-exp(x)) is the primitive of 
 //! the softplus function log(1+exp(x)).
 real dilogarithm(real x);

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -121,13 +121,13 @@
     //! compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
-    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! Computes \f$ -log(\sum_{possible values of h} exp(h' unit_activations)\f$)
     //! This quantity is used for computing the free energy of a sample x in
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
     //! Computes gradient of the result of freeEnergyContribution
-    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! \f$ -log(\sum_{possible values of h} exp(h' unit_activations)) \f$
     //! with respect to unit_activations. Optionally, a gradient
     //! with respect to freeEnergyContribution can be given
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -143,7 +143,7 @@
     //! compute bias' unit_values + min_quad_coeff.^2' unit_values.^2
     virtual real energy(const Vec& unit_values) const;
 
-    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! Computes \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! This quantity is used for computing the free energy of a sample x in
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -100,7 +100,7 @@
     //! Size of batches when using mini-batch
     int batch_size;
 
-    //! activation value: \sum Wx + b
+    //! activation value: \f$ \sum Wx + b \f$
     Vec activation;
     Mat activations; // for mini-batch operations
 
@@ -285,13 +285,13 @@
 
     virtual real energy(const Vec& unit_values) const;
 
-    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! Computes \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! This quantity is used for computing the free energy of a sample x in
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
     //! Computes gradient of the result of freeEnergyContribution
-    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! with respect to unit_activations. Optionally, a gradient
     //! with respect to freeEnergyContribution can be given
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,

Modified: trunk/plearn_learners/online/RBMLocalMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLocalMultinomialLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMLocalMultinomialLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -136,7 +136,7 @@
     // Compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
-    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! Computes \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! This quantity is used for computing the free energy of a sample x in
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;

Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -59,7 +59,7 @@
 
     //#####  Learned Options  #################################################
 
-    //! Matrix containing unit-to-unit weights (output_size \times input_size)
+    //! Matrix containing unit-to-unit weights (\f$output_size \times input_size\f$)
     Mat weights;
 
     //! RBMMatrixConnection from which the weights are taken

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -186,13 +186,13 @@
     //! Compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
-    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! Computes \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! This quantity is used for computing the free energy of a sample x in
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
     //! Computes gradient of the result of freeEnergyContribution
-    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! with respect to unit_activations. Optionally, a gradient
     //! with respect to freeEnergyContribution can be given
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -119,13 +119,13 @@
     // Compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
-    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! Computes \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! This quantity is used for computing the free energy of a sample x in
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
     //! Computes gradient of the result of freeEnergyContribution
-    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! with respect to unit_activations. Optionally, a gradient
     //! with respect to freeEnergyContribution can be given
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,

Modified: trunk/plearn_learners/online/RBMRateLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMRateLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMRateLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -101,13 +101,13 @@
     // Compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
-    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! Computes \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! This quantity is used for computing the free energy of a sample x in
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
     //! Computes gradient of the result of freeEnergyContribution
-    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! with respect to unit_activations. Optionally, a gradient
     //! with respect to freeEnergyContribution can be given
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,

Modified: trunk/plearn_learners/online/RBMWoodsLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners/online/RBMWoodsLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -126,14 +126,14 @@
     //! compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
-    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! Computes \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! This quantity is used for computing the free energy of a sample x in
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
 
     //! Computes gradient of the result of freeEnergyContribution
-    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! \f$ -log(\sum_{possible values of h} exp(h' unit_activations))\f$
     //! with respect to unit_activations. Optionally, a gradient
     //! with respect to freeEnergyContribution can be given
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,

Modified: trunk/plearn_learners_experimental/onlineNNLM/NnlmOutputLayer.h
===================================================================
--- trunk/plearn_learners_experimental/onlineNNLM/NnlmOutputLayer.h	2008-09-02 18:14:50 UTC (rev 9425)
+++ trunk/plearn_learners_experimental/onlineNNLM/NnlmOutputLayer.h	2008-09-02 20:15:23 UTC (rev 9426)
@@ -80,17 +80,17 @@
     //! specifies the range of the values of 'context' (ex: + 'missing' tag)
     int context_cardinality;
 
-    //! minimal value \sigma^2 can have
+    //! minimal value \f$ \sigma^2 \f$ can have 
     real sigma2min;
 
-    //! Discriminant learning (of \mu_u and \Sigma_u) - dl
+    //! Discriminant learning (of \f$ \mu_u \f$ and \f$ \Sigma_u \f$) - dl
     real dl_start_learning_rate;
     real dl_decrease_constant;
 
-    //! Empirical learning (of \mu_u and \Sigma_u) - el
+    //! Empirical learning (of \f$ \mu_u \f$ and \f$ \Sigma_u \f$) - el
     // how much the first example of this word in the trainset should be worth in the sum
-    // relatively to the last. /mu' = (1-/alpha) /mu + /alpha r
-    // Determine (1-/alpha)^n = el_start_discount_factor
+    // relatively to the last. \f$ /mu' = (1-/alpha) /mu + /alpha r \f$
+    // Determine \f$ (1-/alpha)^n = el_start_discount_factor \f$
     real el_start_discount_factor; 
 
 



From nouiz at mail.berlios.de  Tue Sep  2 22:31:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Sep 2008 22:31:01 +0200
Subject: [Plearn-commits] r9427 - trunk/scripts
Message-ID: <200809022031.m82KV1NM008906@sheep.berlios.de>

Author: nouiz
Date: 2008-09-02 22:31:01 +0200 (Tue, 02 Sep 2008)
New Revision: 9427

Modified:
   trunk/scripts/dbidispatch
Log:
removed print and put a condition on an error.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-02 20:15:23 UTC (rev 9426)
+++ trunk/scripts/dbidispatch	2008-09-02 20:31:01 UTC (rev 9427)
@@ -374,8 +374,6 @@
 
 n="base_tasks_log_file"
 dbi_param[n]=[""]*len(commands)
-print "dbi_param",dbi_param[n]
-print "tasks_filename",tasks_filename
 
 def merge_pattern(new_list):
     return [x+'.'+y for (x,y) in  zip(dbi_param[n], new_list)]
@@ -411,8 +409,8 @@
                     del sp[i]
                 else:
                     i+=1
-            if stdout_file==stderr_file:
-                print "Condor can't redirect the stdout and stderr to the same file!"
+            if stdout_file==stderr_file and launch_cmd=="Condor":
+                print "ERROR: Condor can't redirect the stdout and stderr to the same file!"
                 sys.exit(1)
             stdouts.append(output)
             stderrs.append(error)
@@ -426,8 +424,6 @@
     assert(not (dbi_param.has_key("stdouts") and (dbi_param[n])==0))
 
 #undef merge_pattern
-print "dbi_param",dbi_param[n]
-print "tasks_filename",tasks_filename
 
 SCRIPT=open(os.getenv("HOME")+"/.dbidispatch.launched",'a');
 SCRIPT.write("["+time.ctime()+"] "+str(sys.argv)+"\n")



From louradou at mail.berlios.de  Tue Sep  2 23:53:31 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 2 Sep 2008 23:53:31 +0200
Subject: [Plearn-commits] r9428 - trunk/plearn/vmat
Message-ID: <200809022153.m82LrVJq020814@sheep.berlios.de>

Author: louradou
Date: 2008-09-02 23:53:30 +0200 (Tue, 02 Sep 2008)
New Revision: 9428

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
Log:
Added the possibility to also duplicate samples with a missing target or negative targets
(these samples can be considered as unlabelled data)



Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-09-02 20:31:01 UTC (rev 9427)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-09-02 21:53:30 UTC (rev 9428)
@@ -138,6 +138,8 @@
     Vec input, target;
     real weight;
     TVec< TVec<int>  > class_indices;  // Indices of samples in each class.
+    TVec<int> nan_indices(0); // Indices of missing class
+    TVec< TVec<int> > negativeclass_indices;
     map<int, int> bag_sizes; // Map a source index to the size of its bag.
     int bag_start_idx = -1;
     int bag_idx = bag_index >= 0 ? bag_index : source->targetsize() - 1;
@@ -149,10 +151,20 @@
             for (int j = 0; j < n_to_add; j++)
                 class_indices.append(TVec<int>());
         }
+        else if ( -c >= negativeclass_indices.length() ) {
+            int n_to_add = -c - negativeclass_indices.length() + 1;
+            for (int j = 0; j < n_to_add; j++)
+                negativeclass_indices.append(TVec<int>());
+        }
         
         if (!operate_on_bags || int(round(target[bag_idx])) &
                                 SumOverBagsVariable::TARGET_COLUMN_FIRST) {
-            class_indices[c].append(i);
+            if( c>= 0 )
+                class_indices[c].append(i);
+            else if( c< 0 )
+                negativeclass_indices[-c].append(i);
+            else if( is_missing(c) )
+                nan_indices.append(i);
             indices.append(i);
             bag_sizes[i] = 0;
             bag_start_idx = i;
@@ -160,6 +172,12 @@
         if (operate_on_bags)
             bag_sizes[bag_start_idx]++;
     }
+    if( nan_indices.length() > 0  )
+        class_indices.append( nan_indices );
+    if( negativeclass_indices.length() > 0 )
+        for(int c = 0; c < negativeclass_indices.length(); c ++ )
+            if( negativeclass_indices[c].length() > 0 )
+                class_indices.append( negativeclass_indices[c] );
     int max_n = -1;
     for (int c = 0; c < class_indices.length(); c++) {
         if (class_indices[c].length() > max_n)



From nouiz at mail.berlios.de  Wed Sep  3 15:59:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Sep 2008 15:59:37 +0200
Subject: [Plearn-commits] r9429 - trunk/python_modules/plearn/parallel
Message-ID: <200809031359.m83DxblM020051@sheep.berlios.de>

Author: nouiz
Date: 2008-09-03 15:59:36 +0200 (Wed, 03 Sep 2008)
New Revision: 9429

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
implemented dbidispatch --condor=nb_concurent_proc


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-02 21:53:30 UTC (rev 9428)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-03 13:59:36 UTC (rev 9429)
@@ -691,6 +691,7 @@
         self.stderrs = ''
         self.base_tasks_log_file = []
         self.set_special_env = True
+        self.nb_proc = 0 # 0 mean unlimited
 
         DBIBase.__init__(self, commands, **args)
         self.mem=int(self.mem)*1024
@@ -802,7 +803,201 @@
                                    self.args))
             id+=1
             #keeps a list of the temporary files created, so that they can be deleted at will
+    def run_dag(self):
+        if len(self.tasks)==0:
+            return #no task to run
 
+        #set special environment variable
+        if self.set_special_env:
+            self.env+='" OMP_NUM_THREADS=$$(CPUS) GOTO_NUM_THREADS=$$(CPUS) MKL_NUM_THREADS=$$(CPUS) "'
+
+
+        condor_file = os.path.join(self.log_dir, "dag.condor")
+        self.temp_files.append(condor_file)
+        condor_dat = open( condor_file, 'w' )
+
+        if self.req:
+            req = self.req
+        else:
+            req = "True"
+        if self.targetcondorplatform == 'BOTH':
+            req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
+        else :
+            req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
+
+        if self.os:
+            req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',
+                       self.os.split(','),
+                       req+'&&(False ')+")"
+
+        source_file=os.getenv("CONDOR_LOCAL_SOURCE")
+        condor_home = os.getenv('CONDOR_HOME')
+        if source_file and source_file.endswith(".cshrc"):
+            launch_file = os.path.join(self.log_dir, 'launch.csh')
+        else:
+            launch_file = os.path.join(self.log_dir, 'launch.sh')
+
+        if self.mem<=0:
+            try:
+                self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size/1024
+            except:
+                pass
+
+        self.log_file = os.path.join("/tmp/bastienf/dbidispatch",self.log_dir)
+        os.system('mkdir -p ' + self.log_file)
+        self.log_file = os.path.join(self.log_file,"condor.log")
+
+        condor_dat.write( dedent('''\
+                executable     = %s
+                universe       = vanilla
+                requirements   = %s
+                output         = $(stdout)
+                error          = $(stderr)
+                log            = %s
+                getenv         = %s
+                nice_user      = %s
+                arguments      = $(args)
+                ''' % (launch_file,req,
+                       self.log_file,str(self.getenv),str(self.nice))))
+        if self.mem>0:
+            #condor need value in Kb
+            condor_dat.write('ImageSize      = %d\n'%(self.mem))
+
+        if self.files: #ON_EXIT_OR_EVICT
+            condor_dat.write( dedent('''\
+                when_to_transfer_output = ON_EXIT
+                should_transfer_files   = Yes
+                transfer_input_files    = %s
+                '''%(self.files+','+launch_file+','+self.tasks[0].commands[0].split()[0]))) # no directory
+        if self.env:
+            condor_dat.write('environment    = '+self.env+'\n')
+        if self.raw:
+            condor_dat.write( self.raw+'\n')
+        if self.rank:
+            condor_dat.write( dedent('''\
+                rank = %s
+                ''' %(self.rank)))
+
+        condor_dat.write("\nqueue\n")
+        condor_dat.close()
+
+        condor_file_dag = condor_file+".dag"
+        condor_dag = open( condor_file_dag, 'w' )
+        for i in range(len(self.tasks)):
+            task=self.tasks[i]
+            argstring =condor_escape_argument(' ; '.join(task.commands))
+            argstring =' ; '.join(task.commands)
+            condor_dag.write("JOB %d %s\n"%(i,condor_file))
+            condor_dag.write('VARS %d args="%s"\n'%(i,argstring))
+            s=os.path.join(self.log_dir,"condor"+self.base_tasks_log_file[i])
+            condor_dag.write('VARS %d stdout="%s"\n'%(i,s+".out"))
+            condor_dag.write('VARS %d stderr="%s"\n\n'%(i,s+".err"))
+        condor_dag.close()
+
+        dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
+        overwrite_launch_file=False
+        if not os.path.exists(dbi_file):
+            print '[DBI] WARNING: Can\'t locate file "dbi.py". Maybe the file "'+launch_file+'" is not up to date!'
+        else:
+            if os.path.exists(launch_file):
+                mtimed=os.stat(dbi_file)[8]
+                mtimel=os.stat(launch_file)[8]
+                if mtimed>mtimel:
+                    print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your needs!'
+                    overwrite_launch_file=True
+
+        if self.copy_local_source_file:
+            source_file_dest = os.path.join(self.log_dir,
+                                            os.path.basename(source_file))
+            shutil.copy( source_file, source_file_dest)
+            self.temp_files.append(source_file_dest)
+            os.chmod(source_file_dest, 0755)
+            source_file=source_file_dest
+
+        if not os.path.exists(launch_file) or overwrite_launch_file:
+            self.temp_files.append(launch_file)
+            launch_dat = open(launch_file,'w')
+            if source_file and not source_file.endswith(".cshrc"):
+                launch_dat.write(dedent('''\
+                    #!/bin/sh
+                    '''))
+                if condor_home:
+                    launch_dat.write('export HOME=%s\n' % condor_home)
+                if source_file:
+                    launch_dat.write('source ' + source_file + '\n')
+
+                launch_dat.write(dedent('''\
+                    echo "Executing on " `/bin/hostname` 1>&2
+                    echo "HOSTNAME: ${HOSTNAME}" 1>&2
+                    echo "PATH: $PATH" 1>&2
+                    echo "PYTHONPATH: $PYTHONPATH" 1>&2
+                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
+                    echo "OMP_NUM_THREADS: $OMP_NUM_THREADS" 1>&2
+                    #which python 1>&2
+                    #echo -n python version: 1>&2
+                    #python -V 1>&2
+                    #echo -n /usr/bin/python version: 1>&2
+                    #/usr/bin/python -V 1>&2
+                    echo "Running: command: sh -c \\"$@\\"" 1>&2
+                    sh -c "$@"
+                    '''))
+            else:
+                launch_dat.write(dedent('''\
+                    #! /bin/tcsh
+                    \n'''))
+                if condor_home:
+                    launch_dat.write('setenv HOME %s\n' % condor_home)
+                if source_file:
+                    launch_dat.write('source ' + source_file + '\n')
+                launch_dat.write(dedent('''\
+                    echo "Executing on " `/bin/hostname`
+                    echo "HOSTNAME: ${HOSTNAME}"
+                    echo "PATH: $PATH"
+                    echo "PYTHONPATH: $PYTHONPATH"
+                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
+                    #which python
+                    #echo -n python version:
+                    #python -V
+                    #echo -n /usr/bin/python version:
+                    #/usr/bin/python -V
+                    #echo ${PROGRAM} $@
+                    #${PROGRAM} "$@"
+                    echo "Running command: $argv"
+                    $argv
+                    '''))
+            launch_dat.close()
+            os.chmod(launch_file, 0755)
+
+        utils_file = os.path.join(self.tmp_dir, 'utils.py')
+        if not os.path.exists(utils_file):
+            shutil.copy( get_plearndir()+
+                         '/python_modules/plearn/parallel/utils.py', utils_file)
+            self.temp_files.append(utils_file)
+            os.chmod(utils_file, 0755)
+
+        configobj_file = os.path.join(self.tmp_dir, 'configobj.py')
+        if not os.path.exists('configobj.py'):
+            shutil.copy( get_plearndir()+
+                         '/python_modules/plearn/parallel/configobj.py',  configobj_file)
+            self.temp_files.append(configobj_file)
+            os.chmod(configobj_file, 0755)
+
+        # Launch condor
+        condor_cmd = 'condor_submit_dag -maxjobs %s %s'%(str(self.nb_proc), condor_file_dag)
+        if self.test == False:
+            (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
+            print "[DBI] Executing: " + condor_cmd
+            for task in self.tasks:
+                task.set_scheduled_time()
+            self.p = Popen( condor_cmd, shell=True)
+            self.p.wait()
+            if self.p.returncode != 0:
+                print "[DBI] condor_submit_dag failed! We can't stard the jobs"
+        else:
+            print "[DBI] In test mode we don't launch the jobs. To to it, you need to execute '"+condor_cmd+"'"
+            if self.dolog:
+                print "[DBI] The scheduling time will not be logged when you will submit the condor file"
+
     def run_all_job(self):
         if len(self.tasks)==0:
             return #no task to run
@@ -855,6 +1050,8 @@
         else:
             launch_file = os.path.join(self.log_dir, 'launch.sh')
 
+        self.log_file= os.path.join(self.log_dir,"condog.log")
+
         if self.mem<=0:
             try:
                 self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size/1024
@@ -866,13 +1063,13 @@
                 requirements   = %s
                 output         = %s/condor.$(Process).out
                 error          = %s/condor.$(Process).error
-                log            = %s/condor.log
+                log            = %s
                 getenv         = %s
                 nice_user      = %s
                 ''' % (launch_file,req,
                        self.log_dir,
                        self.log_dir,
-                       self.log_dir,str(self.getenv),str(self.nice))))
+                       self.log_file,str(self.getenv),str(self.nice))))
         if self.mem>0:
             #condor need value in Kb
             condor_dat.write('ImageSize      = %d\n'%(self.mem))
@@ -1041,13 +1238,15 @@
         print "[DBI] The Log file are under %s"%self.log_dir
 
         self.exec_pre_batch()
+        if self.nb_proc != 0:
+            self.run_dag()
+        else:
+            self.run_all_job()
 
-        self.run_all_job()
-
         self.exec_post_batch()
 
     def wait(self):
-        print "[DBI] WARNING no waiting for all job to finish implemented for condor, use 'condor_q' or 'condor_wait %s/condor.log'"%(self.log_dir)
+        print "[DBI] WARNING no waiting for all job to finish implemented for condor, use 'condor_q' or 'condor_wait %s'"%(self.log_file)
 
     def clean(self):
         pass



From nouiz at mail.berlios.de  Wed Sep  3 19:23:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Sep 2008 19:23:49 +0200
Subject: [Plearn-commits] r9430 - trunk/scripts
Message-ID: <200809031723.m83HNnM9015377@sheep.berlios.de>

Author: nouiz
Date: 2008-09-03 19:23:38 +0200 (Wed, 03 Sep 2008)
New Revision: 9430

Modified:
   trunk/scripts/dbidispatch
Log:
better help message and condor=N don't generate warning anymore


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-03 13:59:36 UTC (rev 9429)
+++ trunk/scripts/dbidispatch	2008-09-03 17:23:38 UTC (rev 9430)
@@ -3,12 +3,11 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] <back-end parameter> {--file=FILEPATH | <command-template>}
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] <back-end parameter> {--file=FILEPATH | <command-template>}
 
 <back-end parameter>:
     bqtools, cluster option  :[--duree=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
-    all except condor options:[--[*no_]nb_proc=N]
     cluster, condor options  : [--32|--64|--3264] [--os=X] [--mem=N]
     condor option            : [--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
                                [--[*no_]getenv] [*--[no_]prefserver] 
@@ -29,21 +28,19 @@
 common options:
   The -h, --help print the long help(this)
   The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
-  The --dbilog (--no_dbilog) tells dbi to generate (or not) an additional log
+  The '--[no_]dbilog' tells dbi to generate (or not) an additional log
   The '--[no_]test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
   The '--testdbi' set only dbi in test mode. Not dbidispatch
-  The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.
-
-dbidispatch --test --file=tests
-
-
-bqtools, cluster, local and ssh options:
-  --nb_proc=nb_proc, specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly. (work for condor, bqtools, cluster with --cwait)
+  The '--file=FILEPATH' specifies a file containing the jobs to execute, one per line. This is instead of specifying one job[s] on the command line.
+  The '--nb_proc=nb_proc', specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly. (work for condor, bqtools, cluster with --cwait)
     --local=N is the same as --local --nb_proc=N
     --cluster=N is the same as --cluster --nb_proc=N
     --bqtools=N is the same as --bqtools --nb_proc=N
     --ssh=N is the same as --ssh --nb_proc=N
+    --condor=N  is the same as --condor --nb_proc=N
+  The '--[*no_]clean_up' set the DBI option clean_up to true(false)
 
+
 bqtools and cluster option:
   The '--duree' option specifies the maximum duration of the jobs. The syntax depends on where the job is dispatched. For the cluster syntax, see 'cluster --help'. For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 13 minutes and 15 seconds.
 
@@ -107,23 +104,22 @@
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
-The arguments may contain segments of the form {{a,b,c,d}}, which trigger
-parallel dispatch: a separate 'cluster --execute' command is issued for
-the rest of the command template, the first time with value a, the second
-time with value b, etc.  For example, the command (NOTE: THERE MUST NOT
+The arguments may contain one or many segments of the form {{a,b,c,d}}, which generate multiple jobs to execute. Each segement will be replaced by one of the vlue in the segment separated by comma. The first will have the a value, the second the b value, etc. If their is many segment, it will generate the cross-product of possible value between the segment. The jobs will be executed serially or in parallel depending of the backend and the nb_proc option.
+
+  For example, the command (NOTE: THERE MUST NOT
 BE ANY SPACES WITHIN THE 'numhidden={{5,10,25}}' part and the quotes are
 important to avoid shell misinterpretation) :
 
   dbidispatch aplearn myscript.plearn 'numhidden={{5,10,25}}'
 
-is equivalent to launching three jobs in parallel on the cluster:
+is equivalent to launching three jobs:
 
   aplearn myscript.plearn numhidden=5
   aplearn myscript.plearn numhidden=10
   aplearn myscript.plearn numhidden=25
 
 If several arguments contain {{ }} forms, all combinations of arguments
-are taken, and the jobs are all launched in parallel.  For instance
+are taken. For instance
 
   dbidispatch aplearn myscript.plearn 'numhidden={{10,25}}' 'wd={{0.01,0.001}}'
 
@@ -136,8 +132,6 @@
 
 In the file of the option --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
 
-The '--[*no_]clean_up' set the DBI option clean_up to true(false)
-
 The environnement variable DBIDISPATCH_DEFAULT_OPTION can contain default option that you always want to pass to dbidispatch. You can override them on the command line.
 The environnement variable DBIDISPATCH_LOGDIR set the name of the directory where all the individual logs directory will be put. If not present default to LOGS.
 """%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}
@@ -274,19 +268,15 @@
     print ShortHelp
     sys.exit(1)
 
-valid_dbi_param=["clean_up", "test", "dolog"]
-if launch_cmd=="Ssh":
-    valid_dbi_param+=["nb_proc"]
-elif launch_cmd=="Cluster":
-    valid_dbi_param +=["cwait","force","nb_proc","arch","interruptible",
+valid_dbi_param=["clean_up", "test", "dolog","nb_proc"]
+if launch_cmd=="Cluster":
+    valid_dbi_param +=["cwait","force","arch","interruptible",
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env"]
 elif launch_cmd=="Bqtools":
-    valid_dbi_param +=["micro", "long","nb_proc","duree"]
-elif launch_cmd=="Local":
-    valid_dbi_param +=["nb_proc"]
+    valid_dbi_param +=["micro", "long", "duree"]
 
 from socket import gethostname
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):



From nouiz at mail.berlios.de  Wed Sep  3 22:15:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Sep 2008 22:15:07 +0200
Subject: [Plearn-commits] r9431 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200809032015.m83KF7XH020805@sheep.berlios.de>

Author: nouiz
Date: 2008-09-03 22:15:06 +0200 (Wed, 03 Sep 2008)
New Revision: 9431

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
better doc, bugfix for dag


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-03 17:23:38 UTC (rev 9430)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-03 20:15:06 UTC (rev 9431)
@@ -883,15 +883,36 @@
 
         condor_file_dag = condor_file+".dag"
         condor_dag = open( condor_file_dag, 'w' )
-        for i in range(len(self.tasks)):
-            task=self.tasks[i]
-            argstring =condor_escape_argument(' ; '.join(task.commands))
-            argstring =' ; '.join(task.commands)
-            condor_dag.write("JOB %d %s\n"%(i,condor_file))
-            condor_dag.write('VARS %d args="%s"\n'%(i,argstring))
-            s=os.path.join(self.log_dir,"condor"+self.base_tasks_log_file[i])
-            condor_dag.write('VARS %d stdout="%s"\n'%(i,s+".out"))
-            condor_dag.write('VARS %d stderr="%s"\n\n'%(i,s+".err"))
+        if self.base_tasks_log_file:
+            for i in range(len(self.tasks)):
+                task=self.tasks[i]
+                argstring =' ; '.join(task.commands)
+                condor_dag.write("JOB %d %s\n"%(i,condor_file))
+                condor_dag.write('VARS %d args="%s"\n'%(i,argstring))
+                s=os.path.join(self.log_dir,"condor"+self.base_tasks_log_file[i])
+                condor_dag.write('VARS %d stdout="%s"\n'%(i,s+".out"))
+                condor_dag.write('VARS %d stderr="%s"\n\n'%(i,s+".err"))
+        elif self.stdouts and self.stderrs:
+            assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
+            for (task,stdout_file,stderr_file) in zip(self.tasks,self.stdouts,self.stderrs):
+                if stdout_file==stderr_file:
+                    print "Condor can't redirect the stdout and stderr to the same file!"
+                    sys.exit(1)
+                argstring =' ; '.join(task.commands)
+                condor_dag.write("JOB %d %s\n"%(i,condor_file))
+                condor_dag.write('VARS %d args="%s"\n'%(i,argstring))
+                s=os.path.join(self.log_dir,"condor"+self.base_tasks_log_file[i])
+                condor_dag.write('VARS %d stdout="%s"\n'%(i,stdout_file))
+                condor_dag.write('VARS %d stderr="%s"\n\n'%(i,stderr_file))
+
+
+        elif self.stdouts or self.stderrs:
+            print "DBICondor should have stdouts and stderrs or none of them"
+            sys.exit(1)
+        else:
+            #should not happen
+            raise NotImplementedError()
+                
         condor_dag.close()
 
         dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-03 17:23:38 UTC (rev 9430)
+++ trunk/scripts/dbidispatch	2008-09-03 20:15:06 UTC (rev 9431)
@@ -21,86 +21,111 @@
 An * after '[', '{' or ',' signals the default value.
 An + after } tell that we can put one or more of the choise separeted by a comma
 '''
-LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
+LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster,
+local and ssh. If no system is selected on the command line, we try them in the
+previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
 
 common options:
   The -h, --help print the long help(this)
-  The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
+  The --condor, --bqtools, --cluster, --local or --ssh option specify on which 
+    system the jobs will be sent. If not present, we will use the first 
+    available in the previously given order. ssh is never automaticaly selected.
   The '--[no_]dbilog' tells dbi to generate (or not) an additional log
-  The '--[no_]test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
+  The '--[no_]test' option makes dbidispatch generate the file %(ScriptName)s,
+    without executing it. That way you can see what dbidispatch generates. Also,
+    this file calls dbi in test mode, so dbi do everything but don't execute the
+    jobs. (so you can check the script).
   The '--testdbi' set only dbi in test mode. Not dbidispatch
-  The '--file=FILEPATH' specifies a file containing the jobs to execute, one per line. This is instead of specifying one job[s] on the command line.
-  The '--nb_proc=nb_proc', specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly. (work for condor, bqtools, cluster with --cwait)
+  The '--file=FILEPATH' specifies a file containing the jobs to execute, one 
+    per line. This is instead of specifying job(s) on the command line.
+  The '--nb_proc=nb_proc', specifies the maximum number of concurrent jobs. 
+    The value -1 will try to execute all jobs concurently. This work for condor,
+    bqtools, but for cluster you SHOULD add  the --cwait option.
     --local=N is the same as --local --nb_proc=N
     --cluster=N is the same as --cluster --nb_proc=N
     --bqtools=N is the same as --bqtools --nb_proc=N
     --ssh=N is the same as --ssh --nb_proc=N
     --condor=N  is the same as --condor --nb_proc=N
-  The '--[*no_]clean_up' set the DBI option clean_up to true(false)
+  The '--[*no_]clean_up' set the DBI option clean_up to true or false
 
-
 bqtools and cluster option:
-  The '--duree' option specifies the maximum duration of the jobs. The syntax depends on where the job is dispatched. For the cluster syntax, see 'cluster --help'. For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 13 minutes and 15 seconds.
+  The '--duree' option specifies the maximum duration of the jobs. The syntax 
+    depends on the back-end. For the cluster syntax, see 'cluster --help'. 
+    For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 
+    13 minutes and 15 seconds.
 
 bqtools only options:
-  The '--micro[=nb_batch]' option can be used with BqTools when launching many jobs that
-  have a very short duration. This may prevent some queue crashes. The nb_batch value
-  is the number of experience to group together in a batch.(default 20)
-
+  If the --long option is not set, the maximum duration of each job will be 
+    120 hours (5 days).
+  The '--micro[=nb_batch]' option can be used with BqTools when launching many 
+    jobs that have a very short duration. This may prevent some queue crashes. 
+    The nb_batch value is the number of experience to group together in a batch.
+    (by default not used, --micro is equivalent to --micro=20)
   The '--long' option must be used with BqTools to launch jobs whose duration
-  is more than 5 days. The maximum duration of a job will be either the
-  BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
-  set, and 1200:00:00 (50 days) otherwise.
-  Since long jobs are launched on a different queue with few nodes, please make
-  sure you are not using too many nodes at once.
-  If this option is not set, the maximum duration of each job will be 120 hours
-  (5 days).
+    is more than 5 days. The maximum duration of a job will be either the
+    BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
+    set, and 1200:00:00 (50 days) otherwise. Since long jobs are launched on a
+    different queue with few nodes, please make sure you are not using too many
+    nodes at once with the --nb_proc option.
 
 cluster and condor options:
-  The '--3264', '--32' or '--64' specify which type of cpu the node must have to execute the commands.
-  The '--mem=X' speficify the number of meg the program need to execute.
-  The '--os=X' speficify the os of the server. Cluster default: fc4. Condor default to the same as the submit host. On condor, --os=FC7,FC9 tell to use FC7 or FC9 hosts.
+  The '--3264', '--32' or '--64' specify the type of cpu for the execution node.
+  The '--mem=X' speficify the number of ram in meg the program need to execute.
+  The '--os=X' speficify the os of the server. 
+    Cluster default: fc7. Cluster accepted value fc4, fc7 and fc9.
+    Condor default to the same as the submit host and --os=FC7,FC9 
+    tell to use FC7 or FC9 hosts.
 
 cluster only options:
-  The '--[no_]cwait' is transfered to cluster. This must be enabled if there is not nb_proc available nodes. Otherwise when there are no nodes available, the launch of that command fails.
+  The '--[no_]cwait' is transfered to cluster. 
+    This must be enabled if there is not nb_proc available nodes. Otherwise 
+    when there are no nodes available, the launch of that command fails.
   The '--force' option is passed to cluster
   The '--interruptible' option is passed to cluster
   The '--cpu=nb_cpu_per_node' option is passed to cluster
 
 condor only options:
-  The '--[no_]getenv' option is forwarded to condor. If True, the current environnement variable will be forwarded to the execution node.
-  The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:
+  If the CONDOR_HOME environment variable is set, then the HOME variable will
+     be set to this value for jobs submitted to condor.
+  The '--[no_]getenv' option is forwarded to condor. If True, the current 
+    environnement variable will be forwarded to the execution node.
+  The '--req=\"CONDOR_REQUIREMENT\"' add requirement for condor. 
+    CONDOR_REQUIREMENT must follow the syntax of requirement for condor with 
+    one exception. The symbol '\"' must be escaped 3 times! So the requirement 
+    (Machine == \"computer.example.com\") must be writen in the following way:
 
-  dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
-     or
-  dbidispatch '--req=Machine=="computer.example.com"'
+    dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
+       or
+    dbidispatch '--req=Machine=="computer.example.com"'
 
-  The '--[no_]server' option add the requirement that the executing host must be a server dedicated to computing. This is equivalent to: dbidispatch '--req=SERVER==True'(SERVER==False)
-  The '--[no_]prefserver' option will tell that you prefer to execute on server first. This is equivalent to 'rank=SERVER=?=True' in the submit file.
+  The '--[no_]server' option add the requirement that the executing host must
+    be a server dedicated to computing. This is equivalent to: 
+    dbidispatch '--req=SERVER==True'(SERVER==False)
+  The '--[no_]prefserver' option will tell that you prefer to execute on server
+    first. This is equivalent to dbidispatch '--rank=SERVER=?=True'.
   The '--rank=STRING' option add rank=STRING in the submit file.
-  The '--machine=full_host_name' option add the requirement that the executing host is full_host_name
-     dbidispatch --machine=computer.example.com
-        witch is equivalent to
-     dbidispatch '--req=Machine=="computer.example.com"'
-  The '--machines=regexp' option add the requirement that the executing host name must be match the regexp
+  The '--machine=full_host_name' option add the requirement that the executing
+     host is full_host_name. Is equivalent to
+     dbidispatch '--req=Machine=="full_host_name"'
+  The '--machines=regexp' option add the requirement that the executing host 
+    name must be match the regexp
      dbidispatch '--machines=computer00*'
         witch is equivalent to
      dbidispatch '--req=regexp("computer0*", target.Machine)'
-  The '--[no_]nice' option set the nice_user option to condor. If nice, the job(s) will have the lowest possible priority.
+  The '--[no_]nice' option set the nice_user option to condor. 
+    If nice, the job(s) will have the lowest possible priority.
   The '--env=VAR=VALUE' option will set in the environment of the executing jobs the variable VAR with value VALUE. To pass many variable you can 1) use one --env option and separ the value by ';'(don't forget to quote) or 2) you can pass many time the --env parameter.
   The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
-  If the CONDOR_HOME environment variable is set, then the HOME variable will
-     be set to this value for jobs submitted to condor.
+  The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
   The '--tasks_filename={compact,explicit,nb0,nb1,sh}+' option will change the filename where the stdout, stderr are redirected. We can put many option separated by comma. They will be appended in the filename with a dot. For all format except sh, they have this pattern condor.X.{out,error} where X=:
       - default : same as nb0
-      - compact : will be a unic string with parameter that change of value between jobs
-      - explicit: will be a unic string that represent the full command to execute
+      - compact : a unic string with parameter that change of value between jobs
+      - explicit: a unic string that represent the full command to execute
       - nb0     : a number from 0 to nb job -1.
       - nb1     : a number from 1 to nb job.
       - sh      : parse the command for > and 2> redirection command. If one or both of them are missing, they are redirected to /dev/null
-  The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
@@ -142,7 +167,7 @@
 FILE = ""
 dbi_param={}
 testmode=False
-tasks_filename=""
+tasks_filename = ["nb0"]
 
 PATH=os.getenv('PATH')
 if search_file('condor_submit',PATH):
@@ -405,7 +430,7 @@
             stdouts.append(output)
             stderrs.append(error)
             commands[x]=' '.join(sp)
-            
+            dbi_param[n]=[]
         dbi_param["stdouts"]=stdouts
         dbi_param["stderrs"]=stderrs
     else:



From nouiz at mail.berlios.de  Wed Sep  3 22:34:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Sep 2008 22:34:37 +0200
Subject: [Plearn-commits] r9432 - trunk/plearn/vmat
Message-ID: <200809032034.m83KYbn5022121@sheep.berlios.de>

Author: nouiz
Date: 2008-09-03 22:34:36 +0200 (Wed, 03 Sep 2008)
New Revision: 9432

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
bugfix


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-09-03 20:15:06 UTC (rev 9431)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-09-03 20:34:36 UTC (rev 9432)
@@ -180,7 +180,7 @@
 
     // Obtain meta information from source.
     setMetaInfoFromSource();
-    if(values.size()==0)
+    if(hasMetaDataDir() && values.size()==0)
         setMetaDataDir(getMetaDataDir());
 }
 



From nouiz at mail.berlios.de  Thu Sep  4 19:13:17 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Sep 2008 19:13:17 +0200
Subject: [Plearn-commits] r9433 - trunk/scripts
Message-ID: <200809041713.m84HDHQa010924@sheep.berlios.de>

Author: nouiz
Date: 2008-09-04 19:13:16 +0200 (Thu, 04 Sep 2008)
New Revision: 9433

Modified:
   trunk/scripts/dbidispatch
Log:
bugfix for the --tasks_filename=compact option. If their was no choise, we would not work correctly.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-03 20:34:36 UTC (rev 9432)
+++ trunk/scripts/dbidispatch	2008-09-04 17:13:16 UTC (rev 9433)
@@ -392,13 +392,13 @@
 
 def merge_pattern(new_list):
     return [x+'.'+y for (x,y) in  zip(dbi_param[n], new_list)]
-
 for pattern in tasks_filename:
     if pattern == "explicit":
         dbi_param[n]=merge_pattern([re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands])
     elif pattern == "compact":
-        dbi_param[n]=merge_pattern([re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in choise_args])
-       
+        l=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in choise_args]
+        if l:
+            dbi_param[n]=merge_pattern(l)
     elif pattern == "nb0":
         dbi_param[n]=merge_pattern(map(str,range(len(commands))))
     elif pattern == "nb1":



From nouiz at mail.berlios.de  Thu Sep  4 19:56:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Sep 2008 19:56:54 +0200
Subject: [Plearn-commits] r9434 - trunk/python_modules/plearn/parallel
Message-ID: <200809041756.m84HusVd017970@sheep.berlios.de>

Author: nouiz
Date: 2008-09-04 19:56:53 +0200 (Thu, 04 Sep 2008)
New Revision: 9434

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
now --condor=X should work. It support double quote in argumetns but not single quote due to a limitation in condor. I ask the mailing is their is a workaround


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-04 17:13:16 UTC (rev 9433)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-04 17:56:53 UTC (rev 9434)
@@ -669,6 +669,14 @@
     # then surrount everything by a pair of double quotes
     return "\"'" + argstring.replace("'", "''").replace('"','""') + "'\""
 
+def condor_dag_escape_argument(argstring):
+    # escape the double quote so that dagman handle it corretly
+    # DAGMAN don't handle single quote!!!
+    if "'" in argstring:
+        print "[DBI] ERROR: the condor back-end with dagman don't support using the ' symbol in the command line"
+        sys.exit(1)
+    return argstring.replace('"',r'\\\"')
+
 class DBICondor(DBIBase):
 
     def __init__( self, commands, **args ):
@@ -886,10 +894,11 @@
         if self.base_tasks_log_file:
             for i in range(len(self.tasks)):
                 task=self.tasks[i]
-                argstring =' ; '.join(task.commands)
+                argstring =condor_dag_escape_argument(' ; '.join(task.commands))
                 condor_dag.write("JOB %d %s\n"%(i,condor_file))
                 condor_dag.write('VARS %d args="%s"\n'%(i,argstring))
-                s=os.path.join(self.log_dir,"condor"+self.base_tasks_log_file[i])
+                s=os.path.join(self.log_dir,
+                               "condor"+self.base_tasks_log_file[i])
                 condor_dag.write('VARS %d stdout="%s"\n'%(i,s+".out"))
                 condor_dag.write('VARS %d stderr="%s"\n\n'%(i,s+".err"))
         elif self.stdouts and self.stderrs:
@@ -898,10 +907,9 @@
                 if stdout_file==stderr_file:
                     print "Condor can't redirect the stdout and stderr to the same file!"
                     sys.exit(1)
-                argstring =' ; '.join(task.commands)
+                argstring =condor_dag_escape_argument(' ; '.join(task.commands))
                 condor_dag.write("JOB %d %s\n"%(i,condor_file))
                 condor_dag.write('VARS %d args="%s"\n'%(i,argstring))
-                s=os.path.join(self.log_dir,"condor"+self.base_tasks_log_file[i])
                 condor_dag.write('VARS %d stdout="%s"\n'%(i,stdout_file))
                 condor_dag.write('VARS %d stderr="%s"\n\n'%(i,stderr_file))
 
@@ -960,7 +968,7 @@
                     #echo -n /usr/bin/python version: 1>&2
                     #/usr/bin/python -V 1>&2
                     echo "Running: command: sh -c \\"$@\\"" 1>&2
-                    sh -c "$@"
+                    $@
                     '''))
             else:
                 launch_dat.write(dedent('''\



From tihocan at mail.berlios.de  Thu Sep  4 20:05:40 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 4 Sep 2008 20:05:40 +0200
Subject: [Plearn-commits] r9435 - trunk/python_modules/plearn/utilities
Message-ID: <200809041805.m84I5ePW018810@sheep.berlios.de>

Author: tihocan
Date: 2008-09-04 20:05:40 +0200 (Thu, 04 Sep 2008)
New Revision: 9435

Modified:
   trunk/python_modules/plearn/utilities/write_results.py
Log:
Committed change by Jerome L. - Looks like the goal was to add an option to not use the lock in writeResults

Modified: trunk/python_modules/plearn/utilities/write_results.py
===================================================================
--- trunk/python_modules/plearn/utilities/write_results.py	2008-09-04 17:56:53 UTC (rev 9434)
+++ trunk/python_modules/plearn/utilities/write_results.py	2008-09-04 18:05:40 UTC (rev 9435)
@@ -1,6 +1,6 @@
 import time, random, os
 
-def writeResults(argdict, costdict, results_amat):
+def writeResults(argdict, costdict, results_amat, need_lock = True):
     """ Write the results of an experiment in a amat file,
         managing the fact that several scripts can try yo write
         at the same time in the amat.
@@ -20,14 +20,17 @@
 
     # Create .amat if it does not exist.
     if not os.path.exists(results_amat):
-        lockFile(results_amat)
+        if need_lock:
+            lockFile(results_amat)
         f = open(results_amat, "w")
         f.write('TO_CREATE')
         f.close()
-        unlockFile(results_amat)
+        if need_lock:
+            unlockFile(results_amat)
 
     # Fill data in the .amat file.
-    lockFile(results_amat)
+    if need_lock:
+        lockFile(results_amat)
     f = open(results_amat, 'r+')
     if f.readline() == 'TO_CREATE':
         # Need to write the header.
@@ -38,19 +41,20 @@
     f.seek(0, 2)   # End of file.
     f.write('%s\n' % ' '.join( str(x) for x in argvals + cost_vals ) )
     f.close()
-    unlockFile(results_amat)
+    if need_lock:
+        unlockFile(results_amat)
 
 def getSortedValues(names_and_vals):
-    if type(names_and_vals) == dict:
+    if isinstance(names_and_vals,dict):
         keys = names_and_vals.keys()
         keys.sort()
         return keys, [names_and_vals[key] for key in keys]
-    elif type(names_and_vals) == list:
+    elif isinstance(names_and_vals,list):
         assert len(names_and_vals) == 2
         assert type(names_and_vals[0]) in [list,str]
-        if type(names_and_vals[0]) == str:
+        if isinstance(names_and_vals[0],str):
             names_and_vals[0] = names_and_vals[0].split()
-        assert type(names_and_vals[1]) == list
+        assert isinstance(names_and_vals[1],list)
         assert len(names_and_vals[0]) == len(names_and_vals[1])
         return names_and_vals[0], names_and_vals[1]
     else:



From nouiz at mail.berlios.de  Thu Sep  4 20:48:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Sep 2008 20:48:57 +0200
Subject: [Plearn-commits] r9436 - in trunk/plearn: base db dict io ker math
	var vmat
Message-ID: <200809041848.m84ImvTt023200@sheep.berlios.de>

Author: nouiz
Date: 2008-09-04 20:48:55 +0200 (Thu, 04 Sep 2008)
New Revision: 9436

Modified:
   trunk/plearn/base/Object.h
   trunk/plearn/db/SDBVMat.h
   trunk/plearn/dict/ConditionalDictionary.h
   trunk/plearn/io/IntStream.h
   trunk/plearn/io/MatIO.h
   trunk/plearn/io/PPath.h
   trunk/plearn/ker/KroneckerBaseKernel.h
   trunk/plearn/ker/ReconstructionWeightsKernel.h
   trunk/plearn/math/BandedSolvers.h
   trunk/plearn/math/Hash.h
   trunk/plearn/math/ProbSparseMatrix.cc
   trunk/plearn/math/distr_maths.h
   trunk/plearn/math/plapack.h
   trunk/plearn/var/MatrixSumOfVariable.h
   trunk/plearn/var/SumOverBagsVariable.h
   trunk/plearn/vmat/DictionaryVMatrix.h
   trunk/plearn/vmat/VMatrix.h
Log:
removed doxygen warning. This also make better doc.


Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/base/Object.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -706,6 +706,8 @@
      *
      *  @param in          PStream from which to read the new option value
      *  @param optionname  Name of the option to read from the stream
+     *  @param id          The plearn pointer of the current object.
+     *                     Usefull for better error message.
      */
     void readOptionVal(PStream &in, const string &optionname, unsigned int id = UINT_MAX);
     
@@ -860,6 +862,8 @@
      *  @endcode
      *
      *  @param in  Stream from which read the object
+     *  @param id  The plearn pointer of the current object.
+     *             Usefull for better error message.
      */
     void newread(PStream& in, unsigned int id = UINT_MAX);
 
@@ -1090,7 +1094,7 @@
      *  mechanism can only distinguish methods based on their string name and
      *  number of arguments, but not on the types of the arguments.
      *
-     *  @param ol  RemoteMethodMap to be constructed for the current class.
+     *  @param rmm  RemoteMethodMap to be constructed for the current class.
      */
     static void declareMethods(RemoteMethodMap& rmm);
 

Modified: trunk/plearn/db/SDBVMat.h
===================================================================
--- trunk/plearn/db/SDBVMat.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/db/SDBVMat.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -854,8 +854,8 @@
 //	x1*(5*2) + x2*2 + x3
 //!  In general, if there are N fields, x_1...x_N, and each can take y_i
 //!  values, then the discrete value is:
-//	\sum_{i=1}^N x_i \product_{j=i+1}^N y_j
-/*!   where \product_{j=N+1}^N is defined to be 1.
+//	/f$ \sum_{i=1}^N x_i \product_{j=i+1}^N y_j /f$
+/*!   where /f$ \product_{j=N+1}^N /f$ is defined to be 1.
   
 For convenience, this class inherites from SDBVMFieldDiscrete, but
 does not use the inherited source_ member.

Modified: trunk/plearn/dict/ConditionalDictionary.h
===================================================================
--- trunk/plearn/dict/ConditionalDictionary.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/dict/ConditionalDictionary.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -53,13 +53,13 @@
   depend on the option fields. It is defined simply by
   giving a mapping file, where each row has the form:
   
-  OPTION_1 ... OPTION_M[\tab]TAG_1 TAG_2 TAG_3 ...
+  OPTION_1 ... OPTION_M[\\tab]TAG_1 TAG_2 TAG_3 ...
 
-  where [\tab] is the tabulation character. For
+  where [\\tab] is the tabulation character. For
   instance, a Part Of Speech Dictionary could be
   defined using lines like:
 
-  pet[\tab]NN VB VBD
+  pet[\\tab]NN VB VBD
 
   When the option fields are not found in the mapping,
   then the possible values are simply the set of

Modified: trunk/plearn/io/IntStream.h
===================================================================
--- trunk/plearn/io/IntStream.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/io/IntStream.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -189,8 +189,8 @@
 };
 
 //*!< *****************************************************
-/*!   Convert <word_sequences> filename into a FilesIntStream stream.
-  This file must contain one line per <word_sequence> filename,
+/*!   Convert \<word_sequences\> filename into a FilesIntStream stream.
+  This file must contain one line per \<word_sequence\> filename,
   and each of these filenames must represent binary integers files
   that can be associated to an IntStream.
 */

Modified: trunk/plearn/io/MatIO.h
===================================================================
--- trunk/plearn/io/MatIO.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/io/MatIO.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -385,7 +385,7 @@
 //! with 'long_binary_dscriptor' being of the form '001100101011',
 //! each character being an entry of the matrix.
 //! (entry_name is ignored).
-//! Header must be: #size: length width
+//! Header must be: \#size: length width
 template<class T>
 void loadAsciiSingleBinaryDescriptor(const PPath& filename, TMat<T>& mat)
 {

Modified: trunk/plearn/io/PPath.h
===================================================================
--- trunk/plearn/io/PPath.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/io/PPath.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -90,7 +90,7 @@
 HOME:foo/bar
 
 maps to /home/dorionc/foo/bar for me, while it could map to
-/u/bengioy/foo/bar for Yoshua and to R:\foo\bar for some Windows
+/u/bengioy/foo/bar for Yoshua and to R:\\foo\\bar for some Windows
 user. Note that the canonical form of a path ALWAYS USES slash chars ('/')
 while the absolute() representation uses the appropriate slash ('/') or
 backslash ('\'). Hence, you should never care for windows' ugly '\' and always use

Modified: trunk/plearn/ker/KroneckerBaseKernel.h
===================================================================
--- trunk/plearn/ker/KroneckerBaseKernel.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/ker/KroneckerBaseKernel.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -53,7 +53,7 @@
  *  but is not currently done for performance reasons).  With these terms, the
  *  kernel function takes the form:
  *
- *    k(x,y) = \product_i delta_x[kr(i)],y[kr(i)]
+ *  \f$  k(x,y) = \product_i delta_x[kr(i)],y[kr(i)] \f$
  *
  *  where kr(i) is the i-th element of 'kronecker_indexes' (representing an
  *  index into the input vectors).  Derived classes can either integrate these

Modified: trunk/plearn/ker/ReconstructionWeightsKernel.h
===================================================================
--- trunk/plearn/ker/ReconstructionWeightsKernel.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/ker/ReconstructionWeightsKernel.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -185,7 +185,7 @@
     virtual real evaluate_sum_k_i_k_j(int i, int j) const;
 
     //! Fill 'lle_mat', which must be of size (n x n), with entries (i,j) equal to
-    //! W_{ij} + W_{ji} - \sum_k W_{ki} W_{kj}
+    //! \f$ W_{ij} + W_{ji} - \sum_k W_{ki} W_{kj} \f$
     //! (this is used in LLE to compute the kernel Gram matrix).
     virtual void computeLLEMatrix(const Mat& lle_mat) const;
 

Modified: trunk/plearn/math/BandedSolvers.h
===================================================================
--- trunk/plearn/math/BandedSolvers.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/math/BandedSolvers.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -64,7 +64,7 @@
  * Internet: www.web-reg.de
  * 
  * Solves the problem Ax=b when A is pentadiagonal and strongly nonsingular. 
- * This is much faster than x=A\y for large matrices.  
+ * This is much faster than \f$ x=A\y \f$ for large matrices.  
  * 
  * Reference: Sp?th, Helmuth "Numerik: Eine Einf?hrung f?r Mathematiker und
  * Informatiker" S. 110 . Vieweg-Verlag Braunschweig/Wiesbaden (1994)

Modified: trunk/plearn/math/Hash.h
===================================================================
--- trunk/plearn/math/Hash.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/math/Hash.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -39,7 +39,7 @@
  * This file is part of the PLearn library.
  ******************************************************* */
 
-/*! DEPRECATED!!!  Use <hash_map> instead.  */
+/*! DEPRECATED!!!  Use \<hash_map\> instead.  */
 
 /*! \file PLearn/plearn/math/Hash.h */
 

Modified: trunk/plearn/math/ProbSparseMatrix.cc
===================================================================
--- trunk/plearn/math/ProbSparseMatrix.cc	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/math/ProbSparseMatrix.cc	2008-09-04 18:48:55 UTC (rev 9436)
@@ -173,7 +173,7 @@
 }
 
 //! Normalize the matrix nXY as a joint probability matrix 
-//! /f$ \sum_i \sum_j x_{ij} =1 /f$
+//! \f$ \sum_i \sum_j x_{ij} =1 \f$
 void ProbSparseMatrix::normalizeJoint(ProbSparseMatrix& nXY, bool clear_nXY)
 {
     clear();

Modified: trunk/plearn/math/distr_maths.h
===================================================================
--- trunk/plearn/math/distr_maths.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/math/distr_maths.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -89,7 +89,7 @@
 
 //! Returns the density of a proportion x under a Beta(alpha,beta) distribution,
 //! equal to 
-//!    x^{alpha-1} (1-x}^{beta-1} / Beta(a,b)
+//!  /f$  x^{alpha-1} (1-x}^{beta-1} / Beta(a,b) /f$
 //! where
 //!    Beta(a,b) = Gamma(a)Gamma(b)/Gamma(a+b)
 real beta_density(real x, real alpha, real beta);

Modified: trunk/plearn/math/plapack.h
===================================================================
--- trunk/plearn/math/plapack.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/math/plapack.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -646,7 +646,7 @@
 void solveTransposeLinearSystem(const Mat& A, const Mat& Y, Mat& X);
 
 /*!   Returns w that minimizes ||X.w - Y||^2 + lambda.||w||^2
-  under constraint \sum w_i = 1
+  under constraint \f$ \sum w_i = 1 \f$
   Xt is the transposed of the input matrix X; Y is the target vector.
   This doesn't include any bias term.
 */

Modified: trunk/plearn/var/MatrixSumOfVariable.h
===================================================================
--- trunk/plearn/var/MatrixSumOfVariable.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/var/MatrixSumOfVariable.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -69,7 +69,7 @@
 public:
     //!  protected default constructor for persistence
     MatrixSumOfVariable() : distr(), f(), nsamples(), input_size(), curpos() {}
-    //!  Sum_{inputs \in distr} f(inputs)
+    //! \f$ Sum_{inputs \in distr} f(inputs)\f$
     MatrixSumOfVariable(VMat the_distr, Func the_f, int the_nsamples=-1, int the_input_size=-1);
     
     PLEARN_DECLARE_OBJECT(MatrixSumOfVariable);

Modified: trunk/plearn/var/SumOverBagsVariable.h
===================================================================
--- trunk/plearn/var/SumOverBagsVariable.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/var/SumOverBagsVariable.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -80,7 +80,7 @@
 public:
     //!  protected default constructor for persistence
     SumOverBagsVariable();
-    //! Sum_{bags \in vmat} f(inputs and targets in bag)
+    //! \f$ Sum_{bags \in vmat} f(inputs and targets in bag)\f$
     //! By convention a bag is a sequence of rows of the vmat in which the last column of the target
     //! indicates whether the row is the first one (and/or) the last one, with its two least significant bits:
     //!   last_column_of_target == 1 ==> first row

Modified: trunk/plearn/vmat/DictionaryVMatrix.h
===================================================================
--- trunk/plearn/vmat/DictionaryVMatrix.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/vmat/DictionaryVMatrix.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -94,7 +94,7 @@
     // * protected options *
     // *********************
 
-    //! Number of attributes in the input text file (\t separated)
+    //! Number of attributes in the input text file (\\t separated)
     int n_attributes;
 
     //! Python code snippet

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-09-04 18:05:40 UTC (rev 9435)
+++ trunk/plearn/vmat/VMatrix.h	2008-09-04 18:48:55 UTC (rev 9436)
@@ -662,8 +662,8 @@
      * @param target The VMat we compare against
      * @param stderror_threshold The threshold allowed for the standard error
      * @param missing_threshold The threshold allowed for the % of missing
-     * @param sumdiff_stderr The sum of all variable differences of stderr
-     * @param sumdiff_missing The sum of all variable differences of missing
+     * @param stderror A measure of difference 
+     * @param missing A measure of difference 
      * @return The number of differences that were found
      */
     void compareStats(VMat target,



From nouiz at mail.berlios.de  Thu Sep  4 20:55:05 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Sep 2008 20:55:05 +0200
Subject: [Plearn-commits] r9437 - in trunk: commands/PLearnCommands
	plearn_learners/online
Message-ID: <200809041855.m84It5ap023508@sheep.berlios.de>

Author: nouiz
Date: 2008-09-04 20:55:05 +0200 (Thu, 04 Sep 2008)
New Revision: 9437

Modified:
   trunk/commands/PLearnCommands/HTMLHelpCommand.h
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
better comment for doxygen...


Modified: trunk/commands/PLearnCommands/HTMLHelpCommand.h
===================================================================
--- trunk/commands/PLearnCommands/HTMLHelpCommand.h	2008-09-04 18:48:55 UTC (rev 9436)
+++ trunk/commands/PLearnCommands/HTMLHelpCommand.h	2008-09-04 18:55:05 UTC (rev 9437)
@@ -146,11 +146,11 @@
     string html_index_document;
     
     //! Filename containing the "top" of the HTML document (including an
-    //! opening <body> tag)
+    //! opening \<body\> tag)
     string html_prolog_document;
 
     //! Filename containing the "bottom" of the HTML document (including the
-    //! closing </body> tag)
+    //! closing \</body\> tag)
     string html_epilog_document;
 
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-09-04 18:48:55 UTC (rev 9436)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-09-04 18:55:05 UTC (rev 9437)
@@ -165,8 +165,8 @@
     //! Standard deviation of Gaussian noise
     real gaussian_std;
 
-    //! Parameter \tau for corrupted input sampling:
-    //!   \tilde{x}_k ~ B((x_k - 0.5) \tau + 0.5)
+    //! Parameter \f$ \tau \f$ for corrupted input sampling:
+    //! \f$ \tilde{x}_k ~ B((x_k - 0.5) \tau + 0.5) \f$
     real binary_sampling_noise_parameter;
 
     //! Number of samples to use for unsupervised fine-tuning



From nouiz at mail.berlios.de  Fri Sep  5 16:10:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 5 Sep 2008 16:10:39 +0200
Subject: [Plearn-commits] r9438 - trunk/python_modules/plearn/parallel
Message-ID: <200809051410.m85EAdFR013711@sheep.berlios.de>

Author: nouiz
Date: 2008-09-05 16:10:39 +0200 (Fri, 05 Sep 2008)
New Revision: 9438

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
last bugfix for --condor=N, added error when we try to execute many command at the same time with dag.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-04 18:55:05 UTC (rev 9437)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-05 14:10:39 UTC (rev 9438)
@@ -891,6 +891,12 @@
 
         condor_file_dag = condor_file+".dag"
         condor_dag = open( condor_file_dag, 'w' )
+        for task in self.tasks:
+            for c in task.commands:
+                if ";" in c:
+                    print "[DBI] ERROR the option --condor=N don't support the symbol ';' in the command to execute!"
+                    sys.exit(1)
+
         if self.base_tasks_log_file:
             for i in range(len(self.tasks)):
                 task=self.tasks[i]
@@ -967,7 +973,7 @@
                     #python -V 1>&2
                     #echo -n /usr/bin/python version: 1>&2
                     #/usr/bin/python -V 1>&2
-                    echo "Running: command: sh -c \\"$@\\"" 1>&2
+                    echo "Running: command: \\"$@\\"" 1>&2
                     $@
                     '''))
             else:
@@ -1181,6 +1187,7 @@
                 if source_file:
                     launch_dat.write('source ' + source_file + '\n')
 
+                #the sh -c "$@" was done to allow running many small jobs in a macro jobs like: "ls ;env"
                 launch_dat.write(dedent('''\
                     echo "Executing on " `/bin/hostname` 1>&2
                     echo "HOSTNAME: ${HOSTNAME}" 1>&2



From chapados at mail.berlios.de  Fri Sep  5 20:22:50 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 5 Sep 2008 20:22:50 +0200
Subject: [Plearn-commits] r9439 - trunk/python_modules/plearn/gui_tools
Message-ID: <200809051822.m85IMo98028660@sheep.berlios.de>

Author: chapados
Date: 2008-09-05 20:22:50 +0200 (Fri, 05 Sep 2008)
New Revision: 9439

Added:
   trunk/python_modules/plearn/gui_tools/xp_workbench_nogui.py
Log:
EXPERIMENTAL: version of ExperimentWorkbench that works when DISPLAY is not defined

Added: trunk/python_modules/plearn/gui_tools/xp_workbench_nogui.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench_nogui.py	2008-09-05 14:10:39 UTC (rev 9438)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench_nogui.py	2008-09-05 18:22:50 UTC (rev 9439)
@@ -0,0 +1,344 @@
+# xp_workbench_nogui.py
+# Copyright (C) 2008 by Nicolas Chapados
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Nicolas Chapados
+
+## System imports
+import ctypes
+import inspect
+import os.path
+import sys
+import threading
+from   datetime import datetime
+
+## Traits
+from enthought.traits.api          import *
+
+## If there is no display, set the matplotlib backend to Agg
+if not os.environ.has_key("DISPLAY"):
+    import matplotlib
+    matplotlib.use('Agg')
+
+
+#####  ExperimentContext  ###################################################
+
+class ExperimentContext(HasTraits):
+    """Contains the context of a single experiment -- either a running one
+    or a reloaded one.
+    """
+    ## Complete path to experiment directory
+    expdir = Str # Directory
+
+    ## Copy of experiment parameters
+    script_params = Instance(HasTraits, ())
+
+    ## Function to call to run experiment
+    expfunc = Function
+
+    def run_experiment(self):
+        """Run the current expfunc with current script_params."""
+        self.expfunc(self.script_params, self)
+
+
+    def figure(self, title="Figure", **kwargs):
+        """Return a new Matplotlib figure and add it to the notebook.
+
+        Note that you must paint on this figure using the Matplotlib OO
+        API, not pylab.  Apart from 'title', the other **kwargs arguments
+        are passed to matplotlib figure constructor.
+        """
+        ## Late import to allow backend to be chosen
+        import pylab
+        f = pylab.figure(**kwargs)
+        f._title = title
+        return f
+   
+    @property
+    def _display_expdir(self):
+        """Shortened version of expdir suitable for display."""
+        return os.path.basename(self.expdir)
+
+
+class _ConsoleOutput(HasTraits):
+    title    = "Output"
+    contents = Str
+
+
+
+class _WorkerThread(threading.Thread):
+    """Utility class to perform experiment in a separate thread.
+    It notifies the workbench when it's done.
+
+    Note: this thread can (theoretically) be killed from outside, which is
+    quite nonstandard in Python.  See
+    http://sebulba.wikispaces.com/recipe+thread2 for details of how this is
+    done.  However it does not seem to work very well for now...
+    """
+    def __init__(self, xp_context, wkbench):
+        def work():
+            ## Call user-defined work function, and before quitting
+            ## notify that we are done
+            xp_context.run_experiment()
+            wkbench.curworker = None
+
+        super(_WorkerThread,self).__init__(target=work)
+        self.setDaemon(True)     # Allow quitting Python even if thread still running
+        self.xp_context = xp_context
+
+    def _get_my_tid(self):
+        """determines this (self's) thread id"""
+        if not self.isAlive():
+            raise threading.ThreadError("the thread is not active")
+        
+        # do we have it cached?
+        if hasattr(self, "_thread_id"):
+            return self._thread_id
+        
+        # no, look for it in the _active dict
+        for tid, tobj in threading._active.items():
+            if tobj is self:
+                self._thread_id = tid
+                return tid
+        
+        raise AssertionError("could not determine the thread's id")
+    
+    def raise_exc(self, exctype):
+        """raises the given exception type in the context of this thread"""
+        _async_raise(self._get_my_tid(), exctype)
+    
+    def terminate(self):
+        """raises SystemExit in the context of the given thread, which should 
+        cause the thread to exit silently (unless caught)"""
+        self.raise_exc(SystemExit)    
+
+
+#####  ExperimentWorkbench  #################################################
+
+class ExperimentWorkbench(HasTraits) :
+    """Manage the interface of a traits-based experiment.
+    """
+    ## Data traits
+    script_params = Instance(HasTraits, (), desc="Script Parameters")
+    experiments   = List(ExperimentContext, desc="Set of experiments")
+
+    expfunc   = Function(desc="Function to run when experiment is running")
+    curworker = Instance(threading.Thread, desc="Worker thread, if any is running")
+
+
+    def _new_xp_context(self):
+        """Initialize an experiment context
+        """
+        expdir = None
+        if hasattr(self.script_params, 'expdir'):
+            expdir = self.script_params.expdir
+        expdir_path = self.expdir_name(self.script_params.expdir_root, expdir)
+        context = ExperimentContext(expdir = expdir_path,
+                                    script_params = self.script_params,
+                                    expfunc = self.expfunc)
+        self.experiments.append(context)
+        return context
+
+
+    def run(self, params, func, gui=False):
+        """Bind the command-line arguments to the params, and run the experiment by
+        calling 'func' in a separate thread.
+
+        Arguments:
+
+        - params: Either a class or instance inheriting from HasTraits
+          and specifying the parameters (hierarchically) for the experiment.
+
+        - func: Function to be called to run the experiment.  The function
+          is called with two arguments: a filled-out params instance
+          containing the experiment parameters, and an ExperimentContext
+          instance giving, among other things, an experiment directory and
+          facility to create Matplotlib figures to be added to the workbench.
+
+        - gui: Either a boolean, or if '__AUTO__', taken from the
+          command-line switch --no-gui or --gui.
+        """
+        assert(not gui)
+        ## Instantiate the params container if a class was passed
+        if isinstance(params,type):
+            params = params()
+        assert isinstance(params, HasTraits), \
+               "The 'params' argument must be an instance or a class inheriting from HasTraits."
+
+        ## Bind the command-line arguments to the parameters
+        self.script_params = self.bind(params, sys.argv)
+        self.expfunc = func
+
+        ## Run the thing
+        context = self._new_xp_context()
+        context.run_experiment()
+
+
+    @staticmethod
+    def bind(params, argv):
+        """Bind any arguments in argv that does not start with a '-' to
+        elements in params and has the form 'K=V'.  This assumes that
+        'params' (and the classes contained therin) inherits from
+        HasStrictTraits so that any assignment to inexistant options raises
+        an exception.
+        """
+        for arg in argv:
+            if arg.startswith('-'):
+                continue
+            if '=' in arg:
+                (k,v) = arg.split('=', 1)
+                v = v.replace("'", "\\'")
+
+                ## For compatibility between mixed plnamespace and traits,
+                ## if the first argument to a compound option does not
+                ## exist, skip it without complaining -- it is destined for
+                ## plnamespace, which have been processed before
+                if '.' in k:
+                    k_parts = k.split('.')
+                    try:
+                        getattr(params, k_parts[0])
+                    except AttributeError:
+                        continue
+                
+                if isinstance(eval("params.%s" % k), str):
+                    exec("params.%s = '%s'" % (k,v))
+                else:
+                    ## Potentially dangerous use of 'eval' with
+                    ## command-line arguments; expeditive solution for now,
+                    ## but may opt for more restricted/lenient parsing in
+                    ## the future, like we had for plargs.
+                    exec("params.%s = eval('%s')" % (k,v))
+
+        return params
+
+
+    @staticmethod
+    def expdir_name(expdir_root, expdir=None):
+        """Return an experiment directory from a root location and possibly a dir name."""
+        if expdir is None or expdir == '':
+            expdir = datetime.now().strftime("expdir_%Y%m%d_%H%M%S")
+        return os.path.join(expdir_root, expdir)
+
+
+    @staticmethod
+    def print_all_traits(root, out = sys.stdout, prefix=""):
+        """Recursively print out all traits in a key=value format.
+        Useful for generating metainfo files for experiments.
+        """
+        traits = root.get()
+        for trait_name in sorted(traits.keys()):
+            trait_value = traits[trait_name]
+            if trait_name in ["trait_added", "trait_modified"] or trait_name.startswith('_'):
+                continue
+            elif isinstance(trait_value, HasTraits):
+                ExperimentWorkbench.print_all_traits(trait_value, out, trait_name+".")
+            else:
+                print >>out, ("%-40s" % (prefix+trait_name)) + " = " + str(trait_value)
+
+
+#####  Top-Level Options  ###################################################
+
+class SingletonOptions(HasStrictTraits):
+    """Subclasses of this class are singletons, designed to provide an
+    approximate drop-in traits-based replacement to plnamespace.  If you
+    don't want or need the singleton behavior, simply have your options
+    classes inherit from HasTraits or HasStrictTraits.
+
+    Note: this class inherits from HasStrictTraits since it is intended to
+    mimic plnamespace as closely as possible to ease migration.  It should
+    be avoided for writing new code.
+    """
+    class __metaclass__(HasStrictTraits.__metaclass__):
+        __instances = {}
+        
+        def __call__(cls, *args, **kwargs):
+            klass_key = str(cls)
+            if klass_key not in SingletonOptions.__instances:
+                instance = type.__call__(cls, *args, **kwargs)
+                SingletonOptions.__instances[klass_key] = instance
+                return instance
+            else:
+                return SingletonOptions.__instances[klass_key]
+                
+
+#####  Utilities  ###########################################################
+
+def _async_raise(tid, exctype):
+    """raises the exception, performs cleanup if needed"""
+    if not inspect.isclass(exctype):
+        raise TypeError("Only types can be raised (not instances)")
+    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))
+    if res == 0:
+        raise ValueError("invalid thread id")
+    elif res != 1:
+        # """if it returns a number greater than one, you're in trouble, 
+        # and you should call it again with exc=NULL to revert the effect"""
+        ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, 0)
+        raise SystemError("PyThreadState_SetAsyncExc failed")
+
+
+
+#####  Test Case  ###########################################################
+
+if __name__ == "__main__":
+    class GlobalOpt(HasStrictTraits):
+        expdir_root       = Str("",      # Note: bug in traits; type Directory does not work if DISPLAY is not defined on UNIX/Linux systems
+                                         desc="where the experiment directory should be created")
+        expdir            = Str("",      desc="experiment directory name")
+        max_train_size    = Trait( -1 ,  desc="maximum size of training set (in days)")
+        nhidden           = Trait(3,     desc="number of hidden units")
+        weight_decay      = Trait(1e-8,  desc="weight decay to use for neural-net training")
+
+    class MinorOpt(HasStrictTraits):
+        earlystop_fraction  = Trait(  0.0, desc="fraction of training set to use for early stopping")
+        earlystop_check     = Trait(    1, desc="check early-stopping criterion every N epochs")
+        earlystop_minstage  = Trait(    1, desc="minimum optimization stage after which early-stopping can kick in")
+
+    class AllOpt(HasStrictTraits):
+        expdir_root = Delegate("GlobalOpt")
+        expdir      = Delegate("GlobalOpt")
+        GlobalOpt   = Instance(GlobalOpt, ())
+        MinorOpt    = Instance(MinorOpt,  ())
+
+    def f(params, context):
+        print "Now running a very complex experiment"
+        print "params.GlobalOpt.nhidden = ", params.GlobalOpt.nhidden
+        print "context.expdir           = ", context.expdir
+
+        print "Sleeping during a long computation..."
+        sys.stdout.flush()
+        try:
+            import time
+            time.sleep(10)
+        finally:
+            print "Done."
+
+    ExperimentWorkbench().run(AllOpt, f)
+    



From nouiz at mail.berlios.de  Fri Sep  5 22:37:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 5 Sep 2008 22:37:26 +0200
Subject: [Plearn-commits] r9440 - trunk/scripts
Message-ID: <200809052037.m85KbQAx003868@sheep.berlios.de>

Author: nouiz
Date: 2008-09-05 22:37:26 +0200 (Fri, 05 Sep 2008)
New Revision: 9440

Modified:
   trunk/scripts/dbidispatch
Log:
small opt


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-05 18:22:50 UTC (rev 9439)
+++ trunk/scripts/dbidispatch	2008-09-05 20:37:26 UTC (rev 9440)
@@ -338,8 +338,8 @@
 def generate_commands(sp):
 ### Find replacement lists in the arguments
     repl = []
+    p = re.compile('\{\{\S*\}\}')
     for arg in sp:
-        p = re.compile('\{\{\S*\}\}')
         reg = p.search(arg)
         if reg:
             curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/



From tihocan at mail.berlios.de  Mon Sep  8 22:01:40 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 8 Sep 2008 22:01:40 +0200
Subject: [Plearn-commits] r9441 - trunk/plearn/vmat
Message-ID: <200809082001.m88K1efd006415@sheep.berlios.de>

Author: tihocan
Date: 2008-09-08 22:01:39 +0200 (Mon, 08 Sep 2008)
New Revision: 9441

Modified:
   trunk/plearn/vmat/MemoryVMatrix.cc
Log:
Fixed serialization issue when modifying the memory data of a MemoryVMatrix created from scratch

Modified: trunk/plearn/vmat/MemoryVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MemoryVMatrix.cc	2008-09-05 20:37:26 UTC (rev 9440)
+++ trunk/plearn/vmat/MemoryVMatrix.cc	2008-09-08 20:01:39 UTC (rev 9441)
@@ -252,13 +252,27 @@
 #endif
     if (v.length() > 0)
         v.copyTo(memory_data[i]+j);
+
+    // Every method that writes data has the following two lines, because:
+    // (1) The correct implementation when using a source VMat should be to
+    // write in this source VMat as well, but it is not currently implemented,
+    // so we just throw an error if we try to do this.
+    // (2) We need to ensure that 'data' points to 'memory_data', in case this
+    // VMat was initially created without any 'data' build option. Otherwise
+    // the data will not be saved.
+    PLASSERT(!source);
+    data = memory_data;
 }
 
 //////////
 // fill //
 //////////
 void MemoryVMatrix::fill(real value)
-{ memory_data.fill(value); }
+{
+    memory_data.fill(value);
+    PLASSERT(!source);
+    data = memory_data;
+}
 
 
 ////////////
@@ -268,13 +282,19 @@
 {
     if (v.length() > 0)
         v.copyTo(memory_data[i]);
+    PLASSERT(!source);
+    data = memory_data;
 }
 
 ////////////
 // putMat //
 ////////////
 void MemoryVMatrix::putMat(int i, int j, Mat m)
-{ memory_data.subMat(i,j,m.length(),m.width()) << m; }
+{
+    memory_data.subMat(i,j,m.length(),m.width()) << m;
+    PLASSERT(!source);
+    data = memory_data;
+}
 
 ///////////////
 // appendRow //
@@ -283,6 +303,8 @@
 {
     memory_data.appendRow(v);
     length_++;
+    PLASSERT(!source);
+    data = memory_data;
 }
 
 ///////////



From nouiz at mail.berlios.de  Tue Sep  9 20:39:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 20:39:48 +0200
Subject: [Plearn-commits] r9442 - trunk/python_modules/plearn/parallel
Message-ID: <200809091839.m89Idmb9002628@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 20:39:47 +0200 (Tue, 09 Sep 2008)
New Revision: 9442

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
fixed typo


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-08 20:01:39 UTC (rev 9441)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-09 18:39:47 UTC (rev 9442)
@@ -1085,7 +1085,7 @@
         else:
             launch_file = os.path.join(self.log_dir, 'launch.sh')
 
-        self.log_file= os.path.join(self.log_dir,"condog.log")
+        self.log_file= os.path.join(self.log_dir,"condor.log")
 
         if self.mem<=0:
             try:



From nouiz at mail.berlios.de  Tue Sep  9 21:28:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 21:28:09 +0200
Subject: [Plearn-commits] r9443 - in trunk: plearn_learners/meta
	python_modules/plearn/learners
Message-ID: <200809091928.m89JS9hE008436@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 21:28:08 +0200 (Tue, 09 Sep 2008)
New Revision: 9443

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
moved more stuff from AdaBoostMultiClasses.py to MultiClassAdaBoost.cc to allow the parallelisation of the train in the futur. Fixed some bug with the getTestCostNames.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-09-09 18:39:47 UTC (rev 9442)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-09-09 19:28:08 UTC (rev 9443)
@@ -232,11 +232,16 @@
 
     
     int n = 0;
+//why we don't always use weak_learner_template?
     if(weak_learners.size()>0)
         n=weak_learners[0]->outputsize();
     else if(weak_learner_template)
         n=weak_learner_template->outputsize();
     weak_learner_output.resize(n);
+    
+    //for RegressionTreeNode
+    if(getTrainingSet())
+        setTrainingSet(getTrainingSet(),false);
 }
 
 // ### Nothing to add here, simply calls build_
@@ -284,7 +289,9 @@
 void AdaBoost::train()
 {
 
-    if (nstages < stage){        //!< Asking to revert to previous stage
+    if(nstages==stage)
+        return;
+    else if (nstages < stage){        //!< Asking to revert to previous stage
         PLCHECK(nstages>0); // should use forget
         cout<<"In AdaBoost::train() - reverting from stage "<<stage
             <<" to stage "<<nstages<<endl;
@@ -681,7 +688,7 @@
 void AdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
     PLASSERT(weak_learners.size()>0);
-    PLASSERT(weak_learner_output.size()==weak_learners[0]->outputsize());
+    PLASSERT(weak_learner_output.size()==weak_learner_template->outputsize());
     PLASSERT(output.size()==1);
     real sum_out=0;
     if(!pseudo_loss_adaboost && !conf_rated_adaboost)
@@ -734,8 +741,8 @@
         costs[3]=costs[4]=MISSING_VALUE;
 
     if(forward_sub_learner_test_costs){
-        Vec weighted_costs(weak_learners[0]->nTestCosts());
-        Vec sum_weighted_costs(weak_learners[0]->nTestCosts());
+        Vec weighted_costs(weak_learner_template->nTestCosts());
+        Vec sum_weighted_costs(weak_learner_template->nTestCosts());
         sum_weighted_costs.clear();
         for(int i=0;i<weak_learners.size();i++){
             weak_learners[i]->computeCostsOnly(input, target, weighted_costs);
@@ -828,6 +835,19 @@
     }
 }
 
+void AdaBoost::setTrainingSet(VMat training_set, bool call_forget)
+{ 
+    PLCHECK(weak_learner_template);
+    inherited::setTrainingSet(training_set, call_forget);
+
+    //we do this as RegressionTreeNode need a train_set for getTestCostNames
+    if(!weak_learner_template->getTrainingSet())
+        weak_learner_template->setTrainingSet(training_set,call_forget);
+    for(int i=0;i<weak_learners.length();i++)
+        if(!weak_learners[i]->getTrainingSet())
+            weak_learners[i]->setTrainingSet(training_set,call_forget);
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2008-09-09 18:39:47 UTC (rev 9442)
+++ trunk/plearn_learners/meta/AdaBoost.h	2008-09-09 19:28:08 UTC (rev 9443)
@@ -200,7 +200,9 @@
     //! Returns the names of the objective costs that the train method computes and 
     //! for which it updates the VecStatsCollector train_stats
     virtual TVec<string> getTrainCostNames() const;
+    virtual void         setTrainingSet(VMat training_set, bool call_forget=true);
 
+
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-09-09 18:39:47 UTC (rev 9442)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-09-09 19:28:08 UTC (rev 9443)
@@ -38,6 +38,7 @@
 
 
 #include "MultiClassAdaBoost.h"
+#include <plearn/vmat/ProcessingVMatrix.h>
 
 namespace PLearn {
 using namespace std;
@@ -48,7 +49,6 @@
     "MULTI-LINE \nHELP");
 
 MultiClassAdaBoost::MultiClassAdaBoost():
-    nb_stage_to_use(-1),
     forward_sub_learner_test_costs(false)
 /* ### Initialize all fields to their default value here */
 {
@@ -81,12 +81,6 @@
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
-    declareOption(ol, "nb_stage_to_use",
-                  &MultiClassAdaBoost::nb_stage_to_use,
-                  OptionBase::buildoption,
-                  "The number of stage to use when testing."
-                  " Can be lower then the number of trained stage,"
-                  " but can't be higher!");
     declareOption(ol, "learner1", &MultiClassAdaBoost::learner1,
                   OptionBase::buildoption,
                   "The sub learner to use.");
@@ -115,10 +109,12 @@
     sub_target_tmp.resize(2);
     for(int i=0;i<sub_target_tmp.size();i++)
         sub_target_tmp[i].resize(1);
-
-    output1.resize(learner1->outputsize());
-    output2.resize(learner2->outputsize());
-
+    if(learner1)
+        output1.resize(learner1->outputsize());
+    if(learner2)
+        output2.resize(learner2->outputsize());
+    if(!train_stats)
+        train_stats=new VecStatsCollector();
 }
 
 // ### Nothing to add here, simply calls build_
@@ -167,59 +163,27 @@
     inherited::forget();
 
     stage = 0;
-    
-    PLWARNING("In MultiClassAdaBoost::forget() - not implemented, training not implemented");
+    train_stats->forget();
+    learner1->forget();
+    learner2->forget();
 }
 
 void MultiClassAdaBoost::train()
 {
-    // The role of the train method is to bring the learner up to
-    // stage==nstages, updating train_stats with training costs measured
-    // on-line in the process.
+    learner1->nstages = nstages;
+    learner1->train();
+    learner2->nstages = nstages;
+    learner2->train();
+    stage=max(learner1->stage,learner2->stage);
 
-    /* TYPICAL CODE:
+    train_stats->stats.resize(0);
+    PP<VecStatsCollector> v;
 
-    static Vec input;  // static so we don't reallocate memory each time...
-    static Vec target; // (but be careful that static means shared!)
-    input.resize(inputsize());    // the train_set's inputsize()
-    target.resize(targetsize());  // the train_set's targetsize()
-    real weight;
-
-    // This generic PLearner method does a number of standard stuff useful for
-    // (almost) any learner, and return 'false' if no training should take
-    // place. See PLearner.h for more details.
-    if (!initTrain())
-        return;
-
-    while(stage<nstages)
-    {
-        // clear statistics of previous epoch
-        train_stats->forget();
-
-        //... train for 1 stage, and update train_stats,
-        // using train_set->getExample(input, target, weight)
-        // and train_stats->update(train_costs)
-
-        ++stage;
-        train_stats->finalize(); // finalize statistics for this epoch
-    }
-    */
-    PLWARNING("In MultiClassAdaBoost::train() - not implemented, should be already trained");
-    int stage1=learner1->stage;
-    int stage2=learner2->stage;
-
-    if(stage1>0 && stage1<nb_stage_to_use)
-        PLERROR("In  MultiClassAdaBoost::train() - asked to use more stage then already trained for learner1");
-    if(stage2>0 && stage2<nb_stage_to_use)
-        PLERROR("In  MultiClassAdaBoost::train() - asked to use more stage then already trained for learner1");
-    if(nb_stage_to_use>0){
-        learner1->nstages=nb_stage_to_use;
-        learner1->train();
-    }
-    if(nb_stage_to_use>0){
-        learner2->nstages=nb_stage_to_use;
-        learner2->train();
-    }
+    //we do it this way in case the learner don't have train_stats
+    if(v=learner1->getTrainStatsCollector())
+        train_stats->append(*(v),"sublearner1.");
+    if(v=learner2->getTrainStatsCollector())
+        train_stats->append(*(v),"sublearner2.");
 }
 
 void MultiClassAdaBoost::computeOutput(const Vec& input, Vec& output) const
@@ -409,6 +373,29 @@
                   "We only support target 0/1/2. We got %f.", target[0]); 
 }
 
+void MultiClassAdaBoost::setTrainingSet(VMat training_set, bool call_forget)
+{ 
+    PLCHECK(learner1 && learner2);
+    inherited::setTrainingSet(training_set, call_forget);
+    string targetname = training_set->fieldName(training_set->inputsize());
+    string input_prg  = "[%0:%"+tostring(training_set->inputsize()-1)+"]";
+    string target_prg1= "@"+targetname+" 1 0 ifelse :"+targetname;
+    string target_prg2= "@"+targetname+" 2 - 0 1 ifelse :"+targetname;
+    string weight_prg = "1 :weight";
+
+    VMat vmat1 = new ProcessingVMatrix(training_set, input_prg,
+                                       target_prg1,  weight_prg);
+    VMat vmat2 = new ProcessingVMatrix(training_set, input_prg,
+                                       target_prg2,  weight_prg);
+
+    //We don't give it if the script give them one explicitly.
+    //This can be usefull for optimization
+    if(!learner1->getTrainingSet())
+        learner1->setTrainingSet(vmat1, call_forget);
+    if(!learner2->getTrainingSet())
+        learner2->setTrainingSet(vmat2, call_forget);
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-09-09 18:39:47 UTC (rev 9442)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-09-09 19:28:08 UTC (rev 9443)
@@ -70,9 +70,6 @@
     //! ### declare public option fields (such as build options) here
     //! Start your comments with Doxygen-compatible comments such as //!
 
-    //! The number of stage that will be used
-    int nb_stage_to_use;
-
     //! Did we add the learner1 and learner2 costs to our costs
     bool forward_sub_learner_test_costs;
 
@@ -161,7 +158,7 @@
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 
-//    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
 
 protected:
     //#####  Protected Options  ###############################################

Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-09-09 18:39:47 UTC (rev 9442)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-09-09 19:28:08 UTC (rev 9443)
@@ -15,6 +15,9 @@
 #        weakLearner should be a function that take the class number for the one vs other
 #                and should return a new weak learner
 #        """
+
+#        print "AdaBoostMultiClasses is deprecated! It is only a wrapper on the MultiClassAdaBoost of plearn. It will be removed in the futur. So use MultiClassAdaBoost directly."
+        
         self.trainSet1=trainSet1
         self.trainSet2=trainSet2
 
@@ -33,6 +36,7 @@
             self.learner2.setTrainingSet(trainSet2,True)
             self.multi_class_adaboost.learner1=self.learner1
             self.multi_class_adaboost.learner2=self.learner2
+            self.multi_class_adaboost.build()
 
         self.nstages = 0
         self.stage = 0
@@ -56,10 +60,20 @@
         tmp=VecStatsCollector()
         tmp.setFieldNames(l.getTrainCostNames())
         l.setTrainStatsCollector(tmp)
+        l.build()
         return l
 
     def train(self):
         t1=time.time()
+        self.multi_class_adaboost.nstages = self.nstages
+        self.multi_class_adaboost.train()
+        self.train_stats = self.multi_class_adaboost.getTrainStatsCollector()
+        t2=time.time()
+        self.train_time+=t2-t1
+        self.stage=self.multi_class_adaboost.stage
+        return
+
+        t1=time.time()
         self.learner1.nstages = self.nstages
         self.learner1.train()
         self.learner2.nstages = self.nstages



From nouiz at mail.berlios.de  Tue Sep  9 21:28:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 21:28:57 +0200
Subject: [Plearn-commits] r9444 - trunk/plearn_learners/generic/DEPRECATED
Message-ID: <200809091928.m89JSvom008470@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 21:28:56 +0200 (Tue, 09 Sep 2008)
New Revision: 9444

Modified:
   trunk/plearn_learners/generic/DEPRECATED/Learner.h
Log:
fixed comment for doxygen


Modified: trunk/plearn_learners/generic/DEPRECATED/Learner.h
===================================================================
--- trunk/plearn_learners/generic/DEPRECATED/Learner.h	2008-09-09 19:28:08 UTC (rev 9443)
+++ trunk/plearn_learners/generic/DEPRECATED/Learner.h	2008-09-09 19:28:56 UTC (rev 9444)
@@ -100,7 +100,7 @@
     //! opens the files in append mode for writing the test results
     void openTestResultsStreams();
 
-    //! Returns the stream corresponding to testset #k (as specified by setTestDuringTrain)
+    //! Returns the stream corresponding to testset \#k (as specified by setTestDuringTrain)
     //! The stream is opened by calling opentestResultsStreams if it wasn's already.
     ostream& getTestResultsStream(int k);
     



From nouiz at mail.berlios.de  Tue Sep  9 21:30:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 21:30:29 +0200
Subject: [Plearn-commits] r9445 - trunk/plearn_learners/regressors
Message-ID: <200809091930.m89JUT43008732@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 21:30:28 +0200 (Tue, 09 Sep 2008)
New Revision: 9445

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
inlined some very short fct.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-09-09 19:28:56 UTC (rev 9444)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-09-09 19:30:28 UTC (rev 9445)
@@ -440,30 +440,6 @@
     return split_col;
 }
 
-int RegressionTreeNode::getSplitBalance()const
-{
-    if (split_col < 0) return train_set->length();
-    return split_balance;
-}
-
-real RegressionTreeNode::getErrorImprovment()const
-{
-    if (split_col < 0) return -1.0;
-    real err=leave_error[0] + leave_error[1] - after_split_error;
-    PLASSERT(is_equal(err,0)||err>0);
-    return err;
-}
-
-int RegressionTreeNode::getSplitCol()const
-{
-    return split_col;
-}
-
-real RegressionTreeNode::getSplitValue() const
-{
-    return split_feature_value;
-}
-
 TVec< PP<RegressionTreeNode> > RegressionTreeNode::getNodes()
 {
     TVec< PP<RegressionTreeNode> > return_value;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-09-09 19:28:56 UTC (rev 9444)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-09-09 19:30:28 UTC (rev 9445)
@@ -99,22 +99,32 @@
     static  void         declareOptions(OptionList& ol);
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
-    void         initNode(PP<RegressionTreeRegisters> train_set, PP<RegressionTreeLeave> leave, PP<RegressionTreeLeave> leave_template);
+    void         initNode(PP<RegressionTreeRegisters> train_set,
+                          PP<RegressionTreeLeave> leave,
+                          PP<RegressionTreeLeave> leave_template);
     void         lookForBestSplit();
-    void         compareSplit(int col, real left_leave_last_feature, real right_leave_first_feature,
-                              Vec left_error, Vec right_error, Vec missing_error);
+    inline void  compareSplit(int col, real left_leave_last_feature,
+                              real right_leave_first_feature,
+                              Vec left_error, Vec right_error,
+                              Vec missing_error);
     int          expandNode();
-    int          getSplitBalance()const;
-    real         getErrorImprovment()const;
-    int          getSplitCol() const;
-    real         getSplitValue() const;
+    inline int   getSplitBalance()const{
+        if (split_col < 0) return train_set->length();
+        return split_balance;}
+    inline real  getErrorImprovment()const{
+        if (split_col < 0) return -1.0;
+        real err=leave_error[0] + leave_error[1] - after_split_error;
+        PLASSERT(is_equal(err,0)||err>0);
+        return err;
+    }
+    inline int          getSplitCol() const{return split_col;}
+    inline real         getSplitValue() const{return split_feature_value;}
     TVec< PP<RegressionTreeNode> >  getNodes();
-    void         computeOutputAndNodes(const Vec& inputv, Vec& outputv, TVec<PP<RegressionTreeNode> >* nodes=0);
-    void         computeOutput(const Vec& inputv, Vec& outputv)
-    {
-        computeOutputAndNodes(inputv,outputv);
-    }
-    bool         haveChildrenNode(){return left_node;}
+    void         computeOutputAndNodes(const Vec& inputv, Vec& outputv,
+                                       TVec<PP<RegressionTreeNode> >* nodes=0);
+    inline void         computeOutput(const Vec& inputv, Vec& outputv)
+    {computeOutputAndNodes(inputv,outputv);}
+    inline bool         haveChildrenNode(){return left_node;}
     
 private:
     void         build_();



From nouiz at mail.berlios.de  Tue Sep  9 21:33:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 21:33:04 +0200
Subject: [Plearn-commits] r9446 - trunk/plearn_learners/regressors
Message-ID: <200809091933.m89JX4tv008910@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 21:33:03 +0200 (Tue, 09 Sep 2008)
New Revision: 9446

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
inlined some fct.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-09-09 19:30:28 UTC (rev 9445)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-09-09 19:33:03 UTC (rev 9446)
@@ -147,49 +147,6 @@
     sortRows();
 }
 
-void RegressionTreeRegisters::registerLeave(RTR_type leave_id, int row)
-{
-    leave_register[row] = leave_id;
-}
-
-real RegressionTreeRegisters::get(int i, int j) const
-{
-    return tsource->get(j,i);
-}
-
-real RegressionTreeRegisters::getTarget(int row)
-{
-    return tsource->get(inputsize(), row);
-}
-
-real RegressionTreeRegisters::getWeight(int row)
-{
-    if (weightsize() <= 0) return 1.0 / length();
-    else return tsource->get(inputsize() + targetsize(), row );
-}
-
-void RegressionTreeRegisters::setWeight(int row, real val)
-{
-    PLASSERT(inputsize()>0&&targetsize()>0);
-    PLASSERT(weightsize() > 0);
-    tsource->put( inputsize() + targetsize(), row, val );
-}
-
-void RegressionTreeRegisters::put(int i, int j, real value)
-{
-    PLASSERT(inputsize()>0&&targetsize()>0);
-    if(j!=inputsize()+targetsize())
-        PLERROR("In RegressionTreeRegisters::put - implemented the put of "
-                "the weightsize only");
-    setWeight(i,value);
-}
-
-RTR_type RegressionTreeRegisters::getNextId()
-{
-    next_id += 1;
-    return next_id;
-}
-
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type leave_id, int col, TVec<RTR_type> &reg)
 {
     for(int i=0;i<length();i++)

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-09-09 19:30:28 UTC (rev 9445)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-09-09 19:33:03 UTC (rev 9446)
@@ -91,18 +91,33 @@
     virtual void         build();
     void         initRegisters(VMat train_set);
     void         reinitRegisters();
-    void         registerLeave(RTR_type leave_id, int row);
-    virtual real get(int row, int col) const;
-    real         getTarget(int row);
-    real         getWeight(int row);
-    void         setWeight(int row,real val);
-    RTR_type     getNextId();
+    inline void         registerLeave(RTR_type leave_id, int row)
+    { leave_register[row] = leave_id;    }
+    inline virtual real get(int i, int j) const{return tsource->get(j,i);}
+    inline real         getTarget(int row)const
+    {return tsource->get(inputsize(), row);}
+    inline real         getWeight(int row)const{
+        if (weightsize() <= 0) return 1.0 / length();
+        else return tsource->get(inputsize() + targetsize(), row );
+    }
+    inline void         setWeight(int row,real val){
+        PLASSERT(inputsize()>0&&targetsize()>0);
+        PLASSERT(weightsize() > 0);
+        tsource->put( inputsize() + targetsize(), row, val );
+    }
+    inline RTR_type     getNextId(){next_id += 1;return next_id;}
     void         getAllRegisteredRow(RTR_type leave_id, int col, TVec<RTR_type> &reg);
     void         sortRows();
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);
-    virtual void put(int i, int j, real value);
-//    virtual void getNewRow(int i, const Vec& v) const;
+    inline virtual void put(int i, int j, real value)
+    {
+        PLASSERT(inputsize()>0&&targetsize()>0);
+        if(j!=inputsize()+targetsize())
+            PLERROR("In RegressionTreeRegisters::put - implemented the put of "
+                    "the weightsize only");
+        setWeight(i,value);
+    }
     VMat source;
 
 private:



From nouiz at mail.berlios.de  Tue Sep  9 21:37:56 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 21:37:56 +0200
Subject: [Plearn-commits] r9447 - trunk/plearn_learners/regressors
Message-ID: <200809091937.m89JbuJn009232@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 21:37:55 +0200 (Tue, 09 Sep 2008)
New Revision: 9447

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
Log:
inlined fcts and made 1 fct const.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-09-09 19:33:03 UTC (rev 9446)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-09-09 19:37:55 UTC (rev 9447)
@@ -155,8 +155,7 @@
     addRow(row);
     getOutputAndError(outputv,errorv);
 }
-
-void RegressionTreeLeave::removeRow(int row, Vec outputv, Vec errorv)
+void RegressionTreeLeave::removeRow(int row, Vec output, Vec error)
 {
     real weight = train_set->getWeight(row);
     real target = train_set->getTarget(row);
@@ -166,10 +165,10 @@
     real squared_target = pow(target, 2);
     weighted_targets_sum -= weight * target;
     weighted_squared_targets_sum -= weight * squared_target; 
-    getOutputAndError(outputv,errorv);
+    getOutputAndError(output, error);
 }
 
-void RegressionTreeLeave::getOutputAndError(Vec& output, Vec& error)
+void RegressionTreeLeave::getOutputAndError(Vec& output, Vec& error)const
 {
     if(length==0){        
         output.clear();
@@ -197,21 +196,6 @@
     }
 }
 
-void RegressionTreeLeave::registerRow(int row)
-{
-    train_set->registerLeave(id, row);
-}
-
-int RegressionTreeLeave::getId()
-{
-    return id;
-}
-
-int RegressionTreeLeave::getLength()
-{
-    return length;
-}
-
 void RegressionTreeLeave::printStats()
 {
     cout << " l " << length;

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-09-09 19:33:03 UTC (rev 9446)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-09-09 19:37:55 UTC (rev 9447)
@@ -43,10 +43,9 @@
 #define RegressionTreeLeave_INC
 
 #include <plearn/base/Object.h>
-
+#include <RegressionTreeRegisters.h>
 namespace PLearn {
 using namespace std;
-class RegressionTreeRegisters;
 
 class RegressionTreeLeave: public Object
 {
@@ -89,13 +88,14 @@
     virtual void         build();
     void         initLeave(PP<RegressionTreeRegisters> the_train_set);
     virtual void         initStats();
+    virtual void         addRow(int row);
     virtual void         addRow(int row, Vec outputv, Vec errorv);
-    virtual void         addRow(int row);
     virtual void         removeRow(int row, Vec outputv, Vec errorv);
-    void         registerRow(int row);
-    int          getId();
-    int          getLength();
-    virtual void         getOutputAndError(Vec& output, Vec& error);
+    inline void          registerRow(int row)
+    {train_set->registerLeave(id, row);}
+    inline int           getId()const{return id;}
+    inline int           getLength()const{return length;}
+    virtual void         getOutputAndError(Vec& output, Vec& error)const;
     virtual void         printStats();
   
 private:



From nouiz at mail.berlios.de  Tue Sep  9 21:38:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 21:38:59 +0200
Subject: [Plearn-commits] r9448 - trunk/plearn_learners/regressors
Message-ID: <200809091938.m89JcxHk009280@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 21:38:58 +0200 (Tue, 09 Sep 2008)
New Revision: 9448

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
Log:
made fct const


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-09-09 19:37:55 UTC (rev 9447)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-09-09 19:38:58 UTC (rev 9448)
@@ -178,7 +178,7 @@
     getOutputAndError(outputv,errorv);
 }
 
-void RegressionTreeMulticlassLeave::getOutputAndError(Vec& output, Vec& error)
+void RegressionTreeMulticlassLeave::getOutputAndError(Vec& output, Vec& error)const
 {
 #ifdef BOUNDCHECK
     if(multiclass_outputs.length()<=0)

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2008-09-09 19:37:55 UTC (rev 9447)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2008-09-09 19:38:58 UTC (rev 9448)
@@ -80,7 +80,7 @@
     void         addRow(int row, Vec outputv, Vec errorv);
     void         addRow(int row);
     void         removeRow(int row, Vec outputv, Vec errorv);
-    virtual void getOutputAndError(Vec& output, Vec& error);
+    void         getOutputAndError(Vec& output, Vec& error)const;
     void         printStats();
   
 private:



From nouiz at mail.berlios.de  Tue Sep  9 22:14:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 22:14:09 +0200
Subject: [Plearn-commits] r9449 - trunk/plearn_learners/regressors
Message-ID: <200809092014.m89KE9TT013191@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 22:14:08 +0200 (Tue, 09 Sep 2008)
New Revision: 9449

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
white space change


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-09-09 19:38:58 UTC (rev 9448)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-09-09 20:14:08 UTC (rev 9449)
@@ -49,7 +49,7 @@
 
 //!used to limit the memory used by limiting the length of the dataset.
 //!work with unsigned int, uint16_t, but fail with uint8_t???
-#define RTR_type uint32_t 
+#define RTR_type uint32_t
 
 namespace PLearn {
 using namespace std;



From nouiz at mail.berlios.de  Tue Sep  9 22:56:33 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 22:56:33 +0200
Subject: [Plearn-commits] r9450 - trunk/scripts
Message-ID: <200809092056.m89KuXjf018726@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 22:56:33 +0200 (Tue, 09 Sep 2008)
New Revision: 9450

Modified:
   trunk/scripts/dbidispatch
Log:
now you can pass many {{}} in one argument and dbidispatch will expand it correctly.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-09 20:14:08 UTC (rev 9449)
+++ trunk/scripts/dbidispatch	2008-09-09 20:56:33 UTC (rev 9450)
@@ -321,33 +321,45 @@
         print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end"
 print "With the command to be expanded:"," ".join(command_argv),"\n\n"
 
-def generate_combination(repl):
+def generate_combination(repl,sep=" "):
     if repl == []:
         return []
     else:
         res = []
         x = repl[0]
-        res1 = generate_combination(repl[1:])
+        res1 = generate_combination(repl[1:],sep)
         for y in x:
             if res1 == []:
                 res.append(y)
             else:
-                res.extend([y+" "+r for r in res1])
+                res.extend([y+sep+r for r in res1])
         return res
 
 def generate_commands(sp):
 ### Find replacement lists in the arguments
     repl = []
-    p = re.compile('\{\{\S*\}\}')
+    p = re.compile('\{\{\S*?\}\}')
     for arg in sp:
-        reg = p.search(arg)
-        if reg:
-            curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/
+        reg = p.findall(arg)
+        if len(reg)==1:
+            reg = p.search(arg)
+            curargs = reg.group()[2:-2].split(",")
             newcurargs = []
             for curarg in curargs:
                 new = p.sub(curarg,arg)
                 newcurargs.append(new)
             repl.append(newcurargs)
+        elif len(reg)>1:
+            s=p.split(arg)
+            tmp=[]
+            for i in range(len(reg)):
+                if s[i]:
+                    tmp.append(s[i])
+                tmp.append(reg[i][2:-2].split(","))
+            i+=1
+            if s[i]:
+                tmp.append(s[i])
+            repl.append(generate_combination(tmp,''))
         else:
             repl.append([arg])
     argscombination = generate_combination(repl)



From nouiz at mail.berlios.de  Tue Sep  9 22:58:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Sep 2008 22:58:20 +0200
Subject: [Plearn-commits] r9451 - trunk/plearn_learners/regressors
Message-ID: <200809092058.m89KwKqI018805@sheep.berlios.de>

Author: nouiz
Date: 2008-09-09 22:58:19 +0200 (Tue, 09 Sep 2008)
New Revision: 9451

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
Log:
fixed typo that the build bot didn't liked


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-09-09 20:56:33 UTC (rev 9450)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-09-09 20:58:19 UTC (rev 9451)
@@ -43,7 +43,7 @@
 #define RegressionTreeLeave_INC
 
 #include <plearn/base/Object.h>
-#include <RegressionTreeRegisters.h>
+#include "RegressionTreeRegisters.h"
 namespace PLearn {
 using namespace std;
 



From nouiz at mail.berlios.de  Wed Sep 10 21:43:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 10 Sep 2008 21:43:13 +0200
Subject: [Plearn-commits] r9452 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200809101943.m8AJhDAc020254@sheep.berlios.de>

Author: nouiz
Date: 2008-09-10 21:43:13 +0200 (Wed, 10 Sep 2008)
New Revision: 9452

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
dbidispatch --cpu=N work for condor. It tell the number of cores to reserve for a job.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-09 20:58:19 UTC (rev 9451)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-10 19:43:13 UTC (rev 9452)
@@ -685,6 +685,7 @@
         # in Meg for initialization for consistency with cluster
         # then in kilo as that is what is needed by condor
         self.mem = 0
+        self.cpu = 0
         self.req = ''
         self.raw = ''
         self.rank = ''
@@ -832,6 +833,8 @@
             req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
         else :
             req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
+        if self.cpu>0:
+            req+='&&(target.CPUS=='+self.cpu+')'
 
         if self.os:
             req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',
@@ -1072,6 +1075,8 @@
             req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
         else :
             req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
+        if self.cpu>0:
+            req+='&&(target.CPUS=='+self.cpu+')'
 
         if self.os:
             req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-09 20:58:19 UTC (rev 9451)
+++ trunk/scripts/dbidispatch	2008-09-10 19:43:13 UTC (rev 9452)
@@ -9,6 +9,7 @@
     bqtools, cluster option  :[--duree=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
     cluster, condor options  : [--32|--64|--3264] [--os=X] [--mem=N]
+                               [--cpu=nb_cpu_per_node]
     condor option            : [--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
                                [--[*no_]getenv] [*--[no_]prefserver] 
                                [--rank=RANK_EXPRESSION] 
@@ -17,7 +18,7 @@
                                [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,*nb0,nb1,sh}+]
                                [*--[no_]set_special_env]
     cluster option           : [*--[no_]cwait]  [--[*no_]force]
-                               [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
+                               [--[*no_]interruptible]
 An * after '[', '{' or ',' signals the default value.
 An + after } tell that we can put one or more of the choise separeted by a comma
 '''
@@ -77,6 +78,8 @@
     Cluster default: fc7. Cluster accepted value fc4, fc7 and fc9.
     Condor default to the same as the submit host and --os=FC7,FC9 
     tell to use FC7 or FC9 hosts.
+  The '--cpu=nb_cpu_per_node' option determine the number of cpu(cores) that 
+    will be reserved for each job.
 
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. 
@@ -84,7 +87,6 @@
     when there are no nodes available, the launch of that command fails.
   The '--force' option is passed to cluster
   The '--interruptible' option is passed to cluster
-  The '--cpu=nb_cpu_per_node' option is passed to cluster
 
 condor only options:
   If the CONDOR_HOME environment variable is set, then the HOME variable will
@@ -299,7 +301,7 @@
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
-                       "raw", "os", "set_special_env"]
+                       "raw", "os", "set_special_env", "mem", "cpu"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long", "duree"]
 



From nouiz at mail.berlios.de  Wed Sep 10 21:45:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 10 Sep 2008 21:45:15 +0200
Subject: [Plearn-commits] r9453 - in trunk: plearn_learners/meta
	python_modules/plearn/learners
Message-ID: <200809101945.m8AJjF6x020406@sheep.berlios.de>

Author: nouiz
Date: 2008-09-10 21:45:14 +0200 (Wed, 10 Sep 2008)
New Revision: 9453

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
parallel version of MultiClassAdaBoost. We must take great care when doing parallel stuff as PLearn was not build with that in mind. So this is not safe for all version of sublearner...


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-09-10 19:43:13 UTC (rev 9452)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-09-10 19:45:14 UTC (rev 9453)
@@ -171,9 +171,18 @@
 void MultiClassAdaBoost::train()
 {
     learner1->nstages = nstages;
+    learner2->nstages = nstages;
+
+//if you use the parallel version, you must disable all verbose, verbosity and report progress int he learner1 and learner2.
+//Otherwise this will cause crash due to the parallel printing to stdout stderr.
+#pragma omp parallel sections
+{
+#pragma omp section 
     learner1->train();
-    learner2->nstages = nstages;
+#pragma omp section 
     learner2->train();
+}
+
     stage=max(learner1->stage,learner2->stage);
 
     train_stats->stats.resize(0);

Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-09-10 19:43:13 UTC (rev 9452)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-09-10 19:45:14 UTC (rev 9453)
@@ -7,7 +7,8 @@
 ## The C version is much faster for all compute* and the test function.
 ##
 class AdaBoostMultiClasses:
-    def __init__(self,trainSet1,trainSet2,weakLearner,confusion_target=1):
+    def __init__(self,trainSet1,trainSet2,weakLearner,confusion_target=1,
+                 report_progress = 0, verbose = 0):
 #        """
 #        Initialize a AdaBoost for 3 classes learner
 #        trainSet1 is used for the first sub AdaBoost learner,
@@ -21,11 +22,21 @@
         self.trainSet1=trainSet1
         self.trainSet2=trainSet2
 
-        self.multi_class_adaboost = pl.MultiClassAdaBoost(forward_sub_learner_test_costs=True,
-                                                          report_progress=1,
-                                                          verbosity=1,
-                                                          nb_stage_to_use=-1
-                                                          )
+        self.nstages = 0
+        self.stage = 0
+        self.train_time = 0
+        self.test_time = 0
+        self.test_sub_time = 0
+        self.confusion_target=confusion_target
+        self.report_progress = report_progress
+        self.verbose = verbose
+
+        self.multi_class_adaboost = pl.MultiClassAdaBoost(
+            forward_sub_learner_test_costs=True,
+            report_progress=report_progress,
+            verbosity=verbose,
+            nb_stage_to_use=-1)
+
         if weakLearner:
             self.learner1 = self.myAdaBoostLearner(weakLearner(0),trainSet1)
             self.learner1.setExperimentDirectory(plargs.expdirr+"/learner1")
@@ -38,13 +49,6 @@
             self.multi_class_adaboost.learner2=self.learner2
             self.multi_class_adaboost.build()
 
-        self.nstages = 0
-        self.stage = 0
-        self.train_time = 0
-        self.test_time = 0
-        self.test_sub_time = 0
-        self.confusion_target=confusion_target
-        
     def myAdaBoostLearner(self,sublearner,trainSet):
         l = pl.AdaBoost()
         l.weak_learner_template=sublearner
@@ -60,6 +64,8 @@
         tmp=VecStatsCollector()
         tmp.setFieldNames(l.getTrainCostNames())
         l.setTrainStatsCollector(tmp)
+        l.report_progress = self.report_progress
+        l.verbose = self.verbose 
         l.build()
         return l
 



From nouiz at mail.berlios.de  Wed Sep 10 22:30:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 10 Sep 2008 22:30:58 +0200
Subject: [Plearn-commits] r9454 - trunk/python_modules/plearn/parallel
Message-ID: <200809102030.m8AKUwwS027884@sheep.berlios.de>

Author: nouiz
Date: 2008-09-10 22:30:57 +0200 (Wed, 10 Sep 2008)
New Revision: 9454

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
some code refactoring


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-10 19:45:14 UTC (rev 9453)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-10 20:30:57 UTC (rev 9454)
@@ -813,34 +813,10 @@
             id+=1
             #keeps a list of the temporary files created, so that they can be deleted at will
     def run_dag(self):
-        if len(self.tasks)==0:
-            return #no task to run
-
-        #set special environment variable
-        if self.set_special_env:
-            self.env+='" OMP_NUM_THREADS=$$(CPUS) GOTO_NUM_THREADS=$$(CPUS) MKL_NUM_THREADS=$$(CPUS) "'
-
-
         condor_file = os.path.join(self.log_dir, "dag.condor")
         self.temp_files.append(condor_file)
         condor_dat = open( condor_file, 'w' )
 
-        if self.req:
-            req = self.req
-        else:
-            req = "True"
-        if self.targetcondorplatform == 'BOTH':
-            req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
-        else :
-            req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
-        if self.cpu>0:
-            req+='&&(target.CPUS=='+self.cpu+')'
-
-        if self.os:
-            req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',
-                       self.os.split(','),
-                       req+'&&(False ')+")"
-
         source_file=os.getenv("CONDOR_LOCAL_SOURCE")
         condor_home = os.getenv('CONDOR_HOME')
         if source_file and source_file.endswith(".cshrc"):
@@ -848,12 +824,6 @@
         else:
             launch_file = os.path.join(self.log_dir, 'launch.sh')
 
-        if self.mem<=0:
-            try:
-                self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size/1024
-            except:
-                pass
-
         self.log_file = os.path.join("/tmp/bastienf/dbidispatch",self.log_dir)
         os.system('mkdir -p ' + self.log_file)
         self.log_file = os.path.join(self.log_file,"condor.log")
@@ -868,7 +838,7 @@
                 getenv         = %s
                 nice_user      = %s
                 arguments      = $(args)
-                ''' % (launch_file,req,
+                ''' % (launch_file,self.req,
                        self.log_file,str(self.getenv),str(self.nice))))
         if self.mem>0:
             #condor need value in Kb
@@ -1006,45 +976,10 @@
             launch_dat.close()
             os.chmod(launch_file, 0755)
 
-        utils_file = os.path.join(self.tmp_dir, 'utils.py')
-        if not os.path.exists(utils_file):
-            shutil.copy( get_plearndir()+
-                         '/python_modules/plearn/parallel/utils.py', utils_file)
-            self.temp_files.append(utils_file)
-            os.chmod(utils_file, 0755)
-
-        configobj_file = os.path.join(self.tmp_dir, 'configobj.py')
-        if not os.path.exists('configobj.py'):
-            shutil.copy( get_plearndir()+
-                         '/python_modules/plearn/parallel/configobj.py',  configobj_file)
-            self.temp_files.append(configobj_file)
-            os.chmod(configobj_file, 0755)
-
-        # Launch condor
         condor_cmd = 'condor_submit_dag -maxjobs %s %s'%(str(self.nb_proc), condor_file_dag)
-        if self.test == False:
-            (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
-            print "[DBI] Executing: " + condor_cmd
-            for task in self.tasks:
-                task.set_scheduled_time()
-            self.p = Popen( condor_cmd, shell=True)
-            self.p.wait()
-            if self.p.returncode != 0:
-                print "[DBI] condor_submit_dag failed! We can't stard the jobs"
-        else:
-            print "[DBI] In test mode we don't launch the jobs. To to it, you need to execute '"+condor_cmd+"'"
-            if self.dolog:
-                print "[DBI] The scheduling time will not be logged when you will submit the condor file"
+        return condor_cmd
 
-    def run_all_job(self):
-        if len(self.tasks)==0:
-            return #no task to run
-
-        #set special environment variable
-        if self.set_special_env:
-            self.env+='" OMP_NUM_THREADS=$$(CPUS) GOTO_NUM_THREADS=$$(CPUS) MKL_NUM_THREADS=$$(CPUS) "'
-
-
+    def run_non_dag(self):
         # create the bqsubmit.dat, with
         condor_datas = []
 
@@ -1067,22 +1002,6 @@
         self.temp_files.append(condor_file)
         condor_dat = open( condor_file, 'w' )
 
-        if self.req:
-            req = self.req
-        else:
-            req = "True"
-        if self.targetcondorplatform == 'BOTH':
-            req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
-        else :
-            req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
-        if self.cpu>0:
-            req+='&&(target.CPUS=='+self.cpu+')'
-
-        if self.os:
-            req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',
-                       self.os.split(','),
-                       req+'&&(False ')+")"
-
         source_file=os.getenv("CONDOR_LOCAL_SOURCE")
         condor_home = os.getenv('CONDOR_HOME')
         if source_file and source_file.endswith(".cshrc"):
@@ -1092,11 +1011,6 @@
 
         self.log_file= os.path.join(self.log_dir,"condor.log")
 
-        if self.mem<=0:
-            try:
-                self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size/1024
-            except:
-                pass
         condor_dat.write( dedent('''\
                 executable     = %s
                 universe       = vanilla
@@ -1106,7 +1020,7 @@
                 log            = %s
                 getenv         = %s
                 nice_user      = %s
-                ''' % (launch_file,req,
+                ''' % (launch_file,self.req,
                        self.log_dir,
                        self.log_dir,
                        self.log_file,str(self.getenv),str(self.nice))))
@@ -1235,6 +1149,61 @@
             launch_dat.close()
             os.chmod(launch_file, 0755)
 
+        return "condor_submit " + condor_file
+
+    def clean(self):
+        if len(self.temp_files)>0:
+            sleep(20)
+            for file_name in self.temp_files:
+                try:
+                    os.remove(file_name)
+                except os.error:
+                    pass
+                pass
+
+
+    def run(self):
+        print "[DBI] The Log file are under %s"%self.log_dir
+
+        self.exec_pre_batch()
+
+        #set special environment variable
+        if len(self.tasks)==0:
+            return #no task to run
+
+        if self.set_special_env:
+            self.env+='" OMP_NUM_THREADS=$$(CPUS) GOTO_NUM_THREADS=$$(CPUS) MKL_NUM_THREADS=$$(CPUS) "'
+
+        if not self.req:
+            self.req = "True"
+        if self.targetcondorplatform == 'BOTH':
+            self.req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
+        else :
+            self.req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
+        if self.cpu>0:
+            self.req+='&&(target.CPUS=='+self.cpu+')'
+
+        if self.os:
+            self.req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',
+                            self.os.split(','),
+                            self.req+'&&(False ')+")"
+
+        #if no mem requirement added, use the executable size.
+        #todo: if they are not the same executable, take the biggest
+        if self.mem<=0:
+            try:
+                self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size/1024
+            except:
+                pass
+
+        #exec dependent code
+        if self.nb_proc != 0:
+            cmd=self.run_dag()
+        else:
+            cmd=self.run_non_dag()
+
+        #add file if needed?
+        #why are they needed?
         utils_file = os.path.join(self.tmp_dir, 'utils.py')
         if not os.path.exists(utils_file):
             shutil.copy( get_plearndir()+
@@ -1249,41 +1218,22 @@
             self.temp_files.append(configobj_file)
             os.chmod(configobj_file, 0755)
 
-        # Launch condor
+            
+        #launch the jobs
         if self.test == False:
             (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
-            print "[DBI] Executing: condor_submit " + condor_file
+            print "[DBI] Executing: " + cmd
             for task in self.tasks:
                 task.set_scheduled_time()
-            self.p = Popen( 'condor_submit '+ condor_file, shell=True)
+            self.p = Popen( cmd, shell=True)
             self.p.wait()
             if self.p.returncode != 0:
-                print "[DBI] condor_submit failed! We can't stard the jobs"
+                print "[DBI] submission failed! We can't stard the jobs"
         else:
-            print "[DBI] Created condor file: " + condor_file
+            print "[DBI] In test mode we don't launch the jobs. To to it, you need to execute '"+cmd+"'"
             if self.dolog:
                 print "[DBI] The scheduling time will not be logged when you will submit the condor file"
 
-    def clean(self):
-        if len(self.temp_files)>0:
-            sleep(20)
-            for file_name in self.temp_files:
-                try:
-                    os.remove(file_name)
-                except os.error:
-                    pass
-                pass
-
-
-    def run(self):
-        print "[DBI] The Log file are under %s"%self.log_dir
-
-        self.exec_pre_batch()
-        if self.nb_proc != 0:
-            self.run_dag()
-        else:
-            self.run_all_job()
-
         self.exec_post_batch()
 
     def wait(self):



From saintmlx at mail.berlios.de  Thu Sep 11 16:46:40 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 11 Sep 2008 16:46:40 +0200
Subject: [Plearn-commits] r9455 - trunk/python_modules/plearn/gui_tools
Message-ID: <200809111446.m8BEkeS5020657@sheep.berlios.de>

Author: saintmlx
Date: 2008-09-11 16:46:39 +0200 (Thu, 11 Sep 2008)
New Revision: 9455

Modified:
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
   trunk/python_modules/plearn/gui_tools/xp_workbench_nogui.py
Log:
- print_all_traits now prints the full prefix (namespace) instead of just the inner-most container



Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-09-10 20:30:57 UTC (rev 9454)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-09-11 14:46:39 UTC (rev 9455)
@@ -412,7 +412,7 @@
             if trait_name in ["trait_added", "trait_modified"] or trait_name.startswith('_'):
                 continue
             elif isinstance(trait_value, HasTraits):
-                ExperimentWorkbench.print_all_traits(trait_value, out, trait_name+".")
+                ExperimentWorkbench.print_all_traits(trait_value, out, prefix+trait_name+".")
             else:
                 print >>out, ("%-40s" % (prefix+trait_name)) + " = " + str(trait_value)
 

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench_nogui.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench_nogui.py	2008-09-10 20:30:57 UTC (rev 9454)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench_nogui.py	2008-09-11 14:46:39 UTC (rev 9455)
@@ -47,7 +47,6 @@
     import matplotlib
     matplotlib.use('Agg')
 
-
 #####  ExperimentContext  ###################################################
 
 class ExperimentContext(HasTraits):
@@ -86,7 +85,6 @@
         """Shortened version of expdir suitable for display."""
         return os.path.basename(self.expdir)
 
-
 class _ConsoleOutput(HasTraits):
     title    = "Output"
     contents = Str
@@ -258,7 +256,7 @@
             if trait_name in ["trait_added", "trait_modified"] or trait_name.startswith('_'):
                 continue
             elif isinstance(trait_value, HasTraits):
-                ExperimentWorkbench.print_all_traits(trait_value, out, trait_name+".")
+                ExperimentWorkbench.print_all_traits(trait_value, out, prefix+trait_name+".")
             else:
                 print >>out, ("%-40s" % (prefix+trait_name)) + " = " + str(trait_value)
 



From tihocan at mail.berlios.de  Mon Sep 15 19:33:18 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 15 Sep 2008 19:33:18 +0200
Subject: [Plearn-commits] r9456 - trunk/plearn/vmat
Message-ID: <200809151733.m8FHXI6X026815@sheep.berlios.de>

Author: tihocan
Date: 2008-09-15 19:33:17 +0200 (Mon, 15 Sep 2008)
New Revision: 9456

Modified:
   trunk/plearn/vmat/OneHotVMatrix.cc
   trunk/plearn/vmat/OneHotVMatrix.h
Log:
Added a way to automatically figure out 'nclasses' from the source VMat

Modified: trunk/plearn/vmat/OneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/OneHotVMatrix.cc	2008-09-11 14:46:39 UTC (rev 9455)
+++ trunk/plearn/vmat/OneHotVMatrix.cc	2008-09-15 17:33:17 UTC (rev 9456)
@@ -47,8 +47,23 @@
 
 /** OneHotVMatrix **/
 
-PLEARN_IMPLEMENT_OBJECT(OneHotVMatrix, "ONE LINE DESC", "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(OneHotVMatrix,
+        "Transform an index into a one-hot vector.",
+        "Sampling from this VMat will return the corresponding sample from\n"
+        "the source VMat with last element ('target_classnum') replaced by\n"
+        "a vector of target_values of size nclasses in which only\n"
+        "target_values[target_classnum] is set to hot_value, and all the\n"
+        "others are set to cold_value.\n"
+        "In the special case where the VMat is built with nclasses==1, then\n"
+        "it is assumed that we have a 2 class classification problem but\n"
+        "we are using a single valued target.  For this special case only\n"
+        "the_cold_value is used as target for classnum 0 and the_hot_value\n"
+        "is used for classnum 1.\n"
+);
 
+///////////////////
+// OneHotVMatrix //
+///////////////////
 OneHotVMatrix::OneHotVMatrix(bool call_build_)
     : inherited(call_build_),
       nclasses(0), cold_value(0.0), hot_value(1.0), index(-1)
@@ -73,12 +88,18 @@
         build_();
 }
 
+///////////
+// build //
+///////////
 void OneHotVMatrix::build()
 {
     inherited::build();
     build_();
 }
 
+////////////
+// build_ //
+////////////
 void OneHotVMatrix::build_()
 {
     int source_inputsize = source->inputsize();
@@ -90,13 +111,13 @@
     length_ = source_length;
     width_ = source_width + nclasses - 1;
 
-
     if(source_inputsize+source_targetsize+source_weightsize != source_width
        || source_targetsize != 1 ) // source->sizes are inconsistent
     {
         if( index < 0 )
         {
             index = source_width - 1;
+            updateNClassesAndWidth();
         }
         if( inputsize_ + targetsize_ + weightsize_ != width() )
         {
@@ -111,6 +132,7 @@
         if( index < 0 )
         {
             index = source_inputsize;
+            updateNClassesAndWidth();
         }
         if( inputsize_ + targetsize_ + weightsize_ != width() )
         {
@@ -142,27 +164,40 @@
     setMetaInfoFromSource();
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void OneHotVMatrix::declareOptions(OptionList &ol)
 {
     declareOption(ol, "underlying_distr", &OneHotVMatrix::source,
                   (OptionBase::learntoption | OptionBase::nosave),
                   "DEPRECATED - use 'source' instead.");
+
     declareOption(ol, "nclasses", &OneHotVMatrix::nclasses,
-                  OptionBase::buildoption, "");
+                  OptionBase::buildoption,
+        "Number of classes. If set to zero, then this number will be\n"
+        "automatically found from the source VMat.");
+
     declareOption(ol, "cold_value", &OneHotVMatrix::cold_value,
-                  OptionBase::buildoption, "");
+                  OptionBase::buildoption,
+        "Value used for non active elements in the one-hot vector.");
+
     declareOption(ol, "hot_value", &OneHotVMatrix::hot_value,
-                  OptionBase::buildoption, "");
+                  OptionBase::buildoption,
+        "Value used for the active element in the one-hot vector.");
+
     declareOption(ol, "index", &OneHotVMatrix::index,
                   OptionBase::buildoption,
-                  "(optional) index of the column on which we apply 'one_hot'."
-                  "\n"
-                  "By default, if targetsize==1 we take corresponding column,"
-                  "\n"
-                  "else the last column.\n");
+        "Index of the column on which we apply the one-hot transformation.\n"
+        "By default, if targetsize==1 we take the target column, otherwise\n"
+        "we take the last column.");
+    
     inherited::declareOptions(ol);
 }
 
+///////////////
+// getNewRow //
+///////////////
 void OneHotVMatrix::getNewRow(int i, const Vec& samplevec) const
 {
 #ifdef BOUNDCHECK
@@ -176,11 +211,14 @@
     Vec modified = samplevec.subVec(index, nclasses);
     Vec right = samplevec.subVec(index+nclasses, width()-index-nclasses);
     source->getSubRow(i, 0, left);
-    int classnum = int(source->get(i, index));
+    int classnum = int(round(source->get(i, index)));
     fill_one_hot(modified, classnum, cold_value, hot_value);
     source->getSubRow(i, index+1, right);
 }
 
+/////////
+// dot //
+/////////
 real OneHotVMatrix::dot(int i1, int i2, int inputsize) const
 {
     return source->dot(i1, i2, inputsize);
@@ -191,6 +229,25 @@
     return source->dot(i, v);
 }
 
+
+////////////////////////////
+// updateNClassesAndWidth //
+////////////////////////////
+void OneHotVMatrix::updateNClassesAndWidth()
+{
+    if (nclasses > 0)
+        return;
+    PLASSERT( nclasses == 0 && index >= 0 );
+    real max = -1;
+    for (int i = 0; i < source->length(); i++) {
+        real val = source->get(i, index);
+        if (val > max)
+            max = val;
+    }
+    nclasses = int(round(max));
+    width_ += nclasses;
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/vmat/OneHotVMatrix.h
===================================================================
--- trunk/plearn/vmat/OneHotVMatrix.h	2008-09-11 14:46:39 UTC (rev 9455)
+++ trunk/plearn/vmat/OneHotVMatrix.h	2008-09-15 17:33:17 UTC (rev 9456)
@@ -50,32 +50,19 @@
 namespace PLearn {
 using namespace std;
 
-
-/*!
-  Sampling from this VMat will return the corresponding sample
-  from the source VMat with last element ('target_classnum')
-  replaced by a vector of target_values of size nclasses in which only
-  target_values[target_classnum] is set to hot_value, and all the
-  others are set to cold_value.
-  In the special case where the VMat is built with nclasses==1, then
-  it is assumed that we have a 2 class classification problem but
-  we are using a single valued target.  For this special case only
-  the_cold_value is used as target for classnum 0 and the_hot_value is
-  used for classnum 1.
-*/
-
 class OneHotVMatrix: public SourceVMatrix
 {
     typedef SourceVMatrix inherited;
 
-protected:
-//    VMat underlying_distr; // DEPRECATED - use 'source' instead
+public:
+
     int nclasses;
     real cold_value;
     real hot_value;
     int index;
 
 public:
+
     // ******************
     // *  Constructors  *
     // ******************
@@ -95,6 +82,12 @@
     virtual void getNewRow(int i, const Vec& samplevec) const;
     static void declareOptions(OptionList &ol);
 
+    //! If 'nclasses' is greater than zero, do nothing.
+    //! Otherwise, obtain 'nclasses' by finding the maximum value of the
+    //! source's 'index' column. Then update the width of this VMat
+    //! accordingly.
+    void updateNClassesAndWidth();
+
 public:
 
     virtual void build();



From tihocan at mail.berlios.de  Mon Sep 15 21:05:17 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 15 Sep 2008 21:05:17 +0200
Subject: [Plearn-commits] r9457 - trunk/plearn/vmat
Message-ID: <200809151905.m8FJ5HD3019474@sheep.berlios.de>

Author: tihocan
Date: 2008-09-15 21:05:16 +0200 (Mon, 15 Sep 2008)
New Revision: 9457

Modified:
   trunk/plearn/vmat/OneHotVMatrix.cc
Log:
Bug fix in automatic detection of nclasses

Modified: trunk/plearn/vmat/OneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/OneHotVMatrix.cc	2008-09-15 17:33:17 UTC (rev 9456)
+++ trunk/plearn/vmat/OneHotVMatrix.cc	2008-09-15 19:05:16 UTC (rev 9457)
@@ -244,7 +244,7 @@
         if (val > max)
             max = val;
     }
-    nclasses = int(round(max));
+    nclasses = int(round(max)) + 1;
     width_ += nclasses;
 }
 



From chrish at mail.berlios.de  Tue Sep 16 22:46:23 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 16 Sep 2008 22:46:23 +0200
Subject: [Plearn-commits] r9458 - trunk/python_modules/plearn/pymake
Message-ID: <200809162046.m8GKkNG7001998@sheep.berlios.de>

Author: chrish
Date: 2008-09-16 22:46:22 +0200 (Tue, 16 Sep 2008)
New Revision: 9458

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Adjust pymake distcc/hosts file parsing to take into account new lots-of-cores machines.

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-09-15 19:05:16 UTC (rev 9457)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-09-16 20:46:22 UTC (rev 9458)
@@ -627,7 +627,7 @@
     return options
 
 ###  Processing of the list of hosts used for compilation
-# (will soon be removed in favor of an external batch launcher)
+# (will soon be removed in favor of an external batch launcher)... NOT
 
 def _process_distcc_hosts(host_list):
     """Processes a distcc-style string describing hosts used for
@@ -638,9 +638,6 @@
     if host_list[-1] == '\n':
         host_list = host_list[:-1]
 
-    # Smallest '/num' found in the distcc file.
-    smallest_num = 10
-
     # Split into lines, filter comments.
     for l in host_list.split('\n'):
         # Filter hash comments
@@ -665,13 +662,11 @@
 
             # Add num times the host to the list of hosts for compilation.
             hosts.extend([host] * num)
-            smallest_num = min(smallest_num, num)
 
-    # For distcc, localhost is usually not added when the list of hosts is
-    # long because localhost is busy doing the preprocessing. This does not
-    # hold for pymake, so add localhost if it is not present.
-    if 'localhost' not in hosts and myhostname not in hosts:
-        hosts.extend(['localhost'] * smallest_num)
+    # For distcc, localhost is given a lesser load because it is busy doing
+    # the preprocessing. This does not hold for pymake, so add some more localhost
+    # back.
+    hosts.extend(['localhost'] * 2)
 
     return hosts
 



From nouiz at mail.berlios.de  Thu Sep 18 18:23:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Sep 2008 18:23:19 +0200
Subject: [Plearn-commits] r9459 - trunk/plearn_learners/regressors
Message-ID: <200809181623.m8IGNJ1Q021806@sheep.berlios.de>

Author: nouiz
Date: 2008-09-18 18:23:15 +0200 (Thu, 18 Sep 2008)
New Revision: 9459

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
   trunk/plearn_learners/regressors/RegressionTreeQueue.cc
   trunk/plearn_learners/regressors/RegressionTreeQueue.h
Log:
code refactoring and cleanup to remove call to tostring. We need this as we can't call tostring in parallel! This cause a bug in MultiClassAdaBoost when compiled with openmp.This don't fix the bug, but lower the change of it happening...


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-09-16 20:46:22 UTC (rev 9458)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-09-18 16:23:15 UTC (rev 9459)
@@ -295,13 +295,13 @@
     }
     //Set value common value of all leave
     // for optimisation, by default they aren't missing leave
-    leave_template->setOption("missing_leave", "0");
-    leave_template->setOption("loss_function_weight", tostring(loss_function_weight));
-    leave_template->setOption("verbosity", tostring(verbosity));
+    leave_template->missing_leave = 0;
+    leave_template->loss_function_weight = loss_function_weight;
+    leave_template->verbosity = verbosity;
     leave_template->initStats();
 
     first_leave = ::PLearn::deepCopy(leave_template);
-    first_leave->setOption("id", tostring(sorted_train_set->getNextId()));
+    first_leave->id=sorted_train_set->getNextId();
     first_leave->initLeave(sorted_train_set);
 
     for (int train_sample_index = 0; train_sample_index < length;
@@ -310,17 +310,13 @@
         first_leave->addRow(train_sample_index);
         first_leave->registerRow(train_sample_index);
     }
-    root = new RegressionTreeNode();
-    root->setOption("missing_is_valid", tostring(missing_is_valid));
-    root->setOption("loss_function_weight", tostring(loss_function_weight));
-    root->setOption("verbosity", tostring(verbosity));
+    root = new RegressionTreeNode(missing_is_valid,loss_function_weight,
+                                  verbosity);
     root->initNode(sorted_train_set, first_leave, leave_template);
     root->lookForBestSplit();
+
     if (maximum_number_of_nodes < nstages) maximum_number_of_nodes = nstages;
-    priority_queue = new RegressionTreeQueue();
-    priority_queue->setOption("verbosity", tostring(verbosity));
-    priority_queue->setOption("maximum_number_of_nodes", tostring(maximum_number_of_nodes));
-    priority_queue->initHeap();
+    priority_queue = new RegressionTreeQueue(verbosity,maximum_number_of_nodes);
     priority_queue->addHeap(root);
 }
 

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-09-16 20:46:22 UTC (rev 9458)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-09-18 16:23:15 UTC (rev 9459)
@@ -54,9 +54,9 @@
 int RegressionTreeLeave::verbosity = 0;
 
 RegressionTreeLeave::RegressionTreeLeave():
-    id(-1),
     missing_leave(false),
     loss_function_weight(0),
+    id(-1),
     length(0),
     weights_sum(0),
     targets_sum(0),

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-09-16 20:46:22 UTC (rev 9458)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-09-18 16:23:15 UTC (rev 9459)
@@ -52,9 +52,15 @@
     typedef Object inherited;
  
     friend class RegressionTreeNode;
+    friend class RegressionTree;
 
     Vec dummy_vec;
 
+public:
+    bool missing_leave;
+    real loss_function_weight;
+    static int  verbosity;
+
 protected:
 
 /*
@@ -62,9 +68,6 @@
 */
 
     int  id;
-    bool  missing_leave;
-    real loss_function_weight;
-    static int  verbosity;
     PP<RegressionTreeRegisters> train_set;
  
 /*

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-09-16 20:46:22 UTC (rev 9458)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-09-18 16:23:15 UTC (rev 9459)
@@ -65,6 +65,19 @@
 {
     build();
 }
+RegressionTreeNode::RegressionTreeNode(int missing_is_valid_,
+                                       real loss_function_weight_,
+                                       int verbosity_):
+    missing_is_valid(missing_is_valid_),
+    loss_function_weight(loss_function_weight_),
+    verbosity(verbosity_),
+    split_col(-1),
+    split_balance(INT_MAX),
+    split_feature_value(REAL_MAX),
+    after_split_error(REAL_MAX)
+{
+    build();
+}
 
 RegressionTreeNode::~RegressionTreeNode()
 {

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-09-16 20:46:22 UTC (rev 9458)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-09-18 16:23:15 UTC (rev 9459)
@@ -67,6 +67,7 @@
     int  missing_is_valid;
     real loss_function_weight;
     int verbosity; 
+
     PP<RegressionTreeLeave> leave_template; 
     PP<RegressionTreeRegisters> train_set;
     PP<RegressionTreeLeave> leave;
@@ -92,6 +93,8 @@
     Vec tmp_vec;
 public:  
     RegressionTreeNode();
+    RegressionTreeNode(int missing_is_valid, real loss_function_weight,
+                       int verbosity);
     virtual              ~RegressionTreeNode();
     
     PLEARN_DECLARE_OBJECT(RegressionTreeNode);

Modified: trunk/plearn_learners/regressors/RegressionTreeQueue.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeQueue.cc	2008-09-16 20:46:22 UTC (rev 9458)
+++ trunk/plearn_learners/regressors/RegressionTreeQueue.cc	2008-09-18 16:23:15 UTC (rev 9459)
@@ -57,7 +57,13 @@
 {
     build();
 }
-
+RegressionTreeQueue::RegressionTreeQueue(int verbosity_,
+                                         int maximum_number_of_nodes_)
+    : verbosity(verbosity_),
+      maximum_number_of_nodes(maximum_number_of_nodes_)
+{
+    build();
+}
 RegressionTreeQueue::~RegressionTreeQueue()
 {
 }
@@ -93,10 +99,6 @@
 
 void RegressionTreeQueue::build_()
 {
-}
-
-void RegressionTreeQueue::initHeap()
-{
     next_available_node = 0;
     nodes.resize(maximum_number_of_nodes);
 }

Modified: trunk/plearn_learners/regressors/RegressionTreeQueue.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeQueue.h	2008-09-16 20:46:22 UTC (rev 9458)
+++ trunk/plearn_learners/regressors/RegressionTreeQueue.h	2008-09-18 16:23:15 UTC (rev 9459)
@@ -71,13 +71,14 @@
 public:
   
     RegressionTreeQueue();
+    RegressionTreeQueue(int verbosity, int maximum_number_of_nodes);
+
     virtual              ~RegressionTreeQueue();
     PLEARN_DECLARE_OBJECT(RegressionTreeQueue);
 
     static  void         declareOptions(OptionList& ol);
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
-    void         initHeap();
     void         addHeap(PP<RegressionTreeNode> new_node);
     PP<RegressionTreeNode>   popHeap();
     PP<RegressionTreeNode>   upHeap(PP<RegressionTreeNode> new_node, int node_ind);



From nouiz at mail.berlios.de  Thu Sep 18 19:55:51 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Sep 2008 19:55:51 +0200
Subject: [Plearn-commits] r9460 - trunk/plearn/base
Message-ID: <200809181755.m8IHtpWR020080@sheep.berlios.de>

Author: nouiz
Date: 2008-09-18 19:55:51 +0200 (Thu, 18 Sep 2008)
New Revision: 9460

Modified:
   trunk/plearn/base/tostring.cc
   trunk/plearn/base/tostring.h
Log:
made tostring thread safe with openmp...


Modified: trunk/plearn/base/tostring.cc
===================================================================
--- trunk/plearn/base/tostring.cc	2008-09-18 16:23:15 UTC (rev 9459)
+++ trunk/plearn/base/tostring.cc	2008-09-18 17:55:51 UTC (rev 9460)
@@ -88,14 +88,20 @@
 
 string tostring(const double& x, PStream::mode_t io_formatting)
 {
-    PStream& out = _tostring_static_pstream_(true, io_formatting);
-    int ix = int(x);
-    if (io_formatting==PStream::raw_ascii && fast_exact_is_equal(ix, x))
-        out << ix;
-    else
-        out << x;
-    return static_cast<StringPStreamBuf*>(
-        (PStreamBuf*)_tostring_static_pstream_(false))->getString();
+    string str;
+#pragma omp critical (tostring)
+    {
+        PStream& out = _tostring_static_pstream_(true, io_formatting);
+        int ix = int(x);
+        if (io_formatting==PStream::raw_ascii && fast_exact_is_equal(ix, x))
+            out << ix;
+        else
+            out << x;
+        str = static_cast<StringPStreamBuf*>(
+            (PStreamBuf*)_tostring_static_pstream_(false))->getString();
+    }
+    return str;
+
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/base/tostring.h
===================================================================
--- trunk/plearn/base/tostring.h	2008-09-18 16:23:15 UTC (rev 9459)
+++ trunk/plearn/base/tostring.h	2008-09-18 17:55:51 UTC (rev 9460)
@@ -71,9 +71,14 @@
 string tostring(const T& x, 
                 PStream::mode_t io_formatting = PStream::raw_ascii)
 {
-    _tostring_static_pstream_(true, io_formatting) << x;
-    return static_cast<StringPStreamBuf*>(
-        (PStreamBuf*)_tostring_static_pstream_(false))->getString();
+    string str;
+#pragma omp critical (tostring)
+    {
+        _tostring_static_pstream_(true, io_formatting) << x;
+        str = static_cast<StringPStreamBuf*>(
+            (PStreamBuf*)_tostring_static_pstream_(false))->getString();
+    }
+    return str;
 }
 
 } // end of namespace PLearn



From nouiz at mail.berlios.de  Thu Sep 18 20:52:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Sep 2008 20:52:58 +0200
Subject: [Plearn-commits] r9461 - trunk/plearn/base
Message-ID: <200809181852.m8IIqwfM024840@sheep.berlios.de>

Author: nouiz
Date: 2008-09-18 20:52:58 +0200 (Thu, 18 Sep 2008)
New Revision: 9461

Modified:
   trunk/plearn/base/tostring.cc
   trunk/plearn/base/tostring.h
Log:
removed gcc warning when not compiling with openmp


Modified: trunk/plearn/base/tostring.cc
===================================================================
--- trunk/plearn/base/tostring.cc	2008-09-18 17:55:51 UTC (rev 9460)
+++ trunk/plearn/base/tostring.cc	2008-09-18 18:52:58 UTC (rev 9461)
@@ -89,7 +89,9 @@
 string tostring(const double& x, PStream::mode_t io_formatting)
 {
     string str;
+#ifdef OPENMP__
 #pragma omp critical (tostring)
+#endif
     {
         PStream& out = _tostring_static_pstream_(true, io_formatting);
         int ix = int(x);

Modified: trunk/plearn/base/tostring.h
===================================================================
--- trunk/plearn/base/tostring.h	2008-09-18 17:55:51 UTC (rev 9460)
+++ trunk/plearn/base/tostring.h	2008-09-18 18:52:58 UTC (rev 9461)
@@ -72,7 +72,9 @@
                 PStream::mode_t io_formatting = PStream::raw_ascii)
 {
     string str;
+#ifdef OPENMP__
 #pragma omp critical (tostring)
+#endif
     {
         _tostring_static_pstream_(true, io_formatting) << x;
         str = static_cast<StringPStreamBuf*>(



From nouiz at mail.berlios.de  Fri Sep 19 16:45:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Sep 2008 16:45:54 +0200
Subject: [Plearn-commits] r9462 - trunk/python_modules/plearn/learners
Message-ID: <200809191445.m8JEjsUt032048@sheep.berlios.de>

Author: nouiz
Date: 2008-09-19 16:45:54 +0200 (Fri, 19 Sep 2008)
New Revision: 9462

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
bugfix when reloading the learner


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-09-18 18:52:58 UTC (rev 9461)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-09-19 14:45:54 UTC (rev 9462)
@@ -328,6 +328,8 @@
         self.learner2.setTrainStatsCollector(VecStatsCollector())
         self.multi_class_adaboost.learner1=self.learner1
         self.multi_class_adaboost.learner2=self.learner2
+        self.multi_class_adaboost.build()
+        
         for (learner,trainSet) in [(self.learner1,trainSet1), (self.learner2,trainSet2)]:
             for weak in learner.weak_learners:
                 weak.setTrainingSet(trainSet,False)



From nouiz at mail.berlios.de  Fri Sep 19 16:46:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Sep 2008 16:46:39 +0200
Subject: [Plearn-commits] r9463 - trunk/plearn_learners/meta
Message-ID: <200809191446.m8JEkdSe032102@sheep.berlios.de>

Author: nouiz
Date: 2008-09-19 16:46:39 +0200 (Fri, 19 Sep 2008)
New Revision: 9463

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
removed warning when not compiling with openmp and added check when compiling with openmp.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-09-19 14:45:54 UTC (rev 9462)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-09-19 14:46:39 UTC (rev 9463)
@@ -175,6 +175,18 @@
 
 //if you use the parallel version, you must disable all verbose, verbosity and report progress int he learner1 and learner2.
 //Otherwise this will cause crash due to the parallel printing to stdout stderr.
+#ifdef OPENMP__
+    //the AdaBoost and the weak learner should not print anything as this will cause race condition on the printing
+    PLCHECK(learner1->verbosity==0);
+    PLCHECK(learner2->verbosity==0);
+    PLCHECK(learner1->report_progress==false);
+    PLCHECK(learner2->report_progress==false);
+
+    PLCHECK(learner1->weak_learner_template->verbosity==0);
+    PLCHECK(learner2->weak_learner_template->verbosity==0);
+    PLCHECK(learner1->weak_learner_template->report_progress==false);
+    PLCHECK(learner2->weak_learner_template->report_progress==false);
+
 #pragma omp parallel sections
 {
 #pragma omp section 
@@ -182,6 +194,10 @@
 #pragma omp section 
     learner2->train();
 }
+#else
+    learner1->train();
+    learner2->train();
+#endif
 
     stage=max(learner1->stage,learner2->stage);
 
@@ -198,6 +214,8 @@
 void MultiClassAdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
     PLASSERT(output.size()==outputsize());
+    PLASSERT(output1.size()==learner1->outputsize());
+    PLASSERT(output2.size()==learner2->outputsize());
 
     learner1->computeOutput(input, output1);
     learner2->computeOutput(input, output2);



From nouiz at mail.berlios.de  Fri Sep 19 17:42:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Sep 2008 17:42:00 +0200
Subject: [Plearn-commits] r9464 - trunk
Message-ID: <200809191542.m8JFg0MR004458@sheep.berlios.de>

Author: nouiz
Date: 2008-09-19 17:42:00 +0200 (Fri, 19 Sep 2008)
New Revision: 9464

Modified:
   trunk/pymake.config.model
Log:
changed the openmp parameter to be independed of opt and dbg


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-09-19 14:46:39 UTC (rev 9463)
+++ trunk/pymake.config.model	2008-09-19 15:42:00 UTC (rev 9464)
@@ -257,7 +257,7 @@
     'purify', 'quantify', 'vc++', 'condor' ],
   
   [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbggprof', 'safegprof',
-    'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg', 'vecgcc', 'openmpgcc' ],
+    'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg', 'vecgcc' ],
   
   [ 'double', 'float' ],
   
@@ -277,6 +277,8 @@
   [ '', 'Wno-uninitialized' ],
   [ '', 'march=native'],
   [ '', 'cygwin-fgets-bugfix'],
+  [ '', 'openmpgcc'],
+  
 ]
 
 ### Using Python code snippets in C++ code
@@ -668,7 +670,7 @@
 
 pymakeOption( name = 'openmpgcc',
               description = 'vectorized with gcc compiler in opt mode',
-              compileroptions = '-Wall -O3 -fopenmp -msse2',
+              compileroptions = '-fopenmp',
               linkeroptions = '-fopenmp',
               cpp_definitions = ['NDEBUG'] )
 



From nouiz at mail.berlios.de  Fri Sep 19 18:01:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Sep 2008 18:01:29 +0200
Subject: [Plearn-commits] r9465 - trunk/plearn/base
Message-ID: <200809191601.m8JG1TeB006557@sheep.berlios.de>

Author: nouiz
Date: 2008-09-19 18:01:29 +0200 (Fri, 19 Sep 2008)
New Revision: 9465

Modified:
   trunk/plearn/base/tostring.cc
   trunk/plearn/base/tostring.h
Log:
typo in macro name


Modified: trunk/plearn/base/tostring.cc
===================================================================
--- trunk/plearn/base/tostring.cc	2008-09-19 15:42:00 UTC (rev 9464)
+++ trunk/plearn/base/tostring.cc	2008-09-19 16:01:29 UTC (rev 9465)
@@ -89,7 +89,7 @@
 string tostring(const double& x, PStream::mode_t io_formatting)
 {
     string str;
-#ifdef OPENMP__
+#ifdef _OPENMP
 #pragma omp critical (tostring)
 #endif
     {

Modified: trunk/plearn/base/tostring.h
===================================================================
--- trunk/plearn/base/tostring.h	2008-09-19 15:42:00 UTC (rev 9464)
+++ trunk/plearn/base/tostring.h	2008-09-19 16:01:29 UTC (rev 9465)
@@ -72,7 +72,7 @@
                 PStream::mode_t io_formatting = PStream::raw_ascii)
 {
     string str;
-#ifdef OPENMP__
+#ifdef _OPENMP
 #pragma omp critical (tostring)
 #endif
     {



From nouiz at mail.berlios.de  Fri Sep 19 19:20:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Sep 2008 19:20:30 +0200
Subject: [Plearn-commits] r9466 - trunk/plearn_learners/meta
Message-ID: <200809191720.m8JHKUH9002354@sheep.berlios.de>

Author: nouiz
Date: 2008-09-19 19:20:29 +0200 (Fri, 19 Sep 2008)
New Revision: 9466

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
fixed typo and added the test() fct that call the parent test() fct for now. It will be used later to save partial result.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-09-19 16:01:29 UTC (rev 9465)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-09-19 17:20:29 UTC (rev 9466)
@@ -175,7 +175,7 @@
 
 //if you use the parallel version, you must disable all verbose, verbosity and report progress int he learner1 and learner2.
 //Otherwise this will cause crash due to the parallel printing to stdout stderr.
-#ifdef OPENMP__
+#ifdef _OPENMP
     //the AdaBoost and the weak learner should not print anything as this will cause race condition on the printing
     PLCHECK(learner1->verbosity==0);
     PLCHECK(learner2->verbosity==0);
@@ -423,6 +423,15 @@
         learner2->setTrainingSet(vmat2, call_forget);
 }
 
+void MultiClassAdaBoost::test(VMat testset, PP<VecStatsCollector> test_stats,
+                              VMat testoutputs, VMat testcosts) const
+{
+    Profiler::pl_profile_start("MultiClassAdaBoost::test");
+    inherited::test(testset,test_stats,testoutputs,testcosts);
+
+    Profiler::pl_profile_end("MultiClassAdaBoost::test");
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-09-19 16:01:29 UTC (rev 9465)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-09-19 17:20:29 UTC (rev 9466)
@@ -135,8 +135,8 @@
                                        Vec& output, Vec& costs) const;
     // virtual void computeCostsOnly(const Vec& input, const Vec& target,
     //                               Vec& costs) const;
-    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
-    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+                      VMat testoutputs=0, VMat testcosts=0) const;
     // virtual int nTestCosts() const;
     // virtual int nTrainCosts() const;
     // virtual void resetInternalState();



From nouiz at mail.berlios.de  Fri Sep 19 20:11:46 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Sep 2008 20:11:46 +0200
Subject: [Plearn-commits] r9467 - trunk/plearn_learners/meta
Message-ID: <200809191811.m8JIBkgJ010532@sheep.berlios.de>

Author: nouiz
Date: 2008-09-19 20:11:46 +0200 (Fri, 19 Sep 2008)
New Revision: 9467

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
fixed typo that saved for no reason the trainset


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-09-19 17:20:29 UTC (rev 9466)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-09-19 18:11:46 UTC (rev 9467)
@@ -220,7 +220,7 @@
 
     declareOption(ol, "train_set",
                   &AdaBoost::train_set,
-                  OptionBase::learntoption||OptionBase::nosave,
+                  OptionBase::learntoption|OptionBase::nosave,
                   "The training set, so we can reload it.\n");
 
 }



From nouiz at mail.berlios.de  Fri Sep 19 21:50:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Sep 2008 21:50:34 +0200
Subject: [Plearn-commits] r9468 - trunk/python_modules/plearn/learners
Message-ID: <200809191950.m8JJoYMx020333@sheep.berlios.de>

Author: nouiz
Date: 2008-09-19 21:50:33 +0200 (Fri, 19 Sep 2008)
New Revision: 9468

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
bugfix


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-09-19 18:11:46 UTC (rev 9467)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-09-19 19:50:33 UTC (rev 9468)
@@ -65,7 +65,7 @@
         tmp.setFieldNames(l.getTrainCostNames())
         l.setTrainStatsCollector(tmp)
         l.report_progress = self.report_progress
-        l.verbose = self.verbose 
+        l.verbosity = self.verbose 
         l.build()
         return l
 



From nouiz at mail.berlios.de  Mon Sep 22 16:00:17 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 22 Sep 2008 16:00:17 +0200
Subject: [Plearn-commits] r9469 - trunk/python_modules/plearn/parallel
Message-ID: <200809221400.m8ME0H5e028079@sheep.berlios.de>

Author: nouiz
Date: 2008-09-22 16:00:10 +0200 (Mon, 22 Sep 2008)
New Revision: 9469

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
code refactoring


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-19 19:50:33 UTC (rev 9468)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-22 14:00:10 UTC (rev 9469)
@@ -701,7 +701,11 @@
         self.base_tasks_log_file = []
         self.set_special_env = True
         self.nb_proc = 0 # 0 mean unlimited
+        self.source_file = ''
+        self.source_file = os.getenv("CONDOR_LOCAL_SOURCE")
+        self.condor_home = os.getenv('CONDOR_HOME')
 
+
         DBIBase.__init__(self, commands, **args)
         self.mem=int(self.mem)*1024
         if not os.path.exists(self.log_dir):
@@ -812,14 +816,88 @@
                                    self.args))
             id+=1
             #keeps a list of the temporary files created, so that they can be deleted at will
+
+    def make_wrapper_script(self,launch_file, bash_exec):
+        dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
+        overwrite_launch_file=False
+        if not os.path.exists(dbi_file):
+            print '[DBI] WARNING: Can\'t locate file "dbi.py". Maybe the file "'+launch_file+'" is not up to date!'
+        else:
+            if os.path.exists(launch_file):
+                mtimed=os.stat(dbi_file)[8]
+                mtimel=os.stat(launch_file)[8]
+                if mtimed>mtimel:
+                    print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your needs!'
+                    overwrite_launch_file=True
+
+        if self.copy_local_source_file:
+            source_file_dest = os.path.join(self.log_dir,
+                                            os.path.basename(self.source_file))
+            shutil.copy( self.source_file, source_file_dest)
+            self.temp_files.append(source_file_dest)
+            os.chmod(source_file_dest, 0755)
+            self.source_file=source_file_dest
+
+        if not os.path.exists(launch_file) or overwrite_launch_file:
+            self.temp_files.append(launch_file)
+            launch_dat = open(launch_file,'w')
+            if self.source_file and not self.source_file.endswith(".cshrc"):
+                launch_dat.write(dedent('''\
+                    #!/bin/sh
+                    '''))
+                if self.condor_home:
+                    launch_dat.write('export HOME=%s\n' % self.condor_home)
+                if self.source_file:
+                    launch_dat.write('source ' + self.source_file + '\n')
+
+                launch_dat.write(dedent('''\
+                    echo "Executing on " `/bin/hostname` 1>&2
+                    echo "HOSTNAME: ${HOSTNAME}" 1>&2
+                    echo "PATH: $PATH" 1>&2
+                    echo "PYTHONPATH: $PYTHONPATH" 1>&2
+                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
+                    echo "OMP_NUM_THREADS: $OMP_NUM_THREADS" 1>&2
+                    #which python 1>&2
+                    #echo -n python version: 1>&2
+                    #python -V 1>&2
+                    #echo -n /usr/bin/python version: 1>&2
+                    #/usr/bin/python -V 1>&2
+                    echo "Running: command: \\"$@\\"" 1>&2
+                    %s
+                    '''%(bash_exec)))
+            else:
+                launch_dat.write(dedent('''\
+                    #! /bin/tcsh
+                    \n'''))
+                if self.condor_home:
+                    launch_dat.write('setenv HOME %s\n' % self.condor_home)
+                if self.source_file:
+                    launch_dat.write('source ' + self.source_file + '\n')
+                launch_dat.write(dedent('''\
+                    echo "Executing on " `/bin/hostname`
+                    echo "HOSTNAME: ${HOSTNAME}"
+                    echo "PATH: $PATH"
+                    echo "PYTHONPATH: $PYTHONPATH"
+                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
+                    #which python
+                    #echo -n python version:
+                    #python -V
+                    #echo -n /usr/bin/python version:
+                    #/usr/bin/python -V
+                    #echo ${PROGRAM} $@
+                    #${PROGRAM} "$@"
+                    echo "Running command: $argv"
+                    $argv
+                    '''))
+            launch_dat.close()
+            os.chmod(launch_file, 0755)
+
     def run_dag(self):
         condor_file = os.path.join(self.log_dir, "dag.condor")
         self.temp_files.append(condor_file)
         condor_dat = open( condor_file, 'w' )
 
-        source_file=os.getenv("CONDOR_LOCAL_SOURCE")
-        condor_home = os.getenv('CONDOR_HOME')
-        if source_file and source_file.endswith(".cshrc"):
+        if self.source_file and self.source_file.endswith(".cshrc"):
             launch_file = os.path.join(self.log_dir, 'launch.csh')
         else:
             launch_file = os.path.join(self.log_dir, 'launch.sh')
@@ -902,80 +980,8 @@
                 
         condor_dag.close()
 
-        dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
-        overwrite_launch_file=False
-        if not os.path.exists(dbi_file):
-            print '[DBI] WARNING: Can\'t locate file "dbi.py". Maybe the file "'+launch_file+'" is not up to date!'
-        else:
-            if os.path.exists(launch_file):
-                mtimed=os.stat(dbi_file)[8]
-                mtimel=os.stat(launch_file)[8]
-                if mtimed>mtimel:
-                    print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your needs!'
-                    overwrite_launch_file=True
+        self.make_wrapper_script(launch_file, '$@')
 
-        if self.copy_local_source_file:
-            source_file_dest = os.path.join(self.log_dir,
-                                            os.path.basename(source_file))
-            shutil.copy( source_file, source_file_dest)
-            self.temp_files.append(source_file_dest)
-            os.chmod(source_file_dest, 0755)
-            source_file=source_file_dest
-
-        if not os.path.exists(launch_file) or overwrite_launch_file:
-            self.temp_files.append(launch_file)
-            launch_dat = open(launch_file,'w')
-            if source_file and not source_file.endswith(".cshrc"):
-                launch_dat.write(dedent('''\
-                    #!/bin/sh
-                    '''))
-                if condor_home:
-                    launch_dat.write('export HOME=%s\n' % condor_home)
-                if source_file:
-                    launch_dat.write('source ' + source_file + '\n')
-
-                launch_dat.write(dedent('''\
-                    echo "Executing on " `/bin/hostname` 1>&2
-                    echo "HOSTNAME: ${HOSTNAME}" 1>&2
-                    echo "PATH: $PATH" 1>&2
-                    echo "PYTHONPATH: $PYTHONPATH" 1>&2
-                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
-                    echo "OMP_NUM_THREADS: $OMP_NUM_THREADS" 1>&2
-                    #which python 1>&2
-                    #echo -n python version: 1>&2
-                    #python -V 1>&2
-                    #echo -n /usr/bin/python version: 1>&2
-                    #/usr/bin/python -V 1>&2
-                    echo "Running: command: \\"$@\\"" 1>&2
-                    $@
-                    '''))
-            else:
-                launch_dat.write(dedent('''\
-                    #! /bin/tcsh
-                    \n'''))
-                if condor_home:
-                    launch_dat.write('setenv HOME %s\n' % condor_home)
-                if source_file:
-                    launch_dat.write('source ' + source_file + '\n')
-                launch_dat.write(dedent('''\
-                    echo "Executing on " `/bin/hostname`
-                    echo "HOSTNAME: ${HOSTNAME}"
-                    echo "PATH: $PATH"
-                    echo "PYTHONPATH: $PYTHONPATH"
-                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
-                    #which python
-                    #echo -n python version:
-                    #python -V
-                    #echo -n /usr/bin/python version:
-                    #/usr/bin/python -V
-                    #echo ${PROGRAM} $@
-                    #${PROGRAM} "$@"
-                    echo "Running command: $argv"
-                    $argv
-                    '''))
-            launch_dat.close()
-            os.chmod(launch_file, 0755)
-
         condor_cmd = 'condor_submit_dag -maxjobs %s %s'%(str(self.nb_proc), condor_file_dag)
         return condor_cmd
 
@@ -1002,9 +1008,7 @@
         self.temp_files.append(condor_file)
         condor_dat = open( condor_file, 'w' )
 
-        source_file=os.getenv("CONDOR_LOCAL_SOURCE")
-        condor_home = os.getenv('CONDOR_HOME')
-        if source_file and source_file.endswith(".cshrc"):
+        if self.source_file and self.source_file.endswith(".cshrc"):
             launch_file = os.path.join(self.log_dir, 'launch.csh')
         else:
             launch_file = os.path.join(self.log_dir, 'launch.sh')
@@ -1074,81 +1078,9 @@
                     condor_dat.write("arguments      = %s \nqueue\n" %argstring)
         condor_dat.close()
 
-        dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
-        overwrite_launch_file=False
-        if not os.path.exists(dbi_file):
-            print '[DBI] WARNING: Can\'t locate file "dbi.py". Maybe the file "'+launch_file+'" is not up to date!'
-        else:
-            if os.path.exists(launch_file):
-                mtimed=os.stat(dbi_file)[8]
-                mtimel=os.stat(launch_file)[8]
-                if mtimed>mtimel:
-                    print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your needs!'
-                    overwrite_launch_file=True
 
-        if self.copy_local_source_file:
-            source_file_dest = os.path.join(self.log_dir,
-                                            os.path.basename(source_file))
-            shutil.copy( source_file, source_file_dest)
-            self.temp_files.append(source_file_dest)
-            os.chmod(source_file_dest, 0755)
-            source_file=source_file_dest
+        self.make_wrapper_script(launch_file,'sh -c "$@"')
 
-        if not os.path.exists(launch_file) or overwrite_launch_file:
-            self.temp_files.append(launch_file)
-            launch_dat = open(launch_file,'w')
-            if source_file and not source_file.endswith(".cshrc"):
-                launch_dat.write(dedent('''\
-                    #!/bin/sh
-                    '''))
-                if condor_home:
-                    launch_dat.write('export HOME=%s\n' % condor_home)
-                if source_file:
-                    launch_dat.write('source ' + source_file + '\n')
-
-                #the sh -c "$@" was done to allow running many small jobs in a macro jobs like: "ls ;env"
-                launch_dat.write(dedent('''\
-                    echo "Executing on " `/bin/hostname` 1>&2
-                    echo "HOSTNAME: ${HOSTNAME}" 1>&2
-                    echo "PATH: $PATH" 1>&2
-                    echo "PYTHONPATH: $PYTHONPATH" 1>&2
-                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
-                    echo "OMP_NUM_THREADS: $OMP_NUM_THREADS" 1>&2
-                    #which python 1>&2
-                    #echo -n python version: 1>&2
-                    #python -V 1>&2
-                    #echo -n /usr/bin/python version: 1>&2
-                    #/usr/bin/python -V 1>&2
-                    echo "Running command: sh -c \\"$@\\"" 1>&2
-                    sh -c "$@"
-                    '''))
-            else:
-                launch_dat.write(dedent('''\
-                    #! /bin/tcsh
-                    \n'''))
-                if condor_home:
-                    launch_dat.write('setenv HOME %s\n' % condor_home)
-                if source_file:
-                    launch_dat.write('source ' + source_file + '\n')
-                launch_dat.write(dedent('''\
-                    echo "Executing on " `/bin/hostname`
-                    echo "HOSTNAME: ${HOSTNAME}"
-                    echo "PATH: $PATH"
-                    echo "PYTHONPATH: $PYTHONPATH"
-                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
-                    #which python
-                    #echo -n python version:
-                    #python -V
-                    #echo -n /usr/bin/python version:
-                    #/usr/bin/python -V
-                    #echo ${PROGRAM} $@
-                    #${PROGRAM} "$@"
-                    echo "Running command: $argv"
-                    $argv
-                    '''))
-            launch_dat.close()
-            os.chmod(launch_file, 0755)
-
         return "condor_submit " + condor_file
 
     def clean(self):



From tihocan at mail.berlios.de  Mon Sep 22 18:00:57 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 22 Sep 2008 18:00:57 +0200
Subject: [Plearn-commits] r9470 - trunk/plearn/vmat
Message-ID: <200809221600.m8MG0vgA010483@sheep.berlios.de>

Author: tihocan
Date: 2008-09-22 18:00:56 +0200 (Mon, 22 Sep 2008)
New Revision: 9470

Modified:
   trunk/plearn/vmat/SelectRowsVMatrix.cc
Log:
Fixed lines too long and typo in warning

Modified: trunk/plearn/vmat/SelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-09-22 14:00:10 UTC (rev 9469)
+++ trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-09-22 16:00:56 UTC (rev 9470)
@@ -210,8 +210,10 @@
         selected_indices << indices;
     }
     //we don't display the warning for SortRowsVMatrix as it always select all row!
-    if(warn_if_all_rows_selected && selected_indices.length()==source.length() && source.length()>0)
-        PLWARNING("In SelectRowsVMatrix::build_() - We select all row!");
+    if(warn_if_all_rows_selected && selected_indices.length()==source.length()
+            && source.length()>0)
+        PLWARNING("In SelectRowsVMatrix::build_() - "
+                  "All rows have been selected!");
 
     length_ = selected_indices.length();
     if (source) {



From nouiz at mail.berlios.de  Mon Sep 22 20:04:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 22 Sep 2008 20:04:23 +0200
Subject: [Plearn-commits] r9471 - trunk/scripts
Message-ID: <200809221804.m8MI4N1J019343@sheep.berlios.de>

Author: nouiz
Date: 2008-09-22 20:04:23 +0200 (Mon, 22 Sep 2008)
New Revision: 9471

Modified:
   trunk/scripts/collectres
Log:
reprint the spec in the outputfile after the list of file. When their is many file, it make it more readable.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2008-09-22 16:00:56 UTC (rev 9470)
+++ trunk/scripts/collectres	2008-09-22 18:04:23 UTC (rev 9471)
@@ -254,6 +254,7 @@
     for file in filenames:
       f.write(file+" ")
   f.write("\n")
+  f.write("# "+speclist + "\n")
   outputres(f,mode,specs[1:],getres(specs[1:],filenames))
   f.flush()
   f.close()



From tihocan at mail.berlios.de  Mon Sep 22 20:14:25 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 22 Sep 2008 20:14:25 +0200
Subject: [Plearn-commits] r9472 -
	trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results
Message-ID: <200809221814.m8MIEPuj022180@sheep.berlios.de>

Author: tihocan
Date: 2008-09-22 20:14:25 +0200 (Mon, 22 Sep 2008)
New Revision: 9472

Modified:
   trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results/RUN.log
Log:
Fixed test PL_SelectRowsVMatrix that I just broke

Modified: trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results/RUN.log
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results/RUN.log	2008-09-22 18:04:23 UTC (rev 9471)
+++ trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results/RUN.log	2008-09-22 18:14:25 UTC (rev 9472)
@@ -1,4 +1,4 @@
- WARNING: In SelectRowsVMatrix::build_() - We select all row!
+ WARNING: In SelectRowsVMatrix::build_() - All rows have been selected!
 2 x 3
 inputsize: 1
 targetsize: 0



From tihocan at mail.berlios.de  Mon Sep 22 20:51:15 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 22 Sep 2008 20:51:15 +0200
Subject: [Plearn-commits] r9473 - trunk/plearn/vmat
Message-ID: <200809221851.m8MIpFDt027693@sheep.berlios.de>

Author: tihocan
Date: 2008-09-22 20:51:15 +0200 (Mon, 22 Sep 2008)
New Revision: 9473

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
Log:
Fixed potential issue with ReplicateSamplesVMatrix when using missing targets: calling is_missing on an integer is not a good idea

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-09-22 18:14:25 UTC (rev 9472)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-09-22 18:51:15 UTC (rev 9473)
@@ -50,9 +50,12 @@
     "the first 'n-n_j' samples of class j (having n_j samples) will be\n"
     "replicated so that each class also has n samples. If required, samples\n"
     "will be replicated more than once.\n"
-    "The class index is assumed to be the first element of the target. When\n"
-    "the 'operate_on_bags' option is set to true, the bag information must\n"
-    "be stored in the last element of the target.\n"
+    "The class index is assumed to be the first element of the target. It\n"
+    "can be either an integer (negative class indices are also allowed) or\n"
+    "a missing value (all examples with missing values are considered as\n"
+    "belonging to the same class).\n"
+    "When the 'operate_on_bags' option is set to true, the bag information\n"
+    "must be stored in the last element of the target.\n"
     "All samples are also shuffled so as to mix classes together."
 );
 
@@ -138,33 +141,38 @@
     Vec input, target;
     real weight;
     TVec< TVec<int>  > class_indices;  // Indices of samples in each class.
-    TVec<int> nan_indices(0); // Indices of missing class
+    TVec<int> nan_indices; // Indices of missing class
     TVec< TVec<int> > negativeclass_indices;
     map<int, int> bag_sizes; // Map a source index to the size of its bag.
     int bag_start_idx = -1;
     int bag_idx = bag_index >= 0 ? bag_index : source->targetsize() - 1;
     for (int i = 0; i < source->length(); i++) {
         source->getExample(i, input, target, weight);
-        int c = int(round(target[0]));
-        if (c >= class_indices.length()) {
-            int n_to_add = c - class_indices.length() + 1;
-            for (int j = 0; j < n_to_add; j++)
-                class_indices.append(TVec<int>());
+        real c_real = target[0];
+        int c = int(round(c_real));
+        if (!is_missing(c_real)) {
+            if (c >= class_indices.length()) {
+                int n_to_add = c - class_indices.length() + 1;
+                for (int j = 0; j < n_to_add; j++)
+                    class_indices.append(TVec<int>());
+            }
+            else if ( -c >= negativeclass_indices.length() ) {
+                int n_to_add = -c - negativeclass_indices.length() + 1;
+                for (int j = 0; j < n_to_add; j++)
+                    negativeclass_indices.append(TVec<int>());
+            }
         }
-        else if ( -c >= negativeclass_indices.length() ) {
-            int n_to_add = -c - negativeclass_indices.length() + 1;
-            for (int j = 0; j < n_to_add; j++)
-                negativeclass_indices.append(TVec<int>());
-        }
         
         if (!operate_on_bags || int(round(target[bag_idx])) &
                                 SumOverBagsVariable::TARGET_COLUMN_FIRST) {
-            if( c>= 0 )
+            if( is_missing(c_real) )
+                nan_indices.append(i);
+            else if( c>= 0 )
                 class_indices[c].append(i);
             else if( c< 0 )
                 negativeclass_indices[-c].append(i);
-            else if( is_missing(c) )
-                nan_indices.append(i);
+            else
+                PLERROR("In ReplicateSamplesVMatrix::build_ - Invalid class");
             indices.append(i);
             bag_sizes[i] = 0;
             bag_start_idx = i;



From nouiz at mail.berlios.de  Tue Sep 23 19:32:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 23 Sep 2008 19:32:13 +0200
Subject: [Plearn-commits] r9474 - trunk/scripts
Message-ID: <200809231732.m8NHWDMG009032@sheep.berlios.de>

Author: nouiz
Date: 2008-09-23 19:32:12 +0200 (Tue, 23 Sep 2008)
New Revision: 9474

Modified:
   trunk/scripts/collectres
Log:
added option --verbose and --separator, renomed one option to --no_printcommand and added their documentation in the help.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2008-09-22 18:51:15 UTC (rev 9473)
+++ trunk/scripts/collectres	2008-09-23 17:32:12 UTC (rev 9474)
@@ -65,7 +65,7 @@
     index=int(colspec)
   return index
 
-def selectres(loc_specs,a,fieldnames):
+def selectres(loc_specs,a,fieldnames,verbose=1):
   res = []
   loc_mode = loc_specs[0]
   if loc_mode=="pos":
@@ -84,7 +84,8 @@
   elif loc_mode=="mincol":
     mcol = get_col_index(fieldnames,loc_specs[1])
     mrow = argmin(a[:,mcol])
-    print "found min row = ",mrow," for col ",mcol,", with value=",a[mrow,mcol]
+    if verbose>0:
+      print "found min row = ",mrow," for col ",mcol,", with value=",a[mrow,mcol]
     i=1
     while len(loc_specs[i:])>0:
       if loc_specs[i]=="all":
@@ -124,12 +125,12 @@
     raise ValueError("Invalid <location-spec> mode, expected 'pos', 'mincol', or 'col', got "+loc_mode)    
   return res
 
-def getres(loc_specs,filenames):
+def getres(loc_specs,filenames,verbose=1):
   all_results = []
   for filename in filenames:
     try:
       file_res = selectres(loc_specs,
-                           *smartReadMat(filename))
+                           *smartReadMat(filename), **{'verbose':verbose})
       all_results.append([file_res,filename])
     except ValueError,v:
       print >>sys.stderr, "caught ValueError exception in", filename
@@ -245,7 +246,7 @@
   else:
     raise ValueError("Invalid <spec> mode, expected 'min', 'sort', or 'plot', got "+mode)
 
-def collectres(outputfile,speclist,filenames,printcommand=True):
+def collectres(outputfile,speclist,filenames,printcommand=True,verbose=1):
   specs= string.split(speclist)
   mode = specs[0]
   f=open(outputfile,"w")
@@ -255,14 +256,14 @@
       f.write(file+" ")
   f.write("\n")
   f.write("# "+speclist + "\n")
-  outputres(f,mode,specs[1:],getres(specs[1:],filenames))
+  outputres(f,mode,specs[1:],getres(specs[1:],filenames,verbose))
   f.flush()
   f.close()
   
 if __name__=='__main__':
   args = sys.argv[:]
   if len(args)<=3:
-    print "Usage: collectres <outputfile> <spec> <file1.pmat> <file2.pmat> ..."
+    print "Usage: collectres [--no_printcommand] [--verbose={0,1}] [--separator=X] <outputfile> <spec> <file1.pmat> <file2.pmat> ..."
     print 
     print "File formats pmat, amat and csv are supported."
     print "The <spec> can be the following (note how the <spec> has to be surrounded by quotes):"
@@ -289,7 +290,14 @@
     print '                          keeping track at the same time of the <coli> values at the min-selected row.'
     sys.exit(1)
   printcommand=True
-  if args[1]=="--printcommand":
+  verbose=1
+  if args[1]=="--no_printcommand":
     printcommand=False
     del args[1]
-  collectres(args[1],args[2],args[3:],printcommand)
+  if args[1].startswith("--verbose="):
+    verbose=int(args[1].split("=",1)[1])
+    del args[1]
+  if args[1].startswith("--separator="):
+    separator=args[1].split("=",1)[1]
+    del args[1]
+  collectres(args[1],args[2],args[3:],printcommand,verbose)



From ducharme at mail.berlios.de  Tue Sep 23 22:57:38 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 23 Sep 2008 22:57:38 +0200
Subject: [Plearn-commits] r9475 - trunk/python_modules/plearn/pymake
Message-ID: <200809232057.m8NKvcbL015376@sheep.berlios.de>

Author: ducharme
Date: 2008-09-23 22:57:36 +0200 (Tue, 23 Sep 2008)
New Revision: 9475

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
New option "-l32_64" which adds "-32" (32-bit machine) or "-64" (64-bit machine) at the end of .so symbolic link files.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-09-23 17:32:12 UTC (rev 9474)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-09-23 20:57:36 UTC (rev 9475)
@@ -109,6 +109,8 @@
                  N.B. you can set the local_ofiles_base_path global
                  variable in your config file to use another path
                  than /tmp/.pymake/local_ofiles/.
+  -l32_64: when compiling with option '-pyso', add '-32' or '64' at the end
+           of the '.so' symbolic link (.so -> .so-32 or .so-64)
   -ssh: run compilation commands on remote hosts with ssh instead of rsh
         (default).
   -symlinkobjs: at link time, will create links to the used object files
@@ -1113,7 +1115,14 @@
                 link_exit_code = new_link_exit_code
             if create_so or create_pyso:
                 so_filename = os.path.basename(ccfile.corresponding_output)
-                ccfile.make_symbolic_link(so_filename, so_filename)
+                link_so_filename = so_filename
+                # Add machine-dependent info ('-32' or '-64') if necessary
+                if create_pyso and link_32_64:
+                    if platform.startswith('linux-x86_64') or platform.startswith('linux-ia64'):
+                        link_so_filename += '-64'
+                    else:
+                        link_so_filename += '-32'
+                ccfile.make_symbolic_link(link_so_filename, so_filename)
             else:
                 ccfile.make_symbolic_link(linkname)
     return link_exit_code
@@ -1975,7 +1984,13 @@
             if create_so:
                 linkbase = 'lib%s.so' % linkbase
             elif create_pyso:
-                linkbase = '%s.so' % linkbase
+                suffix = ''
+                if link_32_64:
+                    if platform.startswith('linux-x86_64') or platform.startswith('linux-ia64'):
+                        suffix = '-64'
+                    else:
+                        suffix = '-32'
+                linkbase = '%s.so%s' % (linkbase, suffix)
             symlink_from = join(self.filedir, linkbase)
         else:
             symlink_from = linkname
@@ -2500,7 +2515,7 @@
     # Variables that can be useful to have read access to in the config file
     global optionargs, otherargs, linkname, link_target_override, \
             create_dll, relocatable_dll, no_cygwin, force_32bits, create_so, \
-            create_pyso, \
+            create_pyso, link_32_64, \
             static_linking, force_recompilation, force_link, \
             local_compilation, symlinkobjs, temp_objs, distribute, vcproj, \
             local_ofiles, local_ofiles_base_path
@@ -2659,6 +2674,13 @@
     else:
         create_pyso = 0
 
+    # add machine-dependent info to link file
+    if 'l32_64' in optionargs:
+        link_32_64 = 1
+        optionargs.remove('l32_64')
+    else:
+        link_32_64 = 0
+
     # do we want to create a statically linked executable
     if 'static' in optionargs:
         static_linking = 1



From nouiz at mail.berlios.de  Thu Sep 25 15:49:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 25 Sep 2008 15:49:16 +0200
Subject: [Plearn-commits] r9476 - trunk/python_modules/plearn/parallel
Message-ID: <200809251349.m8PDnGqD016076@sheep.berlios.de>

Author: nouiz
Date: 2008-09-25 15:49:16 +0200 (Thu, 25 Sep 2008)
New Revision: 9476

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
by default with the bqtools backend we lauch all jobs in parallel


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-23 20:57:36 UTC (rev 9475)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-25 13:49:16 UTC (rev 9476)
@@ -533,7 +533,7 @@
 class DBIBqtools(DBIBase):
 
     def __init__( self, commands, **args ):
-        self.nb_proc = 1
+        self.nb_proc = -1
         self.clean_up = True
         self.micro = 1
         self.queue = "qwork at ms"
@@ -631,10 +631,12 @@
                 submitOptions = -q %s -l walltime=%s
                 param1 = (task, logfile) = load tasks, logfiles
                 linkFiles = launcher
-                concurrentJobs = %d
                 preBatch = rm -f _*.BQ
                 microJobs = %d
-                '''%(self.unique_id[1:12],self.queue,self.duree,self.nb_proc,self.micro)) )
+                '''%(self.unique_id[1:12],self.queue,self.duree,self.micro)) )
+        if self.nb_proc>0:
+            bqsubmit_dat.write('''\nconcurrentJobs = %d\n'''%(self.nb_proc))
+
         print self.unique_id
         if self.clean_up:
             bqsubmit_dat.write('postBatch = rm -rf dbi_batch*.BQ ; rm -f logfiles tasks launcher bqsubmit.dat ;')
@@ -700,7 +702,7 @@
         self.stderrs = ''
         self.base_tasks_log_file = []
         self.set_special_env = True
-        self.nb_proc = 0 # 0 mean unlimited
+        self.nb_proc = -1 # < 0   mean unlimited
         self.source_file = ''
         self.source_file = os.getenv("CONDOR_LOCAL_SOURCE")
         self.condor_home = os.getenv('CONDOR_HOME')
@@ -1129,7 +1131,7 @@
                 pass
 
         #exec dependent code
-        if self.nb_proc != 0:
+        if self.nb_proc > 0:
             cmd=self.run_dag()
         else:
             cmd=self.run_non_dag()



From tihocan at mail.berlios.de  Thu Sep 25 15:55:53 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 25 Sep 2008 15:55:53 +0200
Subject: [Plearn-commits] r9477 - trunk/plearn/math
Message-ID: <200809251355.m8PDtrCe016649@sheep.berlios.de>

Author: tihocan
Date: 2008-09-25 15:55:53 +0200 (Thu, 25 Sep 2008)
New Revision: 9477

Added:
   trunk/plearn/math/TMat_maths.cc
Modified:
   trunk/plearn/math/TMat_maths.h
Log:
Added remote call to solveLinearSystemByCholesky

Added: trunk/plearn/math/TMat_maths.cc
===================================================================
--- trunk/plearn/math/TMat_maths.cc	2008-09-25 13:49:16 UTC (rev 9476)
+++ trunk/plearn/math/TMat_maths.cc	2008-09-25 13:55:53 UTC (rev 9477)
@@ -0,0 +1,90 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 2008 University of Montreal
+//
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: TMat_maths.cc 8235 2007-11-07 21:32:01Z nouiz $
+ * AUTHORS: Olivier Delalleau
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+/*! \file PLearn/plearn/math/TMat_maths.cc */
+
+#include <plearn/base/RemoteDeclareMethod.h>
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+
+BEGIN_DECLARE_REMOTE_FUNCTIONS
+
+    declareFunction("solveLinearSystemByCholesky",
+                    &remote_solveLinearSystemByCholesky,
+        (BodyDoc("Solve a linear regression problem using Cholesky "
+                 "decomposition."),
+         ArgDoc("XtX", 
+             "Result of X'X, where X is the input data matrix, with samples "
+             "as rows. A constant input can be added to compute a bias term."
+             "Weight decay can be added on the diagonal terms (that do not "
+             "correspond to the constant input when a bias is computed)."),
+         ArgDoc("XtY", 
+             "Result of X'Y, where Y is the target data matrix, with samples "
+             " as rows."),
+         RetDoc ("The weights W of the linear regression, s.t. XW ~= Y")));
+
+END_DECLARE_REMOTE_FUNCTIONS
+
+////////////////////////////////////////
+// remote_solveLinearSystemByCholesky //
+////////////////////////////////////////
+Mat remote_solveLinearSystemByCholesky(const Mat& A, const Mat& B)
+{
+    Mat weights(A.length(), B.width());
+    solveLinearSystemByCholesky(A, B, weights);
+    return weights;
+}
+ 
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/math/TMat_maths.h
===================================================================
--- trunk/plearn/math/TMat_maths.h	2008-09-25 13:49:16 UTC (rev 9476)
+++ trunk/plearn/math/TMat_maths.h	2008-09-25 13:55:53 UTC (rev 9477)
@@ -51,6 +51,13 @@
 #include "TMat_maths_impl.h"
 #include "TMat_maths_specialisation.h"
 
+namespace PLearn {
+
+//! Remote method for 'solveLinearSystemByCholesky'.
+Mat remote_solveLinearSystemByCholesky(const Mat& A, const Mat& B);
+ 
+} // end of namespace PLearn
+
 #endif
 
 



From nouiz at mail.berlios.de  Thu Sep 25 16:05:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 25 Sep 2008 16:05:31 +0200
Subject: [Plearn-commits] r9478 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200809251405.m8PE5V8p017831@sheep.berlios.de>

Author: nouiz
Date: 2008-09-25 16:05:30 +0200 (Thu, 25 Sep 2008)
New Revision: 9478

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
-added in the help the default value for nb_proc for each back-end
-added option --pkdilly to test it.
-for this, refactored the executable used by the condor back-end


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-09-25 13:55:53 UTC (rev 9477)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-09-25 14:05:30 UTC (rev 9478)
@@ -706,8 +706,9 @@
         self.source_file = ''
         self.source_file = os.getenv("CONDOR_LOCAL_SOURCE")
         self.condor_home = os.getenv('CONDOR_HOME')
+        self.condor_submit_exec = "condor_submit"
+        self.condor_submit_dag_exec = "condor_submit_dag"
 
-
         DBIBase.__init__(self, commands, **args)
         self.mem=int(self.mem)*1024
         if not os.path.exists(self.log_dir):
@@ -716,6 +717,7 @@
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
         self.args = args
+
         self.add_commands(commands)
 
     def add_commands(self,commands):
@@ -984,7 +986,7 @@
 
         self.make_wrapper_script(launch_file, '$@')
 
-        condor_cmd = 'condor_submit_dag -maxjobs %s %s'%(str(self.nb_proc), condor_file_dag)
+        condor_cmd = self.condor_submit_dag_exec+' -maxjobs %s %s'%(str(self.nb_proc), condor_file_dag)
         return condor_cmd
 
     def run_non_dag(self):
@@ -1083,7 +1085,7 @@
 
         self.make_wrapper_script(launch_file,'sh -c "$@"')
 
-        return "condor_submit " + condor_file
+        return self.condor_submit_exec + " " + condor_file
 
     def clean(self):
         if len(self.temp_files)>0:

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-25 13:55:53 UTC (rev 9477)
+++ trunk/scripts/dbidispatch	2008-09-25 14:05:30 UTC (rev 9478)
@@ -49,6 +49,7 @@
     --bqtools=N is the same as --bqtools --nb_proc=N
     --ssh=N is the same as --ssh --nb_proc=N
     --condor=N  is the same as --condor --nb_proc=N
+    condor and bqtools default -1. Cluster default 50.  local and ssh default 1
   The '--[*no_]clean_up' set the DBI option clean_up to true or false
 
 bqtools and cluster option:
@@ -231,12 +232,16 @@
         tasks_filename = val
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
-                   "--set_special_env"]:
+                   "--set_special_env", "--pkdilly"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
-                  "--no_set_special_env"]:
+                  "--no_set_special_env", "--no_pkdilly"]:
         dbi_param[argv[5:]]=False
+    elif argc=="--pkdilly":
+        dbi_param["condor_submit_exec"]=="pkdilly"
+    elif argc=="--no_pkdilly":
+        del dbi_param["condor_submit_exec"]
     elif argv=="--testdbi":
         dbi_param["test"]=True
     elif argv=="--no_testdbi":
@@ -310,6 +315,8 @@
     p = os.path.abspath(os.path.curdir)
     if any([p.startswith(x) for x in ["/home/fringant1/","/home/fringant2/","/cluster/"]]) or dbi_param.get('files'):
         pass
+    elif dbi_param.get("pkdilly")==True:
+        pass
     else:
         raise Exception("You must be in a subfolder of /home/fringant2/")
     f=os.getenv("CONDOR_LOCAL_SOURCE")



From nouiz at mail.berlios.de  Thu Sep 25 16:12:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 25 Sep 2008 16:12:00 +0200
Subject: [Plearn-commits] r9479 - trunk/scripts
Message-ID: <200809251412.m8PEC0p4018280@sheep.berlios.de>

Author: nouiz
Date: 2008-09-25 16:12:00 +0200 (Thu, 25 Sep 2008)
New Revision: 9479

Modified:
   trunk/scripts/dbidispatch
Log:
bugfix for last commit


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-09-25 14:05:30 UTC (rev 9478)
+++ trunk/scripts/dbidispatch	2008-09-25 14:12:00 UTC (rev 9479)
@@ -232,16 +232,17 @@
         tasks_filename = val
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
-                   "--set_special_env", "--pkdilly"]:
+                   "--set_special_env"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
-                  "--no_set_special_env", "--no_pkdilly"]:
+                  "--no_set_special_env"]:
         dbi_param[argv[5:]]=False
-    elif argc=="--pkdilly":
-        dbi_param["condor_submit_exec"]=="pkdilly"
-    elif argc=="--no_pkdilly":
-        del dbi_param["condor_submit_exec"]
+    elif argv=="--pkdilly":
+        dbi_param["condor_submit_exec"]="pkdilly"
+    elif argv=="--no_pkdilly":
+        if dbi_param.has_key("condor_submit_exec"):
+            del dbi_param["condor_submit_exec"]
     elif argv=="--testdbi":
         dbi_param["test"]=True
     elif argv=="--no_testdbi":



From saintmlx at mail.berlios.de  Thu Sep 25 20:14:59 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 25 Sep 2008 20:14:59 +0200
Subject: [Plearn-commits] r9480 - trunk/plearn_learners_experimental
Message-ID: <200809251814.m8PIExg0006327@sheep.berlios.de>

Author: saintmlx
Date: 2008-09-25 20:14:59 +0200 (Thu, 25 Sep 2008)
New Revision: 9480

Modified:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
Log:
- DiscriminativeRBM::forget now forgets the target layer



Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-09-25 14:12:00 UTC (rev 9479)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-09-25 18:14:59 UTC (rev 9480)
@@ -576,6 +576,7 @@
 
     input_layer->forget();
     hidden_layer->forget();
+    target_layer->forget();
     connection->forget();
     if( targetsize() > 1 )
     {



From saintmlx at mail.berlios.de  Thu Sep 25 20:22:35 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 25 Sep 2008 20:22:35 +0200
Subject: [Plearn-commits] r9481 - trunk/plearn_learners_experimental
Message-ID: <200809251822.m8PIMZwG008558@sheep.berlios.de>

Author: saintmlx
Date: 2008-09-25 20:22:35 +0200 (Thu, 25 Sep 2008)
New Revision: 9481

Modified:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
Log:
- revert prev. commit: classification module is responsible for forgetting target_layer



Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-09-25 18:14:59 UTC (rev 9480)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-09-25 18:22:35 UTC (rev 9481)
@@ -576,7 +576,6 @@
 
     input_layer->forget();
     hidden_layer->forget();
-    target_layer->forget();
     connection->forget();
     if( targetsize() > 1 )
     {



From larocheh at mail.berlios.de  Thu Sep 25 20:25:24 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 25 Sep 2008 20:25:24 +0200
Subject: [Plearn-commits] r9482 - trunk/plearn_learners/online
Message-ID: <200809251825.m8PIPOfo008876@sheep.berlios.de>

Author: larocheh
Date: 2008-09-25 20:25:23 +0200 (Thu, 25 Sep 2008)
New Revision: 9482

Modified:
   trunk/plearn_learners/online/RBMClassificationModule.cc
Log:
Did not call forget on target_layer....


Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2008-09-25 18:22:35 UTC (rev 9481)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2008-09-25 18:25:23 UTC (rev 9482)
@@ -318,6 +318,10 @@
     if( !(joint_connection->random_gen) )
         joint_connection->random_gen = random_gen;
     joint_connection->forget();
+    if( !(target_layer->random_gen) )
+        target_layer->random_gen = random_gen;
+    target_layer->forget();
+
 }
 
 /* THIS METHOD IS OPTIONAL



From louradou at mail.berlios.de  Thu Sep 25 22:08:04 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 25 Sep 2008 22:08:04 +0200
Subject: [Plearn-commits] r9483 - trunk/plearn_learners_experimental
Message-ID: <200809252008.m8PK84Vu023927@sheep.berlios.de>

Author: louradou
Date: 2008-09-25 22:08:04 +0200 (Thu, 25 Sep 2008)
New Revision: 9483

Modified:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
Log:
making semi-supervised learning work with negative targets (as well as missing targets)



Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-09-25 18:25:23 UTC (rev 9482)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-09-25 20:08:04 UTC (rev 9483)
@@ -643,7 +643,7 @@
         if( targetsize() == 1 )
         {
             target_one_hot.clear();
-            if( !is_missing(target[0]) )
+            if( !is_missing(target[0]) && (target[0] >= 0) )
             {
                 target_index = (int)round( target[0] );
                 target_one_hot[ target_index ] = 1;
@@ -659,7 +659,7 @@
         // ... for discriminative learning
         if( !do_not_use_discriminative_learning && 
             !use_exact_disc_gradient && 
-            ( !is_missing(target[0]) || targetsize() > 1 ) )
+            ( ( !is_missing(target[0]) && (target[0] >= 0) ) || targetsize() > 1 ) )
         {
             // Positive phase
 
@@ -696,7 +696,7 @@
         // ... for generative learning
         if( (stage + offset) % gen_learning_every_n_samples == 0 )
         {            
-            if( ( !is_missing(target[0]) || targetsize() > 1 ) && 
+            if( ( ( !is_missing(target[0]) && (target[0] >= 0) ) || targetsize() > 1 ) && 
                 gen_learning_weight > 0 )
             {
                 // Positive phase
@@ -767,7 +767,7 @@
             PLERROR("DiscriminativeRBM::train(): semi-supervised learning "
                 "is not implemented yet for multi-task learning.");
 
-        if( is_missing(target[0]) && semi_sup_learning_weight > 0 )
+        if( ( is_missing(target[0]) || target[0] < 0 ) && semi_sup_learning_weight > 0 )
         {
             // Positive phase
 
@@ -822,7 +822,7 @@
 
         if( !do_not_use_discriminative_learning && 
             use_exact_disc_gradient && 
-            ( !is_missing(target[0]) || targetsize() > 1 ) )
+            ( ( !is_missing(target[0]) && (target[0] >= 0)  ) || targetsize() > 1 ) )
         {
             if( targetsize() == 1)
             {
@@ -901,7 +901,7 @@
 
         // CD Updates
         if( !do_not_use_discriminative_learning && 
-            !use_exact_disc_gradient && !is_missing(target[0]) )
+            !use_exact_disc_gradient && ( !is_missing(target[0]) && (target[0] >= 0) ) )
         {
             joint_layer->update( disc_pos_down_val, disc_neg_down_val );
             hidden_layer->update( disc_pos_up_val, disc_neg_up_val );
@@ -911,7 +911,7 @@
 
         if( (stage + offset) % gen_learning_every_n_samples == 0 )
         { 
-            if( !is_missing(target[0]) && gen_learning_weight > 0 )
+            if( ( !is_missing(target[0]) && (target[0] >= 0) ) && gen_learning_weight > 0 )
             {
                 setLearningRate( gen_learning_every_n_samples * gen_learning_weight * disc_learning_rate / 
                                  (1. + disc_decrease_ct * stage ));
@@ -922,7 +922,7 @@
             }
         }            
 
-        if( is_missing(target[0]) && semi_sup_learning_weight > 0 )
+        if( ( is_missing(target[0]) || (target[0] < 0)  ) && semi_sup_learning_weight > 0 )
         {
             setLearningRate( semi_sup_learning_weight * disc_learning_rate / 
                              (1. + disc_decrease_ct * stage ));
@@ -976,7 +976,7 @@
 
     if( targetsize() == 1 )
     {
-        if( !is_missing(target[0]) )
+        if( !is_missing(target[0]) && (target[0] >= 0) )
         {
             //classification_cost->fprop( output, target, costs[nll_cost_index] );
             //classification_cost->CostModule::fprop( output, target, costs[nll_cost_index] );



From saintmlx at mail.berlios.de  Thu Sep 25 23:22:23 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 25 Sep 2008 23:22:23 +0200
Subject: [Plearn-commits] r9484 - trunk
Message-ID: <200809252122.m8PLMNRB032388@sheep.berlios.de>

Author: saintmlx
Date: 2008-09-25 23:22:22 +0200 (Thu, 25 Sep 2008)
New Revision: 9484

Modified:
   trunk/pymake.config.model
Log:
- compile w/python 2.5 at apstat from now on



Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-09-25 20:08:04 UTC (rev 9483)
+++ trunk/pymake.config.model	2008-09-25 21:22:22 UTC (rev 9484)
@@ -311,7 +311,7 @@
     numpy_includedirs = []
 
     if domain_name.endswith('apstat.com'):
-        python_version = '2.4'
+        python_version = '2.5'
         optionargs += [ 'python%s' % python_version.replace('.', '') ]
         python_lib_root = '/usr/lib'
         numpy_site_packages = '-L/usr/lib/python' + python_version + '/site-packages/numarray'



From saintmlx at mail.berlios.de  Fri Sep 26 02:06:53 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 26 Sep 2008 02:06:53 +0200
Subject: [Plearn-commits] r9485 -
	trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results
Message-ID: <200809260006.m8Q06rGN016513@sheep.berlios.de>

Author: saintmlx
Date: 2008-09-26 02:06:40 +0200 (Fri, 26 Sep 2008)
New Revision: 9485

Modified:
   trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
Log:
- new cgitb format w/python25



Modified: trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
===================================================================
--- trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2008-09-25 21:22:22 UTC (rev 9484)
+++ trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2008-09-26 00:06:40 UTC (rev 9485)
@@ -26,6 +26,10 @@
  <string> in get_value()
 
 NameError: global name 'buf' is not defined
+    __class__ = NameError
+    __delattr__ = <method-wrapper '__delattr__' of exceptions.NameError object at 0x[memory_address]>
+    __dict__ = {}
+        message = "global name 'buf' is not defined"
 
 The above is a description of an error in a Python program.  Here is
 the original traceback:



From nouiz at mail.berlios.de  Mon Sep 29 21:13:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 29 Sep 2008 21:13:54 +0200
Subject: [Plearn-commits] r9486 - trunk/python_modules/plearn/pymake
Message-ID: <200809291913.m8TJDsj8001200@sheep.berlios.de>

Author: nouiz
Date: 2008-09-29 21:13:54 +0200 (Mon, 29 Sep 2008)
New Revision: 9486

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
-print then number of files that failed to compile
-the we can't chdir to the home directory, we will retry the compilation else where.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-09-26 00:06:40 UTC (rev 9485)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-09-29 19:13:54 UTC (rev 9486)
@@ -1092,7 +1092,7 @@
         failures =  ccfile.failed_ccfiles_to_link()
         if failures:
             print '[ Executable target',ccfile.filebase,'not remade because of previous compilation errors. ]'
-            print '   Errors were while compiling files:'
+            print '   Errors were while compiling',len(failures),'file(s):'
             print '   '+string.join(failures,'\n   ')
             if link_exit_code == 0:
                 link_exit_code = 1
@@ -2173,6 +2173,10 @@
                     # same machine. No need to remove it from the list, we will
                     # try again.
                     self.retry_compilation = True
+                elif warningmsgs[0].startswith('Could not chdir to home directory '):
+                    #this happen when the /tmp folder is full
+                    self.remove_hostname = True
+                    self.retry_compilation = True
                 else:
                     # Warning messages were uninformative, abort
                     self.compilation_status = -2



