<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6688 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-February/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6688%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702270647.l1R6lTlY017891%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000136.html">
   <LINK REL="Next"  HREF="000138.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6688 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6688%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702270647.l1R6lTlY017891%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6688 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Tue Feb 27 07:47:29 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000136.html">[Plearn-commits] r6687 - trunk
</A></li>
        <LI>Next message: <A HREF="000138.html">[Plearn-commits] r6689 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#137">[ date ]</a>
              <a href="thread.html#137">[ thread ]</a>
              <a href="subject.html#137">[ subject ]</a>
              <a href="author.html#137">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-02-27 07:47:27 +0100 (Tue, 27 Feb 2007)
New Revision: 6688

Modified:
   trunk/plearn_learners/online/BackConvolution2DModule.cc
   trunk/plearn_learners/online/ClassErrorCostModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.h
   trunk/plearn_learners/online/Convolution2DModule.cc
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
   trunk/plearn_learners/online/ModuleStackModule.cc
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/ProcessInputCostModule.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMClassificationModule.cc
   trunk/plearn_learners/online/RBMClassificationModule.h
   trunk/plearn_learners/online/RBMConv2DConnection.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
   trunk/plearn_learners/online/RBMMixedConnection.cc
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
   trunk/plearn_learners/online/SoftmaxModule.cc
   trunk/plearn_learners/online/SquaredErrorCostModule.cc
   trunk/plearn_learners/online/SquaredErrorCostModule.h
   trunk/plearn_learners/online/Subsampling2DModule.cc
   trunk/plearn_learners/online/Supersampling2DModule.cc
   trunk/plearn_learners/online/TanhModule.cc
Log:
Implements bpropUpdate(..., accumulate) in classes deriving from
OnlineLearningModule.
Note that it should not be possible to accumulate a gradient into a Vec
that does not have the right size.



Modified: trunk/plearn_learners/online/BackConvolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -376,7 +376,6 @@
                                           const Vec&amp; output_gradient,
                                           bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // Check size
     if( input.size() != input_size )
         PLERROR(&quot;BackConvolution2DModule::bpropUpdate: input.size() should&quot;
@@ -393,7 +392,13 @@
                 &quot;equal to output_size (%i != %i).\n&quot;,
                 output_gradient.size(), output_size);
 
-    input_gradient.resize(input_size);
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+        input_gradient.resize(input_size);
 
     // Since fprop() has just been called, we assume that input_images and
     // output_images are up-to-date
@@ -418,7 +423,8 @@
                 backConvolve2Dbackprop( kernels(i,j), input_images[i],
                                         input_gradients[i],
                                         output_gradients[j], kernel_gradient,
-                                        kernel_step1, kernel_step2, false );
+                                        kernel_step1, kernel_step2,
+                                        accumulate );
 
                 // kernel(i,j) -= learning_rate * kernel_gradient
                 multiplyAcc( kernels(i,j), kernel_gradient, -learning_rate );
@@ -495,7 +501,6 @@
                                            const Vec&amp; output_diag_hessian,
                                            bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     // This version forwards the second order information, but does not
     // actually use it for the update.
 
@@ -505,8 +510,16 @@
                 &quot; output_diag_hessian.size()\n&quot;
                 &quot;should be equal to output_size (%i != %i).\n&quot;,
                 output_diag_hessian.size(), output_size);
-    input_diag_hessian.resize(input_size);
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+    else
+        input_diag_hessian.resize(input_size);
+
     // Make input_diag_hessians and output_diag_hessians point to the right
     // places
     for( int i=0 ; i&lt;n_input_images ; i++ )
@@ -529,11 +542,11 @@
 
                 convolve2D( output_diag_hessians[j], squared_kernel,
                             input_diag_hessians[i],
-                            kernel_step1, kernel_step2, false );
+                            kernel_step1, kernel_step2, accumulate );
             }
 
     // Call bpropUpdate()
-    bpropUpdate( input, output, input_gradient, output_gradient );
+    bpropUpdate( input, output, input_gradient, output_gradient, accumulate );
 }
 
 

Modified: trunk/plearn_learners/online/ClassErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/ClassErrorCostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -124,7 +124,8 @@
 /* Not supposed to happen
 void ClassErrorCostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target,
                                        real cost,
-                                       Vec&amp; input_gradient)
+                                       Vec&amp; input_gradient,
+                                       bool accumulate=false)
 {
 }
 */
@@ -141,7 +142,8 @@
 void ClassErrorCostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target,
                                        real cost,
                                        Vec&amp; input_gradient,
-                                       Vec&amp; input_diag_hessian)
+                                       Vec&amp; input_diag_hessian,
+                                       bool accumulate=false)
 {
 }
 */

Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -136,23 +136,41 @@
                                        real cost, Vec&amp; input_gradient,
                                        bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
-    input_gradient.resize( input_size );
-    input_gradient.clear();
 
-    Vec partial_gradient;
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
     for( int i=0 ; i&lt;n_sub_costs ; i++ )
     {
-        if( cost_weights[i] != 0. )
+        if( cost_weights[i] == 0. )
         {
+            // Don't compute input_gradient
+            sub_costs[i]-&gt;bpropUpdate( input, target, sub_costs_values[i] );
+        }
+        else if( cost_weights[i] == 1. )
+        {
+            // Accumulate directly into input_gradient
             sub_costs[i]-&gt;bpropUpdate( input, target, sub_costs_values[i],
-                                       partial_gradient );
+                                       input_gradient, true );
+        }
+        else
+        {
+            // Put the result into partial_gradient, then accumulate into
+            // input_gradient with the appropriate weight
+            sub_costs[i]-&gt;bpropUpdate( input, target, sub_costs_values[i],
+                                       partial_gradient, false );
             multiplyAcc( input_gradient, partial_gradient, cost_weights[i] );
         }
-        else
-            sub_costs[i]-&gt;bpropUpdate( input, target, sub_costs_values[i] );
     }
 }
 
@@ -172,23 +190,50 @@
                                         Vec&amp; input_diag_hessian,
                                         bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
-    input_gradient.resize( input_size );
-    input_gradient.clear();
-    input_diag_hessian.resize( input_size );
-    input_diag_hessian.clear();
 
-    Vec partial_gradient;
-    Vec partial_diag_hessian;
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+        input_diag_hessian.resize( input_size );
+        input_diag_hessian.clear();
+    }
+
     for( int i=0 ; i&lt;n_sub_costs ; i++ )
     {
-        sub_costs[i]-&gt;bbpropUpdate( input, target, sub_costs_values[i],
-                                    partial_gradient, partial_diag_hessian );
-        multiplyAcc( input_gradient, partial_gradient, sub_costs_values[i] );
-        multiplyAcc( input_diag_hessian, partial_diag_hessian,
-                     sub_costs_values[i] );
+        if( cost_weights[i] == 0. )
+        {
+            // Don't compute input_gradient nor input_diag_hessian
+            sub_costs[i]-&gt;bbpropUpdate( input, target, sub_costs_values[i] );
+        }
+        else if( cost_weights[i] == 1. )
+        {
+            // Accumulate directly into input_gradient and input_diag_hessian
+            sub_costs[i]-&gt;bbpropUpdate( input, target, sub_costs_values[i],
+                                        input_gradient, input_diag_hessian,
+                                        true );
+        }
+        else
+        {
+            // Put temporary results into partial_*, then multiply and add to
+            // input_*
+            sub_costs[i]-&gt;bbpropUpdate( input, target, sub_costs_values[i],
+                                        partial_gradient, partial_diag_hessian,
+                                        false );
+            multiplyAcc( input_gradient, partial_gradient, cost_weights[i] );
+            multiplyAcc( input_diag_hessian, partial_diag_hessian,
+                         cost_weights[i] );
+        }
     }
 }
 
@@ -199,7 +244,7 @@
     PLASSERT( target.size() == target_size );
 
     for( int i=0 ; i&lt;n_sub_costs ; i++ )
-        sub_costs[i]-&gt;bpropUpdate( input, target, sub_costs_values[i] );
+        sub_costs[i]-&gt;bbpropUpdate( input, target, sub_costs_values[i] );
 }
 
 

Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -81,7 +81,7 @@
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                             Vec&amp; input_gradient, bool accumulate = false);
+                             Vec&amp; input_gradient, bool accumulate=false);
 
     //! Calls this method on the sub_costs
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost);
@@ -141,6 +141,12 @@
 
     //! Stores the output values of the sub_costs
     mutable Vec sub_costs_values;
+
+    //! Stores intermediate values of the input gradient
+    mutable Vec partial_gradient;
+
+    //! Stores intermediate values of the input diagonal of Hessian
+    mutable Vec partial_diag_hessian;
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/online/Convolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/Convolution2DModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -383,7 +383,6 @@
                                       const Vec&amp; output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // Check size
     if( input.size() != input_size )
         PLERROR(&quot;Convolution2DModule::bpropUpdate: input.size() should be\n&quot;
@@ -398,7 +397,13 @@
                 &quot;equal to output_size (%i != %i).\n&quot;,
                 output_gradient.size(), output_size);
 
-    input_gradient.resize(input_size);
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+        input_gradient.resize(input_size);
 
     // Since fprop() has just been called, we assume that input_images and
     // output_images are up-to-date
@@ -423,7 +428,7 @@
                 convolve2Dbackprop( input_images[i], kernels(i,j),
                                     output_gradients[j],
                                     input_gradients[i], kernel_gradient,
-                                    kernel_step1, kernel_step2, false );
+                                    kernel_step1, kernel_step2, accumulate );
 
                 // kernel(i,j) -= learning_rate * kernel_gradient
                 multiplyAcc( kernels(i,j), kernel_gradient, -learning_rate );
@@ -502,8 +507,6 @@
                                        const Vec&amp; output_diag_hessian,
                                        bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
-
     // This version forwards the second order information, but does not
     // actually use it for the update.
 
@@ -513,8 +516,16 @@
                 &quot;\n&quot;
                 &quot;should be equal to output_size (%i != %i).\n&quot;,
                 output_diag_hessian.size(), output_size);
-    input_diag_hessian.resize(input_size);
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+    else
+        input_diag_hessian.resize(input_size);
+
     // Make input_diag_hessians and output_diag_hessians point to the right
     // places
     for( int i=0 ; i&lt;n_input_images ; i++ )
@@ -537,11 +548,11 @@
 
                 backConvolve2D( input_diag_hessians[i], squared_kernel,
                                 output_diag_hessians[j],
-                                kernel_step1, kernel_step2, false );
+                                kernel_step1, kernel_step2, accumulate );
             }
 
     // Call bpropUpdate()
-    bpropUpdate( input, output, input_gradient, output_gradient );
+    bpropUpdate( input, output, input_gradient, output_gradient, accumulate );
 }
 
 

Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/CostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -39,7 +39,9 @@
 
 
 #include &quot;CostModule.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
 
+
 namespace PLearn {
 using namespace std;
 
@@ -124,9 +126,16 @@
     Vec the_cost( 1, cost );
     Vec one( 1, 1 );
 
-    bpropUpdate( input_and_target, the_cost, input_and_target_gradient, one, accumulate );
+    bpropUpdate( input_and_target, the_cost, input_and_target_gradient, one );
 
-    input_gradient = input_and_target_gradient.subVec( 0, input_size );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+        input_gradient += input_and_target_gradient.subVec( 0, input_size );
+    }
+    else
+        input_gradient = input_and_target_gradient.subVec( 0, input_size );
 }
 
 void CostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost)
@@ -140,10 +149,9 @@
                              const Vec&amp; output_gradient,
                              bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
-
     inherited::bpropUpdate( input_and_target, output,
-                            input_and_target_gradient, output_gradient );
+                            input_and_target_gradient, output_gradient,
+                            accumulate );
 }
 
 
@@ -166,8 +174,24 @@
                   input_and_target_diag_hessian, zero,
                   accumulate );
 
-    input_gradient = input_and_target_gradient.subVec( 0, input_size );
-    input_diag_hessian = input_and_target_diag_hessian.subVec( 0, input_size );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+
+        input_gradient += input_and_target_gradient.subVec( 0, input_size );
+        input_diag_hessian +=
+            input_and_target_diag_hessian.subVec( 0, input_size );
+    }
+    else
+    {
+        input_gradient = input_and_target_gradient.subVec( 0, input_size );
+        input_diag_hessian =
+            input_and_target_diag_hessian.subVec( 0, input_size );
+    }
 }
 
 void CostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost)
@@ -184,12 +208,12 @@
                               const Vec&amp; output_diag_hessian,
                               bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     inherited::bbpropUpdate( input_and_target, output,
                              input_and_target_gradient,
                              output_gradient,
                              input_and_target_diag_hessian,
-                             output_diag_hessian );
+                             output_diag_hessian,
+                             accumulate );
 }
 
 void CostModule::forget()

Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -137,7 +137,6 @@
                                       const Vec&amp; output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT_MSG( input.size() == input_size,
                   &quot;input.size() should be equal to this-&gt;input_size&quot; );
     PLASSERT_MSG( output.size() == output_size,
@@ -146,8 +145,16 @@
                   &quot;output_gradient.size() should be equal to this-&gt;output_size&quot;
                 );
 
-    input_gradient.resize( input_size );
-    input_gradient.clear();
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
 
     learning_rate = start_learning_rate / (1+decrease_constant*step_number);
 
@@ -204,9 +211,10 @@
                                        Vec&amp;  input_gradient,
                                        const Vec&amp; output_gradient,
                                        Vec&amp;  input_diag_hessian,
-                                       const Vec&amp; output_diag_hessian)
+                                       const Vec&amp; output_diag_hessian,
+                                       bool accumulate)
 {
-    bpropUpdate( input, output, input_gradient, output_gradient );
+    bpropUpdate( input, output, input_gradient, output_gradient, accumulate );
 }
 */
 

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -124,7 +124,8 @@
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
     */
 
     virtual void forget();

Modified: trunk/plearn_learners/online/ModuleStackModule.cc
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -129,22 +129,29 @@
                                     const Vec&amp; output_gradient,
                                     bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( n_modules &gt; 0 );
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+
     // bpropUpdate should be called just after the corresponding fprop,
     // so values should be up-to-date.
     modules[n_modules-1]-&gt;bpropUpdate( values[n_modules-2], output,
-                                       gradients[n_modules-2], output_gradient );
+                                       gradients[n_modules-2],
+                                       output_gradient );
 
     for( int i=n_modules-2 ; i&gt;0 ; i-- )
         modules[i]-&gt;bpropUpdate( values[i-1], values[i],
                                  gradients[i-1], gradients[i] );
 
-    modules[0]-&gt;bpropUpdate( input, values[0], input_gradient, gradients[0] );
+    modules[0]-&gt;bpropUpdate( input, values[0], input_gradient, gradients[0],
+                             accumulate );
 }
 
 void ModuleStackModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
@@ -178,13 +185,21 @@
                                      const Vec&amp; output_diag_hessian,
                                      bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( n_modules &gt; 0 );
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
     PLASSERT( output_diag_hessian.size() == output_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+
     // bbpropUpdate should be called just after the corresponding fprop,
     // so values should be up-to-date.
     modules[n_modules-1]-&gt;bbpropUpdate( values[n_modules-2], output,
@@ -198,7 +213,8 @@
                                   diag_hessians[i-1], diag_hessians[i] );
 
     modules[0]-&gt;bbpropUpdate( input, values[0], input_gradient, gradients[0],
-                              input_diag_hessian, diag_hessians[0] );
+                              input_diag_hessian, diag_hessians[0],
+                              accumulate );
 }
 
 void ModuleStackModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -97,15 +97,23 @@
 void NLLCostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
                                 Vec&amp; input_gradient, bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
-    input_gradient.resize( input_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
     int the_target = (int) round( target[0] );
     // input_gradient[ i ] = 0 if i != t,
     // input_gradient[ t ] = -1/x[t]
-    input_gradient.clear();
     input_gradient[ the_target ] = - 1. / input[ the_target ];
 }
 
@@ -115,16 +123,25 @@
                                  Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
                                  bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
-    bpropUpdate( input, target, cost, input_gradient );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+    else
+    {
+        input_diag_hessian.resize( input_size );
+        input_diag_hessian.clear();
+    }
 
-    int the_target = (int) round( target[0] );
-    real input_gradient_t = input_gradient[ the_target ];
     // input_diag_hessian[ i ] = 0 if i!=t
     // input_diag_hessian[ t ] = 1/(x[t])^2
-    input_diag_hessian.resize( input_size );
-    input_diag_hessian.clear();
-    input_diag_hessian[ the_target ] = input_gradient_t * input_gradient_t;
+    int the_target = (int) round( target[0] );
+    real input_t = input[ the_target ];
+    input_diag_hessian[ the_target ] += 1. / (input_t * input_t);
+
+    bpropUpdate( input, target, cost, input_gradient, accumulate );
 }
 
 TVec&lt;string&gt; NLLCostModule::name()

Modified: trunk/plearn_learners/online/ProcessInputCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -147,18 +147,25 @@
 // bpropUpdate //
 /////////////////
 void ProcessInputCostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target,
-                                         real cost, Vec&amp; input_gradient, bool accumulate)
+                                         real cost, Vec&amp; input_gradient,
+                                         bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( processing_module );
     PLASSERT( cost_module );
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+
     cost_module-&gt;bpropUpdate( processed_value, target, cost,
                               processed_gradient );
     processing_module-&gt;bpropUpdate( input, processed_value,
-                                    input_gradient, processed_gradient );
+                                    input_gradient, processed_gradient,
+                                    accumulate );
 }
 
 
@@ -167,20 +174,30 @@
 /////////////////
 void ProcessInputCostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target,
                                           real cost, Vec&amp; input_gradient,
-                                          Vec&amp; input_diag_hessian, bool accumulate)
+                                          Vec&amp; input_diag_hessian,
+                                          bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( processing_module );
     PLASSERT( cost_module );
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+
     cost_module-&gt;bbpropUpdate( processed_value, target, cost,
                                processed_gradient, processed_diag_hessian );
     processing_module-&gt;bbpropUpdate( input, processed_value,
                                      input_gradient, processed_gradient,
                                      input_diag_hessian,
-                                     processed_diag_hessian );
+                                     processed_diag_hessian,
+                                     accumulate );
 }
 
 

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -34,7 +34,7 @@
 
 // Authors: Pascal Lamblin &amp; Dan Popovici
 
-/*! \file RBMPLayer.cc */
+/*! \file RBMBinomialLayer.cc */
 
 
 
@@ -116,7 +116,7 @@
 }
 
 void RBMBinomialLayer::fprop( const Vec&amp; input, const Vec&amp; rbm_bias,
-                      Vec&amp; output ) const
+                              Vec&amp; output ) const
 {
     PLASSERT( input.size() == input_size );
     PLASSERT( rbm_bias.size() == input_size );
@@ -131,38 +131,51 @@
                                    const Vec&amp; output_gradient,
                                    bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
-    input_gradient.resize( size );
 
-    for( int i=0 ; i&lt;size ; i++ )
+    if( accumulate )
     {
-        real output_i = output[i];
-        input_gradient[i] = - output_i * (1-output_i) * output_gradient[i];
+        PLASSERT_MSG( input_gradient.size() == size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
     }
-
-    if( momentum == 0. )
+    else
     {
-        // update the bias: bias -= learning_rate * input_gradient
-        multiplyAcc( bias, input_gradient, -learning_rate );
+        input_gradient.resize( size );
+        input_gradient.clear();
     }
-    else
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    for( int i=0 ; i&lt;size ; i++ )
     {
-        bias_inc.resize( size );
-        // The update rule becomes:
-        // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-        // bias += bias_inc
-        multiplyScaledAdd(input_gradient, momentum, -learning_rate, bias_inc);
-        bias += bias_inc;
+        real output_i = output[i];
+        real in_grad_i = - output_i * (1-output_i) * output_gradient[i];
+        input_gradient[i] += in_grad_i;
+
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
     }
 }
 
+//! TODO: add &quot;accumulate&quot; here
 void RBMBinomialLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,
-                           const Vec&amp; output,
-                           Vec&amp; input_gradient, Vec&amp; rbm_bias_gradient,
-                           const Vec&amp; output_gradient)
+                                   const Vec&amp; output,
+                                   Vec&amp; input_gradient, Vec&amp; rbm_bias_gradient,
+                                   const Vec&amp; output_gradient)
 {
     PLASSERT( input.size() == size );
     PLASSERT( rbm_bias.size() == size );
@@ -177,7 +190,7 @@
         input_gradient[i] = - output_i * (1-output_i) * output_gradient[i];
     }
 
-    rbm_bias &lt;&lt; input_gradient;
+    rbm_bias_gradient &lt;&lt; input_gradient;
 }
 
 real RBMBinomialLayer::fpropNLL(const Vec&amp; target)

Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -212,14 +212,17 @@
                                           const Vec&amp; output_gradient,
                                           bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // size checks
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
-    input_gradient.resize( input_size );
-    input_gradient.clear();
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+
     // bpropUpdate in target_layer,
     // assuming target_layer-&gt;activation is up-to-date, but it should be the
     // case if fprop() has been called just before.
@@ -249,7 +252,7 @@
     // at this point, the gradient can be backpropagated through
     // previous_to_last the usual way (even if output is wrong)
     previous_to_last-&gt;bpropUpdate( input, last_act,
-                                   input_gradient, d_last_act );
+                                   input_gradient, d_last_act, accumulate );
 
 }
 
@@ -273,8 +276,8 @@
 //!                  in_hess, out_hess)
 //! AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
 void RBMClassificationModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                                const Vec&amp; output_gradient,
-                                const Vec&amp; output_diag_hessian)
+                                           const Vec&amp; output_gradient,
+                                           const Vec&amp; output_diag_hessian)
 {
 }
 */
@@ -287,10 +290,11 @@
 //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
 //! RAISES A PLERROR.
 void RBMClassificationModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                                Vec&amp; input_gradient,
-                                const Vec&amp; output_gradient,
-                                Vec&amp; input_diag_hessian,
-                                const Vec&amp; output_diag_hessian)
+                                           Vec&amp; input_gradient,
+                                           const Vec&amp; output_gradient,
+                                           Vec&amp; input_diag_hessian,
+                                           const Vec&amp; output_diag_hessian,
+                                           bool accumulate)
 {
 }
 */

Modified: trunk/plearn_learners/online/RBMClassificationModule.h
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMClassificationModule.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -92,7 +92,7 @@
 
     // Your other public member functions go here
 
-    //! given the input, compute the output (possibly resize it  appropriately)
+    //! given the input, compute the output (possibly resize it appropriately)
     virtual void fprop(const Vec&amp; input, Vec&amp; output) const;
 
     //! Adapt based on the output gradient: this method should only
@@ -134,7 +134,8 @@
     //                           Vec&amp; input_gradient,
     //                           const Vec&amp; output_gradient,
     //                           Vec&amp; input_diag_hessian,
-    //                           const Vec&amp; output_diag_hessian);
+    //                           const Vec&amp; output_diag_hessian
+    //                           bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -431,12 +431,18 @@
                                       const Vec&amp; output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );
-    input_gradient.resize( down_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+        input_gradient.resize( down_size );
+
     down_image = input.toMat( down_image_length, down_image_width );
     up_image = output.toMat( up_image_length, up_image_width );
     down_image_gradient = input_gradient.toMat( down_image_length,
@@ -448,7 +454,7 @@
     convolve2Dbackprop( down_image, kernel,
                         up_image_gradient, down_image_gradient,
                         kernel_gradient,
-                        kernel_step1, kernel_step2, false );
+                        kernel_step1, kernel_step2, accumulate );
 
     // kernel -= learning_rate * kernel_gradient
     multiplyAcc( kernel, kernel_gradient, -learning_rate );

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -128,63 +128,71 @@
                                    const Vec&amp; output_gradient,
                                    bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
-    input_gradient.resize( size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+    {
+        bias_inc.resize( size );
+        //quad_coeff_inc.resize( size );
+    }
+
+    // real two_lr = 2 * learning_rate;
     for( int i=0 ; i&lt;size ; ++i )
     {
         real a_i = quad_coeff[i];
-        input_gradient[i] = - output_gradient[i] / (2 * a_i * a_i);
-    }
+        real in_grad_i = - output_gradient[i] / (2 * a_i * a_i);
+        input_gradient[i] += in_grad_i;
 
-    if( momentum == 0. )
-    {
-        /*
-        // update the quadratic coefficient:
-        // a_i += learning_rate * out_grad_i * (b_i + input_i) / a_i^3
-        // (or a_i += 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
-        real two_lr = 2 * learning_rate;
-        for( int i=0 ; i&lt;size ; i++ )
+        if( momentum == 0. )
         {
-            quad_coeff[i] += two_lr * input_gradient[i] * (bias[i] + input[i])
-                                                            / quad_coeff[i];
+            // bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+
+            /* For the moment, we do not want to change the quadratic
+               coefficient during the gradient descent phase.
+
+            // update the quadratic coefficient:
+            // a_i += learning_rate * out_grad_i * (b_i + input_i) / a_i^3
+            // (or a_i += 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
+            quad_coeff[i] += two_lr * in_grad_i * (bias[i] + input[i])
+                                                    / quad_coeff[i];
             if( quad_coeff[i] &lt; min_quad_coeff )
                 quad_coeff[i] = min_quad_coeff;
+            */
         }
-        */
+        else
+        {
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
 
-        // bias -= learning_rate * input_gradient
-        multiplyAcc( bias, input_gradient, -learning_rate );
-    }
-    else
-    {
-        bias_inc.resize( size );
-        //quad_coeff_inc.resize( size );
-
-        /*
-        // The update rule becomes:
-        // a_inc_i = momentum * a_i_inc + learning_rate * out_grad_i
-        //                                  * (b_i + input_i) / a_i^3
-        // a_i += a_inc_i
-        real two_lr = 2 * learning_rate;
-        for( int i=0 ; i&lt;size ; i++ )
-        {
+            /*
+            // The update rule becomes:
+            // a_inc_i = momentum * a_i_inc + learning_rate * out_grad_i
+            //                                  * (b_i + input_i) / a_i^3
+            // a_i += a_inc_i
             quad_coeff_inc[i] += momentum * quad_coeff_inc[i]
-                + two_lr * input_gradient[i] * (bias[i] + input[i])
-                                                / quad_coeff[i];
+                + two_lr * in_grad_i * (bias[i] + input[i])
+                                         / quad_coeff[i];
             quad_coeff[i] += quad_coeff_inc[i];
             if( quad_coeff[i] &lt; min_quad_coeff )
                 quad_coeff[i] = min_quad_coeff;
+            */
         }
-        */
-
-        // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-        // bias += bias_inc
-        multiplyScaledAdd(input_gradient, momentum, -learning_rate, bias_inc);
-        bias += bias_inc;
     }
 }
 

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -207,9 +207,10 @@
     Vec bias_pos_stats;
     //! Accumulates negative contribution to the gradient of bias
     Vec bias_neg_stats;
-    //! Stores the momenconst Vec&amp; pos, const Vec&amp; neg, tum of the gradient
+    //! Stores the momentum of the gradient
     Vec bias_inc;
 
+
     //! Count of positive examples
     int pos_count;
     //! Count of negative examples
@@ -232,6 +233,10 @@
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
+    //! Stores the computed input gradient (useful when accumulate)
+    Vec tmp_input_gradient;
+    //! Stores the computed input diag hessian (useful when accumulate)
+    Vec tmp_input_diag_hessian;
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -276,16 +276,29 @@
 //! this version allows to obtain the input gradient as well
 void RBMMatrixConnection::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                       Vec&amp; input_gradient,
-                                      const Vec&amp; output_gradient)
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
 {
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );
-    input_gradient.resize( down_size );
 
-    // input_gradient = weights' * output_gradient
-    transposeProduct( input_gradient, weights, output_gradient );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
 
+        // input_gradient += weights' * output_gradient
+        transposeProductAcc( input_gradient, weights, output_gradient );
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+
+        // input_gradient = weights' * output_gradient
+        transposeProduct( input_gradient, weights, output_gradient );
+    }
+
     // weights -= learning_rate * output_gradient * input'
     externalProductScaleAcc( weights, output_gradient, input, -learning_rate );
 }

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -127,7 +127,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMMixedConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMixedConnection.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -507,20 +507,27 @@
                                      const Vec&amp; output_gradient,
                                      bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );
-    input_gradient.resize( down_size );
-    input_gradient.clear();
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+        input_gradient.clear();
+    }
+
     for( int j=0 ; j&lt;n_down_blocks ; j++ )
     {
         int init_j = down_init_positions[j];
         int down_size_j = down_block_sizes[j];
         Vec sub_input = input.subVec( init_j, down_size_j );
         Vec sub_input_gradient = input_gradient.subVec( init_j, down_size_j );
-        Vec part_input_gradient( down_size_j );
 
         for( int i=0 ; i&lt;n_up_blocks ; i++ )
         {
@@ -532,10 +539,9 @@
                 Vec sub_output_gradient = output_gradient.subVec( init_i,
                                                                   up_size_i );
                 sub_connections(i,j)-&gt;bpropUpdate( sub_input, sub_output,
-                                                   part_input_gradient,
-                                                   sub_output_gradient );
-
-                sub_input_gradient += part_input_gradient;
+                                                   sub_input_gradient,
+                                                   sub_output_gradient,
+                                                   true );
             }
         }
     }

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -122,9 +122,10 @@
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]-&gt;size;
+        Vec sub_input = input.subVec(begin, size_i);
+        Vec sub_output = output.subVec(begin, size_i);
 
-        sub_layers[i]-&gt;fprop( input.subVec(begin, size_i), tmp );
-        output.subVec( begin, size_i ) &lt;&lt; tmp;
+        sub_layers[i]-&gt;fprop( sub_input, sub_output );
     }
 }
 
@@ -139,10 +140,11 @@
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]-&gt;size;
+        Vec sub_input = input.subVec(begin, size_i);
+        Vec sub_rbm_bias = rbm_bias.subVec(begin, size_i);
+        Vec sub_output = output.subVec(begin, size_i);
 
-        sub_layers[i]-&gt;fprop( input.subVec(begin, size_i),
-                              rbm_bias.subVec(begin,size_i), tmp );
-        output.subVec( begin, size_i ) &lt;&lt; tmp;
+        sub_layers[i]-&gt;fprop( sub_input, sub_rbm_bias, sub_output );
     }
 }
 
@@ -152,25 +154,30 @@
                                  const Vec&amp; output_gradient,
                                  bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
 
-    input_gradient.resize( size );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+        input_gradient.resize( size );
 
     for( int i=0 ; i&lt;n_layers ; i++ )
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]-&gt;size;
+        Vec sub_input = input.subVec( begin, size_i );
+        Vec sub_output = output.subVec( begin, size_i );
+        Vec sub_input_gradient = input_gradient.subVec( begin, size_i );
+        Vec sub_output_gradient = output_gradient.subVec( begin, size_i );
 
-        sub_layers[i]-&gt;bpropUpdate( input.subVec( begin, size_i ),
-                                    output.subVec( begin, size_i ),
-                                    tmp,
-                                    output_gradient.subVec( begin, size_i ) );
-
-        // because tmp is resizeable
-        input_gradient.subVec( begin, size_i ) &lt;&lt; tmp;
+        sub_layers[i]-&gt;bpropUpdate( sub_input, sub_output,
+                                    sub_input_gradient, sub_output_gradient,
+                                    accumulate );
     }
 }
 
@@ -191,16 +198,16 @@
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]-&gt;size;
+        Vec sub_input = input.subVec( begin, size_i );
+        Vec sub_rbm_bias = rbm_bias.subVec( begin, size_i );
+        Vec sub_output = output.subVec( begin, size_i );
+        Vec sub_input_gradient = input_gradient.subVec( begin, size_i );
+        Vec sub_rbm_bias_gradient = rbm_bias_gradient.subVec( begin, size_i);
+        Vec sub_output_gradient = output_gradient.subVec( begin, size_i );
 
-        sub_layers[i]-&gt;bpropUpdate( input.subVec( begin, size_i ),
-                                    rbm_bias.subVec( begin, size_i),
-                                    output.subVec( begin, size_i ),
-                                    tmp, tmpb,
-                                    output_gradient.subVec( begin, size_i ) );
-
-        // because tmp and tmpb is resizeable
-        input_gradient.subVec( begin, size_i ) &lt;&lt; tmp;
-        rbm_bias_gradient.subVec( begin, size_i) &lt;&lt; tmpb;
+        sub_layers[i]-&gt;bpropUpdate( sub_input, sub_rbm_bias, sub_output,
+                                    sub_input_gradient, sub_rbm_bias_gradient,
+                                    sub_output_gradient );
     }
 }
 
@@ -235,9 +242,10 @@
     {
         int begin = init_positions[i];
         int size_i = sub_layers[i]-&gt;size;
-        sub_layers[i]-&gt;bpropNLL( target.subVec(begin, size_i), nlls[i],
-                                 tmpb );
-        bias_gradient.subVec(begin, size_i) &lt;&lt; tmpb;
+
+        Vec sub_target = target.subVec(begin, size_i);
+        Vec sub_bias_gradient = bias_gradient.subVec(begin, size_i);
+        sub_layers[i]-&gt;bpropNLL( sub_target, nlls[i], sub_bias_gradient );
     }
 }
 

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -177,7 +177,7 @@
     //#####  Private Data Members  ############################################
 
     // The rest of the private stuff goes here
-    mutable Vec tmp, tmpb, nlls;
+    mutable Vec nlls;
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -130,40 +130,59 @@
                                       const Vec&amp; output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
-    input_gradient.resize( size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
     // input_gradient[i] =
     //      (output_gradient . output - output_gradient[i] ) output[i]
     real outg_dot_out = dot( output_gradient, output );
     real* out = output.data();
     real* outg = output_gradient.data();
     real* ing = input_gradient.data();
+    real* b = bias.data();
+    real* binc = bias_inc.data();
+
     for( int i=0 ; i&lt;size ; i++ )
-        ing[i] = (outg_dot_out - outg[i]) * out[i];
+    {
+        real ing_i = (outg_dot_out - outg[i]) * out[i];
+        ing[i] += ing_i;
 
-    if( momentum == 0. )
-    {
-        // update the bias: bias -= learning_rate * input_gradient
-        multiplyAcc( bias, input_gradient, -learning_rate );
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            b[i] -= learning_rate * ing_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            binc[i] = momentum * binc[i] - learning_rate * ing_i;
+            b[i] += binc[i];
+        }
     }
-    else
-    {
-        bias_inc.resize( size );
-        // The update rule becomes:
-        // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-        // bias += bias_inc
-        multiplyScaledAdd(input_gradient, momentum, -learning_rate, bias_inc);
-        bias += bias_inc;
-    }
 }
 
+//! TODO: add &quot;accumulate&quot; here
 void RBMMultinomialLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,
                                       const Vec&amp; output,
-                                      Vec&amp; input_gradient, Vec&amp; rbm_bias_gradient,
+                                      Vec&amp; input_gradient,
+                                      Vec&amp; rbm_bias_gradient,
                                       const Vec&amp; output_gradient)
 {
     PLASSERT( input.size() == size );
@@ -204,7 +223,8 @@
     return ret;
 }
 
-void RBMMultinomialLayer::bpropNLL(const Vec&amp; target, real nll, Vec bias_gradient)
+void RBMMultinomialLayer::bpropNLL(const Vec&amp; target, real nll,
+                                   Vec bias_gradient)
 {
     computeExpectation();
 

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -161,47 +161,60 @@
                                    const Vec&amp; output_gradient,
                                    bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );
-    input_gradient.resize( size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
     // df/da = exp(a)/(1-exp(a))^2 - 1/a^2
 
     for( int i=0 ; i&lt;size ; i++ )
     {
         real a_i = input[i] + bias[i];
+        real in_grad_i;
 
         // Polynomial approximation to avoid numerical instability
         // df/da = -1/12 + a^2/240 + O(a^4)
         if( fabs( a_i ) &lt;= 0.01 )
         {
-            input_gradient[i] = output_gradient[i] * (
-                -1./12. + a_i * a_i / 240. );
+            in_grad_i = output_gradient[i] * ( -1./12. + a_i * a_i / 240. );
         }
         else
         {
             real ea_i = exp( a_i );
-            input_gradient[i] = output_gradient[i] * (
+            in_grad_i = output_gradient[i] * (
                 ea_i/( (1 - ea_i) * (1 - ea_i) ) + 1/(a_i * a_i) );
         }
-    }
 
-    if( momentum == 0. )
-    {
-        // update the bias: bias -= learning_rate * input_gradient
-        multiplyAcc( bias, input_gradient, -learning_rate );
+        input_gradient[i] += in_grad_i;
+
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
     }
-    else
-    {
-        bias_inc.resize( size );
-        // The update rule becomes:
-        // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-        // bias += bias_inc
-        multiplyScaledAdd(input_gradient, momentum, -learning_rate, bias_inc);
-        bias += bias_inc;
-    }
 }
 
 

Modified: trunk/plearn_learners/online/SoftmaxModule.cc
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/SoftmaxModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -95,16 +95,29 @@
                                 const Vec&amp; output_gradient,
                                 bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
-    input_gradient.resize( input_size );
 
-    // input_gradient = output_gradient * output
-    //                  - (output_gradient . output ) output
-    multiply( output_gradient, output, input_gradient );
-    multiplyAcc( input_gradient, output, -dot( output_gradient, output ) );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
+    // input_gradient[i] = output_gradient[i] * output[i]
+    //                  - (output_gradient . output ) output[i]
+    real outg_dot_out = dot( output_gradient, output );
+    for( int i=0 ; i&lt;input_size ; i++ )
+    {
+        real in_grad_i = (output_gradient[i] - outg_dot_out) * output[i];
+        input_gradient[i] += in_grad_i;
+    }
 }
 
 //! reset the parameters to the state they would be BEFORE starting training.
@@ -120,7 +133,6 @@
                                  const Vec&amp; output_diag_hessian,
                                  bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLERROR( &quot;Not implemented yet, please come back later or complaint to&quot;
              &quot; lamblinp.&quot; );
 }

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -95,29 +95,51 @@
 
 
 void SquaredErrorCostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target,
-                                         real cost, Vec&amp; input_gradient, 
+                                         real cost, Vec&amp; input_gradient,
                                          bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
-    input_gradient.resize( input_size );
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
     // input_gradient = 2*(input - target)
-    substract( input, target, input_gradient );
-    input_gradient *= 2.0;
+    for( int i=0 ; i&lt;input_size ; i++ )
+    {
+        input_gradient[i] += 2*(input[i] - target[i]);
+    }
 }
 
 
 void SquaredErrorCostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target,
                                           real cost,
                                           Vec&amp; input_gradient,
-                                          Vec&amp; input_diag_hessian, bool accumulate)
+                                          Vec&amp; input_diag_hessian,
+                                          bool accumulate)
 {
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+        input_diag_hessian += 2.;
+    }
+    else
+    {
+        input_diag_hessian.resize( input_size );
+        input_diag_hessian.fill( 2. );
+    }
+
     bpropUpdate( input, target, cost, input_gradient, accumulate );
-
-    input_diag_hessian.resize( input_size );
-    input_diag_hessian.fill( 2. );
 }
 
 TVec&lt;string&gt; SquaredErrorCostModule::name()

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.h	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.h	2007-02-27 06:47:27 UTC (rev 6688)
@@ -79,7 +79,7 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian, 
+                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
                               bool accumulate=false);
 
     //! Does nothing

Modified: trunk/plearn_learners/online/Subsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -287,7 +287,6 @@
                                       const Vec&amp; output_gradient,
                                       bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // Check size
     if( input.size() != input_size )
         PLERROR(&quot;Subsampling2DModule::bpropUpdate: input.size() should be\n&quot;
@@ -302,7 +301,13 @@
                 &quot;equal to output_size (%i != %i).\n&quot;,
                 output_gradient.size(), output_size);
 
-    input_gradient.resize(input_size);
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+        input_gradient.resize(input_size);
 
     // Since fprop() has just been called, we assume that input_images,
     // output_images and gradient are up-to-date
@@ -326,7 +331,7 @@
         convolve2Dbackprop( input_images[i], kernel,
                             output_gradients[i],
                             input_gradients[i], kernel_gradient,
-                            kernel_length, kernel_width, false );
+                            kernel_length, kernel_width, accumulate );
 
         // The scale's gradient is the sum of contributions to kernel_gradient
         scale[i] -= learning_rate * sum( kernel_gradient );
@@ -389,7 +394,6 @@
                                        const Vec&amp; output_diag_hessian,
                                        bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     // This version forwards the second order information, but does not
     // actually use it for the update.
 
@@ -399,8 +403,16 @@
                 &quot;\n&quot;
                 &quot;should be equal to output_size (%i != %i).\n&quot;,
                 output_diag_hessian.size(), output_size);
-    input_diag_hessian.resize(input_size);
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+    else
+        input_diag_hessian.resize(input_size);
+
     // Make input_diag_hessians and output_diag_hessians point to the right
     // places
     for( int i=0 ; i&lt;n_input_images ; i++ )
@@ -421,7 +433,7 @@
         squared_kernel.fill( scale[i]*scale[i] );
         backConvolve2D( input_diag_hessians[i], squared_kernel,
                         output_diag_hessians[i],
-                        kernel_length, kernel_width, false );
+                        kernel_length, kernel_width, accumulate );
     }
 
     // Call bpropUpdate()

Modified: trunk/plearn_learners/online/Supersampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Supersampling2DModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/Supersampling2DModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -278,7 +278,6 @@
                                         const Vec&amp; output_gradient,
                                         bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // Check size
     if( input.size() != input_size )
         PLERROR(&quot;Supersampling2DModule::bpropUpdate: input.size() should be\n&quot;
@@ -293,7 +292,13 @@
                 &quot;equal to output_size (%i != %i).\n&quot;,
                 output_gradient.size(), output_size);
 
-    input_gradient.resize(input_size);
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+        input_gradient.resize(input_size);
 
     // Since fprop() has just been called, we assume that input_images,
     // output_images and gradient are up-to-date
@@ -317,7 +322,7 @@
         backConvolve2Dbackprop( kernel, input_images[i],
                                 input_gradients[i],
                                 output_gradients[i], kernel_gradient,
-                                kernel_length, kernel_width, false );
+                                kernel_length, kernel_width, accumulate );
 
         // The scale's gradient is the sum of contributions to kernel_gradient
         scale[i] -= learning_rate * sum( kernel_gradient );
@@ -380,7 +385,6 @@
                                          const Vec&amp; output_diag_hessian,
                                          bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     // This version forwards the second order information, but does not
     // actually use it for the update.
 
@@ -390,8 +394,16 @@
                 &quot; output_diag_hessian.size()\n&quot;
                 &quot;should be equal to output_size (%i != %i).\n&quot;,
                 output_diag_hessian.size(), output_size);
-    input_diag_hessian.resize(input_size);
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+    else
+        input_diag_hessian.resize(input_size);
+
     // Make input_diag_hessians and output_diag_hessians point to the right
     // places
     for( int i=0 ; i&lt;n_input_images ; i++ )
@@ -412,7 +424,7 @@
         squared_kernel.fill( scale[i]*scale[i] );
         convolve2D( output_diag_hessians[i], squared_kernel,
                     input_diag_hessians[i],
-                    kernel_length, kernel_width, false );
+                    kernel_length, kernel_width, accumulate );
     }
 
     // Call bpropUpdate()

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2007-02-27 02:38:49 UTC (rev 6687)
+++ trunk/plearn_learners/online/TanhModule.cc	2007-02-27 06:47:27 UTC (rev 6688)
@@ -111,8 +111,6 @@
                              Vec&amp; input_gradient, const Vec&amp; output_gradient,
                              bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
-
     int in_size = input.size();
     int out_size = output.size();
     int og_size = output_gradient.size();
@@ -135,11 +133,21 @@
                 og_size, output_size);
     }
 
-    input_gradient.resize( input_size );
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
     for( int i=0 ; i&lt;input_size ; i++ )
     {
         real output_i = output[i];
-        input_gradient[i] = in_scale *
+        input_gradient[i] += in_scale *
             (ex_scale - output_i*output_i/ex_scale)*output_gradient[i];
     }
 
@@ -170,8 +178,6 @@
                               const Vec&amp; output_diag_hessian,
                               bool accumulate)
 {
-    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
-
     int odh_size = output_diag_hessian.size();
 
     // size check
@@ -183,24 +189,33 @@
                 odh_size, output_size);
     }
 
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_diag_hessian.size() == input_size,
+                      &quot;Cannot resize input_diag_hessian AND accumulate into it&quot;
+                    );
+    }
+    else
+    {
+        input_diag_hessian.resize( input_size );
+        input_diag_hessian.clear();
+    }
 
-    bpropUpdate( input, output, input_gradient, output_gradient );
-
-    input_diag_hessian.resize( input_size );
     for( int i=0 ; i&lt;input_size ; i++ )
     {
         real output_i = output[i];
         real fprime_i = in_scale * (ex_scale-output_i*output_i / ex_scale);
 
         if( estimate_simpler_diag_hessian )
-            input_diag_hessian[i] =
+            input_diag_hessian[i] +=
                 fprime_i*fprime_i*output_diag_hessian[i];
         else
-            input_diag_hessian[i] =
+            input_diag_hessian[i] +=
                 fprime_i*fprime_i*output_diag_hessian[i]
                 - 2*in_scale/ex_scale*fprime_i*output_i*output_gradient[i];
     }
 
+    bpropUpdate( input, output, input_gradient, output_gradient, accumulate );
 }
 
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000136.html">[Plearn-commits] r6687 - trunk
</A></li>
	<LI>Next message: <A HREF="000138.html">[Plearn-commits] r6689 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#137">[ date ]</a>
              <a href="thread.html#137">[ thread ]</a>
              <a href="subject.html#137">[ subject ]</a>
              <a href="author.html#137">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
