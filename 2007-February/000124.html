<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6675 - in trunk: plearn/var	plearn_learners/regressors
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-February/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6675%20-%20in%20trunk%3A%20plearn/var%0A%09plearn_learners/regressors&In-Reply-To=%3C200702211549.l1LFnR7c005068%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000123.html">
   <LINK REL="Next"  HREF="000125.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6675 - in trunk: plearn/var	plearn_learners/regressors</H1>
    <B>chapados at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6675%20-%20in%20trunk%3A%20plearn/var%0A%09plearn_learners/regressors&In-Reply-To=%3C200702211549.l1LFnR7c005068%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6675 - in trunk: plearn/var	plearn_learners/regressors">chapados at mail.berlios.de
       </A><BR>
    <I>Wed Feb 21 16:49:27 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000123.html">[Plearn-commits] r6674 - trunk/python_modules/plearn/pyplearn
</A></li>
        <LI>Next message: <A HREF="000125.html">[Plearn-commits] r6676 - in trunk: commands/PLearnCommands	examples/Demo/Tasks/2d_classif/Datasets/moons.amat.metadata	plearn/base plearn/io plearn/math plearn/misc plearn/sys	plearn/vmat plearn_learners/generic plearn_learners/testers	python_modules/plearn/io python_modules/plearn/parallel	python_modules/plearn/utilities scripts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#124">[ date ]</a>
              <a href="thread.html#124">[ thread ]</a>
              <a href="subject.html#124">[ subject ]</a>
              <a href="author.html#124">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: chapados
Date: 2007-02-21 16:49:27 +0100 (Wed, 21 Feb 2007)
New Revision: 6675

Modified:
   trunk/plearn/var/GaussianProcessNLLVariable.cc
   trunk/plearn/var/GaussianProcessNLLVariable.h
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.h
Log:
Bug fixes in gaussian processes: kernel hyperparameters were not always updated following changes to the bound ObjectOptionVariable

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-21 05:43:23 UTC (rev 6674)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-02-21 15:49:27 UTC (rev 6675)
@@ -36,6 +36,10 @@
 
 /*! \file GaussianProcessNLLVariable.cc */
 
+#define PL_LOG_MODULE_NAME &quot;GaussianProcessNLLVariable&quot;
+
+// From PLearn
+#include &lt;plearn/io/pl_log.h&gt;
 #include &lt;plearn/math/TMat_maths.h&gt;
 #include &lt;plearn/io/MatIO.h&gt;
 #include &quot;GaussianProcessNLLVariable.h&quot;
@@ -160,6 +164,12 @@
 // ### computes value from varray values
 void GaussianProcessNLLVariable::fprop()
 {
+    logVarray(m_hyperparam_vars, &quot;FProp current hyperparameters:&quot;, true);
+
+    // Ensure that the current hyperparameter variable values are propagated
+    // into kernel options
+    m_hyperparam_vars.fprop();
+    
     fbpropFragments(m_kernel, m_noise, m_inputs, m_targets, m_allow_bprop,
                     m_save_gram_matrix, m_expdir,
                     m_gram, m_cholesky_gram, m_alpha_t, m_inverse_gram,
@@ -293,6 +303,12 @@
         savePMat(filename, gram);
     }
 
+    // Dump a fragment of the Gram Matrix to the debug log
+    DBG_MODULE_LOG &lt;&lt; &quot;Gram fragment: &quot;
+                   &lt;&lt; gram(0,0) &lt;&lt; ' '
+                   &lt;&lt; gram(1,0) &lt;&lt; ' '
+                   &lt;&lt; gram(1,1) &lt;&lt; endl;
+
     // Compute Cholesky decomposition and solve the linear system
     alpha_t.resize(trainlength, rhs_width);
     L.resize(trainlength, trainlength);
@@ -353,6 +369,12 @@
         savePMat(filename, gram);
     }
 
+    // Dump a fragment of the Gram Matrix to the debug log
+    DBG_MODULE_LOG &lt;&lt; &quot;Gram fragment: &quot;
+                   &lt;&lt; gram(0,0) &lt;&lt; ' '
+                   &lt;&lt; gram(1,0) &lt;&lt; ' '
+                   &lt;&lt; gram(1,1) &lt;&lt; endl;
+
     // Compute Cholesky decomposition and solve the linear system.  LAPACK
     // solves in-place, but luckily we don't need either the Gram and RHS
     // matrices after solving.  Note that for now we don't bother to create an
@@ -369,6 +391,26 @@
 }
 #endif
 
+
+//#####  logVarray  ###########################################################
+
+void GaussianProcessNLLVariable::logVarray(const VarArray&amp; varr,
+                                           const string&amp; title, bool debug)
+{
+    string entry = title + '\n';
+    for (int i=0, n=varr.size() ; i&lt;n ; ++i) {
+        entry += right(varr[i]-&gt;getName(), 35) + &quot;: &quot; + tostring(varr[i]-&gt;value[0]);
+        if (i &lt; n-1)
+            entry += '\n';
+    }
+    if (debug) {
+        DBG_MODULE_LOG &lt;&lt; entry &lt;&lt; endl;
+    }
+    else {
+        MODULE_LOG &lt;&lt; entry &lt;&lt; endl;
+    }
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/var/GaussianProcessNLLVariable.h
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.h	2007-02-21 05:43:23 UTC (rev 6674)
+++ trunk/plearn/var/GaussianProcessNLLVariable.h	2007-02-21 15:49:27 UTC (rev 6675)
@@ -147,6 +147,10 @@
     //! Accessor to the last computed gram matrix inverse in an fprop
     const Mat&amp; gramInverse() const { return m_inverse_gram; }
 
+    /// Minor utility function to dump the contents of a varray to a log
+    static void logVarray(const VarArray&amp; varr, const string&amp; title=&quot;&quot;,
+                          bool debug=false);
+    
 
     //#####  PLearn::Object Protocol  #########################################
 

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-02-21 05:43:23 UTC (rev 6674)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-02-21 15:49:27 UTC (rev 6675)
@@ -346,15 +346,22 @@
     }
 
     // Optimize hyperparameters
-    PP&lt;GaussianProcessNLLVariable&gt; nll = hyperOptimize(m_training_inputs, targets);
+    VarArray hyperparam_vars;
+    PP&lt;GaussianProcessNLLVariable&gt; nll = hyperOptimize(m_training_inputs, targets,
+                                                       hyperparam_vars);
     PLASSERT( nll );
     
-    // Compute parameters
+    // Compute parameters.  Be careful to also propagate through the
+    // hyperparameter variables to ensure the latest values are correctly set
+    // into their respective kernels.
+    hyperparam_vars.fprop();
     nll-&gt;fprop();
     m_alpha = nll-&gt;alpha();
     m_gram_inverse = nll-&gt;gramInverse();
 
     // Compute train MSE, as 1/(2N) * dot(z,z), with z=K*alpha - y
+    // *** FIXME: MSE IS NOT COMPUTED CORRECTLY SINCE GRAM MATRIX IS
+    // OVERWRITTEN BY CHOLESKY DECOMPOSITION ***
     Mat residuals(m_alpha.length(), m_alpha.width());
     product(residuals, nll-&gt;gram(), m_alpha);
     residuals -= targets;
@@ -564,7 +571,8 @@
 //#####  hyperOptimize  #######################################################
 
 PP&lt;GaussianProcessNLLVariable&gt;
-GaussianProcessRegressor::hyperOptimize(const Mat&amp; inputs, const Mat&amp; targets)
+GaussianProcessRegressor::hyperOptimize(const Mat&amp; inputs, const Mat&amp; targets,
+                                        VarArray&amp; hyperparam_vars)
 {
     // If there are no hyperparameters or optimizer, just create a simple
     // variable and return it right away.
@@ -581,7 +589,7 @@
     const int numhyper  = m_hyperparameters.size();
     const int numinputs = ( ! m_ARD_hyperprefix_initval.first.empty() ?
                             inputsize() : 0 );
-    VarArray     hyperparam_vars (numhyper + numinputs);
+    hyperparam_vars = VarArray(numhyper + numinputs);
     TVec&lt;string&gt; hyperparam_names(numhyper + numinputs);
     int i;
     for (i=0 ; i&lt;numhyper ; ++i) {
@@ -614,7 +622,8 @@
     nll-&gt;setName(&quot;GaussianProcessNLLVariable&quot;);
 
     // Some logging about the initial values
-    logVarray(hyperparam_vars, &quot;Hyperparameter initial values:&quot;);
+    GaussianProcessNLLVariable::logVarray(hyperparam_vars,
+                                          &quot;Hyperparameter initial values:&quot;);
     
     // And optimize for nstages
     m_optimizer-&gt;setToOptimize(hyperparam_vars, (Variable*)nll);
@@ -639,25 +648,11 @@
     pb = 0;                                  // Finish progress bar right now
 
     // Some logging about the final values
-    logVarray(hyperparam_vars, &quot;Hyperparameter final values:&quot;);
+    GaussianProcessNLLVariable::logVarray(hyperparam_vars,
+                                          &quot;Hyperparameter final values:&quot;);
     return nll;
 }
 
-
-//#####  logVarray  ###########################################################
-
-void GaussianProcessRegressor::logVarray(const VarArray&amp; varr,
-                                         const string&amp; title)
-{
-    string entry = title + '\n';
-    for (int i=0, n=varr.size() ; i&lt;n ; ++i) {
-        entry += right(varr[i]-&gt;getName(), 35) + &quot;: &quot; + tostring(varr[i]-&gt;value[0]);
-        if (i &lt; n-1)
-            entry += '\n';
-    }
-    MODULE_LOG &lt;&lt; entry &lt;&lt; endl; 
-}
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-02-21 05:43:23 UTC (rev 6674)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-02-21 15:49:27 UTC (rev 6675)
@@ -257,12 +257,9 @@
     /// Optimize the hyperparameters if any.  Return a Variable on which
     /// train() carries out a final fprop for obtaining the final trained
     /// learner parameters.
-    PP&lt;GaussianProcessNLLVariable&gt; hyperOptimize(const Mat&amp; inputs,
-                                                 const Mat&amp; targets);
+    PP&lt;GaussianProcessNLLVariable&gt; hyperOptimize(
+        const Mat&amp; inputs, const Mat&amp; targets, VarArray&amp; hyperparam_vars);
 
-    /// Minor utility function to dump the contents of a varray to a log
-    static void logVarray(const VarArray&amp; varr, const string&amp; title=&quot;&quot;);
-    
 protected:
     //#####  Protected Options  ###############################################
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000123.html">[Plearn-commits] r6674 - trunk/python_modules/plearn/pyplearn
</A></li>
	<LI>Next message: <A HREF="000125.html">[Plearn-commits] r6676 - in trunk: commands/PLearnCommands	examples/Demo/Tasks/2d_classif/Datasets/moons.amat.metadata	plearn/base plearn/io plearn/math plearn/misc plearn/sys	plearn/vmat plearn_learners/generic plearn_learners/testers	python_modules/plearn/io python_modules/plearn/parallel	python_modules/plearn/utilities scripts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#124">[ date ]</a>
              <a href="thread.html#124">[ thread ]</a>
              <a href="subject.html#124">[ subject ]</a>
              <a href="author.html#124">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
