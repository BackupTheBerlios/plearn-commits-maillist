<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6679 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-February/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6679%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702232147.l1NLln90027313%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000127.html">
   <LINK REL="Next"  HREF="000129.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6679 - trunk/plearn_learners/online</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6679%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702232147.l1NLln90027313%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6679 - trunk/plearn_learners/online">yoshua at mail.berlios.de
       </A><BR>
    <I>Fri Feb 23 22:47:49 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000127.html">[Plearn-commits] r6678 - in trunk: plearn/misc	python_modules/plearn/pytest
</A></li>
        <LI>Next message: <A HREF="000129.html">[Plearn-commits] r6680 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#128">[ date ]</a>
              <a href="thread.html#128">[ thread ]</a>
              <a href="subject.html#128">[ subject ]</a>
              <a href="author.html#128">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-02-23 22:47:48 +0100 (Fri, 23 Feb 2007)
New Revision: 6679

Modified:
   trunk/plearn_learners/online/BackConvolution2DModule.cc
   trunk/plearn_learners/online/BackConvolution2DModule.h
   trunk/plearn_learners/online/Convolution2DModule.cc
   trunk/plearn_learners/online/Convolution2DModule.h
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/CostModule.h
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
   trunk/plearn_learners/online/ModuleStackModule.cc
   trunk/plearn_learners/online/ModuleStackModule.h
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMClassificationModule.cc
   trunk/plearn_learners/online/RBMClassificationModule.h
   trunk/plearn_learners/online/RBMConv2DConnection.cc
   trunk/plearn_learners/online/RBMConv2DConnection.h
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMixedConnection.cc
   trunk/plearn_learners/online/RBMMixedConnection.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.h
   trunk/plearn_learners/online/SoftmaxModule.cc
   trunk/plearn_learners/online/SoftmaxModule.h
   trunk/plearn_learners/online/Subsampling2DModule.cc
   trunk/plearn_learners/online/Subsampling2DModule.h
   trunk/plearn_learners/online/Supersampling2DModule.cc
   trunk/plearn_learners/online/Supersampling2DModule.h
   trunk/plearn_learners/online/TanhModule.cc
   trunk/plearn_learners/online/TanhModule.h
Log:


Modified: trunk/plearn_learners/online/BackConvolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -373,8 +373,10 @@
 //! this version allows to obtain the input gradient as well
 void BackConvolution2DModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                           Vec&amp; input_gradient,
-                                          const Vec&amp; output_gradient)
+                                          const Vec&amp; output_gradient,
+                                          bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // Check size
     if( input.size() != input_size )
         PLERROR(&quot;BackConvolution2DModule::bpropUpdate: input.size() should&quot;
@@ -490,8 +492,10 @@
                                            Vec&amp; input_gradient,
                                            const Vec&amp; output_gradient,
                                            Vec&amp; input_diag_hessian,
-                                           const Vec&amp; output_diag_hessian)
+                                           const Vec&amp; output_diag_hessian,
+                                           bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     // This version forwards the second order information, but does not
     // actually use it for the update.
 

Modified: trunk/plearn_learners/online/BackConvolution2DModule.h
===================================================================
--- trunk/plearn_learners/online/BackConvolution2DModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/BackConvolution2DModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -149,9 +149,12 @@
 
     //! this version allows to obtain the input gradient as well
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    //! The flag indicates whether the input_gradients gets
+    //! accumulated into or set with the computed derivatives.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -171,7 +174,8 @@
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/Convolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Convolution2DModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -380,8 +380,10 @@
 //! this version allows to obtain the input gradient as well
 void Convolution2DModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                       Vec&amp; input_gradient,
-                                      const Vec&amp; output_gradient)
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // Check size
     if( input.size() != input_size )
         PLERROR(&quot;Convolution2DModule::bpropUpdate: input.size() should be\n&quot;
@@ -497,8 +499,11 @@
                                        Vec&amp; input_gradient,
                                        const Vec&amp; output_gradient,
                                        Vec&amp; input_diag_hessian,
-                                       const Vec&amp; output_diag_hessian)
+                                       const Vec&amp; output_diag_hessian,
+                                       bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
+
     // This version forwards the second order information, but does not
     // actually use it for the update.
 

Modified: trunk/plearn_learners/online/Convolution2DModule.h
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Convolution2DModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -151,7 +151,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -171,7 +172,8 @@
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/CostModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -137,8 +137,11 @@
 
 void CostModule::bpropUpdate(const Vec&amp; input_and_target, const Vec&amp; output,
                              Vec&amp; input_and_target_gradient,
-                             const Vec&amp; output_gradient)
+                             const Vec&amp; output_gradient,
+                             bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
+
     inherited::bpropUpdate( input_and_target, output,
                             input_and_target_gradient, output_gradient );
 }
@@ -176,8 +179,10 @@
                               Vec&amp; input_and_target_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_and_target_diag_hessian,
-                              const Vec&amp; output_diag_hessian)
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     inherited::bbpropUpdate( input_and_target, output,
                              input_and_target_gradient,
                              output_gradient,

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/CostModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -87,7 +87,8 @@
     //! this version is provided for compatibility with the parent class.
     virtual void bpropUpdate(const Vec&amp; input_and_target, const Vec&amp; output,
                              Vec&amp; input_and_target_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
@@ -102,8 +103,8 @@
                               Vec&amp; input_and_target_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_and_target_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
-
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
     virtual void forget();
 
     //! Indicates the name of the computed costs

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -58,6 +58,7 @@
     // grad_weight_decay( 0. ),
     use_classification_cost( true ),
     n_layers( 0 ),
+    online ( false ), 
     final_module_has_learning_rate( false ),
     final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
@@ -99,8 +100,12 @@
 
     declareOption(ol, &quot;training_schedule&quot;, &amp;DeepBeliefNet::training_schedule,
                   OptionBase::buildoption,
-                  &quot;Number of examples to use during each phase of learning:\n&quot;
-                  &quot;first the greedy phases, and then the gradient descent.\n&quot;);
+                  &quot;Number of examples before starting each phase except the first\n&quot;
+                  &quot;(first the greedy phases, and then the fine-tuning phase).\n&quot;
+                  &quot;For example for 2 hidden layers, with 1000 examples in each\n&quot;
+                  &quot;greedy phase, this option should be [1000 2000] and the last\n&quot;
+                  &quot;nstages - 2000 examples will be used for fine-tuning.\n&quot;
+                  &quot;When online = true, this vector is ignored and should be empty.\n&quot;);
 
     declareOption(ol, &quot;use_classification_cost&quot;,
                   &amp;DeepBeliefNet::use_classification_cost,
@@ -173,6 +178,11 @@
                   &quot;(except the first one) of the RBM. These costs are not\n&quot;
                   &quot;back-propagated to previous layers.\n&quot;);
 
+    declareOption(ol, &quot;online&quot;, &amp;DeepBeliefNet::online,
+                  OptionBase::buildoption,
+                  &quot;If true then all unsupervised training stages (as well as the fine-tuning stage)\n&quot;
+                  &quot;are done simultaneously.\n&quot;);
+
     declareOption(ol, &quot;n_layers&quot;, &amp;DeepBeliefNet::n_layers,
                   OptionBase::learntoption,
                   &quot;Number of layers&quot;);
@@ -211,7 +221,7 @@
     // Initialize some learnt variables
     n_layers = layers.length();
 
-    if( training_schedule.length() != n_layers-1 )
+    if( training_schedule.length() != n_layers-1  &amp;&amp; training_schedule.length()!=0)
     {
         MODULE_LOG &lt;&lt; &quot;training_schedule.length() != n_layers-1, resizing and&quot;
             &quot; zeroing&quot; &lt;&lt; endl;
@@ -515,148 +525,173 @@
 
     PP&lt;ProgressBar&gt; pb;
 
+    real train_recons_error = 0.0;
+
     // clear stats of previous epoch
     train_stats-&gt;forget();
 
-    /***** initial greedy training *****/
-    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    if (online)
+        // train all layers simultaneously AND fine-tuning as well!
     {
-        if( use_classification_cost &amp;&amp; i == n_layers-2 )
-            break; // we will do a joint supervised learning instead
+        if( report_progress &amp;&amp; stage &lt; nstages )
+            pb = new ProgressBar( &quot;Training &quot;+classname(),
+                                  nstages - stage );
 
-        MODULE_LOG &lt;&lt; &quot;Training connection weights between layers &quot; &lt;&lt; i
-            &lt;&lt; &quot; and &quot; &lt;&lt; i+1 &lt;&lt; endl;
+        for (int i=0; i&lt;n_layers;i++)
+        {
+            layers[i]-&gt;setLearningRate( cd_learning_rate );
+            connections[i]-&gt;setLearningRate( cd_learning_rate );
+        }
 
-        int end_stage = min( training_schedule[i], nstages );
-
-        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  cd_learning_rate = &quot; &lt;&lt; cd_learning_rate &lt;&lt; endl;
-
-        if( report_progress &amp;&amp; stage &lt; end_stage )
-            pb = new ProgressBar( &quot;Training layer &quot;+tostring(i)
-                                  +&quot; of &quot;+classname(),
-                                  end_stage - stage );
-
-        layers[i]-&gt;setLearningRate( cd_learning_rate );
-        connections[i]-&gt;setLearningRate( cd_learning_rate );
-        layers[i+1]-&gt;setLearningRate( cd_learning_rate );
-
-        for( ; stage&lt;end_stage ; stage++ )
+        for( ; stage&lt;nstages; stage++)
         {
             int sample = stage % nsamples;
             train_set-&gt;getExample(sample, input, target, weight);
-            greedyStep( input, target, i );
-
+            onlineStep( input, target );
             if( pb )
-                if( i == 0 )
-                    pb-&gt;update( stage + 1 );
-                else
-                    pb-&gt;update( stage - training_schedule[i-1] + 1 );
+                pb-&gt;update( stage + 1 );
         }
     }
-
-    // possible supervised part
-    if( use_classification_cost )
+    else // by stages
     {
-        MODULE_LOG &lt;&lt; &quot;Training the classification module&quot; &lt;&lt; endl;
+        /***** initial greedy training *****/
+        for( int i=0 ; i&lt;n_layers-1 ; i++ )
+        {
+            if( use_classification_cost &amp;&amp; i == n_layers-2 )
+                break; // we will do a joint supervised learning instead
 
-        int end_stage = min( training_schedule[n_layers-2], nstages );
+            MODULE_LOG &lt;&lt; &quot;Training connection weights between layers &quot; &lt;&lt; i
+                       &lt;&lt; &quot; and &quot; &lt;&lt; i+1 &lt;&lt; endl;
 
-        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  cd_learning_rate = &quot; &lt;&lt; cd_learning_rate &lt;&lt; endl;
+            int end_stage = min( training_schedule[i], nstages );
 
-        if( report_progress &amp;&amp; stage &lt; end_stage )
-             pb = new ProgressBar( &quot;Training the classification module&quot;,
-                                   end_stage - stage );
+            MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  cd_learning_rate = &quot; &lt;&lt; cd_learning_rate &lt;&lt; endl;
 
-        // set appropriate learning rate
-        setLearningRate( cd_learning_rate );
+            if( report_progress &amp;&amp; stage &lt; end_stage )
+                pb = new ProgressBar( &quot;Training layer &quot;+tostring(i)
+                                      +&quot; of &quot;+classname(),
+                                      end_stage - stage );
 
-        int previous_stage = (n_layers &lt; 3) ? 0
-                                            : training_schedule[n_layers-3];
-        for( ; stage&lt;end_stage ; stage++ )
+            layers[i]-&gt;setLearningRate( cd_learning_rate );
+            connections[i]-&gt;setLearningRate( cd_learning_rate );
+            layers[i+1]-&gt;setLearningRate( cd_learning_rate );
+
+            for( ; stage&lt;end_stage ; stage++ )
+            {
+                int sample = stage % nsamples;
+                train_set-&gt;getExample(sample, input, target, weight);
+                greedyStep( input, target, i );
+
+                if( pb )
+                    if( i == 0 )
+                        pb-&gt;update( stage + 1 );
+                    else
+                        pb-&gt;update( stage - training_schedule[i-1] + 1 );
+            }
+        }
+
+        // possible supervised part
+        if(use_classification_cost )
         {
-            int sample = stage % nsamples;
-            train_set-&gt;getExample( sample, input, target, weight );
-            jointGreedyStep( input, target );
+            MODULE_LOG &lt;&lt; &quot;Training the classification module&quot; &lt;&lt; endl;
 
-            if( pb )
-                pb-&gt;update( stage - previous_stage + 1 );
+            int end_stage = min( training_schedule[n_layers-2], nstages );
+
+            MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
+            MODULE_LOG &lt;&lt; &quot;  cd_learning_rate = &quot; &lt;&lt; cd_learning_rate &lt;&lt; endl;
+
+            if( report_progress &amp;&amp; stage &lt; end_stage )
+                pb = new ProgressBar( &quot;Training the classification module&quot;,
+                                      end_stage - stage );
+
+            // set appropriate learning rate
+            setLearningRate( cd_learning_rate );
+
+            int previous_stage = (n_layers &lt; 3) ? 0
+                : training_schedule[n_layers-3];
+            for( ; stage&lt;end_stage ; stage++ )
+            {
+                int sample = stage % nsamples;
+                train_set-&gt;getExample( sample, input, target, weight );
+                jointGreedyStep( input, target );
+
+                if( pb )
+                    pb-&gt;update( stage - previous_stage + 1 );
+            }
         }
-    }
 
-    /**** compute reconstruction error*****/
-    real train_recons_error = 0.0;
+        /**** compute reconstruction error*****/
+        RBMLayer * down_layer = get_pointer(layers[0]) ;
+        RBMLayer * up_layer =  get_pointer(layers[1]) ;
+        RBMConnection * parameters = get_pointer(connections[0]);
 
-    RBMLayer * down_layer = get_pointer(layers[0]) ;
-    RBMLayer * up_layer =  get_pointer(layers[1]) ;
-    RBMConnection * parameters = get_pointer(connections[0]);
+        for(int train_index = 0 ; train_index &lt; nsamples ; train_index++)
+        {
 
-    for(int train_index = 0 ; train_index &lt; nsamples ; train_index++)
-    {
+            train_set-&gt;getExample( train_index, input, target, weight );
 
-        train_set-&gt;getExample( train_index, input, target, weight );
+            down_layer-&gt;expectation &lt;&lt; input;
 
-          down_layer-&gt;expectation &lt;&lt; input;
+            // up
+            parameters-&gt;setAsDownInput( down_layer-&gt;expectation );
+            up_layer-&gt;getAllActivations( parameters );
+            up_layer-&gt;generateSample();
 
-          // up
-          parameters-&gt;setAsDownInput( down_layer-&gt;expectation );
-          up_layer-&gt;getAllActivations( parameters );
-          up_layer-&gt;generateSample();
+            // down
+            parameters-&gt;setAsUpInput( up_layer-&gt;sample );
 
-          // down
-          parameters-&gt;setAsUpInput( up_layer-&gt;sample );
+            down_layer-&gt;getAllActivations( parameters );
+            down_layer-&gt;computeExpectation();
+            down_layer-&gt;generateSample();
 
-          down_layer-&gt;getAllActivations( parameters );
-          down_layer-&gt;computeExpectation();
-          down_layer-&gt;generateSample();
+            //    result += powdistance( input, down_layer-&gt;expectation );
 
-          //    result += powdistance( input, down_layer-&gt;expectation );
+            for( int i=0 ; i&lt;input.size() ; i++ )
+                train_recons_error += (input[i] - down_layer-&gt;expectation[i])
+                    * (input[i] - down_layer-&gt;expectation[i]);
 
-          for( int i=0 ; i&lt;input.size() ; i++ )
-              train_recons_error += (input[i] - down_layer-&gt;expectation[i])
-                                  * (input[i] - down_layer-&gt;expectation[i]);
+        }
 
-    }
+        train_recons_error /= nsamples ;
 
-    train_recons_error /= nsamples ;
 
+        /***** fine-tuning by gradient descent *****/
+        if( stage &gt;= nstages )
+            return;
 
-    /***** fine-tuning by gradient descent *****/
-    if( stage &gt;= nstages )
-        return;
+        MODULE_LOG &lt;&lt; &quot;Fine-tuning all parameters, by gradient descent&quot; &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  grad_learning_rate = &quot; &lt;&lt; grad_learning_rate &lt;&lt; endl;
 
-    MODULE_LOG &lt;&lt; &quot;Fine-tuning all parameters, by gradient descent&quot; &lt;&lt; endl;
-    MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
-    MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
-    MODULE_LOG &lt;&lt; &quot;  grad_learning_rate = &quot; &lt;&lt; grad_learning_rate &lt;&lt; endl;
+        int init_stage = stage;
+        if( report_progress &amp;&amp; stage &lt; nstages )
+            pb = new ProgressBar( &quot;Fine-tuning parameters of all layers of &quot;
+                                  + classname(),
+                                  nstages - init_stage );
 
-    int init_stage = stage;
-    if( report_progress &amp;&amp; stage &lt; nstages )
-        pb = new ProgressBar( &quot;Fine-tuning parameters of all layers of &quot;
-                              + classname(),
-                              nstages - init_stage );
+        setLearningRate( grad_learning_rate );
 
-    setLearningRate( grad_learning_rate );
+        int begin_sample = stage % nsamples;
+        for( ; stage&lt;nstages ; stage++ )
+        {
+            int sample = stage % nsamples;
+            if( sample == begin_sample )
+                train_stats-&gt;forget();
+            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                setLearningRate( grad_learning_rate
+                                 / (1. + grad_decrease_ct * (stage - init_stage) ) );
 
-    int begin_sample = stage % nsamples;
-    for( ; stage&lt;nstages ; stage++ )
-    {
-        int sample = stage % nsamples;
-        if( sample == begin_sample )
-            train_stats-&gt;forget();
-        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-            setLearningRate( grad_learning_rate
-                / (1. + grad_decrease_ct * (stage - init_stage) ) );
+            train_set-&gt;getExample( sample, input, target, weight );
+            fineTuningStep( input, target, train_costs );
+            train_stats-&gt;update( train_costs );
 
-        train_set-&gt;getExample( sample, input, target, weight );
-        fineTuningStep( input, target, train_costs );
-        train_stats-&gt;update( train_costs );
-
-        if( pb )
-            pb-&gt;update( stage - init_stage + 1 );
+            if( pb )
+                pb-&gt;update( stage - init_stage + 1 );
+        }
     }
 
     //update the reconstruction error
@@ -667,6 +702,65 @@
     train_stats-&gt;finalize();
 }
 
+void DeepBeliefNet::onlineStep( const Vec&amp; input, const Vec&amp; target)
+{
+    Vec cost;
+    if (partial_costs)
+        cost.resize(n_layers);
+
+    layers[0]-&gt;expectation &lt;&lt; input;
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        // mean-field fprop from layer i to layer i+1
+        connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+        layers[i+1]-&gt;getAllActivations( connections[i] );
+        layers[i+1]-&gt;computeExpectation();
+        // propagate into local cost
+        if( partial_costs &amp;&amp; partial_costs[ i ] )
+            partial_costs[ i ]-&gt;fprop( layers[ i+1 ]-&gt;expectation,
+                                       target, cost[i] );
+
+        if( partial_costs &amp;&amp; partial_costs[ i ] )
+        {
+            // put appropriate learning rate
+            connections[ i ]-&gt;setLearningRate( grad_learning_rate );
+            layers[ i+1 ]-&gt;setLearningRate( grad_learning_rate );
+
+            // Backward pass
+
+            partial_costs[ i ]-&gt;bpropUpdate( layers[ i+1 ]-&gt;expectation,
+                                             target, cost,
+                                             expectation_gradients[ i+1 ] );
+
+            // YB - LOUCHE: activation n'est pas vraiment l'output du connection ni l'input de layer i+1
+            // puisque c'est l'output de connection + le biais
+            layers[ i+1 ]-&gt;bpropUpdate( layers[ i+1 ]-&gt;activation, // - biais
+                                        layers[ i+1 ]-&gt;expectation,  
+                                        activation_gradients[ i+1 ],
+                                        expectation_gradients[ i+1 ] );
+
+            connections[ i ]-&gt;bpropUpdate( layers[ i ]-&gt;expectation,
+                                           layers[ i+1 ]-&gt;activation,  // - biais
+                                           expectation_gradients[ i ],
+                                           activation_gradients[ i+1 ] );
+
+            // put back old learning rate
+            connections[ i ]-&gt;setLearningRate( cd_learning_rate );
+            layers[ i+1 ]-&gt;setLearningRate( cd_learning_rate );
+
+            layers[i]-&gt;setLearningRate( cd_learning_rate );
+            connections[i]-&gt;setLearningRate( cd_learning_rate );
+        }
+
+        contrastiveDivergenceStep( layers[ i ],
+                                   connections[ i ],
+                                   layers[ i+1 ] );
+
+    }
+    // fprop in joint layer
+    
+}
+
 void DeepBeliefNet::greedyStep( const Vec&amp; input, const Vec&amp; target, int index )
 {
     PLASSERT( index &lt; n_layers );

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -126,6 +126,9 @@
     //! Number of layers
     int n_layers;
 
+    //! whether to do things by stages, including fine-tuning, or on-line
+    bool online;
+
     //#####  Not Options  #####################################################
 
     //! Timer for monitoring the speed
@@ -185,6 +188,8 @@
     virtual TVec&lt;std::string&gt; getTrainCostNames() const;
 
 
+    void onlineStep( const Vec&amp; input, const Vec&amp; target );
+
     void greedyStep( const Vec&amp; input, const Vec&amp; target, int index );
 
     void jointGreedyStep( const Vec&amp; input, const Vec&amp; target );

Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -134,8 +134,10 @@
 // Simply updates and propagates back gradient
 void GradNNetLayerModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                       Vec&amp; input_gradient,
-                                      const Vec&amp; output_gradient)
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT_MSG( input.size() == input_size,
                   &quot;input.size() should be equal to this-&gt;input_size&quot; );
     PLASSERT_MSG( output.size() == output_size,

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -112,7 +112,8 @@
                              const Vec&amp; output_gradient);
 
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient);
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                               const Vec&amp; output_gradient,

Modified: trunk/plearn_learners/online/ModuleStackModule.cc
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/ModuleStackModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -126,8 +126,10 @@
 /////////////////
 void ModuleStackModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                     Vec&amp; input_gradient,
-                                    const Vec&amp; output_gradient)
+                                    const Vec&amp; output_gradient,
+                                    bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( n_modules &gt; 0 );
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
@@ -173,8 +175,10 @@
                                      Vec&amp; input_gradient,
                                      const Vec&amp; output_gradient,
                                      Vec&amp; input_diag_hessian,
-                                     const Vec&amp; output_diag_hessian)
+                                     const Vec&amp; output_diag_hessian,
+                                     bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( n_modules &gt; 0 );
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );

Modified: trunk/plearn_learners/online/ModuleStackModule.h
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/ModuleStackModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -85,7 +85,8 @@
     //! is 'ready-to-be-used' just after any bpropUpdate.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! This version does not obtain the input gradient.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
@@ -99,7 +100,8 @@
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
 
     //! This version does not obtain the input gradient and diag_hessian.
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -66,7 +66,8 @@
 
 void OnlineLearningModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                        Vec&amp; input_gradient,
-                                       const Vec&amp; output_gradient)
+                                       const Vec&amp; output_gradient,
+                                       bool accumulate)
 {
     PLERROR(&quot;In OnlineLearningModule.cc: method 'bpropUpdate' not&quot;
             &quot; implemented.\n&quot;
@@ -97,7 +98,8 @@
                                         Vec&amp; input_gradient,
                                         const Vec&amp; output_gradient,
                                         Vec&amp; input_diag_hessian,
-                                        const Vec&amp; output_diag_hessian)
+                                        const Vec&amp; output_diag_hessian,
+                                        bool accumulate)
 {
     PLERROR(&quot;In OnlineLearningModule.cc: method 'bbpropUpdate' not&quot;
             &quot;implemented.\n&quot;

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -120,8 +120,11 @@
 
     //! this version allows to obtain the input gradient as well
     //! N.B. THE DEFAULT IMPLEMENTATION JUST RAISES A PLERROR.
+    //! The flag indicates whether the input_gradients gets
+    //! accumulated into or set with the computed derivatives.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient);
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -137,11 +140,14 @@
                               const Vec&amp; output_diag_hessian);
 
     //! this version allows to obtain the input gradient and diag_hessian
+    //! The flag indicates whether the input_gradient and input_diag_hessian gets
+    //! accumulated into or set with the computed derivatives.
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -128,8 +128,10 @@
 
 void RBMBinomialLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                    Vec&amp; input_gradient,
-                                   const Vec&amp; output_gradient)
+                                   const Vec&amp; output_gradient,
+                                   bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -95,7 +95,8 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient);
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,

Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -209,8 +209,10 @@
 //! this version allows to obtain the input gradient as well
 void RBMClassificationModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                           Vec&amp; input_gradient,
-                                          const Vec&amp; output_gradient)
+                                          const Vec&amp; output_gradient,
+                                          bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // size checks
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );

Modified: trunk/plearn_learners/online/RBMClassificationModule.h
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMClassificationModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -111,7 +111,8 @@
     //! this version allows to obtain the input gradient as well
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this

Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -428,8 +428,10 @@
 //! this version allows to obtain the input gradient as well
 void RBMConv2DConnection::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                       Vec&amp; input_gradient,
-                                      const Vec&amp; output_gradient)
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );

Modified: trunk/plearn_learners/online/RBMConv2DConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMConv2DConnection.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -151,7 +151,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -125,8 +125,10 @@
 
 void RBMGaussianLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                    Vec&amp; input_gradient,
-                                   const Vec&amp; output_gradient)
+                                   const Vec&amp; output_gradient,
+                                   bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -96,7 +96,8 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient);
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! Accumulates positive phase statistics
     virtual void accumulatePosStats( const Vec&amp; pos_values );

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -137,13 +137,14 @@
     //! and update the bias (and possibly the quadratic term)
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient) = 0 ;
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false) = 0 ;
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,
                              const Vec&amp; output,
                              Vec&amp; input_gradient, Vec&amp; rbm_bias_gradient,
-                             const Vec&amp; output_gradient) ;
+                             const Vec&amp; output_gradient);
 
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer

Modified: trunk/plearn_learners/online/RBMMixedConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMixedConnection.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -503,9 +503,11 @@
 
 //! this version allows to obtain the input gradient as well
 void RBMMixedConnection::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                                      Vec&amp; input_gradient,
-                                      const Vec&amp; output_gradient)
+                                     Vec&amp; input_gradient,
+                                     const Vec&amp; output_gradient,
+                                     bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
     PLASSERT( output_gradient.size() == up_size );

Modified: trunk/plearn_learners/online/RBMMixedConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedConnection.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMixedConnection.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -127,7 +127,8 @@
     //! this version allows to obtain the input gradient as well
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -149,8 +149,10 @@
 
 void RBMMixedLayer::bpropUpdate( const Vec&amp; input, const Vec&amp; output,
                                  Vec&amp; input_gradient,
-                                 const Vec&amp; output_gradient )
+                                 const Vec&amp; output_gradient,
+                                 bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -101,7 +101,8 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient);
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -127,8 +127,10 @@
 
 void RBMMultinomialLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                       Vec&amp; input_gradient,
-                                      const Vec&amp; output_gradient)
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -95,7 +95,8 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient);
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! back-propagates the output gradient to the input and the bias
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -158,8 +158,10 @@
 
 void RBMTruncExpLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                    Vec&amp; input_gradient,
-                                   const Vec&amp; output_gradient)
+                                   const Vec&amp; output_gradient,
+                                   bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == size );
     PLASSERT( output.size() == size );
     PLASSERT( output_gradient.size() == size );

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -92,9 +92,9 @@
 
     //! back-propagates the output gradient to the input
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient);
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
-
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/SoftmaxModule.cc
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/SoftmaxModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -92,8 +92,10 @@
 
 void SoftmaxModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                 Vec&amp; input_gradient,
-                                const Vec&amp; output_gradient)
+                                const Vec&amp; output_gradient,
+                                bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( output.size() == output_size );
     PLASSERT( output_gradient.size() == output_size );
@@ -112,11 +114,13 @@
 }
 
 void SoftmaxModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                                Vec&amp; input_gradient,
-                                const Vec&amp; output_gradient,
-                                Vec&amp; input_diag_hessian,
-                                const Vec&amp; output_diag_hessian)
+                                 Vec&amp; input_gradient,
+                                 const Vec&amp; output_gradient,
+                                 Vec&amp; input_diag_hessian,
+                                 const Vec&amp; output_diag_hessian,
+                                 bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLERROR( &quot;Not implemented yet, please come back later or complaint to&quot;
              &quot; lamblinp.&quot; );
 }

Modified: trunk/plearn_learners/online/SoftmaxModule.h
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/SoftmaxModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -69,14 +69,16 @@
     //! this version allows to obtain the input gradient as well
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! this version allows to obtain the input gradient and diag_hessian
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/Subsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Subsampling2DModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -284,8 +284,10 @@
 //! this version allows to obtain the input gradient as well
 void Subsampling2DModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                       Vec&amp; input_gradient,
-                                      const Vec&amp; output_gradient)
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // Check size
     if( input.size() != input_size )
         PLERROR(&quot;Subsampling2DModule::bpropUpdate: input.size() should be\n&quot;
@@ -384,8 +386,10 @@
                                        Vec&amp; input_gradient,
                                        const Vec&amp; output_gradient,
                                        Vec&amp; input_diag_hessian,
-                                       const Vec&amp; output_diag_hessian)
+                                       const Vec&amp; output_diag_hessian,
+                                       bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     // This version forwards the second order information, but does not
     // actually use it for the update.
 

Modified: trunk/plearn_learners/online/Subsampling2DModule.h
===================================================================
--- trunk/plearn_learners/online/Subsampling2DModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Subsampling2DModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -135,7 +135,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -155,7 +156,8 @@
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/Supersampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Supersampling2DModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Supersampling2DModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -274,9 +274,11 @@
 
 //! this version allows to obtain the input gradient as well
 void Supersampling2DModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                                      Vec&amp; input_gradient,
-                                      const Vec&amp; output_gradient)
+                                        Vec&amp; input_gradient,
+                                        const Vec&amp; output_gradient,
+                                        bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     // Check size
     if( input.size() != input_size )
         PLERROR(&quot;Supersampling2DModule::bpropUpdate: input.size() should be\n&quot;
@@ -375,8 +377,10 @@
                                          Vec&amp; input_gradient,
                                          const Vec&amp; output_gradient,
                                          Vec&amp; input_diag_hessian,
-                                         const Vec&amp; output_diag_hessian)
+                                         const Vec&amp; output_diag_hessian,
+                                         bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     // This version forwards the second order information, but does not
     // actually use it for the update.
 

Modified: trunk/plearn_learners/online/Supersampling2DModule.h
===================================================================
--- trunk/plearn_learners/online/Supersampling2DModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/Supersampling2DModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -135,7 +135,8 @@
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              Vec&amp; input_gradient,
-                             const Vec&amp; output_gradient);
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
@@ -155,7 +156,8 @@
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/TanhModule.cc	2007-02-23 21:47:48 UTC (rev 6679)
@@ -108,8 +108,11 @@
 
 // Simply propagates output_gradient to input_gradient
 void TanhModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient)
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
+
     int in_size = input.size();
     int out_size = output.size();
     int og_size = output_gradient.size();
@@ -164,8 +167,11 @@
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian)
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
+
     int odh_size = output_diag_hessian.size();
 
     // size check

Modified: trunk/plearn_learners/online/TanhModule.h
===================================================================
--- trunk/plearn_learners/online/TanhModule.h	2007-02-22 22:55:09 UTC (rev 6678)
+++ trunk/plearn_learners/online/TanhModule.h	2007-02-23 21:47:48 UTC (rev 6679)
@@ -79,7 +79,8 @@
                              const Vec&amp; output_gradient);
 
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                             Vec&amp; input_gradient, const Vec&amp; output_gradient);
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
 
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                               const Vec&amp; output_gradient,
@@ -89,7 +90,8 @@
                               Vec&amp; input_gradient,
                               const Vec&amp; output_gradient,
                               Vec&amp; input_diag_hessian,
-                              const Vec&amp; output_diag_hessian);
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
 
     virtual void forget();
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000127.html">[Plearn-commits] r6678 - in trunk: plearn/misc	python_modules/plearn/pytest
</A></li>
	<LI>Next message: <A HREF="000129.html">[Plearn-commits] r6680 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#128">[ date ]</a>
              <a href="thread.html#128">[ thread ]</a>
              <a href="subject.html#128">[ subject ]</a>
              <a href="author.html#128">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
