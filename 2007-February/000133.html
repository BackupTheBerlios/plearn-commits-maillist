<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6684 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-February/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6684%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702262235.l1QMZU2i031559%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000132.html">
   <LINK REL="Next"  HREF="000134.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6684 - trunk/plearn_learners/online</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6684%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702262235.l1QMZU2i031559%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6684 - trunk/plearn_learners/online">yoshua at mail.berlios.de
       </A><BR>
    <I>Mon Feb 26 23:35:30 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000132.html">[Plearn-commits] r6683 - trunk/plearn/io
</A></li>
        <LI>Next message: <A HREF="000134.html">[Plearn-commits] r6685 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#133">[ date ]</a>
              <a href="thread.html#133">[ thread ]</a>
              <a href="subject.html#133">[ subject ]</a>
              <a href="author.html#133">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-02-26 23:35:29 +0100 (Mon, 26 Feb 2007)
New Revision: 6684

Modified:
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.h
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/CostModule.h
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/NLLCostModule.h
   trunk/plearn_learners/online/ProcessInputCostModule.cc
   trunk/plearn_learners/online/ProcessInputCostModule.h
   trunk/plearn_learners/online/SquaredErrorCostModule.cc
   trunk/plearn_learners/online/SquaredErrorCostModule.h
Log:


Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -133,8 +133,10 @@
 }
 
 void CombiningCostsModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target,
-                                       real cost, Vec&amp; input_gradient)
+                                       real cost, Vec&amp; input_gradient,
+                                       bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );
@@ -167,8 +169,10 @@
 void CombiningCostsModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target,
                                         real cost,
                                         Vec&amp; input_gradient,
-                                        Vec&amp; input_diag_hessian)
+                                        Vec&amp; input_diag_hessian,
+                                        bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );

Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -81,7 +81,7 @@
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                             Vec&amp; input_gradient);
+                             Vec&amp; input_gradient, bool accumulate = false);
 
     //! Calls this method on the sub_costs
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost);
@@ -89,7 +89,8 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian);
+                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
+                              bool accumulate=false);
 
     //! Calls this method on the sub_costs
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost);

Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/CostModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -114,7 +114,7 @@
 
 
 void CostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                             Vec&amp; input_gradient)
+                             Vec&amp; input_gradient, bool accumulate)
 {
     // default version, calling the bpropUpdate with inherited prototype
     Vec input_and_target( input_size + target_size );
@@ -124,7 +124,7 @@
     Vec the_cost( 1, cost );
     Vec one( 1, 1 );
 
-    bpropUpdate( input_and_target, the_cost, input_and_target_gradient, one );
+    bpropUpdate( input_and_target, the_cost, input_and_target_gradient, one, accumulate );
 
     input_gradient = input_and_target_gradient.subVec( 0, input_size );
 }
@@ -148,7 +148,8 @@
 
 
 void CostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian)
+                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
+                              bool accumulate)
 {
     // default version, calling the bpropUpdate with inherited prototype
     Vec input_and_target( input_size + target_size );
@@ -162,7 +163,8 @@
 
     bbpropUpdate( input_and_target, the_cost,
                   input_and_target_gradient, one,
-                  input_and_target_diag_hessian, zero );
+                  input_and_target_diag_hessian, zero,
+                  accumulate );
 
     input_gradient = input_and_target_gradient.subVec( 0, input_size );
     input_diag_hessian = input_and_target_diag_hessian.subVec( 0, input_size );

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/CostModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -79,7 +79,7 @@
 
     //! Adapt based on the cost gradient, and obtain the input gradient
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                             Vec&amp; input_gradient);
+                             Vec&amp; input_gradient, bool accumulate=false);
 
     //! Without the input gradient
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost );
@@ -93,7 +93,8 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian );
+                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
+                              bool accumulate=false);
 
     //! Without the input gradient and diag_hessian
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost );

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -180,8 +180,8 @@
 
     declareOption(ol, &quot;online&quot;, &amp;DeepBeliefNet::online,
                   OptionBase::buildoption,
-                  &quot;If true then all unsupervised training stages (as well as the fine-tuning stage)\n&quot;
-                  &quot;are done simultaneously.\n&quot;);
+                  &quot;If true then all unsupervised training stages (as well as\n&quot;
+                  &quot;the fine-tuning stage) are done simultaneously.\n&quot;);
 
     declareOption(ol, &quot;n_layers&quot;, &amp;DeepBeliefNet::n_layers,
                   OptionBase::learntoption,
@@ -709,49 +709,94 @@
         cost.resize(n_layers);
 
     layers[0]-&gt;expectation &lt;&lt; input;
-    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    for( int i=0 ; i&lt;n_layers ; i++ )
     {
         // mean-field fprop from layer i to layer i+1
-        connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
-        layers[i+1]-&gt;getAllActivations( connections[i] );
+
+        Vec input;
+        if (i==n_layers-1 &amp;&amp; use_classification_cost) // top layer is a joint layer
+        {
+            // set the input of the joint layer 
+            Vec joint_exp = joint_layer-&gt;expectation;
+            joint_exp.subVec( 0, layers[ n_layers-2 ]-&gt;size )
+                &lt;&lt; layers[ n_layers-2 ]-&gt;expectation;
+            fill_one_hot( joint_exp.subVec( layers[ n_layers-2 ]-&gt;size, n_classes ),
+                          (int) round(target[0]), 0., 1. );
+            classification_module-&gt;joint_connection-&gt;setAsDownInput(
+                joint_layer-&gt;expectation );
+            layers[ n_layers-1 ]-&gt;getAllActivations(
+                get_pointer( classification_module-&gt;joint_connection ) );
+        }
+        else
+        {
+            connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
+            layers[i+1]-&gt;getAllActivations( connections[i] );
+        }
         layers[i+1]-&gt;computeExpectation();
+
         // propagate into local cost
         if( partial_costs &amp;&amp; partial_costs[ i ] )
+        {
             partial_costs[ i ]-&gt;fprop( layers[ i+1 ]-&gt;expectation,
                                        target, cost[i] );
 
-        if( partial_costs &amp;&amp; partial_costs[ i ] )
-        {
-            // put appropriate learning rate
-            connections[ i ]-&gt;setLearningRate( grad_learning_rate );
-            layers[ i+1 ]-&gt;setLearningRate( grad_learning_rate );
-
             // Backward pass
-
             partial_costs[ i ]-&gt;bpropUpdate( layers[ i+1 ]-&gt;expectation,
                                              target, cost,
                                              expectation_gradients[ i+1 ] );
+            // HACK: since the update will combine the gradients from all sources,
+            // weigh the gradients according to the desired learning rates
+            expectation_gradients[i+1] *= grad_learning_rate/cd_learning_rate;
+        }
+        else
+            expectation_gradients[i+1].clear();
 
-            // YB - LOUCHE: activation n'est pas vraiment l'output du connection ni l'input de layer i+1
-            // puisque c'est l'output de connection + le biais
-            layers[ i+1 ]-&gt;bpropUpdate( layers[ i+1 ]-&gt;activation, // - biais
-                                        layers[ i+1 ]-&gt;expectation,  
-                                        activation_gradients[ i+1 ],
-                                        expectation_gradients[ i+1 ] );
+        if( i==n_layers-1 &amp;&amp; final_cost )
+        {
+            if( final_module )
+            {
+                final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
+                                     final_cost_input );
+                final_cost-&gt;fprop( final_cost_input, target, final_cost_value );
+                final_cost-&gt;bpropUpdate( final_cost_input, target,
+                                         final_cost_value[0],
+                                         final_cost_gradient );
+                // HACK: since the update will combine the gradients from all sources,
+                // weigh the gradients according to the desired learning rates
+                final_cost_gradient *= grad_learning_rate/cd_learning_rate;
+                final_module-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;expectation,
+                                           final_cost_input,
+                                           expectation_gradients[ n_layers-1 ],
+                                           final_cost_gradient );
+            }
+            else
+            {
+                final_cost-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation, target,
+                                   final_cost_value );
+                final_cost-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;expectation,
+                                         target, final_cost_value[0],
+                                         expectation_gradients[ n_layers-1 ] );
+            }
 
-            connections[ i ]-&gt;bpropUpdate( layers[ i ]-&gt;expectation,
-                                           layers[ i+1 ]-&gt;activation,  // - biais
-                                           expectation_gradients[ i ],
-                                           activation_gradients[ i+1 ] );
+            train_costs[final_cost_index] = final_cost_value[0];
 
-            // put back old learning rate
-            connections[ i ]-&gt;setLearningRate( cd_learning_rate );
-            layers[ i+1 ]-&gt;setLearningRate( cd_learning_rate );
+            layers[ n_layers-1 ]-&gt;bpropUpdate( layers[ n_layers-1 ]-&gt;activation,
+                                               layers[ n_layers-1 ]-&gt;expectation,
+                                               activation_gradients[ n_layers-1 ],
+                                               expectation_gradients[ n_layers-1 ]);
 
-            layers[i]-&gt;setLearningRate( cd_learning_rate );
-            connections[i]-&gt;setLearningRate( cd_learning_rate );
+            connections[ n_layers-2 ]-&gt;bpropUpdate(
+                layers[ n_layers-2 ]-&gt;expectation,
+                layers[ n_layers-1 ]-&gt;activation,
+                expectation_gradients[ n_layers-2 ],
+                activation_gradients[ n_layers-1 ],
+                true); // accumulate into expectation_gradients[n_layers-2]
         }
 
+
+
+    }
+        
         contrastiveDivergenceStep( layers[ i ],
                                    connections[ i ],
                                    layers[ i+1 ] );

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -95,8 +95,9 @@
 }
 
 void NLLCostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                                Vec&amp; input_gradient)
+                                Vec&amp; input_gradient, bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );
@@ -111,8 +112,10 @@
 
 void NLLCostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target,
                                  real cost,
-                                 Vec&amp; input_gradient, Vec&amp; input_diag_hessian)
+                                 Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
+                                 bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     bpropUpdate( input, target, cost, input_gradient );
 
     int the_target = (int) round( target[0] );

Modified: trunk/plearn_learners/online/NLLCostModule.h
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/NLLCostModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -72,7 +72,7 @@
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                             Vec&amp; input_gradient);
+                             Vec&amp; input_gradient, bool accumulate=false);
 
     //! Does nothing
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost)
@@ -82,7 +82,8 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian);
+                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
+                              bool accumulate=false);
 
     //! Does nothing
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost)

Modified: trunk/plearn_learners/online/ProcessInputCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -147,8 +147,9 @@
 // bpropUpdate //
 /////////////////
 void ProcessInputCostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target,
-                                         real cost, Vec&amp; input_gradient)
+                                         real cost, Vec&amp; input_gradient, bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( processing_module );
     PLASSERT( cost_module );
     PLASSERT( input.size() == input_size );
@@ -166,8 +167,9 @@
 /////////////////
 void ProcessInputCostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target,
                                           real cost, Vec&amp; input_gradient,
-                                          Vec&amp; input_diag_hessian)
+                                          Vec&amp; input_diag_hessian, bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bbpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( processing_module );
     PLASSERT( cost_module );
     PLASSERT( input.size() == input_size );

Modified: trunk/plearn_learners/online/ProcessInputCostModule.h
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/ProcessInputCostModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -86,7 +86,7 @@
 
     //! Adapt based on the cost, and compute input gradient to backpropagate.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                             Vec&amp; input_gradient);
+                             Vec&amp; input_gradient, bool accumulate=false);
 
     /* Optional
        N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
@@ -103,7 +103,8 @@
     //! If these methods are defined, you can use them INSTEAD of
     //! bpropUpdate(...)
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian);
+                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian,
+                              bool accumulate=false);
 
     /* Optional
        N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.cc	2007-02-26 22:35:29 UTC (rev 6684)
@@ -95,8 +95,10 @@
 
 
 void SquaredErrorCostModule::bpropUpdate(const Vec&amp; input, const Vec&amp; target,
-                                         real cost, Vec&amp; input_gradient)
+                                         real cost, Vec&amp; input_gradient, 
+                                         bool accumulate)
 {
+    PLASSERT_MSG(!accumulate,&quot;Implementation of bpropUpdate cannot yet handle accumulate=false&quot;);
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
     input_gradient.resize( input_size );
@@ -110,9 +112,9 @@
 void SquaredErrorCostModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; target,
                                           real cost,
                                           Vec&amp; input_gradient,
-                                          Vec&amp; input_diag_hessian)
+                                          Vec&amp; input_diag_hessian, bool accumulate)
 {
-    bpropUpdate( input, target, cost, input_gradient );
+    bpropUpdate( input, target, cost, input_gradient, accumulate );
 
     input_diag_hessian.resize( input_size );
     input_diag_hessian.fill( 2. );

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.h	2007-02-26 20:53:52 UTC (rev 6683)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.h	2007-02-26 22:35:29 UTC (rev 6684)
@@ -69,7 +69,7 @@
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                             Vec&amp; input_gradient);
+                             Vec&amp; input_gradient, bool accumulate=false);
 
     //! Does nothing
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost)
@@ -79,7 +79,8 @@
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this back.
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost,
-                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian);
+                              Vec&amp; input_gradient, Vec&amp; input_diag_hessian, 
+                              bool accumulate=false);
 
     //! Does nothing
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; target, real cost)


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000132.html">[Plearn-commits] r6683 - trunk/plearn/io
</A></li>
	<LI>Next message: <A HREF="000134.html">[Plearn-commits] r6685 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#133">[ date ]</a>
              <a href="thread.html#133">[ thread ]</a>
              <a href="subject.html#133">[ subject ]</a>
              <a href="author.html#133">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
