<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6636 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-February/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6636%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702072108.l17L8pwE000391%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000084.html">
   <LINK REL="Next"  HREF="000086.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6636 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6636%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702072108.l17L8pwE000391%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6636 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Wed Feb  7 22:08:51 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000084.html">[Plearn-commits] r6635 - in trunk: commands plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000086.html">[Plearn-commits] r6637 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#85">[ date ]</a>
              <a href="thread.html#85">[ thread ]</a>
              <a href="subject.html#85">[ subject ]</a>
              <a href="author.html#85">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-02-07 22:08:50 +0100 (Wed, 07 Feb 2007)
New Revision: 6636

Modified:
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
Log:
Don't copy the input at each fprop() or bpropUpdate() anymore
Does the updates due to weight decay in the same loop, in bpropUpdate()


Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-06 20:46:17 UTC (rev 6635)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-02-07 21:08:50 UTC (rev 6636)
@@ -48,18 +48,19 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     GradNNetLayerModule,
-    &quot;Neural Network layer, using stochastic gradient to update neuron weights&quot;,
-    &quot;       Output = weights * (1,Input)\n&quot;
-    &quot;Weights are updated by online gradient with learning rate possibly decreasing\n&quot;
-    &quot;in 1/(1 + n_updates_done_up_to_now * decrease_constant).\n&quot;
-    &quot;An L1 and/or L2 regularization penalty can be added to push weights to 0.\n&quot;
+    &quot;Affine transformation module, with stochastic gradient descent updates&quot;,
+    &quot;Neural Network layer, using stochastic gradient to update neuron weights\n&quot;
+    &quot;       Output = weights * Input + bias\n&quot;
+    &quot;Weights and bias are updated by online gradient descent, with learning\n&quot;
+    &quot;rate possibly decreasing in 1/(1 + n_updates_done * decrease_constant).\n&quot;
+    &quot;An L1 and L2 regularization penalty can be added to push weights to 0.\n&quot;
     &quot;Weights can be initialized to 0, to a given initial matrix, or randomly\n&quot;
     &quot;from a uniform distribution.\n&quot;
     );
 
 GradNNetLayerModule::GradNNetLayerModule():
     start_learning_rate( .001 ),
-    decrease_constant( 0 ),
+    decrease_constant( 0. ),
     init_weights_random_scale( 1. ),
     L1_penalty_factor( 0. ),
     L2_penalty_factor( 0. ),
@@ -70,246 +71,178 @@
 // Applies linear transformation
 void GradNNetLayerModule::fprop(const Vec&amp; input, Vec&amp; output) const
 {
-    int in_size = input.size();
+    PLASSERT_MSG( input.size() == input_size,
+                  &quot;input.size() should be equal to this-&gt;input_size&quot; );
+
     output.resize( output_size );
 
-    // size check
-    if( in_size != input_size )
-    {
-        PLERROR(&quot;GradNNetLayerModule::fprop: 'input.size()' should be equal\n&quot;
-                &quot; to 'input_size' (%i != %i)\n&quot;, in_size, input_size);
-    }
-
-    bias_input.subVec( 1, input_size ) &lt;&lt; input;
-
-    product( output, weights, bias_input );
-
+    for( int i=0 ; i&lt;output_size ; i++ )
+        output[i] = dot( weights(i), input ) + bias[i];
 }
 
+// We are not using blas routines anymore, because we would iterate several
+// times over the weight matrix.
 void GradNNetLayerModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                       const Vec&amp; output_gradient)
 {
-    int in_size = input.size();
-    int out_size = output.size();
-    int og_size = output_gradient.size();
+    PLASSERT_MSG( input.size() == input_size,
+                  &quot;input.size() should be equal to this-&gt;input_size&quot; );
+    PLASSERT_MSG( output.size() == output_size,
+                  &quot;output.size() should be equal to this-&gt;output_size&quot; );
+    PLASSERT_MSG( output_gradient.size() == output_size,
+                  &quot;output_gradient.size() should be equal to this-&gt;output_size&quot;
+                );
 
-    // size check
-    if( in_size != input_size )
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
+
+    for( int i=0; i&lt;output_size; i++ )
     {
-        PLERROR(&quot;GradNNetLayerModule::bpropUpdate: 'input.size()' should be&quot;
-                &quot; equal\n&quot;
-                &quot; to 'input_size' (%i != %i)\n&quot;, in_size, input_size);
-    }
-    if( out_size != output_size )
-    {
-        PLERROR(&quot;GradNNetLayerModule::bpropUpdate: 'output.size()' should be&quot;
-                &quot; equal\n&quot;
-                &quot; to 'output_size' (%i != %i)\n&quot;, out_size, output_size);
-    }
-    if( og_size != output_size )
-    {
-        PLERROR(&quot;GradNNetLayerModule::bpropUpdate: 'output_gradient.size()'&quot;
-                &quot; should\n&quot;
-                &quot; be equal to 'output_size' (%i != %i)\n&quot;,
-                og_size, output_size);
-    }
+        real og_i = output_gradient[i];
+        real* w_ = weights[i];
 
-    learning_rate = start_learning_rate / ( 1+decrease_constant*step_number);
+        real delta_L1 = learning_rate * L1_penalty_factor;
+        real delta_L2 = learning_rate * L2_penalty_factor;
+        if( delta_L2 &gt; 1 )
+            PLWARNING(&quot;GradNNetLayerModule::bpropUpdate:\n&quot;
+                      &quot;learning rate = %f is too large!\n&quot;, learning_rate);
 
-    bias_input.subVec( 1,input_size ) &lt;&lt; input;
+        bias[i] -= learning_rate * og_i;
 
-    externalProductScaleAcc( weights, output_gradient, bias_input,
-                             -learning_rate );
-
-    if (L1_penalty_factor!=0)
-    {
-        real delta = learning_rate * L1_penalty_factor;
-        for (int i=0;i&lt;output_size;i++)
+        for( int j=0; j&lt;input_size; j++ )
         {
-            real* Wi = weights[i]+1; // don't apply penalty on bias
-            for (int j=0;j&lt;input_size;j++)
+            w_[j] -= learning_rate * input[j] * og_i;
+
+            if( delta_L1 &gt; 0. )
             {
-                real Wij =  Wi[j];
-                if (Wij&gt;delta)
-                    Wi[j] -=delta;
-                else if (Wij&lt;-delta)
-                    Wi[j] +=delta;
+                if( w_[j] &gt; delta_L1 )
+                    w_[j] -= delta_L1;
+                else if( w_[j] &lt; -delta_L1 )
+                    w_[j] += delta_L1;
                 else
-                    Wi[j]=0;
+                    w_[j] = 0.;
             }
+
+            if( delta_L2 &gt; 0. )
+                w_[j] *= (1 - delta_L2);
         }
     }
-    if (L2_penalty_factor!=0)
-    {
-        real delta = learning_rate*L2_penalty_factor;
-        if (delta&gt;1)
-            PLWARNING(&quot;GradNNetLayerModule::bpropUpdate: learning rate = %f is too large!&quot;,learning_rate);
-        weights.subMatColumns(1,input_size) *= 1 - delta; // no weight decay on the bias
-    }
-
     step_number++;
-
 }
 
 
 // Simply updates and propagates back gradient
-// PA - the original version of this function propagated the error to the inputs,
-// then called the above bpropUpdate() - this proved inefficient as the weight
-// matrix had to be iterated over twice.
-// However, since we're not using blas anymore, the speedup is only when
-// compiled in optimized mode (ie debug is much slower).
 void GradNNetLayerModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                       Vec&amp; input_gradient,
                                       const Vec&amp; output_gradient)
 {
+    PLASSERT_MSG( input.size() == input_size,
+                  &quot;input.size() should be equal to this-&gt;input_size&quot; );
+    PLASSERT_MSG( output.size() == output_size,
+                  &quot;output.size() should be equal to this-&gt;output_size&quot; );
+    PLASSERT_MSG( output_gradient.size() == output_size,
+                  &quot;output_gradient.size() should be equal to this-&gt;output_size&quot;
+                );
 
-#ifdef BOUNDCHECK
-    int in_size = input.size();
-    int out_size = output.size();
-    int og_size = output_gradient.size();
-
-    // size check
-    if( in_size != input_size )
-    {
-        PLERROR(&quot;GradNNetLayerModule::bpropUpdate: 'input.size()' should be&quot;
-                &quot; equal\n&quot;
-                &quot; to 'input_size' (%i != %i)\n&quot;, in_size, input_size);
-    }
-    if( out_size != output_size )
-    {
-        PLERROR(&quot;GradNNetLayerModule::bpropUpdate: 'output.size()' should be&quot;
-                &quot; equal\n&quot;
-                &quot; to 'output_size' (%i != %i)\n&quot;, out_size, output_size);
-    }
-    if( og_size != output_size )
-    {
-        PLERROR(&quot;GradNNetLayerModule::bpropUpdate: 'output_gradient.size()'&quot;
-                &quot; should\n&quot;
-                &quot; be equal to 'output_size' (%i != %i)\n&quot;,
-                og_size, output_size);
-    }
-#endif
-
     input_gradient.resize( input_size );
     input_gradient.clear();
 
-    learning_rate = start_learning_rate / ( 1+decrease_constant*step_number);
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
 
-    bias_input.subVec( 1,input_size ) &lt;&lt; input;
-
-    for(int i=0; i&lt;output_size; i++)  {
-
+    for( int i=0; i&lt;output_size; i++ )
+    {
         real og_i = output_gradient[i];
         real* w_ = weights[i];
 
-        w_[0] -= learning_rate * og_i; // * bias_input[0], ie 1
-        for(int j=0, jj=1; j&lt;input_size; j++, jj++) {
-            input_gradient[j] += w_[jj] * og_i;
-            w_[jj] -= learning_rate * bias_input[jj] * og_i;
-        }
-    }
+        real delta_L1 = learning_rate * L1_penalty_factor;
+        real delta_L2 = learning_rate * L2_penalty_factor;
+        if( delta_L2 &gt; 1 )
+            PLWARNING(&quot;GradNNetLayerModule::bpropUpdate:\n&quot;
+                      &quot;learning rate = %f is too large!\n&quot;, learning_rate);
 
-    // TODO When weight decays, combine penalties using similar loop as above!
+        bias[i] -= learning_rate * og_i;
 
-    if (L1_penalty_factor!=0)
-    {
-        real delta = learning_rate * L1_penalty_factor;
-        for (int i=0;i&lt;output_size;i++)
+        for( int j=0; j&lt;input_size; j++ )
         {
-            real* Wi = weights[i]+1; // don't apply penalty on bias
-            for (int j=0;j&lt;input_size;j++)
+            input_gradient[j] += w_[j] * og_i;
+            w_[j] -= learning_rate * input[j] * og_i;
+
+            if( delta_L1 &gt; 0. )
             {
-                real Wij =  Wi[j];
-                if (Wij&gt;delta)
-                    Wi[j] -=delta;
-                else if (Wij&lt;-delta)
-                    Wi[j] +=delta;
+                if( w_[j] &gt; delta_L1 )
+                    w_[j] -= delta_L1;
+                else if( w_[j] &lt; -delta_L1 )
+                    w_[j] += delta_L1;
                 else
-                    Wi[j]=0;
+                    w_[j] = 0.;
             }
+
+            if( delta_L2 &gt; 0. )
+                w_[j] *= (1 - delta_L2);
         }
     }
-    if (L2_penalty_factor!=0)
-    {
-        real delta = learning_rate*L2_penalty_factor;
-        if (delta&gt;1)
-            PLWARNING(&quot;GradNNetLayerModule::bpropUpdate: learning rate = %f is too large!&quot;,learning_rate);
-        weights.subMatColumns(1,input_size) *= 1 - delta; // no weight decay on the bias
-    }
-
     step_number++;
-
-
-
-    ///***OLD VERSION***
-    // compute input_gradient from initial weights
-    //input_gradient.resize( input_size );
-    //transposeProduct( input_gradient,
-    //                  weights.subMatColumns(1,input_size), output_gradient );
-    // do the update (and size check)
-    //bpropUpdate( input, output, output_gradient );
-    ///***OLD VERSION - END***
-
 }
 
-
-
 // Update
 void GradNNetLayerModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                        const Vec&amp; output_gradient,
                                        const Vec&amp; output_diag_hessian)
 {
-    int odh_size = output_diag_hessian.size();
-    if( odh_size != output_size )
-    {
-        PLERROR(&quot;GradNNetLayerModule::bbpropUpdate:&quot;
-                &quot; 'output_diag_hessian.size()'\n&quot;
-                &quot; should be equal to 'output_size' (%i != %i)\n&quot;,
-                odh_size, output_size);
-    }
-
+    PLASSERT_MSG( output_diag_hessian.size() == output_size,
+                  &quot;output_diag_hessian.size() should be equal to&quot;
+                  &quot; this-&gt;output_size&quot; );
     bpropUpdate( input, output, output_gradient );
-
 }
 
+/* This implementation is incorrect. Let the PLERROR defined in parent version
 // Propagates back output_gradient and output_diag_hessian
 void GradNNetLayerModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
-                              Vec&amp;  input_gradient,
-                              const Vec&amp; output_gradient,
-                              Vec&amp;  input_diag_hessian,
-                              const Vec&amp; output_diag_hessian)
+                                       Vec&amp;  input_gradient,
+                                       const Vec&amp; output_gradient,
+                                       Vec&amp;  input_diag_hessian,
+                                       const Vec&amp; output_diag_hessian)
 {
     bpropUpdate( input, output, input_gradient, output_gradient );
 }
+*/
 
-
-// Nothing to forget
+// Forget the bias and reinitialize the weights
 void GradNNetLayerModule::forget()
 {
-    weights.resize( output_size, 1+input_size );
+    bias.resize( output_size );
+    if( init_bias.size() &gt; 0 )
+    {
+        if( init_bias.size() != output_size )
+            PLERROR( &quot;init_bias (%d) should have length equal to output_size (%d)&quot;,
+                     init_bias.size(), output_size );
+        bias &lt;&lt; init_bias;
+    }
+    else
+        bias.clear();
 
-    if( init_weights.size() !=0 )
+    weights.resize( output_size, input_size );
+    if( init_weights.size() &gt; 0 )
+    {
+        if( weights.length() != output_size || weights.width() != input_size )
+            PLERROR( &quot;weights (%d,%d) should have size equal to (output_size, input_size) (%d,%d)&quot;,
+                     weights.length(), weights.width(),
+                     output_size, input_size );
+
         weights &lt;&lt; init_weights;
-    else if (init_weights_random_scale!=0)
+    }
+    else if(init_weights_random_scale != 0. )
     {
         real r = init_weights_random_scale / input_size;
-        random_gen-&gt;fill_random_uniform(weights.subMatColumns(1,input_size),
-                                        -r,r);
-        weights.subMatColumns(0,1).fill(0);
+        random_gen-&gt;fill_random_uniform(weights, -r, r);
     }
     else
-        weights.fill(0);
+        weights.clear();
 
     learning_rate = start_learning_rate;
     step_number = 0;
-
-    bias_input.resize( 1+input_size );
-    bias_input[0] = 1;
-
 }
 
 
-// ### Nothing to add here, simply calls build_
 void GradNNetLayerModule::build()
 {
     inherited::build();
@@ -321,8 +254,9 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(init_weights, copies);
+    deepCopyField(init_bias, copies);
     deepCopyField(weights, copies);
-    deepCopyField(bias_input, copies);
+    deepCopyField(bias, copies);
 }
 
 void GradNNetLayerModule::declareOptions(OptionList&amp; ol)
@@ -339,23 +273,29 @@
 
     declareOption(ol, &quot;init_weights&quot;, &amp;GradNNetLayerModule::init_weights,
                   OptionBase::buildoption,
-                  &quot;Optional initial weights of the neurons (bias on first column,\n&quot;
-                  &quot;one row per neuron. If not provided then weights are initialized\n&quot;
-                  &quot;according to a uniform distribution (see init_weights_random_scale)\n&quot;
-                  &quot;and biases are initialized to 0.\n&quot;);
+                  &quot;Optional initial weights of the neurons (one row per neuron).\n&quot;
+                  &quot;If not provided then weights are initialized according to a uniform\n&quot;
+                  &quot;distribution (see init_weights_random_scale)\n&quot;);
 
-    declareOption(ol, &quot;init_weights_random_scale&quot;, &amp;GradNNetLayerModule::init_weights_random_scale,
+    declareOption(ol, &quot;init_bias&quot;, &amp;GradNNetLayerModule::init_bias,
                   OptionBase::buildoption,
+                  &quot;Optional initial bias of the neurons. If not provided, they are set to 0.\n&quot;);
+
+    declareOption(ol, &quot;init_weights_random_scale&quot;,
+                  &amp;GradNNetLayerModule::init_weights_random_scale,
+                  OptionBase::buildoption,
                   &quot;If init_weights is not provided, the weights are initialized randomly\n&quot;
                   &quot;from a uniform in [-r,r], with r = init_weights_random_scale/input_size.\n&quot;
                   &quot;To clear the weights initially, just set this option to 0.&quot;);
 
-    declareOption(ol, &quot;L1_penalty_factor&quot;, &amp;GradNNetLayerModule::L1_penalty_factor,
+    declareOption(ol, &quot;L1_penalty_factor&quot;,
+                  &amp;GradNNetLayerModule::L1_penalty_factor,
                   OptionBase::buildoption,
                   &quot;Optional (default=0) factor of L1 regularization term, i.e.\n&quot;
                   &quot;minimize L1_penalty_factor * sum_{ij} |weights(i,j)| during training.\n&quot;);
 
-    declareOption(ol, &quot;L2_penalty_factor&quot;, &amp;GradNNetLayerModule::L2_penalty_factor,
+    declareOption(ol, &quot;L2_penalty_factor&quot;,
+                  &amp;GradNNetLayerModule::L2_penalty_factor,
                   OptionBase::buildoption,
                   &quot;Optional (default=0) factor of L2 regularization term, i.e.\n&quot;
                   &quot;minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 during training.\n&quot;);
@@ -363,9 +303,12 @@
 
     declareOption(ol, &quot;weights&quot;, &amp;GradNNetLayerModule::weights,
                   OptionBase::learntoption,
-                  &quot;Input weights of the neurons (bias on first column,&quot;
-                  &quot; one row per neuron&quot; );
+                  &quot;Input weights of the neurons (one row per neuron)&quot;);
 
+    declareOption(ol, &quot;bias&quot;, &amp;GradNNetLayerModule::bias,
+                  OptionBase::learntoption,
+                  &quot;Bias of the neurons&quot;);
+
     inherited::declareOptions(ol);
 }
 
@@ -382,9 +325,12 @@
     if (init_weights.size()==0 &amp;&amp; init_weights_random_scale!=0 &amp;&amp; !random_gen)
         random_gen = new PRandom();
 
-    if( weights.length() != output_size || weights.width() != 1+input_size )
+    if( weights.length() != output_size
+        || weights.width() != input_size
+        || bias.size() != output_size )
+    {
         forget();
-
+    }
 }
 
 

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-06 20:46:17 UTC (rev 6635)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-02-07 21:08:50 UTC (rev 6636)
@@ -51,8 +51,15 @@
 namespace PLearn {
 
 /**
- * This class
+ * Affine transformation module, with stochastic gradient descent updates.
  *
+ * Neural Network layer, using stochastic gradient to update neuron weights,
+ *      Output = weights * Input + bias
+ * Weights and bias are updated by online gradient descent, with learning
+ * rate possibly decreasing in 1/(1 + n_updates_done * decrease_constant).
+ * An L1 and L2 regularization penalty can be added to push weights to 0.
+ * Weights can be initialized to 0, to a given initial matrix, or randomly
+ * from a uniform distribution.
  *
  */
 class GradNNetLayerModule : public OnlineLearningModule
@@ -69,10 +76,12 @@
     //! where t is the number of updates since the beginning
     real decrease_constant;
 
-    //! Optional initial weights of the neurons (bias on first column,
-    //! one row per neuron.
+    //! Optional initial weights of the neurons (one row per neuron).
     Mat init_weights;
 
+    //! Optional initial bias of the neurons.
+    Vec init_bias;
+
     //! If init_weights is not provided, the weights are initialized randomly
     //! from a uniform in [-r,r], with r = init_weights_random_scale/input_size
     real init_weights_random_scale;
@@ -83,9 +92,12 @@
     //! Optional (default=0) factor of L2 regularization term
     real L2_penalty_factor;
 
-    //! The weights, one neuron per line, bias first
+    //! The weights, one neuron per line
     Mat weights;
 
+    //! The bias
+    Vec bias;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -123,7 +135,6 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
 
 protected:
@@ -147,8 +158,6 @@
     // The rest of the private stuff goes here
     real learning_rate;
     int step_number;
-    mutable Vec bias_input;
-
 };
 
 // Declares a few other classes and functions related to this class


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000084.html">[Plearn-commits] r6635 - in trunk: commands plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000086.html">[Plearn-commits] r6637 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#85">[ date ]</a>
              <a href="thread.html#85">[ thread ]</a>
              <a href="subject.html#85">[ subject ]</a>
              <a href="author.html#85">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
