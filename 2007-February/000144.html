<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6695 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-February/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6695%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702272337.l1RNbjh6008007%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000143.html">
   <LINK REL="Next"  HREF="000145.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6695 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6695%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200702272337.l1RNbjh6008007%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6695 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Wed Feb 28 00:37:45 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000143.html">[Plearn-commits] r6694 - in trunk: commands plearn_learners/generic	plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000145.html">[Plearn-commits] r6696 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#144">[ date ]</a>
              <a href="thread.html#144">[ thread ]</a>
              <a href="subject.html#144">[ subject ]</a>
              <a href="author.html#144">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-02-28 00:37:43 +0100 (Wed, 28 Feb 2007)
New Revision: 6695

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/OnlineLearningModule.cc
Log:
Various minor improvements


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 22:55:35 UTC (rev 6694)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-02-27 23:37:43 UTC (rev 6695)
@@ -333,6 +333,7 @@
     MODULE_LOG &lt;&lt; &quot;build_final_cost() called&quot; &lt;&lt; endl;
 
     final_cost_gradient.resize( final_cost-&gt;input_size );
+    final_cost-&gt;setLearningRate( grad_learning_rate );
 
     if( final_module )
     {
@@ -348,17 +349,7 @@
                     &quot;\n&quot;, n_layers-1, layers[n_layers-1]-&gt;size,
                     final_module-&gt;input_size);
 
-        Object* obj = 0;
-        OptionList::iterator it;
-        string opt_id;
-
-        // try to see if final_cost has an option named &quot;learning_rate&quot;
-        final_module_has_learning_rate =
-            final_cost-&gt;parseOptionName( &quot;learning_rate&quot;, obj, it, opt_id );
-
-        final_cost_has_learning_rate = final_module &amp;&amp;
-            final_module-&gt;parseOptionName( &quot;learning_rate&quot;, obj, it, opt_id );
-
+        final_module-&gt;setLearningRate( grad_learning_rate );
     }
     else
     {
@@ -368,10 +359,33 @@
                     &quot;\n&quot;, n_layers-1, layers[n_layers-1]-&gt;size,
                     final_cost-&gt;input_size);
     }
-    // TODO: check target size
+
+    // check target size and final_cost-&gt;input_size
+    if( n_classes == 0 ) // regression
+    {
+        if( final_cost-&gt;input_size != targetsize() )
+            PLERROR(&quot;DeepBeliefNet::build_final_cost() - \n&quot;
+                    &quot;final_cost-&gt;input_size (%d) != targetsize() (%d),\n&quot;
+                    &quot;although we are doing regression (n_classes == 0).\n&quot;,
+                    final_cost-&gt;input_size, targetsize());
+    }
+    else
+    {
+        if( final_cost-&gt;input_size != n_classes )
+            PLERROR(&quot;DeepBeliefNet::build_final_cost() - \n&quot;
+                    &quot;final_cost-&gt;input_size (%d) != n_classes (%d),\n&quot;
+                    &quot;although we are doing classification (n_classes != 0).\n&quot;,
+                    final_cost-&gt;input_size, n_classes);
+
+        if( targetsize() != 1 )
+            PLERROR(&quot;DeepBeliefNet::build_final_cost() - \n&quot;
+                    &quot;targetsize() (%d) != 1,\n&quot;
+                    &quot;although we are doing regression (n_classes == 0).\n&quot;,
+                    targetsize());
+    }
+
 }
 
-// ### Nothing to add here, simply calls build_
 void DeepBeliefNet::build()
 {
     inherited::build();
@@ -421,10 +435,6 @@
         out_size += layers[n_layers-1]-&gt;size;
 
     return out_size;
-/*
-    return (n_classes &gt; 0) ? n_classes
-                           : targetsize();
-*/
 }
 
 void DeepBeliefNet::forget()
@@ -602,7 +612,7 @@
         }
 
         // possible supervised part
-        if(use_classification_cost )
+        if( use_classification_cost )
         {
             MODULE_LOG &lt;&lt; &quot;Training the classification module&quot; &lt;&lt; endl;
 
@@ -617,7 +627,10 @@
                                       end_stage - stage );
 
             // set appropriate learning rate
-            setLearningRate( cd_learning_rate );
+            joint_layer-&gt;setLearningRate( cd_learning_rate );
+            classification_module-&gt;joint_connection-&gt;setLearningRate(
+                cd_learning_rate );
+            layers[ n_layers-1 ]-&gt;setLearningRate( cd_learning_rate );
 
             int previous_stage = (n_layers &lt; 3) ? 0
                 : training_schedule[n_layers-3];
@@ -903,7 +916,7 @@
     PLASSERT( index &lt; n_layers );
 
     layers[0]-&gt;expectation &lt;&lt; input;
-    for( int i=0 ; i&lt;index ; i++ )
+    for( int i=0 ; i&lt;=index ; i++ )
     {
         connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
         layers[i+1]-&gt;getAllActivations( connections[i] );
@@ -913,11 +926,6 @@
     // TODO: add another learning rate?
     if( partial_costs &amp;&amp; partial_costs[ index ] )
     {
-        // Deterministic forward pass
-        connections[ index ]-&gt;setAsDownInput( layers[ index ]-&gt;expectation );
-        layers[ index+1 ]-&gt;getAllActivations( connections[ index ] );
-        layers[ index+1 ]-&gt;computeExpectation();
-
         // put appropriate learning rate
         connections[ index ]-&gt;setLearningRate( grad_learning_rate );
         layers[ index+1 ]-&gt;setLearningRate( grad_learning_rate );
@@ -949,7 +957,8 @@
 
     contrastiveDivergenceStep( layers[ index ],
                                connections[ index ],
-                               layers[ index+1 ] );
+                               layers[ index+1 ],
+                               true );
 }
 
 void DeepBeliefNet::jointGreedyStep( const Vec&amp; input, const Vec&amp; target )
@@ -964,21 +973,16 @@
         layers[i+1]-&gt;computeExpectation();
     }
 
-    Vec target_exp = classification_module-&gt;target_layer-&gt;expectation;
-    fill_one_hot( target_exp, (int) round(target[0]), 0., 1. );
-
     if( partial_costs &amp;&amp; partial_costs[ n_layers-2 ] )
     {
         // Deterministic forward pass
-        classification_module-&gt;joint_connection-&gt;setAsDownInput(
-            joint_layer-&gt;expectation );
-        layers[ n_layers-1 ]-&gt;getAllActivations(
-            get_pointer( classification_module-&gt;joint_connection ) );
+        connections[ n_layers-2 ]-&gt;setAsDownInput(
+            layers[ n_layers-2 ]-&gt;expectation );
+        layers[ n_layers-1 ]-&gt;getAllActivations( connections[ n_layers-2 ] );
         layers[ n_layers-1 ]-&gt;computeExpectation();
 
         // put appropriate learning rate
-        classification_module-&gt;previous_to_last-&gt;setLearningRate(
-            grad_learning_rate );
+        connections[ n_layers-2 ]-&gt;setLearningRate( grad_learning_rate );
         layers[ n_layers-1 ]-&gt;setLearningRate( grad_learning_rate );
 
         // Backward pass
@@ -996,18 +1000,20 @@
                                            expectation_gradients[ n_layers-1 ]
                                          );
 
-        classification_module-&gt;previous_to_last-&gt;bpropUpdate(
+        connections[ n_layers-2 ]-&gt;bpropUpdate(
             layers[ n_layers-2 ]-&gt;expectation,
             layers[ n_layers-1 ]-&gt;activation,
             expectation_gradients[ n_layers-2 ],
             activation_gradients[ n_layers-1 ] );
 
         // put back old learning rate
-        classification_module-&gt;previous_to_last-&gt;setLearningRate(
-            cd_learning_rate );
+        connections[ n_layers-2 ]-&gt;setLearningRate( cd_learning_rate );
         layers[ n_layers-1 ]-&gt;setLearningRate( cd_learning_rate );
     }
 
+    Vec target_exp = classification_module-&gt;target_layer-&gt;expectation;
+    fill_one_hot( target_exp, (int) round(target[0]), 0., 1. );
+
     contrastiveDivergenceStep(
         get_pointer( joint_layer ),
         get_pointer( classification_module-&gt;joint_connection ),
@@ -1271,17 +1277,19 @@
     if( final_cost )
         cost_names.append( final_cost-&gt;name() );
 
-    if (partial_costs)
-        for (int i=0;i&lt;n_layers-1;i++)
-            if (partial_costs[i])
-                cost_names.append( partial_costs[i]-&gt;name()[0] + &quot;_&quot; + tostring(i+1) );
-            
     return cost_names;
 }
 
 TVec&lt;string&gt; DeepBeliefNet::getTrainCostNames() const
 {
     TVec&lt;string&gt; cost_names = getTestCostNames() ;
+
+    if (partial_costs)
+        for (int i=0;i&lt;n_layers-1;i++)
+            if (partial_costs[i])
+                cost_names.append( partial_costs[i]-&gt;name()[0]
+                                   + &quot;_&quot; + tostring(i+1) );
+
     cost_names.append(&quot;recons_error&quot;);
     return cost_names;
 }
@@ -1304,14 +1312,6 @@
             the_learning_rate );
         joint_layer-&gt;setLearningRate( the_learning_rate );
     }
-
-    if( final_module_has_learning_rate )
-        final_cost-&gt;setOption( &quot;learning_rate&quot;,
-                               tostring(the_learning_rate ) );
-
-    if( final_cost_has_learning_rate )
-        final_cost-&gt;setOption( &quot;learning_rate&quot;,
-                               tostring(the_learning_rate ) );
 }
 
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 22:55:35 UTC (rev 6694)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-02-27 23:37:43 UTC (rev 6695)
@@ -130,6 +130,7 @@
     bool online;
 
     //! Wether we do a step of joint contrastive divergence on top-layer
+    //! Only used if online for the moment
     bool top_layer_joint_cd;
 
     //#####  Not Options  #####################################################

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-02-27 22:55:35 UTC (rev 6694)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-02-27 23:37:43 UTC (rev 6695)
@@ -109,10 +109,10 @@
 
 void OnlineLearningModule::setLearningRate( real dynamic_learning_rate )
 {
-    PLERROR(&quot;OnlineLearningModule does not have a learning rate that can be\n&quot;
-            &quot;changed from outside.\n&quot;
-            &quot;If your derived class has one, please implement setLearningrate()&quot;
-            &quot; in it.\n&quot;);
+    PLWARNING(&quot;OnlineLearningModule does not have a learning rate that can be\n&quot;
+              &quot;changed from outside.\n&quot;
+              &quot;If your derived class has one, please implement setLearningrate()&quot;
+              &quot; in it.\n&quot;);
 }
 
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000143.html">[Plearn-commits] r6694 - in trunk: commands plearn_learners/generic	plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000145.html">[Plearn-commits] r6696 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#144">[ date ]</a>
              <a href="thread.html#144">[ thread ]</a>
              <a href="subject.html#144">[ subject ]</a>
              <a href="author.html#144">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
