From tihocan at mail.berlios.de  Thu Jan  3 20:24:53 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 3 Jan 2008 20:24:53 +0100
Subject: [Plearn-commits] r8374 - trunk/plearn_learners/classifiers
Message-ID: <200801031924.m03JOr29009860@sheep.berlios.de>

Author: tihocan
Date: 2008-01-03 20:24:52 +0100 (Thu, 03 Jan 2008)
New Revision: 8374

Modified:
   trunk/plearn_learners/classifiers/ToBagClassifier.cc
   trunk/plearn_learners/classifiers/ToBagClassifier.h
Log:
Can now compute the confusion matrix

Modified: trunk/plearn_learners/classifiers/ToBagClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.cc	2007-12-28 20:54:06 UTC (rev 8373)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.cc	2008-01-03 19:24:52 UTC (rev 8374)
@@ -50,25 +50,28 @@
     "For testing, a majority vote is performed on each bag: assuming the\n"
     "inner learner's output is made of the probabilities for each class,\n"
     "these probabilities are summed over a full bag, and the class with\n"
-    "highest sum is taken as prediction.");
+    "highest sum is taken as prediction.\n"
+    "This learner can also compute the confusion matrix as a test cost, in\n"
+    "addition to classification error. Each element of the confusion matrix\n"
+    "is named 'cm_ij' with i the index of the true class, and j the index of\n"
+    "the predicted class.");
 
 /////////////////////
 // ToBagClassifier //
 /////////////////////
-ToBagClassifier::ToBagClassifier()
-{
-}
+ToBagClassifier::ToBagClassifier():
+    n_classes(-1)
+{}
 
 ////////////////////
 // declareOptions //
 ////////////////////
 void ToBagClassifier::declareOptions(OptionList& ol)
 {
-    // ### ex:
-    // declareOption(ol, "myoption", &ToBagClassifier::myoption,
-    //               OptionBase::buildoption,
-    //               "Help text describing this option");
-    // ...
+    declareOption(ol, "n_classes", &ToBagClassifier::n_classes,
+                  OptionBase::buildoption,
+        "Number of classes in the dataset. This option is required to\n"
+        "compute the confusion matrix, but may be ignored otherwise.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -101,22 +104,29 @@
     sub_target.resize(target.length() - 1);
     sub_target << target.subVec(0, sub_target.length());
     inherited::computeCostsFromOutputs(input, output, sub_target, costs);
+    PLASSERT( is_equal(sum(output), 1) );   // Ensure probabilities sum to 1.
     int bag_info = int(round(target.lastElement()));
     if (bag_info % 2 == 1)
         bag_output.resize(0, 0);
     bag_output.appendRow(output);
-    costs.resize(1);
+    costs.resize(nTestCosts());
+    costs.fill(MISSING_VALUE);
     if (bag_info >= 2) {
         // Perform majority vote.
         votes.resize(bag_output.width());
         columnSum(bag_output, votes);
         int target_class = int(round(target[0]));
-        if (argmax(votes) == target_class)
+        int prediction = argmax(votes);
+        if (prediction == target_class)
             costs[0] = 0;
         else
             costs[0] = 1;
-    } else
-        costs.fill(MISSING_VALUE);
+        if (n_classes > 0) {
+            int i_start = 1 + target_class * n_classes;
+            costs.subVec(i_start, n_classes).fill(0);
+            costs[i_start + prediction] = 1;
+        }
+    }
 }
 
 //////////////////////
@@ -124,9 +134,12 @@
 //////////////////////
 TVec<string> ToBagClassifier::getTestCostNames() const
 {
-    static TVec<string> costs;
-    if (costs.isEmpty())
-        costs.append("class_error");
+    TVec<string> costs;
+    costs.append("class_error");
+    if (n_classes > 0)
+        for (int i = 0; i < n_classes; i++)
+            for (int j = 0; j < n_classes; j++)
+                costs.append("cm_" + tostring(i) + tostring(j));
     return costs;
 }
 

Modified: trunk/plearn_learners/classifiers/ToBagClassifier.h
===================================================================
--- trunk/plearn_learners/classifiers/ToBagClassifier.h	2007-12-28 20:54:06 UTC (rev 8373)
+++ trunk/plearn_learners/classifiers/ToBagClassifier.h	2008-01-03 19:24:52 UTC (rev 8374)
@@ -61,8 +61,7 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! ### declare public option fields (such as build options) here
-    //! Start your comments with Doxygen-compatible comments such as //!
+    int n_classes;
 
 public:
     //#####  Public Member Functions  #########################################



From plearner at mail.berlios.de  Thu Jan  3 23:01:52 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 3 Jan 2008 23:01:52 +0100
Subject: [Plearn-commits] r8375 - in trunk: commands/EXPERIMENTAL
	plearn/opt/EXPERIMENTAL plearn/var
	plearn_learners/classifiers/EXPERIMENTAL
	plearn_learners/generic/EXPERIMENTAL
	python_modules/plearn/var scripts scripts/EXPERIMENTAL
Message-ID: <200801032201.m03M1qC3020473@sheep.berlios.de>

Author: plearner
Date: 2008-01-03 23:01:45 +0100 (Thu, 03 Jan 2008)
New Revision: 8375

Added:
   trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.cc
   trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h
   trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.cc
   trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h
Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/plearn/var/TimesConstantVariable.cc
   trunk/plearn/var/TimesConstantVariable.h
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/var/Var.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
   trunk/scripts/pyplot
Log:
Experimental classes for learning with stochastic reconstruciton error (and localgaussian classifier)


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -119,6 +119,7 @@
 // #include <plearn/opt/AdaptGradientOptimizer.h>
 // #include <plearn/opt/ConjGradientOptimizer.h>
 #include <plearn/opt/GradientOptimizer.h>
+#include <plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h>
 
 // /****************
 //  * OptionOracle *
@@ -294,7 +295,7 @@
 // #include <plearn/vmat/RandomSamplesVMatrix.h>
 // #include <plearn/vmat/RandomSamplesFromVMatrix.h>
 // #include <plearn/vmat/RankedVMatrix.h>
-// #include <plearn/vmat/RegularGridVMatrix.h>
+#include <plearn/vmat/RegularGridVMatrix.h>
 // #include <plearn/vmat/RemoveDuplicateVMatrix.h>
 // #include <plearn/vmat/ReorderByMissingVMatrix.h>
 // //#include <plearn/vmat/SelectAttributsSequenceVMatrix.h>
@@ -382,10 +383,16 @@
 #include <plearn/var/EXPERIMENTAL/RandomForcedValuesVariable.h>
 #include <plearn/var/EXPERIMENTAL/BernoulliSampleVariable.h>
 #include <plearn/var/EXPERIMENTAL/TimesConstantScalarVariable2.h>
+#include <plearn/var/PlusConstantVariable.h>
+#include <plearn/var/TimesConstantVariable.h>
 
 // Stuff used for transformationLearner experiments
 #include <plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h>
 
+// Stuff used for local Gaussian classifier
+#include <plearn_learners/classifiers/KNNClassifier.h>
+#include <plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h>
+
 using namespace PLearn;
 
 int main(int argc, char** argv)

Added: trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.cc
===================================================================
--- trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -0,0 +1,272 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2002 Pascal Vincent and Yoshua Bengio
+// Copyright (C) 1999-2002, 2006 University of Montreal
+//
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+ 
+
+/* *******************************************************      
+ * $Id: AutoScaledGradientOptimizer.cc 5852 2006-06-14 14:40:03Z larocheh $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#define PL_LOG_MODULE_NAME "AutoScaledGradientOptimizer"
+
+#include "AutoScaledGradientOptimizer.h"
+#include <plearn/io/pl_log.h>
+#include <plearn/math/TMat_maths.h>
+#include <plearn/display/DisplayUtils.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    AutoScaledGradientOptimizer,
+    "Optimization by gradient descent with adapted scaling for each parameter.", 
+    "This is a simple variation on the basic GradientOptimizer \n"
+    "in which the gradient is scaled elementwise (for each parameter) \n"
+    "by a scaling factor that is 1 over an average of the \n"
+    "absolute value of the gradient plus some small epsilon. \n"
+    "\n"
+);
+
+AutoScaledGradientOptimizer::AutoScaledGradientOptimizer():
+    learning_rate(0.),   
+    start_learning_rate(1e-2),
+    decrease_constant(0),
+    verbosity(0),
+    evaluate_scaling_every(1000),
+    evaluate_scaling_during(1000),
+    epsilon(1e-6),
+    nsteps_remaining_for_evaluation(-1)
+{}
+
+
+void AutoScaledGradientOptimizer::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "start_learning_rate", &AutoScaledGradientOptimizer::start_learning_rate,
+        OptionBase::buildoption, 
+        "The initial learning rate\n");
+
+    declareOption(
+        ol, "learning_rate", &AutoScaledGradientOptimizer::learning_rate,
+        OptionBase::learntoption, 
+        "The current learning rate\n");
+
+    declareOption(
+        ol, "decrease_constant", &AutoScaledGradientOptimizer::decrease_constant,
+        OptionBase::buildoption, 
+        "The learning rate decrease constant \n");
+
+    declareOption(
+        ol, "lr_schedule", &AutoScaledGradientOptimizer::lr_schedule,
+        OptionBase::buildoption, 
+        "Fixed schedule instead of decrease_constant. This matrix has 2 columns: iteration_threshold \n"
+        "and learning_rate_factor. As soon as the iteration number goes above the iteration_threshold,\n"
+        "the corresponding learning_rate_factor is applied (multiplied) to the start_learning_rate to\n"
+        "obtain the learning_rate.\n");
+
+    declareOption(
+        ol, "verbosity", &AutoScaledGradientOptimizer::verbosity,
+        OptionBase::buildoption, 
+        "Controls the amount of output.  If zero, does not print anything.\n"
+        "If 'verbosity'=V, print the current cost and learning rate if\n"
+        "\n"
+        "    stage % V == 0\n"
+        "\n"
+        "i.e. every V stages.  (Default=0)\n");
+
+    declareOption(
+        ol, "evaluate_scaling_every", &AutoScaledGradientOptimizer::evaluate_scaling_every,
+        OptionBase::buildoption, 
+        "every how-many steps should the mean and scaling be reevaluated\n");
+
+    declareOption(
+        ol, "evaluate_scaling_during", &AutoScaledGradientOptimizer::evaluate_scaling_during,
+        OptionBase::buildoption, 
+        "how many steps should be used to re-evaluate the mean and scaling\n");
+
+    declareOption(
+        ol, "epsilon", &AutoScaledGradientOptimizer::epsilon,
+        OptionBase::buildoption, 
+        "scaling will be 1/(mean_abs_grad + epsilon)\n");
+
+    inherited::declareOptions(ol);
+}
+
+
+void AutoScaledGradientOptimizer::setToOptimize(const VarArray& the_params, Var the_cost, VarArray the_other_costs, TVec<VarArray> the_other_params, real the_other_weight)
+{
+    inherited::setToOptimize(the_params, the_cost, the_other_costs, the_other_params, the_other_weight);
+    int n = params.nelems();
+    param_values = Vec(n);
+    param_gradients = Vec(n);
+    params.makeSharedValue(param_values);
+    params.makeSharedGradient(param_gradients);
+    scaling.resize(n);
+    scaling.clear();
+    if(epsilon<0)
+        scaling.fill(1.0);
+    meanabsgrad.resize(n);
+    meanabsgrad.clear();
+}
+
+
+// static bool displayvg=false;
+
+bool AutoScaledGradientOptimizer::optimizeN(VecStatsCollector& stats_coll) 
+{
+    PLASSERT_MSG(other_costs.length()==0, "gradient on other costs not currently supported");
+
+    param_gradients.clear();
+
+    int stage_max = stage + nstages; // the stage to reach
+
+    int current_schedule = 0;
+    int n_schedules = lr_schedule.length();
+    if (n_schedules>0)
+        while (current_schedule+1 < n_schedules && stage > lr_schedule(current_schedule,0))
+            current_schedule++;
+    
+    while (stage < stage_max) 
+    {        
+        if (n_schedules>0)
+        {
+            while (current_schedule+1 < n_schedules && stage > lr_schedule(current_schedule,0))
+                current_schedule++;
+            learning_rate = start_learning_rate * lr_schedule(current_schedule,1);
+        }
+        else
+            learning_rate = start_learning_rate/(1.0+decrease_constant*stage);
+
+        proppath.clearGradient();
+        cost->gradient[0] = 1.0;
+
+        static bool display_var_graph_before_fbprop=false;
+        if (display_var_graph_before_fbprop)
+            displayVarGraph(proppath, true, 333);
+        proppath.fbprop(); 
+#ifdef BOUNDCHECK
+        int np = params.size();
+        for(int i=0; i<np; i++)
+            if (params[i]->value.hasMissing())
+                PLERROR("parameter updated with NaN");
+#endif
+        static bool display_var_graph=false;
+        if (display_var_graph)
+            displayVarGraph(proppath, true, 333);
+
+//       // Debugging of negative NLL bug...
+//       if (cost->value[0] <= 0) {
+//         displayVarGraph(proppath, true, 333);
+//         cerr << "Negative NLL cost vector = " << cost << endl;
+//         PLERROR("Negative NLL encountered in optimization");
+//       }
+
+        // set params += -learning_rate * params.gradient * scaling
+        {
+        real* p_val = param_values.data();
+        real* p_grad = param_gradients.data();
+        real* p_scale = scaling.data();
+        real neg_learning_rate = -learning_rate;
+
+        int n = param_values.length();
+        while(n--)
+            *p_val++ += neg_learning_rate*(*p_grad++)*(*p_scale++);
+        }
+
+        if(stage%evaluate_scaling_every==0)
+        {
+            nsteps_remaining_for_evaluation = evaluate_scaling_during;
+            meanabsgrad.clear();
+            if(verbosity>=4)
+                perr << "At stage " << stage << " beginning evaluating meanabsgrad during " << evaluate_scaling_during << " stages" << endl;
+        }
+
+        if(nsteps_remaining_for_evaluation>0)
+        {
+            real* p_grad = param_gradients.data();
+            real* p_mean = meanabsgrad.data();
+            int n = param_gradients.length();
+            while(n--)
+                *p_mean++ += fabs(*p_grad++);
+            --nsteps_remaining_for_evaluation;
+
+            if(nsteps_remaining_for_evaluation==0) // finalize evaluation
+            {
+                int n = param_gradients.length();                
+                for(int i=0; i<n; i++)
+                {
+                    meanabsgrad[i] /= evaluate_scaling_during;
+                    scaling[i] = 1.0/(meanabsgrad[i]+epsilon);
+                }
+                if(verbosity>=4)
+                    perr << "At stage " << stage 
+                         << " finished evaluating meanabsgrad. It's in range: ( " 
+                         << min(meanabsgrad) << ",  " << max(meanabsgrad) << " )" << endl;
+                if(verbosity>=5)
+                    perr << meanabsgrad << endl;
+
+                if(epsilon<0)
+                    scaling.fill(1.0);
+            }
+        }
+        param_gradients.clear();
+
+        if (verbosity > 0 && stage % verbosity == 0) {
+            MODULE_LOG << "Stage " << stage << ": " << cost->value
+                       << "\tlr=" << learning_rate
+                       << endl;
+        }
+        stats_coll.update(cost->value);
+        ++stage;
+    }
+
+    return false;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h
===================================================================
--- trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h	2008-01-03 22:01:45 UTC (rev 8375)
@@ -0,0 +1,141 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2002 Pascal Vincent and Yoshua Bengio
+// Copyright (C) 1999-2002, 2006 University of Montreal
+//
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: AutoScaledGradientOptimizer.h 8247 2007-11-12 20:22:12Z nouiz $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+
+/*! \file AutoScaledGradientOptimizer.h */
+
+#ifndef AutoScaledGradientOptimizer_INC
+#define AutoScaledGradientOptimizer_INC
+
+#include <plearn/opt/Optimizer.h>
+
+namespace PLearn {
+using namespace std;
+
+
+class AutoScaledGradientOptimizer : public Optimizer
+{
+    typedef Optimizer inherited;
+      
+public:
+
+    //!  gradient descent specific parameters
+    //!  (directly modifiable by the user)
+    real learning_rate; // current learning rate
+
+    // Options (also available through setOption)
+    real start_learning_rate;
+    real decrease_constant;
+
+    // optionally the user can instead of using the decrease_constant
+    // use a fixed schedule. This matrix has 2 columns: iteration_threshold and learning_rate_factor
+    // As soon as the iteration number goes above the iteration_threshold, the corresponding learning_rate_factor
+    // is applied (multiplied) to the start_learning_rate to obtain the learning_rate.
+    Mat lr_schedule;
+
+    int verbosity;
+
+    // every how-many steps should the mean and scaling be reevaluated
+    int evaluate_scaling_every;
+    // how many steps should be used to re-evaluate the mean and scaling
+    int evaluate_scaling_during; 
+    // scaling will be 1/(mean_abs_grad + epsilon)
+    real epsilon;
+
+    AutoScaledGradientOptimizer();
+
+    PLEARN_DECLARE_OBJECT(AutoScaledGradientOptimizer);
+
+    virtual void setToOptimize(const VarArray& the_params, Var the_cost, VarArray the_other_costs = VarArray(0), TVec<VarArray> the_other_params = TVec<VarArray>(0), real the_other_weight = 1);
+
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies)
+    { inherited::makeDeepCopyFromShallowCopy(copies); }
+
+    virtual void build()
+    {
+        inherited::build();
+        build_();
+    }
+
+protected:
+    Vec scaling; // by how much to multiply the gradient before performing an update
+    Vec meanabsgrad; // the mean absolute value of the gradient computed for 
+    int nsteps_remaining_for_evaluation;
+
+    // Vecs pointing to the value and graident of parameters (setup with the makeSharedValue and makeShared Gradient hack)
+    Vec param_values;
+    Vec param_gradients;
+
+private:
+    void build_()
+    {}
+    
+public:
+
+    // virtual void oldwrite(ostream& out) const;
+    // virtual void oldread(istream& in);
+    //virtual real optimize();
+    virtual bool optimizeN(VecStatsCollector& stats_coll);
+
+protected:
+
+    static void declareOptions(OptionList& ol);
+};
+
+DECLARE_OBJECT_PTR(AutoScaledGradientOptimizer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/var/TimesConstantVariable.cc
===================================================================
--- trunk/plearn/var/TimesConstantVariable.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn/var/TimesConstantVariable.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -57,6 +57,14 @@
     : inherited(input, input->length(), input->width()), cst(c) 
 {}
 
+void
+
+TimesConstantVariable::declareOptions(OptionList &ol)
+{
+    declareOption(ol, "cst", &TimesConstantVariable::cst, OptionBase::buildoption, "");
+    inherited::declareOptions(ol);
+}
+
 void TimesConstantVariable::recomputeSize(int& l, int& w) const
 {
     if (input) {

Modified: trunk/plearn/var/TimesConstantVariable.h
===================================================================
--- trunk/plearn/var/TimesConstantVariable.h	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn/var/TimesConstantVariable.h	2008-01-03 22:01:45 UTC (rev 8375)
@@ -69,6 +69,8 @@
 
     PLEARN_DECLARE_OBJECT(TimesConstantVariable);
 
+    static void declareOptions(OptionList &ol);
+
     virtual string info() const
     { return string("TimesConstant (* ")+tostring(cst)+")"; }
 

Added: trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -0,0 +1,407 @@
+// -*- C++ -*-
+
+// LocalGaussianClassifier.cc
+//
+// Copyright (C) 2007 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file LocalGaussianClassifier.cc */
+
+
+#include "LocalGaussianClassifier.h"
+#include <plearn/math/TMat_maths.h>
+#include <plearn/math/distr_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    LocalGaussianClassifier,
+    "ONE LINE DESCRIPTION",
+    "MULTI-LINE \nHELP");
+
+LocalGaussianClassifier::LocalGaussianClassifier()
+    :nclasses(-1),
+     computation_neighbors(-1),
+     kernel_sigma(0.1),
+     regularization_sigma(1e-6),
+     ignore_weights_below(1e-8),
+     minus_one_half_over_kernel_sigma_square(0),
+     traintarget_ptr(0),
+     trainweight_ptr(0)
+{
+    // ### You may (or not) want to call build_() to finish building the object
+    // ### (doing so assumes the parent classes' build_() have been called too
+    // ### in the parent classes' constructors, something that you must ensure)
+
+    // ### If this learner needs to generate random numbers, uncomment the
+    // ### line below to enable the use of the inherited PRandom object.
+    // random_gen = new PRandom();
+}
+
+void LocalGaussianClassifier::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, "myoption", &LocalGaussianClassifier::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+    // ...
+
+    declareOption(ol, "nclasses", &LocalGaussianClassifier::nclasses, OptionBase::buildoption,
+                  "The number of different classes.\n"
+                  "Note that the 'target' part of trining set samples must be an integer\n"
+                  "with values between 0 and nclasses-1.\n");
+
+    declareOption(ol, "computation_neighbors", &LocalGaussianClassifier::computation_neighbors, OptionBase::buildoption,
+                  "This indicates to how many neighbors we should restrict ourselves for the computation\n"
+                  "of the covariance matrix only (since they are much cheaper, weight and mean are always\n"
+                  "computed using all points.)\n"
+                  "If =0 we do not compute a covariance matrix (i.e. use a spherical cov. of width regularization_sigma).\n"
+                  "If <0 we use all training points (with an appropriate weight).\n"
+                  "If >1 we consider only that many neighbors of the test point;\n"
+                  "If between 0 and 1, it's considered a coefficient by which to multiply\n"
+                  "the square root of the number of training points, to yield the actual \n"
+                  "number of computation neighbors used");
+
+    declareOption(ol, "kernel_sigma", &LocalGaussianClassifier::kernel_sigma, OptionBase::buildoption,
+                  "The sigma (standard deviation) of the weighting Gaussian Kernel\n");
+
+    declareOption(ol, "regularization_sigma", &LocalGaussianClassifier::regularization_sigma, OptionBase::buildoption,
+                  "This quantity squared is added to the diagonal of the local empirical covariance matrices.\n");
+
+    declareOption(ol, "ignore_weights_below", &LocalGaussianClassifier::ignore_weights_below, OptionBase::buildoption,
+                  "minimal weight below which we ignore the point (i.e. consider the weight is 0)\n");
+
+    declareOption(ol, "train_set", &LocalGaussianClassifier::train_set, OptionBase::learntoption,
+                  "We need to store the training set, as this learner is memory-based...");
+
+    /*
+    declareOption(ol, "NN", &LocalGaussianClassifier::NN, OptionBase::learntoption,
+                  "The nearest neighbor algorithm used to find nearest neighbors");
+    */
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void LocalGaussianClassifier::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+    // PLASSERT(weighting_kernel.isNotNull());
+
+    if(train_set.isNotNull())
+        setTrainingSet(train_set, false);
+    
+    minus_one_half_over_kernel_sigma_square = -0.5/(kernel_sigma*kernel_sigma);
+}
+
+// ### Nothing to add here, simply calls build_
+void LocalGaussianClassifier::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void LocalGaussianClassifier::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+    // deepCopyField(weighting_kernel, copies);
+    // deepCopyField(NN, copies);
+}
+
+
+int LocalGaussianClassifier::outputsize() const
+{
+    return nclasses;
+}
+
+void LocalGaussianClassifier::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+}
+
+void LocalGaussianClassifier::train()
+{
+    // The role of the train method is to bring the learner up to
+    // stage==nstages, updating train_stats with training costs measured
+    // on-line in the process.
+
+    /* TYPICAL CODE:
+
+    static Vec input;  // static so we don't reallocate memory each time...
+    static Vec target; // (but be careful that static means shared!)
+    input.resize(inputsize());    // the train_set's inputsize()
+    target.resize(targetsize());  // the train_set's targetsize()
+    real weight;
+
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    if (!initTrain())
+        return;
+
+    while(stage<nstages)
+    {
+        // clear statistics of previous epoch
+        train_stats->forget();
+
+        //... train for 1 stage, and update train_stats,
+        // using train_set->getExample(input, target, weight)
+        // and train_stats->update(train_costs)
+
+        ++stage;
+        train_stats->finalize(); // finalize statistics for this epoch
+    }
+    */
+}
+
+void LocalGaussianClassifier::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set, call_forget);
+    
+    // int l = train_set.length();
+    int is = inputsize();
+    int ts = targetsize();
+    PLASSERT(ts==1);
+    int ws = weightsize();
+    PLASSERT(ws==0 || ws==1);
+    trainsample.resize(is+ts+ws);
+    traininput = trainsample.subVec(0,is);
+    traintarget_ptr = &trainsample[is];
+    trainweight_ptr = NULL;
+    if(ws==1)
+        trainweight_ptr = &trainsample[is+ts];
+    
+    log_counts.resize(nclasses);
+    log_counts2.resize(nclasses);
+    means.resize(nclasses, is);
+    allcovars.resize(nclasses*is, is);
+    covars.resize(nclasses);
+    for(int c=0; c<nclasses; c++)
+        covars[c] = allcovars.subMatRows(c*is, is);
+}
+
+real LocalGaussianClassifier::computeLogWeight(const Vec& input, const Vec& traininput) const
+{
+    return powdistance(input, traininput, 2.0, true)*minus_one_half_over_kernel_sigma_square;
+}
+
+void LocalGaussianClassifier::computeOutput(const Vec& input, Vec& output) const
+{
+    int l = train_set.length();
+    PLASSERT(input.length()==inputsize());
+
+    int K = 0;
+    if(computation_neighbors>1)
+        K = int(computation_neighbors);
+    else if(computation_neighbors>0)
+        K = int(computation_neighbors*sqrt(l));
+    else if(computation_neighbors<0)
+        K = l;
+    if(K>l)
+        K = l;
+
+    pqvec.resize(K+1);
+    pair<real,int>* pq = pqvec.begin();
+    int pqsize = 0;
+    
+    log_counts.fill(-FLT_MAX);
+    if(K>0)
+        log_counts2.fill(-FLT_MAX);
+        
+    if(verbosity>=3)
+        perr << "______________________________________" << endl;
+    means.clear();
+    real ignore_log_weights_below = pl_log(ignore_weights_below);
+
+    for(int i=0; i<l; i++)
+    {
+        train_set->getRow(i,trainsample);
+        real log_w = computeLogWeight(input, traininput);
+        if(trainweight_ptr) 
+            log_w += pl_log(*trainweight_ptr);
+        if(log_w>=ignore_log_weights_below)
+        {
+            if(K>0)
+                {
+                    real d = -log_w;
+                    if(pqsize<K)
+                    {
+                        pq[pqsize++] = pair<real,int>(d,i);
+                        if(K<l) // need to maintain heap structure only if K<l
+                            push_heap(pq,pq+pqsize);
+                    }
+                    else if(d<pq->first)
+                    {
+                        pop_heap(pq,pq+pqsize);
+                        pq[pqsize-1] = pair<real,int>(d,i);
+                        push_heap(pq,pq+pqsize);
+                    }
+                }
+            int c = int(*traintarget_ptr);
+            real lcc = log_counts[c];
+            log_counts[c] = (lcc<ignore_log_weights_below ?log_w :logadd(lcc, log_w));
+            multiplyAcc(means(c), traininput, exp(log_w));
+        }
+    }
+
+    if(verbosity>=3)
+        perr << "log_counts: " << log_counts << endl;
+
+    for(int c=0; c<nclasses; c++)
+        if(log_counts[c]>=ignore_log_weights_below)
+            means(c) *= exp(-log_counts[c]);
+
+    allcovars.fill(0.);
+    if(K>0) // compute covars?
+    {
+        for(int k=0; k<pqsize; k++)
+        {
+            int i = pq[k].second;
+            real log_w = -pq[k].first;
+            train_set->getRow(i,trainsample);
+            int c = int(*traintarget_ptr);
+            real lcc = log_counts2[c];
+            log_counts2[c] = (lcc<ignore_log_weights_below ?log_w :logadd(lcc, log_w));
+            traininput -= means(c);
+            externalProductScaleAcc(covars[c], traininput, traininput, exp(log_w));
+        }
+        
+        for(int c=0; c<nclasses; c++)
+            if(log_counts2[c]>=ignore_log_weights_below)
+                covars[c] *= exp(-log_counts2[c]);
+    if(verbosity>=3)
+        perr << "log_counts2: " << log_counts2 << endl;
+    }
+
+    output.resize(nclasses);
+    output.clear();
+
+    for(int c=0; c<nclasses; c++)
+    {
+        if(log_counts[c]<ignore_log_weights_below)
+            output[c] = -FLT_MAX;
+        else
+        {
+            Mat cov = covars[c];
+            addToDiagonal(cov, square(regularization_sigma));
+            real log_p_x = logOfNormal(input, means(c), cov);
+            output[c] = log_p_x + log_counts[c];
+            if(verbosity>=4)
+            {
+                perr << "** Class " << c << " **" << endl;
+                perr << "log_p_x: " << log_p_x << endl;
+                perr << "log_count: " << log_counts[c] << endl;
+                perr << "mean: " << means(c) << endl;
+                perr << "regularized covar: \n" << cov << endl;
+            }
+        }
+    }
+    if(verbosity>=2)
+    {
+        perr << "Scores: " << output << endl;
+        perr << "argmax: " << argmax(output) << endl;
+    }
+}
+
+void LocalGaussianClassifier::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    costs.resize(2);
+    int c = int(target[0]);
+    costs[0] = (argmax(output)==c ?0.0 :1.0);
+    costs[1] = logadd(output)-output[c];
+}
+
+TVec<string> LocalGaussianClassifier::getTestCostNames() const
+{
+    TVec<string> names(2);
+    names[0] = "class_error";
+    names[1] = "NLL";
+    return names;
+}
+
+TVec<string> LocalGaussianClassifier::getTrainCostNames() const
+{
+    TVec<string> names;
+    return names;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h	2008-01-03 22:01:45 UTC (rev 8375)
@@ -0,0 +1,231 @@
+// -*- C++ -*-
+
+// LocalGaussianClassifier.h
+//
+// Copyright (C) 2007 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file LocalGaussianClassifier.h */
+
+
+#ifndef LocalGaussianClassifier_INC
+#define LocalGaussianClassifier_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+// From C++ stdlib
+#include <utility>                           //!< for pair
+#include <algorithm>                         //!< for push_heap
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class LocalGaussianClassifier : public PLearner
+{
+    typedef PLearner inherited;
+    
+
+protected:
+
+    // Vec emptyvec;
+    // mutable Vec NN_outputs;
+    // mutable Vec NN_costs;
+
+    // *********************
+    // * protected options *
+    // *********************
+
+    // PP<GenericNearestNeighbors> NN;
+
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int nclasses;
+    real computation_neighbors;
+    real kernel_sigma;
+    real regularization_sigma;
+    real ignore_weights_below;
+
+private:
+    real minus_one_half_over_kernel_sigma_square;
+
+    //! Global storage to save memory allocations.
+    Vec trainsample;
+    Vec traininput;
+    real* traintarget_ptr;
+    real* trainweight_ptr;
+    mutable TVec< pair<real,int> > pqvec; // priority queue (heap) vector 
+
+    Vec log_counts;  // the log_counts considering all points
+    Vec log_counts2; // the log_counts considering only the points kept for computing the covariance
+    Mat means;    
+    Mat allcovars;
+    TVec<Mat> covars;
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    LocalGaussianClassifier();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTestCostNames() const;
+
+    real computeLogWeight(const Vec& input, const Vec& traininput) const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(LocalGaussianClassifier);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LocalGaussianClassifier);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -69,7 +69,6 @@
     // ### You can also combine flags, for example with OptionBase::nosave:
     // ### (OptionBase::buildoption | OptionBase::nosave)
 
-    
     declareOption(ol, "unsupervised_nepochs", &DeepReconstructorNet::unsupervised_nepochs,
                   OptionBase::buildoption,
                   "unsupervised_nepochs[k] contains a pair of integers giving the minimum and\n"
@@ -98,6 +97,10 @@
                   OptionBase::buildoption,
                   "recontruction_costs[k] is the reconstruction cost for layer[k]");
 
+    declareOption(ol, "reconstruction_costs_names", &DeepReconstructorNet::reconstruction_costs_names,
+                  OptionBase::buildoption,
+                  "The names to be given to each of the elements of a vector cost");
+
     declareOption(ol, "reconstructed_layers", &DeepReconstructorNet::reconstructed_layers,
                   OptionBase::buildoption,
                   "reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]");
@@ -617,13 +620,9 @@
         reconstruction_optimizer->reset();    
     }
 
-    TVec<string> colnames(4);
-    colnames[0] = "nepochs";
-    colnames[1] = "reconstr_cost";
-    colnames[2] = "stderror";
-    colnames[3] = "relative_improvement";
-    VMat training_curve = new FileVMatrix(expdir/"training_costs_layer_"+tostring(which_input_layer+1)+".pmat",0,colnames);
-    Vec costrow(4);
+    Vec costrow;
+    TVec<string> colnames;
+    VMat training_curve;
 
     VecStatsCollector st;
     real prev_mean = -1;
@@ -641,19 +640,49 @@
             reconstruction_optimizer->nstages = l/minibatch_size;
             reconstruction_optimizer->optimizeN(st);
         }        
-        const StatsCollector& s = st.getStats(0);
-        real m = s.mean();
-        real er = s.stderror();
-        perr << "Epoch " << n+1 << " mean error: " << m << " +- " << s.stderror() << endl;
-        if(prev_mean>0)
+        int reconstr_cost_pos = 0;
+
+        Vec means = st.getMean();
+        Vec stderrs = st.getStdError();
+        perr << "Epoch " << n+1 << ": " << means << " +- " << stderrs;
+        real m = means[reconstr_cost_pos];
+        real er = stderrs[reconstr_cost_pos];
+        if(n>0)
         {
-            relative_improvement = (prev_mean-m)/prev_mean;
-            perr << "Relative improvement: " << relative_improvement*100 << " %"<< endl;
+            relative_improvement = (prev_mean-m)/fabs(prev_mean);
+            perr << "  improvement: " << relative_improvement*100 << " %";
         }
+        perr << endl;
+
+        int ncosts = means.length();
+        if(reconstruction_costs_names.length()!=ncosts)
+        {
+            reconstruction_costs_names.resize(ncosts);
+            for(int k=0; k<ncosts; k++)
+                reconstruction_costs_names[k] = "cost"+tostring(k);
+        }
+
+        if(colnames.length()==0)
+        {
+            colnames.append("nepochs");
+            colnames.append("relative_improvement");
+            for(int k=0; k<ncosts; k++)
+            {
+                colnames.append(reconstruction_costs_names[k]+"_mean");
+                colnames.append(reconstruction_costs_names[k]+"_stderr");
+            }
+            training_curve = new FileVMatrix(expdir/"training_costs_layer_"+tostring(which_input_layer+1)+".pmat",0,colnames);
+        }
+
+        costrow.resize(colnames.length());
+        int k=0;
         costrow[0] = n+1;
-        costrow[1] = m;
-        costrow[2] = er;
-        costrow[3] = relative_improvement*100;
+        costrow[1] = relative_improvement*100;
+        for(int k=0; k<ncosts; k++)
+        {
+            costrow[2+k*2] = means[k];
+            costrow[2+k*2+1] = stderrs[k];
+        }
         training_curve->appendRow(costrow);
         training_curve->flush();
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2008-01-03 22:01:45 UTC (rev 8375)
@@ -69,7 +69,6 @@
     //! ### declare public option fields (such as build options) here
     //! Start your comments with Doxygen-compatible comments such as //!
     
-
     TVec< pair<int,int> > unsupervised_nepochs;
     Vec unsupervised_min_improvement_rate;
 
@@ -82,6 +81,9 @@
 
     // reconstruction_costs[k] is the reconstruction cost for layers[k]
     VarArray reconstruction_costs;
+    
+    // The names to be given to each of the elements of a vector cost 
+    TVec<string> reconstruction_costs_names;
 
     // reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]
     VarArray reconstructed_layers;

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/python_modules/plearn/var/Var.py	2008-01-03 22:01:45 UTC (rev 8375)
@@ -77,6 +77,12 @@
     def sigmoid(self):
         return Var(pl.SigmoidVariable(input=self.v))
 
+    def sigmoidInRange(self, minp=0.0, maxp=1.0):
+        if minp==0.0 and maxp==1.0:
+            return self.sigmoid()
+        else:
+            return (self*(maxp-minp)+minp).sigmoid()
+
     def tanh(self):
         return Var(pl.TanhVariable(input=self.v))
 
@@ -160,7 +166,10 @@
         return Var(pl.SquareVariable(input=self.v))
 
     def add(self, other):
-        return Var(pl.PlusVariable(input1=self.v, input2=other.v))
+        if type(other) in (int, float):
+            return Var(pl.PlusConstantVariable(input=self.v, cst=other))
+        else:
+            return Var(pl.PlusVariable(input1=self.v, input2=other.v))
 
     def classificationLoss(self, class_index):
         return Var(pl.ClassificationLossVariable(input1=self.v, input2=class_index.v))
@@ -171,6 +180,12 @@
     def __sub__(self, other):
         return Var(pl.MinusVariable(input1=self.v,input2=other.v))
 
+    def __mul__(self, other):
+        if type(other) in (int, float):
+            return Var(pl.TimesConstantVariable(input=self.v, cst=other))
+        else:
+            raise NotImplementedError
+
     def neg(self):
         return Var(pl.NegateElementsVariable(input=self.v))
 

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2008-01-03 22:01:45 UTC (rev 8375)
@@ -310,7 +310,7 @@
             # reconstruction -- r
                 
             elif char == 'r':
-                if i<self.size()-2:
+                if i<self.size()-1:
                     print 'reconstructing...'
                     self.__reconstructLayer(i)
                     self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)                 

Modified: trunk/scripts/pyplot
===================================================================
--- trunk/scripts/pyplot	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/scripts/pyplot	2008-01-03 22:01:45 UTC (rev 8375)
@@ -42,7 +42,11 @@
 import os
 import sys
 import time
-from fpconst import isNaN
+try:
+    from fpconst import isNaN
+except ImportError:
+    pass
+
 from plearn.io.server import *
 from plearn.plotting import *
 



From saintmlx at mail.berlios.de  Thu Jan 10 16:08:20 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 10 Jan 2008 16:08:20 +0100
Subject: [Plearn-commits] r8376 - in trunk: commands plearn/var
	python_modules/plearn/pyext python_modules/plearn/pyplearn
Message-ID: <200801101508.m0AF8KNY007277@sheep.berlios.de>

Author: saintmlx
Date: 2008-01-10 16:08:15 +0100 (Thu, 10 Jan 2008)
New Revision: 8376

Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn/var/NegCrossEntropySigmoidVariable.cc
   trunk/python_modules/plearn/pyext/__init__.py
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
- minor mods, new includes


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-01-03 22:01:45 UTC (rev 8375)
+++ trunk/commands/plearn_noblas_inc.h	2008-01-10 15:08:15 UTC (rev 8376)
@@ -149,6 +149,7 @@
 //#include <plearn_learners/classifiers/SVMClassificationTorch.h>
 #include <plearn_learners/classifiers/MultiInstanceNNet.h>
 //#include <plearn_learners/classifiers/OverlappingAdaBoost.h> // Does not currently compile.
+#include <plearn_learners/classifiers/ToBagClassifier.h>
 
 // Generic
 #include <plearn_learners/generic/AddCostToLearner.h>
@@ -279,6 +280,7 @@
  * VMatrix *
  ***********/
 #include <plearn/vmat/AddMissingVMatrix.h>
+#include <plearn/vmat/AddBagInformationVMatrix.h>
 #include <plearn/vmat/AppendNeighborsVMatrix.h>
 #include <plearn/vmat/AsciiVMatrix.h>
 #include <plearn/vmat/AutoVMatrix.h>

Modified: trunk/plearn/var/NegCrossEntropySigmoidVariable.cc
===================================================================
--- trunk/plearn/var/NegCrossEntropySigmoidVariable.cc	2008-01-03 22:01:45 UTC (rev 8375)
+++ trunk/plearn/var/NegCrossEntropySigmoidVariable.cc	2008-01-10 15:08:15 UTC (rev 8376)
@@ -52,7 +52,7 @@
                         "Compute sigmoid of its first input, and then computes the negative "
                         "cross-entropy cost",
                         "Let o the first input ant t te second input, this computes\n"
-                        "result = - \sum_i t_i*log(o_i) + (1-t_i)*log(1-o_i)");
+                        "result = - \\sum_i t_i*log(o_i) + (1-t_i)*log(1-o_i)");
 
 ////////////////////////////////////
 // NegCrossEntropySigmoidVariable //

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2008-01-03 22:01:45 UTC (rev 8375)
+++ trunk/python_modules/plearn/pyext/__init__.py	2008-01-10 15:08:15 UTC (rev 8376)
@@ -41,7 +41,8 @@
 def cleanupWrappedObjects():
     import gc
     gc.collect()
-    ramassePoubelles()
+    if pl: #if plext still loaded
+        ramassePoubelles()
 atexit.register(cleanupWrappedObjects)
 
 print versionString()

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2008-01-03 22:01:45 UTC (rev 8375)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2008-01-10 15:08:15 UTC (rev 8376)
@@ -1068,6 +1068,21 @@
         for opt in plopt.iterator(namespace):
             print >>out, '   ',opt.getName()+':', opt
 
+
+def currentNamespacesHelp():
+    s= ''
+    for namespace in plargs.getNamespaces():
+        s+= "Namespace: " + namespace.__name__ + '\n'
+        od= plopt.optdict2(namespace)
+        for o in od:
+            opt= od[o]
+            s+= '    ' + o + ' : ' + str(opt) + '\n'
+            s+= '        ' + opt.getDoc() + '\n'
+    return s
+            
+
+    
+
 def test_plargs():                
     print "#######  Binders  #############################################################\n"
     class binder(plargs):



From tihocan at mail.berlios.de  Thu Jan 10 21:08:40 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 10 Jan 2008 21:08:40 +0100
Subject: [Plearn-commits] r8377 - trunk/plearn/vmat
Message-ID: <200801102008.m0AK8eFO023305@sheep.berlios.de>

Author: tihocan
Date: 2008-01-10 21:08:40 +0100 (Thu, 10 Jan 2008)
New Revision: 8377

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
More explicit error message

Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2008-01-10 15:08:15 UTC (rev 8376)
+++ trunk/plearn/vmat/VMatLanguage.cc	2008-01-10 20:08:40 UTC (rev 8377)
@@ -405,7 +405,9 @@
                     PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
 
                 if (a > b)
-                    PLERROR("In copyfield macro, you have specified a start field that is after the end field. Eg : [%10:%5]");
+                    PLERROR("In copyfield macro '%s', you have specified a "
+                            "start field (%d) that is after the end field "
+                            "(%d). E.g.: [%10:%5]", token.c_str(), a, b);
                 if (a == -1)
                     PLERROR("In copyfield macro, unknown field : '%s'", astr.c_str());
                 if (b == -1)



From larocheh at mail.berlios.de  Thu Jan 10 23:55:04 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 10 Jan 2008 23:55:04 +0100
Subject: [Plearn-commits] r8378 - trunk/plearn_learners_experimental
Message-ID: <200801102255.m0AMt41D005010@sheep.berlios.de>

Author: larocheh
Date: 2008-01-10 23:55:03 +0100 (Thu, 10 Jan 2008)
New Revision: 8378

Added:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
   trunk/plearn_learners_experimental/DiscriminativeRBM.h
Log:
Classifier that uses an RBM


Added: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-10 20:08:40 UTC (rev 8377)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-10 22:55:03 UTC (rev 8378)
@@ -0,0 +1,713 @@
+// -*- C++ -*-
+
+// DiscriminativeRBM.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file DiscriminativeRBM.cc */
+
+
+#define PL_LOG_MODULE_NAME "DiscriminativeRBM"
+#include "DiscriminativeRBM.h"
+#include <plearn/io/pl_log.h>
+
+#define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    DiscriminativeRBM,
+    "Discriminative Restricted Boltzmann Machine classifier.",
+    "This classifier supports semi-supervised learning, as well as\n"
+    "hybrid generative/discriminative learning. It is based on a\n"
+    "Restricted Boltzmann Machine where the visible units contain the\n"
+    "the input and the class target.");
+
+///////////////////
+// DiscriminativeRBM //
+///////////////////
+DiscriminativeRBM::DiscriminativeRBM() :
+    disc_learning_rate( 0. ),
+    disc_decrease_ct( 0. ),
+    use_exact_disc_gradient( 0. ),
+    gen_learning_weight( 0. ),
+    use_multi_conditional_learning( false ),
+    semi_sup_learning_weight( 0. ),
+    n_classes( -1 ),
+    target_weights_L1_penalty_factor( 0. ),
+    target_weights_L2_penalty_factor( 0. )
+{
+    random_gen = new PRandom();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void DiscriminativeRBM::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "disc_learning_rate", &DiscriminativeRBM::disc_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used for discriminative learning.\n");
+
+    declareOption(ol, "disc_decrease_ct", &DiscriminativeRBM::disc_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the discriminative learning rate.\n");
+
+    declareOption(ol, "use_exact_disc_gradient", 
+                  &DiscriminativeRBM::use_exact_disc_gradient,
+                  OptionBase::buildoption,
+                  "Indication that the exact gradient should be used for\n"
+                  "discriminative learning (instead of the CD gradient).\n");
+
+    declareOption(ol, "gen_learning_weight", &DiscriminativeRBM::gen_learning_weight,
+                  OptionBase::buildoption,
+                  "The weight of the generative learning term, for\n"
+                  "hybrid discriminative/generative learning.\n");
+
+    declareOption(ol, "use_multi_conditional_learning", 
+                  &DiscriminativeRBM::use_multi_conditional_learning,
+                  OptionBase::buildoption,
+                  "Indication that multi-conditional learning should\n"
+                  "be used instead of generative learning.\n");
+
+    declareOption(ol, "semi_sup_learning_weight", 
+                  &DiscriminativeRBM::semi_sup_learning_weight,
+                  OptionBase::buildoption,
+                  "The weight of the semi-supervised learning term, for\n"
+                  "unsupervised learning on unlabeled data.\n");
+
+    declareOption(ol, "n_classes", &DiscriminativeRBM::n_classes,
+                  OptionBase::buildoption,
+                  "Number of classes in the training set.\n"
+                  );
+
+    declareOption(ol, "input_layer", &DiscriminativeRBM::input_layer,
+                  OptionBase::buildoption,
+                  "The input layer of the RBM.\n");
+
+    declareOption(ol, "hidden_layer", &DiscriminativeRBM::hidden_layer,
+                  OptionBase::buildoption,
+                  "The hidden layer of the RBM.\n");
+
+    declareOption(ol, "connection", &DiscriminativeRBM::connection,
+                  OptionBase::buildoption,
+                  "The connection weights between the input and hidden layer.\n");
+
+    declareOption(ol, "target_weights_L1_penalty_factor", 
+                  &DiscriminativeRBM::target_weights_L1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Target weights' L1_penalty_factor.\n");
+
+    declareOption(ol, "target_weights_L2_penalty_factor", 
+                  &DiscriminativeRBM::target_weights_L2_penalty_factor,
+                  OptionBase::buildoption,
+                  "Target weights' L2_penalty_factor.\n");
+
+    declareOption(ol, "classification_module",
+                  &DiscriminativeRBM::classification_module,
+                  OptionBase::learntoption,
+                  "The module computing the class probabilities.\n"
+                  );
+
+    declareOption(ol, "classification_cost",
+                  &DiscriminativeRBM::classification_cost,
+                  OptionBase::nosave,
+                  "The module computing the classification cost function (NLL)"
+                  " on top\n"
+                  "of classification_module.\n"
+                  );
+
+    declareOption(ol, "joint_layer", &DiscriminativeRBM::joint_layer,
+                  OptionBase::nosave,
+                  "Concatenation of input_layer and the target layer\n"
+                  "(that is inside classification_module).\n"
+                 );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void DiscriminativeRBM::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+
+    if( inputsize_ > 0 && targetsize_ > 0)
+    {
+        PLASSERT( n_classes >= 2 );
+        PLASSERT( gen_learning_weight >= 0 );
+        PLASSERT( semi_sup_learning_weight >= 0 );
+        
+        build_layers_and_connections();
+        build_costs();
+    }
+}
+
+/////////////////
+// build_costs //
+/////////////////
+void DiscriminativeRBM::build_costs()
+{
+    cost_names.resize(0);
+    
+    // build the classification module, its cost and the joint layer
+    build_classification_cost();
+
+    int current_index = 0;
+    cost_names.append("NLL");
+    nll_cost_index = current_index;
+    current_index++;
+    
+    cost_names.append("class_error");
+    class_cost_index = current_index;
+    current_index++;
+
+    PLASSERT( current_index == cost_names.length() );
+}
+
+//////////////////////////////////
+// build_layers_and_connections //
+//////////////////////////////////
+void DiscriminativeRBM::build_layers_and_connections()
+{
+    MODULE_LOG << "build_layers_and_connections() called" << endl;
+
+    if( !input_layer )
+        PLERROR("In DiscriminativeRBM::build_layers_and_connections(): "
+                "input_layer must be provided");
+    if( !hidden_layer )
+        PLERROR("In DiscriminativeRBM::build_layers_and_connections(): "
+                "hidden_layer must be provided");
+
+    if( !connection )
+        PLERROR("DiscriminativeRBM::build_layers_and_connections(): \n"
+                "connection must be provided");
+
+    if( connection->up_size != hidden_layer->size ||
+        connection->down_size != input_layer->size )
+        PLERROR("DiscriminativeRBM::build_layers_and_connections(): \n"
+                "connection's size (%d x %d) should be %d x %d",
+                connection->up_size, connection->down_size,
+                hidden_layer->size, input_layer->size);
+
+    if( inputsize_ >= 0 )
+        PLASSERT( input_layer->size == inputsize() );
+
+    input_gradient.resize( inputsize() );
+    class_output.resize( n_classes );
+    class_gradient.resize( n_classes );
+
+    target_one_hot.resize( n_classes );
+
+    disc_pos_down_val.resize( inputsize() + n_classes );
+    disc_pos_up_val.resize( hidden_layer->size );
+    disc_neg_down_val.resize( inputsize() + n_classes );
+    disc_neg_up_val.resize( hidden_layer->size );
+  
+    gen_pos_down_val.resize( inputsize() + n_classes );
+    gen_pos_up_val.resize( hidden_layer->size );
+    gen_neg_down_val.resize( inputsize() + n_classes );
+    gen_neg_up_val.resize( hidden_layer->size );
+
+    semi_sup_pos_down_val.resize( inputsize() + n_classes );
+    semi_sup_pos_up_val.resize( hidden_layer->size );
+    semi_sup_neg_down_val.resize( inputsize() + n_classes );
+    semi_sup_neg_up_val.resize( hidden_layer->size );
+
+
+
+    if( !input_layer->random_gen )
+    {
+        input_layer->random_gen = random_gen;
+        input_layer->forget();
+    }
+
+    if( !hidden_layer->random_gen )
+    {
+        hidden_layer->random_gen = random_gen;
+        hidden_layer->forget();
+    }
+
+    if( !connection->random_gen )
+    {
+        connection->random_gen = random_gen;
+        connection->forget();
+    }
+}
+
+///////////////////////////////
+// build_classification_cost //
+///////////////////////////////
+void DiscriminativeRBM::build_classification_cost()
+{
+    MODULE_LOG << "build_classification_cost() called" << endl;
+
+    if (!classification_module ||
+        classification_module->target_layer->size != n_classes ||
+        classification_module->last_layer != hidden_layer || 
+        classification_module->previous_to_last != connection )
+    {
+        // We need to (re-)create 'last_to_target', and thus the classification
+        // module too.
+        // This is not systematically done so that the learner can be
+        // saved and loaded without losing learned parameters.
+        last_to_target = new RBMMatrixConnection();
+        last_to_target->up_size = hidden_layer->size;
+        last_to_target->down_size = n_classes;
+        last_to_target->L1_penalty_factor = target_weights_L1_penalty_factor;
+        last_to_target->L2_penalty_factor = target_weights_L2_penalty_factor;
+        last_to_target->random_gen = random_gen;
+        last_to_target->build();
+
+        target_layer = new RBMMultinomialLayer();
+        target_layer->size = n_classes;
+        target_layer->random_gen = random_gen;
+        target_layer->build();
+
+        classification_module = new RBMClassificationModule();
+        classification_module->previous_to_last = connection;
+        classification_module->last_layer = hidden_layer;
+        classification_module->last_to_target = last_to_target;
+        classification_module->target_layer = target_layer;
+        classification_module->random_gen = random_gen;
+        classification_module->build();
+    }
+
+    classification_cost = new NLLCostModule();
+    classification_cost->input_size = n_classes;
+    classification_cost->target_size = 1;
+    classification_cost->build();
+
+    last_to_target = classification_module->last_to_target;
+    last_to_target_connection = 
+        (RBMMatrixConnection*) classification_module->last_to_target;
+    target_layer = classification_module->target_layer;
+    joint_connection = classification_module->joint_connection;
+
+    joint_layer = new RBMMixedLayer();
+    joint_layer->sub_layers.resize( 2 );
+    joint_layer->sub_layers[0] = input_layer;
+    joint_layer->sub_layers[1] = target_layer;
+    joint_layer->random_gen = random_gen;
+    joint_layer->build();
+}
+
+///////////
+// build //
+///////////
+void DiscriminativeRBM::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void DiscriminativeRBM::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(input_layer, copies);
+    deepCopyField(hidden_layer, copies);
+    deepCopyField(connection, copies);
+    deepCopyField(classification_module, copies);
+    deepCopyField(cost_names, copies);
+    deepCopyField(classification_cost, copies);
+    deepCopyField(joint_layer, copies);
+    deepCopyField(last_to_target, copies);
+    deepCopyField(last_to_target_connection, copies);
+    deepCopyField(joint_connection, copies);
+    deepCopyField(target_layer, copies);
+    deepCopyField(target_one_hot, copies);
+    deepCopyField(disc_pos_down_val, copies);
+    deepCopyField(disc_pos_up_val, copies);
+    deepCopyField(disc_neg_down_val, copies);
+    deepCopyField(disc_neg_up_val, copies);
+    deepCopyField(gen_pos_down_val, copies);
+    deepCopyField(gen_pos_up_val, copies);
+    deepCopyField(gen_neg_down_val, copies);
+    deepCopyField(gen_neg_up_val, copies);
+    deepCopyField(semi_sup_pos_down_val, copies);
+    deepCopyField(semi_sup_pos_up_val, copies);
+    deepCopyField(semi_sup_neg_down_val, copies);
+    deepCopyField(semi_sup_neg_up_val, copies);
+    deepCopyField(input_gradient, copies);
+    deepCopyField(class_output, copies);
+    deepCopyField(class_gradient, copies);
+}
+
+
+////////////////
+// outputsize //
+////////////////
+int DiscriminativeRBM::outputsize() const
+{
+    return n_classes;
+}
+
+////////////
+// forget //
+////////////
+void DiscriminativeRBM::forget()
+{
+    inherited::forget();
+
+    input_layer->forget();
+    hidden_layer->forget();
+    connection->forget();
+    classification_cost->forget();
+    classification_module->forget();
+}
+
+///////////
+// train //
+///////////
+void DiscriminativeRBM::train()
+{
+    MODULE_LOG << "train() called " << endl;
+
+    MODULE_LOG << "stage = " << stage
+        << ", target nstages = " << nstages << endl;
+
+    PLASSERT( train_set );
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    int target_index;
+    real weight; // unused
+
+    real nll_cost; 
+    real class_error;
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set->length();
+    int init_stage = stage;
+    if( !initTrain() )
+    {
+        MODULE_LOG << "train() aborted" << endl;
+        return;
+    }
+
+    PP<ProgressBar> pb;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+    if( report_progress )
+        pb = new ProgressBar( "Training "
+                              + classname(),
+                              nstages - stage );
+        
+    for( ; stage<nstages ; stage++ )
+    {
+        train_set->getExample(stage%nsamples, input, target, weight);
+        target_index = (int)round( target[0] );
+        if( pb )
+            pb->update( stage - init_stage + 1 );
+
+        // Get CD stats...
+        target_one_hot.clear();
+        target_one_hot[ target_index ] = 1;
+
+        // ... for discriminative learning
+        if( !use_exact_disc_gradient && target_index >= 0 )
+        {
+            // Positive phase
+
+            // Clamp visible units
+            target_layer->sample << target_one_hot;
+            input_layer->sample << input ;
+
+            // Up pass
+            joint_connection->setAsDownInput( joint_layer->sample );
+            hidden_layer->getAllActivations( joint_connection );
+            hidden_layer->computeExpectation();
+            hidden_layer->generateSample();
+
+            disc_pos_down_val << joint_layer->sample;
+            disc_pos_up_val << hidden_layer->expectation;
+
+            // Negative phase
+
+            // Down pass
+            last_to_target_connection->setAsUpInput( hidden_layer->sample );
+            target_layer->getAllActivations( last_to_target_connection );
+            target_layer->computeExpectation();
+            target_layer->generateSample();
+
+            // Up pass
+            joint_connection->setAsDownInput( joint_layer->sample );
+            hidden_layer->getAllActivations( joint_connection );
+            hidden_layer->computeExpectation();
+
+            disc_neg_down_val << joint_layer->sample;
+            disc_neg_up_val << hidden_layer->expectation;
+        }
+
+        // ... for generative learning        
+        if( target_index >= 0 && gen_learning_weight > 0 )
+        {
+            // Positive phase
+            if( !use_exact_disc_gradient )
+            {
+                // Use previous computations
+                gen_pos_down_val << disc_pos_down_val;
+                gen_pos_up_val << disc_pos_up_val;
+
+                hidden_layer->setExpectation( gen_pos_up_val );
+                hidden_layer->generateSample();
+            }
+            else
+            {
+                // Clamp visible units
+                target_layer->sample << target_one_hot;
+                input_layer->sample << input ;
+                
+                // Up pass
+                joint_connection->setAsDownInput( joint_layer->sample );
+                hidden_layer->getAllActivations( joint_connection );
+                hidden_layer->computeExpectation();
+                hidden_layer->generateSample();
+                
+                gen_pos_down_val << joint_layer->sample;
+                gen_pos_up_val << hidden_layer->expectation;
+            }
+
+            // Negative phase
+
+            if( !use_multi_conditional_learning )
+            {
+                // Down pass
+                joint_connection->setAsUpInput( hidden_layer->sample );
+                joint_layer->getAllActivations( joint_connection );
+                joint_layer->computeExpectation();
+                joint_layer->generateSample();
+                
+                // Up pass
+                joint_connection->setAsDownInput( joint_layer->sample );
+                hidden_layer->getAllActivations( joint_connection );
+                hidden_layer->computeExpectation();
+            }
+            else
+            {
+                target_layer->sample << target_one_hot;
+
+                // Down pass
+                connection->setAsUpInput( hidden_layer->sample );
+                input_layer->getAllActivations( connection );
+                input_layer->computeExpectation();
+                input_layer->generateSample();
+                
+                // Up pass
+                joint_connection->setAsDownInput( joint_layer->sample );
+                hidden_layer->getAllActivations( joint_connection );
+                hidden_layer->computeExpectation(); 
+            }
+
+            gen_neg_down_val << joint_layer->sample;
+            gen_neg_up_val << hidden_layer->expectation;
+
+        }
+
+        // ... and for semi-supervised learning        
+        if( target_index < 0 && semi_sup_learning_weight > 0 )
+        {
+            // Positive phase
+
+            // Clamp visible units and sample from p(y|x)
+            classification_module->fprop( input,
+                                          class_output );
+            target_layer->setExpectation( class_output );
+            target_layer->generateSample();            
+            input_layer->sample << input ;
+            
+            // Up pass
+            joint_connection->setAsDownInput( joint_layer->sample );
+            hidden_layer->getAllActivations( joint_connection );
+            hidden_layer->computeExpectation();
+            hidden_layer->generateSample();
+            
+            semi_sup_pos_down_val << joint_layer->sample;
+            semi_sup_pos_up_val << hidden_layer->expectation;
+            
+            // Negative phase
+
+            // Down pass
+            joint_connection->setAsUpInput( hidden_layer->sample );
+            joint_layer->getAllActivations( joint_connection );
+            joint_layer->computeExpectation();
+            joint_layer->generateSample();
+            
+            // Up pass
+            joint_connection->setAsDownInput( joint_layer->sample );
+            hidden_layer->getAllActivations( joint_connection );
+            hidden_layer->computeExpectation();
+
+            semi_sup_neg_down_val << joint_layer->sample;
+            semi_sup_neg_up_val << hidden_layer->expectation;
+        }
+
+        setLearningRate( disc_learning_rate / (1. + disc_decrease_ct * stage ));
+
+        // Get gradient and update
+
+        if( use_exact_disc_gradient && target_index >= 0 )
+        {
+            classification_module->fprop( input, class_output );
+            // This doesn't work. gcc bug?
+            //classification_cost->fprop( class_output, target, nll_cost );
+            classification_cost->CostModule::fprop( class_output, target,
+                                                    nll_cost );
+
+            class_error =  ( argmax(class_output) == target_index ) ? 0: 1;  
+            train_costs[nll_cost_index] = nll_cost;
+            train_costs[class_cost_index] = class_error;
+
+            classification_cost->bpropUpdate( class_output, target, nll_cost,
+                                              class_gradient );
+
+            classification_module->bpropUpdate( input,  class_output,
+                                                input_gradient, class_gradient );
+
+            train_stats->update( train_costs );
+        }
+
+        // CD Updates
+        if( !use_exact_disc_gradient && target_index >= 0 )
+        {
+            joint_layer->update( disc_pos_down_val, disc_neg_down_val );
+            hidden_layer->update( disc_pos_up_val, disc_neg_up_val );
+            joint_connection->update( disc_pos_down_val, disc_pos_up_val,
+                                disc_neg_down_val, disc_neg_up_val);
+        }
+
+        
+        if( target_index >= 0 && gen_learning_weight > 0 )
+        {
+            setLearningRate( gen_learning_weight * disc_learning_rate / 
+                             (1. + disc_decrease_ct * stage ));
+            joint_layer->update( gen_pos_down_val, gen_neg_down_val );
+            hidden_layer->update( gen_pos_up_val, gen_neg_up_val );
+            joint_connection->update( gen_pos_down_val, gen_pos_up_val,
+                                gen_neg_down_val, gen_neg_up_val);
+        }
+
+        if( target_index >= 0 && semi_sup_learning_weight > 0 )
+        {
+            setLearningRate( semi_sup_learning_weight * disc_learning_rate / 
+                             (1. + disc_decrease_ct * stage ));
+            joint_layer->update( semi_sup_pos_down_val, semi_sup_neg_down_val );
+            hidden_layer->update( semi_sup_pos_up_val, semi_sup_neg_up_val );
+            joint_connection->update( semi_sup_pos_down_val, semi_sup_pos_up_val,
+                                semi_sup_neg_down_val, semi_sup_neg_up_val);
+        }
+
+    }
+    
+    train_stats->finalize();
+}
+
+
+///////////////////
+// computeOutput //
+///////////////////
+void DiscriminativeRBM::computeOutput(const Vec& input, Vec& output) const
+{
+    // Compute the output from the input.
+    output.resize(0);
+    classification_module->fprop( input, output );
+}
+
+
+void DiscriminativeRBM::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+
+    // Compute the costs from *already* computed output.
+    costs.resize( cost_names.length() );
+    costs.fill( MISSING_VALUE );
+    
+    //classification_cost->fprop( output, target, costs[nll_cost_index] );
+    classification_cost->CostModule::fprop( output, target, costs[nll_cost_index] );
+    costs[class_cost_index] =
+        (argmax(output) == (int) round(target[0]))? 0 : 1;
+}
+
+TVec<string> DiscriminativeRBM::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    return cost_names;
+}
+
+TVec<string> DiscriminativeRBM::getTrainCostNames() const
+{
+    return cost_names;
+}
+
+
+//#####  Helper functions  ##################################################
+
+void DiscriminativeRBM::setLearningRate( real the_learning_rate )
+{
+    input_layer->setLearningRate( the_learning_rate );
+    hidden_layer->setLearningRate( the_learning_rate );
+    connection->setLearningRate( the_learning_rate );
+    target_layer->setLearningRate( the_learning_rate );
+    last_to_target->setLearningRate( the_learning_rate );
+    classification_cost->setLearningRate( the_learning_rate );
+    //classification_module->setLearningRate( the_learning_rate );
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/DiscriminativeRBM.h
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-01-10 20:08:40 UTC (rev 8377)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-01-10 22:55:03 UTC (rev 8378)
@@ -0,0 +1,289 @@
+// -*- C++ -*-
+
+// DiscriminativeRBM.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file DiscriminativeRBM.h */
+
+#ifndef DiscriminativeRBM_INC
+#define DiscriminativeRBM_INC
+
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/CostModule.h>
+#include <plearn_learners/online/NLLCostModule.h>
+#include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn/misc/PTimer.h>
+#include <plearn/sys/Profiler.h>
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Discriminative Restricted Boltzmann Machine classifier
+ */
+class DiscriminativeRBM : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The learning rate used for discriminative learning
+    real disc_learning_rate;
+
+    //! The decrease constant of the discriminative learning rate
+    real disc_decrease_ct;
+
+    //! Indication that the exact gradient should be used for
+    //! discriminative learning (instead of the CD gradient)
+    bool use_exact_disc_gradient;
+
+    //! The weight of the generative learning term, for
+    //! hybrid discriminative/generative learning
+    real gen_learning_weight;
+
+    //! Indication that multi-conditional learning should
+    //! be used instead of generative learning
+    bool use_multi_conditional_learning;
+
+    //! The weight of the semi-supervised learning term, for
+    //! unsupervised learning on unlabeled data
+    real semi_sup_learning_weight;
+
+    //! Number of classes in the training set
+    int n_classes;
+
+    //! The input layer of the RBM
+    PP<RBMLayer> input_layer;
+
+    //! The hidden layer of the RBM
+    PP<RBMBinomialLayer> hidden_layer;
+
+    //! The connection weights between the input and hidden layer
+    PP<RBMConnection> connection;
+
+    //! Target weights' L1_penalty_factor
+    real target_weights_L1_penalty_factor;
+
+    //! Target weights' L2_penalty_factor
+    real target_weights_L2_penalty_factor;
+
+    //#####  Public Learnt Options  ###########################################
+    //! The module computing the probabilities of the different classes.
+    PP<RBMClassificationModule> classification_module;
+
+    //! The computed cost names
+    TVec<string> cost_names;
+
+    //! The module computing the classification cost function (NLL) on top of
+    //! classification_module.
+    PP<NLLCostModule> classification_cost;
+
+    //! Concatenation of input_layer and the target layer (that is
+    //! inside classification_module)
+    PP<RBMMixedLayer> joint_layer;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    DiscriminativeRBM();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(DiscriminativeRBM);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    //#####  Not Options  #####################################################
+
+    //! Matrix connection weights between the hidden layer and the target layer
+    //! (pointer to classification_module->last_to_target)
+    PP<RBMMatrixConnection> last_to_target;
+
+    //! Connection weights between the hidden layer and the target layer
+    //! (pointer to classification_module->last_to_target)
+    PP<RBMConnection> last_to_target_connection;
+
+    //! Connection weights between the hidden layer and the visible layer
+    //! (pointer to classification_module->joint_connection)
+    PP<RBMConnection> joint_connection;
+
+    //! Part of the RBM visible layer corresponding to the target
+    //! (pointer to classification_module->target_layer)
+    PP<RBMMultinomialLayer> target_layer;
+
+    //! Temporary variables for Contrastive Divergence
+    mutable Vec target_one_hot;
+
+    mutable Vec disc_pos_down_val;
+    mutable Vec disc_pos_up_val;
+    mutable Vec disc_neg_down_val;
+    mutable Vec disc_neg_up_val;
+
+    mutable Vec gen_pos_down_val;
+    mutable Vec gen_pos_up_val;
+    mutable Vec gen_neg_down_val;
+    mutable Vec gen_neg_up_val;
+
+    mutable Vec semi_sup_pos_down_val;
+    mutable Vec semi_sup_pos_up_val;
+    mutable Vec semi_sup_neg_down_val;
+    mutable Vec semi_sup_neg_up_val;  
+
+    //! Temporary variables for gradient descent
+    mutable Vec input_gradient;
+    mutable Vec class_output;
+    mutable Vec class_gradient;
+
+    //! Keeps the index of the NLL cost in train_costs
+    int nll_cost_index;
+
+    //! Keeps the index of the class_error cost in train_costs
+    int class_cost_index;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_costs();
+
+    void build_classification_cost();
+
+    void setLearningRate( real the_learning_rate );
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(DiscriminativeRBM);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From larocheh at mail.berlios.de  Thu Jan 10 23:56:44 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 10 Jan 2008 23:56:44 +0100
Subject: [Plearn-commits] r8379 - trunk/plearn_learners_experimental
Message-ID: <200801102256.m0AMuiXC005089@sheep.berlios.de>

Author: larocheh
Date: 2008-01-10 23:56:44 +0100 (Thu, 10 Jan 2008)
New Revision: 8379

Added:
   trunk/plearn_learners_experimental/DiscriminativeDeepBeliefNet.cc
   trunk/plearn_learners_experimental/DiscriminativeDeepBeliefNet.h
Log:
Deep Belief Net with an additional discriminative knn-based criteria for the greedy layer-wise learning.


Added: trunk/plearn_learners_experimental/DiscriminativeDeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeDeepBeliefNet.cc	2008-01-10 22:55:03 UTC (rev 8378)
+++ trunk/plearn_learners_experimental/DiscriminativeDeepBeliefNet.cc	2008-01-10 22:56:44 UTC (rev 8379)
@@ -0,0 +1,1087 @@
+// -*- C++ -*-
+
+// DiscriminativeDeepBeliefNet.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file DiscriminativeDeepBeliefNet.cc */
+
+
+#define PL_LOG_MODULE_NAME "DiscriminativeDeepBeliefNet"
+#include <plearn/io/pl_log.h>
+
+#include "DiscriminativeDeepBeliefNet.h"
+#include <plearn/vmat/VMat_computeNearestNeighbors.h>
+#include <plearn/vmat/GetInputVMatrix.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMMixedConnection.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    DiscriminativeDeepBeliefNet,
+    "Neural net, trained layer-wise in a greedy but focused fashion using autoassociators/RBMs and a supervised non-parametric gradient.",
+    "It is highly inspired by the StackedFocusedAutoassociators class,\n"
+    "and can use use the same RBMLayer and RBMConnection components.\n"
+    );
+
+DiscriminativeDeepBeliefNet::DiscriminativeDeepBeliefNet() :
+    cd_learning_rate( 0. ),
+    cd_decrease_ct( 0. ),
+    fine_tuning_learning_rate( 0. ),
+    fine_tuning_decrease_ct( 0. ),
+    k_neighbors( 1 ),
+    n_classes( -1 ),
+    discriminative_criteria_weight( 0. ), 
+    output_weights_l1_penalty_factor(0),
+    output_weights_l2_penalty_factor(0),
+    compare_joint_in_discriminative_criteria( false ),
+    do_not_use_generative_criteria( false ),
+//    cancel_normalization_terms( false ),
+    n_layers( 0 ),
+    nearest_neighbors_are_up_to_date( false ),
+    currently_trained_layer( 0 )
+{
+    // random_gen will be initialized in PLearner::build_()
+    random_gen = new PRandom();
+    nstages = 0;
+}
+
+void DiscriminativeDeepBeliefNet::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "cd_learning_rate", 
+                  &DiscriminativeDeepBeliefNet::cd_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the RBM "
+                  "contrastive divergence training.\n");
+
+    declareOption(ol, "cd_decrease_ct", 
+                  &DiscriminativeDeepBeliefNet::cd_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "the RBMs contrastive\n"
+                  "divergence training. When a hidden layer has finished "
+                  "its training,\n"
+                  "the learning rate is reset to it's initial value.\n");
+
+    declareOption(ol, "fine_tuning_learning_rate", 
+                  &DiscriminativeDeepBeliefNet::fine_tuning_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the fine tuning gradient descent.\n");
+
+    declareOption(ol, "fine_tuning_decrease_ct", 
+                  &DiscriminativeDeepBeliefNet::fine_tuning_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "fine tuning\n"
+                  "gradient descent.\n");
+
+    declareOption(ol, "training_schedule", 
+                  &DiscriminativeDeepBeliefNet::training_schedule,
+                  OptionBase::buildoption,
+                  "Number of examples to use during each phase of greedy pre-training.\n"
+                  "The number of fine-tunig steps is defined by nstages.\n"
+        );
+
+    declareOption(ol, "layers", &DiscriminativeDeepBeliefNet::layers,
+                  OptionBase::buildoption,
+                  "The layers of units in the network. The first element\n"
+                  "of this vector should be the input layer and the\n"
+                  "subsequent elements should be the hidden layers. The\n"
+                  "output layer should not be included in layers.\n");
+
+    declareOption(ol, "connections", &DiscriminativeDeepBeliefNet::connections,
+                  OptionBase::buildoption,
+                  "The weights of the connections between the layers.\n");
+
+    declareOption(ol, "unsupervised_layers", 
+                  &DiscriminativeDeepBeliefNet::unsupervised_layers,
+                  OptionBase::buildoption,
+                  "Additional units for greedy unsupervised learning.\n");
+
+    declareOption(ol, "unsupervised_connections", 
+                  &DiscriminativeDeepBeliefNet::unsupervised_connections,
+                  OptionBase::buildoption,
+                  "Additional connections for greedy unsupervised learning.\n");
+
+    declareOption(ol, "k_neighbors", 
+                  &DiscriminativeDeepBeliefNet::k_neighbors,
+                  OptionBase::buildoption,
+                  "Number of good nearest neighbors to attract and bad nearest "
+                  "neighbors to repel.\n");
+
+    declareOption(ol, "n_classes", 
+                  &DiscriminativeDeepBeliefNet::n_classes,
+                  OptionBase::buildoption,
+                  "Number of classes.\n");
+
+    declareOption(ol, "discriminative_criteria_weight", 
+                  &DiscriminativeDeepBeliefNet::discriminative_criteria_weight,
+                  OptionBase::buildoption,
+                  "Weight of the discriminative criteria.\n");
+
+    declareOption(ol, "output_weights_l1_penalty_factor", 
+                  &DiscriminativeDeepBeliefNet::output_weights_l1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Output weights l1_penalty_factor.\n");
+
+    declareOption(ol, "output_weights_l2_penalty_factor", 
+                  &DiscriminativeDeepBeliefNet::output_weights_l2_penalty_factor,
+                  OptionBase::buildoption,
+                  "Output weights l2_penalty_factor.\n");
+
+    declareOption(ol, "compare_joint_in_discriminative_criteria", 
+                  &DiscriminativeDeepBeliefNet::compare_joint_in_discriminative_criteria,
+                  OptionBase::buildoption,
+                  "Indication that the discriminative criteria should use the joint\n"
+                  "over the input and the hidden units, instead of the conditional\n"
+                  "over the hidden units given the input units.\n");
+
+    declareOption(ol, "do_not_use_generative_criteria", 
+                  &DiscriminativeDeepBeliefNet::do_not_use_generative_criteria,
+                  OptionBase::buildoption,
+                  "Indication that the generative criteria should not be used during learning\n"
+                  "(does not work with compare_joint_in_discriminative_criteria = true).\n");
+
+//    declareOption(ol, "cancel_normalization_terms", 
+//                  &DiscriminativeDeepBeliefNet::cancel_normalization_terms,
+//                  OptionBase::buildoption,
+//                  "Indication that the discriminative and generative criteria should cancel\n"
+//                  "their normalization terms. This is for the "
+//                  "compare_joint_in_discriminative_criteria\n"
+//                  "option, and this option ignores the value of discriminative_criteria_weight.\n");
+
+    declareOption(ol, "greedy_stages", 
+                  &DiscriminativeDeepBeliefNet::greedy_stages,
+                  OptionBase::learntoption,
+                  "Number of training samples seen in the different greedy "
+                  "phases.\n"
+        );
+
+    declareOption(ol, "n_layers", &DiscriminativeDeepBeliefNet::n_layers,
+                  OptionBase::learntoption,
+                  "Number of layers.\n"
+        );
+
+    declareOption(ol, "final_module", 
+                  &DiscriminativeDeepBeliefNet::final_module,
+                  OptionBase::learntoption,
+                  "Output layer of neural net.\n"
+        );
+
+    declareOption(ol, "final_cost", 
+                  &DiscriminativeDeepBeliefNet::final_cost,
+                  OptionBase::learntoption,
+                  "Cost on output layer of neural net.\n"
+        );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void DiscriminativeDeepBeliefNet::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    MODULE_LOG << "build_() called" << endl;
+
+    if(inputsize_ > 0 && targetsize_ > 0)
+    {
+        // Initialize some learnt variables
+        n_layers = layers.length();
+        
+        if( n_classes <= 0 )
+            PLERROR("DiscriminativeDeepBeliefNet::build_() - \n"
+                    "n_classes should be > 0.\n");
+
+        if( k_neighbors <= 0 )
+            PLERROR("DiscriminativeDeepBeliefNet::build_() - \n"
+                    "k_neighbors should be > 0.\n");
+
+        if( weightsize_ > 0 )
+            PLERROR("DiscriminativeDeepBeliefNet::build_() - \n"
+                    "usage of weighted samples (weight size > 0) is not\n"
+                    "implemented yet.\n");
+
+        if( training_schedule.length() != n_layers-1 )        
+            PLERROR("DiscriminativeDeepBeliefNet::build_() - \n"
+                    "training_schedule should have %d elements.\n",
+                    n_layers-1);
+        
+        if( compare_joint_in_discriminative_criteria && do_not_use_generative_criteria)
+            PLERROR("DiscriminativeDeepBeliefNet::build_() - \n"
+                    "compare_joint_in_discriminative_criteria can't be used with\n"
+                    "do_not_use_generative_criteria.\n");
+
+//        if( (!compare_joint_in_discriminative_criteria || do_not_use_generative_criteria)
+//            && cancel_normalization_terms )
+//            PLERROR("DiscriminativeDeepBeliefNet::build_() - \n"
+//                    "cancel_normalization_terms should be used with\n"
+//                    "compare_joint_in_discriminative_criteria and \n"
+//                    "do_not_use_generative_criteria without .\n");
+            
+        if(greedy_stages.length() == 0)
+        {
+            greedy_stages.resize(n_layers-1);
+            greedy_stages.clear();
+        }
+
+        if(stage > 0)
+            currently_trained_layer = n_layers;
+        else
+        {            
+            currently_trained_layer = n_layers-1;
+            while(currently_trained_layer>1
+                  && greedy_stages[currently_trained_layer-1] <= 0)
+                currently_trained_layer--;
+        }
+
+        build_layers_and_connections();
+
+        if( !final_module || !final_cost )
+            build_output_layer_and_cost();
+    }
+}
+
+void DiscriminativeDeepBeliefNet::build_output_layer_and_cost()
+{
+    GradNNetLayerModule* gnl = new GradNNetLayerModule();
+    gnl->input_size = layers[n_layers-1]->size;
+    gnl->output_size = n_classes;
+    gnl->L1_penalty_factor = output_weights_l1_penalty_factor;
+    gnl->L2_penalty_factor = output_weights_l2_penalty_factor;
+    gnl->random_gen = random_gen;
+    gnl->build();
+
+    SoftmaxModule* sm = new SoftmaxModule();
+    sm->input_size = n_classes;
+    sm->random_gen = random_gen;
+    sm->build();
+
+    ModuleStackModule* msm = new ModuleStackModule();
+    msm->modules.resize(2);
+    msm->modules[0] = gnl;
+    msm->modules[1] = sm;
+    msm->random_gen = random_gen;
+    msm->build();
+    final_module = msm;
+
+    final_module->forget();
+
+    NLLCostModule* nll = new NLLCostModule();
+    nll->input_size = n_classes;
+    nll->random_gen = random_gen;
+    nll->build();
+    
+    ClassErrorCostModule* class_error = new ClassErrorCostModule();
+    class_error->input_size = n_classes;
+    class_error->random_gen = random_gen;
+    class_error->build();
+
+    CombiningCostsModule* comb_costs = new CombiningCostsModule();
+    comb_costs->cost_weights.resize(2);
+    comb_costs->cost_weights[0] = 1;
+    comb_costs->cost_weights[1] = 0;
+    comb_costs->sub_costs.resize(2);
+    comb_costs->sub_costs[0] = nll;
+    comb_costs->sub_costs[1] = class_error;
+    comb_costs->build();
+
+    final_cost = comb_costs;
+    final_cost->forget();
+}
+
+void DiscriminativeDeepBeliefNet::build_layers_and_connections()
+{
+    MODULE_LOG << "build_layers_and_connections() called" << endl;
+
+    if( connections.length() != n_layers-1 )
+        PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() - \n"
+                "there should be %d connections.\n",
+                n_layers-1);
+     
+    if(unsupervised_layers.length() != n_layers-1 
+       && unsupervised_layers.length() != 0)
+        PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() - \n"
+                "there should be either 0 of %d unsupervised_layers.\n",
+                n_layers-1);
+        
+    if(unsupervised_connections.length() != n_layers-1 
+       && unsupervised_connections.length() != 0)
+        PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() - \n"
+                "there should be either 0 of %d unsupervised_connections.\n",
+                n_layers-1);
+        
+    if(unsupervised_connections.length() != unsupervised_layers.length())
+        PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() - \n"
+                "there should be as many unsupervised_connections and "
+                "unsupervised_layers.\n");
+        
+
+    if(layers[0]->size != inputsize_)
+        PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() - \n"
+                "layers[0] should have a size of %d.\n",
+                inputsize_);
+    
+
+    activations.resize( n_layers );
+    expectations.resize( n_layers );
+    activation_gradients.resize( n_layers );
+    expectation_gradients.resize( n_layers );
+
+    greedy_layers.resize(n_layers-1);
+    greedy_connections.resize(n_layers-1);
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        if( layers[i]->size != connections[i]->down_size )
+            PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() "
+                    "- \n"
+                    "connections[%i] should have a down_size of %d.\n",
+                    i, layers[i]->size);
+
+        if( connections[i]->up_size != layers[i+1]->size )
+            PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() "
+                    "- \n"
+                    "connections[%i] should have a up_size of %d.\n",
+                    i, layers[i+1]->size);
+
+        if(unsupervised_layers.length() != 0 &&
+           unsupervised_connections.length() != 0 && 
+           unsupervised_layers[i] && unsupervised_connections[i])
+        {
+            if( layers[i]->size != 
+                unsupervised_connections[i]->down_size )
+                PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() "
+                        "- \n"
+                        "connections[%i] should have a down_size of %d.\n",
+                        i, unsupervised_layers[i]->size);
+            
+            if( unsupervised_connections[i]->up_size != 
+                unsupervised_layers[i]->size )
+                PLERROR("DiscriminativeDeepBeliefNet::build_layers_and_connections() "
+                        "- \n"
+                        "connections[%i] should have a up_size of %d.\n",
+                        i, unsupervised_layers[i+1]->size);
+            
+            if( !(unsupervised_layers[i]->random_gen) )
+            {
+                unsupervised_layers[i]->random_gen = random_gen;
+                unsupervised_layers[i]->forget();
+            }
+            
+            if( !(unsupervised_connections[i]->random_gen) )
+            {
+                unsupervised_connections[i]->random_gen = random_gen;
+                unsupervised_connections[i]->forget();
+            }
+
+            PP<RBMMixedLayer> greedy_layer = new RBMMixedLayer();
+            greedy_layer->sub_layers.resize(2);
+            greedy_layer->sub_layers[0] = layers[i+1];
+            greedy_layer->sub_layers[1] = unsupervised_layers[i];
+            greedy_layer->size = layers[i+1]->size + unsupervised_layers[i]->size;
+            greedy_layer->build();
+
+            PP<RBMMixedConnection> greedy_connection = new RBMMixedConnection();
+            greedy_connection->sub_connections.resize(2,1);
+            greedy_connection->sub_connections(0,0) = connections[i];
+            greedy_connection->sub_connections(1,0) = unsupervised_connections[i];
+            greedy_connection->build();
+            
+            greedy_layers[i] = greedy_layer;
+            greedy_connections[i] = greedy_connection;
+        }
+        else
+        {
+            greedy_layers[i] = layers[i+1];
+            greedy_connections[i] = connections[i];
+        }
+
+        if( !(layers[i]->random_gen) )
+        {
+            layers[i]->random_gen = random_gen;
+            layers[i]->forget();
+        }
+
+        if( !(connections[i]->random_gen) )
+        {
+            connections[i]->random_gen = random_gen;
+            connections[i]->forget();
+        }
+
+        activations[i].resize( layers[i]->size );
+        expectations[i].resize( layers[i]->size );
+        activation_gradients[i].resize( layers[i]->size );
+        expectation_gradients[i].resize( layers[i]->size );
+    }
+
+    if( !(layers[n_layers-1]->random_gen) )
+    {
+        layers[n_layers-1]->random_gen = random_gen;
+        layers[n_layers-1]->forget();
+    }
+    activations[n_layers-1].resize( layers[n_layers-1]->size );
+    expectations[n_layers-1].resize( layers[n_layers-1]->size );
+    activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+    expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+}
+
+// ### Nothing to add here, simply calls build_
+void DiscriminativeDeepBeliefNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void DiscriminativeDeepBeliefNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(, copies);
+
+    // Public options
+    deepCopyField(training_schedule, copies);
+    deepCopyField(layers, copies);
+    deepCopyField(connections, copies);
+    deepCopyField(unsupervised_layers, copies);
+    deepCopyField(unsupervised_connections, copies);
+
+    // Protected options
+    deepCopyField(activations, copies);
+    deepCopyField(expectations, copies);
+    deepCopyField(activation_gradients, copies);
+    deepCopyField(expectation_gradients, copies);
+    deepCopyField(greedy_layers, copies);
+    deepCopyField(greedy_connections, copies);
+    deepCopyField(dissimilar_example_representation, copies);
+    deepCopyField(input_representation, copies);
+    deepCopyField(pos_down_val, copies);
+    deepCopyField(pos_up_val, copies);
+    deepCopyField(neg_down_val, copies);
+    deepCopyField(neg_up_val, copies);
+    deepCopyField(disc_pos_down_val1, copies);
+    deepCopyField(disc_pos_up_val1, copies);
+    deepCopyField(disc_pos_down_val2, copies);
+    deepCopyField(disc_pos_up_val2, copies);
+    deepCopyField(disc_neg_down_val, copies);
+    deepCopyField(disc_neg_up_val, copies);
+    deepCopyField(final_cost_input, copies);
+    deepCopyField(final_cost_value, copies);
+    deepCopyField(final_cost_gradient, copies);
+    deepCopyField(other_class_datasets, copies);
+    deepCopyField(nearest_neighbors_indices, copies);
+    deepCopyField(greedy_stages, copies);
+    deepCopyField(final_module, copies);
+    deepCopyField(final_cost, copies);
+}
+
+
+int DiscriminativeDeepBeliefNet::outputsize() const
+{
+    if( currently_trained_layer>n_layers-1 )
+        return 1;
+    else
+        return layers[currently_trained_layer]->size;
+}
+
+void DiscriminativeDeepBeliefNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+
+    for( int i=0 ; i<n_layers ; i++ )
+        layers[i]->forget();
+    
+    for( int i=0 ; i<n_layers-1 ; i++ )
+        connections[i]->forget();
+    
+    if(unsupervised_layers.length() != 0)
+        for( int i=0 ; i<n_layers-1 ; i++ )
+            unsupervised_layers[i]->forget();
+    
+    if(unsupervised_connections.length() != 0)
+        for( int i=0 ; i<n_layers-1 ; i++ )
+            unsupervised_connections[i]->forget();
+    
+    build_output_layer_and_cost();
+
+    stage = 0;
+    greedy_stages.clear();
+}
+
+void DiscriminativeDeepBeliefNet::train()
+{
+    MODULE_LOG << "train() called " << endl;
+    MODULE_LOG << "  training_schedule = " << training_schedule << endl;
+
+    Vec input( inputsize() );
+    Vec dissimilar_example( inputsize() );
+    Vec target( targetsize() );
+    Vec target2( targetsize() );
+    real weight; // unused
+    real weight2; // unused
+    
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set->length();
+    int sample;
+
+    PP<ProgressBar> pb;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+    int init_stage;
+
+    /***** initial greedy training *****/
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        updateNearestNeighbors();
+            
+        MODULE_LOG << "Training connection weights between layers " << i
+            << " and " << i+1 << endl;
+
+        int end_stage = training_schedule[i];
+        int* this_stage = greedy_stages.subVec(i,1).data();
+        init_stage = *this_stage;
+
+        MODULE_LOG << "  stage = " << *this_stage << endl;
+        MODULE_LOG << "  end_stage = " << end_stage << endl;
+
+        if( report_progress && *this_stage < end_stage )
+            pb = new ProgressBar( "Training layer "+tostring(i)
+                                  +" of "+classname(),
+                                  end_stage - init_stage );
+
+        train_costs.fill(MISSING_VALUE);
+ 
+        dissimilar_example_representation.resize(layers[i]->size);
+        input_representation.resize(layers[i]->size);
+
+        pos_down_val.resize(layers[i]->size);
+        pos_up_val.resize(greedy_layers[i]->size);
+        neg_down_val.resize(layers[i]->size);
+        neg_up_val.resize(greedy_layers[i]->size);
+
+        disc_pos_down_val1.resize(layers[i]->size);
+        disc_pos_up_val1.resize(layers[i+1]->size);
+        disc_pos_down_val2.resize(layers[i]->size);
+        disc_pos_up_val2.resize(layers[i+1]->size);
+        disc_neg_down_val.resize(layers[i]->size);
+        disc_neg_up_val.resize(layers[i+1]->size);
+
+        for( ; *this_stage<end_stage ; (*this_stage)++ )
+        {
+            sample = *this_stage % nsamples;
+            train_set->getExample(sample, input, target, weight);
+
+            // Find dissimilar example
+            int dissim_index = nearest_neighbors_indices(
+                sample,random_gen->uniform_multinomial_sample(k_neighbors));
+            
+            other_class_datasets[(int)round(target[0])]->getExample(dissim_index,
+                                                                    dissimilar_example, 
+                                                                    target2, weight2);
+            
+            if(((int)round(target[0])) == ((int)round(target2[0])))
+                PLERROR("DiscriminativeDeepBeliefNet::train(): dissimilar"
+                        " example is from same class!");
+
+            greedyStep( input, target, i, train_costs, *this_stage,
+                        dissimilar_example);
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( *this_stage - init_stage + 1 );
+        }
+    }
+
+    /***** fine-tuning by gradient descent *****/
+    if( stage < nstages )
+    {
+
+        MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  nstages = " << nstages << endl;
+        MODULE_LOG << "  fine_tuning_learning_rate = " << 
+            fine_tuning_learning_rate << endl;
+
+        init_stage = stage;
+        if( report_progress && stage < nstages )
+            pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                                  + classname(),
+                                  nstages - init_stage );
+
+        setLearningRate( fine_tuning_learning_rate );
+        train_costs.fill(MISSING_VALUE);
+
+        final_cost_input.resize(n_classes);
+        final_cost_value.resize(2); // Should be resized anyways
+        final_cost_gradient.resize(n_classes);
+
+        for( ; stage<nstages ; stage++ )
+        {
+            sample = stage % nsamples;
+            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                setLearningRate( fine_tuning_learning_rate
+                                 / (1. + fine_tuning_decrease_ct * stage ) );
+
+            train_set->getExample( sample, input, target, weight );
+
+            fineTuningStep( input, target, train_costs );
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( stage - init_stage + 1 );
+        }
+
+    }
+    
+    train_stats->finalize();
+    MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
+
+
+    // Update currently_trained_layer
+    if(stage > 0)
+        currently_trained_layer = n_layers;
+    else
+    {            
+        currently_trained_layer = n_layers-1;
+        while(currently_trained_layer>1 
+              && greedy_stages[currently_trained_layer-1] <= 0)
+            currently_trained_layer--;
+    }
+}
+
+void DiscriminativeDeepBeliefNet::greedyStep( 
+    const Vec& input, const Vec& target, int index, 
+    Vec train_costs, int this_stage, Vec dissimilar_example )
+{
+    PLASSERT( index < n_layers );
+    real lr;
+
+    // Get dissimilar example representation
+    computeRepresentation(dissimilar_example, dissimilar_example_representation, 
+                          index);
+
+    // Get example representation
+    computeRepresentation(input, input_representation, 
+                          index);
+
+    if( !do_not_use_generative_criteria )
+    {
+        // CD generative learning stats
+        
+        // Positive phase
+        greedy_connections[index]->setAsDownInput( input_representation );
+        greedy_layers[index]->getAllActivations( greedy_connections[index] );
+        greedy_layers[index]->computeExpectation();
+        greedy_layers[index]->generateSample();
+        
+        pos_down_val << input_representation;
+        pos_up_val << greedy_layers[index]->expectation;
+        
+        if( !compare_joint_in_discriminative_criteria )
+        {
+            disc_pos_down_val1 << input_representation;
+            disc_pos_up_val1 << layers[index+1]->expectation;
+        }
+        
+//        if( !cancel_normalization_terms )
+//        {
+        // Negative phase
+        greedy_connections[index]->setAsUpInput( greedy_layers[index]->sample );    
+        layers[index]->getAllActivations( greedy_connections[index] );
+        layers[index]->computeExpectation();
+        layers[index]->generateSample();
+        
+        greedy_connections[index]->setAsDownInput( layers[index]->sample );
+        greedy_layers[index]->getAllActivations( greedy_connections[index] );
+        greedy_layers[index]->computeExpectation();
+        
+        neg_down_val << layers[index]->sample;
+        neg_up_val << greedy_layers[index]->expectation;
+//      }
+    }
+    else if( !compare_joint_in_discriminative_criteria )
+    {
+        
+        connections[index]->setAsDownInput( input_representation );
+        layers[index+1]->getAllActivations( connections[index] );
+        layers[index+1]->computeExpectation();
+        
+        disc_pos_down_val1 << input_representation;
+        disc_pos_up_val1 << layers[index+1]->expectation;
+    }
+
+    // CD discriminative criteria stats
+
+    if( !compare_joint_in_discriminative_criteria )
+    {
+        // Positive phase
+        connections[index]->setAsDownInput( dissimilar_example_representation );
+        layers[index+1]->getAllActivations( connections[index] );
+        layers[index+1]->computeExpectation();
+        
+        disc_pos_down_val2 << dissimilar_example_representation;
+        disc_pos_up_val2 << layers[index+1]->expectation;
+    }
+
+    // Negative phase
+    disc_neg_down_val << input_representation;
+    disc_neg_down_val += dissimilar_example_representation;
+    disc_neg_down_val /= 2;
+    connections[index]->setAsDownInput( disc_neg_down_val );
+    layers[index+1]->getAllActivations( connections[index] );
+    layers[index+1]->computeExpectation();
+
+    disc_neg_up_val << layers[index+1]->expectation;
+
+    if( compare_joint_in_discriminative_criteria )
+        //&& !cancel_normalization_terms)
+    {
+        layers[index+1]->generateSample();
+        connections[index]->setAsUpInput( layers[index+1]->sample );
+        layers[index]->getAllActivations( connections[index] );
+        layers[index]->computeExpectation();
+        layers[index]->generateSample();
+
+        connections[index]->setAsDownInput( layers[index]->sample );
+        layers[index+1]->getAllActivations( connections[index] );
+        layers[index+1]->computeExpectation();
+
+        disc_pos_down_val1 << layers[index]->sample;
+        disc_pos_up_val1 << layers[index+1]->expectation;
+    }
+
+    // RBM updates
+    if( !do_not_use_generative_criteria )
+        //&& !cancel_normalization_terms )
+    {
+        lr = cd_learning_rate/(1 + cd_decrease_ct 
+                               * this_stage); 
+        
+        layers[index]->setLearningRate( lr );
+        greedy_connections[index]->setLearningRate( lr );
+        greedy_layers[index]->setLearningRate( lr );
+        
+        layers[index]->update( pos_down_val, neg_down_val );
+        greedy_connections[index]->update( pos_down_val, pos_up_val,
+                                           neg_down_val, neg_up_val );
+        greedy_layers[index]->update( pos_up_val, neg_up_val );
+    }
+    
+    if( //cancel_normalization_terms || 
+        discriminative_criteria_weight != 0 )
+    {
+        lr = discriminative_criteria_weight * 
+            cd_learning_rate/(1 + cd_decrease_ct 
+                              * this_stage); 
+        
+        if( !compare_joint_in_discriminative_criteria )
+        {
+            layers[index]->setLearningRate( lr );
+            connections[index]->setLearningRate( lr );
+            layers[index+1]->setLearningRate( lr );
+            
+            layers[index]->accumulatePosStats( disc_pos_down_val1 );
+            layers[index]->accumulatePosStats( disc_pos_down_val2 );
+            layers[index]->accumulateNegStats( disc_neg_down_val );
+            layers[index]->update();
+            
+            connections[index]->accumulatePosStats( disc_pos_down_val1,
+                                                    disc_pos_up_val1 );
+            connections[index]->accumulatePosStats( disc_pos_down_val2,
+                                                    disc_pos_up_val2 );
+            connections[index]->accumulateNegStats( disc_neg_down_val,
+                                                    disc_neg_up_val );
+            connections[index]->update();
+            
+            layers[index+1]->accumulatePosStats( disc_pos_up_val1 );
+            layers[index+1]->accumulatePosStats( disc_pos_up_val2 );
+            layers[index+1]->accumulateNegStats( disc_neg_up_val );
+            layers[index+1]->update();
+        }
+        else //if( !cancel_normalization_terms )
+        {
+            layers[index]->setLearningRate( lr );
+            connections[index]->setLearningRate( lr );
+            layers[index+1]->setLearningRate( lr );
+            
+            layers[index]->accumulatePosStats( disc_pos_down_val1 );
+            layers[index]->accumulateNegStats( disc_neg_down_val );
+            layers[index]->update();
+            
+            connections[index]->accumulatePosStats( disc_pos_down_val1,
+                                                    disc_pos_up_val1 );
+            connections[index]->accumulateNegStats( disc_neg_down_val,
+                                                    disc_neg_up_val );
+            connections[index]->update();
+            
+            layers[index+1]->accumulatePosStats( disc_pos_up_val1 );
+            layers[index+1]->accumulateNegStats( disc_neg_up_val );
+            layers[index+1]->update();
+        }
+//        else
+//        {
+//            lr = cd_learning_rate/(1 + cd_decrease_ct 
+//                                   * this_stage); 
+//            layers[index]->setLearningRate( lr );
+//            connections[index]->setLearningRate( lr );
+//            layers[index+1]->setLearningRate( lr );
+//            
+//            layers[index]->accumulatePosStats( pos_down_val );
+//            layers[index]->accumulateNegStats( disc_neg_down_val );
+//            layers[index]->update();
+//            
+//            connections[index]->accumulatePosStats( pos_down_val,
+//                                                    pos_up_val );
+//            connections[index]->accumulateNegStats( disc_neg_down_val,
+//                                                    disc_neg_up_val );
+//            connections[index]->update();
+//            
+//            layers[index+1]->accumulatePosStats( pos_up_val );
+//            layers[index+1]->accumulateNegStats( disc_neg_up_val );
+//            layers[index+1]->update();
+//        }
+    }
+}
+
+void DiscriminativeDeepBeliefNet::fineTuningStep( 
+    const Vec& input, const Vec& target,
+    Vec& train_costs )
+{
+    // Get example representation
+
+    computeRepresentation(input, input_representation, 
+                          n_layers-1);
+
+    // Compute supervised gradient
+    final_module->fprop( input_representation, final_cost_input );
+    final_cost->fprop( final_cost_input, target, final_cost_value );
+    
+    final_cost->bpropUpdate( final_cost_input, target,
+                             final_cost_value[0],
+                             final_cost_gradient );
+    final_module->bpropUpdate( input_representation,
+                               final_cost_input,
+                               expectation_gradients[ n_layers-1 ],
+                               final_cost_gradient );
+
+    for( int i=n_layers-1 ; i>0 ; i-- )
+    {
+        layers[i]->bpropUpdate( activations[i],
+                                expectations[i],
+                                activation_gradients[i],
+                                expectation_gradients[i] );
+        
+        
+        connections[i-1]->bpropUpdate( expectations[i-1],
+                                       activations[i],
+                                       expectation_gradients[i-1],
+                                       activation_gradients[i] );
+    }        
+}
+
+void DiscriminativeDeepBeliefNet::computeRepresentation(const Vec& input,
+                                                             Vec& representation,
+                                                             int layer) const
+{
+    if(layer == 0)
+    {
+        representation.resize(input.length());
+        expectations[0] << input;
+        representation << input;
+        return;
+    }
+
+    expectations[0] << input;
+    for( int i=0 ; i<layer; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+    representation.resize(expectations[layer].length());
+    representation << expectations[layer];
+}
+
+void DiscriminativeDeepBeliefNet::computeOutput(const Vec& input, Vec& output) const
+{
+    if( currently_trained_layer>n_layers-1 )
+    {
+        computeRepresentation(input,input_representation, 
+                              n_layers-1);
+        final_module->fprop( input_representation, final_cost_input );
+        output[0] = argmax(final_cost_input);
+    }
+    else
+    {
+        computeRepresentation(input, output,
+                              currently_trained_layer);
+    }
+}
+
+void DiscriminativeDeepBeliefNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    //Assumes that computeOutput has been called
+
+    costs.resize( getTestCostNames().length() );
+    costs.fill( MISSING_VALUE );
+
+    if( currently_trained_layer>n_layers-1 )
+        if( ((int)round(output[0])) == ((int)round(target[0])) )
+            costs.last() = 0;
+        else
+            costs.last() = 1;
+}
+
+TVec<string> DiscriminativeDeepBeliefNet::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    TVec<string> cost_names(0);
+
+    cost_names.append( "class_error" );
+
+    return cost_names;
+}
+
+TVec<string> DiscriminativeDeepBeliefNet::getTrainCostNames() const
+{
+    return getTestCostNames();
+}
+
+void DiscriminativeDeepBeliefNet::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set,call_forget);
+    nearest_neighbors_are_up_to_date = false;
+}
+
+void DiscriminativeDeepBeliefNet::updateNearestNeighbors()
+{
+    if( !nearest_neighbors_are_up_to_date )
+    {
+        MODULE_LOG << "Computing nearest neighbors" << endl;
+
+        Vec input( inputsize() );
+        Vec target( targetsize() );
+        real weight; // unused
+        
+        other_class_datasets.resize(n_classes);
+        for(int k=0; k<n_classes; k++)
+        {
+            other_class_datasets[k] = new ClassSubsetVMatrix();
+            other_class_datasets[k]->classes.resize(0);
+            for(int l=0; l<n_classes; l++)
+                if( l != k )
+                    other_class_datasets[k]->classes.append(l);
+            other_class_datasets[k]->source = train_set;
+            other_class_datasets[k]->build();
+        }
+        
+        
+        // Find training nearest neighbors
+        input.resize(train_set->inputsize());
+        target.resize(train_set->targetsize());
+        nearest_neighbors_indices.resize(train_set->length(), k_neighbors);
+        TVec<int> nearest_neighbors_indices_row;
+        for(int i=0; i<train_set.length(); i++)
+        {
+            train_set->getExample(i,input,target,weight);
+            nearest_neighbors_indices_row = nearest_neighbors_indices(i);
+            computeNearestNeighbors(
+                new GetInputVMatrix((VMatrix *)
+                                    other_class_datasets[(int)round(target[0])]),
+                input,
+                nearest_neighbors_indices_row,
+                -1);
+        }
+    }
+    
+    nearest_neighbors_are_up_to_date = true;
+}
+//#####  Helper functions  ##################################################
+
+void DiscriminativeDeepBeliefNet::setLearningRate( real the_learning_rate )
+{
+    layers[0]->setLearningRate( the_learning_rate );
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        greedy_layers[i]->setLearningRate( the_learning_rate );
+        greedy_connections[i]->setLearningRate( the_learning_rate );
+    }
+
+    final_module->setLearningRate( the_learning_rate );
+    final_cost->setLearningRate( the_learning_rate );
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/DiscriminativeDeepBeliefNet.h
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeDeepBeliefNet.h	2008-01-10 22:55:03 UTC (rev 8378)
+++ trunk/plearn_learners_experimental/DiscriminativeDeepBeliefNet.h	2008-01-10 22:56:44 UTC (rev 8379)
@@ -0,0 +1,332 @@
+// -*- C++ -*-
+
+// DiscriminativeDeepBeliefNet.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file DiscriminativeDeepBeliefNet.h */
+
+
+#ifndef DiscriminativeDeepBeliefNet_INC
+#define DiscriminativeDeepBeliefNet_INC
+
+#include <plearn/vmat/ClassSubsetVMatrix.h>
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/GradNNetLayerModule.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/CostModule.h>
+#include <plearn_learners/online/ModuleStackModule.h>
+#include <plearn_learners/online/NLLCostModule.h>
+#include <plearn_learners/online/ClassErrorCostModule.h>
+#include <plearn_learners/online/CombiningCostsModule.h>
+#include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn_learners/online/SoftmaxModule.h>
+#include <plearn/misc/PTimer.h>
+
+namespace PLearn {
+
+/**
+ * Deep Belief Net where the stacked RBMs also use a discriminative criteria
+ */
+class DiscriminativeDeepBeliefNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Contrastive divergence learning rate
+    real cd_learning_rate;
+    
+    //! Contrastive divergence decrease constant
+    real cd_decrease_ct;
+
+    //! The learning rate used during the fine tuning gradient descent
+    real fine_tuning_learning_rate;
+
+    //! The decrease constant of the learning rate used during fine tuning
+    //! gradient descent
+    real fine_tuning_decrease_ct;
+
+    //! Number of examples to use during each phase of greedy pre-training.
+    //! The number of fine-tunig steps is defined by nstages.
+    TVec<int> training_schedule;
+
+    //! The layers of units in the network
+    TVec< PP<RBMLayer> > layers;
+
+    //! The weights of the connections between the layers
+    TVec< PP<RBMConnection> > connections;
+
+    //! Additional units for greedy unsupervised learning
+    TVec< PP<RBMLayer> > unsupervised_layers;
+
+    //! Additional connections for greedy unsupervised learning
+    TVec< PP<RBMConnection> > unsupervised_connections;
+
+    //! Number of good nearest neighbors to attract and
+    //! bad nearest neighbors to repel.
+    int k_neighbors;
+
+    //! Number of classes
+    int n_classes;
+
+    //! Weight of the discriminative criteria
+    real discriminative_criteria_weight;
+
+    //! Output weights l1_penalty_factor
+    real output_weights_l1_penalty_factor;
+
+    //! Output weights l2_penalty_factor
+    real output_weights_l2_penalty_factor;
+
+    //! Indication that the discriminative criteria should use the joint
+    //! over the input and the hidden units, instead of the conditional
+    //! over the hidden units given the input units.
+    bool compare_joint_in_discriminative_criteria;
+
+    //! Indication that the generative criteria should not be used during learning
+    //! (does not work with compare_joint_in_discriminative_criteria = true).
+    bool do_not_use_generative_criteria;
+
+//    //! Indication that the discriminative and generative criteria should cancel
+//    //! their normalization terms. This is for the compare_joint_in_discriminative_criteria
+//    //! option, and this option ignores the value of discriminative_criteria_weight.
+//    bool cancel_normalization_terms;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Number of layers
+    int n_layers;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    DiscriminativeDeepBeliefNet();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+    /**
+     *  Declares the training set.  Then calls build() and forget() if
+     *  necessary.  Also sets this learner's inputsize_ targetsize_ weightsize_
+     *  from those of the training_set.  Note: You shouldn't have to override
+     *  this in subclasses, except in maybe to forward the call to an
+     *  underlying learner.
+     */
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+
+    void updateNearestNeighbors();
+
+    void greedyStep( const Vec& input, const Vec& target, int index, 
+                     Vec train_costs, int stage, Vec dissimilar_example);
+
+    void fineTuningStep( const Vec& input, const Vec& target,
+                         Vec& train_costs);
+
+    void computeRepresentation( const Vec& input, 
+                                Vec& representation, int layer) const;
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(DiscriminativeDeepBeliefNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    //! Indication that nearest_neighbors_indices is up to date
+    bool nearest_neighbors_are_up_to_date;
+
+    //! Stores the activations of the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Vec> activations;
+
+    //! Stores the expectations of the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Vec> expectations;
+
+    //! Stores the gradient of the cost wrt the activations of 
+    //! the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Vec> activation_gradients;
+
+    //! Stores the gradient of the cost wrt the expectations of 
+    //! the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Vec> expectation_gradients;
+
+    //! Layers used for greedy learning
+    TVec< PP<RBMLayer> > greedy_layers;
+
+    //! Connections used for greedy learning
+    TVec< PP<RBMConnection> > greedy_connections;
+
+    //! Dissimilar example representation
+    Vec dissimilar_example_representation;
+
+    //! Example representation
+    mutable Vec input_representation;
+
+    //! Positive down statistic
+    Vec pos_down_val;
+    //! Positive up statistic
+    Vec pos_up_val;
+    //! Negative down statistic
+    Vec neg_down_val;
+    //! Negative up statistic
+    Vec neg_up_val;
+
+    //! First discriminative positive down statistic
+    Vec disc_pos_down_val1;
+    //! First discriminative positive up statistic
+    Vec disc_pos_up_val1;
+    //! Second discriminative positive down statistic
+    Vec disc_pos_down_val2;
+    //! Second discriminative positive up statistic
+    Vec disc_pos_up_val2;
+    //! Negative down statistic
+    Vec disc_neg_down_val;
+    //! Negative up statistic
+    Vec disc_neg_up_val;
+
+    //! Input of cost function
+    mutable Vec final_cost_input;
+    //! Cost value
+    mutable Vec final_cost_value;
+    //! Cost gradient on output layer
+    mutable Vec final_cost_gradient;
+
+    //! Datasets for each class
+    TVec< PP<ClassSubsetVMatrix> > other_class_datasets;
+
+    //! Nearest neighbors for each training example
+    TMat<int> nearest_neighbors_indices;
+
+    //! Stages of the different greedy phases
+    TVec<int> greedy_stages;
+
+    //! Currently trained layer (1 means the first hidden layer,
+    //! n_layers means the output layer)
+    int currently_trained_layer;
+
+    //! Output layer of neural net
+    PP<OnlineLearningModule> final_module;
+
+    //! Cost on output layer of neural net
+    PP<CostModule> final_cost;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_output_layer_and_cost();
+
+    void setLearningRate( real the_learning_rate );
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(DiscriminativeDeepBeliefNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From louradou at mail.berlios.de  Fri Jan 11 20:53:12 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 11 Jan 2008 20:53:12 +0100
Subject: [Plearn-commits] r8380 - trunk/python_modules/plearn/learners
Message-ID: <200801111953.m0BJrCA3019549@sheep.berlios.de>

Author: louradou
Date: 2008-01-11 20:53:12 +0100 (Fri, 11 Jan 2008)
New Revision: 8380

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-01-10 22:56:44 UTC (rev 8379)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-01-11 19:53:12 UTC (rev 8380)
@@ -239,6 +239,14 @@
 
       def test(self, samples_target_list):
           check_samples_target_list([samples_target_list])
+          if self.save_filename != None:
+                        try:
+                           FID=open(self.save_filename,'a')
+                           #FID.write('------------\nTry with '+kernel_type+' kernel :( parameters : '+str(parameters)+' )\n')
+                           FID.write('\n'+'='*10+'\n ==> Test Error rate = '+str(test_error_rate)+'\n'+'='*10+'\n')
+                           FID.close()
+                        except:
+                           print "COULD not write in save_filename"
           return test_model(self.best_model, [[x_i for x_i in x] for x in samples_target_list[0]], [float(l) for l in samples_target_list[1]])['error_rate']
 
 
@@ -246,10 +254,18 @@
           check_samples_target_list(samples_target_list)
           if len(samples_target_list) == 1:
              print "\nCross-validation...\n"
+             train_error_name=str(self.nr_fold)+"-fold Cross-Valid Error Rate"
+             test_error_name=train_error_name
           elif len(samples_target_list) == 2:
              print "\nSimple validation...\n"
+             train_error_name="Valid Error Rate"
+             test_error_name=train_error_name
           elif len(samples_target_list) == 3:
              print "\nValidation + test...\n"
+             train_error_name="Valid Error Rate"
+             test_error_name="Test Error Rate"
+          else:
+             raise TypeError, "last argument of train_and_tune() should be a list, with length between 1 and 3"
            
           expert = eval( 'self.'+kernel_type+'_expert' )
           
@@ -280,22 +296,25 @@
                      model = svm_model(train_problem, param)
                      error_rate = test_model(model, samples_target_list[1][0], samples_target_list[1][1])['error_rate']
                      
-                     if self.save_filename != None:
-                        try:
-                           FID=open(self.save_filename,'a')
-                           FID.write('------------\nTry with '+kernel_type+' kernel :( parameters : '+str(parameters)+' )\n')
-                           FID.write(' --> Error rate = '+str(error_rate)+'\n')
-                           FID.close()
-                        except:
-                           print "COULD not write in save_filename"
-
-                  self.add_result_to_result_list(kernel_type, parameters, error_rate)
+                  self.add_result_to_result_list(kernel_type, parameters, self.error_rate)
                   if error_rate < best_error_rate:
                      best_parameters = parameters
                      best_error_rate = error_rate
 		     if len(samples_target_list) <> 1: # in case of cross-validation, we will compute the model later
                         best_model   = model
 
+                  if self.save_filename != None:
+                     try:
+                        FID=open(self.save_filename,'a')
+                        FID.write('------------\nTry with '+kernel_type+' kernel :( parameters : '+str(parameters)+' )\n')
+                        FID.write(' --> Error rate = '+str(error_rate)+'\n')
+                        FID.close()
+                     except:
+                        print "COULD not write in save_filename"
+                  print '------------\nTry with '+kernel_type+' kernel :( parameters : '+str(parameters)
+                  print ' --> '+train_error_name+' = '+str(error_rate)+'\n'
+                  print '...'
+
           if best_error_rate < expert.error_rate:
              expert.best_parameters = best_parameters
              expert.error_rate = best_error_rate
@@ -317,6 +336,23 @@
 	     if expert.should_be_tuned_again():
 	        self.train_and_tune(kernel_type, samples_target_list)
 	  
+          if self.save_filename != None:
+             try:
+                  FID=open(self.save_filename,'a')
+                  FID.write('==============================================\n')
+                  if len(samples_target_list) == 3: # train-valid-test
+                     FID.write(' --> Best '+train_error_name+' = '+str(self.valid_error_rate)+'\n')
+                  FID.write(' --> '+test_error_name+' = '+str(self.error_rate)+'\n')
+                  FID.write('     for prms: '+str(self.best_parameters)+'\n')
+                  FID.close()
+             except:
+                  print "COULD not write in save_filename"
+
+          print '=============================================='
+          print ' --> Best error rate = '+str(self.error_rate)
+          print '     for prms: '+str(self.best_parameters)
+
+
 	  return self.error_rate
           
 
@@ -386,6 +422,7 @@
     diffs = {}
     for i in range(N):
           diff = abs(model.predict([float(x_i) for x_i in samples[i]]) - float(targets[i]))
+          #print "prediction: ",model.predict([float(x_i) for x_i in samples[i]]),"  -  real: ",float(targets[i]),"  ( ",diff," )"
           if diffs.has_key(diff):
                 diffs[diff] += 1
           else:
@@ -394,7 +431,7 @@
     linear_class_error = 0
     square_class_error = 0
     for diff, nbdiff in diffs.iteritems():
-          if not diff == 0:
+          if abs(diff) > 0.001:
                 error_rate += 1*nbdiff
           linear_class_error += diff*nbdiff
           square_class_error += (diff*diff)*nbdiff
@@ -452,13 +489,13 @@
     stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
     return sum(stds)/len(stds)
 def get_std_cmp(data,i):
-    values=[vec[i] for vec in data]
+    values=[float(vec[i]) for vec in data]
     tot = sum(values)
     avg = tot*1.0/len(values)
     sdsq = sum([(i-avg)**2 for i in values])
     return (sdsq*1.0/(len(values)-1 or 1))**.5
 def get_mean_cmp(data,i):
-    values=[vec[i] for vec in data]
+    values=[float(vec[i]) for vec in data]
     return  sum(values)/len(values)
 
 def arithm_mean(data):
@@ -569,7 +606,7 @@
     my_svm.train_and_tune( 'POLY' ,    DATA )
     #[..]
 
-    valid_error_rate =  my_svm.error_rate
+    valid_error_rate =  my_svm.valid_error_rate
     print valid_error_rate
     print my_svm.best_parameters
     print my_svm.tried_parameters
@@ -598,7 +635,7 @@
     # (retrain the model on new train data, but no search for better parameters)
     my_svm.train_and_test( NEW_DATA )
     
-    valid_error_rate = my_svm.error_rate
+    valid_error_rate = my_svm.valid_error_rate
 
 #<<<#
 #>>># To try the best trained model with new data (and obtain "fair" error rates)
@@ -608,3 +645,14 @@
     test_error_rate = my_svm.test( TEST_DATA )
     
     print test_error_rate
+
+#<<<#
+#>>># Doing all this amounts to the same as the following
+
+    ALL_DATA = [ [train_samples , train_targets], [valid_samples , valid_targets], [test_samples , test_targets]  ]
+    my_svm.train_and_tune( 'RBF' , ALL_DATA )
+    # [...]
+    
+    valid_error_rate = my_svm.valid_error_rate
+    test_error_rate  = my_svm.error_rate
+    



From tihocan at mail.berlios.de  Tue Jan 15 16:57:36 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 15 Jan 2008 16:57:36 +0100
Subject: [Plearn-commits] r8381 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200801151557.m0FFvaF9013056@sheep.berlios.de>

Author: tihocan
Date: 2008-01-15 16:57:35 +0100 (Tue, 15 Jan 2008)
New Revision: 8381

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Added some (commented) debug lines

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-01-11 19:53:12 UTC (rev 8380)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-01-15 15:57:35 UTC (rev 8381)
@@ -939,6 +939,7 @@
                     // updates.
                     //printf("CPU %d updating (nsteps = %d)\n", iam, nsteps);
                     all_params += params_update;
+                    //params_update += all_params;
                     params_update.clear();
                     performed_update = true;
                 }
@@ -1247,10 +1248,11 @@
             //Profiler::pl_profile_start("ProducScaleAccOnlineStep");
             if (delayed_update) {
                 // Store updates in 'layer_params_update'.
+                //layer_params_update[i - 1].fill(0);
                 productScaleAcc(layer_params_update[i - 1],
                         next_neurons_gradient, true,
                         neuron_extended_outputs_per_layer[i-1], false,
-                        -layer_lrate_factor*lrate /* /minibatch_size */, 1);
+                        -layer_lrate_factor*lrate, 1);
             } else {
                 // Directly update the parameters.
                 productScaleAcc(layer_params[i-1],next_neurons_gradient,true,



From saintmlx at mail.berlios.de  Tue Jan 15 18:05:21 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 15 Jan 2008 18:05:21 +0100
Subject: [Plearn-commits] r8382 - trunk/python_modules/plearn/math
Message-ID: <200801151705.m0FH5LRF018339@sheep.berlios.de>

Author: saintmlx
Date: 2008-01-15 18:05:20 +0100 (Tue, 15 Jan 2008)
New Revision: 8382

Added:
   trunk/python_modules/plearn/math/spline.py
Log:
- added Spline class for cubic splines


Added: trunk/python_modules/plearn/math/spline.py
===================================================================
--- trunk/python_modules/plearn/math/spline.py	2008-01-15 15:57:35 UTC (rev 8381)
+++ trunk/python_modules/plearn/math/spline.py	2008-01-15 17:05:20 UTC (rev 8382)
@@ -0,0 +1,93 @@
+# spline.py
+# Copyright (C) 2008 Xavier Saint-Mleux
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Xavier Saint-Mleux
+
+
+from numpy import *
+
+
+class Spline(object):
+    """
+    Cubic spline
+        (adapted from Numerical Recipes in Fortran 77, 2nd ed.)
+    """
+    def __init__(self, x, y):
+        self.calc_ypp(x, y)
+        
+    def calc_ypp(self, x, y):
+        """
+        find cubic spline coeffs.
+        N.B. assumes slope of 0 at both ends
+        """
+        l= len(x)
+        y2= zeros(l, dtype=float)
+        u= zeros(l, dtype=float)
+        for i in range(1,l-1):
+            px, cx, nx= map(float,x[i-1:i+2])
+            py, cy, ny= map(float,y[i-1:i+2])
+            dx= cx-px
+            dx2= nx-px
+            sig= dx/dx2
+            p= sig * y2[i-1] + 2.
+            y2[i]= (sig-1.) / p
+            u[i]= (6.*((ny-cy)/(nx-cx) - (cy-py)/dx) / dx2 - sig*u[i-1])/p
+        for i in range(l-2,-1,-1):
+            y2[i]= y2[i]*y2[i+1]+u[i]
+        self.ox= x
+        self.oy= y
+        self.y2= y2
+
+    def __call__(self, x):
+        if isinstance(x, ndarray):
+            return vectorize(self.call)(x)
+        else:
+            return self.call(x)
+
+    def call(self, x):
+        """
+        calc. interpolated value from tabulated fn. + spline coeffs.
+        """
+        ox, oy, y2= self.ox, self.oy, self.y2
+        l= len(ox)
+        i0= 0
+        i1= l-1
+        while i1-i0 > 1:
+            i= (i0+i1)/2
+            if ox[i] > x:
+                i1= i
+            else:
+                i0= i
+        h= ox[i1]-ox[i0]
+        a= (ox[i1]-x) / h
+        b= (x-ox[i0]) / h
+        y= a*oy[i0] + b*oy[i1] + ((a**3-a)*y2[i0] + (b**3-b)*y2[i1])*(h**2)/6.
+        return y



From larocheh at mail.berlios.de  Tue Jan 15 19:49:17 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 15 Jan 2008 19:49:17 +0100
Subject: [Plearn-commits] r8383 - trunk/plearn_learners_experimental
Message-ID: <200801151849.m0FInHGv013247@sheep.berlios.de>

Author: larocheh
Date: 2008-01-15 19:49:17 +0100 (Tue, 15 Jan 2008)
New Revision: 8383

Modified:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
Log:
Corrected a bug for semi-supervised learning.


Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-15 17:05:20 UTC (rev 8382)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-15 18:49:17 UTC (rev 8383)
@@ -437,16 +437,20 @@
     for( ; stage<nstages ; stage++ )
     {
         train_set->getExample(stage%nsamples, input, target, weight);
-        target_index = (int)round( target[0] );
+
+
         if( pb )
             pb->update( stage - init_stage + 1 );
 
         // Get CD stats...
         target_one_hot.clear();
-        target_one_hot[ target_index ] = 1;
-
+        if( !is_missing(target[0]) )
+        {
+            target_index = (int)round( target[0] );
+            target_one_hot[ target_index ] = 1;
+        }
         // ... for discriminative learning
-        if( !use_exact_disc_gradient && target_index >= 0 )
+        if( !use_exact_disc_gradient && !is_missing(target[0]) )
         {
             // Positive phase
 
@@ -481,7 +485,7 @@
         }
 
         // ... for generative learning        
-        if( target_index >= 0 && gen_learning_weight > 0 )
+        if( !is_missing(target[0]) && gen_learning_weight > 0 )
         {
             // Positive phase
             if( !use_exact_disc_gradient )
@@ -546,7 +550,7 @@
         }
 
         // ... and for semi-supervised learning        
-        if( target_index < 0 && semi_sup_learning_weight > 0 )
+        if( is_missing(target[0]) && semi_sup_learning_weight > 0 )
         {
             // Positive phase
 
@@ -557,7 +561,7 @@
             target_layer->generateSample();            
             input_layer->sample << input ;
             
-            // Up pass
+             // Up pass
             joint_connection->setAsDownInput( joint_layer->sample );
             hidden_layer->getAllActivations( joint_connection );
             hidden_layer->computeExpectation();
@@ -587,7 +591,7 @@
 
         // Get gradient and update
 
-        if( use_exact_disc_gradient && target_index >= 0 )
+        if( use_exact_disc_gradient && !is_missing(target[0]) )
         {
             classification_module->fprop( input, class_output );
             // This doesn't work. gcc bug?
@@ -609,7 +613,7 @@
         }
 
         // CD Updates
-        if( !use_exact_disc_gradient && target_index >= 0 )
+        if( !use_exact_disc_gradient && !is_missing(target[0]) )
         {
             joint_layer->update( disc_pos_down_val, disc_neg_down_val );
             hidden_layer->update( disc_pos_up_val, disc_neg_up_val );
@@ -618,7 +622,7 @@
         }
 
         
-        if( target_index >= 0 && gen_learning_weight > 0 )
+        if( !is_missing(target[0]) && gen_learning_weight > 0 )
         {
             setLearningRate( gen_learning_weight * disc_learning_rate / 
                              (1. + disc_decrease_ct * stage ));
@@ -628,7 +632,7 @@
                                 gen_neg_down_val, gen_neg_up_val);
         }
 
-        if( target_index >= 0 && semi_sup_learning_weight > 0 )
+        if( is_missing(target[0]) && semi_sup_learning_weight > 0 )
         {
             setLearningRate( semi_sup_learning_weight * disc_learning_rate / 
                              (1. + disc_decrease_ct * stage ));
@@ -663,10 +667,13 @@
     costs.resize( cost_names.length() );
     costs.fill( MISSING_VALUE );
     
-    //classification_cost->fprop( output, target, costs[nll_cost_index] );
-    classification_cost->CostModule::fprop( output, target, costs[nll_cost_index] );
-    costs[class_cost_index] =
-        (argmax(output) == (int) round(target[0]))? 0 : 1;
+    if( !is_missing(target[0]) )
+    {
+        //classification_cost->fprop( output, target, costs[nll_cost_index] );
+        classification_cost->CostModule::fprop( output, target, costs[nll_cost_index] );
+        costs[class_cost_index] =
+            (argmax(output) == (int) round(target[0]))? 0 : 1;
+    }
 }
 
 TVec<string> DiscriminativeRBM::getTestCostNames() const



From lamblin at mail.berlios.de  Tue Jan 15 21:57:31 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 15 Jan 2008 21:57:31 +0100
Subject: [Plearn-commits] r8384 - trunk/plearn_learners/online
Message-ID: <200801152057.m0FKvVJL024140@sheep.berlios.de>

Author: lamblin
Date: 2008-01-15 21:57:31 +0100 (Tue, 15 Jan 2008)
New Revision: 8384

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Cosmetic changes


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-01-15 18:49:17 UTC (rev 8383)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-01-15 20:57:31 UTC (rev 8384)
@@ -659,12 +659,12 @@
 {
     int hidden_configurations = hidden_layer->getConfigurationCount();
     int visible_configurations = visible_layer->getConfigurationCount();
-	
+
     PLASSERT_MSG(hidden_configurations != RBMLayer::INFINITE_CONFIGURATIONS ||
                  visible_configurations != RBMLayer::INFINITE_CONFIGURATIONS,
                  "To compute exact log-likelihood of an RBM maximum configurations of hidden "
                  "or visible layer must be less than 2^31.");
-	
+
     // Compute partition function
     if (hidden_configurations > visible_configurations)
         // do it by log-summing minus-free-energy of visible configurations
@@ -1014,39 +1014,60 @@
         found_a_valid_configuration = true;
     }
 
-    // Compute column matrix with one entry -log P(x) = -log( sum_h P(x|h) P(h) ) for
-    // each row x of "visible", and where {P(h)}_h is provided 
+    // Compute column matrix with one entry:
+    //      -log P(x) = -log( sum_h P(x|h) P(h) )
+    // for each row x of "visible", and where {P(h)}_h is provided
     // in "neg_log_phidden" for the set of h's in "hidden".
-    // neg_log_phidden is an optional column matrix with one element -log P(h) for each 
-    // row h of "hidden", used as an input port, with neg_log_pvisible_given_phidden as output. 
-    // If neg_log_phidden is provided, it is assumed to be 1/n_h (n_h=h->length()).
-    if (neg_log_pvisible_given_phidden && neg_log_pvisible_given_phidden_is_output &&
-        hidden && !hidden_is_output && visible && !visible_is_output)
+    //
+    // neg_log_phidden is an optional column matrix with one element:
+    //      -log P(h)
+    // for each row h of "hidden", used as an input port,
+    // with neg_log_pvisible_given_phidden as output.
+    //
+    // If neg_log_phidden is provided, it is assumed to be
+    // 1/n_h (n_h=h->length()).
+    if (neg_log_pvisible_given_phidden
+        && neg_log_pvisible_given_phidden_is_output
+        && hidden && !hidden_is_output
+        && visible && !visible_is_output)
     {
-        // estimate P(x) by sum_h P(x|h) P(h) where P(h) is either constant or provided by neg_log_phidden
+        // estimate P(x) by sum_h P(x|h) P(h) where P(h) is either constant
+        // or provided by neg_log_phidden
         if (neg_log_phidden)
         {
-            PLASSERT_MSG(!neg_log_phidden_is_output,"If neg_log_phidden is provided, it must be an input");
+            PLASSERT_MSG(!neg_log_phidden_is_output,
+                         "If neg_log_phidden is provided, it must be an input");
             PLASSERT_MSG(neg_log_phidden->length()==hidden->length(),
-                     "If neg_log_phidden is provided, it must have the same length as hidden.state");
-            PLASSERT_MSG(neg_log_phidden->width()==1,"neg_log_phidden must have width 1 (single column)");
+                        "If neg_log_phidden is provided, it must have the same"
+                        " length as hidden.state");
+            PLASSERT_MSG(neg_log_phidden->width()==1,
+                         "neg_log_phidden must have width 1 (single column)");
         }
-        computeNegLogPVisibleGivenPHidden(*visible,*hidden,neg_log_phidden,*neg_log_pvisible_given_phidden);
+        computeNegLogPVisibleGivenPHidden(*visible,
+                                          *hidden,
+                                          neg_log_phidden,
+                                          *neg_log_pvisible_given_phidden);
         found_a_valid_configuration = true;
     }
 
     // SAMPLING
-    if ((visible_sample && visible_sample_is_output)               // is asked to sample visible units (discrete)
-        || (visible_expectation && visible_expectation_is_output)  //              "                   (continous)
-        || (hidden_sample && hidden_sample_is_output))             // or to sample hidden units
+    if ((visible_sample && visible_sample_is_output)
+            // is asked to sample visible units (discrete)
+        || (visible_expectation && visible_expectation_is_output)
+            //              "                   (continous)
+        || (hidden_sample && hidden_sample_is_output)
+            // or to sample hidden units
+        )
     {
-        if (hidden_sample && !hidden_sample_is_output) // sample visible conditionally on hidden
+        if (hidden_sample && !hidden_sample_is_output)
+            // sample visible conditionally on hidden
         {
             sampleVisibleGivenHidden(*hidden_sample);
             Gibbs_step=0;
             //cout << "sampling visible from hidden" << endl;
         }
-        else if (visible_sample && !visible_sample_is_output) // if an input is provided, sample hidden conditionally
+        else if (visible_sample && !visible_sample_is_output)
+            // if an input is provided, sample hidden conditionally
         {
             sampleHiddenGivenVisible(*visible_sample);
             hidden_activations_are_computed = false;
@@ -1068,9 +1089,9 @@
                 else if (!visible_layer->getExpectations().isEmpty())
                     visible_layer->samples << visible_layer->getExpectations();
                 else if (!hidden_layer->samples.isEmpty())
-                    sampleVisibleGivenHidden(hidden_layer->samples);    
+                    sampleVisibleGivenHidden(hidden_layer->samples);
                 else if (!hidden_layer->getExpectations().isEmpty())
-                    sampleVisibleGivenHidden(hidden_layer->getExpectations());    
+                    sampleVisibleGivenHidden(hidden_layer->getExpectations());
             }
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
@@ -1084,25 +1105,29 @@
             //cout << " -> " << Gibbs_step << endl;
         }
 
-        if ( hidden && hidden_is_output)   // fill hidden.state with expectations
+        if ( hidden && hidden_is_output)
+            // fill hidden.state with expectations
         {
               const Mat& hidden_expect = hidden_layer->getExpectations();
               hidden->resize(hidden_expect.length(), hidden_expect.width());
               *hidden << hidden_expect;
         }
-        if (visible_sample && visible_sample_is_output) // provide sample of the visible units
+        if (visible_sample && visible_sample_is_output)
+            // provide sample of the visible units
         {
             visible_sample->resize(visible_layer->samples.length(),
                                    visible_layer->samples.width());
             *visible_sample << visible_layer->samples;
         }
-        if (hidden_sample && hidden_sample_is_output) // provide sample of the hidden units
+        if (hidden_sample && hidden_sample_is_output)
+            // provide sample of the hidden units
         {
             hidden_sample->resize(hidden_layer->samples.length(),
                                   hidden_layer->samples.width());
             *hidden_sample << hidden_layer->samples;
         }
-        if (visible_expectation && visible_expectation_is_output) // provide expectation of the visible units
+        if (visible_expectation && visible_expectation_is_output)
+            // provide expectation of the visible units
         {
             const Mat& to_store = visible_layer->getExpectations();
             visible_expectation->resize(to_store.length(),
@@ -1135,7 +1160,8 @@
             const Mat& hidden_expectations = hidden_layer->getExpectations();
             Mat* h=0;
             Mat* h_act=0;
-            if (!hidden_activations_are_computed) // it must be because neither hidden nor hidden_act were asked
+            if (!hidden_activations_are_computed)
+                // it must be because neither hidden nor hidden_act were asked
             {
                 PLASSERT(!hidden_act);
                 computePositivePhaseHiddenActivations(*visible);
@@ -1147,11 +1173,13 @@
             }
             else
             {
-                // hidden_act must have been computed above if they were requested on port
+                // hidden_act must have been computed above if they were
+                // requested on port
                 PLASSERT(hidden_act && !hidden_act->isEmpty());
                 h_act = hidden_act;
             }
-            if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
+            if (!hidden_expectations_are_computed)
+                // it must be because hidden outputs were not asked
             {
                 PLASSERT(!hidden);
                 hidden_layer->computeExpectations();
@@ -1163,7 +1191,8 @@
             }
             else
             {
-                // hidden exp. must have been computed above if they were requested on port
+                // hidden exp. must have been computed above if they were
+                // requested on port
                 PLASSERT(hidden && !hidden->isEmpty());
                 h = hidden;
             }
@@ -1196,11 +1225,14 @@
                      negative_phase_hidden_activations_is_output);
             negative_phase_visible_samples->resize(mbs,visible_layer->size);
             if (deterministic_reconstruction_in_cd)
-               *negative_phase_visible_samples << visible_layer->getExpectations();
+               *negative_phase_visible_samples <<
+                   visible_layer->getExpectations();
             else
                *negative_phase_visible_samples << visible_layer->samples;
-            negative_phase_hidden_expectations->resize(hidden_expectations.length(),
-                                                       hidden_expectations.width());
+
+            negative_phase_hidden_expectations->resize(
+                hidden_expectations.length(),
+                hidden_expectations.width());
             *negative_phase_hidden_expectations << hidden_expectations;
             const Mat& neg_hidden_act = hidden_layer->activations;
             negative_phase_hidden_activations->resize(neg_hidden_act.length(),
@@ -1246,7 +1278,7 @@
         negative_phase_hidden_expectations->resize(1,1);
     if (negative_phase_hidden_activations && negative_phase_hidden_activations->isEmpty())
         negative_phase_hidden_activations->resize(1,1);
-    
+
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;



From lamblin at mail.berlios.de  Tue Jan 15 21:57:47 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 15 Jan 2008 21:57:47 +0100
Subject: [Plearn-commits] r8385 - trunk/plearn_learners/online
Message-ID: <200801152057.m0FKvlC5024165@sheep.berlios.de>

Author: lamblin
Date: 2008-01-15 21:57:46 +0100 (Tue, 15 Jan 2008)
New Revision: 8385

Modified:
   trunk/plearn_learners/online/MaxSubsampling2DModule.cc
Log:
Minor changes


Modified: trunk/plearn_learners/online/MaxSubsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/MaxSubsampling2DModule.cc	2008-01-15 20:57:31 UTC (rev 8384)
+++ trunk/plearn_learners/online/MaxSubsampling2DModule.cc	2008-01-15 20:57:46 UTC (rev 8385)
@@ -252,11 +252,15 @@
     // output port).
 
     Mat* input = ports_value[0];
+#ifdef BOUNDCHECK
     Mat* output = ports_value[1];
+#endif
     Mat* argmax = ports_value[2];
     Mat* input_grad = ports_gradient[0];
     Mat* output_grad = ports_gradient[1];
+#ifdef BOUNDCHECK
     Mat* argmax_grad = ports_gradient[2];
+#endif
 
     // If we want input_grad and we have output_grad
     if( input_grad && input_grad->isEmpty()



From lamblin at mail.berlios.de  Tue Jan 15 22:00:03 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 15 Jan 2008 22:00:03 +0100
Subject: [Plearn-commits] r8386 - trunk/plearn_learners/online
Message-ID: <200801152100.m0FL03FT024375@sheep.berlios.de>

Author: lamblin
Date: 2008-01-15 22:00:03 +0100 (Tue, 15 Jan 2008)
New Revision: 8386

Modified:
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
Log:
Minor changes


Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-01-15 20:57:46 UTC (rev 8385)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-01-15 21:00:03 UTC (rev 8386)
@@ -136,7 +136,9 @@
 
     Mat* prediction = ports_value[0];
     Mat* target = ports_value[1];
+#ifdef BOUNDCHECK
     Mat* cost = ports_value[2];
+#endif
     Mat* prediction_grad = ports_gradient[0];
     Mat* target_grad = ports_gradient[1];
     Mat* cost_grad = ports_gradient[2];

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2008-01-15 20:57:46 UTC (rev 8385)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2008-01-15 21:00:03 UTC (rev 8386)
@@ -241,7 +241,9 @@
 
     Mat* prediction = ports_value[0];
     Mat* target = ports_value[1];
+#ifdef BOUNDCHECK
     Mat* cost = ports_value[2];
+#endif
     Mat* prediction_grad = ports_gradient[0];
     Mat* target_grad = ports_gradient[1];
     Mat* cost_grad = ports_gradient[2];

Modified: trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2008-01-15 20:57:46 UTC (rev 8385)
+++ trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2008-01-15 21:00:03 UTC (rev 8386)
@@ -93,7 +93,9 @@
     real u = rg->uniform_sample();
     TMatElementIterator<real> pi = distribution.begin();
     real s = *pi;
+#ifdef BOUNDCHECK
     int w = distribution.width();
+#endif
     int n = distribution.size();
     int i = 0;
 



From nouiz at mail.berlios.de  Wed Jan 16 15:52:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 16 Jan 2008 15:52:32 +0100
Subject: [Plearn-commits] r8387 - trunk/plearn/math
Message-ID: <200801161452.m0GEqWEv008056@sheep.berlios.de>

Author: nouiz
Date: 2008-01-16 15:52:32 +0100 (Wed, 16 Jan 2008)
New Revision: 8387

Modified:
   trunk/plearn/math/StatsCollector.cc
Log:
changed order of print in case their is too many item for the screen.


Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2008-01-15 21:00:03 UTC (rev 8386)
+++ trunk/plearn/math/StatsCollector.cc	2008-01-16 14:52:32 UTC (rev 8387)
@@ -1016,7 +1016,16 @@
     case PStream::raw_ascii:
     case PStream::pretty_ascii:
     {
-        out << "# samples: " << n() << "\n";
+        map<real,StatsCollectorCounts>::const_iterator it = counts.begin();
+        map<real,StatsCollectorCounts>::const_iterator itend = counts.end();
+        for(; it!=itend; ++it)
+        {
+            out << "value: " << it->first 
+                << "  #equal:" << it->second.n
+                << "  #less:" << it->second.nbelow
+                << "  avg_of_less:" << it->second.sum/it->second.nbelow << endl;
+        }
+        out << "\n# samples: " << n() << "\n";
         out << "# missing: " << nmissing() << "\n";
         out << "mean: " << mean() << "\n";
         out << "stddev: " << stddev() << "\n";
@@ -1026,15 +1035,6 @@
         out << "first: " << first_obs() << "\n";
         out << "last:  " << last_obs()  << "\n\n";
         out << "counts size: " << (unsigned int) counts.size() << "\n";
-        map<real,StatsCollectorCounts>::const_iterator it = counts.begin();
-        map<real,StatsCollectorCounts>::const_iterator itend = counts.end();
-        for(; it!=itend; ++it)
-        {
-            out << "value: " << it->first 
-                << "  #equal:" << it->second.n
-                << "  #less:" << it->second.nbelow
-                << "  avg_of_less:" << it->second.sum/it->second.nbelow << endl;
-        }
         break;
     }
     default:



From nouiz at mail.berlios.de  Wed Jan 16 15:56:18 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 16 Jan 2008 15:56:18 +0100
Subject: [Plearn-commits] r8388 - trunk/plearn_learners/regressors
Message-ID: <200801161456.m0GEuIGr008314@sheep.berlios.de>

Author: nouiz
Date: 2008-01-16 15:56:18 +0100 (Wed, 16 Jan 2008)
New Revision: 8388

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
bug fix for having a MemoryVMatrixNoSave for the train_set


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-01-16 14:52:32 UTC (rev 8387)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-01-16 14:56:18 UTC (rev 8388)
@@ -41,7 +41,7 @@
 
 #include "RegressionTreeRegisters.h"
 #include <plearn/vmat/TransposeVMatrix.h>
-#include <plearn/vmat/MemoryVMatrix.h>
+#include <plearn/vmat/MemoryVMatrixNoSave.h>
 
 namespace PLearn {
 using namespace std;
@@ -109,7 +109,9 @@
 
 void RegressionTreeRegisters::initRegisters(VMat the_train_set)
 {
-    tsource = VMat(MemoryVMatrix(TransposeVMatrix(the_train_set)));
+    VMat tmp = VMat(new TransposeVMatrix(the_train_set));
+    PP<MemoryVMatrixNoSave> tmp2 = new MemoryVMatrixNoSave(tmp);
+    tsource = VMat(tmp2 );
     length_ = the_train_set->length();
     width_ = the_train_set->width();
     inputsize_ = the_train_set->inputsize();



From nouiz at mail.berlios.de  Wed Jan 16 16:02:28 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 16 Jan 2008 16:02:28 +0100
Subject: [Plearn-commits] r8389 - trunk/plearn_learners_experimental
Message-ID: <200801161502.m0GF2SMI009036@sheep.berlios.de>

Author: nouiz
Date: 2008-01-16 16:02:27 +0100 (Wed, 16 Jan 2008)
New Revision: 8389

Modified:
   trunk/plearn_learners_experimental/linearalign.h
Log:
corrected comment for doxygen


Modified: trunk/plearn_learners_experimental/linearalign.h
===================================================================
--- trunk/plearn_learners_experimental/linearalign.h	2008-01-16 14:56:18 UTC (rev 8388)
+++ trunk/plearn_learners_experimental/linearalign.h	2008-01-16 15:02:27 UTC (rev 8389)
@@ -19,10 +19,10 @@
 
 /**
 calculates the nodekernel...note that more the similarity more the nodekernel.its computing the exp(-distance).distance is computed taking into account the deviation and an additional scaling factor sigma
- at param : x : property matrix of first molecule
- at param : y : property matrix of the model (template)
- at param : sigma : additional scaling factor. 
- at param : ans : the answer...i.e. the nodekernel matrix. ans is assigned a newly created matrix */
+ at param x property matrix of first molecule
+ at param y property matrix of the model (template)
+ at param sigma additional scaling factor. 
+ at param ans the answer...i.e. the nodekernel matrix. ans is assigned a newly created matrix */
 void static nodekernel(const Mat& x,const Mat& y,const Mat& dev,real sigma,Mat& ans){
 	int xrows=x.nrows();
 	int yrows=y.nrows();
@@ -50,8 +50,8 @@
 }
 
 /**
- at param : tosort : a plearn Vec which you want to sort 
- at param : slist : it will contain the list of indices of tosort in a sorted manner. thus last entry of slist is the index of largest value in tosort */
+ at param tosort a plearn Vec which you want to sort 
+ at param slist it will contain the list of indices of tosort in a sorted manner. thus last entry of slist is the index of largest value in tosort */
 void static sortedIndexList(const Vec& tosort,vector<int>& slist){
 	int n = tosort.length();
 	vector< pair<real,int> > ilist;
@@ -70,9 +70,9 @@
 
 /**
 given a node kernel it picks the top n node kernels in each row and column . only these weights need to considered as all other weights will probably come out to be zero anyway. this way we can reduce the number of variables in our optimization problem 
- at param : nkmat : node kernel matrix
- at param : weights : this is a boolean matrix which will contain which weights are to be considered
- at param : n : minimum number of weights to be considered in a row or column
+ at param nkmat node kernel matrix
+ at param weights this is a boolean matrix which will contain which weights are to be considered
+ at param n minimum number of weights to be considered in a row or column
 */
 void static findRelevantWeights(const Mat& nkmat,vector< vector<bool> >& weights , int n){
 	const int nx = nkmat.nrows();
@@ -114,8 +114,8 @@
 }
 
 /**
- at param : wfilter : the boolean matrix containing info about which weights are to be considered
- at param : wlist : list of pairs of indices which will contain the same info as wfilter but it carries it in a list form. this makes it possible to traverse over the list of weights */
+ at param wfilter the boolean matrix containing info about which weights are to be considered
+ at param wlist list of pairs of indices which will contain the same info as wfilter but it carries it in a list form. this makes it possible to traverse over the list of weights */
 void static extractWeightList(const vector< vector<bool> >& wfilter , vector< pair<int,int> >& wlist){
 	int nx = wfilter.size();
 	int count = 0;
@@ -136,8 +136,8 @@
 }
 
 /**
- at param : coords : coordinates of the vertices among which distances are to be calculated
- at param : dist : euclidean distances will be reported in this mat */
+ at param coords coordinates of the vertices among which distances are to be calculated
+ at param dist euclidean distances will be reported in this mat */
 void static calculateEuclDist(const Mat& coords,Mat& dist){
 	int n = coords.nrows();
 	dist = Mat(n,n,0.0);
@@ -156,13 +156,13 @@
 
 /**
 see documenation about the alignment procedure. this is the function being used to align
- at param : xmat : coordinates of molecule x (to be transformed)
- at param : ymat : coordinates of molecule y
- at param : wij : weight matrix
- at param : nk : nodekernel 
- at param : xm : weighted centroid of x
- at param : ym : weighted centroid of y
- at return : the error which gives an estimate of how well the molecules were aligned 
+ at param xmat coordinates of molecule x (to be transformed)
+ at param ymat coordinates of molecule y
+ at param wij weight matrix
+ at param nk nodekernel 
+ at param xm weighted centroid of x
+ at param ym weighted centroid of y
+ at return the error which gives an estimate of how well the molecules were aligned 
 */
 real static calcTransformation4(const Mat &xmat,const Mat& ymat,const Mat& wij,const Mat &nk,Mat& rot,Vec& xm,Vec& ym){
 	int newn = xmat.nrows()+ymat.nrows();
@@ -214,7 +214,7 @@
 
 /**
 extension of calcLinearWeights ... uses fixed sigma and automatically selects suitable thresh from a fixed list. this is the version used for all calculations.
- at see : calcLinearWeights */
+ at see calcLinearWeights */
 void static autoThreshLP(const Mat& dist1,const Mat& dist2,const Mat& nk,const vector< pair<int,int> >& wlist,const vector< vector<bool> >& wfilter,Mat& wm){
 	int nterms = 0;
 	int n = wlist.size();
@@ -343,11 +343,11 @@
 
 /**
 given the molecule names , reads the vrml files and the properties and passes on the appropriate data to autoThreshLP. this is the front end that is used.
- at param : name1 : name of molecule to be aligned
- at param : name2 : name of molecule with which to align
- at param : wm : weight matrix
- at param : isweighted : take into account deviations ?
- at see : autoThreshLP */
+ at param name1 name of molecule to be aligned
+ at param name2 name of molecule with which to align
+ at param wm weight matrix
+ at param isweighted take into account deviations ?
+ at see autoThreshLP */
 void static performLP(PMolecule name1,MoleculeTemplate name2,Mat& wm,bool isweighted){
 try{	
 //	cout<<"performing lp on "<<name1<<" "<<name2<<endl;



From nouiz at mail.berlios.de  Wed Jan 16 16:53:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 16 Jan 2008 16:53:19 +0100
Subject: [Plearn-commits] r8390 - in trunk/plearn/opt/test/.pytest:
	PL_ConjGradientRosenbrock100/expected_results
	PL_ConjGradientRosenbrock2/expected_results
Message-ID: <200801161553.m0GFrJLY012631@sheep.berlios.de>

Author: nouiz
Date: 2008-01-16 16:53:18 +0100 (Wed, 16 Jan 2008)
New Revision: 8390

Modified:
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log
   trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log
Log:
modified the test acording to last change in printing


Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log	2008-01-16 15:02:27 UTC (rev 8389)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock100/expected_results/RUN.log	2008-01-16 15:53:18 UTC (rev 8390)
@@ -48,7 +48,8 @@
 full_update_frequency = -1;
 window_nan_code = 0;
 no_removal_warnings = 0;
-stats = # samples: 12
+stats = 
+# samples: 12
 # missing: 0
 mean: 95.8829597321088301
 stddev: 1.43759313057489235

Modified: trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log
===================================================================
--- trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log	2008-01-16 15:02:27 UTC (rev 8389)
+++ trunk/plearn/opt/test/.pytest/PL_ConjGradientRosenbrock2/expected_results/RUN.log	2008-01-16 15:53:18 UTC (rev 8390)
@@ -48,7 +48,8 @@
 full_update_frequency = -1;
 window_nan_code = 0;
 no_removal_warnings = 0;
-stats = # samples: 12
+stats = 
+# samples: 12
 # missing: 0
 mean: 0.226699939445132337
 stddev: 0.266560802017491161



From manzagop at mail.berlios.de  Thu Jan 17 20:18:58 2008
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 17 Jan 2008 20:18:58 +0100
Subject: [Plearn-commits] r8391 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200801171918.m0HJIwr9010945@sheep.berlios.de>

Author: manzagop
Date: 2008-01-17 20:18:58 +0100 (Thu, 17 Jan 2008)
New Revision: 8391

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
Added code to memorize the training curve for the supervised training of the output layer.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-01-16 15:53:18 UTC (rev 8390)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-01-17 19:18:58 UTC (rev 8391)
@@ -569,6 +569,22 @@
     VarArray params = totalcost->parents();
     supervised_optimizer->setToOptimize(params, totalcost);
     supervised_optimizer->reset();
+
+    TVec<string> colnames;
+    VMat training_curve;
+    Vec costrow;
+
+    colnames.append("nepochs");
+    colnames.append("relative_improvement");
+    int ncosts=supervised_costs_names.length();
+    for(int k=0; k<ncosts; k++)
+    {
+        colnames.append(supervised_costs_names[k]+"_mean");
+        colnames.append(supervised_costs_names[k]+"_stderr");
+    }
+    training_curve = new FileVMatrix(expdir/"training_costs_output.pmat",0,colnames);
+    costrow.resize(colnames.length());
+
     VecStatsCollector st;
     real prev_mean = -1;
     real relative_improvement = 1000;
@@ -579,7 +595,9 @@
         supervised_optimizer->optimizeN(st);
         const StatsCollector& s = st.getStats(0);
         real m = s.mean();
-        perr << "Epoch " << n+1 << " Mean costs: " << st.getMean() << " stderr " << st.getStdError() << endl;
+        Vec means = st.getMean();
+        Vec stderrs = st.getStdError();
+        perr << "Epoch " << n+1 << " Mean costs: " << means << " stderr " << stderrs << endl;
         perr << "mean error: " << m << " +- " << s.stderror() << endl;
         if(prev_mean>0)
         {
@@ -589,6 +607,16 @@
         prev_mean = m;
         //displayVarGraph(supervised_costvec, true);
 
+        // save to a file
+        costrow[0] = (real)n+1;
+        costrow[1] = relative_improvement*100;
+        for(int k=0; k<ncosts; k++) {
+            costrow[2+k*2] = means[k];
+            costrow[2+k*2+1] = stderrs[k];
+        }
+        training_curve->appendRow(costrow);
+        training_curve->flush();
+
     }
     
 }
@@ -675,8 +703,8 @@
         }
 
         costrow.resize(colnames.length());
-        int k=0;
-        costrow[0] = n+1;
+//        int k=0;
+        costrow[0] = (real)n+1;
         costrow[1] = relative_improvement*100;
         for(int k=0; k<ncosts; k++)
         {
@@ -789,4 +817,4 @@
   fill-column:79
   End:
 */
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :50
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From larocheh at mail.berlios.de  Mon Jan 21 01:28:29 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 21 Jan 2008 01:28:29 +0100
Subject: [Plearn-commits] r8392 - trunk/plearn_learners/online
Message-ID: <200801210028.m0L0STHe007986@sheep.berlios.de>

Author: larocheh
Date: 2008-01-21 01:28:29 +0100 (Mon, 21 Jan 2008)
New Revision: 8392

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Added the possibility to do unsupervised fine-tuning.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-01-17 19:18:58 UTC (rev 8391)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-01-21 00:28:29 UTC (rev 8392)
@@ -63,7 +63,11 @@
     compute_all_test_costs( false ),
     reconstruct_hidden( false ),
     fraction_of_masked_inputs( 0 ),
+    unsupervised_nstages( 0 ),
+    unsupervised_fine_tuning_learning_rate( 0. ),
+    unsupervised_fine_tuning_decrease_ct( 0. ),
     n_layers( 0 ),
+    unsupervised_stage( 0 ),
     currently_trained_layer( 0 )
 {
     // random_gen will be initialized in PLearner::build_()
@@ -210,6 +214,23 @@
                   "masked, i.e. unsused to reconstruct the input.\n"
         );
 
+    declareOption(ol, "unsupervised_nstages", 
+                  &StackedAutoassociatorsNet::unsupervised_nstages,
+                  OptionBase::buildoption,
+                  "Number of samples to use for unsupervised fine-tuning.\n");
+
+    declareOption(ol, "unsupervised_fine_tuning_learning_rate", 
+                  &StackedAutoassociatorsNet::unsupervised_fine_tuning_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the unsupervised "
+                  "fine tuning gradient descent");
+
+    declareOption(ol, "unsupervised_fine_tuning_decrease_ct", 
+                  &StackedAutoassociatorsNet::unsupervised_fine_tuning_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during\n"
+                  "unsupervised fine tuning gradient descent.\n");
+
     declareOption(ol, "greedy_stages", 
                   &StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -222,6 +243,12 @@
                   "Number of layers"
         );
 
+    declareOption(ol, "unsupervised_stage", 
+                  &StackedAutoassociatorsNet::unsupervised_stage,
+                  OptionBase::learntoption,
+                  "Number of samples visited so far during unsupervised "
+                  "fine-tuning.\n");
+
     declareOption(ol, "correlation_layers", 
                   &StackedAutoassociatorsNet::correlation_layers,
                   OptionBase::learntoption,
@@ -278,6 +305,11 @@
                     " - \n"
                     "cannot use online setting with reconstruct_hidden=true.\n");
 
+        if( unsupervised_nstages > 0 && correlation_connections.length() != 0 )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "cannot use unsupervised fine-tuning with correlation connections.\n");
+
         if( fraction_of_masked_inputs < 0 )
             PLERROR("StackedAutoassociatorsNet::build_()"
                     " - \n"
@@ -565,7 +597,7 @@
                         "partial_costs[%i] should have an input_size of %d.\n", 
                         i,layers[i+1]->size);
             if(i==0)
-                partial_costs_positions[i] = n_layers-1;
+                partial_costs_positions[i] = n_layers;
             else
                 partial_costs_positions[i] = partial_costs_positions[i-1]
                     + partial_costs[i-1]->name().length();
@@ -613,6 +645,10 @@
     deepCopyField(reconstruction_activations, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
     deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(fine_tuning_reconstruction_activations, copies);
+    deepCopyField(fine_tuning_reconstruction_expectations, copies);
+    deepCopyField(fine_tuning_reconstruction_activation_gradients, copies);
+    deepCopyField(fine_tuning_reconstruction_expectation_gradients, copies);
     deepCopyField(reconstruction_activation_gradients_from_hid_rec, copies);
     deepCopyField(reconstruction_expectation_gradients_from_hid_rec, copies);
     deepCopyField(hidden_reconstruction_activations, copies);
@@ -631,6 +667,9 @@
     deepCopyField(final_cost_value, copies);
     deepCopyField(final_cost_gradient, copies);
     deepCopyField(masked_autoassociator_input, copies);
+    deepCopyField(masked_autoassociator_expectations, copies);
+    deepCopyField(autoassociator_input_indices, copies);
+    deepCopyField(autoassociator_expectation_indices, copies);
     deepCopyField(greedy_stages, copies);
 }
 
@@ -688,6 +727,7 @@
     }
 
     stage = 0;
+    unsupervised_stage = 0;
     greedy_stages.clear();
 }
 
@@ -818,6 +858,78 @@
             }
         }
 
+        /***** unsupervised fine-tuning by gradient descent *****/
+        if( unsupervised_stage < unsupervised_nstages )
+        {
+            if( unsupervised_nstages > 0 && correlation_connections.length() != 0 )
+                PLERROR("StackedAutoassociatorsNet::train()"
+                        " - \n"
+                        "cannot use unsupervised fine-tuning with correlation connections.\n");
+            
+            MODULE_LOG << "Unsupervised fine-tuning all parameters, ";
+            MODULE_LOG << "by gradient descent" << endl;
+            MODULE_LOG << "  unsupervised_stage = " << unsupervised_stage << endl;
+            MODULE_LOG << "  unsupervised_nstages = " << 
+                unsupervised_nstages << endl;
+            MODULE_LOG << "  unsupervised_fine_tuning_learning_rate = " << 
+                unsupervised_fine_tuning_learning_rate << endl;
+
+            init_stage = unsupervised_stage;
+            if( report_progress && unsupervised_stage < unsupervised_nstages )
+                pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                                      + classname(),
+                                      unsupervised_nstages - init_stage );
+
+            fine_tuning_reconstruction_activations.resize( n_layers );
+            fine_tuning_reconstruction_expectations.resize( n_layers );
+            fine_tuning_reconstruction_activation_gradients.resize( n_layers );
+            fine_tuning_reconstruction_expectation_gradients.resize( n_layers );
+            for( int i=0 ; i<n_layers ; i++ )
+            {
+                fine_tuning_reconstruction_activations[i].resize( 
+                    layers[i]->size );
+                fine_tuning_reconstruction_expectations[i].resize( 
+                    layers[i]->size );
+                fine_tuning_reconstruction_activation_gradients[i].resize( 
+                    layers[i]->size );
+                fine_tuning_reconstruction_expectation_gradients[i].resize( 
+                    layers[i]->size );
+            }
+
+            if( fraction_of_masked_inputs > 0 )
+            {
+                masked_autoassociator_expectations.resize( n_layers-1 );
+                autoassociator_expectation_indices.resize( n_layers-1 );
+                
+                for( int i=0 ; i<n_layers-1 ; i++ )
+                {
+                    masked_autoassociator_expectations[i].resize( layers[i]->size );
+                    autoassociator_expectation_indices[i].resize( layers[i]->size );
+                    for( int j=0 ; j < autoassociator_expectation_indices[i].length() ; j++ )
+                        autoassociator_expectation_indices[i][j] = j;
+                }
+            }
+
+            setLearningRate( unsupervised_fine_tuning_learning_rate );
+            train_costs.fill(MISSING_VALUE);
+            for( ; unsupervised_stage<unsupervised_nstages ; unsupervised_stage++ )
+            {
+                sample = unsupervised_stage % nsamples;
+                if( !fast_exact_is_equal( unsupervised_fine_tuning_decrease_ct, 0. ) )
+                    setLearningRate( 
+                        unsupervised_fine_tuning_learning_rate
+                        / (1. + unsupervised_fine_tuning_decrease_ct 
+                           * unsupervised_stage ) );
+
+                train_set->getExample( sample, input, target, weight );
+                unsupervisedFineTuningStep( input, target, train_costs );
+                train_stats->update( train_costs );
+
+                if( pb )
+                    pb->update( unsupervised_stage - init_stage + 1 );
+            }
+        }
+
         /***** fine-tuning by gradient descent *****/
         if( stage < nstages )
         {
@@ -868,6 +980,12 @@
     }
     else
     {
+
+        if( unsupervised_nstages > 0 )
+            PLERROR("StackedAutoassociatorsNet::train()"
+                    " - \n"
+                    "unsupervised fine-tuning with online=true is not implemented.\n");
+        
         // Train all layers simultaneously AND fine-tuning as well!
         if( stage < nstages )
         {
@@ -1178,8 +1296,96 @@
 
 }
 
+void StackedAutoassociatorsNet::unsupervisedFineTuningStep( const Vec& input, 
+                                                            const Vec& target,
+                                                            Vec& train_costs )
+{
+    // fprop
+    expectations[0] << input;
+
+    if( fraction_of_masked_inputs > 0 )
+    {
+        for( int i=0; i<autoassociator_expectation_indices.length(); i++ )
+            random_gen->shuffleElements(autoassociator_expectation_indices[i]);
+        
+        for( int i=0 ; i<n_layers-1; i++ )
+        {
+            masked_autoassociator_expectations[i] << expectations[i];
+            for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
+                masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
+            
+            connections[i]->fprop( masked_autoassociator_expectations[i], 
+                                   activations[i+1] );
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        }
+    }
+    else
+    {
+        for( int i=0 ; i<n_layers-1; i++ )
+        {
+            connections[i]->fprop( expectations[i], activations[i+1] );
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        }
+    }
+
+    fine_tuning_reconstruction_expectations[ n_layers-1 ] << 
+        expectations[ n_layers-1 ];
+
+    for( int i=n_layers-2 ; i>=0; i-- )
+    {
+        reconstruction_connections[i]->fprop( 
+            fine_tuning_reconstruction_expectations[i+1], 
+            fine_tuning_reconstruction_activations[i] );
+        layers[i]->fprop( fine_tuning_reconstruction_activations[i],
+                          fine_tuning_reconstruction_expectations[i]);
+    }
+    
+    layers[ 0 ]->setExpectation( fine_tuning_reconstruction_expectations[ 0 ] );
+    layers[ 0 ]->activation << fine_tuning_reconstruction_activations[0];
+    real rec_err = layers[ 0 ]->fpropNLL( input );
+    train_costs[n_layers-1] = rec_err;
+    
+    layers[ 0 ]->bpropNLL( input, rec_err,
+                           fine_tuning_reconstruction_activation_gradients[ 0 ] );
+
+    layers[ 0 ]->update( fine_tuning_reconstruction_activation_gradients[ 0 ] );
+
+    for( int i=0 ; i<n_layers-1; i++ )
+    {
+        if( i != 0)
+            layers[i]->bpropUpdate( fine_tuning_reconstruction_activations[i],
+                                    fine_tuning_reconstruction_expectations[i],
+                                    fine_tuning_reconstruction_activation_gradients[i],
+                                    fine_tuning_reconstruction_expectation_gradients[i]);
+        reconstruction_connections[i]->bpropUpdate( 
+            fine_tuning_reconstruction_expectations[i+1], 
+            fine_tuning_reconstruction_activations[i],
+            fine_tuning_reconstruction_expectation_gradients[i+1], 
+            fine_tuning_reconstruction_activation_gradients[i]);
+    }
+
+    expectation_gradients[ n_layers-1 ] << 
+        fine_tuning_reconstruction_expectation_gradients[ n_layers-1 ];
+    
+    for( int i=n_layers-2 ; i>=0; i-- )
+    {
+        layers[i+1]->bpropUpdate(
+            activations[i+1],expectations[i+1],
+            activation_gradients[i+1],expectation_gradients[i+1]);
+        if( fraction_of_masked_inputs > 0 )
+            connections[i]->bpropUpdate( 
+                masked_autoassociator_expectations[i], activations[i+1],
+                expectation_gradients[i], activation_gradients[i+1] );
+        else
+            connections[i]->bpropUpdate( 
+                expectations[i], activations[i+1],
+                expectation_gradients[i], activation_gradients[i+1] );
+
+    }
+}
+
 void StackedAutoassociatorsNet::fineTuningStep( const Vec& input, const Vec& target,
-                                    Vec& train_costs )
+                                                Vec& train_costs )
 {
     // fprop
     expectations[0] << input;
@@ -1729,7 +1935,24 @@
 
 TVec<string> StackedAutoassociatorsNet::getTrainCostNames() const
 {
-    return getTestCostNames() ;    
+    TVec<string> cost_names(0);
+
+    for( int i=0; i<layers.size()-1; i++)
+        cost_names.push_back("reconstruction_error_" + tostring(i+1));
+
+    cost_names.push_back("global_reconstruction_error");
+    
+    for( int i=0 ; i<partial_costs.size() ; i++ )
+    {
+        TVec<string> names = partial_costs[i]->name();
+        for(int j=0; j<names.length(); j++)
+            cost_names.push_back("partial" + tostring(i) + "." + 
+                names[j]);
+    }
+
+    cost_names.append( final_cost->name() );
+
+    return cost_names;
 }
 
 
@@ -1750,6 +1973,7 @@
         {
             direct_connections[i]->setLearningRate( the_learning_rate );
         }
+        reconstruction_connections[i]->setLearningRate( the_learning_rate );
     }
     layers[n_layers-1]->setLearningRate( the_learning_rate );
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-01-17 19:18:58 UTC (rev 8391)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-01-21 00:28:29 UTC (rev 8392)
@@ -150,11 +150,24 @@
     //! masked, i.e. unsused to reconstruct the input.
     real fraction_of_masked_inputs;
 
+    //! Number of samples to use for unsupervised fine-tuning
+    int unsupervised_nstages;
+
+    //! The learning rate used during the unsupervised fine tuning gradient descent
+    real unsupervised_fine_tuning_learning_rate;
+
+    //! The decrease constant of the learning rate used during 
+    //! unsupervised fine tuning gradient descent
+    real unsupervised_fine_tuning_decrease_ct;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
     int n_layers;
 
+    //! Number of samples visited so far during unsupervised fine-tuning
+    int unsupervised_stage;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -197,6 +210,9 @@
     void greedyStep( const Vec& input, const Vec& target, int index, 
                      Vec train_costs );
 
+    void unsupervisedFineTuningStep( const Vec& input, const Vec& target,
+                                     Vec& train_costs );
+
     void fineTuningStep( const Vec& input, const Vec& target,
                          Vec& train_costs );
 
@@ -241,15 +257,24 @@
     //! Reconstruction activations
     mutable Vec reconstruction_activations;
     
-    //! Reconstruction expectations
-    mutable Vec reconstruction_expectations;
-        
     //! Reconstruction activation gradients
     mutable Vec reconstruction_activation_gradients;
 
     //! Reconstruction expectation gradients
     mutable Vec reconstruction_expectation_gradients;
 
+    //! Unsupervised fine-tuning reconstruction activations
+    TVec< Vec > fine_tuning_reconstruction_activations;
+    
+    //! Unsupervised fine-tuning reconstruction expectations
+    TVec< Vec > fine_tuning_reconstruction_expectations;
+
+    //! Unsupervised fine-tuning reconstruction activations gradients
+    TVec< Vec > fine_tuning_reconstruction_activation_gradients;
+    
+    //! Unsupervised fine-tuning reconstruction expectations gradients
+    TVec< Vec > fine_tuning_reconstruction_expectation_gradients;
+
     //! Reconstruction activation gradients coming from hidden reconstruction
     mutable Vec reconstruction_activation_gradients_from_hid_rec;
     
@@ -305,9 +330,15 @@
     //! have been masked (set to 0) randomly.
     Vec masked_autoassociator_input;
 
+    //! Layers randomly masked, for unsupervised fine-tuning.
+    TVec< Vec > masked_autoassociator_expectations;
+
     //! Indices of the input components
     TVec<int> autoassociator_input_indices;
 
+    //! Indices of the expectation components
+    TVec< TVec<int> > autoassociator_expectation_indices;
+
     //! Stages of the different greedy phases
     TVec<int> greedy_stages;
 
@@ -315,12 +346,6 @@
     //! n_layers means the output layer)
     int currently_trained_layer;
 
-    //! Indication whether final_module has learning rate
-    bool final_module_has_learning_rate;
-    
-    //! Indication whether final_cost has learning rate
-    bool final_cost_has_learning_rate;
-
 protected:
     //#####  Protected Member Functions  ######################################
 



From larocheh at mail.berlios.de  Mon Jan 21 01:52:36 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 21 Jan 2008 01:52:36 +0100
Subject: [Plearn-commits] r8393 - trunk/plearn_learners_experimental
Message-ID: <200801210052.m0L0qa6D010433@sheep.berlios.de>

Author: larocheh
Date: 2008-01-21 01:52:36 +0100 (Mon, 21 Jan 2008)
New Revision: 8393

Modified:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
   trunk/plearn_learners_experimental/DiscriminativeRBM.h
Log:
Added option to use only generative learning.


Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-21 00:28:29 UTC (rev 8392)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-21 00:52:36 UTC (rev 8393)
@@ -66,7 +66,8 @@
     semi_sup_learning_weight( 0. ),
     n_classes( -1 ),
     target_weights_L1_penalty_factor( 0. ),
-    target_weights_L2_penalty_factor( 0. )
+    target_weights_L2_penalty_factor( 0. ),
+    do_not_use_discriminative_learning( false )
 {
     random_gen = new PRandom();
 }
@@ -134,6 +135,11 @@
                   OptionBase::buildoption,
                   "Target weights' L2_penalty_factor.\n");
 
+    declareOption(ol, "do_not_use_discriminative_learning", 
+                  &DiscriminativeRBM::do_not_use_discriminative_learning,
+                  OptionBase::buildoption,
+                  "Indication that discriminative learning should not be used.\n");
+
     declareOption(ol, "classification_module",
                   &DiscriminativeRBM::classification_module,
                   OptionBase::learntoption,
@@ -450,7 +456,8 @@
             target_one_hot[ target_index ] = 1;
         }
         // ... for discriminative learning
-        if( !use_exact_disc_gradient && !is_missing(target[0]) )
+        if( !do_not_use_discriminative_learning && 
+            !use_exact_disc_gradient && !is_missing(target[0]) )
         {
             // Positive phase
 
@@ -488,7 +495,7 @@
         if( !is_missing(target[0]) && gen_learning_weight > 0 )
         {
             // Positive phase
-            if( !use_exact_disc_gradient )
+            if( !use_exact_disc_gradient && !do_not_use_discriminative_learning )
             {
                 // Use previous computations
                 gen_pos_down_val << disc_pos_down_val;
@@ -591,7 +598,8 @@
 
         // Get gradient and update
 
-        if( use_exact_disc_gradient && !is_missing(target[0]) )
+        if( !do_not_use_discriminative_learning && 
+            use_exact_disc_gradient && !is_missing(target[0]) )
         {
             classification_module->fprop( input, class_output );
             // This doesn't work. gcc bug?
@@ -613,7 +621,8 @@
         }
 
         // CD Updates
-        if( !use_exact_disc_gradient && !is_missing(target[0]) )
+        if( !do_not_use_discriminative_learning && 
+            !use_exact_disc_gradient && !is_missing(target[0]) )
         {
             joint_layer->update( disc_pos_down_val, disc_neg_down_val );
             hidden_layer->update( disc_pos_up_val, disc_neg_up_val );

Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.h
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-01-21 00:28:29 UTC (rev 8392)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-01-21 00:52:36 UTC (rev 8393)
@@ -103,6 +103,9 @@
     //! Target weights' L2_penalty_factor
     real target_weights_L2_penalty_factor;
 
+    //! Indication that discriminative learning should not be used
+    bool do_not_use_discriminative_learning;
+
     //#####  Public Learnt Options  ###########################################
     //! The module computing the probabilities of the different classes.
     PP<RBMClassificationModule> classification_module;



From nouiz at mail.berlios.de  Mon Jan 21 15:48:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Jan 2008 15:48:07 +0100
Subject: [Plearn-commits] r8394 - in trunk/plearn_learners/regressors: .
	test test/RegressionTree test/RegressionTree/.pytest
	test/RegressionTree/.pytest/PL_RegressionTree
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat.metadata
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata
Message-ID: <200801211448.m0LEm74i017301@sheep.berlios.de>

Author: nouiz
Date: 2008-01-21 15:48:05 +0100 (Mon, 21 Jan 2008)
New Revision: 8394

Added:
   trunk/plearn_learners/regressors/test/RegressionTree/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/train_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/pytest.config
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
Added test for RegressionTree


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-01-21 14:48:05 UTC (rev 8394)
@@ -52,7 +52,8 @@
                         "a left leave for samples with values below the value of the splitting attribute, and a right leave for the others,\n"
     );
 
-RegressionTreeNode::RegressionTreeNode()
+RegressionTreeNode::RegressionTreeNode():
+    dummy_int(0)
 {
     build();
 }


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest
___________________________________________________________________
Name: svn:ignore
   + *.compilation_log




Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results


Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log
===================================================================

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,4526 @@
+*1 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 50 ;
+compute_train_stats = 1 ;
+complexity_penalty_factor = 0 ;
+multiclass_outputs = []
+;
+leave_template = *2 ->RegressionTreeLeave(
+id = 0 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *0 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+sorted_train_set = *3 ->RegressionTreeRegisters(
+report_progress = 1 ;
+verbosity = 0 ;
+tsource = *4 ->MemoryVMatrixNoSave(
+source = *5 ->TransposeVMatrix(
+source = *6 ->MemoryVMatrix(
+data = 3132  9  [ 
+2 	0.469999999999999973 	0.364999999999999991 	0.135000000000000009 	0.52200000000000002 	0.239499999999999991 	0.152499999999999997 	0.14499999999999999 	10 	
+2 	0.57999999999999996 	0.46000000000000002 	0.165000000000000008 	1.22750000000000004 	0.472999999999999976 	0.196500000000000008 	0.434999999999999998 	16 	
+1 	0.445000000000000007 	0.359999999999999987 	0.110000000000000001 	0.423499999999999988 	0.181999999999999995 	0.0764999999999999986 	0.140000000000000013 	9 	
+1 	0.284999999999999976 	0.209999999999999992 	0.0550000000000000003 	0.101000000000000006 	0.0415000000000000022 	0.0170000000000000012 	0.033500000000000002 	5 	
+1 	0.424999999999999989 	0.315000000000000002 	0.0950000000000000011 	0.367499999999999993 	0.186499999999999999 	0.0675000000000000044 	0.0985000000000000042 	7 	
+2 	0.260000000000000009 	0.200000000000000011 	0.0650000000000000022 	0.096000000000000002 	0.0439999999999999974 	0.0269999999999999997 	0.0299999999999999989 	6 	
+0 	0.46000000000000002 	0.349999999999999978 	0.115000000000000005 	0.46000000000000002 	0.202500000000000013 	0.111500000000000002 	0.116500000000000006 	6 	
+0 	0.57999999999999996 	0.455000000000000016 	0.165000000000000008 	1.13650000000000007 	0.368999999999999995 	0.300499999999999989 	0.275000000000000022 	13 	
+2 	0.665000000000000036 	0.5 	0.174999999999999989 	1.2975000000000001 	0.60750000000000004 	0.314000000000000001 	0.315000000000000002 	9 	
+0 	0.635000000000000009 	0.5 	0.179999999999999993 	1.31200000000000006 	0.529000000000000026 	0.248499999999999999 	0.484999999999999987 	18 	
+1 	0.520000000000000018 	0.390000000000000013 	0.130000000000000004 	0.554499999999999993 	0.235499999999999987 	0.1095 	0.189500000000000002 	7 	
+2 	0.320000000000000007 	0.239999999999999991 	0.0800000000000000017 	0.179999999999999993 	0.0800000000000000017 	0.0384999999999999995 	0.0550000000000000003 	6 	
+2 	0.424999999999999989 	0.315000000000000002 	0.125 	0.35249999999999998 	0.113500000000000004 	0.0565000000000000016 	0.130000000000000004 	18 	
+2 	0.594999999999999973 	0.450000000000000011 	0.14499999999999999 	0.958999999999999964 	0.463000000000000023 	0.206499999999999989 	0.253500000000000003 	10 	
+2 	0.474999999999999978 	0.354999999999999982 	0.119999999999999996 	0.479999999999999982 	0.234000000000000014 	0.101500000000000007 	0.135000000000000009 	8 	
+0 	0.445000000000000007 	0.33500000000000002 	0.110000000000000001 	0.435499999999999998 	0.202500000000000013 	0.1095 	0.119499999999999995 	6 	
+1 	0.385000000000000009 	0.294999999999999984 	0.0850000000000000061 	0.253500000000000003 	0.102999999999999994 	0.0575000000000000025 	0.0850000000000000061 	7 	
+2 	0.555000000000000049 	0.440000000000000002 	0.149999999999999994 	1.09200000000000008 	0.415999999999999981 	0.211999999999999994 	0.440500000000000003 	15 	
+1 	0.424999999999999989 	0.309999999999999998 	0.0899999999999999967 	0.30099999999999999 	0.138500000000000012 	0.0650000000000000022 	0.0800000000000000017 	7 	
+1 	0.560000000000000053 	0.429999999999999993 	0.130000000000000004 	0.72799999999999998 	0.33550000000000002 	0.143499999999999989 	0.217499999999999999 	8 	
+1 	0.209999999999999992 	0.149999999999999994 	0.0500000000000000028 	0.0420000000000000026 	0.0175000000000000017 	0.0125000000000000007 	0.0149999999999999994 	4 	
+2 	0.604999999999999982 	0.469999999999999973 	0.179999999999999993 	1.14050000000000007 	0.3755 	0.280500000000000027 	0.385000000000000009 	15 	
+1 	0.275000000000000022 	0.200000000000000011 	0.0550000000000000003 	0.0924999999999999989 	0.0379999999999999991 	0.0210000000000000013 	0.0259999999999999988 	4 	
+2 	0.619999999999999996 	0.474999999999999978 	0.195000000000000007 	1.35850000000000004 	0.593500000000000028 	0.336500000000000021 	0.3745 	10 	
+2 	0.535000000000000031 	0.419999999999999984 	0.125 	0.737999999999999989 	0.354999999999999982 	0.189500000000000002 	0.179499999999999993 	8 	
+0 	0.609999999999999987 	0.494999999999999996 	0.184999999999999998 	1.15300000000000002 	0.536000000000000032 	0.29049999999999998 	0.244999999999999996 	8 	
+1 	0.429999999999999993 	0.315000000000000002 	0.0950000000000000011 	0.378000000000000003 	0.174999999999999989 	0.0800000000000000017 	0.104499999999999996 	8 	
+2 	0.469999999999999973 	0.369999999999999996 	0.135000000000000009 	0.547000000000000042 	0.222000000000000003 	0.132500000000000007 	0.170000000000000012 	12 	
+0 	0.535000000000000031 	0.41499999999999998 	0.170000000000000012 	0.879000000000000004 	0.294999999999999984 	0.196500000000000008 	0.284999999999999976 	10 	
+2 	0.640000000000000013 	0.484999999999999987 	0.149999999999999994 	1.09800000000000009 	0.519499999999999962 	0.222000000000000003 	0.317500000000000004 	10 	
+1 	0.5 	0.390000000000000013 	0.125 	0.582999999999999963 	0.293999999999999984 	0.132000000000000006 	0.160500000000000004 	8 	
+0 	0.655000000000000027 	0.515000000000000013 	0.200000000000000011 	1.49399999999999999 	0.725500000000000034 	0.308999999999999997 	0.405000000000000027 	12 	
+1 	0.479999999999999982 	0.369999999999999996 	0.125 	0.543499999999999983 	0.243999999999999995 	0.101000000000000006 	0.165000000000000008 	9 	
+2 	0.625 	0.489999999999999991 	0.184999999999999998 	1.16900000000000004 	0.527499999999999969 	0.253500000000000003 	0.343999999999999972 	11 	
+1 	0.354999999999999982 	0.270000000000000018 	0.0749999999999999972 	0.203999999999999987 	0.304499999999999993 	0.0459999999999999992 	0.0594999999999999973 	7 	
+2 	0.604999999999999982 	0.469999999999999973 	0.160000000000000003 	1.17349999999999999 	0.497499999999999998 	0.240499999999999992 	0.344999999999999973 	12 	
+2 	0.584999999999999964 	0.469999999999999973 	0.165000000000000008 	1.40900000000000003 	0.800000000000000044 	0.229000000000000009 	0.294999999999999984 	10 	
+1 	0.419999999999999984 	0.320000000000000007 	0.115000000000000005 	0.376000000000000001 	0.169000000000000011 	0.0919999999999999984 	0.100000000000000006 	5 	
+1 	0.349999999999999978 	0.260000000000000009 	0.0749999999999999972 	0.179999999999999993 	0.0899999999999999967 	0.0245000000000000009 	0.0550000000000000003 	5 	
+0 	0.440000000000000002 	0.340000000000000024 	0.104999999999999996 	0.36399999999999999 	0.147999999999999993 	0.0805000000000000021 	0.117499999999999993 	8 	
+0 	0.434999999999999998 	0.325000000000000011 	0.115000000000000005 	0.391500000000000015 	0.153999999999999998 	0.0940000000000000002 	0.119999999999999996 	7 	
+2 	0.594999999999999973 	0.469999999999999973 	0.165000000000000008 	1.1080000000000001 	0.491499999999999992 	0.232500000000000012 	0.33450000000000002 	9 	
+2 	0.67000000000000004 	0.510000000000000009 	0.174999999999999989 	1.52649999999999997 	0.651000000000000023 	0.447500000000000009 	0.344999999999999973 	10 	
+1 	0.625 	0.469999999999999973 	0.154999999999999999 	1.19550000000000001 	0.643000000000000016 	0.205499999999999988 	0.314500000000000002 	12 	
+0 	0.630000000000000004 	0.484999999999999987 	0.170000000000000012 	1.32050000000000001 	0.594500000000000028 	0.344999999999999973 	0.344999999999999973 	9 	
+2 	0.589999999999999969 	0.474999999999999978 	0.160000000000000003 	0.945500000000000007 	0.381500000000000006 	0.183999999999999997 	0.270000000000000018 	19 	
+1 	0.604999999999999982 	0.434999999999999998 	0.130000000000000004 	0.902499999999999969 	0.431999999999999995 	0.173999999999999988 	0.260000000000000009 	11 	
+2 	0.594999999999999973 	0.455000000000000016 	0.195000000000000007 	1.33050000000000002 	0.45950000000000002 	0.32350000000000001 	0.344999999999999973 	19 	
+2 	0.309999999999999998 	0.225000000000000006 	0.0800000000000000017 	0.134500000000000008 	0.0539999999999999994 	0.0240000000000000005 	0.0500000000000000028 	7 	
+0 	0.520000000000000018 	0.400000000000000022 	0.130000000000000004 	0.624500000000000055 	0.214999999999999997 	0.206499999999999989 	0.170000000000000012 	15 	
+1 	0.200000000000000011 	0.14499999999999999 	0.0500000000000000028 	0.0359999999999999973 	0.0125000000000000007 	0.00800000000000000017 	0.0109999999999999994 	4 	
+1 	0.330000000000000016 	0.265000000000000013 	0.0850000000000000061 	0.196000000000000008 	0.0774999999999999994 	0.0304999999999999993 	0.0444999999999999979 	6 	
+2 	0.645000000000000018 	0.505000000000000004 	0.149999999999999994 	1.16050000000000009 	0.519000000000000017 	0.26150000000000001 	0.33500000000000002 	10 	
+2 	0.560000000000000053 	0.429999999999999993 	0.14499999999999999 	0.899499999999999966 	0.464000000000000024 	0.177499999999999991 	0.234000000000000014 	9 	
+2 	0.560000000000000053 	0.450000000000000011 	0.174999999999999989 	1.0109999999999999 	0.383500000000000008 	0.206499999999999989 	0.369999999999999996 	15 	
+0 	0.540000000000000036 	0.419999999999999984 	0.130000000000000004 	0.750499999999999945 	0.367999999999999994 	0.16750000000000001 	0.184499999999999997 	9 	
+0 	0.520000000000000018 	0.405000000000000027 	0.115000000000000005 	0.776000000000000023 	0.320000000000000007 	0.184499999999999997 	0.220000000000000001 	8 	
+2 	0.630000000000000004 	0.489999999999999991 	0.179999999999999993 	1.12999999999999989 	0.458000000000000018 	0.276500000000000024 	0.315000000000000002 	12 	
+2 	0.614999999999999991 	0.479999999999999982 	0.174999999999999989 	1.1180000000000001 	0.446000000000000008 	0.319500000000000006 	0.299999999999999989 	9 	
+1 	0.494999999999999996 	0.369999999999999996 	0.125 	0.471499999999999975 	0.20749999999999999 	0.0909999999999999976 	0.149999999999999994 	8 	
+1 	0.409999999999999976 	0.304999999999999993 	0.0899999999999999967 	0.353499999999999981 	0.157000000000000001 	0.0744999999999999968 	0.100000000000000006 	7 	
+1 	0.395000000000000018 	0.28999999999999998 	0.0950000000000000011 	0.303999999999999992 	0.127000000000000002 	0.0840000000000000052 	0.076999999999999999 	6 	
+1 	0.469999999999999973 	0.364999999999999991 	0.100000000000000006 	0.410999999999999976 	0.174999999999999989 	0.0855000000000000066 	0.135000000000000009 	8 	
+1 	0.369999999999999996 	0.270000000000000018 	0.0950000000000000011 	0.217499999999999999 	0.0970000000000000029 	0.0459999999999999992 	0.0650000000000000022 	6 	
+2 	0.320000000000000007 	0.244999999999999996 	0.0749999999999999972 	0.155499999999999999 	0.0585000000000000034 	0.0379999999999999991 	0.0490000000000000019 	11 	
+2 	0.630000000000000004 	0.479999999999999982 	0.165000000000000008 	1.28600000000000003 	0.603999999999999981 	0.271000000000000019 	0.349999999999999978 	8 	
+2 	0.625 	0.515000000000000013 	0.170000000000000012 	1.33099999999999996 	0.572500000000000009 	0.300499999999999989 	0.360999999999999988 	9 	
+2 	0.349999999999999978 	0.275000000000000022 	0.110000000000000001 	0.292499999999999982 	0.122499999999999998 	0.0635000000000000009 	0.0904999999999999971 	8 	
+2 	0.359999999999999987 	0.294999999999999984 	0.104999999999999996 	0.240999999999999992 	0.0864999999999999936 	0.0529999999999999985 	0.0950000000000000011 	8 	
+0 	0.434999999999999998 	0.325000000000000011 	0.110000000000000001 	0.433499999999999996 	0.177999999999999992 	0.0985000000000000042 	0.154999999999999999 	7 	
+2 	0.685000000000000053 	0.505000000000000004 	0.190000000000000002 	1.53299999999999992 	0.667000000000000037 	0.405500000000000027 	0.409999999999999976 	10 	
+0 	0.655000000000000027 	0.510000000000000009 	0.174999999999999989 	1.41500000000000004 	0.588500000000000023 	0.372499999999999998 	0.36399999999999999 	10 	
+0 	0.645000000000000018 	0.489999999999999991 	0.214999999999999997 	1.40599999999999992 	0.42649999999999999 	0.228500000000000009 	0.510000000000000009 	25 	
+2 	0.625 	0.5 	0.130000000000000004 	1.08200000000000007 	0.578500000000000014 	0.204499999999999987 	0.25 	8 	
+2 	0.709999999999999964 	0.540000000000000036 	0.165000000000000008 	1.95900000000000007 	0.766499999999999959 	0.26100000000000001 	0.780000000000000027 	18 	
+0 	0.75 	0.569999999999999951 	0.209999999999999992 	2.23600000000000021 	1.10899999999999999 	0.519499999999999962 	0.54500000000000004 	11 	
+0 	0.655000000000000027 	0.510000000000000009 	0.149999999999999994 	1.04299999999999993 	0.479499999999999982 	0.223000000000000004 	0.304999999999999993 	9 	
+1 	0.520000000000000018 	0.380000000000000004 	0.125 	0.554499999999999993 	0.287999999999999978 	0.129500000000000004 	0.16700000000000001 	8 	
+1 	0.434999999999999998 	0.33500000000000002 	0.100000000000000006 	0.324500000000000011 	0.135000000000000009 	0.0785000000000000003 	0.0980000000000000038 	7 	
+2 	0.655000000000000027 	0.510000000000000009 	0.214999999999999997 	1.78350000000000009 	0.888499999999999956 	0.409499999999999975 	0.419499999999999984 	11 	
+1 	0.465000000000000024 	0.354999999999999982 	0.110000000000000001 	0.473999999999999977 	0.23000000000000001 	0.100500000000000006 	0.119999999999999996 	7 	
+0 	0.614999999999999991 	0.474999999999999978 	0.174999999999999989 	1.19399999999999995 	0.559000000000000052 	0.259000000000000008 	0.316500000000000004 	11 	
+2 	0.550000000000000044 	0.41499999999999998 	0.174999999999999989 	1.04200000000000004 	0.329500000000000015 	0.232500000000000012 	0.29049999999999998 	15 	
+0 	0.550000000000000044 	0.380000000000000004 	0.165000000000000008 	1.20500000000000007 	0.543000000000000038 	0.293999999999999984 	0.33450000000000002 	10 	
+1 	0.424999999999999989 	0.325000000000000011 	0.110000000000000001 	0.333500000000000019 	0.172999999999999987 	0.0449999999999999983 	0.100000000000000006 	7 	
+1 	0.560000000000000053 	0.440000000000000002 	0.165000000000000008 	0.800000000000000044 	0.33500000000000002 	0.173499999999999988 	0.25 	12 	
+1 	0.515000000000000013 	0.419999999999999984 	0.149999999999999994 	0.672499999999999987 	0.255500000000000005 	0.133500000000000008 	0.234999999999999987 	10 	
+0 	0.619999999999999996 	0.5 	0.174999999999999989 	1.18599999999999994 	0.498499999999999999 	0.30149999999999999 	0.349999999999999978 	12 	
+2 	0.354999999999999982 	0.265000000000000013 	0.0899999999999999967 	0.16800000000000001 	0.0500000000000000028 	0.0410000000000000017 	0.0630000000000000004 	8 	
+0 	0.635000000000000009 	0.474999999999999978 	0.149999999999999994 	1.18450000000000011 	0.533000000000000029 	0.306999999999999995 	0.290999999999999981 	10 	
+1 	0.275000000000000022 	0.214999999999999997 	0.0749999999999999972 	0.115500000000000005 	0.0485000000000000014 	0.0290000000000000015 	0.0350000000000000033 	7 	
+1 	0.369999999999999996 	0.28999999999999998 	0.0800000000000000017 	0.254500000000000004 	0.107999999999999999 	0.0565000000000000016 	0.0700000000000000067 	6 	
+1 	0.630000000000000004 	0.474999999999999978 	0.14499999999999999 	1.0605 	0.516499999999999959 	0.219500000000000001 	0.280000000000000027 	10 	
+1 	0.405000000000000027 	0.299999999999999989 	0.110000000000000001 	0.320000000000000007 	0.171999999999999986 	0.0439999999999999974 	0.0929999999999999993 	7 	
+1 	0.330000000000000016 	0.255000000000000004 	0.0800000000000000017 	0.204999999999999988 	0.0894999999999999962 	0.0395000000000000004 	0.0550000000000000003 	7 	
+1 	0.46000000000000002 	0.349999999999999978 	0.110000000000000001 	0.400000000000000022 	0.17599999999999999 	0.0830000000000000043 	0.120499999999999996 	7 	
+1 	0.359999999999999987 	0.270000000000000018 	0.0850000000000000061 	0.196000000000000008 	0.0904999999999999971 	0.0340000000000000024 	0.0529999999999999985 	7 	
+2 	0.484999999999999987 	0.400000000000000022 	0.135000000000000009 	0.663000000000000034 	0.313 	0.137000000000000011 	0.200000000000000011 	10 	
+0 	0.5 	0.400000000000000022 	0.125 	0.667499999999999982 	0.26100000000000001 	0.131500000000000006 	0.220000000000000001 	10 	
+0 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.805000000000000049 	0.368999999999999995 	0.172499999999999987 	0.209999999999999992 	11 	
+1 	0.530000000000000027 	0.400000000000000022 	0.125 	0.616999999999999993 	0.279000000000000026 	0.127000000000000002 	0.190000000000000002 	8 	
+1 	0.405000000000000027 	0.299999999999999989 	0.0899999999999999967 	0.269000000000000017 	0.102999999999999994 	0.067000000000000004 	0.110000000000000001 	6 	
+2 	0.594999999999999973 	0.469999999999999973 	0.149999999999999994 	0.891499999999999959 	0.358999999999999986 	0.210499999999999993 	0.244999999999999996 	12 	
+2 	0.589999999999999969 	0.484999999999999987 	0.119999999999999996 	0.911000000000000032 	0.390000000000000013 	0.181999999999999995 	0.28999999999999998 	16 	
+0 	0.5 	0.440000000000000002 	0.154999999999999999 	0.741999999999999993 	0.202500000000000013 	0.200500000000000012 	0.211499999999999994 	14 	
+2 	0.550000000000000044 	0.429999999999999993 	0.160000000000000003 	0.929499999999999993 	0.317000000000000004 	0.173499999999999988 	0.354999999999999982 	13 	
+2 	0.660000000000000031 	0.530000000000000027 	0.170000000000000012 	1.39050000000000007 	0.590500000000000025 	0.211999999999999994 	0.453000000000000014 	15 	
+1 	0.280000000000000027 	0.204999999999999988 	0.0800000000000000017 	0.127000000000000002 	0.0519999999999999976 	0.0389999999999999999 	0.0420000000000000026 	9 	
+1 	0.5 	0.385000000000000009 	0.154999999999999999 	0.762000000000000011 	0.379500000000000004 	0.161000000000000004 	0.190000000000000002 	14 	
+0 	0.625 	0.515000000000000013 	0.179999999999999993 	1.34850000000000003 	0.525499999999999967 	0.252000000000000002 	0.392500000000000016 	14 	
+1 	0.484999999999999987 	0.380000000000000004 	0.140000000000000013 	0.673000000000000043 	0.217499999999999999 	0.130000000000000004 	0.195000000000000007 	18 	
+0 	0.550000000000000044 	0.429999999999999993 	0.125 	0.923000000000000043 	0.403500000000000025 	0.174999999999999989 	0.282999999999999974 	8 	
+2 	0.555000000000000049 	0.440000000000000002 	0.135000000000000009 	0.902499999999999969 	0.380500000000000005 	0.210499999999999993 	0.280000000000000027 	13 	
+0 	0.689999999999999947 	0.540000000000000036 	0.195000000000000007 	1.25249999999999995 	0.729999999999999982 	0.39750000000000002 	0.462000000000000022 	12 	
+1 	0.294999999999999984 	0.214999999999999997 	0.0850000000000000061 	0.128000000000000003 	0.0490000000000000019 	0.0340000000000000024 	0.0400000000000000008 	6 	
+2 	0.599999999999999978 	0.469999999999999973 	0.165000000000000008 	1.05899999999999994 	0.504000000000000004 	0.240999999999999992 	0.275000000000000022 	9 	
+2 	0.5 	0.409999999999999976 	0.149999999999999994 	0.662000000000000033 	0.281499999999999972 	0.137000000000000011 	0.220000000000000001 	11 	
+1 	0.535000000000000031 	0.385000000000000009 	0.179999999999999993 	1.08349999999999991 	0.495499999999999996 	0.22950000000000001 	0.303999999999999992 	8 	
+0 	0.540000000000000036 	0.474999999999999978 	0.154999999999999999 	1.21700000000000008 	0.530499999999999972 	0.307499999999999996 	0.340000000000000024 	16 	
+2 	0.635000000000000009 	0.525000000000000022 	0.160000000000000003 	1.19500000000000006 	0.543499999999999983 	0.245999999999999996 	0.33500000000000002 	12 	
+0 	0.535000000000000031 	0.400000000000000022 	0.135000000000000009 	0.821500000000000008 	0.393500000000000016 	0.196000000000000008 	0.204999999999999988 	8 	
+2 	0.5 	0.41499999999999998 	0.165000000000000008 	0.688500000000000001 	0.248999999999999999 	0.138000000000000012 	0.25 	13 	
+2 	0.465000000000000024 	0.359999999999999987 	0.115000000000000005 	0.579500000000000015 	0.294999999999999984 	0.139500000000000013 	0.119999999999999996 	7 	
+1 	0.234999999999999987 	0.170000000000000012 	0.0650000000000000022 	0.0625 	0.0229999999999999996 	0.0140000000000000003 	0.0219999999999999987 	6 	
+0 	0.5 	0.385000000000000009 	0.115000000000000005 	0.678499999999999992 	0.294499999999999984 	0.138000000000000012 	0.195000000000000007 	12 	
+0 	0.599999999999999978 	0.5 	0.154999999999999999 	1.33200000000000007 	0.623500000000000054 	0.283499999999999974 	0.349999999999999978 	8 	
+1 	0.555000000000000049 	0.450000000000000011 	0.174999999999999989 	0.737999999999999989 	0.303999999999999992 	0.175499999999999989 	0.220000000000000001 	9 	
+1 	0.41499999999999998 	0.330000000000000016 	0.0899999999999999967 	0.359499999999999986 	0.170000000000000012 	0.0810000000000000026 	0.0899999999999999967 	6 	
+0 	0.525000000000000022 	0.429999999999999993 	0.135000000000000009 	0.843500000000000028 	0.432499999999999996 	0.179999999999999993 	0.181499999999999995 	9 	
+1 	0.309999999999999998 	0.239999999999999991 	0.104999999999999996 	0.288499999999999979 	0.117999999999999994 	0.0650000000000000022 	0.0830000000000000043 	6 	
+0 	0.675000000000000044 	0.510000000000000009 	0.195000000000000007 	1.3819999999999999 	0.604500000000000037 	0.317500000000000004 	0.396500000000000019 	10 	
+2 	0.685000000000000053 	0.54500000000000004 	0.204999999999999988 	1.79249999999999998 	0.814500000000000002 	0.415999999999999981 	0.461000000000000021 	9 	
+1 	0.330000000000000016 	0.265000000000000013 	0.0899999999999999967 	0.179999999999999993 	0.0680000000000000049 	0.0359999999999999973 	0.0599999999999999978 	6 	
+0 	0.494999999999999996 	0.380000000000000004 	0.119999999999999996 	0.572999999999999954 	0.265500000000000014 	0.128500000000000003 	0.143999999999999989 	7 	
+1 	0.315000000000000002 	0.23000000000000001 	0.0700000000000000067 	0.114500000000000005 	0.0459999999999999992 	0.0235000000000000001 	0.0384999999999999995 	5 	
+2 	0.765000000000000013 	0.599999999999999978 	0.220000000000000001 	2.30200000000000005 	1.0069999999999999 	0.509000000000000008 	0.620500000000000052 	12 	
+2 	0.699999999999999956 	0.564999999999999947 	0.179999999999999993 	1.75099999999999989 	0.895000000000000018 	0.33550000000000002 	0.446000000000000008 	9 	
+0 	0.594999999999999973 	0.494999999999999996 	0.234999999999999987 	1.3660000000000001 	0.50649999999999995 	0.219 	0.520000000000000018 	13 	
+2 	0.609999999999999987 	0.469999999999999973 	0.165000000000000008 	1.05200000000000005 	0.497999999999999998 	0.241999999999999993 	0.267000000000000015 	9 	
+0 	0.594999999999999973 	0.474999999999999978 	0.170000000000000012 	1.24700000000000011 	0.479999999999999982 	0.225000000000000006 	0.424999999999999989 	20 	
+1 	0.405000000000000027 	0.299999999999999989 	0.0899999999999999967 	0.288499999999999979 	0.138000000000000012 	0.0635000000000000009 	0.0764999999999999986 	6 	
+2 	0.625 	0.494999999999999996 	0.154999999999999999 	1.04849999999999999 	0.486999999999999988 	0.211999999999999994 	0.321500000000000008 	11 	
+0 	0.635000000000000009 	0.510000000000000009 	0.184999999999999998 	1.28600000000000003 	0.526000000000000023 	0.294999999999999984 	0.410499999999999976 	12 	
+2 	0.719999999999999973 	0.550000000000000044 	0.204999999999999988 	2.125 	1.14549999999999996 	0.442500000000000004 	0.51100000000000001 	13 	
+1 	0.46000000000000002 	0.359999999999999987 	0.100000000000000006 	0.463500000000000023 	0.232500000000000012 	0.0929999999999999993 	0.115000000000000005 	7 	
+2 	0.694999999999999951 	0.550000000000000044 	0.220000000000000001 	1.5515000000000001 	0.565999999999999948 	0.383500000000000008 	0.445000000000000007 	13 	
+0 	0.440000000000000002 	0.349999999999999978 	0.125 	0.403500000000000025 	0.174999999999999989 	0.0630000000000000004 	0.129000000000000004 	9 	
+2 	0.569999999999999951 	0.479999999999999982 	0.174999999999999989 	1.18500000000000005 	0.473999999999999977 	0.26100000000000001 	0.380000000000000004 	11 	
+1 	0.340000000000000024 	0.255000000000000004 	0.0749999999999999972 	0.179999999999999993 	0.0744999999999999968 	0.0400000000000000008 	0.0524999999999999981 	6 	
+2 	0.604999999999999982 	0.489999999999999991 	0.140000000000000013 	0.975500000000000034 	0.418999999999999984 	0.205999999999999989 	0.315000000000000002 	10 	
+1 	0.505000000000000004 	0.400000000000000022 	0.14499999999999999 	0.704500000000000015 	0.334000000000000019 	0.142499999999999988 	0.20699999999999999 	8 	
+1 	0.484999999999999987 	0.390000000000000013 	0.125 	0.59099999999999997 	0.286999999999999977 	0.140999999999999986 	0.119999999999999996 	9 	
+2 	0.400000000000000022 	0.304999999999999993 	0.0850000000000000061 	0.296999999999999986 	0.107999999999999999 	0.0704999999999999932 	0.100000000000000006 	10 	
+2 	0.630000000000000004 	0.510000000000000009 	0.23000000000000001 	1.53899999999999992 	0.563500000000000001 	0.281499999999999972 	0.569999999999999951 	17 	
+1 	0.340000000000000024 	0.25 	0.0749999999999999972 	0.178499999999999992 	0.0665000000000000036 	0.0454999999999999988 	0.0449999999999999983 	5 	
+0 	0.680000000000000049 	0.550000000000000044 	0.200000000000000011 	1.59600000000000009 	0.525000000000000022 	0.407499999999999973 	0.584999999999999964 	21 	
+2 	0.635000000000000009 	0.5 	0.179999999999999993 	1.15399999999999991 	0.440500000000000003 	0.231500000000000011 	0.387000000000000011 	9 	
+2 	0.739999999999999991 	0.569999999999999951 	0.179999999999999993 	1.87250000000000005 	0.911499999999999977 	0.426999999999999991 	0.446000000000000008 	10 	
+1 	0.469999999999999973 	0.354999999999999982 	0.119999999999999996 	0.368499999999999994 	0.126000000000000001 	0.0835000000000000048 	0.13650000000000001 	6 	
+0 	0.640000000000000013 	0.484999999999999987 	0.184999999999999998 	1.41949999999999998 	0.673499999999999988 	0.346499999999999975 	0.325500000000000012 	11 	
+2 	0.650000000000000022 	0.525000000000000022 	0.184999999999999998 	1.62200000000000011 	0.66449999999999998 	0.322500000000000009 	0.47699999999999998 	10 	
+0 	0.465000000000000024 	0.349999999999999978 	0.135000000000000009 	0.626499999999999946 	0.259000000000000008 	0.14449999999999999 	0.174999999999999989 	8 	
+1 	0.375 	0.275000000000000022 	0.100000000000000006 	0.232500000000000012 	0.116500000000000006 	0.0420000000000000026 	0.0650000000000000022 	6 	
+0 	0.574999999999999956 	0.434999999999999998 	0.154999999999999999 	0.897499999999999964 	0.411499999999999977 	0.232500000000000012 	0.23000000000000001 	9 	
+0 	0.599999999999999978 	0.465000000000000024 	0.165000000000000008 	0.887499999999999956 	0.308999999999999997 	0.245999999999999996 	0.262000000000000011 	12 	
+0 	0.660000000000000031 	0.474999999999999978 	0.179999999999999993 	1.36949999999999994 	0.641000000000000014 	0.293999999999999984 	0.33500000000000002 	6 	
+2 	0.540000000000000036 	0.41499999999999998 	0.170000000000000012 	0.879000000000000004 	0.339000000000000024 	0.20799999999999999 	0.255000000000000004 	10 	
+2 	0.67000000000000004 	0.550000000000000044 	0.170000000000000012 	1.24700000000000011 	0.471999999999999975 	0.245499999999999996 	0.400000000000000022 	21 	
+2 	0.609999999999999987 	0.469999999999999973 	0.170000000000000012 	1.11850000000000005 	0.522499999999999964 	0.240499999999999992 	0.309999999999999998 	9 	
+2 	0.599999999999999978 	0.494999999999999996 	0.165000000000000008 	1.24150000000000005 	0.484999999999999987 	0.277500000000000024 	0.340000000000000024 	15 	
+1 	0.560000000000000053 	0.424999999999999989 	0.14499999999999999 	0.687999999999999945 	0.309499999999999997 	0.130500000000000005 	0.216499999999999998 	9 	
+0 	0.569999999999999951 	0.465000000000000024 	0.179999999999999993 	0.999500000000000055 	0.405000000000000027 	0.277000000000000024 	0.294999999999999984 	16 	
+0 	0.569999999999999951 	0.440000000000000002 	0.125 	0.864999999999999991 	0.367499999999999993 	0.172499999999999987 	0.270000000000000018 	12 	
+1 	0.33500000000000002 	0.25 	0.0800000000000000017 	0.182999999999999996 	0.0734999999999999959 	0.0400000000000000008 	0.0575000000000000025 	6 	
+0 	0.54500000000000004 	0.445000000000000007 	0.149999999999999994 	0.800000000000000044 	0.353499999999999981 	0.163000000000000006 	0.20699999999999999 	9 	
+1 	0.434999999999999998 	0.325000000000000011 	0.119999999999999996 	0.345999999999999974 	0.159000000000000002 	0.0840000000000000052 	0.0950000000000000011 	7 	
+2 	0.505000000000000004 	0.405000000000000027 	0.110000000000000001 	0.625 	0.304999999999999993 	0.160000000000000003 	0.174999999999999989 	9 	
+2 	0.645000000000000018 	0.494999999999999996 	0.149999999999999994 	1.20950000000000002 	0.60299999999999998 	0.222500000000000003 	0.339000000000000024 	9 	
+2 	0.599999999999999978 	0.46000000000000002 	0.154999999999999999 	0.95950000000000002 	0.445500000000000007 	0.189000000000000001 	0.294999999999999984 	11 	
+1 	0.204999999999999988 	0.140000000000000013 	0.0500000000000000028 	0.0459999999999999992 	0.0165000000000000008 	0.0120000000000000002 	0.0134999999999999998 	6 	
+0 	0.599999999999999978 	0.465000000000000024 	0.149999999999999994 	1.10250000000000004 	0.545499999999999985 	0.262000000000000011 	0.25 	8 	
+1 	0.450000000000000011 	0.340000000000000024 	0.119999999999999996 	0.492499999999999993 	0.240999999999999992 	0.107499999999999998 	0.119999999999999996 	6 	
+1 	0.440000000000000002 	0.354999999999999982 	0.165000000000000008 	0.434999999999999998 	0.159000000000000002 	0.104999999999999996 	0.140000000000000013 	16 	
+1 	0.465000000000000024 	0.344999999999999973 	0.110000000000000001 	0.393000000000000016 	0.182499999999999996 	0.0734999999999999959 	0.119999999999999996 	8 	
+1 	0.275000000000000022 	0.195000000000000007 	0.0700000000000000067 	0.0874999999999999944 	0.0345000000000000029 	0.0219999999999999987 	0.0254999999999999984 	4 	
+0 	0.660000000000000031 	0.5 	0.165000000000000008 	1.19049999999999989 	0.458500000000000019 	0.297999999999999987 	0.369999999999999996 	12 	
+0 	0.574999999999999956 	0.434999999999999998 	0.149999999999999994 	1.03049999999999997 	0.46050000000000002 	0.217999999999999999 	0.359999999999999987 	8 	
+1 	0.424999999999999989 	0.320000000000000007 	0.100000000000000006 	0.305499999999999994 	0.126000000000000001 	0.0599999999999999978 	0.105999999999999997 	7 	
+1 	0.520000000000000018 	0.409999999999999976 	0.119999999999999996 	0.594999999999999973 	0.23849999999999999 	0.111000000000000001 	0.190000000000000002 	8 	
+0 	0.70499999999999996 	0.535000000000000031 	0.220000000000000001 	1.8660000000000001 	0.929000000000000048 	0.383500000000000008 	0.439500000000000002 	10 	
+2 	0.525000000000000022 	0.41499999999999998 	0.14499999999999999 	0.844999999999999973 	0.35249999999999998 	0.163500000000000006 	0.287499999999999978 	8 	
+1 	0.494999999999999996 	0.400000000000000022 	0.104999999999999996 	0.60199999999999998 	0.2505 	0.126500000000000001 	0.190000000000000002 	8 	
+2 	0.650000000000000022 	0.510000000000000009 	0.154999999999999999 	1.40700000000000003 	0.72150000000000003 	0.297999999999999987 	0.33500000000000002 	9 	
+2 	0.609999999999999987 	0.479999999999999982 	0.184999999999999998 	1.30649999999999999 	0.689500000000000002 	0.291499999999999981 	0.28999999999999998 	10 	
+2 	0.419999999999999984 	0.340000000000000024 	0.115000000000000005 	0.421499999999999986 	0.174999999999999989 	0.0929999999999999993 	0.135000000000000009 	8 	
+2 	0.729999999999999982 	0.574999999999999956 	0.209999999999999992 	2.06899999999999995 	0.928499999999999992 	0.408999999999999975 	0.643000000000000016 	11 	
+2 	0.584999999999999964 	0.455000000000000016 	0.149999999999999994 	0.906000000000000028 	0.409499999999999975 	0.23000000000000001 	0.233500000000000013 	8 	
+0 	0.505000000000000004 	0.385000000000000009 	0.115000000000000005 	0.615999999999999992 	0.242999999999999994 	0.107499999999999998 	0.209999999999999992 	11 	
+2 	0.599999999999999978 	0.494999999999999996 	0.174999999999999989 	1.30049999999999999 	0.619500000000000051 	0.283999999999999975 	0.328500000000000014 	11 	
+2 	0.375 	0.28999999999999998 	0.100000000000000006 	0.276000000000000023 	0.117499999999999993 	0.0565000000000000016 	0.0850000000000000061 	9 	
+0 	0.520000000000000018 	0.405000000000000027 	0.140000000000000013 	0.691500000000000004 	0.276000000000000023 	0.137000000000000011 	0.214999999999999997 	11 	
+1 	0.419999999999999984 	0.304999999999999993 	0.110000000000000001 	0.280000000000000027 	0.0940000000000000002 	0.0785000000000000003 	0.0955000000000000016 	9 	
+1 	0.54500000000000004 	0.400000000000000022 	0.130000000000000004 	0.686000000000000054 	0.328500000000000014 	0.14549999999999999 	0.179999999999999993 	9 	
+2 	0.465000000000000024 	0.359999999999999987 	0.110000000000000001 	0.495499999999999996 	0.266500000000000015 	0.0850000000000000061 	0.120999999999999996 	7 	
+1 	0.349999999999999978 	0.25 	0.0700000000000000067 	0.160500000000000004 	0.0714999999999999941 	0.033500000000000002 	0.0459999999999999992 	6 	
+0 	0.655000000000000027 	0.515000000000000013 	0.154999999999999999 	1.30899999999999994 	0.524000000000000021 	0.345999999999999974 	0.385000000000000009 	11 	
+1 	0.530000000000000027 	0.419999999999999984 	0.154999999999999999 	0.810000000000000053 	0.472499999999999976 	0.111000000000000001 	0.192000000000000004 	10 	
+1 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.807000000000000051 	0.361499999999999988 	0.17599999999999999 	0.254000000000000004 	10 	
+1 	0.489999999999999991 	0.400000000000000022 	0.135000000000000009 	0.623999999999999999 	0.303499999999999992 	0.128500000000000003 	0.169000000000000011 	8 	
+1 	0.5 	0.375 	0.14499999999999999 	0.579500000000000015 	0.23899999999999999 	0.137500000000000011 	0.184999999999999998 	9 	
+2 	0.28999999999999998 	0.23000000000000001 	0.0749999999999999972 	0.116500000000000006 	0.0429999999999999966 	0.0254999999999999984 	0.0400000000000000008 	7 	
+1 	0.599999999999999978 	0.474999999999999978 	0.149999999999999994 	1.12000000000000011 	0.564999999999999947 	0.246499999999999997 	0.270000000000000018 	10 	
+1 	0.174999999999999989 	0.125 	0.0500000000000000028 	0.0235000000000000001 	0.00800000000000000017 	0.00350000000000000007 	0.00800000000000000017 	5 	
+2 	0.655000000000000027 	0.530000000000000027 	0.195000000000000007 	1.3879999999999999 	0.566999999999999948 	0.273500000000000021 	0.409999999999999976 	13 	
+2 	0.645000000000000018 	0.5 	0.160000000000000003 	1.38149999999999995 	0.672000000000000042 	0.326000000000000012 	0.315000000000000002 	9 	
+2 	0.609999999999999987 	0.484999999999999987 	0.170000000000000012 	1.28099999999999992 	0.596999999999999975 	0.303499999999999992 	0.330000000000000016 	9 	
+2 	0.660000000000000031 	0.535000000000000031 	0.200000000000000011 	1.79099999999999993 	0.732999999999999985 	0.318000000000000005 	0.540000000000000036 	15 	
+2 	0.67000000000000004 	0.520000000000000018 	0.190000000000000002 	1.63850000000000007 	0.811499999999999999 	0.368999999999999995 	0.391000000000000014 	9 	
+2 	0.724999999999999978 	0.505000000000000004 	0.184999999999999998 	1.97799999999999998 	1.02600000000000002 	0.425499999999999989 	0.450500000000000012 	12 	
+1 	0.560000000000000053 	0.445000000000000007 	0.154999999999999999 	0.873500000000000054 	0.300499999999999989 	0.208999999999999991 	0.275000000000000022 	16 	
+1 	0.469999999999999973 	0.359999999999999987 	0.130000000000000004 	0.522499999999999964 	0.198000000000000009 	0.106499999999999997 	0.165000000000000008 	9 	
+2 	0.424999999999999989 	0.330000000000000016 	0.130000000000000004 	0.440500000000000003 	0.151999999999999996 	0.0934999999999999998 	0.154999999999999999 	9 	
+2 	0.574999999999999956 	0.445000000000000007 	0.170000000000000012 	1.02249999999999996 	0.549000000000000044 	0.217499999999999999 	0.228000000000000008 	9 	
+2 	0.440000000000000002 	0.325000000000000011 	0.0800000000000000017 	0.412999999999999978 	0.143999999999999989 	0.101500000000000007 	0.130000000000000004 	8 	
+0 	0.689999999999999947 	0.530000000000000027 	0.170000000000000012 	1.5535000000000001 	0.794499999999999984 	0.348499999999999976 	0.369499999999999995 	9 	
+2 	0.699999999999999956 	0.57999999999999996 	0.204999999999999988 	2.12999999999999989 	0.741500000000000048 	0.489999999999999991 	0.57999999999999996 	20 	
+2 	0.57999999999999996 	0.455000000000000016 	0.149999999999999994 	1.01200000000000001 	0.498499999999999999 	0.211499999999999994 	0.283499999999999974 	10 	
+2 	0.614999999999999991 	0.484999999999999987 	0.214999999999999997 	0.961500000000000021 	0.421999999999999986 	0.17599999999999999 	0.28999999999999998 	11 	
+0 	0.660000000000000031 	0.515000000000000013 	0.170000000000000012 	1.33699999999999997 	0.614999999999999991 	0.3125 	0.357499999999999984 	10 	
+1 	0.469999999999999973 	0.354999999999999982 	0.125 	0.498999999999999999 	0.209999999999999992 	0.0985000000000000042 	0.154999999999999999 	8 	
+0 	0.530000000000000027 	0.409999999999999976 	0.130000000000000004 	0.696500000000000008 	0.301999999999999991 	0.193500000000000005 	0.200000000000000011 	10 	
+2 	0.680000000000000049 	0.540000000000000036 	0.154999999999999999 	1.53400000000000003 	0.671000000000000041 	0.379000000000000004 	0.384000000000000008 	10 	
+1 	0.409999999999999976 	0.309999999999999998 	0.0899999999999999967 	0.333500000000000019 	0.163500000000000006 	0.0609999999999999987 	0.0909999999999999976 	6 	
+0 	0.650000000000000022 	0.564999999999999947 	0.200000000000000011 	1.66450000000000009 	0.753000000000000003 	0.366999999999999993 	0.429999999999999993 	12 	
+2 	0.574999999999999956 	0.450000000000000011 	0.165000000000000008 	0.965500000000000025 	0.497999999999999998 	0.190000000000000002 	0.23000000000000001 	8 	
+0 	0.699999999999999956 	0.584999999999999964 	0.184999999999999998 	1.80750000000000011 	0.705500000000000016 	0.321500000000000008 	0.474999999999999978 	29 	
+1 	0.265000000000000013 	0.195000000000000007 	0.0550000000000000003 	0.0840000000000000052 	0.0364999999999999977 	0.0175000000000000017 	0.0250000000000000014 	7 	
+2 	0.429999999999999993 	0.33500000000000002 	0.119999999999999996 	0.39700000000000002 	0.19850000000000001 	0.0864999999999999936 	0.103499999999999995 	7 	
+2 	0.609999999999999987 	0.484999999999999987 	0.160000000000000003 	1.01449999999999996 	0.531499999999999972 	0.211999999999999994 	0.241499999999999992 	8 	
+2 	0.525000000000000022 	0.400000000000000022 	0.140000000000000013 	0.720500000000000029 	0.368499999999999994 	0.14499999999999999 	0.173499999999999988 	8 	
+0 	0.489999999999999991 	0.364999999999999991 	0.130000000000000004 	0.683499999999999996 	0.165000000000000008 	0.131500000000000006 	0.204999999999999988 	21 	
+2 	0.609999999999999987 	0.474999999999999978 	0.154999999999999999 	1.16799999999999993 	0.554000000000000048 	0.23899999999999999 	0.329500000000000015 	10 	
+2 	0.57999999999999996 	0.440000000000000002 	0.174999999999999989 	1.22550000000000003 	0.54049999999999998 	0.270500000000000018 	0.326500000000000012 	10 	
+1 	0.239999999999999991 	0.170000000000000012 	0.0500000000000000028 	0.0544999999999999998 	0.0205000000000000009 	0.0160000000000000003 	0.0154999999999999999 	5 	
+0 	0.479999999999999982 	0.400000000000000022 	0.125 	0.759000000000000008 	0.212499999999999994 	0.178999999999999992 	0.239999999999999991 	15 	
+2 	0.535000000000000031 	0.405000000000000027 	0.184999999999999998 	0.83450000000000002 	0.317500000000000004 	0.172499999999999987 	0.28999999999999998 	16 	
+0 	0.724999999999999978 	0.599999999999999978 	0.200000000000000011 	1.7370000000000001 	0.696999999999999953 	0.358499999999999985 	0.594999999999999973 	11 	
+2 	0.494999999999999996 	0.395000000000000018 	0.119999999999999996 	0.553000000000000047 	0.224000000000000005 	0.137500000000000011 	0.16700000000000001 	8 	
+2 	0.405000000000000027 	0.304999999999999993 	0.119999999999999996 	0.318500000000000005 	0.123499999999999999 	0.0904999999999999971 	0.0950000000000000011 	7 	
+2 	0.28999999999999998 	0.225000000000000006 	0.0800000000000000017 	0.129500000000000004 	0.0534999999999999989 	0.0259999999999999988 	0.0449999999999999983 	10 	
+2 	0.594999999999999973 	0.46000000000000002 	0.140000000000000013 	0.85199999999999998 	0.421499999999999986 	0.225500000000000006 	0.227000000000000007 	9 	
+1 	0.450000000000000011 	0.330000000000000016 	0.100000000000000006 	0.410999999999999976 	0.194500000000000006 	0.100000000000000006 	0.0980000000000000038 	6 	
+2 	0.520000000000000018 	0.385000000000000009 	0.115000000000000005 	0.669000000000000039 	0.23849999999999999 	0.171999999999999986 	0.204999999999999988 	12 	
+1 	0.540000000000000036 	0.424999999999999989 	0.135000000000000009 	0.686000000000000054 	0.347499999999999976 	0.154499999999999998 	0.212999999999999995 	8 	
+0 	0.630000000000000004 	0.494999999999999996 	0.165000000000000008 	1.30750000000000011 	0.598999999999999977 	0.283999999999999975 	0.315000000000000002 	11 	
+2 	0.440000000000000002 	0.375 	0.130000000000000004 	0.486999999999999988 	0.226000000000000006 	0.0965000000000000024 	0.154999999999999999 	9 	
+2 	0.424999999999999989 	0.330000000000000016 	0.0800000000000000017 	0.360999999999999988 	0.134000000000000008 	0.0825000000000000039 	0.125 	7 	
+2 	0.309999999999999998 	0.244999999999999996 	0.0950000000000000011 	0.149999999999999994 	0.0524999999999999981 	0.0340000000000000024 	0.048000000000000001 	7 	
+2 	0.614999999999999991 	0.505000000000000004 	0.165000000000000008 	1.34000000000000008 	0.531499999999999972 	0.281499999999999972 	0.409999999999999976 	12 	
+2 	0.5 	0.405000000000000027 	0.140000000000000013 	0.615500000000000047 	0.240999999999999992 	0.135500000000000009 	0.204999999999999988 	9 	
+0 	0.614999999999999991 	0.474999999999999978 	0.154999999999999999 	1.004 	0.447500000000000009 	0.193000000000000005 	0.28949999999999998 	10 	
+1 	0.375 	0.275000000000000022 	0.0899999999999999967 	0.217999999999999999 	0.0929999999999999993 	0.0405000000000000013 	0.0754999999999999977 	6 	
+1 	0.340000000000000024 	0.244999999999999996 	0.0850000000000000061 	0.201500000000000012 	0.100500000000000006 	0.0379999999999999991 	0.0529999999999999985 	6 	
+0 	0.619999999999999996 	0.5 	0.170000000000000012 	1.14799999999999991 	0.547499999999999987 	0.220000000000000001 	0.331500000000000017 	10 	
+2 	0.599999999999999978 	0.479999999999999982 	0.0899999999999999967 	1.05000000000000004 	0.457000000000000017 	0.268500000000000016 	0.280000000000000027 	8 	
+1 	0.409999999999999976 	0.325000000000000011 	0.110000000000000001 	0.326000000000000012 	0.132500000000000007 	0.0749999999999999972 	0.101000000000000006 	8 	
+1 	0.275000000000000022 	0.200000000000000011 	0.0749999999999999972 	0.0859999999999999931 	0.0304999999999999993 	0.0189999999999999995 	0.0299999999999999989 	7 	
+0 	0.655000000000000027 	0.510000000000000009 	0.154999999999999999 	1.28950000000000009 	0.534499999999999975 	0.285499999999999976 	0.409999999999999976 	11 	
+1 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.645499999999999963 	0.325000000000000011 	0.1245 	0.170000000000000012 	8 	
+0 	0.680000000000000049 	0.560000000000000053 	0.165000000000000008 	1.63900000000000001 	0.605500000000000038 	0.280500000000000027 	0.46000000000000002 	15 	
+2 	0.625 	0.469999999999999973 	0.179999999999999993 	1.1359999999999999 	0.451000000000000012 	0.324500000000000011 	0.304999999999999993 	11 	
+0 	0.409999999999999976 	0.304999999999999993 	0.100000000000000006 	0.362999999999999989 	0.173499999999999988 	0.0650000000000000022 	0.110000000000000001 	11 	
+1 	0.474999999999999978 	0.354999999999999982 	0.115000000000000005 	0.519499999999999962 	0.279000000000000026 	0.0879999999999999949 	0.132500000000000007 	7 	
+2 	0.645000000000000018 	0.530000000000000027 	0.195000000000000007 	1.3899999999999999 	0.646499999999999964 	0.294499999999999984 	0.373499999999999999 	10 	
+1 	0.434999999999999998 	0.325000000000000011 	0.119999999999999996 	0.399500000000000022 	0.181499999999999995 	0.0609999999999999987 	0.112500000000000003 	8 	
+0 	0.429999999999999993 	0.340000000000000024 	0.110000000000000001 	0.382000000000000006 	0.153999999999999998 	0.0955000000000000016 	0.109 	8 	
+2 	0.505000000000000004 	0.385000000000000009 	0.149999999999999994 	0.641499999999999959 	0.245999999999999996 	0.151999999999999996 	0.214999999999999997 	12 	
+0 	0.419999999999999984 	0.330000000000000016 	0.125 	0.463000000000000023 	0.185999999999999999 	0.110000000000000001 	0.14499999999999999 	10 	
+1 	0.130000000000000004 	0.0950000000000000011 	0.0350000000000000033 	0.0105000000000000007 	0.0050000000000000001 	0.0064999999999999997 	0.00350000000000000007 	4 	
+1 	0.484999999999999987 	0.41499999999999998 	0.140000000000000013 	0.570500000000000007 	0.25 	0.134000000000000008 	0.184999999999999998 	8 	
+0 	0.530000000000000027 	0.424999999999999989 	0.130000000000000004 	0.758499999999999952 	0.325000000000000011 	0.197000000000000008 	0.204999999999999988 	8 	
+1 	0.380000000000000004 	0.275000000000000022 	0.110000000000000001 	0.256000000000000005 	0.110000000000000001 	0.0534999999999999989 	0.0754999999999999977 	6 	
+1 	0.315000000000000002 	0.244999999999999996 	0.0850000000000000061 	0.143499999999999989 	0.0529999999999999985 	0.0475000000000000006 	0.0500000000000000028 	8 	
+1 	0.28999999999999998 	0.209999999999999992 	0.0599999999999999978 	0.119499999999999995 	0.0560000000000000012 	0.0235000000000000001 	0.0299999999999999989 	6 	
+1 	0.550000000000000044 	0.424999999999999989 	0.14499999999999999 	0.890000000000000013 	0.432499999999999996 	0.171000000000000013 	0.235999999999999988 	10 	
+1 	0.340000000000000024 	0.260000000000000009 	0.0800000000000000017 	0.200000000000000011 	0.0800000000000000017 	0.0555000000000000007 	0.0550000000000000003 	6 	
+1 	0.359999999999999987 	0.280000000000000027 	0.0800000000000000017 	0.175499999999999989 	0.0810000000000000026 	0.0505000000000000032 	0.0700000000000000067 	6 	
+1 	0.619999999999999996 	0.450000000000000011 	0.135000000000000009 	0.924000000000000044 	0.357999999999999985 	0.226500000000000007 	0.296499999999999986 	10 	
+2 	0.400000000000000022 	0.28999999999999998 	0.115000000000000005 	0.279500000000000026 	0.111500000000000002 	0.0575000000000000025 	0.0749999999999999972 	9 	
+1 	0.390000000000000013 	0.280000000000000027 	0.0899999999999999967 	0.214999999999999997 	0.0845000000000000057 	0.0340000000000000024 	0.0790000000000000008 	8 	
+1 	0.244999999999999996 	0.204999999999999988 	0.0599999999999999978 	0.0764999999999999986 	0.0340000000000000024 	0.0140000000000000003 	0.0214999999999999983 	4 	
+2 	0.67000000000000004 	0.520000000000000018 	0.165000000000000008 	1.3899999999999999 	0.710999999999999965 	0.286499999999999977 	0.299999999999999989 	11 	
+2 	0.589999999999999969 	0.484999999999999987 	0.154999999999999999 	1.07850000000000001 	0.453500000000000014 	0.243499999999999994 	0.309999999999999998 	9 	
+0 	0.515000000000000013 	0.405000000000000027 	0.119999999999999996 	0.646000000000000019 	0.28949999999999998 	0.140500000000000014 	0.176999999999999991 	10 	
+1 	0.445000000000000007 	0.320000000000000007 	0.119999999999999996 	0.378000000000000003 	0.151999999999999996 	0.0825000000000000039 	0.119999999999999996 	8 	
+2 	0.650000000000000022 	0.510000000000000009 	0.160000000000000003 	1.38349999999999995 	0.638499999999999956 	0.29049999999999998 	0.366499999999999992 	9 	
+0 	0.645000000000000018 	0.5 	0.160000000000000003 	1.24649999999999994 	0.547499999999999987 	0.327000000000000013 	0.299999999999999989 	10 	
+1 	0.220000000000000001 	0.160000000000000003 	0.0500000000000000028 	0.0490000000000000019 	0.0214999999999999983 	0.0100000000000000002 	0.0149999999999999994 	4 	
+2 	0.625 	0.479999999999999982 	0.184999999999999998 	1.20649999999999991 	0.586999999999999966 	0.28999999999999998 	0.285999999999999976 	8 	
+2 	0.564999999999999947 	0.445000000000000007 	0.14499999999999999 	0.925499999999999989 	0.434499999999999997 	0.211999999999999994 	0.247499999999999998 	9 	
+2 	0.484999999999999987 	0.359999999999999987 	0.130000000000000004 	0.541499999999999981 	0.259500000000000008 	0.096000000000000002 	0.160000000000000003 	10 	
+1 	0.46000000000000002 	0.359999999999999987 	0.104999999999999996 	0.466000000000000025 	0.222500000000000003 	0.0990000000000000047 	0.110000000000000001 	7 	
+2 	0.57999999999999996 	0.429999999999999993 	0.125 	0.911499999999999977 	0.446000000000000008 	0.20749999999999999 	0.120999999999999996 	10 	
+1 	0.25 	0.179999999999999993 	0.0599999999999999978 	0.0729999999999999954 	0.0280000000000000006 	0.0170000000000000012 	0.0224999999999999992 	5 	
+0 	0.505000000000000004 	0.380000000000000004 	0.135000000000000009 	0.685499999999999998 	0.360999999999999988 	0.1565 	0.161000000000000004 	9 	
+1 	0.320000000000000007 	0.239999999999999991 	0.0700000000000000067 	0.133000000000000007 	0.0585000000000000034 	0.0254999999999999984 	0.0410000000000000017 	6 	
+2 	0.340000000000000024 	0.255000000000000004 	0.0950000000000000011 	0.212999999999999995 	0.0810000000000000026 	0.0340000000000000024 	0.0700000000000000067 	9 	
+0 	0.645000000000000018 	0.479999999999999982 	0.190000000000000002 	1.371 	0.692500000000000004 	0.29049999999999998 	0.349999999999999978 	12 	
+2 	0.604999999999999982 	0.474999999999999978 	0.154999999999999999 	1.16100000000000003 	0.571999999999999953 	0.245499999999999996 	0.275000000000000022 	9 	
+2 	0.465000000000000024 	0.359999999999999987 	0.0800000000000000017 	0.487999999999999989 	0.191000000000000003 	0.125 	0.154999999999999999 	11 	
+1 	0.474999999999999978 	0.359999999999999987 	0.110000000000000001 	0.455500000000000016 	0.176999999999999991 	0.0965000000000000024 	0.14499999999999999 	9 	
+1 	0.354999999999999982 	0.275000000000000022 	0.0899999999999999967 	0.251000000000000001 	0.0970000000000000029 	0.0529999999999999985 	0.0800000000000000017 	7 	
+2 	0.484999999999999987 	0.369999999999999996 	0.130000000000000004 	0.526000000000000023 	0.248499999999999999 	0.104999999999999996 	0.155499999999999999 	6 	
+0 	0.599999999999999978 	0.474999999999999978 	0.135000000000000009 	1.44049999999999989 	0.588500000000000023 	0.191000000000000003 	0.317500000000000004 	9 	
+1 	0.325000000000000011 	0.25 	0.0749999999999999972 	0.158500000000000002 	0.0749999999999999972 	0.0304999999999999993 	0.0454999999999999988 	6 	
+1 	0.584999999999999964 	0.46000000000000002 	0.14499999999999999 	0.84650000000000003 	0.339000000000000024 	0.16700000000000001 	0.294999999999999984 	10 	
+0 	0.569999999999999951 	0.450000000000000011 	0.165000000000000008 	0.903000000000000025 	0.330500000000000016 	0.184499999999999997 	0.294999999999999984 	14 	
+1 	0.280000000000000027 	0.119999999999999996 	0.0749999999999999972 	0.117000000000000007 	0.0454999999999999988 	0.0290000000000000015 	0.0345000000000000029 	4 	
+2 	0.645000000000000018 	0.5 	0.195000000000000007 	1.40100000000000002 	0.616500000000000048 	0.351499999999999979 	0.372499999999999998 	10 	
+0 	0.709999999999999964 	0.540000000000000036 	0.204999999999999988 	1.58050000000000002 	0.802000000000000046 	0.286999999999999977 	0.434999999999999998 	10 	
+0 	0.450000000000000011 	0.33500000000000002 	0.104999999999999996 	0.424999999999999989 	0.186499999999999999 	0.0909999999999999976 	0.115000000000000005 	9 	
+1 	0.23000000000000001 	0.165000000000000008 	0.0599999999999999978 	0.0514999999999999972 	0.0189999999999999995 	0.0145000000000000007 	0.0359999999999999973 	4 	
+0 	0.625 	0.520000000000000018 	0.179999999999999993 	1.35400000000000009 	0.484499999999999986 	0.350999999999999979 	0.375 	11 	
+2 	0.625 	0.489999999999999991 	0.119999999999999996 	0.876499999999999946 	0.456000000000000016 	0.179999999999999993 	0.233000000000000013 	10 	
+1 	0.165000000000000008 	0.115000000000000005 	0.0149999999999999994 	0.0145000000000000007 	0.00549999999999999968 	0.00300000000000000006 	0.0050000000000000001 	4 	
+1 	0.450000000000000011 	0.349999999999999978 	0.140000000000000013 	0.473999999999999977 	0.209999999999999992 	0.109 	0.127500000000000002 	16 	
+0 	0.625 	0.479999999999999982 	0.154999999999999999 	1.20350000000000001 	0.586500000000000021 	0.23899999999999999 	0.318500000000000005 	12 	
+0 	0.57999999999999996 	0.450000000000000011 	0.14499999999999999 	1.13700000000000001 	0.558499999999999996 	0.220000000000000001 	0.28999999999999998 	8 	
+0 	0.675000000000000044 	0.555000000000000049 	0.204999999999999988 	1.92500000000000004 	0.712999999999999967 	0.357999999999999985 	0.453500000000000014 	13 	
+1 	0.560000000000000053 	0.445000000000000007 	0.149999999999999994 	0.822500000000000009 	0.368499999999999994 	0.187 	0.235999999999999988 	10 	
+0 	0.589999999999999969 	0.465000000000000024 	0.149999999999999994 	0.996999999999999997 	0.392000000000000015 	0.245999999999999996 	0.340000000000000024 	12 	
+2 	0.765000000000000013 	0.584999999999999964 	0.179999999999999993 	2.39800000000000013 	1.12799999999999989 	0.512000000000000011 	0.533499999999999974 	12 	
+1 	0.46000000000000002 	0.344999999999999973 	0.104999999999999996 	0.41499999999999998 	0.187 	0.086999999999999994 	0.110000000000000001 	8 	
+0 	0.67000000000000004 	0.540000000000000036 	0.165000000000000008 	1.50150000000000006 	0.518000000000000016 	0.357999999999999985 	0.505000000000000004 	14 	
+2 	0.540000000000000036 	0.41499999999999998 	0.14499999999999999 	0.739999999999999991 	0.263500000000000012 	0.16800000000000001 	0.244999999999999996 	12 	
+2 	0.494999999999999996 	0.395000000000000018 	0.135000000000000009 	0.633499999999999952 	0.303499999999999992 	0.129500000000000004 	0.149499999999999994 	8 	
+2 	0.525000000000000022 	0.41499999999999998 	0.135000000000000009 	0.794499999999999984 	0.394000000000000017 	0.189000000000000001 	0.202000000000000013 	7 	
+2 	0.584999999999999964 	0.46000000000000002 	0.149999999999999994 	1.20599999999999996 	0.580999999999999961 	0.215999999999999998 	0.323000000000000009 	10 	
+2 	0.599999999999999978 	0.46000000000000002 	0.149999999999999994 	1.24700000000000011 	0.533499999999999974 	0.273500000000000021 	0.28999999999999998 	9 	
+2 	0.520000000000000018 	0.400000000000000022 	0.14499999999999999 	0.776499999999999968 	0.35249999999999998 	0.184499999999999997 	0.184999999999999998 	9 	
+1 	0.315000000000000002 	0.234999999999999987 	0.0749999999999999972 	0.128500000000000003 	0.0509999999999999967 	0.0280000000000000006 	0.0405000000000000013 	4 	
+0 	0.635000000000000009 	0.5 	0.190000000000000002 	1.29000000000000004 	0.592999999999999972 	0.304499999999999993 	0.35199999999999998 	8 	
+1 	0.280000000000000027 	0.209999999999999992 	0.0749999999999999972 	0.119499999999999995 	0.0529999999999999985 	0.0264999999999999993 	0.0299999999999999989 	6 	
+2 	0.550000000000000044 	0.405000000000000027 	0.140000000000000013 	0.802499999999999991 	0.243999999999999995 	0.163500000000000006 	0.255000000000000004 	10 	
+0 	0.645000000000000018 	0.510000000000000009 	0.200000000000000011 	1.56749999999999989 	0.620999999999999996 	0.366999999999999993 	0.46000000000000002 	12 	
+0 	0.609999999999999987 	0.484999999999999987 	0.179999999999999993 	1.27950000000000008 	0.57350000000000001 	0.285499999999999976 	0.354999999999999982 	7 	
+2 	0.67000000000000004 	0.525000000000000022 	0.165000000000000008 	1.60850000000000004 	0.682000000000000051 	0.314500000000000002 	0.400500000000000023 	11 	
+1 	0.584999999999999964 	0.419999999999999984 	0.14499999999999999 	0.673499999999999988 	0.28949999999999998 	0.134500000000000008 	0.220000000000000001 	9 	
+0 	0.599999999999999978 	0.474999999999999978 	0.160000000000000003 	1.02649999999999997 	0.484999999999999987 	0.2495 	0.256500000000000006 	9 	
+2 	0.630000000000000004 	0.515000000000000013 	0.154999999999999999 	1.2589999999999999 	0.410499999999999976 	0.197000000000000008 	0.409999999999999976 	13 	
+2 	0.494999999999999996 	0.390000000000000013 	0.149999999999999994 	0.85299999999999998 	0.328500000000000014 	0.189000000000000001 	0.270000000000000018 	14 	
+2 	0.474999999999999978 	0.375 	0.125 	0.592999999999999972 	0.277000000000000024 	0.115000000000000005 	0.179999999999999993 	10 	
+2 	0.594999999999999973 	0.469999999999999973 	0.14499999999999999 	0.990999999999999992 	0.403500000000000025 	0.150499999999999995 	0.340000000000000024 	16 	
+2 	0.515000000000000013 	0.405000000000000027 	0.140000000000000013 	0.850500000000000034 	0.312 	0.145999999999999991 	0.315000000000000002 	17 	
+0 	0.5 	0.405000000000000027 	0.149999999999999994 	0.59650000000000003 	0.253000000000000003 	0.126000000000000001 	0.184999999999999998 	12 	
+1 	0.455000000000000016 	0.330000000000000016 	0.100000000000000006 	0.371999999999999997 	0.357999999999999985 	0.0774999999999999994 	0.110000000000000001 	8 	
+0 	0.550000000000000044 	0.429999999999999993 	0.140000000000000013 	0.713500000000000023 	0.256500000000000006 	0.185999999999999999 	0.225000000000000006 	9 	
+2 	0.405000000000000027 	0.304999999999999993 	0.0850000000000000061 	0.260500000000000009 	0.114500000000000005 	0.0594999999999999973 	0.0850000000000000061 	8 	
+1 	0.340000000000000024 	0.25 	0.0899999999999999967 	0.178999999999999992 	0.0774999999999999994 	0.0330000000000000016 	0.0550000000000000003 	6 	
+0 	0.564999999999999947 	0.450000000000000011 	0.174999999999999989 	1.00950000000000006 	0.447000000000000008 	0.237499999999999989 	0.264500000000000013 	9 	
+2 	0.67000000000000004 	0.505000000000000004 	0.160000000000000003 	1.25849999999999995 	0.625499999999999945 	0.310999999999999999 	0.307999999999999996 	12 	
+2 	0.535000000000000031 	0.434999999999999998 	0.154999999999999999 	0.891499999999999959 	0.341500000000000026 	0.176999999999999991 	0.25 	13 	
+0 	0.369999999999999996 	0.28999999999999998 	0.115000000000000005 	0.25 	0.111000000000000001 	0.0570000000000000021 	0.0749999999999999972 	9 	
+2 	0.535000000000000031 	0.434999999999999998 	0.149999999999999994 	0.716999999999999971 	0.347499999999999976 	0.14449999999999999 	0.194000000000000006 	9 	
+1 	0.385000000000000009 	0.28999999999999998 	0.0899999999999999967 	0.232000000000000012 	0.0855000000000000066 	0.0495000000000000023 	0.0800000000000000017 	7 	
+0 	0.54500000000000004 	0.409999999999999976 	0.140000000000000013 	0.740500000000000047 	0.356499999999999984 	0.177499999999999991 	0.203000000000000014 	9 	
+0 	0.359999999999999987 	0.265000000000000013 	0.0899999999999999967 	0.216499999999999998 	0.096000000000000002 	0.0369999999999999982 	0.0734999999999999959 	10 	
+0 	0.680000000000000049 	0.550000000000000044 	0.174999999999999989 	1.79800000000000004 	0.814999999999999947 	0.392500000000000016 	0.455000000000000016 	19 	
+2 	0.569999999999999951 	0.479999999999999982 	0.179999999999999993 	0.939500000000000002 	0.399000000000000021 	0.200000000000000011 	0.294999999999999984 	14 	
+0 	0.609999999999999987 	0.484999999999999987 	0.170000000000000012 	1.10050000000000003 	0.512499999999999956 	0.229000000000000009 	0.304999999999999993 	11 	
+1 	0.385000000000000009 	0.280000000000000027 	0.0850000000000000061 	0.217499999999999999 	0.0970000000000000029 	0.0379999999999999991 	0.067000000000000004 	8 	
+0 	0.645000000000000018 	0.479999999999999982 	0.170000000000000012 	1.13450000000000006 	0.528000000000000025 	0.254000000000000004 	0.304999999999999993 	10 	
+0 	0.474999999999999978 	0.380000000000000004 	0.140000000000000013 	0.688999999999999946 	0.316500000000000004 	0.131500000000000006 	0.195500000000000007 	7 	
+1 	0.479999999999999982 	0.380000000000000004 	0.125 	0.624500000000000055 	0.339500000000000024 	0.108499999999999999 	0.166500000000000009 	8 	
+0 	0.474999999999999978 	0.359999999999999987 	0.119999999999999996 	0.591500000000000026 	0.324500000000000011 	0.110000000000000001 	0.127000000000000002 	6 	
+2 	0.555000000000000049 	0.434999999999999998 	0.14499999999999999 	0.968500000000000028 	0.498499999999999999 	0.16800000000000001 	0.23849999999999999 	9 	
+1 	0.46000000000000002 	0.364999999999999991 	0.115000000000000005 	0.51100000000000001 	0.236499999999999988 	0.117999999999999994 	0.122999999999999998 	7 	
+1 	0.405000000000000027 	0.304999999999999993 	0.100000000000000006 	0.271000000000000019 	0.0965000000000000024 	0.0609999999999999987 	0.0909999999999999976 	7 	
+0 	0.719999999999999973 	0.564999999999999947 	0.170000000000000012 	1.61299999999999999 	0.722999999999999976 	0.325500000000000012 	0.494499999999999995 	12 	
+0 	0.584999999999999964 	0.465000000000000024 	0.140000000000000013 	0.908000000000000029 	0.381000000000000005 	0.161500000000000005 	0.315000000000000002 	13 	
+0 	0.645000000000000018 	0.525000000000000022 	0.190000000000000002 	1.8085 	0.703500000000000014 	0.388500000000000012 	0.395000000000000018 	18 	
+1 	0.354999999999999982 	0.280000000000000027 	0.110000000000000001 	0.223500000000000004 	0.081500000000000003 	0.0524999999999999981 	0.0800000000000000017 	7 	
+1 	0.190000000000000002 	0.140000000000000013 	0.0299999999999999989 	0.0315000000000000002 	0.0125000000000000007 	0.0050000000000000001 	0.0105000000000000007 	3 	
+1 	0.190000000000000002 	0.14499999999999999 	0.0400000000000000008 	0.0379999999999999991 	0.0165000000000000008 	0.0064999999999999997 	0.0149999999999999994 	4 	
+2 	0.540000000000000036 	0.409999999999999976 	0.14499999999999999 	0.98899999999999999 	0.281499999999999972 	0.212999999999999995 	0.354999999999999982 	19 	
+1 	0.440000000000000002 	0.349999999999999978 	0.135000000000000009 	0.434999999999999998 	0.181499999999999995 	0.0830000000000000043 	0.125 	12 	
+0 	0.510000000000000009 	0.390000000000000013 	0.104999999999999996 	0.611999999999999988 	0.187 	0.149999999999999994 	0.195000000000000007 	13 	
+2 	0.41499999999999998 	0.325000000000000011 	0.140000000000000013 	0.416999999999999982 	0.153499999999999998 	0.101500000000000007 	0.143999999999999989 	10 	
+1 	0.375 	0.265000000000000013 	0.0950000000000000011 	0.196000000000000008 	0.0850000000000000061 	0.0420000000000000026 	0.0585000000000000034 	5 	
+0 	0.655000000000000027 	0.489999999999999991 	0.160000000000000003 	1.20399999999999996 	0.545499999999999985 	0.26150000000000001 	0.322500000000000009 	9 	
+2 	0.599999999999999978 	0.469999999999999973 	0.160000000000000003 	1.19399999999999995 	0.5625 	0.304499999999999993 	0.263500000000000012 	10 	
+2 	0.564999999999999947 	0.440000000000000002 	0.184999999999999998 	0.90900000000000003 	0.343999999999999972 	0.232500000000000012 	0.255000000000000004 	15 	
+0 	0.599999999999999978 	0.474999999999999978 	0.179999999999999993 	1.1805000000000001 	0.434499999999999997 	0.247499999999999998 	0.424999999999999989 	19 	
+2 	0.525000000000000022 	0.424999999999999989 	0.125 	0.812000000000000055 	0.403500000000000025 	0.170500000000000013 	0.195000000000000007 	8 	
+2 	0.619999999999999996 	0.484999999999999987 	0.154999999999999999 	1.02950000000000008 	0.424999999999999989 	0.231500000000000011 	0.33500000000000002 	12 	
+2 	0.574999999999999956 	0.46000000000000002 	0.154999999999999999 	0.892000000000000015 	0.441500000000000004 	0.17599999999999999 	0.220000000000000001 	10 	
+1 	0.474999999999999978 	0.364999999999999991 	0.115000000000000005 	0.459000000000000019 	0.217499999999999999 	0.0929999999999999993 	0.116500000000000006 	7 	
+2 	0.530000000000000027 	0.409999999999999976 	0.140000000000000013 	0.754499999999999948 	0.349499999999999977 	0.171500000000000014 	0.210499999999999993 	8 	
+2 	0.515000000000000013 	0.349999999999999978 	0.154999999999999999 	0.922499999999999987 	0.418499999999999983 	0.198000000000000009 	0.27300000000000002 	9 	
+2 	0.479999999999999982 	0.359999999999999987 	0.100000000000000006 	0.439000000000000001 	0.194000000000000006 	0.0990000000000000047 	0.115000000000000005 	8 	
+2 	0.525000000000000022 	0.395000000000000018 	0.130000000000000004 	0.763499999999999956 	0.337500000000000022 	0.142499999999999988 	0.225000000000000006 	8 	
+1 	0.550000000000000044 	0.424999999999999989 	0.149999999999999994 	0.639000000000000012 	0.269000000000000017 	0.134500000000000008 	0.216999999999999998 	9 	
+2 	0.614999999999999991 	0.474999999999999978 	0.170000000000000012 	1.129 	0.479499999999999982 	0.301999999999999991 	0.299999999999999989 	10 	
+1 	0.160000000000000003 	0.119999999999999996 	0.0350000000000000033 	0.0210000000000000013 	0.00749999999999999972 	0.00449999999999999966 	0.0050000000000000001 	5 	
+2 	0.455000000000000016 	0.354999999999999982 	0.130000000000000004 	0.515000000000000013 	0.200000000000000011 	0.127500000000000002 	0.174999999999999989 	11 	
+2 	0.369999999999999996 	0.280000000000000027 	0.0950000000000000011 	0.222500000000000003 	0.0805000000000000021 	0.0509999999999999967 	0.0749999999999999972 	7 	
+0 	0.70499999999999996 	0.54500000000000004 	0.170000000000000012 	1.58000000000000007 	0.643499999999999961 	0.456500000000000017 	0.265000000000000013 	11 	
+0 	0.614999999999999991 	0.469999999999999973 	0.154999999999999999 	1.08400000000000007 	0.588500000000000023 	0.208999999999999991 	0.245999999999999996 	9 	
+1 	0.400000000000000022 	0.309999999999999998 	0.100000000000000006 	0.127000000000000002 	0.105999999999999997 	0.0709999999999999937 	0.0850000000000000061 	7 	
+1 	0.560000000000000053 	0.434999999999999998 	0.130000000000000004 	0.777000000000000024 	0.353999999999999981 	0.172999999999999987 	0.222000000000000003 	9 	
+0 	0.510000000000000009 	0.400000000000000022 	0.119999999999999996 	0.700500000000000012 	0.346999999999999975 	0.110500000000000001 	0.195000000000000007 	10 	
+2 	0.640000000000000013 	0.510000000000000009 	0.174999999999999989 	1.3680000000000001 	0.515000000000000013 	0.266000000000000014 	0.569999999999999951 	21 	
+1 	0.380000000000000004 	0.275000000000000022 	0.0950000000000000011 	0.137500000000000011 	0.0859999999999999931 	0.0585000000000000034 	0.0604999999999999982 	7 	
+2 	0.424999999999999989 	0.349999999999999978 	0.104999999999999996 	0.393000000000000016 	0.130000000000000004 	0.0630000000000000004 	0.165000000000000008 	9 	
+1 	0.469999999999999973 	0.380000000000000004 	0.125 	0.484499999999999986 	0.210999999999999993 	0.107499999999999998 	0.141999999999999987 	6 	
+0 	0.5 	0.369999999999999996 	0.135000000000000009 	0.450000000000000011 	0.171500000000000014 	0.105499999999999997 	0.154999999999999999 	9 	
+2 	0.574999999999999956 	0.445000000000000007 	0.140000000000000013 	0.736999999999999988 	0.325000000000000011 	0.140500000000000014 	0.236999999999999988 	10 	
+0 	0.625 	0.494999999999999996 	0.165000000000000008 	1.26200000000000001 	0.507000000000000006 	0.318000000000000005 	0.390000000000000013 	10 	
+0 	0.550000000000000044 	0.469999999999999973 	0.149999999999999994 	0.920499999999999985 	0.381000000000000005 	0.243499999999999994 	0.267500000000000016 	10 	
+1 	0.574999999999999956 	0.450000000000000011 	0.130000000000000004 	0.814500000000000002 	0.403000000000000025 	0.171500000000000014 	0.212999999999999995 	10 	
+2 	0.699999999999999956 	0.540000000000000036 	0.204999999999999988 	1.73999999999999999 	0.788499999999999979 	0.372999999999999998 	0.486499999999999988 	13 	
+0 	0.574999999999999956 	0.469999999999999973 	0.154999999999999999 	1.1160000000000001 	0.509000000000000008 	0.237999999999999989 	0.340000000000000024 	10 	
+2 	0.614999999999999991 	0.469999999999999973 	0.160000000000000003 	1.01750000000000007 	0.472999999999999976 	0.239499999999999991 	0.280000000000000027 	10 	
+1 	0.330000000000000016 	0.255000000000000004 	0.0950000000000000011 	0.171999999999999986 	0.0660000000000000031 	0.0254999999999999984 	0.0599999999999999978 	6 	
+0 	0.484999999999999987 	0.369999999999999996 	0.115000000000000005 	0.478499999999999981 	0.199500000000000011 	0.0955000000000000016 	0.129000000000000004 	7 	
+2 	0.520000000000000018 	0.385000000000000009 	0.140000000000000013 	0.659499999999999975 	0.248499999999999999 	0.203499999999999986 	0.160000000000000003 	9 	
+2 	0.5 	0.380000000000000004 	0.154999999999999999 	0.595500000000000029 	0.213499999999999995 	0.161000000000000004 	0.200000000000000011 	12 	
+0 	0.640000000000000013 	0.540000000000000036 	0.174999999999999989 	1.22100000000000009 	0.510000000000000009 	0.259000000000000008 	0.390000000000000013 	15 	
+0 	0.614999999999999991 	0.494999999999999996 	0.160000000000000003 	1.25499999999999989 	0.581500000000000017 	0.319500000000000006 	0.322500000000000009 	12 	
+0 	0.599999999999999978 	0.469999999999999973 	0.190000000000000002 	1.13450000000000006 	0.491999999999999993 	0.259500000000000008 	0.337500000000000022 	10 	
+0 	0.494999999999999996 	0.400000000000000022 	0.154999999999999999 	0.644499999999999962 	0.241999999999999993 	0.132500000000000007 	0.204999999999999988 	17 	
+2 	0.54500000000000004 	0.419999999999999984 	0.119999999999999996 	0.786499999999999977 	0.403000000000000025 	0.184999999999999998 	0.170000000000000012 	7 	
+2 	0.619999999999999996 	0.469999999999999973 	0.14499999999999999 	1.08650000000000002 	0.51100000000000001 	0.271500000000000019 	0.256500000000000006 	10 	
+2 	0.54500000000000004 	0.419999999999999984 	0.14499999999999999 	0.778000000000000025 	0.3745 	0.154499999999999998 	0.204999999999999988 	7 	
+0 	0.589999999999999969 	0.455000000000000016 	0.14499999999999999 	1.06299999999999994 	0.515499999999999958 	0.244499999999999995 	0.25 	8 	
+2 	0.619999999999999996 	0.465000000000000024 	0.190000000000000002 	1.34149999999999991 	0.570500000000000007 	0.317500000000000004 	0.354999999999999982 	11 	
+2 	0.665000000000000036 	0.515000000000000013 	0.200000000000000011 	1.26950000000000007 	0.511499999999999955 	0.267500000000000016 	0.435999999999999999 	12 	
+2 	0.635000000000000009 	0.5 	0.170000000000000012 	1.43450000000000011 	0.610999999999999988 	0.308999999999999997 	0.417999999999999983 	12 	
+1 	0.619999999999999996 	0.474999999999999978 	0.160000000000000003 	0.907000000000000028 	0.370999999999999996 	0.16700000000000001 	0.307499999999999996 	11 	
+2 	0.640000000000000013 	0.5 	0.184999999999999998 	1.3035000000000001 	0.444500000000000006 	0.263500000000000012 	0.465000000000000024 	16 	
+1 	0.525000000000000022 	0.375 	0.119999999999999996 	0.63149999999999995 	0.304499999999999993 	0.114000000000000004 	0.190000000000000002 	9 	
+0 	0.465000000000000024 	0.349999999999999978 	0.130000000000000004 	0.493999999999999995 	0.194500000000000006 	0.102999999999999994 	0.154999999999999999 	18 	
+2 	0.479999999999999982 	0.375 	0.119999999999999996 	0.589500000000000024 	0.253500000000000003 	0.128000000000000003 	0.171999999999999986 	11 	
+1 	0.609999999999999987 	0.489999999999999991 	0.160000000000000003 	1.15450000000000008 	0.586500000000000021 	0.23849999999999999 	0.291499999999999981 	11 	
+1 	0.234999999999999987 	0.160000000000000003 	0.0400000000000000008 	0.048000000000000001 	0.0184999999999999991 	0.0179999999999999986 	0.0149999999999999994 	5 	
+0 	0.630000000000000004 	0.484999999999999987 	0.184999999999999998 	1.16700000000000004 	0.548000000000000043 	0.248499999999999999 	0.340000000000000024 	10 	
+0 	0.640000000000000013 	0.5 	0.170000000000000012 	1.51750000000000007 	0.692999999999999949 	0.326000000000000012 	0.408999999999999975 	11 	
+2 	0.530000000000000027 	0.434999999999999998 	0.160000000000000003 	0.883000000000000007 	0.316000000000000003 	0.164000000000000007 	0.33500000000000002 	15 	
+1 	0.400000000000000022 	0.320000000000000007 	0.0950000000000000011 	0.347999999999999976 	0.194000000000000006 	0.0529999999999999985 	0.086999999999999994 	6 	
+1 	0.584999999999999964 	0.450000000000000011 	0.135000000000000009 	0.854999999999999982 	0.379500000000000004 	0.187 	0.260000000000000009 	9 	
+1 	0.530000000000000027 	0.395000000000000018 	0.130000000000000004 	0.574999999999999956 	0.246999999999999997 	0.115000000000000005 	0.182999999999999996 	9 	
+1 	0.599999999999999978 	0.479999999999999982 	0.170000000000000012 	0.917499999999999982 	0.380000000000000004 	0.222500000000000003 	0.28999999999999998 	8 	
+1 	0.465000000000000024 	0.325000000000000011 	0.140000000000000013 	0.761499999999999955 	0.361999999999999988 	0.153499999999999998 	0.208999999999999991 	10 	
+1 	0.405000000000000027 	0.28999999999999998 	0.0899999999999999967 	0.282499999999999973 	0.112000000000000002 	0.0749999999999999972 	0.081500000000000003 	7 	
+0 	0.584999999999999964 	0.450000000000000011 	0.149999999999999994 	0.937999999999999945 	0.467000000000000026 	0.203000000000000014 	0.225000000000000006 	7 	
+0 	0.369999999999999996 	0.280000000000000027 	0.110000000000000001 	0.23050000000000001 	0.0945000000000000007 	0.0464999999999999997 	0.0749999999999999972 	10 	
+1 	0.434999999999999998 	0.33500000000000002 	0.104999999999999996 	0.353499999999999981 	0.156 	0.0500000000000000028 	0.113500000000000004 	7 	
+2 	0.619999999999999996 	0.489999999999999991 	0.149999999999999994 	1.19500000000000006 	0.46050000000000002 	0.301999999999999991 	0.354999999999999982 	9 	
+2 	0.685000000000000053 	0.520000000000000018 	0.165000000000000008 	1.51899999999999991 	0.698999999999999955 	0.368499999999999994 	0.400000000000000022 	10 	
+2 	0.57999999999999996 	0.455000000000000016 	0.130000000000000004 	0.85199999999999998 	0.409999999999999976 	0.172499999999999987 	0.225000000000000006 	8 	
+0 	0.714999999999999969 	0.564999999999999947 	0.239999999999999991 	2.19950000000000001 	0.724500000000000033 	0.465000000000000024 	0.885000000000000009 	17 	
+0 	0.655000000000000027 	0.540000000000000036 	0.214999999999999997 	1.5555000000000001 	0.694999999999999951 	0.295999999999999985 	0.444000000000000006 	11 	
+0 	0.5 	0.400000000000000022 	0.125 	0.576500000000000012 	0.239499999999999991 	0.126000000000000001 	0.184999999999999998 	10 	
+2 	0.560000000000000053 	0.450000000000000011 	0.160000000000000003 	0.922000000000000042 	0.431999999999999995 	0.177999999999999992 	0.260000000000000009 	15 	
+0 	0.455000000000000016 	0.364999999999999991 	0.115000000000000005 	0.430499999999999994 	0.183999999999999997 	0.107999999999999999 	0.1245 	8 	
+2 	0.609999999999999987 	0.479999999999999982 	0.140000000000000013 	1.03099999999999992 	0.4375 	0.26150000000000001 	0.270000000000000018 	8 	
+0 	0.400000000000000022 	0.320000000000000007 	0.110000000000000001 	0.35299999999999998 	0.140500000000000014 	0.0985000000000000042 	0.100000000000000006 	8 	
+2 	0.445000000000000007 	0.349999999999999978 	0.115000000000000005 	0.361499999999999988 	0.1565 	0.0695000000000000062 	0.117000000000000007 	8 	
+2 	0.569999999999999951 	0.455000000000000016 	0.149999999999999994 	0.951999999999999957 	0.389500000000000013 	0.215499999999999997 	0.274500000000000022 	9 	
+0 	0.719999999999999973 	0.550000000000000044 	0.200000000000000011 	1.99649999999999994 	0.90349999999999997 	0.468999999999999972 	0.521499999999999964 	10 	
+0 	0.469999999999999973 	0.354999999999999982 	0.179999999999999993 	0.441000000000000003 	0.152499999999999997 	0.116500000000000006 	0.135000000000000009 	8 	
+2 	0.564999999999999947 	0.445000000000000007 	0.149999999999999994 	0.796000000000000041 	0.36349999999999999 	0.183999999999999997 	0.219 	8 	
+2 	0.574999999999999956 	0.434999999999999998 	0.149999999999999994 	0.805000000000000049 	0.292999999999999983 	0.162500000000000006 	0.270000000000000018 	17 	
+2 	0.525000000000000022 	0.385000000000000009 	0.100000000000000006 	0.511499999999999955 	0.245999999999999996 	0.100500000000000006 	0.14549999999999999 	8 	
+0 	0.489999999999999991 	0.364999999999999991 	0.14499999999999999 	0.634499999999999953 	0.199500000000000011 	0.162500000000000006 	0.220000000000000001 	10 	
+2 	0.474999999999999978 	0.369999999999999996 	0.125 	0.537000000000000033 	0.222000000000000003 	0.121499999999999997 	0.149999999999999994 	9 	
+2 	0.645000000000000018 	0.510000000000000009 	0.195000000000000007 	1.22599999999999998 	0.588500000000000023 	0.221500000000000002 	0.3745 	10 	
+1 	0.33500000000000002 	0.244999999999999996 	0.0899999999999999967 	0.201500000000000012 	0.096000000000000002 	0.0405000000000000013 	0.048000000000000001 	7 	
+1 	0.574999999999999956 	0.434999999999999998 	0.130000000000000004 	0.805000000000000049 	0.315500000000000003 	0.215499999999999997 	0.244999999999999996 	10 	
+0 	0.489999999999999991 	0.354999999999999982 	0.160000000000000003 	0.879499999999999948 	0.348499999999999976 	0.214999999999999997 	0.282499999999999973 	8 	
+1 	0.375 	0.270000000000000018 	0.0850000000000000061 	0.217999999999999999 	0.0945000000000000007 	0.0389999999999999999 	0.0700000000000000067 	7 	
+2 	0.584999999999999964 	0.465000000000000024 	0.154999999999999999 	0.91449999999999998 	0.455500000000000016 	0.196500000000000008 	0.234999999999999987 	9 	
+2 	0.340000000000000024 	0.265000000000000013 	0.0850000000000000061 	0.183499999999999996 	0.076999999999999999 	0.0459999999999999992 	0.0650000000000000022 	10 	
+2 	0.574999999999999956 	0.434999999999999998 	0.135000000000000009 	0.991999999999999993 	0.431999999999999995 	0.222500000000000003 	0.23899999999999999 	10 	
+0 	0.630000000000000004 	0.5 	0.154999999999999999 	1.00499999999999989 	0.366999999999999993 	0.19900000000000001 	0.359999999999999987 	16 	
+0 	0.665000000000000036 	0.535000000000000031 	0.190000000000000002 	1.496 	0.577500000000000013 	0.281499999999999972 	0.474999999999999978 	17 	
+2 	0.640000000000000013 	0.530000000000000027 	0.165000000000000008 	1.1895 	0.476499999999999979 	0.299999999999999989 	0.349999999999999978 	11 	
+1 	0.41499999999999998 	0.309999999999999998 	0.0950000000000000011 	0.310999999999999999 	0.112500000000000003 	0.0625 	0.115000000000000005 	8 	
+0 	0.535000000000000031 	0.419999999999999984 	0.149999999999999994 	0.736500000000000044 	0.278500000000000025 	0.185999999999999999 	0.214999999999999997 	14 	
+1 	0.574999999999999956 	0.450000000000000011 	0.125 	0.780000000000000027 	0.327500000000000013 	0.188 	0.234999999999999987 	9 	
+0 	0.505000000000000004 	0.424999999999999989 	0.140000000000000013 	0.849999999999999978 	0.275000000000000022 	0.162500000000000006 	0.284999999999999976 	19 	
+1 	0.424999999999999989 	0.325000000000000011 	0.104999999999999996 	0.39750000000000002 	0.181499999999999995 	0.0810000000000000026 	0.117499999999999993 	7 	
+0 	0.564999999999999947 	0.455000000000000016 	0.149999999999999994 	0.820500000000000007 	0.364999999999999991 	0.159000000000000002 	0.260000000000000009 	18 	
+1 	0.455000000000000016 	0.325000000000000011 	0.135000000000000009 	0.819999999999999951 	0.400500000000000023 	0.171500000000000014 	0.210999999999999993 	8 	
+0 	0.599999999999999978 	0.474999999999999978 	0.179999999999999993 	1.16199999999999992 	0.51100000000000001 	0.267500000000000016 	0.320000000000000007 	18 	
+2 	0.465000000000000024 	0.354999999999999982 	0.125 	0.525499999999999967 	0.202500000000000013 	0.135000000000000009 	0.14499999999999999 	13 	
+2 	0.469999999999999973 	0.375 	0.119999999999999996 	0.580500000000000016 	0.266000000000000014 	0.0934999999999999998 	0.169000000000000011 	8 	
+1 	0.434999999999999998 	0.299999999999999989 	0.119999999999999996 	0.59650000000000003 	0.259000000000000008 	0.139000000000000012 	0.164500000000000007 	8 	
+2 	0.640000000000000013 	0.505000000000000004 	0.154999999999999999 	1.19550000000000001 	0.556499999999999995 	0.210999999999999993 	0.345999999999999974 	11 	
+0 	0.57999999999999996 	0.5 	0.165000000000000008 	0.925000000000000044 	0.369999999999999996 	0.184999999999999998 	0.300499999999999989 	10 	
+1 	0.465000000000000024 	0.369999999999999996 	0.110000000000000001 	0.445000000000000007 	0.163500000000000006 	0.096000000000000002 	0.166000000000000009 	7 	
+1 	0.510000000000000009 	0.400000000000000022 	0.125 	0.593500000000000028 	0.23899999999999999 	0.130000000000000004 	0.203999999999999987 	8 	
+2 	0.645000000000000018 	0.5 	0.174999999999999989 	1.33850000000000002 	0.633000000000000007 	0.298999999999999988 	0.348999999999999977 	11 	
+1 	0.525000000000000022 	0.380000000000000004 	0.135000000000000009 	0.614999999999999991 	0.26100000000000001 	0.159000000000000002 	0.174999999999999989 	8 	
+1 	0.409999999999999976 	0.325000000000000011 	0.100000000000000006 	0.324500000000000011 	0.132000000000000006 	0.0719999999999999946 	0.105999999999999997 	6 	
+2 	0.560000000000000053 	0.440000000000000002 	0.140000000000000013 	0.970999999999999974 	0.443000000000000005 	0.204499999999999987 	0.265000000000000013 	14 	
+2 	0.594999999999999973 	0.469999999999999973 	0.154999999999999999 	1.20150000000000001 	0.491999999999999993 	0.38650000000000001 	0.265000000000000013 	10 	
+1 	0.54500000000000004 	0.434999999999999998 	0.135000000000000009 	0.771499999999999964 	0.371999999999999997 	0.147999999999999993 	0.227000000000000007 	8 	
+1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.466500000000000026 	0.228500000000000009 	0.106499999999999997 	0.114000000000000004 	7 	
+0 	0.630000000000000004 	0.479999999999999982 	0.174999999999999989 	1.36749999999999994 	0.501499999999999946 	0.303499999999999992 	0.515000000000000013 	17 	
+2 	0.560000000000000053 	0.440000000000000002 	0.160000000000000003 	0.864500000000000046 	0.330500000000000016 	0.20749999999999999 	0.260000000000000009 	10 	
+0 	0.409999999999999976 	0.325000000000000011 	0.104999999999999996 	0.36349999999999999 	0.159000000000000002 	0.076999999999999999 	0.119999999999999996 	10 	
+0 	0.699999999999999956 	0.525000000000000022 	0.190000000000000002 	1.64650000000000007 	0.854500000000000037 	0.306999999999999995 	0.399500000000000022 	9 	
+0 	0.635000000000000009 	0.525000000000000022 	0.179999999999999993 	1.36949999999999994 	0.634000000000000008 	0.318000000000000005 	0.362999999999999989 	11 	
+2 	0.375 	0.284999999999999976 	0.0950000000000000011 	0.253000000000000003 	0.096000000000000002 	0.0575000000000000025 	0.0924999999999999989 	9 	
+0 	0.584999999999999964 	0.484999999999999987 	0.149999999999999994 	1.07899999999999996 	0.41449999999999998 	0.211499999999999994 	0.355999999999999983 	11 	
+0 	0.380000000000000004 	0.304999999999999993 	0.104999999999999996 	0.281000000000000028 	0.104499999999999996 	0.0614999999999999991 	0.0899999999999999967 	12 	
+0 	0.614999999999999991 	0.5 	0.174999999999999989 	1.377 	0.558499999999999996 	0.330000000000000016 	0.291999999999999982 	12 	
+2 	0.385000000000000009 	0.299999999999999989 	0.0950000000000000011 	0.239999999999999991 	0.0884999999999999953 	0.0589999999999999969 	0.0850000000000000061 	9 	
+0 	0.609999999999999987 	0.474999999999999978 	0.160000000000000003 	1.11549999999999994 	0.383500000000000008 	0.223000000000000004 	0.379000000000000004 	10 	
+1 	0.419999999999999984 	0.330000000000000016 	0.100000000000000006 	0.35199999999999998 	0.163500000000000006 	0.0889999999999999958 	0.100000000000000006 	9 	
+0 	0.520000000000000018 	0.46000000000000002 	0.149999999999999994 	1.01899999999999991 	0.52300000000000002 	0.19850000000000001 	0.254000000000000004 	7 	
+0 	0.390000000000000013 	0.299999999999999989 	0.100000000000000006 	0.265000000000000013 	0.107499999999999998 	0.0599999999999999978 	0.0864999999999999936 	13 	
+2 	0.54500000000000004 	0.46000000000000002 	0.160000000000000003 	0.897499999999999964 	0.341000000000000025 	0.165500000000000008 	0.344999999999999973 	10 	
+0 	0.440000000000000002 	0.340000000000000024 	0.100000000000000006 	0.451000000000000012 	0.188 	0.086999999999999994 	0.130000000000000004 	10 	
+2 	0.520000000000000018 	0.400000000000000022 	0.125 	0.559000000000000052 	0.254000000000000004 	0.139000000000000012 	0.148999999999999994 	8 	
+0 	0.5 	0.380000000000000004 	0.140000000000000013 	0.635499999999999954 	0.277000000000000024 	0.142999999999999988 	0.178499999999999992 	8 	
+2 	0.525000000000000022 	0.405000000000000027 	0.130000000000000004 	0.718500000000000028 	0.326500000000000012 	0.197500000000000009 	0.174999999999999989 	8 	
+1 	0.239999999999999991 	0.174999999999999989 	0.0550000000000000003 	0.0704999999999999932 	0.0250000000000000014 	0.0140000000000000003 	0.0210000000000000013 	5 	
+1 	0.135000000000000009 	0.130000000000000004 	0.0400000000000000008 	0.0290000000000000015 	0.0125000000000000007 	0.0064999999999999997 	0.00800000000000000017 	4 	
+0 	0.560000000000000053 	0.440000000000000002 	0.140000000000000013 	0.928499999999999992 	0.382500000000000007 	0.188 	0.299999999999999989 	11 	
+1 	0.385000000000000009 	0.28999999999999998 	0.0950000000000000011 	0.312 	0.142999999999999988 	0.0635000000000000009 	0.0859999999999999931 	6 	
+1 	0.385000000000000009 	0.280000000000000027 	0.0899999999999999967 	0.228000000000000008 	0.102499999999999994 	0.0420000000000000026 	0.0655000000000000027 	5 	
+0 	0.630000000000000004 	0.510000000000000009 	0.184999999999999998 	1.2350000000000001 	0.511499999999999955 	0.348999999999999977 	0.306499999999999995 	11 	
+0 	0.484999999999999987 	0.375 	0.14499999999999999 	0.588500000000000023 	0.23849999999999999 	0.115500000000000005 	0.190000000000000002 	13 	
+2 	0.665000000000000036 	0.525000000000000022 	0.165000000000000008 	1.33800000000000008 	0.55149999999999999 	0.357499999999999984 	0.349999999999999978 	18 	
+1 	0.5 	0.385000000000000009 	0.149999999999999994 	0.626499999999999946 	0.260500000000000009 	0.166500000000000009 	0.160000000000000003 	10 	
+0 	0.57999999999999996 	0.450000000000000011 	0.170000000000000012 	0.970500000000000029 	0.461500000000000021 	0.232000000000000012 	0.247999999999999998 	9 	
+0 	0.429999999999999993 	0.33500000000000002 	0.119999999999999996 	0.444000000000000006 	0.154999999999999999 	0.114500000000000005 	0.140000000000000013 	13 	
+1 	0.28999999999999998 	0.209999999999999992 	0.0599999999999999978 	0.104499999999999996 	0.0415000000000000022 	0.0219999999999999987 	0.0350000000000000033 	5 	
+0 	0.640000000000000013 	0.5 	0.149999999999999994 	1.07050000000000001 	0.370999999999999996 	0.270500000000000018 	0.359999999999999987 	8 	
+1 	0.46000000000000002 	0.33500000000000002 	0.110000000000000001 	0.444000000000000006 	0.225000000000000006 	0.0744999999999999968 	0.110000000000000001 	8 	
+2 	0.260000000000000009 	0.190000000000000002 	0.0749999999999999972 	0.0945000000000000007 	0.0444999999999999979 	0.0200000000000000004 	0.0299999999999999989 	6 	
+2 	0.594999999999999973 	0.474999999999999978 	0.165000000000000008 	1.21300000000000008 	0.620999999999999996 	0.243499999999999994 	0.274000000000000021 	9 	
+0 	0.584999999999999964 	0.450000000000000011 	0.160000000000000003 	1.07699999999999996 	0.4995 	0.287499999999999978 	0.25 	10 	
+0 	0.614999999999999991 	0.474999999999999978 	0.170000000000000012 	1.10250000000000004 	0.469499999999999973 	0.235499999999999987 	0.344999999999999973 	14 	
+2 	0.724999999999999978 	0.569999999999999951 	0.190000000000000002 	2.33049999999999979 	1.25299999999999989 	0.541000000000000036 	0.520000000000000018 	9 	
+0 	0.57999999999999996 	0.455000000000000016 	0.154999999999999999 	0.836500000000000021 	0.315000000000000002 	0.138500000000000012 	0.320000000000000007 	18 	
+2 	0.525000000000000022 	0.424999999999999989 	0.119999999999999996 	0.866500000000000048 	0.282499999999999973 	0.17599999999999999 	0.28999999999999998 	18 	
+1 	0.170000000000000012 	0.130000000000000004 	0.0950000000000000011 	0.0299999999999999989 	0.0129999999999999994 	0.00800000000000000017 	0.0100000000000000002 	4 	
+2 	0.645000000000000018 	0.5 	0.179999999999999993 	1.46100000000000008 	0.598500000000000032 	0.242499999999999993 	0.439000000000000001 	11 	
+1 	0.424999999999999989 	0.340000000000000024 	0.104999999999999996 	0.389000000000000012 	0.201500000000000012 	0.0904999999999999971 	0.0879999999999999949 	6 	
+2 	0.574999999999999956 	0.455000000000000016 	0.154999999999999999 	1.0129999999999999 	0.468500000000000028 	0.208499999999999991 	0.294999999999999984 	11 	
+1 	0.479999999999999982 	0.359999999999999987 	0.125 	0.542000000000000037 	0.279500000000000026 	0.102499999999999994 	0.146999999999999992 	7 	
+1 	0.405000000000000027 	0.284999999999999976 	0.0899999999999999967 	0.264500000000000013 	0.126500000000000001 	0.0505000000000000032 	0.0749999999999999972 	6 	
+0 	0.569999999999999951 	0.450000000000000011 	0.160000000000000003 	0.97150000000000003 	0.396500000000000019 	0.255000000000000004 	0.260000000000000009 	12 	
+2 	0.255000000000000004 	0.195000000000000007 	0.0650000000000000022 	0.0800000000000000017 	0.0315000000000000002 	0.0179999999999999986 	0.0269999999999999997 	8 	
+1 	0.41499999999999998 	0.309999999999999998 	0.0899999999999999967 	0.281499999999999972 	0.1245 	0.0614999999999999991 	0.0850000000000000061 	6 	
+2 	0.75 	0.555000000000000049 	0.214999999999999997 	2.20100000000000007 	1.06150000000000011 	0.523499999999999965 	0.52849999999999997 	11 	
+2 	0.535000000000000031 	0.419999999999999984 	0.165000000000000008 	0.919499999999999984 	0.33550000000000002 	0.19850000000000001 	0.260000000000000009 	16 	
+1 	0.550000000000000044 	0.429999999999999993 	0.14499999999999999 	0.711999999999999966 	0.302499999999999991 	0.151999999999999996 	0.225000000000000006 	10 	
+0 	0.625 	0.419999999999999984 	0.165000000000000008 	1.05950000000000011 	0.357999999999999985 	0.165000000000000008 	0.445000000000000007 	21 	
+1 	0.434999999999999998 	0.340000000000000024 	0.115000000000000005 	0.392500000000000016 	0.182499999999999996 	0.0779999999999999999 	0.114500000000000005 	6 	
+0 	0.550000000000000044 	0.41499999999999998 	0.135000000000000009 	0.763499999999999956 	0.318000000000000005 	0.209999999999999992 	0.200000000000000011 	9 	
+2 	0.484999999999999987 	0.395000000000000018 	0.140000000000000013 	0.629499999999999948 	0.228500000000000009 	0.127000000000000002 	0.225000000000000006 	14 	
+1 	0.330000000000000016 	0.204999999999999988 	0.0950000000000000011 	0.159500000000000003 	0.076999999999999999 	0.0320000000000000007 	0.043499999999999997 	5 	
+0 	0.530000000000000027 	0.419999999999999984 	0.130000000000000004 	1.00099999999999989 	0.340000000000000024 	0.226000000000000006 	0.265000000000000013 	17 	
+0 	0.655000000000000027 	0.505000000000000004 	0.174999999999999989 	1.29049999999999998 	0.620500000000000052 	0.296499999999999986 	0.326000000000000012 	10 	
+2 	0.594999999999999973 	0.474999999999999978 	0.140000000000000013 	0.94399999999999995 	0.362499999999999989 	0.189000000000000001 	0.315000000000000002 	9 	
+2 	0.574999999999999956 	0.469999999999999973 	0.149999999999999994 	0.978500000000000036 	0.450500000000000012 	0.196000000000000008 	0.276000000000000023 	9 	
+2 	0.599999999999999978 	0.450000000000000011 	0.14499999999999999 	0.877000000000000002 	0.432499999999999996 	0.154999999999999999 	0.239999999999999991 	9 	
+1 	0.294999999999999984 	0.220000000000000001 	0.0700000000000000067 	0.126000000000000001 	0.0514999999999999972 	0.0275000000000000001 	0.0350000000000000033 	6 	
+1 	0.385000000000000009 	0.299999999999999989 	0.0899999999999999967 	0.246999999999999997 	0.122499999999999998 	0.0439999999999999974 	0.0675000000000000044 	5 	
+1 	0.530000000000000027 	0.429999999999999993 	0.130000000000000004 	0.704500000000000015 	0.345999999999999974 	0.141499999999999987 	0.189000000000000001 	9 	
+0 	0.484999999999999987 	0.375 	0.135000000000000009 	0.55600000000000005 	0.192500000000000004 	0.131500000000000006 	0.168500000000000011 	10 	
+1 	0.660000000000000031 	0.525000000000000022 	0.214999999999999997 	1.78600000000000003 	0.672499999999999987 	0.361499999999999988 	0.406499999999999972 	11 	
+1 	0.520000000000000018 	0.409999999999999976 	0.140000000000000013 	0.662499999999999978 	0.277500000000000024 	0.155499999999999999 	0.196000000000000008 	11 	
+2 	0.505000000000000004 	0.405000000000000027 	0.140000000000000013 	0.875 	0.266500000000000015 	0.173999999999999988 	0.284999999999999976 	12 	
+2 	0.640000000000000013 	0.515000000000000013 	0.165000000000000008 	1.36899999999999999 	0.632000000000000006 	0.341500000000000026 	0.357999999999999985 	10 	
+2 	0.5 	0.390000000000000013 	0.135000000000000009 	0.781499999999999972 	0.360999999999999988 	0.157500000000000001 	0.23849999999999999 	9 	
+2 	0.619999999999999996 	0.510000000000000009 	0.174999999999999989 	1.28150000000000008 	0.571500000000000008 	0.23849999999999999 	0.390000000000000013 	10 	
+2 	0.619999999999999996 	0.494999999999999996 	0.174999999999999989 	1.80600000000000005 	0.643000000000000016 	0.328500000000000014 	0.724999999999999978 	17 	
+2 	0.46000000000000002 	0.375 	0.140000000000000013 	0.510499999999999954 	0.192000000000000004 	0.104499999999999996 	0.204999999999999988 	9 	
+0 	0.614999999999999991 	0.484999999999999987 	0.160000000000000003 	1.15749999999999997 	0.500499999999999945 	0.2495 	0.315000000000000002 	10 	
+1 	0.395000000000000018 	0.294999999999999984 	0.0950000000000000011 	0.27250000000000002 	0.115000000000000005 	0.0625 	0.0850000000000000061 	8 	
+2 	0.645000000000000018 	0.520000000000000018 	0.174999999999999989 	1.6359999999999999 	0.779000000000000026 	0.342000000000000026 	0.431999999999999995 	11 	
+2 	0.594999999999999973 	0.450000000000000011 	0.140000000000000013 	0.837999999999999967 	0.396500000000000019 	0.194000000000000006 	0.216999999999999998 	10 	
+2 	0.484999999999999987 	0.390000000000000013 	0.135000000000000009 	0.616999999999999993 	0.25 	0.134500000000000008 	0.163500000000000006 	8 	
+1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.321500000000000008 	0.153499999999999998 	0.0594999999999999973 	0.104999999999999996 	10 	
+2 	0.57999999999999996 	0.440000000000000002 	0.149999999999999994 	1.04649999999999999 	0.518000000000000016 	0.2185 	0.279500000000000026 	10 	
+0 	0.54500000000000004 	0.400000000000000022 	0.140000000000000013 	0.778000000000000025 	0.367999999999999994 	0.214999999999999997 	0.179999999999999993 	9 	
+2 	0.455000000000000016 	0.349999999999999978 	0.104999999999999996 	0.401000000000000023 	0.157500000000000001 	0.0830000000000000043 	0.135000000000000009 	9 	
+2 	0.569999999999999951 	0.46000000000000002 	0.170000000000000012 	0.90349999999999997 	0.407499999999999973 	0.193500000000000005 	0.213999999999999996 	7 	
+1 	0.429999999999999993 	0.340000000000000024 	0.104999999999999996 	0.440500000000000003 	0.23849999999999999 	0.0744999999999999968 	0.107499999999999998 	6 	
+1 	0.469999999999999973 	0.369999999999999996 	0.140000000000000013 	0.498499999999999999 	0.209499999999999992 	0.122499999999999998 	0.14499999999999999 	10 	
+0 	0.675000000000000044 	0.535000000000000031 	0.160000000000000003 	1.40999999999999992 	0.591999999999999971 	0.317500000000000004 	0.419999999999999984 	16 	
+2 	0.589999999999999969 	0.469999999999999973 	0.179999999999999993 	1.18700000000000006 	0.598500000000000032 	0.227000000000000007 	0.309999999999999998 	9 	
+2 	0.505000000000000004 	0.380000000000000004 	0.130000000000000004 	0.656000000000000028 	0.227000000000000007 	0.178499999999999992 	0.220000000000000001 	13 	
+1 	0.479999999999999982 	0.369999999999999996 	0.119999999999999996 	0.536000000000000032 	0.251000000000000001 	0.114000000000000004 	0.149999999999999994 	8 	
+1 	0.450000000000000011 	0.330000000000000016 	0.115000000000000005 	0.364999999999999991 	0.140000000000000013 	0.0825000000000000039 	0.1245 	8 	
+2 	0.474999999999999978 	0.369999999999999996 	0.125 	0.649000000000000021 	0.346999999999999975 	0.13600000000000001 	0.141999999999999987 	8 	
+0 	0.569999999999999951 	0.465000000000000024 	0.179999999999999993 	1.29499999999999993 	0.339000000000000024 	0.222500000000000003 	0.440000000000000002 	12 	
+1 	0.550000000000000044 	0.419999999999999984 	0.135000000000000009 	0.815999999999999948 	0.399500000000000022 	0.148499999999999993 	0.23000000000000001 	12 	
+2 	0.719999999999999973 	0.560000000000000053 	0.179999999999999993 	1.58650000000000002 	0.690999999999999948 	0.375 	0.442500000000000004 	11 	
+2 	0.450000000000000011 	0.340000000000000024 	0.130000000000000004 	0.371499999999999997 	0.160500000000000004 	0.0795000000000000012 	0.104999999999999996 	9 	
+1 	0.510000000000000009 	0.385000000000000009 	0.149999999999999994 	0.625 	0.309499999999999997 	0.118999999999999995 	0.172499999999999987 	8 	
+0 	0.625 	0.5 	0.165000000000000008 	1.28800000000000003 	0.572999999999999954 	0.303499999999999992 	0.315000000000000002 	9 	
+0 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.803499999999999992 	0.380000000000000004 	0.180499999999999994 	0.209999999999999992 	9 	
+1 	0.320000000000000007 	0.234999999999999987 	0.0899999999999999967 	0.182999999999999996 	0.0980000000000000038 	0.033500000000000002 	0.0420000000000000026 	7 	
+1 	0.54500000000000004 	0.405000000000000027 	0.135000000000000009 	0.594500000000000028 	0.270000000000000018 	0.118499999999999994 	0.184999999999999998 	8 	
+2 	0.5 	0.400000000000000022 	0.125 	0.597500000000000031 	0.270000000000000018 	0.127500000000000002 	0.166000000000000009 	9 	
+0 	0.594999999999999973 	0.479999999999999982 	0.149999999999999994 	1.1100000000000001 	0.497999999999999998 	0.228000000000000008 	0.330000000000000016 	10 	
+2 	0.320000000000000007 	0.239999999999999991 	0.0850000000000000061 	0.170000000000000012 	0.0655000000000000027 	0.0470000000000000001 	0.0490000000000000019 	7 	
+0 	0.594999999999999973 	0.479999999999999982 	0.200000000000000011 	0.974999999999999978 	0.357999999999999985 	0.203499999999999986 	0.340000000000000024 	15 	
+0 	0.594999999999999973 	0.450000000000000011 	0.149999999999999994 	1.1140000000000001 	0.586500000000000021 	0.220500000000000002 	0.25 	11 	
+2 	0.584999999999999964 	0.46000000000000002 	0.184999999999999998 	0.922000000000000042 	0.36349999999999999 	0.212999999999999995 	0.284999999999999976 	10 	
+2 	0.5 	0.375 	0.14499999999999999 	0.621500000000000052 	0.274000000000000021 	0.166000000000000009 	0.148499999999999993 	7 	
+1 	0.515000000000000013 	0.385000000000000009 	0.125 	0.611500000000000044 	0.317500000000000004 	0.126500000000000001 	0.149999999999999994 	8 	
+0 	0.505000000000000004 	0.400000000000000022 	0.165000000000000008 	0.728999999999999981 	0.267500000000000016 	0.154999999999999999 	0.25 	9 	
+0 	0.574999999999999956 	0.46000000000000002 	0.190000000000000002 	0.993999999999999995 	0.392000000000000015 	0.242499999999999993 	0.340000000000000024 	13 	
+1 	0.474999999999999978 	0.364999999999999991 	0.119999999999999996 	0.518499999999999961 	0.268000000000000016 	0.1095 	0.13650000000000001 	8 	
+2 	0.505000000000000004 	0.400000000000000022 	0.125 	0.770000000000000018 	0.273500000000000021 	0.159000000000000002 	0.255000000000000004 	13 	
+2 	0.440000000000000002 	0.364999999999999991 	0.125 	0.516000000000000014 	0.215499999999999997 	0.114000000000000004 	0.154999999999999999 	10 	
+0 	0.445000000000000007 	0.354999999999999982 	0.149999999999999994 	0.484999999999999987 	0.180999999999999994 	0.125 	0.154999999999999999 	11 	
+2 	0.5 	0.390000000000000013 	0.125 	0.521499999999999964 	0.248499999999999999 	0.117000000000000007 	0.131000000000000005 	6 	
+0 	0.465000000000000024 	0.359999999999999987 	0.119999999999999996 	0.476499999999999979 	0.192000000000000004 	0.112500000000000003 	0.160000000000000003 	10 	
+2 	0.630000000000000004 	0.479999999999999982 	0.149999999999999994 	1.27099999999999991 	0.660499999999999976 	0.242499999999999993 	0.309999999999999998 	11 	
+1 	0.484999999999999987 	0.354999999999999982 	0.104999999999999996 	0.497999999999999998 	0.217499999999999999 	0.096000000000000002 	0.152499999999999997 	9 	
+2 	0.665000000000000036 	0.540000000000000036 	0.174999999999999989 	1.34699999999999998 	0.495499999999999996 	0.254000000000000004 	0.41499999999999998 	17 	
+0 	0.800000000000000044 	0.630000000000000004 	0.195000000000000007 	2.5259999999999998 	0.933000000000000052 	0.589999999999999969 	0.619999999999999996 	23 	
+0 	0.650000000000000022 	0.510000000000000009 	0.154999999999999999 	1.18900000000000006 	0.482999999999999985 	0.278000000000000025 	0.364499999999999991 	13 	
+1 	0.455000000000000016 	0.33500000000000002 	0.135000000000000009 	0.501000000000000001 	0.274000000000000021 	0.0995000000000000051 	0.106499999999999997 	7 	
+1 	0.474999999999999978 	0.385000000000000009 	0.110000000000000001 	0.57350000000000001 	0.310999999999999999 	0.102499999999999994 	0.13600000000000001 	7 	
+0 	0.655000000000000027 	0.455000000000000016 	0.170000000000000012 	1.28950000000000009 	0.586999999999999966 	0.316500000000000004 	0.341500000000000026 	11 	
+2 	0.46000000000000002 	0.364999999999999991 	0.125 	0.467000000000000026 	0.189500000000000002 	0.0945000000000000007 	0.158000000000000002 	10 	
+1 	0.530000000000000027 	0.419999999999999984 	0.130000000000000004 	0.836500000000000021 	0.3745 	0.16700000000000001 	0.248999999999999999 	11 	
+0 	0.530000000000000027 	0.395000000000000018 	0.115000000000000005 	0.568500000000000005 	0.248999999999999999 	0.137500000000000011 	0.161000000000000004 	9 	
+0 	0.655000000000000027 	0.525000000000000022 	0.174999999999999989 	1.34800000000000009 	0.58550000000000002 	0.260500000000000009 	0.394000000000000017 	10 	
+1 	0.469999999999999973 	0.344999999999999973 	0.115000000000000005 	0.48849999999999999 	0.200500000000000012 	0.107999999999999999 	0.166000000000000009 	11 	
+0 	0.540000000000000036 	0.41499999999999998 	0.149999999999999994 	0.811499999999999999 	0.387500000000000011 	0.1875 	0.203499999999999986 	9 	
+0 	0.28999999999999998 	0.225000000000000006 	0.0749999999999999972 	0.140000000000000013 	0.0514999999999999972 	0.0235000000000000001 	0.0400000000000000008 	5 	
+2 	0.680000000000000049 	0.540000000000000036 	0.190000000000000002 	1.623 	0.716500000000000026 	0.353999999999999981 	0.471499999999999975 	12 	
+1 	0.405000000000000027 	0.299999999999999989 	0.104999999999999996 	0.303999999999999992 	0.14549999999999999 	0.0609999999999999987 	0.0805000000000000021 	6 	
+0 	0.599999999999999978 	0.474999999999999978 	0.154999999999999999 	1.20999999999999996 	0.653000000000000025 	0.169500000000000012 	0.320500000000000007 	10 	
+2 	0.589999999999999969 	0.469999999999999973 	0.160000000000000003 	1.20599999999999996 	0.478999999999999981 	0.242499999999999993 	0.308999999999999997 	8 	
+1 	0.359999999999999987 	0.265000000000000013 	0.0749999999999999972 	0.184499999999999997 	0.0830000000000000043 	0.0364999999999999977 	0.0550000000000000003 	7 	
+1 	0.375 	0.304999999999999993 	0.115000000000000005 	0.271500000000000019 	0.0919999999999999984 	0.0739999999999999963 	0.0899999999999999967 	8 	
+1 	0.469999999999999973 	0.369999999999999996 	0.110000000000000001 	0.555499999999999994 	0.25 	0.115000000000000005 	0.163000000000000006 	8 	
+0 	0.5 	0.400000000000000022 	0.14499999999999999 	0.630000000000000004 	0.234000000000000014 	0.146499999999999991 	0.23000000000000001 	12 	
+1 	0.440000000000000002 	0.325000000000000011 	0.0899999999999999967 	0.349999999999999978 	0.147999999999999993 	0.067000000000000004 	0.104999999999999996 	7 	
+1 	0.520000000000000018 	0.380000000000000004 	0.135000000000000009 	0.53949999999999998 	0.22950000000000001 	0.133000000000000007 	0.157000000000000001 	8 	
+0 	0.660000000000000031 	0.494999999999999996 	0.209999999999999992 	1.54800000000000004 	0.723999999999999977 	0.35249999999999998 	0.392500000000000016 	10 	
+1 	0.584999999999999964 	0.474999999999999978 	0.160000000000000003 	1.05049999999999999 	0.479999999999999982 	0.234000000000000014 	0.284999999999999976 	10 	
+1 	0.57999999999999996 	0.489999999999999991 	0.195000000000000007 	1.3165 	0.530499999999999972 	0.254000000000000004 	0.409999999999999976 	18 	
+2 	0.614999999999999991 	0.525000000000000022 	0.154999999999999999 	1.13749999999999996 	0.366999999999999993 	0.235999999999999988 	0.369999999999999996 	20 	
+0 	0.614999999999999991 	0.465000000000000024 	0.149999999999999994 	0.923000000000000043 	0.461500000000000021 	0.182499999999999996 	0.241499999999999992 	9 	
+2 	0.645000000000000018 	0.484999999999999987 	0.154999999999999999 	1.4890000000000001 	0.591500000000000026 	0.312 	0.380000000000000004 	18 	
+2 	0.574999999999999956 	0.445000000000000007 	0.14499999999999999 	0.846999999999999975 	0.41499999999999998 	0.194500000000000006 	0.220000000000000001 	9 	
+0 	0.275000000000000022 	0.195000000000000007 	0.0700000000000000067 	0.0800000000000000017 	0.0309999999999999998 	0.0214999999999999983 	0.0250000000000000014 	5 	
+2 	0.564999999999999947 	0.434999999999999998 	0.149999999999999994 	0.989999999999999991 	0.579500000000000015 	0.182499999999999996 	0.205999999999999989 	8 	
+0 	0.660000000000000031 	0.530000000000000027 	0.184999999999999998 	1.34600000000000009 	0.546000000000000041 	0.270500000000000018 	0.475999999999999979 	11 	
+1 	0.369999999999999996 	0.275000000000000022 	0.140000000000000013 	0.221500000000000002 	0.0970000000000000029 	0.0454999999999999988 	0.0614999999999999991 	6 	
+2 	0.469999999999999973 	0.364999999999999991 	0.119999999999999996 	0.611999999999999988 	0.327000000000000013 	0.149999999999999994 	0.140000000000000013 	8 	
+0 	0.445000000000000007 	0.325000000000000011 	0.125 	0.455000000000000016 	0.178499999999999992 	0.112500000000000003 	0.140000000000000013 	9 	
+1 	0.41499999999999998 	0.325000000000000011 	0.115000000000000005 	0.328500000000000014 	0.140500000000000014 	0.0509999999999999967 	0.105999999999999997 	12 	
+1 	0.569999999999999951 	0.434999999999999998 	0.170000000000000012 	0.847999999999999976 	0.400000000000000022 	0.166000000000000009 	0.25 	9 	
+2 	0.515000000000000013 	0.390000000000000013 	0.154999999999999999 	0.712500000000000022 	0.369499999999999995 	0.137000000000000011 	0.154999999999999999 	7 	
+0 	0.380000000000000004 	0.304999999999999993 	0.0950000000000000011 	0.281499999999999972 	0.1255 	0.0524999999999999981 	0.0899999999999999967 	8 	
+1 	0.280000000000000027 	0.204999999999999988 	0.0550000000000000003 	0.113500000000000004 	0.0449999999999999983 	0.0275000000000000001 	0.033500000000000002 	7 	
+0 	0.630000000000000004 	0.489999999999999991 	0.170000000000000012 	1.21550000000000002 	0.462500000000000022 	0.204499999999999987 	0.310499999999999998 	10 	
+0 	0.609999999999999987 	0.474999999999999978 	0.149999999999999994 	1.11349999999999993 	0.519499999999999962 	0.257500000000000007 	0.300499999999999989 	11 	
+2 	0.729999999999999982 	0.594999999999999973 	0.23000000000000001 	2.8254999999999999 	1.14650000000000007 	0.418999999999999984 	0.89700000000000002 	17 	
+2 	0.630000000000000004 	0.515000000000000013 	0.174999999999999989 	1.19550000000000001 	0.491999999999999993 	0.246999999999999997 	0.369999999999999996 	11 	
+2 	0.5 	0.354999999999999982 	0.140000000000000013 	0.528000000000000025 	0.212499999999999994 	0.148999999999999994 	0.140000000000000013 	9 	
+2 	0.550000000000000044 	0.440000000000000002 	0.135000000000000009 	0.879000000000000004 	0.367999999999999994 	0.209499999999999992 	0.265000000000000013 	10 	
+2 	0.469999999999999973 	0.375 	0.119999999999999996 	0.556499999999999995 	0.226000000000000006 	0.121999999999999997 	0.195000000000000007 	12 	
+1 	0.304999999999999993 	0.220000000000000001 	0.0700000000000000067 	0.140999999999999986 	0.0619999999999999996 	0.0309999999999999998 	0.0369999999999999982 	5 	
+2 	0.46000000000000002 	0.344999999999999973 	0.110000000000000001 	0.45950000000000002 	0.234999999999999987 	0.0884999999999999953 	0.116000000000000006 	7 	
+0 	0.655000000000000027 	0.54500000000000004 	0.165000000000000008 	1.62250000000000005 	0.655499999999999972 	0.298999999999999988 	0.513000000000000012 	12 	
+0 	0.434999999999999998 	0.33500000000000002 	0.110000000000000001 	0.380000000000000004 	0.169500000000000012 	0.0859999999999999931 	0.110000000000000001 	9 	
+1 	0.555000000000000049 	0.429999999999999993 	0.154999999999999999 	0.739500000000000046 	0.313500000000000001 	0.143499999999999989 	0.280000000000000027 	10 	
+2 	0.574999999999999956 	0.450000000000000011 	0.130000000000000004 	0.785000000000000031 	0.318000000000000005 	0.193000000000000005 	0.226500000000000007 	9 	
+1 	0.375 	0.280000000000000027 	0.0800000000000000017 	0.202500000000000013 	0.0825000000000000039 	0.048000000000000001 	0.0650000000000000022 	8 	
+1 	0.515000000000000013 	0.395000000000000018 	0.125 	0.55600000000000005 	0.269500000000000017 	0.096000000000000002 	0.170000000000000012 	8 	
+2 	0.535000000000000031 	0.419999999999999984 	0.130000000000000004 	0.805499999999999994 	0.30099999999999999 	0.180999999999999994 	0.280000000000000027 	14 	
+1 	0.445000000000000007 	0.344999999999999973 	0.104999999999999996 	0.408999999999999975 	0.16750000000000001 	0.101500000000000007 	0.117000000000000007 	7 	
+2 	0.599999999999999978 	0.465000000000000024 	0.154999999999999999 	1.01649999999999996 	0.512000000000000011 	0.246499999999999997 	0.225000000000000006 	10 	
+0 	0.584999999999999964 	0.445000000000000007 	0.140000000000000013 	0.913000000000000034 	0.430499999999999994 	0.220500000000000002 	0.253000000000000003 	10 	
+1 	0.650000000000000022 	0.515000000000000013 	0.160000000000000003 	1.16250000000000009 	0.494999999999999996 	0.203000000000000014 	0.330000000000000016 	17 	
+0 	0.515000000000000013 	0.390000000000000013 	0.130000000000000004 	0.575500000000000012 	0.197500000000000009 	0.130000000000000004 	0.184499999999999997 	9 	
+2 	0.479999999999999982 	0.364999999999999991 	0.119999999999999996 	0.601500000000000035 	0.312 	0.117000000000000007 	0.140000000000000013 	7 	
+0 	0.385000000000000009 	0.315000000000000002 	0.110000000000000001 	0.285999999999999976 	0.122499999999999998 	0.0635000000000000009 	0.0835000000000000048 	10 	
+2 	0.455000000000000016 	0.349999999999999978 	0.119999999999999996 	0.483499999999999985 	0.181499999999999995 	0.143999999999999989 	0.160000000000000003 	11 	
+2 	0.479999999999999982 	0.354999999999999982 	0.160000000000000003 	0.464000000000000024 	0.221000000000000002 	0.105999999999999997 	0.23899999999999999 	8 	
+1 	0.46000000000000002 	0.344999999999999973 	0.115000000000000005 	0.421499999999999986 	0.189500000000000002 	0.101999999999999993 	0.111000000000000001 	6 	
+2 	0.625 	0.520000000000000018 	0.174999999999999989 	1.41050000000000009 	0.690999999999999948 	0.322000000000000008 	0.346499999999999975 	10 	
+0 	0.530000000000000027 	0.419999999999999984 	0.170000000000000012 	0.827999999999999958 	0.409999999999999976 	0.20799999999999999 	0.150499999999999995 	6 	
+1 	0.640000000000000013 	0.489999999999999991 	0.135000000000000009 	1.10000000000000009 	0.487999999999999989 	0.2505 	0.292499999999999982 	10 	
+2 	0.510000000000000009 	0.400000000000000022 	0.130000000000000004 	0.643499999999999961 	0.270000000000000018 	0.166500000000000009 	0.204999999999999988 	12 	
+1 	0.550000000000000044 	0.419999999999999984 	0.115000000000000005 	0.668000000000000038 	0.292499999999999982 	0.137000000000000011 	0.208999999999999991 	11 	
+0 	0.405000000000000027 	0.304999999999999993 	0.0950000000000000011 	0.348499999999999976 	0.14549999999999999 	0.0894999999999999962 	0.100000000000000006 	9 	
+2 	0.650000000000000022 	0.494999999999999996 	0.179999999999999993 	1.79299999999999993 	0.800499999999999989 	0.339000000000000024 	0.530000000000000027 	14 	
+1 	0.33500000000000002 	0.239999999999999991 	0.0950000000000000011 	0.170000000000000012 	0.0619999999999999996 	0.0389999999999999999 	0.0550000000000000003 	9 	
+2 	0.46000000000000002 	0.344999999999999973 	0.119999999999999996 	0.493499999999999994 	0.243499999999999994 	0.117499999999999993 	0.132000000000000006 	8 	
+0 	0.619999999999999996 	0.510000000000000009 	0.174999999999999989 	1.27049999999999996 	0.541499999999999981 	0.323000000000000009 	0.322500000000000009 	9 	
+1 	0.405000000000000027 	0.25 	0.0899999999999999967 	0.287499999999999978 	0.128000000000000003 	0.0630000000000000004 	0.0805000000000000021 	7 	
+1 	0.220000000000000001 	0.165000000000000008 	0.0550000000000000003 	0.0544999999999999998 	0.0214999999999999983 	0.0120000000000000002 	0.0200000000000000004 	5 	
+2 	0.689999999999999947 	0.525000000000000022 	0.174999999999999989 	1.7004999999999999 	0.825500000000000012 	0.361999999999999988 	0.405000000000000027 	8 	
+0 	0.450000000000000011 	0.344999999999999973 	0.115000000000000005 	0.495999999999999996 	0.190500000000000003 	0.117000000000000007 	0.140000000000000013 	12 	
+1 	0.46000000000000002 	0.349999999999999978 	0.115000000000000005 	0.41549999999999998 	0.179999999999999993 	0.0980000000000000038 	0.117499999999999993 	7 	
+0 	0.57999999999999996 	0.46000000000000002 	0.149999999999999994 	0.995500000000000052 	0.428999999999999992 	0.211999999999999994 	0.260000000000000009 	19 	
+2 	0.650000000000000022 	0.510000000000000009 	0.174999999999999989 	1.3165 	0.634499999999999953 	0.260500000000000009 	0.36399999999999999 	12 	
+2 	0.535000000000000031 	0.434999999999999998 	0.140000000000000013 	0.873999999999999999 	0.373499999999999999 	0.229000000000000009 	0.219500000000000001 	8 	
+2 	0.390000000000000013 	0.299999999999999989 	0.0899999999999999967 	0.305499999999999994 	0.142999999999999988 	0.0645000000000000018 	0.0850000000000000061 	9 	
+0 	0.719999999999999973 	0.564999999999999947 	0.179999999999999993 	1.71900000000000008 	0.84650000000000003 	0.406999999999999973 	0.387500000000000011 	11 	
+1 	0.275000000000000022 	0.220000000000000001 	0.0800000000000000017 	0.13650000000000001 	0.0565000000000000016 	0.028500000000000001 	0.0420000000000000026 	6 	
+0 	0.694999999999999951 	0.530000000000000027 	0.200000000000000011 	2.04749999999999988 	0.75 	0.419499999999999984 	0.609500000000000042 	14 	
+2 	0.714999999999999969 	0.550000000000000044 	0.174999999999999989 	1.82499999999999996 	0.937999999999999945 	0.380500000000000005 	0.440000000000000002 	11 	
+2 	0.510000000000000009 	0.390000000000000013 	0.135000000000000009 	0.769000000000000017 	0.393500000000000016 	0.14549999999999999 	0.190000000000000002 	8 	
+2 	0.599999999999999978 	0.455000000000000016 	0.154999999999999999 	0.944999999999999951 	0.436499999999999999 	0.208499999999999991 	0.25 	8 	
+2 	0.515000000000000013 	0.409999999999999976 	0.140000000000000013 	0.735500000000000043 	0.306499999999999995 	0.137000000000000011 	0.200000000000000011 	7 	
+1 	0.369999999999999996 	0.270000000000000018 	0.0899999999999999967 	0.185499999999999998 	0.0700000000000000067 	0.0425000000000000031 	0.0650000000000000022 	7 	
+1 	0.409999999999999976 	0.33500000000000002 	0.110000000000000001 	0.330000000000000016 	0.157000000000000001 	0.0704999999999999932 	0.170000000000000012 	7 	
+1 	0.419999999999999984 	0.344999999999999973 	0.115000000000000005 	0.343500000000000028 	0.151499999999999996 	0.0795000000000000012 	0.115000000000000005 	9 	
+0 	0.5 	0.385000000000000009 	0.130000000000000004 	0.768000000000000016 	0.262500000000000011 	0.0950000000000000011 	0.270000000000000018 	13 	
+1 	0.340000000000000024 	0.265000000000000013 	0.0700000000000000067 	0.184999999999999998 	0.0625 	0.0395000000000000004 	0.0700000000000000067 	7 	
+0 	0.640000000000000013 	0.525000000000000022 	0.214999999999999997 	1.77899999999999991 	0.453500000000000014 	0.285499999999999976 	0.550000000000000044 	22 	
+2 	0.594999999999999973 	0.465000000000000024 	0.125 	0.799000000000000044 	0.324500000000000011 	0.200000000000000011 	0.23000000000000001 	10 	
+1 	0.395000000000000018 	0.28999999999999998 	0.0950000000000000011 	0.319000000000000006 	0.138000000000000012 	0.0800000000000000017 	0.0820000000000000034 	7 	
+2 	0.354999999999999982 	0.260000000000000009 	0.0850000000000000061 	0.190500000000000003 	0.0810000000000000026 	0.0485000000000000014 	0.0550000000000000003 	6 	
+1 	0.429999999999999993 	0.320000000000000007 	0.110000000000000001 	0.367499999999999993 	0.16750000000000001 	0.101999999999999993 	0.104999999999999996 	8 	
+2 	0.46000000000000002 	0.340000000000000024 	0.135000000000000009 	0.494999999999999996 	0.165500000000000008 	0.117000000000000007 	0.184999999999999998 	10 	
+2 	0.609999999999999987 	0.450000000000000011 	0.149999999999999994 	0.870999999999999996 	0.406999999999999973 	0.183499999999999996 	0.25 	10 	
+1 	0.450000000000000011 	0.364999999999999991 	0.125 	0.462000000000000022 	0.213499999999999995 	0.0985000000000000042 	0.131500000000000006 	8 	
+2 	0.54500000000000004 	0.409999999999999976 	0.140000000000000013 	0.625 	0.223000000000000004 	0.160000000000000003 	0.234999999999999987 	13 	
+0 	0.560000000000000053 	0.455000000000000016 	0.125 	0.942999999999999949 	0.343999999999999972 	0.129000000000000004 	0.375 	21 	
+2 	0.660000000000000031 	0.520000000000000018 	0.190000000000000002 	1.55800000000000005 	0.755000000000000004 	0.297999999999999987 	0.400000000000000022 	10 	
+2 	0.589999999999999969 	0.474999999999999978 	0.14499999999999999 	1.05299999999999994 	0.441500000000000004 	0.262000000000000011 	0.325000000000000011 	15 	
+2 	0.469999999999999973 	0.369999999999999996 	0.130000000000000004 	0.522499999999999964 	0.201000000000000012 	0.133000000000000007 	0.165000000000000008 	7 	
+1 	0.445000000000000007 	0.325000000000000011 	0.100000000000000006 	0.378000000000000003 	0.179499999999999993 	0.100000000000000006 	0.0889999999999999958 	7 	
+2 	0.645000000000000018 	0.515000000000000013 	0.239999999999999991 	1.54150000000000009 	0.470999999999999974 	0.368999999999999995 	0.535000000000000031 	13 	
+0 	0.604999999999999982 	0.474999999999999978 	0.165000000000000008 	1.05600000000000005 	0.432999999999999996 	0.219500000000000001 	0.356999999999999984 	9 	
+1 	0.574999999999999956 	0.450000000000000011 	0.170000000000000012 	0.931499999999999995 	0.357999999999999985 	0.214499999999999996 	0.260000000000000009 	13 	
+0 	0.535000000000000031 	0.41499999999999998 	0.184999999999999998 	0.841500000000000026 	0.314000000000000001 	0.158500000000000002 	0.299999999999999989 	15 	
+1 	0.515000000000000013 	0.390000000000000013 	0.110000000000000001 	0.531000000000000028 	0.241499999999999992 	0.0980000000000000038 	0.161500000000000005 	8 	
+0 	0.520000000000000018 	0.405000000000000027 	0.119999999999999996 	0.627000000000000002 	0.264500000000000013 	0.141499999999999987 	0.180999999999999994 	11 	
+2 	0.640000000000000013 	0.515000000000000013 	0.179999999999999993 	1.24700000000000011 	0.547499999999999987 	0.292499999999999982 	0.368499999999999994 	10 	
+1 	0.315000000000000002 	0.209999999999999992 	0.0599999999999999978 	0.125 	0.0599999999999999978 	0.0374999999999999986 	0.0350000000000000033 	5 	
+2 	0.574999999999999956 	0.455000000000000016 	0.165000000000000008 	0.866999999999999993 	0.376500000000000001 	0.180499999999999994 	0.268000000000000016 	8 	
+0 	0.484999999999999987 	0.380000000000000004 	0.149999999999999994 	0.604999999999999982 	0.215499999999999997 	0.140000000000000013 	0.179999999999999993 	15 	
+0 	0.57999999999999996 	0.46000000000000002 	0.119999999999999996 	0.99350000000000005 	0.462500000000000022 	0.23849999999999999 	0.280000000000000027 	11 	
+2 	0.515000000000000013 	0.405000000000000027 	0.14499999999999999 	0.694999999999999951 	0.214999999999999997 	0.163500000000000006 	0.234000000000000014 	15 	
+0 	0.54500000000000004 	0.440000000000000002 	0.135000000000000009 	0.918499999999999983 	0.428999999999999992 	0.201500000000000012 	0.237499999999999989 	10 	
+2 	0.469999999999999973 	0.375 	0.115000000000000005 	0.42649999999999999 	0.168500000000000011 	0.0754999999999999977 	0.149999999999999994 	8 	
+2 	0.694999999999999951 	0.530000000000000027 	0.190000000000000002 	1.72599999999999998 	0.762499999999999956 	0.435999999999999999 	0.455000000000000016 	11 	
+1 	0.520000000000000018 	0.395000000000000018 	0.135000000000000009 	0.633000000000000007 	0.298499999999999988 	0.129500000000000004 	0.174999999999999989 	9 	
+2 	0.569999999999999951 	0.465000000000000024 	0.125 	0.848999999999999977 	0.378500000000000003 	0.17649999999999999 	0.239999999999999991 	15 	
+2 	0.555000000000000049 	0.450000000000000011 	0.174999999999999989 	0.873999999999999999 	0.327500000000000013 	0.202000000000000013 	0.304999999999999993 	10 	
+1 	0.409999999999999976 	0.315000000000000002 	0.0950000000000000011 	0.280500000000000027 	0.114000000000000004 	0.0345000000000000029 	0.110000000000000001 	7 	
+1 	0.505000000000000004 	0.385000000000000009 	0.125 	0.595999999999999974 	0.244999999999999996 	0.0970000000000000029 	0.209999999999999992 	9 	
+2 	0.619999999999999996 	0.479999999999999982 	0.149999999999999994 	1.26600000000000001 	0.628499999999999948 	0.257500000000000007 	0.308999999999999997 	12 	
+1 	0.41499999999999998 	0.320000000000000007 	0.110000000000000001 	0.373499999999999999 	0.174999999999999989 	0.0754999999999999977 	0.109 	7 	
+2 	0.635000000000000009 	0.520000000000000018 	0.174999999999999989 	1.29200000000000004 	0.599999999999999978 	0.269000000000000017 	0.366999999999999993 	11 	
+1 	0.349999999999999978 	0.270000000000000018 	0.0899999999999999967 	0.205499999999999988 	0.0749999999999999972 	0.0575000000000000025 	0.0619999999999999996 	6 	
+2 	0.57999999999999996 	0.474999999999999978 	0.149999999999999994 	0.969999999999999973 	0.385000000000000009 	0.216499999999999998 	0.349999999999999978 	11 	
+2 	0.619999999999999996 	0.450000000000000011 	0.200000000000000011 	0.857999999999999985 	0.428499999999999992 	0.152499999999999997 	0.240499999999999992 	8 	
+0 	0.660000000000000031 	0.530000000000000027 	0.184999999999999998 	1.34850000000000003 	0.492999999999999994 	0.244999999999999996 	0.489999999999999991 	12 	
+0 	0.489999999999999991 	0.380000000000000004 	0.14499999999999999 	0.672499999999999987 	0.248999999999999999 	0.180999999999999994 	0.209999999999999992 	10 	
+0 	0.525000000000000022 	0.41499999999999998 	0.149999999999999994 	0.705500000000000016 	0.329000000000000015 	0.146999999999999992 	0.19900000000000001 	10 	
+0 	0.630000000000000004 	0.5 	0.179999999999999993 	1.1964999999999999 	0.514000000000000012 	0.232500000000000012 	0.399500000000000022 	8 	
+1 	0.255000000000000004 	0.190000000000000002 	0.0500000000000000028 	0.0830000000000000043 	0.0294999999999999984 	0.0214999999999999983 	0.0269999999999999997 	6 	
+1 	0.450000000000000011 	0.349999999999999978 	0.130000000000000004 	0.547000000000000042 	0.244999999999999996 	0.140500000000000014 	0.140500000000000014 	8 	
+1 	0.530000000000000027 	0.424999999999999989 	0.130000000000000004 	0.76749999999999996 	0.418999999999999984 	0.120499999999999996 	0.209999999999999992 	9 	
+1 	0.465000000000000024 	0.354999999999999982 	0.104999999999999996 	0.442000000000000004 	0.208499999999999991 	0.0975000000000000033 	0.118499999999999994 	7 	
+1 	0.560000000000000053 	0.424999999999999989 	0.140000000000000013 	0.917499999999999982 	0.400500000000000023 	0.197500000000000009 	0.260000000000000009 	10 	
+2 	0.589999999999999969 	0.465000000000000024 	0.154999999999999999 	1.1359999999999999 	0.524499999999999966 	0.26150000000000001 	0.275000000000000022 	11 	
+2 	0.560000000000000053 	0.450000000000000011 	0.184999999999999998 	1.07000000000000006 	0.380500000000000005 	0.174999999999999989 	0.409999999999999976 	19 	
+1 	0.385000000000000009 	0.28999999999999998 	0.0850000000000000061 	0.2505 	0.112000000000000002 	0.0609999999999999987 	0.0800000000000000017 	8 	
+2 	0.540000000000000036 	0.455000000000000016 	0.140000000000000013 	0.971999999999999975 	0.418999999999999984 	0.255000000000000004 	0.269000000000000017 	10 	
+2 	0.574999999999999956 	0.469999999999999973 	0.149999999999999994 	1.14149999999999996 	0.451500000000000012 	0.203999999999999987 	0.400000000000000022 	13 	
+1 	0.23000000000000001 	0.174999999999999989 	0.0650000000000000022 	0.0645000000000000018 	0.0259999999999999988 	0.0105000000000000007 	0.0200000000000000004 	5 	
+0 	0.599999999999999978 	0.505000000000000004 	0.190000000000000002 	1.129 	0.438500000000000001 	0.256000000000000005 	0.359999999999999987 	13 	
+2 	0.584999999999999964 	0.450000000000000011 	0.179999999999999993 	0.799499999999999988 	0.336000000000000021 	0.185499999999999998 	0.236999999999999988 	8 	
+1 	0.445000000000000007 	0.349999999999999978 	0.130000000000000004 	0.419499999999999984 	0.169500000000000012 	0.0945000000000000007 	0.119499999999999995 	7 	
+0 	0.660000000000000031 	0.515000000000000013 	0.179999999999999993 	1.52299999999999991 	0.540000000000000036 	0.336500000000000021 	0.555000000000000049 	16 	
+2 	0.719999999999999973 	0.584999999999999964 	0.220000000000000001 	1.91399999999999992 	0.91549999999999998 	0.448000000000000009 	0.478999999999999981 	11 	
+1 	0.344999999999999973 	0.255000000000000004 	0.0950000000000000011 	0.194500000000000006 	0.0924999999999999989 	0.0369999999999999982 	0.0550000000000000003 	6 	
+2 	0.465000000000000024 	0.359999999999999987 	0.104999999999999996 	0.430999999999999994 	0.171999999999999986 	0.106999999999999998 	0.174999999999999989 	9 	
+2 	0.589999999999999969 	0.469999999999999973 	0.154999999999999999 	1.17349999999999999 	0.624500000000000055 	0.233000000000000013 	0.259500000000000008 	9 	
+0 	0.54500000000000004 	0.429999999999999993 	0.165000000000000008 	0.802000000000000046 	0.293499999999999983 	0.182999999999999996 	0.280000000000000027 	11 	
+0 	0.54500000000000004 	0.41499999999999998 	0.160000000000000003 	0.771499999999999964 	0.27200000000000002 	0.14549999999999999 	0.276500000000000024 	10 	
+0 	0.41499999999999998 	0.304999999999999993 	0.104999999999999996 	0.360499999999999987 	0.119999999999999996 	0.0820000000000000034 	0.100000000000000006 	10 	
+2 	0.625 	0.5 	0.170000000000000012 	1.09850000000000003 	0.464500000000000024 	0.220000000000000001 	0.353999999999999981 	9 	
+1 	0.320000000000000007 	0.244999999999999996 	0.0800000000000000017 	0.158500000000000002 	0.0635000000000000009 	0.0325000000000000011 	0.0500000000000000028 	13 	
+2 	0.484999999999999987 	0.390000000000000013 	0.0850000000000000061 	0.643499999999999961 	0.294499999999999984 	0.102999999999999994 	0.198000000000000009 	8 	
+1 	0.445000000000000007 	0.33500000000000002 	0.110000000000000001 	0.410999999999999976 	0.19850000000000001 	0.0934999999999999998 	0.109 	8 	
+0 	0.604999999999999982 	0.479999999999999982 	0.149999999999999994 	1.07899999999999996 	0.450500000000000012 	0.283499999999999974 	0.292999999999999983 	10 	
+1 	0.299999999999999989 	0.220000000000000001 	0.0899999999999999967 	0.142499999999999988 	0.0570000000000000021 	0.033500000000000002 	0.0429999999999999966 	7 	
+0 	0.589999999999999969 	0.455000000000000016 	0.174999999999999989 	0.96599999999999997 	0.391000000000000014 	0.245499999999999996 	0.309999999999999998 	10 	
+1 	0.429999999999999993 	0.320000000000000007 	0.100000000000000006 	0.346499999999999975 	0.163500000000000006 	0.0800000000000000017 	0.0899999999999999967 	7 	
+0 	0.564999999999999947 	0.400000000000000022 	0.130000000000000004 	0.697500000000000009 	0.307499999999999996 	0.166500000000000009 	0.179999999999999993 	8 	
+1 	0.409999999999999976 	0.299999999999999989 	0.100000000000000006 	0.281999999999999973 	0.1255 	0.0570000000000000021 	0.0874999999999999944 	7 	
+1 	0.33500000000000002 	0.260000000000000009 	0.100000000000000006 	0.192000000000000004 	0.0785000000000000003 	0.0585000000000000034 	0.0700000000000000067 	8 	
+0 	0.550000000000000044 	0.405000000000000027 	0.125 	0.651000000000000023 	0.296499999999999986 	0.137000000000000011 	0.200000000000000011 	9 	
+0 	0.675000000000000044 	0.550000000000000044 	0.174999999999999989 	1.68900000000000006 	0.69399999999999995 	0.370999999999999996 	0.473999999999999977 	13 	
+1 	0.244999999999999996 	0.179999999999999993 	0.0650000000000000022 	0.0709999999999999937 	0.0299999999999999989 	0.0129999999999999994 	0.0214999999999999983 	4 	
+2 	0.599999999999999978 	0.494999999999999996 	0.195000000000000007 	1.05750000000000011 	0.384000000000000008 	0.190000000000000002 	0.375 	26 	
+0 	0.455000000000000016 	0.349999999999999978 	0.140000000000000013 	0.572500000000000009 	0.196500000000000008 	0.132500000000000007 	0.174999999999999989 	10 	
+0 	0.469999999999999973 	0.359999999999999987 	0.119999999999999996 	0.47749999999999998 	0.210499999999999993 	0.105499999999999997 	0.149999999999999994 	10 	
+1 	0.445000000000000007 	0.309999999999999998 	0.0899999999999999967 	0.336000000000000021 	0.155499999999999999 	0.0899999999999999967 	0.0855000000000000066 	7 	
+0 	0.574999999999999956 	0.450000000000000011 	0.160000000000000003 	1.06800000000000006 	0.55600000000000005 	0.213999999999999996 	0.257500000000000007 	10 	
+0 	0.625 	0.484999999999999987 	0.160000000000000003 	1.254 	0.59099999999999997 	0.259000000000000008 	0.348499999999999976 	9 	
+1 	0.359999999999999987 	0.280000000000000027 	0.104999999999999996 	0.19900000000000001 	0.0695000000000000062 	0.0449999999999999983 	0.0800000000000000017 	9 	
+1 	0.354999999999999982 	0.260000000000000009 	0.0899999999999999967 	0.19850000000000001 	0.0714999999999999941 	0.0495000000000000023 	0.0580000000000000029 	7 	
+0 	0.594999999999999973 	0.46000000000000002 	0.160000000000000003 	0.921000000000000041 	0.400500000000000023 	0.202500000000000013 	0.287499999999999978 	9 	
+1 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.666499999999999981 	0.3125 	0.138000000000000012 	0.189500000000000002 	10 	
+2 	0.640000000000000013 	0.484999999999999987 	0.160000000000000003 	1.00600000000000001 	0.456000000000000016 	0.224500000000000005 	0.283499999999999974 	9 	
+1 	0.28999999999999998 	0.225000000000000006 	0.0700000000000000067 	0.101000000000000006 	0.0359999999999999973 	0.0235000000000000001 	0.0350000000000000033 	8 	
+0 	0.635000000000000009 	0.494999999999999996 	0.0149999999999999994 	1.15650000000000008 	0.511499999999999955 	0.307999999999999996 	0.288499999999999979 	9 	
+1 	0.484999999999999987 	0.385000000000000009 	0.130000000000000004 	0.567999999999999949 	0.2505 	0.177999999999999992 	0.153999999999999998 	7 	
+1 	0.450000000000000011 	0.349999999999999978 	0.125 	0.47749999999999998 	0.223500000000000004 	0.0889999999999999958 	0.117999999999999994 	6 	
+2 	0.515000000000000013 	0.405000000000000027 	0.130000000000000004 	0.721999999999999975 	0.320000000000000007 	0.131000000000000005 	0.209999999999999992 	10 	
+0 	0.489999999999999991 	0.375 	0.135000000000000009 	0.612500000000000044 	0.255500000000000005 	0.101999999999999993 	0.220000000000000001 	11 	
+1 	0.309999999999999998 	0.225000000000000006 	0.0700000000000000067 	0.105499999999999997 	0.434999999999999998 	0.0149999999999999994 	0.0400000000000000008 	5 	
+2 	0.569999999999999951 	0.41499999999999998 	0.130000000000000004 	0.880000000000000004 	0.427499999999999991 	0.195500000000000007 	0.237999999999999989 	13 	
+0 	0.635000000000000009 	0.484999999999999987 	0.165000000000000008 	1.29449999999999998 	0.668000000000000038 	0.260500000000000009 	0.271500000000000019 	9 	
+2 	0.665000000000000036 	0.525000000000000022 	0.179999999999999993 	1.42900000000000005 	0.671499999999999986 	0.28999999999999998 	0.400000000000000022 	12 	
+1 	0.359999999999999987 	0.275000000000000022 	0.0850000000000000061 	0.197500000000000009 	0.0744999999999999968 	0.0415000000000000022 	0.0700000000000000067 	9 	
+1 	0.33500000000000002 	0.255000000000000004 	0.0800000000000000017 	0.16800000000000001 	0.0790000000000000008 	0.0354999999999999968 	0.0500000000000000028 	5 	
+1 	0.440000000000000002 	0.325000000000000011 	0.100000000000000006 	0.416499999999999981 	0.184999999999999998 	0.0864999999999999936 	0.110000000000000001 	6 	
+1 	0.390000000000000013 	0.294999999999999984 	0.0950000000000000011 	0.203000000000000014 	0.0874999999999999944 	0.0449999999999999983 	0.0749999999999999972 	7 	
+1 	0.574999999999999956 	0.445000000000000007 	0.160000000000000003 	0.917499999999999982 	0.450000000000000011 	0.193500000000000005 	0.239999999999999991 	9 	
+1 	0.359999999999999987 	0.299999999999999989 	0.0850000000000000061 	0.270000000000000018 	0.118499999999999994 	0.0640000000000000013 	0.0744999999999999968 	7 	
+0 	0.594999999999999973 	0.469999999999999973 	0.154999999999999999 	1.17749999999999999 	0.542000000000000037 	0.269000000000000017 	0.309999999999999998 	9 	
+1 	0.450000000000000011 	0.340000000000000024 	0.125 	0.404500000000000026 	0.171000000000000013 	0.0700000000000000067 	0.134500000000000008 	8 	
+1 	0.364999999999999991 	0.294999999999999984 	0.0950000000000000011 	0.25 	0.107499999999999998 	0.0544999999999999998 	0.0800000000000000017 	9 	
+1 	0.344999999999999973 	0.255000000000000004 	0.0950000000000000011 	0.182999999999999996 	0.0749999999999999972 	0.0384999999999999995 	0.0599999999999999978 	6 	
+1 	0.515000000000000013 	0.400000000000000022 	0.125 	0.592500000000000027 	0.265000000000000013 	0.117499999999999993 	0.16800000000000001 	9 	
+2 	0.474999999999999978 	0.375 	0.130000000000000004 	0.51749999999999996 	0.20749999999999999 	0.116500000000000006 	0.170000000000000012 	10 	
+2 	0.640000000000000013 	0.505000000000000004 	0.179999999999999993 	1.29699999999999993 	0.589999999999999969 	0.3125 	0.362999999999999989 	11 	
+1 	0.455000000000000016 	0.354999999999999982 	0.0800000000000000017 	0.452000000000000013 	0.216499999999999998 	0.0995000000000000051 	0.125 	9 	
+2 	0.525000000000000022 	0.429999999999999993 	0.165000000000000008 	0.864500000000000046 	0.376000000000000001 	0.194500000000000006 	0.251500000000000001 	16 	
+2 	0.655000000000000027 	0.520000000000000018 	0.165000000000000008 	1.40949999999999998 	0.585999999999999965 	0.290999999999999981 	0.405000000000000027 	9 	
+2 	0.369999999999999996 	0.28999999999999998 	0.0899999999999999967 	0.240999999999999992 	0.110000000000000001 	0.0449999999999999983 	0.0690000000000000058 	10 	
+1 	0.484999999999999987 	0.359999999999999987 	0.119999999999999996 	0.515499999999999958 	0.246499999999999997 	0.102499999999999994 	0.146999999999999992 	8 	
+1 	0.255000000000000004 	0.179999999999999993 	0.0550000000000000003 	0.0830000000000000043 	0.0309999999999999998 	0.0214999999999999983 	0.0200000000000000004 	4 	
+2 	0.465000000000000024 	0.405000000000000027 	0.135000000000000009 	0.777499999999999969 	0.435999999999999999 	0.171500000000000014 	0.14549999999999999 	10 	
+1 	0.354999999999999982 	0.260000000000000009 	0.0899999999999999967 	0.192500000000000004 	0.076999999999999999 	0.0379999999999999991 	0.0650000000000000022 	8 	
+2 	0.510000000000000009 	0.395000000000000018 	0.125 	0.580500000000000016 	0.243999999999999995 	0.133500000000000008 	0.188 	11 	
+1 	0.494999999999999996 	0.354999999999999982 	0.119999999999999996 	0.496499999999999997 	0.213999999999999996 	0.104499999999999996 	0.149499999999999994 	8 	
+2 	0.525000000000000022 	0.380000000000000004 	0.125 	0.650000000000000022 	0.302999999999999992 	0.154999999999999999 	0.159000000000000002 	7 	
+1 	0.530000000000000027 	0.419999999999999984 	0.119999999999999996 	0.59650000000000003 	0.255500000000000005 	0.140999999999999986 	0.176999999999999991 	7 	
+2 	0.349999999999999978 	0.260000000000000009 	0.0899999999999999967 	0.198000000000000009 	0.072499999999999995 	0.0560000000000000012 	0.0599999999999999978 	10 	
+0 	0.540000000000000036 	0.41499999999999998 	0.174999999999999989 	0.897499999999999964 	0.275000000000000022 	0.240999999999999992 	0.275000000000000022 	14 	
+1 	0.419999999999999984 	0.299999999999999989 	0.104999999999999996 	0.316000000000000003 	0.1255 	0.0700000000000000067 	0.103499999999999995 	7 	
+2 	0.489999999999999991 	0.380000000000000004 	0.110000000000000001 	0.554000000000000048 	0.293499999999999983 	0.100500000000000006 	0.149999999999999994 	8 	
+0 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	0.979999999999999982 	0.411499999999999977 	0.196000000000000008 	0.225500000000000006 	10 	
+0 	0.625 	0.5 	0.149999999999999994 	0.952999999999999958 	0.344499999999999973 	0.223500000000000004 	0.304999999999999993 	15 	
+0 	0.619999999999999996 	0.469999999999999973 	0.140000000000000013 	1.03249999999999997 	0.360499999999999987 	0.224000000000000005 	0.359999999999999987 	15 	
+2 	0.57999999999999996 	0.46000000000000002 	0.179999999999999993 	1.14500000000000002 	0.479999999999999982 	0.277000000000000024 	0.325000000000000011 	11 	
+1 	0.535000000000000031 	0.390000000000000013 	0.125 	0.598999999999999977 	0.259500000000000008 	0.148999999999999994 	0.169000000000000011 	9 	
+1 	0.525000000000000022 	0.395000000000000018 	0.119999999999999996 	0.607999999999999985 	0.296999999999999986 	0.139500000000000013 	0.140500000000000014 	8 	
+0 	0.630000000000000004 	0.484999999999999987 	0.190000000000000002 	1.24350000000000005 	0.463500000000000023 	0.305499999999999994 	0.390000000000000013 	21 	
+1 	0.369999999999999996 	0.275000000000000022 	0.100000000000000006 	0.22950000000000001 	0.0884999999999999953 	0.0464999999999999997 	0.0700000000000000067 	7 	
+1 	0.434999999999999998 	0.325000000000000011 	0.110000000000000001 	0.366999999999999993 	0.159500000000000003 	0.0800000000000000017 	0.104999999999999996 	6 	
+1 	0.385000000000000009 	0.28999999999999998 	0.0899999999999999967 	0.26150000000000001 	0.111000000000000001 	0.0594999999999999973 	0.0744999999999999968 	9 	
+1 	0.474999999999999978 	0.349999999999999978 	0.110000000000000001 	0.456500000000000017 	0.205999999999999989 	0.0990000000000000047 	0.130000000000000004 	6 	
+2 	0.630000000000000004 	0.469999999999999973 	0.154999999999999999 	1.13250000000000006 	0.588999999999999968 	0.210999999999999993 	0.286999999999999977 	8 	
+1 	0.424999999999999989 	0.299999999999999989 	0.0950000000000000011 	0.351499999999999979 	0.140999999999999986 	0.0774999999999999994 	0.119999999999999996 	8 	
+0 	0.57999999999999996 	0.434999999999999998 	0.140000000000000013 	0.952999999999999958 	0.474999999999999978 	0.216499999999999998 	0.209499999999999992 	9 	
+1 	0.375 	0.28999999999999998 	0.0850000000000000061 	0.23849999999999999 	0.117999999999999994 	0.0449999999999999983 	0.0695000000000000062 	7 	
+0 	0.645000000000000018 	0.505000000000000004 	0.165000000000000008 	1.43250000000000011 	0.684000000000000052 	0.307999999999999996 	0.336000000000000021 	8 	
+0 	0.429999999999999993 	0.325000000000000011 	0.115000000000000005 	0.38650000000000001 	0.147499999999999992 	0.106499999999999997 	0.110000000000000001 	11 	
+2 	0.655000000000000027 	0.484999999999999987 	0.195000000000000007 	1.62000000000000011 	0.627499999999999947 	0.357999999999999985 	0.484999999999999987 	17 	
+2 	0.369999999999999996 	0.280000000000000027 	0.104999999999999996 	0.234000000000000014 	0.0904999999999999971 	0.0585000000000000034 	0.0749999999999999972 	9 	
+1 	0.680000000000000049 	0.530000000000000027 	0.184999999999999998 	1.10949999999999993 	0.439000000000000001 	0.244999999999999996 	0.340000000000000024 	10 	
+2 	0.614999999999999991 	0.5 	0.170000000000000012 	1.05400000000000005 	0.484499999999999986 	0.228000000000000008 	0.294999999999999984 	10 	
+0 	0.330000000000000016 	0.260000000000000009 	0.0800000000000000017 	0.200000000000000011 	0.0625 	0.0500000000000000028 	0.0700000000000000067 	9 	
+1 	0.515000000000000013 	0.400000000000000022 	0.135000000000000009 	0.696500000000000008 	0.320000000000000007 	0.1255 	0.174999999999999989 	9 	
+1 	0.510000000000000009 	0.375 	0.100000000000000006 	0.578500000000000014 	0.237999999999999989 	0.122499999999999998 	0.174999999999999989 	7 	
+0 	0.625 	0.484999999999999987 	0.190000000000000002 	1.1745000000000001 	0.438500000000000001 	0.23050000000000001 	0.419999999999999984 	17 	
+2 	0.560000000000000053 	0.424999999999999989 	0.135000000000000009 	0.848999999999999977 	0.326500000000000012 	0.221000000000000002 	0.264500000000000013 	10 	
+1 	0.349999999999999978 	0.270000000000000018 	0.0749999999999999972 	0.214999999999999997 	0.100000000000000006 	0.0359999999999999973 	0.0650000000000000022 	6 	
+0 	0.54500000000000004 	0.41499999999999998 	0.149999999999999994 	0.733500000000000041 	0.279500000000000026 	0.163000000000000006 	0.2185 	11 	
+0 	0.344999999999999973 	0.255000000000000004 	0.100000000000000006 	0.197000000000000008 	0.0709999999999999937 	0.0509999999999999967 	0.0599999999999999978 	9 	
+2 	0.640000000000000013 	0.564999999999999947 	0.23000000000000001 	1.52099999999999991 	0.644000000000000017 	0.371999999999999997 	0.406000000000000028 	15 	
+2 	0.41499999999999998 	0.304999999999999993 	0.100000000000000006 	0.325000000000000011 	0.156 	0.0505000000000000032 	0.0909999999999999976 	6 	
+2 	0.494999999999999996 	0.41499999999999998 	0.165000000000000008 	0.748500000000000054 	0.264000000000000012 	0.134000000000000008 	0.284999999999999976 	13 	
+0 	0.560000000000000053 	0.445000000000000007 	0.195000000000000007 	0.980999999999999983 	0.304999999999999993 	0.224500000000000005 	0.33500000000000002 	16 	
+2 	0.525000000000000022 	0.400000000000000022 	0.140000000000000013 	0.73250000000000004 	0.334000000000000019 	0.157500000000000001 	0.170000000000000012 	11 	
+2 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.829500000000000015 	0.240499999999999992 	0.182499999999999996 	0.275000000000000022 	11 	
+1 	0.625 	0.46000000000000002 	0.160000000000000003 	1.23950000000000005 	0.550000000000000044 	0.27300000000000002 	0.380000000000000004 	14 	
+0 	0.440000000000000002 	0.340000000000000024 	0.140000000000000013 	0.481999999999999984 	0.185999999999999999 	0.108499999999999999 	0.160000000000000003 	9 	
+2 	0.689999999999999947 	0.550000000000000044 	0.179999999999999993 	1.6915 	0.66549999999999998 	0.402000000000000024 	0.5 	11 	
+0 	0.635000000000000009 	0.5 	0.165000000000000008 	1.45950000000000002 	0.70499999999999996 	0.264500000000000013 	0.390000000000000013 	9 	
+1 	0.440000000000000002 	0.33500000000000002 	0.110000000000000001 	0.388500000000000012 	0.174999999999999989 	0.0835000000000000048 	0.111000000000000001 	7 	
+2 	0.494999999999999996 	0.380000000000000004 	0.119999999999999996 	0.473999999999999977 	0.197000000000000008 	0.106499999999999997 	0.154499999999999998 	10 	
+2 	0.604999999999999982 	0.474999999999999978 	0.179999999999999993 	0.936499999999999999 	0.394000000000000017 	0.219 	0.294999999999999984 	15 	
+2 	0.494999999999999996 	0.385000000000000009 	0.135000000000000009 	0.708999999999999964 	0.210999999999999993 	0.137500000000000011 	0.262000000000000011 	12 	
+0 	0.584999999999999964 	0.455000000000000016 	0.165000000000000008 	0.997999999999999998 	0.344999999999999973 	0.2495 	0.315000000000000002 	12 	
+1 	0.46000000000000002 	0.349999999999999978 	0.110000000000000001 	0.394500000000000017 	0.168500000000000011 	0.0864999999999999936 	0.125 	9 	
+1 	0.434999999999999998 	0.315000000000000002 	0.110000000000000001 	0.368499999999999994 	0.161500000000000005 	0.0714999999999999941 	0.119999999999999996 	7 	
+1 	0.589999999999999969 	0.46000000000000002 	0.14499999999999999 	0.901499999999999968 	0.418999999999999984 	0.178499999999999992 	0.260000000000000009 	11 	
+0 	0.729999999999999982 	0.550000000000000044 	0.204999999999999988 	1.90799999999999992 	0.541499999999999981 	0.356499999999999984 	0.59650000000000003 	14 	
+2 	0.589999999999999969 	0.479999999999999982 	0.160000000000000003 	1.26200000000000001 	0.568500000000000005 	0.27250000000000002 	0.33500000000000002 	9 	
+1 	0.515000000000000013 	0.390000000000000013 	0.125 	0.570500000000000007 	0.237999999999999989 	0.126500000000000001 	0.184999999999999998 	8 	
+2 	0.494999999999999996 	0.400000000000000022 	0.135000000000000009 	0.609999999999999987 	0.27200000000000002 	0.143499999999999989 	0.143999999999999989 	7 	
+0 	0.525000000000000022 	0.424999999999999989 	0.14499999999999999 	0.799499999999999988 	0.33450000000000002 	0.208999999999999991 	0.239999999999999991 	15 	
+2 	0.645000000000000018 	0.510000000000000009 	0.160000000000000003 	1.33000000000000007 	0.666499999999999981 	0.308999999999999997 	0.317000000000000004 	9 	
+2 	0.57999999999999996 	0.46000000000000002 	0.154999999999999999 	1.4395 	0.671499999999999986 	0.27300000000000002 	0.295499999999999985 	10 	
+1 	0.489999999999999991 	0.375 	0.115000000000000005 	0.461500000000000021 	0.203999999999999987 	0.0945000000000000007 	0.142999999999999988 	8 	
+0 	0.46000000000000002 	0.424999999999999989 	0.154999999999999999 	0.745999999999999996 	0.300499999999999989 	0.151999999999999996 	0.239999999999999991 	8 	
+2 	0.564999999999999947 	0.429999999999999993 	0.130000000000000004 	0.78400000000000003 	0.349499999999999977 	0.188500000000000001 	0.212999999999999995 	9 	
+1 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.627499999999999947 	0.2505 	0.117499999999999993 	0.234999999999999987 	9 	
+2 	0.424999999999999989 	0.325000000000000011 	0.0950000000000000011 	0.378500000000000003 	0.170500000000000013 	0.0800000000000000017 	0.100000000000000006 	7 	
+0 	0.599999999999999978 	0.469999999999999973 	0.200000000000000011 	1.03099999999999992 	0.392000000000000015 	0.203499999999999986 	0.28999999999999998 	15 	
+2 	0.57999999999999996 	0.46000000000000002 	0.149999999999999994 	1.04899999999999993 	0.520499999999999963 	0.193500000000000005 	0.304999999999999993 	10 	
+1 	0.395000000000000018 	0.294999999999999984 	0.100000000000000006 	0.271500000000000019 	0.134000000000000008 	0.0325000000000000011 	0.0850000000000000061 	10 	
+1 	0.200000000000000011 	0.154999999999999999 	0.0400000000000000008 	0.043499999999999997 	0.0154999999999999999 	0.00899999999999999932 	0.00700000000000000015 	4 	
+2 	0.354999999999999982 	0.28999999999999998 	0.0899999999999999967 	0.327500000000000013 	0.134000000000000008 	0.0859999999999999931 	0.0899999999999999967 	9 	
+2 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.621999999999999997 	0.265500000000000014 	0.146999999999999992 	0.183999999999999997 	9 	
+0 	0.560000000000000053 	0.434999999999999998 	0.184999999999999998 	1.10600000000000009 	0.421999999999999986 	0.243499999999999994 	0.330000000000000016 	15 	
+2 	0.309999999999999998 	0.220000000000000001 	0.0850000000000000061 	0.145999999999999991 	0.0609999999999999987 	0.0364999999999999977 	0.0449999999999999983 	6 	
+0 	0.630000000000000004 	0.5 	0.184999999999999998 	1.38300000000000001 	0.540000000000000036 	0.331500000000000017 	0.380000000000000004 	10 	
+0 	0.625 	0.479999999999999982 	0.170000000000000012 	1.35250000000000004 	0.623500000000000054 	0.278000000000000025 	0.364999999999999991 	10 	
+0 	0.665000000000000036 	0.530000000000000027 	0.179999999999999993 	1.4910000000000001 	0.634499999999999953 	0.342000000000000026 	0.434999999999999998 	10 	
+2 	0.675000000000000044 	0.515000000000000013 	0.149999999999999994 	1.31200000000000006 	0.55600000000000005 	0.284499999999999975 	0.411499999999999977 	11 	
+1 	0.325000000000000011 	0.25 	0.0800000000000000017 	0.17599999999999999 	0.0594999999999999973 	0.0354999999999999968 	0.0630000000000000004 	7 	
+1 	0.589999999999999969 	0.445000000000000007 	0.135000000000000009 	0.771499999999999964 	0.328000000000000014 	0.174499999999999988 	0.23000000000000001 	9 	
+0 	0.535000000000000031 	0.450000000000000011 	0.135000000000000009 	0.807499999999999996 	0.322000000000000008 	0.180999999999999994 	0.25 	13 	
+2 	0.57999999999999996 	0.46000000000000002 	0.130000000000000004 	0.921000000000000041 	0.356999999999999984 	0.180999999999999994 	0.28999999999999998 	13 	
+1 	0.450000000000000011 	0.33500000000000002 	0.104999999999999996 	0.361999999999999988 	0.157500000000000001 	0.0795000000000000012 	0.1095 	7 	
+1 	0.174999999999999989 	0.130000000000000004 	0.0550000000000000003 	0.0315000000000000002 	0.0105000000000000007 	0.0064999999999999997 	0.0125000000000000007 	5 	
+2 	0.494999999999999996 	0.395000000000000018 	0.125 	0.541499999999999981 	0.237499999999999989 	0.134500000000000008 	0.154999999999999999 	9 	
+1 	0.589999999999999969 	0.46000000000000002 	0.125 	0.755000000000000004 	0.334000000000000019 	0.149999999999999994 	0.237999999999999989 	9 	
+2 	0.505000000000000004 	0.400000000000000022 	0.154999999999999999 	0.841500000000000026 	0.271500000000000019 	0.177499999999999991 	0.284999999999999976 	12 	
+1 	0.560000000000000053 	0.440000000000000002 	0.170000000000000012 	0.944500000000000006 	0.354499999999999982 	0.217499999999999999 	0.299999999999999989 	12 	
+1 	0.5 	0.369999999999999996 	0.119999999999999996 	0.544499999999999984 	0.248999999999999999 	0.106499999999999997 	0.151999999999999996 	8 	
+2 	0.395000000000000018 	0.304999999999999993 	0.104999999999999996 	0.281999999999999973 	0.0975000000000000033 	0.0650000000000000022 	0.096000000000000002 	9 	
+1 	0.440000000000000002 	0.364999999999999991 	0.110000000000000001 	0.446500000000000008 	0.212999999999999995 	0.0889999999999999958 	0.113500000000000004 	9 	
+0 	0.619999999999999996 	0.510000000000000009 	0.179999999999999993 	1.33149999999999991 	0.593999999999999972 	0.276000000000000023 	0.388000000000000012 	11 	
+2 	0.530000000000000027 	0.400000000000000022 	0.125 	0.757499999999999951 	0.39800000000000002 	0.150999999999999995 	0.174999999999999989 	8 	
+0 	0.555000000000000049 	0.424999999999999989 	0.140000000000000013 	0.962999999999999967 	0.440000000000000002 	0.224000000000000005 	0.239999999999999991 	7 	
+2 	0.724999999999999978 	0.569999999999999951 	0.190000000000000002 	2.54999999999999982 	1.07050000000000001 	0.482999999999999985 	0.724999999999999978 	14 	
+2 	0.650000000000000022 	0.520000000000000018 	0.190000000000000002 	1.34450000000000003 	0.519000000000000017 	0.305999999999999994 	0.446500000000000008 	16 	
+2 	0.689999999999999947 	0.530000000000000027 	0.209999999999999992 	1.58299999999999996 	0.735500000000000043 	0.405000000000000027 	0.38650000000000001 	12 	
+1 	0.385000000000000009 	0.304999999999999993 	0.0950000000000000011 	0.252000000000000002 	0.091499999999999998 	0.0550000000000000003 	0.0899999999999999967 	14 	
+2 	0.604999999999999982 	0.469999999999999973 	0.179999999999999993 	1.11549999999999994 	0.478999999999999981 	0.256500000000000006 	0.321000000000000008 	10 	
+1 	0.369999999999999996 	0.280000000000000027 	0.0850000000000000061 	0.198000000000000009 	0.0805000000000000021 	0.0454999999999999988 	0.0580000000000000029 	5 	
+0 	0.505000000000000004 	0.390000000000000013 	0.125 	0.544499999999999984 	0.245999999999999996 	0.149999999999999994 	0.140500000000000014 	7 	
+1 	0.540000000000000036 	0.424999999999999989 	0.130000000000000004 	0.815500000000000003 	0.367499999999999993 	0.13650000000000001 	0.245999999999999996 	11 	
+0 	0.719999999999999973 	0.525000000000000022 	0.179999999999999993 	1.44500000000000006 	0.631000000000000005 	0.321500000000000008 	0.434999999999999998 	7 	
+2 	0.699999999999999956 	0.550000000000000044 	0.174999999999999989 	1.44049999999999989 	0.656499999999999972 	0.298499999999999988 	0.375 	12 	
+0 	0.589999999999999969 	0.445000000000000007 	0.130000000000000004 	1.13250000000000006 	0.382500000000000007 	0.234000000000000014 	0.320000000000000007 	13 	
+2 	0.645000000000000018 	0.489999999999999991 	0.174999999999999989 	1.32000000000000006 	0.652499999999999969 	0.237499999999999989 	0.338500000000000023 	11 	
+1 	0.424999999999999989 	0.340000000000000024 	0.100000000000000006 	0.382000000000000006 	0.164000000000000007 	0.096000000000000002 	0.100000000000000006 	6 	
+2 	0.594999999999999973 	0.455000000000000016 	0.154999999999999999 	1.04099999999999993 	0.415999999999999981 	0.210499999999999993 	0.364999999999999991 	14 	
+0 	0.640000000000000013 	0.505000000000000004 	0.165000000000000008 	1.22350000000000003 	0.521499999999999964 	0.269500000000000017 	0.359999999999999987 	10 	
+1 	0.469999999999999973 	0.354999999999999982 	0.179999999999999993 	0.479999999999999982 	0.205499999999999988 	0.104999999999999996 	0.150499999999999995 	8 	
+2 	0.569999999999999951 	0.419999999999999984 	0.140000000000000013 	0.874500000000000055 	0.415999999999999981 	0.165000000000000008 	0.25 	8 	
+1 	0.284999999999999976 	0.214999999999999997 	0.0700000000000000067 	0.107499999999999998 	0.0509999999999999967 	0.0224999999999999992 	0.0269999999999999997 	6 	
+2 	0.574999999999999956 	0.450000000000000011 	0.154999999999999999 	0.947999999999999954 	0.428999999999999992 	0.205999999999999989 	0.259000000000000008 	7 	
+2 	0.630000000000000004 	0.469999999999999973 	0.14499999999999999 	1.10050000000000003 	0.520000000000000018 	0.260000000000000009 	0.276000000000000023 	9 	
+1 	0.325000000000000011 	0.270000000000000018 	0.100000000000000006 	0.184999999999999998 	0.0800000000000000017 	0.043499999999999997 	0.0650000000000000022 	6 	
+1 	0.400000000000000022 	0.28999999999999998 	0.100000000000000006 	0.267500000000000016 	0.120499999999999996 	0.0604999999999999982 	0.0764999999999999986 	5 	
+1 	0.270000000000000018 	0.190000000000000002 	0.0800000000000000017 	0.0810000000000000026 	0.0264999999999999993 	0.0195 	0.0299999999999999989 	6 	
+1 	0.280000000000000027 	0.220000000000000001 	0.0800000000000000017 	0.131500000000000006 	0.0660000000000000031 	0.0240000000000000005 	0.0299999999999999989 	5 	
+0 	0.635000000000000009 	0.510000000000000009 	0.174999999999999989 	1.21249999999999991 	0.57350000000000001 	0.26100000000000001 	0.359999999999999987 	14 	
+0 	0.469999999999999973 	0.364999999999999991 	0.119999999999999996 	0.581999999999999962 	0.28999999999999998 	0.0919999999999999984 	0.145999999999999991 	8 	
+0 	0.359999999999999987 	0.265000000000000013 	0.0899999999999999967 	0.206499999999999989 	0.0779999999999999999 	0.0570000000000000021 	0.0599999999999999978 	8 	
+2 	0.515000000000000013 	0.395000000000000018 	0.135000000000000009 	1.0069999999999999 	0.471999999999999975 	0.2495 	0.252000000000000002 	8 	
+0 	0.550000000000000044 	0.445000000000000007 	0.154999999999999999 	0.990500000000000047 	0.544000000000000039 	0.177999999999999992 	0.217999999999999999 	9 	
+1 	0.535000000000000031 	0.400000000000000022 	0.135000000000000009 	0.602500000000000036 	0.28949999999999998 	0.120999999999999996 	0.153999999999999998 	9 	
+0 	0.515000000000000013 	0.395000000000000018 	0.135000000000000009 	0.516000000000000014 	0.201500000000000012 	0.132000000000000006 	0.162000000000000005 	9 	
+0 	0.400000000000000022 	0.309999999999999998 	0.115000000000000005 	0.346499999999999975 	0.147499999999999992 	0.0695000000000000062 	0.115000000000000005 	10 	
+2 	0.450000000000000011 	0.325000000000000011 	0.115000000000000005 	0.430499999999999994 	0.223500000000000004 	0.0785000000000000003 	0.115500000000000005 	8 	
+0 	0.699999999999999956 	0.535000000000000031 	0.174999999999999989 	1.77299999999999991 	0.680499999999999994 	0.479999999999999982 	0.512000000000000011 	15 	
+0 	0.484999999999999987 	0.364999999999999991 	0.119999999999999996 	0.588500000000000023 	0.270000000000000018 	0.131000000000000005 	0.174999999999999989 	9 	
+0 	0.535000000000000031 	0.445000000000000007 	0.125 	0.872500000000000053 	0.416999999999999982 	0.19900000000000001 	0.239999999999999991 	8 	
+2 	0.564999999999999947 	0.455000000000000016 	0.170000000000000012 	0.906499999999999972 	0.342000000000000026 	0.156 	0.320000000000000007 	18 	
+2 	0.609999999999999987 	0.474999999999999978 	0.154999999999999999 	0.982999999999999985 	0.456500000000000017 	0.228000000000000008 	0.266000000000000014 	10 	
+2 	0.5 	0.385000000000000009 	0.135000000000000009 	0.64249999999999996 	0.319500000000000006 	0.129000000000000004 	0.153499999999999998 	7 	
+2 	0.775000000000000022 	0.630000000000000004 	0.25 	2.77950000000000008 	1.34850000000000003 	0.760000000000000009 	0.577999999999999958 	12 	
+0 	0.424999999999999989 	0.344999999999999973 	0.110000000000000001 	0.366499999999999992 	0.125 	0.0810000000000000026 	0.117000000000000007 	11 	
+0 	0.564999999999999947 	0.479999999999999982 	0.174999999999999989 	0.956999999999999962 	0.388500000000000012 	0.214999999999999997 	0.275000000000000022 	18 	
+0 	0.564999999999999947 	0.450000000000000011 	0.195000000000000007 	1.00350000000000006 	0.406000000000000028 	0.2505 	0.284999999999999976 	15 	
+0 	0.660000000000000031 	0.564999999999999947 	0.195000000000000007 	1.76049999999999995 	0.691999999999999948 	0.326500000000000012 	0.5 	16 	
+1 	0.395000000000000018 	0.270000000000000018 	0.100000000000000006 	0.298499999999999988 	0.14449999999999999 	0.0609999999999999987 	0.0820000000000000034 	5 	
+0 	0.665000000000000036 	0.505000000000000004 	0.160000000000000003 	1.29150000000000009 	0.631000000000000005 	0.292499999999999982 	0.320000000000000007 	11 	
+0 	0.614999999999999991 	0.515000000000000013 	0.135000000000000009 	1.12149999999999994 	0.54500000000000004 	0.23050000000000001 	0.28999999999999998 	9 	
+0 	0.619999999999999996 	0.434999999999999998 	0.154999999999999999 	1.01200000000000001 	0.47699999999999998 	0.235999999999999988 	0.275000000000000022 	8 	
+1 	0.400000000000000022 	0.299999999999999989 	0.0899999999999999967 	0.281499999999999972 	0.118499999999999994 	0.0609999999999999987 	0.0800000000000000017 	7 	
+0 	0.650000000000000022 	0.505000000000000004 	0.165000000000000008 	1.15999999999999992 	0.478499999999999981 	0.274000000000000021 	0.348999999999999977 	11 	
+1 	0.564999999999999947 	0.46000000000000002 	0.154999999999999999 	0.871500000000000052 	0.3755 	0.214999999999999997 	0.25 	10 	
+1 	0.574999999999999956 	0.445000000000000007 	0.170000000000000012 	0.80149999999999999 	0.347499999999999976 	0.146499999999999991 	0.25 	9 	
+0 	0.685000000000000053 	0.540000000000000036 	0.214999999999999997 	1.7024999999999999 	0.664000000000000035 	0.365499999999999992 	0.473499999999999976 	14 	
+2 	0.530000000000000027 	0.41499999999999998 	0.130000000000000004 	0.842500000000000027 	0.275000000000000022 	0.194500000000000006 	0.265000000000000013 	20 	
+2 	0.57999999999999996 	0.46000000000000002 	0.154999999999999999 	1.03350000000000009 	0.468999999999999972 	0.222500000000000003 	0.294999999999999984 	10 	
+2 	0.57999999999999996 	0.465000000000000024 	0.14499999999999999 	0.887000000000000011 	0.440500000000000003 	0.165500000000000008 	0.265000000000000013 	11 	
+2 	0.744999999999999996 	0.574999999999999956 	0.200000000000000011 	1.8839999999999999 	0.953999999999999959 	0.336000000000000021 	0.494999999999999996 	12 	
+1 	0.33500000000000002 	0.25 	0.0749999999999999972 	0.185999999999999999 	0.0945000000000000007 	0.0379999999999999991 	0.0444999999999999979 	7 	
+2 	0.515000000000000013 	0.380000000000000004 	0.174999999999999989 	0.956500000000000017 	0.325000000000000011 	0.158000000000000002 	0.309999999999999998 	14 	
+2 	0.57999999999999996 	0.455000000000000016 	0.149999999999999994 	1.1140000000000001 	0.476499999999999979 	0.215499999999999997 	0.265000000000000013 	8 	
+0 	0.814999999999999947 	0.650000000000000022 	0.25 	2.25499999999999989 	0.890499999999999958 	0.419999999999999984 	0.797499999999999987 	14 	
+0 	0.729999999999999982 	0.560000000000000053 	0.190000000000000002 	1.94249999999999989 	0.799000000000000044 	0.519499999999999962 	0.565500000000000003 	11 	
+2 	0.630000000000000004 	0.515000000000000013 	0.165000000000000008 	1.35200000000000009 	0.487999999999999989 	0.348999999999999977 	0.450000000000000011 	20 	
+0 	0.640000000000000013 	0.5 	0.149999999999999994 	1.20150000000000001 	0.559000000000000052 	0.231000000000000011 	0.33550000000000002 	9 	
+2 	0.635000000000000009 	0.515000000000000013 	0.160000000000000003 	1.20750000000000002 	0.538499999999999979 	0.281999999999999973 	0.344999999999999973 	11 	
+1 	0.550000000000000044 	0.424999999999999989 	0.149999999999999994 	0.80600000000000005 	0.376000000000000001 	0.171000000000000013 	0.244999999999999996 	14 	
+2 	0.630000000000000004 	0.479999999999999982 	0.149999999999999994 	1.1785000000000001 	0.518499999999999961 	0.247999999999999998 	0.32350000000000001 	8 	
+0 	0.520000000000000018 	0.424999999999999989 	0.149999999999999994 	0.812999999999999945 	0.385000000000000009 	0.201500000000000012 	0.23000000000000001 	10 	
+2 	0.609999999999999987 	0.469999999999999973 	0.149999999999999994 	1.16250000000000009 	0.564999999999999947 	0.258000000000000007 	0.308499999999999996 	11 	
+2 	0.680000000000000049 	0.515000000000000013 	0.170000000000000012 	1.61149999999999993 	0.841500000000000026 	0.305999999999999994 	0.395000000000000018 	11 	
+2 	0.589999999999999969 	0.450000000000000011 	0.184999999999999998 	1.28299999999999992 	0.472999999999999976 	0.276000000000000023 	0.424999999999999989 	16 	
+0 	0.594999999999999973 	0.494999999999999996 	0.184999999999999998 	1.28499999999999992 	0.415999999999999981 	0.224000000000000005 	0.484999999999999987 	13 	
+1 	0.28999999999999998 	0.204999999999999988 	0.0700000000000000067 	0.0975000000000000033 	0.0359999999999999973 	0.0189999999999999995 	0.0350000000000000033 	8 	
+1 	0.359999999999999987 	0.260000000000000009 	0.0899999999999999967 	0.178499999999999992 	0.0645000000000000018 	0.0369999999999999982 	0.0749999999999999972 	7 	
+2 	0.630000000000000004 	0.489999999999999991 	0.190000000000000002 	1.17749999999999999 	0.493499999999999994 	0.336500000000000021 	0.284999999999999976 	11 	
+2 	0.574999999999999956 	0.469999999999999973 	0.184999999999999998 	0.984999999999999987 	0.3745 	0.217499999999999999 	0.354999999999999982 	10 	
+2 	0.344999999999999973 	0.255000000000000004 	0.0800000000000000017 	0.169000000000000011 	0.0599999999999999978 	0.0425000000000000031 	0.0539999999999999994 	10 	
+0 	0.574999999999999956 	0.46000000000000002 	0.165000000000000008 	1.12400000000000011 	0.298499999999999988 	0.178499999999999992 	0.440000000000000002 	13 	
+2 	0.450000000000000011 	0.354999999999999982 	0.115000000000000005 	0.478999999999999981 	0.212499999999999994 	0.104499999999999996 	0.149999999999999994 	8 	
+0 	0.405000000000000027 	0.325000000000000011 	0.110000000000000001 	0.357499999999999984 	0.14499999999999999 	0.072499999999999995 	0.110000000000000001 	12 	
+2 	0.645000000000000018 	0.5 	0.190000000000000002 	1.55950000000000011 	0.740999999999999992 	0.371499999999999997 	0.384500000000000008 	14 	
+1 	0.54500000000000004 	0.429999999999999993 	0.130000000000000004 	0.759499999999999953 	0.357999999999999985 	0.152999999999999997 	0.205499999999999988 	8 	
+1 	0.375 	0.275000000000000022 	0.0950000000000000011 	0.22950000000000001 	0.0950000000000000011 	0.0544999999999999998 	0.0660000000000000031 	7 	
+2 	0.630000000000000004 	0.505000000000000004 	0.165000000000000008 	1.26000000000000001 	0.452500000000000013 	0.275500000000000023 	0.406000000000000028 	14 	
+2 	0.655000000000000027 	0.525000000000000022 	0.184999999999999998 	1.2589999999999999 	0.486999999999999988 	0.221500000000000002 	0.445000000000000007 	20 	
+1 	0.474999999999999978 	0.349999999999999978 	0.100000000000000006 	0.454500000000000015 	0.216499999999999998 	0.111000000000000001 	0.115000000000000005 	7 	
+1 	0.409999999999999976 	0.304999999999999993 	0.0950000000000000011 	0.262500000000000011 	0.100000000000000006 	0.0514999999999999972 	0.0899999999999999967 	6 	
+0 	0.405000000000000027 	0.325000000000000011 	0.110000000000000001 	0.355499999999999983 	0.150999999999999995 	0.0630000000000000004 	0.117000000000000007 	9 	
+0 	0.599999999999999978 	0.484999999999999987 	0.165000000000000008 	1.14050000000000007 	0.586999999999999966 	0.217499999999999999 	0.287999999999999978 	9 	
+0 	0.614999999999999991 	0.474999999999999978 	0.165000000000000008 	1.02299999999999991 	0.490499999999999992 	0.195500000000000007 	0.303499999999999992 	12 	
+2 	0.489999999999999991 	0.395000000000000018 	0.140000000000000013 	0.549000000000000044 	0.221500000000000002 	0.127500000000000002 	0.149999999999999994 	11 	
+2 	0.599999999999999978 	0.474999999999999978 	0.149999999999999994 	0.989999999999999991 	0.38600000000000001 	0.219500000000000001 	0.310499999999999998 	10 	
+0 	0.75 	0.609999999999999987 	0.234999999999999987 	2.50850000000000017 	1.23199999999999998 	0.519000000000000017 	0.611999999999999988 	14 	
+0 	0.709999999999999964 	0.550000000000000044 	0.170000000000000012 	1.6140000000000001 	0.742999999999999994 	0.344999999999999973 	0.450000000000000011 	11 	
+2 	0.41499999999999998 	0.344999999999999973 	0.135000000000000009 	0.38650000000000001 	0.128000000000000003 	0.0700000000000000067 	0.147999999999999993 	13 	
+0 	0.390000000000000013 	0.28999999999999998 	0.125 	0.305499999999999994 	0.120999999999999996 	0.0820000000000000034 	0.0899999999999999967 	7 	
+2 	0.574999999999999956 	0.440000000000000002 	0.184999999999999998 	1.02499999999999991 	0.507499999999999951 	0.224500000000000005 	0.248499999999999999 	10 	
+0 	0.675000000000000044 	0.525000000000000022 	0.170000000000000012 	1.80950000000000011 	0.78400000000000003 	0.391000000000000014 	0.455000000000000016 	12 	
+2 	0.609999999999999987 	0.5 	0.165000000000000008 	1.27150000000000007 	0.491499999999999992 	0.184999999999999998 	0.489999999999999991 	12 	
+0 	0.685000000000000053 	0.564999999999999947 	0.174999999999999989 	1.6379999999999999 	0.777499999999999969 	0.375 	0.438 	11 	
+0 	0.409999999999999976 	0.315000000000000002 	0.110000000000000001 	0.321000000000000008 	0.1255 	0.0655000000000000027 	0.0950000000000000011 	10 	
+2 	0.714999999999999969 	0.550000000000000044 	0.190000000000000002 	2.00450000000000017 	1.04649999999999999 	0.406999999999999973 	0.507499999999999951 	12 	
+1 	0.609999999999999987 	0.474999999999999978 	0.170000000000000012 	1.03849999999999998 	0.443500000000000005 	0.240999999999999992 	0.320000000000000007 	14 	
+2 	0.515000000000000013 	0.385000000000000009 	0.110000000000000001 	0.578500000000000014 	0.253000000000000003 	0.160000000000000003 	0.140000000000000013 	8 	
+2 	0.5 	0.390000000000000013 	0.135000000000000009 	0.659499999999999975 	0.314500000000000002 	0.153499999999999998 	0.1565 	6 	
+0 	0.589999999999999969 	0.465000000000000024 	0.160000000000000003 	1.10050000000000003 	0.506000000000000005 	0.252500000000000002 	0.294999999999999984 	13 	
+0 	0.680000000000000049 	0.505000000000000004 	0.170000000000000012 	1.34349999999999992 	0.657000000000000028 	0.296999999999999986 	0.354999999999999982 	12 	
+2 	0.309999999999999998 	0.234999999999999987 	0.0599999999999999978 	0.119999999999999996 	0.0415000000000000022 	0.0330000000000000016 	0.0400000000000000008 	11 	
+0 	0.5 	0.395000000000000018 	0.149999999999999994 	0.714500000000000024 	0.32350000000000001 	0.172999999999999987 	0.195000000000000007 	9 	
+1 	0.424999999999999989 	0.380000000000000004 	0.104999999999999996 	0.326500000000000012 	0.128500000000000003 	0.0785000000000000003 	0.100000000000000006 	10 	
+2 	0.645000000000000018 	0.484999999999999987 	0.149999999999999994 	1.22150000000000003 	0.569500000000000006 	0.273500000000000021 	0.330000000000000016 	9 	
+1 	0.484999999999999987 	0.375 	0.140000000000000013 	0.521000000000000019 	0.200000000000000011 	0.122999999999999998 	0.170000000000000012 	8 	
+1 	0.434999999999999998 	0.33500000000000002 	0.0950000000000000011 	0.297999999999999987 	0.109 	0.0580000000000000029 	0.115000000000000005 	7 	
+0 	0.584999999999999964 	0.450000000000000011 	0.170000000000000012 	0.86850000000000005 	0.332500000000000018 	0.163500000000000006 	0.270000000000000018 	22 	
+0 	0.540000000000000036 	0.385000000000000009 	0.140000000000000013 	0.765499999999999958 	0.326500000000000012 	0.116000000000000006 	0.236499999999999988 	10 	
+0 	0.569999999999999951 	0.455000000000000016 	0.149999999999999994 	1.10699999999999998 	0.540000000000000036 	0.255000000000000004 	0.270000000000000018 	8 	
+0 	0.515000000000000013 	0.41499999999999998 	0.140000000000000013 	0.693500000000000005 	0.311499999999999999 	0.151999999999999996 	0.200000000000000011 	10 	
+1 	0.455000000000000016 	0.354999999999999982 	0.104999999999999996 	0.371999999999999997 	0.138000000000000012 	0.0764999999999999986 	0.135000000000000009 	9 	
+0 	0.714999999999999969 	0.525000000000000022 	0.184999999999999998 	1.56000000000000005 	0.66549999999999998 	0.383000000000000007 	0.405000000000000027 	11 	
+1 	0.484999999999999987 	0.344999999999999973 	0.160000000000000003 	0.868999999999999995 	0.308499999999999996 	0.184999999999999998 	0.319000000000000006 	9 	
+2 	0.340000000000000024 	0.275000000000000022 	0.0899999999999999967 	0.206499999999999989 	0.072499999999999995 	0.0429999999999999966 	0.0700000000000000067 	10 	
+0 	0.520000000000000018 	0.405000000000000027 	0.140000000000000013 	0.817500000000000004 	0.279500000000000026 	0.182999999999999996 	0.260000000000000009 	17 	
+2 	0.550000000000000044 	0.450000000000000011 	0.130000000000000004 	0.92000000000000004 	0.378000000000000003 	0.23849999999999999 	0.28999999999999998 	11 	
+2 	0.489999999999999991 	0.354999999999999982 	0.154999999999999999 	0.980999999999999983 	0.465000000000000024 	0.201500000000000012 	0.2505 	8 	
+2 	0.525000000000000022 	0.434999999999999998 	0.154999999999999999 	1.06499999999999995 	0.485999999999999988 	0.233000000000000013 	0.284999999999999976 	8 	
+1 	0.209999999999999992 	0.170000000000000012 	0.0449999999999999983 	0.0475000000000000006 	0.0189999999999999995 	0.0109999999999999994 	0.0129999999999999994 	5 	
+0 	0.574999999999999956 	0.479999999999999982 	0.165000000000000008 	1.07800000000000007 	0.51100000000000001 	0.209499999999999992 	0.305999999999999994 	9 	
+1 	0.525000000000000022 	0.385000000000000009 	0.130000000000000004 	0.606999999999999984 	0.235499999999999987 	0.125 	0.195000000000000007 	8 	
+2 	0.455000000000000016 	0.364999999999999991 	0.0950000000000000011 	0.514000000000000012 	0.224500000000000005 	0.101000000000000006 	0.149999999999999994 	15 	
+0 	0.479999999999999982 	0.385000000000000009 	0.135000000000000009 	0.536000000000000032 	0.189500000000000002 	0.141999999999999987 	0.172999999999999987 	14 	
+1 	0.23000000000000001 	0.179999999999999993 	0.0500000000000000028 	0.0640000000000000013 	0.0214999999999999983 	0.0134999999999999998 	0.0200000000000000004 	5 	
+2 	0.584999999999999964 	0.489999999999999991 	0.184999999999999998 	1.17100000000000004 	0.52200000000000002 	0.253500000000000003 	0.33500000000000002 	10 	
+1 	0.515000000000000013 	0.419999999999999984 	0.135000000000000009 	0.710999999999999965 	0.337000000000000022 	0.143999999999999989 	0.204999999999999988 	13 	
+2 	0.67000000000000004 	0.525000000000000022 	0.170000000000000012 	1.40050000000000008 	0.714999999999999969 	0.302499999999999991 	0.387000000000000011 	9 	
+2 	0.619999999999999996 	0.510000000000000009 	0.174999999999999989 	1.15050000000000008 	0.4375 	0.226500000000000007 	0.400000000000000022 	12 	
+2 	0.609999999999999987 	0.479999999999999982 	0.170000000000000012 	1.13700000000000001 	0.456500000000000017 	0.28999999999999998 	0.346999999999999975 	10 	
+2 	0.489999999999999991 	0.390000000000000013 	0.149999999999999994 	0.572999999999999954 	0.225000000000000006 	0.123999999999999999 	0.170000000000000012 	21 	
+1 	0.41499999999999998 	0.304999999999999993 	0.119999999999999996 	0.336000000000000021 	0.165000000000000008 	0.0759999999999999981 	0.0805000000000000021 	7 	
+2 	0.660000000000000031 	0.525000000000000022 	0.200000000000000011 	1.4890000000000001 	0.606500000000000039 	0.379500000000000004 	0.420999999999999985 	10 	
+2 	0.650000000000000022 	0.494999999999999996 	0.160000000000000003 	1.30400000000000005 	0.569999999999999951 	0.312 	0.372499999999999998 	9 	
+2 	0.599999999999999978 	0.455000000000000016 	0.170000000000000012 	1.1915 	0.695999999999999952 	0.239499999999999991 	0.239999999999999991 	8 	
+0 	0.709999999999999964 	0.564999999999999947 	0.195000000000000007 	1.72649999999999992 	0.638000000000000012 	0.336500000000000021 	0.564999999999999947 	17 	
+0 	0.625 	0.474999999999999978 	0.174999999999999989 	1.14349999999999996 	0.475499999999999978 	0.247499999999999998 	0.348999999999999977 	10 	
+2 	0.614999999999999991 	0.479999999999999982 	0.190000000000000002 	1.3600000000000001 	0.530499999999999972 	0.237499999999999989 	0.469999999999999973 	18 	
+2 	0.535000000000000031 	0.419999999999999984 	0.149999999999999994 	0.699500000000000011 	0.257500000000000007 	0.152999999999999997 	0.239999999999999991 	12 	
+2 	0.594999999999999973 	0.474999999999999978 	0.160000000000000003 	1.31749999999999989 	0.407999999999999974 	0.234000000000000014 	0.57999999999999996 	21 	
+0 	0.665000000000000036 	0.515000000000000013 	0.179999999999999993 	1.38900000000000001 	0.594500000000000028 	0.32400000000000001 	0.395000000000000018 	10 	
+2 	0.489999999999999991 	0.395000000000000018 	0.135000000000000009 	0.554499999999999993 	0.212999999999999995 	0.0924999999999999989 	0.214999999999999997 	14 	
+2 	0.574999999999999956 	0.450000000000000011 	0.154999999999999999 	0.976500000000000035 	0.494999999999999996 	0.214499999999999996 	0.234999999999999987 	9 	
+1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.196000000000000008 	0.0825000000000000039 	0.0374999999999999986 	0.0599999999999999978 	7 	
+0 	0.520000000000000018 	0.424999999999999989 	0.14499999999999999 	0.699999999999999956 	0.20699999999999999 	0.190500000000000003 	0.239999999999999991 	13 	
+0 	0.304999999999999993 	0.23000000000000001 	0.0800000000000000017 	0.156 	0.0675000000000000044 	0.0345000000000000029 	0.048000000000000001 	7 	
+1 	0.5 	0.380000000000000004 	0.110000000000000001 	0.560499999999999998 	0.280000000000000027 	0.105999999999999997 	0.149999999999999994 	9 	
+1 	0.569999999999999951 	0.450000000000000011 	0.14499999999999999 	0.751000000000000001 	0.282499999999999973 	0.219500000000000001 	0.221500000000000002 	10 	
+2 	0.540000000000000036 	0.434999999999999998 	0.179999999999999993 	0.995999999999999996 	0.383500000000000008 	0.226000000000000006 	0.325000000000000011 	17 	
+1 	0.550000000000000044 	0.465000000000000024 	0.149999999999999994 	0.936000000000000054 	0.480999999999999983 	0.173999999999999988 	0.243499999999999994 	9 	
+1 	0.5 	0.409999999999999976 	0.140000000000000013 	0.661499999999999977 	0.258500000000000008 	0.162500000000000006 	0.196000000000000008 	9 	
+2 	0.564999999999999947 	0.440000000000000002 	0.125 	0.802000000000000046 	0.359499999999999986 	0.182499999999999996 	0.214999999999999997 	9 	
+2 	0.57999999999999996 	0.450000000000000011 	0.140000000000000013 	1.0129999999999999 	0.380000000000000004 	0.215999999999999998 	0.359999999999999987 	14 	
+0 	0.630000000000000004 	0.479999999999999982 	0.160000000000000003 	1.19900000000000007 	0.526499999999999968 	0.33500000000000002 	0.315000000000000002 	11 	
+2 	0.630000000000000004 	0.494999999999999996 	0.174999999999999989 	1.26950000000000007 	0.604999999999999982 	0.271000000000000019 	0.328000000000000014 	11 	
+2 	0.450000000000000011 	0.349999999999999978 	0.130000000000000004 	0.465500000000000025 	0.20749999999999999 	0.104499999999999996 	0.135000000000000009 	8 	
+0 	0.609999999999999987 	0.450000000000000011 	0.130000000000000004 	0.872500000000000053 	0.389000000000000012 	0.171500000000000014 	0.27200000000000002 	11 	
+0 	0.625 	0.469999999999999973 	0.14499999999999999 	0.983999999999999986 	0.474999999999999978 	0.200000000000000011 	0.265000000000000013 	11 	
+2 	0.569999999999999951 	0.46000000000000002 	0.149999999999999994 	1.03750000000000009 	0.541499999999999981 	0.203499999999999986 	0.25 	9 	
+2 	0.294999999999999984 	0.214999999999999997 	0.0749999999999999972 	0.129000000000000004 	0.0500000000000000028 	0.0294999999999999984 	0.0400000000000000008 	7 	
+0 	0.535000000000000031 	0.405000000000000027 	0.14499999999999999 	0.684499999999999997 	0.27250000000000002 	0.171000000000000013 	0.204999999999999988 	10 	
+0 	0.640000000000000013 	0.484999999999999987 	0.14499999999999999 	1.13349999999999995 	0.552499999999999991 	0.2505 	0.30149999999999999 	11 	
+1 	0.424999999999999989 	0.315000000000000002 	0.100000000000000006 	0.377000000000000002 	0.164500000000000007 	0.0719999999999999946 	0.104999999999999996 	6 	
+0 	0.489999999999999991 	0.400000000000000022 	0.14499999999999999 	0.663499999999999979 	0.209999999999999992 	0.129500000000000004 	0.251500000000000001 	13 	
+2 	0.640000000000000013 	0.515000000000000013 	0.0800000000000000017 	1.04200000000000004 	0.515000000000000013 	0.175499999999999989 	0.174999999999999989 	10 	
+2 	0.369999999999999996 	0.280000000000000027 	0.100000000000000006 	0.252000000000000002 	0.106499999999999997 	0.0594999999999999973 	0.0739999999999999963 	8 	
+1 	0.265000000000000013 	0.195000000000000007 	0.0599999999999999978 	0.0919999999999999984 	0.0345000000000000029 	0.0250000000000000014 	0.0245000000000000009 	6 	
+2 	0.510000000000000009 	0.390000000000000013 	0.125 	0.656499999999999972 	0.262000000000000011 	0.183499999999999996 	0.174999999999999989 	10 	
+2 	0.760000000000000009 	0.604999999999999982 	0.214999999999999997 	2.17300000000000004 	0.801000000000000045 	0.491499999999999992 	0.646000000000000019 	13 	
+2 	0.400000000000000022 	0.304999999999999993 	0.130000000000000004 	0.293499999999999983 	0.096000000000000002 	0.0675000000000000044 	0.104999999999999996 	9 	
+1 	0.200000000000000011 	0.14499999999999999 	0.0599999999999999978 	0.0369999999999999982 	0.0125000000000000007 	0.00949999999999999976 	0.0109999999999999994 	4 	
+1 	0.550000000000000044 	0.424999999999999989 	0.130000000000000004 	0.664000000000000035 	0.269500000000000017 	0.163000000000000006 	0.209999999999999992 	8 	
+2 	0.270000000000000018 	0.200000000000000011 	0.0800000000000000017 	0.120499999999999996 	0.0464999999999999997 	0.0280000000000000006 	0.0400000000000000008 	6 	
+1 	0.419999999999999984 	0.315000000000000002 	0.110000000000000001 	0.402500000000000024 	0.185499999999999998 	0.0830000000000000043 	0.101500000000000007 	8 	
+2 	0.67000000000000004 	0.5 	0.200000000000000011 	1.26899999999999991 	0.575999999999999956 	0.298499999999999988 	0.350999999999999979 	11 	
+1 	0.564999999999999947 	0.440000000000000002 	0.135000000000000009 	0.768000000000000016 	0.330500000000000016 	0.138500000000000012 	0.247499999999999998 	9 	
+1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.333500000000000019 	0.14449999999999999 	0.0714999999999999941 	0.0950000000000000011 	7 	
+1 	0.54500000000000004 	0.375 	0.119999999999999996 	0.543000000000000038 	0.237499999999999989 	0.115500000000000005 	0.172499999999999987 	8 	
+1 	0.41499999999999998 	0.320000000000000007 	0.115000000000000005 	0.304499999999999993 	0.121499999999999997 	0.0734999999999999959 	0.0940000000000000002 	7 	
+1 	0.375 	0.280000000000000027 	0.0800000000000000017 	0.216499999999999998 	0.0934999999999999998 	0.0924999999999999989 	0.0700000000000000067 	7 	
+0 	0.604999999999999982 	0.474999999999999978 	0.174999999999999989 	1.07600000000000007 	0.463000000000000023 	0.219500000000000001 	0.33500000000000002 	9 	
+2 	0.450000000000000011 	0.349999999999999978 	0.100000000000000006 	0.367499999999999993 	0.146499999999999991 	0.101500000000000007 	0.119999999999999996 	10 	
+0 	0.325000000000000011 	0.260000000000000009 	0.0899999999999999967 	0.191500000000000004 	0.0850000000000000061 	0.0359999999999999973 	0.0619999999999999996 	7 	
+2 	0.555000000000000049 	0.440000000000000002 	0.149999999999999994 	0.755000000000000004 	0.306999999999999995 	0.152499999999999997 	0.260000000000000009 	12 	
+0 	0.550000000000000044 	0.424999999999999989 	0.14499999999999999 	0.797000000000000042 	0.296999999999999986 	0.149999999999999994 	0.265000000000000013 	9 	
+1 	0.369999999999999996 	0.280000000000000027 	0.100000000000000006 	0.221000000000000002 	0.116500000000000006 	0.0264999999999999993 	0.0635000000000000009 	6 	
+0 	0.530000000000000027 	0.424999999999999989 	0.149999999999999994 	0.849500000000000033 	0.328000000000000014 	0.232000000000000012 	0.202000000000000013 	8 	
+0 	0.660000000000000031 	0.505000000000000004 	0.190000000000000002 	1.40450000000000008 	0.625499999999999945 	0.337500000000000022 	0.3745 	9 	
+0 	0.46000000000000002 	0.375 	0.119999999999999996 	0.46050000000000002 	0.177499999999999991 	0.110000000000000001 	0.149999999999999994 	7 	
+0 	0.375 	0.28999999999999998 	0.0800000000000000017 	0.281999999999999973 	0.140500000000000014 	0.072499999999999995 	0.0800000000000000017 	7 	
+0 	0.469999999999999973 	0.354999999999999982 	0.140000000000000013 	0.432999999999999996 	0.152499999999999997 	0.0950000000000000011 	0.151999999999999996 	12 	
+1 	0.204999999999999988 	0.149999999999999994 	0.0550000000000000003 	0.0420000000000000026 	0.0254999999999999984 	0.0149999999999999994 	0.0120000000000000002 	5 	
+0 	0.635000000000000009 	0.520000000000000018 	0.165000000000000008 	1.34050000000000002 	0.50649999999999995 	0.295999999999999985 	0.411999999999999977 	11 	
+0 	0.75 	0.564999999999999947 	0.214999999999999997 	1.93799999999999994 	0.773499999999999965 	0.482499999999999984 	0.574999999999999956 	11 	
+0 	0.650000000000000022 	0.525000000000000022 	0.190000000000000002 	1.38500000000000001 	0.887499999999999956 	0.309499999999999997 	0.405000000000000027 	11 	
+2 	0.450000000000000011 	0.344999999999999973 	0.104999999999999996 	0.411499999999999977 	0.179999999999999993 	0.112500000000000003 	0.135000000000000009 	7 	
+0 	0.564999999999999947 	0.505000000000000004 	0.209999999999999992 	1.27649999999999997 	0.501000000000000001 	0.279000000000000026 	0.354999999999999982 	12 	
+1 	0.445000000000000007 	0.33500000000000002 	0.100000000000000006 	0.489499999999999991 	0.274500000000000022 	0.0859999999999999931 	0.110500000000000001 	7 	
+1 	0.234999999999999987 	0.174999999999999989 	0.0400000000000000008 	0.0704999999999999932 	0.033500000000000002 	0.0149999999999999994 	0.0200000000000000004 	5 	
+2 	0.385000000000000009 	0.294999999999999984 	0.0950000000000000011 	0.33500000000000002 	0.146999999999999992 	0.0940000000000000002 	0.0899999999999999967 	7 	
+2 	0.635000000000000009 	0.484999999999999987 	0.179999999999999993 	1.17949999999999999 	0.478499999999999981 	0.277500000000000024 	0.354999999999999982 	10 	
+1 	0.255000000000000004 	0.190000000000000002 	0.0599999999999999978 	0.0859999999999999931 	0.0400000000000000008 	0.0184999999999999991 	0.0250000000000000014 	5 	
+1 	0.604999999999999982 	0.469999999999999973 	0.140000000000000013 	0.938999999999999946 	0.338500000000000023 	0.201000000000000012 	0.320000000000000007 	13 	
+2 	0.689999999999999947 	0.515000000000000013 	0.179999999999999993 	1.84450000000000003 	0.981500000000000039 	0.465500000000000025 	0.341000000000000025 	13 	
+0 	0.510000000000000009 	0.400000000000000022 	0.140000000000000013 	0.814500000000000002 	0.459000000000000019 	0.196500000000000008 	0.195000000000000007 	10 	
+2 	0.589999999999999969 	0.46000000000000002 	0.130000000000000004 	1.10200000000000009 	0.455000000000000016 	0.205499999999999988 	0.330000000000000016 	12 	
+2 	0.645000000000000018 	0.505000000000000004 	0.184999999999999998 	1.46300000000000008 	0.591999999999999971 	0.390500000000000014 	0.415999999999999981 	10 	
+2 	0.630000000000000004 	0.489999999999999991 	0.154999999999999999 	1.25249999999999995 	0.630000000000000004 	0.245999999999999996 	0.288999999999999979 	9 	
+2 	0.599999999999999978 	0.474999999999999978 	0.190000000000000002 	1.08749999999999991 	0.403000000000000025 	0.265500000000000014 	0.325000000000000011 	14 	
+2 	0.465000000000000024 	0.359999999999999987 	0.130000000000000004 	0.526499999999999968 	0.210499999999999993 	0.118499999999999994 	0.165000000000000008 	10 	
+2 	0.445000000000000007 	0.349999999999999978 	0.119999999999999996 	0.442500000000000004 	0.192000000000000004 	0.0955000000000000016 	0.135000000000000009 	8 	
+0 	0.54500000000000004 	0.429999999999999993 	0.160000000000000003 	0.843999999999999972 	0.394500000000000017 	0.185499999999999998 	0.231000000000000011 	9 	
+2 	0.70499999999999996 	0.555000000000000049 	0.214999999999999997 	2.14100000000000001 	1.04649999999999999 	0.383000000000000007 	0.528000000000000025 	11 	
+2 	0.699999999999999956 	0.555000000000000049 	0.200000000000000011 	1.8580000000000001 	0.729999999999999982 	0.366499999999999992 	0.594999999999999973 	11 	
+1 	0.364999999999999991 	0.265000000000000013 	0.135000000000000009 	0.221500000000000002 	0.104999999999999996 	0.0470000000000000001 	0.0604999999999999982 	7 	
+1 	0.569999999999999951 	0.440000000000000002 	0.149999999999999994 	0.755000000000000004 	0.342500000000000027 	0.160000000000000003 	0.224000000000000005 	8 	
+0 	0.569999999999999951 	0.445000000000000007 	0.154999999999999999 	0.732999999999999985 	0.281999999999999973 	0.159000000000000002 	0.234999999999999987 	14 	
+1 	0.395000000000000018 	0.320000000000000007 	0.100000000000000006 	0.307499999999999996 	0.148999999999999994 	0.0534999999999999989 	0.0899999999999999967 	8 	
+1 	0.494999999999999996 	0.369999999999999996 	0.125 	0.47749999999999998 	0.184999999999999998 	0.0704999999999999932 	0.169000000000000011 	18 	
+1 	0.160000000000000003 	0.110000000000000001 	0.0250000000000000014 	0.0179999999999999986 	0.0064999999999999997 	0.00549999999999999968 	0.0050000000000000001 	3 	
+1 	0.465000000000000024 	0.375 	0.119999999999999996 	0.470999999999999974 	0.222000000000000003 	0.118999999999999995 	0.140000000000000013 	9 	
+0 	0.619999999999999996 	0.505000000000000004 	0.160000000000000003 	1.37250000000000005 	0.628499999999999948 	0.275000000000000022 	0.368499999999999994 	11 	
+2 	0.510000000000000009 	0.400000000000000022 	0.140000000000000013 	0.651499999999999968 	0.245499999999999996 	0.166500000000000009 	0.184999999999999998 	10 	
+2 	0.234999999999999987 	0.160000000000000003 	0.0599999999999999978 	0.0544999999999999998 	0.0264999999999999993 	0.00949999999999999976 	0.0149999999999999994 	4 	
+0 	0.70499999999999996 	0.54500000000000004 	0.179999999999999993 	1.53950000000000009 	0.60750000000000004 	0.367499999999999993 	0.464500000000000024 	13 	
+0 	0.57999999999999996 	0.455000000000000016 	0.119999999999999996 	1.0734999999999999 	0.478999999999999981 	0.273500000000000021 	0.265000000000000013 	10 	
+1 	0.409999999999999976 	0.309999999999999998 	0.110000000000000001 	0.315000000000000002 	0.123999999999999999 	0.0820000000000000034 	0.0950000000000000011 	9 	
+1 	0.385000000000000009 	0.265000000000000013 	0.0800000000000000017 	0.251000000000000001 	0.123999999999999999 	0.0369999999999999982 	0.0700000000000000067 	6 	
+0 	0.67000000000000004 	0.505000000000000004 	0.174999999999999989 	1.01449999999999996 	0.4375 	0.271000000000000019 	0.3745 	10 	
+0 	0.680000000000000049 	0.560000000000000053 	0.195000000000000007 	1.66399999999999992 	0.57999999999999996 	0.385500000000000009 	0.54500000000000004 	11 	
+1 	0.540000000000000036 	0.424999999999999989 	0.130000000000000004 	0.720500000000000029 	0.295499999999999985 	0.169000000000000011 	0.225000000000000006 	10 	
+1 	0.569999999999999951 	0.450000000000000011 	0.135000000000000009 	0.794000000000000039 	0.381500000000000006 	0.141499999999999987 	0.244999999999999996 	8 	
+2 	0.599999999999999978 	0.465000000000000024 	0.200000000000000011 	1.2589999999999999 	0.640499999999999958 	0.19850000000000001 	0.356999999999999984 	9 	
+1 	0.665000000000000036 	0.5 	0.170000000000000012 	1.2975000000000001 	0.603500000000000036 	0.290999999999999981 	0.359499999999999986 	9 	
+1 	0.340000000000000024 	0.260000000000000009 	0.0850000000000000061 	0.188500000000000001 	0.081500000000000003 	0.033500000000000002 	0.0599999999999999978 	6 	
+0 	0.75 	0.614999999999999991 	0.204999999999999988 	2.26350000000000007 	0.820999999999999952 	0.422999999999999987 	0.725999999999999979 	12 	
+0 	0.609999999999999987 	0.494999999999999996 	0.160000000000000003 	1.08899999999999997 	0.468999999999999972 	0.198000000000000009 	0.384000000000000008 	11 	
+2 	0.650000000000000022 	0.520000000000000018 	0.195000000000000007 	1.67599999999999993 	0.692999999999999949 	0.440000000000000002 	0.469999999999999973 	15 	
+1 	0.46000000000000002 	0.344999999999999973 	0.104999999999999996 	0.44900000000000001 	0.196000000000000008 	0.0945000000000000007 	0.126500000000000001 	7 	
+1 	0.455000000000000016 	0.349999999999999978 	0.104999999999999996 	0.444500000000000006 	0.212999999999999995 	0.106999999999999998 	0.111500000000000002 	7 	
+1 	0.359999999999999987 	0.270000000000000018 	0.0950000000000000011 	0.200000000000000011 	0.0729999999999999954 	0.0560000000000000012 	0.0609999999999999987 	8 	
+2 	0.650000000000000022 	0.484999999999999987 	0.140000000000000013 	1.17500000000000004 	0.474999999999999978 	0.243499999999999994 	0.214999999999999997 	8 	
+1 	0.479999999999999982 	0.364999999999999991 	0.100000000000000006 	0.461000000000000021 	0.220500000000000002 	0.0835000000000000048 	0.135000000000000009 	8 	
+1 	0.535000000000000031 	0.409999999999999976 	0.154999999999999999 	0.63149999999999995 	0.274500000000000022 	0.141499999999999987 	0.181499999999999995 	12 	
+0 	0.484999999999999987 	0.364999999999999991 	0.140000000000000013 	0.619500000000000051 	0.259500000000000008 	0.14449999999999999 	0.176999999999999991 	14 	
+1 	0.405000000000000027 	0.315000000000000002 	0.104999999999999996 	0.346999999999999975 	0.160500000000000004 	0.0785000000000000003 	0.100000000000000006 	9 	
+1 	0.299999999999999989 	0.234999999999999987 	0.0800000000000000017 	0.131000000000000005 	0.0500000000000000028 	0.0264999999999999993 	0.0429999999999999966 	4 	
+0 	0.520000000000000018 	0.41499999999999998 	0.14499999999999999 	0.804499999999999993 	0.332500000000000018 	0.172499999999999987 	0.284999999999999976 	10 	
+1 	0.41499999999999998 	0.309999999999999998 	0.100000000000000006 	0.280500000000000027 	0.114000000000000004 	0.0565000000000000016 	0.0975000000000000033 	6 	
+2 	0.325000000000000011 	0.239999999999999991 	0.0850000000000000061 	0.172999999999999987 	0.0795000000000000012 	0.0379999999999999991 	0.0500000000000000028 	7 	
+0 	0.57999999999999996 	0.450000000000000011 	0.184999999999999998 	0.995500000000000052 	0.394500000000000017 	0.27200000000000002 	0.284999999999999976 	11 	
+0 	0.550000000000000044 	0.450000000000000011 	0.149999999999999994 	0.875 	0.361999999999999988 	0.175499999999999989 	0.276500000000000024 	10 	
+0 	0.619999999999999996 	0.479999999999999982 	0.174999999999999989 	1.04049999999999998 	0.464000000000000024 	0.222500000000000003 	0.299999999999999989 	9 	
+1 	0.385000000000000009 	0.284999999999999976 	0.0850000000000000061 	0.243999999999999995 	0.121499999999999997 	0.0444999999999999979 	0.0680000000000000049 	8 	
+2 	0.5 	0.400000000000000022 	0.165000000000000008 	0.824999999999999956 	0.254000000000000004 	0.204999999999999988 	0.284999999999999976 	13 	
+2 	0.604999999999999982 	0.465000000000000024 	0.165000000000000008 	1.05600000000000005 	0.421499999999999986 	0.247499999999999998 	0.340000000000000024 	13 	
+2 	0.440000000000000002 	0.354999999999999982 	0.125 	0.47749999999999998 	0.132000000000000006 	0.081500000000000003 	0.190000000000000002 	9 	
+1 	0.239999999999999991 	0.174999999999999989 	0.0650000000000000022 	0.0665000000000000036 	0.0309999999999999998 	0.0134999999999999998 	0.0170000000000000012 	3 	
+1 	0.520000000000000018 	0.41499999999999998 	0.140000000000000013 	0.637499999999999956 	0.307999999999999996 	0.133500000000000008 	0.16800000000000001 	9 	
+0 	0.530000000000000027 	0.419999999999999984 	0.135000000000000009 	0.677000000000000046 	0.256500000000000006 	0.141499999999999987 	0.209999999999999992 	9 	
+2 	0.550000000000000044 	0.409999999999999976 	0.130000000000000004 	0.870500000000000052 	0.445500000000000007 	0.211499999999999994 	0.212999999999999995 	9 	
+1 	0.555000000000000049 	0.46000000000000002 	0.14499999999999999 	0.900499999999999967 	0.384500000000000008 	0.158000000000000002 	0.276500000000000024 	11 	
+2 	0.665000000000000036 	0.505000000000000004 	0.160000000000000003 	1.28899999999999992 	0.614500000000000046 	0.253000000000000003 	0.366499999999999992 	11 	
+1 	0.25 	0.184999999999999998 	0.0650000000000000022 	0.0685000000000000053 	0.0294999999999999984 	0.0140000000000000003 	0.0224999999999999992 	5 	
+2 	0.564999999999999947 	0.424999999999999989 	0.160000000000000003 	0.942500000000000004 	0.349499999999999977 	0.2185 	0.275000000000000022 	17 	
+0 	0.589999999999999969 	0.469999999999999973 	0.170000000000000012 	0.900000000000000022 	0.354999999999999982 	0.190500000000000003 	0.25 	11 	
+2 	0.405000000000000027 	0.309999999999999998 	0.100000000000000006 	0.385000000000000009 	0.172999999999999987 	0.091499999999999998 	0.110000000000000001 	7 	
+1 	0.260000000000000009 	0.214999999999999997 	0.0800000000000000017 	0.0990000000000000047 	0.0369999999999999982 	0.0254999999999999984 	0.0449999999999999983 	5 	
+1 	0.400000000000000022 	0.304999999999999993 	0.100000000000000006 	0.341500000000000026 	0.17599999999999999 	0.0625 	0.0864999999999999936 	7 	
+2 	0.694999999999999951 	0.569999999999999951 	0.200000000000000011 	2.03299999999999992 	0.751000000000000001 	0.425499999999999989 	0.685000000000000053 	15 	
+2 	0.560000000000000053 	0.445000000000000007 	0.160000000000000003 	0.896499999999999964 	0.419999999999999984 	0.217499999999999999 	0.221500000000000002 	8 	
+2 	0.609999999999999987 	0.484999999999999987 	0.170000000000000012 	1.02249999999999996 	0.418999999999999984 	0.240499999999999992 	0.359999999999999987 	12 	
+1 	0.375 	0.280000000000000027 	0.0800000000000000017 	0.234499999999999986 	0.112500000000000003 	0.0454999999999999988 	0.067000000000000004 	6 	
+0 	0.505000000000000004 	0.385000000000000009 	0.135000000000000009 	0.61850000000000005 	0.251000000000000001 	0.117499999999999993 	0.200000000000000011 	12 	
+0 	0.589999999999999969 	0.455000000000000016 	0.149999999999999994 	0.975999999999999979 	0.465000000000000024 	0.205499999999999988 	0.276500000000000024 	10 	
+1 	0.609999999999999987 	0.465000000000000024 	0.125 	0.922499999999999987 	0.435999999999999999 	0.190000000000000002 	0.260000000000000009 	9 	
+1 	0.375 	0.275000000000000022 	0.0850000000000000061 	0.220000000000000001 	0.109 	0.0500000000000000028 	0.0604999999999999982 	7 	
+2 	0.419999999999999984 	0.33500000000000002 	0.115000000000000005 	0.368999999999999995 	0.171000000000000013 	0.0709999999999999937 	0.119999999999999996 	8 	
+1 	0.344999999999999973 	0.270000000000000018 	0.110000000000000001 	0.213499999999999995 	0.0820000000000000034 	0.0544999999999999998 	0.0700000000000000067 	7 	
+2 	0.655000000000000027 	0.515000000000000013 	0.160000000000000003 	1.31000000000000005 	0.553000000000000047 	0.368999999999999995 	0.344999999999999973 	11 	
+2 	0.484999999999999987 	0.354999999999999982 	0.119999999999999996 	0.547000000000000042 	0.214999999999999997 	0.161500000000000005 	0.140000000000000013 	10 	
+1 	0.349999999999999978 	0.260000000000000009 	0.0950000000000000011 	0.210999999999999993 	0.0859999999999999931 	0.0560000000000000012 	0.0680000000000000049 	7 	
+2 	0.675000000000000044 	0.525000000000000022 	0.184999999999999998 	1.58699999999999997 	0.693500000000000005 	0.336000000000000021 	0.395000000000000018 	13 	
+0 	0.515000000000000013 	0.400000000000000022 	0.170000000000000012 	0.796000000000000041 	0.258000000000000007 	0.175499999999999989 	0.280000000000000027 	16 	
+1 	0.424999999999999989 	0.309999999999999998 	0.104999999999999996 	0.364999999999999991 	0.159000000000000002 	0.0825000000000000039 	0.104999999999999996 	6 	
+1 	0.179999999999999993 	0.130000000000000004 	0.0449999999999999983 	0.0275000000000000001 	0.0125000000000000007 	0.0100000000000000002 	0.00899999999999999932 	3 	
+0 	0.640000000000000013 	0.474999999999999978 	0.140000000000000013 	1.07250000000000001 	0.489499999999999991 	0.22950000000000001 	0.309999999999999998 	8 	
+0 	0.694999999999999951 	0.564999999999999947 	0.190000000000000002 	1.76350000000000007 	0.746500000000000052 	0.399000000000000021 	0.497499999999999998 	11 	
+2 	0.560000000000000053 	0.455000000000000016 	0.165000000000000008 	0.859999999999999987 	0.401500000000000024 	0.169500000000000012 	0.244999999999999996 	11 	
+0 	0.525000000000000022 	0.409999999999999976 	0.135000000000000009 	0.79049999999999998 	0.406499999999999972 	0.198000000000000009 	0.176999999999999991 	8 	
+1 	0.550000000000000044 	0.434999999999999998 	0.140000000000000013 	0.753499999999999948 	0.328500000000000014 	0.155499999999999999 	0.232500000000000012 	10 	
+1 	0.619999999999999996 	0.489999999999999991 	0.160000000000000003 	1.06600000000000006 	0.446000000000000008 	0.245999999999999996 	0.304999999999999993 	11 	
+0 	0.469999999999999973 	0.349999999999999978 	0.14499999999999999 	0.51749999999999996 	0.187 	0.123499999999999999 	0.179999999999999993 	11 	
+1 	0.619999999999999996 	0.484999999999999987 	0.179999999999999993 	1.1785000000000001 	0.467500000000000027 	0.265500000000000014 	0.390000000000000013 	13 	
+0 	0.589999999999999969 	0.465000000000000024 	0.149999999999999994 	1.15100000000000002 	0.612999999999999989 	0.23899999999999999 	0.251500000000000001 	9 	
+2 	0.57999999999999996 	0.465000000000000024 	0.174999999999999989 	1.03499999999999992 	0.401000000000000023 	0.186499999999999999 	0.385000000000000009 	17 	
+2 	0.694999999999999951 	0.525000000000000022 	0.174999999999999989 	1.74199999999999999 	0.695999999999999952 	0.389000000000000012 	0.505000000000000004 	12 	
+2 	0.619999999999999996 	0.505000000000000004 	0.184999999999999998 	1.52750000000000008 	0.689999999999999947 	0.367999999999999994 	0.349999999999999978 	13 	
+2 	0.380000000000000004 	0.284999999999999976 	0.100000000000000006 	0.266500000000000015 	0.115000000000000005 	0.0609999999999999987 	0.0749999999999999972 	11 	
+2 	0.67000000000000004 	0.54500000000000004 	0.200000000000000011 	1.7024999999999999 	0.832999999999999963 	0.373999999999999999 	0.409999999999999976 	11 	
+2 	0.630000000000000004 	0.494999999999999996 	0.179999999999999993 	1.31000000000000005 	0.494999999999999996 	0.294999999999999984 	0.469499999999999973 	10 	
+0 	0.70499999999999996 	0.535000000000000031 	0.179999999999999993 	1.68500000000000005 	0.692999999999999949 	0.419999999999999984 	0.404500000000000026 	12 	
+2 	0.594999999999999973 	0.434999999999999998 	0.160000000000000003 	1.05699999999999994 	0.425499999999999989 	0.224000000000000005 	0.309999999999999998 	9 	
+1 	0.505000000000000004 	0.390000000000000013 	0.184999999999999998 	0.612500000000000044 	0.267000000000000015 	0.141999999999999987 	0.171999999999999986 	7 	
+1 	0.380000000000000004 	0.284999999999999976 	0.0850000000000000061 	0.236999999999999988 	0.115000000000000005 	0.0405000000000000013 	0.0700000000000000067 	6 	
+2 	0.489999999999999991 	0.419999999999999984 	0.125 	0.608999999999999986 	0.23899999999999999 	0.143499999999999989 	0.220000000000000001 	14 	
+0 	0.660000000000000031 	0.525000000000000022 	0.204999999999999988 	1.36650000000000005 	0.500499999999999945 	0.290999999999999981 	0.409999999999999976 	18 	
+1 	0.330000000000000016 	0.255000000000000004 	0.0850000000000000061 	0.165500000000000008 	0.0630000000000000004 	0.0389999999999999999 	0.0599999999999999978 	8 	
+2 	0.584999999999999964 	0.405000000000000027 	0.149999999999999994 	1.25649999999999995 	0.434999999999999998 	0.202000000000000013 	0.325000000000000011 	15 	
+2 	0.474999999999999978 	0.369999999999999996 	0.125 	0.509499999999999953 	0.216499999999999998 	0.112500000000000003 	0.165000000000000008 	9 	
+0 	0.660000000000000031 	0.520000000000000018 	0.179999999999999993 	1.51400000000000001 	0.526000000000000023 	0.297499999999999987 	0.419999999999999984 	19 	
+2 	0.510000000000000009 	0.405000000000000027 	0.130000000000000004 	0.717500000000000027 	0.372499999999999998 	0.158000000000000002 	0.170000000000000012 	9 	
+1 	0.584999999999999964 	0.450000000000000011 	0.149999999999999994 	0.891499999999999959 	0.39750000000000002 	0.203499999999999986 	0.253000000000000003 	8 	
+0 	0.560000000000000053 	0.445000000000000007 	0.154999999999999999 	1.22399999999999998 	0.556499999999999995 	0.322500000000000009 	0.269500000000000017 	10 	
+2 	0.469999999999999973 	0.385000000000000009 	0.135000000000000009 	0.589500000000000024 	0.276500000000000024 	0.119999999999999996 	0.170000000000000012 	8 	
+0 	0.619999999999999996 	0.494999999999999996 	0.170000000000000012 	1.06200000000000006 	0.371999999999999997 	0.212999999999999995 	0.340000000000000024 	11 	
+2 	0.614999999999999991 	0.479999999999999982 	0.179999999999999993 	1.15949999999999998 	0.484499999999999986 	0.216499999999999998 	0.325000000000000011 	13 	
+2 	0.515000000000000013 	0.400000000000000022 	0.160000000000000003 	0.817500000000000004 	0.251500000000000001 	0.156 	0.299999999999999989 	23 	
+1 	0.520000000000000018 	0.400000000000000022 	0.14499999999999999 	0.660000000000000031 	0.267000000000000015 	0.105499999999999997 	0.220000000000000001 	13 	
+1 	0.46000000000000002 	0.349999999999999978 	0.110000000000000001 	0.467500000000000027 	0.212499999999999994 	0.0990000000000000047 	0.137500000000000011 	7 	
+1 	0.275000000000000022 	0.195000000000000007 	0.0650000000000000022 	0.105999999999999997 	0.0539999999999999994 	0.0200000000000000004 	0.0280000000000000006 	6 	
+1 	0.469999999999999973 	0.349999999999999978 	0.130000000000000004 	0.466000000000000025 	0.184499999999999997 	0.0990000000000000047 	0.14499999999999999 	11 	
+0 	0.469999999999999973 	0.354999999999999982 	0.100000000000000006 	0.475499999999999978 	0.16750000000000001 	0.0805000000000000021 	0.184999999999999998 	10 	
+0 	0.729999999999999982 	0.569999999999999951 	0.165000000000000008 	2.01650000000000018 	1.06850000000000001 	0.417999999999999983 	0.434999999999999998 	10 	
+1 	0.469999999999999973 	0.375 	0.119999999999999996 	0.486999999999999988 	0.196000000000000008 	0.0990000000000000047 	0.135000000000000009 	8 	
+0 	0.689999999999999947 	0.560000000000000053 	0.214999999999999997 	1.71900000000000008 	0.680000000000000049 	0.298999999999999988 	0.469999999999999973 	17 	
+0 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	1.07650000000000001 	0.490999999999999992 	0.220000000000000001 	0.286999999999999977 	9 	
+2 	0.569999999999999951 	0.419999999999999984 	0.154999999999999999 	1.00800000000000001 	0.377000000000000002 	0.193000000000000005 	0.340000000000000024 	13 	
+2 	0.555000000000000049 	0.455000000000000016 	0.160000000000000003 	1.05750000000000011 	0.392500000000000016 	0.228000000000000008 	0.292999999999999983 	13 	
+2 	0.650000000000000022 	0.5 	0.140000000000000013 	1.23799999999999999 	0.616500000000000048 	0.235499999999999987 	0.320000000000000007 	8 	
+0 	0.555000000000000049 	0.424999999999999989 	0.140000000000000013 	0.788000000000000034 	0.281999999999999973 	0.159500000000000003 	0.284999999999999976 	11 	
+1 	0.455000000000000016 	0.369999999999999996 	0.125 	0.432999999999999996 	0.201000000000000012 	0.126500000000000001 	0.14499999999999999 	9 	
+2 	0.604999999999999982 	0.474999999999999978 	0.14499999999999999 	0.884000000000000008 	0.383500000000000008 	0.190500000000000003 	0.270000000000000018 	8 	
+2 	0.505000000000000004 	0.385000000000000009 	0.104999999999999996 	0.552499999999999991 	0.23899999999999999 	0.1245 	0.155499999999999999 	9 	
+0 	0.694999999999999951 	0.535000000000000031 	0.174999999999999989 	1.83850000000000002 	0.803499999999999992 	0.396000000000000019 	0.503000000000000003 	10 	
+2 	0.630000000000000004 	0.489999999999999991 	0.165000000000000008 	1.2004999999999999 	0.574999999999999956 	0.27300000000000002 	0.293999999999999984 	10 	
+1 	0.364999999999999991 	0.270000000000000018 	0.0749999999999999972 	0.221500000000000002 	0.0950000000000000011 	0.0444999999999999979 	0.0700000000000000067 	6 	
+1 	0.46000000000000002 	0.344999999999999973 	0.110000000000000001 	0.3755 	0.152499999999999997 	0.0580000000000000029 	0.125 	7 	
+1 	0.214999999999999997 	0.154999999999999999 	0.0599999999999999978 	0.0524999999999999981 	0.0210000000000000013 	0.0165000000000000008 	0.0149999999999999994 	5 	
+0 	0.525000000000000022 	0.41499999999999998 	0.140000000000000013 	0.723999999999999977 	0.347499999999999976 	0.172999999999999987 	0.174999999999999989 	8 	
+0 	0.604999999999999982 	0.5 	0.184999999999999998 	1.11850000000000005 	0.468999999999999972 	0.258500000000000008 	0.33500000000000002 	9 	
+0 	0.57999999999999996 	0.450000000000000011 	0.154999999999999999 	0.930000000000000049 	0.385000000000000009 	0.245999999999999996 	0.265000000000000013 	9 	
+2 	0.57999999999999996 	0.450000000000000011 	0.149999999999999994 	0.927000000000000046 	0.276000000000000023 	0.181499999999999995 	0.359999999999999987 	14 	
+2 	0.694999999999999951 	0.560000000000000053 	0.190000000000000002 	1.49399999999999999 	0.587999999999999967 	0.342500000000000027 	0.484999999999999987 	15 	
+2 	0.555000000000000049 	0.440000000000000002 	0.140000000000000013 	0.870500000000000052 	0.406999999999999973 	0.156 	0.255000000000000004 	9 	
+1 	0.550000000000000044 	0.419999999999999984 	0.130000000000000004 	0.63600000000000001 	0.293999999999999984 	0.143999999999999989 	0.175499999999999989 	8 	
+2 	0.589999999999999969 	0.455000000000000016 	0.14499999999999999 	1.07299999999999995 	0.474999999999999978 	0.190000000000000002 	0.284999999999999976 	14 	
+0 	0.635000000000000009 	0.484999999999999987 	0.165000000000000008 	1.26950000000000007 	0.563500000000000001 	0.306499999999999995 	0.339500000000000024 	11 	
+0 	0.429999999999999993 	0.340000000000000024 	0.119999999999999996 	0.391000000000000014 	0.155499999999999999 	0.0950000000000000011 	0.140500000000000014 	7 	
+1 	0.299999999999999989 	0.23000000000000001 	0.0850000000000000061 	0.117000000000000007 	0.0500000000000000028 	0.0175000000000000017 	0.0415000000000000022 	6 	
+0 	0.645000000000000018 	0.510000000000000009 	0.190000000000000002 	1.36299999999999999 	0.572999999999999954 	0.361999999999999988 	0.359999999999999987 	10 	
+1 	0.364999999999999991 	0.275000000000000022 	0.135000000000000009 	0.239999999999999991 	0.107999999999999999 	0.0444999999999999979 	0.0734999999999999959 	7 	
+2 	0.569999999999999951 	0.450000000000000011 	0.154999999999999999 	1.19350000000000001 	0.513000000000000012 	0.209999999999999992 	0.343000000000000027 	10 	
+0 	0.699999999999999956 	0.525000000000000022 	0.190000000000000002 	1.60149999999999992 	0.706999999999999962 	0.364999999999999991 	0.429999999999999993 	10 	
+0 	0.599999999999999978 	0.450000000000000011 	0.149999999999999994 	0.962500000000000022 	0.4375 	0.222500000000000003 	0.277500000000000024 	9 	
+1 	0.214999999999999997 	0.149999999999999994 	0.0299999999999999989 	0.0384999999999999995 	0.0114999999999999998 	0.0050000000000000001 	0.0100000000000000002 	5 	
+0 	0.5 	0.375 	0.115000000000000005 	0.594500000000000028 	0.184999999999999998 	0.147999999999999993 	0.190000000000000002 	11 	
+1 	0.540000000000000036 	0.41499999999999998 	0.110000000000000001 	0.618999999999999995 	0.275500000000000023 	0.149999999999999994 	0.17649999999999999 	10 	
+0 	0.505000000000000004 	0.390000000000000013 	0.115000000000000005 	0.660000000000000031 	0.304499999999999993 	0.155499999999999999 	0.174999999999999989 	8 	
+0 	0.489999999999999991 	0.359999999999999987 	0.110000000000000001 	0.500499999999999945 	0.161000000000000004 	0.106999999999999998 	0.195000000000000007 	17 	
+0 	0.609999999999999987 	0.465000000000000024 	0.160000000000000003 	1.07250000000000001 	0.483499999999999985 	0.251500000000000001 	0.280000000000000027 	10 	
+1 	0.46000000000000002 	0.369999999999999996 	0.110000000000000001 	0.396500000000000019 	0.148499999999999993 	0.0855000000000000066 	0.14549999999999999 	8 	
+0 	0.564999999999999947 	0.455000000000000016 	0.174999999999999989 	1.0129999999999999 	0.342000000000000026 	0.20699999999999999 	0.349999999999999978 	19 	
+2 	0.57999999999999996 	0.445000000000000007 	0.149999999999999994 	0.952500000000000013 	0.431499999999999995 	0.194500000000000006 	0.286999999999999977 	11 	
+0 	0.680000000000000049 	0.515000000000000013 	0.174999999999999989 	1.61850000000000005 	0.512499999999999956 	0.408999999999999975 	0.619999999999999996 	12 	
+0 	0.655000000000000027 	0.5 	0.204999999999999988 	1.52800000000000002 	0.621500000000000052 	0.372499999999999998 	0.453500000000000014 	11 	
+0 	0.689999999999999947 	0.550000000000000044 	0.179999999999999993 	1.65900000000000003 	0.871500000000000052 	0.265500000000000014 	0.439500000000000002 	9 	
+0 	0.535000000000000031 	0.409999999999999976 	0.130000000000000004 	0.714500000000000024 	0.33500000000000002 	0.143999999999999989 	0.20749999999999999 	9 	
+1 	0.320000000000000007 	0.239999999999999991 	0.0899999999999999967 	0.157500000000000001 	0.0700000000000000067 	0.0264999999999999993 	0.0425000000000000031 	5 	
+1 	0.330000000000000016 	0.25 	0.104999999999999996 	0.171500000000000014 	0.0655000000000000027 	0.0350000000000000033 	0.0599999999999999978 	7 	
+0 	0.699999999999999956 	0.574999999999999956 	0.204999999999999988 	1.77299999999999991 	0.604999999999999982 	0.447000000000000008 	0.538000000000000034 	13 	
+1 	0.510000000000000009 	0.400000000000000022 	0.149999999999999994 	0.744999999999999996 	0.286499999999999977 	0.16750000000000001 	0.234999999999999987 	13 	
+2 	0.655000000000000027 	0.54500000000000004 	0.190000000000000002 	1.4245000000000001 	0.632499999999999951 	0.333000000000000018 	0.378000000000000003 	10 	
+0 	0.574999999999999956 	0.419999999999999984 	0.135000000000000009 	0.856999999999999984 	0.461000000000000021 	0.146999999999999992 	0.212499999999999994 	10 	
+2 	0.489999999999999991 	0.390000000000000013 	0.135000000000000009 	0.591999999999999971 	0.241999999999999993 	0.096000000000000002 	0.183499999999999996 	15 	
+0 	0.619999999999999996 	0.520000000000000018 	0.225000000000000006 	1.1835 	0.378000000000000003 	0.270000000000000018 	0.395000000000000018 	23 	
+2 	0.589999999999999969 	0.46000000000000002 	0.140000000000000013 	1.004 	0.495999999999999996 	0.216499999999999998 	0.260000000000000009 	9 	
+0 	0.724999999999999978 	0.569999999999999951 	0.204999999999999988 	1.61949999999999994 	0.743999999999999995 	0.315000000000000002 	0.487999999999999989 	11 	
+2 	0.630000000000000004 	0.455000000000000016 	0.149999999999999994 	1.13149999999999995 	0.480999999999999983 	0.274500000000000022 	0.304999999999999993 	9 	
+0 	0.57999999999999996 	0.46000000000000002 	0.184999999999999998 	1.0169999999999999 	0.351499999999999979 	0.200000000000000011 	0.320000000000000007 	10 	
+0 	0.515000000000000013 	0.424999999999999989 	0.140000000000000013 	0.766000000000000014 	0.303999999999999992 	0.172499999999999987 	0.255000000000000004 	14 	
+0 	0.46000000000000002 	0.364999999999999991 	0.125 	0.478499999999999981 	0.205999999999999989 	0.104499999999999996 	0.140999999999999986 	8 	
+0 	0.619999999999999996 	0.510000000000000009 	0.149999999999999994 	1.45599999999999996 	0.580999999999999961 	0.287499999999999978 	0.320000000000000007 	13 	
+2 	0.540000000000000036 	0.419999999999999984 	0.154999999999999999 	0.738500000000000045 	0.351499999999999979 	0.151999999999999996 	0.214999999999999997 	12 	
+0 	0.609999999999999987 	0.479999999999999982 	0.174999999999999989 	1.06749999999999989 	0.391000000000000014 	0.215999999999999998 	0.419999999999999984 	15 	
+0 	0.650000000000000022 	0.54500000000000004 	0.174999999999999989 	1.52449999999999997 	0.589999999999999969 	0.326000000000000012 	0.494999999999999996 	10 	
+2 	0.599999999999999978 	0.465000000000000024 	0.179999999999999993 	1.19300000000000006 	0.514499999999999957 	0.315000000000000002 	0.305499999999999994 	8 	
+0 	0.564999999999999947 	0.440000000000000002 	0.135000000000000009 	0.82999999999999996 	0.393000000000000016 	0.173499999999999988 	0.237999999999999989 	9 	
+1 	0.315000000000000002 	0.239999999999999991 	0.0700000000000000067 	0.137000000000000011 	0.0544999999999999998 	0.0315000000000000002 	0.0400000000000000008 	8 	
+0 	0.744999999999999996 	0.584999999999999964 	0.190000000000000002 	1.96599999999999997 	0.843500000000000028 	0.437 	0.58550000000000002 	18 	
+0 	0.675000000000000044 	0.525000000000000022 	0.170000000000000012 	1.71100000000000008 	0.836500000000000021 	0.35199999999999998 	0.474999999999999978 	9 	
+1 	0.469999999999999973 	0.344999999999999973 	0.140000000000000013 	0.461500000000000021 	0.229000000000000009 	0.110500000000000001 	0.116000000000000006 	9 	
+1 	0.424999999999999989 	0.325000000000000011 	0.110000000000000001 	0.317000000000000004 	0.135000000000000009 	0.048000000000000001 	0.0899999999999999967 	8 	
+2 	0.675000000000000044 	0.505000000000000004 	0.160000000000000003 	1.53200000000000003 	0.739999999999999991 	0.356999999999999984 	0.381500000000000006 	11 	
+1 	0.550000000000000044 	0.445000000000000007 	0.14499999999999999 	0.783000000000000029 	0.304499999999999993 	0.157000000000000001 	0.265000000000000013 	11 	
+1 	0.574999999999999956 	0.429999999999999993 	0.130000000000000004 	0.742500000000000049 	0.28949999999999998 	0.200500000000000012 	0.220000000000000001 	8 	
+2 	0.619999999999999996 	0.474999999999999978 	0.184999999999999998 	1.32499999999999996 	0.604500000000000037 	0.325000000000000011 	0.330000000000000016 	13 	
+2 	0.505000000000000004 	0.364999999999999991 	0.115000000000000005 	0.521000000000000019 	0.25 	0.096000000000000002 	0.149999999999999994 	8 	
+2 	0.584999999999999964 	0.46000000000000002 	0.165000000000000008 	1.11349999999999993 	0.582500000000000018 	0.234499999999999986 	0.274000000000000021 	10 	
+1 	0.375 	0.280000000000000027 	0.0899999999999999967 	0.214999999999999997 	0.0840000000000000052 	0.0599999999999999978 	0.0550000000000000003 	6 	
+1 	0.280000000000000027 	0.209999999999999992 	0.0850000000000000061 	0.106499999999999997 	0.0389999999999999999 	0.0294999999999999984 	0.0299999999999999989 	4 	
+0 	0.739999999999999991 	0.564999999999999947 	0.204999999999999988 	2.11900000000000022 	0.965500000000000025 	0.518499999999999961 	0.481999999999999984 	12 	
+2 	0.564999999999999947 	0.465000000000000024 	0.174999999999999989 	0.994999999999999996 	0.389500000000000013 	0.182999999999999996 	0.369999999999999996 	15 	
+0 	0.520000000000000018 	0.400000000000000022 	0.125 	0.686499999999999999 	0.294999999999999984 	0.171500000000000014 	0.184999999999999998 	9 	
+1 	0.424999999999999989 	0.325000000000000011 	0.115000000000000005 	0.368499999999999994 	0.162000000000000005 	0.0864999999999999936 	0.104499999999999996 	7 	
+2 	0.569999999999999951 	0.434999999999999998 	0.125 	0.896499999999999964 	0.383000000000000007 	0.183499999999999996 	0.275000000000000022 	9 	
+2 	0.614999999999999991 	0.494999999999999996 	0.200000000000000011 	1.21900000000000008 	0.563999999999999946 	0.227000000000000007 	0.388500000000000012 	10 	
+0 	0.685000000000000053 	0.530000000000000027 	0.170000000000000012 	1.51049999999999995 	0.738500000000000045 	0.35249999999999998 	0.372499999999999998 	10 	
+2 	0.589999999999999969 	0.474999999999999978 	0.140000000000000013 	0.97699999999999998 	0.462500000000000022 	0.202500000000000013 	0.275000000000000022 	10 	
+2 	0.424999999999999989 	0.325000000000000011 	0.119999999999999996 	0.3755 	0.141999999999999987 	0.106499999999999997 	0.104999999999999996 	9 	
+0 	0.729999999999999982 	0.574999999999999956 	0.184999999999999998 	1.87949999999999995 	0.93100000000000005 	0.380000000000000004 	0.482499999999999984 	12 	
+1 	0.555000000000000049 	0.455000000000000016 	0.170000000000000012 	0.843500000000000028 	0.308999999999999997 	0.190500000000000003 	0.299999999999999989 	15 	
+1 	0.594999999999999973 	0.46000000000000002 	0.149999999999999994 	0.833500000000000019 	0.377000000000000002 	0.192500000000000004 	0.234999999999999987 	8 	
+1 	0.57999999999999996 	0.445000000000000007 	0.125 	0.70950000000000002 	0.302999999999999992 	0.140500000000000014 	0.234999999999999987 	9 	
+2 	0.630000000000000004 	0.505000000000000004 	0.149999999999999994 	1.3165 	0.632499999999999951 	0.246499999999999997 	0.369999999999999996 	11 	
+2 	0.574999999999999956 	0.455000000000000016 	0.135000000000000009 	0.907000000000000028 	0.424499999999999988 	0.197000000000000008 	0.260000000000000009 	9 	
+1 	0.450000000000000011 	0.33500000000000002 	0.110000000000000001 	0.419499999999999984 	0.180999999999999994 	0.0850000000000000061 	0.134500000000000008 	7 	
+0 	0.5 	0.400000000000000022 	0.140000000000000013 	0.661499999999999977 	0.256500000000000006 	0.175499999999999989 	0.220000000000000001 	8 	
+0 	0.574999999999999956 	0.450000000000000011 	0.170000000000000012 	1.0475000000000001 	0.377500000000000002 	0.170500000000000013 	0.385000000000000009 	18 	
+0 	0.625 	0.489999999999999991 	0.174999999999999989 	1.2330000000000001 	0.556499999999999995 	0.246999999999999997 	0.364999999999999991 	11 	
+2 	0.635000000000000009 	0.510000000000000009 	0.154999999999999999 	0.985999999999999988 	0.405000000000000027 	0.225500000000000006 	0.309999999999999998 	10 	
+0 	0.550000000000000044 	0.41499999999999998 	0.135000000000000009 	0.775000000000000022 	0.301999999999999991 	0.178999999999999992 	0.260000000000000009 	23 	
+2 	0.57999999999999996 	0.450000000000000011 	0.174999999999999989 	1.06800000000000006 	0.424999999999999989 	0.203000000000000014 	0.320000000000000007 	13 	
+1 	0.484999999999999987 	0.369999999999999996 	0.115000000000000005 	0.457000000000000017 	0.188500000000000001 	0.0965000000000000024 	0.149999999999999994 	9 	
+0 	0.400000000000000022 	0.304999999999999993 	0.160000000000000003 	0.367999999999999994 	0.172999999999999987 	0.0704999999999999932 	0.104999999999999996 	7 	
+2 	0.660000000000000031 	0.515000000000000013 	0.195000000000000007 	1.56549999999999989 	0.734500000000000042 	0.35299999999999998 	0.38600000000000001 	9 	
+2 	0.469999999999999973 	0.359999999999999987 	0.104999999999999996 	0.544000000000000039 	0.270000000000000018 	0.139500000000000013 	0.129000000000000004 	7 	
+2 	0.75 	0.594999999999999973 	0.204999999999999988 	2.22049999999999992 	1.08299999999999996 	0.420999999999999985 	0.630000000000000004 	12 	
+2 	0.569999999999999951 	0.450000000000000011 	0.14499999999999999 	0.949999999999999956 	0.400500000000000023 	0.223500000000000004 	0.284499999999999975 	10 	
+0 	0.645000000000000018 	0.525000000000000022 	0.190000000000000002 	1.46350000000000002 	0.661499999999999977 	0.343500000000000028 	0.434999999999999998 	19 	
+2 	0.699999999999999956 	0.550000000000000044 	0.195000000000000007 	1.62450000000000006 	0.675000000000000044 	0.346999999999999975 	0.535000000000000031 	13 	
+1 	0.474999999999999978 	0.364999999999999991 	0.125 	0.546499999999999986 	0.229000000000000009 	0.118499999999999994 	0.171999999999999986 	9 	
+1 	0.599999999999999978 	0.445000000000000007 	0.135000000000000009 	0.920499999999999985 	0.445000000000000007 	0.203499999999999986 	0.253000000000000003 	9 	
+1 	0.244999999999999996 	0.190000000000000002 	0.0599999999999999978 	0.0859999999999999931 	0.0420000000000000026 	0.0140000000000000003 	0.0250000000000000014 	4 	
+1 	0.619999999999999996 	0.484999999999999987 	0.179999999999999993 	1.15399999999999991 	0.493499999999999994 	0.256000000000000005 	0.315000000000000002 	12 	
+2 	0.309999999999999998 	0.225000000000000006 	0.0749999999999999972 	0.129500000000000004 	0.0454999999999999988 	0.033500000000000002 	0.0439999999999999974 	9 	
+2 	0.584999999999999964 	0.474999999999999978 	0.119999999999999996 	0.944999999999999951 	0.409999999999999976 	0.211499999999999994 	0.280000000000000027 	14 	
+2 	0.660000000000000031 	0.535000000000000031 	0.190000000000000002 	1.59050000000000002 	0.64249999999999996 	0.296999999999999986 	0.51749999999999996 	9 	
+0 	0.574999999999999956 	0.445000000000000007 	0.135000000000000009 	0.883000000000000007 	0.381000000000000005 	0.203499999999999986 	0.260000000000000009 	11 	
+2 	0.525000000000000022 	0.419999999999999984 	0.154999999999999999 	0.841999999999999971 	0.427999999999999992 	0.141499999999999987 	0.204499999999999987 	9 	
+0 	0.685000000000000053 	0.535000000000000031 	0.174999999999999989 	1.58450000000000002 	0.717500000000000027 	0.377500000000000002 	0.421499999999999986 	9 	
+1 	0.275000000000000022 	0.204999999999999988 	0.0700000000000000067 	0.105499999999999997 	0.494999999999999996 	0.0189999999999999995 	0.0315000000000000002 	5 	
+1 	0.280000000000000027 	0.214999999999999997 	0.0800000000000000017 	0.132000000000000006 	0.0719999999999999946 	0.0219999999999999987 	0.0330000000000000016 	5 	
+1 	0.54500000000000004 	0.419999999999999984 	0.165000000000000008 	0.893499999999999961 	0.423499999999999988 	0.219500000000000001 	0.228000000000000008 	8 	
+2 	0.5 	0.390000000000000013 	0.14499999999999999 	0.651000000000000023 	0.27300000000000002 	0.132000000000000006 	0.220000000000000001 	11 	
+0 	0.675000000000000044 	0.569999999999999951 	0.225000000000000006 	1.58699999999999997 	0.73899999999999999 	0.299499999999999988 	0.434999999999999998 	10 	
+0 	0.640000000000000013 	0.5 	0.174999999999999989 	1.39399999999999991 	0.493499999999999994 	0.290999999999999981 	0.400000000000000022 	10 	
+2 	0.535000000000000031 	0.405000000000000027 	0.140000000000000013 	0.817999999999999949 	0.402000000000000024 	0.171500000000000014 	0.189000000000000001 	7 	
+1 	0.400000000000000022 	0.28999999999999998 	0.110000000000000001 	0.329000000000000015 	0.188 	0.0454999999999999988 	0.0825000000000000039 	6 	
+0 	0.505000000000000004 	0.390000000000000013 	0.160000000000000003 	0.644000000000000017 	0.247499999999999998 	0.202500000000000013 	0.163500000000000006 	9 	
+1 	0.440000000000000002 	0.304999999999999993 	0.115000000000000005 	0.379000000000000004 	0.162000000000000005 	0.0909999999999999976 	0.110000000000000001 	9 	
+2 	0.54500000000000004 	0.419999999999999984 	0.130000000000000004 	0.879000000000000004 	0.373999999999999999 	0.169500000000000012 	0.23000000000000001 	13 	
+1 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.871500000000000052 	0.450000000000000011 	0.162000000000000005 	0.225000000000000006 	10 	
+1 	0.515000000000000013 	0.400000000000000022 	0.119999999999999996 	0.65900000000000003 	0.270500000000000018 	0.178999999999999992 	0.170000000000000012 	13 	
+1 	0.419999999999999984 	0.325000000000000011 	0.100000000000000006 	0.367999999999999994 	0.16750000000000001 	0.0625 	0.113500000000000004 	11 	
+1 	0.474999999999999978 	0.380000000000000004 	0.119999999999999996 	0.441000000000000003 	0.178499999999999992 	0.0884999999999999953 	0.150499999999999995 	8 	
+2 	0.640000000000000013 	0.505000000000000004 	0.154999999999999999 	1.40250000000000008 	0.70499999999999996 	0.265500000000000014 	0.33500000000000002 	10 	
+1 	0.294999999999999984 	0.214999999999999997 	0.0700000000000000067 	0.120999999999999996 	0.0470000000000000001 	0.0154999999999999999 	0.0405000000000000013 	6 	
+1 	0.494999999999999996 	0.330000000000000016 	0.100000000000000006 	0.440000000000000002 	0.176999999999999991 	0.0950000000000000011 	0.149999999999999994 	7 	
+1 	0.304999999999999993 	0.244999999999999996 	0.0749999999999999972 	0.156 	0.0675000000000000044 	0.0379999999999999991 	0.0449999999999999983 	7 	
+1 	0.619999999999999996 	0.469999999999999973 	0.140000000000000013 	0.856500000000000039 	0.359499999999999986 	0.160000000000000003 	0.294999999999999984 	9 	
+2 	0.5 	0.400000000000000022 	0.130000000000000004 	0.771499999999999964 	0.369999999999999996 	0.160000000000000003 	0.210999999999999993 	8 	
+1 	0.535000000000000031 	0.400000000000000022 	0.130000000000000004 	0.657000000000000028 	0.283499999999999974 	0.162000000000000005 	0.174999999999999989 	7 	
+0 	0.574999999999999956 	0.484999999999999987 	0.165000000000000008 	1.04049999999999998 	0.418999999999999984 	0.264000000000000012 	0.299999999999999989 	14 	
+1 	0.489999999999999991 	0.380000000000000004 	0.119999999999999996 	0.529000000000000026 	0.216499999999999998 	0.139000000000000012 	0.154999999999999999 	11 	
+1 	0.569999999999999951 	0.429999999999999993 	0.14499999999999999 	0.832999999999999963 	0.353999999999999981 	0.143999999999999989 	0.281499999999999972 	10 	
+2 	0.255000000000000004 	0.179999999999999993 	0.0650000000000000022 	0.0790000000000000008 	0.0340000000000000024 	0.0140000000000000003 	0.0250000000000000014 	5 	
+2 	0.57999999999999996 	0.46000000000000002 	0.160000000000000003 	1.06299999999999994 	0.513000000000000012 	0.270500000000000018 	0.262500000000000011 	9 	
+0 	0.494999999999999996 	0.409999999999999976 	0.125 	0.755499999999999949 	0.33550000000000002 	0.129000000000000004 	0.213999999999999996 	9 	
+2 	0.455000000000000016 	0.349999999999999978 	0.110000000000000001 	0.458000000000000018 	0.200000000000000011 	0.111000000000000001 	0.130500000000000005 	8 	
+2 	0.709999999999999964 	0.564999999999999947 	0.200000000000000011 	1.60099999999999998 	0.705999999999999961 	0.321000000000000008 	0.450000000000000011 	11 	
+1 	0.609999999999999987 	0.465000000000000024 	0.149999999999999994 	0.96050000000000002 	0.449500000000000011 	0.172499999999999987 	0.285999999999999976 	9 	
+0 	0.520000000000000018 	0.424999999999999989 	0.165000000000000008 	0.988500000000000045 	0.396000000000000019 	0.225000000000000006 	0.320000000000000007 	16 	
+2 	0.560000000000000053 	0.455000000000000016 	0.154999999999999999 	0.797000000000000042 	0.340000000000000024 	0.190000000000000002 	0.242499999999999993 	11 	
+2 	0.520000000000000018 	0.400000000000000022 	0.119999999999999996 	0.822999999999999954 	0.297999999999999987 	0.180499999999999994 	0.265000000000000013 	15 	
+1 	0.369999999999999996 	0.28999999999999998 	0.0899999999999999967 	0.244499999999999995 	0.0889999999999999958 	0.0655000000000000027 	0.0749999999999999972 	7 	
+0 	0.505000000000000004 	0.390000000000000013 	0.130000000000000004 	0.674000000000000044 	0.316500000000000004 	0.140999999999999986 	0.178499999999999992 	9 	
+2 	0.645000000000000018 	0.489999999999999991 	0.160000000000000003 	1.25099999999999989 	0.535499999999999976 	0.33450000000000002 	0.316500000000000004 	9 	
+0 	0.515000000000000013 	0.434999999999999998 	0.170000000000000012 	0.631000000000000005 	0.276500000000000024 	0.111000000000000001 	0.215999999999999998 	12 	
+0 	0.525000000000000022 	0.419999999999999984 	0.160000000000000003 	0.756000000000000005 	0.274500000000000022 	0.172999999999999987 	0.275000000000000022 	9 	
+0 	0.599999999999999978 	0.450000000000000011 	0.140000000000000013 	0.868999999999999995 	0.342500000000000027 	0.195000000000000007 	0.290999999999999981 	11 	
+2 	0.560000000000000053 	0.434999999999999998 	0.179999999999999993 	0.889000000000000012 	0.359999999999999987 	0.203999999999999987 	0.25 	11 	
+0 	0.550000000000000044 	0.440000000000000002 	0.154999999999999999 	0.945999999999999952 	0.313 	0.182499999999999996 	0.33500000000000002 	16 	
+0 	0.689999999999999947 	0.540000000000000036 	0.154999999999999999 	1.45399999999999996 	0.623999999999999999 	0.310499999999999998 	0.390000000000000013 	9 	
+2 	0.54500000000000004 	0.419999999999999984 	0.140000000000000013 	0.750499999999999945 	0.247499999999999998 	0.130000000000000004 	0.255000000000000004 	22 	
+2 	0.515000000000000013 	0.395000000000000018 	0.119999999999999996 	0.646000000000000019 	0.284999999999999976 	0.13650000000000001 	0.171999999999999986 	9 	
+2 	0.594999999999999973 	0.474999999999999978 	0.140000000000000013 	1.03049999999999997 	0.492499999999999993 	0.216999999999999998 	0.278000000000000025 	10 	
+2 	0.689999999999999947 	0.540000000000000036 	0.184999999999999998 	1.70999999999999996 	0.772499999999999964 	0.385500000000000009 	0.432499999999999996 	8 	
+0 	0.694999999999999951 	0.550000000000000044 	0.184999999999999998 	1.67900000000000005 	0.805000000000000049 	0.401500000000000024 	0.396500000000000019 	10 	
+0 	0.589999999999999969 	0.455000000000000016 	0.165000000000000008 	1.16100000000000003 	0.380000000000000004 	0.245499999999999996 	0.280000000000000027 	12 	
+0 	0.574999999999999956 	0.429999999999999993 	0.154999999999999999 	0.795499999999999985 	0.348499999999999976 	0.192500000000000004 	0.220000000000000001 	9 	
+2 	0.325000000000000011 	0.23000000000000001 	0.0899999999999999967 	0.146999999999999992 	0.0599999999999999978 	0.0340000000000000024 	0.0449999999999999983 	4 	
+0 	0.719999999999999973 	0.574999999999999956 	0.214999999999999997 	2.22599999999999998 	0.895499999999999963 	0.405000000000000027 	0.619999999999999996 	13 	
+1 	0.325000000000000011 	0.244999999999999996 	0.0700000000000000067 	0.161000000000000004 	0.0754999999999999977 	0.0254999999999999984 	0.0449999999999999983 	6 	
+1 	0.195000000000000007 	0.149999999999999994 	0.0449999999999999983 	0.0374999999999999986 	0.0179999999999999986 	0.00600000000000000012 	0.0109999999999999994 	3 	
+1 	0.589999999999999969 	0.450000000000000011 	0.160000000000000003 	0.893000000000000016 	0.274500000000000022 	0.2185 	0.344999999999999973 	14 	
+0 	0.685000000000000053 	0.54500000000000004 	0.179999999999999993 	1.76800000000000002 	0.749500000000000055 	0.392000000000000015 	0.484999999999999987 	16 	
+1 	0.385000000000000009 	0.299999999999999989 	0.0899999999999999967 	0.307999999999999996 	0.152499999999999997 	0.0560000000000000012 	0.0835000000000000048 	8 	
+2 	0.469999999999999973 	0.375 	0.130000000000000004 	0.579500000000000015 	0.214499999999999996 	0.164000000000000007 	0.195000000000000007 	13 	
+0 	0.57999999999999996 	0.455000000000000016 	0.119999999999999996 	0.939999999999999947 	0.399000000000000021 	0.257000000000000006 	0.265000000000000013 	11 	
+2 	0.589999999999999969 	0.474999999999999978 	0.165000000000000008 	1.07699999999999996 	0.454500000000000015 	0.243999999999999995 	0.309499999999999997 	9 	
+1 	0.484999999999999987 	0.354999999999999982 	0.130000000000000004 	0.580999999999999961 	0.244999999999999996 	0.132000000000000006 	0.16800000000000001 	12 	
+2 	0.530000000000000027 	0.405000000000000027 	0.149999999999999994 	0.831500000000000017 	0.35199999999999998 	0.187 	0.252500000000000002 	10 	
+2 	0.650000000000000022 	0.515000000000000013 	0.125 	1.1805000000000001 	0.523499999999999965 	0.282999999999999974 	0.327500000000000013 	9 	
+1 	0.359999999999999987 	0.265000000000000013 	0.0950000000000000011 	0.231500000000000011 	0.104999999999999996 	0.0459999999999999992 	0.0749999999999999972 	7 	
+0 	0.67000000000000004 	0.530000000000000027 	0.204999999999999988 	1.40149999999999997 	0.643000000000000016 	0.246499999999999997 	0.415999999999999981 	12 	
+2 	0.594999999999999973 	0.479999999999999982 	0.140000000000000013 	0.912499999999999978 	0.409499999999999975 	0.182499999999999996 	0.288999999999999979 	9 	
+2 	0.450000000000000011 	0.33500000000000002 	0.125 	0.447500000000000009 	0.216499999999999998 	0.126000000000000001 	0.110000000000000001 	6 	
+2 	0.594999999999999973 	0.455000000000000016 	0.14499999999999999 	0.941999999999999948 	0.429999999999999993 	0.181999999999999995 	0.277000000000000024 	11 	
+2 	0.625 	0.505000000000000004 	0.214999999999999997 	1.44550000000000001 	0.495999999999999996 	0.286999999999999977 	0.434999999999999998 	22 	
+1 	0.465000000000000024 	0.369999999999999996 	0.119999999999999996 	0.436499999999999999 	0.188 	0.081500000000000003 	0.146999999999999992 	9 	
+2 	0.364999999999999991 	0.294999999999999984 	0.0800000000000000017 	0.255500000000000005 	0.0970000000000000029 	0.0429999999999999966 	0.100000000000000006 	7 	
+0 	0.494999999999999996 	0.385000000000000009 	0.130000000000000004 	0.690500000000000003 	0.3125 	0.178999999999999992 	0.174999999999999989 	10 	
+2 	0.555000000000000049 	0.429999999999999993 	0.165000000000000008 	0.757499999999999951 	0.273500000000000021 	0.163500000000000006 	0.275000000000000022 	13 	
+2 	0.589999999999999969 	0.5 	0.165000000000000008 	1.10450000000000004 	0.456500000000000017 	0.242499999999999993 	0.340000000000000024 	15 	
+2 	0.67000000000000004 	0.5 	0.190000000000000002 	1.51899999999999991 	0.615999999999999992 	0.388000000000000012 	0.41499999999999998 	10 	
+0 	0.594999999999999973 	0.455000000000000016 	0.160000000000000003 	1.04000000000000004 	0.452000000000000013 	0.265500000000000014 	0.287999999999999978 	9 	
+2 	0.530000000000000027 	0.434999999999999998 	0.154999999999999999 	0.698999999999999955 	0.287999999999999978 	0.159500000000000003 	0.204999999999999988 	10 	
+1 	0.520000000000000018 	0.41499999999999998 	0.160000000000000003 	0.594999999999999973 	0.210499999999999993 	0.141999999999999987 	0.260000000000000009 	15 	
+0 	0.409999999999999976 	0.320000000000000007 	0.115000000000000005 	0.387000000000000011 	0.165000000000000008 	0.100500000000000006 	0.0985000000000000042 	11 	
+2 	0.609999999999999987 	0.46000000000000002 	0.160000000000000003 	1 	0.493999999999999995 	0.197000000000000008 	0.275000000000000022 	10 	
+1 	0.364999999999999991 	0.260000000000000009 	0.115000000000000005 	0.217999999999999999 	0.0934999999999999998 	0.0444999999999999979 	0.0700000000000000067 	9 	
+1 	0.520000000000000018 	0.400000000000000022 	0.110000000000000001 	0.596999999999999975 	0.293499999999999983 	0.115500000000000005 	0.160000000000000003 	8 	
+2 	0.550000000000000044 	0.434999999999999998 	0.140000000000000013 	0.762499999999999956 	0.327000000000000013 	0.168500000000000011 	0.259000000000000008 	10 	
+2 	0.57999999999999996 	0.450000000000000011 	0.119999999999999996 	0.86850000000000005 	0.417999999999999983 	0.147499999999999992 	0.260500000000000009 	8 	
+2 	0.479999999999999982 	0.385000000000000009 	0.14499999999999999 	0.640000000000000013 	0.292499999999999982 	0.140500000000000014 	0.157500000000000001 	6 	
+1 	0.520000000000000018 	0.395000000000000018 	0.115000000000000005 	0.644499999999999962 	0.315500000000000003 	0.1245 	0.185999999999999999 	11 	
+0 	0.505000000000000004 	0.380000000000000004 	0.14499999999999999 	0.651000000000000023 	0.293499999999999983 	0.190000000000000002 	0.170000000000000012 	12 	
+0 	0.619999999999999996 	0.469999999999999973 	0.200000000000000011 	1.22550000000000003 	0.381000000000000005 	0.270000000000000018 	0.434999999999999998 	23 	
+2 	0.569999999999999951 	0.46000000000000002 	0.140000000000000013 	0.953500000000000014 	0.446500000000000008 	0.206499999999999989 	0.244999999999999996 	12 	
+2 	0.530000000000000027 	0.440000000000000002 	0.204999999999999988 	0.834999999999999964 	0.320000000000000007 	0.217499999999999999 	0.244999999999999996 	14 	
+1 	0.505000000000000004 	0.380000000000000004 	0.119999999999999996 	0.593999999999999972 	0.259500000000000008 	0.143499999999999989 	0.179999999999999993 	7 	
+1 	0.234999999999999987 	0.174999999999999989 	0.0550000000000000003 	0.067000000000000004 	0.0269999999999999997 	0.0125000000000000007 	0.0179999999999999986 	6 	
+0 	0.489999999999999991 	0.375 	0.149999999999999994 	0.575500000000000012 	0.220000000000000001 	0.143999999999999989 	0.190000000000000002 	9 	
+2 	0.655000000000000027 	0.550000000000000044 	0.179999999999999993 	1.27400000000000002 	0.585999999999999965 	0.281000000000000028 	0.364999999999999991 	10 	
+2 	0.41499999999999998 	0.309999999999999998 	0.0899999999999999967 	0.324500000000000011 	0.130500000000000005 	0.0734999999999999959 	0.115000000000000005 	8 	
+1 	0.33500000000000002 	0.244999999999999996 	0.0899999999999999967 	0.166500000000000009 	0.0594999999999999973 	0.0400000000000000008 	0.0599999999999999978 	6 	
+2 	0.599999999999999978 	0.494999999999999996 	0.184999999999999998 	1.11450000000000005 	0.505499999999999949 	0.263500000000000012 	0.366999999999999993 	11 	
+0 	0.535000000000000031 	0.46000000000000002 	0.14499999999999999 	0.787499999999999978 	0.339500000000000024 	0.200500000000000012 	0.200000000000000011 	8 	
+0 	0.599999999999999978 	0.469999999999999973 	0.149999999999999994 	0.922000000000000042 	0.362999999999999989 	0.194000000000000006 	0.304999999999999993 	10 	
+0 	0.57999999999999996 	0.429999999999999993 	0.170000000000000012 	1.47999999999999998 	0.65349999999999997 	0.32400000000000001 	0.41549999999999998 	10 	
+1 	0.625 	0.429999999999999993 	0.174999999999999989 	1.41100000000000003 	0.571999999999999953 	0.296999999999999986 	0.395000000000000018 	12 	
+1 	0.255000000000000004 	0.184999999999999998 	0.0599999999999999978 	0.0924999999999999989 	0.0389999999999999999 	0.0210000000000000013 	0.0250000000000000014 	6 	
+0 	0.619999999999999996 	0.510000000000000009 	0.204999999999999988 	1.34749999999999992 	0.47749999999999998 	0.256500000000000006 	0.479999999999999982 	14 	
+2 	0.349999999999999978 	0.265000000000000013 	0.0899999999999999967 	0.225500000000000006 	0.0995000000000000051 	0.0485000000000000014 	0.0700000000000000067 	7 	
+2 	0.560000000000000053 	0.419999999999999984 	0.195000000000000007 	0.808499999999999996 	0.302499999999999991 	0.179499999999999993 	0.284999999999999976 	14 	
+2 	0.630000000000000004 	0.469999999999999973 	0.149999999999999994 	1.13549999999999995 	0.539000000000000035 	0.232500000000000012 	0.311499999999999999 	12 	
+2 	0.589999999999999969 	0.465000000000000024 	0.165000000000000008 	1.11499999999999999 	0.516499999999999959 	0.27300000000000002 	0.275000000000000022 	10 	
+1 	0.555000000000000049 	0.395000000000000018 	0.130000000000000004 	0.558499999999999996 	0.222000000000000003 	0.1245 	0.170000000000000012 	9 	
+1 	0.469999999999999973 	0.375 	0.125 	0.522499999999999964 	0.226500000000000007 	0.103999999999999995 	0.162000000000000005 	8 	
+2 	0.489999999999999991 	0.385000000000000009 	0.125 	0.608999999999999986 	0.306499999999999995 	0.096000000000000002 	0.177499999999999991 	8 	
+1 	0.550000000000000044 	0.424999999999999989 	0.135000000000000009 	0.656000000000000028 	0.257000000000000006 	0.170000000000000012 	0.203000000000000014 	10 	
+2 	0.635000000000000009 	0.489999999999999991 	0.174999999999999989 	1.375 	0.622999999999999998 	0.270500000000000018 	0.395000000000000018 	11 	
+1 	0.375 	0.260000000000000009 	0.0800000000000000017 	0.20749999999999999 	0.0899999999999999967 	0.0415000000000000022 	0.0700000000000000067 	6 	
+0 	0.619999999999999996 	0.479999999999999982 	0.160000000000000003 	1.11250000000000004 	0.563500000000000001 	0.244499999999999995 	0.281000000000000028 	8 	
+1 	0.560000000000000053 	0.440000000000000002 	0.130000000000000004 	0.723500000000000032 	0.348999999999999977 	0.148999999999999994 	0.200000000000000011 	8 	
+1 	0.440000000000000002 	0.340000000000000024 	0.119999999999999996 	0.438 	0.211499999999999994 	0.0830000000000000043 	0.119999999999999996 	9 	
+0 	0.564999999999999947 	0.440000000000000002 	0.160000000000000003 	0.915000000000000036 	0.353999999999999981 	0.193500000000000005 	0.320000000000000007 	12 	
+0 	0.359999999999999987 	0.270000000000000018 	0.0899999999999999967 	0.188500000000000001 	0.0845000000000000057 	0.0384999999999999995 	0.0550000000000000003 	5 	
+1 	0.369999999999999996 	0.280000000000000027 	0.0850000000000000061 	0.216999999999999998 	0.1095 	0.0350000000000000033 	0.0619999999999999996 	6 	
+1 	0.494999999999999996 	0.380000000000000004 	0.14499999999999999 	0.5 	0.204999999999999988 	0.147999999999999993 	0.150499999999999995 	8 	
+1 	0.5 	0.395000000000000018 	0.119999999999999996 	0.537000000000000033 	0.216499999999999998 	0.108499999999999999 	0.178499999999999992 	9 	
+0 	0.680000000000000049 	0.569999999999999951 	0.204999999999999988 	1.84200000000000008 	0.625 	0.407999999999999974 	0.650000000000000022 	20 	
+2 	0.660000000000000031 	0.510000000000000009 	0.165000000000000008 	1.63749999999999996 	0.768499999999999961 	0.354499999999999982 	0.392500000000000016 	14 	
+1 	0.489999999999999991 	0.369999999999999996 	0.110000000000000001 	0.538000000000000034 	0.271000000000000019 	0.103499999999999995 	0.139000000000000012 	8 	
+0 	0.395000000000000018 	0.294999999999999984 	0.0950000000000000011 	0.224500000000000005 	0.0779999999999999999 	0.0539999999999999994 	0.0800000000000000017 	10 	
+1 	0.260000000000000009 	0.200000000000000011 	0.0700000000000000067 	0.0919999999999999984 	0.0369999999999999982 	0.0200000000000000004 	0.0299999999999999989 	6 	
+0 	0.530000000000000027 	0.429999999999999993 	0.149999999999999994 	0.740999999999999992 	0.325000000000000011 	0.185499999999999998 	0.196000000000000008 	9 	
+0 	0.719999999999999973 	0.574999999999999956 	0.214999999999999997 	2.10000000000000009 	0.856500000000000039 	0.482499999999999984 	0.60199999999999998 	12 	
+2 	0.569999999999999951 	0.440000000000000002 	0.174999999999999989 	0.941500000000000004 	0.380500000000000005 	0.228500000000000009 	0.282999999999999974 	9 	
+2 	0.589999999999999969 	0.469999999999999973 	0.149999999999999994 	0.860999999999999988 	0.412999999999999978 	0.164000000000000007 	0.248999999999999999 	8 	
+1 	0.635000000000000009 	0.5 	0.179999999999999993 	1.31899999999999995 	0.548499999999999988 	0.291999999999999982 	0.489999999999999991 	16 	
+0 	0.67000000000000004 	0.525000000000000022 	0.190000000000000002 	1.52699999999999991 	0.575500000000000012 	0.35299999999999998 	0.440000000000000002 	12 	
+1 	0.409999999999999976 	0.299999999999999989 	0.0899999999999999967 	0.303999999999999992 	0.129000000000000004 	0.0709999999999999937 	0.0955000000000000016 	8 	
+1 	0.450000000000000011 	0.330000000000000016 	0.110000000000000001 	0.368499999999999994 	0.160000000000000003 	0.0884999999999999953 	0.101999999999999993 	6 	
+0 	0.645000000000000018 	0.489999999999999991 	0.160000000000000003 	1.16650000000000009 	0.493499999999999994 	0.315500000000000003 	0.298999999999999988 	9 	
+0 	0.70499999999999996 	0.550000000000000044 	0.200000000000000011 	1.70950000000000002 	0.633000000000000007 	0.411499999999999977 	0.489999999999999991 	13 	
+0 	0.510000000000000009 	0.400000000000000022 	0.130000000000000004 	0.57350000000000001 	0.219 	0.13650000000000001 	0.195000000000000007 	13 	
+2 	0.599999999999999978 	0.469999999999999973 	0.154999999999999999 	1.03600000000000003 	0.4375 	0.196000000000000008 	0.325000000000000011 	20 	
+1 	0.474999999999999978 	0.364999999999999991 	0.100000000000000006 	0.131500000000000006 	0.202500000000000013 	0.0874999999999999944 	0.122999999999999998 	7 	
+0 	0.57999999999999996 	0.474999999999999978 	0.154999999999999999 	0.973999999999999977 	0.430499999999999994 	0.23000000000000001 	0.284999999999999976 	10 	
+1 	0.474999999999999978 	0.354999999999999982 	0.100000000000000006 	0.503499999999999948 	0.253500000000000003 	0.0909999999999999976 	0.140000000000000013 	8 	
+2 	0.315000000000000002 	0.25 	0.0899999999999999967 	0.203000000000000014 	0.0614999999999999991 	0.0369999999999999982 	0.0795000000000000012 	11 	
+2 	0.54500000000000004 	0.440000000000000002 	0.140000000000000013 	0.839500000000000024 	0.355999999999999983 	0.190500000000000003 	0.23849999999999999 	11 	
+2 	0.179999999999999993 	0.125 	0.0500000000000000028 	0.0229999999999999996 	0.00850000000000000061 	0.00549999999999999968 	0.0100000000000000002 	3 	
+2 	0.680000000000000049 	0.520000000000000018 	0.195000000000000007 	1.45350000000000001 	0.591999999999999971 	0.391000000000000014 	0.412499999999999978 	10 	
+2 	0.594999999999999973 	0.465000000000000024 	0.174999999999999989 	1.11499999999999999 	0.401500000000000024 	0.254000000000000004 	0.390000000000000013 	13 	
+2 	0.479999999999999982 	0.369999999999999996 	0.100000000000000006 	0.513499999999999956 	0.242999999999999994 	0.101500000000000007 	0.135000000000000009 	8 	
+2 	0.574999999999999956 	0.465000000000000024 	0.149999999999999994 	1.08000000000000007 	0.594999999999999973 	0.206499999999999989 	0.237999999999999989 	9 	
+0 	0.625 	0.445000000000000007 	0.160000000000000003 	1.09000000000000008 	0.46000000000000002 	0.296499999999999986 	0.303999999999999992 	11 	
+2 	0.650000000000000022 	0.525000000000000022 	0.174999999999999989 	1.53649999999999998 	0.686499999999999999 	0.358499999999999985 	0.405000000000000027 	11 	
+2 	0.599999999999999978 	0.484999999999999987 	0.174999999999999989 	1.26750000000000007 	0.4995 	0.281499999999999972 	0.380000000000000004 	13 	
+1 	0.550000000000000044 	0.424999999999999989 	0.135000000000000009 	0.730500000000000038 	0.332500000000000018 	0.154499999999999998 	0.214999999999999997 	9 	
+2 	0.599999999999999978 	0.474999999999999978 	0.174999999999999989 	1.34450000000000003 	0.549000000000000044 	0.287499999999999978 	0.359999999999999987 	11 	
+0 	0.645000000000000018 	0.515000000000000013 	0.149999999999999994 	1.21199999999999997 	0.515000000000000013 	0.205499999999999988 	0.385000000000000009 	10 	
+1 	0.5 	0.390000000000000013 	0.119999999999999996 	0.595500000000000029 	0.245499999999999996 	0.146999999999999992 	0.172999999999999987 	8 	
+2 	0.694999999999999951 	0.569999999999999951 	0.23000000000000001 	1.88500000000000001 	0.866500000000000048 	0.434999999999999998 	0.5 	19 	
+2 	0.445000000000000007 	0.349999999999999978 	0.140000000000000013 	0.590500000000000025 	0.202500000000000013 	0.158000000000000002 	0.190000000000000002 	14 	
+2 	0.569999999999999951 	0.455000000000000016 	0.174999999999999989 	1.02000000000000002 	0.480499999999999983 	0.214499999999999996 	0.28999999999999998 	9 	
+1 	0.209999999999999992 	0.149999999999999994 	0.0449999999999999983 	0.0400000000000000008 	0.0134999999999999998 	0.00800000000000000017 	0.0105000000000000007 	4 	
+1 	0.564999999999999947 	0.434999999999999998 	0.154999999999999999 	0.782000000000000028 	0.271500000000000019 	0.16800000000000001 	0.284999999999999976 	14 	
+2 	0.535000000000000031 	0.429999999999999993 	0.154999999999999999 	0.784499999999999975 	0.328500000000000014 	0.169000000000000011 	0.244999999999999996 	10 	
+1 	0.405000000000000027 	0.304999999999999993 	0.100000000000000006 	0.268000000000000016 	0.114500000000000005 	0.0529999999999999985 	0.0850000000000000061 	7 	
+0 	0.589999999999999969 	0.455000000000000016 	0.154999999999999999 	1.06600000000000006 	0.382000000000000006 	0.227500000000000008 	0.41499999999999998 	20 	
+2 	0.520000000000000018 	0.380000000000000004 	0.135000000000000009 	0.582500000000000018 	0.2505 	0.1565 	0.174999999999999989 	8 	
+1 	0.555000000000000049 	0.424999999999999989 	0.130000000000000004 	0.64800000000000002 	0.283499999999999974 	0.133000000000000007 	0.210499999999999993 	8 	
+0 	0.645000000000000018 	0.5 	0.170000000000000012 	1.18450000000000011 	0.480499999999999983 	0.274000000000000021 	0.354999999999999982 	13 	
+1 	0.450000000000000011 	0.349999999999999978 	0.14499999999999999 	0.525000000000000022 	0.208499999999999991 	0.100000000000000006 	0.165500000000000008 	15 	
+1 	0.455000000000000016 	0.375 	0.119999999999999996 	0.496999999999999997 	0.235499999999999987 	0.105499999999999997 	0.129500000000000004 	6 	
+1 	0.299999999999999989 	0.220000000000000001 	0.0800000000000000017 	0.120999999999999996 	0.0475000000000000006 	0.0420000000000000026 	0.0350000000000000033 	5 	
+2 	0.650000000000000022 	0.484999999999999987 	0.160000000000000003 	1.73950000000000005 	0.571500000000000008 	0.278500000000000025 	0.307499999999999996 	10 	
+2 	0.609999999999999987 	0.489999999999999991 	0.160000000000000003 	1.1120000000000001 	0.465000000000000024 	0.228000000000000008 	0.341000000000000025 	10 	
+0 	0.57999999999999996 	0.450000000000000011 	0.195000000000000007 	0.826500000000000012 	0.403500000000000025 	0.172999999999999987 	0.225000000000000006 	9 	
+2 	0.445000000000000007 	0.344999999999999973 	0.0899999999999999967 	0.379500000000000004 	0.142999999999999988 	0.0739999999999999963 	0.125 	10 	
+1 	0.520000000000000018 	0.385000000000000009 	0.115000000000000005 	0.580999999999999961 	0.255500000000000005 	0.156 	0.142999999999999988 	10 	
+0 	0.54500000000000004 	0.424999999999999989 	0.125 	0.768000000000000016 	0.293999999999999984 	0.149499999999999994 	0.260000000000000009 	16 	
+0 	0.574999999999999956 	0.479999999999999982 	0.149999999999999994 	0.89700000000000002 	0.423499999999999988 	0.190500000000000003 	0.247999999999999998 	8 	
+2 	0.54500000000000004 	0.409999999999999976 	0.119999999999999996 	0.793000000000000038 	0.433999999999999997 	0.140500000000000014 	0.190000000000000002 	9 	
+0 	0.57999999999999996 	0.474999999999999978 	0.135000000000000009 	0.925000000000000044 	0.391000000000000014 	0.165000000000000008 	0.275000000000000022 	14 	
+0 	0.474999999999999978 	0.390000000000000013 	0.119999999999999996 	0.530499999999999972 	0.213499999999999995 	0.115500000000000005 	0.170000000000000012 	10 	
+1 	0.46000000000000002 	0.344999999999999973 	0.119999999999999996 	0.41549999999999998 	0.198000000000000009 	0.0884999999999999953 	0.106999999999999998 	7 	
+2 	0.349999999999999978 	0.255000000000000004 	0.0650000000000000022 	0.178999999999999992 	0.0704999999999999932 	0.0384999999999999995 	0.0599999999999999978 	10 	
+2 	0.630000000000000004 	0.515000000000000013 	0.160000000000000003 	1.01600000000000001 	0.421499999999999986 	0.243999999999999995 	0.354999999999999982 	19 	
+2 	0.739999999999999991 	0.594999999999999973 	0.190000000000000002 	2.32350000000000012 	1.14949999999999997 	0.511499999999999955 	0.505000000000000004 	11 	
+2 	0.469999999999999973 	0.359999999999999987 	0.135000000000000009 	0.501000000000000001 	0.166500000000000009 	0.115000000000000005 	0.165000000000000008 	10 	
+1 	0.525000000000000022 	0.400000000000000022 	0.14499999999999999 	0.609500000000000042 	0.247999999999999998 	0.159000000000000002 	0.174999999999999989 	9 	
+1 	0.280000000000000027 	0.209999999999999992 	0.0550000000000000003 	0.105999999999999997 	0.0415000000000000022 	0.0264999999999999993 	0.0309999999999999998 	5 	
+1 	0.375 	0.284999999999999976 	0.0800000000000000017 	0.226000000000000006 	0.0975000000000000033 	0.0400000000000000008 	0.072499999999999995 	7 	
+2 	0.609999999999999987 	0.484999999999999987 	0.174999999999999989 	1.24449999999999994 	0.544000000000000039 	0.296999999999999986 	0.344999999999999973 	12 	
+2 	0.46000000000000002 	0.359999999999999987 	0.135000000000000009 	0.610500000000000043 	0.195500000000000007 	0.106999999999999998 	0.234999999999999987 	14 	
+0 	0.550000000000000044 	0.429999999999999993 	0.140000000000000013 	0.810499999999999998 	0.367999999999999994 	0.161000000000000004 	0.275000000000000022 	9 	
+1 	0.359999999999999987 	0.280000000000000027 	0.0899999999999999967 	0.225500000000000006 	0.0884999999999999953 	0.0400000000000000008 	0.0899999999999999967 	8 	
+2 	0.770000000000000018 	0.619999999999999996 	0.195000000000000007 	2.51549999999999985 	1.11549999999999994 	0.641499999999999959 	0.642000000000000015 	12 	
+0 	0.724999999999999978 	0.574999999999999956 	0.174999999999999989 	2.12400000000000011 	0.765000000000000013 	0.451500000000000012 	0.849999999999999978 	20 	
+2 	0.505000000000000004 	0.385000000000000009 	0.130000000000000004 	0.643499999999999961 	0.313500000000000001 	0.148999999999999994 	0.151499999999999996 	7 	
+1 	0.364999999999999991 	0.265000000000000013 	0.0850000000000000061 	0.212999999999999995 	0.0945000000000000007 	0.0490000000000000019 	0.0599999999999999978 	7 	
+2 	0.520000000000000018 	0.400000000000000022 	0.165000000000000008 	0.856500000000000039 	0.274500000000000022 	0.201000000000000012 	0.209999999999999992 	12 	
+0 	0.599999999999999978 	0.479999999999999982 	0.149999999999999994 	1.02899999999999991 	0.408499999999999974 	0.270500000000000018 	0.294999999999999984 	16 	
+1 	0.395000000000000018 	0.304999999999999993 	0.104999999999999996 	0.283999999999999975 	0.113500000000000004 	0.0594999999999999973 	0.0945000000000000007 	8 	
+1 	0.530000000000000027 	0.419999999999999984 	0.184999999999999998 	0.752000000000000002 	0.298999999999999988 	0.156 	0.204999999999999988 	20 	
+1 	0.429999999999999993 	0.344999999999999973 	0.115000000000000005 	0.429499999999999993 	0.211999999999999994 	0.107999999999999999 	0.109 	8 	
+2 	0.584999999999999964 	0.455000000000000016 	0.14499999999999999 	0.952999999999999958 	0.394500000000000017 	0.268500000000000016 	0.258000000000000007 	10 	
+1 	0.390000000000000013 	0.299999999999999989 	0.0899999999999999967 	0.252000000000000002 	0.106499999999999997 	0.0529999999999999985 	0.0800000000000000017 	7 	
+0 	0.699999999999999956 	0.54500000000000004 	0.130000000000000004 	1.55600000000000005 	0.672499999999999987 	0.373999999999999999 	0.195000000000000007 	12 	
+0 	0.57999999999999996 	0.445000000000000007 	0.14499999999999999 	0.888000000000000012 	0.409999999999999976 	0.181499999999999995 	0.242499999999999993 	8 	
+1 	0.275000000000000022 	0.200000000000000011 	0.0650000000000000022 	0.103499999999999995 	0.0475000000000000006 	0.0205000000000000009 	0.0299999999999999989 	7 	
+2 	0.574999999999999956 	0.434999999999999998 	0.140000000000000013 	0.845500000000000029 	0.401000000000000023 	0.191000000000000003 	0.222000000000000003 	9 	
+0 	0.57999999999999996 	0.434999999999999998 	0.149999999999999994 	0.833999999999999964 	0.427999999999999992 	0.151499999999999996 	0.23000000000000001 	8 	
+1 	0.209999999999999992 	0.149999999999999994 	0.0550000000000000003 	0.0464999999999999997 	0.0170000000000000012 	0.0120000000000000002 	0.0149999999999999994 	5 	
+0 	0.625 	0.525000000000000022 	0.195000000000000007 	1.35200000000000009 	0.450500000000000012 	0.244499999999999995 	0.530000000000000027 	13 	
+1 	0.465000000000000024 	0.354999999999999982 	0.0899999999999999967 	0.432499999999999996 	0.200500000000000012 	0.0739999999999999963 	0.127500000000000002 	9 	
+2 	0.41499999999999998 	0.299999999999999989 	0.100000000000000006 	0.33550000000000002 	0.154499999999999998 	0.0685000000000000053 	0.0950000000000000011 	7 	
+2 	0.445000000000000007 	0.340000000000000024 	0.119999999999999996 	0.447500000000000009 	0.193000000000000005 	0.103499999999999995 	0.130000000000000004 	9 	
+2 	0.614999999999999991 	0.455000000000000016 	0.149999999999999994 	0.933499999999999996 	0.382000000000000006 	0.246999999999999997 	0.26150000000000001 	10 	
+1 	0.525000000000000022 	0.405000000000000027 	0.14499999999999999 	0.696500000000000008 	0.304499999999999993 	0.153499999999999998 	0.209999999999999992 	8 	
+2 	0.54500000000000004 	0.450000000000000011 	0.149999999999999994 	0.97799999999999998 	0.336500000000000021 	0.190500000000000003 	0.299999999999999989 	11 	
+1 	0.270000000000000018 	0.195000000000000007 	0.0599999999999999978 	0.0729999999999999954 	0.028500000000000001 	0.0235000000000000001 	0.0299999999999999989 	5 	
+2 	0.619999999999999996 	0.465000000000000024 	0.184999999999999998 	1.27400000000000002 	0.578999999999999959 	0.306499999999999995 	0.320000000000000007 	12 	
+0 	0.474999999999999978 	0.364999999999999991 	0.130000000000000004 	0.480499999999999983 	0.190500000000000003 	0.114000000000000004 	0.147499999999999992 	12 	
+2 	0.635000000000000009 	0.5 	0.165000000000000008 	1.27299999999999991 	0.65349999999999997 	0.212999999999999995 	0.364999999999999991 	12 	
+1 	0.309999999999999998 	0.239999999999999991 	0.0899999999999999967 	0.14549999999999999 	0.0604999999999999982 	0.0315000000000000002 	0.0449999999999999983 	7 	
+1 	0.474999999999999978 	0.340000000000000024 	0.104999999999999996 	0.453500000000000014 	0.203000000000000014 	0.0800000000000000017 	0.146499999999999991 	9 	
+2 	0.479999999999999982 	0.369999999999999996 	0.135000000000000009 	0.63149999999999995 	0.344499999999999973 	0.101500000000000007 	0.161000000000000004 	7 	
+2 	0.280000000000000027 	0.200000000000000011 	0.0800000000000000017 	0.091499999999999998 	0.0330000000000000016 	0.0214999999999999983 	0.0299999999999999989 	5 	
+2 	0.645000000000000018 	0.5 	0.174999999999999989 	1.33749999999999991 	0.554000000000000048 	0.307999999999999996 	0.41499999999999998 	10 	
+0 	0.70499999999999996 	0.569999999999999951 	0.184999999999999998 	1.7609999999999999 	0.746999999999999997 	0.372499999999999998 	0.487999999999999989 	10 	
+1 	0.315000000000000002 	0.23000000000000001 	0.0700000000000000067 	0.143999999999999989 	0.0529999999999999985 	0.0304999999999999993 	0.0400000000000000008 	8 	
+1 	0.510000000000000009 	0.385000000000000009 	0.14499999999999999 	0.766499999999999959 	0.398500000000000021 	0.140000000000000013 	0.180499999999999994 	8 	
+2 	0.57999999999999996 	0.429999999999999993 	0.130000000000000004 	0.798000000000000043 	0.364999999999999991 	0.172999999999999987 	0.228500000000000009 	10 	
+2 	0.650000000000000022 	0.54500000000000004 	0.160000000000000003 	1.24249999999999994 	0.486999999999999988 	0.295999999999999985 	0.479999999999999982 	15 	
+2 	0.70499999999999996 	0.560000000000000053 	0.220000000000000001 	1.98100000000000009 	0.817500000000000004 	0.308499999999999996 	0.760000000000000009 	14 	
+0 	0.440000000000000002 	0.344999999999999973 	0.104999999999999996 	0.428499999999999992 	0.165000000000000008 	0.0830000000000000043 	0.132000000000000006 	11 	
+1 	0.469999999999999973 	0.349999999999999978 	0.125 	0.431499999999999995 	0.190000000000000002 	0.116500000000000006 	0.117499999999999993 	6 	
+2 	0.564999999999999947 	0.424999999999999989 	0.100000000000000006 	0.714500000000000024 	0.305499999999999994 	0.166000000000000009 	0.179999999999999993 	12 	
+2 	0.619999999999999996 	0.489999999999999991 	0.190000000000000002 	1.21799999999999997 	0.545499999999999985 	0.296499999999999986 	0.354999999999999982 	13 	
+0 	0.619999999999999996 	0.479999999999999982 	0.23000000000000001 	1.09349999999999992 	0.403000000000000025 	0.244999999999999996 	0.354999999999999982 	14 	
+2 	0.574999999999999956 	0.450000000000000011 	0.165000000000000008 	0.921499999999999986 	0.327500000000000013 	0.225000000000000006 	0.256000000000000005 	12 	
+0 	0.46000000000000002 	0.354999999999999982 	0.130000000000000004 	0.517000000000000015 	0.220500000000000002 	0.114000000000000004 	0.165000000000000008 	9 	
+2 	0.525000000000000022 	0.400000000000000022 	0.154999999999999999 	0.706999999999999962 	0.281999999999999973 	0.160500000000000004 	0.225000000000000006 	9 	
+1 	0.369999999999999996 	0.280000000000000027 	0.0950000000000000011 	0.265500000000000014 	0.121999999999999997 	0.0519999999999999976 	0.0800000000000000017 	7 	
+1 	0.574999999999999956 	0.424999999999999989 	0.135000000000000009 	0.796499999999999986 	0.36399999999999999 	0.196000000000000008 	0.23899999999999999 	10 	
+2 	0.57999999999999996 	0.445000000000000007 	0.160000000000000003 	0.983999999999999986 	0.489999999999999991 	0.201000000000000012 	0.270000000000000018 	9 	
+2 	0.680000000000000049 	0.530000000000000027 	0.204999999999999988 	1.496 	0.582500000000000018 	0.337000000000000022 	0.465000000000000024 	14 	
+0 	0.640000000000000013 	0.515000000000000013 	0.165000000000000008 	1.31150000000000011 	0.494499999999999995 	0.255500000000000005 	0.409999999999999976 	10 	
+2 	0.284999999999999976 	0.209999999999999992 	0.0749999999999999972 	0.118499999999999994 	0.0550000000000000003 	0.028500000000000001 	0.0400000000000000008 	7 	
+2 	0.535000000000000031 	0.409999999999999976 	0.119999999999999996 	0.683499999999999996 	0.3125 	0.165500000000000008 	0.159000000000000002 	8 	
+2 	0.635000000000000009 	0.474999999999999978 	0.170000000000000012 	1.19350000000000001 	0.520499999999999963 	0.269500000000000017 	0.366499999999999992 	10 	
+1 	0.474999999999999978 	0.375 	0.115000000000000005 	0.520499999999999963 	0.233000000000000013 	0.118999999999999995 	0.14549999999999999 	7 	
+0 	0.57999999999999996 	0.445000000000000007 	0.149999999999999994 	0.857999999999999985 	0.400000000000000022 	0.156 	0.253000000000000003 	8 	
+2 	0.469999999999999973 	0.375 	0.130000000000000004 	0.52300000000000002 	0.213999999999999996 	0.132000000000000006 	0.14499999999999999 	8 	
+0 	0.530000000000000027 	0.41499999999999998 	0.160000000000000003 	0.783000000000000029 	0.293499999999999983 	0.158000000000000002 	0.244999999999999996 	15 	
+2 	0.564999999999999947 	0.41499999999999998 	0.125 	0.667000000000000037 	0.301999999999999991 	0.154499999999999998 	0.184999999999999998 	7 	
+1 	0.419999999999999984 	0.325000000000000011 	0.115000000000000005 	0.314000000000000001 	0.129500000000000004 	0.0635000000000000009 	0.100000000000000006 	8 	
+0 	0.57999999999999996 	0.455000000000000016 	0.160000000000000003 	0.921499999999999986 	0.312 	0.196000000000000008 	0.299999999999999989 	17 	
+1 	0.560000000000000053 	0.445000000000000007 	0.165000000000000008 	0.831999999999999962 	0.345499999999999974 	0.178999999999999992 	0.279000000000000026 	9 	
+2 	0.429999999999999993 	0.33500000000000002 	0.115000000000000005 	0.406000000000000028 	0.166000000000000009 	0.0934999999999999998 	0.135000000000000009 	8 	
+1 	0.550000000000000044 	0.429999999999999993 	0.14499999999999999 	0.78949999999999998 	0.3745 	0.171000000000000013 	0.223000000000000004 	11 	
+0 	0.630000000000000004 	0.489999999999999991 	0.225000000000000006 	1.33600000000000008 	0.680499999999999994 	0.259000000000000008 	0.324500000000000011 	10 	
+1 	0.560000000000000053 	0.424999999999999989 	0.135000000000000009 	0.820500000000000007 	0.371499999999999997 	0.184999999999999998 	0.235999999999999988 	9 	
+0 	0.344999999999999973 	0.25 	0.0899999999999999967 	0.203000000000000014 	0.0779999999999999999 	0.0589999999999999969 	0.0550000000000000003 	6 	
+0 	0.599999999999999978 	0.469999999999999973 	0.135000000000000009 	0.969999999999999973 	0.465500000000000025 	0.195500000000000007 	0.264000000000000012 	11 	
+2 	0.584999999999999964 	0.465000000000000024 	0.160000000000000003 	0.955500000000000016 	0.45950000000000002 	0.235999999999999988 	0.265000000000000013 	7 	
+0 	0.660000000000000031 	0.530000000000000027 	0.170000000000000012 	1.32600000000000007 	0.519000000000000017 	0.262500000000000011 	0.440000000000000002 	13 	
+2 	0.330000000000000016 	0.255000000000000004 	0.0950000000000000011 	0.1875 	0.0734999999999999959 	0.0449999999999999983 	0.0599999999999999978 	7 	
+0 	0.550000000000000044 	0.440000000000000002 	0.135000000000000009 	0.843500000000000028 	0.433999999999999997 	0.199500000000000011 	0.184999999999999998 	8 	
+0 	0.474999999999999978 	0.364999999999999991 	0.115000000000000005 	0.565999999999999948 	0.281000000000000028 	0.117000000000000007 	0.133500000000000008 	7 	
+2 	0.609999999999999987 	0.469999999999999973 	0.160000000000000003 	1.02200000000000002 	0.44900000000000001 	0.234499999999999986 	0.294499999999999984 	9 	
+2 	0.510000000000000009 	0.41499999999999998 	0.140000000000000013 	0.818500000000000005 	0.302499999999999991 	0.215499999999999997 	0.234999999999999987 	16 	
+1 	0.294999999999999984 	0.225000000000000006 	0.0899999999999999967 	0.110500000000000001 	0.0405000000000000013 	0.0245000000000000009 	0.0320000000000000007 	7 	
+1 	0.409999999999999976 	0.299999999999999989 	0.0899999999999999967 	0.280000000000000027 	0.140999999999999986 	0.0575000000000000025 	0.0749999999999999972 	8 	
+1 	0.550000000000000044 	0.434999999999999998 	0.165000000000000008 	0.804000000000000048 	0.340000000000000024 	0.194000000000000006 	0.243999999999999995 	8 	
+2 	0.630000000000000004 	0.505000000000000004 	0.170000000000000012 	1.09149999999999991 	0.461500000000000021 	0.266000000000000014 	0.299999999999999989 	9 	
+2 	0.604999999999999982 	0.445000000000000007 	0.140000000000000013 	0.981999999999999984 	0.429499999999999993 	0.208499999999999991 	0.294999999999999984 	12 	
+2 	0.525000000000000022 	0.409999999999999976 	0.130000000000000004 	0.989999999999999991 	0.38650000000000001 	0.242999999999999994 	0.294999999999999984 	15 	
+0 	0.625 	0.489999999999999991 	0.110000000000000001 	1.1359999999999999 	0.526499999999999968 	0.191500000000000004 	0.292499999999999982 	9 	
+2 	0.54500000000000004 	0.390000000000000013 	0.135000000000000009 	0.783499999999999974 	0.422499999999999987 	0.181499999999999995 	0.156 	7 	
+2 	0.599999999999999978 	0.479999999999999982 	0.165000000000000008 	0.916499999999999981 	0.413499999999999979 	0.196500000000000008 	0.27250000000000002 	9 	
+2 	0.525000000000000022 	0.405000000000000027 	0.135000000000000009 	0.757499999999999951 	0.330500000000000016 	0.215999999999999998 	0.195000000000000007 	10 	
+2 	0.625 	0.494999999999999996 	0.184999999999999998 	1.38349999999999995 	0.71050000000000002 	0.300499999999999989 	0.344999999999999973 	11 	
+1 	0.604999999999999982 	0.484999999999999987 	0.149999999999999994 	1.23799999999999999 	0.63149999999999995 	0.226000000000000006 	0.330000000000000016 	11 	
+1 	0.440000000000000002 	0.344999999999999973 	0.115000000000000005 	0.54500000000000004 	0.269000000000000017 	0.111000000000000001 	0.130500000000000005 	6 	
+1 	0.520000000000000018 	0.409999999999999976 	0.14499999999999999 	0.646000000000000019 	0.296499999999999986 	0.159500000000000003 	0.165000000000000008 	9 	
+0 	0.699999999999999956 	0.574999999999999956 	0.170000000000000012 	1.31000000000000005 	0.509499999999999953 	0.314000000000000001 	0.419999999999999984 	14 	
+1 	0.304999999999999993 	0.225000000000000006 	0.0899999999999999967 	0.146499999999999991 	0.0630000000000000004 	0.0340000000000000024 	0.0415000000000000022 	6 	
+0 	0.609999999999999987 	0.469999999999999973 	0.165000000000000008 	1.1785000000000001 	0.565999999999999948 	0.278500000000000025 	0.293999999999999984 	11 	
+0 	0.54500000000000004 	0.409999999999999976 	0.125 	0.693500000000000005 	0.297499999999999987 	0.145999999999999991 	0.209999999999999992 	11 	
+2 	0.709999999999999964 	0.564999999999999947 	0.204999999999999988 	2.19799999999999995 	1.01200000000000001 	0.522499999999999964 	0.547499999999999987 	11 	
+1 	0.455000000000000016 	0.354999999999999982 	0.125 	0.532499999999999973 	0.225000000000000006 	0.126000000000000001 	0.146499999999999991 	7 	
+2 	0.630000000000000004 	0.525000000000000022 	0.195000000000000007 	1.31349999999999989 	0.493499999999999994 	0.256500000000000006 	0.465000000000000024 	10 	
+1 	0.434999999999999998 	0.320000000000000007 	0.0800000000000000017 	0.332500000000000018 	0.148499999999999993 	0.0635000000000000009 	0.104999999999999996 	9 	
+2 	0.609999999999999987 	0.479999999999999982 	0.149999999999999994 	1.14949999999999997 	0.563999999999999946 	0.274000000000000021 	0.264000000000000012 	8 	
+0 	0.525000000000000022 	0.380000000000000004 	0.140000000000000013 	0.606500000000000039 	0.194000000000000006 	0.147499999999999992 	0.209999999999999992 	14 	
+0 	0.650000000000000022 	0.54500000000000004 	0.165000000000000008 	1.56600000000000006 	0.66449999999999998 	0.345499999999999974 	0.41499999999999998 	16 	
+0 	0.574999999999999956 	0.465000000000000024 	0.174999999999999989 	1.09899999999999998 	0.473499999999999976 	0.202000000000000013 	0.349999999999999978 	9 	
+1 	0.510000000000000009 	0.395000000000000018 	0.154999999999999999 	0.53949999999999998 	0.246499999999999997 	0.108499999999999999 	0.16700000000000001 	8 	
+2 	0.589999999999999969 	0.474999999999999978 	0.154999999999999999 	0.856999999999999984 	0.355999999999999983 	0.173999999999999988 	0.280000000000000027 	13 	
+1 	0.204999999999999988 	0.149999999999999994 	0.0650000000000000022 	0.0400000000000000008 	0.0200000000000000004 	0.0109999999999999994 	0.0129999999999999994 	4 	
+1 	0.369999999999999996 	0.28999999999999998 	0.100000000000000006 	0.25 	0.102499999999999994 	0.0505000000000000032 	0.0850000000000000061 	10 	
+0 	0.550000000000000044 	0.469999999999999973 	0.149999999999999994 	0.89700000000000002 	0.377000000000000002 	0.183999999999999997 	0.28999999999999998 	9 	
+1 	0.239999999999999991 	0.184999999999999998 	0.0700000000000000067 	0.0714999999999999941 	0.0259999999999999988 	0.0179999999999999986 	0.0250000000000000014 	6 	
+1 	0.555000000000000049 	0.429999999999999993 	0.140000000000000013 	0.766499999999999959 	0.341000000000000025 	0.165000000000000008 	0.23000000000000001 	9 	
+0 	0.474999999999999978 	0.359999999999999987 	0.125 	0.447000000000000008 	0.169500000000000012 	0.0810000000000000026 	0.140000000000000013 	9 	
+1 	0.450000000000000011 	0.340000000000000024 	0.0950000000000000011 	0.324500000000000011 	0.138500000000000012 	0.0640000000000000013 	0.104999999999999996 	8 	
+1 	0.440000000000000002 	0.344999999999999973 	0.119999999999999996 	0.486999999999999988 	0.196500000000000008 	0.107999999999999999 	0.160000000000000003 	14 	
+0 	0.574999999999999956 	0.445000000000000007 	0.140000000000000013 	0.940999999999999948 	0.384500000000000008 	0.252000000000000002 	0.284999999999999976 	9 	
+2 	0.474999999999999978 	0.359999999999999987 	0.119999999999999996 	0.577999999999999958 	0.282499999999999973 	0.119999999999999996 	0.170000000000000012 	8 	
+1 	0.525000000000000022 	0.400000000000000022 	0.140000000000000013 	0.654000000000000026 	0.304999999999999993 	0.160000000000000003 	0.169000000000000011 	7 	
+2 	0.550000000000000044 	0.434999999999999998 	0.14499999999999999 	0.842999999999999972 	0.328000000000000014 	0.191500000000000004 	0.255000000000000004 	15 	
+0 	0.635000000000000009 	0.489999999999999991 	0.170000000000000012 	1.26150000000000007 	0.538499999999999979 	0.266500000000000015 	0.380000000000000004 	9 	
+0 	0.550000000000000044 	0.409999999999999976 	0.14499999999999999 	0.828500000000000014 	0.309499999999999997 	0.190500000000000003 	0.25 	13 	
+2 	0.54500000000000004 	0.424999999999999989 	0.135000000000000009 	0.844500000000000028 	0.372999999999999998 	0.209999999999999992 	0.234999999999999987 	10 	
+1 	0.5 	0.400000000000000022 	0.119999999999999996 	0.615999999999999992 	0.26100000000000001 	0.142999999999999988 	0.193500000000000005 	8 	
+1 	0.450000000000000011 	0.330000000000000016 	0.104999999999999996 	0.371499999999999997 	0.186499999999999999 	0.0785000000000000003 	0.0975000000000000033 	7 	
+1 	0.409999999999999976 	0.320000000000000007 	0.0950000000000000011 	0.29049999999999998 	0.140999999999999986 	0.0630000000000000004 	0.0729999999999999954 	5 	
+2 	0.604999999999999982 	0.5 	0.174999999999999989 	1.09800000000000009 	0.476499999999999979 	0.232000000000000012 	0.375 	12 	
+1 	0.195000000000000007 	0.135000000000000009 	0.0400000000000000008 	0.0325000000000000011 	0.0134999999999999998 	0.0050000000000000001 	0.00949999999999999976 	4 	
+1 	0.275000000000000022 	0.174999999999999989 	0.0899999999999999967 	0.231500000000000011 	0.096000000000000002 	0.0570000000000000021 	0.0704999999999999932 	5 	
+1 	0.520000000000000018 	0.400000000000000022 	0.130000000000000004 	0.582500000000000018 	0.233000000000000013 	0.13650000000000001 	0.179999999999999993 	10 	
+2 	0.589999999999999969 	0.455000000000000016 	0.160000000000000003 	1.09000000000000008 	0.5 	0.221500000000000002 	0.291999999999999982 	9 	
+0 	0.604999999999999982 	0.494999999999999996 	0.170000000000000012 	1.09149999999999991 	0.436499999999999999 	0.271500000000000019 	0.33500000000000002 	13 	
+2 	0.660000000000000031 	0.494999999999999996 	0.195000000000000007 	1.62749999999999995 	0.593999999999999972 	0.359499999999999986 	0.484999999999999987 	10 	
+2 	0.665000000000000036 	0.525000000000000022 	0.174999999999999989 	1.44300000000000006 	0.663499999999999979 	0.384500000000000008 	0.35299999999999998 	11 	
+1 	0.440000000000000002 	0.344999999999999973 	0.100000000000000006 	0.365999999999999992 	0.121999999999999997 	0.0904999999999999971 	0.119999999999999996 	13 	
+2 	0.400000000000000022 	0.315000000000000002 	0.104999999999999996 	0.286999999999999977 	0.113500000000000004 	0.0369999999999999982 	0.113000000000000003 	10 	
+1 	0.540000000000000036 	0.429999999999999993 	0.140000000000000013 	0.819500000000000006 	0.393500000000000016 	0.172499999999999987 	0.22950000000000001 	9 	
+2 	0.635000000000000009 	0.489999999999999991 	0.160000000000000003 	1.10099999999999998 	0.53400000000000003 	0.186499999999999999 	0.345499999999999974 	10 	
+1 	0.510000000000000009 	0.405000000000000027 	0.125 	0.679499999999999993 	0.346499999999999975 	0.139500000000000013 	0.181999999999999995 	8 	
+0 	0.635000000000000009 	0.489999999999999991 	0.174999999999999989 	1.24350000000000005 	0.580500000000000016 	0.313 	0.304999999999999993 	10 	
+2 	0.709999999999999964 	0.560000000000000053 	0.174999999999999989 	1.72399999999999998 	0.565999999999999948 	0.457500000000000018 	0.462500000000000022 	13 	
+0 	0.665000000000000036 	0.505000000000000004 	0.165000000000000008 	1.34899999999999998 	0.598500000000000032 	0.317500000000000004 	0.359999999999999987 	9 	
+1 	0.440000000000000002 	0.320000000000000007 	0.104999999999999996 	0.387500000000000011 	0.175499999999999989 	0.0739999999999999963 	0.119999999999999996 	9 	
+1 	0.33500000000000002 	0.25 	0.0800000000000000017 	0.169500000000000012 	0.0695000000000000062 	0.0439999999999999974 	0.0495000000000000023 	6 	
+0 	0.660000000000000031 	0.530000000000000027 	0.179999999999999993 	1.51750000000000007 	0.776499999999999968 	0.301999999999999991 	0.401000000000000023 	10 	
+2 	0.344999999999999973 	0.270000000000000018 	0.0899999999999999967 	0.195000000000000007 	0.0779999999999999999 	0.0454999999999999988 	0.0589999999999999969 	9 	
+2 	0.494999999999999996 	0.400000000000000022 	0.140000000000000013 	0.777499999999999969 	0.201500000000000012 	0.179999999999999993 	0.25 	15 	
+0 	0.609999999999999987 	0.469999999999999973 	0.195000000000000007 	1.27350000000000008 	0.468999999999999972 	0.331500000000000017 	0.39800000000000002 	12 	
+1 	0.54500000000000004 	0.429999999999999993 	0.149999999999999994 	0.728500000000000036 	0.301999999999999991 	0.131500000000000006 	0.254500000000000004 	10 	
+1 	0.400000000000000022 	0.299999999999999989 	0.110000000000000001 	0.315000000000000002 	0.109 	0.067000000000000004 	0.119999999999999996 	9 	
+1 	0.325000000000000011 	0.239999999999999991 	0.0700000000000000067 	0.151999999999999996 	0.0565000000000000016 	0.0304999999999999993 	0.0539999999999999994 	8 	
+2 	0.33500000000000002 	0.25 	0.0899999999999999967 	0.180999999999999994 	0.0754999999999999977 	0.0415000000000000022 	0.0599999999999999978 	7 	
+1 	0.375 	0.284999999999999976 	0.0899999999999999967 	0.254500000000000004 	0.118999999999999995 	0.0594999999999999973 	0.0675000000000000044 	6 	
+1 	0.450000000000000011 	0.354999999999999982 	0.119999999999999996 	0.411999999999999977 	0.114500000000000005 	0.0665000000000000036 	0.160000000000000003 	19 	
+1 	0.395000000000000018 	0.294999999999999984 	0.0899999999999999967 	0.302499999999999991 	0.142999999999999988 	0.0665000000000000036 	0.0764999999999999986 	5 	
+1 	0.375 	0.280000000000000027 	0.0850000000000000061 	0.315500000000000003 	0.187 	0.0459999999999999992 	0.067000000000000004 	7 	
+0 	0.635000000000000009 	0.494999999999999996 	0.174999999999999989 	1.21100000000000008 	0.706999999999999962 	0.27250000000000002 	0.323000000000000009 	9 	
+1 	0.325000000000000011 	0.200000000000000011 	0.0800000000000000017 	0.0995000000000000051 	0.0395000000000000004 	0.0224999999999999992 	0.0320000000000000007 	8 	
+1 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.644499999999999962 	0.344999999999999973 	0.128500000000000003 	0.200000000000000011 	8 	
+0 	0.400000000000000022 	0.299999999999999989 	0.115000000000000005 	0.302499999999999991 	0.133500000000000008 	0.0464999999999999997 	0.0934999999999999998 	8 	
+0 	0.564999999999999947 	0.450000000000000011 	0.14499999999999999 	0.849500000000000033 	0.421499999999999986 	0.168500000000000011 	0.225000000000000006 	8 	
+0 	0.619999999999999996 	0.479999999999999982 	0.179999999999999993 	1.22150000000000003 	0.581999999999999962 	0.269500000000000017 	0.313 	12 	
+2 	0.474999999999999978 	0.395000000000000018 	0.135000000000000009 	0.591999999999999971 	0.246499999999999997 	0.164500000000000007 	0.200000000000000011 	13 	
+0 	0.569999999999999951 	0.455000000000000016 	0.165000000000000008 	1.05950000000000011 	0.440000000000000002 	0.219500000000000001 	0.284999999999999976 	14 	
+0 	0.375 	0.28999999999999998 	0.115000000000000005 	0.270500000000000018 	0.0929999999999999993 	0.0660000000000000031 	0.0884999999999999953 	10 	
+1 	0.520000000000000018 	0.380000000000000004 	0.115000000000000005 	0.66449999999999998 	0.328500000000000014 	0.170000000000000012 	0.142499999999999988 	7 	
+1 	0.594999999999999973 	0.429999999999999993 	0.165000000000000008 	0.984500000000000042 	0.452500000000000013 	0.20699999999999999 	0.27250000000000002 	8 	
+2 	0.555000000000000049 	0.424999999999999989 	0.130000000000000004 	0.766499999999999959 	0.264000000000000012 	0.16800000000000001 	0.275000000000000022 	13 	
+2 	0.5 	0.380000000000000004 	0.154999999999999999 	0.660000000000000031 	0.265500000000000014 	0.13650000000000001 	0.214999999999999997 	19 	
+0 	0.665000000000000036 	0.525000000000000022 	0.209999999999999992 	1.64399999999999991 	0.817999999999999949 	0.339500000000000024 	0.427499999999999991 	10 	
+1 	0.130000000000000004 	0.100000000000000006 	0.0299999999999999989 	0.0129999999999999994 	0.00449999999999999966 	0.00300000000000000006 	0.00400000000000000008 	3 	
+0 	0.525000000000000022 	0.424999999999999989 	0.160000000000000003 	0.83550000000000002 	0.354499999999999982 	0.213499999999999995 	0.244999999999999996 	9 	
+0 	0.584999999999999964 	0.465000000000000024 	0.170000000000000012 	0.991500000000000048 	0.38650000000000001 	0.224000000000000005 	0.265000000000000013 	12 	
+2 	0.550000000000000044 	0.424999999999999989 	0.154999999999999999 	0.917499999999999982 	0.277500000000000024 	0.242999999999999994 	0.33500000000000002 	13 	
+1 	0.375 	0.28999999999999998 	0.0950000000000000011 	0.212999999999999995 	0.096000000000000002 	0.0410000000000000017 	0.0609999999999999987 	5 	
+1 	0.340000000000000024 	0.260000000000000009 	0.0899999999999999967 	0.178999999999999992 	0.0759999999999999981 	0.0524999999999999981 	0.0550000000000000003 	6 	
+1 	0.390000000000000013 	0.28999999999999998 	0.100000000000000006 	0.222500000000000003 	0.0950000000000000011 	0.0464999999999999997 	0.0729999999999999954 	7 	
+0 	0.505000000000000004 	0.390000000000000013 	0.174999999999999989 	0.691999999999999948 	0.267000000000000015 	0.149999999999999994 	0.214999999999999997 	12 	
+0 	0.550000000000000044 	0.434999999999999998 	0.140000000000000013 	0.744999999999999996 	0.346999999999999975 	0.173999999999999988 	0.226500000000000007 	9 	
+0 	0.645000000000000018 	0.510000000000000009 	0.179999999999999993 	1.61949999999999994 	0.781499999999999972 	0.322000000000000008 	0.467500000000000027 	12 	
+1 	0.315000000000000002 	0.239999999999999991 	0.0850000000000000061 	0.171500000000000014 	0.0709999999999999937 	0.0345000000000000029 	0.0534999999999999989 	7 	
+2 	0.510000000000000009 	0.395000000000000018 	0.14499999999999999 	0.61850000000000005 	0.215999999999999998 	0.138500000000000012 	0.239999999999999991 	12 	
+2 	0.604999999999999982 	0.469999999999999973 	0.165000000000000008 	1.23150000000000004 	0.602500000000000036 	0.262000000000000011 	0.292499999999999982 	11 	
+0 	0.455000000000000016 	0.349999999999999978 	0.119999999999999996 	0.455500000000000016 	0.194500000000000006 	0.104499999999999996 	0.137500000000000011 	7 	
+1 	0.225000000000000006 	0.165000000000000008 	0.0550000000000000003 	0.0589999999999999969 	0.0269999999999999997 	0.0125000000000000007 	0.0149999999999999994 	4 	
+2 	0.569999999999999951 	0.445000000000000007 	0.149999999999999994 	0.987500000000000044 	0.504000000000000004 	0.20699999999999999 	0.248999999999999999 	8 	
+0 	0.525000000000000022 	0.41499999999999998 	0.170000000000000012 	0.832500000000000018 	0.275500000000000023 	0.168500000000000011 	0.309999999999999998 	13 	
+1 	0.429999999999999993 	0.330000000000000016 	0.100000000000000006 	0.44900000000000001 	0.254000000000000004 	0.0825000000000000039 	0.0970000000000000029 	6 	
+2 	0.550000000000000044 	0.424999999999999989 	0.149999999999999994 	0.831500000000000017 	0.410999999999999976 	0.17649999999999999 	0.216499999999999998 	10 	
+1 	0.33500000000000002 	0.25 	0.0800000000000000017 	0.16700000000000001 	0.0675000000000000044 	0.0325000000000000011 	0.0575000000000000025 	6 	
+1 	0.380000000000000004 	0.275000000000000022 	0.100000000000000006 	0.225500000000000006 	0.0800000000000000017 	0.0490000000000000019 	0.0850000000000000061 	10 	
+1 	0.525000000000000022 	0.390000000000000013 	0.119999999999999996 	0.664000000000000035 	0.311499999999999999 	0.146999999999999992 	0.177999999999999992 	9 	
+1 	0.385000000000000009 	0.280000000000000027 	0.125 	0.243999999999999995 	0.101999999999999993 	0.0379999999999999991 	0.0850000000000000061 	6 	
+2 	0.204999999999999988 	0.154999999999999999 	0.0449999999999999983 	0.0425000000000000031 	0.0170000000000000012 	0.00549999999999999968 	0.0154999999999999999 	7 	
+1 	0.5 	0.375 	0.140000000000000013 	0.549499999999999988 	0.247999999999999998 	0.112000000000000002 	0.158500000000000002 	7 	
+2 	0.555000000000000049 	0.455000000000000016 	0.135000000000000009 	0.836999999999999966 	0.382000000000000006 	0.171000000000000013 	0.234999999999999987 	9 	
+0 	0.57999999999999996 	0.440000000000000002 	0.125 	0.785499999999999976 	0.362999999999999989 	0.195500000000000007 	0.195000000000000007 	11 	
+1 	0.515000000000000013 	0.400000000000000022 	0.125 	0.5625 	0.25 	0.1245 	0.170000000000000012 	7 	
+0 	0.505000000000000004 	0.380000000000000004 	0.130000000000000004 	0.692999999999999949 	0.391000000000000014 	0.119499999999999995 	0.151499999999999996 	8 	
+2 	0.619999999999999996 	0.469999999999999973 	0.149999999999999994 	1.30899999999999994 	0.586999999999999966 	0.440500000000000003 	0.325000000000000011 	9 	
+2 	0.614999999999999991 	0.494999999999999996 	0.154999999999999999 	1.28649999999999998 	0.434999999999999998 	0.292999999999999983 	0.324500000000000011 	11 	
+2 	0.469999999999999973 	0.349999999999999978 	0.100000000000000006 	0.47749999999999998 	0.188500000000000001 	0.0884999999999999953 	0.174999999999999989 	8 	
+0 	0.635000000000000009 	0.510000000000000009 	0.170000000000000012 	1.22350000000000003 	0.532000000000000028 	0.271000000000000019 	0.353999999999999981 	9 	
+0 	0.650000000000000022 	0.5 	0.165000000000000008 	1.14450000000000007 	0.484999999999999987 	0.217999999999999999 	0.364999999999999991 	12 	
+2 	0.270000000000000018 	0.195000000000000007 	0.0800000000000000017 	0.100000000000000006 	0.0384999999999999995 	0.0195 	0.0299999999999999989 	6 	
+2 	0.645000000000000018 	0.5 	0.174999999999999989 	1.51049999999999995 	0.673499999999999988 	0.3755 	0.377500000000000002 	12 	
+1 	0.5 	0.395000000000000018 	0.140000000000000013 	0.621500000000000052 	0.292499999999999982 	0.120499999999999996 	0.195000000000000007 	9 	
+0 	0.455000000000000016 	0.354999999999999982 	1.12999999999999989 	0.593999999999999972 	0.332000000000000017 	0.116000000000000006 	0.133500000000000008 	8 	
+2 	0.574999999999999956 	0.445000000000000007 	0.14499999999999999 	0.876000000000000001 	0.379500000000000004 	0.161500000000000005 	0.270000000000000018 	10 	
+2 	0.685000000000000053 	0.550000000000000044 	0.200000000000000011 	1.77249999999999996 	0.812999999999999945 	0.387000000000000011 	0.489999999999999991 	11 	
+2 	0.535000000000000031 	0.429999999999999993 	0.140000000000000013 	0.716500000000000026 	0.285499999999999976 	0.159500000000000003 	0.215499999999999997 	8 	
+2 	0.574999999999999956 	0.46000000000000002 	0.165000000000000008 	0.91549999999999998 	0.400500000000000023 	0.246499999999999997 	0.23849999999999999 	8 	
+1 	0.375 	0.28999999999999998 	0.100000000000000006 	0.219 	0.0924999999999999989 	0.0379999999999999991 	0.0749999999999999972 	6 	
+0 	0.465000000000000024 	0.375 	0.135000000000000009 	0.599999999999999978 	0.222500000000000003 	0.129000000000000004 	0.23000000000000001 	16 	
+1 	0.440000000000000002 	0.340000000000000024 	0.100000000000000006 	0.406999999999999973 	0.208999999999999991 	0.0734999999999999959 	0.102999999999999994 	7 	
+1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.197000000000000008 	0.081500000000000003 	0.0325000000000000011 	0.0650000000000000022 	6 	
+2 	0.574999999999999956 	0.450000000000000011 	0.184999999999999998 	0.925000000000000044 	0.342000000000000026 	0.197000000000000008 	0.349999999999999978 	12 	
+0 	0.594999999999999973 	0.455000000000000016 	0.154999999999999999 	1.0605 	0.513499999999999956 	0.216499999999999998 	0.299999999999999989 	12 	
+1 	0.390000000000000013 	0.299999999999999989 	0.0950000000000000011 	0.326500000000000012 	0.166500000000000009 	0.0575000000000000025 	0.0889999999999999958 	7 	
+0 	0.450000000000000011 	0.344999999999999973 	0.119999999999999996 	0.416499999999999981 	0.165500000000000008 	0.0950000000000000011 	0.135000000000000009 	9 	
+0 	0.680000000000000049 	0.510000000000000009 	0.200000000000000011 	1.60749999999999993 	0.713999999999999968 	0.339000000000000024 	0.470499999999999974 	11 	
+2 	0.550000000000000044 	0.46000000000000002 	0.174999999999999989 	0.868999999999999995 	0.315500000000000003 	0.182499999999999996 	0.320000000000000007 	10 	
+2 	0.699999999999999956 	0.574999999999999956 	0.190000000000000002 	2.27300000000000013 	1.09499999999999997 	0.417999999999999983 	0.638000000000000012 	12 	
+1 	0.390000000000000013 	0.309999999999999998 	0.100000000000000006 	0.301999999999999991 	0.116000000000000006 	0.0640000000000000013 	0.115000000000000005 	11 	
+0 	0.650000000000000022 	0.494999999999999996 	0.154999999999999999 	1.33699999999999997 	0.614999999999999991 	0.319500000000000006 	0.33500000000000002 	9 	
+0 	0.755000000000000004 	0.574999999999999956 	0.200000000000000011 	2.07299999999999995 	1.01350000000000007 	0.465500000000000025 	0.479999999999999982 	11 	
+2 	0.724999999999999978 	0.564999999999999947 	0.214999999999999997 	1.89100000000000001 	0.697500000000000009 	0.472499999999999976 	0.57999999999999996 	16 	
+0 	0.675000000000000044 	0.564999999999999947 	0.195000000000000007 	1.83749999999999991 	0.764499999999999957 	0.361499999999999988 	0.553000000000000047 	12 	
+0 	0.395000000000000018 	0.315000000000000002 	0.104999999999999996 	0.351499999999999979 	0.118499999999999994 	0.0909999999999999976 	0.119499999999999995 	16 	
+2 	0.560000000000000053 	0.409999999999999976 	0.165000000000000008 	0.930000000000000049 	0.350499999999999978 	0.236999999999999988 	0.299999999999999989 	13 	
+2 	0.530000000000000027 	0.409999999999999976 	0.140000000000000013 	0.68100000000000005 	0.309499999999999997 	0.141499999999999987 	0.183499999999999996 	6 	
+0 	0.594999999999999973 	0.429999999999999993 	0.209999999999999992 	1.52449999999999997 	0.653000000000000025 	0.396000000000000019 	0.409999999999999976 	11 	
+0 	0.594999999999999973 	0.469999999999999973 	0.25 	1.28299999999999992 	0.462000000000000022 	0.247499999999999998 	0.445000000000000007 	14 	
+0 	0.635000000000000009 	0.494999999999999996 	0.179999999999999993 	1.59600000000000009 	0.616999999999999993 	0.317000000000000004 	0.369999999999999996 	11 	
+1 	0.275000000000000022 	0.204999999999999988 	0.0800000000000000017 	0.096000000000000002 	0.0359999999999999973 	0.0184999999999999991 	0.0299999999999999989 	6 	
+2 	0.724999999999999978 	0.574999999999999956 	0.239999999999999991 	2.20999999999999996 	1.35099999999999998 	0.412999999999999978 	0.501499999999999946 	13 	
+0 	0.665000000000000036 	0.555000000000000049 	0.195000000000000007 	1.43849999999999989 	0.580999999999999961 	0.353999999999999981 	0.359999999999999987 	17 	
+0 	0.550000000000000044 	0.429999999999999993 	0.140000000000000013 	0.839999999999999969 	0.375 	0.217999999999999999 	0.194500000000000006 	8 	
+0 	0.434999999999999998 	0.395000000000000018 	0.104999999999999996 	0.36349999999999999 	0.13600000000000001 	0.0980000000000000038 	0.130000000000000004 	9 	
+2 	0.535000000000000031 	0.440000000000000002 	0.149999999999999994 	0.67649999999999999 	0.256000000000000005 	0.139000000000000012 	0.260000000000000009 	12 	
+0 	0.555000000000000049 	0.434999999999999998 	0.165000000000000008 	0.969999999999999973 	0.336000000000000021 	0.231500000000000011 	0.294999999999999984 	17 	
+1 	0.520000000000000018 	0.395000000000000018 	0.125 	0.580500000000000016 	0.244499999999999995 	0.145999999999999991 	0.165000000000000008 	9 	
+0 	0.465000000000000024 	0.349999999999999978 	0.115000000000000005 	0.420999999999999985 	0.1565 	0.0909999999999999976 	0.134500000000000008 	9 	
+1 	0.479999999999999982 	0.369999999999999996 	0.125 	0.473999999999999977 	0.178999999999999992 	0.103499999999999995 	0.174999999999999989 	9 	
+1 	0.655000000000000027 	0.515000000000000013 	0.14499999999999999 	1.25 	0.526499999999999968 	0.282999999999999974 	0.315000000000000002 	15 	
+2 	0.619999999999999996 	0.489999999999999991 	0.154999999999999999 	1.10000000000000009 	0.505000000000000004 	0.247499999999999998 	0.309999999999999998 	9 	
+1 	0.54500000000000004 	0.424999999999999989 	0.140000000000000013 	0.814500000000000002 	0.304999999999999993 	0.231000000000000011 	0.243999999999999995 	10 	
+1 	0.530000000000000027 	0.429999999999999993 	0.140000000000000013 	0.677000000000000046 	0.297999999999999987 	0.0965000000000000024 	0.23000000000000001 	8 	
+0 	0.515000000000000013 	0.375 	0.110000000000000001 	0.606500000000000039 	0.300499999999999989 	0.131000000000000005 	0.149999999999999994 	6 	
+0 	0.599999999999999978 	0.46000000000000002 	0.149999999999999994 	1.2350000000000001 	0.602500000000000036 	0.274000000000000021 	0.28999999999999998 	8 	
+2 	0.614999999999999991 	0.450000000000000011 	0.149999999999999994 	1.19799999999999995 	0.706999999999999962 	0.209499999999999992 	0.2505 	7 	
+1 	0.455000000000000016 	0.344999999999999973 	0.104999999999999996 	0.400500000000000023 	0.164000000000000007 	0.0754999999999999977 	0.126000000000000001 	8 	
+0 	0.525000000000000022 	0.390000000000000013 	0.135000000000000009 	0.600500000000000034 	0.226500000000000007 	0.131000000000000005 	0.209999999999999992 	16 	
+1 	0.455000000000000016 	0.359999999999999987 	0.115000000000000005 	0.457000000000000017 	0.208499999999999991 	0.0855000000000000066 	0.146999999999999992 	10 	
+1 	0.330000000000000016 	0.239999999999999991 	0.0749999999999999972 	0.163000000000000006 	0.0744999999999999968 	0.0330000000000000016 	0.048000000000000001 	6 	
+2 	0.54500000000000004 	0.440000000000000002 	0.119999999999999996 	0.856500000000000039 	0.347499999999999976 	0.171500000000000014 	0.239999999999999991 	12 	
+2 	0.465000000000000024 	0.375 	0.110000000000000001 	0.5 	0.209999999999999992 	0.113000000000000003 	0.150499999999999995 	8 	
+0 	0.614999999999999991 	0.46000000000000002 	0.149999999999999994 	1.02649999999999997 	0.493499999999999994 	0.201000000000000012 	0.274500000000000022 	10 	
+0 	0.530000000000000027 	0.440000000000000002 	0.135000000000000009 	0.783499999999999974 	0.313 	0.171500000000000014 	0.2185 	9 	
+2 	0.640000000000000013 	0.489999999999999991 	0.154999999999999999 	1.12850000000000006 	0.47699999999999998 	0.269000000000000017 	0.340000000000000024 	9 	
+1 	0.170000000000000012 	0.125 	0.0550000000000000003 	0.0235000000000000001 	0.00899999999999999932 	0.00549999999999999968 	0.00800000000000000017 	6 	
+2 	0.635000000000000009 	0.525000000000000022 	0.184999999999999998 	1.40650000000000008 	0.684000000000000052 	0.299999999999999989 	0.3745 	10 	
+0 	0.57999999999999996 	0.445000000000000007 	0.135000000000000009 	0.949999999999999956 	0.483999999999999986 	0.181999999999999995 	0.232500000000000012 	8 	
+2 	0.694999999999999951 	0.560000000000000053 	0.184999999999999998 	1.73999999999999999 	0.885000000000000009 	0.371499999999999997 	0.4375 	10 	
+0 	0.630000000000000004 	0.494999999999999996 	0.190000000000000002 	1.16549999999999998 	0.536000000000000032 	0.211499999999999994 	0.162500000000000006 	10 	
+2 	0.5 	0.385000000000000009 	0.14499999999999999 	0.761499999999999955 	0.245999999999999996 	0.195000000000000007 	0.203999999999999987 	14 	
+2 	0.604999999999999982 	0.479999999999999982 	0.170000000000000012 	1.1835 	0.581999999999999962 	0.236499999999999988 	0.317000000000000004 	10 	
+2 	0.349999999999999978 	0.265000000000000013 	0.110000000000000001 	0.296499999999999986 	0.13650000000000001 	0.0630000000000000004 	0.0850000000000000061 	7 	
+2 	0.515000000000000013 	0.380000000000000004 	0.135000000000000009 	0.661499999999999977 	0.287499999999999978 	0.209499999999999992 	0.154999999999999999 	10 	
+1 	0.320000000000000007 	0.234999999999999987 	0.0800000000000000017 	0.148499999999999993 	0.0640000000000000013 	0.0309999999999999998 	0.0449999999999999983 	6 	
+2 	0.729999999999999982 	0.584999999999999964 	0.225000000000000006 	2.23050000000000015 	1.23950000000000005 	0.421999999999999986 	0.562999999999999945 	14 	
+2 	0.645000000000000018 	0.520000000000000018 	0.174999999999999989 	1.56099999999999994 	0.708999999999999964 	0.355499999999999983 	0.400000000000000022 	8 	
+1 	0.325000000000000011 	0.25 	0.0800000000000000017 	0.173499999999999988 	0.0764999999999999986 	0.0345000000000000029 	0.0490000000000000019 	7 	
+2 	0.5 	0.405000000000000027 	0.154999999999999999 	0.77200000000000002 	0.345999999999999974 	0.153499999999999998 	0.244999999999999996 	12 	
+2 	0.589999999999999969 	0.469999999999999973 	0.14499999999999999 	0.923499999999999988 	0.454500000000000015 	0.172999999999999987 	0.254000000000000004 	9 	
+0 	0.455000000000000016 	0.369999999999999996 	0.104999999999999996 	0.492499999999999993 	0.215999999999999998 	0.1245 	0.135000000000000009 	9 	
+2 	0.640000000000000013 	0.584999999999999964 	0.195000000000000007 	1.64700000000000002 	0.722500000000000031 	0.331000000000000016 	0.470999999999999974 	12 	
+1 	0.46000000000000002 	0.349999999999999978 	0.119999999999999996 	0.48849999999999999 	0.193000000000000005 	0.104999999999999996 	0.154999999999999999 	11 	
+1 	0.494999999999999996 	0.380000000000000004 	0.130000000000000004 	0.512499999999999956 	0.2185 	0.116000000000000006 	0.160000000000000003 	7 	
+1 	0.354999999999999982 	0.270000000000000018 	0.104999999999999996 	0.271000000000000019 	0.142499999999999988 	0.0524999999999999981 	0.0734999999999999959 	9 	
+1 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.726500000000000035 	0.320500000000000007 	0.14449999999999999 	0.229000000000000009 	9 	
+0 	0.729999999999999982 	0.555000000000000049 	0.179999999999999993 	1.6895 	0.655499999999999972 	0.196500000000000008 	0.493499999999999994 	10 	
+1 	0.465000000000000024 	0.364999999999999991 	0.115000000000000005 	0.467000000000000026 	0.231500000000000011 	0.0924999999999999989 	0.113000000000000003 	7 	
+2 	0.57999999999999996 	0.450000000000000011 	0.140000000000000013 	0.961500000000000021 	0.485999999999999988 	0.181499999999999995 	0.253000000000000003 	9 	
+2 	0.604999999999999982 	0.455000000000000016 	0.160000000000000003 	1.10349999999999993 	0.420999999999999985 	0.30149999999999999 	0.325000000000000011 	9 	
+0 	0.724999999999999978 	0.564999999999999947 	0.209999999999999992 	2.14250000000000007 	1.03000000000000003 	0.486999999999999988 	0.503000000000000003 	14 	
+0 	0.675000000000000044 	0.530000000000000027 	0.174999999999999989 	1.4464999999999999 	0.677499999999999991 	0.330000000000000016 	0.389000000000000012 	10 	
+0 	0.440000000000000002 	0.354999999999999982 	0.115000000000000005 	0.41499999999999998 	0.158500000000000002 	0.0924999999999999989 	0.131000000000000005 	11 	
+2 	0.719999999999999973 	0.550000000000000044 	0.204999999999999988 	2.16500000000000004 	1.10549999999999993 	0.525000000000000022 	0.404000000000000026 	10 	
+0 	0.655000000000000027 	0.525000000000000022 	0.190000000000000002 	1.35949999999999993 	0.563999999999999946 	0.321500000000000008 	0.398500000000000021 	10 	
+2 	0.560000000000000053 	0.41499999999999998 	0.14499999999999999 	0.85199999999999998 	0.429999999999999993 	0.188500000000000001 	0.204999999999999988 	8 	
+0 	0.465000000000000024 	0.380000000000000004 	0.135000000000000009 	0.578999999999999959 	0.20799999999999999 	0.1095 	0.220000000000000001 	14 	
+2 	0.650000000000000022 	0.520000000000000018 	0.209999999999999992 	1.6785000000000001 	0.666499999999999981 	0.307999999999999996 	0.46000000000000002 	11 	
+0 	0.619999999999999996 	0.489999999999999991 	0.160000000000000003 	1.05600000000000005 	0.492999999999999994 	0.243999999999999995 	0.27250000000000002 	9 	
+2 	0.540000000000000036 	0.419999999999999984 	0.119999999999999996 	0.811499999999999999 	0.392000000000000015 	0.14549999999999999 	0.223500000000000004 	9 	
+2 	0.630000000000000004 	0.494999999999999996 	0.160000000000000003 	1.09299999999999997 	0.496999999999999997 	0.221000000000000002 	0.315000000000000002 	12 	
+2 	0.614999999999999991 	0.474999999999999978 	0.174999999999999989 	1.10299999999999998 	0.463500000000000023 	0.309499999999999997 	0.27250000000000002 	10 	
+0 	0.645000000000000018 	0.484999999999999987 	0.149999999999999994 	1.15100000000000002 	0.593500000000000028 	0.231500000000000011 	0.292999999999999983 	12 	
+0 	0.380000000000000004 	0.28999999999999998 	0.104999999999999996 	0.257000000000000006 	0.0990000000000000047 	0.0509999999999999967 	0.0850000000000000061 	10 	
+2 	0.505000000000000004 	0.385000000000000009 	0.14499999999999999 	0.677499999999999991 	0.235999999999999988 	0.178999999999999992 	0.200000000000000011 	15 	
+1 	0.419999999999999984 	0.315000000000000002 	0.115000000000000005 	0.354999999999999982 	0.189500000000000002 	0.0650000000000000022 	0.086999999999999994 	6 	
+0 	0.599999999999999978 	0.479999999999999982 	0.179999999999999993 	1.0645 	0.449500000000000011 	0.245499999999999996 	0.325000000000000011 	10 	
+2 	0.564999999999999947 	0.455000000000000016 	0.184999999999999998 	0.92649999999999999 	0.353999999999999981 	0.157500000000000001 	0.375 	16 	
+2 	0.589999999999999969 	0.489999999999999991 	0.135000000000000009 	1.00800000000000001 	0.421999999999999986 	0.224500000000000005 	0.284999999999999976 	11 	
+2 	0.469999999999999973 	0.369999999999999996 	0.179999999999999993 	0.510000000000000009 	0.191500000000000004 	0.128500000000000003 	0.162500000000000006 	9 	
+1 	0.419999999999999984 	0.309999999999999998 	0.0950000000000000011 	0.279000000000000026 	0.1255 	0.0509999999999999967 	0.0879999999999999949 	6 	
+0 	0.625 	0.515000000000000013 	0.154999999999999999 	1.16349999999999998 	0.487499999999999989 	0.259000000000000008 	0.354999999999999982 	11 	
+0 	0.560000000000000053 	0.455000000000000016 	0.160000000000000003 	0.966999999999999971 	0.452500000000000013 	0.20699999999999999 	0.274000000000000021 	9 	
+1 	0.555000000000000049 	0.434999999999999998 	0.14499999999999999 	0.697500000000000009 	0.262000000000000011 	0.157500000000000001 	0.239999999999999991 	11 	
+2 	0.479999999999999982 	0.375 	0.115000000000000005 	0.67649999999999999 	0.320500000000000007 	0.106499999999999997 	0.170000000000000012 	6 	
+1 	0.510000000000000009 	0.400000000000000022 	0.125 	0.557499999999999996 	0.26150000000000001 	0.119499999999999995 	0.152499999999999997 	9 	
+1 	0.395000000000000018 	0.28999999999999998 	0.0950000000000000011 	0.299999999999999989 	0.158000000000000002 	0.0680000000000000049 	0.0779999999999999999 	7 	
+0 	0.614999999999999991 	0.474999999999999978 	0.170000000000000012 	1.05499999999999994 	0.543000000000000038 	0.245999999999999996 	0.234499999999999986 	9 	
+1 	0.390000000000000013 	0.284999999999999976 	0.100000000000000006 	0.281000000000000028 	0.127500000000000002 	0.0619999999999999996 	0.076999999999999999 	7 	
+2 	0.450000000000000011 	0.330000000000000016 	0.104999999999999996 	0.495499999999999996 	0.257500000000000007 	0.0820000000000000034 	0.129000000000000004 	8 	
+0 	0.515000000000000013 	0.41499999999999998 	0.130000000000000004 	0.764000000000000012 	0.276000000000000023 	0.196000000000000008 	0.25 	13 	
+2 	0.584999999999999964 	0.455000000000000016 	0.225000000000000006 	1.05499999999999994 	0.381500000000000006 	0.221000000000000002 	0.364999999999999991 	15 	
+1 	0.445000000000000007 	0.340000000000000024 	0.14499999999999999 	0.433999999999999997 	0.194500000000000006 	0.0904999999999999971 	0.130000000000000004 	7 	
+0 	0.550000000000000044 	0.434999999999999998 	0.170000000000000012 	0.884000000000000008 	0.287499999999999978 	0.164500000000000007 	0.280000000000000027 	14 	
+1 	0.635000000000000009 	0.505000000000000004 	0.190000000000000002 	1.33149999999999991 	0.580500000000000016 	0.252000000000000002 	0.434999999999999998 	17 	
+2 	0.635000000000000009 	0.515000000000000013 	0.170000000000000012 	1.27499999999999991 	0.509000000000000008 	0.285999999999999976 	0.340000000000000024 	16 	
+0 	0.584999999999999964 	0.434999999999999998 	0.140000000000000013 	0.695500000000000007 	0.308499999999999996 	0.129000000000000004 	0.224500000000000005 	8 	
+2 	0.660000000000000031 	0.530000000000000027 	0.195000000000000007 	1.55049999999999999 	0.650499999999999967 	0.329500000000000015 	0.494999999999999996 	10 	
+0 	0.614999999999999991 	0.520000000000000018 	0.149999999999999994 	1.34349999999999992 	0.629000000000000004 	0.260500000000000009 	0.344999999999999973 	10 	
+1 	0.520000000000000018 	0.409999999999999976 	0.110000000000000001 	0.518499999999999961 	0.216499999999999998 	0.091499999999999998 	0.183999999999999997 	8 	
+1 	0.535000000000000031 	0.419999999999999984 	0.14499999999999999 	0.926000000000000045 	0.39800000000000002 	0.196500000000000008 	0.25 	17 	
+1 	0.315000000000000002 	0.23000000000000001 	0.0899999999999999967 	0.128500000000000003 	0.0429999999999999966 	0.0400000000000000008 	0.0400000000000000008 	7 	
+0 	0.625 	0.484999999999999987 	0.174999999999999989 	1.3620000000000001 	0.67649999999999999 	0.26150000000000001 	0.370499999999999996 	10 	
+1 	0.424999999999999989 	0.315000000000000002 	0.0800000000000000017 	0.302999999999999992 	0.131000000000000005 	0.0585000000000000034 	0.0950000000000000011 	7 	
+2 	0.390000000000000013 	0.280000000000000027 	0.125 	0.563999999999999946 	0.303499999999999992 	0.0955000000000000016 	0.142999999999999988 	7 	
+2 	0.380000000000000004 	0.28999999999999998 	0.119999999999999996 	0.282999999999999974 	0.117499999999999993 	0.0655000000000000027 	0.0850000000000000061 	9 	
+2 	0.325000000000000011 	0.239999999999999991 	0.0749999999999999972 	0.154999999999999999 	0.0475000000000000006 	0.0354999999999999968 	0.0599999999999999978 	9 	
+2 	0.650000000000000022 	0.510000000000000009 	0.190000000000000002 	1.54200000000000004 	0.715500000000000025 	0.373499999999999999 	0.375 	9 	
+2 	0.645000000000000018 	0.484999999999999987 	0.214999999999999997 	1.51400000000000001 	0.546000000000000041 	0.26150000000000001 	0.635000000000000009 	16 	
+1 	0.5 	0.380000000000000004 	0.135000000000000009 	0.52849999999999997 	0.226000000000000006 	0.122999999999999998 	0.208999999999999991 	8 	
+0 	0.645000000000000018 	0.5 	0.200000000000000011 	1.4285000000000001 	0.639000000000000012 	0.304999999999999993 	0.359999999999999987 	11 	
+0 	0.614999999999999991 	0.494999999999999996 	0.165000000000000008 	1.19799999999999995 	0.541499999999999981 	0.286499999999999977 	0.318500000000000005 	10 	
+2 	0.445000000000000007 	0.320000000000000007 	0.119999999999999996 	0.413999999999999979 	0.19900000000000001 	0.0899999999999999967 	0.117000000000000007 	7 	
+0 	0.675000000000000044 	0.550000000000000044 	0.190000000000000002 	1.55099999999999993 	0.71050000000000002 	0.368499999999999994 	0.411999999999999977 	13 	
+0 	0.46000000000000002 	0.380000000000000004 	0.130000000000000004 	0.639000000000000012 	0.299999999999999989 	0.152499999999999997 	0.160000000000000003 	11 	
+0 	0.5 	0.380000000000000004 	0.154999999999999999 	0.655000000000000027 	0.240499999999999992 	0.142999999999999988 	0.204999999999999988 	17 	
+1 	0.315000000000000002 	0.23000000000000001 	0.0800000000000000017 	0.137500000000000011 	0.0544999999999999998 	0.0309999999999999998 	0.0444999999999999979 	5 	
+0 	0.709999999999999964 	0.5 	0.149999999999999994 	1.3165 	0.683499999999999996 	0.281499999999999972 	0.280000000000000027 	10 	
+0 	0.515000000000000013 	0.419999999999999984 	0.135000000000000009 	0.629499999999999948 	0.281499999999999972 	0.127000000000000002 	0.214999999999999997 	9 	
+2 	0.655000000000000027 	0.57999999999999996 	0.204999999999999988 	2.08049999999999979 	0.958999999999999964 	0.341500000000000026 	0.600999999999999979 	17 	
+1 	0.469999999999999973 	0.369999999999999996 	0.119999999999999996 	0.470499999999999974 	0.184499999999999997 	0.105499999999999997 	0.154999999999999999 	12 	
+2 	0.630000000000000004 	0.484999999999999987 	0.14499999999999999 	1.06200000000000006 	0.50649999999999995 	0.178499999999999992 	0.336500000000000021 	12 	
+2 	0.619999999999999996 	0.510000000000000009 	0.174999999999999989 	1.61499999999999999 	0.510499999999999954 	0.192000000000000004 	0.675000000000000044 	12 	
+0 	0.609999999999999987 	0.450000000000000011 	0.160000000000000003 	1.1359999999999999 	0.413999999999999979 	0.310999999999999999 	0.299999999999999989 	9 	
+0 	0.635000000000000009 	0.505000000000000004 	0.14499999999999999 	1.13450000000000006 	0.505000000000000004 	0.265500000000000014 	0.315000000000000002 	10 	
+1 	0.424999999999999989 	0.344999999999999973 	0.125 	0.424999999999999989 	0.160000000000000003 	0.0795000000000000012 	0.153999999999999998 	13 	
+1 	0.405000000000000027 	0.299999999999999989 	0.119999999999999996 	0.32400000000000001 	0.126500000000000001 	0.0700000000000000067 	0.110000000000000001 	7 	
+1 	0.214999999999999997 	0.170000000000000012 	0.0550000000000000003 	0.0604999999999999982 	0.0205000000000000009 	0.0140000000000000003 	0.0200000000000000004 	6 	
+1 	0.469999999999999973 	0.375 	0.104999999999999996 	0.441000000000000003 	0.16700000000000001 	0.0864999999999999936 	0.14499999999999999 	10 	
+2 	0.584999999999999964 	0.479999999999999982 	0.184999999999999998 	1.04000000000000004 	0.433999999999999997 	0.265000000000000013 	0.284999999999999976 	10 	
+1 	0.390000000000000013 	0.28999999999999998 	0.0899999999999999967 	0.262500000000000011 	0.117000000000000007 	0.0539999999999999994 	0.076999999999999999 	7 	
+2 	0.419999999999999984 	0.344999999999999973 	0.104999999999999996 	0.429999999999999993 	0.174999999999999989 	0.096000000000000002 	0.130000000000000004 	7 	
+0 	0.650000000000000022 	0.5 	0.160000000000000003 	1.38250000000000006 	0.701999999999999957 	0.303999999999999992 	0.319500000000000006 	9 	
+2 	0.569999999999999951 	0.450000000000000011 	0.140000000000000013 	0.79500000000000004 	0.338500000000000023 	0.147999999999999993 	0.244999999999999996 	9 	
+1 	0.540000000000000036 	0.424999999999999989 	0.140000000000000013 	0.741999999999999993 	0.320000000000000007 	0.139500000000000013 	0.25 	9 	
+1 	0.535000000000000031 	0.455000000000000016 	0.140000000000000013 	1.00150000000000006 	0.530000000000000027 	0.17649999999999999 	0.243999999999999995 	9 	
+2 	0.385000000000000009 	0.309999999999999998 	0.100000000000000006 	0.284499999999999975 	0.106499999999999997 	0.0749999999999999972 	0.100000000000000006 	11 	
+2 	0.574999999999999956 	0.445000000000000007 	0.160000000000000003 	0.838999999999999968 	0.400500000000000023 	0.198000000000000009 	0.23899999999999999 	9 	
+1 	0.510000000000000009 	0.405000000000000027 	0.135000000000000009 	0.769000000000000017 	0.365499999999999992 	0.158500000000000002 	0.179999999999999993 	7 	
+2 	0.719999999999999973 	0.564999999999999947 	0.200000000000000011 	2.10550000000000015 	1.0169999999999999 	0.362999999999999989 	0.493999999999999995 	12 	
+2 	0.685000000000000053 	0.535000000000000031 	0.154999999999999999 	1.38450000000000006 	0.661499999999999977 	0.214499999999999996 	0.407499999999999973 	10 	
+2 	0.515000000000000013 	0.400000000000000022 	0.140000000000000013 	0.633499999999999952 	0.287999999999999978 	0.14499999999999999 	0.16800000000000001 	9 	
+1 	0.344999999999999973 	0.265000000000000013 	0.100000000000000006 	0.245499999999999996 	0.111000000000000001 	0.0534999999999999989 	0.0650000000000000022 	7 	
+0 	0.469999999999999973 	0.364999999999999991 	0.119999999999999996 	0.543000000000000038 	0.22950000000000001 	0.149499999999999994 	0.149999999999999994 	9 	
+1 	0.614999999999999991 	0.489999999999999991 	0.154999999999999999 	0.988500000000000045 	0.41449999999999998 	0.195000000000000007 	0.344999999999999973 	13 	
+2 	0.660000000000000031 	0.54500000000000004 	0.184999999999999998 	1.32000000000000006 	0.530499999999999972 	0.263500000000000012 	0.455000000000000016 	16 	
+1 	0.380000000000000004 	0.28999999999999998 	0.0850000000000000061 	0.228500000000000009 	0.0879999999999999949 	0.0464999999999999997 	0.0749999999999999972 	7 	
+2 	0.569999999999999951 	0.440000000000000002 	0.0950000000000000011 	0.826999999999999957 	0.339500000000000024 	0.221500000000000002 	0.234999999999999987 	8 	
+1 	0.400000000000000022 	0.309999999999999998 	0.100000000000000006 	0.287499999999999978 	0.114500000000000005 	0.0635000000000000009 	0.0950000000000000011 	10 	
+0 	0.530000000000000027 	0.429999999999999993 	0.170000000000000012 	0.775000000000000022 	0.349999999999999978 	0.151999999999999996 	0.234999999999999987 	17 	
+0 	0.630000000000000004 	0.479999999999999982 	0.149999999999999994 	1.05249999999999999 	0.392000000000000015 	0.336000000000000021 	0.284999999999999976 	12 	
+2 	0.525000000000000022 	0.405000000000000027 	0.119999999999999996 	0.755499999999999949 	0.3755 	0.155499999999999999 	0.201000000000000012 	9 	
+0 	0.584999999999999964 	0.455000000000000016 	0.130000000000000004 	0.875499999999999945 	0.410999999999999976 	0.206499999999999989 	0.225000000000000006 	8 	
+2 	0.445000000000000007 	0.354999999999999982 	0.110000000000000001 	0.441500000000000004 	0.180499999999999994 	0.103499999999999995 	0.150499999999999995 	10 	
+2 	0.694999999999999951 	0.515000000000000013 	0.174999999999999989 	1.51649999999999996 	0.577999999999999958 	0.410499999999999976 	0.390000000000000013 	15 	
+1 	0.390000000000000013 	0.294999999999999984 	0.100000000000000006 	0.279000000000000026 	0.115500000000000005 	0.0589999999999999969 	0.0800000000000000017 	7 	
+0 	0.709999999999999964 	0.569999999999999951 	0.195000000000000007 	1.98049999999999993 	0.992500000000000049 	0.492499999999999993 	0.479999999999999982 	12 	
+2 	0.734999999999999987 	0.589999999999999969 	0.214999999999999997 	1.74700000000000011 	0.727500000000000036 	0.403000000000000025 	0.557000000000000051 	11 	
+0 	0.625 	0.5 	0.160000000000000003 	1.21700000000000008 	0.572500000000000009 	0.20699999999999999 	0.354999999999999982 	11 	
+0 	0.574999999999999956 	0.46000000000000002 	0.14499999999999999 	0.994500000000000051 	0.466000000000000025 	0.229000000000000009 	0.265000000000000013 	7 	
+0 	0.755000000000000004 	0.625 	0.209999999999999992 	2.50499999999999989 	1.1964999999999999 	0.513000000000000012 	0.678499999999999992 	11 	
+1 	0.46000000000000002 	0.349999999999999978 	0.100000000000000006 	0.470999999999999974 	0.252000000000000002 	0.076999999999999999 	0.122999999999999998 	8 	
+0 	0.564999999999999947 	0.450000000000000011 	0.135000000000000009 	0.988500000000000045 	0.387000000000000011 	0.149499999999999994 	0.309999999999999998 	12 	
+0 	0.564999999999999947 	0.440000000000000002 	0.154999999999999999 	0.939500000000000002 	0.427499999999999991 	0.213999999999999996 	0.270000000000000018 	12 	
+1 	0.650000000000000022 	0.520000000000000018 	0.149999999999999994 	1.23799999999999999 	0.549499999999999988 	0.295999999999999985 	0.330500000000000016 	10 	
+0 	0.635000000000000009 	0.505000000000000004 	0.154999999999999999 	1.28950000000000009 	0.593999999999999972 	0.314000000000000001 	0.344999999999999973 	11 	
+2 	0.450000000000000011 	0.33500000000000002 	0.125 	0.348999999999999977 	0.118999999999999995 	0.105499999999999997 	0.115000000000000005 	10 	
+0 	0.609999999999999987 	0.474999999999999978 	0.140000000000000013 	1.13300000000000001 	0.527499999999999969 	0.235499999999999987 	0.349999999999999978 	11 	
+2 	0.689999999999999947 	0.525000000000000022 	0.200000000000000011 	1.78249999999999997 	0.916499999999999981 	0.332500000000000018 	0.461000000000000021 	12 	
+2 	0.655000000000000027 	0.489999999999999991 	0.174999999999999989 	1.35850000000000004 	0.639499999999999957 	0.293999999999999984 	0.364999999999999991 	10 	
+2 	0.530000000000000027 	0.409999999999999976 	0.154999999999999999 	0.715500000000000025 	0.280500000000000027 	0.168500000000000011 	0.213999999999999996 	11 	
+1 	0.505000000000000004 	0.405000000000000027 	0.130000000000000004 	0.601500000000000035 	0.30149999999999999 	0.110000000000000001 	0.179999999999999993 	8 	
+1 	0.409999999999999976 	0.315000000000000002 	0.100000000000000006 	0.299999999999999989 	0.123999999999999999 	0.0575000000000000025 	0.100000000000000006 	8 	
+1 	0.484999999999999987 	0.380000000000000004 	0.119999999999999996 	0.472499999999999976 	0.20749999999999999 	0.107499999999999998 	0.146999999999999992 	6 	
+1 	0.450000000000000011 	0.354999999999999982 	0.110000000000000001 	0.458500000000000019 	0.194000000000000006 	0.067000000000000004 	0.140000000000000013 	8 	
+2 	0.599999999999999978 	0.489999999999999991 	0.209999999999999992 	1.98750000000000004 	1.00499999999999989 	0.418999999999999984 	0.490999999999999992 	10 	
+2 	0.530000000000000027 	0.419999999999999984 	0.165000000000000008 	0.894499999999999962 	0.319000000000000006 	0.23899999999999999 	0.244999999999999996 	11 	
+2 	0.550000000000000044 	0.450000000000000011 	0.149999999999999994 	1.01449999999999996 	0.406999999999999973 	0.201500000000000012 	0.287499999999999978 	10 	
+1 	0.465000000000000024 	0.380000000000000004 	0.130000000000000004 	0.454000000000000015 	0.189500000000000002 	0.0800000000000000017 	0.154999999999999999 	11 	
+2 	0.474999999999999978 	0.354999999999999982 	0.125 	0.462500000000000022 	0.185999999999999999 	0.106999999999999998 	0.14499999999999999 	9 	
+2 	0.594999999999999973 	0.489999999999999991 	0.184999999999999998 	1.18500000000000005 	0.481999999999999984 	0.201500000000000012 	0.360999999999999988 	10 	
+2 	0.275000000000000022 	0.204999999999999988 	0.0700000000000000067 	0.0940000000000000002 	0.033500000000000002 	0.0200000000000000004 	0.0325000000000000011 	5 	
+1 	0.349999999999999978 	0.265000000000000013 	0.0800000000000000017 	0.200000000000000011 	0.0899999999999999967 	0.0420000000000000026 	0.0599999999999999978 	7 	
+0 	0.685000000000000053 	0.540000000000000036 	0.160000000000000003 	1.66749999999999998 	0.832999999999999963 	0.377500000000000002 	0.474999999999999978 	11 	
+0 	0.5 	0.375 	0.140000000000000013 	0.603999999999999981 	0.241999999999999993 	0.141499999999999987 	0.178999999999999992 	15 	
+2 	0.699999999999999956 	0.550000000000000044 	0.200000000000000011 	1.52299999999999991 	0.692999999999999949 	0.305999999999999994 	0.440500000000000003 	13 	
+0 	0.589999999999999969 	0.450000000000000011 	0.160000000000000003 	0.900000000000000022 	0.357999999999999985 	0.156 	0.315000000000000002 	19 	
+1 	0.405000000000000027 	0.309999999999999998 	0.0950000000000000011 	0.342500000000000027 	0.178499999999999992 	0.0640000000000000013 	0.0855000000000000066 	8 	
+0 	0.619999999999999996 	0.479999999999999982 	0.170000000000000012 	1.10450000000000004 	0.535000000000000031 	0.25 	0.286999999999999977 	10 	
+1 	0.474999999999999978 	0.354999999999999982 	0.104999999999999996 	0.468000000000000027 	0.201000000000000012 	0.111500000000000002 	0.119999999999999996 	8 	
+1 	0.530000000000000027 	0.405000000000000027 	0.119999999999999996 	0.632000000000000006 	0.271500000000000019 	0.147999999999999993 	0.1875 	9 	
+1 	0.5 	0.375 	0.119999999999999996 	0.529000000000000026 	0.223500000000000004 	0.122999999999999998 	0.160000000000000003 	8 	
+2 	0.70499999999999996 	0.564999999999999947 	0.515000000000000013 	2.20999999999999996 	1.10749999999999993 	0.486499999999999988 	0.512000000000000011 	10 	
+2 	0.635000000000000009 	0.494999999999999996 	0.149999999999999994 	1.08099999999999996 	0.482499999999999984 	0.241999999999999993 	0.309999999999999998 	11 	
+0 	0.67000000000000004 	0.515000000000000013 	0.170000000000000012 	1.4265000000000001 	0.660499999999999976 	0.339500000000000024 	0.369999999999999996 	11 	
+0 	0.505000000000000004 	0.375 	0.179999999999999993 	0.567999999999999949 	0.232500000000000012 	0.149499999999999994 	0.170000000000000012 	12 	
+1 	0.385000000000000009 	0.280000000000000027 	0.0950000000000000011 	0.257000000000000006 	0.118999999999999995 	0.0589999999999999969 	0.0700000000000000067 	7 	
+0 	0.440000000000000002 	0.330000000000000016 	0.115000000000000005 	0.400500000000000023 	0.142999999999999988 	0.113000000000000003 	0.119999999999999996 	8 	
+2 	0.625 	0.465000000000000024 	0.140000000000000013 	1.19500000000000006 	0.482499999999999984 	0.204999999999999988 	0.400000000000000022 	13 	
+0 	0.650000000000000022 	0.510000000000000009 	0.170000000000000012 	1.56699999999999995 	0.724500000000000033 	0.348999999999999977 	0.391000000000000014 	10 	
+0 	0.550000000000000044 	0.429999999999999993 	0.149999999999999994 	0.839999999999999969 	0.395000000000000018 	0.195000000000000007 	0.223000000000000004 	8 	
+1 	0.484999999999999987 	0.369999999999999996 	0.130000000000000004 	0.458000000000000018 	0.180999999999999994 	0.113000000000000003 	0.13600000000000001 	10 	
+2 	0.640000000000000013 	0.510000000000000009 	0.190000000000000002 	1.61299999999999999 	0.621500000000000052 	0.360999999999999988 	0.469999999999999973 	14 	
+0 	0.540000000000000036 	0.474999999999999978 	0.154999999999999999 	0.928000000000000047 	0.394000000000000017 	0.194000000000000006 	0.260000000000000009 	11 	
+1 	0.530000000000000027 	0.429999999999999993 	0.135000000000000009 	0.625499999999999945 	0.244999999999999996 	0.14549999999999999 	0.213499999999999995 	10 	
+0 	0.57999999999999996 	0.465000000000000024 	0.14499999999999999 	0.986500000000000044 	0.469999999999999973 	0.215499999999999997 	0.25 	11 	
+1 	0.184999999999999998 	0.135000000000000009 	0.0400000000000000008 	0.0269999999999999997 	0.0105000000000000007 	0.00549999999999999968 	0.00899999999999999932 	5 	
+1 	0.474999999999999978 	0.375 	0.110000000000000001 	0.456000000000000016 	0.181999999999999995 	0.0990000000000000047 	0.160000000000000003 	9 	
+0 	0.455000000000000016 	0.354999999999999982 	0.119999999999999996 	0.449500000000000011 	0.176999999999999991 	0.103999999999999995 	0.149999999999999994 	9 	
+2 	0.505000000000000004 	0.395000000000000018 	0.135000000000000009 	0.591500000000000026 	0.287999999999999978 	0.131500000000000006 	0.184999999999999998 	12 	
+2 	0.550000000000000044 	0.424999999999999989 	0.160000000000000003 	0.793000000000000038 	0.343000000000000027 	0.203499999999999986 	0.214999999999999997 	9 	
+0 	0.650000000000000022 	0.515000000000000013 	0.195000000000000007 	1.40050000000000008 	0.519499999999999962 	0.359999999999999987 	0.440000000000000002 	13 	
+1 	0.429999999999999993 	0.325000000000000011 	0.0899999999999999967 	0.424999999999999989 	0.216999999999999998 	0.086999999999999994 	0.0950000000000000011 	7 	
+2 	0.645000000000000018 	0.510000000000000009 	0.154999999999999999 	1.53899999999999992 	0.640499999999999958 	0.358499999999999985 	0.429999999999999993 	11 	
+2 	0.574999999999999956 	0.469999999999999973 	0.140000000000000013 	0.837500000000000022 	0.348499999999999976 	0.173499999999999988 	0.239999999999999991 	11 	
+1 	0.614999999999999991 	0.474999999999999978 	0.130000000000000004 	0.842500000000000027 	0.35299999999999998 	0.191500000000000004 	0.251000000000000001 	8 	
+0 	0.619999999999999996 	0.474999999999999978 	0.174999999999999989 	1.01649999999999996 	0.435499999999999998 	0.213999999999999996 	0.325000000000000011 	10 	
+1 	0.625 	0.465000000000000024 	0.154999999999999999 	0.971999999999999975 	0.404000000000000026 	0.184499999999999997 	0.349999999999999978 	14 	
+1 	0.320000000000000007 	0.255000000000000004 	0.100000000000000006 	0.175499999999999989 	0.0729999999999999954 	0.0415000000000000022 	0.0650000000000000022 	7 	
+2 	0.484999999999999987 	0.369999999999999996 	0.140000000000000013 	0.572500000000000009 	0.203999999999999987 	0.141499999999999987 	0.174999999999999989 	10 	
+2 	0.589999999999999969 	0.440000000000000002 	0.149999999999999994 	0.955500000000000016 	0.365999999999999992 	0.242499999999999993 	0.294999999999999984 	11 	
+2 	0.550000000000000044 	0.450000000000000011 	0.154999999999999999 	0.78949999999999998 	0.343000000000000027 	0.159000000000000002 	0.25 	12 	
+0 	0.489999999999999991 	0.385000000000000009 	0.149999999999999994 	0.786499999999999977 	0.240999999999999992 	0.140000000000000013 	0.239999999999999991 	23 	
+0 	0.67000000000000004 	0.520000000000000018 	0.190000000000000002 	1.32000000000000006 	0.523499999999999965 	0.309499999999999997 	0.427499999999999991 	13 	
+0 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.828500000000000014 	0.361999999999999988 	0.165500000000000008 	0.235999999999999988 	10 	
+2 	0.555000000000000049 	0.400000000000000022 	0.130000000000000004 	0.707500000000000018 	0.332000000000000017 	0.158500000000000002 	0.179999999999999993 	7 	
+1 	0.405000000000000027 	0.309999999999999998 	0.0650000000000000022 	0.320500000000000007 	0.157500000000000001 	0.0660000000000000031 	0.0879999999999999949 	6 	
+1 	0.354999999999999982 	0.284999999999999976 	0.0950000000000000011 	0.227500000000000008 	0.0955000000000000016 	0.0475000000000000006 	0.0714999999999999941 	6 	
+0 	0.520000000000000018 	0.409999999999999976 	0.125 	0.69850000000000001 	0.294499999999999984 	0.162500000000000006 	0.214999999999999997 	10 	
+0 	0.645000000000000018 	0.520000000000000018 	0.170000000000000012 	1.19700000000000006 	0.526000000000000023 	0.292499999999999982 	0.317000000000000004 	11 	
+1 	0.375 	0.299999999999999989 	0.0749999999999999972 	0.143999999999999989 	0.0589999999999999969 	0.0299999999999999989 	0.0439999999999999974 	7 	
+1 	0.349999999999999978 	0.260000000000000009 	0.0850000000000000061 	0.173999999999999988 	0.0704999999999999932 	0.0345000000000000029 	0.0599999999999999978 	10 	
+2 	0.354999999999999982 	0.265000000000000013 	0.0850000000000000061 	0.201000000000000012 	0.0690000000000000058 	0.0529999999999999985 	0.0695000000000000062 	8 	
+1 	0.510000000000000009 	0.380000000000000004 	0.115000000000000005 	0.515499999999999958 	0.214999999999999997 	0.113500000000000004 	0.166000000000000009 	8 	
+2 	0.564999999999999947 	0.390000000000000013 	0.125 	0.743999999999999995 	0.35199999999999998 	0.130000000000000004 	0.168500000000000011 	11 	
+2 	0.609999999999999987 	0.474999999999999978 	0.174999999999999989 	1.02400000000000002 	0.408999999999999975 	0.26100000000000001 	0.322000000000000008 	9 	
+2 	0.619999999999999996 	0.5 	0.165000000000000008 	1.30699999999999994 	0.635499999999999954 	0.254500000000000004 	0.315000000000000002 	9 	
+0 	0.344999999999999973 	0.260000000000000009 	0.0899999999999999967 	0.20699999999999999 	0.0774999999999999994 	0.043499999999999997 	0.0764999999999999986 	10 	
+2 	0.505000000000000004 	0.390000000000000013 	0.119999999999999996 	0.653000000000000025 	0.331500000000000017 	0.138500000000000012 	0.16700000000000001 	9 	
+0 	0.369999999999999996 	0.275000000000000022 	0.100000000000000006 	0.222500000000000003 	0.0929999999999999993 	0.0259999999999999988 	0.0800000000000000017 	8 	
+2 	0.550000000000000044 	0.429999999999999993 	0.149999999999999994 	0.874500000000000055 	0.412999999999999978 	0.190500000000000003 	0.247999999999999998 	9 	
+2 	0.775000000000000022 	0.569999999999999951 	0.220000000000000001 	2.03200000000000003 	0.734999999999999987 	0.475499999999999978 	0.658499999999999974 	17 	
+2 	0.369999999999999996 	0.265000000000000013 	0.0749999999999999972 	0.213999999999999996 	0.0899999999999999967 	0.0509999999999999967 	0.0700000000000000067 	6 	
+2 	0.599999999999999978 	0.465000000000000024 	0.165000000000000008 	1.0475000000000001 	0.465000000000000024 	0.234499999999999986 	0.315000000000000002 	11 	
+0 	0.469999999999999973 	0.354999999999999982 	0.130000000000000004 	0.546499999999999986 	0.200500000000000012 	0.126000000000000001 	0.184999999999999998 	14 	
+2 	0.599999999999999978 	0.479999999999999982 	0.154999999999999999 	1.01400000000000001 	0.451000000000000012 	0.188500000000000001 	0.325000000000000011 	11 	
+0 	0.625 	0.489999999999999991 	0.165000000000000008 	1.127 	0.47699999999999998 	0.236499999999999988 	0.318500000000000005 	9 	
+2 	0.625 	0.474999999999999978 	0.174999999999999989 	1.34050000000000002 	0.656000000000000028 	0.282999999999999974 	0.337000000000000022 	10 	
+2 	0.469999999999999973 	0.369999999999999996 	0.104999999999999996 	0.466500000000000026 	0.202500000000000013 	0.101500000000000007 	0.154999999999999999 	10 	
+1 	0.429999999999999993 	0.325000000000000011 	0.110000000000000001 	0.367499999999999993 	0.135500000000000009 	0.0934999999999999998 	0.119999999999999996 	13 	
+1 	0.23000000000000001 	0.170000000000000012 	0.0500000000000000028 	0.0570000000000000021 	0.0259999999999999988 	0.0129999999999999994 	0.0160000000000000003 	5 	
+0 	0.484999999999999987 	0.395000000000000018 	0.160000000000000003 	0.660000000000000031 	0.247499999999999998 	0.128000000000000003 	0.234999999999999987 	14 	
+1 	0.375 	0.244999999999999996 	0.100000000000000006 	0.394000000000000017 	0.166000000000000009 	0.0909999999999999976 	0.112500000000000003 	6 	
+0 	0.625 	0.489999999999999991 	0.174999999999999989 	1.10749999999999993 	0.44850000000000001 	0.216499999999999998 	0.359499999999999986 	8 	
+2 	0.589999999999999969 	0.489999999999999991 	0.165000000000000008 	1.20700000000000007 	0.559000000000000052 	0.234999999999999987 	0.308999999999999997 	10 	
+0 	0.564999999999999947 	0.450000000000000011 	0.160000000000000003 	0.79500000000000004 	0.360499999999999987 	0.155499999999999999 	0.23000000000000001 	12 	
+0 	0.380000000000000004 	0.299999999999999989 	0.0899999999999999967 	0.321500000000000008 	0.154499999999999998 	0.0749999999999999972 	0.0950000000000000011 	9 	
+2 	0.614999999999999991 	0.469999999999999973 	0.149999999999999994 	1.08749999999999991 	0.497499999999999998 	0.282999999999999974 	0.268500000000000016 	9 	
+2 	0.540000000000000036 	0.434999999999999998 	0.140000000000000013 	0.734500000000000042 	0.330000000000000016 	0.159500000000000003 	0.212999999999999995 	9 	
+2 	0.530000000000000027 	0.41499999999999998 	0.174999999999999989 	0.739500000000000046 	0.26100000000000001 	0.139500000000000013 	0.264500000000000013 	17 	
+0 	0.724999999999999978 	0.530000000000000027 	0.190000000000000002 	1.73150000000000004 	0.82999999999999996 	0.39800000000000002 	0.405000000000000027 	11 	
+1 	0.5 	0.380000000000000004 	0.135000000000000009 	0.593999999999999972 	0.294499999999999984 	0.103999999999999995 	0.1565 	9 	
+0 	0.535000000000000031 	0.419999999999999984 	0.130000000000000004 	0.698999999999999955 	0.3125 	0.1565 	0.203499999999999986 	8 	
+1 	0.424999999999999989 	0.320000000000000007 	0.0850000000000000061 	0.262000000000000011 	0.123499999999999999 	0.067000000000000004 	0.072499999999999995 	8 	
+1 	0.525000000000000022 	0.400000000000000022 	0.125 	0.696500000000000008 	0.368999999999999995 	0.138500000000000012 	0.164000000000000007 	9 	
+2 	0.584999999999999964 	0.450000000000000011 	0.174999999999999989 	1.12749999999999995 	0.492499999999999993 	0.262000000000000011 	0.33500000000000002 	11 	
+1 	0.535000000000000031 	0.41499999999999998 	0.149999999999999994 	0.576500000000000012 	0.359499999999999986 	0.135000000000000009 	0.225000000000000006 	8 	
+2 	0.625 	0.455000000000000016 	0.170000000000000012 	1.08200000000000007 	0.495499999999999996 	0.234499999999999986 	0.315000000000000002 	9 	
+2 	0.589999999999999969 	0.46000000000000002 	0.154999999999999999 	0.906000000000000028 	0.327000000000000013 	0.148499999999999993 	0.33500000000000002 	15 	
+2 	0.505000000000000004 	0.400000000000000022 	0.135000000000000009 	0.722999999999999976 	0.377000000000000002 	0.148999999999999994 	0.177999999999999992 	7 	
+0 	0.474999999999999978 	0.380000000000000004 	0.14499999999999999 	0.569999999999999951 	0.16700000000000001 	0.117999999999999994 	0.187 	11 	
+2 	0.630000000000000004 	0.510000000000000009 	0.170000000000000012 	1.18849999999999989 	0.491499999999999992 	0.306499999999999995 	0.347999999999999976 	7 	
+0 	0.434999999999999998 	0.320000000000000007 	0.119999999999999996 	0.378500000000000003 	0.151999999999999996 	0.091499999999999998 	0.125 	11 	
+2 	0.594999999999999973 	0.484999999999999987 	0.149999999999999994 	1.08349999999999991 	0.530499999999999972 	0.231000000000000011 	0.276000000000000023 	8 	
+0 	0.400000000000000022 	0.325000000000000011 	0.119999999999999996 	0.318500000000000005 	0.134000000000000008 	0.0565000000000000016 	0.0950000000000000011 	8 	
+0 	0.530000000000000027 	0.385000000000000009 	0.125 	0.669499999999999984 	0.288999999999999979 	0.150999999999999995 	0.179999999999999993 	10 	
+2 	0.429999999999999993 	0.344999999999999973 	0.115000000000000005 	0.304499999999999993 	0.0924999999999999989 	0.0550000000000000003 	0.119999999999999996 	11 	
+0 	0.569999999999999951 	0.445000000000000007 	0.149999999999999994 	0.994999999999999996 	0.504000000000000004 	0.184999999999999998 	0.2505 	9 	
+2 	0.734999999999999987 	0.569999999999999951 	0.209999999999999992 	2.23550000000000004 	1.1705000000000001 	0.463000000000000023 	0.531499999999999972 	10 	
+0 	0.550000000000000044 	0.41499999999999998 	0.135000000000000009 	0.814500000000000002 	0.426999999999999991 	0.185499999999999998 	0.174999999999999989 	8 	
+0 	0.560000000000000053 	0.409999999999999976 	0.160000000000000003 	0.821500000000000008 	0.342000000000000026 	0.183999999999999997 	0.253000000000000003 	9 	
+0 	0.569999999999999951 	0.434999999999999998 	0.140000000000000013 	0.858500000000000041 	0.390500000000000014 	0.196000000000000008 	0.22950000000000001 	8 	
+0 	0.530000000000000027 	0.434999999999999998 	0.170000000000000012 	0.815500000000000003 	0.298499999999999988 	0.154999999999999999 	0.275000000000000022 	13 	
+2 	0.450000000000000011 	0.359999999999999987 	0.160000000000000003 	0.566999999999999948 	0.173999999999999988 	0.1245 	0.225000000000000006 	12 	
+1 	0.400000000000000022 	0.315000000000000002 	0.0899999999999999967 	0.324500000000000011 	0.150999999999999995 	0.0729999999999999954 	0.0879999999999999949 	8 	
+1 	0.434999999999999998 	0.325000000000000011 	0.104999999999999996 	0.33500000000000002 	0.13600000000000001 	0.0650000000000000022 	0.115000000000000005 	8 	
+0 	0.569999999999999951 	0.46000000000000002 	0.170000000000000012 	1.10000000000000009 	0.412499999999999978 	0.220500000000000002 	0.380000000000000004 	14 	
+1 	0.110000000000000001 	0.0899999999999999967 	0.0299999999999999989 	0.00800000000000000017 	0.00250000000000000005 	0.00200000000000000004 	0.00300000000000000006 	3 	
+2 	0.510000000000000009 	0.41499999999999998 	0.14499999999999999 	0.751000000000000001 	0.329500000000000015 	0.183499999999999996 	0.203000000000000014 	8 	
+1 	0.340000000000000024 	0.25 	0.0700000000000000067 	0.222500000000000003 	0.103999999999999995 	0.0425000000000000031 	0.0550000000000000003 	7 	
+2 	0.564999999999999947 	0.450000000000000011 	0.154999999999999999 	1.05950000000000011 	0.473499999999999976 	0.239999999999999991 	0.265000000000000013 	10 	
+1 	0.359999999999999987 	0.260000000000000009 	0.0800000000000000017 	0.179499999999999993 	0.0739999999999999963 	0.0315000000000000002 	0.0599999999999999978 	5 	
+0 	0.560000000000000053 	0.429999999999999993 	0.149999999999999994 	0.874500000000000055 	0.453000000000000014 	0.161000000000000004 	0.220000000000000001 	8 	
+0 	0.550000000000000044 	0.450000000000000011 	0.14499999999999999 	0.740999999999999992 	0.294999999999999984 	0.143499999999999989 	0.266500000000000015 	10 	
+0 	0.699999999999999956 	0.574999999999999956 	0.204999999999999988 	1.7975000000000001 	0.729500000000000037 	0.393500000000000016 	0.516499999999999959 	13 	
+2 	0.650000000000000022 	0.505000000000000004 	0.190000000000000002 	1.27400000000000002 	0.589999999999999969 	0.23000000000000001 	0.391000000000000014 	11 	
+2 	0.174999999999999989 	0.125 	0.0400000000000000008 	0.0240000000000000005 	0.00949999999999999976 	0.00600000000000000012 	0.0050000000000000001 	4 	
+2 	0.330000000000000016 	0.214999999999999997 	0.0749999999999999972 	0.114500000000000005 	0.0449999999999999983 	0.0264999999999999993 	0.0350000000000000033 	6 	
+1 	0.650000000000000022 	0.525000000000000022 	0.179999999999999993 	1.62599999999999989 	0.596999999999999975 	0.344499999999999973 	0.530000000000000027 	18 	
+2 	0.625 	0.474999999999999978 	0.160000000000000003 	1.08450000000000002 	0.500499999999999945 	0.235499999999999987 	0.310499999999999998 	10 	
+0 	0.450000000000000011 	0.380000000000000004 	0.165000000000000008 	0.816500000000000004 	0.25 	0.191500000000000004 	0.265000000000000013 	23 	
+0 	0.640000000000000013 	0.515000000000000013 	0.204999999999999988 	1.53350000000000009 	0.663499999999999979 	0.33450000000000002 	0.402500000000000024 	9 	
+2 	0.584999999999999964 	0.465000000000000024 	0.190000000000000002 	1.17100000000000004 	0.390500000000000014 	0.235499999999999987 	0.400000000000000022 	17 	
+2 	0.555000000000000049 	0.434999999999999998 	0.140000000000000013 	0.749500000000000055 	0.341000000000000025 	0.164500000000000007 	0.213999999999999996 	8 	
+0 	0.57999999999999996 	0.424999999999999989 	0.149999999999999994 	0.843999999999999972 	0.364499999999999991 	0.184999999999999998 	0.270500000000000018 	9 	
+0 	0.584999999999999964 	0.46000000000000002 	0.170000000000000012 	0.932499999999999996 	0.364999999999999991 	0.271000000000000019 	0.28999999999999998 	9 	
+2 	0.625 	0.479999999999999982 	0.170000000000000012 	1.35549999999999993 	0.671000000000000041 	0.268000000000000016 	0.338500000000000023 	10 	
+1 	0.57999999999999996 	0.440000000000000002 	0.14499999999999999 	0.79049999999999998 	0.35249999999999998 	0.164500000000000007 	0.241999999999999993 	10 	
+2 	0.375 	0.299999999999999989 	0.100000000000000006 	0.246499999999999997 	0.103999999999999995 	0.0475000000000000006 	0.0830000000000000043 	11 	
+0 	0.520000000000000018 	0.400000000000000022 	0.119999999999999996 	0.651499999999999968 	0.26100000000000001 	0.201500000000000012 	0.165000000000000008 	15 	
+1 	0.385000000000000009 	0.299999999999999989 	0.100000000000000006 	0.28949999999999998 	0.121499999999999997 	0.0630000000000000004 	0.0899999999999999967 	7 	
+0 	0.465000000000000024 	0.349999999999999978 	0.110000000000000001 	0.408499999999999974 	0.165000000000000008 	0.101999999999999993 	0.131000000000000005 	8 	
+0 	0.625 	0.515000000000000013 	0.160000000000000003 	1.26400000000000001 	0.571500000000000008 	0.326000000000000012 	0.321000000000000008 	9 	
+0 	0.604999999999999982 	0.489999999999999991 	0.149999999999999994 	1.13450000000000006 	0.526499999999999968 	0.264500000000000013 	0.294999999999999984 	9 	
+2 	0.714999999999999969 	0.564999999999999947 	0.174999999999999989 	1.9524999999999999 	0.764499999999999957 	0.418499999999999983 	0.413499999999999979 	10 	
+0 	0.450000000000000011 	0.354999999999999982 	0.104999999999999996 	0.522499999999999964 	0.236999999999999988 	0.116500000000000006 	0.14499999999999999 	8 	
+0 	0.589999999999999969 	0.46000000000000002 	0.14499999999999999 	0.990500000000000047 	0.453000000000000014 	0.220500000000000002 	0.275000000000000022 	8 	
+2 	0.630000000000000004 	0.484999999999999987 	0.154999999999999999 	1.27800000000000002 	0.637000000000000011 	0.275000000000000022 	0.309999999999999998 	8 	
+2 	0.359999999999999987 	0.294999999999999984 	0.100000000000000006 	0.210499999999999993 	0.0660000000000000031 	0.0524999999999999981 	0.0749999999999999972 	9 	
+2 	0.645000000000000018 	0.479999999999999982 	0.149999999999999994 	1.19199999999999995 	0.605500000000000038 	0.259500000000000008 	0.284999999999999976 	9 	
+2 	0.535000000000000031 	0.409999999999999976 	0.135000000000000009 	0.861999999999999988 	0.285499999999999976 	0.152499999999999997 	0.320000000000000007 	14 	
+0 	0.525000000000000022 	0.409999999999999976 	0.135000000000000009 	0.708500000000000019 	0.292999999999999983 	0.152499999999999997 	0.234999999999999987 	11 	
+0 	0.599999999999999978 	0.455000000000000016 	0.14499999999999999 	0.889499999999999957 	0.418999999999999984 	0.171500000000000014 	0.269000000000000017 	10 	
+0 	0.569999999999999951 	0.445000000000000007 	0.154999999999999999 	1.0169999999999999 	0.526499999999999968 	0.202500000000000013 	0.265000000000000013 	10 	
+1 	0.309999999999999998 	0.225000000000000006 	0.0500000000000000028 	0.14449999999999999 	0.0675000000000000044 	0.0384999999999999995 	0.0449999999999999983 	6 	
+0 	0.510000000000000009 	0.369999999999999996 	0.209999999999999992 	1.18300000000000005 	0.508000000000000007 	0.291999999999999982 	0.343000000000000027 	9 	
+0 	0.569999999999999951 	0.440000000000000002 	0.119999999999999996 	0.803000000000000047 	0.382000000000000006 	0.152499999999999997 	0.234000000000000014 	9 	
+2 	0.645000000000000018 	0.510000000000000009 	0.190000000000000002 	1.47449999999999992 	0.604999999999999982 	0.344999999999999973 	0.479999999999999982 	9 	
+2 	0.630000000000000004 	0.505000000000000004 	0.174999999999999989 	1.22100000000000009 	0.555000000000000049 	0.252000000000000002 	0.340000000000000024 	12 	
+2 	0.675000000000000044 	0.520000000000000018 	0.14499999999999999 	1.36450000000000005 	0.557000000000000051 	0.340500000000000025 	0.385000000000000009 	11 	
+0 	0.655000000000000027 	0.510000000000000009 	0.174999999999999989 	1.65250000000000008 	0.851500000000000035 	0.336500000000000021 	0.403000000000000025 	10 	
+2 	0.349999999999999978 	0.255000000000000004 	0.0800000000000000017 	0.191500000000000004 	0.0800000000000000017 	0.0384999999999999995 	0.0630000000000000004 	9 	
+0 	0.614999999999999991 	0.474999999999999978 	0.154999999999999999 	1.02699999999999991 	0.447000000000000008 	0.25 	0.284999999999999976 	9 	
+1 	0.33500000000000002 	0.260000000000000009 	0.0899999999999999967 	0.196500000000000008 	0.0874999999999999944 	0.0410000000000000017 	0.0560000000000000012 	7 	
+2 	0.57999999999999996 	0.450000000000000011 	0.14499999999999999 	1.00249999999999995 	0.547000000000000042 	0.197500000000000009 	0.22950000000000001 	8 	
+1 	0.450000000000000011 	0.359999999999999987 	0.130000000000000004 	0.47799999999999998 	0.191000000000000003 	0.127000000000000002 	0.137000000000000011 	7 	
+1 	0.46000000000000002 	0.354999999999999982 	0.140000000000000013 	0.493499999999999994 	0.215999999999999998 	0.133000000000000007 	0.115000000000000005 	13 	
+1 	0.520000000000000018 	0.380000000000000004 	0.140000000000000013 	0.525000000000000022 	0.177499999999999991 	0.115000000000000005 	0.184999999999999998 	11 	
+0 	0.665000000000000036 	0.510000000000000009 	0.174999999999999989 	1.38050000000000006 	0.675000000000000044 	0.298499999999999988 	0.325000000000000011 	10 	
+0 	0.599999999999999978 	0.474999999999999978 	0.149999999999999994 	1.12999999999999989 	0.574999999999999956 	0.196000000000000008 	0.304999999999999993 	9 	
+1 	0.494999999999999996 	0.375 	0.119999999999999996 	0.588999999999999968 	0.307499999999999996 	0.121499999999999997 	0.140500000000000014 	8 	
+1 	0.255000000000000004 	0.195000000000000007 	0.0550000000000000003 	0.072499999999999995 	0.028500000000000001 	0.0170000000000000012 	0.0210000000000000013 	4 	
+1 	0.429999999999999993 	0.325000000000000011 	0.100000000000000006 	0.364499999999999991 	0.157500000000000001 	0.0825000000000000039 	0.104999999999999996 	7 	
+2 	0.584999999999999964 	0.455000000000000016 	0.149999999999999994 	0.986999999999999988 	0.435499999999999998 	0.20749999999999999 	0.309999999999999998 	11 	
+0 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	1.10050000000000003 	0.541499999999999981 	0.166000000000000009 	0.265000000000000013 	8 	
+1 	0.354999999999999982 	0.270000000000000018 	0.0749999999999999972 	0.177499999999999991 	0.0790000000000000008 	0.0315000000000000002 	0.0539999999999999994 	6 	
+1 	0.244999999999999996 	0.195000000000000007 	0.0599999999999999978 	0.0950000000000000011 	0.0444999999999999979 	0.0245000000000000009 	0.0259999999999999988 	4 	
+0 	0.560000000000000053 	0.429999999999999993 	0.125 	0.802499999999999991 	0.313 	0.171500000000000014 	0.263000000000000012 	13 	
+0 	0.560000000000000053 	0.445000000000000007 	0.179999999999999993 	0.903000000000000025 	0.357499999999999984 	0.204499999999999987 	0.294999999999999984 	9 	
+1 	0.550000000000000044 	0.41499999999999998 	0.14499999999999999 	0.781499999999999972 	0.372999999999999998 	0.160000000000000003 	0.221500000000000002 	8 	
+0 	0.434999999999999998 	0.349999999999999978 	0.119999999999999996 	0.458500000000000019 	0.192000000000000004 	0.100000000000000006 	0.130000000000000004 	11 	
+1 	0.369999999999999996 	0.270000000000000018 	0.0950000000000000011 	0.232000000000000012 	0.132500000000000007 	0.0410000000000000017 	0.0614999999999999991 	6 	
+2 	0.589999999999999969 	0.455000000000000016 	0.154999999999999999 	0.885499999999999954 	0.388000000000000012 	0.188 	0.275000000000000022 	10 	
+0 	0.660000000000000031 	0.525000000000000022 	0.160000000000000003 	1.27699999999999991 	0.497499999999999998 	0.319000000000000006 	0.394000000000000017 	13 	
+0 	0.479999999999999982 	0.405000000000000027 	0.130000000000000004 	0.637499999999999956 	0.277000000000000024 	0.14449999999999999 	0.209999999999999992 	10 	
+2 	0.589999999999999969 	0.445000000000000007 	0.140000000000000013 	0.93100000000000005 	0.355999999999999983 	0.234000000000000014 	0.280000000000000027 	12 	
+0 	0.434999999999999998 	0.349999999999999978 	0.110000000000000001 	0.384000000000000008 	0.142999999999999988 	0.100500000000000006 	0.125 	13 	
+2 	0.57999999999999996 	0.489999999999999991 	0.130000000000000004 	1.13349999999999995 	0.585999999999999965 	0.256500000000000006 	0.236999999999999988 	9 	
+2 	0.625 	0.494999999999999996 	0.174999999999999989 	1.254 	0.581500000000000017 	0.285999999999999976 	0.318500000000000005 	9 	
+1 	0.270000000000000018 	0.195000000000000007 	0.0650000000000000022 	0.106499999999999997 	0.0475000000000000006 	0.0224999999999999992 	0.028500000000000001 	5 	
+0 	0.640000000000000013 	0.479999999999999982 	0.14499999999999999 	1.11450000000000005 	0.508000000000000007 	0.239999999999999991 	0.340000000000000024 	10 	
+0 	0.574999999999999956 	0.465000000000000024 	0.195000000000000007 	0.996500000000000052 	0.416999999999999982 	0.246999999999999997 	0.469999999999999973 	8 	
+1 	0.424999999999999989 	0.330000000000000016 	0.115000000000000005 	0.326500000000000012 	0.131500000000000006 	0.076999999999999999 	0.102999999999999994 	6 	
+2 	0.359999999999999987 	0.294999999999999984 	0.130000000000000004 	0.276500000000000024 	0.0894999999999999962 	0.0570000000000000021 	0.100500000000000006 	10 	
+1 	0.265000000000000013 	0.204999999999999988 	0.0700000000000000067 	0.105499999999999997 	0.0389999999999999999 	0.0410000000000000017 	0.0350000000000000033 	5 	
+1 	0.484999999999999987 	0.375 	0.130000000000000004 	0.602500000000000036 	0.293499999999999983 	0.128500000000000003 	0.160000000000000003 	7 	
+2 	0.645000000000000018 	0.515000000000000013 	0.174999999999999989 	1.61149999999999993 	0.674499999999999988 	0.384000000000000008 	0.385000000000000009 	14 	
+2 	0.604999999999999982 	0.474999999999999978 	0.174999999999999989 	1.25249999999999995 	0.557499999999999996 	0.305499999999999994 	0.343000000000000027 	9 	
+1 	0.574999999999999956 	0.455000000000000016 	0.179999999999999993 	0.852500000000000036 	0.30149999999999999 	0.182499999999999996 	0.299999999999999989 	13 	
+0 	0.515000000000000013 	0.395000000000000018 	0.165000000000000008 	0.75649999999999995 	0.190500000000000003 	0.170000000000000012 	0.320500000000000007 	10 	
+1 	0.489999999999999991 	0.364999999999999991 	0.125 	0.558499999999999996 	0.252000000000000002 	0.126000000000000001 	0.161500000000000005 	10 	
+0 	0.604999999999999982 	0.484999999999999987 	0.160000000000000003 	1.05649999999999999 	0.369999999999999996 	0.235499999999999987 	0.354999999999999982 	10 	
+1 	0.515000000000000013 	0.395000000000000018 	0.125 	0.663499999999999979 	0.320000000000000007 	0.140000000000000013 	0.170000000000000012 	8 	
+1 	0.550000000000000044 	0.450000000000000011 	0.130000000000000004 	0.804000000000000048 	0.337500000000000022 	0.140500000000000014 	0.23000000000000001 	6 	
+0 	0.604999999999999982 	0.515000000000000013 	0.170000000000000012 	1.28899999999999992 	0.599999999999999978 	0.294499999999999984 	0.331500000000000017 	9 	
+1 	0.440000000000000002 	0.340000000000000024 	0.100000000000000006 	0.379000000000000004 	0.172499999999999987 	0.081500000000000003 	0.101000000000000006 	7 	
+0 	0.530000000000000027 	0.455000000000000016 	0.165000000000000008 	0.980500000000000038 	0.315500000000000003 	0.281499999999999972 	0.296499999999999986 	11 	
+1 	0.28999999999999998 	0.214999999999999997 	0.0650000000000000022 	0.0985000000000000042 	0.0425000000000000031 	0.0210000000000000013 	0.0309999999999999998 	5 	
+1 	0.494999999999999996 	0.375 	0.119999999999999996 	0.61399999999999999 	0.285499999999999976 	0.13650000000000001 	0.161000000000000004 	8 	
+1 	0.369999999999999996 	0.275000000000000022 	0.100000000000000006 	0.220000000000000001 	0.0940000000000000002 	0.0449999999999999983 	0.0650000000000000022 	7 	
+0 	0.540000000000000036 	0.434999999999999998 	0.174999999999999989 	0.892000000000000015 	0.322000000000000008 	0.173999999999999988 	0.33500000000000002 	13 	
+0 	0.719999999999999973 	0.550000000000000044 	0.195000000000000007 	2.07299999999999995 	1.0714999999999999 	0.42649999999999999 	0.501499999999999946 	9 	
+1 	0.535000000000000031 	0.450000000000000011 	0.170000000000000012 	0.781000000000000028 	0.305499999999999994 	0.155499999999999999 	0.294999999999999984 	11 	
+2 	0.680000000000000049 	0.520000000000000018 	0.165000000000000008 	1.47750000000000004 	0.723999999999999977 	0.279000000000000026 	0.406000000000000028 	11 	
+0 	0.640000000000000013 	0.494999999999999996 	0.170000000000000012 	1.22649999999999992 	0.489999999999999991 	0.377000000000000002 	0.287499999999999978 	11 	
+1 	0.424999999999999989 	0.325000000000000011 	0.100000000000000006 	0.39800000000000002 	0.118499999999999994 	0.0645000000000000018 	0.0945000000000000007 	6 	
+2 	0.630000000000000004 	0.484999999999999987 	0.160000000000000003 	1.2430000000000001 	0.622999999999999998 	0.275000000000000022 	0.299999999999999989 	10 	
+1 	0.299999999999999989 	0.23000000000000001 	0.0800000000000000017 	0.127500000000000002 	0.043499999999999997 	0.0264999999999999993 	0.0400000000000000008 	8 	
+1 	0.320000000000000007 	0.225000000000000006 	0.0850000000000000061 	0.141499999999999987 	0.0675000000000000044 	0.0294999999999999984 	0.0405000000000000013 	6 	
+0 	0.614999999999999991 	0.525000000000000022 	0.154999999999999999 	1.03849999999999998 	0.426999999999999991 	0.231500000000000011 	0.344999999999999973 	11 	
+0 	0.665000000000000036 	0.5 	0.149999999999999994 	1.24750000000000005 	0.462500000000000022 	0.295499999999999985 	0.359499999999999986 	10 	
+0 	0.574999999999999956 	0.450000000000000011 	0.160000000000000003 	0.977500000000000036 	0.313500000000000001 	0.231000000000000011 	0.330000000000000016 	12 	
+0 	0.57999999999999996 	0.450000000000000011 	0.234999999999999987 	1.07099999999999995 	0.299999999999999989 	0.205999999999999989 	0.395000000000000018 	14 	
+0 	0.57999999999999996 	0.465000000000000024 	0.165000000000000008 	1.10149999999999992 	0.404000000000000026 	0.209499999999999992 	0.349999999999999978 	11 	
+1 	0.419999999999999984 	0.320000000000000007 	0.110000000000000001 	0.362499999999999989 	0.173999999999999988 	0.0635000000000000009 	0.104999999999999996 	7 	
+2 	0.564999999999999947 	0.455000000000000016 	0.154999999999999999 	0.935499999999999998 	0.420999999999999985 	0.182999999999999996 	0.260000000000000009 	11 	
+0 	0.625 	0.505000000000000004 	0.174999999999999989 	1.14999999999999991 	0.547499999999999987 	0.256000000000000005 	0.304499999999999993 	11 	
+2 	0.489999999999999991 	0.385000000000000009 	0.125 	0.649000000000000021 	0.320000000000000007 	0.123999999999999999 	0.169500000000000012 	8 	
+2 	0.594999999999999973 	0.46000000000000002 	0.170000000000000012 	1.12949999999999995 	0.569999999999999951 	0.255500000000000005 	0.265000000000000013 	10 	
+1 	0.550000000000000044 	0.434999999999999998 	0.140000000000000013 	0.799499999999999988 	0.294999999999999984 	0.190500000000000003 	0.237999999999999989 	10 	
+0 	0.560000000000000053 	0.424999999999999989 	0.125 	0.932000000000000051 	0.360999999999999988 	0.212999999999999995 	0.33500000000000002 	9 	
+2 	0.609999999999999987 	0.479999999999999982 	0.165000000000000008 	1.24350000000000005 	0.557499999999999996 	0.267500000000000016 	0.371999999999999997 	8 	
+1 	0.405000000000000027 	0.304999999999999993 	0.104999999999999996 	0.362499999999999989 	0.1565 	0.0704999999999999932 	0.125 	10 	
+2 	0.510000000000000009 	0.409999999999999976 	0.14499999999999999 	0.796000000000000041 	0.38650000000000001 	0.181499999999999995 	0.195500000000000007 	8 	
+0 	0.520000000000000018 	0.429999999999999993 	0.149999999999999994 	0.72799999999999998 	0.301999999999999991 	0.157500000000000001 	0.234999999999999987 	11 	
+0 	0.469999999999999973 	0.359999999999999987 	0.100000000000000006 	0.470499999999999974 	0.163500000000000006 	0.0889999999999999958 	0.138500000000000012 	8 	
+2 	0.699999999999999956 	0.530000000000000027 	0.190000000000000002 	1.31850000000000001 	0.548000000000000043 	0.233000000000000013 	0.419999999999999984 	18 	
+2 	0.655000000000000027 	0.535000000000000031 	0.204999999999999988 	1.64450000000000007 	0.730500000000000038 	0.359499999999999986 	0.46000000000000002 	13 	
+1 	0.465000000000000024 	0.359999999999999987 	0.104999999999999996 	0.497999999999999998 	0.213999999999999996 	0.116000000000000006 	0.140000000000000013 	15 	
+0 	0.604999999999999982 	0.494999999999999996 	0.170000000000000012 	1.23849999999999993 	0.528000000000000025 	0.246499999999999997 	0.390000000000000013 	14 	
+0 	0.535000000000000031 	0.434999999999999998 	0.160000000000000003 	0.810499999999999998 	0.315500000000000003 	0.179499999999999993 	0.239999999999999991 	10 	
+0 	0.479999999999999982 	0.364999999999999991 	0.135000000000000009 	0.639499999999999957 	0.294499999999999984 	0.113000000000000003 	0.174999999999999989 	8 	
+2 	0.599999999999999978 	0.474999999999999978 	0.170000000000000012 	1.13149999999999995 	0.508000000000000007 	0.27200000000000002 	0.308999999999999997 	10 	
+2 	0.489999999999999991 	0.380000000000000004 	0.135000000000000009 	0.541499999999999981 	0.217499999999999999 	0.0950000000000000011 	0.190000000000000002 	11 	
+0 	0.564999999999999947 	0.489999999999999991 	0.154999999999999999 	0.924499999999999988 	0.405000000000000027 	0.219500000000000001 	0.255000000000000004 	11 	
+0 	0.67000000000000004 	0.550000000000000044 	0.154999999999999999 	1.56600000000000006 	0.857999999999999985 	0.339000000000000024 	0.353999999999999981 	10 	
+2 	0.719999999999999973 	0.574999999999999956 	0.214999999999999997 	2.17300000000000004 	0.951500000000000012 	0.563999999999999946 	0.536499999999999977 	12 	
+0 	0.525000000000000022 	0.440000000000000002 	0.149999999999999994 	0.842500000000000027 	0.368499999999999994 	0.19850000000000001 	0.239999999999999991 	12 	
+0 	0.650000000000000022 	0.5 	0.184999999999999998 	1.4415 	0.740999999999999992 	0.295499999999999985 	0.341000000000000025 	9 	
+0 	0.619999999999999996 	0.479999999999999982 	0.165000000000000008 	1.01249999999999996 	0.532499999999999973 	0.436499999999999999 	0.32400000000000001 	10 	
+0 	0.719999999999999973 	0.574999999999999956 	0.170000000000000012 	1.9335 	0.913000000000000034 	0.389000000000000012 	0.510000000000000009 	13 	
+0 	0.594999999999999973 	0.465000000000000024 	0.154999999999999999 	1.02600000000000002 	0.464500000000000024 	0.112000000000000002 	0.304999999999999993 	12 	
+1 	0.41499999999999998 	0.309999999999999998 	0.110000000000000001 	0.296499999999999986 	0.122999999999999998 	0.0570000000000000021 	0.0995000000000000051 	10 	
+0 	0.645000000000000018 	0.525000000000000022 	0.200000000000000011 	1.44900000000000007 	0.600999999999999979 	0.256500000000000006 	0.505000000000000004 	13 	
+2 	0.635000000000000009 	0.494999999999999996 	0.195000000000000007 	1.17199999999999993 	0.445000000000000007 	0.311499999999999999 	0.347499999999999976 	11 	
+1 	0.619999999999999996 	0.484999999999999987 	0.170000000000000012 	1.20799999999999996 	0.480499999999999983 	0.304499999999999993 	0.330000000000000016 	15 	
+2 	0.479999999999999982 	0.375 	0.14499999999999999 	0.777000000000000024 	0.215999999999999998 	0.130000000000000004 	0.170000000000000012 	9 	
+2 	0.635000000000000009 	0.515000000000000013 	0.165000000000000008 	1.22900000000000009 	0.505499999999999949 	0.297499999999999987 	0.353499999999999981 	10 	
+1 	0.604999999999999982 	0.479999999999999982 	0.154999999999999999 	0.999500000000000055 	0.424999999999999989 	0.19850000000000001 	0.299999999999999989 	10 	
+1 	0.535000000000000031 	0.419999999999999984 	0.14499999999999999 	0.688500000000000001 	0.27300000000000002 	0.151499999999999996 	0.236999999999999988 	9 	
+1 	0.434999999999999998 	0.344999999999999973 	0.119999999999999996 	0.447500000000000009 	0.221000000000000002 	0.112000000000000002 	0.125 	7 	
+0 	0.675000000000000044 	0.525000000000000022 	0.154999999999999999 	1.47849999999999993 	0.628000000000000003 	0.340500000000000025 	0.419999999999999984 	9 	
+0 	0.489999999999999991 	0.385000000000000009 	0.125 	0.53949999999999998 	0.217499999999999999 	0.128000000000000003 	0.165000000000000008 	11 	
+2 	0.719999999999999973 	0.569999999999999951 	0.200000000000000011 	1.8274999999999999 	0.919000000000000039 	0.365999999999999992 	0.484999999999999987 	10 	
+0 	0.625 	0.484999999999999987 	0.174999999999999989 	1.37450000000000006 	0.733500000000000041 	0.271500000000000019 	0.332000000000000017 	9 	
+2 	0.489999999999999991 	0.465000000000000024 	0.125 	0.522499999999999964 	0.234999999999999987 	0.130000000000000004 	0.140999999999999986 	7 	
+1 	0.489999999999999991 	0.375 	0.125 	0.544499999999999984 	0.279000000000000026 	0.115000000000000005 	0.130000000000000004 	8 	
+2 	0.635000000000000009 	0.510000000000000009 	0.170000000000000012 	1.35549999999999993 	0.618999999999999995 	0.304999999999999993 	0.390000000000000013 	9 	
+0 	0.619999999999999996 	0.474999999999999978 	0.160000000000000003 	1.12949999999999995 	0.463000000000000023 	0.268500000000000016 	0.330000000000000016 	10 	
+2 	0.614999999999999991 	0.489999999999999991 	0.170000000000000012 	1.14500000000000002 	0.491499999999999992 	0.20799999999999999 	0.343000000000000027 	13 	
+2 	0.390000000000000013 	0.284999999999999976 	0.0950000000000000011 	0.271000000000000019 	0.110000000000000001 	0.0599999999999999978 	0.0800000000000000017 	8 	
+0 	0.609999999999999987 	0.484999999999999987 	0.209999999999999992 	1.34450000000000003 	0.535000000000000031 	0.220500000000000002 	0.515000000000000013 	20 	
+0 	0.604999999999999982 	0.484999999999999987 	0.160000000000000003 	1.20100000000000007 	0.416999999999999982 	0.287499999999999978 	0.380000000000000004 	9 	
+1 	0.369999999999999996 	0.28999999999999998 	0.0950000000000000011 	0.248999999999999999 	0.104499999999999996 	0.0580000000000000029 	0.067000000000000004 	6 	
+1 	0.400000000000000022 	0.309999999999999998 	0.100000000000000006 	0.305999999999999994 	0.130000000000000004 	0.0599999999999999978 	0.0940000000000000002 	6 	
+1 	0.315000000000000002 	0.234999999999999987 	0.0550000000000000003 	0.150999999999999995 	0.0650000000000000022 	0.0269999999999999997 	0.0389999999999999999 	6 	
+2 	0.530000000000000027 	0.434999999999999998 	0.135000000000000009 	0.736500000000000044 	0.327500000000000013 	0.131500000000000006 	0.220000000000000001 	12 	
+2 	0.450000000000000011 	0.320000000000000007 	0.100000000000000006 	0.381000000000000005 	0.170500000000000013 	0.0749999999999999972 	0.115000000000000005 	9 	
+2 	0.650000000000000022 	0.525000000000000022 	0.190000000000000002 	1.61250000000000004 	0.777000000000000024 	0.368499999999999994 	0.396500000000000019 	11 	
+0 	0.614999999999999991 	0.5 	0.165000000000000008 	1.32699999999999996 	0.599999999999999978 	0.30149999999999999 	0.354999999999999982 	10 	
+2 	0.46000000000000002 	0.380000000000000004 	0.135000000000000009 	0.481999999999999984 	0.20699999999999999 	0.122499999999999998 	0.14499999999999999 	10 	
+0 	0.734999999999999987 	0.599999999999999978 	0.220000000000000001 	2.55500000000000016 	1.13349999999999995 	0.440000000000000002 	0.599999999999999978 	11 	
+2 	0.589999999999999969 	0.469999999999999973 	0.179999999999999993 	1.12349999999999994 	0.420499999999999985 	0.280500000000000027 	0.359999999999999987 	13 	
+0 	0.584999999999999964 	0.434999999999999998 	0.174999999999999989 	0.981999999999999984 	0.405500000000000027 	0.2495 	0.270000000000000018 	10 	
+2 	0.689999999999999947 	0.505000000000000004 	0.200000000000000011 	1.87200000000000011 	0.893000000000000016 	0.401500000000000024 	0.479999999999999982 	10 	
+2 	0.474999999999999978 	0.385000000000000009 	0.14499999999999999 	0.617500000000000049 	0.234999999999999987 	0.107999999999999999 	0.214999999999999997 	14 	
+2 	0.665000000000000036 	0.525000000000000022 	0.160000000000000003 	1.36299999999999999 	0.629000000000000004 	0.279000000000000026 	0.340000000000000024 	8 	
+1 	0.28999999999999998 	0.209999999999999992 	0.0650000000000000022 	0.0970000000000000029 	0.0374999999999999986 	0.0219999999999999987 	0.0299999999999999989 	6 	
+2 	0.569999999999999951 	0.445000000000000007 	0.140000000000000013 	1.06349999999999989 	0.526499999999999968 	0.219500000000000001 	0.239999999999999991 	8 	
+1 	0.325000000000000011 	0.239999999999999991 	0.0749999999999999972 	0.187 	0.0825000000000000039 	0.0444999999999999979 	0.0500000000000000028 	6 	
+1 	0.515000000000000013 	0.349999999999999978 	0.104999999999999996 	0.474499999999999977 	0.212999999999999995 	0.122999999999999998 	0.127500000000000002 	10 	
+1 	0.239999999999999991 	0.174999999999999989 	0.0449999999999999983 	0.0700000000000000067 	0.0315000000000000002 	0.0235000000000000001 	0.0200000000000000004 	5 	
+0 	0.424999999999999989 	0.33500000000000002 	0.0950000000000000011 	0.322000000000000008 	0.120499999999999996 	0.0609999999999999987 	0.125 	10 	
+0 	0.604999999999999982 	0.484999999999999987 	0.165000000000000008 	1.01049999999999995 	0.434999999999999998 	0.208999999999999991 	0.299999999999999989 	19 	
+2 	0.515000000000000013 	0.424999999999999989 	0.14499999999999999 	0.936499999999999999 	0.496999999999999997 	0.180999999999999994 	0.2185 	8 	
+2 	0.564999999999999947 	0.440000000000000002 	0.174999999999999989 	0.902499999999999969 	0.309999999999999998 	0.193000000000000005 	0.325000000000000011 	14 	
+2 	0.604999999999999982 	0.489999999999999991 	0.179999999999999993 	1.22700000000000009 	0.479999999999999982 	0.286999999999999977 	0.349999999999999978 	18 	
+2 	0.675000000000000044 	0.525000000000000022 	0.160000000000000003 	1.28350000000000009 	0.571999999999999953 	0.275500000000000023 	0.354499999999999982 	13 	
+0 	0.70499999999999996 	0.569999999999999951 	0.179999999999999993 	1.53449999999999998 	0.959999999999999964 	0.419499999999999984 	0.429999999999999993 	12 	
+1 	0.57999999999999996 	0.445000000000000007 	0.149999999999999994 	0.886499999999999955 	0.383000000000000007 	0.208999999999999991 	0.255000000000000004 	11 	
+2 	0.619999999999999996 	0.469999999999999973 	0.135000000000000009 	1.01950000000000007 	0.531499999999999972 	0.200500000000000012 	0.247499999999999998 	8 	
+0 	0.724999999999999978 	0.560000000000000053 	0.209999999999999992 	2.14100000000000001 	0.650000000000000022 	0.39800000000000002 	1.00499999999999989 	18 	
+0 	0.550000000000000044 	0.440000000000000002 	0.149999999999999994 	0.894499999999999962 	0.314500000000000002 	0.150999999999999995 	0.320000000000000007 	19 	
+1 	0.589999999999999969 	0.469999999999999973 	0.14499999999999999 	0.973999999999999977 	0.453000000000000014 	0.235999999999999988 	0.288999999999999979 	8 	
+2 	0.450000000000000011 	0.349999999999999978 	0.130000000000000004 	0.46000000000000002 	0.173999999999999988 	0.111000000000000001 	0.135000000000000009 	8 	
+2 	0.67000000000000004 	0.525000000000000022 	0.195000000000000007 	1.44049999999999989 	0.659499999999999975 	0.267500000000000016 	0.424999999999999989 	9 	
+0 	0.450000000000000011 	0.359999999999999987 	0.125 	0.4995 	0.203499999999999986 	0.100000000000000006 	0.170000000000000012 	13 	
+2 	0.609999999999999987 	0.479999999999999982 	0.149999999999999994 	1.19999999999999996 	0.560000000000000053 	0.245499999999999996 	0.280000000000000027 	14 	
+1 	0.604999999999999982 	0.489999999999999991 	0.165000000000000008 	1.07099999999999995 	0.481999999999999984 	0.193500000000000005 	0.35199999999999998 	10 	
+2 	0.680000000000000049 	0.540000000000000036 	0.209999999999999992 	1.78849999999999998 	0.83450000000000002 	0.407999999999999974 	0.437 	13 	
+0 	0.520000000000000018 	0.395000000000000018 	0.179999999999999993 	0.640000000000000013 	0.158000000000000002 	0.110000000000000001 	0.244999999999999996 	22 	
+1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.204999999999999988 	0.0779999999999999999 	0.0485000000000000014 	0.0700000000000000067 	7 	
+0 	0.5 	0.385000000000000009 	0.104999999999999996 	0.497999999999999998 	0.179499999999999993 	0.1095 	0.170000000000000012 	17 	
+1 	0.550000000000000044 	0.424999999999999989 	0.149999999999999994 	0.766499999999999959 	0.339000000000000024 	0.17599999999999999 	0.209999999999999992 	8 	
+1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.313 	0.139000000000000012 	0.0625 	0.0965000000000000024 	7 	
+1 	0.479999999999999982 	0.369999999999999996 	0.119999999999999996 	0.514000000000000012 	0.20749999999999999 	0.131000000000000005 	0.154999999999999999 	13 	
+2 	0.455000000000000016 	0.340000000000000024 	0.135000000000000009 	0.462000000000000022 	0.16750000000000001 	0.158000000000000002 	0.119999999999999996 	9 	
+2 	0.584999999999999964 	0.429999999999999993 	0.160000000000000003 	0.95499999999999996 	0.362499999999999989 	0.17599999999999999 	0.270000000000000018 	11 	
+0 	0.569999999999999951 	0.429999999999999993 	0.160000000000000003 	0.811000000000000054 	0.387500000000000011 	0.159000000000000002 	0.228500000000000009 	9 	
+1 	0.474999999999999978 	0.375 	0.110000000000000001 	0.493999999999999995 	0.210999999999999993 	0.109 	0.154499999999999998 	8 	
+2 	0.584999999999999964 	0.465000000000000024 	0.165000000000000008 	0.885000000000000009 	0.402500000000000024 	0.162500000000000006 	0.274000000000000021 	10 	
+1 	0.395000000000000018 	0.299999999999999989 	0.0899999999999999967 	0.253000000000000003 	0.115500000000000005 	0.0500000000000000028 	0.0749999999999999972 	6 	
+1 	0.530000000000000027 	0.41499999999999998 	0.130000000000000004 	0.69399999999999995 	0.390500000000000014 	0.111000000000000001 	0.16700000000000001 	9 	
+1 	0.450000000000000011 	0.33500000000000002 	0.104999999999999996 	0.447000000000000008 	0.233500000000000013 	0.152999999999999997 	0.118999999999999995 	7 	
+0 	0.645000000000000018 	0.510000000000000009 	0.160000000000000003 	1.24150000000000005 	0.581500000000000017 	0.276000000000000023 	0.315000000000000002 	9 	
+0 	0.650000000000000022 	0.525000000000000022 	0.174999999999999989 	1.4225000000000001 	0.609999999999999987 	0.299499999999999988 	0.445000000000000007 	20 	
+1 	0.525000000000000022 	0.405000000000000027 	0.149999999999999994 	0.79500000000000004 	0.307499999999999996 	0.204999999999999988 	0.255000000000000004 	14 	
+1 	0.515000000000000013 	0.385000000000000009 	0.125 	0.571999999999999953 	0.236999999999999988 	0.143499999999999989 	0.165000000000000008 	7 	
+2 	0.665000000000000036 	0.535000000000000031 	0.225000000000000006 	2.1835 	0.753499999999999948 	0.391000000000000014 	0.885000000000000009 	27 	
+2 	0.584999999999999964 	0.46000000000000002 	0.14499999999999999 	0.933499999999999996 	0.47799999999999998 	0.182499999999999996 	0.234999999999999987 	9 	
+0 	0.589999999999999969 	0.474999999999999978 	0.160000000000000003 	1.10149999999999992 	0.47749999999999998 	0.255500000000000005 	0.294999999999999984 	13 	
+2 	0.699999999999999956 	0.564999999999999947 	0.174999999999999989 	1.85650000000000004 	0.844500000000000028 	0.393500000000000016 	0.540000000000000036 	10 	
+1 	0.299999999999999989 	0.23000000000000001 	0.0950000000000000011 	0.138500000000000012 	0.0560000000000000012 	0.0364999999999999977 	0.0369999999999999982 	6 	
+0 	0.680000000000000049 	0.560000000000000053 	0.195000000000000007 	1.77750000000000008 	0.860999999999999988 	0.322000000000000008 	0.41499999999999998 	11 	
+0 	0.535000000000000031 	0.400000000000000022 	0.149999999999999994 	0.804499999999999993 	0.33450000000000002 	0.212499999999999994 	0.209999999999999992 	13 	
+0 	0.689999999999999947 	0.550000000000000044 	0.200000000000000011 	1.56899999999999995 	0.687000000000000055 	0.367499999999999993 	0.46000000000000002 	12 	
+1 	0.589999999999999969 	0.450000000000000011 	0.14499999999999999 	1.02200000000000002 	0.427999999999999992 	0.268000000000000016 	0.265000000000000013 	10 	
+1 	0.564999999999999947 	0.429999999999999993 	0.149999999999999994 	0.821500000000000008 	0.332000000000000017 	0.168500000000000011 	0.28999999999999998 	11 	
+2 	0.594999999999999973 	0.479999999999999982 	0.165000000000000008 	1.26200000000000001 	0.483499999999999985 	0.282999999999999974 	0.409999999999999976 	17 	
+2 	0.380000000000000004 	0.234999999999999987 	0.100000000000000006 	0.258000000000000007 	0.105499999999999997 	0.0539999999999999994 	0.0800000000000000017 	7 	
+0 	0.719999999999999973 	0.550000000000000044 	0.179999999999999993 	1.52000000000000002 	0.637000000000000011 	0.325000000000000011 	0.434999999999999998 	10 	
+2 	0.699999999999999956 	0.535000000000000031 	0.160000000000000003 	1.72550000000000003 	0.630000000000000004 	0.263500000000000012 	0.540000000000000036 	19 	
+2 	0.630000000000000004 	0.479999999999999982 	0.14499999999999999 	1.01150000000000007 	0.423499999999999988 	0.236999999999999988 	0.304999999999999993 	12 	
+1 	0.520000000000000018 	0.405000000000000027 	0.140000000000000013 	0.577500000000000013 	0.200000000000000011 	0.14499999999999999 	0.178999999999999992 	11 	
+1 	0.469999999999999973 	0.349999999999999978 	0.135000000000000009 	0.566999999999999948 	0.231500000000000011 	0.146499999999999991 	0.152499999999999997 	11 	
+2 	0.734999999999999987 	0.589999999999999969 	0.204999999999999988 	2.08700000000000019 	0.90900000000000003 	0.473999999999999977 	0.625 	12 	
+1 	0.505000000000000004 	0.385000000000000009 	0.119999999999999996 	0.600500000000000034 	0.23899999999999999 	0.141999999999999987 	0.184999999999999998 	7 	
+1 	0.395000000000000018 	0.294999999999999984 	0.100000000000000006 	0.292999999999999983 	0.140000000000000013 	0.0619999999999999996 	0.0820000000000000034 	7 	
+2 	0.619999999999999996 	0.479999999999999982 	0.149999999999999994 	1.10149999999999992 	0.496499999999999997 	0.242999999999999994 	0.304999999999999993 	10 	
+1 	0.555000000000000049 	0.424999999999999989 	0.14499999999999999 	0.79049999999999998 	0.348499999999999976 	0.17649999999999999 	0.225000000000000006 	9 	
+1 	0.555000000000000049 	0.424999999999999989 	0.179999999999999993 	0.875 	0.369499999999999995 	0.200500000000000012 	0.255000000000000004 	11 	
+1 	0.184999999999999998 	0.375 	0.119999999999999996 	0.464500000000000024 	0.196000000000000008 	0.104499999999999996 	0.149999999999999994 	6 	
+2 	0.489999999999999991 	0.380000000000000004 	0.140000000000000013 	0.638499999999999956 	0.23050000000000001 	0.141999999999999987 	0.195000000000000007 	13 	
+0 	0.349999999999999978 	0.265000000000000013 	0.0899999999999999967 	0.185499999999999998 	0.0744999999999999968 	0.0415000000000000022 	0.0599999999999999978 	7 	
+0 	0.729999999999999982 	0.57999999999999996 	0.190000000000000002 	1.73750000000000004 	0.678499999999999992 	0.434499999999999997 	0.520000000000000018 	11 	
+1 	0.239999999999999991 	0.179999999999999993 	0.0550000000000000003 	0.0555000000000000007 	0.0235000000000000001 	0.0129999999999999994 	0.0179999999999999986 	4 	
+1 	0.604999999999999982 	0.469999999999999973 	0.14499999999999999 	0.802499999999999991 	0.379000000000000004 	0.226500000000000007 	0.220000000000000001 	9 	
+2 	0.550000000000000044 	0.419999999999999984 	0.160000000000000003 	1.34050000000000002 	0.632499999999999951 	0.310999999999999999 	0.343999999999999972 	10 	
+2 	0.520000000000000018 	0.400000000000000022 	0.104999999999999996 	0.871999999999999997 	0.451500000000000012 	0.161500000000000005 	0.19850000000000001 	9 	
+1 	0.54500000000000004 	0.419999999999999984 	0.125 	0.716999999999999971 	0.357999999999999985 	0.112000000000000002 	0.220000000000000001 	8 	
+2 	0.699999999999999956 	0.525000000000000022 	0.174999999999999989 	1.75849999999999995 	0.874500000000000055 	0.361499999999999988 	0.469999999999999973 	10 	
+0 	0.625 	0.489999999999999991 	0.154999999999999999 	1.11499999999999999 	0.483999999999999986 	0.277000000000000024 	0.309499999999999997 	9 	
+0 	0.67000000000000004 	0.540000000000000036 	0.195000000000000007 	1.61899999999999999 	0.739999999999999991 	0.330500000000000016 	0.465000000000000024 	11 	
+2 	0.689999999999999947 	0.550000000000000044 	0.200000000000000011 	1.84650000000000003 	0.731999999999999984 	0.471999999999999975 	0.569999999999999951 	19 	
+2 	0.619999999999999996 	0.494999999999999996 	0.179999999999999993 	1.25550000000000006 	0.576500000000000012 	0.254000000000000004 	0.354999999999999982 	12 	
+2 	0.604999999999999982 	0.489999999999999991 	0.14499999999999999 	1.30000000000000004 	0.517000000000000015 	0.328500000000000014 	0.309999999999999998 	14 	
+2 	0.680000000000000049 	0.530000000000000027 	0.179999999999999993 	1.52899999999999991 	0.763499999999999956 	0.311499999999999999 	0.402500000000000024 	11 	
+1 	0.569999999999999951 	0.445000000000000007 	0.154999999999999999 	0.866999999999999993 	0.370499999999999996 	0.170500000000000013 	0.280000000000000027 	9 	
+1 	0.525000000000000022 	0.41499999999999998 	0.119999999999999996 	0.595999999999999974 	0.280500000000000027 	0.119999999999999996 	0.169500000000000012 	9 	
+2 	0.625 	0.489999999999999991 	0.165000000000000008 	1.11650000000000005 	0.489499999999999991 	0.26150000000000001 	0.332500000000000018 	11 	
+1 	0.385000000000000009 	0.299999999999999989 	0.125 	0.343000000000000027 	0.170500000000000013 	0.0734999999999999959 	0.0810000000000000026 	7 	
+1 	0.309999999999999998 	0.214999999999999997 	0.0749999999999999972 	0.127500000000000002 	0.0565000000000000016 	0.0275000000000000001 	0.0359999999999999973 	7 	
+1 	0.429999999999999993 	0.340000000000000024 	0.100000000000000006 	0.340500000000000025 	0.139500000000000013 	0.0665000000000000036 	0.119999999999999996 	8 	
+2 	0.520000000000000018 	0.419999999999999984 	0.160000000000000003 	0.744999999999999996 	0.255000000000000004 	0.157000000000000001 	0.288499999999999979 	11 	
+1 	0.604999999999999982 	0.494999999999999996 	0.14499999999999999 	1.05400000000000005 	0.368999999999999995 	0.225500000000000006 	0.359999999999999987 	12 	
+2 	0.400000000000000022 	0.299999999999999989 	0.125 	0.416999999999999982 	0.191000000000000003 	0.0899999999999999967 	0.117499999999999993 	9 	
+0 	0.474999999999999978 	0.375 	0.125 	0.578500000000000014 	0.277500000000000024 	0.0850000000000000061 	0.154999999999999999 	10 	
+1 	0.41499999999999998 	0.315000000000000002 	0.104999999999999996 	0.330000000000000016 	0.140500000000000014 	0.0704999999999999932 	0.0950000000000000011 	6 	
+2 	0.625 	0.494999999999999996 	0.174999999999999989 	1.20750000000000002 	0.531000000000000028 	0.281000000000000028 	0.35249999999999998 	11 	
+2 	0.665000000000000036 	0.535000000000000031 	0.195000000000000007 	1.60600000000000009 	0.575500000000000012 	0.388000000000000012 	0.479999999999999982 	14 	
+1 	0.364999999999999991 	0.275000000000000022 	0.0850000000000000061 	0.223000000000000004 	0.0980000000000000038 	0.0374999999999999986 	0.0749999999999999972 	7 	
+2 	0.625 	0.489999999999999991 	0.165000000000000008 	1.20500000000000007 	0.51749999999999996 	0.310499999999999998 	0.346499999999999975 	10 	
+2 	0.54500000000000004 	0.41499999999999998 	0.140000000000000013 	0.819999999999999951 	0.461500000000000021 	0.127000000000000002 	0.217999999999999999 	9 	
+1 	0.569999999999999951 	0.440000000000000002 	0.130000000000000004 	0.766499999999999959 	0.346999999999999975 	0.178499999999999992 	0.202000000000000013 	10 	
+1 	0.515000000000000013 	0.390000000000000013 	0.140000000000000013 	0.555499999999999994 	0.200000000000000011 	0.113500000000000004 	0.223500000000000004 	12 	
+1 	0.630000000000000004 	0.505000000000000004 	0.179999999999999993 	1.27200000000000002 	0.602500000000000036 	0.294999999999999984 	0.315000000000000002 	11 	
+2 	0.645000000000000018 	0.525000000000000022 	0.160000000000000003 	1.50750000000000006 	0.745500000000000052 	0.244999999999999996 	0.432499999999999996 	11 	
+0 	0.555000000000000049 	0.455000000000000016 	0.179999999999999993 	0.957999999999999963 	0.295999999999999985 	0.195000000000000007 	0.390000000000000013 	14 	
+2 	0.640000000000000013 	0.520000000000000018 	0.200000000000000011 	1.40700000000000003 	0.565999999999999948 	0.303999999999999992 	0.455000000000000016 	17 	
+2 	0.535000000000000031 	0.419999999999999984 	0.14499999999999999 	0.791000000000000036 	0.330000000000000016 	0.189000000000000001 	0.25 	10 	
+0 	0.525000000000000022 	0.400000000000000022 	0.135000000000000009 	0.713999999999999968 	0.318000000000000005 	0.138000000000000012 	0.20799999999999999 	10 	
+1 	0.479999999999999982 	0.390000000000000013 	0.14499999999999999 	0.582500000000000018 	0.231500000000000011 	0.120999999999999996 	0.255000000000000004 	15 	
+2 	0.46000000000000002 	0.349999999999999978 	0.119999999999999996 	0.515000000000000013 	0.224000000000000005 	0.107999999999999999 	0.1565 	10 	
+1 	0.280000000000000027 	0.200000000000000011 	0.0749999999999999972 	0.122499999999999998 	0.0544999999999999998 	0.0114999999999999998 	0.0350000000000000033 	5 	
+0 	0.54500000000000004 	0.445000000000000007 	0.174999999999999989 	0.852500000000000036 	0.346499999999999975 	0.189000000000000001 	0.294999999999999984 	13 	
+0 	0.385000000000000009 	0.304999999999999993 	0.104999999999999996 	0.331500000000000017 	0.13650000000000001 	0.0744999999999999968 	0.100000000000000006 	7 	
+2 	0.569999999999999951 	0.474999999999999978 	0.195000000000000007 	1.02950000000000008 	0.463500000000000023 	0.190500000000000003 	0.304999999999999993 	18 	
+1 	0.359999999999999987 	0.270000000000000018 	0.0899999999999999967 	0.232000000000000012 	0.119999999999999996 	0.043499999999999997 	0.0560000000000000012 	8 	
+2 	0.505000000000000004 	0.409999999999999976 	0.125 	0.642000000000000015 	0.288999999999999979 	0.133000000000000007 	0.154999999999999999 	9 	
+2 	0.469999999999999973 	0.390000000000000013 	0.149999999999999994 	0.635499999999999954 	0.2185 	0.0884999999999999953 	0.255000000000000004 	9 	
+1 	0.474999999999999978 	0.359999999999999987 	0.14499999999999999 	0.632499999999999951 	0.282499999999999973 	0.137000000000000011 	0.190000000000000002 	8 	
+0 	0.594999999999999973 	0.465000000000000024 	0.140000000000000013 	1.11299999999999999 	0.51749999999999996 	0.243999999999999995 	0.304999999999999993 	12 	
+2 	0.440000000000000002 	0.349999999999999978 	0.110000000000000001 	0.458500000000000019 	0.200000000000000011 	0.0884999999999999953 	0.130000000000000004 	9 	
+2 	0.589999999999999969 	0.465000000000000024 	0.140000000000000013 	1.04600000000000004 	0.469499999999999973 	0.263000000000000012 	0.263000000000000012 	7 	
+2 	0.719999999999999973 	0.57999999999999996 	0.190000000000000002 	2.0884999999999998 	0.995500000000000052 	0.47799999999999998 	0.530499999999999972 	13 	
+1 	0.405000000000000027 	0.309999999999999998 	0.0899999999999999967 	0.312 	0.138000000000000012 	0.0599999999999999978 	0.086999999999999994 	8 	
+2 	0.270000000000000018 	0.195000000000000007 	0.0700000000000000067 	0.105999999999999997 	0.0464999999999999997 	0.0179999999999999986 	0.0359999999999999973 	7 	
+0 	0.640000000000000013 	0.5 	0.179999999999999993 	1.49950000000000006 	0.592999999999999972 	0.314000000000000001 	0.430999999999999994 	11 	
+0 	0.604999999999999982 	0.474999999999999978 	0.174999999999999989 	1.3819999999999999 	0.608999999999999986 	0.232500000000000012 	0.398500000000000021 	10 	
+0 	0.655000000000000027 	0.515000000000000013 	0.179999999999999993 	1.41199999999999992 	0.619500000000000051 	0.248499999999999999 	0.496999999999999997 	11 	
+1 	0.154999999999999999 	0.104999999999999996 	0.0500000000000000028 	0.0175000000000000017 	0.0050000000000000001 	0.00350000000000000007 	0.0050000000000000001 	4 	
+0 	0.594999999999999973 	0.434999999999999998 	0.149999999999999994 	0.900000000000000022 	0.417499999999999982 	0.170000000000000012 	0.265000000000000013 	8 	
+2 	0.609999999999999987 	0.479999999999999982 	0.165000000000000008 	1.24399999999999999 	0.634499999999999953 	0.257000000000000006 	0.304999999999999993 	12 	
+2 	0.619999999999999996 	0.489999999999999991 	0.160000000000000003 	1.03499999999999992 	0.440000000000000002 	0.252500000000000002 	0.284999999999999976 	11 	
+2 	0.530000000000000027 	0.41499999999999998 	0.140000000000000013 	0.723999999999999977 	0.310499999999999998 	0.16750000000000001 	0.204999999999999988 	10 	
+2 	0.569999999999999951 	0.450000000000000011 	0.154999999999999999 	1.19500000000000006 	0.5625 	0.256500000000000006 	0.294999999999999984 	10 	
+1 	0.344999999999999973 	0.275000000000000022 	0.0950000000000000011 	0.199500000000000011 	0.0754999999999999977 	0.0534999999999999989 	0.0700000000000000067 	6 	
+2 	0.770000000000000018 	0.599999999999999978 	0.214999999999999997 	2.19450000000000012 	1.0515000000000001 	0.481999999999999984 	0.583999999999999964 	10 	
+2 	0.594999999999999973 	0.455000000000000016 	0.149999999999999994 	1.04400000000000004 	0.518000000000000016 	0.220500000000000002 	0.270000000000000018 	9 	
+0 	0.424999999999999989 	0.330000000000000016 	0.115000000000000005 	0.406000000000000028 	0.163500000000000006 	0.0810000000000000026 	0.135500000000000009 	8 	
+2 	0.574999999999999956 	0.474999999999999978 	0.160000000000000003 	1.1140000000000001 	0.495499999999999996 	0.274500000000000022 	0.28999999999999998 	9 	
+0 	0.569999999999999951 	0.450000000000000011 	0.179999999999999993 	0.908000000000000029 	0.401500000000000024 	0.216999999999999998 	0.255000000000000004 	9 	
+0 	0.455000000000000016 	0.349999999999999978 	0.125 	0.44850000000000001 	0.158500000000000002 	0.101999999999999993 	0.133500000000000008 	16 	
+1 	0.494999999999999996 	0.380000000000000004 	0.119999999999999996 	0.512000000000000011 	0.233000000000000013 	0.120499999999999996 	0.13600000000000001 	7 	
+2 	0.560000000000000053 	0.46000000000000002 	0.234999999999999987 	0.839500000000000024 	0.332500000000000018 	0.157000000000000001 	0.304999999999999993 	12 	
+0 	0.530000000000000027 	0.395000000000000018 	0.14499999999999999 	0.775000000000000022 	0.307999999999999996 	0.169000000000000011 	0.255000000000000004 	7 	
+2 	0.569999999999999951 	0.405000000000000027 	0.160000000000000003 	0.924499999999999988 	0.344499999999999973 	0.2185 	0.294999999999999984 	19 	
+2 	0.614999999999999991 	0.474999999999999978 	0.170000000000000012 	1.18250000000000011 	0.473999999999999977 	0.28949999999999998 	0.239999999999999991 	11 	
+1 	0.450000000000000011 	0.344999999999999973 	0.135000000000000009 	0.443000000000000005 	0.197500000000000009 	0.0874999999999999944 	0.117499999999999993 	14 	
+2 	0.57999999999999996 	0.440000000000000002 	0.160000000000000003 	0.829500000000000015 	0.336500000000000021 	0.200500000000000012 	0.248499999999999999 	9 	
+2 	0.385000000000000009 	0.275000000000000022 	0.115000000000000005 	0.268500000000000016 	0.0975000000000000033 	0.0825000000000000039 	0.0850000000000000061 	8 	
+0 	0.604999999999999982 	0.450000000000000011 	0.195000000000000007 	1.09800000000000009 	0.480999999999999983 	0.28949999999999998 	0.315000000000000002 	13 	
+2 	0.599999999999999978 	0.450000000000000011 	0.160000000000000003 	1.1419999999999999 	0.539000000000000035 	0.225000000000000006 	0.306999999999999995 	10 	
+2 	0.734999999999999987 	0.589999999999999969 	0.225000000000000006 	1.75600000000000001 	0.637000000000000011 	0.340500000000000025 	0.57999999999999996 	21 	
+1 	0.25 	0.184999999999999998 	0.0650000000000000022 	0.0709999999999999937 	0.0269999999999999997 	0.0184999999999999991 	0.0224999999999999992 	5 	
+1 	0.530000000000000027 	0.41499999999999998 	0.110000000000000001 	0.574500000000000011 	0.252500000000000002 	0.123499999999999999 	0.189000000000000001 	9 	
+1 	0.505000000000000004 	0.390000000000000013 	0.149999999999999994 	0.685000000000000053 	0.361999999999999988 	0.131000000000000005 	0.156 	8 	
+0 	0.660000000000000031 	0.535000000000000031 	0.204999999999999988 	1.4415 	0.592500000000000027 	0.277500000000000024 	0.489999999999999991 	10 	
+1 	0.375 	0.280000000000000027 	0.0850000000000000061 	0.214499999999999996 	0.0855000000000000066 	0.0485000000000000014 	0.0719999999999999946 	7 	
+1 	0.5 	0.380000000000000004 	0.110000000000000001 	0.493999999999999995 	0.217999999999999999 	0.0899999999999999967 	0.132500000000000007 	7 	
+1 	0.560000000000000053 	0.434999999999999998 	0.135000000000000009 	0.719999999999999973 	0.329000000000000015 	0.102999999999999994 	0.251000000000000001 	11 	
+1 	0.184999999999999998 	0.130000000000000004 	0.0449999999999999983 	0.0290000000000000015 	0.0120000000000000002 	0.00749999999999999972 	0.00949999999999999976 	4 	
+2 	0.630000000000000004 	0.5 	0.174999999999999989 	1.26449999999999996 	0.563500000000000001 	0.306499999999999995 	0.342500000000000027 	10 	
+1 	0.140000000000000013 	0.104999999999999996 	0.0350000000000000033 	0.0140000000000000003 	0.00549999999999999968 	0.00250000000000000005 	0.00400000000000000008 	3 	
+2 	0.609999999999999987 	0.484999999999999987 	0.14499999999999999 	1.33050000000000002 	0.783000000000000029 	0.225500000000000006 	0.286499999999999977 	9 	
+1 	0.469999999999999973 	0.354999999999999982 	0.119999999999999996 	0.393000000000000016 	0.16700000000000001 	0.0884999999999999953 	0.115000000000000005 	8 	
+1 	0.284999999999999976 	0.209999999999999992 	0.0700000000000000067 	0.109 	0.0439999999999999974 	0.0264999999999999993 	0.0330000000000000016 	5 	
+1 	0.525000000000000022 	0.429999999999999993 	0.149999999999999994 	0.736500000000000044 	0.322500000000000009 	0.161000000000000004 	0.214999999999999997 	11 	
+1 	0.270000000000000018 	0.200000000000000011 	0.0700000000000000067 	0.100000000000000006 	0.0340000000000000024 	0.0245000000000000009 	0.0350000000000000033 	5 	
+0 	0.505000000000000004 	0.375 	0.115000000000000005 	0.589500000000000024 	0.263500000000000012 	0.119999999999999996 	0.16700000000000001 	10 	
+2 	0.630000000000000004 	0.484999999999999987 	0.165000000000000008 	1.2330000000000001 	0.656499999999999972 	0.231500000000000011 	0.303499999999999992 	10 	
+2 	0.630000000000000004 	0.484999999999999987 	0.174999999999999989 	1.30000000000000004 	0.433499999999999996 	0.294499999999999984 	0.46000000000000002 	23 	
+0 	0.67000000000000004 	0.584999999999999964 	0.160000000000000003 	1.30899999999999994 	0.544499999999999984 	0.294499999999999984 	0.412999999999999978 	10 	
+2 	0.494999999999999996 	0.385000000000000009 	0.135000000000000009 	0.633499999999999952 	0.200000000000000011 	0.122499999999999998 	0.260000000000000009 	14 	
+1 	0.325000000000000011 	0.25 	0.0550000000000000003 	0.166000000000000009 	0.0759999999999999981 	0.0509999999999999967 	0.0449999999999999983 	5 	
+1 	0.574999999999999956 	0.455000000000000016 	0.160000000000000003 	0.989500000000000046 	0.494999999999999996 	0.195000000000000007 	0.245999999999999996 	9 	
+2 	0.685000000000000053 	0.520000000000000018 	0.149999999999999994 	1.34299999999999997 	0.463500000000000023 	0.291999999999999982 	0.400000000000000022 	13 	
+2 	0.589999999999999969 	0.5 	0.149999999999999994 	1.1419999999999999 	0.484999999999999987 	0.265000000000000013 	0.344999999999999973 	9 	
+0 	0.520000000000000018 	0.405000000000000027 	0.125 	0.643499999999999961 	0.241499999999999992 	0.173499999999999988 	0.209999999999999992 	12 	
+0 	0.599999999999999978 	0.479999999999999982 	0.170000000000000012 	1.05600000000000005 	0.457500000000000018 	0.243499999999999994 	0.313500000000000001 	10 	
+0 	0.479999999999999982 	0.390000000000000013 	0.125 	0.690500000000000003 	0.219 	0.154999999999999999 	0.200000000000000011 	12 	
+0 	0.520000000000000018 	0.409999999999999976 	0.170000000000000012 	0.870500000000000052 	0.373499999999999999 	0.219 	0.25 	14 	
+1 	0.385000000000000009 	0.28999999999999998 	0.0899999999999999967 	0.236499999999999988 	0.100000000000000006 	0.0505000000000000032 	0.0759999999999999981 	8 	
+1 	0.560000000000000053 	0.440000000000000002 	0.140000000000000013 	0.824999999999999956 	0.402000000000000024 	0.139000000000000012 	0.244999999999999996 	10 	
+0 	0.469999999999999973 	0.364999999999999991 	0.104999999999999996 	0.420499999999999985 	0.163000000000000006 	0.103499999999999995 	0.140000000000000013 	9 	
+1 	0.445000000000000007 	0.330000000000000016 	0.100000000000000006 	0.437 	0.163000000000000006 	0.0754999999999999977 	0.170000000000000012 	13 	
+0 	0.540000000000000036 	0.424999999999999989 	0.160000000000000003 	0.945500000000000007 	0.367499999999999993 	0.200500000000000012 	0.294999999999999984 	9 	
+1 	0.359999999999999987 	0.275000000000000022 	0.0950000000000000011 	0.216999999999999998 	0.0840000000000000052 	0.043499999999999997 	0.0899999999999999967 	7 	
+1 	0.440000000000000002 	0.354999999999999982 	0.119999999999999996 	0.494999999999999996 	0.231000000000000011 	0.110000000000000001 	0.125 	7 	
+2 	0.614999999999999991 	0.510000000000000009 	0.149999999999999994 	1.29600000000000004 	0.54500000000000004 	0.331500000000000017 	0.320000000000000007 	9 	
+2 	0.57999999999999996 	0.445000000000000007 	0.135000000000000009 	0.813999999999999946 	0.377500000000000002 	0.191500000000000004 	0.220000000000000001 	9 	
+0 	0.650000000000000022 	0.510000000000000009 	0.184999999999999998 	1.375 	0.531000000000000028 	0.384000000000000008 	0.398500000000000021 	10 	
+2 	0.5 	0.400000000000000022 	0.125 	0.672499999999999987 	0.336000000000000021 	0.119999999999999996 	0.182499999999999996 	7 	
+2 	0.614999999999999991 	0.474999999999999978 	0.174999999999999989 	1.22399999999999998 	0.603500000000000036 	0.26100000000000001 	0.310999999999999999 	9 	
+2 	0.67000000000000004 	0.515000000000000013 	0.165000000000000008 	1.17349999999999999 	0.526000000000000023 	0.284999999999999976 	0.316000000000000003 	11 	
+1 	0.419999999999999984 	0.304999999999999993 	0.0899999999999999967 	0.328000000000000014 	0.16800000000000001 	0.0614999999999999991 	0.0820000000000000034 	6 	
+0 	0.655000000000000027 	0.46000000000000002 	0.160000000000000003 	1.49399999999999999 	0.689500000000000002 	0.331000000000000016 	0.182499999999999996 	9 	
+2 	0.609999999999999987 	0.489999999999999991 	0.149999999999999994 	1.10299999999999998 	0.424999999999999989 	0.202500000000000013 	0.359999999999999987 	23 	
+0 	0.680000000000000049 	0.57999999999999996 	0.200000000000000011 	1.78699999999999992 	0.584999999999999964 	0.453000000000000014 	0.599999999999999978 	19 	
+0 	0.41499999999999998 	0.325000000000000011 	0.104999999999999996 	0.380000000000000004 	0.159500000000000003 	0.0785000000000000003 	0.119999999999999996 	12 	
+1 	0.280000000000000027 	0.200000000000000011 	0.0650000000000000022 	0.0894999999999999962 	0.0359999999999999973 	0.0184999999999999991 	0.0299999999999999989 	7 	
+1 	0.429999999999999993 	0.340000000000000024 	0.119999999999999996 	0.357499999999999984 	0.150999999999999995 	0.0645000000000000018 	0.104499999999999996 	9 	
+2 	0.630000000000000004 	0.505000000000000004 	0.225000000000000006 	1.52499999999999991 	0.560000000000000053 	0.333500000000000019 	0.450000000000000011 	15 	
+1 	0.515000000000000013 	0.41499999999999998 	0.135000000000000009 	0.712500000000000022 	0.284999999999999976 	0.151999999999999996 	0.244999999999999996 	10 	
+0 	0.619999999999999996 	0.484999999999999987 	0.174999999999999989 	1.21550000000000002 	0.54500000000000004 	0.253000000000000003 	0.344999999999999973 	10 	
+1 	0.359999999999999987 	0.275000000000000022 	0.0749999999999999972 	0.220500000000000002 	0.0985000000000000042 	0.0439999999999999974 	0.0660000000000000031 	7 	
+2 	0.57999999999999996 	0.455000000000000016 	0.170000000000000012 	0.930000000000000049 	0.407999999999999974 	0.259000000000000008 	0.220000000000000001 	9 	
+2 	0.599999999999999978 	0.450000000000000011 	0.149999999999999994 	0.866500000000000048 	0.369499999999999995 	0.195500000000000007 	0.255000000000000004 	12 	
+2 	0.349999999999999978 	0.265000000000000013 	0.0899999999999999967 	0.177499999999999991 	0.0575000000000000025 	0.0420000000000000026 	0.0680000000000000049 	12 	
+2 	0.555000000000000049 	0.409999999999999976 	0.125 	0.598999999999999977 	0.234499999999999986 	0.146499999999999991 	0.194000000000000006 	8 	
+2 	0.520000000000000018 	0.450000000000000011 	0.149999999999999994 	0.895000000000000018 	0.361499999999999988 	0.185999999999999999 	0.234999999999999987 	14 	
+2 	0.709999999999999964 	0.574999999999999956 	0.214999999999999997 	2.0089999999999999 	0.989500000000000046 	0.447500000000000009 	0.502000000000000002 	11 	
+1 	0.409999999999999976 	0.309999999999999998 	0.0899999999999999967 	0.339000000000000024 	0.154999999999999999 	0.0695000000000000062 	0.0899999999999999967 	7 	
+2 	0.584999999999999964 	0.455000000000000016 	0.140000000000000013 	0.969999999999999973 	0.462000000000000022 	0.184999999999999998 	0.294999999999999984 	9 	
+1 	0.450000000000000011 	0.349999999999999978 	0.119999999999999996 	0.468000000000000027 	0.200500000000000012 	0.106499999999999997 	0.132500000000000007 	8 	
+1 	0.320000000000000007 	0.255000000000000004 	0.0850000000000000061 	0.174499999999999988 	0.0719999999999999946 	0.0330000000000000016 	0.0570000000000000021 	8 	
+0 	0.625 	0.489999999999999991 	0.154999999999999999 	1.20849999999999991 	0.465000000000000024 	0.162000000000000005 	0.410999999999999976 	11 	
+2 	0.604999999999999982 	0.474999999999999978 	0.149999999999999994 	1.14999999999999991 	0.574999999999999956 	0.232000000000000012 	0.296999999999999986 	10 	
+0 	0.614999999999999991 	0.469999999999999973 	0.174999999999999989 	1.29849999999999999 	0.513499999999999956 	0.343000000000000027 	0.320000000000000007 	14 	
+0 	0.67000000000000004 	0.520000000000000018 	0.149999999999999994 	1.40599999999999992 	0.519000000000000017 	0.347999999999999976 	0.369999999999999996 	13 	
+0 	0.660000000000000031 	0.525000000000000022 	0.200000000000000011 	1.46300000000000008 	0.652499999999999969 	0.299499999999999988 	0.421999999999999986 	11 	
+2 	0.650000000000000022 	0.5 	0.154999999999999999 	1.20199999999999996 	0.564999999999999947 	0.313500000000000001 	0.293999999999999984 	11 	
+0 	0.564999999999999947 	0.440000000000000002 	0.149999999999999994 	0.982999999999999985 	0.447500000000000009 	0.235499999999999987 	0.248499999999999999 	9 	
+1 	0.609999999999999987 	0.424999999999999989 	0.154999999999999999 	1.04849999999999999 	0.507000000000000006 	0.195500000000000007 	0.274000000000000021 	11 	
+1 	0.409999999999999976 	0.325000000000000011 	0.104999999999999996 	0.360999999999999988 	0.160500000000000004 	0.0665000000000000036 	0.102999999999999994 	8 	
+0 	0.515000000000000013 	0.424999999999999989 	0.135000000000000009 	0.711999999999999966 	0.266500000000000015 	0.160500000000000004 	0.25 	11 	
+2 	0.625 	0.515000000000000013 	0.165000000000000008 	1.21700000000000008 	0.667000000000000037 	0.206499999999999989 	0.311499999999999999 	10 	
+1 	0.255000000000000004 	0.184999999999999998 	0.0650000000000000022 	0.0739999999999999963 	0.0304999999999999993 	0.0165000000000000008 	0.0200000000000000004 	4 	
+1 	0.320000000000000007 	0.214999999999999997 	0.0950000000000000011 	0.304999999999999993 	0.140000000000000013 	0.067000000000000004 	0.0884999999999999953 	6 	
+2 	0.494999999999999996 	0.400000000000000022 	0.154999999999999999 	0.808499999999999996 	0.234499999999999986 	0.115500000000000005 	0.349999999999999978 	6 	
+0 	0.57999999999999996 	0.450000000000000011 	0.149999999999999994 	0.92000000000000004 	0.393000000000000016 	0.211999999999999994 	0.28949999999999998 	9 	
+2 	0.5 	0.390000000000000013 	0.130000000000000004 	0.708999999999999964 	0.275000000000000022 	0.16800000000000001 	0.179999999999999993 	11 	
+2 	0.57999999999999996 	0.465000000000000024 	0.149999999999999994 	0.906499999999999972 	0.370999999999999996 	0.196500000000000008 	0.28999999999999998 	8 	
+1 	0.165000000000000008 	0.119999999999999996 	0.0299999999999999989 	0.0214999999999999983 	0.00700000000000000015 	0.0050000000000000001 	0.0050000000000000001 	3 	
+2 	0.625 	0.505000000000000004 	0.184999999999999998 	1.15650000000000008 	0.520000000000000018 	0.240499999999999992 	0.353499999999999981 	10 	
+2 	0.535000000000000031 	0.434999999999999998 	0.149999999999999994 	0.724999999999999978 	0.269000000000000017 	0.138500000000000012 	0.25 	9 	
+2 	0.564999999999999947 	0.434999999999999998 	0.184999999999999998 	0.981500000000000039 	0.329000000000000015 	0.13600000000000001 	0.390000000000000013 	13 	
+1 	0.255000000000000004 	0.184999999999999998 	0.0700000000000000067 	0.0749999999999999972 	0.0280000000000000006 	0.0179999999999999986 	0.0250000000000000014 	6 	
+2 	0.650000000000000022 	0.525000000000000022 	0.174999999999999989 	1.47150000000000003 	0.675000000000000044 	0.315000000000000002 	0.399000000000000021 	11 	
+1 	0.255000000000000004 	0.184999999999999998 	0.0599999999999999978 	0.0879999999999999949 	0.0364999999999999977 	0.0210000000000000013 	0.0229999999999999996 	5 	
+0 	0.645000000000000018 	0.550000000000000044 	0.174999999999999989 	1.29150000000000009 	0.569999999999999951 	0.304499999999999993 	0.330000000000000016 	14 	
+0 	0.375 	0.270000000000000018 	0.135000000000000009 	0.596999999999999975 	0.27200000000000002 	0.131000000000000005 	0.16750000000000001 	7 	
+1 	0.375 	0.280000000000000027 	0.0800000000000000017 	0.226000000000000006 	0.104999999999999996 	0.0470000000000000001 	0.0650000000000000022 	6 	
+0 	0.619999999999999996 	0.484999999999999987 	0.154999999999999999 	1.1944999999999999 	0.510499999999999954 	0.271000000000000019 	0.35199999999999998 	9 	
+2 	0.5 	0.375 	0.149999999999999994 	0.63600000000000001 	0.253500000000000003 	0.14499999999999999 	0.190000000000000002 	10 	
+1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.1875 	0.0810000000000000026 	0.0420000000000000026 	0.0580000000000000029 	6 	
+2 	0.520000000000000018 	0.400000000000000022 	0.119999999999999996 	0.57999999999999996 	0.234000000000000014 	0.131500000000000006 	0.184999999999999998 	8 	
+2 	0.604999999999999982 	0.474999999999999978 	0.140000000000000013 	1.11749999999999994 	0.555000000000000049 	0.257000000000000006 	0.274000000000000021 	9 	
+0 	0.450000000000000011 	0.359999999999999987 	0.125 	0.50649999999999995 	0.222000000000000003 	0.104999999999999996 	0.160000000000000003 	10 	
+0 	0.630000000000000004 	0.494999999999999996 	0.200000000000000011 	1.42549999999999999 	0.65900000000000003 	0.336000000000000021 	0.380000000000000004 	11 	
+0 	0.369999999999999996 	0.275000000000000022 	0.0800000000000000017 	0.227000000000000007 	0.0929999999999999993 	0.0625 	0.0700000000000000067 	8 	
+0 	0.535000000000000031 	0.405000000000000027 	0.125 	0.927000000000000046 	0.260000000000000009 	0.142499999999999988 	0.344999999999999973 	16 	
+1 	0.445000000000000007 	0.330000000000000016 	0.110000000000000001 	0.357999999999999985 	0.152499999999999997 	0.067000000000000004 	0.118499999999999994 	8 	
+2 	0.650000000000000022 	0.515000000000000013 	0.174999999999999989 	1.46599999999999997 	0.677000000000000046 	0.304499999999999993 	0.400000000000000022 	10 	
+2 	0.645000000000000018 	0.510000000000000009 	0.160000000000000003 	1.1835 	0.55600000000000005 	0.23849999999999999 	0.344999999999999973 	11 	
+2 	0.555000000000000049 	0.445000000000000007 	0.135000000000000009 	0.835999999999999965 	0.336000000000000021 	0.162500000000000006 	0.275000000000000022 	13 	
+2 	0.174999999999999989 	0.135000000000000009 	0.0400000000000000008 	0.0304999999999999993 	0.0109999999999999994 	0.00749999999999999972 	0.0100000000000000002 	5 	
+1 	0.369999999999999996 	0.280000000000000027 	0.0899999999999999967 	0.233000000000000013 	0.0904999999999999971 	0.0544999999999999998 	0.0700000000000000067 	11 	
+1 	0.510000000000000009 	0.395000000000000018 	0.130000000000000004 	0.602500000000000036 	0.281000000000000028 	0.142999999999999988 	0.162000000000000005 	7 	
+0 	0.599999999999999978 	0.465000000000000024 	0.154999999999999999 	1.04000000000000004 	0.475499999999999978 	0.25 	0.280000000000000027 	11 	
+2 	0.440000000000000002 	0.320000000000000007 	0.119999999999999996 	0.456500000000000017 	0.243499999999999994 	0.0919999999999999984 	0.102499999999999994 	8 	
+2 	0.614999999999999991 	0.46000000000000002 	0.170000000000000012 	1.05649999999999999 	0.481499999999999984 	0.27200000000000002 	0.270000000000000018 	10 	
+2 	0.569999999999999951 	0.450000000000000011 	0.160000000000000003 	0.861500000000000044 	0.372499999999999998 	0.217499999999999999 	0.255000000000000004 	12 	
+1 	0.469999999999999973 	0.385000000000000009 	0.130000000000000004 	0.586999999999999966 	0.264000000000000012 	0.117000000000000007 	0.173999999999999988 	8 	
+0 	0.535000000000000031 	0.400000000000000022 	0.149999999999999994 	1.22399999999999998 	0.617999999999999994 	0.275000000000000022 	0.287499999999999978 	10 	
+0 	0.569999999999999951 	0.450000000000000011 	0.174999999999999989 	0.955500000000000016 	0.380000000000000004 	0.166500000000000009 	0.294999999999999984 	18 	
+2 	0.359999999999999987 	0.270000000000000018 	0.0899999999999999967 	0.222500000000000003 	0.0830000000000000043 	0.0529999999999999985 	0.0749999999999999972 	6 	
+2 	0.46000000000000002 	0.375 	0.135000000000000009 	0.493499999999999994 	0.185999999999999999 	0.0845000000000000057 	0.170000000000000012 	12 	
+0 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.699500000000000011 	0.311499999999999999 	0.131000000000000005 	0.223000000000000004 	9 	
+2 	0.640000000000000013 	0.494999999999999996 	0.170000000000000012 	1.13900000000000001 	0.53949999999999998 	0.281999999999999973 	0.284999999999999976 	10 	
+1 	0.25 	0.190000000000000002 	0.0599999999999999978 	0.0764999999999999986 	0.0359999999999999973 	0.0114999999999999998 	0.0245000000000000009 	6 	
+0 	0.614999999999999991 	0.474999999999999978 	0.154999999999999999 	1.11499999999999999 	0.483999999999999986 	0.211499999999999994 	0.354999999999999982 	10 	
+0 	0.530000000000000027 	0.409999999999999976 	0.165000000000000008 	0.811499999999999999 	0.239999999999999991 	0.169000000000000011 	0.239999999999999991 	19 	
+1 	0.349999999999999978 	0.255000000000000004 	0.0899999999999999967 	0.178499999999999992 	0.0855000000000000066 	0.0304999999999999993 	0.0524999999999999981 	8 	
+2 	0.354999999999999982 	0.280000000000000027 	0.0950000000000000011 	0.245499999999999996 	0.0955000000000000016 	0.0619999999999999996 	0.0749999999999999972 	11 	
+2 	0.330000000000000016 	0.25 	0.0899999999999999967 	0.197000000000000008 	0.0850000000000000061 	0.0410000000000000017 	0.0604999999999999982 	10 	
+1 	0.380000000000000004 	0.280000000000000027 	0.0850000000000000061 	0.273500000000000021 	0.115000000000000005 	0.0609999999999999987 	0.0850000000000000061 	6 	
+2 	0.70499999999999996 	0.560000000000000053 	0.165000000000000008 	1.67500000000000004 	0.797000000000000042 	0.409499999999999975 	0.388000000000000012 	10 	
+2 	0.469999999999999973 	0.380000000000000004 	0.14499999999999999 	0.586500000000000021 	0.23849999999999999 	0.143999999999999989 	0.184999999999999998 	8 	
+2 	0.614999999999999991 	0.469999999999999973 	0.165000000000000008 	1.12799999999999989 	0.446500000000000008 	0.219500000000000001 	0.340000000000000024 	10 	
+2 	0.474999999999999978 	0.359999999999999987 	0.100000000000000006 	0.428499999999999992 	0.196500000000000008 	0.0990000000000000047 	0.112000000000000002 	7 	
+1 	0.315000000000000002 	0.234999999999999987 	0.0700000000000000067 	0.148999999999999994 	0.0580000000000000029 	0.0325000000000000011 	0.0470000000000000001 	7 	
+1 	0.555000000000000049 	0.434999999999999998 	0.140000000000000013 	0.765000000000000013 	0.394500000000000017 	0.149999999999999994 	0.205999999999999989 	8 	
+2 	0.440000000000000002 	0.340000000000000024 	0.104999999999999996 	0.402000000000000024 	0.130500000000000005 	0.0955000000000000016 	0.165000000000000008 	10 	
+1 	0.465000000000000024 	0.340000000000000024 	0.110000000000000001 	0.345999999999999974 	0.142499999999999988 	0.0729999999999999954 	0.113000000000000003 	11 	
+2 	0.484999999999999987 	0.369999999999999996 	0.154999999999999999 	0.967999999999999972 	0.418999999999999984 	0.245499999999999996 	0.236499999999999988 	9 	
+2 	0.604999999999999982 	0.474999999999999978 	0.174999999999999989 	1.20100000000000007 	0.53949999999999998 	0.275000000000000022 	0.308999999999999997 	10 	
+0 	0.530000000000000027 	0.41499999999999998 	0.149999999999999994 	0.777499999999999969 	0.236999999999999988 	0.141499999999999987 	0.330000000000000016 	20 	
+0 	0.604999999999999982 	0.469999999999999973 	0.160000000000000003 	1.08349999999999991 	0.54049999999999998 	0.221500000000000002 	0.275000000000000022 	12 	
+2 	0.625 	0.479999999999999982 	0.14499999999999999 	1.08499999999999996 	0.464500000000000024 	0.244499999999999995 	0.327000000000000013 	10 	
+2 	0.195000000000000007 	0.14499999999999999 	0.0500000000000000028 	0.0320000000000000007 	0.0100000000000000002 	0.00800000000000000017 	0.0120000000000000002 	4 	
+2 	0.525000000000000022 	0.424999999999999989 	0.119999999999999996 	0.701999999999999957 	0.333500000000000019 	0.146499999999999991 	0.220000000000000001 	12 	
+2 	0.560000000000000053 	0.450000000000000011 	0.14499999999999999 	0.894000000000000017 	0.388500000000000012 	0.209499999999999992 	0.264000000000000012 	9 	
+2 	0.755000000000000004 	0.57999999999999996 	0.204999999999999988 	2.00649999999999995 	0.829500000000000015 	0.401500000000000024 	0.594999999999999973 	10 	
+1 	0.494999999999999996 	0.385000000000000009 	0.125 	0.584999999999999964 	0.275500000000000023 	0.123499999999999999 	0.165000000000000008 	8 	
+0 	0.584999999999999964 	0.469999999999999973 	0.170000000000000012 	1.09899999999999998 	0.39750000000000002 	0.232500000000000012 	0.357999999999999985 	20 	
+1 	0.294999999999999984 	0.225000000000000006 	0.0800000000000000017 	0.123999999999999999 	0.0485000000000000014 	0.0320000000000000007 	0.0400000000000000008 	9 	
+1 	0.359999999999999987 	0.284999999999999976 	0.104999999999999996 	0.241499999999999992 	0.091499999999999998 	0.0570000000000000021 	0.0749999999999999972 	7 	
+0 	0.564999999999999947 	0.440000000000000002 	0.149999999999999994 	0.862999999999999989 	0.434999999999999998 	0.148999999999999994 	0.270000000000000018 	9 	
+0 	0.550000000000000044 	0.424999999999999989 	0.135000000000000009 	0.851500000000000035 	0.361999999999999988 	0.196000000000000008 	0.270000000000000018 	14 	
+2 	0.57999999999999996 	0.455000000000000016 	0.135000000000000009 	0.795499999999999985 	0.405000000000000027 	0.16700000000000001 	0.203999999999999987 	10 	
+2 	0.589999999999999969 	0.469999999999999973 	0.149999999999999994 	0.995500000000000052 	0.480999999999999983 	0.232000000000000012 	0.239999999999999991 	8 	
+2 	0.650000000000000022 	0.520000000000000018 	0.174999999999999989 	1.26550000000000007 	0.614999999999999991 	0.277500000000000024 	0.336000000000000021 	9 	
+0 	0.625 	0.484999999999999987 	0.200000000000000011 	1.37999999999999989 	0.58450000000000002 	0.301999999999999991 	0.401000000000000023 	9 	
+1 	0.405000000000000027 	0.309999999999999998 	0.110000000000000001 	0.910000000000000031 	0.415999999999999981 	0.20749999999999999 	0.0995000000000000051 	8 	
+1 	0.375 	0.28999999999999998 	0.140000000000000013 	0.299999999999999989 	0.140000000000000013 	0.0625 	0.0825000000000000039 	8 	
+2 	0.440000000000000002 	0.33500000000000002 	0.110000000000000001 	0.394000000000000017 	0.157000000000000001 	0.096000000000000002 	0.121999999999999997 	9 	
+2 	0.640000000000000013 	0.525000000000000022 	0.179999999999999993 	1.31349999999999989 	0.486499999999999988 	0.299499999999999988 	0.407499999999999973 	10 	
+0 	0.604999999999999982 	0.469999999999999973 	0.165000000000000008 	1.17749999999999999 	0.610999999999999988 	0.227500000000000008 	0.291999999999999982 	9 	
+2 	0.635000000000000009 	0.510000000000000009 	0.209999999999999992 	1.59800000000000009 	0.65349999999999997 	0.283499999999999974 	0.57999999999999996 	15 	
+1 	0.255000000000000004 	0.195000000000000007 	0.0700000000000000067 	0.0734999999999999959 	0.0254999999999999984 	0.0200000000000000004 	0.0250000000000000014 	6 	
+0 	0.67000000000000004 	0.505000000000000004 	0.204999999999999988 	1.36450000000000005 	0.60750000000000004 	0.302499999999999991 	0.35299999999999998 	9 	
+2 	0.614999999999999991 	0.469999999999999973 	0.14499999999999999 	1.02849999999999997 	0.443500000000000005 	0.282499999999999973 	0.284999999999999976 	11 	
+2 	0.660000000000000031 	0.505000000000000004 	0.200000000000000011 	1.63050000000000006 	0.486499999999999988 	0.296999999999999986 	0.609999999999999987 	18 	
+0 	0.619999999999999996 	0.474999999999999978 	0.149999999999999994 	0.954500000000000015 	0.455000000000000016 	0.186499999999999999 	0.277000000000000024 	9 	
+2 	0.41499999999999998 	0.315000000000000002 	0.115000000000000005 	0.389500000000000013 	0.201500000000000012 	0.0650000000000000022 	0.102999999999999994 	9 	
+2 	0.520000000000000018 	0.465000000000000024 	0.149999999999999994 	0.950500000000000012 	0.456000000000000016 	0.19900000000000001 	0.255000000000000004 	8 	
+0 	0.604999999999999982 	0.479999999999999982 	0.140000000000000013 	0.990999999999999992 	0.473499999999999976 	0.234499999999999986 	0.239999999999999991 	8 	
+2 	0.344999999999999973 	0.270000000000000018 	0.0950000000000000011 	0.197000000000000008 	0.0665000000000000036 	0.0500000000000000028 	0.0700000000000000067 	9 	
+0 	0.680000000000000049 	0.520000000000000018 	0.184999999999999998 	1.49399999999999999 	0.614999999999999991 	0.393500000000000016 	0.406000000000000028 	11 	
+2 	0.154999999999999999 	0.110000000000000001 	0.0400000000000000008 	0.0154999999999999999 	0.0064999999999999997 	0.00300000000000000006 	0.0050000000000000001 	3 	
+1 	0.455000000000000016 	0.33500000000000002 	0.104999999999999996 	0.421999999999999986 	0.229000000000000009 	0.0864999999999999936 	0.100000000000000006 	6 	
+0 	0.574999999999999956 	0.46000000000000002 	0.184999999999999998 	1.09400000000000008 	0.44850000000000001 	0.216999999999999998 	0.344999999999999973 	15 	
+1 	0.484999999999999987 	0.364999999999999991 	0.140000000000000013 	0.447500000000000009 	0.189500000000000002 	0.0924999999999999989 	0.23050000000000001 	8 	
+1 	0.419999999999999984 	0.325000000000000011 	0.115000000000000005 	0.353999999999999981 	0.162500000000000006 	0.0640000000000000013 	0.104999999999999996 	8 	
+0 	0.650000000000000022 	0.54500000000000004 	0.23000000000000001 	1.752 	0.560499999999999998 	0.28949999999999998 	0.814999999999999947 	16 	
+0 	0.640000000000000013 	0.510000000000000009 	0.165000000000000008 	1.48599999999999999 	0.759499999999999953 	0.332000000000000017 	0.321000000000000008 	8 	
+2 	0.619999999999999996 	0.489999999999999991 	0.170000000000000012 	1.21049999999999991 	0.518499999999999961 	0.255500000000000005 	0.33500000000000002 	13 	
+1 	0.455000000000000016 	0.344999999999999973 	0.110000000000000001 	0.433999999999999997 	0.20699999999999999 	0.0855000000000000066 	0.121499999999999997 	8 	
+2 	0.599999999999999978 	0.479999999999999982 	0.174999999999999989 	1.22900000000000009 	0.412499999999999978 	0.273500000000000021 	0.41499999999999998 	13 	
+1 	0.450000000000000011 	0.33500000000000002 	0.0950000000000000011 	0.350499999999999978 	0.161500000000000005 	0.0625 	0.118499999999999994 	7 	
+1 	0.484999999999999987 	0.375 	0.130000000000000004 	0.553499999999999992 	0.266000000000000014 	0.112000000000000002 	0.157000000000000001 	8 	
+1 	0.429999999999999993 	0.340000000000000024 	0 	0.427999999999999992 	0.206499999999999989 	0.0859999999999999931 	0.115000000000000005 	8 	
+0 	0.609999999999999987 	0.429999999999999993 	0.140000000000000013 	0.90900000000000003 	0.438 	0.200000000000000011 	0.220000000000000001 	8 	
+1 	0.510000000000000009 	0.455000000000000016 	0.135000000000000009 	0.685499999999999998 	0.287499999999999978 	0.153999999999999998 	0.203499999999999986 	9 	
+2 	0.655000000000000027 	0.540000000000000036 	0.214999999999999997 	1.84400000000000008 	0.742500000000000049 	0.327000000000000013 	0.584999999999999964 	22 	
+1 	0.525000000000000022 	0.409999999999999976 	0.174999999999999989 	0.873999999999999999 	0.358499999999999985 	0.20699999999999999 	0.204999999999999988 	18 	
+0 	0.569999999999999951 	0.424999999999999989 	0.130000000000000004 	0.782000000000000028 	0.369499999999999995 	0.174499999999999988 	0.196500000000000008 	8 	
+1 	0.349999999999999978 	0.25 	0.0700000000000000067 	0.179999999999999993 	0.0655000000000000027 	0.048000000000000001 	0.0539999999999999994 	6 	
+1 	0.429999999999999993 	0.340000000000000024 	0.110000000000000001 	0.364499999999999991 	0.159000000000000002 	0.0855000000000000066 	0.104999999999999996 	7 	
+2 	0.709999999999999964 	0.560000000000000053 	0.220000000000000001 	2.01500000000000012 	0.921499999999999986 	0.454000000000000015 	0.565999999999999948 	11 	
+1 	0.369999999999999996 	0.275000000000000022 	0.100000000000000006 	0.281499999999999972 	0.150499999999999995 	0.0505000000000000032 	0.0680000000000000049 	5 	
+1 	0.395000000000000018 	0.325000000000000011 	0.104999999999999996 	0.305999999999999994 	0.111000000000000001 	0.0734999999999999959 	0.0950000000000000011 	8 	
+0 	0.560000000000000053 	0.450000000000000011 	0.160000000000000003 	1.02350000000000008 	0.428999999999999992 	0.268000000000000016 	0.299999999999999989 	10 	
+0 	0.604999999999999982 	0.505000000000000004 	0.179999999999999993 	1.43399999999999994 	0.728500000000000036 	0.264000000000000012 	0.430999999999999994 	11 	
+0 	0.560000000000000053 	0.429999999999999993 	0.149999999999999994 	0.882499999999999951 	0.346499999999999975 	0.171999999999999986 	0.309999999999999998 	9 	
+0 	0.57999999999999996 	0.434999999999999998 	0.154999999999999999 	0.878499999999999948 	0.424999999999999989 	0.168500000000000011 	0.242499999999999993 	10 	
+0 	0.515000000000000013 	0.429999999999999993 	0.140000000000000013 	0.833999999999999964 	0.366999999999999993 	0.200000000000000011 	0.23000000000000001 	8 	
+1 	0.440000000000000002 	0.340000000000000024 	0.104999999999999996 	0.368999999999999995 	0.164000000000000007 	0.0800000000000000017 	0.101500000000000007 	5 	
+1 	0.505000000000000004 	0.395000000000000018 	0.104999999999999996 	0.551000000000000045 	0.247999999999999998 	0.102999999999999994 	0.171000000000000013 	8 	
+1 	0.434999999999999998 	0.330000000000000016 	0.104999999999999996 	0.33500000000000002 	0.156 	0.0555000000000000007 	0.104999999999999996 	8 	
+0 	0.28999999999999998 	0.209999999999999992 	0.0749999999999999972 	0.275000000000000022 	0.113000000000000003 	0.0675000000000000044 	0.0350000000000000033 	6 	
+0 	0.569999999999999951 	0.434999999999999998 	0.149999999999999994 	0.829500000000000015 	0.387500000000000011 	0.156 	0.244999999999999996 	10 	
+2 	0.154999999999999999 	0.115000000000000005 	0.0250000000000000014 	0.0240000000000000005 	0.00899999999999999932 	0.0050000000000000001 	0.00749999999999999972 	5 	
+0 	0.46000000000000002 	0.354999999999999982 	0.130000000000000004 	0.458000000000000018 	0.192000000000000004 	0.105499999999999997 	0.130000000000000004 	13 	
+1 	0.165000000000000008 	0.110000000000000001 	0.0200000000000000004 	0.0189999999999999995 	0.0064999999999999997 	0.00250000000000000005 	0.0050000000000000001 	4 	
+0 	0.655000000000000027 	0.515000000000000013 	0.170000000000000012 	1.52699999999999991 	0.848500000000000032 	0.263500000000000012 	0.331000000000000016 	11 	
+0 	0.469999999999999973 	0.375 	0.119999999999999996 	0.601500000000000035 	0.276500000000000024 	0.14549999999999999 	0.135000000000000009 	8 	
+2 	0.630000000000000004 	0.489999999999999991 	0.170000000000000012 	1.1745000000000001 	0.525499999999999967 	0.27300000000000002 	0.339000000000000024 	11 	
+0 	0.67000000000000004 	0.530000000000000027 	0.225000000000000006 	1.56150000000000011 	0.630000000000000004 	0.486999999999999988 	0.372499999999999998 	11 	
+1 	0.330000000000000016 	0.260000000000000009 	0.0800000000000000017 	0.190000000000000002 	0.0764999999999999986 	0.0384999999999999995 	0.0650000000000000022 	7 	
+0 	0.640000000000000013 	0.520000000000000018 	0.174999999999999989 	1.248 	0.424499999999999988 	0.259500000000000008 	0.479999999999999982 	12 	
+1 	0.434999999999999998 	0.33500000000000002 	0.100000000000000006 	0.329500000000000015 	0.129000000000000004 	0.0700000000000000067 	0.110000000000000001 	7 	
+2 	0.5 	0.380000000000000004 	0.135000000000000009 	0.583500000000000019 	0.22950000000000001 	0.126500000000000001 	0.179999999999999993 	12 	
+1 	0.41499999999999998 	0.325000000000000011 	0.110000000000000001 	0.316000000000000003 	0.138500000000000012 	0.0795000000000000012 	0.0924999999999999989 	8 	
+2 	0.5 	0.380000000000000004 	0.119999999999999996 	0.576500000000000012 	0.27300000000000002 	0.135000000000000009 	0.14499999999999999 	9 	
+0 	0.445000000000000007 	0.33500000000000002 	0.140000000000000013 	0.456500000000000017 	0.178499999999999992 	0.114000000000000004 	0.140000000000000013 	11 	
+2 	0.635000000000000009 	0.510000000000000009 	0.184999999999999998 	1.30800000000000005 	0.544000000000000039 	0.318000000000000005 	0.377000000000000002 	8 	
+1 	0.465000000000000024 	0.344999999999999973 	0.110000000000000001 	0.441500000000000004 	0.175499999999999989 	0.0904999999999999971 	0.119999999999999996 	7 	
+2 	0.530000000000000027 	0.409999999999999976 	0.165000000000000008 	0.731999999999999984 	0.189000000000000001 	0.170000000000000012 	0.309999999999999998 	11 	
+0 	0.520000000000000018 	0.409999999999999976 	0.115000000000000005 	0.807000000000000051 	0.285499999999999976 	0.178999999999999992 	0.234999999999999987 	12 	
+1 	0.0749999999999999972 	0.0550000000000000003 	0.0100000000000000002 	0.00200000000000000004 	0.00100000000000000002 	0.00050000000000000001 	0.00150000000000000003 	1 	
+1 	0.225000000000000006 	0.170000000000000012 	0.0500000000000000028 	0.0514999999999999972 	0.0189999999999999995 	0.0120000000000000002 	0.0170000000000000012 	4 	
+0 	0.660000000000000031 	0.505000000000000004 	0.184999999999999998 	1.52800000000000002 	0.689999999999999947 	0.302499999999999991 	0.441000000000000003 	11 	
+1 	0.320000000000000007 	0.204999999999999988 	0.0800000000000000017 	0.180999999999999994 	0.0879999999999999949 	0.0340000000000000024 	0.0495000000000000023 	5 	
+2 	0.369999999999999996 	0.284999999999999976 	0.100000000000000006 	0.228000000000000008 	0.0675000000000000044 	0.0675000000000000044 	0.0810000000000000026 	10 	
+0 	0.640000000000000013 	0.510000000000000009 	0.200000000000000011 	1.39050000000000007 	0.609999999999999987 	0.331500000000000017 	0.409999999999999976 	12 	
+2 	0.344999999999999973 	0.255000000000000004 	0.0899999999999999967 	0.200500000000000012 	0.0940000000000000002 	0.0294999999999999984 	0.0630000000000000004 	9 	
+1 	0.330000000000000016 	0.25 	0.0950000000000000011 	0.208499999999999991 	0.101999999999999993 	0.0395000000000000004 	0.0519999999999999976 	7 	
+2 	0.484999999999999987 	0.364999999999999991 	0.154999999999999999 	1.02899999999999991 	0.423499999999999988 	0.228500000000000009 	0.313 	8 	
+0 	0.780000000000000027 	0.630000000000000004 	0.214999999999999997 	2.65700000000000003 	1.48799999999999999 	0.498499999999999999 	0.585999999999999965 	11 	
+0 	0.54500000000000004 	0.429999999999999993 	0.140000000000000013 	0.831999999999999962 	0.435499999999999998 	0.170000000000000012 	0.201000000000000012 	9 	
+0 	0.369999999999999996 	0.284999999999999976 	0.104999999999999996 	0.270000000000000018 	0.112500000000000003 	0.0585000000000000034 	0.0835000000000000048 	9 	
+1 	0.54500000000000004 	0.429999999999999993 	0.149999999999999994 	0.741999999999999993 	0.35249999999999998 	0.158000000000000002 	0.20799999999999999 	10 	
+0 	0.699999999999999956 	0.550000000000000044 	0.170000000000000012 	1.68399999999999994 	0.753499999999999948 	0.326500000000000012 	0.320000000000000007 	11 	
+1 	0.294999999999999984 	0.23000000000000001 	0.0800000000000000017 	0.162500000000000006 	0.0650000000000000022 	0.0500000000000000028 	0.0384999999999999995 	5 	
+2 	0.535000000000000031 	0.405000000000000027 	0.174999999999999989 	1.27049999999999996 	0.548000000000000043 	0.326500000000000012 	0.337000000000000022 	13 	
+0 	0.41499999999999998 	0.340000000000000024 	0.130000000000000004 	0.367499999999999993 	0.145999999999999991 	0.0884999999999999953 	0.119999999999999996 	10 	
+1 	0.450000000000000011 	0.354999999999999982 	0.104999999999999996 	0.444500000000000006 	0.197000000000000008 	0.0929999999999999993 	0.133500000000000008 	8 	
+2 	0.685000000000000053 	0.510000000000000009 	0.165000000000000008 	1.54499999999999993 	0.686000000000000054 	0.377500000000000002 	0.405500000000000027 	10 	
+0 	0.739999999999999991 	0.599999999999999978 	0.195000000000000007 	1.97399999999999998 	0.597999999999999976 	0.408499999999999974 	0.709999999999999964 	16 	
+0 	0.614999999999999991 	0.479999999999999982 	0.160000000000000003 	1.25249999999999995 	0.584999999999999964 	0.259500000000000008 	0.330000000000000016 	8 	
+0 	0.574999999999999956 	0.424999999999999989 	0.149999999999999994 	0.876499999999999946 	0.455000000000000016 	0.179999999999999993 	0.228000000000000008 	8 	
+0 	0.614999999999999991 	0.515000000000000013 	0.170000000000000012 	1.1399999999999999 	0.430499999999999994 	0.224500000000000005 	0.419999999999999984 	16 	
+2 	0.489999999999999991 	0.390000000000000013 	0.140000000000000013 	0.706999999999999962 	0.279500000000000026 	0.2185 	0.179999999999999993 	13 	
+1 	0.434999999999999998 	0.330000000000000016 	0.110000000000000001 	0.412999999999999978 	0.205499999999999988 	0.096000000000000002 	0.096000000000000002 	6 	
+2 	0.660000000000000031 	0.505000000000000004 	0.165000000000000008 	1.37400000000000011 	0.588999999999999968 	0.350999999999999979 	0.344999999999999973 	10 	
+2 	0.440000000000000002 	0.325000000000000011 	0.115000000000000005 	0.390000000000000013 	0.163000000000000006 	0.086999999999999994 	0.113000000000000003 	7 	
+2 	0.75 	0.550000000000000044 	0.179999999999999993 	1.89300000000000002 	0.941999999999999948 	0.39700000000000002 	0.445000000000000007 	11 	
+2 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	0.919000000000000039 	0.433499999999999996 	0.17649999999999999 	0.262000000000000011 	9 	
+1 	0.525000000000000022 	0.400000000000000022 	0.140000000000000013 	0.600999999999999979 	0.262500000000000011 	0.128500000000000003 	0.183499999999999996 	9 	
+0 	0.650000000000000022 	0.489999999999999991 	0.154999999999999999 	1.12200000000000011 	0.54500000000000004 	0.228000000000000008 	0.305499999999999994 	9 	
+2 	0.584999999999999964 	0.474999999999999978 	0.149999999999999994 	1.06499999999999995 	0.531499999999999972 	0.19900000000000001 	0.288499999999999979 	10 	
+1 	0.564999999999999947 	0.440000000000000002 	0.174999999999999989 	0.873500000000000054 	0.413999999999999979 	0.209999999999999992 	0.209999999999999992 	11 	
+2 	0.574999999999999956 	0.474999999999999978 	0.14499999999999999 	0.856999999999999984 	0.366499999999999992 	0.172999999999999987 	0.269000000000000017 	9 	
+0 	0.530000000000000027 	0.424999999999999989 	0.170000000000000012 	0.948999999999999955 	0.348499999999999976 	0.239499999999999991 	0.278000000000000025 	17 	
+0 	0.609999999999999987 	0.469999999999999973 	0.14499999999999999 	1.15300000000000002 	0.403000000000000025 	0.295999999999999985 	0.320000000000000007 	14 	
+1 	0.364999999999999991 	0.255000000000000004 	0.0800000000000000017 	0.19850000000000001 	0.0785000000000000003 	0.0345000000000000029 	0.0529999999999999985 	5 	
+0 	0.609999999999999987 	0.46000000000000002 	0.154999999999999999 	0.956999999999999962 	0.425499999999999989 	0.197500000000000009 	0.265000000000000013 	8 	
+0 	0.455000000000000016 	0.349999999999999978 	0.140000000000000013 	0.518499999999999961 	0.221000000000000002 	0.126500000000000001 	0.135000000000000009 	10 	
+0 	0.505000000000000004 	0.400000000000000022 	0.125 	0.582999999999999963 	0.245999999999999996 	0.130000000000000004 	0.174999999999999989 	7 	
+1 	0.434999999999999998 	0.33500000000000002 	0.110000000000000001 	0.383000000000000007 	0.155499999999999999 	0.0675000000000000044 	0.135000000000000009 	12 	
+2 	0.625 	0.5 	0.195000000000000007 	1.36899999999999999 	0.587500000000000022 	0.2185 	0.369999999999999996 	17 	
+2 	0.450000000000000011 	0.33500000000000002 	0.140000000000000013 	0.462500000000000022 	0.164000000000000007 	0.0759999999999999981 	0.149999999999999994 	14 	
+2 	0.660000000000000031 	0.510000000000000009 	0.174999999999999989 	1.21799999999999997 	0.505499999999999949 	0.302999999999999992 	0.369999999999999996 	11 	
+1 	0.484999999999999987 	0.364999999999999991 	0.104999999999999996 	0.520499999999999963 	0.195000000000000007 	0.122999999999999998 	0.181999999999999995 	8 	
+2 	0.665000000000000036 	0.515000000000000013 	0.190000000000000002 	1.63850000000000007 	0.830999999999999961 	0.357499999999999984 	0.370999999999999996 	11 	
+1 	0.440000000000000002 	0.33500000000000002 	0.115000000000000005 	0.421499999999999986 	0.172999999999999987 	0.0764999999999999986 	0.113000000000000003 	7 	
+0 	0.57999999999999996 	0.46000000000000002 	0.174999999999999989 	1.16500000000000004 	0.650000000000000022 	0.220500000000000002 	0.305499999999999994 	9 	
+2 	0.280000000000000027 	0.204999999999999988 	0.100000000000000006 	0.116500000000000006 	0.0544999999999999998 	0.028500000000000001 	0.0299999999999999989 	5 	
+1 	0.515000000000000013 	0.380000000000000004 	0.119999999999999996 	0.625 	0.326500000000000012 	0.129500000000000004 	0.160000000000000003 	7 	
+1 	0.450000000000000011 	0.330000000000000016 	0.104999999999999996 	0.448000000000000009 	0.20799999999999999 	0.0889999999999999958 	0.119999999999999996 	9 	
+2 	0.734999999999999987 	0.584999999999999964 	0.184999999999999998 	2.12400000000000011 	0.951999999999999957 	0.550000000000000044 	0.5 	11 	
+0 	0.54500000000000004 	0.385000000000000009 	0.149999999999999994 	1.11850000000000005 	0.542499999999999982 	0.244499999999999995 	0.284499999999999975 	9 	
+1 	0.299999999999999989 	0.220000000000000001 	0.0800000000000000017 	0.1255 	0.0550000000000000003 	0.0264999999999999993 	0.0389999999999999999 	6 	
+2 	0.505000000000000004 	0.440000000000000002 	0.140000000000000013 	0.827500000000000013 	0.341500000000000026 	0.185499999999999998 	0.23899999999999999 	8 	
+1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.385000000000000009 	0.16700000000000001 	0.0800000000000000017 	0.125 	7 	
+0 	0.604999999999999982 	0.494999999999999996 	0.190000000000000002 	1.43700000000000006 	0.468999999999999972 	0.265500000000000014 	0.409999999999999976 	15 	
+0 	0.525000000000000022 	0.405000000000000027 	0.160000000000000003 	0.658000000000000029 	0.265500000000000014 	0.112500000000000003 	0.225000000000000006 	12 	
+2 	0.744999999999999996 	0.584999999999999964 	0.214999999999999997 	2.49900000000000011 	0.92649999999999999 	0.471999999999999975 	0.699999999999999956 	17 	
+1 	0.474999999999999978 	0.364999999999999991 	0.119999999999999996 	0.530000000000000027 	0.2505 	0.0975000000000000033 	0.162500000000000006 	10 	
+2 	0.694999999999999951 	0.54500000000000004 	0.184999999999999998 	1.5714999999999999 	0.66449999999999998 	0.383500000000000008 	0.450500000000000012 	13 	
+2 	0.57999999999999996 	0.455000000000000016 	0.195000000000000007 	1.85899999999999999 	0.944999999999999951 	0.42599999999999999 	0.441000000000000003 	9 	
+2 	0.564999999999999947 	0.469999999999999973 	0.195000000000000007 	1.1419999999999999 	0.387000000000000011 	0.258000000000000007 	0.349999999999999978 	17 	
+1 	0.280000000000000027 	0.209999999999999992 	0.0850000000000000061 	0.107499999999999998 	0.0415000000000000022 	0.0240000000000000005 	0.0340000000000000024 	5 	
+2 	0.540000000000000036 	0.41499999999999998 	0.130000000000000004 	0.824500000000000011 	0.27200000000000002 	0.226000000000000006 	0.239999999999999991 	13 	
+2 	0.685000000000000053 	0.520000000000000018 	0.149999999999999994 	1.37349999999999994 	0.718500000000000028 	0.292999999999999983 	0.320000000000000007 	11 	
+2 	0.375 	0.304999999999999993 	0.0899999999999999967 	0.324500000000000011 	0.139500000000000013 	0.0565000000000000016 	0.0950000000000000011 	5 	
+2 	0.67000000000000004 	0.540000000000000036 	0.174999999999999989 	1.48199999999999998 	0.73899999999999999 	0.292499999999999982 	0.364999999999999991 	10 	
+0 	0.645000000000000018 	0.505000000000000004 	0.165000000000000008 	1.31800000000000006 	0.550000000000000044 	0.30149999999999999 	0.33500000000000002 	11 	
+2 	0.655000000000000027 	0.520000000000000018 	0.170000000000000012 	1.14450000000000007 	0.530000000000000027 	0.223000000000000004 	0.347999999999999976 	9 	
+1 	0.515000000000000013 	0.359999999999999987 	0.125 	0.472499999999999976 	0.181499999999999995 	0.125 	0.138000000000000012 	9 	
+0 	0.635000000000000009 	0.489999999999999991 	0.154999999999999999 	1.14500000000000002 	0.47749999999999998 	0.303499999999999992 	0.315500000000000003 	9 	
+2 	0.57999999999999996 	0.465000000000000024 	0.160000000000000003 	1.03449999999999998 	0.315000000000000002 	0.260000000000000009 	0.36349999999999999 	12 	
+2 	0.419999999999999984 	0.340000000000000024 	0.125 	0.449500000000000011 	0.165000000000000008 	0.112500000000000003 	0.143999999999999989 	11 	
+1 	0.46000000000000002 	0.354999999999999982 	0.110000000000000001 	0.425499999999999989 	0.201500000000000012 	0.0810000000000000026 	0.130000000000000004 	7 	
+1 	0.149999999999999994 	0.100000000000000006 	0.0250000000000000014 	0.0149999999999999994 	0.00449999999999999966 	0.00400000000000000008 	0.0050000000000000001 	2 	
+0 	0.469999999999999973 	0.359999999999999987 	0.14499999999999999 	0.537000000000000033 	0.172499999999999987 	0.137500000000000011 	0.195000000000000007 	15 	
+0 	0.630000000000000004 	0.494999999999999996 	0.14499999999999999 	1.14700000000000002 	0.545499999999999985 	0.266000000000000014 	0.288499999999999979 	9 	
+0 	0.469999999999999973 	0.375 	0.125 	0.561499999999999999 	0.252000000000000002 	0.137000000000000011 	0.179999999999999993 	10 	
+2 	0.574999999999999956 	0.400000000000000022 	0.154999999999999999 	0.932499999999999996 	0.360499999999999987 	0.244499999999999995 	0.299999999999999989 	17 	
+0 	0.599999999999999978 	0.450000000000000011 	0.140000000000000013 	0.836999999999999966 	0.369999999999999996 	0.176999999999999991 	0.242499999999999993 	10 	
+2 	0.630000000000000004 	0.5 	0.184999999999999998 	1.3620000000000001 	0.578500000000000014 	0.3125 	0.384000000000000008 	10 	
+0 	0.530000000000000027 	0.405000000000000027 	0.149999999999999994 	0.889000000000000012 	0.405500000000000027 	0.227500000000000008 	0.214999999999999997 	8 	
+0 	0.469999999999999973 	0.359999999999999987 	0.130000000000000004 	0.471999999999999975 	0.181999999999999995 	0.114000000000000004 	0.149999999999999994 	10 	
+2 	0.5 	0.419999999999999984 	0.135000000000000009 	0.67649999999999999 	0.301999999999999991 	0.141499999999999987 	0.206499999999999989 	9 	
+2 	0.424999999999999989 	0.325000000000000011 	0.100000000000000006 	0.329500000000000015 	0.13650000000000001 	0.072499999999999995 	0.110000000000000001 	7 	
+2 	0.5 	0.400000000000000022 	0.130000000000000004 	0.66449999999999998 	0.258000000000000007 	0.133000000000000007 	0.239999999999999991 	12 	
+2 	0.650000000000000022 	0.515000000000000013 	0.174999999999999989 	1.48049999999999993 	0.529499999999999971 	0.27200000000000002 	0.525000000000000022 	20 	
+2 	0.694999999999999951 	0.540000000000000036 	0.195000000000000007 	1.69100000000000006 	0.768000000000000016 	0.362999999999999989 	0.475499999999999978 	11 	
+1 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.824500000000000011 	0.337500000000000022 	0.211499999999999994 	0.23899999999999999 	11 	
+2 	0.555000000000000049 	0.424999999999999989 	0.149999999999999994 	0.872999999999999998 	0.462500000000000022 	0.184499999999999997 	0.196500000000000008 	9 	
+2 	0.569999999999999951 	0.440000000000000002 	0.154999999999999999 	1.1160000000000001 	0.47749999999999998 	0.231500000000000011 	0.270000000000000018 	13 	
+0 	0.479999999999999982 	0.375 	0.104999999999999996 	0.525000000000000022 	0.2185 	0.119499999999999995 	0.154999999999999999 	12 	
+0 	0.655000000000000027 	0.520000000000000018 	0.200000000000000011 	1.5475000000000001 	0.712999999999999967 	0.314000000000000001 	0.466000000000000025 	9 	
+0 	0.550000000000000044 	0.424999999999999989 	0.140000000000000013 	0.951999999999999957 	0.489499999999999991 	0.194500000000000006 	0.2185 	7 	
+2 	0.555000000000000049 	0.405000000000000027 	0.190000000000000002 	1.40599999999999992 	0.611500000000000044 	0.342000000000000026 	0.389000000000000012 	10 	
+2 	0.569999999999999951 	0.434999999999999998 	0.14499999999999999 	0.905499999999999972 	0.392500000000000016 	0.235499999999999987 	0.275000000000000022 	10 	
+1 	0.505000000000000004 	0.380000000000000004 	0.135000000000000009 	0.538499999999999979 	0.264500000000000013 	0.0950000000000000011 	0.165000000000000008 	9 	
+0 	0.569999999999999951 	0.450000000000000011 	0.135000000000000009 	0.780499999999999972 	0.33450000000000002 	0.184999999999999998 	0.209999999999999992 	8 	
+2 	0.564999999999999947 	0.455000000000000016 	0.149999999999999994 	0.95950000000000002 	0.456500000000000017 	0.239499999999999991 	0.23000000000000001 	9 	
+2 	0.619999999999999996 	0.515000000000000013 	0.174999999999999989 	1.22100000000000009 	0.535000000000000031 	0.240999999999999992 	0.395000000000000018 	13 	
+0 	0.474999999999999978 	0.380000000000000004 	0.135000000000000009 	0.485999999999999988 	0.173499999999999988 	0.0700000000000000067 	0.184999999999999998 	7 	
+0 	0.574999999999999956 	0.46000000000000002 	0.160000000000000003 	1.10299999999999998 	0.538000000000000034 	0.221000000000000002 	0.248999999999999999 	9 	
+0 	0.550000000000000044 	0.429999999999999993 	0.154999999999999999 	0.785000000000000031 	0.288999999999999979 	0.227000000000000007 	0.233000000000000013 	11 	
+0 	0.650000000000000022 	0.5 	0.190000000000000002 	1.46399999999999997 	0.641499999999999959 	0.339000000000000024 	0.424499999999999988 	9 	
+0 	0.719999999999999973 	0.57999999999999996 	0.195000000000000007 	2.1030000000000002 	1.02649999999999997 	0.479999999999999982 	0.537499999999999978 	10 	
+0 	0.450000000000000011 	0.359999999999999987 	0.104999999999999996 	0.471499999999999975 	0.203499999999999986 	0.0934999999999999998 	0.148999999999999994 	9 	
+2 	0.564999999999999947 	0.424999999999999989 	0.135000000000000009 	0.811499999999999999 	0.341000000000000025 	0.16750000000000001 	0.255000000000000004 	15 	
+0 	0.510000000000000009 	0.390000000000000013 	0.135000000000000009 	0.633499999999999952 	0.231000000000000011 	0.178999999999999992 	0.200000000000000011 	9 	
+2 	0.599999999999999978 	0.474999999999999978 	0.174999999999999989 	1.1100000000000001 	0.510499999999999954 	0.256000000000000005 	0.284999999999999976 	9 	
+2 	0.564999999999999947 	0.450000000000000011 	0.160000000000000003 	0.895000000000000018 	0.41499999999999998 	0.195000000000000007 	0.245999999999999996 	9 	
+2 	0.655000000000000027 	0.530000000000000027 	0.174999999999999989 	1.26350000000000007 	0.485999999999999988 	0.263500000000000012 	0.41499999999999998 	15 	
+0 	0.380000000000000004 	0.325000000000000011 	0.110000000000000001 	0.310499999999999998 	0.119999999999999996 	0.0739999999999999963 	0.104999999999999996 	10 	
+0 	0.574999999999999956 	0.465000000000000024 	0.140000000000000013 	0.957999999999999963 	0.442000000000000004 	0.181499999999999995 	0.270500000000000018 	9 	
+1 	0.434999999999999998 	0.344999999999999973 	0.119999999999999996 	0.321500000000000008 	0.130000000000000004 	0.0560000000000000012 	0.118499999999999994 	7 	
+1 	0.479999999999999982 	0.349999999999999978 	0.135000000000000009 	0.546499999999999986 	0.273500000000000021 	0.0995000000000000051 	0.158000000000000002 	8 	
+1 	0.299999999999999989 	0.220000000000000001 	0.0650000000000000022 	0.123499999999999999 	0.0589999999999999969 	0.0259999999999999988 	0.0315000000000000002 	5 	
+0 	0.689999999999999947 	0.54500000000000004 	0.204999999999999988 	1.93300000000000005 	0.785499999999999976 	0.428999999999999992 	0.497999999999999998 	13 	
+1 	0.550000000000000044 	0.419999999999999984 	0.154999999999999999 	0.912000000000000033 	0.494999999999999996 	0.180499999999999994 	0.204999999999999988 	9 	
+1 	0.359999999999999987 	0.270000000000000018 	0.0850000000000000061 	0.2185 	0.106499999999999997 	0.0379999999999999991 	0.0619999999999999996 	6 	
+1 	0.309999999999999998 	0.234999999999999987 	0.0700000000000000067 	0.150999999999999995 	0.0630000000000000004 	0.0405000000000000013 	0.0449999999999999983 	6 	
+1 	0.609999999999999987 	0.474999999999999978 	0.149999999999999994 	0.966500000000000026 	0.41449999999999998 	0.200000000000000011 	0.344999999999999973 	10 	
+2 	0.555000000000000049 	0.450000000000000011 	0.14499999999999999 	0.915000000000000036 	0.400000000000000022 	0.245999999999999996 	0.284999999999999976 	11 	
+0 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	1.02550000000000008 	0.411999999999999977 	0.274500000000000022 	0.288999999999999979 	11 	
+2 	0.640000000000000013 	0.574999999999999956 	0.174999999999999989 	1.45849999999999991 	0.625 	0.266000000000000014 	0.439500000000000002 	11 	
+2 	0.589999999999999969 	0.440000000000000002 	0.149999999999999994 	0.872500000000000053 	0.387000000000000011 	0.214999999999999997 	0.244999999999999996 	8 	
+0 	0.445000000000000007 	0.330000000000000016 	0.104999999999999996 	0.452500000000000013 	0.179999999999999993 	0.102999999999999994 	0.122999999999999998 	9 	
+2 	0.609999999999999987 	0.5 	0.239999999999999991 	1.6419999999999999 	0.532000000000000028 	0.33450000000000002 	0.689999999999999947 	18 	
+2 	0.645000000000000018 	0.515000000000000013 	0.160000000000000003 	1.18450000000000011 	0.506000000000000005 	0.310999999999999999 	0.33500000000000002 	9 	
+2 	0.515000000000000013 	0.419999999999999984 	0.140000000000000013 	0.769000000000000017 	0.2505 	0.153999999999999998 	0.28999999999999998 	13 	
+1 	0.419999999999999984 	0.320000000000000007 	0.100000000000000006 	0.340000000000000024 	0.174499999999999988 	0.0500000000000000028 	0.0945000000000000007 	8 	
+0 	0.505000000000000004 	0.409999999999999976 	0.149999999999999994 	0.644000000000000017 	0.284999999999999976 	0.14499999999999999 	0.209999999999999992 	11 	
+1 	0.225000000000000006 	0.160000000000000003 	0.0449999999999999983 	0.0464999999999999997 	0.0250000000000000014 	0.0149999999999999994 	0.0149999999999999994 	4 	
+2 	0.650000000000000022 	0.510000000000000009 	0.174999999999999989 	1.44599999999999995 	0.648499999999999965 	0.270500000000000018 	0.450000000000000011 	12 	
+0 	0.630000000000000004 	0.5 	0.170000000000000012 	1.31349999999999989 	0.559499999999999997 	0.267000000000000015 	0.400000000000000022 	20 	
+0 	0.574999999999999956 	0.469999999999999973 	0.165000000000000008 	0.868999999999999995 	0.434999999999999998 	0.197000000000000008 	0.237999999999999989 	9 	
+2 	0.540000000000000036 	0.419999999999999984 	0.135000000000000009 	0.807499999999999996 	0.348499999999999976 	0.179499999999999993 	0.234999999999999987 	11 	
+2 	0.489999999999999991 	0.380000000000000004 	0.140000000000000013 	0.760499999999999954 	0.244999999999999996 	0.16700000000000001 	0.184999999999999998 	10 	
+1 	0.505000000000000004 	0.400000000000000022 	0.125 	0.560499999999999998 	0.225500000000000006 	0.143499999999999989 	0.170000000000000012 	8 	
+2 	0.530000000000000027 	0.41499999999999998 	0.119999999999999996 	0.705999999999999961 	0.33550000000000002 	0.163500000000000006 	0.134500000000000008 	9 	
+2 	0.599999999999999978 	0.465000000000000024 	0.165000000000000008 	1.03800000000000003 	0.497499999999999998 	0.220500000000000002 	0.251000000000000001 	9 	
+2 	0.599999999999999978 	0.5 	0.160000000000000003 	1.0149999999999999 	0.399500000000000022 	0.173499999999999988 	0.330000000000000016 	19 	
+0 	0.709999999999999964 	0.560000000000000053 	0.179999999999999993 	1.65199999999999991 	0.734999999999999987 	0.381000000000000005 	0.452500000000000013 	11 	
+2 	0.719999999999999973 	0.564999999999999947 	0.14499999999999999 	1.18700000000000006 	0.690999999999999948 	0.194500000000000006 	0.268500000000000016 	8 	
+0 	0.489999999999999991 	0.369999999999999996 	0.140000000000000013 	0.584999999999999964 	0.242999999999999994 	0.115000000000000005 	0.195000000000000007 	10 	
+1 	0.440000000000000002 	0.330000000000000016 	0.110000000000000001 	0.370499999999999996 	0.154499999999999998 	0.0840000000000000052 	0.119999999999999996 	7 	
+0 	0.369999999999999996 	0.275000000000000022 	0.0850000000000000061 	0.240499999999999992 	0.103999999999999995 	0.0534999999999999989 	0.0700000000000000067 	5 	
+0 	0.54500000000000004 	0.440000000000000002 	0.149999999999999994 	0.947500000000000009 	0.365999999999999992 	0.23899999999999999 	0.275000000000000022 	8 	
+1 	0.275000000000000022 	0.204999999999999988 	0.0749999999999999972 	0.110500000000000001 	0.0449999999999999983 	0.028500000000000001 	0.0350000000000000033 	6 	
+0 	0.479999999999999982 	0.380000000000000004 	0.119999999999999996 	0.607999999999999985 	0.270500000000000018 	0.140500000000000014 	0.184999999999999998 	8 	
+1 	0.409999999999999976 	0.325000000000000011 	0.100000000000000006 	0.394000000000000017 	0.20799999999999999 	0.0655000000000000027 	0.105999999999999997 	6 	
+1 	0.5 	0.390000000000000013 	0.130000000000000004 	0.507499999999999951 	0.211499999999999994 	0.103999999999999995 	0.175499999999999989 	9 	
+2 	0.530000000000000027 	0.424999999999999989 	0.130000000000000004 	0.745500000000000052 	0.299499999999999988 	0.135500000000000009 	0.244999999999999996 	10 	
+0 	0.719999999999999973 	0.594999999999999973 	0.225000000000000006 	1.96900000000000008 	0.804499999999999993 	0.422999999999999987 	0.660000000000000031 	16 	
+0 	0.555000000000000049 	0.445000000000000007 	0.174999999999999989 	1.14650000000000007 	0.551000000000000045 	0.243999999999999995 	0.278500000000000025 	8 	
+1 	0.375 	0.284999999999999976 	0.0899999999999999967 	0.236999999999999988 	0.105999999999999997 	0.0395000000000000004 	0.0800000000000000017 	8 	
+1 	0.564999999999999947 	0.434999999999999998 	0.14499999999999999 	0.844500000000000028 	0.39750000000000002 	0.158000000000000002 	0.255000000000000004 	9 	
+2 	0.465000000000000024 	0.349999999999999978 	0.140000000000000013 	0.575500000000000012 	0.201500000000000012 	0.150499999999999995 	0.190000000000000002 	15 	
+1 	0.424999999999999989 	0.309999999999999998 	0.0950000000000000011 	0.307499999999999996 	0.139000000000000012 	0.0744999999999999968 	0.0929999999999999993 	7 	
+1 	0.46000000000000002 	0.369999999999999996 	0.119999999999999996 	0.533499999999999974 	0.264500000000000013 	0.107999999999999999 	0.134500000000000008 	6 	
+1 	0.494999999999999996 	0.375 	0.140000000000000013 	0.493999999999999995 	0.180999999999999994 	0.0975000000000000033 	0.191000000000000003 	8 	
+0 	0.5 	0.395000000000000018 	0.140000000000000013 	0.715500000000000025 	0.316500000000000004 	0.17599999999999999 	0.239999999999999991 	10 	
+0 	0.645000000000000018 	0.520000000000000018 	0.179999999999999993 	1.28499999999999992 	0.577500000000000013 	0.35199999999999998 	0.317000000000000004 	9 	
+1 	0.494999999999999996 	0.375 	0.115000000000000005 	0.507000000000000006 	0.240999999999999992 	0.102999999999999994 	0.149999999999999994 	8 	
+0 	0.584999999999999964 	0.450000000000000011 	0.160000000000000003 	0.904499999999999971 	0.405000000000000027 	0.221500000000000002 	0.233500000000000013 	8 	
+2 	0.5 	0.364999999999999991 	0.130000000000000004 	0.594500000000000028 	0.308999999999999997 	0.108499999999999999 	0.153499999999999998 	9 	
+1 	0.375 	0.275000000000000022 	0.0899999999999999967 	0.237999999999999989 	0.107499999999999998 	0.0544999999999999998 	0.0700000000000000067 	6 	
+1 	0.190000000000000002 	0.130000000000000004 	0.0299999999999999989 	0.0294999999999999984 	0.0154999999999999999 	0.0149999999999999994 	0.0100000000000000002 	6 	
+0 	0.54500000000000004 	0.440000000000000002 	0.174999999999999989 	0.774499999999999966 	0.298499999999999988 	0.1875 	0.265000000000000013 	11 	
+1 	0.359999999999999987 	0.270000000000000018 	0.0850000000000000061 	0.196000000000000008 	0.0874999999999999944 	0.0350000000000000033 	0.0640000000000000013 	4 	
+2 	0.619999999999999996 	0.479999999999999982 	0.165000000000000008 	1.07250000000000001 	0.481499999999999984 	0.234999999999999987 	0.312 	9 	
+0 	0.564999999999999947 	0.445000000000000007 	0.154999999999999999 	0.825999999999999956 	0.341000000000000025 	0.205499999999999988 	0.247499999999999998 	10 	
+1 	0.525000000000000022 	0.400000000000000022 	0.110000000000000001 	0.627499999999999947 	0.30149999999999999 	0.126000000000000001 	0.179999999999999993 	8 	
+1 	0.385000000000000009 	0.280000000000000027 	0.100000000000000006 	0.275500000000000023 	0.130500000000000005 	0.0609999999999999987 	0.072499999999999995 	8 	
+1 	0.434999999999999998 	0.330000000000000016 	0.110000000000000001 	0.380000000000000004 	0.151499999999999996 	0.0945000000000000007 	0.110000000000000001 	7 	
+0 	0.635000000000000009 	0.505000000000000004 	0.170000000000000012 	1.41500000000000004 	0.604999999999999982 	0.296999999999999986 	0.364999999999999991 	15 	
+0 	0.569999999999999951 	0.469999999999999973 	0.140000000000000013 	0.870999999999999996 	0.385000000000000009 	0.210999999999999993 	0.231500000000000011 	10 	
+0 	0.635000000000000009 	0.5 	0.149999999999999994 	1.37599999999999989 	0.649499999999999966 	0.360999999999999988 	0.309999999999999998 	10 	
+0 	0.719999999999999973 	0.574999999999999956 	0.179999999999999993 	1.6705000000000001 	0.731999999999999984 	0.360499999999999987 	0.501000000000000001 	12 	
+0 	0.619999999999999996 	0.540000000000000036 	0.165000000000000008 	1.13900000000000001 	0.4995 	0.243499999999999994 	0.356999999999999984 	11 	
+0 	0.589999999999999969 	0.46000000000000002 	0.160000000000000003 	1.01150000000000007 	0.445000000000000007 	0.26150000000000001 	0.256500000000000006 	8 	
+2 	0.564999999999999947 	0.429999999999999993 	0.149999999999999994 	0.830999999999999961 	0.424499999999999988 	0.173499999999999988 	0.219 	10 	
+1 	0.440000000000000002 	0.340000000000000024 	0.119999999999999996 	0.4995 	0.296499999999999986 	0.0945000000000000007 	0.118499999999999994 	6 	
+1 	0.41499999999999998 	0.315000000000000002 	0.100000000000000006 	0.364499999999999991 	0.17649999999999999 	0.0795000000000000012 	0.0950000000000000011 	8 	
+2 	0.525000000000000022 	0.364999999999999991 	0.170000000000000012 	0.96050000000000002 	0.438 	0.222500000000000003 	0.276000000000000023 	10 	
+0 	0.599999999999999978 	0.465000000000000024 	0.160000000000000003 	1.13300000000000001 	0.466000000000000025 	0.288499999999999979 	0.297999999999999987 	11 	
+2 	0.46000000000000002 	0.354999999999999982 	0.140000000000000013 	0.490999999999999992 	0.20699999999999999 	0.115000000000000005 	0.173999999999999988 	10 	
+2 	0.209999999999999992 	0.149999999999999994 	0.0500000000000000028 	0.0384999999999999995 	0.0154999999999999999 	0.00850000000000000061 	0.0100000000000000002 	3 	
+1 	0.510000000000000009 	0.400000000000000022 	0.14499999999999999 	0.577500000000000013 	0.231000000000000011 	0.142999999999999988 	0.176999999999999991 	9 	
+2 	0.660000000000000031 	0.5 	0.165000000000000008 	1.3194999999999999 	0.667000000000000037 	0.269000000000000017 	0.341000000000000025 	9 	
+2 	0.569999999999999951 	0.455000000000000016 	0.154999999999999999 	0.831999999999999962 	0.358499999999999985 	0.173999999999999988 	0.277000000000000024 	11 	
+0 	0.645000000000000018 	0.489999999999999991 	0.160000000000000003 	1.14399999999999991 	0.501499999999999946 	0.288999999999999979 	0.319000000000000006 	8 	
+2 	0.67000000000000004 	0.510000000000000009 	0.179999999999999993 	1.67999999999999994 	0.926000000000000045 	0.297499999999999987 	0.393500000000000016 	13 	
+1 	0.540000000000000036 	0.395000000000000018 	0.135000000000000009 	0.655499999999999972 	0.270500000000000018 	0.154999999999999999 	0.192000000000000004 	9 	
+0 	0.540000000000000036 	0.419999999999999984 	0.14499999999999999 	0.865500000000000047 	0.431499999999999995 	0.163000000000000006 	0.217499999999999999 	10 	
+0 	0.625 	0.489999999999999991 	0.200000000000000011 	1.38250000000000006 	0.589500000000000024 	0.284999999999999976 	0.381000000000000005 	11 	
+2 	0.680000000000000049 	0.515000000000000013 	0.160000000000000003 	1.23449999999999993 	0.617999999999999994 	0.262500000000000011 	0.325000000000000011 	11 	
+0 	0.609999999999999987 	0.484999999999999987 	0.149999999999999994 	1.24049999999999994 	0.602500000000000036 	0.291499999999999981 	0.308499999999999996 	12 	
+2 	0.650000000000000022 	0.520000000000000018 	0.154999999999999999 	1.3680000000000001 	0.61850000000000005 	0.287999999999999978 	0.364999999999999991 	9 	
+1 	0.25 	0.190000000000000002 	0.0650000000000000022 	0.0835000000000000048 	0.0389999999999999999 	0.0149999999999999994 	0.0250000000000000014 	5 	
+1 	0.535000000000000031 	0.400000000000000022 	0.135000000000000009 	0.775000000000000022 	0.367999999999999994 	0.20799999999999999 	0.205499999999999988 	8 	
+1 	0.354999999999999982 	0.255000000000000004 	0.0800000000000000017 	0.187 	0.0779999999999999999 	0.0505000000000000032 	0.0580000000000000029 	7 	
+0 	0.440000000000000002 	0.340000000000000024 	0.130000000000000004 	0.419499999999999984 	0.152999999999999997 	0.115500000000000005 	0.130000000000000004 	10 	
+0 	0.41499999999999998 	0.304999999999999993 	0.130000000000000004 	0.320000000000000007 	0.130500000000000005 	0.0754999999999999977 	0.104999999999999996 	8 	
+0 	0.635000000000000009 	0.505000000000000004 	0.165000000000000008 	1.25099999999999989 	0.576999999999999957 	0.227000000000000007 	0.382500000000000007 	11 	
+1 	0.275000000000000022 	0.200000000000000011 	0.0650000000000000022 	0.0919999999999999984 	0.0384999999999999995 	0.0235000000000000001 	0.0269999999999999997 	5 	
+1 	0.320000000000000007 	0.239999999999999991 	0.0749999999999999972 	0.173499999999999988 	0.0759999999999999981 	0.0354999999999999968 	0.0500000000000000028 	7 	
+1 	0.479999999999999982 	0.354999999999999982 	0.115000000000000005 	0.472499999999999976 	0.206499999999999989 	0.112000000000000002 	0.132000000000000006 	8 	
+1 	0.359999999999999987 	0.275000000000000022 	0.110000000000000001 	0.233500000000000013 	0.0950000000000000011 	0.0524999999999999981 	0.0850000000000000061 	10 	
+0 	0.569999999999999951 	0.46000000000000002 	0.135000000000000009 	0.979500000000000037 	0.39700000000000002 	0.252500000000000002 	0.265500000000000014 	9 	
+2 	0.699999999999999956 	0.54500000000000004 	0.184999999999999998 	1.61349999999999993 	0.75 	0.403500000000000025 	0.368499999999999994 	11 	
+0 	0.635000000000000009 	0.484999999999999987 	0.190000000000000002 	1.37650000000000006 	0.634000000000000008 	0.288499999999999979 	0.406000000000000028 	11 	
+1 	0.530000000000000027 	0.400000000000000022 	0.14499999999999999 	0.555000000000000049 	0.193500000000000005 	0.130500000000000005 	0.195000000000000007 	9 	
+1 	0.280000000000000027 	0.204999999999999988 	0.0700000000000000067 	0.101500000000000007 	0.0410000000000000017 	0.0299999999999999989 	0.0299999999999999989 	6 	
+2 	0.655000000000000027 	0.540000000000000036 	0.165000000000000008 	1.40300000000000002 	0.695500000000000007 	0.23849999999999999 	0.419999999999999984 	11 	
+2 	0.645000000000000018 	0.494999999999999996 	0.190000000000000002 	1.53899999999999992 	0.611500000000000044 	0.407999999999999974 	0.445000000000000007 	12 	
+0 	0.57999999999999996 	0.455000000000000016 	0.170000000000000012 	0.907499999999999973 	0.373999999999999999 	0.213499999999999995 	0.284999999999999976 	13 	
+0 	0.57999999999999996 	0.46000000000000002 	0.179999999999999993 	1.0515000000000001 	0.409499999999999975 	0.259500000000000008 	0.276000000000000023 	8 	
+2 	0.560000000000000053 	0.424999999999999989 	0.135000000000000009 	0.941500000000000004 	0.509000000000000008 	0.201500000000000012 	0.197500000000000009 	9 	
+2 	0.469999999999999973 	0.369999999999999996 	0.119999999999999996 	0.579500000000000015 	0.292999999999999983 	0.227000000000000007 	0.140000000000000013 	9 	
+1 	0.369999999999999996 	0.275000000000000022 	0.0899999999999999967 	0.206499999999999989 	0.096000000000000002 	0.0395000000000000004 	0.0580000000000000029 	7 	
+2 	0.525000000000000022 	0.409999999999999976 	0.130000000000000004 	0.6875 	0.343500000000000028 	0.149499999999999994 	0.17649999999999999 	9 	
+2 	0.54500000000000004 	0.450000000000000011 	0.149999999999999994 	0.879499999999999948 	0.387000000000000011 	0.149999999999999994 	0.262500000000000011 	11 	
+0 	0.429999999999999993 	0.325000000000000011 	0.119999999999999996 	0.445000000000000007 	0.165000000000000008 	0.0995000000000000051 	0.154999999999999999 	8 	
+1 	0.520000000000000018 	0.424999999999999989 	0.170000000000000012 	0.680499999999999994 	0.280000000000000027 	0.173999999999999988 	0.195000000000000007 	10 	
+0 	0.665000000000000036 	0.515000000000000013 	0.184999999999999998 	1.34050000000000002 	0.559499999999999997 	0.292999999999999983 	0.4375 	11 	
+0 	0.599999999999999978 	0.474999999999999978 	0.149999999999999994 	1.00750000000000006 	0.442500000000000004 	0.221000000000000002 	0.280000000000000027 	15 	
+1 	0.440000000000000002 	0.364999999999999991 	0.115000000000000005 	0.501000000000000001 	0.243499999999999994 	0.0840000000000000052 	0.146499999999999991 	9 	
+2 	0.385000000000000009 	0.284999999999999976 	0.104999999999999996 	0.29049999999999998 	0.121499999999999997 	0.0685000000000000053 	0.0874999999999999944 	12 	
+0 	0.594999999999999973 	0.479999999999999982 	0.160000000000000003 	1.20950000000000002 	0.522499999999999964 	0.295999999999999985 	0.320000000000000007 	8 	
+1 	0.530000000000000027 	0.429999999999999993 	0.160000000000000003 	0.724500000000000033 	0.321000000000000008 	0.127500000000000002 	0.239999999999999991 	9 	
+2 	0.574999999999999956 	0.424999999999999989 	0.140000000000000013 	0.863500000000000045 	0.393000000000000016 	0.227000000000000007 	0.200000000000000011 	11 	
+1 	0.325000000000000011 	0.25 	0.0700000000000000067 	0.174499999999999988 	0.0874999999999999944 	0.0354999999999999968 	0.0400000000000000008 	7 	
+1 	0.28999999999999998 	0.214999999999999997 	0.0599999999999999978 	0.111500000000000002 	0.0529999999999999985 	0.0184999999999999991 	0.0320000000000000007 	5 	
+0 	0.675000000000000044 	0.510000000000000009 	0.184999999999999998 	1.47300000000000009 	0.629499999999999948 	0.302499999999999991 	0.424499999999999988 	11 	
+1 	0.589999999999999969 	0.474999999999999978 	0.14499999999999999 	0.974500000000000033 	0.467500000000000027 	0.20699999999999999 	0.259000000000000008 	10 	
+0 	0.535000000000000031 	0.424999999999999989 	0.135000000000000009 	0.771000000000000019 	0.376500000000000001 	0.181499999999999995 	0.179499999999999993 	8 	
+2 	0.594999999999999973 	0.469999999999999973 	0.174999999999999989 	0.990999999999999992 	0.382000000000000006 	0.239499999999999991 	0.5 	12 	
+2 	0.474999999999999978 	0.375 	0.119999999999999996 	0.562999999999999945 	0.252500000000000002 	0.120499999999999996 	0.184999999999999998 	10 	
+1 	0.474999999999999978 	0.364999999999999991 	0.115000000000000005 	0.489999999999999991 	0.223000000000000004 	0.123499999999999999 	0.133500000000000008 	9 	
+1 	0.419999999999999984 	0.309999999999999998 	0.100000000000000006 	0.286499999999999977 	0.115000000000000005 	0.0734999999999999959 	0.0850000000000000061 	8 	
+1 	0.474999999999999978 	0.359999999999999987 	0.125 	0.490499999999999992 	0.204999999999999988 	0.130500000000000005 	0.125 	8 	
+2 	0.635000000000000009 	0.525000000000000022 	0.204999999999999988 	1.48399999999999999 	0.550000000000000044 	0.311499999999999999 	0.429999999999999993 	20 	
+0 	0.540000000000000036 	0.440000000000000002 	0.135000000000000009 	0.958999999999999964 	0.23849999999999999 	0.221000000000000002 	0.299999999999999989 	17 	
+1 	0.33500000000000002 	0.25 	0.0749999999999999972 	0.182499999999999996 	0.0704999999999999932 	0.0439999999999999974 	0.0550000000000000003 	7 	
+0 	0.660000000000000031 	0.525000000000000022 	0.179999999999999993 	1.59650000000000003 	0.776499999999999968 	0.39700000000000002 	0.360499999999999987 	10 	
+0 	0.489999999999999991 	0.380000000000000004 	0.125 	0.549000000000000044 	0.244999999999999996 	0.107499999999999998 	0.173999999999999988 	10 	
+2 	0.734999999999999987 	0.569999999999999951 	0.174999999999999989 	1.87999999999999989 	0.909499999999999975 	0.387000000000000011 	0.487999999999999989 	11 	
+0 	0.510000000000000009 	0.400000000000000022 	0.125 	0.54500000000000004 	0.26100000000000001 	0.115000000000000005 	0.138500000000000012 	6 	
+2 	0.564999999999999947 	0.440000000000000002 	0.174999999999999989 	1.12200000000000011 	0.393000000000000016 	0.200000000000000011 	0.375 	20 	
+1 	0.354999999999999982 	0.275000000000000022 	0.0850000000000000061 	0.220000000000000001 	0.0919999999999999984 	0.0599999999999999978 	0.149999999999999994 	8 	
+1 	0.309999999999999998 	0.225000000000000006 	0.0749999999999999972 	0.154999999999999999 	0.0650000000000000022 	0.0369999999999999982 	0.0364999999999999977 	6 	
+1 	0.354999999999999982 	0.280000000000000027 	0.0850000000000000061 	0.29049999999999998 	0.0950000000000000011 	0.0395000000000000004 	0.115000000000000005 	7 	
+0 	0.650000000000000022 	0.535000000000000031 	0.174999999999999989 	1.28950000000000009 	0.609500000000000042 	0.276500000000000024 	0.343999999999999972 	10 	
+0 	0.675000000000000044 	0.54500000000000004 	0.195000000000000007 	1.73449999999999993 	0.684499999999999997 	0.369499999999999995 	0.604999999999999982 	20 	
+1 	0.369999999999999996 	0.280000000000000027 	0.0950000000000000011 	0.286499999999999977 	0.150499999999999995 	0.0690000000000000058 	0.0795000000000000012 	7 	
+2 	0.599999999999999978 	0.450000000000000011 	0.195000000000000007 	1.34000000000000008 	0.616999999999999993 	0.325500000000000012 	0.360499999999999987 	10 	
+1 	0.560000000000000053 	0.455000000000000016 	0.14499999999999999 	0.973999999999999977 	0.547000000000000042 	0.161500000000000005 	0.234999999999999987 	9 	
+0 	0.635000000000000009 	0.494999999999999996 	0.195000000000000007 	1.29699999999999993 	0.55600000000000005 	0.298499999999999988 	0.369999999999999996 	11 	
+0 	0.609999999999999987 	0.484999999999999987 	0.165000000000000008 	1.09149999999999991 	0.393500000000000016 	0.243499999999999994 	0.330000000000000016 	18 	
+2 	0.614999999999999991 	0.469999999999999973 	0.154999999999999999 	1.19999999999999996 	0.508499999999999952 	0.320000000000000007 	0.291999999999999982 	8 	
+1 	0.349999999999999978 	0.234999999999999987 	0.0800000000000000017 	0.170000000000000012 	0.072499999999999995 	0.0464999999999999997 	0.0495000000000000023 	7 	
+0 	0.619999999999999996 	0.474999999999999978 	0.160000000000000003 	1.32450000000000001 	0.686499999999999999 	0.233000000000000013 	0.327500000000000013 	9 	
+1 	0.315000000000000002 	0.234999999999999987 	0.0749999999999999972 	0.148499999999999993 	0.0585000000000000034 	0.0374999999999999986 	0.0425000000000000031 	6 	
+2 	0.675000000000000044 	0.54500000000000004 	0.184999999999999998 	1.73750000000000004 	0.876000000000000001 	0.313500000000000001 	0.468999999999999972 	13 	
+0 	0.594999999999999973 	0.474999999999999978 	0.160000000000000003 	1.14050000000000007 	0.547000000000000042 	0.231000000000000011 	0.271000000000000019 	6 	
+2 	0.550000000000000044 	0.409999999999999976 	0.125 	0.760499999999999954 	0.2505 	0.163500000000000006 	0.195000000000000007 	14 	
+1 	0.550000000000000044 	0.405000000000000027 	0.149999999999999994 	0.675499999999999989 	0.30149999999999999 	0.146499999999999991 	0.209999999999999992 	10 	
+2 	0.584999999999999964 	0.455000000000000016 	0.125 	1.02699999999999991 	0.391000000000000014 	0.211999999999999994 	0.25 	17 	
+1 	0.540000000000000036 	0.429999999999999993 	0.170000000000000012 	0.835999999999999965 	0.372499999999999998 	0.181499999999999995 	0.239999999999999991 	9 	
+1 	0.494999999999999996 	0.390000000000000013 	0.125 	0.66549999999999998 	0.283999999999999975 	0.162000000000000005 	0.200000000000000011 	11 	
+1 	0.484999999999999987 	0.380000000000000004 	0.125 	0.521499999999999964 	0.221500000000000002 	0.117999999999999994 	0.160000000000000003 	8 	
+1 	0.41499999999999998 	0.315000000000000002 	0.0899999999999999967 	0.362499999999999989 	0.174999999999999989 	0.0835000000000000048 	0.0929999999999999993 	6 	
+2 	0.465000000000000024 	0.349999999999999978 	0.119999999999999996 	0.520499999999999963 	0.201500000000000012 	0.162500000000000006 	0.184999999999999998 	11 	
+0 	0.625 	0.489999999999999991 	0.14499999999999999 	0.92000000000000004 	0.437 	0.173499999999999988 	0.280000000000000027 	10 	
+1 	0.28999999999999998 	0.209999999999999992 	0.0700000000000000067 	0.111500000000000002 	0.048000000000000001 	0.0205000000000000009 	0.0299999999999999989 	5 	
+2 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.921499999999999986 	0.353999999999999981 	0.208999999999999991 	0.236499999999999988 	9 	
+0 	0.550000000000000044 	0.465000000000000024 	0.179999999999999993 	1.21249999999999991 	0.324500000000000011 	0.204999999999999988 	0.525000000000000022 	27 	
+0 	0.57999999999999996 	0.434999999999999998 	0.149999999999999994 	0.838999999999999968 	0.348499999999999976 	0.20699999999999999 	0.192000000000000004 	7 	
+2 	0.589999999999999969 	0.5 	0.200000000000000011 	1.18700000000000006 	0.411999999999999977 	0.270500000000000018 	0.369999999999999996 	16 	
+2 	0.660000000000000031 	0.515000000000000013 	0.165000000000000008 	1.4464999999999999 	0.69399999999999995 	0.297999999999999987 	0.3755 	10 	
+0 	0.734999999999999987 	0.564999999999999947 	0.204999999999999988 	2.12749999999999995 	0.948999999999999955 	0.46000000000000002 	0.564999999999999947 	12 	
+0 	0.46000000000000002 	0.359999999999999987 	0.115000000000000005 	0.475499999999999978 	0.210499999999999993 	0.104999999999999996 	0.160000000000000003 	8 	
+0 	0.550000000000000044 	0.429999999999999993 	0.149999999999999994 	0.655000000000000027 	0.263500000000000012 	0.121999999999999997 	0.221000000000000002 	8 	
+1 	0.494999999999999996 	0.380000000000000004 	0.135000000000000009 	0.509499999999999953 	0.206499999999999989 	0.116500000000000006 	0.165000000000000008 	8 	
+1 	0.385000000000000009 	0.299999999999999989 	0.0950000000000000011 	0.301999999999999991 	0.151999999999999996 	0.0614999999999999991 	0.0734999999999999959 	7 	
+2 	0.604999999999999982 	0.489999999999999991 	0.179999999999999993 	1.16700000000000004 	0.457000000000000017 	0.28999999999999998 	0.3745 	9 	
+1 	0.395000000000000018 	0.299999999999999989 	0.119999999999999996 	0.299499999999999988 	0.126500000000000001 	0.0680000000000000049 	0.0894999999999999962 	8 	
+2 	0.655000000000000027 	0.589999999999999969 	0.200000000000000011 	1.5455000000000001 	0.654000000000000026 	0.376500000000000001 	0.41499999999999998 	11 	
+0 	0.699999999999999956 	0.555000000000000049 	0.220000000000000001 	1.66599999999999993 	0.64700000000000002 	0.428499999999999992 	0.455000000000000016 	11 	
+0 	0.594999999999999973 	0.46000000000000002 	0.140000000000000013 	1.00449999999999995 	0.465500000000000025 	0.209499999999999992 	0.251500000000000001 	9 	
+0 	0.560000000000000053 	0.434999999999999998 	0.149999999999999994 	0.871500000000000052 	0.475499999999999978 	0.183499999999999996 	0.183499999999999996 	9 	
+1 	0.41499999999999998 	0.33500000000000002 	0.100000000000000006 	0.357999999999999985 	0.169000000000000011 	0.067000000000000004 	0.104999999999999996 	7 	
+0 	0.635000000000000009 	0.535000000000000031 	0.190000000000000002 	1.24199999999999999 	0.575999999999999956 	0.247499999999999998 	0.390000000000000013 	14 	
+0 	0.41499999999999998 	0.325000000000000011 	0.115000000000000005 	0.345499999999999974 	0.140500000000000014 	0.0764999999999999986 	0.110000000000000001 	9 	
+2 	0.609999999999999987 	0.474999999999999978 	0.165000000000000008 	1.1160000000000001 	0.427999999999999992 	0.220500000000000002 	0.315000000000000002 	15 	
+2 	0.569999999999999951 	0.450000000000000011 	0.135000000000000009 	1.02000000000000002 	0.546000000000000041 	0.203999999999999987 	0.25 	9 	
+0 	0.645000000000000018 	0.520000000000000018 	0.209999999999999992 	1.5535000000000001 	0.615999999999999992 	0.365499999999999992 	0.473999999999999977 	16 	
+1 	0.225000000000000006 	0.170000000000000012 	0.0700000000000000067 	0.0565000000000000016 	0.0240000000000000005 	0.0129999999999999994 	0.0160000000000000003 	4 	
+2 	0.57999999999999996 	0.469999999999999973 	0.165000000000000008 	0.997500000000000053 	0.393500000000000016 	0.241999999999999993 	0.330000000000000016 	10 	
+2 	0.569999999999999951 	0.434999999999999998 	0.130000000000000004 	0.753499999999999948 	0.348999999999999977 	0.175499999999999989 	0.194000000000000006 	10 	
+0 	0.609999999999999987 	0.494999999999999996 	0.184999999999999998 	1.10850000000000004 	0.370499999999999996 	0.313500000000000001 	0.330000000000000016 	12 	
+0 	0.604999999999999982 	0.455000000000000016 	0.14499999999999999 	0.977500000000000036 	0.468000000000000027 	0.177499999999999991 	0.275000000000000022 	9 	
+2 	0.675000000000000044 	0.555000000000000049 	0.200000000000000011 	1.43849999999999989 	0.54500000000000004 	0.266500000000000015 	0.465000000000000024 	21 	
+0 	0.564999999999999947 	0.445000000000000007 	0.125 	0.830500000000000016 	0.313500000000000001 	0.178499999999999992 	0.23000000000000001 	11 	
+1 	0.160000000000000003 	0.110000000000000001 	0.0250000000000000014 	0.0195 	0.00749999999999999972 	0.0050000000000000001 	0.00600000000000000012 	4 	
+0 	0.645000000000000018 	0.5 	0.154999999999999999 	1.22049999999999992 	0.614500000000000046 	0.235999999999999988 	0.318500000000000005 	10 	
+1 	0.375 	0.275000000000000022 	0.0950000000000000011 	0.246499999999999997 	0.110000000000000001 	0.0415000000000000022 	0.0774999999999999994 	6 	
+1 	0.474999999999999978 	0.359999999999999987 	0.110000000000000001 	0.491999999999999993 	0.210999999999999993 	0.110000000000000001 	0.149999999999999994 	8 	
+1 	0.625 	0.484999999999999987 	0.149999999999999994 	1.04400000000000004 	0.438 	0.286499999999999977 	0.278000000000000025 	9 	
+2 	0.41499999999999998 	0.315000000000000002 	0.125 	0.388000000000000012 	0.0680000000000000049 	0.0899999999999999967 	0.125 	12 	
+1 	0.505000000000000004 	0.424999999999999989 	0.125 	0.611500000000000044 	0.244999999999999996 	0.137500000000000011 	0.200000000000000011 	9 	
+0 	0.604999999999999982 	0.469999999999999973 	0.154999999999999999 	0.973999999999999977 	0.393000000000000016 	0.224000000000000005 	0.33450000000000002 	9 	
+1 	0.445000000000000007 	0.330000000000000016 	0.119999999999999996 	0.346999999999999975 	0.119999999999999996 	0.0840000000000000052 	0.104999999999999996 	11 	
+1 	0.46000000000000002 	0.359999999999999987 	0.140000000000000013 	0.447000000000000008 	0.161000000000000004 	0.086999999999999994 	0.160000000000000003 	9 	
+1 	0.405000000000000027 	0.304999999999999993 	0.0899999999999999967 	0.282499999999999973 	0.114000000000000004 	0.0575000000000000025 	0.0950000000000000011 	7 	
+1 	0.354999999999999982 	0.280000000000000027 	0.100000000000000006 	0.227500000000000008 	0.0934999999999999998 	0.0454999999999999988 	0.0850000000000000061 	11 	
+2 	0.530000000000000027 	0.405000000000000027 	0.125 	0.651499999999999968 	0.271500000000000019 	0.160500000000000004 	0.185999999999999999 	7 	
+0 	0.625 	0.515000000000000013 	0.149999999999999994 	1.24150000000000005 	0.523499999999999965 	0.306499999999999995 	0.359999999999999987 	15 	
+2 	0.594999999999999973 	0.474999999999999978 	0.170000000000000012 	1.09650000000000003 	0.418999999999999984 	0.229000000000000009 	0.349999999999999978 	17 	
+2 	0.465000000000000024 	0.340000000000000024 	0.104999999999999996 	0.485999999999999988 	0.231000000000000011 	0.103499999999999995 	0.122499999999999998 	9 	
+1 	0.304999999999999993 	0.214999999999999997 	0.0650000000000000022 	0.107499999999999998 	0.0439999999999999974 	0.0205000000000000009 	0.0379999999999999991 	5 	
+1 	0.540000000000000036 	0.390000000000000013 	0.125 	0.625499999999999945 	0.252500000000000002 	0.158000000000000002 	0.190000000000000002 	8 	
+1 	0.5 	0.375 	0.140000000000000013 	0.559000000000000052 	0.237499999999999989 	0.135000000000000009 	0.169000000000000011 	9 	
+0 	0.530000000000000027 	0.405000000000000027 	0.130000000000000004 	0.635499999999999954 	0.263500000000000012 	0.1565 	0.184999999999999998 	9 	
+0 	0.465000000000000024 	0.349999999999999978 	0.125 	0.481999999999999984 	0.23000000000000001 	0.105999999999999997 	0.1095 	6 	
+0 	0.655000000000000027 	0.505000000000000004 	0.190000000000000002 	1.34850000000000003 	0.593500000000000028 	0.274500000000000022 	0.424999999999999989 	12 	
+0 	0.645000000000000018 	0.515000000000000013 	0.174999999999999989 	1.54600000000000004 	0.703500000000000014 	0.364999999999999991 	0.41499999999999998 	10 	
+2 	0.280000000000000027 	0.209999999999999992 	0.0800000000000000017 	0.108499999999999999 	0.0410000000000000017 	0.0264999999999999993 	0.0345000000000000029 	7 	
+0 	0.609999999999999987 	0.46000000000000002 	0.14499999999999999 	1.11850000000000005 	0.47799999999999998 	0.294499999999999984 	0.298499999999999988 	10 	
+1 	0.450000000000000011 	0.344999999999999973 	0.110000000000000001 	0.469999999999999973 	0.235499999999999987 	0.0855000000000000066 	0.113500000000000004 	7 	
+2 	0.694999999999999951 	0.550000000000000044 	0.214999999999999997 	1.95649999999999991 	0.712500000000000022 	0.541000000000000036 	0.589999999999999969 	14 	
+2 	0.349999999999999978 	0.255000000000000004 	0.0850000000000000061 	0.214499999999999996 	0.100000000000000006 	0.0464999999999999997 	0.0599999999999999978 	13 	
+0 	0.520000000000000018 	0.409999999999999976 	0.154999999999999999 	0.72699999999999998 	0.290999999999999981 	0.183499999999999996 	0.234999999999999987 	12 	
+2 	0.5 	0.369999999999999996 	0.149999999999999994 	1.06150000000000011 	0.493999999999999995 	0.223000000000000004 	0.295999999999999985 	9 	
+2 	0.724999999999999978 	0.550000000000000044 	0.200000000000000011 	1.51000000000000001 	0.873500000000000054 	0.42649999999999999 	0.508499999999999952 	9 	
+1 	0.25 	0.179999999999999993 	0.0650000000000000022 	0.0685000000000000053 	0.0245000000000000009 	0.0154999999999999999 	0.0224999999999999992 	5 	
+0 	0.619999999999999996 	0.469999999999999973 	0.225000000000000006 	1.11499999999999999 	0.378000000000000003 	0.214499999999999996 	0.359999999999999987 	15 	
+1 	0.530000000000000027 	0.41499999999999998 	0.14499999999999999 	0.94399999999999995 	0.384500000000000008 	0.184999999999999998 	0.265000000000000013 	21 	
+0 	0.619999999999999996 	0.515000000000000013 	0.154999999999999999 	1.3254999999999999 	0.668499999999999983 	0.260500000000000009 	0.33500000000000002 	12 	
+1 	0.419999999999999984 	0.315000000000000002 	0.100000000000000006 	0.343500000000000028 	0.157000000000000001 	0.0795000000000000012 	0.0899999999999999967 	6 	
+2 	0.54500000000000004 	0.434999999999999998 	0.14499999999999999 	0.938500000000000001 	0.368499999999999994 	0.1245 	0.344999999999999973 	11 	
+0 	0.614999999999999991 	0.455000000000000016 	0.14499999999999999 	1.11549999999999994 	0.504499999999999948 	0.237999999999999989 	0.315000000000000002 	10 	
+2 	0.505000000000000004 	0.400000000000000022 	0.130000000000000004 	0.764000000000000012 	0.303499999999999992 	0.189000000000000001 	0.217499999999999999 	11 	
+0 	0.604999999999999982 	0.455000000000000016 	0.14499999999999999 	0.861999999999999988 	0.334000000000000019 	0.19850000000000001 	0.299999999999999989 	9 	
+2 	0.625 	0.5 	0.140000000000000013 	1.09600000000000009 	0.544499999999999984 	0.216499999999999998 	0.294999999999999984 	10 	
+2 	0.619999999999999996 	0.494999999999999996 	0.195000000000000007 	1.51449999999999996 	0.578999999999999959 	0.345999999999999974 	0.519499999999999962 	15 	
+1 	0.419999999999999984 	0.320000000000000007 	0.110000000000000001 	0.308999999999999997 	0.115000000000000005 	0.0645000000000000018 	0.0945000000000000007 	6 	
+0 	0.569999999999999951 	0.450000000000000011 	0.170000000000000012 	1.09800000000000009 	0.413999999999999979 	0.187 	0.405000000000000027 	20 	
+0 	0.505000000000000004 	0.395000000000000018 	0.14499999999999999 	0.651499999999999968 	0.269500000000000017 	0.152999999999999997 	0.204999999999999988 	15 	
+0 	0.569999999999999951 	0.445000000000000007 	0.14499999999999999 	0.877499999999999947 	0.411999999999999977 	0.216999999999999998 	0.220000000000000001 	8 	
+0 	0.619999999999999996 	0.5 	0.149999999999999994 	1.29299999999999993 	0.595999999999999974 	0.313500000000000001 	0.353999999999999981 	10 	
+2 	0.395000000000000018 	0.294999999999999984 	0.115000000000000005 	0.316000000000000003 	0.120499999999999996 	0.0594999999999999973 	0.110500000000000001 	12 	
+2 	0.525000000000000022 	0.400000000000000022 	0.170000000000000012 	0.730500000000000038 	0.279000000000000026 	0.205499999999999988 	0.195000000000000007 	11 	
+0 	0.489999999999999991 	0.390000000000000013 	0.135000000000000009 	0.578500000000000014 	0.246499999999999997 	0.122999999999999998 	0.200000000000000011 	13 	
+2 	0.645000000000000018 	0.494999999999999996 	0.184999999999999998 	1.49350000000000005 	0.526499999999999968 	0.278500000000000025 	0.455000000000000016 	15 	
+1 	0.409999999999999976 	0.33500000000000002 	0.104999999999999996 	0.330500000000000016 	0.140500000000000014 	0.0640000000000000013 	0.104999999999999996 	7 	
+1 	0.505000000000000004 	0.405000000000000027 	0.130000000000000004 	0.598999999999999977 	0.224500000000000005 	0.117499999999999993 	0.225000000000000006 	11 	
+1 	0.505000000000000004 	0.354999999999999982 	0.125 	0.600999999999999979 	0.25 	0.120499999999999996 	0.184999999999999998 	8 	
+0 	0.70499999999999996 	0.550000000000000044 	0.170000000000000012 	1.21900000000000008 	0.639499999999999957 	0.235999999999999988 	0.30099999999999999 	9 	
+2 	0.33500000000000002 	0.234999999999999987 	0.0850000000000000061 	0.154499999999999998 	0.0660000000000000031 	0.0345000000000000029 	0.0449999999999999983 	6 	
+1 	0.434999999999999998 	0.340000000000000024 	0.110000000000000001 	0.379500000000000004 	0.149499999999999994 	0.0850000000000000061 	0.119999999999999996 	8 	
+1 	0.265000000000000013 	0.209999999999999992 	0.0599999999999999978 	0.0965000000000000024 	0.0425000000000000031 	0.0219999999999999987 	0.0299999999999999989 	5 	
+1 	0.179999999999999993 	0.125 	0.0350000000000000033 	0.0264999999999999993 	0.00949999999999999976 	0.00549999999999999968 	0.00850000000000000061 	4 	
+0 	0.655000000000000027 	0.505000000000000004 	0.195000000000000007 	1.44049999999999989 	0.687999999999999945 	0.380500000000000005 	0.362999999999999989 	11 	
+0 	0.625 	0.489999999999999991 	0.154999999999999999 	1.33000000000000007 	0.667499999999999982 	0.259000000000000008 	0.330000000000000016 	10 	
+2 	0.650000000000000022 	0.505000000000000004 	0.170000000000000012 	1.55950000000000011 	0.694999999999999951 	0.351499999999999979 	0.395000000000000018 	11 	
+1 	0.340000000000000024 	0.265000000000000013 	0.0800000000000000017 	0.201500000000000012 	0.0899999999999999967 	0.0475000000000000006 	0.0550000000000000003 	5 	
+0 	0.569999999999999951 	0.465000000000000024 	0.160000000000000003 	0.893499999999999961 	0.314500000000000002 	0.257500000000000007 	0.263000000000000012 	10 	
+0 	0.630000000000000004 	0.479999999999999982 	0.165000000000000008 	1.26150000000000007 	0.550499999999999989 	0.277000000000000024 	0.388500000000000012 	10 	
+2 	0.569999999999999951 	0.46000000000000002 	0.154999999999999999 	1.00049999999999994 	0.454000000000000015 	0.204999999999999988 	0.265000000000000013 	11 	
+2 	0.630000000000000004 	0.465000000000000024 	0.149999999999999994 	1.02699999999999991 	0.537000000000000033 	0.188 	0.17599999999999999 	8 	
+1 	0.465000000000000024 	0.369999999999999996 	0.115000000000000005 	0.53400000000000003 	0.26100000000000001 	0.0980000000000000038 	0.142999999999999988 	7 	
+2 	0.599999999999999978 	0.46000000000000002 	0.170000000000000012 	1.1805000000000001 	0.456000000000000016 	0.337000000000000022 	0.329000000000000015 	11 	
+0 	0.660000000000000031 	0.535000000000000031 	0.174999999999999989 	1.51750000000000007 	0.710999999999999965 	0.3125 	0.41499999999999998 	12 	
+1 	0.54500000000000004 	0.429999999999999993 	0.140000000000000013 	0.687000000000000055 	0.26150000000000001 	0.140500000000000014 	0.25 	9 	
+0 	0.655000000000000027 	0.520000000000000018 	0.190000000000000002 	1.4544999999999999 	0.599999999999999978 	0.38650000000000001 	0.383000000000000007 	10 	
+1 	0.5 	0.375 	0.119999999999999996 	0.542000000000000037 	0.214999999999999997 	0.116000000000000006 	0.170000000000000012 	9 	
+2 	0.680000000000000049 	0.535000000000000031 	0.184999999999999998 	1.60699999999999998 	0.724500000000000033 	0.321500000000000008 	0.497999999999999998 	12 	
+2 	0.665000000000000036 	0.520000000000000018 	0.174999999999999989 	1.37250000000000005 	0.605999999999999983 	0.320000000000000007 	0.395000000000000018 	12 	
+2 	0.609999999999999987 	0.479999999999999982 	0.140000000000000013 	1.0625 	0.516000000000000014 	0.225000000000000006 	0.291499999999999981 	11 	
+0 	0.630000000000000004 	0.5 	0.174999999999999989 	1.11050000000000004 	0.467000000000000026 	0.268000000000000016 	0.329000000000000015 	10 	
+1 	0.515000000000000013 	0.375 	0.140000000000000013 	0.650499999999999967 	0.2495 	0.140999999999999986 	0.221500000000000002 	10 	
+2 	0.57999999999999996 	0.469999999999999973 	0.165000000000000008 	0.927000000000000046 	0.321500000000000008 	0.19850000000000001 	0.315000000000000002 	11 	
+2 	0.599999999999999978 	0.510000000000000009 	0.184999999999999998 	1.28499999999999992 	0.609500000000000042 	0.274500000000000022 	0.315000000000000002 	9 	
+2 	0.560000000000000053 	0.419999999999999984 	0.149999999999999994 	0.875499999999999945 	0.440000000000000002 	0.196500000000000008 	0.231500000000000011 	8 	
+1 	0.385000000000000009 	0.28999999999999998 	0.0800000000000000017 	0.248499999999999999 	0.121999999999999997 	0.0495000000000000023 	0.0650000000000000022 	7 	
+1 	0.520000000000000018 	0.395000000000000018 	0.14499999999999999 	0.770000000000000018 	0.423999999999999988 	0.141999999999999987 	0.189500000000000002 	7 	
+2 	0.515000000000000013 	0.455000000000000016 	0.135000000000000009 	0.722500000000000031 	0.294999999999999984 	0.162500000000000006 	0.234999999999999987 	9 	
+1 	0.520000000000000018 	0.395000000000000018 	0.125 	0.663000000000000034 	0.300499999999999989 	0.131000000000000005 	0.190500000000000003 	9 	
+2 	0.589999999999999969 	0.405000000000000027 	0.149999999999999994 	0.85299999999999998 	0.326000000000000012 	0.26150000000000001 	0.244999999999999996 	9 	
+1 	0.325000000000000011 	0.239999999999999991 	0.0749999999999999972 	0.152499999999999997 	0.0719999999999999946 	0.0645000000000000018 	0.0429999999999999966 	6 	
+1 	0.67000000000000004 	0.484999999999999987 	0.174999999999999989 	1.25649999999999995 	0.535499999999999976 	0.322000000000000008 	0.38600000000000001 	9 	
+0 	0.594999999999999973 	0.469999999999999973 	0.154999999999999999 	1.121 	0.451500000000000012 	0.177999999999999992 	0.154999999999999999 	11 	
+1 	0.390000000000000013 	0.299999999999999989 	0.100000000000000006 	0.266500000000000015 	0.110500000000000001 	0.0589999999999999969 	0.0840000000000000052 	7 	
+0 	0.614999999999999991 	0.479999999999999982 	0.165000000000000008 	1.16149999999999998 	0.513000000000000012 	0.30099999999999999 	0.304999999999999993 	10 	
+0 	0.680000000000000049 	0.5 	0.184999999999999998 	1.7410000000000001 	0.766499999999999959 	0.325500000000000012 	0.468500000000000028 	12 	
+0 	0.70499999999999996 	0.560000000000000053 	0.204999999999999988 	2.38099999999999978 	0.991500000000000048 	0.500499999999999945 	0.623999999999999999 	10 	
+1 	0.424999999999999989 	0.340000000000000024 	0.100000000000000006 	0.351499999999999979 	0.162500000000000006 	0.0820000000000000034 	0.0940000000000000002 	7 	
+1 	0.284999999999999976 	0.220000000000000001 	0.0650000000000000022 	0.096000000000000002 	0.0405000000000000013 	0.0205000000000000009 	0.0299999999999999989 	5 	
+0 	0.405000000000000027 	0.309999999999999998 	0.119999999999999996 	0.309499999999999997 	0.138000000000000012 	0.0580000000000000029 	0.0950000000000000011 	13 	
+1 	0.564999999999999947 	0.419999999999999984 	0.154999999999999999 	0.742999999999999994 	0.309999999999999998 	0.185999999999999999 	0.231000000000000011 	9 	
+1 	0.5 	0.369999999999999996 	0.115000000000000005 	0.574500000000000011 	0.305999999999999994 	0.112000000000000002 	0.140999999999999986 	7 	
+0 	0.604999999999999982 	0.484999999999999987 	0.160000000000000003 	1.22199999999999998 	0.530000000000000027 	0.257500000000000007 	0.280000000000000027 	13 	
+2 	0.465000000000000024 	0.354999999999999982 	0.104999999999999996 	0.479499999999999982 	0.227000000000000007 	0.123999999999999999 	0.125 	8 	
+0 	0.694999999999999951 	0.560000000000000053 	0.220000000000000001 	1.83400000000000007 	0.845500000000000029 	0.421999999999999986 	0.455000000000000016 	11 	
+2 	0.640000000000000013 	0.494999999999999996 	0.165000000000000008 	1.30699999999999994 	0.678000000000000047 	0.291999999999999982 	0.266000000000000014 	11 	
+1 	0.440000000000000002 	0.344999999999999973 	0.130000000000000004 	0.449500000000000011 	0.208999999999999991 	0.0835000000000000048 	0.134000000000000008 	6 	
+1 	0.5 	0.385000000000000009 	0.119999999999999996 	0.560000000000000053 	0.283499999999999974 	0.102999999999999994 	0.135000000000000009 	8 	
+0 	0.67000000000000004 	0.550000000000000044 	0.190000000000000002 	1.39050000000000007 	0.542499999999999982 	0.303499999999999992 	0.400000000000000022 	12 	
+2 	0.589999999999999969 	0.434999999999999998 	0.165000000000000008 	0.976500000000000035 	0.452500000000000013 	0.239499999999999991 	0.234999999999999987 	9 	
+1 	0.380000000000000004 	0.28999999999999998 	0.100000000000000006 	0.236999999999999988 	0.107999999999999999 	0.0395000000000000004 	0.0820000000000000034 	6 	
+1 	0.265000000000000013 	0.200000000000000011 	0.0650000000000000022 	0.0975000000000000033 	0.0400000000000000008 	0.0205000000000000009 	0.0280000000000000006 	7 	
+1 	0.440000000000000002 	0.344999999999999973 	0.119999999999999996 	0.364999999999999991 	0.165500000000000008 	0.0830000000000000043 	0.110000000000000001 	7 	
+0 	0.57999999999999996 	0.440000000000000002 	0.179999999999999993 	0.853999999999999981 	0.366499999999999992 	0.163500000000000006 	0.244999999999999996 	12 	
+0 	0.589999999999999969 	0.440000000000000002 	0.140000000000000013 	1.0069999999999999 	0.47749999999999998 	0.210499999999999993 	0.292499999999999982 	9 	
+0 	0.630000000000000004 	0.505000000000000004 	0.165000000000000008 	1.06499999999999995 	0.45950000000000002 	0.215999999999999998 	0.315000000000000002 	12 	
+2 	0.630000000000000004 	0.530000000000000027 	0.179999999999999993 	1.27950000000000008 	0.617999999999999994 	0.256000000000000005 	0.315000000000000002 	9 	
+2 	0.569999999999999951 	0.434999999999999998 	0.170000000000000012 	0.872999999999999998 	0.382000000000000006 	0.182999999999999996 	0.270500000000000018 	10 	
+0 	0.474999999999999978 	0.349999999999999978 	0.115000000000000005 	0.452000000000000013 	0.171500000000000014 	0.0919999999999999984 	0.154999999999999999 	11 	
+1 	0.569999999999999951 	0.445000000000000007 	0.14499999999999999 	0.740500000000000047 	0.305999999999999994 	0.171999999999999986 	0.182499999999999996 	12 	
+1 	0.299999999999999989 	0.23000000000000001 	0.0749999999999999972 	0.127000000000000002 	0.0519999999999999976 	0.0299999999999999989 	0.0345000000000000029 	6 	
+2 	0.619999999999999996 	0.484999999999999987 	0.154999999999999999 	1.04899999999999993 	0.462000000000000022 	0.231000000000000011 	0.25 	10 	
+0 	0.569999999999999951 	0.440000000000000002 	0.140000000000000013 	0.953500000000000014 	0.378500000000000003 	0.201000000000000012 	0.304999999999999993 	17 	
+1 	0.25 	0.174999999999999989 	0.0599999999999999978 	0.0635000000000000009 	0.0275000000000000001 	0.00800000000000000017 	0.0200000000000000004 	4 	
+1 	0.284999999999999976 	0.214999999999999997 	0.0599999999999999978 	0.0934999999999999998 	0.0309999999999999998 	0.0229999999999999996 	0.0299999999999999989 	6 	
+1 	0.5 	0.375 	0.125 	0.569500000000000006 	0.259000000000000008 	0.123999999999999999 	0.157000000000000001 	7 	
+2 	0.359999999999999987 	0.270000000000000018 	0.100000000000000006 	0.216999999999999998 	0.0884999999999999953 	0.0495000000000000023 	0.0714999999999999941 	6 	
+0 	0.560000000000000053 	0.455000000000000016 	0.190000000000000002 	0.713999999999999968 	0.282999999999999974 	0.129000000000000004 	0.275000000000000022 	9 	
+2 	0.625 	0.489999999999999991 	0.174999999999999989 	1.33250000000000002 	0.570500000000000007 	0.271000000000000019 	0.405000000000000027 	10 	
+1 	0.46000000000000002 	0.349999999999999978 	0.104999999999999996 	0.370499999999999996 	0.157500000000000001 	0.076999999999999999 	0.114000000000000004 	9 	
+2 	0.400000000000000022 	0.320000000000000007 	0.0950000000000000011 	0.302999999999999992 	0.133500000000000008 	0.0599999999999999978 	0.100000000000000006 	7 	
+0 	0.70499999999999996 	0.540000000000000036 	0.204999999999999988 	1.7569999999999999 	0.826500000000000012 	0.416999999999999982 	0.461000000000000021 	9 	
+2 	0.630000000000000004 	0.510000000000000009 	0.174999999999999989 	1.34149999999999991 	0.657499999999999973 	0.262000000000000011 	0.375 	10 	
+2 	0.200000000000000011 	0.140000000000000013 	0.0550000000000000003 	0.0350000000000000033 	0.0145000000000000007 	0.00800000000000000017 	0.0100000000000000002 	5 	
+0 	0.599999999999999978 	0.46000000000000002 	0.14499999999999999 	0.932499999999999996 	0.398500000000000021 	0.224500000000000005 	0.247999999999999998 	8 	
+2 	0.525000000000000022 	0.424999999999999989 	0.190000000000000002 	0.871999999999999997 	0.462500000000000022 	0.172499999999999987 	0.19900000000000001 	9 	
+0 	0.694999999999999951 	0.550000000000000044 	0.154999999999999999 	1.84949999999999992 	0.767000000000000015 	0.442000000000000004 	0.417499999999999982 	10 	
+0 	0.609999999999999987 	0.484999999999999987 	0.165000000000000008 	1.08699999999999997 	0.425499999999999989 	0.232000000000000012 	0.380000000000000004 	11 	
+0 	0.440000000000000002 	0.344999999999999973 	0.170000000000000012 	0.408499999999999974 	0.149999999999999994 	0.0825000000000000039 	0.151499999999999996 	12 	
+2 	0.640000000000000013 	0.525000000000000022 	0.184999999999999998 	1.70700000000000007 	0.763000000000000012 	0.420499999999999985 	0.443500000000000005 	11 	
+1 	0.434999999999999998 	0.330000000000000016 	0.125 	0.406000000000000028 	0.168500000000000011 	0.105499999999999997 	0.096000000000000002 	12 	
+1 	0.309999999999999998 	0.23000000000000001 	0.0700000000000000067 	0.1245 	0.0505000000000000032 	0.0264999999999999993 	0.0379999999999999991 	6 	
+2 	0.619999999999999996 	0.484999999999999987 	0.204999999999999988 	1.21900000000000008 	0.387500000000000011 	0.2505 	0.385000000000000009 	14 	
+0 	0.550000000000000044 	0.424999999999999989 	0.125 	0.963999999999999968 	0.547499999999999987 	0.159000000000000002 	0.214999999999999997 	8 	
+0 	0.675000000000000044 	0.550000000000000044 	0.179999999999999993 	1.68849999999999989 	0.562000000000000055 	0.370499999999999996 	0.599999999999999978 	15 	
+2 	0.510000000000000009 	0.405000000000000027 	0.149999999999999994 	0.703500000000000014 	0.346999999999999975 	0.134000000000000008 	0.188500000000000001 	8 	
+0 	0.530000000000000027 	0.41499999999999998 	0.115000000000000005 	0.591500000000000026 	0.233000000000000013 	0.158500000000000002 	0.179999999999999993 	11 	
+0 	0.57999999999999996 	0.46000000000000002 	0.149999999999999994 	1.11549999999999994 	0.557499999999999996 	0.225500000000000006 	0.28999999999999998 	7 	
+0 	0.625 	0.525000000000000022 	0.214999999999999997 	1.57650000000000001 	0.511499999999999955 	0.259500000000000008 	0.665000000000000036 	16 	
+0 	0.604999999999999982 	0.484999999999999987 	0.165000000000000008 	0.951500000000000012 	0.453500000000000014 	0.193000000000000005 	0.276500000000000024 	11 	
+1 	0.325000000000000011 	0.225000000000000006 	0.0749999999999999972 	0.139000000000000012 	0.0565000000000000016 	0.0320000000000000007 	0.0899999999999999967 	6 	
+2 	0.429999999999999993 	0.349999999999999978 	0.110000000000000001 	0.406000000000000028 	0.16750000000000001 	0.0810000000000000026 	0.135000000000000009 	10 	
+1 	0.5 	0.380000000000000004 	0.125 	0.519000000000000017 	0.248499999999999999 	0.113500000000000004 	0.134000000000000008 	8 	
+0 	0.739999999999999991 	0.574999999999999956 	0.220000000000000001 	2.01200000000000001 	0.891499999999999959 	0.526499999999999968 	0.470999999999999974 	12 	
+0 	0.450000000000000011 	0.349999999999999978 	0.14499999999999999 	0.542499999999999982 	0.17649999999999999 	0.122999999999999998 	0.174999999999999989 	13 	
+0 	0.635000000000000009 	0.515000000000000013 	0.190000000000000002 	1.37149999999999994 	0.50649999999999995 	0.304999999999999993 	0.450000000000000011 	10 	
+0 	0.619999999999999996 	0.465000000000000024 	0.140000000000000013 	1.16050000000000009 	0.600500000000000034 	0.219500000000000001 	0.306999999999999995 	9 	
+2 	0.574999999999999956 	0.455000000000000016 	0.14499999999999999 	1.16500000000000004 	0.580999999999999961 	0.227500000000000008 	0.299999999999999989 	14 	
+0 	0.630000000000000004 	0.474999999999999978 	0.154999999999999999 	1.00049999999999994 	0.452000000000000013 	0.252000000000000002 	0.265000000000000013 	10 	
+2 	0.645000000000000018 	0.5 	0.174999999999999989 	1.28600000000000003 	0.564500000000000002 	0.287999999999999978 	0.38600000000000001 	12 	
+0 	0.660000000000000031 	0.520000000000000018 	0.200000000000000011 	1.67599999999999993 	0.673000000000000043 	0.480499999999999983 	0.450000000000000011 	17 	
+0 	0.584999999999999964 	0.474999999999999978 	0.184999999999999998 	0.958500000000000019 	0.41449999999999998 	0.161500000000000005 	0.330000000000000016 	11 	
+1 	0.255000000000000004 	0.190000000000000002 	0.0749999999999999972 	0.0864999999999999936 	0.0345000000000000029 	0.0205000000000000009 	0.0250000000000000014 	5 	
+2 	0.234999999999999987 	0.170000000000000012 	0.0550000000000000003 	0.0514999999999999972 	0.0179999999999999986 	0.0105000000000000007 	0.0195 	7 	
+2 	0.594999999999999973 	0.455000000000000016 	0.149999999999999994 	0.88600000000000001 	0.431499999999999995 	0.201000000000000012 	0.223000000000000004 	10 	
+0 	0.584999999999999964 	0.41499999999999998 	0.154999999999999999 	0.69850000000000001 	0.299999999999999989 	0.145999999999999991 	0.195000000000000007 	12 	
+0 	0.680000000000000049 	0.550000000000000044 	0.209999999999999992 	1.74449999999999994 	0.597500000000000031 	0.304999999999999993 	0.625 	17 	
+0 	0.599999999999999978 	0.46000000000000002 	0.154999999999999999 	0.973500000000000032 	0.426999999999999991 	0.204499999999999987 	0.299999999999999989 	8 	
+2 	0.46000000000000002 	0.375 	0.130000000000000004 	0.57350000000000001 	0.2505 	0.118999999999999995 	0.195000000000000007 	9 	
+1 	0.330000000000000016 	0.225000000000000006 	0.0749999999999999972 	0.187 	0.0945000000000000007 	0.0395000000000000004 	0.0425000000000000031 	7 	
+0 	0.709999999999999964 	0.564999999999999947 	0.195000000000000007 	1.81699999999999995 	0.785000000000000031 	0.491999999999999993 	0.489999999999999991 	11 	
+2 	0.719999999999999973 	0.564999999999999947 	0.190000000000000002 	2.08099999999999996 	1.08149999999999991 	0.430499999999999994 	0.503000000000000003 	11 	
+1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.222500000000000003 	0.0934999999999999998 	0.0524999999999999981 	0.0660000000000000031 	7 	
+2 	0.540000000000000036 	0.419999999999999984 	0.190000000000000002 	0.685499999999999998 	0.292999999999999983 	0.163000000000000006 	0.380000000000000004 	10 	
+0 	0.560000000000000053 	0.440000000000000002 	0.154999999999999999 	0.640499999999999958 	0.336000000000000021 	0.17649999999999999 	0.244999999999999996 	8 	
+1 	0.380000000000000004 	0.270000000000000018 	0.0800000000000000017 	0.210499999999999993 	0.0864999999999999936 	0.0420000000000000026 	0.0700000000000000067 	8 	
+2 	0.419999999999999984 	0.325000000000000011 	0.115000000000000005 	0.288499999999999979 	0.100000000000000006 	0.0570000000000000021 	0.113500000000000004 	15 	
+0 	0.719999999999999973 	0.560000000000000053 	0.174999999999999989 	1.72649999999999992 	0.637000000000000011 	0.341500000000000026 	0.525000000000000022 	17 	
+2 	0.744999999999999996 	0.564999999999999947 	0.214999999999999997 	1.93100000000000005 	0.896000000000000019 	0.458500000000000019 	0.5 	11 	
+2 	0.604999999999999982 	0.474999999999999978 	0.190000000000000002 	1.12549999999999994 	0.589999999999999969 	0.246999999999999997 	0.260000000000000009 	10 	
+0 	0.54500000000000004 	0.41499999999999998 	0.200000000000000011 	1.3580000000000001 	0.566999999999999948 	0.318000000000000005 	0.403000000000000025 	10 	
+2 	0.650000000000000022 	0.525000000000000022 	0.184999999999999998 	1.48799999999999999 	0.665000000000000036 	0.337000000000000022 	0.378000000000000003 	11 	
+1 	0.530000000000000027 	0.380000000000000004 	0.125 	0.615999999999999992 	0.291999999999999982 	0.113000000000000003 	0.184999999999999998 	8 	
+2 	0.599999999999999978 	0.469999999999999973 	0.174999999999999989 	1.10499999999999998 	0.486499999999999988 	0.246999999999999997 	0.315000000000000002 	15 	
+0 	0.75 	0.550000000000000044 	0.195000000000000007 	1.83250000000000002 	0.82999999999999996 	0.365999999999999992 	0.440000000000000002 	11 	
+1 	0.424999999999999989 	0.340000000000000024 	0.100000000000000006 	0.370999999999999996 	0.149999999999999994 	0.0864999999999999936 	0.115000000000000005 	8 	
+2 	0.540000000000000036 	0.405000000000000027 	0.125 	0.891000000000000014 	0.481499999999999984 	0.191500000000000004 	0.202000000000000013 	9 	
+2 	0.515000000000000013 	0.434999999999999998 	0.14499999999999999 	0.88149999999999995 	0.291999999999999982 	0.205999999999999989 	0.255000000000000004 	10 	
+1 	0.434999999999999998 	0.344999999999999973 	0.115000000000000005 	0.417999999999999983 	0.222000000000000003 	0.0734999999999999959 	0.105999999999999997 	7 	
+0 	0.604999999999999982 	0.46000000000000002 	0.170000000000000012 	1.12200000000000011 	0.346999999999999975 	0.304499999999999993 	0.315000000000000002 	13 	
+2 	0.569999999999999951 	0.429999999999999993 	0.119999999999999996 	1.06150000000000011 	0.347999999999999976 	0.16700000000000001 	0.309999999999999998 	15 	
+2 	0.599999999999999978 	0.494999999999999996 	0.174999999999999989 	1.29000000000000004 	0.605999999999999983 	0.276000000000000023 	0.344499999999999973 	11 	
+2 	0.505000000000000004 	0.385000000000000009 	0.115000000000000005 	0.482499999999999984 	0.209999999999999992 	0.103499999999999995 	0.153499999999999998 	10 	
+1 	0.635000000000000009 	0.5 	0.165000000000000008 	1.4890000000000001 	0.714999999999999969 	0.344499999999999973 	0.361499999999999988 	13 	
+1 	0.46000000000000002 	0.354999999999999982 	0.110000000000000001 	0.435999999999999999 	0.197500000000000009 	0.096000000000000002 	0.125 	8 	
+0 	0.650000000000000022 	0.589999999999999969 	0.220000000000000001 	1.66199999999999992 	0.770000000000000018 	0.378000000000000003 	0.434999999999999998 	11 	
+0 	0.744999999999999996 	0.569999999999999951 	0.214999999999999997 	2.25 	1.15650000000000008 	0.446000000000000008 	0.558000000000000052 	9 	
+2 	0.574999999999999956 	0.440000000000000002 	0.160000000000000003 	0.961500000000000021 	0.482999999999999985 	0.166000000000000009 	0.275000000000000022 	13 	
+2 	0.574999999999999956 	0.479999999999999982 	0.149999999999999994 	0.946500000000000008 	0.435499999999999998 	0.260500000000000009 	0.2505 	9 	
+1 	0.344999999999999973 	0.255000000000000004 	0.0850000000000000061 	0.200500000000000012 	0.104999999999999996 	0.0369999999999999982 	0.0500000000000000028 	5 	
+1 	0.275000000000000022 	0.195000000000000007 	0.0899999999999999967 	0.112500000000000003 	0.0544999999999999998 	0.0294999999999999984 	0.0354999999999999968 	6 	
+1 	0.380000000000000004 	0.284999999999999976 	0.0950000000000000011 	0.242999999999999994 	0.0894999999999999962 	0.0665000000000000036 	0.0749999999999999972 	7 	
+0 	0.694999999999999951 	0.535000000000000031 	0.200000000000000011 	1.58549999999999991 	0.667000000000000037 	0.334000000000000019 	0.470999999999999974 	11 	
+1 	0.455000000000000016 	0.340000000000000024 	0.115000000000000005 	0.485999999999999988 	0.26100000000000001 	0.0655000000000000027 	0.131500000000000006 	8 	
+1 	0.390000000000000013 	0.309999999999999998 	0.104999999999999996 	0.266500000000000015 	0.118499999999999994 	0.0524999999999999981 	0.0810000000000000026 	8 	
+2 	0.665000000000000036 	0.525000000000000022 	0.154999999999999999 	1.35749999999999993 	0.532499999999999973 	0.304499999999999993 	0.44850000000000001 	10 	
+2 	0.714999999999999969 	0.535000000000000031 	0.190000000000000002 	1.67549999999999999 	0.889000000000000012 	0.313 	0.419999999999999984 	10 	
+2 	0.33500000000000002 	0.265000000000000013 	0.104999999999999996 	0.222000000000000003 	0.0934999999999999998 	0.0560000000000000012 	0.0749999999999999972 	7 	
+]
+;
+source = *0 ;
+fieldnames = []
+;
+deep_copy_memory_data = 1 ;
+writable = 0 ;
+length = 3132 ;
+width = 9 ;
+inputsize = 8 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+writable = 0 ;
+length = 9 ;
+width = 3132 ;
+inputsize = 3132 ;
+targetsize = 0 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+fieldnames = []
+;
+deep_copy_memory_data = 1 ;
+writable = 0 ;
+length = 9 ;
+width = 3132 ;
+inputsize = 3132 ;
+targetsize = 0 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+next_id = 58 ;
+leave_register = 3132 [ 7 21 7 6 7 6 7 31 34 21 7 6 7 33 7 7 7 21 7 37 6 31 6 34 7 33 7 7 31 34 7 34 7 34 6 34 34 7 6 7 7 34 34 34 34 31 33 34 6 7 6 6 34 33 31 7 37 34 34 7 7 7 7 6 6 34 34 7 7 7 27 34 21 33 28 27 34 7 7 27 7 34 31 34 7 37 37 34 6 33 6 7 33 7 6 7 6 37 37 37 7 7 37 31 36 31 27 6 7 34 36 33 31 27 6 33 37 34 34 34 37 37 7 6 37 34 37 7 7 7 34 27 6 7 6 28 27 21 33 21 7 34 21 27 7 21 7 34 6 34 37 7 7 21 6 21 34 27 7 34 27 7 6 33 31 34 31 34 34 34 37 34 31 6 37 7 7 34 34 6 33 7 7 7 6 34 34 7 7 27 31 7 34 33 7 28 33 37 34 7 37 7 7 7 6 34 7 37 7 7 6 33 6 21 34 34 27 34 27 31 7 7 33 7 34 28 33 33 34 7 37 34 7 27 33 27 6 7 33 7 36 34 34 6 36 31 28 7 7 6 33 7 36 37 34 7 7 6 21 36 33 7 6 34 33 7 6 21 7 27 34 7 7 34 7 7 37 7 6 7 37 7 6 6 33 6 7 31 7 7 6 34 34 7 7 34 34 6 33 33 7 7 7 6 7 6 7 34 33 7 7 7 7 34 6 31 31 6 34 27 7 6 34 33 6 7 34 33 27 37 31 27 7 21 37 7 37 34 33 7 6 34 6 31 27 34 34 37 33 21 31 7 34 31 7 7 37 7 6 33 34 37 7 7 7 37 7 27 31 34 6 34 37 7 7 33 7 7 !
 27 31 34 7 6 6 31 7 36 7 6 34 33 31 21 33 34 33 7 37 33 7 37 37 34 6 7 7 33 33 7 37 37 21 6 7 7 7 37 34 31 33 27 34 33 6 7 7 36 34 34 34 36 7 33 37 33 34 21 27 31 21 7 7 7 33 6 34 34 31 7 31 7 31 37 7 33 7 7 34 34 33 28 27 7 33 7 33 7 7 31 27 7 37 31 7 36 7 34 6 37 31 7 33 6 33 31 21 34 7 37 37 31 7 31 33 34 7 7 7 34 31 7 36 34 7 7 33 33 37 7 21 31 7 34 34 7 34 7 33 7 31 7 33 7 31 7 7 7 7 6 6 31 7 6 34 7 34 7 33 7 6 31 7 6 33 33 34 27 31 31 6 27 7 34 7 7 31 6 7 27 31 37 21 7 37 36 6 31 34 31 33 33 6 6 7 7 34 37 31 34 37 34 28 36 34 7 27 37 7 7 33 7 7 33 7 7 27 34 36 7 7 7 21 37 27 7 7 34 37 6 7 7 34 6 31 33 31 7 7 37 31 7 31 7 7 7 7 34 7 21 28 34 7 7 34 7 37 7 34 7 37 6 27 7 34 34 6 7 7 36 7 7 34 33 21 31 33 34 33 6 33 21 6 7 7 7 33 7 7 6 34 34 28 34 7 31 36 6 7 27 7 31 37 6 7 31 7 33 33 34 7 7 7 7 36 7 34 7 33 37 37 7 27 6 7 34 7 6 34 7 7 33 34 37 7 34 6 28 27 7 33 37 6 7 7 31 7 21 37 7 6 7 7 33 7 36 31 34 34 7 7 21 34 31 31 7 7 34 6 31 7 33 36 33 7 27 7 37 31 7 37 34 7 34!
  6 31 33 21 37 37 34 6 7 33 7 33 33 21 7 33 34 6 34 37 7 21 27!
  6 7 33 
31 31 7 34 6 37 7 34 6 31 7 7 7 7 37 27 6 31 7 7 7 33 34 7 6 33 7 33 6 33 7 7 37 37 6 33 33 34 7 6 7 7 33 7 34 7 7 6 7 7 34 7 37 34 7 7 6 7 6 7 7 7 7 6 31 7 7 33 31 31 34 7 7 34 7 7 7 7 33 7 33 7 34 7 27 7 34 34 7 7 7 21 31 6 37 6 34 7 31 31 7 31 34 7 27 34 7 7 31 31 31 7 7 33 21 34 7 7 37 34 34 7 37 37 37 7 31 34 7 6 7 7 34 6 34 34 27 21 6 37 37 31 7 6 7 37 31 31 7 7 7 34 7 33 28 21 34 7 34 6 7 37 27 34 31 34 7 34 34 7 33 6 33 33 6 7 6 6 34 7 6 33 33 7 7 7 7 27 7 33 31 33 7 28 7 31 33 27 7 34 33 33 7 34 37 37 27 31 34 33 27 6 31 33 28 27 21 34 34 37 34 37 34 34 21 21 6 7 33 31 6 21 7 7 34 37 6 34 21 7 7 7 33 34 7 31 28 27 7 7 33 27 21 27 7 27 34 7 7 34 34 6 37 7 34 7 7 31 37 33 37 7 34 31 7 31 31 33 33 6 34 36 7 7 6 34 37 34 34 34 7 7 27 34 33 27 34 21 37 21 34 36 33 6 36 6 7 37 31 33 37 37 31 34 34 7 31 33 33 6 37 34 7 36 7 7 6 7 28 7 6 37 6 7 34 37 7 7 7 7 34 7 6 31 31 6 37 34 7 7 7 6 21 28 34 7 34 7 6 7 34 6 31 34 33 34 27 33 34 7 7 37 27 28 6 37 37 7 7 6 7 34 7 6 27 33 !
 7 7 34 21 37 37 34 34 6 28 34 27 7 7 6 33 7 7 7 7 6 31 7 6 31 31 34 7 31 34 7 6 7 37 33 31 34 6 31 37 7 6 7 28 33 34 6 37 33 33 6 7 7 34 7 7 34 31 7 6 34 27 33 7 37 34 7 34 33 34 27 34 7 27 21 34 34 7 7 36 21 6 34 7 21 7 37 33 7 31 34 31 37 7 6 7 7 27 7 27 33 31 31 34 31 7 31 7 27 34 7 7 6 7 34 31 31 21 33 7 33 34 7 6 34 7 34 27 33 6 7 7 7 36 33 7 31 33 21 27 27 37 6 6 27 37 34 33 7 31 33 27 34 31 31 7 34 37 21 27 34 37 6 28 27 7 7 34 31 37 34 7 33 6 6 27 31 7 7 31 34 34 33 7 27 31 37 37 34 33 7 37 31 34 34 31 34 7 7 34 7 28 33 27 27 7 33 6 34 6 33 27 31 33 27 6 6 33 37 27 34 7 7 7 7 37 33 7 7 7 34 6 7 6 31 37 7 34 7 31 6 33 37 7 27 33 31 37 31 7 7 34 37 31 31 37 31 34 31 7 33 27 34 31 37 6 28 6 6 31 27 7 36 31 34 7 37 34 7 27 33 7 33 21 7 7 7 31 34 27 33 37 31 7 33 7 7 31 33 7 7 7 21 33 37 7 6 7 34 7 6 34 37 31 27 34 6 21 7 31 34 33 7 7 7 37 34 7 33 37 7 31 6 6 7 7 28 34 7 7 6 37 28 31 33 21 21 7 7 34 27 36 34 7 33 7 7 37 6 27 34 7 33 34 34 34 37 34 34 7 27 7 33 6 31 37 7 !
 21 7 37 34 7 7 6 34 34 33 7 7 31 33 7 31 7 7 6 34 27 7 7 6 7 3!
 4 36 31 
7 28 28 7 6 37 34 7 37 7 31 7 33 33 6 33 33 6 21 7 7 7 31 37 31 6 34 7 34 6 7 7 6 21 27 6 7 37 21 28 7 7 7 34 34 31 7 37 7 37 33 21 21 6 7 34 7 33 7 37 7 7 31 31 7 37 34 37 6 33 33 21 6 7 7 34 37 6 7 37 34 34 31 33 7 33 37 34 34 7 7 21 6 34 37 27 7 21 7 33 36 27 34 7 31 6 7 31 6 37 7 7 7 31 7 7 31 34 37 37 7 7 7 34 6 7 7 33 34 27 34 7 7 37 34 7 34 21 34 7 6 34 6 36 34 37 7 6 6 6 7 7 6 34 6 37 7 33 34 37 33 7 7 33 31 37 27 6 37 31 31 6 6 7 37 37 27 6 36 33 7 6 33 31 7 33 6 7 7 7 6 7 37 37 7 7 34 34 7 34 34 6 34 37 7 31 27 37 33 7 36 7 6 31 34 7 7 27 31 28 7 34 27 28 27 7 31 7 27 21 34 6 27 34 7 7 31 31 7 7 7 34 34 37 37 7 33 33 7 36 7 6 37 7 33 37 34 6 34 33 27 7 37 34 7 7 6 27 34 6 37 33 7 27 7 7 7 37 27 7 33 34 27 34 7 34 34 33 36 27 33 37 34 33 34 7 36 7 34 31 33 7 7 34 33 37 7 7 7 33 7 7 37 31 7 31 21 34 37 27 34 7 37 6 34 7 7 7 6 34 21 36 34 34 7 27 7 36 6 33 37 28 7 34 21 34 34 7 7 6 7 33 7 7 34 37 37 33 7 33 7 27 34 7 6 7 34 21 7 37 7 37 31 37 33 7 34 7 27 27 34 33 28 !
 7 31 33 34 34 7 34 27 34 37 7 7 7 7 27 37 33 7 7 34 6 6 27 7 27 31 7 33 7 7 7 27 34 34 7 7 7 34 34 37 7 27 31 37 33 6 7 7 7 37 21 7 27 37 37 34 34 6 7 31 37 36 21 37 7 7 7 37 34 6 6 7 7 7 34 34 7 7 7 33 28 7 34 7 34 34 34 7 7 6 37 7 34 34 37 7 33 37 31 34 7 37 7 7 34 37 34 31 7 7 34 7 33 7 7 7 33 27 7 37 37 31 36 7 7 34 6 37 6 33 6 33 31 27 34 6 6 27 34 31 34 31 37 31 31 34 37 7 7 7 7 34 34 27 7 33 34 7 33 31 37 33 33 6 34 37 27 34 34 34 6 33 6 33 7 7 7 34 34 7 6 7 34 33 6 6 31 31 37 7 6 31 34 37 31 7 33 34 6 34 21 7 7 6 7 34 34 31 31 7 31 7 37 34 7 31 6 7 6 31 27 31 34 33 7 34 6 6 34 34 31 31 34 7 33 34 7 33 37 31 34 7 37 37 7 21 27 7 34 37 7 34 7 33 34 27 37 34 34 27 34 7 27 34 34 7 34 34 37 7 27 7 27 34 7 7 34 34 34 7 21 34 6 7 6 37 7 34 34 7 28 34 33 27 36 34 6 33 6 7 6 7 34 33 31 34 34 27 31 33 28 31 33 7 27 7 33 34 27 36 7 7 37 7 7 7 31 37 7 33 7 7 7 34 27 31 7 28 33 34 27 6 27 37 27 33 31 21 7 27 27 34 7 7 28 7 7 34 37 31 7 36 6 27 6 37 34 33 37 27 34 27 28 34 34 34 !
 31 7 34 7 6 7 31 31 7 7 7 34 21 7 34 33 37 36 34 27 31 21 37 3!
 7 31 7 6
 31 7 34 6 7 31 7 34 7 33 27 7 6 27 34 27 6 33 34 33 37 34 7 28 33 7 33 33 7 7 31 31 31 33 7 37 7 34 34 28 6 7 7 27 7 7 37 6 34 6 33 7 6 37 6 7 34 21 21 31 6 33 34 34 36 34 36 37 7 33 7 7 31 7 7 34 37 34 7 34 34 7 7 34 21 7 6 7 21 37 34 6 33 31 7 7 37 27 7 34 7 6 21 34 34 34 27 34 33 33 7 37 34 6 7 31 31 7 31 6 34 37 31 6 34 6 34 7 6 34 7 6 7 33 7 34 7 31 7 34 34 31 6 7 7 33 7 33 31 7 33 31 7 7 37 33 6 34 36 6 7 6 7 34 7 34 7 6 37 7 7 33 34 31 33 34 6 37 31 28 7 31 6 7 33 31 33 33 34 34 7 7 7 34 33 28 6 34 33 21 33 7 33 33 7 34 6 7 34 36 7 21 34 34 7 21 7 7 7 33 37 28 37 37 6 7 27 7 7 34 27 31 33 37 7 7 7 6 37 6 7 6 34 7 34 34 6 21 7 7 7 7 7 34 7 31 37 6 6 27 6 7 27 6 6 34 28 33 7 37 34 6 34 7 7 34 28 34 33 21 7 7 34 7 27 33 7 34 33 33 31 31 34 6 33 7 7 7 34 7 34 7 34 7 34 6 7 7 27 33 6 37 7 21 37 28 7 27 27 31 6 37 34 7 34 34 34 7 34 31 7 7 6 36 33 7 31 37 34 33 7 37 7 37 21 27 37 33 33 7 27 33 34 31 7 37 33 34 7 33 37 27 27 7 31 36 33 33 21 7 33 7 7 6 27 33 6 6 34 33 33 27!
  37 7 21 34 31 7 37 6 27 34 33 37 7 7 7 33 31 27 33 37 7 7 31 6 7 7 7 37 28 33 7 31 7 7 7 7 37 34 7 33 7 7 6 31 6 34 37 7 7 7 34 37 34 27 34 33 33 7 7 33 34 7 6 7 34 31 34 34 7 33 34 34 34 34 6 37 6 7 7 34 6 6 7 7 31 34 34 36 6 27 27 31 33 33 7 6 7 31 7 37 21 33 7 7 34 37 37 6 6 27 33 7 21 7 7 7 7 21 31 6 34 7 27 7 31 7 6 7 34 28 7 34 33 34 31 33 6 34 6 27 33 37 37 37 37 37 7 7 7 33 6 37 21 7 34 34 27 7 37 7 7 34 7 27 27 33 7 7 34 7 34 33 27 6 31 7 31 33 21 37 6 34 7 7 33 7 37 31 7 7 7 7 7 34 34 7 6 7 7 7 7 27 27 6 34 7 28 6 37 34 27 6 31 31 34 7 31 34 37 31 34 21 7 34 37 33 34 7 37 37 21 7 36 7 34 6 7 6 6 34 34 34 6 31 34 33 7 7 34 27 37 34 7 27 34 33 34 37 31 34 33 6 7 37 7 37 6 34 7 7 34 27 28 7 6 7 37 7 33 7 27 33 7 7 34 33 7 6 7 37 33 34 34 31 7 7 6 33 31 6 6 7 7 31 34 7 7 27 34 6 37 33 27 34 7 27 7 6 31 33 21 7 7 33 21 33 7 7 7 27 7 21 34 34 33 34 27 34 6 6 33 37 28 34 37 6 27 27 6 31 37 7 7 27 27 33 34 34 7 34 27 7 33 31 7 31 31 34 7 34 7 27 27 33 33 6 6 7 27 7 7 21 !
 27 7 ] ;
+writable = 0 ;
+length = 3132 ;
+width = 9 ;
+inputsize = 8 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+root = *7 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *8 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 3132 ;
+weights_sum = 1.00000000000005396 ;
+targets_sum = 31044 ;
+weighted_targets_sum = 9.91187739463616069 ;
+weighted_squared_targets_sum = 108.97190293742257 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *8  ;
+leave_output = 2 [ 9.911877394635626 1 ] ;
+leave_error = 3 [ 21.4531789022570862 0 2.00000000000010791 ] ;
+split_col = 7 ;
+split_balance = 536 ;
+split_feature_value = 0.194750000000000006 ;
+after_split_error = 15.4025990807452686 ;
+missing_node = *0 ;
+missing_leave = *9 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *10 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *11 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1298 ;
+weights_sum = 0.414431673052358729 ;
+targets_sum = 10182 ;
+weighted_targets_sum = 3.25095785440610197 ;
+weighted_squared_targets_sum = 27.6296296296295161 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *11  ;
+leave_output = 2 [ 7.84437596302003826 1 ] ;
+leave_error = 3 [ 4.25578795947018751 0 0.828863346104717458 ] ;
+split_col = 7 ;
+split_balance = 618 ;
+split_feature_value = 0.0677500000000000047 ;
+after_split_error = 3.17040827833524297 ;
+missing_node = *0 ;
+missing_leave = *12 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *13 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *14 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 340 ;
+weights_sum = 0.108556832694763336 ;
+targets_sum = 2014 ;
+weighted_targets_sum = 0.643039591315452297 ;
+weighted_squared_targets_sum = 4.09195402298852073 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *14  ;
+leave_output = 2 [ 5.9235294117647177 1 ] ;
+leave_error = 3 [ 0.565780181804550253 0 0.217113665389526672 ] ;
+split_col = 7 ;
+split_balance = 148 ;
+split_feature_value = 0.0264999999999999993 ;
+after_split_error = 0.390135822533501653 ;
+missing_node = *0 ;
+missing_leave = *15 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *16 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.000319284802043429908 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00031928480204337765 ;
+weighted_squared_targets_sum = 0.000319284802043344257 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *17 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 339 ;
+weights_sum = 0.108237547892719915 ;
+targets_sum = 2013 ;
+weighted_targets_sum = 0.642720306513410722 ;
+weighted_squared_targets_sum = 4.09163473818648615 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *14  ;
+right_node = *18 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *19 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 958 ;
+weights_sum = 0.305874840357600375 ;
+targets_sum = 8168 ;
+weighted_targets_sum = 2.60791826309066277 ;
+weighted_squared_targets_sum = 23.5376756066410842 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *19  ;
+leave_output = 2 [ 8.52609603340283684 1 ] ;
+leave_error = 3 [ 2.60462809652993066 0 0.61174968071520075 ] ;
+split_col = 7 ;
+split_balance = 186 ;
+split_feature_value = 0.119249999999999995 ;
+after_split_error = 2.34506637014711616 ;
+missing_node = *0 ;
+missing_leave = *20 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *21 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.000319284802043416031 ;
+targets_sum = 8 ;
+weighted_targets_sum = 0.00255427841634700212 ;
+weighted_squared_targets_sum = 0.0204342273307801178 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *22 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 957 ;
+weights_sum = 0.305555555555556968 ;
+targets_sum = 8160 ;
+weighted_targets_sum = 2.60536398467430619 ;
+weighted_squared_targets_sum = 23.5172413793105299 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *19   )
+;
+left_leave = *11  ;
+right_node = *23 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *24 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1834 ;
+weights_sum = 0.585568326947639717 ;
+targets_sum = 20862 ;
+weighted_targets_sum = 6.66091954022990951 ;
+weighted_squared_targets_sum = 81.3422733077908617 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *24  ;
+leave_output = 2 [ 11.3751363140676069 1 ] ;
+leave_error = 3 [ 11.1468111212782066 0 1.17113665389527943 ] ;
+split_col = 7 ;
+split_balance = 1132 ;
+split_feature_value = 0.409499999999999975 ;
+after_split_error = 10.0808309880025568 ;
+missing_node = *0 ;
+missing_leave = *25 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *26 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *27 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1483 ;
+weights_sum = 0.47349936143038901 ;
+targets_sum = 16181 ;
+weighted_targets_sum = 5.16634738186464926 ;
+weighted_squared_targets_sum = 59.951149425287305 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *27  ;
+leave_output = 2 [ 10.9109912339853796 1 ] ;
+leave_error = 3 [ 7.1623568600776073 0 0.946998722860778019 ] ;
+split_col = 5 ;
+split_balance = 271 ;
+split_feature_value = 0.39975000000000005 ;
+after_split_error = 6.77385057026893733 ;
+missing_node = *0 ;
+missing_leave = *28 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *29 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *30 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 606 ;
+weights_sum = 0.193486590038316253 ;
+targets_sum = 7079 ;
+weighted_targets_sum = 2.26021711366538325 ;
+weighted_squared_targets_sum = 28.4773307790548458 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *30  ;
+leave_output = 2 [ 11.6815181518150233 1 ] ;
+leave_error = 3 [ 4.14912707745942111 0 0.386973180076632506 ] ;
+split_col = 7 ;
+split_balance = 68 ;
+split_feature_value = 0.254750000000000032 ;
+after_split_error = 3.62419886116776269 ;
+missing_node = *0 ;
+missing_leave = *31 ->RegressionTreeLeave(
+id = 29 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *32 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *33 ->RegressionTreeLeave(
+id = 30 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 337 ;
+weights_sum = 0.107598978288633074 ;
+targets_sum = 3586 ;
+weighted_targets_sum = 1.14495530012771329 ;
+weighted_squared_targets_sum = 12.9355044699872739 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *33  ;
+leave_output = 2 [ 10.6409495548961743 1 ] ;
+leave_error = 3 [ 1.50418575743453542 0 0.215197956577266147 ] ;
+split_col = 5 ;
+split_balance = 235 ;
+split_feature_value = 0.242499999999999993 ;
+after_split_error = 1.21784951331944691 ;
+missing_node = *0 ;
+missing_leave = *34 ->RegressionTreeLeave(
+id = 35 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *35 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *36 ->RegressionTreeLeave(
+id = 36 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 51 ;
+weights_sum = 0.0162835249042145545 ;
+targets_sum = 682 ;
+weighted_targets_sum = 0.217752234993614374 ;
+weighted_squared_targets_sum = 3.10472541507024236 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *36  ;
+leave_output = 2 [ 13.3725490196078507 1 ] ;
+leave_error = 3 [ 0.385645956977931859 0 0.0325670498084291091 ] ;
+split_col = 4 ;
+split_balance = 15 ;
+split_feature_value = 0.639249999999999985 ;
+after_split_error = 0.273165886192706542 ;
+missing_node = *0 ;
+missing_leave = *37 ->RegressionTreeLeave(
+id = 47 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *38 ->RegressionTreeLeave(
+id = 48 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.00031928480204342297 ;
+targets_sum = 13 ;
+weighted_targets_sum = 0.00415070242656449383 ;
+weighted_squared_targets_sum = 0.0539591315453387121 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *39 ->RegressionTreeLeave(
+id = 49 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 50 ;
+weights_sum = 0.0159642401021711303 ;
+targets_sum = 669 ;
+weighted_targets_sum = 0.21360153256704989 ;
+weighted_squared_targets_sum = 3.05076628352490475 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *36  ;
+right_node = *40 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *41 ->RegressionTreeLeave(
+id = 37 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 286 ;
+weights_sum = 0.0913154533844186128 ;
+targets_sum = 2904 ;
+weighted_targets_sum = 0.927203065134099558 ;
+weighted_squared_targets_sum = 9.83077905491701109 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *41  ;
+leave_output = 2 [ 10.1538461538461853 1 ] ;
+leave_error = 3 [ 0.832203556341481798 0 0.182630906768837226 ] ;
+split_col = 3 ;
+split_balance = 176 ;
+split_feature_value = 0.152499999999999997 ;
+after_split_error = 0.76663404709379912 ;
+missing_node = *0 ;
+missing_leave = *42 ->RegressionTreeLeave(
+id = 50 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *43 ->RegressionTreeLeave(
+id = 51 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.000319284802043429908 ;
+targets_sum = 9 ;
+weighted_targets_sum = 0.00287356321839083231 ;
+weighted_squared_targets_sum = 0.0258620689655178135 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *44 ->RegressionTreeLeave(
+id = 52 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 285 ;
+weights_sum = 0.090996168582375192 ;
+targets_sum = 2895 ;
+weighted_targets_sum = 0.924329501915708396 ;
+weighted_squared_targets_sum = 9.80491698595149153 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *41   )
+;
+left_leave = *33  ;
+right_node = *45 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *46 ->RegressionTreeLeave(
+id = 31 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 269 ;
+weights_sum = 0.0858876117496804592 ;
+targets_sum = 3493 ;
+weighted_targets_sum = 1.11526181353767462 ;
+weighted_squared_targets_sum = 15.5418263090677033 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *46  ;
+leave_output = 2 [ 12.9851301115241906 1 ] ;
+leave_error = 3 [ 2.12001310373313379 0 0.171775223499360918 ] ;
+split_col = 7 ;
+split_balance = 233 ;
+split_feature_value = 0.364249999999999963 ;
+after_split_error = 1.97259889065841376 ;
+missing_node = *0 ;
+missing_leave = *47 ->RegressionTreeLeave(
+id = 38 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *48 ->RegressionTreeLeave(
+id = 39 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.000319284802043429908 ;
+targets_sum = 11 ;
+weighted_targets_sum = 0.00351213282247765483 ;
+weighted_squared_targets_sum = 0.0386334610472539378 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *49 ->RegressionTreeLeave(
+id = 40 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 268 ;
+weights_sum = 0.0855683269476370384 ;
+targets_sum = 3482 ;
+weighted_targets_sum = 1.11174968071519653 ;
+weighted_squared_targets_sum = 15.5031928480204844 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *46   )
+;
+left_leave = *30  ;
+right_node = *50 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *51 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 877 ;
+weights_sum = 0.280012771392084414 ;
+targets_sum = 9102 ;
+weighted_targets_sum = 2.90613026819921494 ;
+weighted_squared_targets_sum = 31.4738186462324094 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *51  ;
+leave_output = 2 [ 10.378563283922297 1 ] ;
+leave_error = 3 [ 2.62472349280956019 0 0.560025542784168828 ] ;
+split_col = 7 ;
+split_balance = 233 ;
+split_feature_value = 0.292749999999999955 ;
+after_split_error = 2.30803457130376533 ;
+missing_node = *0 ;
+missing_leave = *52 ->RegressionTreeLeave(
+id = 32 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *53 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *54 ->RegressionTreeLeave(
+id = 33 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 322 ;
+weights_sum = 0.102809706257981762 ;
+targets_sum = 3024 ;
+weighted_targets_sum = 0.965517241379309277 ;
+weighted_squared_targets_sum = 9.29885057471267551 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *54  ;
+leave_output = 2 [ 9.39130434782610912 1 ] ;
+leave_error = 3 [ 0.462768615692195073 0 0.205619412515963523 ] ;
+split_col = 7 ;
+split_balance = 16 ;
+split_feature_value = 0.259750000000000036 ;
+after_split_error = 0.420542772062299286 ;
+missing_node = *0 ;
+missing_leave = *55 ->RegressionTreeLeave(
+id = 41 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *56 ->RegressionTreeLeave(
+id = 42 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.000319284802043429908 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.00383141762452103227 ;
+weighted_squared_targets_sum = 0.0459770114942528868 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *57 ->RegressionTreeLeave(
+id = 43 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 321 ;
+weights_sum = 0.102490421455938341 ;
+targets_sum = 3012 ;
+weighted_targets_sum = 0.961685823754788838 ;
+weighted_squared_targets_sum = 9.25287356321841159 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *54  ;
+right_node = *58 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *59 ->RegressionTreeLeave(
+id = 34 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 555 ;
+weights_sum = 0.177203065134101084 ;
+targets_sum = 6078 ;
+weighted_targets_sum = 1.94061302681991688 ;
+weighted_squared_targets_sum = 22.1749680715198565 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *59  ;
+leave_output = 2 [ 10.9513513513512244 1 ] ;
+leave_error = 3 [ 1.84526595561154205 0 0.354406130268202169 ] ;
+split_col = 5 ;
+split_balance = 437 ;
+split_feature_value = 0.444250000000000034 ;
+after_split_error = 1.69370165776774684 ;
+missing_node = *0 ;
+missing_leave = *60 ->RegressionTreeLeave(
+id = 44 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *61 ->RegressionTreeLeave(
+id = 45 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.000319284802043443786 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.00383141762452116627 ;
+weighted_squared_targets_sum = 0.0459770114942545938 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *62 ->RegressionTreeLeave(
+id = 46 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 554 ;
+weights_sum = 0.17688378033205765 ;
+targets_sum = 6066 ;
+weighted_targets_sum = 1.93678160919539266 ;
+weighted_squared_targets_sum = 22.1289910600255588 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *59   )
+;
+right_leave = *51   )
+;
+left_leave = *27  ;
+right_node = *63 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *64 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 351 ;
+weights_sum = 0.112068965517240965 ;
+targets_sum = 4681 ;
+weighted_targets_sum = 1.49457215836526025 ;
+weighted_squared_targets_sum = 21.3911238825032299 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *64  ;
+leave_output = 2 [ 13.3361823361823717 1 ] ;
+leave_error = 3 [ 2.91847412792496907 0 0.22413793103448193 ] ;
+split_col = 5 ;
+split_balance = 143 ;
+split_feature_value = 0.588999999999999968 ;
+after_split_error = 2.50366757410772811 ;
+missing_node = *0 ;
+missing_leave = *65 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *66 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *67 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 104 ;
+weights_sum = 0.0332056194125160201 ;
+targets_sum = 1605 ;
+weighted_targets_sum = 0.512452107279693592 ;
+weighted_squared_targets_sum = 8.37771392081737432 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *67  ;
+leave_output = 2 [ 15.4326923076922853 1 ] ;
+leave_error = 3 [ 0.938396453482690629 0 0.0664112388250320401 ] ;
+split_col = 2 ;
+split_balance = 48 ;
+split_feature_value = 0.492499999999999993 ;
+after_split_error = 0.847781570785234084 ;
+missing_node = *0 ;
+missing_leave = *68 ->RegressionTreeLeave(
+id = 23 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *69 ->RegressionTreeLeave(
+id = 24 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.00031928480204342297 ;
+targets_sum = 13 ;
+weighted_targets_sum = 0.00415070242656454067 ;
+weighted_squared_targets_sum = 0.0539591315453373105 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *70 ->RegressionTreeLeave(
+id = 25 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 103 ;
+weights_sum = 0.0328863346104725993 ;
+targets_sum = 1592 ;
+weighted_targets_sum = 0.508301404853129024 ;
+weighted_squared_targets_sum = 8.32375478927203361 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *67  ;
+right_node = *71 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *72 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 247 ;
+weights_sum = 0.0788633461047252016 ;
+targets_sum = 3076 ;
+weighted_targets_sum = 0.982120051085567769 ;
+weighted_squared_targets_sum = 13.0134099616858769 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *72  ;
+leave_output = 2 [ 12.4534412955465861 1 ] ;
+leave_error = 3 [ 1.56527112062509044 0 0.157726692209450403 ] ;
+split_col = 7 ;
+split_balance = 147 ;
+split_feature_value = 0.567999999999999949 ;
+after_split_error = 1.28062106566572131 ;
+missing_node = *0 ;
+missing_leave = *73 ->RegressionTreeLeave(
+id = 26 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *74 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *75 ->RegressionTreeLeave(
+id = 27 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 197 ;
+weights_sum = 0.0628991060025541615 ;
+targets_sum = 2320 ;
+weighted_targets_sum = 0.740740740740740256 ;
+weighted_squared_targets_sum = 9.12771392081738675 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *75  ;
+leave_output = 2 [ 11.7766497461929074 1 ] ;
+leave_error = 3 [ 0.808539328756404108 0 0.125798212005108323 ] ;
+split_col = 7 ;
+split_balance = 71 ;
+split_feature_value = 0.444500000000000006 ;
+after_split_error = 0.761926849363249947 ;
+missing_node = *0 ;
+missing_leave = *76 ->RegressionTreeLeave(
+id = 53 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *77 ->RegressionTreeLeave(
+id = 54 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.000319284802043429908 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.00383141762452108301 ;
+weighted_squared_targets_sum = 0.045977011494252179 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *78 ->RegressionTreeLeave(
+id = 55 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 196 ;
+weights_sum = 0.0625798212005107407 ;
+targets_sum = 2308 ;
+weighted_targets_sum = 0.736909323116218484 ;
+weighted_squared_targets_sum = 9.08173690932313882 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *75  ;
+right_node = *79 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+leave_template = *80 ->RegressionTreeLeave(
+id = 28 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 50 ;
+weights_sum = 0.0159642401021711303 ;
+targets_sum = 756 ;
+weighted_targets_sum = 0.241379310344827597 ;
+weighted_squared_targets_sum = 3.88569604086845288 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *3  ;
+leave = *80  ;
+leave_output = 2 [ 15.1200000000000063 1 ] ;
+leave_error = 3 [ 0.472081736909316374 0 0.0319284802043422605 ] ;
+split_col = 5 ;
+split_balance = 6 ;
+split_feature_value = 0.770000000000000018 ;
+after_split_error = 0.329980842911872685 ;
+missing_node = *0 ;
+missing_leave = *81 ->RegressionTreeLeave(
+id = 56 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *82 ->RegressionTreeLeave(
+id = 57 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 1 ;
+weights_sum = 0.00031928480204342297 ;
+targets_sum = 19 ;
+weighted_targets_sum = 0.00606641123882503252 ;
+weighted_squared_targets_sum = 0.115261813537675867 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *83 ->RegressionTreeLeave(
+id = 58 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 0 ;
+train_set = *3  ;
+length = 49 ;
+weights_sum = 0.015644955300127706 ;
+targets_sum = 737 ;
+weighted_targets_sum = 0.23531289910600256 ;
+weighted_squared_targets_sum = 3.7704342273307776 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *80   )
+;
+right_leave = *72   )
+;
+right_leave = *64   )
+;
+right_leave = *24   )
+;
+priority_queue = *84 ->RegressionTreeQueue(
+verbosity = 0 ;
+maximum_number_of_nodes = 50 ;
+next_available_node = 10 ;
+nodes = 50 [ *18  *13  *45  *58  *79  *40  *53  *35  *74  *66  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
+;
+first_leave = *8  ;
+first_leave_output = 2 [ 9.911877394635626 1 ] ;
+first_leave_error = 3 [ 21.4531789022570862 0 2.00000000000010791 ] ;
+split_cols = 9 [ 7 7 7 5 5 7 7 5 7 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 10 ;
+n_examples = 3132 ;
+inputsize = 8 ;
+targetsize = 1 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 1 ;
+nstages = 10 ;
+report_progress = 1 ;
+verbosity = 0 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat.metadata/sizes	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat.metadata/sizes	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,4 @@
+mse	0
+base_confidence	0
+base_reward_l2	0
+base_reward_l1	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,2 @@
+out0	0
+out1	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1045 ;
+sumsquarew_ = 1045 ;
+sum_ = 4830.09365816082482 ;
+sumsquare_ = 139626.922815131547 ;
+sumcube_ = 6450152.61848150287 ;
+sumfourth_ = 381367387.208392203 ;
+min_ = 0.000221113583283008883 ;
+max_ = 96.9467455621295642 ;
+agmemin_ = 935 ;
+agemax_ = 998 ;
+first_ = 0.905069393717800819 ;
+last_ = 2.32896910316787231 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1045 ;
+sumsquarew_ = 1045 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 1044 ;
+agemax_ = 1044 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1045 ;
+sumsquarew_ = 1045 ;
+sum_ = -9660.18731632164963 ;
+sumsquare_ = 558507.69126052619 ;
+sumcube_ = -51601220.9478520229 ;
+sumfourth_ = 6101878195.33427525 ;
+min_ = -192.893491124259128 ;
+max_ = 0.999557772833433944 ;
+agmemin_ = 998 ;
+agemax_ = 935 ;
+first_ = -0.810138787435601637 ;
+last_ = -3.65793820633574462 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1045 ;
+sumsquarew_ = 1045 ;
+sum_ = -1617.89347877840351 ;
+sumsquare_ = 13163.6340431307199 ;
+sumcube_ = -110813.165274897692 ;
+sumfourth_ = 1200028.692267237 ;
+min_ = -18.6923076923076295 ;
+max_ = 0.970260223048381221 ;
+agmemin_ = 998 ;
+agemax_ = 935 ;
+first_ = -0.902702702702448789 ;
+last_ = -2.05219206680567368 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 3132 ;
+sumsquarew_ = 3132 ;
+sum_ = 10477.381224406141 ;
+sumsquare_ = 613374.62650835875 ;
+sumcube_ = 66193004.0028153807 ;
+sumfourth_ = 11773621014.3345871 ;
+min_ = 0.000221113583283008883 ;
+max_ = 296.643793965316775 ;
+agmemin_ = 3124 ;
+agemax_ = 2896 ;
+first_ = 2.17239290275085128 ;
+last_ = 2.32896910316787231 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 3132 ;
+sumsquarew_ = 3132 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 3131 ;
+agemax_ = 3131 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 3132 ;
+sumsquarew_ = 3132 ;
+sum_ = -20954.7624488122819 ;
+sumsquare_ = 2453498.506033435 ;
+sumcube_ = -529544032.022523046 ;
+sumfourth_ = 188377936229.353394 ;
+min_ = -592.287587930633549 ;
+max_ = 0.999557772833433944 ;
+agmemin_ = 2896 ;
+agemax_ = 3124 ;
+first_ = -3.34478580550170257 ;
+last_ = -3.65793820633574462 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 3132 ;
+sumsquarew_ = 3132 ;
+sum_ = -1284.71671482389047 ;
+sumsquare_ = 34335.3286498939269 ;
+sumcube_ = -311184.047184544965 ;
+sumfourth_ = 4951313.62387488503 ;
+min_ = -33.4467005076141817 ;
+max_ = 0.970260223048381221 ;
+agmemin_ = 2896 ;
+agemax_ = 3124 ;
+first_ = -1.94780793319432632 ;
+last_ = -2.05219206680567368 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,40 @@
+*5 -> PTester(
+    dataset = *1 -> AutoVMatrix(
+        inputsize = 8,
+        specification = "PLEARNDIR:examples/data/uci_mldb/abalone_all.vmat",
+        targetsize = 1,
+        weightsize = 0
+        ),
+    expdir = "expdir",
+    learner = *3 -> RegressionTree(
+        complexity_penalty_factor = 0.0,
+        compute_train_stats = 1,
+        forget_when_training_set_changes = 1,
+        leave_template = *2 -> RegressionTreeLeave( ),
+        loss_function_weight = 1,
+        maximum_number_of_nodes = 50,
+        nstages = 10,
+        report_progress = 1,
+        verbosity = 0
+        ),
+    provide_learner_expdir = 1,
+    save_test_confidence = 1,
+    save_test_costs = 1,
+    save_test_outputs = 1,
+    splitter = *4 -> FractionSplitter(
+        splits = 1 2 [
+                (0, 0.75),
+                (0.75, 1)
+                ]
+        ),
+    statnames = [
+        "E[train.E[mse]]",
+        "E[train.E[base_confidence]]",
+        "E[train.E[base_reward_l2]]",
+        "E[train.E[base_reward_l1]]",
+        "E[test1.E[mse]]",
+        "E[test1.E[base_confidence]]",
+        "E[test1.E[base_reward_l2]]",
+        "E[test1.E[base_reward_l1]]"
+        ]
+    )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,8 @@
+E[train.E[mse]]	0
+E[train.E[base_confidence]]	0
+E[train.E[base_reward_l2]]	0
+E[train.E[base_reward_l1]]	0
+E[test1.E[mse]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/sizes	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/sizes	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,2 @@
+__REVISION__ = "PL8390"
+data                                          = PLEARNDIR:examples/data/uci_mldb/abalone_all.vmat

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,9 @@
+splitnum	0
+train.E[mse]	0
+train.E[base_confidence]	0
+train.E[base_reward_l2]	0
+train.E[base_reward_l1]	0
+test1.E[mse]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/sizes	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/sizes	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,4 @@
+mse
+base_confidence
+base_reward_l2
+base_reward_l1

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,94 @@
+PTester(
+expdir = "PYTEST__PL_RegressionTree__RESULTS:expdir/" ;
+dataset = *1 ->AutoVMatrix(
+filename = "UCI_MLDB:abalone_all.vmat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 4177 ;
+width = 9 ;
+inputsize = 8 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "UCI_MLDB:abalone_all.vmat.metadata/"  )
+;
+splitter = *2 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  2  [ 
+(0 , 0.75 )	(0.75 , 1 )	
+]
+ )
+;
+statnames = 8 [ "E[train.E[mse]]" "E[train.E[base_confidence]]" "E[train.E[base_reward_l2]]" "E[train.E[base_reward_l1]]" "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *3 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 50 ;
+compute_train_stats = 1 ;
+complexity_penalty_factor = 0 ;
+multiclass_outputs = []
+;
+leave_template = *4 ->RegressionTreeLeave(
+id = 0 ;
+missing_leave = 0 ;
+loss_function_weight = 0 ;
+verbosity = 0 ;
+train_set = *0 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 0 ;
+output = []
+;
+error = []
+ )
+;
+sorted_train_set = *0 ;
+root = *0 ;
+priority_queue = *0 ;
+first_leave = *0 ;
+first_leave_output = []
+;
+first_leave_error = []
+;
+split_cols = []
+;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 0 ;
+n_examples = -1 ;
+inputsize = -1 ;
+targetsize = -1 ;
+weightsize = -1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 10 ;
+report_progress = 1 ;
+verbosity = 0 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 1 ;
+save_stat_collectors = 1 ;
+save_learners = 1 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 1 ;
+call_forget_in_run = 1 ;
+save_test_costs = 1 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 1 ;
+enforce_clean_expdir = 1  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/train_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/train_cost_names.txt	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/train_cost_names.txt	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,4 @@
+mse
+base_confidence
+base_reward_l2
+base_reward_l1

Added: trunk/plearn_learners/regressors/test/RegressionTree/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/pytest.config	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/pytest.config	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,89 @@
+"""Pytest config file.
+
+Test is a class regrouping the elements that define a test for PyTest.
+    
+    For each Test instance you declare in a config file, a test will be ran
+    by PyTest.
+    
+    @ivar name: The name of the Test must uniquely determine the
+    test. Among others, it will be used to identify the test's results
+    (.PyTest/I{name}/*_results/) and to report test informations.
+    @type name: String
+    
+    @ivar description: The description must provide other users an
+    insight of what exactly is the Test testing. You are encouraged
+    to used triple quoted strings for indented multi-lines
+    descriptions.
+    @type description: String
+    
+    @ivar category: The category to which this test belongs. By default, a
+    test is considered a 'General' test.
+    
+    It is not desirable to let an extensive and lengthy test as 'General',
+    while one shall refrain abusive use of categories since it is likely
+    that only 'General' tests will be ran before most commits...
+    
+    @type category: string
+    
+    @ivar program: The program to be run by the Test. The program's name
+    PRGNAME is used to lookup for the program in the following manner::
+    
+    1) Look for a local program named PRGNAME
+    2) Look for a plearn-like command (plearn, plearn_tests, ...) named PRGNAME
+    3) Call 'which PRGNAME'
+    4) Fail
+    
+    Compilable program should provide the keyword argument 'compiler'
+    mapping to a string interpreted as the compiler name (e.g.
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument.  @type program:
+    L{Program}
+    
+    @ivar arguments: The command line arguments to be passed to the program
+    for the test to proceed.
+    @type arguments: String
+    
+    @ivar resources: A list of resources that are used by your program
+    either in the command line or directly in the code (plearn or pyplearn
+    files, databases, ...).  The elements of the list must be string
+    representations of the path, absolute or relative, to the resource.
+    @type resources: List of Strings
+    
+    @ivar precision: The precision (absolute and relative) used when comparing
+    floating numbers in the test output (default = 1e-6)
+    @type precision: float
+    
+    @ivar pfileprg: The program to be used for comparing files of psave &
+    vmat formats. It can be either::
+    - "__program__": maps to this test's program if its compilable;
+    maps to 'plearn_tests' otherwise (default);
+    - "__plearn__": always maps to 'plearn_tests' (for when the program
+    under test is not a version of PLearn);
+    - A Program (see 'program' option) instance
+    - None: if you are sure no files are to be compared.
+    
+    @ivar ignored_files_re: Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+    @type ignored_files_re: list of regular expressions
+    
+    @ivar disabled: If true, the test will not be ran.
+    @type disabled: bool
+    
+"""
+Test(
+    name = "PL_RegressionTree",
+    description = "Exercise basic functionality of RegressionTree",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "regression_tree.pyplearn",
+    resources = [ "regression_tree.pyplearn" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False
+    )

Added: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-01-21 00:52:36 UTC (rev 8393)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-01-21 14:48:05 UTC (rev 8394)
@@ -0,0 +1,45 @@
+import os.path
+from plearn.pyplearn import *
+
+plarg_defaults.data    = "PLEARNDIR:examples/data/uci_mldb/abalone_all.vmat"
+
+dataset = pl.AutoVMatrix(
+    specification = plargs.data,
+    inputsize = 8,
+    targetsize = 1,
+    weightsize = 0
+    )
+
+#learner = pl.RegresssionTree()
+learner = pl.RegressionTree(
+        nstages = 10
+        ,loss_function_weight = 1
+#        ,missing_is_valid = 0
+#        ,multiclass_outputs = []
+        ,maximum_number_of_nodes = 50
+        ,compute_train_stats = 1
+        ,complexity_penalty_factor = 0.0
+        ,verbosity = 0
+        ,report_progress = 1
+        ,forget_when_training_set_changes = 1
+#        ,conf_rated_adaboost = 0
+        ,leave_template = pl.RegressionTreeLeave( )
+        )
+splitter = pl.FractionSplitter(
+    splits = TMat(1,2, [ (0,0.75),  (0.75,1) ])
+    )
+tester = pl.PTester(
+    expdir = plargs.expdir,
+    dataset = dataset,
+    splitter = splitter,
+    learner = learner,
+    statnames = ['E[train.E[mse]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[mse]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]'],
+    provide_learner_expdir = 1,
+    save_test_costs = 1,
+    save_test_outputs = 1,
+    save_test_confidence = 1
+    )
+
+def main():
+    return tester



From manzagop at mail.berlios.de  Mon Jan 21 16:54:49 2008
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 21 Jan 2008 16:54:49 +0100
Subject: [Plearn-commits] r8395 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200801211554.m0LFsntE023329@sheep.berlios.de>

Author: manzagop
Date: 2008-01-21 16:54:48 +0100 (Mon, 21 Jan 2008)
New Revision: 8395

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
PLearn::save() seems to have a different behavior when compiled on mammouth's icc.
If called with pointer, it will not serialize the object, but only save the pointer's 
value. Temporary fix for this learner: replace save(...,this) by save(...,*this).


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-01-21 14:48:05 UTC (rev 8394)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-01-21 15:54:48 UTC (rev 8395)
@@ -370,11 +370,11 @@
 
             bool must_train_supervised_layer = supervised_nepochs.second>0;
             
-            PLearn::save(expdir/"learner.psave", this);
+            PLearn::save(expdir/"learner.psave", *this);
             for(int k=0; k<nreconstructions; k++)
             {
                 trainHiddenLayer(k, dset);
-                PLearn::save(expdir/"learner.psave", this);
+                PLearn::save(expdir/"learner.psave", *this);
                 // 'if' is a hack to avoid precomputing last hidden layer if not needed
                 if(k<nreconstructions-1 ||  must_train_supervised_layer) 
                 { 
@@ -389,7 +389,7 @@
             if(must_train_supervised_layer)
             {
                 trainSupervisedLayer(dset, targets);
-                PLearn::save(expdir/"learner.psave", this);
+                PLearn::save(expdir/"learner.psave", *this);
             }
             perr << "\n\n*********************************************" << endl;
             perr << "****      Now performing fine tuning     ****" << endl;



From nouiz at mail.berlios.de  Mon Jan 21 17:56:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Jan 2008 17:56:57 +0100
Subject: [Plearn-commits] r8396 - trunk/python_modules/plearn/utilities
Message-ID: <200801211656.m0LGuv58027774@sheep.berlios.de>

Author: nouiz
Date: 2008-01-21 17:56:56 +0100 (Mon, 21 Jan 2008)
New Revision: 8396

Added:
   trunk/python_modules/plearn/utilities/hg.py
Modified:
   trunk/python_modules/plearn/utilities/ppath.py
   trunk/python_modules/plearn/utilities/version_control.py
Log:
try to add mercurial(hg) as version control system to allow compiling without svn


Added: trunk/python_modules/plearn/utilities/hg.py
===================================================================
--- trunk/python_modules/plearn/utilities/hg.py	2008-01-21 15:54:48 UTC (rev 8395)
+++ trunk/python_modules/plearn/utilities/hg.py	2008-01-21 16:56:56 UTC (rev 8396)
@@ -0,0 +1,135 @@
+import logging, os, sys, tempfile
+from ppath import ppath
+
+def __report_status( cmd ):
+    if sys.platform == 'win32':
+        # os.WEXITSTATUS is not implemented under Windows. However, on recent
+        # Windows platform, this should be the command exit status.
+        return os.system( cmd ) == 0
+    else:
+        return os.WEXITSTATUS( os.system( cmd ) ) == 0
+    
+def add(path):
+    raise NotImplementedError
+    add_cmd = "svn add -N %s" % path
+    logging.debug("Adding: %s" % add_cmd)    
+
+    return __report_status( add_cmd )
+
+def commit(files, msg):
+    raise NotImplementedError
+    
+    if isinstance( files, str ):
+        files = [files]
+    elif not isinstance(files, type([])):
+        raise TypeError("The commit procedure accepts argument of type string of"
+                        "array of string: type (%s) is not valid.\n" % type(files))
+    
+    commit_cmd = "svn commit -m '" + msg + "' " + " ".join(files)
+
+    logging.info("\n+++ Commiting (from "+ os.getcwd() +"):\n" + commit_cmd)
+    os.system(commit_cmd) 
+
+def ignore( path, list_of_paths ):
+    raise NotImplementedError
+    
+    get_ign_prop_cmd = "svn propget --strict svn:ignore %s" % path
+    h = os.popen(get_ign_prop_cmd)
+    already_ignored = h.readlines()
+    h.close()
+    to_ignore = []
+
+    # Remove trailing carriage returns.
+    for file in already_ignored:
+        to_ignore.append(file.strip(os.linesep))
+
+    for added in list_of_paths:
+        if added not in to_ignore:
+            to_ignore.append(added)
+
+    to_ignore_full_string = ""
+    for file in to_ignore:
+        to_ignore_full_string += file + os.linesep
+    propfile = tempfile.NamedTemporaryFile()
+    propfile.write(to_ignore_full_string)
+    propfile.flush()
+    # Remove trailing carriage return.
+    to_ignore_full_string = to_ignore_full_string.strip(os.linesep)
+    ignore_cmd = "svn propset svn:ignore -F %s %s" % (propfile.name, path)
+    result = __report_status( ignore_cmd )
+    propfile.close()
+    return result
+
+def last_user_to_commit(file_path):
+    raise NotImplementedError
+
+def project_version(project_path, build_list):
+    raise NotImplementedError
+
+    """Automatic version control.
+    
+    Builds are tuples of the form (major, minor, plearn_revision) where
+    plearn_revision is the SVN revision of the PLearn project when the
+    project's version was released.
+    """    
+    #plearn_version  = int(repository_revision(ppath('PLEARN')))
+    #project_version = int(repository_revision(ppath(project_path)))
+    version_number, release_version = build_list[-1]
+    return "%s (Released on PL%d)"%(version_number, release_version)
+
+def query(option, fname, lookingFor, delim = "\n"):
+    raise NotImplementedError
+
+def recursive_add( path ):
+    raise NotImplementedError
+
+    add_cmd = "svn add %s" % path
+    logging.debug("Adding: %s" % add_cmd)
+
+    return __report_status( add_cmd )
+
+def recursive_remove(path, options=""):
+    raise NotImplementedError
+
+    rm_cmd = "svn remove %s %s"%(path, options)
+    logging.debug("Removing: %s" % rm_cmd)
+
+    return __report_status( rm_cmd )
+
+
+def repository_revision( path ):
+    "Return the hg repository version (as a string) within 'path'."
+    try:
+        current_path = os.getcwd()
+
+        # First move to the given path, as svn does not always like absolute
+        # directories.
+        os.chdir(path)
+        h = os.popen("hg identify")
+
+        # Move back to the previous current directory.
+        os.chdir(current_path)
+
+        x = h.readlines()
+        h.close()
+        return x[0].strip()
+    except Exception, e:
+        print 'Could not find hg revision number:', e
+        return "NO_REVISION"
+
+def update( path ):
+    raise NotImplementedError
+
+    up_cmd = "svn update %s" % path
+    logging.debug("Updating: %s" % up_cmd)
+    return __report_status( up_cmd )
+
+
+## def remove( path ):
+##     rm_cmd = "svn remove -N %s" % path
+##     vprint("Removing: %s" % rm_cmd, 2)
+
+##     return __report_status( rm_cmd )
+
+
+    

Modified: trunk/python_modules/plearn/utilities/ppath.py
===================================================================
--- trunk/python_modules/plearn/utilities/ppath.py	2008-01-21 15:54:48 UTC (rev 8395)
+++ trunk/python_modules/plearn/utilities/ppath.py	2008-01-21 16:56:56 UTC (rev 8396)
@@ -66,11 +66,12 @@
 pymake_hidden          = ".pymake"
 cvs_directory          = "CVS"
 subversion_hidden      = ".svn"
+hg_hidden              = ".hg"
 pytest_dir             = ".pytest"
 
 special_directories    = [ skeldir,
                            pymake_objs,   pymake_hidden,
-                           cvs_directory, subversion_hidden,
+                           cvs_directory, subversion_hidden, hg_hidden,
                            pytest_dir     ]
 
 

Modified: trunk/python_modules/plearn/utilities/version_control.py
===================================================================
--- trunk/python_modules/plearn/utilities/version_control.py	2008-01-21 15:54:48 UTC (rev 8395)
+++ trunk/python_modules/plearn/utilities/version_control.py	2008-01-21 16:56:56 UTC (rev 8396)
@@ -1,4 +1,4 @@
-import cvs, svn, os
+import cvs, svn, os, hg
 
 from plearn.utilities.ppath     import cvs_directory, subversion_hidden
 from plearn.utilities.verbosity import vprint
@@ -10,6 +10,8 @@
         return cvs
     elif os.path.isdir(subversion_hidden):
         return svn
+    elif os.path.isdir(hg_hidden):
+        return hg
     else:
         raise VersionControlError("Could not figure out version control system to use!")
 



From nouiz at mail.berlios.de  Mon Jan 21 19:35:03 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Jan 2008 19:35:03 +0100
Subject: [Plearn-commits] r8397 - trunk/examples/data/uci_mldb
Message-ID: <200801211835.m0LIZ3pH020269@sheep.berlios.de>

Author: nouiz
Date: 2008-01-21 19:35:03 +0100 (Mon, 21 Jan 2008)
New Revision: 8397

Modified:
   trunk/examples/data/uci_mldb/abalone_test.vmat
   trunk/examples/data/uci_mldb/abalone_test_norm.vmat
   trunk/examples/data/uci_mldb/abalone_train.vmat
   trunk/examples/data/uci_mldb/abalone_train_norm.vmat
Log:
updated to the new option.


Modified: trunk/examples/data/uci_mldb/abalone_test.vmat
===================================================================
--- trunk/examples/data/uci_mldb/abalone_test.vmat	2008-01-21 16:56:56 UTC (rev 8396)
+++ trunk/examples/data/uci_mldb/abalone_test.vmat	2008-01-21 18:35:03 UTC (rev 8397)
@@ -4,7 +4,7 @@
 BootstrapVMatrix( 
   shuffle = 1
   frac = 1
-  own_seed = 12345678
+  seed = 12345678
   source =
     $INCLUDE{UCI_MLDB:abalone_test_noshuffle.vmat}
 )

Modified: trunk/examples/data/uci_mldb/abalone_test_norm.vmat
===================================================================
--- trunk/examples/data/uci_mldb/abalone_test_norm.vmat	2008-01-21 16:56:56 UTC (rev 8396)
+++ trunk/examples/data/uci_mldb/abalone_test_norm.vmat	2008-01-21 18:35:03 UTC (rev 8397)
@@ -4,7 +4,7 @@
 BootstrapVMatrix( 
   shuffle = 1
   frac = 1
-  own_seed = 12345678
+  seed = 12345678
   source =
     $INCLUDE{UCI_MLDB:abalone_test_norm_noshuffle.vmat}
 )

Modified: trunk/examples/data/uci_mldb/abalone_train.vmat
===================================================================
--- trunk/examples/data/uci_mldb/abalone_train.vmat	2008-01-21 16:56:56 UTC (rev 8396)
+++ trunk/examples/data/uci_mldb/abalone_train.vmat	2008-01-21 18:35:03 UTC (rev 8397)
@@ -4,7 +4,7 @@
 BootstrapVMatrix( 
   shuffle = 1
   frac = 1
-  own_seed = 12345678
+  seed = 12345678
   source =
     $INCLUDE{UCI_MLDB:abalone_train_noshuffle.vmat}
 )

Modified: trunk/examples/data/uci_mldb/abalone_train_norm.vmat
===================================================================
--- trunk/examples/data/uci_mldb/abalone_train_norm.vmat	2008-01-21 16:56:56 UTC (rev 8396)
+++ trunk/examples/data/uci_mldb/abalone_train_norm.vmat	2008-01-21 18:35:03 UTC (rev 8397)
@@ -4,7 +4,7 @@
 BootstrapVMatrix( 
   shuffle = 1
   frac = 1
-  own_seed = 12345678
+  seed = 12345678
   source =
     $INCLUDE{UCI_MLDB:abalone_train_norm_noshuffle.vmat}
 )



From nouiz at mail.berlios.de  Mon Jan 21 20:09:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Jan 2008 20:09:23 +0100
Subject: [Plearn-commits] r8398 - trunk/python_modules/plearn/utilities
Message-ID: <200801211909.m0LJ9NH8022259@sheep.berlios.de>

Author: nouiz
Date: 2008-01-21 20:09:22 +0100 (Mon, 21 Jan 2008)
New Revision: 8398

Modified:
   trunk/python_modules/plearn/utilities/hg.py
   trunk/python_modules/plearn/utilities/version_control.py
Log:
bugfix to allow compiling with pymake with mercurial


Modified: trunk/python_modules/plearn/utilities/hg.py
===================================================================
--- trunk/python_modules/plearn/utilities/hg.py	2008-01-21 18:35:03 UTC (rev 8397)
+++ trunk/python_modules/plearn/utilities/hg.py	2008-01-21 19:09:22 UTC (rev 8398)
@@ -105,7 +105,7 @@
         # First move to the given path, as svn does not always like absolute
         # directories.
         os.chdir(path)
-        h = os.popen("hg identify")
+        h = os.popen("hg identify | cut -d' ' -f1")
 
         # Move back to the previous current directory.
         os.chdir(current_path)

Modified: trunk/python_modules/plearn/utilities/version_control.py
===================================================================
--- trunk/python_modules/plearn/utilities/version_control.py	2008-01-21 18:35:03 UTC (rev 8397)
+++ trunk/python_modules/plearn/utilities/version_control.py	2008-01-21 19:09:22 UTC (rev 8398)
@@ -1,6 +1,6 @@
 import cvs, svn, os, hg
 
-from plearn.utilities.ppath     import cvs_directory, subversion_hidden
+from plearn.utilities.ppath     import cvs_directory, subversion_hidden, hg_hidden
 from plearn.utilities.verbosity import vprint
 
 class VersionControlError( Exception ): pass



From nouiz at mail.berlios.de  Tue Jan 22 21:05:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 22 Jan 2008 21:05:49 +0100
Subject: [Plearn-commits] r8399 - trunk
Message-ID: <200801222005.m0MK5nwK006080@sheep.berlios.de>

Author: nouiz
Date: 2008-01-22 21:05:48 +0100 (Tue, 22 Jan 2008)
New Revision: 8399

Modified:
   trunk/pymake.config.model
Log:
add option to use the vectorizing capacity of gcc. Theis is no evidence right now that this is faster.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-01-21 19:09:22 UTC (rev 8398)
+++ trunk/pymake.config.model	2008-01-22 20:05:48 UTC (rev 8399)
@@ -257,7 +257,7 @@
     'purify', 'quantify', 'vc++', 'condor' ],
   
   [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbggprof', 'safegprof',
-    'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg' ],
+    'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg', 'vecgcc' ],
   
   [ 'double', 'float' ],
   
@@ -639,6 +639,11 @@
               compileroptions = '-Wall -O3',
               cpp_definitions = ['NDEBUG'] )
 
+pymakeOption( name = 'vecgcc',
+              description = 'vectorized with gcc compiler in opt mode',
+              compileroptions = '-Wall -O3 -ftree-vectorize -ftree-vectorizer-verbose=5 -msse2',
+              cpp_definitions = ['NDEBUG'] )
+
 pymakeOption( name = 'pintel',
               description = 'parallelized for intel compiler',
               compileroptions = '-O3 -parallel',



From nouiz at mail.berlios.de  Tue Jan 22 21:08:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 22 Jan 2008 21:08:00 +0100
Subject: [Plearn-commits] r8400 - in trunk/plearn: misc vmat
Message-ID: <200801222008.m0MK807f006246@sheep.berlios.de>

Author: nouiz
Date: 2008-01-22 21:07:59 +0100 (Tue, 22 Jan 2008)
New Revision: 8400

Modified:
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
in compareStats check for more corner case and calculate the sum of difference


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-01-22 20:05:48 UTC (rev 8399)
+++ trunk/plearn/misc/vmatmain.cc	2008-01-22 20:07:59 UTC (rev 8400)
@@ -957,8 +957,14 @@
             stderror_threshold=toreal(argv[4]);
         if(argc>5)
             missing_threshold=toreal(argv[5]);
-        int diff = m1->compareStats(m2, stderror_threshold, missing_threshold);
-        cout<<"Their is "<<diff<<" fields that have different stats"<<endl;
+        real sumdiff_stderr = 0;
+        real sumdiff_missing = 0;
+        int diff = m1->compareStats(m2, stderror_threshold, missing_threshold,
+                                    &sumdiff_stderr, &sumdiff_missing);
+        cout<<"Their is "<<diff<<"/"<<m1.width()
+            <<" fields that have different stats"<<endl;
+        cout <<"The sum of stderror difference is "<<sumdiff_stderr<<endl;
+        cout <<"The sum of missing difference is "<<sumdiff_missing<<endl;
 
     }
     else

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-01-22 20:05:48 UTC (rev 8399)
+++ trunk/plearn/vmat/VMatrix.cc	2008-01-22 20:07:59 UTC (rev 8400)
@@ -1896,14 +1896,16 @@
 
 int VMatrix:: compareStats(const VMat& target,
                            const real stderror_threshold,
-                           const real missing_threshold) const
+                           const real missing_threshold,
+                           real* sumdiff_stderr,
+                           real* sumdiff_missing) const
 {
-#ifdef BOUNDCHECK
     if(target->width()!=width())
         PLERROR("In VecStatsCollector:: compareStats() - this vmatris have width %d witch differ from the target width of %d", width(), target->width());
-#endif
-    int diff = 0;
 
+    int nbdiff            = 0;
+    real sumdiff_stderr_  = 0;
+    real sumdiff_missing_ = 0;
     for(int i=0;i<width();i++)
     {
         const StatsCollector tstats = target->getStats(i);
@@ -1911,26 +1913,47 @@
 
         real tmissing = tstats.nmissing()/tstats.n();
         real lmissing = lstats.nmissing()/lstats.n();
-        if(lmissing<(tmissing-missing_threshold/100) || lmissing>(tmissing+missing_threshold/100))
+        real terr = sqrt(tmissing*(1-tmissing)+lmissing*(1-lmissing));
+        real th = fabs(tmissing-lmissing)/terr;
+        if(terr==0)
+            PLCHECK(tmissing==0 && lmissing==0);
+        else if(isnan(th))
+            PLWARNING("In VMatrix::compareStats - should not happen!");
+        else
+            sumdiff_missing_ += th;
+        if(th>missing_threshold)
         {
-            PLWARNING("In VMatrix::compareStats - field %d(%s) have %f missing while target stats have %f",
-                      i, fieldName(i).c_str(), lmissing, tmissing);
-            diff++;
+            PLWARNING("In VMatrix::compareStats - field %d(%s) have %f"
+                      " missing while target stats have %f."
+                      " The stats difference is %f.", 
+                      i, fieldName(i).c_str(), lmissing, tmissing, th);
+            nbdiff++;
         }
         real tmean = tstats.mean();
         real lmean = lstats.mean();
-        real tstderror = tstats.stderror();
-        real  th = (lmean-tmean)/tstderror;
-
+        real tstderror = sqrt(pow(tstats.stderror(),2.) + 
+                              pow(lstats.stderror(),2.));
+        th = fabs(lmean-tmean)/tstderror;
+        if(tstderror==0)
+            PLWARNING("In VMatrix::compareStats - field %d(%s) have a"
+                      " stderror of 0 for both matrice.",
+                      i, fieldName(i).c_str());
+        else
+            sumdiff_stderr_+=th;
         if(th>stderror_threshold)
         {
             PLWARNING("In VMatrix::compareStats - field %d(%s) have mean %f"
-                      " while target mean is %f and target stderror is %f. They differ by %f stderror",
+                      " while target mean is %f and target stderror is %f."
+                      " They differ by %f stderror",
                       i, fieldName(i).c_str(), lmean, tmean, tstderror, th);
-            diff++;
+            nbdiff++;
         }
     }
-     return diff;
+    if(sumdiff_stderr!=NULL)
+        *sumdiff_stderr = sumdiff_stderr_;
+    if(sumdiff_missing!=NULL)
+        *sumdiff_missing = sumdiff_missing_;
+    return nbdiff;
 }
 } // end of namespace PLearn
 

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-01-22 20:05:48 UTC (rev 8399)
+++ trunk/plearn/vmat/VMatrix.h	2008-01-22 20:07:59 UTC (rev 8400)
@@ -598,14 +598,16 @@
 
     
     //! Compare the stats of this VecStatsCollector with the target one.
-    //! @param target the VecStatsCollector we compare again
-    //! @param stderror_threshold The threshold alowed for the standard error
-    //! @param missing_threshold The threshold alowed for the % of missing
+    //! @param target the VMat we compare again
+    //! @param stderror_threshold The threshold allowed for the standard error
+    //! @param missing_threshold The threshold allowed for the % of missing
     //! @return If they are comparable with respect to the gived threshold,
-    //! we return true. Otherwise false
+    //! we return true. Otherwise we return false.
     int compareStats(const VMat& target,
                      const real stderror_threshold = 2,
-                     const real missing_threshold = 10) const;
+                     const real missing_threshold = 2,
+                     real* sumdiff_stderr = NULL,
+                     real* sumdiff_missing = NULL) const;
 
 
     /**



From nouiz at mail.berlios.de  Tue Jan 22 21:09:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 22 Jan 2008 21:09:50 +0100
Subject: [Plearn-commits] r8401 - trunk/scripts
Message-ID: <200801222009.m0MK9o2k006475@sheep.berlios.de>

Author: nouiz
Date: 2008-01-22 21:09:50 +0100 (Tue, 22 Jan 2008)
New Revision: 8401

Modified:
   trunk/scripts/multipymake
Log:
print the date when finished. usefull sometime


Modified: trunk/scripts/multipymake
===================================================================
--- trunk/scripts/multipymake	2008-01-22 20:07:59 UTC (rev 8400)
+++ trunk/scripts/multipymake	2008-01-22 20:09:50 UTC (rev 8401)
@@ -70,3 +70,5 @@
   pymake $i ${PYMAKEFLAGS[@]} ${BASEPROG}${iname}.cc || ( echo "Build failed for $i"; exit)
   echo -e "Ended with status: $?\n"
 done
+
+echo "Time finished:" `date`



From nouiz at mail.berlios.de  Tue Jan 22 21:12:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 22 Jan 2008 21:12:24 +0100
Subject: [Plearn-commits] r8402 - trunk/python_modules/plearn/learners
Message-ID: <200801222012.m0MKCODl006839@sheep.berlios.de>

Author: nouiz
Date: 2008-01-22 21:12:24 +0100 (Tue, 22 Jan 2008)
New Revision: 8402

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
use os.path.join when needed, always use learner[12]_stage=* instead of learner[12]_stage#*


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-01-22 20:09:50 UTC (rev 8401)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-01-22 20:12:24 UTC (rev 8402)
@@ -1,6 +1,6 @@
 from plearn.pyext import *
 from plearn.pyplearn.plargs import *
-import time
+import time,os.path
 
 class AdaBoostMultiClasses:
     def __init__(self,trainSet1,trainSet2,weakLearner,confusion_target=1):
@@ -215,12 +215,11 @@
     def save(self,path="",encoding="plearn_ascii"):
         if not os.path.exists(path):
             os.mkdir(path)
-        if path:
-            path+="/"
-        else:
-            print "WARNING: AdaBoost3PLearner - no path for saving the learner, we use the current directory"
-        self.learner1.save(path+"learner1_stage="+str(self.stage)+".psave",encoding)
-        self.learner2.save(path+"learner2_stage="+str(self.stage)+".psave",encoding)
+
+        file1=os.path.join(path,"learner1_stage="+str(self.stage)+".psave")
+        file2=os.path.join(path,"learner2_stage="+str(self.stage)+".psave")
+        self.learner1.save(file1,encoding)
+        self.learner2.save(file2,encoding)
     
     def load_old_learner(self,filepath=None,trainSet1=None,trainSet2=None,stage1=-1,stage2=-1):
         assert(trainSet1 and trainSet2)
@@ -235,7 +234,7 @@
             filepath=max(tmp)
         #if stage=-1 we find the last one
         if stage1 == -1:
-            s="learner1_stage#"
+            s="learner1_stage="
             lens=len(s)
             e=".psave"
             lene=len(e)
@@ -245,7 +244,7 @@
                 if t>stage1: stage1=t
         #We must split stage1 and stage2 as one learner can early stop.
         if stage2 == -1:
-            s="learner2_stage#"
+            s="learner2_stage="
             lens=len(s)
             e=".psave"
             lene=len(e)
@@ -254,10 +253,11 @@
                 t=int(x[lens:-lene])
                 if t>stage2: stage2=t
                 
-        file1=filepath+"/learner1_stage#"+str(stage1)+".psave"
-        file2=filepath+"/learner2_stage#"+str(stage2)+".psave"
+        file1=os.path.join(filepath,"learner1_stage="+str(stage1)+".psave")
+        file2=os.path.join(filepath,"learner2_stage="+str(stage2)+".psave")
         if (not os.path.exists(file1)) or (not os.path.exists(file2)):
             print "ERROR: no file to load in the gived directory"
+            print "file",file1,"and file",file2, "do not exist"
             sys.exit(1)
         self.learner1=loadObject(file1)
         self.learner2=loadObject(file2)



From saintmlx at mail.berlios.de  Tue Jan 22 22:29:58 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 22 Jan 2008 22:29:58 +0100
Subject: [Plearn-commits] r8403 - trunk/python_modules/plearn/math
Message-ID: <200801222129.m0MLTwKF012288@sheep.berlios.de>

Author: saintmlx
Date: 2008-01-22 22:29:57 +0100 (Tue, 22 Jan 2008)
New Revision: 8403

Modified:
   trunk/python_modules/plearn/math/spline.py
Log:
- make sure input is float



Modified: trunk/python_modules/plearn/math/spline.py
===================================================================
--- trunk/python_modules/plearn/math/spline.py	2008-01-22 20:12:24 UTC (rev 8402)
+++ trunk/python_modules/plearn/math/spline.py	2008-01-22 21:29:57 UTC (rev 8403)
@@ -77,6 +77,7 @@
         calc. interpolated value from tabulated fn. + spline coeffs.
         """
         ox, oy, y2= self.ox, self.oy, self.y2
+        x= float(x)
         l= len(ox)
         i0= 0
         i1= l-1



From nouiz at mail.berlios.de  Tue Jan 22 23:06:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 22 Jan 2008 23:06:13 +0100
Subject: [Plearn-commits] r8404 - trunk/plearn_learners/testers
Message-ID: <200801222206.m0MM6DVs014984@sheep.berlios.de>

Author: nouiz
Date: 2008-01-22 23:06:12 +0100 (Tue, 22 Jan 2008)
New Revision: 8404

Modified:
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
added an option to save_test_name initialized to true so that it don't change the current behavior.
when set to false the file train_cost_names.txt and test_cost_names.txt are not created


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2008-01-22 21:29:57 UTC (rev 8403)
+++ trunk/plearn_learners/testers/PTester.cc	2008-01-22 22:06:12 UTC (rev 8404)
@@ -90,6 +90,7 @@
        save_stat_collectors(true),
        save_test_costs(false),
        save_test_outputs(false),
+       save_test_names(true),
        call_forget_in_run(true),
        save_test_confidence(false),
        should_train(true),
@@ -172,8 +173,7 @@
         ol, "report_stats", &PTester::report_stats, OptionBase::buildoption,
         "If true, the computed global statistics specified in statnames will be saved in global_stats.pmat \n"
         "and the corresponding per-split statistics will be saved in split_stats.pmat \n"
-        "For reference, all cost names (as given by the learner's getTrainCostNames() and getTestCostNames() ) \n"
-        "will be reported in files train_cost_names.txt and test_cost_names.txt");
+        "For reference, all cost names can be saved with the option save_test_names.");
 
     declareOption(
         ol, "save_initial_tester", &PTester::save_initial_tester, OptionBase::buildoption,
@@ -209,6 +209,11 @@
         "If true, the costs of the test for split #k will be saved in Split#k/test#i_costs.pmat");
 
     declareOption(
+        ol, "save_test_names", &PTester::save_test_names, OptionBase::buildoption,
+        "For reference, all cost names (as given by the learner's getTrainCostNames() and getTestCostNames() ) \n"
+        "will be reported in files train_cost_names.txt and test_cost_names.txt");
+
+    declareOption(
         ol, "provide_learner_expdir", &PTester::provide_learner_expdir, OptionBase::buildoption,
         "If true, each learner to be trained will have its experiment directory set to Split#k/LearnerExpdir/");
 
@@ -542,9 +547,10 @@
         
     if (expdir != "" && report_stats)
     {
-        saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
-        saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
-
+        if(save_test_names){
+            saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
+            saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
+        }
         global_stats_vm = new FileVMatrix(expdir / "global_stats.pmat",
                                           1, nstats);
         for (int k = 0; k < nstats; k++)
@@ -1052,9 +1058,10 @@
         
     if (expdir != "" && report_stats)
     {
-        saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
-        saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
-
+        if(save_test_names){
+            saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
+            saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
+        }
         global_stats_vm = new FileVMatrix(expdir / "global_stats.pmat",
                                           1, nstats);
         for (int k = 0; k < nstats; k++)

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2008-01-22 21:29:57 UTC (rev 8403)
+++ trunk/plearn_learners/testers/PTester.h	2008-01-22 22:06:12 UTC (rev 8404)
@@ -90,6 +90,7 @@
     bool save_stat_collectors;
     bool save_test_costs;
     bool save_test_outputs;
+    bool save_test_names;
     bool call_forget_in_run;
     PP<Splitter> splitter;
     TVec<TVec<string> > statmask;



From nouiz at mail.berlios.de  Tue Jan 22 23:07:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 22 Jan 2008 23:07:40 +0100
Subject: [Plearn-commits] r8405 - trunk/plearn_learners/testers
Message-ID: <200801222207.m0MM7eKM015025@sheep.berlios.de>

Author: nouiz
Date: 2008-01-22 23:07:39 +0100 (Tue, 22 Jan 2008)
New Revision: 8405

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
remove useless warning in case of badly formated string


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2008-01-22 22:06:12 UTC (rev 8404)
+++ trunk/plearn_learners/testers/PTester.cc	2008-01-22 22:07:39 UTC (rev 8405)
@@ -405,7 +405,7 @@
             int id = statnames_processed[i].find('[');
             char c=statnames_processed[i][id+5];
             if(c=='n'){}
-            else if(c>(nb_testset+'0'))
+            else if(pl_islong(tostring(c)) && c>(nb_testset+'0'))
                 PLWARNING("In PTester::build_() - the statnames %s ask for"
                           " test set %c while their is only %d test set.",
                           statnames_processed[i].c_str(),



From nouiz at mail.berlios.de  Wed Jan 23 18:01:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 23 Jan 2008 18:01:40 +0100
Subject: [Plearn-commits] r8406 - trunk/plearn_learners/hyper
Message-ID: <200801231701.m0NH1ejW018376@sheep.berlios.de>

Author: nouiz
Date: 2008-01-23 18:01:39 +0100 (Wed, 23 Jan 2008)
New Revision: 8406

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
Better message error


Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-01-22 22:07:39 UTC (rev 8405)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-01-23 17:01:39 UTC (rev 8406)
@@ -299,8 +299,13 @@
     int trialnum = 0;
 
     which_cost_pos= getResultNames().find(which_cost);
-    if(which_cost_pos < 0)
+    if(which_cost_pos < 0){
+        if(!pl_islong(which_cost))
+            PLERROR("In HyperOptimize::optimize() -  option 'which_cost' with "
+                    "value '%s' is not a number and is not a valid result test name",
+                    which_cost.c_str());
         which_cost_pos= toint(which_cost);
+    }
 
     Vec results;
     while(option_vals)



From nouiz at mail.berlios.de  Wed Jan 23 18:18:25 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 23 Jan 2008 18:18:25 +0100
Subject: [Plearn-commits] r8407 - in
 trunk/plearn_learners/regressors/test/RegressionTree: .
 .pytest/PL_RegressionTree/expected_results
 .pytest/PL_RegressionTree/expected_results/expdir
 .pytest/PL_RegressionTree/expected_results/expdir/Split0
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 .pytest/PL_RegressionTree/expec!
 ted_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata
 .pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata
 .pytest/PL_RegressionTree/expected_!
 results/expdir/global_stats.pmat.metadata .pytest/PL_Regressio! nTree/ex
Message-ID: <200801231718.m0NHIPdk022535@sheep.berlios.de>

Author: nouiz
Date: 2008-01-23 18:18:21 +0100 (Wed, 23 Jan 2008)
New Revision: 8407

Added:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/train_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
Log:
test with early stopping and changed dataset so that it should work at apstat


Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,5 @@
+HyperLearner: starting the optimization
+split_cols: []
+
+split_cols: 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 2 
+split_cols: 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 2 3 3 3 1 3 3 1 2 1 3 3 3 1 3 1 1 1 1 3 3 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 925509.521859630011 ;
+sumsquare_ = 25141454081.3727722 ;
+sumcube_ = 876439142897729.75 ;
+sumfourth_ = 4.10303450723266068e+19 ;
+min_ = 0.0830570112682627176 ;
+max_ = 73642.6125143663958 ;
+agmemin_ = 10 ;
+agemax_ = 95 ;
+first_ = 2696.74734765470976 ;
+last_ = 1462.79140675898816 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -1851019.04371926002 ;
+sumsquare_ = 100565816325.491089 ;
+sumcube_ = -7011513143181838 ;
+sumfourth_ = 6.5648552115722571e+20 ;
+min_ = -147284.225028732792 ;
+max_ = 0.833885977463474592 ;
+agmemin_ = 95 ;
+agemax_ = 10 ;
+first_ = -5392.49469530941951 ;
+last_ = -2924.58281351797632 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -7047.2320373333414 ;
+sumsquare_ = 2238180.95607663365 ;
+sumcube_ = -435311323.447215438 ;
+sumfourth_ = 124843839496.819016 ;
+min_ = -541.743447733333369 ;
+max_ = 0.423607733333408731 ;
+agmemin_ = 95 ;
+agemax_ = 10 ;
+first_ = -102.860432266666592 ;
+last_ = -75.4929122666665933 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 406342.24990348448 ;
+sumsquare_ = 12393944637.4200306 ;
+sumcube_ = 499488915281934.375 ;
+sumfourth_ = 2.36333474982238822e+19 ;
+min_ = 0.0327125581869371004 ;
+max_ = 61858.7314611160909 ;
+agmemin_ = 20 ;
+agemax_ = 15 ;
+first_ = 2228.12756426559145 ;
+last_ = 9063.38769912142379 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -812684.499806968961 ;
+sumsquare_ = 49575778549.6801224 ;
+sumcube_ = -3995911322255475 ;
+sumfourth_ = 3.78133559971582116e+20 ;
+min_ = -123716.462922232182 ;
+max_ = 0.934574883626125841 ;
+agmemin_ = 15 ;
+agemax_ = 20 ;
+first_ = -4455.25512853118289 ;
+last_ = -18125.7753982428476 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -3525.57301386666904 ;
+sumsquare_ = 959697.857134004356 ;
+sumcube_ = -237882864.922130883 ;
+sumfourth_ = 74259479439.1794434 ;
+min_ = -496.42831226666658 ;
+max_ = 0.638267733333410803 ;
+agmemin_ = 15 ;
+agemax_ = 20 ;
+first_ = -93.4060922666665903 ;
+last_ = -189.403652266666597 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 925509.521859630011 ;
+sumsquare_ = 25141454081.3727722 ;
+sumcube_ = 876439142897729.75 ;
+sumfourth_ = 4.10303450723266068e+19 ;
+min_ = 0.0830570112682627176 ;
+max_ = 73642.6125143663958 ;
+agmemin_ = 10 ;
+agemax_ = 95 ;
+first_ = 2696.74734765470976 ;
+last_ = 1462.79140675898816 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -1851019.04371926002 ;
+sumsquare_ = 100565816325.491089 ;
+sumcube_ = -7011513143181838 ;
+sumfourth_ = 6.5648552115722571e+20 ;
+min_ = -147284.225028732792 ;
+max_ = 0.833885977463474592 ;
+agmemin_ = 95 ;
+agemax_ = 10 ;
+first_ = -5392.49469530941951 ;
+last_ = -2924.58281351797632 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -7047.2320373333414 ;
+sumsquare_ = 2238180.95607663365 ;
+sumcube_ = -435311323.447215438 ;
+sumfourth_ = 124843839496.819016 ;
+min_ = -541.743447733333369 ;
+max_ = 0.423607733333408731 ;
+agmemin_ = 95 ;
+agemax_ = 10 ;
+first_ = -102.860432266666592 ;
+last_ = -75.4929122666665933 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,8 @@
+E[test1.E[mse]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[mse]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[mse]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[mse]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 8640.51363742984176 ;
+sumsquare_ = 1374774.3118040429 ;
+sumcube_ = 323112262.385960281 ;
+sumfourth_ = 93349681763.0307617 ;
+min_ = 0 ;
+max_ = 434.308100803600041 ;
+agmemin_ = 120 ;
+agemax_ = 102 ;
+first_ = 3.3058115703008113 ;
+last_ = 39.3579314881000286 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -17281.0272748596835 ;
+sumsquare_ = 5499097.24721617159 ;
+sumcube_ = -2584898099.08768225 ;
+sumfourth_ = 1493594908208.49219 ;
+min_ = -867.616201607200082 ;
+max_ = 1 ;
+agmemin_ = 102 ;
+agemax_ = 120 ;
+first_ = -5.61162314060162259 ;
+last_ = -77.7158629762000572 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -1392.4175074365653 ;
+sumsquare_ = 24435.3412616714304 ;
+sumcube_ = -524149.893896332418 ;
+sumfourth_ = 13079901.8943236303 ;
+min_ = -40.6801200000000023 ;
+max_ = 1 ;
+agmemin_ = 102 ;
+agemax_ = 120 ;
+first_ = -2.63637818181817352 ;
+last_ = -11.5471800000000044 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 10321.7894884868674 ;
+sumsquare_ = 71200030.8763843179 ;
+sumcube_ = 534446154111.910217 ;
+sumfourth_ = 4273059163655376.5 ;
+min_ = 0.296827275773477062 ;
+max_ = 8311.49117606439722 ;
+agmemin_ = 45 ;
+agemax_ = 35 ;
+first_ = 231.958382832400019 ;
+last_ = 187.926810649600071 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -20643.5789769737348 ;
+sumsquare_ = 284800123.505537271 ;
+sumcube_ = -4275569232895.28174 ;
+sumfourth_ = 68368946618486024 ;
+min_ = -16621.9823521287944 ;
+max_ = 0.406345448453045877 ;
+agmemin_ = 35 ;
+agemax_ = 45 ;
+first_ = -462.916765664800039 ;
+last_ = -374.853621299200142 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -50.2688274199133431 ;
+sumsquare_ = 38224.7447939705817 ;
+sumcube_ = -3656520.3377262922 ;
+sumfourth_ = 551819990.898841023 ;
+min_ = -181.33475999999996 ;
+max_ = -0.0896371428571569595 ;
+agmemin_ = 35 ;
+agemax_ = 45 ;
+first_ = -29.4603600000000014 ;
+last_ = -26.4172800000000052 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 8640.51363742984176 ;
+sumsquare_ = 1374774.3118040429 ;
+sumcube_ = 323112262.385960281 ;
+sumfourth_ = 93349681763.0307617 ;
+min_ = 0 ;
+max_ = 434.308100803600041 ;
+agmemin_ = 120 ;
+agemax_ = 102 ;
+first_ = 3.3058115703008113 ;
+last_ = 39.3579314881000286 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -17281.0272748596835 ;
+sumsquare_ = 5499097.24721617159 ;
+sumcube_ = -2584898099.08768225 ;
+sumfourth_ = 1493594908208.49219 ;
+min_ = -867.616201607200082 ;
+max_ = 1 ;
+agmemin_ = 102 ;
+agemax_ = 120 ;
+first_ = -5.61162314060162259 ;
+last_ = -77.7158629762000572 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -1392.4175074365653 ;
+sumsquare_ = 24435.3412616714304 ;
+sumcube_ = -524149.893896332418 ;
+sumfourth_ = 13079901.8943236303 ;
+min_ = -40.6801200000000023 ;
+max_ = 1 ;
+agmemin_ = 102 ;
+agemax_ = 120 ;
+first_ = -2.63637818181817352 ;
+last_ = -11.5471800000000044 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,8 @@
+E[test1.E[mse]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[mse]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[mse]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[mse]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 179.545344601616335 ;
+sumsquare_ = 58456.8760552968452 ;
+sumcube_ = 2844509.12695805216 ;
+sumfourth_ = 247849777.418584049 ;
+min_ = 0 ;
+max_ = 128.059475553344555 ;
+agmemin_ = 120 ;
+agemax_ = 16 ;
+first_ = 13.8781581156000211 ;
+last_ = 9.08582363289999861 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -359.090689203232671 ;
+sumsquare_ = 233827.504221187381 ;
+sumcube_ = -22756073.0156644173 ;
+sumfourth_ = 3965596438.69734478 ;
+min_ = -255.11895110668911 ;
+max_ = 1 ;
+agmemin_ = 16 ;
+agemax_ = 120 ;
+first_ = -26.7563162312000422 ;
+last_ = -17.1716472657999972 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 192.890804746032558 ;
+sumsquare_ = 3592.51670061680807 ;
+sumcube_ = 2141.61368714041009 ;
+sumfourth_ = 201415.693085262843 ;
+min_ = -21.6326733333333436 ;
+max_ = 1 ;
+agmemin_ = 16 ;
+agemax_ = 120 ;
+first_ = -6.45068000000000552 ;
+last_ = -5.02853999999999957 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 10974.0895327640446 ;
+sumsquare_ = 76330936.5701396614 ;
+sumcube_ = 569842056881.908203 ;
+sumfourth_ = 4522877790823688 ;
+min_ = 0.650915482711108107 ;
+max_ = 8311.49117606439722 ;
+agmemin_ = 17 ;
+agemax_ = 35 ;
+first_ = 143.301489139599965 ;
+last_ = 138.08415022370167 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -21948.1790655280893 ;
+sumsquare_ = 305323746.280558646 ;
+sumcube_ = -4558736455055.26562 ;
+sumfourth_ = 72366044653179008 ;
+min_ = -16621.9823521287944 ;
+max_ = -0.301830965422216213 ;
+agmemin_ = 35 ;
+agemax_ = 17 ;
+first_ = -285.602978279199931 ;
+last_ = -275.16830044740334 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -61.8945591190477131 ;
+sumsquare_ = 40932.6337231527941 ;
+sumcube_ = -4578700.6447631754 ;
+sumfourth_ = 688955792.048339963 ;
+min_ = -181.33475999999996 ;
+max_ = -0.61358666666666295 ;
+agmemin_ = 35 ;
+agemax_ = 17 ;
+first_ = -22.9417199999999966 ;
+last_ = -22.5018425000000093 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 179.545344601616335 ;
+sumsquare_ = 58456.8760552968452 ;
+sumcube_ = 2844509.12695805216 ;
+sumfourth_ = 247849777.418584049 ;
+min_ = 0 ;
+max_ = 128.059475553344555 ;
+agmemin_ = 120 ;
+agemax_ = 16 ;
+first_ = 13.8781581156000211 ;
+last_ = 9.08582363289999861 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -359.090689203232671 ;
+sumsquare_ = 233827.504221187381 ;
+sumcube_ = -22756073.0156644173 ;
+sumfourth_ = 3965596438.69734478 ;
+min_ = -255.11895110668911 ;
+max_ = 1 ;
+agmemin_ = 16 ;
+agemax_ = 120 ;
+first_ = -26.7563162312000422 ;
+last_ = -17.1716472657999972 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 192.890804746032558 ;
+sumsquare_ = 3592.51670061680807 ;
+sumcube_ = 2141.61368714041009 ;
+sumfourth_ = 201415.693085262843 ;
+min_ = -21.6326733333333436 ;
+max_ = 1 ;
+agmemin_ = 16 ;
+agemax_ = 120 ;
+first_ = -6.45068000000000552 ;
+last_ = -5.02853999999999957 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,8 @@
+E[test1.E[mse]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[mse]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[mse]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[mse]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,11 @@
+_trial_	0
+_objective_	0
+nstages	0
+E[test1.E[mse]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[mse]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,4 +1,45 @@
-*1 ->RegressionTree(
+*1 ->HyperLearner(
+tester = *2 ->PTester(
+expdir = "PYTEST__PL_RegressionTree__RESULTS:expdir/Split0/LearnerExpdir/Strat0/Trials2/" ;
+dataset = *3 ->SubVMatrix(
+parent = *4 ->FileVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat" ;
+remove_when_done = -1 ;
+track_ref = -1 ;
+writable = 0 ;
+length = 200 ;
+width = 6 ;
+inputsize = 6 ;
+targetsize = 0 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat.metadata/"  )
+;
+istart = 0 ;
+jstart = 0 ;
+fistart = -1 ;
+flength = -1 ;
+source = *4  ;
+writable = 0 ;
+length = 200 ;
+width = 6 ;
+inputsize = 4 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+splitter = *5 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 0.75 )	(0 , 0.75 )	(0.75 , 1 )	
+]
+ )
+;
+statnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *6 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 50 ;
@@ -6,11 +47,11 @@
 complexity_penalty_factor = 0 ;
 multiclass_outputs = []
 ;
-leave_template = *2 ->RegressionTreeLeave(
+leave_template = *7 ->RegressionTreeLeave(
 id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
+verbosity = 2 ;
 train_set = *0 ;
 length = 0 ;
 weights_sum = 0 ;
@@ -23,3164 +64,31 @@
 error = []
  )
 ;
-sorted_train_set = *3 ->RegressionTreeRegisters(
+sorted_train_set = *8 ->RegressionTreeRegisters(
 report_progress = 1 ;
-verbosity = 0 ;
-tsource = *4 ->MemoryVMatrixNoSave(
-source = *5 ->TransposeVMatrix(
-source = *6 ->MemoryVMatrix(
-data = 3132  9  [ 
-2 	0.469999999999999973 	0.364999999999999991 	0.135000000000000009 	0.52200000000000002 	0.239499999999999991 	0.152499999999999997 	0.14499999999999999 	10 	
-2 	0.57999999999999996 	0.46000000000000002 	0.165000000000000008 	1.22750000000000004 	0.472999999999999976 	0.196500000000000008 	0.434999999999999998 	16 	
-1 	0.445000000000000007 	0.359999999999999987 	0.110000000000000001 	0.423499999999999988 	0.181999999999999995 	0.0764999999999999986 	0.140000000000000013 	9 	
-1 	0.284999999999999976 	0.209999999999999992 	0.0550000000000000003 	0.101000000000000006 	0.0415000000000000022 	0.0170000000000000012 	0.033500000000000002 	5 	
-1 	0.424999999999999989 	0.315000000000000002 	0.0950000000000000011 	0.367499999999999993 	0.186499999999999999 	0.0675000000000000044 	0.0985000000000000042 	7 	
-2 	0.260000000000000009 	0.200000000000000011 	0.0650000000000000022 	0.096000000000000002 	0.0439999999999999974 	0.0269999999999999997 	0.0299999999999999989 	6 	
-0 	0.46000000000000002 	0.349999999999999978 	0.115000000000000005 	0.46000000000000002 	0.202500000000000013 	0.111500000000000002 	0.116500000000000006 	6 	
-0 	0.57999999999999996 	0.455000000000000016 	0.165000000000000008 	1.13650000000000007 	0.368999999999999995 	0.300499999999999989 	0.275000000000000022 	13 	
-2 	0.665000000000000036 	0.5 	0.174999999999999989 	1.2975000000000001 	0.60750000000000004 	0.314000000000000001 	0.315000000000000002 	9 	
-0 	0.635000000000000009 	0.5 	0.179999999999999993 	1.31200000000000006 	0.529000000000000026 	0.248499999999999999 	0.484999999999999987 	18 	
-1 	0.520000000000000018 	0.390000000000000013 	0.130000000000000004 	0.554499999999999993 	0.235499999999999987 	0.1095 	0.189500000000000002 	7 	
-2 	0.320000000000000007 	0.239999999999999991 	0.0800000000000000017 	0.179999999999999993 	0.0800000000000000017 	0.0384999999999999995 	0.0550000000000000003 	6 	
-2 	0.424999999999999989 	0.315000000000000002 	0.125 	0.35249999999999998 	0.113500000000000004 	0.0565000000000000016 	0.130000000000000004 	18 	
-2 	0.594999999999999973 	0.450000000000000011 	0.14499999999999999 	0.958999999999999964 	0.463000000000000023 	0.206499999999999989 	0.253500000000000003 	10 	
-2 	0.474999999999999978 	0.354999999999999982 	0.119999999999999996 	0.479999999999999982 	0.234000000000000014 	0.101500000000000007 	0.135000000000000009 	8 	
-0 	0.445000000000000007 	0.33500000000000002 	0.110000000000000001 	0.435499999999999998 	0.202500000000000013 	0.1095 	0.119499999999999995 	6 	
-1 	0.385000000000000009 	0.294999999999999984 	0.0850000000000000061 	0.253500000000000003 	0.102999999999999994 	0.0575000000000000025 	0.0850000000000000061 	7 	
-2 	0.555000000000000049 	0.440000000000000002 	0.149999999999999994 	1.09200000000000008 	0.415999999999999981 	0.211999999999999994 	0.440500000000000003 	15 	
-1 	0.424999999999999989 	0.309999999999999998 	0.0899999999999999967 	0.30099999999999999 	0.138500000000000012 	0.0650000000000000022 	0.0800000000000000017 	7 	
-1 	0.560000000000000053 	0.429999999999999993 	0.130000000000000004 	0.72799999999999998 	0.33550000000000002 	0.143499999999999989 	0.217499999999999999 	8 	
-1 	0.209999999999999992 	0.149999999999999994 	0.0500000000000000028 	0.0420000000000000026 	0.0175000000000000017 	0.0125000000000000007 	0.0149999999999999994 	4 	
-2 	0.604999999999999982 	0.469999999999999973 	0.179999999999999993 	1.14050000000000007 	0.3755 	0.280500000000000027 	0.385000000000000009 	15 	
-1 	0.275000000000000022 	0.200000000000000011 	0.0550000000000000003 	0.0924999999999999989 	0.0379999999999999991 	0.0210000000000000013 	0.0259999999999999988 	4 	
-2 	0.619999999999999996 	0.474999999999999978 	0.195000000000000007 	1.35850000000000004 	0.593500000000000028 	0.336500000000000021 	0.3745 	10 	
-2 	0.535000000000000031 	0.419999999999999984 	0.125 	0.737999999999999989 	0.354999999999999982 	0.189500000000000002 	0.179499999999999993 	8 	
-0 	0.609999999999999987 	0.494999999999999996 	0.184999999999999998 	1.15300000000000002 	0.536000000000000032 	0.29049999999999998 	0.244999999999999996 	8 	
-1 	0.429999999999999993 	0.315000000000000002 	0.0950000000000000011 	0.378000000000000003 	0.174999999999999989 	0.0800000000000000017 	0.104499999999999996 	8 	
-2 	0.469999999999999973 	0.369999999999999996 	0.135000000000000009 	0.547000000000000042 	0.222000000000000003 	0.132500000000000007 	0.170000000000000012 	12 	
-0 	0.535000000000000031 	0.41499999999999998 	0.170000000000000012 	0.879000000000000004 	0.294999999999999984 	0.196500000000000008 	0.284999999999999976 	10 	
-2 	0.640000000000000013 	0.484999999999999987 	0.149999999999999994 	1.09800000000000009 	0.519499999999999962 	0.222000000000000003 	0.317500000000000004 	10 	
-1 	0.5 	0.390000000000000013 	0.125 	0.582999999999999963 	0.293999999999999984 	0.132000000000000006 	0.160500000000000004 	8 	
-0 	0.655000000000000027 	0.515000000000000013 	0.200000000000000011 	1.49399999999999999 	0.725500000000000034 	0.308999999999999997 	0.405000000000000027 	12 	
-1 	0.479999999999999982 	0.369999999999999996 	0.125 	0.543499999999999983 	0.243999999999999995 	0.101000000000000006 	0.165000000000000008 	9 	
-2 	0.625 	0.489999999999999991 	0.184999999999999998 	1.16900000000000004 	0.527499999999999969 	0.253500000000000003 	0.343999999999999972 	11 	
-1 	0.354999999999999982 	0.270000000000000018 	0.0749999999999999972 	0.203999999999999987 	0.304499999999999993 	0.0459999999999999992 	0.0594999999999999973 	7 	
-2 	0.604999999999999982 	0.469999999999999973 	0.160000000000000003 	1.17349999999999999 	0.497499999999999998 	0.240499999999999992 	0.344999999999999973 	12 	
-2 	0.584999999999999964 	0.469999999999999973 	0.165000000000000008 	1.40900000000000003 	0.800000000000000044 	0.229000000000000009 	0.294999999999999984 	10 	
-1 	0.419999999999999984 	0.320000000000000007 	0.115000000000000005 	0.376000000000000001 	0.169000000000000011 	0.0919999999999999984 	0.100000000000000006 	5 	
-1 	0.349999999999999978 	0.260000000000000009 	0.0749999999999999972 	0.179999999999999993 	0.0899999999999999967 	0.0245000000000000009 	0.0550000000000000003 	5 	
-0 	0.440000000000000002 	0.340000000000000024 	0.104999999999999996 	0.36399999999999999 	0.147999999999999993 	0.0805000000000000021 	0.117499999999999993 	8 	
-0 	0.434999999999999998 	0.325000000000000011 	0.115000000000000005 	0.391500000000000015 	0.153999999999999998 	0.0940000000000000002 	0.119999999999999996 	7 	
-2 	0.594999999999999973 	0.469999999999999973 	0.165000000000000008 	1.1080000000000001 	0.491499999999999992 	0.232500000000000012 	0.33450000000000002 	9 	
-2 	0.67000000000000004 	0.510000000000000009 	0.174999999999999989 	1.52649999999999997 	0.651000000000000023 	0.447500000000000009 	0.344999999999999973 	10 	
-1 	0.625 	0.469999999999999973 	0.154999999999999999 	1.19550000000000001 	0.643000000000000016 	0.205499999999999988 	0.314500000000000002 	12 	
-0 	0.630000000000000004 	0.484999999999999987 	0.170000000000000012 	1.32050000000000001 	0.594500000000000028 	0.344999999999999973 	0.344999999999999973 	9 	
-2 	0.589999999999999969 	0.474999999999999978 	0.160000000000000003 	0.945500000000000007 	0.381500000000000006 	0.183999999999999997 	0.270000000000000018 	19 	
-1 	0.604999999999999982 	0.434999999999999998 	0.130000000000000004 	0.902499999999999969 	0.431999999999999995 	0.173999999999999988 	0.260000000000000009 	11 	
-2 	0.594999999999999973 	0.455000000000000016 	0.195000000000000007 	1.33050000000000002 	0.45950000000000002 	0.32350000000000001 	0.344999999999999973 	19 	
-2 	0.309999999999999998 	0.225000000000000006 	0.0800000000000000017 	0.134500000000000008 	0.0539999999999999994 	0.0240000000000000005 	0.0500000000000000028 	7 	
-0 	0.520000000000000018 	0.400000000000000022 	0.130000000000000004 	0.624500000000000055 	0.214999999999999997 	0.206499999999999989 	0.170000000000000012 	15 	
-1 	0.200000000000000011 	0.14499999999999999 	0.0500000000000000028 	0.0359999999999999973 	0.0125000000000000007 	0.00800000000000000017 	0.0109999999999999994 	4 	
-1 	0.330000000000000016 	0.265000000000000013 	0.0850000000000000061 	0.196000000000000008 	0.0774999999999999994 	0.0304999999999999993 	0.0444999999999999979 	6 	
-2 	0.645000000000000018 	0.505000000000000004 	0.149999999999999994 	1.16050000000000009 	0.519000000000000017 	0.26150000000000001 	0.33500000000000002 	10 	
-2 	0.560000000000000053 	0.429999999999999993 	0.14499999999999999 	0.899499999999999966 	0.464000000000000024 	0.177499999999999991 	0.234000000000000014 	9 	
-2 	0.560000000000000053 	0.450000000000000011 	0.174999999999999989 	1.0109999999999999 	0.383500000000000008 	0.206499999999999989 	0.369999999999999996 	15 	
-0 	0.540000000000000036 	0.419999999999999984 	0.130000000000000004 	0.750499999999999945 	0.367999999999999994 	0.16750000000000001 	0.184499999999999997 	9 	
-0 	0.520000000000000018 	0.405000000000000027 	0.115000000000000005 	0.776000000000000023 	0.320000000000000007 	0.184499999999999997 	0.220000000000000001 	8 	
-2 	0.630000000000000004 	0.489999999999999991 	0.179999999999999993 	1.12999999999999989 	0.458000000000000018 	0.276500000000000024 	0.315000000000000002 	12 	
-2 	0.614999999999999991 	0.479999999999999982 	0.174999999999999989 	1.1180000000000001 	0.446000000000000008 	0.319500000000000006 	0.299999999999999989 	9 	
-1 	0.494999999999999996 	0.369999999999999996 	0.125 	0.471499999999999975 	0.20749999999999999 	0.0909999999999999976 	0.149999999999999994 	8 	
-1 	0.409999999999999976 	0.304999999999999993 	0.0899999999999999967 	0.353499999999999981 	0.157000000000000001 	0.0744999999999999968 	0.100000000000000006 	7 	
-1 	0.395000000000000018 	0.28999999999999998 	0.0950000000000000011 	0.303999999999999992 	0.127000000000000002 	0.0840000000000000052 	0.076999999999999999 	6 	
-1 	0.469999999999999973 	0.364999999999999991 	0.100000000000000006 	0.410999999999999976 	0.174999999999999989 	0.0855000000000000066 	0.135000000000000009 	8 	
-1 	0.369999999999999996 	0.270000000000000018 	0.0950000000000000011 	0.217499999999999999 	0.0970000000000000029 	0.0459999999999999992 	0.0650000000000000022 	6 	
-2 	0.320000000000000007 	0.244999999999999996 	0.0749999999999999972 	0.155499999999999999 	0.0585000000000000034 	0.0379999999999999991 	0.0490000000000000019 	11 	
-2 	0.630000000000000004 	0.479999999999999982 	0.165000000000000008 	1.28600000000000003 	0.603999999999999981 	0.271000000000000019 	0.349999999999999978 	8 	
-2 	0.625 	0.515000000000000013 	0.170000000000000012 	1.33099999999999996 	0.572500000000000009 	0.300499999999999989 	0.360999999999999988 	9 	
-2 	0.349999999999999978 	0.275000000000000022 	0.110000000000000001 	0.292499999999999982 	0.122499999999999998 	0.0635000000000000009 	0.0904999999999999971 	8 	
-2 	0.359999999999999987 	0.294999999999999984 	0.104999999999999996 	0.240999999999999992 	0.0864999999999999936 	0.0529999999999999985 	0.0950000000000000011 	8 	
-0 	0.434999999999999998 	0.325000000000000011 	0.110000000000000001 	0.433499999999999996 	0.177999999999999992 	0.0985000000000000042 	0.154999999999999999 	7 	
-2 	0.685000000000000053 	0.505000000000000004 	0.190000000000000002 	1.53299999999999992 	0.667000000000000037 	0.405500000000000027 	0.409999999999999976 	10 	
-0 	0.655000000000000027 	0.510000000000000009 	0.174999999999999989 	1.41500000000000004 	0.588500000000000023 	0.372499999999999998 	0.36399999999999999 	10 	
-0 	0.645000000000000018 	0.489999999999999991 	0.214999999999999997 	1.40599999999999992 	0.42649999999999999 	0.228500000000000009 	0.510000000000000009 	25 	
-2 	0.625 	0.5 	0.130000000000000004 	1.08200000000000007 	0.578500000000000014 	0.204499999999999987 	0.25 	8 	
-2 	0.709999999999999964 	0.540000000000000036 	0.165000000000000008 	1.95900000000000007 	0.766499999999999959 	0.26100000000000001 	0.780000000000000027 	18 	
-0 	0.75 	0.569999999999999951 	0.209999999999999992 	2.23600000000000021 	1.10899999999999999 	0.519499999999999962 	0.54500000000000004 	11 	
-0 	0.655000000000000027 	0.510000000000000009 	0.149999999999999994 	1.04299999999999993 	0.479499999999999982 	0.223000000000000004 	0.304999999999999993 	9 	
-1 	0.520000000000000018 	0.380000000000000004 	0.125 	0.554499999999999993 	0.287999999999999978 	0.129500000000000004 	0.16700000000000001 	8 	
-1 	0.434999999999999998 	0.33500000000000002 	0.100000000000000006 	0.324500000000000011 	0.135000000000000009 	0.0785000000000000003 	0.0980000000000000038 	7 	
-2 	0.655000000000000027 	0.510000000000000009 	0.214999999999999997 	1.78350000000000009 	0.888499999999999956 	0.409499999999999975 	0.419499999999999984 	11 	
-1 	0.465000000000000024 	0.354999999999999982 	0.110000000000000001 	0.473999999999999977 	0.23000000000000001 	0.100500000000000006 	0.119999999999999996 	7 	
-0 	0.614999999999999991 	0.474999999999999978 	0.174999999999999989 	1.19399999999999995 	0.559000000000000052 	0.259000000000000008 	0.316500000000000004 	11 	
-2 	0.550000000000000044 	0.41499999999999998 	0.174999999999999989 	1.04200000000000004 	0.329500000000000015 	0.232500000000000012 	0.29049999999999998 	15 	
-0 	0.550000000000000044 	0.380000000000000004 	0.165000000000000008 	1.20500000000000007 	0.543000000000000038 	0.293999999999999984 	0.33450000000000002 	10 	
-1 	0.424999999999999989 	0.325000000000000011 	0.110000000000000001 	0.333500000000000019 	0.172999999999999987 	0.0449999999999999983 	0.100000000000000006 	7 	
-1 	0.560000000000000053 	0.440000000000000002 	0.165000000000000008 	0.800000000000000044 	0.33500000000000002 	0.173499999999999988 	0.25 	12 	
-1 	0.515000000000000013 	0.419999999999999984 	0.149999999999999994 	0.672499999999999987 	0.255500000000000005 	0.133500000000000008 	0.234999999999999987 	10 	
-0 	0.619999999999999996 	0.5 	0.174999999999999989 	1.18599999999999994 	0.498499999999999999 	0.30149999999999999 	0.349999999999999978 	12 	
-2 	0.354999999999999982 	0.265000000000000013 	0.0899999999999999967 	0.16800000000000001 	0.0500000000000000028 	0.0410000000000000017 	0.0630000000000000004 	8 	
-0 	0.635000000000000009 	0.474999999999999978 	0.149999999999999994 	1.18450000000000011 	0.533000000000000029 	0.306999999999999995 	0.290999999999999981 	10 	
-1 	0.275000000000000022 	0.214999999999999997 	0.0749999999999999972 	0.115500000000000005 	0.0485000000000000014 	0.0290000000000000015 	0.0350000000000000033 	7 	
-1 	0.369999999999999996 	0.28999999999999998 	0.0800000000000000017 	0.254500000000000004 	0.107999999999999999 	0.0565000000000000016 	0.0700000000000000067 	6 	
-1 	0.630000000000000004 	0.474999999999999978 	0.14499999999999999 	1.0605 	0.516499999999999959 	0.219500000000000001 	0.280000000000000027 	10 	
-1 	0.405000000000000027 	0.299999999999999989 	0.110000000000000001 	0.320000000000000007 	0.171999999999999986 	0.0439999999999999974 	0.0929999999999999993 	7 	
-1 	0.330000000000000016 	0.255000000000000004 	0.0800000000000000017 	0.204999999999999988 	0.0894999999999999962 	0.0395000000000000004 	0.0550000000000000003 	7 	
-1 	0.46000000000000002 	0.349999999999999978 	0.110000000000000001 	0.400000000000000022 	0.17599999999999999 	0.0830000000000000043 	0.120499999999999996 	7 	
-1 	0.359999999999999987 	0.270000000000000018 	0.0850000000000000061 	0.196000000000000008 	0.0904999999999999971 	0.0340000000000000024 	0.0529999999999999985 	7 	
-2 	0.484999999999999987 	0.400000000000000022 	0.135000000000000009 	0.663000000000000034 	0.313 	0.137000000000000011 	0.200000000000000011 	10 	
-0 	0.5 	0.400000000000000022 	0.125 	0.667499999999999982 	0.26100000000000001 	0.131500000000000006 	0.220000000000000001 	10 	
-0 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.805000000000000049 	0.368999999999999995 	0.172499999999999987 	0.209999999999999992 	11 	
-1 	0.530000000000000027 	0.400000000000000022 	0.125 	0.616999999999999993 	0.279000000000000026 	0.127000000000000002 	0.190000000000000002 	8 	
-1 	0.405000000000000027 	0.299999999999999989 	0.0899999999999999967 	0.269000000000000017 	0.102999999999999994 	0.067000000000000004 	0.110000000000000001 	6 	
-2 	0.594999999999999973 	0.469999999999999973 	0.149999999999999994 	0.891499999999999959 	0.358999999999999986 	0.210499999999999993 	0.244999999999999996 	12 	
-2 	0.589999999999999969 	0.484999999999999987 	0.119999999999999996 	0.911000000000000032 	0.390000000000000013 	0.181999999999999995 	0.28999999999999998 	16 	
-0 	0.5 	0.440000000000000002 	0.154999999999999999 	0.741999999999999993 	0.202500000000000013 	0.200500000000000012 	0.211499999999999994 	14 	
-2 	0.550000000000000044 	0.429999999999999993 	0.160000000000000003 	0.929499999999999993 	0.317000000000000004 	0.173499999999999988 	0.354999999999999982 	13 	
-2 	0.660000000000000031 	0.530000000000000027 	0.170000000000000012 	1.39050000000000007 	0.590500000000000025 	0.211999999999999994 	0.453000000000000014 	15 	
-1 	0.280000000000000027 	0.204999999999999988 	0.0800000000000000017 	0.127000000000000002 	0.0519999999999999976 	0.0389999999999999999 	0.0420000000000000026 	9 	
-1 	0.5 	0.385000000000000009 	0.154999999999999999 	0.762000000000000011 	0.379500000000000004 	0.161000000000000004 	0.190000000000000002 	14 	
-0 	0.625 	0.515000000000000013 	0.179999999999999993 	1.34850000000000003 	0.525499999999999967 	0.252000000000000002 	0.392500000000000016 	14 	
-1 	0.484999999999999987 	0.380000000000000004 	0.140000000000000013 	0.673000000000000043 	0.217499999999999999 	0.130000000000000004 	0.195000000000000007 	18 	
-0 	0.550000000000000044 	0.429999999999999993 	0.125 	0.923000000000000043 	0.403500000000000025 	0.174999999999999989 	0.282999999999999974 	8 	
-2 	0.555000000000000049 	0.440000000000000002 	0.135000000000000009 	0.902499999999999969 	0.380500000000000005 	0.210499999999999993 	0.280000000000000027 	13 	
-0 	0.689999999999999947 	0.540000000000000036 	0.195000000000000007 	1.25249999999999995 	0.729999999999999982 	0.39750000000000002 	0.462000000000000022 	12 	
-1 	0.294999999999999984 	0.214999999999999997 	0.0850000000000000061 	0.128000000000000003 	0.0490000000000000019 	0.0340000000000000024 	0.0400000000000000008 	6 	
-2 	0.599999999999999978 	0.469999999999999973 	0.165000000000000008 	1.05899999999999994 	0.504000000000000004 	0.240999999999999992 	0.275000000000000022 	9 	
-2 	0.5 	0.409999999999999976 	0.149999999999999994 	0.662000000000000033 	0.281499999999999972 	0.137000000000000011 	0.220000000000000001 	11 	
-1 	0.535000000000000031 	0.385000000000000009 	0.179999999999999993 	1.08349999999999991 	0.495499999999999996 	0.22950000000000001 	0.303999999999999992 	8 	
-0 	0.540000000000000036 	0.474999999999999978 	0.154999999999999999 	1.21700000000000008 	0.530499999999999972 	0.307499999999999996 	0.340000000000000024 	16 	
-2 	0.635000000000000009 	0.525000000000000022 	0.160000000000000003 	1.19500000000000006 	0.543499999999999983 	0.245999999999999996 	0.33500000000000002 	12 	
-0 	0.535000000000000031 	0.400000000000000022 	0.135000000000000009 	0.821500000000000008 	0.393500000000000016 	0.196000000000000008 	0.204999999999999988 	8 	
-2 	0.5 	0.41499999999999998 	0.165000000000000008 	0.688500000000000001 	0.248999999999999999 	0.138000000000000012 	0.25 	13 	
-2 	0.465000000000000024 	0.359999999999999987 	0.115000000000000005 	0.579500000000000015 	0.294999999999999984 	0.139500000000000013 	0.119999999999999996 	7 	
-1 	0.234999999999999987 	0.170000000000000012 	0.0650000000000000022 	0.0625 	0.0229999999999999996 	0.0140000000000000003 	0.0219999999999999987 	6 	
-0 	0.5 	0.385000000000000009 	0.115000000000000005 	0.678499999999999992 	0.294499999999999984 	0.138000000000000012 	0.195000000000000007 	12 	
-0 	0.599999999999999978 	0.5 	0.154999999999999999 	1.33200000000000007 	0.623500000000000054 	0.283499999999999974 	0.349999999999999978 	8 	
-1 	0.555000000000000049 	0.450000000000000011 	0.174999999999999989 	0.737999999999999989 	0.303999999999999992 	0.175499999999999989 	0.220000000000000001 	9 	
-1 	0.41499999999999998 	0.330000000000000016 	0.0899999999999999967 	0.359499999999999986 	0.170000000000000012 	0.0810000000000000026 	0.0899999999999999967 	6 	
-0 	0.525000000000000022 	0.429999999999999993 	0.135000000000000009 	0.843500000000000028 	0.432499999999999996 	0.179999999999999993 	0.181499999999999995 	9 	
-1 	0.309999999999999998 	0.239999999999999991 	0.104999999999999996 	0.288499999999999979 	0.117999999999999994 	0.0650000000000000022 	0.0830000000000000043 	6 	
-0 	0.675000000000000044 	0.510000000000000009 	0.195000000000000007 	1.3819999999999999 	0.604500000000000037 	0.317500000000000004 	0.396500000000000019 	10 	
-2 	0.685000000000000053 	0.54500000000000004 	0.204999999999999988 	1.79249999999999998 	0.814500000000000002 	0.415999999999999981 	0.461000000000000021 	9 	
-1 	0.330000000000000016 	0.265000000000000013 	0.0899999999999999967 	0.179999999999999993 	0.0680000000000000049 	0.0359999999999999973 	0.0599999999999999978 	6 	
-0 	0.494999999999999996 	0.380000000000000004 	0.119999999999999996 	0.572999999999999954 	0.265500000000000014 	0.128500000000000003 	0.143999999999999989 	7 	
-1 	0.315000000000000002 	0.23000000000000001 	0.0700000000000000067 	0.114500000000000005 	0.0459999999999999992 	0.0235000000000000001 	0.0384999999999999995 	5 	
-2 	0.765000000000000013 	0.599999999999999978 	0.220000000000000001 	2.30200000000000005 	1.0069999999999999 	0.509000000000000008 	0.620500000000000052 	12 	
-2 	0.699999999999999956 	0.564999999999999947 	0.179999999999999993 	1.75099999999999989 	0.895000000000000018 	0.33550000000000002 	0.446000000000000008 	9 	
-0 	0.594999999999999973 	0.494999999999999996 	0.234999999999999987 	1.3660000000000001 	0.50649999999999995 	0.219 	0.520000000000000018 	13 	
-2 	0.609999999999999987 	0.469999999999999973 	0.165000000000000008 	1.05200000000000005 	0.497999999999999998 	0.241999999999999993 	0.267000000000000015 	9 	
-0 	0.594999999999999973 	0.474999999999999978 	0.170000000000000012 	1.24700000000000011 	0.479999999999999982 	0.225000000000000006 	0.424999999999999989 	20 	
-1 	0.405000000000000027 	0.299999999999999989 	0.0899999999999999967 	0.288499999999999979 	0.138000000000000012 	0.0635000000000000009 	0.0764999999999999986 	6 	
-2 	0.625 	0.494999999999999996 	0.154999999999999999 	1.04849999999999999 	0.486999999999999988 	0.211999999999999994 	0.321500000000000008 	11 	
-0 	0.635000000000000009 	0.510000000000000009 	0.184999999999999998 	1.28600000000000003 	0.526000000000000023 	0.294999999999999984 	0.410499999999999976 	12 	
-2 	0.719999999999999973 	0.550000000000000044 	0.204999999999999988 	2.125 	1.14549999999999996 	0.442500000000000004 	0.51100000000000001 	13 	
-1 	0.46000000000000002 	0.359999999999999987 	0.100000000000000006 	0.463500000000000023 	0.232500000000000012 	0.0929999999999999993 	0.115000000000000005 	7 	
-2 	0.694999999999999951 	0.550000000000000044 	0.220000000000000001 	1.5515000000000001 	0.565999999999999948 	0.383500000000000008 	0.445000000000000007 	13 	
-0 	0.440000000000000002 	0.349999999999999978 	0.125 	0.403500000000000025 	0.174999999999999989 	0.0630000000000000004 	0.129000000000000004 	9 	
-2 	0.569999999999999951 	0.479999999999999982 	0.174999999999999989 	1.18500000000000005 	0.473999999999999977 	0.26100000000000001 	0.380000000000000004 	11 	
-1 	0.340000000000000024 	0.255000000000000004 	0.0749999999999999972 	0.179999999999999993 	0.0744999999999999968 	0.0400000000000000008 	0.0524999999999999981 	6 	
-2 	0.604999999999999982 	0.489999999999999991 	0.140000000000000013 	0.975500000000000034 	0.418999999999999984 	0.205999999999999989 	0.315000000000000002 	10 	
-1 	0.505000000000000004 	0.400000000000000022 	0.14499999999999999 	0.704500000000000015 	0.334000000000000019 	0.142499999999999988 	0.20699999999999999 	8 	
-1 	0.484999999999999987 	0.390000000000000013 	0.125 	0.59099999999999997 	0.286999999999999977 	0.140999999999999986 	0.119999999999999996 	9 	
-2 	0.400000000000000022 	0.304999999999999993 	0.0850000000000000061 	0.296999999999999986 	0.107999999999999999 	0.0704999999999999932 	0.100000000000000006 	10 	
-2 	0.630000000000000004 	0.510000000000000009 	0.23000000000000001 	1.53899999999999992 	0.563500000000000001 	0.281499999999999972 	0.569999999999999951 	17 	
-1 	0.340000000000000024 	0.25 	0.0749999999999999972 	0.178499999999999992 	0.0665000000000000036 	0.0454999999999999988 	0.0449999999999999983 	5 	
-0 	0.680000000000000049 	0.550000000000000044 	0.200000000000000011 	1.59600000000000009 	0.525000000000000022 	0.407499999999999973 	0.584999999999999964 	21 	
-2 	0.635000000000000009 	0.5 	0.179999999999999993 	1.15399999999999991 	0.440500000000000003 	0.231500000000000011 	0.387000000000000011 	9 	
-2 	0.739999999999999991 	0.569999999999999951 	0.179999999999999993 	1.87250000000000005 	0.911499999999999977 	0.426999999999999991 	0.446000000000000008 	10 	
-1 	0.469999999999999973 	0.354999999999999982 	0.119999999999999996 	0.368499999999999994 	0.126000000000000001 	0.0835000000000000048 	0.13650000000000001 	6 	
-0 	0.640000000000000013 	0.484999999999999987 	0.184999999999999998 	1.41949999999999998 	0.673499999999999988 	0.346499999999999975 	0.325500000000000012 	11 	
-2 	0.650000000000000022 	0.525000000000000022 	0.184999999999999998 	1.62200000000000011 	0.66449999999999998 	0.322500000000000009 	0.47699999999999998 	10 	
-0 	0.465000000000000024 	0.349999999999999978 	0.135000000000000009 	0.626499999999999946 	0.259000000000000008 	0.14449999999999999 	0.174999999999999989 	8 	
-1 	0.375 	0.275000000000000022 	0.100000000000000006 	0.232500000000000012 	0.116500000000000006 	0.0420000000000000026 	0.0650000000000000022 	6 	
-0 	0.574999999999999956 	0.434999999999999998 	0.154999999999999999 	0.897499999999999964 	0.411499999999999977 	0.232500000000000012 	0.23000000000000001 	9 	
-0 	0.599999999999999978 	0.465000000000000024 	0.165000000000000008 	0.887499999999999956 	0.308999999999999997 	0.245999999999999996 	0.262000000000000011 	12 	
-0 	0.660000000000000031 	0.474999999999999978 	0.179999999999999993 	1.36949999999999994 	0.641000000000000014 	0.293999999999999984 	0.33500000000000002 	6 	
-2 	0.540000000000000036 	0.41499999999999998 	0.170000000000000012 	0.879000000000000004 	0.339000000000000024 	0.20799999999999999 	0.255000000000000004 	10 	
-2 	0.67000000000000004 	0.550000000000000044 	0.170000000000000012 	1.24700000000000011 	0.471999999999999975 	0.245499999999999996 	0.400000000000000022 	21 	
-2 	0.609999999999999987 	0.469999999999999973 	0.170000000000000012 	1.11850000000000005 	0.522499999999999964 	0.240499999999999992 	0.309999999999999998 	9 	
-2 	0.599999999999999978 	0.494999999999999996 	0.165000000000000008 	1.24150000000000005 	0.484999999999999987 	0.277500000000000024 	0.340000000000000024 	15 	
-1 	0.560000000000000053 	0.424999999999999989 	0.14499999999999999 	0.687999999999999945 	0.309499999999999997 	0.130500000000000005 	0.216499999999999998 	9 	
-0 	0.569999999999999951 	0.465000000000000024 	0.179999999999999993 	0.999500000000000055 	0.405000000000000027 	0.277000000000000024 	0.294999999999999984 	16 	
-0 	0.569999999999999951 	0.440000000000000002 	0.125 	0.864999999999999991 	0.367499999999999993 	0.172499999999999987 	0.270000000000000018 	12 	
-1 	0.33500000000000002 	0.25 	0.0800000000000000017 	0.182999999999999996 	0.0734999999999999959 	0.0400000000000000008 	0.0575000000000000025 	6 	
-0 	0.54500000000000004 	0.445000000000000007 	0.149999999999999994 	0.800000000000000044 	0.353499999999999981 	0.163000000000000006 	0.20699999999999999 	9 	
-1 	0.434999999999999998 	0.325000000000000011 	0.119999999999999996 	0.345999999999999974 	0.159000000000000002 	0.0840000000000000052 	0.0950000000000000011 	7 	
-2 	0.505000000000000004 	0.405000000000000027 	0.110000000000000001 	0.625 	0.304999999999999993 	0.160000000000000003 	0.174999999999999989 	9 	
-2 	0.645000000000000018 	0.494999999999999996 	0.149999999999999994 	1.20950000000000002 	0.60299999999999998 	0.222500000000000003 	0.339000000000000024 	9 	
-2 	0.599999999999999978 	0.46000000000000002 	0.154999999999999999 	0.95950000000000002 	0.445500000000000007 	0.189000000000000001 	0.294999999999999984 	11 	
-1 	0.204999999999999988 	0.140000000000000013 	0.0500000000000000028 	0.0459999999999999992 	0.0165000000000000008 	0.0120000000000000002 	0.0134999999999999998 	6 	
-0 	0.599999999999999978 	0.465000000000000024 	0.149999999999999994 	1.10250000000000004 	0.545499999999999985 	0.262000000000000011 	0.25 	8 	
-1 	0.450000000000000011 	0.340000000000000024 	0.119999999999999996 	0.492499999999999993 	0.240999999999999992 	0.107499999999999998 	0.119999999999999996 	6 	
-1 	0.440000000000000002 	0.354999999999999982 	0.165000000000000008 	0.434999999999999998 	0.159000000000000002 	0.104999999999999996 	0.140000000000000013 	16 	
-1 	0.465000000000000024 	0.344999999999999973 	0.110000000000000001 	0.393000000000000016 	0.182499999999999996 	0.0734999999999999959 	0.119999999999999996 	8 	
-1 	0.275000000000000022 	0.195000000000000007 	0.0700000000000000067 	0.0874999999999999944 	0.0345000000000000029 	0.0219999999999999987 	0.0254999999999999984 	4 	
-0 	0.660000000000000031 	0.5 	0.165000000000000008 	1.19049999999999989 	0.458500000000000019 	0.297999999999999987 	0.369999999999999996 	12 	
-0 	0.574999999999999956 	0.434999999999999998 	0.149999999999999994 	1.03049999999999997 	0.46050000000000002 	0.217999999999999999 	0.359999999999999987 	8 	
-1 	0.424999999999999989 	0.320000000000000007 	0.100000000000000006 	0.305499999999999994 	0.126000000000000001 	0.0599999999999999978 	0.105999999999999997 	7 	
-1 	0.520000000000000018 	0.409999999999999976 	0.119999999999999996 	0.594999999999999973 	0.23849999999999999 	0.111000000000000001 	0.190000000000000002 	8 	
-0 	0.70499999999999996 	0.535000000000000031 	0.220000000000000001 	1.8660000000000001 	0.929000000000000048 	0.383500000000000008 	0.439500000000000002 	10 	
-2 	0.525000000000000022 	0.41499999999999998 	0.14499999999999999 	0.844999999999999973 	0.35249999999999998 	0.163500000000000006 	0.287499999999999978 	8 	
-1 	0.494999999999999996 	0.400000000000000022 	0.104999999999999996 	0.60199999999999998 	0.2505 	0.126500000000000001 	0.190000000000000002 	8 	
-2 	0.650000000000000022 	0.510000000000000009 	0.154999999999999999 	1.40700000000000003 	0.72150000000000003 	0.297999999999999987 	0.33500000000000002 	9 	
-2 	0.609999999999999987 	0.479999999999999982 	0.184999999999999998 	1.30649999999999999 	0.689500000000000002 	0.291499999999999981 	0.28999999999999998 	10 	
-2 	0.419999999999999984 	0.340000000000000024 	0.115000000000000005 	0.421499999999999986 	0.174999999999999989 	0.0929999999999999993 	0.135000000000000009 	8 	
-2 	0.729999999999999982 	0.574999999999999956 	0.209999999999999992 	2.06899999999999995 	0.928499999999999992 	0.408999999999999975 	0.643000000000000016 	11 	
-2 	0.584999999999999964 	0.455000000000000016 	0.149999999999999994 	0.906000000000000028 	0.409499999999999975 	0.23000000000000001 	0.233500000000000013 	8 	
-0 	0.505000000000000004 	0.385000000000000009 	0.115000000000000005 	0.615999999999999992 	0.242999999999999994 	0.107499999999999998 	0.209999999999999992 	11 	
-2 	0.599999999999999978 	0.494999999999999996 	0.174999999999999989 	1.30049999999999999 	0.619500000000000051 	0.283999999999999975 	0.328500000000000014 	11 	
-2 	0.375 	0.28999999999999998 	0.100000000000000006 	0.276000000000000023 	0.117499999999999993 	0.0565000000000000016 	0.0850000000000000061 	9 	
-0 	0.520000000000000018 	0.405000000000000027 	0.140000000000000013 	0.691500000000000004 	0.276000000000000023 	0.137000000000000011 	0.214999999999999997 	11 	
-1 	0.419999999999999984 	0.304999999999999993 	0.110000000000000001 	0.280000000000000027 	0.0940000000000000002 	0.0785000000000000003 	0.0955000000000000016 	9 	
-1 	0.54500000000000004 	0.400000000000000022 	0.130000000000000004 	0.686000000000000054 	0.328500000000000014 	0.14549999999999999 	0.179999999999999993 	9 	
-2 	0.465000000000000024 	0.359999999999999987 	0.110000000000000001 	0.495499999999999996 	0.266500000000000015 	0.0850000000000000061 	0.120999999999999996 	7 	
-1 	0.349999999999999978 	0.25 	0.0700000000000000067 	0.160500000000000004 	0.0714999999999999941 	0.033500000000000002 	0.0459999999999999992 	6 	
-0 	0.655000000000000027 	0.515000000000000013 	0.154999999999999999 	1.30899999999999994 	0.524000000000000021 	0.345999999999999974 	0.385000000000000009 	11 	
-1 	0.530000000000000027 	0.419999999999999984 	0.154999999999999999 	0.810000000000000053 	0.472499999999999976 	0.111000000000000001 	0.192000000000000004 	10 	
-1 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.807000000000000051 	0.361499999999999988 	0.17599999999999999 	0.254000000000000004 	10 	
-1 	0.489999999999999991 	0.400000000000000022 	0.135000000000000009 	0.623999999999999999 	0.303499999999999992 	0.128500000000000003 	0.169000000000000011 	8 	
-1 	0.5 	0.375 	0.14499999999999999 	0.579500000000000015 	0.23899999999999999 	0.137500000000000011 	0.184999999999999998 	9 	
-2 	0.28999999999999998 	0.23000000000000001 	0.0749999999999999972 	0.116500000000000006 	0.0429999999999999966 	0.0254999999999999984 	0.0400000000000000008 	7 	
-1 	0.599999999999999978 	0.474999999999999978 	0.149999999999999994 	1.12000000000000011 	0.564999999999999947 	0.246499999999999997 	0.270000000000000018 	10 	
-1 	0.174999999999999989 	0.125 	0.0500000000000000028 	0.0235000000000000001 	0.00800000000000000017 	0.00350000000000000007 	0.00800000000000000017 	5 	
-2 	0.655000000000000027 	0.530000000000000027 	0.195000000000000007 	1.3879999999999999 	0.566999999999999948 	0.273500000000000021 	0.409999999999999976 	13 	
-2 	0.645000000000000018 	0.5 	0.160000000000000003 	1.38149999999999995 	0.672000000000000042 	0.326000000000000012 	0.315000000000000002 	9 	
-2 	0.609999999999999987 	0.484999999999999987 	0.170000000000000012 	1.28099999999999992 	0.596999999999999975 	0.303499999999999992 	0.330000000000000016 	9 	
-2 	0.660000000000000031 	0.535000000000000031 	0.200000000000000011 	1.79099999999999993 	0.732999999999999985 	0.318000000000000005 	0.540000000000000036 	15 	
-2 	0.67000000000000004 	0.520000000000000018 	0.190000000000000002 	1.63850000000000007 	0.811499999999999999 	0.368999999999999995 	0.391000000000000014 	9 	
-2 	0.724999999999999978 	0.505000000000000004 	0.184999999999999998 	1.97799999999999998 	1.02600000000000002 	0.425499999999999989 	0.450500000000000012 	12 	
-1 	0.560000000000000053 	0.445000000000000007 	0.154999999999999999 	0.873500000000000054 	0.300499999999999989 	0.208999999999999991 	0.275000000000000022 	16 	
-1 	0.469999999999999973 	0.359999999999999987 	0.130000000000000004 	0.522499999999999964 	0.198000000000000009 	0.106499999999999997 	0.165000000000000008 	9 	
-2 	0.424999999999999989 	0.330000000000000016 	0.130000000000000004 	0.440500000000000003 	0.151999999999999996 	0.0934999999999999998 	0.154999999999999999 	9 	
-2 	0.574999999999999956 	0.445000000000000007 	0.170000000000000012 	1.02249999999999996 	0.549000000000000044 	0.217499999999999999 	0.228000000000000008 	9 	
-2 	0.440000000000000002 	0.325000000000000011 	0.0800000000000000017 	0.412999999999999978 	0.143999999999999989 	0.101500000000000007 	0.130000000000000004 	8 	
-0 	0.689999999999999947 	0.530000000000000027 	0.170000000000000012 	1.5535000000000001 	0.794499999999999984 	0.348499999999999976 	0.369499999999999995 	9 	
-2 	0.699999999999999956 	0.57999999999999996 	0.204999999999999988 	2.12999999999999989 	0.741500000000000048 	0.489999999999999991 	0.57999999999999996 	20 	
-2 	0.57999999999999996 	0.455000000000000016 	0.149999999999999994 	1.01200000000000001 	0.498499999999999999 	0.211499999999999994 	0.283499999999999974 	10 	
-2 	0.614999999999999991 	0.484999999999999987 	0.214999999999999997 	0.961500000000000021 	0.421999999999999986 	0.17599999999999999 	0.28999999999999998 	11 	
-0 	0.660000000000000031 	0.515000000000000013 	0.170000000000000012 	1.33699999999999997 	0.614999999999999991 	0.3125 	0.357499999999999984 	10 	
-1 	0.469999999999999973 	0.354999999999999982 	0.125 	0.498999999999999999 	0.209999999999999992 	0.0985000000000000042 	0.154999999999999999 	8 	
-0 	0.530000000000000027 	0.409999999999999976 	0.130000000000000004 	0.696500000000000008 	0.301999999999999991 	0.193500000000000005 	0.200000000000000011 	10 	
-2 	0.680000000000000049 	0.540000000000000036 	0.154999999999999999 	1.53400000000000003 	0.671000000000000041 	0.379000000000000004 	0.384000000000000008 	10 	
-1 	0.409999999999999976 	0.309999999999999998 	0.0899999999999999967 	0.333500000000000019 	0.163500000000000006 	0.0609999999999999987 	0.0909999999999999976 	6 	
-0 	0.650000000000000022 	0.564999999999999947 	0.200000000000000011 	1.66450000000000009 	0.753000000000000003 	0.366999999999999993 	0.429999999999999993 	12 	
-2 	0.574999999999999956 	0.450000000000000011 	0.165000000000000008 	0.965500000000000025 	0.497999999999999998 	0.190000000000000002 	0.23000000000000001 	8 	
-0 	0.699999999999999956 	0.584999999999999964 	0.184999999999999998 	1.80750000000000011 	0.705500000000000016 	0.321500000000000008 	0.474999999999999978 	29 	
-1 	0.265000000000000013 	0.195000000000000007 	0.0550000000000000003 	0.0840000000000000052 	0.0364999999999999977 	0.0175000000000000017 	0.0250000000000000014 	7 	
-2 	0.429999999999999993 	0.33500000000000002 	0.119999999999999996 	0.39700000000000002 	0.19850000000000001 	0.0864999999999999936 	0.103499999999999995 	7 	
-2 	0.609999999999999987 	0.484999999999999987 	0.160000000000000003 	1.01449999999999996 	0.531499999999999972 	0.211999999999999994 	0.241499999999999992 	8 	
-2 	0.525000000000000022 	0.400000000000000022 	0.140000000000000013 	0.720500000000000029 	0.368499999999999994 	0.14499999999999999 	0.173499999999999988 	8 	
-0 	0.489999999999999991 	0.364999999999999991 	0.130000000000000004 	0.683499999999999996 	0.165000000000000008 	0.131500000000000006 	0.204999999999999988 	21 	
-2 	0.609999999999999987 	0.474999999999999978 	0.154999999999999999 	1.16799999999999993 	0.554000000000000048 	0.23899999999999999 	0.329500000000000015 	10 	
-2 	0.57999999999999996 	0.440000000000000002 	0.174999999999999989 	1.22550000000000003 	0.54049999999999998 	0.270500000000000018 	0.326500000000000012 	10 	
-1 	0.239999999999999991 	0.170000000000000012 	0.0500000000000000028 	0.0544999999999999998 	0.0205000000000000009 	0.0160000000000000003 	0.0154999999999999999 	5 	
-0 	0.479999999999999982 	0.400000000000000022 	0.125 	0.759000000000000008 	0.212499999999999994 	0.178999999999999992 	0.239999999999999991 	15 	
-2 	0.535000000000000031 	0.405000000000000027 	0.184999999999999998 	0.83450000000000002 	0.317500000000000004 	0.172499999999999987 	0.28999999999999998 	16 	
-0 	0.724999999999999978 	0.599999999999999978 	0.200000000000000011 	1.7370000000000001 	0.696999999999999953 	0.358499999999999985 	0.594999999999999973 	11 	
-2 	0.494999999999999996 	0.395000000000000018 	0.119999999999999996 	0.553000000000000047 	0.224000000000000005 	0.137500000000000011 	0.16700000000000001 	8 	
-2 	0.405000000000000027 	0.304999999999999993 	0.119999999999999996 	0.318500000000000005 	0.123499999999999999 	0.0904999999999999971 	0.0950000000000000011 	7 	
-2 	0.28999999999999998 	0.225000000000000006 	0.0800000000000000017 	0.129500000000000004 	0.0534999999999999989 	0.0259999999999999988 	0.0449999999999999983 	10 	
-2 	0.594999999999999973 	0.46000000000000002 	0.140000000000000013 	0.85199999999999998 	0.421499999999999986 	0.225500000000000006 	0.227000000000000007 	9 	
-1 	0.450000000000000011 	0.330000000000000016 	0.100000000000000006 	0.410999999999999976 	0.194500000000000006 	0.100000000000000006 	0.0980000000000000038 	6 	
-2 	0.520000000000000018 	0.385000000000000009 	0.115000000000000005 	0.669000000000000039 	0.23849999999999999 	0.171999999999999986 	0.204999999999999988 	12 	
-1 	0.540000000000000036 	0.424999999999999989 	0.135000000000000009 	0.686000000000000054 	0.347499999999999976 	0.154499999999999998 	0.212999999999999995 	8 	
-0 	0.630000000000000004 	0.494999999999999996 	0.165000000000000008 	1.30750000000000011 	0.598999999999999977 	0.283999999999999975 	0.315000000000000002 	11 	
-2 	0.440000000000000002 	0.375 	0.130000000000000004 	0.486999999999999988 	0.226000000000000006 	0.0965000000000000024 	0.154999999999999999 	9 	
-2 	0.424999999999999989 	0.330000000000000016 	0.0800000000000000017 	0.360999999999999988 	0.134000000000000008 	0.0825000000000000039 	0.125 	7 	
-2 	0.309999999999999998 	0.244999999999999996 	0.0950000000000000011 	0.149999999999999994 	0.0524999999999999981 	0.0340000000000000024 	0.048000000000000001 	7 	
-2 	0.614999999999999991 	0.505000000000000004 	0.165000000000000008 	1.34000000000000008 	0.531499999999999972 	0.281499999999999972 	0.409999999999999976 	12 	
-2 	0.5 	0.405000000000000027 	0.140000000000000013 	0.615500000000000047 	0.240999999999999992 	0.135500000000000009 	0.204999999999999988 	9 	
-0 	0.614999999999999991 	0.474999999999999978 	0.154999999999999999 	1.004 	0.447500000000000009 	0.193000000000000005 	0.28949999999999998 	10 	
-1 	0.375 	0.275000000000000022 	0.0899999999999999967 	0.217999999999999999 	0.0929999999999999993 	0.0405000000000000013 	0.0754999999999999977 	6 	
-1 	0.340000000000000024 	0.244999999999999996 	0.0850000000000000061 	0.201500000000000012 	0.100500000000000006 	0.0379999999999999991 	0.0529999999999999985 	6 	
-0 	0.619999999999999996 	0.5 	0.170000000000000012 	1.14799999999999991 	0.547499999999999987 	0.220000000000000001 	0.331500000000000017 	10 	
-2 	0.599999999999999978 	0.479999999999999982 	0.0899999999999999967 	1.05000000000000004 	0.457000000000000017 	0.268500000000000016 	0.280000000000000027 	8 	
-1 	0.409999999999999976 	0.325000000000000011 	0.110000000000000001 	0.326000000000000012 	0.132500000000000007 	0.0749999999999999972 	0.101000000000000006 	8 	
-1 	0.275000000000000022 	0.200000000000000011 	0.0749999999999999972 	0.0859999999999999931 	0.0304999999999999993 	0.0189999999999999995 	0.0299999999999999989 	7 	
-0 	0.655000000000000027 	0.510000000000000009 	0.154999999999999999 	1.28950000000000009 	0.534499999999999975 	0.285499999999999976 	0.409999999999999976 	11 	
-1 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.645499999999999963 	0.325000000000000011 	0.1245 	0.170000000000000012 	8 	
-0 	0.680000000000000049 	0.560000000000000053 	0.165000000000000008 	1.63900000000000001 	0.605500000000000038 	0.280500000000000027 	0.46000000000000002 	15 	
-2 	0.625 	0.469999999999999973 	0.179999999999999993 	1.1359999999999999 	0.451000000000000012 	0.324500000000000011 	0.304999999999999993 	11 	
-0 	0.409999999999999976 	0.304999999999999993 	0.100000000000000006 	0.362999999999999989 	0.173499999999999988 	0.0650000000000000022 	0.110000000000000001 	11 	
-1 	0.474999999999999978 	0.354999999999999982 	0.115000000000000005 	0.519499999999999962 	0.279000000000000026 	0.0879999999999999949 	0.132500000000000007 	7 	
-2 	0.645000000000000018 	0.530000000000000027 	0.195000000000000007 	1.3899999999999999 	0.646499999999999964 	0.294499999999999984 	0.373499999999999999 	10 	
-1 	0.434999999999999998 	0.325000000000000011 	0.119999999999999996 	0.399500000000000022 	0.181499999999999995 	0.0609999999999999987 	0.112500000000000003 	8 	
-0 	0.429999999999999993 	0.340000000000000024 	0.110000000000000001 	0.382000000000000006 	0.153999999999999998 	0.0955000000000000016 	0.109 	8 	
-2 	0.505000000000000004 	0.385000000000000009 	0.149999999999999994 	0.641499999999999959 	0.245999999999999996 	0.151999999999999996 	0.214999999999999997 	12 	
-0 	0.419999999999999984 	0.330000000000000016 	0.125 	0.463000000000000023 	0.185999999999999999 	0.110000000000000001 	0.14499999999999999 	10 	
-1 	0.130000000000000004 	0.0950000000000000011 	0.0350000000000000033 	0.0105000000000000007 	0.0050000000000000001 	0.0064999999999999997 	0.00350000000000000007 	4 	
-1 	0.484999999999999987 	0.41499999999999998 	0.140000000000000013 	0.570500000000000007 	0.25 	0.134000000000000008 	0.184999999999999998 	8 	
-0 	0.530000000000000027 	0.424999999999999989 	0.130000000000000004 	0.758499999999999952 	0.325000000000000011 	0.197000000000000008 	0.204999999999999988 	8 	
-1 	0.380000000000000004 	0.275000000000000022 	0.110000000000000001 	0.256000000000000005 	0.110000000000000001 	0.0534999999999999989 	0.0754999999999999977 	6 	
-1 	0.315000000000000002 	0.244999999999999996 	0.0850000000000000061 	0.143499999999999989 	0.0529999999999999985 	0.0475000000000000006 	0.0500000000000000028 	8 	
-1 	0.28999999999999998 	0.209999999999999992 	0.0599999999999999978 	0.119499999999999995 	0.0560000000000000012 	0.0235000000000000001 	0.0299999999999999989 	6 	
-1 	0.550000000000000044 	0.424999999999999989 	0.14499999999999999 	0.890000000000000013 	0.432499999999999996 	0.171000000000000013 	0.235999999999999988 	10 	
-1 	0.340000000000000024 	0.260000000000000009 	0.0800000000000000017 	0.200000000000000011 	0.0800000000000000017 	0.0555000000000000007 	0.0550000000000000003 	6 	
-1 	0.359999999999999987 	0.280000000000000027 	0.0800000000000000017 	0.175499999999999989 	0.0810000000000000026 	0.0505000000000000032 	0.0700000000000000067 	6 	
-1 	0.619999999999999996 	0.450000000000000011 	0.135000000000000009 	0.924000000000000044 	0.357999999999999985 	0.226500000000000007 	0.296499999999999986 	10 	
-2 	0.400000000000000022 	0.28999999999999998 	0.115000000000000005 	0.279500000000000026 	0.111500000000000002 	0.0575000000000000025 	0.0749999999999999972 	9 	
-1 	0.390000000000000013 	0.280000000000000027 	0.0899999999999999967 	0.214999999999999997 	0.0845000000000000057 	0.0340000000000000024 	0.0790000000000000008 	8 	
-1 	0.244999999999999996 	0.204999999999999988 	0.0599999999999999978 	0.0764999999999999986 	0.0340000000000000024 	0.0140000000000000003 	0.0214999999999999983 	4 	
-2 	0.67000000000000004 	0.520000000000000018 	0.165000000000000008 	1.3899999999999999 	0.710999999999999965 	0.286499999999999977 	0.299999999999999989 	11 	
-2 	0.589999999999999969 	0.484999999999999987 	0.154999999999999999 	1.07850000000000001 	0.453500000000000014 	0.243499999999999994 	0.309999999999999998 	9 	
-0 	0.515000000000000013 	0.405000000000000027 	0.119999999999999996 	0.646000000000000019 	0.28949999999999998 	0.140500000000000014 	0.176999999999999991 	10 	
-1 	0.445000000000000007 	0.320000000000000007 	0.119999999999999996 	0.378000000000000003 	0.151999999999999996 	0.0825000000000000039 	0.119999999999999996 	8 	
-2 	0.650000000000000022 	0.510000000000000009 	0.160000000000000003 	1.38349999999999995 	0.638499999999999956 	0.29049999999999998 	0.366499999999999992 	9 	
-0 	0.645000000000000018 	0.5 	0.160000000000000003 	1.24649999999999994 	0.547499999999999987 	0.327000000000000013 	0.299999999999999989 	10 	
-1 	0.220000000000000001 	0.160000000000000003 	0.0500000000000000028 	0.0490000000000000019 	0.0214999999999999983 	0.0100000000000000002 	0.0149999999999999994 	4 	
-2 	0.625 	0.479999999999999982 	0.184999999999999998 	1.20649999999999991 	0.586999999999999966 	0.28999999999999998 	0.285999999999999976 	8 	
-2 	0.564999999999999947 	0.445000000000000007 	0.14499999999999999 	0.925499999999999989 	0.434499999999999997 	0.211999999999999994 	0.247499999999999998 	9 	
-2 	0.484999999999999987 	0.359999999999999987 	0.130000000000000004 	0.541499999999999981 	0.259500000000000008 	0.096000000000000002 	0.160000000000000003 	10 	
-1 	0.46000000000000002 	0.359999999999999987 	0.104999999999999996 	0.466000000000000025 	0.222500000000000003 	0.0990000000000000047 	0.110000000000000001 	7 	
-2 	0.57999999999999996 	0.429999999999999993 	0.125 	0.911499999999999977 	0.446000000000000008 	0.20749999999999999 	0.120999999999999996 	10 	
-1 	0.25 	0.179999999999999993 	0.0599999999999999978 	0.0729999999999999954 	0.0280000000000000006 	0.0170000000000000012 	0.0224999999999999992 	5 	
-0 	0.505000000000000004 	0.380000000000000004 	0.135000000000000009 	0.685499999999999998 	0.360999999999999988 	0.1565 	0.161000000000000004 	9 	
-1 	0.320000000000000007 	0.239999999999999991 	0.0700000000000000067 	0.133000000000000007 	0.0585000000000000034 	0.0254999999999999984 	0.0410000000000000017 	6 	
-2 	0.340000000000000024 	0.255000000000000004 	0.0950000000000000011 	0.212999999999999995 	0.0810000000000000026 	0.0340000000000000024 	0.0700000000000000067 	9 	
-0 	0.645000000000000018 	0.479999999999999982 	0.190000000000000002 	1.371 	0.692500000000000004 	0.29049999999999998 	0.349999999999999978 	12 	
-2 	0.604999999999999982 	0.474999999999999978 	0.154999999999999999 	1.16100000000000003 	0.571999999999999953 	0.245499999999999996 	0.275000000000000022 	9 	
-2 	0.465000000000000024 	0.359999999999999987 	0.0800000000000000017 	0.487999999999999989 	0.191000000000000003 	0.125 	0.154999999999999999 	11 	
-1 	0.474999999999999978 	0.359999999999999987 	0.110000000000000001 	0.455500000000000016 	0.176999999999999991 	0.0965000000000000024 	0.14499999999999999 	9 	
-1 	0.354999999999999982 	0.275000000000000022 	0.0899999999999999967 	0.251000000000000001 	0.0970000000000000029 	0.0529999999999999985 	0.0800000000000000017 	7 	
-2 	0.484999999999999987 	0.369999999999999996 	0.130000000000000004 	0.526000000000000023 	0.248499999999999999 	0.104999999999999996 	0.155499999999999999 	6 	
-0 	0.599999999999999978 	0.474999999999999978 	0.135000000000000009 	1.44049999999999989 	0.588500000000000023 	0.191000000000000003 	0.317500000000000004 	9 	
-1 	0.325000000000000011 	0.25 	0.0749999999999999972 	0.158500000000000002 	0.0749999999999999972 	0.0304999999999999993 	0.0454999999999999988 	6 	
-1 	0.584999999999999964 	0.46000000000000002 	0.14499999999999999 	0.84650000000000003 	0.339000000000000024 	0.16700000000000001 	0.294999999999999984 	10 	
-0 	0.569999999999999951 	0.450000000000000011 	0.165000000000000008 	0.903000000000000025 	0.330500000000000016 	0.184499999999999997 	0.294999999999999984 	14 	
-1 	0.280000000000000027 	0.119999999999999996 	0.0749999999999999972 	0.117000000000000007 	0.0454999999999999988 	0.0290000000000000015 	0.0345000000000000029 	4 	
-2 	0.645000000000000018 	0.5 	0.195000000000000007 	1.40100000000000002 	0.616500000000000048 	0.351499999999999979 	0.372499999999999998 	10 	
-0 	0.709999999999999964 	0.540000000000000036 	0.204999999999999988 	1.58050000000000002 	0.802000000000000046 	0.286999999999999977 	0.434999999999999998 	10 	
-0 	0.450000000000000011 	0.33500000000000002 	0.104999999999999996 	0.424999999999999989 	0.186499999999999999 	0.0909999999999999976 	0.115000000000000005 	9 	
-1 	0.23000000000000001 	0.165000000000000008 	0.0599999999999999978 	0.0514999999999999972 	0.0189999999999999995 	0.0145000000000000007 	0.0359999999999999973 	4 	
-0 	0.625 	0.520000000000000018 	0.179999999999999993 	1.35400000000000009 	0.484499999999999986 	0.350999999999999979 	0.375 	11 	
-2 	0.625 	0.489999999999999991 	0.119999999999999996 	0.876499999999999946 	0.456000000000000016 	0.179999999999999993 	0.233000000000000013 	10 	
-1 	0.165000000000000008 	0.115000000000000005 	0.0149999999999999994 	0.0145000000000000007 	0.00549999999999999968 	0.00300000000000000006 	0.0050000000000000001 	4 	
-1 	0.450000000000000011 	0.349999999999999978 	0.140000000000000013 	0.473999999999999977 	0.209999999999999992 	0.109 	0.127500000000000002 	16 	
-0 	0.625 	0.479999999999999982 	0.154999999999999999 	1.20350000000000001 	0.586500000000000021 	0.23899999999999999 	0.318500000000000005 	12 	
-0 	0.57999999999999996 	0.450000000000000011 	0.14499999999999999 	1.13700000000000001 	0.558499999999999996 	0.220000000000000001 	0.28999999999999998 	8 	
-0 	0.675000000000000044 	0.555000000000000049 	0.204999999999999988 	1.92500000000000004 	0.712999999999999967 	0.357999999999999985 	0.453500000000000014 	13 	
-1 	0.560000000000000053 	0.445000000000000007 	0.149999999999999994 	0.822500000000000009 	0.368499999999999994 	0.187 	0.235999999999999988 	10 	
-0 	0.589999999999999969 	0.465000000000000024 	0.149999999999999994 	0.996999999999999997 	0.392000000000000015 	0.245999999999999996 	0.340000000000000024 	12 	
-2 	0.765000000000000013 	0.584999999999999964 	0.179999999999999993 	2.39800000000000013 	1.12799999999999989 	0.512000000000000011 	0.533499999999999974 	12 	
-1 	0.46000000000000002 	0.344999999999999973 	0.104999999999999996 	0.41499999999999998 	0.187 	0.086999999999999994 	0.110000000000000001 	8 	
-0 	0.67000000000000004 	0.540000000000000036 	0.165000000000000008 	1.50150000000000006 	0.518000000000000016 	0.357999999999999985 	0.505000000000000004 	14 	
-2 	0.540000000000000036 	0.41499999999999998 	0.14499999999999999 	0.739999999999999991 	0.263500000000000012 	0.16800000000000001 	0.244999999999999996 	12 	
-2 	0.494999999999999996 	0.395000000000000018 	0.135000000000000009 	0.633499999999999952 	0.303499999999999992 	0.129500000000000004 	0.149499999999999994 	8 	
-2 	0.525000000000000022 	0.41499999999999998 	0.135000000000000009 	0.794499999999999984 	0.394000000000000017 	0.189000000000000001 	0.202000000000000013 	7 	
-2 	0.584999999999999964 	0.46000000000000002 	0.149999999999999994 	1.20599999999999996 	0.580999999999999961 	0.215999999999999998 	0.323000000000000009 	10 	
-2 	0.599999999999999978 	0.46000000000000002 	0.149999999999999994 	1.24700000000000011 	0.533499999999999974 	0.273500000000000021 	0.28999999999999998 	9 	
-2 	0.520000000000000018 	0.400000000000000022 	0.14499999999999999 	0.776499999999999968 	0.35249999999999998 	0.184499999999999997 	0.184999999999999998 	9 	
-1 	0.315000000000000002 	0.234999999999999987 	0.0749999999999999972 	0.128500000000000003 	0.0509999999999999967 	0.0280000000000000006 	0.0405000000000000013 	4 	
-0 	0.635000000000000009 	0.5 	0.190000000000000002 	1.29000000000000004 	0.592999999999999972 	0.304499999999999993 	0.35199999999999998 	8 	
-1 	0.280000000000000027 	0.209999999999999992 	0.0749999999999999972 	0.119499999999999995 	0.0529999999999999985 	0.0264999999999999993 	0.0299999999999999989 	6 	
-2 	0.550000000000000044 	0.405000000000000027 	0.140000000000000013 	0.802499999999999991 	0.243999999999999995 	0.163500000000000006 	0.255000000000000004 	10 	
-0 	0.645000000000000018 	0.510000000000000009 	0.200000000000000011 	1.56749999999999989 	0.620999999999999996 	0.366999999999999993 	0.46000000000000002 	12 	
-0 	0.609999999999999987 	0.484999999999999987 	0.179999999999999993 	1.27950000000000008 	0.57350000000000001 	0.285499999999999976 	0.354999999999999982 	7 	
-2 	0.67000000000000004 	0.525000000000000022 	0.165000000000000008 	1.60850000000000004 	0.682000000000000051 	0.314500000000000002 	0.400500000000000023 	11 	
-1 	0.584999999999999964 	0.419999999999999984 	0.14499999999999999 	0.673499999999999988 	0.28949999999999998 	0.134500000000000008 	0.220000000000000001 	9 	
-0 	0.599999999999999978 	0.474999999999999978 	0.160000000000000003 	1.02649999999999997 	0.484999999999999987 	0.2495 	0.256500000000000006 	9 	
-2 	0.630000000000000004 	0.515000000000000013 	0.154999999999999999 	1.2589999999999999 	0.410499999999999976 	0.197000000000000008 	0.409999999999999976 	13 	
-2 	0.494999999999999996 	0.390000000000000013 	0.149999999999999994 	0.85299999999999998 	0.328500000000000014 	0.189000000000000001 	0.270000000000000018 	14 	
-2 	0.474999999999999978 	0.375 	0.125 	0.592999999999999972 	0.277000000000000024 	0.115000000000000005 	0.179999999999999993 	10 	
-2 	0.594999999999999973 	0.469999999999999973 	0.14499999999999999 	0.990999999999999992 	0.403500000000000025 	0.150499999999999995 	0.340000000000000024 	16 	
-2 	0.515000000000000013 	0.405000000000000027 	0.140000000000000013 	0.850500000000000034 	0.312 	0.145999999999999991 	0.315000000000000002 	17 	
-0 	0.5 	0.405000000000000027 	0.149999999999999994 	0.59650000000000003 	0.253000000000000003 	0.126000000000000001 	0.184999999999999998 	12 	
-1 	0.455000000000000016 	0.330000000000000016 	0.100000000000000006 	0.371999999999999997 	0.357999999999999985 	0.0774999999999999994 	0.110000000000000001 	8 	
-0 	0.550000000000000044 	0.429999999999999993 	0.140000000000000013 	0.713500000000000023 	0.256500000000000006 	0.185999999999999999 	0.225000000000000006 	9 	
-2 	0.405000000000000027 	0.304999999999999993 	0.0850000000000000061 	0.260500000000000009 	0.114500000000000005 	0.0594999999999999973 	0.0850000000000000061 	8 	
-1 	0.340000000000000024 	0.25 	0.0899999999999999967 	0.178999999999999992 	0.0774999999999999994 	0.0330000000000000016 	0.0550000000000000003 	6 	
-0 	0.564999999999999947 	0.450000000000000011 	0.174999999999999989 	1.00950000000000006 	0.447000000000000008 	0.237499999999999989 	0.264500000000000013 	9 	
-2 	0.67000000000000004 	0.505000000000000004 	0.160000000000000003 	1.25849999999999995 	0.625499999999999945 	0.310999999999999999 	0.307999999999999996 	12 	
-2 	0.535000000000000031 	0.434999999999999998 	0.154999999999999999 	0.891499999999999959 	0.341500000000000026 	0.176999999999999991 	0.25 	13 	
-0 	0.369999999999999996 	0.28999999999999998 	0.115000000000000005 	0.25 	0.111000000000000001 	0.0570000000000000021 	0.0749999999999999972 	9 	
-2 	0.535000000000000031 	0.434999999999999998 	0.149999999999999994 	0.716999999999999971 	0.347499999999999976 	0.14449999999999999 	0.194000000000000006 	9 	
-1 	0.385000000000000009 	0.28999999999999998 	0.0899999999999999967 	0.232000000000000012 	0.0855000000000000066 	0.0495000000000000023 	0.0800000000000000017 	7 	
-0 	0.54500000000000004 	0.409999999999999976 	0.140000000000000013 	0.740500000000000047 	0.356499999999999984 	0.177499999999999991 	0.203000000000000014 	9 	
-0 	0.359999999999999987 	0.265000000000000013 	0.0899999999999999967 	0.216499999999999998 	0.096000000000000002 	0.0369999999999999982 	0.0734999999999999959 	10 	
-0 	0.680000000000000049 	0.550000000000000044 	0.174999999999999989 	1.79800000000000004 	0.814999999999999947 	0.392500000000000016 	0.455000000000000016 	19 	
-2 	0.569999999999999951 	0.479999999999999982 	0.179999999999999993 	0.939500000000000002 	0.399000000000000021 	0.200000000000000011 	0.294999999999999984 	14 	
-0 	0.609999999999999987 	0.484999999999999987 	0.170000000000000012 	1.10050000000000003 	0.512499999999999956 	0.229000000000000009 	0.304999999999999993 	11 	
-1 	0.385000000000000009 	0.280000000000000027 	0.0850000000000000061 	0.217499999999999999 	0.0970000000000000029 	0.0379999999999999991 	0.067000000000000004 	8 	
-0 	0.645000000000000018 	0.479999999999999982 	0.170000000000000012 	1.13450000000000006 	0.528000000000000025 	0.254000000000000004 	0.304999999999999993 	10 	
-0 	0.474999999999999978 	0.380000000000000004 	0.140000000000000013 	0.688999999999999946 	0.316500000000000004 	0.131500000000000006 	0.195500000000000007 	7 	
-1 	0.479999999999999982 	0.380000000000000004 	0.125 	0.624500000000000055 	0.339500000000000024 	0.108499999999999999 	0.166500000000000009 	8 	
-0 	0.474999999999999978 	0.359999999999999987 	0.119999999999999996 	0.591500000000000026 	0.324500000000000011 	0.110000000000000001 	0.127000000000000002 	6 	
-2 	0.555000000000000049 	0.434999999999999998 	0.14499999999999999 	0.968500000000000028 	0.498499999999999999 	0.16800000000000001 	0.23849999999999999 	9 	
-1 	0.46000000000000002 	0.364999999999999991 	0.115000000000000005 	0.51100000000000001 	0.236499999999999988 	0.117999999999999994 	0.122999999999999998 	7 	
-1 	0.405000000000000027 	0.304999999999999993 	0.100000000000000006 	0.271000000000000019 	0.0965000000000000024 	0.0609999999999999987 	0.0909999999999999976 	7 	
-0 	0.719999999999999973 	0.564999999999999947 	0.170000000000000012 	1.61299999999999999 	0.722999999999999976 	0.325500000000000012 	0.494499999999999995 	12 	
-0 	0.584999999999999964 	0.465000000000000024 	0.140000000000000013 	0.908000000000000029 	0.381000000000000005 	0.161500000000000005 	0.315000000000000002 	13 	
-0 	0.645000000000000018 	0.525000000000000022 	0.190000000000000002 	1.8085 	0.703500000000000014 	0.388500000000000012 	0.395000000000000018 	18 	
-1 	0.354999999999999982 	0.280000000000000027 	0.110000000000000001 	0.223500000000000004 	0.081500000000000003 	0.0524999999999999981 	0.0800000000000000017 	7 	
-1 	0.190000000000000002 	0.140000000000000013 	0.0299999999999999989 	0.0315000000000000002 	0.0125000000000000007 	0.0050000000000000001 	0.0105000000000000007 	3 	
-1 	0.190000000000000002 	0.14499999999999999 	0.0400000000000000008 	0.0379999999999999991 	0.0165000000000000008 	0.0064999999999999997 	0.0149999999999999994 	4 	
-2 	0.540000000000000036 	0.409999999999999976 	0.14499999999999999 	0.98899999999999999 	0.281499999999999972 	0.212999999999999995 	0.354999999999999982 	19 	
-1 	0.440000000000000002 	0.349999999999999978 	0.135000000000000009 	0.434999999999999998 	0.181499999999999995 	0.0830000000000000043 	0.125 	12 	
-0 	0.510000000000000009 	0.390000000000000013 	0.104999999999999996 	0.611999999999999988 	0.187 	0.149999999999999994 	0.195000000000000007 	13 	
-2 	0.41499999999999998 	0.325000000000000011 	0.140000000000000013 	0.416999999999999982 	0.153499999999999998 	0.101500000000000007 	0.143999999999999989 	10 	
-1 	0.375 	0.265000000000000013 	0.0950000000000000011 	0.196000000000000008 	0.0850000000000000061 	0.0420000000000000026 	0.0585000000000000034 	5 	
-0 	0.655000000000000027 	0.489999999999999991 	0.160000000000000003 	1.20399999999999996 	0.545499999999999985 	0.26150000000000001 	0.322500000000000009 	9 	
-2 	0.599999999999999978 	0.469999999999999973 	0.160000000000000003 	1.19399999999999995 	0.5625 	0.304499999999999993 	0.263500000000000012 	10 	
-2 	0.564999999999999947 	0.440000000000000002 	0.184999999999999998 	0.90900000000000003 	0.343999999999999972 	0.232500000000000012 	0.255000000000000004 	15 	
-0 	0.599999999999999978 	0.474999999999999978 	0.179999999999999993 	1.1805000000000001 	0.434499999999999997 	0.247499999999999998 	0.424999999999999989 	19 	
-2 	0.525000000000000022 	0.424999999999999989 	0.125 	0.812000000000000055 	0.403500000000000025 	0.170500000000000013 	0.195000000000000007 	8 	
-2 	0.619999999999999996 	0.484999999999999987 	0.154999999999999999 	1.02950000000000008 	0.424999999999999989 	0.231500000000000011 	0.33500000000000002 	12 	
-2 	0.574999999999999956 	0.46000000000000002 	0.154999999999999999 	0.892000000000000015 	0.441500000000000004 	0.17599999999999999 	0.220000000000000001 	10 	
-1 	0.474999999999999978 	0.364999999999999991 	0.115000000000000005 	0.459000000000000019 	0.217499999999999999 	0.0929999999999999993 	0.116500000000000006 	7 	
-2 	0.530000000000000027 	0.409999999999999976 	0.140000000000000013 	0.754499999999999948 	0.349499999999999977 	0.171500000000000014 	0.210499999999999993 	8 	
-2 	0.515000000000000013 	0.349999999999999978 	0.154999999999999999 	0.922499999999999987 	0.418499999999999983 	0.198000000000000009 	0.27300000000000002 	9 	
-2 	0.479999999999999982 	0.359999999999999987 	0.100000000000000006 	0.439000000000000001 	0.194000000000000006 	0.0990000000000000047 	0.115000000000000005 	8 	
-2 	0.525000000000000022 	0.395000000000000018 	0.130000000000000004 	0.763499999999999956 	0.337500000000000022 	0.142499999999999988 	0.225000000000000006 	8 	
-1 	0.550000000000000044 	0.424999999999999989 	0.149999999999999994 	0.639000000000000012 	0.269000000000000017 	0.134500000000000008 	0.216999999999999998 	9 	
-2 	0.614999999999999991 	0.474999999999999978 	0.170000000000000012 	1.129 	0.479499999999999982 	0.301999999999999991 	0.299999999999999989 	10 	
-1 	0.160000000000000003 	0.119999999999999996 	0.0350000000000000033 	0.0210000000000000013 	0.00749999999999999972 	0.00449999999999999966 	0.0050000000000000001 	5 	
-2 	0.455000000000000016 	0.354999999999999982 	0.130000000000000004 	0.515000000000000013 	0.200000000000000011 	0.127500000000000002 	0.174999999999999989 	11 	
-2 	0.369999999999999996 	0.280000000000000027 	0.0950000000000000011 	0.222500000000000003 	0.0805000000000000021 	0.0509999999999999967 	0.0749999999999999972 	7 	
-0 	0.70499999999999996 	0.54500000000000004 	0.170000000000000012 	1.58000000000000007 	0.643499999999999961 	0.456500000000000017 	0.265000000000000013 	11 	
-0 	0.614999999999999991 	0.469999999999999973 	0.154999999999999999 	1.08400000000000007 	0.588500000000000023 	0.208999999999999991 	0.245999999999999996 	9 	
-1 	0.400000000000000022 	0.309999999999999998 	0.100000000000000006 	0.127000000000000002 	0.105999999999999997 	0.0709999999999999937 	0.0850000000000000061 	7 	
-1 	0.560000000000000053 	0.434999999999999998 	0.130000000000000004 	0.777000000000000024 	0.353999999999999981 	0.172999999999999987 	0.222000000000000003 	9 	
-0 	0.510000000000000009 	0.400000000000000022 	0.119999999999999996 	0.700500000000000012 	0.346999999999999975 	0.110500000000000001 	0.195000000000000007 	10 	
-2 	0.640000000000000013 	0.510000000000000009 	0.174999999999999989 	1.3680000000000001 	0.515000000000000013 	0.266000000000000014 	0.569999999999999951 	21 	
-1 	0.380000000000000004 	0.275000000000000022 	0.0950000000000000011 	0.137500000000000011 	0.0859999999999999931 	0.0585000000000000034 	0.0604999999999999982 	7 	
-2 	0.424999999999999989 	0.349999999999999978 	0.104999999999999996 	0.393000000000000016 	0.130000000000000004 	0.0630000000000000004 	0.165000000000000008 	9 	
-1 	0.469999999999999973 	0.380000000000000004 	0.125 	0.484499999999999986 	0.210999999999999993 	0.107499999999999998 	0.141999999999999987 	6 	
-0 	0.5 	0.369999999999999996 	0.135000000000000009 	0.450000000000000011 	0.171500000000000014 	0.105499999999999997 	0.154999999999999999 	9 	
-2 	0.574999999999999956 	0.445000000000000007 	0.140000000000000013 	0.736999999999999988 	0.325000000000000011 	0.140500000000000014 	0.236999999999999988 	10 	
-0 	0.625 	0.494999999999999996 	0.165000000000000008 	1.26200000000000001 	0.507000000000000006 	0.318000000000000005 	0.390000000000000013 	10 	
-0 	0.550000000000000044 	0.469999999999999973 	0.149999999999999994 	0.920499999999999985 	0.381000000000000005 	0.243499999999999994 	0.267500000000000016 	10 	
-1 	0.574999999999999956 	0.450000000000000011 	0.130000000000000004 	0.814500000000000002 	0.403000000000000025 	0.171500000000000014 	0.212999999999999995 	10 	
-2 	0.699999999999999956 	0.540000000000000036 	0.204999999999999988 	1.73999999999999999 	0.788499999999999979 	0.372999999999999998 	0.486499999999999988 	13 	
-0 	0.574999999999999956 	0.469999999999999973 	0.154999999999999999 	1.1160000000000001 	0.509000000000000008 	0.237999999999999989 	0.340000000000000024 	10 	
-2 	0.614999999999999991 	0.469999999999999973 	0.160000000000000003 	1.01750000000000007 	0.472999999999999976 	0.239499999999999991 	0.280000000000000027 	10 	
-1 	0.330000000000000016 	0.255000000000000004 	0.0950000000000000011 	0.171999999999999986 	0.0660000000000000031 	0.0254999999999999984 	0.0599999999999999978 	6 	
-0 	0.484999999999999987 	0.369999999999999996 	0.115000000000000005 	0.478499999999999981 	0.199500000000000011 	0.0955000000000000016 	0.129000000000000004 	7 	
-2 	0.520000000000000018 	0.385000000000000009 	0.140000000000000013 	0.659499999999999975 	0.248499999999999999 	0.203499999999999986 	0.160000000000000003 	9 	
-2 	0.5 	0.380000000000000004 	0.154999999999999999 	0.595500000000000029 	0.213499999999999995 	0.161000000000000004 	0.200000000000000011 	12 	
-0 	0.640000000000000013 	0.540000000000000036 	0.174999999999999989 	1.22100000000000009 	0.510000000000000009 	0.259000000000000008 	0.390000000000000013 	15 	
-0 	0.614999999999999991 	0.494999999999999996 	0.160000000000000003 	1.25499999999999989 	0.581500000000000017 	0.319500000000000006 	0.322500000000000009 	12 	
-0 	0.599999999999999978 	0.469999999999999973 	0.190000000000000002 	1.13450000000000006 	0.491999999999999993 	0.259500000000000008 	0.337500000000000022 	10 	
-0 	0.494999999999999996 	0.400000000000000022 	0.154999999999999999 	0.644499999999999962 	0.241999999999999993 	0.132500000000000007 	0.204999999999999988 	17 	
-2 	0.54500000000000004 	0.419999999999999984 	0.119999999999999996 	0.786499999999999977 	0.403000000000000025 	0.184999999999999998 	0.170000000000000012 	7 	
-2 	0.619999999999999996 	0.469999999999999973 	0.14499999999999999 	1.08650000000000002 	0.51100000000000001 	0.271500000000000019 	0.256500000000000006 	10 	
-2 	0.54500000000000004 	0.419999999999999984 	0.14499999999999999 	0.778000000000000025 	0.3745 	0.154499999999999998 	0.204999999999999988 	7 	
-0 	0.589999999999999969 	0.455000000000000016 	0.14499999999999999 	1.06299999999999994 	0.515499999999999958 	0.244499999999999995 	0.25 	8 	
-2 	0.619999999999999996 	0.465000000000000024 	0.190000000000000002 	1.34149999999999991 	0.570500000000000007 	0.317500000000000004 	0.354999999999999982 	11 	
-2 	0.665000000000000036 	0.515000000000000013 	0.200000000000000011 	1.26950000000000007 	0.511499999999999955 	0.267500000000000016 	0.435999999999999999 	12 	
-2 	0.635000000000000009 	0.5 	0.170000000000000012 	1.43450000000000011 	0.610999999999999988 	0.308999999999999997 	0.417999999999999983 	12 	
-1 	0.619999999999999996 	0.474999999999999978 	0.160000000000000003 	0.907000000000000028 	0.370999999999999996 	0.16700000000000001 	0.307499999999999996 	11 	
-2 	0.640000000000000013 	0.5 	0.184999999999999998 	1.3035000000000001 	0.444500000000000006 	0.263500000000000012 	0.465000000000000024 	16 	
-1 	0.525000000000000022 	0.375 	0.119999999999999996 	0.63149999999999995 	0.304499999999999993 	0.114000000000000004 	0.190000000000000002 	9 	
-0 	0.465000000000000024 	0.349999999999999978 	0.130000000000000004 	0.493999999999999995 	0.194500000000000006 	0.102999999999999994 	0.154999999999999999 	18 	
-2 	0.479999999999999982 	0.375 	0.119999999999999996 	0.589500000000000024 	0.253500000000000003 	0.128000000000000003 	0.171999999999999986 	11 	
-1 	0.609999999999999987 	0.489999999999999991 	0.160000000000000003 	1.15450000000000008 	0.586500000000000021 	0.23849999999999999 	0.291499999999999981 	11 	
-1 	0.234999999999999987 	0.160000000000000003 	0.0400000000000000008 	0.048000000000000001 	0.0184999999999999991 	0.0179999999999999986 	0.0149999999999999994 	5 	
-0 	0.630000000000000004 	0.484999999999999987 	0.184999999999999998 	1.16700000000000004 	0.548000000000000043 	0.248499999999999999 	0.340000000000000024 	10 	
-0 	0.640000000000000013 	0.5 	0.170000000000000012 	1.51750000000000007 	0.692999999999999949 	0.326000000000000012 	0.408999999999999975 	11 	
-2 	0.530000000000000027 	0.434999999999999998 	0.160000000000000003 	0.883000000000000007 	0.316000000000000003 	0.164000000000000007 	0.33500000000000002 	15 	
-1 	0.400000000000000022 	0.320000000000000007 	0.0950000000000000011 	0.347999999999999976 	0.194000000000000006 	0.0529999999999999985 	0.086999999999999994 	6 	
-1 	0.584999999999999964 	0.450000000000000011 	0.135000000000000009 	0.854999999999999982 	0.379500000000000004 	0.187 	0.260000000000000009 	9 	
-1 	0.530000000000000027 	0.395000000000000018 	0.130000000000000004 	0.574999999999999956 	0.246999999999999997 	0.115000000000000005 	0.182999999999999996 	9 	
-1 	0.599999999999999978 	0.479999999999999982 	0.170000000000000012 	0.917499999999999982 	0.380000000000000004 	0.222500000000000003 	0.28999999999999998 	8 	
-1 	0.465000000000000024 	0.325000000000000011 	0.140000000000000013 	0.761499999999999955 	0.361999999999999988 	0.153499999999999998 	0.208999999999999991 	10 	
-1 	0.405000000000000027 	0.28999999999999998 	0.0899999999999999967 	0.282499999999999973 	0.112000000000000002 	0.0749999999999999972 	0.081500000000000003 	7 	
-0 	0.584999999999999964 	0.450000000000000011 	0.149999999999999994 	0.937999999999999945 	0.467000000000000026 	0.203000000000000014 	0.225000000000000006 	7 	
-0 	0.369999999999999996 	0.280000000000000027 	0.110000000000000001 	0.23050000000000001 	0.0945000000000000007 	0.0464999999999999997 	0.0749999999999999972 	10 	
-1 	0.434999999999999998 	0.33500000000000002 	0.104999999999999996 	0.353499999999999981 	0.156 	0.0500000000000000028 	0.113500000000000004 	7 	
-2 	0.619999999999999996 	0.489999999999999991 	0.149999999999999994 	1.19500000000000006 	0.46050000000000002 	0.301999999999999991 	0.354999999999999982 	9 	
-2 	0.685000000000000053 	0.520000000000000018 	0.165000000000000008 	1.51899999999999991 	0.698999999999999955 	0.368499999999999994 	0.400000000000000022 	10 	
-2 	0.57999999999999996 	0.455000000000000016 	0.130000000000000004 	0.85199999999999998 	0.409999999999999976 	0.172499999999999987 	0.225000000000000006 	8 	
-0 	0.714999999999999969 	0.564999999999999947 	0.239999999999999991 	2.19950000000000001 	0.724500000000000033 	0.465000000000000024 	0.885000000000000009 	17 	
-0 	0.655000000000000027 	0.540000000000000036 	0.214999999999999997 	1.5555000000000001 	0.694999999999999951 	0.295999999999999985 	0.444000000000000006 	11 	
-0 	0.5 	0.400000000000000022 	0.125 	0.576500000000000012 	0.239499999999999991 	0.126000000000000001 	0.184999999999999998 	10 	
-2 	0.560000000000000053 	0.450000000000000011 	0.160000000000000003 	0.922000000000000042 	0.431999999999999995 	0.177999999999999992 	0.260000000000000009 	15 	
-0 	0.455000000000000016 	0.364999999999999991 	0.115000000000000005 	0.430499999999999994 	0.183999999999999997 	0.107999999999999999 	0.1245 	8 	
-2 	0.609999999999999987 	0.479999999999999982 	0.140000000000000013 	1.03099999999999992 	0.4375 	0.26150000000000001 	0.270000000000000018 	8 	
-0 	0.400000000000000022 	0.320000000000000007 	0.110000000000000001 	0.35299999999999998 	0.140500000000000014 	0.0985000000000000042 	0.100000000000000006 	8 	
-2 	0.445000000000000007 	0.349999999999999978 	0.115000000000000005 	0.361499999999999988 	0.1565 	0.0695000000000000062 	0.117000000000000007 	8 	
-2 	0.569999999999999951 	0.455000000000000016 	0.149999999999999994 	0.951999999999999957 	0.389500000000000013 	0.215499999999999997 	0.274500000000000022 	9 	
-0 	0.719999999999999973 	0.550000000000000044 	0.200000000000000011 	1.99649999999999994 	0.90349999999999997 	0.468999999999999972 	0.521499999999999964 	10 	
-0 	0.469999999999999973 	0.354999999999999982 	0.179999999999999993 	0.441000000000000003 	0.152499999999999997 	0.116500000000000006 	0.135000000000000009 	8 	
-2 	0.564999999999999947 	0.445000000000000007 	0.149999999999999994 	0.796000000000000041 	0.36349999999999999 	0.183999999999999997 	0.219 	8 	
-2 	0.574999999999999956 	0.434999999999999998 	0.149999999999999994 	0.805000000000000049 	0.292999999999999983 	0.162500000000000006 	0.270000000000000018 	17 	
-2 	0.525000000000000022 	0.385000000000000009 	0.100000000000000006 	0.511499999999999955 	0.245999999999999996 	0.100500000000000006 	0.14549999999999999 	8 	
-0 	0.489999999999999991 	0.364999999999999991 	0.14499999999999999 	0.634499999999999953 	0.199500000000000011 	0.162500000000000006 	0.220000000000000001 	10 	
-2 	0.474999999999999978 	0.369999999999999996 	0.125 	0.537000000000000033 	0.222000000000000003 	0.121499999999999997 	0.149999999999999994 	9 	
-2 	0.645000000000000018 	0.510000000000000009 	0.195000000000000007 	1.22599999999999998 	0.588500000000000023 	0.221500000000000002 	0.3745 	10 	
-1 	0.33500000000000002 	0.244999999999999996 	0.0899999999999999967 	0.201500000000000012 	0.096000000000000002 	0.0405000000000000013 	0.048000000000000001 	7 	
-1 	0.574999999999999956 	0.434999999999999998 	0.130000000000000004 	0.805000000000000049 	0.315500000000000003 	0.215499999999999997 	0.244999999999999996 	10 	
-0 	0.489999999999999991 	0.354999999999999982 	0.160000000000000003 	0.879499999999999948 	0.348499999999999976 	0.214999999999999997 	0.282499999999999973 	8 	
-1 	0.375 	0.270000000000000018 	0.0850000000000000061 	0.217999999999999999 	0.0945000000000000007 	0.0389999999999999999 	0.0700000000000000067 	7 	
-2 	0.584999999999999964 	0.465000000000000024 	0.154999999999999999 	0.91449999999999998 	0.455500000000000016 	0.196500000000000008 	0.234999999999999987 	9 	
-2 	0.340000000000000024 	0.265000000000000013 	0.0850000000000000061 	0.183499999999999996 	0.076999999999999999 	0.0459999999999999992 	0.0650000000000000022 	10 	
-2 	0.574999999999999956 	0.434999999999999998 	0.135000000000000009 	0.991999999999999993 	0.431999999999999995 	0.222500000000000003 	0.23899999999999999 	10 	
-0 	0.630000000000000004 	0.5 	0.154999999999999999 	1.00499999999999989 	0.366999999999999993 	0.19900000000000001 	0.359999999999999987 	16 	
-0 	0.665000000000000036 	0.535000000000000031 	0.190000000000000002 	1.496 	0.577500000000000013 	0.281499999999999972 	0.474999999999999978 	17 	
-2 	0.640000000000000013 	0.530000000000000027 	0.165000000000000008 	1.1895 	0.476499999999999979 	0.299999999999999989 	0.349999999999999978 	11 	
-1 	0.41499999999999998 	0.309999999999999998 	0.0950000000000000011 	0.310999999999999999 	0.112500000000000003 	0.0625 	0.115000000000000005 	8 	
-0 	0.535000000000000031 	0.419999999999999984 	0.149999999999999994 	0.736500000000000044 	0.278500000000000025 	0.185999999999999999 	0.214999999999999997 	14 	
-1 	0.574999999999999956 	0.450000000000000011 	0.125 	0.780000000000000027 	0.327500000000000013 	0.188 	0.234999999999999987 	9 	
-0 	0.505000000000000004 	0.424999999999999989 	0.140000000000000013 	0.849999999999999978 	0.275000000000000022 	0.162500000000000006 	0.284999999999999976 	19 	
-1 	0.424999999999999989 	0.325000000000000011 	0.104999999999999996 	0.39750000000000002 	0.181499999999999995 	0.0810000000000000026 	0.117499999999999993 	7 	
-0 	0.564999999999999947 	0.455000000000000016 	0.149999999999999994 	0.820500000000000007 	0.364999999999999991 	0.159000000000000002 	0.260000000000000009 	18 	
-1 	0.455000000000000016 	0.325000000000000011 	0.135000000000000009 	0.819999999999999951 	0.400500000000000023 	0.171500000000000014 	0.210999999999999993 	8 	
-0 	0.599999999999999978 	0.474999999999999978 	0.179999999999999993 	1.16199999999999992 	0.51100000000000001 	0.267500000000000016 	0.320000000000000007 	18 	
-2 	0.465000000000000024 	0.354999999999999982 	0.125 	0.525499999999999967 	0.202500000000000013 	0.135000000000000009 	0.14499999999999999 	13 	
-2 	0.469999999999999973 	0.375 	0.119999999999999996 	0.580500000000000016 	0.266000000000000014 	0.0934999999999999998 	0.169000000000000011 	8 	
-1 	0.434999999999999998 	0.299999999999999989 	0.119999999999999996 	0.59650000000000003 	0.259000000000000008 	0.139000000000000012 	0.164500000000000007 	8 	
-2 	0.640000000000000013 	0.505000000000000004 	0.154999999999999999 	1.19550000000000001 	0.556499999999999995 	0.210999999999999993 	0.345999999999999974 	11 	
-0 	0.57999999999999996 	0.5 	0.165000000000000008 	0.925000000000000044 	0.369999999999999996 	0.184999999999999998 	0.300499999999999989 	10 	
-1 	0.465000000000000024 	0.369999999999999996 	0.110000000000000001 	0.445000000000000007 	0.163500000000000006 	0.096000000000000002 	0.166000000000000009 	7 	
-1 	0.510000000000000009 	0.400000000000000022 	0.125 	0.593500000000000028 	0.23899999999999999 	0.130000000000000004 	0.203999999999999987 	8 	
-2 	0.645000000000000018 	0.5 	0.174999999999999989 	1.33850000000000002 	0.633000000000000007 	0.298999999999999988 	0.348999999999999977 	11 	
-1 	0.525000000000000022 	0.380000000000000004 	0.135000000000000009 	0.614999999999999991 	0.26100000000000001 	0.159000000000000002 	0.174999999999999989 	8 	
-1 	0.409999999999999976 	0.325000000000000011 	0.100000000000000006 	0.324500000000000011 	0.132000000000000006 	0.0719999999999999946 	0.105999999999999997 	6 	
-2 	0.560000000000000053 	0.440000000000000002 	0.140000000000000013 	0.970999999999999974 	0.443000000000000005 	0.204499999999999987 	0.265000000000000013 	14 	
-2 	0.594999999999999973 	0.469999999999999973 	0.154999999999999999 	1.20150000000000001 	0.491999999999999993 	0.38650000000000001 	0.265000000000000013 	10 	
-1 	0.54500000000000004 	0.434999999999999998 	0.135000000000000009 	0.771499999999999964 	0.371999999999999997 	0.147999999999999993 	0.227000000000000007 	8 	
-1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.466500000000000026 	0.228500000000000009 	0.106499999999999997 	0.114000000000000004 	7 	
-0 	0.630000000000000004 	0.479999999999999982 	0.174999999999999989 	1.36749999999999994 	0.501499999999999946 	0.303499999999999992 	0.515000000000000013 	17 	
-2 	0.560000000000000053 	0.440000000000000002 	0.160000000000000003 	0.864500000000000046 	0.330500000000000016 	0.20749999999999999 	0.260000000000000009 	10 	
-0 	0.409999999999999976 	0.325000000000000011 	0.104999999999999996 	0.36349999999999999 	0.159000000000000002 	0.076999999999999999 	0.119999999999999996 	10 	
-0 	0.699999999999999956 	0.525000000000000022 	0.190000000000000002 	1.64650000000000007 	0.854500000000000037 	0.306999999999999995 	0.399500000000000022 	9 	
-0 	0.635000000000000009 	0.525000000000000022 	0.179999999999999993 	1.36949999999999994 	0.634000000000000008 	0.318000000000000005 	0.362999999999999989 	11 	
-2 	0.375 	0.284999999999999976 	0.0950000000000000011 	0.253000000000000003 	0.096000000000000002 	0.0575000000000000025 	0.0924999999999999989 	9 	
-0 	0.584999999999999964 	0.484999999999999987 	0.149999999999999994 	1.07899999999999996 	0.41449999999999998 	0.211499999999999994 	0.355999999999999983 	11 	
-0 	0.380000000000000004 	0.304999999999999993 	0.104999999999999996 	0.281000000000000028 	0.104499999999999996 	0.0614999999999999991 	0.0899999999999999967 	12 	
-0 	0.614999999999999991 	0.5 	0.174999999999999989 	1.377 	0.558499999999999996 	0.330000000000000016 	0.291999999999999982 	12 	
-2 	0.385000000000000009 	0.299999999999999989 	0.0950000000000000011 	0.239999999999999991 	0.0884999999999999953 	0.0589999999999999969 	0.0850000000000000061 	9 	
-0 	0.609999999999999987 	0.474999999999999978 	0.160000000000000003 	1.11549999999999994 	0.383500000000000008 	0.223000000000000004 	0.379000000000000004 	10 	
-1 	0.419999999999999984 	0.330000000000000016 	0.100000000000000006 	0.35199999999999998 	0.163500000000000006 	0.0889999999999999958 	0.100000000000000006 	9 	
-0 	0.520000000000000018 	0.46000000000000002 	0.149999999999999994 	1.01899999999999991 	0.52300000000000002 	0.19850000000000001 	0.254000000000000004 	7 	
-0 	0.390000000000000013 	0.299999999999999989 	0.100000000000000006 	0.265000000000000013 	0.107499999999999998 	0.0599999999999999978 	0.0864999999999999936 	13 	
-2 	0.54500000000000004 	0.46000000000000002 	0.160000000000000003 	0.897499999999999964 	0.341000000000000025 	0.165500000000000008 	0.344999999999999973 	10 	
-0 	0.440000000000000002 	0.340000000000000024 	0.100000000000000006 	0.451000000000000012 	0.188 	0.086999999999999994 	0.130000000000000004 	10 	
-2 	0.520000000000000018 	0.400000000000000022 	0.125 	0.559000000000000052 	0.254000000000000004 	0.139000000000000012 	0.148999999999999994 	8 	
-0 	0.5 	0.380000000000000004 	0.140000000000000013 	0.635499999999999954 	0.277000000000000024 	0.142999999999999988 	0.178499999999999992 	8 	
-2 	0.525000000000000022 	0.405000000000000027 	0.130000000000000004 	0.718500000000000028 	0.326500000000000012 	0.197500000000000009 	0.174999999999999989 	8 	
-1 	0.239999999999999991 	0.174999999999999989 	0.0550000000000000003 	0.0704999999999999932 	0.0250000000000000014 	0.0140000000000000003 	0.0210000000000000013 	5 	
-1 	0.135000000000000009 	0.130000000000000004 	0.0400000000000000008 	0.0290000000000000015 	0.0125000000000000007 	0.0064999999999999997 	0.00800000000000000017 	4 	
-0 	0.560000000000000053 	0.440000000000000002 	0.140000000000000013 	0.928499999999999992 	0.382500000000000007 	0.188 	0.299999999999999989 	11 	
-1 	0.385000000000000009 	0.28999999999999998 	0.0950000000000000011 	0.312 	0.142999999999999988 	0.0635000000000000009 	0.0859999999999999931 	6 	
-1 	0.385000000000000009 	0.280000000000000027 	0.0899999999999999967 	0.228000000000000008 	0.102499999999999994 	0.0420000000000000026 	0.0655000000000000027 	5 	
-0 	0.630000000000000004 	0.510000000000000009 	0.184999999999999998 	1.2350000000000001 	0.511499999999999955 	0.348999999999999977 	0.306499999999999995 	11 	
-0 	0.484999999999999987 	0.375 	0.14499999999999999 	0.588500000000000023 	0.23849999999999999 	0.115500000000000005 	0.190000000000000002 	13 	
-2 	0.665000000000000036 	0.525000000000000022 	0.165000000000000008 	1.33800000000000008 	0.55149999999999999 	0.357499999999999984 	0.349999999999999978 	18 	
-1 	0.5 	0.385000000000000009 	0.149999999999999994 	0.626499999999999946 	0.260500000000000009 	0.166500000000000009 	0.160000000000000003 	10 	
-0 	0.57999999999999996 	0.450000000000000011 	0.170000000000000012 	0.970500000000000029 	0.461500000000000021 	0.232000000000000012 	0.247999999999999998 	9 	
-0 	0.429999999999999993 	0.33500000000000002 	0.119999999999999996 	0.444000000000000006 	0.154999999999999999 	0.114500000000000005 	0.140000000000000013 	13 	
-1 	0.28999999999999998 	0.209999999999999992 	0.0599999999999999978 	0.104499999999999996 	0.0415000000000000022 	0.0219999999999999987 	0.0350000000000000033 	5 	
-0 	0.640000000000000013 	0.5 	0.149999999999999994 	1.07050000000000001 	0.370999999999999996 	0.270500000000000018 	0.359999999999999987 	8 	
-1 	0.46000000000000002 	0.33500000000000002 	0.110000000000000001 	0.444000000000000006 	0.225000000000000006 	0.0744999999999999968 	0.110000000000000001 	8 	
-2 	0.260000000000000009 	0.190000000000000002 	0.0749999999999999972 	0.0945000000000000007 	0.0444999999999999979 	0.0200000000000000004 	0.0299999999999999989 	6 	
-2 	0.594999999999999973 	0.474999999999999978 	0.165000000000000008 	1.21300000000000008 	0.620999999999999996 	0.243499999999999994 	0.274000000000000021 	9 	
-0 	0.584999999999999964 	0.450000000000000011 	0.160000000000000003 	1.07699999999999996 	0.4995 	0.287499999999999978 	0.25 	10 	
-0 	0.614999999999999991 	0.474999999999999978 	0.170000000000000012 	1.10250000000000004 	0.469499999999999973 	0.235499999999999987 	0.344999999999999973 	14 	
-2 	0.724999999999999978 	0.569999999999999951 	0.190000000000000002 	2.33049999999999979 	1.25299999999999989 	0.541000000000000036 	0.520000000000000018 	9 	
-0 	0.57999999999999996 	0.455000000000000016 	0.154999999999999999 	0.836500000000000021 	0.315000000000000002 	0.138500000000000012 	0.320000000000000007 	18 	
-2 	0.525000000000000022 	0.424999999999999989 	0.119999999999999996 	0.866500000000000048 	0.282499999999999973 	0.17599999999999999 	0.28999999999999998 	18 	
-1 	0.170000000000000012 	0.130000000000000004 	0.0950000000000000011 	0.0299999999999999989 	0.0129999999999999994 	0.00800000000000000017 	0.0100000000000000002 	4 	
-2 	0.645000000000000018 	0.5 	0.179999999999999993 	1.46100000000000008 	0.598500000000000032 	0.242499999999999993 	0.439000000000000001 	11 	
-1 	0.424999999999999989 	0.340000000000000024 	0.104999999999999996 	0.389000000000000012 	0.201500000000000012 	0.0904999999999999971 	0.0879999999999999949 	6 	
-2 	0.574999999999999956 	0.455000000000000016 	0.154999999999999999 	1.0129999999999999 	0.468500000000000028 	0.208499999999999991 	0.294999999999999984 	11 	
-1 	0.479999999999999982 	0.359999999999999987 	0.125 	0.542000000000000037 	0.279500000000000026 	0.102499999999999994 	0.146999999999999992 	7 	
-1 	0.405000000000000027 	0.284999999999999976 	0.0899999999999999967 	0.264500000000000013 	0.126500000000000001 	0.0505000000000000032 	0.0749999999999999972 	6 	
-0 	0.569999999999999951 	0.450000000000000011 	0.160000000000000003 	0.97150000000000003 	0.396500000000000019 	0.255000000000000004 	0.260000000000000009 	12 	
-2 	0.255000000000000004 	0.195000000000000007 	0.0650000000000000022 	0.0800000000000000017 	0.0315000000000000002 	0.0179999999999999986 	0.0269999999999999997 	8 	
-1 	0.41499999999999998 	0.309999999999999998 	0.0899999999999999967 	0.281499999999999972 	0.1245 	0.0614999999999999991 	0.0850000000000000061 	6 	
-2 	0.75 	0.555000000000000049 	0.214999999999999997 	2.20100000000000007 	1.06150000000000011 	0.523499999999999965 	0.52849999999999997 	11 	
-2 	0.535000000000000031 	0.419999999999999984 	0.165000000000000008 	0.919499999999999984 	0.33550000000000002 	0.19850000000000001 	0.260000000000000009 	16 	
-1 	0.550000000000000044 	0.429999999999999993 	0.14499999999999999 	0.711999999999999966 	0.302499999999999991 	0.151999999999999996 	0.225000000000000006 	10 	
-0 	0.625 	0.419999999999999984 	0.165000000000000008 	1.05950000000000011 	0.357999999999999985 	0.165000000000000008 	0.445000000000000007 	21 	
-1 	0.434999999999999998 	0.340000000000000024 	0.115000000000000005 	0.392500000000000016 	0.182499999999999996 	0.0779999999999999999 	0.114500000000000005 	6 	
-0 	0.550000000000000044 	0.41499999999999998 	0.135000000000000009 	0.763499999999999956 	0.318000000000000005 	0.209999999999999992 	0.200000000000000011 	9 	
-2 	0.484999999999999987 	0.395000000000000018 	0.140000000000000013 	0.629499999999999948 	0.228500000000000009 	0.127000000000000002 	0.225000000000000006 	14 	
-1 	0.330000000000000016 	0.204999999999999988 	0.0950000000000000011 	0.159500000000000003 	0.076999999999999999 	0.0320000000000000007 	0.043499999999999997 	5 	
-0 	0.530000000000000027 	0.419999999999999984 	0.130000000000000004 	1.00099999999999989 	0.340000000000000024 	0.226000000000000006 	0.265000000000000013 	17 	
-0 	0.655000000000000027 	0.505000000000000004 	0.174999999999999989 	1.29049999999999998 	0.620500000000000052 	0.296499999999999986 	0.326000000000000012 	10 	
-2 	0.594999999999999973 	0.474999999999999978 	0.140000000000000013 	0.94399999999999995 	0.362499999999999989 	0.189000000000000001 	0.315000000000000002 	9 	
-2 	0.574999999999999956 	0.469999999999999973 	0.149999999999999994 	0.978500000000000036 	0.450500000000000012 	0.196000000000000008 	0.276000000000000023 	9 	
-2 	0.599999999999999978 	0.450000000000000011 	0.14499999999999999 	0.877000000000000002 	0.432499999999999996 	0.154999999999999999 	0.239999999999999991 	9 	
-1 	0.294999999999999984 	0.220000000000000001 	0.0700000000000000067 	0.126000000000000001 	0.0514999999999999972 	0.0275000000000000001 	0.0350000000000000033 	6 	
-1 	0.385000000000000009 	0.299999999999999989 	0.0899999999999999967 	0.246999999999999997 	0.122499999999999998 	0.0439999999999999974 	0.0675000000000000044 	5 	
-1 	0.530000000000000027 	0.429999999999999993 	0.130000000000000004 	0.704500000000000015 	0.345999999999999974 	0.141499999999999987 	0.189000000000000001 	9 	
-0 	0.484999999999999987 	0.375 	0.135000000000000009 	0.55600000000000005 	0.192500000000000004 	0.131500000000000006 	0.168500000000000011 	10 	
-1 	0.660000000000000031 	0.525000000000000022 	0.214999999999999997 	1.78600000000000003 	0.672499999999999987 	0.361499999999999988 	0.406499999999999972 	11 	
-1 	0.520000000000000018 	0.409999999999999976 	0.140000000000000013 	0.662499999999999978 	0.277500000000000024 	0.155499999999999999 	0.196000000000000008 	11 	
-2 	0.505000000000000004 	0.405000000000000027 	0.140000000000000013 	0.875 	0.266500000000000015 	0.173999999999999988 	0.284999999999999976 	12 	
-2 	0.640000000000000013 	0.515000000000000013 	0.165000000000000008 	1.36899999999999999 	0.632000000000000006 	0.341500000000000026 	0.357999999999999985 	10 	
-2 	0.5 	0.390000000000000013 	0.135000000000000009 	0.781499999999999972 	0.360999999999999988 	0.157500000000000001 	0.23849999999999999 	9 	
-2 	0.619999999999999996 	0.510000000000000009 	0.174999999999999989 	1.28150000000000008 	0.571500000000000008 	0.23849999999999999 	0.390000000000000013 	10 	
-2 	0.619999999999999996 	0.494999999999999996 	0.174999999999999989 	1.80600000000000005 	0.643000000000000016 	0.328500000000000014 	0.724999999999999978 	17 	
-2 	0.46000000000000002 	0.375 	0.140000000000000013 	0.510499999999999954 	0.192000000000000004 	0.104499999999999996 	0.204999999999999988 	9 	
-0 	0.614999999999999991 	0.484999999999999987 	0.160000000000000003 	1.15749999999999997 	0.500499999999999945 	0.2495 	0.315000000000000002 	10 	
-1 	0.395000000000000018 	0.294999999999999984 	0.0950000000000000011 	0.27250000000000002 	0.115000000000000005 	0.0625 	0.0850000000000000061 	8 	
-2 	0.645000000000000018 	0.520000000000000018 	0.174999999999999989 	1.6359999999999999 	0.779000000000000026 	0.342000000000000026 	0.431999999999999995 	11 	
-2 	0.594999999999999973 	0.450000000000000011 	0.140000000000000013 	0.837999999999999967 	0.396500000000000019 	0.194000000000000006 	0.216999999999999998 	10 	
-2 	0.484999999999999987 	0.390000000000000013 	0.135000000000000009 	0.616999999999999993 	0.25 	0.134500000000000008 	0.163500000000000006 	8 	
-1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.321500000000000008 	0.153499999999999998 	0.0594999999999999973 	0.104999999999999996 	10 	
-2 	0.57999999999999996 	0.440000000000000002 	0.149999999999999994 	1.04649999999999999 	0.518000000000000016 	0.2185 	0.279500000000000026 	10 	
-0 	0.54500000000000004 	0.400000000000000022 	0.140000000000000013 	0.778000000000000025 	0.367999999999999994 	0.214999999999999997 	0.179999999999999993 	9 	
-2 	0.455000000000000016 	0.349999999999999978 	0.104999999999999996 	0.401000000000000023 	0.157500000000000001 	0.0830000000000000043 	0.135000000000000009 	9 	
-2 	0.569999999999999951 	0.46000000000000002 	0.170000000000000012 	0.90349999999999997 	0.407499999999999973 	0.193500000000000005 	0.213999999999999996 	7 	
-1 	0.429999999999999993 	0.340000000000000024 	0.104999999999999996 	0.440500000000000003 	0.23849999999999999 	0.0744999999999999968 	0.107499999999999998 	6 	
-1 	0.469999999999999973 	0.369999999999999996 	0.140000000000000013 	0.498499999999999999 	0.209499999999999992 	0.122499999999999998 	0.14499999999999999 	10 	
-0 	0.675000000000000044 	0.535000000000000031 	0.160000000000000003 	1.40999999999999992 	0.591999999999999971 	0.317500000000000004 	0.419999999999999984 	16 	
-2 	0.589999999999999969 	0.469999999999999973 	0.179999999999999993 	1.18700000000000006 	0.598500000000000032 	0.227000000000000007 	0.309999999999999998 	9 	
-2 	0.505000000000000004 	0.380000000000000004 	0.130000000000000004 	0.656000000000000028 	0.227000000000000007 	0.178499999999999992 	0.220000000000000001 	13 	
-1 	0.479999999999999982 	0.369999999999999996 	0.119999999999999996 	0.536000000000000032 	0.251000000000000001 	0.114000000000000004 	0.149999999999999994 	8 	
-1 	0.450000000000000011 	0.330000000000000016 	0.115000000000000005 	0.364999999999999991 	0.140000000000000013 	0.0825000000000000039 	0.1245 	8 	
-2 	0.474999999999999978 	0.369999999999999996 	0.125 	0.649000000000000021 	0.346999999999999975 	0.13600000000000001 	0.141999999999999987 	8 	
-0 	0.569999999999999951 	0.465000000000000024 	0.179999999999999993 	1.29499999999999993 	0.339000000000000024 	0.222500000000000003 	0.440000000000000002 	12 	
-1 	0.550000000000000044 	0.419999999999999984 	0.135000000000000009 	0.815999999999999948 	0.399500000000000022 	0.148499999999999993 	0.23000000000000001 	12 	
-2 	0.719999999999999973 	0.560000000000000053 	0.179999999999999993 	1.58650000000000002 	0.690999999999999948 	0.375 	0.442500000000000004 	11 	
-2 	0.450000000000000011 	0.340000000000000024 	0.130000000000000004 	0.371499999999999997 	0.160500000000000004 	0.0795000000000000012 	0.104999999999999996 	9 	
-1 	0.510000000000000009 	0.385000000000000009 	0.149999999999999994 	0.625 	0.309499999999999997 	0.118999999999999995 	0.172499999999999987 	8 	
-0 	0.625 	0.5 	0.165000000000000008 	1.28800000000000003 	0.572999999999999954 	0.303499999999999992 	0.315000000000000002 	9 	
-0 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.803499999999999992 	0.380000000000000004 	0.180499999999999994 	0.209999999999999992 	9 	
-1 	0.320000000000000007 	0.234999999999999987 	0.0899999999999999967 	0.182999999999999996 	0.0980000000000000038 	0.033500000000000002 	0.0420000000000000026 	7 	
-1 	0.54500000000000004 	0.405000000000000027 	0.135000000000000009 	0.594500000000000028 	0.270000000000000018 	0.118499999999999994 	0.184999999999999998 	8 	
-2 	0.5 	0.400000000000000022 	0.125 	0.597500000000000031 	0.270000000000000018 	0.127500000000000002 	0.166000000000000009 	9 	
-0 	0.594999999999999973 	0.479999999999999982 	0.149999999999999994 	1.1100000000000001 	0.497999999999999998 	0.228000000000000008 	0.330000000000000016 	10 	
-2 	0.320000000000000007 	0.239999999999999991 	0.0850000000000000061 	0.170000000000000012 	0.0655000000000000027 	0.0470000000000000001 	0.0490000000000000019 	7 	
-0 	0.594999999999999973 	0.479999999999999982 	0.200000000000000011 	0.974999999999999978 	0.357999999999999985 	0.203499999999999986 	0.340000000000000024 	15 	
-0 	0.594999999999999973 	0.450000000000000011 	0.149999999999999994 	1.1140000000000001 	0.586500000000000021 	0.220500000000000002 	0.25 	11 	
-2 	0.584999999999999964 	0.46000000000000002 	0.184999999999999998 	0.922000000000000042 	0.36349999999999999 	0.212999999999999995 	0.284999999999999976 	10 	
-2 	0.5 	0.375 	0.14499999999999999 	0.621500000000000052 	0.274000000000000021 	0.166000000000000009 	0.148499999999999993 	7 	
-1 	0.515000000000000013 	0.385000000000000009 	0.125 	0.611500000000000044 	0.317500000000000004 	0.126500000000000001 	0.149999999999999994 	8 	
-0 	0.505000000000000004 	0.400000000000000022 	0.165000000000000008 	0.728999999999999981 	0.267500000000000016 	0.154999999999999999 	0.25 	9 	
-0 	0.574999999999999956 	0.46000000000000002 	0.190000000000000002 	0.993999999999999995 	0.392000000000000015 	0.242499999999999993 	0.340000000000000024 	13 	
-1 	0.474999999999999978 	0.364999999999999991 	0.119999999999999996 	0.518499999999999961 	0.268000000000000016 	0.1095 	0.13650000000000001 	8 	
-2 	0.505000000000000004 	0.400000000000000022 	0.125 	0.770000000000000018 	0.273500000000000021 	0.159000000000000002 	0.255000000000000004 	13 	
-2 	0.440000000000000002 	0.364999999999999991 	0.125 	0.516000000000000014 	0.215499999999999997 	0.114000000000000004 	0.154999999999999999 	10 	
-0 	0.445000000000000007 	0.354999999999999982 	0.149999999999999994 	0.484999999999999987 	0.180999999999999994 	0.125 	0.154999999999999999 	11 	
-2 	0.5 	0.390000000000000013 	0.125 	0.521499999999999964 	0.248499999999999999 	0.117000000000000007 	0.131000000000000005 	6 	
-0 	0.465000000000000024 	0.359999999999999987 	0.119999999999999996 	0.476499999999999979 	0.192000000000000004 	0.112500000000000003 	0.160000000000000003 	10 	
-2 	0.630000000000000004 	0.479999999999999982 	0.149999999999999994 	1.27099999999999991 	0.660499999999999976 	0.242499999999999993 	0.309999999999999998 	11 	
-1 	0.484999999999999987 	0.354999999999999982 	0.104999999999999996 	0.497999999999999998 	0.217499999999999999 	0.096000000000000002 	0.152499999999999997 	9 	
-2 	0.665000000000000036 	0.540000000000000036 	0.174999999999999989 	1.34699999999999998 	0.495499999999999996 	0.254000000000000004 	0.41499999999999998 	17 	
-0 	0.800000000000000044 	0.630000000000000004 	0.195000000000000007 	2.5259999999999998 	0.933000000000000052 	0.589999999999999969 	0.619999999999999996 	23 	
-0 	0.650000000000000022 	0.510000000000000009 	0.154999999999999999 	1.18900000000000006 	0.482999999999999985 	0.278000000000000025 	0.364499999999999991 	13 	
-1 	0.455000000000000016 	0.33500000000000002 	0.135000000000000009 	0.501000000000000001 	0.274000000000000021 	0.0995000000000000051 	0.106499999999999997 	7 	
-1 	0.474999999999999978 	0.385000000000000009 	0.110000000000000001 	0.57350000000000001 	0.310999999999999999 	0.102499999999999994 	0.13600000000000001 	7 	
-0 	0.655000000000000027 	0.455000000000000016 	0.170000000000000012 	1.28950000000000009 	0.586999999999999966 	0.316500000000000004 	0.341500000000000026 	11 	
-2 	0.46000000000000002 	0.364999999999999991 	0.125 	0.467000000000000026 	0.189500000000000002 	0.0945000000000000007 	0.158000000000000002 	10 	
-1 	0.530000000000000027 	0.419999999999999984 	0.130000000000000004 	0.836500000000000021 	0.3745 	0.16700000000000001 	0.248999999999999999 	11 	
-0 	0.530000000000000027 	0.395000000000000018 	0.115000000000000005 	0.568500000000000005 	0.248999999999999999 	0.137500000000000011 	0.161000000000000004 	9 	
-0 	0.655000000000000027 	0.525000000000000022 	0.174999999999999989 	1.34800000000000009 	0.58550000000000002 	0.260500000000000009 	0.394000000000000017 	10 	
-1 	0.469999999999999973 	0.344999999999999973 	0.115000000000000005 	0.48849999999999999 	0.200500000000000012 	0.107999999999999999 	0.166000000000000009 	11 	
-0 	0.540000000000000036 	0.41499999999999998 	0.149999999999999994 	0.811499999999999999 	0.387500000000000011 	0.1875 	0.203499999999999986 	9 	
-0 	0.28999999999999998 	0.225000000000000006 	0.0749999999999999972 	0.140000000000000013 	0.0514999999999999972 	0.0235000000000000001 	0.0400000000000000008 	5 	
-2 	0.680000000000000049 	0.540000000000000036 	0.190000000000000002 	1.623 	0.716500000000000026 	0.353999999999999981 	0.471499999999999975 	12 	
-1 	0.405000000000000027 	0.299999999999999989 	0.104999999999999996 	0.303999999999999992 	0.14549999999999999 	0.0609999999999999987 	0.0805000000000000021 	6 	
-0 	0.599999999999999978 	0.474999999999999978 	0.154999999999999999 	1.20999999999999996 	0.653000000000000025 	0.169500000000000012 	0.320500000000000007 	10 	
-2 	0.589999999999999969 	0.469999999999999973 	0.160000000000000003 	1.20599999999999996 	0.478999999999999981 	0.242499999999999993 	0.308999999999999997 	8 	
-1 	0.359999999999999987 	0.265000000000000013 	0.0749999999999999972 	0.184499999999999997 	0.0830000000000000043 	0.0364999999999999977 	0.0550000000000000003 	7 	
-1 	0.375 	0.304999999999999993 	0.115000000000000005 	0.271500000000000019 	0.0919999999999999984 	0.0739999999999999963 	0.0899999999999999967 	8 	
-1 	0.469999999999999973 	0.369999999999999996 	0.110000000000000001 	0.555499999999999994 	0.25 	0.115000000000000005 	0.163000000000000006 	8 	
-0 	0.5 	0.400000000000000022 	0.14499999999999999 	0.630000000000000004 	0.234000000000000014 	0.146499999999999991 	0.23000000000000001 	12 	
-1 	0.440000000000000002 	0.325000000000000011 	0.0899999999999999967 	0.349999999999999978 	0.147999999999999993 	0.067000000000000004 	0.104999999999999996 	7 	
-1 	0.520000000000000018 	0.380000000000000004 	0.135000000000000009 	0.53949999999999998 	0.22950000000000001 	0.133000000000000007 	0.157000000000000001 	8 	
-0 	0.660000000000000031 	0.494999999999999996 	0.209999999999999992 	1.54800000000000004 	0.723999999999999977 	0.35249999999999998 	0.392500000000000016 	10 	
-1 	0.584999999999999964 	0.474999999999999978 	0.160000000000000003 	1.05049999999999999 	0.479999999999999982 	0.234000000000000014 	0.284999999999999976 	10 	
-1 	0.57999999999999996 	0.489999999999999991 	0.195000000000000007 	1.3165 	0.530499999999999972 	0.254000000000000004 	0.409999999999999976 	18 	
-2 	0.614999999999999991 	0.525000000000000022 	0.154999999999999999 	1.13749999999999996 	0.366999999999999993 	0.235999999999999988 	0.369999999999999996 	20 	
-0 	0.614999999999999991 	0.465000000000000024 	0.149999999999999994 	0.923000000000000043 	0.461500000000000021 	0.182499999999999996 	0.241499999999999992 	9 	
-2 	0.645000000000000018 	0.484999999999999987 	0.154999999999999999 	1.4890000000000001 	0.591500000000000026 	0.312 	0.380000000000000004 	18 	
-2 	0.574999999999999956 	0.445000000000000007 	0.14499999999999999 	0.846999999999999975 	0.41499999999999998 	0.194500000000000006 	0.220000000000000001 	9 	
-0 	0.275000000000000022 	0.195000000000000007 	0.0700000000000000067 	0.0800000000000000017 	0.0309999999999999998 	0.0214999999999999983 	0.0250000000000000014 	5 	
-2 	0.564999999999999947 	0.434999999999999998 	0.149999999999999994 	0.989999999999999991 	0.579500000000000015 	0.182499999999999996 	0.205999999999999989 	8 	
-0 	0.660000000000000031 	0.530000000000000027 	0.184999999999999998 	1.34600000000000009 	0.546000000000000041 	0.270500000000000018 	0.475999999999999979 	11 	
-1 	0.369999999999999996 	0.275000000000000022 	0.140000000000000013 	0.221500000000000002 	0.0970000000000000029 	0.0454999999999999988 	0.0614999999999999991 	6 	
-2 	0.469999999999999973 	0.364999999999999991 	0.119999999999999996 	0.611999999999999988 	0.327000000000000013 	0.149999999999999994 	0.140000000000000013 	8 	
-0 	0.445000000000000007 	0.325000000000000011 	0.125 	0.455000000000000016 	0.178499999999999992 	0.112500000000000003 	0.140000000000000013 	9 	
-1 	0.41499999999999998 	0.325000000000000011 	0.115000000000000005 	0.328500000000000014 	0.140500000000000014 	0.0509999999999999967 	0.105999999999999997 	12 	
-1 	0.569999999999999951 	0.434999999999999998 	0.170000000000000012 	0.847999999999999976 	0.400000000000000022 	0.166000000000000009 	0.25 	9 	
-2 	0.515000000000000013 	0.390000000000000013 	0.154999999999999999 	0.712500000000000022 	0.369499999999999995 	0.137000000000000011 	0.154999999999999999 	7 	
-0 	0.380000000000000004 	0.304999999999999993 	0.0950000000000000011 	0.281499999999999972 	0.1255 	0.0524999999999999981 	0.0899999999999999967 	8 	
-1 	0.280000000000000027 	0.204999999999999988 	0.0550000000000000003 	0.113500000000000004 	0.0449999999999999983 	0.0275000000000000001 	0.033500000000000002 	7 	
-0 	0.630000000000000004 	0.489999999999999991 	0.170000000000000012 	1.21550000000000002 	0.462500000000000022 	0.204499999999999987 	0.310499999999999998 	10 	
-0 	0.609999999999999987 	0.474999999999999978 	0.149999999999999994 	1.11349999999999993 	0.519499999999999962 	0.257500000000000007 	0.300499999999999989 	11 	
-2 	0.729999999999999982 	0.594999999999999973 	0.23000000000000001 	2.8254999999999999 	1.14650000000000007 	0.418999999999999984 	0.89700000000000002 	17 	
-2 	0.630000000000000004 	0.515000000000000013 	0.174999999999999989 	1.19550000000000001 	0.491999999999999993 	0.246999999999999997 	0.369999999999999996 	11 	
-2 	0.5 	0.354999999999999982 	0.140000000000000013 	0.528000000000000025 	0.212499999999999994 	0.148999999999999994 	0.140000000000000013 	9 	
-2 	0.550000000000000044 	0.440000000000000002 	0.135000000000000009 	0.879000000000000004 	0.367999999999999994 	0.209499999999999992 	0.265000000000000013 	10 	
-2 	0.469999999999999973 	0.375 	0.119999999999999996 	0.556499999999999995 	0.226000000000000006 	0.121999999999999997 	0.195000000000000007 	12 	
-1 	0.304999999999999993 	0.220000000000000001 	0.0700000000000000067 	0.140999999999999986 	0.0619999999999999996 	0.0309999999999999998 	0.0369999999999999982 	5 	
-2 	0.46000000000000002 	0.344999999999999973 	0.110000000000000001 	0.45950000000000002 	0.234999999999999987 	0.0884999999999999953 	0.116000000000000006 	7 	
-0 	0.655000000000000027 	0.54500000000000004 	0.165000000000000008 	1.62250000000000005 	0.655499999999999972 	0.298999999999999988 	0.513000000000000012 	12 	
-0 	0.434999999999999998 	0.33500000000000002 	0.110000000000000001 	0.380000000000000004 	0.169500000000000012 	0.0859999999999999931 	0.110000000000000001 	9 	
-1 	0.555000000000000049 	0.429999999999999993 	0.154999999999999999 	0.739500000000000046 	0.313500000000000001 	0.143499999999999989 	0.280000000000000027 	10 	
-2 	0.574999999999999956 	0.450000000000000011 	0.130000000000000004 	0.785000000000000031 	0.318000000000000005 	0.193000000000000005 	0.226500000000000007 	9 	
-1 	0.375 	0.280000000000000027 	0.0800000000000000017 	0.202500000000000013 	0.0825000000000000039 	0.048000000000000001 	0.0650000000000000022 	8 	
-1 	0.515000000000000013 	0.395000000000000018 	0.125 	0.55600000000000005 	0.269500000000000017 	0.096000000000000002 	0.170000000000000012 	8 	
-2 	0.535000000000000031 	0.419999999999999984 	0.130000000000000004 	0.805499999999999994 	0.30099999999999999 	0.180999999999999994 	0.280000000000000027 	14 	
-1 	0.445000000000000007 	0.344999999999999973 	0.104999999999999996 	0.408999999999999975 	0.16750000000000001 	0.101500000000000007 	0.117000000000000007 	7 	
-2 	0.599999999999999978 	0.465000000000000024 	0.154999999999999999 	1.01649999999999996 	0.512000000000000011 	0.246499999999999997 	0.225000000000000006 	10 	
-0 	0.584999999999999964 	0.445000000000000007 	0.140000000000000013 	0.913000000000000034 	0.430499999999999994 	0.220500000000000002 	0.253000000000000003 	10 	
-1 	0.650000000000000022 	0.515000000000000013 	0.160000000000000003 	1.16250000000000009 	0.494999999999999996 	0.203000000000000014 	0.330000000000000016 	17 	
-0 	0.515000000000000013 	0.390000000000000013 	0.130000000000000004 	0.575500000000000012 	0.197500000000000009 	0.130000000000000004 	0.184499999999999997 	9 	
-2 	0.479999999999999982 	0.364999999999999991 	0.119999999999999996 	0.601500000000000035 	0.312 	0.117000000000000007 	0.140000000000000013 	7 	
-0 	0.385000000000000009 	0.315000000000000002 	0.110000000000000001 	0.285999999999999976 	0.122499999999999998 	0.0635000000000000009 	0.0835000000000000048 	10 	
-2 	0.455000000000000016 	0.349999999999999978 	0.119999999999999996 	0.483499999999999985 	0.181499999999999995 	0.143999999999999989 	0.160000000000000003 	11 	
-2 	0.479999999999999982 	0.354999999999999982 	0.160000000000000003 	0.464000000000000024 	0.221000000000000002 	0.105999999999999997 	0.23899999999999999 	8 	
-1 	0.46000000000000002 	0.344999999999999973 	0.115000000000000005 	0.421499999999999986 	0.189500000000000002 	0.101999999999999993 	0.111000000000000001 	6 	
-2 	0.625 	0.520000000000000018 	0.174999999999999989 	1.41050000000000009 	0.690999999999999948 	0.322000000000000008 	0.346499999999999975 	10 	
-0 	0.530000000000000027 	0.419999999999999984 	0.170000000000000012 	0.827999999999999958 	0.409999999999999976 	0.20799999999999999 	0.150499999999999995 	6 	
-1 	0.640000000000000013 	0.489999999999999991 	0.135000000000000009 	1.10000000000000009 	0.487999999999999989 	0.2505 	0.292499999999999982 	10 	
-2 	0.510000000000000009 	0.400000000000000022 	0.130000000000000004 	0.643499999999999961 	0.270000000000000018 	0.166500000000000009 	0.204999999999999988 	12 	
-1 	0.550000000000000044 	0.419999999999999984 	0.115000000000000005 	0.668000000000000038 	0.292499999999999982 	0.137000000000000011 	0.208999999999999991 	11 	
-0 	0.405000000000000027 	0.304999999999999993 	0.0950000000000000011 	0.348499999999999976 	0.14549999999999999 	0.0894999999999999962 	0.100000000000000006 	9 	
-2 	0.650000000000000022 	0.494999999999999996 	0.179999999999999993 	1.79299999999999993 	0.800499999999999989 	0.339000000000000024 	0.530000000000000027 	14 	
-1 	0.33500000000000002 	0.239999999999999991 	0.0950000000000000011 	0.170000000000000012 	0.0619999999999999996 	0.0389999999999999999 	0.0550000000000000003 	9 	
-2 	0.46000000000000002 	0.344999999999999973 	0.119999999999999996 	0.493499999999999994 	0.243499999999999994 	0.117499999999999993 	0.132000000000000006 	8 	
-0 	0.619999999999999996 	0.510000000000000009 	0.174999999999999989 	1.27049999999999996 	0.541499999999999981 	0.323000000000000009 	0.322500000000000009 	9 	
-1 	0.405000000000000027 	0.25 	0.0899999999999999967 	0.287499999999999978 	0.128000000000000003 	0.0630000000000000004 	0.0805000000000000021 	7 	
-1 	0.220000000000000001 	0.165000000000000008 	0.0550000000000000003 	0.0544999999999999998 	0.0214999999999999983 	0.0120000000000000002 	0.0200000000000000004 	5 	
-2 	0.689999999999999947 	0.525000000000000022 	0.174999999999999989 	1.7004999999999999 	0.825500000000000012 	0.361999999999999988 	0.405000000000000027 	8 	
-0 	0.450000000000000011 	0.344999999999999973 	0.115000000000000005 	0.495999999999999996 	0.190500000000000003 	0.117000000000000007 	0.140000000000000013 	12 	
-1 	0.46000000000000002 	0.349999999999999978 	0.115000000000000005 	0.41549999999999998 	0.179999999999999993 	0.0980000000000000038 	0.117499999999999993 	7 	
-0 	0.57999999999999996 	0.46000000000000002 	0.149999999999999994 	0.995500000000000052 	0.428999999999999992 	0.211999999999999994 	0.260000000000000009 	19 	
-2 	0.650000000000000022 	0.510000000000000009 	0.174999999999999989 	1.3165 	0.634499999999999953 	0.260500000000000009 	0.36399999999999999 	12 	
-2 	0.535000000000000031 	0.434999999999999998 	0.140000000000000013 	0.873999999999999999 	0.373499999999999999 	0.229000000000000009 	0.219500000000000001 	8 	
-2 	0.390000000000000013 	0.299999999999999989 	0.0899999999999999967 	0.305499999999999994 	0.142999999999999988 	0.0645000000000000018 	0.0850000000000000061 	9 	
-0 	0.719999999999999973 	0.564999999999999947 	0.179999999999999993 	1.71900000000000008 	0.84650000000000003 	0.406999999999999973 	0.387500000000000011 	11 	
-1 	0.275000000000000022 	0.220000000000000001 	0.0800000000000000017 	0.13650000000000001 	0.0565000000000000016 	0.028500000000000001 	0.0420000000000000026 	6 	
-0 	0.694999999999999951 	0.530000000000000027 	0.200000000000000011 	2.04749999999999988 	0.75 	0.419499999999999984 	0.609500000000000042 	14 	
-2 	0.714999999999999969 	0.550000000000000044 	0.174999999999999989 	1.82499999999999996 	0.937999999999999945 	0.380500000000000005 	0.440000000000000002 	11 	
-2 	0.510000000000000009 	0.390000000000000013 	0.135000000000000009 	0.769000000000000017 	0.393500000000000016 	0.14549999999999999 	0.190000000000000002 	8 	
-2 	0.599999999999999978 	0.455000000000000016 	0.154999999999999999 	0.944999999999999951 	0.436499999999999999 	0.208499999999999991 	0.25 	8 	
-2 	0.515000000000000013 	0.409999999999999976 	0.140000000000000013 	0.735500000000000043 	0.306499999999999995 	0.137000000000000011 	0.200000000000000011 	7 	
-1 	0.369999999999999996 	0.270000000000000018 	0.0899999999999999967 	0.185499999999999998 	0.0700000000000000067 	0.0425000000000000031 	0.0650000000000000022 	7 	
-1 	0.409999999999999976 	0.33500000000000002 	0.110000000000000001 	0.330000000000000016 	0.157000000000000001 	0.0704999999999999932 	0.170000000000000012 	7 	
-1 	0.419999999999999984 	0.344999999999999973 	0.115000000000000005 	0.343500000000000028 	0.151499999999999996 	0.0795000000000000012 	0.115000000000000005 	9 	
-0 	0.5 	0.385000000000000009 	0.130000000000000004 	0.768000000000000016 	0.262500000000000011 	0.0950000000000000011 	0.270000000000000018 	13 	
-1 	0.340000000000000024 	0.265000000000000013 	0.0700000000000000067 	0.184999999999999998 	0.0625 	0.0395000000000000004 	0.0700000000000000067 	7 	
-0 	0.640000000000000013 	0.525000000000000022 	0.214999999999999997 	1.77899999999999991 	0.453500000000000014 	0.285499999999999976 	0.550000000000000044 	22 	
-2 	0.594999999999999973 	0.465000000000000024 	0.125 	0.799000000000000044 	0.324500000000000011 	0.200000000000000011 	0.23000000000000001 	10 	
-1 	0.395000000000000018 	0.28999999999999998 	0.0950000000000000011 	0.319000000000000006 	0.138000000000000012 	0.0800000000000000017 	0.0820000000000000034 	7 	
-2 	0.354999999999999982 	0.260000000000000009 	0.0850000000000000061 	0.190500000000000003 	0.0810000000000000026 	0.0485000000000000014 	0.0550000000000000003 	6 	
-1 	0.429999999999999993 	0.320000000000000007 	0.110000000000000001 	0.367499999999999993 	0.16750000000000001 	0.101999999999999993 	0.104999999999999996 	8 	
-2 	0.46000000000000002 	0.340000000000000024 	0.135000000000000009 	0.494999999999999996 	0.165500000000000008 	0.117000000000000007 	0.184999999999999998 	10 	
-2 	0.609999999999999987 	0.450000000000000011 	0.149999999999999994 	0.870999999999999996 	0.406999999999999973 	0.183499999999999996 	0.25 	10 	
-1 	0.450000000000000011 	0.364999999999999991 	0.125 	0.462000000000000022 	0.213499999999999995 	0.0985000000000000042 	0.131500000000000006 	8 	
-2 	0.54500000000000004 	0.409999999999999976 	0.140000000000000013 	0.625 	0.223000000000000004 	0.160000000000000003 	0.234999999999999987 	13 	
-0 	0.560000000000000053 	0.455000000000000016 	0.125 	0.942999999999999949 	0.343999999999999972 	0.129000000000000004 	0.375 	21 	
-2 	0.660000000000000031 	0.520000000000000018 	0.190000000000000002 	1.55800000000000005 	0.755000000000000004 	0.297999999999999987 	0.400000000000000022 	10 	
-2 	0.589999999999999969 	0.474999999999999978 	0.14499999999999999 	1.05299999999999994 	0.441500000000000004 	0.262000000000000011 	0.325000000000000011 	15 	
-2 	0.469999999999999973 	0.369999999999999996 	0.130000000000000004 	0.522499999999999964 	0.201000000000000012 	0.133000000000000007 	0.165000000000000008 	7 	
-1 	0.445000000000000007 	0.325000000000000011 	0.100000000000000006 	0.378000000000000003 	0.179499999999999993 	0.100000000000000006 	0.0889999999999999958 	7 	
-2 	0.645000000000000018 	0.515000000000000013 	0.239999999999999991 	1.54150000000000009 	0.470999999999999974 	0.368999999999999995 	0.535000000000000031 	13 	
-0 	0.604999999999999982 	0.474999999999999978 	0.165000000000000008 	1.05600000000000005 	0.432999999999999996 	0.219500000000000001 	0.356999999999999984 	9 	
-1 	0.574999999999999956 	0.450000000000000011 	0.170000000000000012 	0.931499999999999995 	0.357999999999999985 	0.214499999999999996 	0.260000000000000009 	13 	
-0 	0.535000000000000031 	0.41499999999999998 	0.184999999999999998 	0.841500000000000026 	0.314000000000000001 	0.158500000000000002 	0.299999999999999989 	15 	
-1 	0.515000000000000013 	0.390000000000000013 	0.110000000000000001 	0.531000000000000028 	0.241499999999999992 	0.0980000000000000038 	0.161500000000000005 	8 	
-0 	0.520000000000000018 	0.405000000000000027 	0.119999999999999996 	0.627000000000000002 	0.264500000000000013 	0.141499999999999987 	0.180999999999999994 	11 	
-2 	0.640000000000000013 	0.515000000000000013 	0.179999999999999993 	1.24700000000000011 	0.547499999999999987 	0.292499999999999982 	0.368499999999999994 	10 	
-1 	0.315000000000000002 	0.209999999999999992 	0.0599999999999999978 	0.125 	0.0599999999999999978 	0.0374999999999999986 	0.0350000000000000033 	5 	
-2 	0.574999999999999956 	0.455000000000000016 	0.165000000000000008 	0.866999999999999993 	0.376500000000000001 	0.180499999999999994 	0.268000000000000016 	8 	
-0 	0.484999999999999987 	0.380000000000000004 	0.149999999999999994 	0.604999999999999982 	0.215499999999999997 	0.140000000000000013 	0.179999999999999993 	15 	
-0 	0.57999999999999996 	0.46000000000000002 	0.119999999999999996 	0.99350000000000005 	0.462500000000000022 	0.23849999999999999 	0.280000000000000027 	11 	
-2 	0.515000000000000013 	0.405000000000000027 	0.14499999999999999 	0.694999999999999951 	0.214999999999999997 	0.163500000000000006 	0.234000000000000014 	15 	
-0 	0.54500000000000004 	0.440000000000000002 	0.135000000000000009 	0.918499999999999983 	0.428999999999999992 	0.201500000000000012 	0.237499999999999989 	10 	
-2 	0.469999999999999973 	0.375 	0.115000000000000005 	0.42649999999999999 	0.168500000000000011 	0.0754999999999999977 	0.149999999999999994 	8 	
-2 	0.694999999999999951 	0.530000000000000027 	0.190000000000000002 	1.72599999999999998 	0.762499999999999956 	0.435999999999999999 	0.455000000000000016 	11 	
-1 	0.520000000000000018 	0.395000000000000018 	0.135000000000000009 	0.633000000000000007 	0.298499999999999988 	0.129500000000000004 	0.174999999999999989 	9 	
-2 	0.569999999999999951 	0.465000000000000024 	0.125 	0.848999999999999977 	0.378500000000000003 	0.17649999999999999 	0.239999999999999991 	15 	
-2 	0.555000000000000049 	0.450000000000000011 	0.174999999999999989 	0.873999999999999999 	0.327500000000000013 	0.202000000000000013 	0.304999999999999993 	10 	
-1 	0.409999999999999976 	0.315000000000000002 	0.0950000000000000011 	0.280500000000000027 	0.114000000000000004 	0.0345000000000000029 	0.110000000000000001 	7 	
-1 	0.505000000000000004 	0.385000000000000009 	0.125 	0.595999999999999974 	0.244999999999999996 	0.0970000000000000029 	0.209999999999999992 	9 	
-2 	0.619999999999999996 	0.479999999999999982 	0.149999999999999994 	1.26600000000000001 	0.628499999999999948 	0.257500000000000007 	0.308999999999999997 	12 	
-1 	0.41499999999999998 	0.320000000000000007 	0.110000000000000001 	0.373499999999999999 	0.174999999999999989 	0.0754999999999999977 	0.109 	7 	
-2 	0.635000000000000009 	0.520000000000000018 	0.174999999999999989 	1.29200000000000004 	0.599999999999999978 	0.269000000000000017 	0.366999999999999993 	11 	
-1 	0.349999999999999978 	0.270000000000000018 	0.0899999999999999967 	0.205499999999999988 	0.0749999999999999972 	0.0575000000000000025 	0.0619999999999999996 	6 	
-2 	0.57999999999999996 	0.474999999999999978 	0.149999999999999994 	0.969999999999999973 	0.385000000000000009 	0.216499999999999998 	0.349999999999999978 	11 	
-2 	0.619999999999999996 	0.450000000000000011 	0.200000000000000011 	0.857999999999999985 	0.428499999999999992 	0.152499999999999997 	0.240499999999999992 	8 	
-0 	0.660000000000000031 	0.530000000000000027 	0.184999999999999998 	1.34850000000000003 	0.492999999999999994 	0.244999999999999996 	0.489999999999999991 	12 	
-0 	0.489999999999999991 	0.380000000000000004 	0.14499999999999999 	0.672499999999999987 	0.248999999999999999 	0.180999999999999994 	0.209999999999999992 	10 	
-0 	0.525000000000000022 	0.41499999999999998 	0.149999999999999994 	0.705500000000000016 	0.329000000000000015 	0.146999999999999992 	0.19900000000000001 	10 	
-0 	0.630000000000000004 	0.5 	0.179999999999999993 	1.1964999999999999 	0.514000000000000012 	0.232500000000000012 	0.399500000000000022 	8 	
-1 	0.255000000000000004 	0.190000000000000002 	0.0500000000000000028 	0.0830000000000000043 	0.0294999999999999984 	0.0214999999999999983 	0.0269999999999999997 	6 	
-1 	0.450000000000000011 	0.349999999999999978 	0.130000000000000004 	0.547000000000000042 	0.244999999999999996 	0.140500000000000014 	0.140500000000000014 	8 	
-1 	0.530000000000000027 	0.424999999999999989 	0.130000000000000004 	0.76749999999999996 	0.418999999999999984 	0.120499999999999996 	0.209999999999999992 	9 	
-1 	0.465000000000000024 	0.354999999999999982 	0.104999999999999996 	0.442000000000000004 	0.208499999999999991 	0.0975000000000000033 	0.118499999999999994 	7 	
-1 	0.560000000000000053 	0.424999999999999989 	0.140000000000000013 	0.917499999999999982 	0.400500000000000023 	0.197500000000000009 	0.260000000000000009 	10 	
-2 	0.589999999999999969 	0.465000000000000024 	0.154999999999999999 	1.1359999999999999 	0.524499999999999966 	0.26150000000000001 	0.275000000000000022 	11 	
-2 	0.560000000000000053 	0.450000000000000011 	0.184999999999999998 	1.07000000000000006 	0.380500000000000005 	0.174999999999999989 	0.409999999999999976 	19 	
-1 	0.385000000000000009 	0.28999999999999998 	0.0850000000000000061 	0.2505 	0.112000000000000002 	0.0609999999999999987 	0.0800000000000000017 	8 	
-2 	0.540000000000000036 	0.455000000000000016 	0.140000000000000013 	0.971999999999999975 	0.418999999999999984 	0.255000000000000004 	0.269000000000000017 	10 	
-2 	0.574999999999999956 	0.469999999999999973 	0.149999999999999994 	1.14149999999999996 	0.451500000000000012 	0.203999999999999987 	0.400000000000000022 	13 	
-1 	0.23000000000000001 	0.174999999999999989 	0.0650000000000000022 	0.0645000000000000018 	0.0259999999999999988 	0.0105000000000000007 	0.0200000000000000004 	5 	
-0 	0.599999999999999978 	0.505000000000000004 	0.190000000000000002 	1.129 	0.438500000000000001 	0.256000000000000005 	0.359999999999999987 	13 	
-2 	0.584999999999999964 	0.450000000000000011 	0.179999999999999993 	0.799499999999999988 	0.336000000000000021 	0.185499999999999998 	0.236999999999999988 	8 	
-1 	0.445000000000000007 	0.349999999999999978 	0.130000000000000004 	0.419499999999999984 	0.169500000000000012 	0.0945000000000000007 	0.119499999999999995 	7 	
-0 	0.660000000000000031 	0.515000000000000013 	0.179999999999999993 	1.52299999999999991 	0.540000000000000036 	0.336500000000000021 	0.555000000000000049 	16 	
-2 	0.719999999999999973 	0.584999999999999964 	0.220000000000000001 	1.91399999999999992 	0.91549999999999998 	0.448000000000000009 	0.478999999999999981 	11 	
-1 	0.344999999999999973 	0.255000000000000004 	0.0950000000000000011 	0.194500000000000006 	0.0924999999999999989 	0.0369999999999999982 	0.0550000000000000003 	6 	
-2 	0.465000000000000024 	0.359999999999999987 	0.104999999999999996 	0.430999999999999994 	0.171999999999999986 	0.106999999999999998 	0.174999999999999989 	9 	
-2 	0.589999999999999969 	0.469999999999999973 	0.154999999999999999 	1.17349999999999999 	0.624500000000000055 	0.233000000000000013 	0.259500000000000008 	9 	
-0 	0.54500000000000004 	0.429999999999999993 	0.165000000000000008 	0.802000000000000046 	0.293499999999999983 	0.182999999999999996 	0.280000000000000027 	11 	
-0 	0.54500000000000004 	0.41499999999999998 	0.160000000000000003 	0.771499999999999964 	0.27200000000000002 	0.14549999999999999 	0.276500000000000024 	10 	
-0 	0.41499999999999998 	0.304999999999999993 	0.104999999999999996 	0.360499999999999987 	0.119999999999999996 	0.0820000000000000034 	0.100000000000000006 	10 	
-2 	0.625 	0.5 	0.170000000000000012 	1.09850000000000003 	0.464500000000000024 	0.220000000000000001 	0.353999999999999981 	9 	
-1 	0.320000000000000007 	0.244999999999999996 	0.0800000000000000017 	0.158500000000000002 	0.0635000000000000009 	0.0325000000000000011 	0.0500000000000000028 	13 	
-2 	0.484999999999999987 	0.390000000000000013 	0.0850000000000000061 	0.643499999999999961 	0.294499999999999984 	0.102999999999999994 	0.198000000000000009 	8 	
-1 	0.445000000000000007 	0.33500000000000002 	0.110000000000000001 	0.410999999999999976 	0.19850000000000001 	0.0934999999999999998 	0.109 	8 	
-0 	0.604999999999999982 	0.479999999999999982 	0.149999999999999994 	1.07899999999999996 	0.450500000000000012 	0.283499999999999974 	0.292999999999999983 	10 	
-1 	0.299999999999999989 	0.220000000000000001 	0.0899999999999999967 	0.142499999999999988 	0.0570000000000000021 	0.033500000000000002 	0.0429999999999999966 	7 	
-0 	0.589999999999999969 	0.455000000000000016 	0.174999999999999989 	0.96599999999999997 	0.391000000000000014 	0.245499999999999996 	0.309999999999999998 	10 	
-1 	0.429999999999999993 	0.320000000000000007 	0.100000000000000006 	0.346499999999999975 	0.163500000000000006 	0.0800000000000000017 	0.0899999999999999967 	7 	
-0 	0.564999999999999947 	0.400000000000000022 	0.130000000000000004 	0.697500000000000009 	0.307499999999999996 	0.166500000000000009 	0.179999999999999993 	8 	
-1 	0.409999999999999976 	0.299999999999999989 	0.100000000000000006 	0.281999999999999973 	0.1255 	0.0570000000000000021 	0.0874999999999999944 	7 	
-1 	0.33500000000000002 	0.260000000000000009 	0.100000000000000006 	0.192000000000000004 	0.0785000000000000003 	0.0585000000000000034 	0.0700000000000000067 	8 	
-0 	0.550000000000000044 	0.405000000000000027 	0.125 	0.651000000000000023 	0.296499999999999986 	0.137000000000000011 	0.200000000000000011 	9 	
-0 	0.675000000000000044 	0.550000000000000044 	0.174999999999999989 	1.68900000000000006 	0.69399999999999995 	0.370999999999999996 	0.473999999999999977 	13 	
-1 	0.244999999999999996 	0.179999999999999993 	0.0650000000000000022 	0.0709999999999999937 	0.0299999999999999989 	0.0129999999999999994 	0.0214999999999999983 	4 	
-2 	0.599999999999999978 	0.494999999999999996 	0.195000000000000007 	1.05750000000000011 	0.384000000000000008 	0.190000000000000002 	0.375 	26 	
-0 	0.455000000000000016 	0.349999999999999978 	0.140000000000000013 	0.572500000000000009 	0.196500000000000008 	0.132500000000000007 	0.174999999999999989 	10 	
-0 	0.469999999999999973 	0.359999999999999987 	0.119999999999999996 	0.47749999999999998 	0.210499999999999993 	0.105499999999999997 	0.149999999999999994 	10 	
-1 	0.445000000000000007 	0.309999999999999998 	0.0899999999999999967 	0.336000000000000021 	0.155499999999999999 	0.0899999999999999967 	0.0855000000000000066 	7 	
-0 	0.574999999999999956 	0.450000000000000011 	0.160000000000000003 	1.06800000000000006 	0.55600000000000005 	0.213999999999999996 	0.257500000000000007 	10 	
-0 	0.625 	0.484999999999999987 	0.160000000000000003 	1.254 	0.59099999999999997 	0.259000000000000008 	0.348499999999999976 	9 	
-1 	0.359999999999999987 	0.280000000000000027 	0.104999999999999996 	0.19900000000000001 	0.0695000000000000062 	0.0449999999999999983 	0.0800000000000000017 	9 	
-1 	0.354999999999999982 	0.260000000000000009 	0.0899999999999999967 	0.19850000000000001 	0.0714999999999999941 	0.0495000000000000023 	0.0580000000000000029 	7 	
-0 	0.594999999999999973 	0.46000000000000002 	0.160000000000000003 	0.921000000000000041 	0.400500000000000023 	0.202500000000000013 	0.287499999999999978 	9 	
-1 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.666499999999999981 	0.3125 	0.138000000000000012 	0.189500000000000002 	10 	
-2 	0.640000000000000013 	0.484999999999999987 	0.160000000000000003 	1.00600000000000001 	0.456000000000000016 	0.224500000000000005 	0.283499999999999974 	9 	
-1 	0.28999999999999998 	0.225000000000000006 	0.0700000000000000067 	0.101000000000000006 	0.0359999999999999973 	0.0235000000000000001 	0.0350000000000000033 	8 	
-0 	0.635000000000000009 	0.494999999999999996 	0.0149999999999999994 	1.15650000000000008 	0.511499999999999955 	0.307999999999999996 	0.288499999999999979 	9 	
-1 	0.484999999999999987 	0.385000000000000009 	0.130000000000000004 	0.567999999999999949 	0.2505 	0.177999999999999992 	0.153999999999999998 	7 	
-1 	0.450000000000000011 	0.349999999999999978 	0.125 	0.47749999999999998 	0.223500000000000004 	0.0889999999999999958 	0.117999999999999994 	6 	
-2 	0.515000000000000013 	0.405000000000000027 	0.130000000000000004 	0.721999999999999975 	0.320000000000000007 	0.131000000000000005 	0.209999999999999992 	10 	
-0 	0.489999999999999991 	0.375 	0.135000000000000009 	0.612500000000000044 	0.255500000000000005 	0.101999999999999993 	0.220000000000000001 	11 	
-1 	0.309999999999999998 	0.225000000000000006 	0.0700000000000000067 	0.105499999999999997 	0.434999999999999998 	0.0149999999999999994 	0.0400000000000000008 	5 	
-2 	0.569999999999999951 	0.41499999999999998 	0.130000000000000004 	0.880000000000000004 	0.427499999999999991 	0.195500000000000007 	0.237999999999999989 	13 	
-0 	0.635000000000000009 	0.484999999999999987 	0.165000000000000008 	1.29449999999999998 	0.668000000000000038 	0.260500000000000009 	0.271500000000000019 	9 	
-2 	0.665000000000000036 	0.525000000000000022 	0.179999999999999993 	1.42900000000000005 	0.671499999999999986 	0.28999999999999998 	0.400000000000000022 	12 	
-1 	0.359999999999999987 	0.275000000000000022 	0.0850000000000000061 	0.197500000000000009 	0.0744999999999999968 	0.0415000000000000022 	0.0700000000000000067 	9 	
-1 	0.33500000000000002 	0.255000000000000004 	0.0800000000000000017 	0.16800000000000001 	0.0790000000000000008 	0.0354999999999999968 	0.0500000000000000028 	5 	
-1 	0.440000000000000002 	0.325000000000000011 	0.100000000000000006 	0.416499999999999981 	0.184999999999999998 	0.0864999999999999936 	0.110000000000000001 	6 	
-1 	0.390000000000000013 	0.294999999999999984 	0.0950000000000000011 	0.203000000000000014 	0.0874999999999999944 	0.0449999999999999983 	0.0749999999999999972 	7 	
-1 	0.574999999999999956 	0.445000000000000007 	0.160000000000000003 	0.917499999999999982 	0.450000000000000011 	0.193500000000000005 	0.239999999999999991 	9 	
-1 	0.359999999999999987 	0.299999999999999989 	0.0850000000000000061 	0.270000000000000018 	0.118499999999999994 	0.0640000000000000013 	0.0744999999999999968 	7 	
-0 	0.594999999999999973 	0.469999999999999973 	0.154999999999999999 	1.17749999999999999 	0.542000000000000037 	0.269000000000000017 	0.309999999999999998 	9 	
-1 	0.450000000000000011 	0.340000000000000024 	0.125 	0.404500000000000026 	0.171000000000000013 	0.0700000000000000067 	0.134500000000000008 	8 	
-1 	0.364999999999999991 	0.294999999999999984 	0.0950000000000000011 	0.25 	0.107499999999999998 	0.0544999999999999998 	0.0800000000000000017 	9 	
-1 	0.344999999999999973 	0.255000000000000004 	0.0950000000000000011 	0.182999999999999996 	0.0749999999999999972 	0.0384999999999999995 	0.0599999999999999978 	6 	
-1 	0.515000000000000013 	0.400000000000000022 	0.125 	0.592500000000000027 	0.265000000000000013 	0.117499999999999993 	0.16800000000000001 	9 	
-2 	0.474999999999999978 	0.375 	0.130000000000000004 	0.51749999999999996 	0.20749999999999999 	0.116500000000000006 	0.170000000000000012 	10 	
-2 	0.640000000000000013 	0.505000000000000004 	0.179999999999999993 	1.29699999999999993 	0.589999999999999969 	0.3125 	0.362999999999999989 	11 	
-1 	0.455000000000000016 	0.354999999999999982 	0.0800000000000000017 	0.452000000000000013 	0.216499999999999998 	0.0995000000000000051 	0.125 	9 	
-2 	0.525000000000000022 	0.429999999999999993 	0.165000000000000008 	0.864500000000000046 	0.376000000000000001 	0.194500000000000006 	0.251500000000000001 	16 	
-2 	0.655000000000000027 	0.520000000000000018 	0.165000000000000008 	1.40949999999999998 	0.585999999999999965 	0.290999999999999981 	0.405000000000000027 	9 	
-2 	0.369999999999999996 	0.28999999999999998 	0.0899999999999999967 	0.240999999999999992 	0.110000000000000001 	0.0449999999999999983 	0.0690000000000000058 	10 	
-1 	0.484999999999999987 	0.359999999999999987 	0.119999999999999996 	0.515499999999999958 	0.246499999999999997 	0.102499999999999994 	0.146999999999999992 	8 	
-1 	0.255000000000000004 	0.179999999999999993 	0.0550000000000000003 	0.0830000000000000043 	0.0309999999999999998 	0.0214999999999999983 	0.0200000000000000004 	4 	
-2 	0.465000000000000024 	0.405000000000000027 	0.135000000000000009 	0.777499999999999969 	0.435999999999999999 	0.171500000000000014 	0.14549999999999999 	10 	
-1 	0.354999999999999982 	0.260000000000000009 	0.0899999999999999967 	0.192500000000000004 	0.076999999999999999 	0.0379999999999999991 	0.0650000000000000022 	8 	
-2 	0.510000000000000009 	0.395000000000000018 	0.125 	0.580500000000000016 	0.243999999999999995 	0.133500000000000008 	0.188 	11 	
-1 	0.494999999999999996 	0.354999999999999982 	0.119999999999999996 	0.496499999999999997 	0.213999999999999996 	0.104499999999999996 	0.149499999999999994 	8 	
-2 	0.525000000000000022 	0.380000000000000004 	0.125 	0.650000000000000022 	0.302999999999999992 	0.154999999999999999 	0.159000000000000002 	7 	
-1 	0.530000000000000027 	0.419999999999999984 	0.119999999999999996 	0.59650000000000003 	0.255500000000000005 	0.140999999999999986 	0.176999999999999991 	7 	
-2 	0.349999999999999978 	0.260000000000000009 	0.0899999999999999967 	0.198000000000000009 	0.072499999999999995 	0.0560000000000000012 	0.0599999999999999978 	10 	
-0 	0.540000000000000036 	0.41499999999999998 	0.174999999999999989 	0.897499999999999964 	0.275000000000000022 	0.240999999999999992 	0.275000000000000022 	14 	
-1 	0.419999999999999984 	0.299999999999999989 	0.104999999999999996 	0.316000000000000003 	0.1255 	0.0700000000000000067 	0.103499999999999995 	7 	
-2 	0.489999999999999991 	0.380000000000000004 	0.110000000000000001 	0.554000000000000048 	0.293499999999999983 	0.100500000000000006 	0.149999999999999994 	8 	
-0 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	0.979999999999999982 	0.411499999999999977 	0.196000000000000008 	0.225500000000000006 	10 	
-0 	0.625 	0.5 	0.149999999999999994 	0.952999999999999958 	0.344499999999999973 	0.223500000000000004 	0.304999999999999993 	15 	
-0 	0.619999999999999996 	0.469999999999999973 	0.140000000000000013 	1.03249999999999997 	0.360499999999999987 	0.224000000000000005 	0.359999999999999987 	15 	
-2 	0.57999999999999996 	0.46000000000000002 	0.179999999999999993 	1.14500000000000002 	0.479999999999999982 	0.277000000000000024 	0.325000000000000011 	11 	
-1 	0.535000000000000031 	0.390000000000000013 	0.125 	0.598999999999999977 	0.259500000000000008 	0.148999999999999994 	0.169000000000000011 	9 	
-1 	0.525000000000000022 	0.395000000000000018 	0.119999999999999996 	0.607999999999999985 	0.296999999999999986 	0.139500000000000013 	0.140500000000000014 	8 	
-0 	0.630000000000000004 	0.484999999999999987 	0.190000000000000002 	1.24350000000000005 	0.463500000000000023 	0.305499999999999994 	0.390000000000000013 	21 	
-1 	0.369999999999999996 	0.275000000000000022 	0.100000000000000006 	0.22950000000000001 	0.0884999999999999953 	0.0464999999999999997 	0.0700000000000000067 	7 	
-1 	0.434999999999999998 	0.325000000000000011 	0.110000000000000001 	0.366999999999999993 	0.159500000000000003 	0.0800000000000000017 	0.104999999999999996 	6 	
-1 	0.385000000000000009 	0.28999999999999998 	0.0899999999999999967 	0.26150000000000001 	0.111000000000000001 	0.0594999999999999973 	0.0744999999999999968 	9 	
-1 	0.474999999999999978 	0.349999999999999978 	0.110000000000000001 	0.456500000000000017 	0.205999999999999989 	0.0990000000000000047 	0.130000000000000004 	6 	
-2 	0.630000000000000004 	0.469999999999999973 	0.154999999999999999 	1.13250000000000006 	0.588999999999999968 	0.210999999999999993 	0.286999999999999977 	8 	
-1 	0.424999999999999989 	0.299999999999999989 	0.0950000000000000011 	0.351499999999999979 	0.140999999999999986 	0.0774999999999999994 	0.119999999999999996 	8 	
-0 	0.57999999999999996 	0.434999999999999998 	0.140000000000000013 	0.952999999999999958 	0.474999999999999978 	0.216499999999999998 	0.209499999999999992 	9 	
-1 	0.375 	0.28999999999999998 	0.0850000000000000061 	0.23849999999999999 	0.117999999999999994 	0.0449999999999999983 	0.0695000000000000062 	7 	
-0 	0.645000000000000018 	0.505000000000000004 	0.165000000000000008 	1.43250000000000011 	0.684000000000000052 	0.307999999999999996 	0.336000000000000021 	8 	
-0 	0.429999999999999993 	0.325000000000000011 	0.115000000000000005 	0.38650000000000001 	0.147499999999999992 	0.106499999999999997 	0.110000000000000001 	11 	
-2 	0.655000000000000027 	0.484999999999999987 	0.195000000000000007 	1.62000000000000011 	0.627499999999999947 	0.357999999999999985 	0.484999999999999987 	17 	
-2 	0.369999999999999996 	0.280000000000000027 	0.104999999999999996 	0.234000000000000014 	0.0904999999999999971 	0.0585000000000000034 	0.0749999999999999972 	9 	
-1 	0.680000000000000049 	0.530000000000000027 	0.184999999999999998 	1.10949999999999993 	0.439000000000000001 	0.244999999999999996 	0.340000000000000024 	10 	
-2 	0.614999999999999991 	0.5 	0.170000000000000012 	1.05400000000000005 	0.484499999999999986 	0.228000000000000008 	0.294999999999999984 	10 	
-0 	0.330000000000000016 	0.260000000000000009 	0.0800000000000000017 	0.200000000000000011 	0.0625 	0.0500000000000000028 	0.0700000000000000067 	9 	
-1 	0.515000000000000013 	0.400000000000000022 	0.135000000000000009 	0.696500000000000008 	0.320000000000000007 	0.1255 	0.174999999999999989 	9 	
-1 	0.510000000000000009 	0.375 	0.100000000000000006 	0.578500000000000014 	0.237999999999999989 	0.122499999999999998 	0.174999999999999989 	7 	
-0 	0.625 	0.484999999999999987 	0.190000000000000002 	1.1745000000000001 	0.438500000000000001 	0.23050000000000001 	0.419999999999999984 	17 	
-2 	0.560000000000000053 	0.424999999999999989 	0.135000000000000009 	0.848999999999999977 	0.326500000000000012 	0.221000000000000002 	0.264500000000000013 	10 	
-1 	0.349999999999999978 	0.270000000000000018 	0.0749999999999999972 	0.214999999999999997 	0.100000000000000006 	0.0359999999999999973 	0.0650000000000000022 	6 	
-0 	0.54500000000000004 	0.41499999999999998 	0.149999999999999994 	0.733500000000000041 	0.279500000000000026 	0.163000000000000006 	0.2185 	11 	
-0 	0.344999999999999973 	0.255000000000000004 	0.100000000000000006 	0.197000000000000008 	0.0709999999999999937 	0.0509999999999999967 	0.0599999999999999978 	9 	
-2 	0.640000000000000013 	0.564999999999999947 	0.23000000000000001 	1.52099999999999991 	0.644000000000000017 	0.371999999999999997 	0.406000000000000028 	15 	
-2 	0.41499999999999998 	0.304999999999999993 	0.100000000000000006 	0.325000000000000011 	0.156 	0.0505000000000000032 	0.0909999999999999976 	6 	
-2 	0.494999999999999996 	0.41499999999999998 	0.165000000000000008 	0.748500000000000054 	0.264000000000000012 	0.134000000000000008 	0.284999999999999976 	13 	
-0 	0.560000000000000053 	0.445000000000000007 	0.195000000000000007 	0.980999999999999983 	0.304999999999999993 	0.224500000000000005 	0.33500000000000002 	16 	
-2 	0.525000000000000022 	0.400000000000000022 	0.140000000000000013 	0.73250000000000004 	0.334000000000000019 	0.157500000000000001 	0.170000000000000012 	11 	
-2 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.829500000000000015 	0.240499999999999992 	0.182499999999999996 	0.275000000000000022 	11 	
-1 	0.625 	0.46000000000000002 	0.160000000000000003 	1.23950000000000005 	0.550000000000000044 	0.27300000000000002 	0.380000000000000004 	14 	
-0 	0.440000000000000002 	0.340000000000000024 	0.140000000000000013 	0.481999999999999984 	0.185999999999999999 	0.108499999999999999 	0.160000000000000003 	9 	
-2 	0.689999999999999947 	0.550000000000000044 	0.179999999999999993 	1.6915 	0.66549999999999998 	0.402000000000000024 	0.5 	11 	
-0 	0.635000000000000009 	0.5 	0.165000000000000008 	1.45950000000000002 	0.70499999999999996 	0.264500000000000013 	0.390000000000000013 	9 	
-1 	0.440000000000000002 	0.33500000000000002 	0.110000000000000001 	0.388500000000000012 	0.174999999999999989 	0.0835000000000000048 	0.111000000000000001 	7 	
-2 	0.494999999999999996 	0.380000000000000004 	0.119999999999999996 	0.473999999999999977 	0.197000000000000008 	0.106499999999999997 	0.154499999999999998 	10 	
-2 	0.604999999999999982 	0.474999999999999978 	0.179999999999999993 	0.936499999999999999 	0.394000000000000017 	0.219 	0.294999999999999984 	15 	
-2 	0.494999999999999996 	0.385000000000000009 	0.135000000000000009 	0.708999999999999964 	0.210999999999999993 	0.137500000000000011 	0.262000000000000011 	12 	
-0 	0.584999999999999964 	0.455000000000000016 	0.165000000000000008 	0.997999999999999998 	0.344999999999999973 	0.2495 	0.315000000000000002 	12 	
-1 	0.46000000000000002 	0.349999999999999978 	0.110000000000000001 	0.394500000000000017 	0.168500000000000011 	0.0864999999999999936 	0.125 	9 	
-1 	0.434999999999999998 	0.315000000000000002 	0.110000000000000001 	0.368499999999999994 	0.161500000000000005 	0.0714999999999999941 	0.119999999999999996 	7 	
-1 	0.589999999999999969 	0.46000000000000002 	0.14499999999999999 	0.901499999999999968 	0.418999999999999984 	0.178499999999999992 	0.260000000000000009 	11 	
-0 	0.729999999999999982 	0.550000000000000044 	0.204999999999999988 	1.90799999999999992 	0.541499999999999981 	0.356499999999999984 	0.59650000000000003 	14 	
-2 	0.589999999999999969 	0.479999999999999982 	0.160000000000000003 	1.26200000000000001 	0.568500000000000005 	0.27250000000000002 	0.33500000000000002 	9 	
-1 	0.515000000000000013 	0.390000000000000013 	0.125 	0.570500000000000007 	0.237999999999999989 	0.126500000000000001 	0.184999999999999998 	8 	
-2 	0.494999999999999996 	0.400000000000000022 	0.135000000000000009 	0.609999999999999987 	0.27200000000000002 	0.143499999999999989 	0.143999999999999989 	7 	
-0 	0.525000000000000022 	0.424999999999999989 	0.14499999999999999 	0.799499999999999988 	0.33450000000000002 	0.208999999999999991 	0.239999999999999991 	15 	
-2 	0.645000000000000018 	0.510000000000000009 	0.160000000000000003 	1.33000000000000007 	0.666499999999999981 	0.308999999999999997 	0.317000000000000004 	9 	
-2 	0.57999999999999996 	0.46000000000000002 	0.154999999999999999 	1.4395 	0.671499999999999986 	0.27300000000000002 	0.295499999999999985 	10 	
-1 	0.489999999999999991 	0.375 	0.115000000000000005 	0.461500000000000021 	0.203999999999999987 	0.0945000000000000007 	0.142999999999999988 	8 	
-0 	0.46000000000000002 	0.424999999999999989 	0.154999999999999999 	0.745999999999999996 	0.300499999999999989 	0.151999999999999996 	0.239999999999999991 	8 	
-2 	0.564999999999999947 	0.429999999999999993 	0.130000000000000004 	0.78400000000000003 	0.349499999999999977 	0.188500000000000001 	0.212999999999999995 	9 	
-1 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.627499999999999947 	0.2505 	0.117499999999999993 	0.234999999999999987 	9 	
-2 	0.424999999999999989 	0.325000000000000011 	0.0950000000000000011 	0.378500000000000003 	0.170500000000000013 	0.0800000000000000017 	0.100000000000000006 	7 	
-0 	0.599999999999999978 	0.469999999999999973 	0.200000000000000011 	1.03099999999999992 	0.392000000000000015 	0.203499999999999986 	0.28999999999999998 	15 	
-2 	0.57999999999999996 	0.46000000000000002 	0.149999999999999994 	1.04899999999999993 	0.520499999999999963 	0.193500000000000005 	0.304999999999999993 	10 	
-1 	0.395000000000000018 	0.294999999999999984 	0.100000000000000006 	0.271500000000000019 	0.134000000000000008 	0.0325000000000000011 	0.0850000000000000061 	10 	
-1 	0.200000000000000011 	0.154999999999999999 	0.0400000000000000008 	0.043499999999999997 	0.0154999999999999999 	0.00899999999999999932 	0.00700000000000000015 	4 	
-2 	0.354999999999999982 	0.28999999999999998 	0.0899999999999999967 	0.327500000000000013 	0.134000000000000008 	0.0859999999999999931 	0.0899999999999999967 	9 	
-2 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.621999999999999997 	0.265500000000000014 	0.146999999999999992 	0.183999999999999997 	9 	
-0 	0.560000000000000053 	0.434999999999999998 	0.184999999999999998 	1.10600000000000009 	0.421999999999999986 	0.243499999999999994 	0.330000000000000016 	15 	
-2 	0.309999999999999998 	0.220000000000000001 	0.0850000000000000061 	0.145999999999999991 	0.0609999999999999987 	0.0364999999999999977 	0.0449999999999999983 	6 	
-0 	0.630000000000000004 	0.5 	0.184999999999999998 	1.38300000000000001 	0.540000000000000036 	0.331500000000000017 	0.380000000000000004 	10 	
-0 	0.625 	0.479999999999999982 	0.170000000000000012 	1.35250000000000004 	0.623500000000000054 	0.278000000000000025 	0.364999999999999991 	10 	
-0 	0.665000000000000036 	0.530000000000000027 	0.179999999999999993 	1.4910000000000001 	0.634499999999999953 	0.342000000000000026 	0.434999999999999998 	10 	
-2 	0.675000000000000044 	0.515000000000000013 	0.149999999999999994 	1.31200000000000006 	0.55600000000000005 	0.284499999999999975 	0.411499999999999977 	11 	
-1 	0.325000000000000011 	0.25 	0.0800000000000000017 	0.17599999999999999 	0.0594999999999999973 	0.0354999999999999968 	0.0630000000000000004 	7 	
-1 	0.589999999999999969 	0.445000000000000007 	0.135000000000000009 	0.771499999999999964 	0.328000000000000014 	0.174499999999999988 	0.23000000000000001 	9 	
-0 	0.535000000000000031 	0.450000000000000011 	0.135000000000000009 	0.807499999999999996 	0.322000000000000008 	0.180999999999999994 	0.25 	13 	
-2 	0.57999999999999996 	0.46000000000000002 	0.130000000000000004 	0.921000000000000041 	0.356999999999999984 	0.180999999999999994 	0.28999999999999998 	13 	
-1 	0.450000000000000011 	0.33500000000000002 	0.104999999999999996 	0.361999999999999988 	0.157500000000000001 	0.0795000000000000012 	0.1095 	7 	
-1 	0.174999999999999989 	0.130000000000000004 	0.0550000000000000003 	0.0315000000000000002 	0.0105000000000000007 	0.0064999999999999997 	0.0125000000000000007 	5 	
-2 	0.494999999999999996 	0.395000000000000018 	0.125 	0.541499999999999981 	0.237499999999999989 	0.134500000000000008 	0.154999999999999999 	9 	
-1 	0.589999999999999969 	0.46000000000000002 	0.125 	0.755000000000000004 	0.334000000000000019 	0.149999999999999994 	0.237999999999999989 	9 	
-2 	0.505000000000000004 	0.400000000000000022 	0.154999999999999999 	0.841500000000000026 	0.271500000000000019 	0.177499999999999991 	0.284999999999999976 	12 	
-1 	0.560000000000000053 	0.440000000000000002 	0.170000000000000012 	0.944500000000000006 	0.354499999999999982 	0.217499999999999999 	0.299999999999999989 	12 	
-1 	0.5 	0.369999999999999996 	0.119999999999999996 	0.544499999999999984 	0.248999999999999999 	0.106499999999999997 	0.151999999999999996 	8 	
-2 	0.395000000000000018 	0.304999999999999993 	0.104999999999999996 	0.281999999999999973 	0.0975000000000000033 	0.0650000000000000022 	0.096000000000000002 	9 	
-1 	0.440000000000000002 	0.364999999999999991 	0.110000000000000001 	0.446500000000000008 	0.212999999999999995 	0.0889999999999999958 	0.113500000000000004 	9 	
-0 	0.619999999999999996 	0.510000000000000009 	0.179999999999999993 	1.33149999999999991 	0.593999999999999972 	0.276000000000000023 	0.388000000000000012 	11 	
-2 	0.530000000000000027 	0.400000000000000022 	0.125 	0.757499999999999951 	0.39800000000000002 	0.150999999999999995 	0.174999999999999989 	8 	
-0 	0.555000000000000049 	0.424999999999999989 	0.140000000000000013 	0.962999999999999967 	0.440000000000000002 	0.224000000000000005 	0.239999999999999991 	7 	
-2 	0.724999999999999978 	0.569999999999999951 	0.190000000000000002 	2.54999999999999982 	1.07050000000000001 	0.482999999999999985 	0.724999999999999978 	14 	
-2 	0.650000000000000022 	0.520000000000000018 	0.190000000000000002 	1.34450000000000003 	0.519000000000000017 	0.305999999999999994 	0.446500000000000008 	16 	
-2 	0.689999999999999947 	0.530000000000000027 	0.209999999999999992 	1.58299999999999996 	0.735500000000000043 	0.405000000000000027 	0.38650000000000001 	12 	
-1 	0.385000000000000009 	0.304999999999999993 	0.0950000000000000011 	0.252000000000000002 	0.091499999999999998 	0.0550000000000000003 	0.0899999999999999967 	14 	
-2 	0.604999999999999982 	0.469999999999999973 	0.179999999999999993 	1.11549999999999994 	0.478999999999999981 	0.256500000000000006 	0.321000000000000008 	10 	
-1 	0.369999999999999996 	0.280000000000000027 	0.0850000000000000061 	0.198000000000000009 	0.0805000000000000021 	0.0454999999999999988 	0.0580000000000000029 	5 	
-0 	0.505000000000000004 	0.390000000000000013 	0.125 	0.544499999999999984 	0.245999999999999996 	0.149999999999999994 	0.140500000000000014 	7 	
-1 	0.540000000000000036 	0.424999999999999989 	0.130000000000000004 	0.815500000000000003 	0.367499999999999993 	0.13650000000000001 	0.245999999999999996 	11 	
-0 	0.719999999999999973 	0.525000000000000022 	0.179999999999999993 	1.44500000000000006 	0.631000000000000005 	0.321500000000000008 	0.434999999999999998 	7 	
-2 	0.699999999999999956 	0.550000000000000044 	0.174999999999999989 	1.44049999999999989 	0.656499999999999972 	0.298499999999999988 	0.375 	12 	
-0 	0.589999999999999969 	0.445000000000000007 	0.130000000000000004 	1.13250000000000006 	0.382500000000000007 	0.234000000000000014 	0.320000000000000007 	13 	
-2 	0.645000000000000018 	0.489999999999999991 	0.174999999999999989 	1.32000000000000006 	0.652499999999999969 	0.237499999999999989 	0.338500000000000023 	11 	
-1 	0.424999999999999989 	0.340000000000000024 	0.100000000000000006 	0.382000000000000006 	0.164000000000000007 	0.096000000000000002 	0.100000000000000006 	6 	
-2 	0.594999999999999973 	0.455000000000000016 	0.154999999999999999 	1.04099999999999993 	0.415999999999999981 	0.210499999999999993 	0.364999999999999991 	14 	
-0 	0.640000000000000013 	0.505000000000000004 	0.165000000000000008 	1.22350000000000003 	0.521499999999999964 	0.269500000000000017 	0.359999999999999987 	10 	
-1 	0.469999999999999973 	0.354999999999999982 	0.179999999999999993 	0.479999999999999982 	0.205499999999999988 	0.104999999999999996 	0.150499999999999995 	8 	
-2 	0.569999999999999951 	0.419999999999999984 	0.140000000000000013 	0.874500000000000055 	0.415999999999999981 	0.165000000000000008 	0.25 	8 	
-1 	0.284999999999999976 	0.214999999999999997 	0.0700000000000000067 	0.107499999999999998 	0.0509999999999999967 	0.0224999999999999992 	0.0269999999999999997 	6 	
-2 	0.574999999999999956 	0.450000000000000011 	0.154999999999999999 	0.947999999999999954 	0.428999999999999992 	0.205999999999999989 	0.259000000000000008 	7 	
-2 	0.630000000000000004 	0.469999999999999973 	0.14499999999999999 	1.10050000000000003 	0.520000000000000018 	0.260000000000000009 	0.276000000000000023 	9 	
-1 	0.325000000000000011 	0.270000000000000018 	0.100000000000000006 	0.184999999999999998 	0.0800000000000000017 	0.043499999999999997 	0.0650000000000000022 	6 	
-1 	0.400000000000000022 	0.28999999999999998 	0.100000000000000006 	0.267500000000000016 	0.120499999999999996 	0.0604999999999999982 	0.0764999999999999986 	5 	
-1 	0.270000000000000018 	0.190000000000000002 	0.0800000000000000017 	0.0810000000000000026 	0.0264999999999999993 	0.0195 	0.0299999999999999989 	6 	
-1 	0.280000000000000027 	0.220000000000000001 	0.0800000000000000017 	0.131500000000000006 	0.0660000000000000031 	0.0240000000000000005 	0.0299999999999999989 	5 	
-0 	0.635000000000000009 	0.510000000000000009 	0.174999999999999989 	1.21249999999999991 	0.57350000000000001 	0.26100000000000001 	0.359999999999999987 	14 	
-0 	0.469999999999999973 	0.364999999999999991 	0.119999999999999996 	0.581999999999999962 	0.28999999999999998 	0.0919999999999999984 	0.145999999999999991 	8 	
-0 	0.359999999999999987 	0.265000000000000013 	0.0899999999999999967 	0.206499999999999989 	0.0779999999999999999 	0.0570000000000000021 	0.0599999999999999978 	8 	
-2 	0.515000000000000013 	0.395000000000000018 	0.135000000000000009 	1.0069999999999999 	0.471999999999999975 	0.2495 	0.252000000000000002 	8 	
-0 	0.550000000000000044 	0.445000000000000007 	0.154999999999999999 	0.990500000000000047 	0.544000000000000039 	0.177999999999999992 	0.217999999999999999 	9 	
-1 	0.535000000000000031 	0.400000000000000022 	0.135000000000000009 	0.602500000000000036 	0.28949999999999998 	0.120999999999999996 	0.153999999999999998 	9 	
-0 	0.515000000000000013 	0.395000000000000018 	0.135000000000000009 	0.516000000000000014 	0.201500000000000012 	0.132000000000000006 	0.162000000000000005 	9 	
-0 	0.400000000000000022 	0.309999999999999998 	0.115000000000000005 	0.346499999999999975 	0.147499999999999992 	0.0695000000000000062 	0.115000000000000005 	10 	
-2 	0.450000000000000011 	0.325000000000000011 	0.115000000000000005 	0.430499999999999994 	0.223500000000000004 	0.0785000000000000003 	0.115500000000000005 	8 	
-0 	0.699999999999999956 	0.535000000000000031 	0.174999999999999989 	1.77299999999999991 	0.680499999999999994 	0.479999999999999982 	0.512000000000000011 	15 	
-0 	0.484999999999999987 	0.364999999999999991 	0.119999999999999996 	0.588500000000000023 	0.270000000000000018 	0.131000000000000005 	0.174999999999999989 	9 	
-0 	0.535000000000000031 	0.445000000000000007 	0.125 	0.872500000000000053 	0.416999999999999982 	0.19900000000000001 	0.239999999999999991 	8 	
-2 	0.564999999999999947 	0.455000000000000016 	0.170000000000000012 	0.906499999999999972 	0.342000000000000026 	0.156 	0.320000000000000007 	18 	
-2 	0.609999999999999987 	0.474999999999999978 	0.154999999999999999 	0.982999999999999985 	0.456500000000000017 	0.228000000000000008 	0.266000000000000014 	10 	
-2 	0.5 	0.385000000000000009 	0.135000000000000009 	0.64249999999999996 	0.319500000000000006 	0.129000000000000004 	0.153499999999999998 	7 	
-2 	0.775000000000000022 	0.630000000000000004 	0.25 	2.77950000000000008 	1.34850000000000003 	0.760000000000000009 	0.577999999999999958 	12 	
-0 	0.424999999999999989 	0.344999999999999973 	0.110000000000000001 	0.366499999999999992 	0.125 	0.0810000000000000026 	0.117000000000000007 	11 	
-0 	0.564999999999999947 	0.479999999999999982 	0.174999999999999989 	0.956999999999999962 	0.388500000000000012 	0.214999999999999997 	0.275000000000000022 	18 	
-0 	0.564999999999999947 	0.450000000000000011 	0.195000000000000007 	1.00350000000000006 	0.406000000000000028 	0.2505 	0.284999999999999976 	15 	
-0 	0.660000000000000031 	0.564999999999999947 	0.195000000000000007 	1.76049999999999995 	0.691999999999999948 	0.326500000000000012 	0.5 	16 	
-1 	0.395000000000000018 	0.270000000000000018 	0.100000000000000006 	0.298499999999999988 	0.14449999999999999 	0.0609999999999999987 	0.0820000000000000034 	5 	
-0 	0.665000000000000036 	0.505000000000000004 	0.160000000000000003 	1.29150000000000009 	0.631000000000000005 	0.292499999999999982 	0.320000000000000007 	11 	
-0 	0.614999999999999991 	0.515000000000000013 	0.135000000000000009 	1.12149999999999994 	0.54500000000000004 	0.23050000000000001 	0.28999999999999998 	9 	
-0 	0.619999999999999996 	0.434999999999999998 	0.154999999999999999 	1.01200000000000001 	0.47699999999999998 	0.235999999999999988 	0.275000000000000022 	8 	
-1 	0.400000000000000022 	0.299999999999999989 	0.0899999999999999967 	0.281499999999999972 	0.118499999999999994 	0.0609999999999999987 	0.0800000000000000017 	7 	
-0 	0.650000000000000022 	0.505000000000000004 	0.165000000000000008 	1.15999999999999992 	0.478499999999999981 	0.274000000000000021 	0.348999999999999977 	11 	
-1 	0.564999999999999947 	0.46000000000000002 	0.154999999999999999 	0.871500000000000052 	0.3755 	0.214999999999999997 	0.25 	10 	
-1 	0.574999999999999956 	0.445000000000000007 	0.170000000000000012 	0.80149999999999999 	0.347499999999999976 	0.146499999999999991 	0.25 	9 	
-0 	0.685000000000000053 	0.540000000000000036 	0.214999999999999997 	1.7024999999999999 	0.664000000000000035 	0.365499999999999992 	0.473499999999999976 	14 	
-2 	0.530000000000000027 	0.41499999999999998 	0.130000000000000004 	0.842500000000000027 	0.275000000000000022 	0.194500000000000006 	0.265000000000000013 	20 	
-2 	0.57999999999999996 	0.46000000000000002 	0.154999999999999999 	1.03350000000000009 	0.468999999999999972 	0.222500000000000003 	0.294999999999999984 	10 	
-2 	0.57999999999999996 	0.465000000000000024 	0.14499999999999999 	0.887000000000000011 	0.440500000000000003 	0.165500000000000008 	0.265000000000000013 	11 	
-2 	0.744999999999999996 	0.574999999999999956 	0.200000000000000011 	1.8839999999999999 	0.953999999999999959 	0.336000000000000021 	0.494999999999999996 	12 	
-1 	0.33500000000000002 	0.25 	0.0749999999999999972 	0.185999999999999999 	0.0945000000000000007 	0.0379999999999999991 	0.0444999999999999979 	7 	
-2 	0.515000000000000013 	0.380000000000000004 	0.174999999999999989 	0.956500000000000017 	0.325000000000000011 	0.158000000000000002 	0.309999999999999998 	14 	
-2 	0.57999999999999996 	0.455000000000000016 	0.149999999999999994 	1.1140000000000001 	0.476499999999999979 	0.215499999999999997 	0.265000000000000013 	8 	
-0 	0.814999999999999947 	0.650000000000000022 	0.25 	2.25499999999999989 	0.890499999999999958 	0.419999999999999984 	0.797499999999999987 	14 	
-0 	0.729999999999999982 	0.560000000000000053 	0.190000000000000002 	1.94249999999999989 	0.799000000000000044 	0.519499999999999962 	0.565500000000000003 	11 	
-2 	0.630000000000000004 	0.515000000000000013 	0.165000000000000008 	1.35200000000000009 	0.487999999999999989 	0.348999999999999977 	0.450000000000000011 	20 	
-0 	0.640000000000000013 	0.5 	0.149999999999999994 	1.20150000000000001 	0.559000000000000052 	0.231000000000000011 	0.33550000000000002 	9 	
-2 	0.635000000000000009 	0.515000000000000013 	0.160000000000000003 	1.20750000000000002 	0.538499999999999979 	0.281999999999999973 	0.344999999999999973 	11 	
-1 	0.550000000000000044 	0.424999999999999989 	0.149999999999999994 	0.80600000000000005 	0.376000000000000001 	0.171000000000000013 	0.244999999999999996 	14 	
-2 	0.630000000000000004 	0.479999999999999982 	0.149999999999999994 	1.1785000000000001 	0.518499999999999961 	0.247999999999999998 	0.32350000000000001 	8 	
-0 	0.520000000000000018 	0.424999999999999989 	0.149999999999999994 	0.812999999999999945 	0.385000000000000009 	0.201500000000000012 	0.23000000000000001 	10 	
-2 	0.609999999999999987 	0.469999999999999973 	0.149999999999999994 	1.16250000000000009 	0.564999999999999947 	0.258000000000000007 	0.308499999999999996 	11 	
-2 	0.680000000000000049 	0.515000000000000013 	0.170000000000000012 	1.61149999999999993 	0.841500000000000026 	0.305999999999999994 	0.395000000000000018 	11 	
-2 	0.589999999999999969 	0.450000000000000011 	0.184999999999999998 	1.28299999999999992 	0.472999999999999976 	0.276000000000000023 	0.424999999999999989 	16 	
-0 	0.594999999999999973 	0.494999999999999996 	0.184999999999999998 	1.28499999999999992 	0.415999999999999981 	0.224000000000000005 	0.484999999999999987 	13 	
-1 	0.28999999999999998 	0.204999999999999988 	0.0700000000000000067 	0.0975000000000000033 	0.0359999999999999973 	0.0189999999999999995 	0.0350000000000000033 	8 	
-1 	0.359999999999999987 	0.260000000000000009 	0.0899999999999999967 	0.178499999999999992 	0.0645000000000000018 	0.0369999999999999982 	0.0749999999999999972 	7 	
-2 	0.630000000000000004 	0.489999999999999991 	0.190000000000000002 	1.17749999999999999 	0.493499999999999994 	0.336500000000000021 	0.284999999999999976 	11 	
-2 	0.574999999999999956 	0.469999999999999973 	0.184999999999999998 	0.984999999999999987 	0.3745 	0.217499999999999999 	0.354999999999999982 	10 	
-2 	0.344999999999999973 	0.255000000000000004 	0.0800000000000000017 	0.169000000000000011 	0.0599999999999999978 	0.0425000000000000031 	0.0539999999999999994 	10 	
-0 	0.574999999999999956 	0.46000000000000002 	0.165000000000000008 	1.12400000000000011 	0.298499999999999988 	0.178499999999999992 	0.440000000000000002 	13 	
-2 	0.450000000000000011 	0.354999999999999982 	0.115000000000000005 	0.478999999999999981 	0.212499999999999994 	0.104499999999999996 	0.149999999999999994 	8 	
-0 	0.405000000000000027 	0.325000000000000011 	0.110000000000000001 	0.357499999999999984 	0.14499999999999999 	0.072499999999999995 	0.110000000000000001 	12 	
-2 	0.645000000000000018 	0.5 	0.190000000000000002 	1.55950000000000011 	0.740999999999999992 	0.371499999999999997 	0.384500000000000008 	14 	
-1 	0.54500000000000004 	0.429999999999999993 	0.130000000000000004 	0.759499999999999953 	0.357999999999999985 	0.152999999999999997 	0.205499999999999988 	8 	
-1 	0.375 	0.275000000000000022 	0.0950000000000000011 	0.22950000000000001 	0.0950000000000000011 	0.0544999999999999998 	0.0660000000000000031 	7 	
-2 	0.630000000000000004 	0.505000000000000004 	0.165000000000000008 	1.26000000000000001 	0.452500000000000013 	0.275500000000000023 	0.406000000000000028 	14 	
-2 	0.655000000000000027 	0.525000000000000022 	0.184999999999999998 	1.2589999999999999 	0.486999999999999988 	0.221500000000000002 	0.445000000000000007 	20 	
-1 	0.474999999999999978 	0.349999999999999978 	0.100000000000000006 	0.454500000000000015 	0.216499999999999998 	0.111000000000000001 	0.115000000000000005 	7 	
-1 	0.409999999999999976 	0.304999999999999993 	0.0950000000000000011 	0.262500000000000011 	0.100000000000000006 	0.0514999999999999972 	0.0899999999999999967 	6 	
-0 	0.405000000000000027 	0.325000000000000011 	0.110000000000000001 	0.355499999999999983 	0.150999999999999995 	0.0630000000000000004 	0.117000000000000007 	9 	
-0 	0.599999999999999978 	0.484999999999999987 	0.165000000000000008 	1.14050000000000007 	0.586999999999999966 	0.217499999999999999 	0.287999999999999978 	9 	
-0 	0.614999999999999991 	0.474999999999999978 	0.165000000000000008 	1.02299999999999991 	0.490499999999999992 	0.195500000000000007 	0.303499999999999992 	12 	
-2 	0.489999999999999991 	0.395000000000000018 	0.140000000000000013 	0.549000000000000044 	0.221500000000000002 	0.127500000000000002 	0.149999999999999994 	11 	
-2 	0.599999999999999978 	0.474999999999999978 	0.149999999999999994 	0.989999999999999991 	0.38600000000000001 	0.219500000000000001 	0.310499999999999998 	10 	
-0 	0.75 	0.609999999999999987 	0.234999999999999987 	2.50850000000000017 	1.23199999999999998 	0.519000000000000017 	0.611999999999999988 	14 	
-0 	0.709999999999999964 	0.550000000000000044 	0.170000000000000012 	1.6140000000000001 	0.742999999999999994 	0.344999999999999973 	0.450000000000000011 	11 	
-2 	0.41499999999999998 	0.344999999999999973 	0.135000000000000009 	0.38650000000000001 	0.128000000000000003 	0.0700000000000000067 	0.147999999999999993 	13 	
-0 	0.390000000000000013 	0.28999999999999998 	0.125 	0.305499999999999994 	0.120999999999999996 	0.0820000000000000034 	0.0899999999999999967 	7 	
-2 	0.574999999999999956 	0.440000000000000002 	0.184999999999999998 	1.02499999999999991 	0.507499999999999951 	0.224500000000000005 	0.248499999999999999 	10 	
-0 	0.675000000000000044 	0.525000000000000022 	0.170000000000000012 	1.80950000000000011 	0.78400000000000003 	0.391000000000000014 	0.455000000000000016 	12 	
-2 	0.609999999999999987 	0.5 	0.165000000000000008 	1.27150000000000007 	0.491499999999999992 	0.184999999999999998 	0.489999999999999991 	12 	
-0 	0.685000000000000053 	0.564999999999999947 	0.174999999999999989 	1.6379999999999999 	0.777499999999999969 	0.375 	0.438 	11 	
-0 	0.409999999999999976 	0.315000000000000002 	0.110000000000000001 	0.321000000000000008 	0.1255 	0.0655000000000000027 	0.0950000000000000011 	10 	
-2 	0.714999999999999969 	0.550000000000000044 	0.190000000000000002 	2.00450000000000017 	1.04649999999999999 	0.406999999999999973 	0.507499999999999951 	12 	
-1 	0.609999999999999987 	0.474999999999999978 	0.170000000000000012 	1.03849999999999998 	0.443500000000000005 	0.240999999999999992 	0.320000000000000007 	14 	
-2 	0.515000000000000013 	0.385000000000000009 	0.110000000000000001 	0.578500000000000014 	0.253000000000000003 	0.160000000000000003 	0.140000000000000013 	8 	
-2 	0.5 	0.390000000000000013 	0.135000000000000009 	0.659499999999999975 	0.314500000000000002 	0.153499999999999998 	0.1565 	6 	
-0 	0.589999999999999969 	0.465000000000000024 	0.160000000000000003 	1.10050000000000003 	0.506000000000000005 	0.252500000000000002 	0.294999999999999984 	13 	
-0 	0.680000000000000049 	0.505000000000000004 	0.170000000000000012 	1.34349999999999992 	0.657000000000000028 	0.296999999999999986 	0.354999999999999982 	12 	
-2 	0.309999999999999998 	0.234999999999999987 	0.0599999999999999978 	0.119999999999999996 	0.0415000000000000022 	0.0330000000000000016 	0.0400000000000000008 	11 	
-0 	0.5 	0.395000000000000018 	0.149999999999999994 	0.714500000000000024 	0.32350000000000001 	0.172999999999999987 	0.195000000000000007 	9 	
-1 	0.424999999999999989 	0.380000000000000004 	0.104999999999999996 	0.326500000000000012 	0.128500000000000003 	0.0785000000000000003 	0.100000000000000006 	10 	
-2 	0.645000000000000018 	0.484999999999999987 	0.149999999999999994 	1.22150000000000003 	0.569500000000000006 	0.273500000000000021 	0.330000000000000016 	9 	
-1 	0.484999999999999987 	0.375 	0.140000000000000013 	0.521000000000000019 	0.200000000000000011 	0.122999999999999998 	0.170000000000000012 	8 	
-1 	0.434999999999999998 	0.33500000000000002 	0.0950000000000000011 	0.297999999999999987 	0.109 	0.0580000000000000029 	0.115000000000000005 	7 	
-0 	0.584999999999999964 	0.450000000000000011 	0.170000000000000012 	0.86850000000000005 	0.332500000000000018 	0.163500000000000006 	0.270000000000000018 	22 	
-0 	0.540000000000000036 	0.385000000000000009 	0.140000000000000013 	0.765499999999999958 	0.326500000000000012 	0.116000000000000006 	0.236499999999999988 	10 	
-0 	0.569999999999999951 	0.455000000000000016 	0.149999999999999994 	1.10699999999999998 	0.540000000000000036 	0.255000000000000004 	0.270000000000000018 	8 	
-0 	0.515000000000000013 	0.41499999999999998 	0.140000000000000013 	0.693500000000000005 	0.311499999999999999 	0.151999999999999996 	0.200000000000000011 	10 	
-1 	0.455000000000000016 	0.354999999999999982 	0.104999999999999996 	0.371999999999999997 	0.138000000000000012 	0.0764999999999999986 	0.135000000000000009 	9 	
-0 	0.714999999999999969 	0.525000000000000022 	0.184999999999999998 	1.56000000000000005 	0.66549999999999998 	0.383000000000000007 	0.405000000000000027 	11 	
-1 	0.484999999999999987 	0.344999999999999973 	0.160000000000000003 	0.868999999999999995 	0.308499999999999996 	0.184999999999999998 	0.319000000000000006 	9 	
-2 	0.340000000000000024 	0.275000000000000022 	0.0899999999999999967 	0.206499999999999989 	0.072499999999999995 	0.0429999999999999966 	0.0700000000000000067 	10 	
-0 	0.520000000000000018 	0.405000000000000027 	0.140000000000000013 	0.817500000000000004 	0.279500000000000026 	0.182999999999999996 	0.260000000000000009 	17 	
-2 	0.550000000000000044 	0.450000000000000011 	0.130000000000000004 	0.92000000000000004 	0.378000000000000003 	0.23849999999999999 	0.28999999999999998 	11 	
-2 	0.489999999999999991 	0.354999999999999982 	0.154999999999999999 	0.980999999999999983 	0.465000000000000024 	0.201500000000000012 	0.2505 	8 	
-2 	0.525000000000000022 	0.434999999999999998 	0.154999999999999999 	1.06499999999999995 	0.485999999999999988 	0.233000000000000013 	0.284999999999999976 	8 	
-1 	0.209999999999999992 	0.170000000000000012 	0.0449999999999999983 	0.0475000000000000006 	0.0189999999999999995 	0.0109999999999999994 	0.0129999999999999994 	5 	
-0 	0.574999999999999956 	0.479999999999999982 	0.165000000000000008 	1.07800000000000007 	0.51100000000000001 	0.209499999999999992 	0.305999999999999994 	9 	
-1 	0.525000000000000022 	0.385000000000000009 	0.130000000000000004 	0.606999999999999984 	0.235499999999999987 	0.125 	0.195000000000000007 	8 	
-2 	0.455000000000000016 	0.364999999999999991 	0.0950000000000000011 	0.514000000000000012 	0.224500000000000005 	0.101000000000000006 	0.149999999999999994 	15 	
-0 	0.479999999999999982 	0.385000000000000009 	0.135000000000000009 	0.536000000000000032 	0.189500000000000002 	0.141999999999999987 	0.172999999999999987 	14 	
-1 	0.23000000000000001 	0.179999999999999993 	0.0500000000000000028 	0.0640000000000000013 	0.0214999999999999983 	0.0134999999999999998 	0.0200000000000000004 	5 	
-2 	0.584999999999999964 	0.489999999999999991 	0.184999999999999998 	1.17100000000000004 	0.52200000000000002 	0.253500000000000003 	0.33500000000000002 	10 	
-1 	0.515000000000000013 	0.419999999999999984 	0.135000000000000009 	0.710999999999999965 	0.337000000000000022 	0.143999999999999989 	0.204999999999999988 	13 	
-2 	0.67000000000000004 	0.525000000000000022 	0.170000000000000012 	1.40050000000000008 	0.714999999999999969 	0.302499999999999991 	0.387000000000000011 	9 	
-2 	0.619999999999999996 	0.510000000000000009 	0.174999999999999989 	1.15050000000000008 	0.4375 	0.226500000000000007 	0.400000000000000022 	12 	
-2 	0.609999999999999987 	0.479999999999999982 	0.170000000000000012 	1.13700000000000001 	0.456500000000000017 	0.28999999999999998 	0.346999999999999975 	10 	
-2 	0.489999999999999991 	0.390000000000000013 	0.149999999999999994 	0.572999999999999954 	0.225000000000000006 	0.123999999999999999 	0.170000000000000012 	21 	
-1 	0.41499999999999998 	0.304999999999999993 	0.119999999999999996 	0.336000000000000021 	0.165000000000000008 	0.0759999999999999981 	0.0805000000000000021 	7 	
-2 	0.660000000000000031 	0.525000000000000022 	0.200000000000000011 	1.4890000000000001 	0.606500000000000039 	0.379500000000000004 	0.420999999999999985 	10 	
-2 	0.650000000000000022 	0.494999999999999996 	0.160000000000000003 	1.30400000000000005 	0.569999999999999951 	0.312 	0.372499999999999998 	9 	
-2 	0.599999999999999978 	0.455000000000000016 	0.170000000000000012 	1.1915 	0.695999999999999952 	0.239499999999999991 	0.239999999999999991 	8 	
-0 	0.709999999999999964 	0.564999999999999947 	0.195000000000000007 	1.72649999999999992 	0.638000000000000012 	0.336500000000000021 	0.564999999999999947 	17 	
-0 	0.625 	0.474999999999999978 	0.174999999999999989 	1.14349999999999996 	0.475499999999999978 	0.247499999999999998 	0.348999999999999977 	10 	
-2 	0.614999999999999991 	0.479999999999999982 	0.190000000000000002 	1.3600000000000001 	0.530499999999999972 	0.237499999999999989 	0.469999999999999973 	18 	
-2 	0.535000000000000031 	0.419999999999999984 	0.149999999999999994 	0.699500000000000011 	0.257500000000000007 	0.152999999999999997 	0.239999999999999991 	12 	
-2 	0.594999999999999973 	0.474999999999999978 	0.160000000000000003 	1.31749999999999989 	0.407999999999999974 	0.234000000000000014 	0.57999999999999996 	21 	
-0 	0.665000000000000036 	0.515000000000000013 	0.179999999999999993 	1.38900000000000001 	0.594500000000000028 	0.32400000000000001 	0.395000000000000018 	10 	
-2 	0.489999999999999991 	0.395000000000000018 	0.135000000000000009 	0.554499999999999993 	0.212999999999999995 	0.0924999999999999989 	0.214999999999999997 	14 	
-2 	0.574999999999999956 	0.450000000000000011 	0.154999999999999999 	0.976500000000000035 	0.494999999999999996 	0.214499999999999996 	0.234999999999999987 	9 	
-1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.196000000000000008 	0.0825000000000000039 	0.0374999999999999986 	0.0599999999999999978 	7 	
-0 	0.520000000000000018 	0.424999999999999989 	0.14499999999999999 	0.699999999999999956 	0.20699999999999999 	0.190500000000000003 	0.239999999999999991 	13 	
-0 	0.304999999999999993 	0.23000000000000001 	0.0800000000000000017 	0.156 	0.0675000000000000044 	0.0345000000000000029 	0.048000000000000001 	7 	
-1 	0.5 	0.380000000000000004 	0.110000000000000001 	0.560499999999999998 	0.280000000000000027 	0.105999999999999997 	0.149999999999999994 	9 	
-1 	0.569999999999999951 	0.450000000000000011 	0.14499999999999999 	0.751000000000000001 	0.282499999999999973 	0.219500000000000001 	0.221500000000000002 	10 	
-2 	0.540000000000000036 	0.434999999999999998 	0.179999999999999993 	0.995999999999999996 	0.383500000000000008 	0.226000000000000006 	0.325000000000000011 	17 	
-1 	0.550000000000000044 	0.465000000000000024 	0.149999999999999994 	0.936000000000000054 	0.480999999999999983 	0.173999999999999988 	0.243499999999999994 	9 	
-1 	0.5 	0.409999999999999976 	0.140000000000000013 	0.661499999999999977 	0.258500000000000008 	0.162500000000000006 	0.196000000000000008 	9 	
-2 	0.564999999999999947 	0.440000000000000002 	0.125 	0.802000000000000046 	0.359499999999999986 	0.182499999999999996 	0.214999999999999997 	9 	
-2 	0.57999999999999996 	0.450000000000000011 	0.140000000000000013 	1.0129999999999999 	0.380000000000000004 	0.215999999999999998 	0.359999999999999987 	14 	
-0 	0.630000000000000004 	0.479999999999999982 	0.160000000000000003 	1.19900000000000007 	0.526499999999999968 	0.33500000000000002 	0.315000000000000002 	11 	
-2 	0.630000000000000004 	0.494999999999999996 	0.174999999999999989 	1.26950000000000007 	0.604999999999999982 	0.271000000000000019 	0.328000000000000014 	11 	
-2 	0.450000000000000011 	0.349999999999999978 	0.130000000000000004 	0.465500000000000025 	0.20749999999999999 	0.104499999999999996 	0.135000000000000009 	8 	
-0 	0.609999999999999987 	0.450000000000000011 	0.130000000000000004 	0.872500000000000053 	0.389000000000000012 	0.171500000000000014 	0.27200000000000002 	11 	
-0 	0.625 	0.469999999999999973 	0.14499999999999999 	0.983999999999999986 	0.474999999999999978 	0.200000000000000011 	0.265000000000000013 	11 	
-2 	0.569999999999999951 	0.46000000000000002 	0.149999999999999994 	1.03750000000000009 	0.541499999999999981 	0.203499999999999986 	0.25 	9 	
-2 	0.294999999999999984 	0.214999999999999997 	0.0749999999999999972 	0.129000000000000004 	0.0500000000000000028 	0.0294999999999999984 	0.0400000000000000008 	7 	
-0 	0.535000000000000031 	0.405000000000000027 	0.14499999999999999 	0.684499999999999997 	0.27250000000000002 	0.171000000000000013 	0.204999999999999988 	10 	
-0 	0.640000000000000013 	0.484999999999999987 	0.14499999999999999 	1.13349999999999995 	0.552499999999999991 	0.2505 	0.30149999999999999 	11 	
-1 	0.424999999999999989 	0.315000000000000002 	0.100000000000000006 	0.377000000000000002 	0.164500000000000007 	0.0719999999999999946 	0.104999999999999996 	6 	
-0 	0.489999999999999991 	0.400000000000000022 	0.14499999999999999 	0.663499999999999979 	0.209999999999999992 	0.129500000000000004 	0.251500000000000001 	13 	
-2 	0.640000000000000013 	0.515000000000000013 	0.0800000000000000017 	1.04200000000000004 	0.515000000000000013 	0.175499999999999989 	0.174999999999999989 	10 	
-2 	0.369999999999999996 	0.280000000000000027 	0.100000000000000006 	0.252000000000000002 	0.106499999999999997 	0.0594999999999999973 	0.0739999999999999963 	8 	
-1 	0.265000000000000013 	0.195000000000000007 	0.0599999999999999978 	0.0919999999999999984 	0.0345000000000000029 	0.0250000000000000014 	0.0245000000000000009 	6 	
-2 	0.510000000000000009 	0.390000000000000013 	0.125 	0.656499999999999972 	0.262000000000000011 	0.183499999999999996 	0.174999999999999989 	10 	
-2 	0.760000000000000009 	0.604999999999999982 	0.214999999999999997 	2.17300000000000004 	0.801000000000000045 	0.491499999999999992 	0.646000000000000019 	13 	
-2 	0.400000000000000022 	0.304999999999999993 	0.130000000000000004 	0.293499999999999983 	0.096000000000000002 	0.0675000000000000044 	0.104999999999999996 	9 	
-1 	0.200000000000000011 	0.14499999999999999 	0.0599999999999999978 	0.0369999999999999982 	0.0125000000000000007 	0.00949999999999999976 	0.0109999999999999994 	4 	
-1 	0.550000000000000044 	0.424999999999999989 	0.130000000000000004 	0.664000000000000035 	0.269500000000000017 	0.163000000000000006 	0.209999999999999992 	8 	
-2 	0.270000000000000018 	0.200000000000000011 	0.0800000000000000017 	0.120499999999999996 	0.0464999999999999997 	0.0280000000000000006 	0.0400000000000000008 	6 	
-1 	0.419999999999999984 	0.315000000000000002 	0.110000000000000001 	0.402500000000000024 	0.185499999999999998 	0.0830000000000000043 	0.101500000000000007 	8 	
-2 	0.67000000000000004 	0.5 	0.200000000000000011 	1.26899999999999991 	0.575999999999999956 	0.298499999999999988 	0.350999999999999979 	11 	
-1 	0.564999999999999947 	0.440000000000000002 	0.135000000000000009 	0.768000000000000016 	0.330500000000000016 	0.138500000000000012 	0.247499999999999998 	9 	
-1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.333500000000000019 	0.14449999999999999 	0.0714999999999999941 	0.0950000000000000011 	7 	
-1 	0.54500000000000004 	0.375 	0.119999999999999996 	0.543000000000000038 	0.237499999999999989 	0.115500000000000005 	0.172499999999999987 	8 	
-1 	0.41499999999999998 	0.320000000000000007 	0.115000000000000005 	0.304499999999999993 	0.121499999999999997 	0.0734999999999999959 	0.0940000000000000002 	7 	
-1 	0.375 	0.280000000000000027 	0.0800000000000000017 	0.216499999999999998 	0.0934999999999999998 	0.0924999999999999989 	0.0700000000000000067 	7 	
-0 	0.604999999999999982 	0.474999999999999978 	0.174999999999999989 	1.07600000000000007 	0.463000000000000023 	0.219500000000000001 	0.33500000000000002 	9 	
-2 	0.450000000000000011 	0.349999999999999978 	0.100000000000000006 	0.367499999999999993 	0.146499999999999991 	0.101500000000000007 	0.119999999999999996 	10 	
-0 	0.325000000000000011 	0.260000000000000009 	0.0899999999999999967 	0.191500000000000004 	0.0850000000000000061 	0.0359999999999999973 	0.0619999999999999996 	7 	
-2 	0.555000000000000049 	0.440000000000000002 	0.149999999999999994 	0.755000000000000004 	0.306999999999999995 	0.152499999999999997 	0.260000000000000009 	12 	
-0 	0.550000000000000044 	0.424999999999999989 	0.14499999999999999 	0.797000000000000042 	0.296999999999999986 	0.149999999999999994 	0.265000000000000013 	9 	
-1 	0.369999999999999996 	0.280000000000000027 	0.100000000000000006 	0.221000000000000002 	0.116500000000000006 	0.0264999999999999993 	0.0635000000000000009 	6 	
-0 	0.530000000000000027 	0.424999999999999989 	0.149999999999999994 	0.849500000000000033 	0.328000000000000014 	0.232000000000000012 	0.202000000000000013 	8 	
-0 	0.660000000000000031 	0.505000000000000004 	0.190000000000000002 	1.40450000000000008 	0.625499999999999945 	0.337500000000000022 	0.3745 	9 	
-0 	0.46000000000000002 	0.375 	0.119999999999999996 	0.46050000000000002 	0.177499999999999991 	0.110000000000000001 	0.149999999999999994 	7 	
-0 	0.375 	0.28999999999999998 	0.0800000000000000017 	0.281999999999999973 	0.140500000000000014 	0.072499999999999995 	0.0800000000000000017 	7 	
-0 	0.469999999999999973 	0.354999999999999982 	0.140000000000000013 	0.432999999999999996 	0.152499999999999997 	0.0950000000000000011 	0.151999999999999996 	12 	
-1 	0.204999999999999988 	0.149999999999999994 	0.0550000000000000003 	0.0420000000000000026 	0.0254999999999999984 	0.0149999999999999994 	0.0120000000000000002 	5 	
-0 	0.635000000000000009 	0.520000000000000018 	0.165000000000000008 	1.34050000000000002 	0.50649999999999995 	0.295999999999999985 	0.411999999999999977 	11 	
-0 	0.75 	0.564999999999999947 	0.214999999999999997 	1.93799999999999994 	0.773499999999999965 	0.482499999999999984 	0.574999999999999956 	11 	
-0 	0.650000000000000022 	0.525000000000000022 	0.190000000000000002 	1.38500000000000001 	0.887499999999999956 	0.309499999999999997 	0.405000000000000027 	11 	
-2 	0.450000000000000011 	0.344999999999999973 	0.104999999999999996 	0.411499999999999977 	0.179999999999999993 	0.112500000000000003 	0.135000000000000009 	7 	
-0 	0.564999999999999947 	0.505000000000000004 	0.209999999999999992 	1.27649999999999997 	0.501000000000000001 	0.279000000000000026 	0.354999999999999982 	12 	
-1 	0.445000000000000007 	0.33500000000000002 	0.100000000000000006 	0.489499999999999991 	0.274500000000000022 	0.0859999999999999931 	0.110500000000000001 	7 	
-1 	0.234999999999999987 	0.174999999999999989 	0.0400000000000000008 	0.0704999999999999932 	0.033500000000000002 	0.0149999999999999994 	0.0200000000000000004 	5 	
-2 	0.385000000000000009 	0.294999999999999984 	0.0950000000000000011 	0.33500000000000002 	0.146999999999999992 	0.0940000000000000002 	0.0899999999999999967 	7 	
-2 	0.635000000000000009 	0.484999999999999987 	0.179999999999999993 	1.17949999999999999 	0.478499999999999981 	0.277500000000000024 	0.354999999999999982 	10 	
-1 	0.255000000000000004 	0.190000000000000002 	0.0599999999999999978 	0.0859999999999999931 	0.0400000000000000008 	0.0184999999999999991 	0.0250000000000000014 	5 	
-1 	0.604999999999999982 	0.469999999999999973 	0.140000000000000013 	0.938999999999999946 	0.338500000000000023 	0.201000000000000012 	0.320000000000000007 	13 	
-2 	0.689999999999999947 	0.515000000000000013 	0.179999999999999993 	1.84450000000000003 	0.981500000000000039 	0.465500000000000025 	0.341000000000000025 	13 	
-0 	0.510000000000000009 	0.400000000000000022 	0.140000000000000013 	0.814500000000000002 	0.459000000000000019 	0.196500000000000008 	0.195000000000000007 	10 	
-2 	0.589999999999999969 	0.46000000000000002 	0.130000000000000004 	1.10200000000000009 	0.455000000000000016 	0.205499999999999988 	0.330000000000000016 	12 	
-2 	0.645000000000000018 	0.505000000000000004 	0.184999999999999998 	1.46300000000000008 	0.591999999999999971 	0.390500000000000014 	0.415999999999999981 	10 	
-2 	0.630000000000000004 	0.489999999999999991 	0.154999999999999999 	1.25249999999999995 	0.630000000000000004 	0.245999999999999996 	0.288999999999999979 	9 	
-2 	0.599999999999999978 	0.474999999999999978 	0.190000000000000002 	1.08749999999999991 	0.403000000000000025 	0.265500000000000014 	0.325000000000000011 	14 	
-2 	0.465000000000000024 	0.359999999999999987 	0.130000000000000004 	0.526499999999999968 	0.210499999999999993 	0.118499999999999994 	0.165000000000000008 	10 	
-2 	0.445000000000000007 	0.349999999999999978 	0.119999999999999996 	0.442500000000000004 	0.192000000000000004 	0.0955000000000000016 	0.135000000000000009 	8 	
-0 	0.54500000000000004 	0.429999999999999993 	0.160000000000000003 	0.843999999999999972 	0.394500000000000017 	0.185499999999999998 	0.231000000000000011 	9 	
-2 	0.70499999999999996 	0.555000000000000049 	0.214999999999999997 	2.14100000000000001 	1.04649999999999999 	0.383000000000000007 	0.528000000000000025 	11 	
-2 	0.699999999999999956 	0.555000000000000049 	0.200000000000000011 	1.8580000000000001 	0.729999999999999982 	0.366499999999999992 	0.594999999999999973 	11 	
-1 	0.364999999999999991 	0.265000000000000013 	0.135000000000000009 	0.221500000000000002 	0.104999999999999996 	0.0470000000000000001 	0.0604999999999999982 	7 	
-1 	0.569999999999999951 	0.440000000000000002 	0.149999999999999994 	0.755000000000000004 	0.342500000000000027 	0.160000000000000003 	0.224000000000000005 	8 	
-0 	0.569999999999999951 	0.445000000000000007 	0.154999999999999999 	0.732999999999999985 	0.281999999999999973 	0.159000000000000002 	0.234999999999999987 	14 	
-1 	0.395000000000000018 	0.320000000000000007 	0.100000000000000006 	0.307499999999999996 	0.148999999999999994 	0.0534999999999999989 	0.0899999999999999967 	8 	
-1 	0.494999999999999996 	0.369999999999999996 	0.125 	0.47749999999999998 	0.184999999999999998 	0.0704999999999999932 	0.169000000000000011 	18 	
-1 	0.160000000000000003 	0.110000000000000001 	0.0250000000000000014 	0.0179999999999999986 	0.0064999999999999997 	0.00549999999999999968 	0.0050000000000000001 	3 	
-1 	0.465000000000000024 	0.375 	0.119999999999999996 	0.470999999999999974 	0.222000000000000003 	0.118999999999999995 	0.140000000000000013 	9 	
-0 	0.619999999999999996 	0.505000000000000004 	0.160000000000000003 	1.37250000000000005 	0.628499999999999948 	0.275000000000000022 	0.368499999999999994 	11 	
-2 	0.510000000000000009 	0.400000000000000022 	0.140000000000000013 	0.651499999999999968 	0.245499999999999996 	0.166500000000000009 	0.184999999999999998 	10 	
-2 	0.234999999999999987 	0.160000000000000003 	0.0599999999999999978 	0.0544999999999999998 	0.0264999999999999993 	0.00949999999999999976 	0.0149999999999999994 	4 	
-0 	0.70499999999999996 	0.54500000000000004 	0.179999999999999993 	1.53950000000000009 	0.60750000000000004 	0.367499999999999993 	0.464500000000000024 	13 	
-0 	0.57999999999999996 	0.455000000000000016 	0.119999999999999996 	1.0734999999999999 	0.478999999999999981 	0.273500000000000021 	0.265000000000000013 	10 	
-1 	0.409999999999999976 	0.309999999999999998 	0.110000000000000001 	0.315000000000000002 	0.123999999999999999 	0.0820000000000000034 	0.0950000000000000011 	9 	
-1 	0.385000000000000009 	0.265000000000000013 	0.0800000000000000017 	0.251000000000000001 	0.123999999999999999 	0.0369999999999999982 	0.0700000000000000067 	6 	
-0 	0.67000000000000004 	0.505000000000000004 	0.174999999999999989 	1.01449999999999996 	0.4375 	0.271000000000000019 	0.3745 	10 	
-0 	0.680000000000000049 	0.560000000000000053 	0.195000000000000007 	1.66399999999999992 	0.57999999999999996 	0.385500000000000009 	0.54500000000000004 	11 	
-1 	0.540000000000000036 	0.424999999999999989 	0.130000000000000004 	0.720500000000000029 	0.295499999999999985 	0.169000000000000011 	0.225000000000000006 	10 	
-1 	0.569999999999999951 	0.450000000000000011 	0.135000000000000009 	0.794000000000000039 	0.381500000000000006 	0.141499999999999987 	0.244999999999999996 	8 	
-2 	0.599999999999999978 	0.465000000000000024 	0.200000000000000011 	1.2589999999999999 	0.640499999999999958 	0.19850000000000001 	0.356999999999999984 	9 	
-1 	0.665000000000000036 	0.5 	0.170000000000000012 	1.2975000000000001 	0.603500000000000036 	0.290999999999999981 	0.359499999999999986 	9 	
-1 	0.340000000000000024 	0.260000000000000009 	0.0850000000000000061 	0.188500000000000001 	0.081500000000000003 	0.033500000000000002 	0.0599999999999999978 	6 	
-0 	0.75 	0.614999999999999991 	0.204999999999999988 	2.26350000000000007 	0.820999999999999952 	0.422999999999999987 	0.725999999999999979 	12 	
-0 	0.609999999999999987 	0.494999999999999996 	0.160000000000000003 	1.08899999999999997 	0.468999999999999972 	0.198000000000000009 	0.384000000000000008 	11 	
-2 	0.650000000000000022 	0.520000000000000018 	0.195000000000000007 	1.67599999999999993 	0.692999999999999949 	0.440000000000000002 	0.469999999999999973 	15 	
-1 	0.46000000000000002 	0.344999999999999973 	0.104999999999999996 	0.44900000000000001 	0.196000000000000008 	0.0945000000000000007 	0.126500000000000001 	7 	
-1 	0.455000000000000016 	0.349999999999999978 	0.104999999999999996 	0.444500000000000006 	0.212999999999999995 	0.106999999999999998 	0.111500000000000002 	7 	
-1 	0.359999999999999987 	0.270000000000000018 	0.0950000000000000011 	0.200000000000000011 	0.0729999999999999954 	0.0560000000000000012 	0.0609999999999999987 	8 	
-2 	0.650000000000000022 	0.484999999999999987 	0.140000000000000013 	1.17500000000000004 	0.474999999999999978 	0.243499999999999994 	0.214999999999999997 	8 	
-1 	0.479999999999999982 	0.364999999999999991 	0.100000000000000006 	0.461000000000000021 	0.220500000000000002 	0.0835000000000000048 	0.135000000000000009 	8 	
-1 	0.535000000000000031 	0.409999999999999976 	0.154999999999999999 	0.63149999999999995 	0.274500000000000022 	0.141499999999999987 	0.181499999999999995 	12 	
-0 	0.484999999999999987 	0.364999999999999991 	0.140000000000000013 	0.619500000000000051 	0.259500000000000008 	0.14449999999999999 	0.176999999999999991 	14 	
-1 	0.405000000000000027 	0.315000000000000002 	0.104999999999999996 	0.346999999999999975 	0.160500000000000004 	0.0785000000000000003 	0.100000000000000006 	9 	
-1 	0.299999999999999989 	0.234999999999999987 	0.0800000000000000017 	0.131000000000000005 	0.0500000000000000028 	0.0264999999999999993 	0.0429999999999999966 	4 	
-0 	0.520000000000000018 	0.41499999999999998 	0.14499999999999999 	0.804499999999999993 	0.332500000000000018 	0.172499999999999987 	0.284999999999999976 	10 	
-1 	0.41499999999999998 	0.309999999999999998 	0.100000000000000006 	0.280500000000000027 	0.114000000000000004 	0.0565000000000000016 	0.0975000000000000033 	6 	
-2 	0.325000000000000011 	0.239999999999999991 	0.0850000000000000061 	0.172999999999999987 	0.0795000000000000012 	0.0379999999999999991 	0.0500000000000000028 	7 	
-0 	0.57999999999999996 	0.450000000000000011 	0.184999999999999998 	0.995500000000000052 	0.394500000000000017 	0.27200000000000002 	0.284999999999999976 	11 	
-0 	0.550000000000000044 	0.450000000000000011 	0.149999999999999994 	0.875 	0.361999999999999988 	0.175499999999999989 	0.276500000000000024 	10 	
-0 	0.619999999999999996 	0.479999999999999982 	0.174999999999999989 	1.04049999999999998 	0.464000000000000024 	0.222500000000000003 	0.299999999999999989 	9 	
-1 	0.385000000000000009 	0.284999999999999976 	0.0850000000000000061 	0.243999999999999995 	0.121499999999999997 	0.0444999999999999979 	0.0680000000000000049 	8 	
-2 	0.5 	0.400000000000000022 	0.165000000000000008 	0.824999999999999956 	0.254000000000000004 	0.204999999999999988 	0.284999999999999976 	13 	
-2 	0.604999999999999982 	0.465000000000000024 	0.165000000000000008 	1.05600000000000005 	0.421499999999999986 	0.247499999999999998 	0.340000000000000024 	13 	
-2 	0.440000000000000002 	0.354999999999999982 	0.125 	0.47749999999999998 	0.132000000000000006 	0.081500000000000003 	0.190000000000000002 	9 	
-1 	0.239999999999999991 	0.174999999999999989 	0.0650000000000000022 	0.0665000000000000036 	0.0309999999999999998 	0.0134999999999999998 	0.0170000000000000012 	3 	
-1 	0.520000000000000018 	0.41499999999999998 	0.140000000000000013 	0.637499999999999956 	0.307999999999999996 	0.133500000000000008 	0.16800000000000001 	9 	
-0 	0.530000000000000027 	0.419999999999999984 	0.135000000000000009 	0.677000000000000046 	0.256500000000000006 	0.141499999999999987 	0.209999999999999992 	9 	
-2 	0.550000000000000044 	0.409999999999999976 	0.130000000000000004 	0.870500000000000052 	0.445500000000000007 	0.211499999999999994 	0.212999999999999995 	9 	
-1 	0.555000000000000049 	0.46000000000000002 	0.14499999999999999 	0.900499999999999967 	0.384500000000000008 	0.158000000000000002 	0.276500000000000024 	11 	
-2 	0.665000000000000036 	0.505000000000000004 	0.160000000000000003 	1.28899999999999992 	0.614500000000000046 	0.253000000000000003 	0.366499999999999992 	11 	
-1 	0.25 	0.184999999999999998 	0.0650000000000000022 	0.0685000000000000053 	0.0294999999999999984 	0.0140000000000000003 	0.0224999999999999992 	5 	
-2 	0.564999999999999947 	0.424999999999999989 	0.160000000000000003 	0.942500000000000004 	0.349499999999999977 	0.2185 	0.275000000000000022 	17 	
-0 	0.589999999999999969 	0.469999999999999973 	0.170000000000000012 	0.900000000000000022 	0.354999999999999982 	0.190500000000000003 	0.25 	11 	
-2 	0.405000000000000027 	0.309999999999999998 	0.100000000000000006 	0.385000000000000009 	0.172999999999999987 	0.091499999999999998 	0.110000000000000001 	7 	
-1 	0.260000000000000009 	0.214999999999999997 	0.0800000000000000017 	0.0990000000000000047 	0.0369999999999999982 	0.0254999999999999984 	0.0449999999999999983 	5 	
-1 	0.400000000000000022 	0.304999999999999993 	0.100000000000000006 	0.341500000000000026 	0.17599999999999999 	0.0625 	0.0864999999999999936 	7 	
-2 	0.694999999999999951 	0.569999999999999951 	0.200000000000000011 	2.03299999999999992 	0.751000000000000001 	0.425499999999999989 	0.685000000000000053 	15 	
-2 	0.560000000000000053 	0.445000000000000007 	0.160000000000000003 	0.896499999999999964 	0.419999999999999984 	0.217499999999999999 	0.221500000000000002 	8 	
-2 	0.609999999999999987 	0.484999999999999987 	0.170000000000000012 	1.02249999999999996 	0.418999999999999984 	0.240499999999999992 	0.359999999999999987 	12 	
-1 	0.375 	0.280000000000000027 	0.0800000000000000017 	0.234499999999999986 	0.112500000000000003 	0.0454999999999999988 	0.067000000000000004 	6 	
-0 	0.505000000000000004 	0.385000000000000009 	0.135000000000000009 	0.61850000000000005 	0.251000000000000001 	0.117499999999999993 	0.200000000000000011 	12 	
-0 	0.589999999999999969 	0.455000000000000016 	0.149999999999999994 	0.975999999999999979 	0.465000000000000024 	0.205499999999999988 	0.276500000000000024 	10 	
-1 	0.609999999999999987 	0.465000000000000024 	0.125 	0.922499999999999987 	0.435999999999999999 	0.190000000000000002 	0.260000000000000009 	9 	
-1 	0.375 	0.275000000000000022 	0.0850000000000000061 	0.220000000000000001 	0.109 	0.0500000000000000028 	0.0604999999999999982 	7 	
-2 	0.419999999999999984 	0.33500000000000002 	0.115000000000000005 	0.368999999999999995 	0.171000000000000013 	0.0709999999999999937 	0.119999999999999996 	8 	
-1 	0.344999999999999973 	0.270000000000000018 	0.110000000000000001 	0.213499999999999995 	0.0820000000000000034 	0.0544999999999999998 	0.0700000000000000067 	7 	
-2 	0.655000000000000027 	0.515000000000000013 	0.160000000000000003 	1.31000000000000005 	0.553000000000000047 	0.368999999999999995 	0.344999999999999973 	11 	
-2 	0.484999999999999987 	0.354999999999999982 	0.119999999999999996 	0.547000000000000042 	0.214999999999999997 	0.161500000000000005 	0.140000000000000013 	10 	
-1 	0.349999999999999978 	0.260000000000000009 	0.0950000000000000011 	0.210999999999999993 	0.0859999999999999931 	0.0560000000000000012 	0.0680000000000000049 	7 	
-2 	0.675000000000000044 	0.525000000000000022 	0.184999999999999998 	1.58699999999999997 	0.693500000000000005 	0.336000000000000021 	0.395000000000000018 	13 	
-0 	0.515000000000000013 	0.400000000000000022 	0.170000000000000012 	0.796000000000000041 	0.258000000000000007 	0.175499999999999989 	0.280000000000000027 	16 	
-1 	0.424999999999999989 	0.309999999999999998 	0.104999999999999996 	0.364999999999999991 	0.159000000000000002 	0.0825000000000000039 	0.104999999999999996 	6 	
-1 	0.179999999999999993 	0.130000000000000004 	0.0449999999999999983 	0.0275000000000000001 	0.0125000000000000007 	0.0100000000000000002 	0.00899999999999999932 	3 	
-0 	0.640000000000000013 	0.474999999999999978 	0.140000000000000013 	1.07250000000000001 	0.489499999999999991 	0.22950000000000001 	0.309999999999999998 	8 	
-0 	0.694999999999999951 	0.564999999999999947 	0.190000000000000002 	1.76350000000000007 	0.746500000000000052 	0.399000000000000021 	0.497499999999999998 	11 	
-2 	0.560000000000000053 	0.455000000000000016 	0.165000000000000008 	0.859999999999999987 	0.401500000000000024 	0.169500000000000012 	0.244999999999999996 	11 	
-0 	0.525000000000000022 	0.409999999999999976 	0.135000000000000009 	0.79049999999999998 	0.406499999999999972 	0.198000000000000009 	0.176999999999999991 	8 	
-1 	0.550000000000000044 	0.434999999999999998 	0.140000000000000013 	0.753499999999999948 	0.328500000000000014 	0.155499999999999999 	0.232500000000000012 	10 	
-1 	0.619999999999999996 	0.489999999999999991 	0.160000000000000003 	1.06600000000000006 	0.446000000000000008 	0.245999999999999996 	0.304999999999999993 	11 	
-0 	0.469999999999999973 	0.349999999999999978 	0.14499999999999999 	0.51749999999999996 	0.187 	0.123499999999999999 	0.179999999999999993 	11 	
-1 	0.619999999999999996 	0.484999999999999987 	0.179999999999999993 	1.1785000000000001 	0.467500000000000027 	0.265500000000000014 	0.390000000000000013 	13 	
-0 	0.589999999999999969 	0.465000000000000024 	0.149999999999999994 	1.15100000000000002 	0.612999999999999989 	0.23899999999999999 	0.251500000000000001 	9 	
-2 	0.57999999999999996 	0.465000000000000024 	0.174999999999999989 	1.03499999999999992 	0.401000000000000023 	0.186499999999999999 	0.385000000000000009 	17 	
-2 	0.694999999999999951 	0.525000000000000022 	0.174999999999999989 	1.74199999999999999 	0.695999999999999952 	0.389000000000000012 	0.505000000000000004 	12 	
-2 	0.619999999999999996 	0.505000000000000004 	0.184999999999999998 	1.52750000000000008 	0.689999999999999947 	0.367999999999999994 	0.349999999999999978 	13 	
-2 	0.380000000000000004 	0.284999999999999976 	0.100000000000000006 	0.266500000000000015 	0.115000000000000005 	0.0609999999999999987 	0.0749999999999999972 	11 	
-2 	0.67000000000000004 	0.54500000000000004 	0.200000000000000011 	1.7024999999999999 	0.832999999999999963 	0.373999999999999999 	0.409999999999999976 	11 	
-2 	0.630000000000000004 	0.494999999999999996 	0.179999999999999993 	1.31000000000000005 	0.494999999999999996 	0.294999999999999984 	0.469499999999999973 	10 	
-0 	0.70499999999999996 	0.535000000000000031 	0.179999999999999993 	1.68500000000000005 	0.692999999999999949 	0.419999999999999984 	0.404500000000000026 	12 	
-2 	0.594999999999999973 	0.434999999999999998 	0.160000000000000003 	1.05699999999999994 	0.425499999999999989 	0.224000000000000005 	0.309999999999999998 	9 	
-1 	0.505000000000000004 	0.390000000000000013 	0.184999999999999998 	0.612500000000000044 	0.267000000000000015 	0.141999999999999987 	0.171999999999999986 	7 	
-1 	0.380000000000000004 	0.284999999999999976 	0.0850000000000000061 	0.236999999999999988 	0.115000000000000005 	0.0405000000000000013 	0.0700000000000000067 	6 	
-2 	0.489999999999999991 	0.419999999999999984 	0.125 	0.608999999999999986 	0.23899999999999999 	0.143499999999999989 	0.220000000000000001 	14 	
-0 	0.660000000000000031 	0.525000000000000022 	0.204999999999999988 	1.36650000000000005 	0.500499999999999945 	0.290999999999999981 	0.409999999999999976 	18 	
-1 	0.330000000000000016 	0.255000000000000004 	0.0850000000000000061 	0.165500000000000008 	0.0630000000000000004 	0.0389999999999999999 	0.0599999999999999978 	8 	
-2 	0.584999999999999964 	0.405000000000000027 	0.149999999999999994 	1.25649999999999995 	0.434999999999999998 	0.202000000000000013 	0.325000000000000011 	15 	
-2 	0.474999999999999978 	0.369999999999999996 	0.125 	0.509499999999999953 	0.216499999999999998 	0.112500000000000003 	0.165000000000000008 	9 	
-0 	0.660000000000000031 	0.520000000000000018 	0.179999999999999993 	1.51400000000000001 	0.526000000000000023 	0.297499999999999987 	0.419999999999999984 	19 	
-2 	0.510000000000000009 	0.405000000000000027 	0.130000000000000004 	0.717500000000000027 	0.372499999999999998 	0.158000000000000002 	0.170000000000000012 	9 	
-1 	0.584999999999999964 	0.450000000000000011 	0.149999999999999994 	0.891499999999999959 	0.39750000000000002 	0.203499999999999986 	0.253000000000000003 	8 	
-0 	0.560000000000000053 	0.445000000000000007 	0.154999999999999999 	1.22399999999999998 	0.556499999999999995 	0.322500000000000009 	0.269500000000000017 	10 	
-2 	0.469999999999999973 	0.385000000000000009 	0.135000000000000009 	0.589500000000000024 	0.276500000000000024 	0.119999999999999996 	0.170000000000000012 	8 	
-0 	0.619999999999999996 	0.494999999999999996 	0.170000000000000012 	1.06200000000000006 	0.371999999999999997 	0.212999999999999995 	0.340000000000000024 	11 	
-2 	0.614999999999999991 	0.479999999999999982 	0.179999999999999993 	1.15949999999999998 	0.484499999999999986 	0.216499999999999998 	0.325000000000000011 	13 	
-2 	0.515000000000000013 	0.400000000000000022 	0.160000000000000003 	0.817500000000000004 	0.251500000000000001 	0.156 	0.299999999999999989 	23 	
-1 	0.520000000000000018 	0.400000000000000022 	0.14499999999999999 	0.660000000000000031 	0.267000000000000015 	0.105499999999999997 	0.220000000000000001 	13 	
-1 	0.46000000000000002 	0.349999999999999978 	0.110000000000000001 	0.467500000000000027 	0.212499999999999994 	0.0990000000000000047 	0.137500000000000011 	7 	
-1 	0.275000000000000022 	0.195000000000000007 	0.0650000000000000022 	0.105999999999999997 	0.0539999999999999994 	0.0200000000000000004 	0.0280000000000000006 	6 	
-1 	0.469999999999999973 	0.349999999999999978 	0.130000000000000004 	0.466000000000000025 	0.184499999999999997 	0.0990000000000000047 	0.14499999999999999 	11 	
-0 	0.469999999999999973 	0.354999999999999982 	0.100000000000000006 	0.475499999999999978 	0.16750000000000001 	0.0805000000000000021 	0.184999999999999998 	10 	
-0 	0.729999999999999982 	0.569999999999999951 	0.165000000000000008 	2.01650000000000018 	1.06850000000000001 	0.417999999999999983 	0.434999999999999998 	10 	
-1 	0.469999999999999973 	0.375 	0.119999999999999996 	0.486999999999999988 	0.196000000000000008 	0.0990000000000000047 	0.135000000000000009 	8 	
-0 	0.689999999999999947 	0.560000000000000053 	0.214999999999999997 	1.71900000000000008 	0.680000000000000049 	0.298999999999999988 	0.469999999999999973 	17 	
-0 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	1.07650000000000001 	0.490999999999999992 	0.220000000000000001 	0.286999999999999977 	9 	
-2 	0.569999999999999951 	0.419999999999999984 	0.154999999999999999 	1.00800000000000001 	0.377000000000000002 	0.193000000000000005 	0.340000000000000024 	13 	
-2 	0.555000000000000049 	0.455000000000000016 	0.160000000000000003 	1.05750000000000011 	0.392500000000000016 	0.228000000000000008 	0.292999999999999983 	13 	
-2 	0.650000000000000022 	0.5 	0.140000000000000013 	1.23799999999999999 	0.616500000000000048 	0.235499999999999987 	0.320000000000000007 	8 	
-0 	0.555000000000000049 	0.424999999999999989 	0.140000000000000013 	0.788000000000000034 	0.281999999999999973 	0.159500000000000003 	0.284999999999999976 	11 	
-1 	0.455000000000000016 	0.369999999999999996 	0.125 	0.432999999999999996 	0.201000000000000012 	0.126500000000000001 	0.14499999999999999 	9 	
-2 	0.604999999999999982 	0.474999999999999978 	0.14499999999999999 	0.884000000000000008 	0.383500000000000008 	0.190500000000000003 	0.270000000000000018 	8 	
-2 	0.505000000000000004 	0.385000000000000009 	0.104999999999999996 	0.552499999999999991 	0.23899999999999999 	0.1245 	0.155499999999999999 	9 	
-0 	0.694999999999999951 	0.535000000000000031 	0.174999999999999989 	1.83850000000000002 	0.803499999999999992 	0.396000000000000019 	0.503000000000000003 	10 	
-2 	0.630000000000000004 	0.489999999999999991 	0.165000000000000008 	1.2004999999999999 	0.574999999999999956 	0.27300000000000002 	0.293999999999999984 	10 	
-1 	0.364999999999999991 	0.270000000000000018 	0.0749999999999999972 	0.221500000000000002 	0.0950000000000000011 	0.0444999999999999979 	0.0700000000000000067 	6 	
-1 	0.46000000000000002 	0.344999999999999973 	0.110000000000000001 	0.3755 	0.152499999999999997 	0.0580000000000000029 	0.125 	7 	
-1 	0.214999999999999997 	0.154999999999999999 	0.0599999999999999978 	0.0524999999999999981 	0.0210000000000000013 	0.0165000000000000008 	0.0149999999999999994 	5 	
-0 	0.525000000000000022 	0.41499999999999998 	0.140000000000000013 	0.723999999999999977 	0.347499999999999976 	0.172999999999999987 	0.174999999999999989 	8 	
-0 	0.604999999999999982 	0.5 	0.184999999999999998 	1.11850000000000005 	0.468999999999999972 	0.258500000000000008 	0.33500000000000002 	9 	
-0 	0.57999999999999996 	0.450000000000000011 	0.154999999999999999 	0.930000000000000049 	0.385000000000000009 	0.245999999999999996 	0.265000000000000013 	9 	
-2 	0.57999999999999996 	0.450000000000000011 	0.149999999999999994 	0.927000000000000046 	0.276000000000000023 	0.181499999999999995 	0.359999999999999987 	14 	
-2 	0.694999999999999951 	0.560000000000000053 	0.190000000000000002 	1.49399999999999999 	0.587999999999999967 	0.342500000000000027 	0.484999999999999987 	15 	
-2 	0.555000000000000049 	0.440000000000000002 	0.140000000000000013 	0.870500000000000052 	0.406999999999999973 	0.156 	0.255000000000000004 	9 	
-1 	0.550000000000000044 	0.419999999999999984 	0.130000000000000004 	0.63600000000000001 	0.293999999999999984 	0.143999999999999989 	0.175499999999999989 	8 	
-2 	0.589999999999999969 	0.455000000000000016 	0.14499999999999999 	1.07299999999999995 	0.474999999999999978 	0.190000000000000002 	0.284999999999999976 	14 	
-0 	0.635000000000000009 	0.484999999999999987 	0.165000000000000008 	1.26950000000000007 	0.563500000000000001 	0.306499999999999995 	0.339500000000000024 	11 	
-0 	0.429999999999999993 	0.340000000000000024 	0.119999999999999996 	0.391000000000000014 	0.155499999999999999 	0.0950000000000000011 	0.140500000000000014 	7 	
-1 	0.299999999999999989 	0.23000000000000001 	0.0850000000000000061 	0.117000000000000007 	0.0500000000000000028 	0.0175000000000000017 	0.0415000000000000022 	6 	
-0 	0.645000000000000018 	0.510000000000000009 	0.190000000000000002 	1.36299999999999999 	0.572999999999999954 	0.361999999999999988 	0.359999999999999987 	10 	
-1 	0.364999999999999991 	0.275000000000000022 	0.135000000000000009 	0.239999999999999991 	0.107999999999999999 	0.0444999999999999979 	0.0734999999999999959 	7 	
-2 	0.569999999999999951 	0.450000000000000011 	0.154999999999999999 	1.19350000000000001 	0.513000000000000012 	0.209999999999999992 	0.343000000000000027 	10 	
-0 	0.699999999999999956 	0.525000000000000022 	0.190000000000000002 	1.60149999999999992 	0.706999999999999962 	0.364999999999999991 	0.429999999999999993 	10 	
-0 	0.599999999999999978 	0.450000000000000011 	0.149999999999999994 	0.962500000000000022 	0.4375 	0.222500000000000003 	0.277500000000000024 	9 	
-1 	0.214999999999999997 	0.149999999999999994 	0.0299999999999999989 	0.0384999999999999995 	0.0114999999999999998 	0.0050000000000000001 	0.0100000000000000002 	5 	
-0 	0.5 	0.375 	0.115000000000000005 	0.594500000000000028 	0.184999999999999998 	0.147999999999999993 	0.190000000000000002 	11 	
-1 	0.540000000000000036 	0.41499999999999998 	0.110000000000000001 	0.618999999999999995 	0.275500000000000023 	0.149999999999999994 	0.17649999999999999 	10 	
-0 	0.505000000000000004 	0.390000000000000013 	0.115000000000000005 	0.660000000000000031 	0.304499999999999993 	0.155499999999999999 	0.174999999999999989 	8 	
-0 	0.489999999999999991 	0.359999999999999987 	0.110000000000000001 	0.500499999999999945 	0.161000000000000004 	0.106999999999999998 	0.195000000000000007 	17 	
-0 	0.609999999999999987 	0.465000000000000024 	0.160000000000000003 	1.07250000000000001 	0.483499999999999985 	0.251500000000000001 	0.280000000000000027 	10 	
-1 	0.46000000000000002 	0.369999999999999996 	0.110000000000000001 	0.396500000000000019 	0.148499999999999993 	0.0855000000000000066 	0.14549999999999999 	8 	
-0 	0.564999999999999947 	0.455000000000000016 	0.174999999999999989 	1.0129999999999999 	0.342000000000000026 	0.20699999999999999 	0.349999999999999978 	19 	
-2 	0.57999999999999996 	0.445000000000000007 	0.149999999999999994 	0.952500000000000013 	0.431499999999999995 	0.194500000000000006 	0.286999999999999977 	11 	
-0 	0.680000000000000049 	0.515000000000000013 	0.174999999999999989 	1.61850000000000005 	0.512499999999999956 	0.408999999999999975 	0.619999999999999996 	12 	
-0 	0.655000000000000027 	0.5 	0.204999999999999988 	1.52800000000000002 	0.621500000000000052 	0.372499999999999998 	0.453500000000000014 	11 	
-0 	0.689999999999999947 	0.550000000000000044 	0.179999999999999993 	1.65900000000000003 	0.871500000000000052 	0.265500000000000014 	0.439500000000000002 	9 	
-0 	0.535000000000000031 	0.409999999999999976 	0.130000000000000004 	0.714500000000000024 	0.33500000000000002 	0.143999999999999989 	0.20749999999999999 	9 	
-1 	0.320000000000000007 	0.239999999999999991 	0.0899999999999999967 	0.157500000000000001 	0.0700000000000000067 	0.0264999999999999993 	0.0425000000000000031 	5 	
-1 	0.330000000000000016 	0.25 	0.104999999999999996 	0.171500000000000014 	0.0655000000000000027 	0.0350000000000000033 	0.0599999999999999978 	7 	
-0 	0.699999999999999956 	0.574999999999999956 	0.204999999999999988 	1.77299999999999991 	0.604999999999999982 	0.447000000000000008 	0.538000000000000034 	13 	
-1 	0.510000000000000009 	0.400000000000000022 	0.149999999999999994 	0.744999999999999996 	0.286499999999999977 	0.16750000000000001 	0.234999999999999987 	13 	
-2 	0.655000000000000027 	0.54500000000000004 	0.190000000000000002 	1.4245000000000001 	0.632499999999999951 	0.333000000000000018 	0.378000000000000003 	10 	
-0 	0.574999999999999956 	0.419999999999999984 	0.135000000000000009 	0.856999999999999984 	0.461000000000000021 	0.146999999999999992 	0.212499999999999994 	10 	
-2 	0.489999999999999991 	0.390000000000000013 	0.135000000000000009 	0.591999999999999971 	0.241999999999999993 	0.096000000000000002 	0.183499999999999996 	15 	
-0 	0.619999999999999996 	0.520000000000000018 	0.225000000000000006 	1.1835 	0.378000000000000003 	0.270000000000000018 	0.395000000000000018 	23 	
-2 	0.589999999999999969 	0.46000000000000002 	0.140000000000000013 	1.004 	0.495999999999999996 	0.216499999999999998 	0.260000000000000009 	9 	
-0 	0.724999999999999978 	0.569999999999999951 	0.204999999999999988 	1.61949999999999994 	0.743999999999999995 	0.315000000000000002 	0.487999999999999989 	11 	
-2 	0.630000000000000004 	0.455000000000000016 	0.149999999999999994 	1.13149999999999995 	0.480999999999999983 	0.274500000000000022 	0.304999999999999993 	9 	
-0 	0.57999999999999996 	0.46000000000000002 	0.184999999999999998 	1.0169999999999999 	0.351499999999999979 	0.200000000000000011 	0.320000000000000007 	10 	
-0 	0.515000000000000013 	0.424999999999999989 	0.140000000000000013 	0.766000000000000014 	0.303999999999999992 	0.172499999999999987 	0.255000000000000004 	14 	
-0 	0.46000000000000002 	0.364999999999999991 	0.125 	0.478499999999999981 	0.205999999999999989 	0.104499999999999996 	0.140999999999999986 	8 	
-0 	0.619999999999999996 	0.510000000000000009 	0.149999999999999994 	1.45599999999999996 	0.580999999999999961 	0.287499999999999978 	0.320000000000000007 	13 	
-2 	0.540000000000000036 	0.419999999999999984 	0.154999999999999999 	0.738500000000000045 	0.351499999999999979 	0.151999999999999996 	0.214999999999999997 	12 	
-0 	0.609999999999999987 	0.479999999999999982 	0.174999999999999989 	1.06749999999999989 	0.391000000000000014 	0.215999999999999998 	0.419999999999999984 	15 	
-0 	0.650000000000000022 	0.54500000000000004 	0.174999999999999989 	1.52449999999999997 	0.589999999999999969 	0.326000000000000012 	0.494999999999999996 	10 	
-2 	0.599999999999999978 	0.465000000000000024 	0.179999999999999993 	1.19300000000000006 	0.514499999999999957 	0.315000000000000002 	0.305499999999999994 	8 	
-0 	0.564999999999999947 	0.440000000000000002 	0.135000000000000009 	0.82999999999999996 	0.393000000000000016 	0.173499999999999988 	0.237999999999999989 	9 	
-1 	0.315000000000000002 	0.239999999999999991 	0.0700000000000000067 	0.137000000000000011 	0.0544999999999999998 	0.0315000000000000002 	0.0400000000000000008 	8 	
-0 	0.744999999999999996 	0.584999999999999964 	0.190000000000000002 	1.96599999999999997 	0.843500000000000028 	0.437 	0.58550000000000002 	18 	
-0 	0.675000000000000044 	0.525000000000000022 	0.170000000000000012 	1.71100000000000008 	0.836500000000000021 	0.35199999999999998 	0.474999999999999978 	9 	
-1 	0.469999999999999973 	0.344999999999999973 	0.140000000000000013 	0.461500000000000021 	0.229000000000000009 	0.110500000000000001 	0.116000000000000006 	9 	
-1 	0.424999999999999989 	0.325000000000000011 	0.110000000000000001 	0.317000000000000004 	0.135000000000000009 	0.048000000000000001 	0.0899999999999999967 	8 	
-2 	0.675000000000000044 	0.505000000000000004 	0.160000000000000003 	1.53200000000000003 	0.739999999999999991 	0.356999999999999984 	0.381500000000000006 	11 	
-1 	0.550000000000000044 	0.445000000000000007 	0.14499999999999999 	0.783000000000000029 	0.304499999999999993 	0.157000000000000001 	0.265000000000000013 	11 	
-1 	0.574999999999999956 	0.429999999999999993 	0.130000000000000004 	0.742500000000000049 	0.28949999999999998 	0.200500000000000012 	0.220000000000000001 	8 	
-2 	0.619999999999999996 	0.474999999999999978 	0.184999999999999998 	1.32499999999999996 	0.604500000000000037 	0.325000000000000011 	0.330000000000000016 	13 	
-2 	0.505000000000000004 	0.364999999999999991 	0.115000000000000005 	0.521000000000000019 	0.25 	0.096000000000000002 	0.149999999999999994 	8 	
-2 	0.584999999999999964 	0.46000000000000002 	0.165000000000000008 	1.11349999999999993 	0.582500000000000018 	0.234499999999999986 	0.274000000000000021 	10 	
-1 	0.375 	0.280000000000000027 	0.0899999999999999967 	0.214999999999999997 	0.0840000000000000052 	0.0599999999999999978 	0.0550000000000000003 	6 	
-1 	0.280000000000000027 	0.209999999999999992 	0.0850000000000000061 	0.106499999999999997 	0.0389999999999999999 	0.0294999999999999984 	0.0299999999999999989 	4 	
-0 	0.739999999999999991 	0.564999999999999947 	0.204999999999999988 	2.11900000000000022 	0.965500000000000025 	0.518499999999999961 	0.481999999999999984 	12 	
-2 	0.564999999999999947 	0.465000000000000024 	0.174999999999999989 	0.994999999999999996 	0.389500000000000013 	0.182999999999999996 	0.369999999999999996 	15 	
-0 	0.520000000000000018 	0.400000000000000022 	0.125 	0.686499999999999999 	0.294999999999999984 	0.171500000000000014 	0.184999999999999998 	9 	
-1 	0.424999999999999989 	0.325000000000000011 	0.115000000000000005 	0.368499999999999994 	0.162000000000000005 	0.0864999999999999936 	0.104499999999999996 	7 	
-2 	0.569999999999999951 	0.434999999999999998 	0.125 	0.896499999999999964 	0.383000000000000007 	0.183499999999999996 	0.275000000000000022 	9 	
-2 	0.614999999999999991 	0.494999999999999996 	0.200000000000000011 	1.21900000000000008 	0.563999999999999946 	0.227000000000000007 	0.388500000000000012 	10 	
-0 	0.685000000000000053 	0.530000000000000027 	0.170000000000000012 	1.51049999999999995 	0.738500000000000045 	0.35249999999999998 	0.372499999999999998 	10 	
-2 	0.589999999999999969 	0.474999999999999978 	0.140000000000000013 	0.97699999999999998 	0.462500000000000022 	0.202500000000000013 	0.275000000000000022 	10 	
-2 	0.424999999999999989 	0.325000000000000011 	0.119999999999999996 	0.3755 	0.141999999999999987 	0.106499999999999997 	0.104999999999999996 	9 	
-0 	0.729999999999999982 	0.574999999999999956 	0.184999999999999998 	1.87949999999999995 	0.93100000000000005 	0.380000000000000004 	0.482499999999999984 	12 	
-1 	0.555000000000000049 	0.455000000000000016 	0.170000000000000012 	0.843500000000000028 	0.308999999999999997 	0.190500000000000003 	0.299999999999999989 	15 	
-1 	0.594999999999999973 	0.46000000000000002 	0.149999999999999994 	0.833500000000000019 	0.377000000000000002 	0.192500000000000004 	0.234999999999999987 	8 	
-1 	0.57999999999999996 	0.445000000000000007 	0.125 	0.70950000000000002 	0.302999999999999992 	0.140500000000000014 	0.234999999999999987 	9 	
-2 	0.630000000000000004 	0.505000000000000004 	0.149999999999999994 	1.3165 	0.632499999999999951 	0.246499999999999997 	0.369999999999999996 	11 	
-2 	0.574999999999999956 	0.455000000000000016 	0.135000000000000009 	0.907000000000000028 	0.424499999999999988 	0.197000000000000008 	0.260000000000000009 	9 	
-1 	0.450000000000000011 	0.33500000000000002 	0.110000000000000001 	0.419499999999999984 	0.180999999999999994 	0.0850000000000000061 	0.134500000000000008 	7 	
-0 	0.5 	0.400000000000000022 	0.140000000000000013 	0.661499999999999977 	0.256500000000000006 	0.175499999999999989 	0.220000000000000001 	8 	
-0 	0.574999999999999956 	0.450000000000000011 	0.170000000000000012 	1.0475000000000001 	0.377500000000000002 	0.170500000000000013 	0.385000000000000009 	18 	
-0 	0.625 	0.489999999999999991 	0.174999999999999989 	1.2330000000000001 	0.556499999999999995 	0.246999999999999997 	0.364999999999999991 	11 	
-2 	0.635000000000000009 	0.510000000000000009 	0.154999999999999999 	0.985999999999999988 	0.405000000000000027 	0.225500000000000006 	0.309999999999999998 	10 	
-0 	0.550000000000000044 	0.41499999999999998 	0.135000000000000009 	0.775000000000000022 	0.301999999999999991 	0.178999999999999992 	0.260000000000000009 	23 	
-2 	0.57999999999999996 	0.450000000000000011 	0.174999999999999989 	1.06800000000000006 	0.424999999999999989 	0.203000000000000014 	0.320000000000000007 	13 	
-1 	0.484999999999999987 	0.369999999999999996 	0.115000000000000005 	0.457000000000000017 	0.188500000000000001 	0.0965000000000000024 	0.149999999999999994 	9 	
-0 	0.400000000000000022 	0.304999999999999993 	0.160000000000000003 	0.367999999999999994 	0.172999999999999987 	0.0704999999999999932 	0.104999999999999996 	7 	
-2 	0.660000000000000031 	0.515000000000000013 	0.195000000000000007 	1.56549999999999989 	0.734500000000000042 	0.35299999999999998 	0.38600000000000001 	9 	
-2 	0.469999999999999973 	0.359999999999999987 	0.104999999999999996 	0.544000000000000039 	0.270000000000000018 	0.139500000000000013 	0.129000000000000004 	7 	
-2 	0.75 	0.594999999999999973 	0.204999999999999988 	2.22049999999999992 	1.08299999999999996 	0.420999999999999985 	0.630000000000000004 	12 	
-2 	0.569999999999999951 	0.450000000000000011 	0.14499999999999999 	0.949999999999999956 	0.400500000000000023 	0.223500000000000004 	0.284499999999999975 	10 	
-0 	0.645000000000000018 	0.525000000000000022 	0.190000000000000002 	1.46350000000000002 	0.661499999999999977 	0.343500000000000028 	0.434999999999999998 	19 	
-2 	0.699999999999999956 	0.550000000000000044 	0.195000000000000007 	1.62450000000000006 	0.675000000000000044 	0.346999999999999975 	0.535000000000000031 	13 	
-1 	0.474999999999999978 	0.364999999999999991 	0.125 	0.546499999999999986 	0.229000000000000009 	0.118499999999999994 	0.171999999999999986 	9 	
-1 	0.599999999999999978 	0.445000000000000007 	0.135000000000000009 	0.920499999999999985 	0.445000000000000007 	0.203499999999999986 	0.253000000000000003 	9 	
-1 	0.244999999999999996 	0.190000000000000002 	0.0599999999999999978 	0.0859999999999999931 	0.0420000000000000026 	0.0140000000000000003 	0.0250000000000000014 	4 	
-1 	0.619999999999999996 	0.484999999999999987 	0.179999999999999993 	1.15399999999999991 	0.493499999999999994 	0.256000000000000005 	0.315000000000000002 	12 	
-2 	0.309999999999999998 	0.225000000000000006 	0.0749999999999999972 	0.129500000000000004 	0.0454999999999999988 	0.033500000000000002 	0.0439999999999999974 	9 	
-2 	0.584999999999999964 	0.474999999999999978 	0.119999999999999996 	0.944999999999999951 	0.409999999999999976 	0.211499999999999994 	0.280000000000000027 	14 	
-2 	0.660000000000000031 	0.535000000000000031 	0.190000000000000002 	1.59050000000000002 	0.64249999999999996 	0.296999999999999986 	0.51749999999999996 	9 	
-0 	0.574999999999999956 	0.445000000000000007 	0.135000000000000009 	0.883000000000000007 	0.381000000000000005 	0.203499999999999986 	0.260000000000000009 	11 	
-2 	0.525000000000000022 	0.419999999999999984 	0.154999999999999999 	0.841999999999999971 	0.427999999999999992 	0.141499999999999987 	0.204499999999999987 	9 	
-0 	0.685000000000000053 	0.535000000000000031 	0.174999999999999989 	1.58450000000000002 	0.717500000000000027 	0.377500000000000002 	0.421499999999999986 	9 	
-1 	0.275000000000000022 	0.204999999999999988 	0.0700000000000000067 	0.105499999999999997 	0.494999999999999996 	0.0189999999999999995 	0.0315000000000000002 	5 	
-1 	0.280000000000000027 	0.214999999999999997 	0.0800000000000000017 	0.132000000000000006 	0.0719999999999999946 	0.0219999999999999987 	0.0330000000000000016 	5 	
-1 	0.54500000000000004 	0.419999999999999984 	0.165000000000000008 	0.893499999999999961 	0.423499999999999988 	0.219500000000000001 	0.228000000000000008 	8 	
-2 	0.5 	0.390000000000000013 	0.14499999999999999 	0.651000000000000023 	0.27300000000000002 	0.132000000000000006 	0.220000000000000001 	11 	
-0 	0.675000000000000044 	0.569999999999999951 	0.225000000000000006 	1.58699999999999997 	0.73899999999999999 	0.299499999999999988 	0.434999999999999998 	10 	
-0 	0.640000000000000013 	0.5 	0.174999999999999989 	1.39399999999999991 	0.493499999999999994 	0.290999999999999981 	0.400000000000000022 	10 	
-2 	0.535000000000000031 	0.405000000000000027 	0.140000000000000013 	0.817999999999999949 	0.402000000000000024 	0.171500000000000014 	0.189000000000000001 	7 	
-1 	0.400000000000000022 	0.28999999999999998 	0.110000000000000001 	0.329000000000000015 	0.188 	0.0454999999999999988 	0.0825000000000000039 	6 	
-0 	0.505000000000000004 	0.390000000000000013 	0.160000000000000003 	0.644000000000000017 	0.247499999999999998 	0.202500000000000013 	0.163500000000000006 	9 	
-1 	0.440000000000000002 	0.304999999999999993 	0.115000000000000005 	0.379000000000000004 	0.162000000000000005 	0.0909999999999999976 	0.110000000000000001 	9 	
-2 	0.54500000000000004 	0.419999999999999984 	0.130000000000000004 	0.879000000000000004 	0.373999999999999999 	0.169500000000000012 	0.23000000000000001 	13 	
-1 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.871500000000000052 	0.450000000000000011 	0.162000000000000005 	0.225000000000000006 	10 	
-1 	0.515000000000000013 	0.400000000000000022 	0.119999999999999996 	0.65900000000000003 	0.270500000000000018 	0.178999999999999992 	0.170000000000000012 	13 	
-1 	0.419999999999999984 	0.325000000000000011 	0.100000000000000006 	0.367999999999999994 	0.16750000000000001 	0.0625 	0.113500000000000004 	11 	
-1 	0.474999999999999978 	0.380000000000000004 	0.119999999999999996 	0.441000000000000003 	0.178499999999999992 	0.0884999999999999953 	0.150499999999999995 	8 	
-2 	0.640000000000000013 	0.505000000000000004 	0.154999999999999999 	1.40250000000000008 	0.70499999999999996 	0.265500000000000014 	0.33500000000000002 	10 	
-1 	0.294999999999999984 	0.214999999999999997 	0.0700000000000000067 	0.120999999999999996 	0.0470000000000000001 	0.0154999999999999999 	0.0405000000000000013 	6 	
-1 	0.494999999999999996 	0.330000000000000016 	0.100000000000000006 	0.440000000000000002 	0.176999999999999991 	0.0950000000000000011 	0.149999999999999994 	7 	
-1 	0.304999999999999993 	0.244999999999999996 	0.0749999999999999972 	0.156 	0.0675000000000000044 	0.0379999999999999991 	0.0449999999999999983 	7 	
-1 	0.619999999999999996 	0.469999999999999973 	0.140000000000000013 	0.856500000000000039 	0.359499999999999986 	0.160000000000000003 	0.294999999999999984 	9 	
-2 	0.5 	0.400000000000000022 	0.130000000000000004 	0.771499999999999964 	0.369999999999999996 	0.160000000000000003 	0.210999999999999993 	8 	
-1 	0.535000000000000031 	0.400000000000000022 	0.130000000000000004 	0.657000000000000028 	0.283499999999999974 	0.162000000000000005 	0.174999999999999989 	7 	
-0 	0.574999999999999956 	0.484999999999999987 	0.165000000000000008 	1.04049999999999998 	0.418999999999999984 	0.264000000000000012 	0.299999999999999989 	14 	
-1 	0.489999999999999991 	0.380000000000000004 	0.119999999999999996 	0.529000000000000026 	0.216499999999999998 	0.139000000000000012 	0.154999999999999999 	11 	
-1 	0.569999999999999951 	0.429999999999999993 	0.14499999999999999 	0.832999999999999963 	0.353999999999999981 	0.143999999999999989 	0.281499999999999972 	10 	
-2 	0.255000000000000004 	0.179999999999999993 	0.0650000000000000022 	0.0790000000000000008 	0.0340000000000000024 	0.0140000000000000003 	0.0250000000000000014 	5 	
-2 	0.57999999999999996 	0.46000000000000002 	0.160000000000000003 	1.06299999999999994 	0.513000000000000012 	0.270500000000000018 	0.262500000000000011 	9 	
-0 	0.494999999999999996 	0.409999999999999976 	0.125 	0.755499999999999949 	0.33550000000000002 	0.129000000000000004 	0.213999999999999996 	9 	
-2 	0.455000000000000016 	0.349999999999999978 	0.110000000000000001 	0.458000000000000018 	0.200000000000000011 	0.111000000000000001 	0.130500000000000005 	8 	
-2 	0.709999999999999964 	0.564999999999999947 	0.200000000000000011 	1.60099999999999998 	0.705999999999999961 	0.321000000000000008 	0.450000000000000011 	11 	
-1 	0.609999999999999987 	0.465000000000000024 	0.149999999999999994 	0.96050000000000002 	0.449500000000000011 	0.172499999999999987 	0.285999999999999976 	9 	
-0 	0.520000000000000018 	0.424999999999999989 	0.165000000000000008 	0.988500000000000045 	0.396000000000000019 	0.225000000000000006 	0.320000000000000007 	16 	
-2 	0.560000000000000053 	0.455000000000000016 	0.154999999999999999 	0.797000000000000042 	0.340000000000000024 	0.190000000000000002 	0.242499999999999993 	11 	
-2 	0.520000000000000018 	0.400000000000000022 	0.119999999999999996 	0.822999999999999954 	0.297999999999999987 	0.180499999999999994 	0.265000000000000013 	15 	
-1 	0.369999999999999996 	0.28999999999999998 	0.0899999999999999967 	0.244499999999999995 	0.0889999999999999958 	0.0655000000000000027 	0.0749999999999999972 	7 	
-0 	0.505000000000000004 	0.390000000000000013 	0.130000000000000004 	0.674000000000000044 	0.316500000000000004 	0.140999999999999986 	0.178499999999999992 	9 	
-2 	0.645000000000000018 	0.489999999999999991 	0.160000000000000003 	1.25099999999999989 	0.535499999999999976 	0.33450000000000002 	0.316500000000000004 	9 	
-0 	0.515000000000000013 	0.434999999999999998 	0.170000000000000012 	0.631000000000000005 	0.276500000000000024 	0.111000000000000001 	0.215999999999999998 	12 	
-0 	0.525000000000000022 	0.419999999999999984 	0.160000000000000003 	0.756000000000000005 	0.274500000000000022 	0.172999999999999987 	0.275000000000000022 	9 	
-0 	0.599999999999999978 	0.450000000000000011 	0.140000000000000013 	0.868999999999999995 	0.342500000000000027 	0.195000000000000007 	0.290999999999999981 	11 	
-2 	0.560000000000000053 	0.434999999999999998 	0.179999999999999993 	0.889000000000000012 	0.359999999999999987 	0.203999999999999987 	0.25 	11 	
-0 	0.550000000000000044 	0.440000000000000002 	0.154999999999999999 	0.945999999999999952 	0.313 	0.182499999999999996 	0.33500000000000002 	16 	
-0 	0.689999999999999947 	0.540000000000000036 	0.154999999999999999 	1.45399999999999996 	0.623999999999999999 	0.310499999999999998 	0.390000000000000013 	9 	
-2 	0.54500000000000004 	0.419999999999999984 	0.140000000000000013 	0.750499999999999945 	0.247499999999999998 	0.130000000000000004 	0.255000000000000004 	22 	
-2 	0.515000000000000013 	0.395000000000000018 	0.119999999999999996 	0.646000000000000019 	0.284999999999999976 	0.13650000000000001 	0.171999999999999986 	9 	
-2 	0.594999999999999973 	0.474999999999999978 	0.140000000000000013 	1.03049999999999997 	0.492499999999999993 	0.216999999999999998 	0.278000000000000025 	10 	
-2 	0.689999999999999947 	0.540000000000000036 	0.184999999999999998 	1.70999999999999996 	0.772499999999999964 	0.385500000000000009 	0.432499999999999996 	8 	
-0 	0.694999999999999951 	0.550000000000000044 	0.184999999999999998 	1.67900000000000005 	0.805000000000000049 	0.401500000000000024 	0.396500000000000019 	10 	
-0 	0.589999999999999969 	0.455000000000000016 	0.165000000000000008 	1.16100000000000003 	0.380000000000000004 	0.245499999999999996 	0.280000000000000027 	12 	
-0 	0.574999999999999956 	0.429999999999999993 	0.154999999999999999 	0.795499999999999985 	0.348499999999999976 	0.192500000000000004 	0.220000000000000001 	9 	
-2 	0.325000000000000011 	0.23000000000000001 	0.0899999999999999967 	0.146999999999999992 	0.0599999999999999978 	0.0340000000000000024 	0.0449999999999999983 	4 	
-0 	0.719999999999999973 	0.574999999999999956 	0.214999999999999997 	2.22599999999999998 	0.895499999999999963 	0.405000000000000027 	0.619999999999999996 	13 	
-1 	0.325000000000000011 	0.244999999999999996 	0.0700000000000000067 	0.161000000000000004 	0.0754999999999999977 	0.0254999999999999984 	0.0449999999999999983 	6 	
-1 	0.195000000000000007 	0.149999999999999994 	0.0449999999999999983 	0.0374999999999999986 	0.0179999999999999986 	0.00600000000000000012 	0.0109999999999999994 	3 	
-1 	0.589999999999999969 	0.450000000000000011 	0.160000000000000003 	0.893000000000000016 	0.274500000000000022 	0.2185 	0.344999999999999973 	14 	
-0 	0.685000000000000053 	0.54500000000000004 	0.179999999999999993 	1.76800000000000002 	0.749500000000000055 	0.392000000000000015 	0.484999999999999987 	16 	
-1 	0.385000000000000009 	0.299999999999999989 	0.0899999999999999967 	0.307999999999999996 	0.152499999999999997 	0.0560000000000000012 	0.0835000000000000048 	8 	
-2 	0.469999999999999973 	0.375 	0.130000000000000004 	0.579500000000000015 	0.214499999999999996 	0.164000000000000007 	0.195000000000000007 	13 	
-0 	0.57999999999999996 	0.455000000000000016 	0.119999999999999996 	0.939999999999999947 	0.399000000000000021 	0.257000000000000006 	0.265000000000000013 	11 	
-2 	0.589999999999999969 	0.474999999999999978 	0.165000000000000008 	1.07699999999999996 	0.454500000000000015 	0.243999999999999995 	0.309499999999999997 	9 	
-1 	0.484999999999999987 	0.354999999999999982 	0.130000000000000004 	0.580999999999999961 	0.244999999999999996 	0.132000000000000006 	0.16800000000000001 	12 	
-2 	0.530000000000000027 	0.405000000000000027 	0.149999999999999994 	0.831500000000000017 	0.35199999999999998 	0.187 	0.252500000000000002 	10 	
-2 	0.650000000000000022 	0.515000000000000013 	0.125 	1.1805000000000001 	0.523499999999999965 	0.282999999999999974 	0.327500000000000013 	9 	
-1 	0.359999999999999987 	0.265000000000000013 	0.0950000000000000011 	0.231500000000000011 	0.104999999999999996 	0.0459999999999999992 	0.0749999999999999972 	7 	
-0 	0.67000000000000004 	0.530000000000000027 	0.204999999999999988 	1.40149999999999997 	0.643000000000000016 	0.246499999999999997 	0.415999999999999981 	12 	
-2 	0.594999999999999973 	0.479999999999999982 	0.140000000000000013 	0.912499999999999978 	0.409499999999999975 	0.182499999999999996 	0.288999999999999979 	9 	
-2 	0.450000000000000011 	0.33500000000000002 	0.125 	0.447500000000000009 	0.216499999999999998 	0.126000000000000001 	0.110000000000000001 	6 	
-2 	0.594999999999999973 	0.455000000000000016 	0.14499999999999999 	0.941999999999999948 	0.429999999999999993 	0.181999999999999995 	0.277000000000000024 	11 	
-2 	0.625 	0.505000000000000004 	0.214999999999999997 	1.44550000000000001 	0.495999999999999996 	0.286999999999999977 	0.434999999999999998 	22 	
-1 	0.465000000000000024 	0.369999999999999996 	0.119999999999999996 	0.436499999999999999 	0.188 	0.081500000000000003 	0.146999999999999992 	9 	
-2 	0.364999999999999991 	0.294999999999999984 	0.0800000000000000017 	0.255500000000000005 	0.0970000000000000029 	0.0429999999999999966 	0.100000000000000006 	7 	
-0 	0.494999999999999996 	0.385000000000000009 	0.130000000000000004 	0.690500000000000003 	0.3125 	0.178999999999999992 	0.174999999999999989 	10 	
-2 	0.555000000000000049 	0.429999999999999993 	0.165000000000000008 	0.757499999999999951 	0.273500000000000021 	0.163500000000000006 	0.275000000000000022 	13 	
-2 	0.589999999999999969 	0.5 	0.165000000000000008 	1.10450000000000004 	0.456500000000000017 	0.242499999999999993 	0.340000000000000024 	15 	
-2 	0.67000000000000004 	0.5 	0.190000000000000002 	1.51899999999999991 	0.615999999999999992 	0.388000000000000012 	0.41499999999999998 	10 	
-0 	0.594999999999999973 	0.455000000000000016 	0.160000000000000003 	1.04000000000000004 	0.452000000000000013 	0.265500000000000014 	0.287999999999999978 	9 	
-2 	0.530000000000000027 	0.434999999999999998 	0.154999999999999999 	0.698999999999999955 	0.287999999999999978 	0.159500000000000003 	0.204999999999999988 	10 	
-1 	0.520000000000000018 	0.41499999999999998 	0.160000000000000003 	0.594999999999999973 	0.210499999999999993 	0.141999999999999987 	0.260000000000000009 	15 	
-0 	0.409999999999999976 	0.320000000000000007 	0.115000000000000005 	0.387000000000000011 	0.165000000000000008 	0.100500000000000006 	0.0985000000000000042 	11 	
-2 	0.609999999999999987 	0.46000000000000002 	0.160000000000000003 	1 	0.493999999999999995 	0.197000000000000008 	0.275000000000000022 	10 	
-1 	0.364999999999999991 	0.260000000000000009 	0.115000000000000005 	0.217999999999999999 	0.0934999999999999998 	0.0444999999999999979 	0.0700000000000000067 	9 	
-1 	0.520000000000000018 	0.400000000000000022 	0.110000000000000001 	0.596999999999999975 	0.293499999999999983 	0.115500000000000005 	0.160000000000000003 	8 	
-2 	0.550000000000000044 	0.434999999999999998 	0.140000000000000013 	0.762499999999999956 	0.327000000000000013 	0.168500000000000011 	0.259000000000000008 	10 	
-2 	0.57999999999999996 	0.450000000000000011 	0.119999999999999996 	0.86850000000000005 	0.417999999999999983 	0.147499999999999992 	0.260500000000000009 	8 	
-2 	0.479999999999999982 	0.385000000000000009 	0.14499999999999999 	0.640000000000000013 	0.292499999999999982 	0.140500000000000014 	0.157500000000000001 	6 	
-1 	0.520000000000000018 	0.395000000000000018 	0.115000000000000005 	0.644499999999999962 	0.315500000000000003 	0.1245 	0.185999999999999999 	11 	
-0 	0.505000000000000004 	0.380000000000000004 	0.14499999999999999 	0.651000000000000023 	0.293499999999999983 	0.190000000000000002 	0.170000000000000012 	12 	
-0 	0.619999999999999996 	0.469999999999999973 	0.200000000000000011 	1.22550000000000003 	0.381000000000000005 	0.270000000000000018 	0.434999999999999998 	23 	
-2 	0.569999999999999951 	0.46000000000000002 	0.140000000000000013 	0.953500000000000014 	0.446500000000000008 	0.206499999999999989 	0.244999999999999996 	12 	
-2 	0.530000000000000027 	0.440000000000000002 	0.204999999999999988 	0.834999999999999964 	0.320000000000000007 	0.217499999999999999 	0.244999999999999996 	14 	
-1 	0.505000000000000004 	0.380000000000000004 	0.119999999999999996 	0.593999999999999972 	0.259500000000000008 	0.143499999999999989 	0.179999999999999993 	7 	
-1 	0.234999999999999987 	0.174999999999999989 	0.0550000000000000003 	0.067000000000000004 	0.0269999999999999997 	0.0125000000000000007 	0.0179999999999999986 	6 	
-0 	0.489999999999999991 	0.375 	0.149999999999999994 	0.575500000000000012 	0.220000000000000001 	0.143999999999999989 	0.190000000000000002 	9 	
-2 	0.655000000000000027 	0.550000000000000044 	0.179999999999999993 	1.27400000000000002 	0.585999999999999965 	0.281000000000000028 	0.364999999999999991 	10 	
-2 	0.41499999999999998 	0.309999999999999998 	0.0899999999999999967 	0.324500000000000011 	0.130500000000000005 	0.0734999999999999959 	0.115000000000000005 	8 	
-1 	0.33500000000000002 	0.244999999999999996 	0.0899999999999999967 	0.166500000000000009 	0.0594999999999999973 	0.0400000000000000008 	0.0599999999999999978 	6 	
-2 	0.599999999999999978 	0.494999999999999996 	0.184999999999999998 	1.11450000000000005 	0.505499999999999949 	0.263500000000000012 	0.366999999999999993 	11 	
-0 	0.535000000000000031 	0.46000000000000002 	0.14499999999999999 	0.787499999999999978 	0.339500000000000024 	0.200500000000000012 	0.200000000000000011 	8 	
-0 	0.599999999999999978 	0.469999999999999973 	0.149999999999999994 	0.922000000000000042 	0.362999999999999989 	0.194000000000000006 	0.304999999999999993 	10 	
-0 	0.57999999999999996 	0.429999999999999993 	0.170000000000000012 	1.47999999999999998 	0.65349999999999997 	0.32400000000000001 	0.41549999999999998 	10 	
-1 	0.625 	0.429999999999999993 	0.174999999999999989 	1.41100000000000003 	0.571999999999999953 	0.296999999999999986 	0.395000000000000018 	12 	
-1 	0.255000000000000004 	0.184999999999999998 	0.0599999999999999978 	0.0924999999999999989 	0.0389999999999999999 	0.0210000000000000013 	0.0250000000000000014 	6 	
-0 	0.619999999999999996 	0.510000000000000009 	0.204999999999999988 	1.34749999999999992 	0.47749999999999998 	0.256500000000000006 	0.479999999999999982 	14 	
-2 	0.349999999999999978 	0.265000000000000013 	0.0899999999999999967 	0.225500000000000006 	0.0995000000000000051 	0.0485000000000000014 	0.0700000000000000067 	7 	
-2 	0.560000000000000053 	0.419999999999999984 	0.195000000000000007 	0.808499999999999996 	0.302499999999999991 	0.179499999999999993 	0.284999999999999976 	14 	
-2 	0.630000000000000004 	0.469999999999999973 	0.149999999999999994 	1.13549999999999995 	0.539000000000000035 	0.232500000000000012 	0.311499999999999999 	12 	
-2 	0.589999999999999969 	0.465000000000000024 	0.165000000000000008 	1.11499999999999999 	0.516499999999999959 	0.27300000000000002 	0.275000000000000022 	10 	
-1 	0.555000000000000049 	0.395000000000000018 	0.130000000000000004 	0.558499999999999996 	0.222000000000000003 	0.1245 	0.170000000000000012 	9 	
-1 	0.469999999999999973 	0.375 	0.125 	0.522499999999999964 	0.226500000000000007 	0.103999999999999995 	0.162000000000000005 	8 	
-2 	0.489999999999999991 	0.385000000000000009 	0.125 	0.608999999999999986 	0.306499999999999995 	0.096000000000000002 	0.177499999999999991 	8 	
-1 	0.550000000000000044 	0.424999999999999989 	0.135000000000000009 	0.656000000000000028 	0.257000000000000006 	0.170000000000000012 	0.203000000000000014 	10 	
-2 	0.635000000000000009 	0.489999999999999991 	0.174999999999999989 	1.375 	0.622999999999999998 	0.270500000000000018 	0.395000000000000018 	11 	
-1 	0.375 	0.260000000000000009 	0.0800000000000000017 	0.20749999999999999 	0.0899999999999999967 	0.0415000000000000022 	0.0700000000000000067 	6 	
-0 	0.619999999999999996 	0.479999999999999982 	0.160000000000000003 	1.11250000000000004 	0.563500000000000001 	0.244499999999999995 	0.281000000000000028 	8 	
-1 	0.560000000000000053 	0.440000000000000002 	0.130000000000000004 	0.723500000000000032 	0.348999999999999977 	0.148999999999999994 	0.200000000000000011 	8 	
-1 	0.440000000000000002 	0.340000000000000024 	0.119999999999999996 	0.438 	0.211499999999999994 	0.0830000000000000043 	0.119999999999999996 	9 	
-0 	0.564999999999999947 	0.440000000000000002 	0.160000000000000003 	0.915000000000000036 	0.353999999999999981 	0.193500000000000005 	0.320000000000000007 	12 	
-0 	0.359999999999999987 	0.270000000000000018 	0.0899999999999999967 	0.188500000000000001 	0.0845000000000000057 	0.0384999999999999995 	0.0550000000000000003 	5 	
-1 	0.369999999999999996 	0.280000000000000027 	0.0850000000000000061 	0.216999999999999998 	0.1095 	0.0350000000000000033 	0.0619999999999999996 	6 	
-1 	0.494999999999999996 	0.380000000000000004 	0.14499999999999999 	0.5 	0.204999999999999988 	0.147999999999999993 	0.150499999999999995 	8 	
-1 	0.5 	0.395000000000000018 	0.119999999999999996 	0.537000000000000033 	0.216499999999999998 	0.108499999999999999 	0.178499999999999992 	9 	
-0 	0.680000000000000049 	0.569999999999999951 	0.204999999999999988 	1.84200000000000008 	0.625 	0.407999999999999974 	0.650000000000000022 	20 	
-2 	0.660000000000000031 	0.510000000000000009 	0.165000000000000008 	1.63749999999999996 	0.768499999999999961 	0.354499999999999982 	0.392500000000000016 	14 	
-1 	0.489999999999999991 	0.369999999999999996 	0.110000000000000001 	0.538000000000000034 	0.271000000000000019 	0.103499999999999995 	0.139000000000000012 	8 	
-0 	0.395000000000000018 	0.294999999999999984 	0.0950000000000000011 	0.224500000000000005 	0.0779999999999999999 	0.0539999999999999994 	0.0800000000000000017 	10 	
-1 	0.260000000000000009 	0.200000000000000011 	0.0700000000000000067 	0.0919999999999999984 	0.0369999999999999982 	0.0200000000000000004 	0.0299999999999999989 	6 	
-0 	0.530000000000000027 	0.429999999999999993 	0.149999999999999994 	0.740999999999999992 	0.325000000000000011 	0.185499999999999998 	0.196000000000000008 	9 	
-0 	0.719999999999999973 	0.574999999999999956 	0.214999999999999997 	2.10000000000000009 	0.856500000000000039 	0.482499999999999984 	0.60199999999999998 	12 	
-2 	0.569999999999999951 	0.440000000000000002 	0.174999999999999989 	0.941500000000000004 	0.380500000000000005 	0.228500000000000009 	0.282999999999999974 	9 	
-2 	0.589999999999999969 	0.469999999999999973 	0.149999999999999994 	0.860999999999999988 	0.412999999999999978 	0.164000000000000007 	0.248999999999999999 	8 	
-1 	0.635000000000000009 	0.5 	0.179999999999999993 	1.31899999999999995 	0.548499999999999988 	0.291999999999999982 	0.489999999999999991 	16 	
-0 	0.67000000000000004 	0.525000000000000022 	0.190000000000000002 	1.52699999999999991 	0.575500000000000012 	0.35299999999999998 	0.440000000000000002 	12 	
-1 	0.409999999999999976 	0.299999999999999989 	0.0899999999999999967 	0.303999999999999992 	0.129000000000000004 	0.0709999999999999937 	0.0955000000000000016 	8 	
-1 	0.450000000000000011 	0.330000000000000016 	0.110000000000000001 	0.368499999999999994 	0.160000000000000003 	0.0884999999999999953 	0.101999999999999993 	6 	
-0 	0.645000000000000018 	0.489999999999999991 	0.160000000000000003 	1.16650000000000009 	0.493499999999999994 	0.315500000000000003 	0.298999999999999988 	9 	
-0 	0.70499999999999996 	0.550000000000000044 	0.200000000000000011 	1.70950000000000002 	0.633000000000000007 	0.411499999999999977 	0.489999999999999991 	13 	
-0 	0.510000000000000009 	0.400000000000000022 	0.130000000000000004 	0.57350000000000001 	0.219 	0.13650000000000001 	0.195000000000000007 	13 	
-2 	0.599999999999999978 	0.469999999999999973 	0.154999999999999999 	1.03600000000000003 	0.4375 	0.196000000000000008 	0.325000000000000011 	20 	
-1 	0.474999999999999978 	0.364999999999999991 	0.100000000000000006 	0.131500000000000006 	0.202500000000000013 	0.0874999999999999944 	0.122999999999999998 	7 	
-0 	0.57999999999999996 	0.474999999999999978 	0.154999999999999999 	0.973999999999999977 	0.430499999999999994 	0.23000000000000001 	0.284999999999999976 	10 	
-1 	0.474999999999999978 	0.354999999999999982 	0.100000000000000006 	0.503499999999999948 	0.253500000000000003 	0.0909999999999999976 	0.140000000000000013 	8 	
-2 	0.315000000000000002 	0.25 	0.0899999999999999967 	0.203000000000000014 	0.0614999999999999991 	0.0369999999999999982 	0.0795000000000000012 	11 	
-2 	0.54500000000000004 	0.440000000000000002 	0.140000000000000013 	0.839500000000000024 	0.355999999999999983 	0.190500000000000003 	0.23849999999999999 	11 	
-2 	0.179999999999999993 	0.125 	0.0500000000000000028 	0.0229999999999999996 	0.00850000000000000061 	0.00549999999999999968 	0.0100000000000000002 	3 	
-2 	0.680000000000000049 	0.520000000000000018 	0.195000000000000007 	1.45350000000000001 	0.591999999999999971 	0.391000000000000014 	0.412499999999999978 	10 	
-2 	0.594999999999999973 	0.465000000000000024 	0.174999999999999989 	1.11499999999999999 	0.401500000000000024 	0.254000000000000004 	0.390000000000000013 	13 	
-2 	0.479999999999999982 	0.369999999999999996 	0.100000000000000006 	0.513499999999999956 	0.242999999999999994 	0.101500000000000007 	0.135000000000000009 	8 	
-2 	0.574999999999999956 	0.465000000000000024 	0.149999999999999994 	1.08000000000000007 	0.594999999999999973 	0.206499999999999989 	0.237999999999999989 	9 	
-0 	0.625 	0.445000000000000007 	0.160000000000000003 	1.09000000000000008 	0.46000000000000002 	0.296499999999999986 	0.303999999999999992 	11 	
-2 	0.650000000000000022 	0.525000000000000022 	0.174999999999999989 	1.53649999999999998 	0.686499999999999999 	0.358499999999999985 	0.405000000000000027 	11 	
-2 	0.599999999999999978 	0.484999999999999987 	0.174999999999999989 	1.26750000000000007 	0.4995 	0.281499999999999972 	0.380000000000000004 	13 	
-1 	0.550000000000000044 	0.424999999999999989 	0.135000000000000009 	0.730500000000000038 	0.332500000000000018 	0.154499999999999998 	0.214999999999999997 	9 	
-2 	0.599999999999999978 	0.474999999999999978 	0.174999999999999989 	1.34450000000000003 	0.549000000000000044 	0.287499999999999978 	0.359999999999999987 	11 	
-0 	0.645000000000000018 	0.515000000000000013 	0.149999999999999994 	1.21199999999999997 	0.515000000000000013 	0.205499999999999988 	0.385000000000000009 	10 	
-1 	0.5 	0.390000000000000013 	0.119999999999999996 	0.595500000000000029 	0.245499999999999996 	0.146999999999999992 	0.172999999999999987 	8 	
-2 	0.694999999999999951 	0.569999999999999951 	0.23000000000000001 	1.88500000000000001 	0.866500000000000048 	0.434999999999999998 	0.5 	19 	
-2 	0.445000000000000007 	0.349999999999999978 	0.140000000000000013 	0.590500000000000025 	0.202500000000000013 	0.158000000000000002 	0.190000000000000002 	14 	
-2 	0.569999999999999951 	0.455000000000000016 	0.174999999999999989 	1.02000000000000002 	0.480499999999999983 	0.214499999999999996 	0.28999999999999998 	9 	
-1 	0.209999999999999992 	0.149999999999999994 	0.0449999999999999983 	0.0400000000000000008 	0.0134999999999999998 	0.00800000000000000017 	0.0105000000000000007 	4 	
-1 	0.564999999999999947 	0.434999999999999998 	0.154999999999999999 	0.782000000000000028 	0.271500000000000019 	0.16800000000000001 	0.284999999999999976 	14 	
-2 	0.535000000000000031 	0.429999999999999993 	0.154999999999999999 	0.784499999999999975 	0.328500000000000014 	0.169000000000000011 	0.244999999999999996 	10 	
-1 	0.405000000000000027 	0.304999999999999993 	0.100000000000000006 	0.268000000000000016 	0.114500000000000005 	0.0529999999999999985 	0.0850000000000000061 	7 	
-0 	0.589999999999999969 	0.455000000000000016 	0.154999999999999999 	1.06600000000000006 	0.382000000000000006 	0.227500000000000008 	0.41499999999999998 	20 	
-2 	0.520000000000000018 	0.380000000000000004 	0.135000000000000009 	0.582500000000000018 	0.2505 	0.1565 	0.174999999999999989 	8 	
-1 	0.555000000000000049 	0.424999999999999989 	0.130000000000000004 	0.64800000000000002 	0.283499999999999974 	0.133000000000000007 	0.210499999999999993 	8 	
-0 	0.645000000000000018 	0.5 	0.170000000000000012 	1.18450000000000011 	0.480499999999999983 	0.274000000000000021 	0.354999999999999982 	13 	
-1 	0.450000000000000011 	0.349999999999999978 	0.14499999999999999 	0.525000000000000022 	0.208499999999999991 	0.100000000000000006 	0.165500000000000008 	15 	
-1 	0.455000000000000016 	0.375 	0.119999999999999996 	0.496999999999999997 	0.235499999999999987 	0.105499999999999997 	0.129500000000000004 	6 	
-1 	0.299999999999999989 	0.220000000000000001 	0.0800000000000000017 	0.120999999999999996 	0.0475000000000000006 	0.0420000000000000026 	0.0350000000000000033 	5 	
-2 	0.650000000000000022 	0.484999999999999987 	0.160000000000000003 	1.73950000000000005 	0.571500000000000008 	0.278500000000000025 	0.307499999999999996 	10 	
-2 	0.609999999999999987 	0.489999999999999991 	0.160000000000000003 	1.1120000000000001 	0.465000000000000024 	0.228000000000000008 	0.341000000000000025 	10 	
-0 	0.57999999999999996 	0.450000000000000011 	0.195000000000000007 	0.826500000000000012 	0.403500000000000025 	0.172999999999999987 	0.225000000000000006 	9 	
-2 	0.445000000000000007 	0.344999999999999973 	0.0899999999999999967 	0.379500000000000004 	0.142999999999999988 	0.0739999999999999963 	0.125 	10 	
-1 	0.520000000000000018 	0.385000000000000009 	0.115000000000000005 	0.580999999999999961 	0.255500000000000005 	0.156 	0.142999999999999988 	10 	
-0 	0.54500000000000004 	0.424999999999999989 	0.125 	0.768000000000000016 	0.293999999999999984 	0.149499999999999994 	0.260000000000000009 	16 	
-0 	0.574999999999999956 	0.479999999999999982 	0.149999999999999994 	0.89700000000000002 	0.423499999999999988 	0.190500000000000003 	0.247999999999999998 	8 	
-2 	0.54500000000000004 	0.409999999999999976 	0.119999999999999996 	0.793000000000000038 	0.433999999999999997 	0.140500000000000014 	0.190000000000000002 	9 	
-0 	0.57999999999999996 	0.474999999999999978 	0.135000000000000009 	0.925000000000000044 	0.391000000000000014 	0.165000000000000008 	0.275000000000000022 	14 	
-0 	0.474999999999999978 	0.390000000000000013 	0.119999999999999996 	0.530499999999999972 	0.213499999999999995 	0.115500000000000005 	0.170000000000000012 	10 	
-1 	0.46000000000000002 	0.344999999999999973 	0.119999999999999996 	0.41549999999999998 	0.198000000000000009 	0.0884999999999999953 	0.106999999999999998 	7 	
-2 	0.349999999999999978 	0.255000000000000004 	0.0650000000000000022 	0.178999999999999992 	0.0704999999999999932 	0.0384999999999999995 	0.0599999999999999978 	10 	
-2 	0.630000000000000004 	0.515000000000000013 	0.160000000000000003 	1.01600000000000001 	0.421499999999999986 	0.243999999999999995 	0.354999999999999982 	19 	
-2 	0.739999999999999991 	0.594999999999999973 	0.190000000000000002 	2.32350000000000012 	1.14949999999999997 	0.511499999999999955 	0.505000000000000004 	11 	
-2 	0.469999999999999973 	0.359999999999999987 	0.135000000000000009 	0.501000000000000001 	0.166500000000000009 	0.115000000000000005 	0.165000000000000008 	10 	
-1 	0.525000000000000022 	0.400000000000000022 	0.14499999999999999 	0.609500000000000042 	0.247999999999999998 	0.159000000000000002 	0.174999999999999989 	9 	
-1 	0.280000000000000027 	0.209999999999999992 	0.0550000000000000003 	0.105999999999999997 	0.0415000000000000022 	0.0264999999999999993 	0.0309999999999999998 	5 	
-1 	0.375 	0.284999999999999976 	0.0800000000000000017 	0.226000000000000006 	0.0975000000000000033 	0.0400000000000000008 	0.072499999999999995 	7 	
-2 	0.609999999999999987 	0.484999999999999987 	0.174999999999999989 	1.24449999999999994 	0.544000000000000039 	0.296999999999999986 	0.344999999999999973 	12 	
-2 	0.46000000000000002 	0.359999999999999987 	0.135000000000000009 	0.610500000000000043 	0.195500000000000007 	0.106999999999999998 	0.234999999999999987 	14 	
-0 	0.550000000000000044 	0.429999999999999993 	0.140000000000000013 	0.810499999999999998 	0.367999999999999994 	0.161000000000000004 	0.275000000000000022 	9 	
-1 	0.359999999999999987 	0.280000000000000027 	0.0899999999999999967 	0.225500000000000006 	0.0884999999999999953 	0.0400000000000000008 	0.0899999999999999967 	8 	
-2 	0.770000000000000018 	0.619999999999999996 	0.195000000000000007 	2.51549999999999985 	1.11549999999999994 	0.641499999999999959 	0.642000000000000015 	12 	
-0 	0.724999999999999978 	0.574999999999999956 	0.174999999999999989 	2.12400000000000011 	0.765000000000000013 	0.451500000000000012 	0.849999999999999978 	20 	
-2 	0.505000000000000004 	0.385000000000000009 	0.130000000000000004 	0.643499999999999961 	0.313500000000000001 	0.148999999999999994 	0.151499999999999996 	7 	
-1 	0.364999999999999991 	0.265000000000000013 	0.0850000000000000061 	0.212999999999999995 	0.0945000000000000007 	0.0490000000000000019 	0.0599999999999999978 	7 	
-2 	0.520000000000000018 	0.400000000000000022 	0.165000000000000008 	0.856500000000000039 	0.274500000000000022 	0.201000000000000012 	0.209999999999999992 	12 	
-0 	0.599999999999999978 	0.479999999999999982 	0.149999999999999994 	1.02899999999999991 	0.408499999999999974 	0.270500000000000018 	0.294999999999999984 	16 	
-1 	0.395000000000000018 	0.304999999999999993 	0.104999999999999996 	0.283999999999999975 	0.113500000000000004 	0.0594999999999999973 	0.0945000000000000007 	8 	
-1 	0.530000000000000027 	0.419999999999999984 	0.184999999999999998 	0.752000000000000002 	0.298999999999999988 	0.156 	0.204999999999999988 	20 	
-1 	0.429999999999999993 	0.344999999999999973 	0.115000000000000005 	0.429499999999999993 	0.211999999999999994 	0.107999999999999999 	0.109 	8 	
-2 	0.584999999999999964 	0.455000000000000016 	0.14499999999999999 	0.952999999999999958 	0.394500000000000017 	0.268500000000000016 	0.258000000000000007 	10 	
-1 	0.390000000000000013 	0.299999999999999989 	0.0899999999999999967 	0.252000000000000002 	0.106499999999999997 	0.0529999999999999985 	0.0800000000000000017 	7 	
-0 	0.699999999999999956 	0.54500000000000004 	0.130000000000000004 	1.55600000000000005 	0.672499999999999987 	0.373999999999999999 	0.195000000000000007 	12 	
-0 	0.57999999999999996 	0.445000000000000007 	0.14499999999999999 	0.888000000000000012 	0.409999999999999976 	0.181499999999999995 	0.242499999999999993 	8 	
-1 	0.275000000000000022 	0.200000000000000011 	0.0650000000000000022 	0.103499999999999995 	0.0475000000000000006 	0.0205000000000000009 	0.0299999999999999989 	7 	
-2 	0.574999999999999956 	0.434999999999999998 	0.140000000000000013 	0.845500000000000029 	0.401000000000000023 	0.191000000000000003 	0.222000000000000003 	9 	
-0 	0.57999999999999996 	0.434999999999999998 	0.149999999999999994 	0.833999999999999964 	0.427999999999999992 	0.151499999999999996 	0.23000000000000001 	8 	
-1 	0.209999999999999992 	0.149999999999999994 	0.0550000000000000003 	0.0464999999999999997 	0.0170000000000000012 	0.0120000000000000002 	0.0149999999999999994 	5 	
-0 	0.625 	0.525000000000000022 	0.195000000000000007 	1.35200000000000009 	0.450500000000000012 	0.244499999999999995 	0.530000000000000027 	13 	
-1 	0.465000000000000024 	0.354999999999999982 	0.0899999999999999967 	0.432499999999999996 	0.200500000000000012 	0.0739999999999999963 	0.127500000000000002 	9 	
-2 	0.41499999999999998 	0.299999999999999989 	0.100000000000000006 	0.33550000000000002 	0.154499999999999998 	0.0685000000000000053 	0.0950000000000000011 	7 	
-2 	0.445000000000000007 	0.340000000000000024 	0.119999999999999996 	0.447500000000000009 	0.193000000000000005 	0.103499999999999995 	0.130000000000000004 	9 	
-2 	0.614999999999999991 	0.455000000000000016 	0.149999999999999994 	0.933499999999999996 	0.382000000000000006 	0.246999999999999997 	0.26150000000000001 	10 	
-1 	0.525000000000000022 	0.405000000000000027 	0.14499999999999999 	0.696500000000000008 	0.304499999999999993 	0.153499999999999998 	0.209999999999999992 	8 	
-2 	0.54500000000000004 	0.450000000000000011 	0.149999999999999994 	0.97799999999999998 	0.336500000000000021 	0.190500000000000003 	0.299999999999999989 	11 	
-1 	0.270000000000000018 	0.195000000000000007 	0.0599999999999999978 	0.0729999999999999954 	0.028500000000000001 	0.0235000000000000001 	0.0299999999999999989 	5 	
-2 	0.619999999999999996 	0.465000000000000024 	0.184999999999999998 	1.27400000000000002 	0.578999999999999959 	0.306499999999999995 	0.320000000000000007 	12 	
-0 	0.474999999999999978 	0.364999999999999991 	0.130000000000000004 	0.480499999999999983 	0.190500000000000003 	0.114000000000000004 	0.147499999999999992 	12 	
-2 	0.635000000000000009 	0.5 	0.165000000000000008 	1.27299999999999991 	0.65349999999999997 	0.212999999999999995 	0.364999999999999991 	12 	
-1 	0.309999999999999998 	0.239999999999999991 	0.0899999999999999967 	0.14549999999999999 	0.0604999999999999982 	0.0315000000000000002 	0.0449999999999999983 	7 	
-1 	0.474999999999999978 	0.340000000000000024 	0.104999999999999996 	0.453500000000000014 	0.203000000000000014 	0.0800000000000000017 	0.146499999999999991 	9 	
-2 	0.479999999999999982 	0.369999999999999996 	0.135000000000000009 	0.63149999999999995 	0.344499999999999973 	0.101500000000000007 	0.161000000000000004 	7 	
-2 	0.280000000000000027 	0.200000000000000011 	0.0800000000000000017 	0.091499999999999998 	0.0330000000000000016 	0.0214999999999999983 	0.0299999999999999989 	5 	
-2 	0.645000000000000018 	0.5 	0.174999999999999989 	1.33749999999999991 	0.554000000000000048 	0.307999999999999996 	0.41499999999999998 	10 	
-0 	0.70499999999999996 	0.569999999999999951 	0.184999999999999998 	1.7609999999999999 	0.746999999999999997 	0.372499999999999998 	0.487999999999999989 	10 	
-1 	0.315000000000000002 	0.23000000000000001 	0.0700000000000000067 	0.143999999999999989 	0.0529999999999999985 	0.0304999999999999993 	0.0400000000000000008 	8 	
-1 	0.510000000000000009 	0.385000000000000009 	0.14499999999999999 	0.766499999999999959 	0.398500000000000021 	0.140000000000000013 	0.180499999999999994 	8 	
-2 	0.57999999999999996 	0.429999999999999993 	0.130000000000000004 	0.798000000000000043 	0.364999999999999991 	0.172999999999999987 	0.228500000000000009 	10 	
-2 	0.650000000000000022 	0.54500000000000004 	0.160000000000000003 	1.24249999999999994 	0.486999999999999988 	0.295999999999999985 	0.479999999999999982 	15 	
-2 	0.70499999999999996 	0.560000000000000053 	0.220000000000000001 	1.98100000000000009 	0.817500000000000004 	0.308499999999999996 	0.760000000000000009 	14 	
-0 	0.440000000000000002 	0.344999999999999973 	0.104999999999999996 	0.428499999999999992 	0.165000000000000008 	0.0830000000000000043 	0.132000000000000006 	11 	
-1 	0.469999999999999973 	0.349999999999999978 	0.125 	0.431499999999999995 	0.190000000000000002 	0.116500000000000006 	0.117499999999999993 	6 	
-2 	0.564999999999999947 	0.424999999999999989 	0.100000000000000006 	0.714500000000000024 	0.305499999999999994 	0.166000000000000009 	0.179999999999999993 	12 	
-2 	0.619999999999999996 	0.489999999999999991 	0.190000000000000002 	1.21799999999999997 	0.545499999999999985 	0.296499999999999986 	0.354999999999999982 	13 	
-0 	0.619999999999999996 	0.479999999999999982 	0.23000000000000001 	1.09349999999999992 	0.403000000000000025 	0.244999999999999996 	0.354999999999999982 	14 	
-2 	0.574999999999999956 	0.450000000000000011 	0.165000000000000008 	0.921499999999999986 	0.327500000000000013 	0.225000000000000006 	0.256000000000000005 	12 	
-0 	0.46000000000000002 	0.354999999999999982 	0.130000000000000004 	0.517000000000000015 	0.220500000000000002 	0.114000000000000004 	0.165000000000000008 	9 	
-2 	0.525000000000000022 	0.400000000000000022 	0.154999999999999999 	0.706999999999999962 	0.281999999999999973 	0.160500000000000004 	0.225000000000000006 	9 	
-1 	0.369999999999999996 	0.280000000000000027 	0.0950000000000000011 	0.265500000000000014 	0.121999999999999997 	0.0519999999999999976 	0.0800000000000000017 	7 	
-1 	0.574999999999999956 	0.424999999999999989 	0.135000000000000009 	0.796499999999999986 	0.36399999999999999 	0.196000000000000008 	0.23899999999999999 	10 	
-2 	0.57999999999999996 	0.445000000000000007 	0.160000000000000003 	0.983999999999999986 	0.489999999999999991 	0.201000000000000012 	0.270000000000000018 	9 	
-2 	0.680000000000000049 	0.530000000000000027 	0.204999999999999988 	1.496 	0.582500000000000018 	0.337000000000000022 	0.465000000000000024 	14 	
-0 	0.640000000000000013 	0.515000000000000013 	0.165000000000000008 	1.31150000000000011 	0.494499999999999995 	0.255500000000000005 	0.409999999999999976 	10 	
-2 	0.284999999999999976 	0.209999999999999992 	0.0749999999999999972 	0.118499999999999994 	0.0550000000000000003 	0.028500000000000001 	0.0400000000000000008 	7 	
-2 	0.535000000000000031 	0.409999999999999976 	0.119999999999999996 	0.683499999999999996 	0.3125 	0.165500000000000008 	0.159000000000000002 	8 	
-2 	0.635000000000000009 	0.474999999999999978 	0.170000000000000012 	1.19350000000000001 	0.520499999999999963 	0.269500000000000017 	0.366499999999999992 	10 	
-1 	0.474999999999999978 	0.375 	0.115000000000000005 	0.520499999999999963 	0.233000000000000013 	0.118999999999999995 	0.14549999999999999 	7 	
-0 	0.57999999999999996 	0.445000000000000007 	0.149999999999999994 	0.857999999999999985 	0.400000000000000022 	0.156 	0.253000000000000003 	8 	
-2 	0.469999999999999973 	0.375 	0.130000000000000004 	0.52300000000000002 	0.213999999999999996 	0.132000000000000006 	0.14499999999999999 	8 	
-0 	0.530000000000000027 	0.41499999999999998 	0.160000000000000003 	0.783000000000000029 	0.293499999999999983 	0.158000000000000002 	0.244999999999999996 	15 	
-2 	0.564999999999999947 	0.41499999999999998 	0.125 	0.667000000000000037 	0.301999999999999991 	0.154499999999999998 	0.184999999999999998 	7 	
-1 	0.419999999999999984 	0.325000000000000011 	0.115000000000000005 	0.314000000000000001 	0.129500000000000004 	0.0635000000000000009 	0.100000000000000006 	8 	
-0 	0.57999999999999996 	0.455000000000000016 	0.160000000000000003 	0.921499999999999986 	0.312 	0.196000000000000008 	0.299999999999999989 	17 	
-1 	0.560000000000000053 	0.445000000000000007 	0.165000000000000008 	0.831999999999999962 	0.345499999999999974 	0.178999999999999992 	0.279000000000000026 	9 	
-2 	0.429999999999999993 	0.33500000000000002 	0.115000000000000005 	0.406000000000000028 	0.166000000000000009 	0.0934999999999999998 	0.135000000000000009 	8 	
-1 	0.550000000000000044 	0.429999999999999993 	0.14499999999999999 	0.78949999999999998 	0.3745 	0.171000000000000013 	0.223000000000000004 	11 	
-0 	0.630000000000000004 	0.489999999999999991 	0.225000000000000006 	1.33600000000000008 	0.680499999999999994 	0.259000000000000008 	0.324500000000000011 	10 	
-1 	0.560000000000000053 	0.424999999999999989 	0.135000000000000009 	0.820500000000000007 	0.371499999999999997 	0.184999999999999998 	0.235999999999999988 	9 	
-0 	0.344999999999999973 	0.25 	0.0899999999999999967 	0.203000000000000014 	0.0779999999999999999 	0.0589999999999999969 	0.0550000000000000003 	6 	
-0 	0.599999999999999978 	0.469999999999999973 	0.135000000000000009 	0.969999999999999973 	0.465500000000000025 	0.195500000000000007 	0.264000000000000012 	11 	
-2 	0.584999999999999964 	0.465000000000000024 	0.160000000000000003 	0.955500000000000016 	0.45950000000000002 	0.235999999999999988 	0.265000000000000013 	7 	
-0 	0.660000000000000031 	0.530000000000000027 	0.170000000000000012 	1.32600000000000007 	0.519000000000000017 	0.262500000000000011 	0.440000000000000002 	13 	
-2 	0.330000000000000016 	0.255000000000000004 	0.0950000000000000011 	0.1875 	0.0734999999999999959 	0.0449999999999999983 	0.0599999999999999978 	7 	
-0 	0.550000000000000044 	0.440000000000000002 	0.135000000000000009 	0.843500000000000028 	0.433999999999999997 	0.199500000000000011 	0.184999999999999998 	8 	
-0 	0.474999999999999978 	0.364999999999999991 	0.115000000000000005 	0.565999999999999948 	0.281000000000000028 	0.117000000000000007 	0.133500000000000008 	7 	
-2 	0.609999999999999987 	0.469999999999999973 	0.160000000000000003 	1.02200000000000002 	0.44900000000000001 	0.234499999999999986 	0.294499999999999984 	9 	
-2 	0.510000000000000009 	0.41499999999999998 	0.140000000000000013 	0.818500000000000005 	0.302499999999999991 	0.215499999999999997 	0.234999999999999987 	16 	
-1 	0.294999999999999984 	0.225000000000000006 	0.0899999999999999967 	0.110500000000000001 	0.0405000000000000013 	0.0245000000000000009 	0.0320000000000000007 	7 	
-1 	0.409999999999999976 	0.299999999999999989 	0.0899999999999999967 	0.280000000000000027 	0.140999999999999986 	0.0575000000000000025 	0.0749999999999999972 	8 	
-1 	0.550000000000000044 	0.434999999999999998 	0.165000000000000008 	0.804000000000000048 	0.340000000000000024 	0.194000000000000006 	0.243999999999999995 	8 	
-2 	0.630000000000000004 	0.505000000000000004 	0.170000000000000012 	1.09149999999999991 	0.461500000000000021 	0.266000000000000014 	0.299999999999999989 	9 	
-2 	0.604999999999999982 	0.445000000000000007 	0.140000000000000013 	0.981999999999999984 	0.429499999999999993 	0.208499999999999991 	0.294999999999999984 	12 	
-2 	0.525000000000000022 	0.409999999999999976 	0.130000000000000004 	0.989999999999999991 	0.38650000000000001 	0.242999999999999994 	0.294999999999999984 	15 	
-0 	0.625 	0.489999999999999991 	0.110000000000000001 	1.1359999999999999 	0.526499999999999968 	0.191500000000000004 	0.292499999999999982 	9 	
-2 	0.54500000000000004 	0.390000000000000013 	0.135000000000000009 	0.783499999999999974 	0.422499999999999987 	0.181499999999999995 	0.156 	7 	
-2 	0.599999999999999978 	0.479999999999999982 	0.165000000000000008 	0.916499999999999981 	0.413499999999999979 	0.196500000000000008 	0.27250000000000002 	9 	
-2 	0.525000000000000022 	0.405000000000000027 	0.135000000000000009 	0.757499999999999951 	0.330500000000000016 	0.215999999999999998 	0.195000000000000007 	10 	
-2 	0.625 	0.494999999999999996 	0.184999999999999998 	1.38349999999999995 	0.71050000000000002 	0.300499999999999989 	0.344999999999999973 	11 	
-1 	0.604999999999999982 	0.484999999999999987 	0.149999999999999994 	1.23799999999999999 	0.63149999999999995 	0.226000000000000006 	0.330000000000000016 	11 	
-1 	0.440000000000000002 	0.344999999999999973 	0.115000000000000005 	0.54500000000000004 	0.269000000000000017 	0.111000000000000001 	0.130500000000000005 	6 	
-1 	0.520000000000000018 	0.409999999999999976 	0.14499999999999999 	0.646000000000000019 	0.296499999999999986 	0.159500000000000003 	0.165000000000000008 	9 	
-0 	0.699999999999999956 	0.574999999999999956 	0.170000000000000012 	1.31000000000000005 	0.509499999999999953 	0.314000000000000001 	0.419999999999999984 	14 	
-1 	0.304999999999999993 	0.225000000000000006 	0.0899999999999999967 	0.146499999999999991 	0.0630000000000000004 	0.0340000000000000024 	0.0415000000000000022 	6 	
-0 	0.609999999999999987 	0.469999999999999973 	0.165000000000000008 	1.1785000000000001 	0.565999999999999948 	0.278500000000000025 	0.293999999999999984 	11 	
-0 	0.54500000000000004 	0.409999999999999976 	0.125 	0.693500000000000005 	0.297499999999999987 	0.145999999999999991 	0.209999999999999992 	11 	
-2 	0.709999999999999964 	0.564999999999999947 	0.204999999999999988 	2.19799999999999995 	1.01200000000000001 	0.522499999999999964 	0.547499999999999987 	11 	
-1 	0.455000000000000016 	0.354999999999999982 	0.125 	0.532499999999999973 	0.225000000000000006 	0.126000000000000001 	0.146499999999999991 	7 	
-2 	0.630000000000000004 	0.525000000000000022 	0.195000000000000007 	1.31349999999999989 	0.493499999999999994 	0.256500000000000006 	0.465000000000000024 	10 	
-1 	0.434999999999999998 	0.320000000000000007 	0.0800000000000000017 	0.332500000000000018 	0.148499999999999993 	0.0635000000000000009 	0.104999999999999996 	9 	
-2 	0.609999999999999987 	0.479999999999999982 	0.149999999999999994 	1.14949999999999997 	0.563999999999999946 	0.274000000000000021 	0.264000000000000012 	8 	
-0 	0.525000000000000022 	0.380000000000000004 	0.140000000000000013 	0.606500000000000039 	0.194000000000000006 	0.147499999999999992 	0.209999999999999992 	14 	
-0 	0.650000000000000022 	0.54500000000000004 	0.165000000000000008 	1.56600000000000006 	0.66449999999999998 	0.345499999999999974 	0.41499999999999998 	16 	
-0 	0.574999999999999956 	0.465000000000000024 	0.174999999999999989 	1.09899999999999998 	0.473499999999999976 	0.202000000000000013 	0.349999999999999978 	9 	
-1 	0.510000000000000009 	0.395000000000000018 	0.154999999999999999 	0.53949999999999998 	0.246499999999999997 	0.108499999999999999 	0.16700000000000001 	8 	
-2 	0.589999999999999969 	0.474999999999999978 	0.154999999999999999 	0.856999999999999984 	0.355999999999999983 	0.173999999999999988 	0.280000000000000027 	13 	
-1 	0.204999999999999988 	0.149999999999999994 	0.0650000000000000022 	0.0400000000000000008 	0.0200000000000000004 	0.0109999999999999994 	0.0129999999999999994 	4 	
-1 	0.369999999999999996 	0.28999999999999998 	0.100000000000000006 	0.25 	0.102499999999999994 	0.0505000000000000032 	0.0850000000000000061 	10 	
-0 	0.550000000000000044 	0.469999999999999973 	0.149999999999999994 	0.89700000000000002 	0.377000000000000002 	0.183999999999999997 	0.28999999999999998 	9 	
-1 	0.239999999999999991 	0.184999999999999998 	0.0700000000000000067 	0.0714999999999999941 	0.0259999999999999988 	0.0179999999999999986 	0.0250000000000000014 	6 	
-1 	0.555000000000000049 	0.429999999999999993 	0.140000000000000013 	0.766499999999999959 	0.341000000000000025 	0.165000000000000008 	0.23000000000000001 	9 	
-0 	0.474999999999999978 	0.359999999999999987 	0.125 	0.447000000000000008 	0.169500000000000012 	0.0810000000000000026 	0.140000000000000013 	9 	
-1 	0.450000000000000011 	0.340000000000000024 	0.0950000000000000011 	0.324500000000000011 	0.138500000000000012 	0.0640000000000000013 	0.104999999999999996 	8 	
-1 	0.440000000000000002 	0.344999999999999973 	0.119999999999999996 	0.486999999999999988 	0.196500000000000008 	0.107999999999999999 	0.160000000000000003 	14 	
-0 	0.574999999999999956 	0.445000000000000007 	0.140000000000000013 	0.940999999999999948 	0.384500000000000008 	0.252000000000000002 	0.284999999999999976 	9 	
-2 	0.474999999999999978 	0.359999999999999987 	0.119999999999999996 	0.577999999999999958 	0.282499999999999973 	0.119999999999999996 	0.170000000000000012 	8 	
-1 	0.525000000000000022 	0.400000000000000022 	0.140000000000000013 	0.654000000000000026 	0.304999999999999993 	0.160000000000000003 	0.169000000000000011 	7 	
-2 	0.550000000000000044 	0.434999999999999998 	0.14499999999999999 	0.842999999999999972 	0.328000000000000014 	0.191500000000000004 	0.255000000000000004 	15 	
-0 	0.635000000000000009 	0.489999999999999991 	0.170000000000000012 	1.26150000000000007 	0.538499999999999979 	0.266500000000000015 	0.380000000000000004 	9 	
-0 	0.550000000000000044 	0.409999999999999976 	0.14499999999999999 	0.828500000000000014 	0.309499999999999997 	0.190500000000000003 	0.25 	13 	
-2 	0.54500000000000004 	0.424999999999999989 	0.135000000000000009 	0.844500000000000028 	0.372999999999999998 	0.209999999999999992 	0.234999999999999987 	10 	
-1 	0.5 	0.400000000000000022 	0.119999999999999996 	0.615999999999999992 	0.26100000000000001 	0.142999999999999988 	0.193500000000000005 	8 	
-1 	0.450000000000000011 	0.330000000000000016 	0.104999999999999996 	0.371499999999999997 	0.186499999999999999 	0.0785000000000000003 	0.0975000000000000033 	7 	
-1 	0.409999999999999976 	0.320000000000000007 	0.0950000000000000011 	0.29049999999999998 	0.140999999999999986 	0.0630000000000000004 	0.0729999999999999954 	5 	
-2 	0.604999999999999982 	0.5 	0.174999999999999989 	1.09800000000000009 	0.476499999999999979 	0.232000000000000012 	0.375 	12 	
-1 	0.195000000000000007 	0.135000000000000009 	0.0400000000000000008 	0.0325000000000000011 	0.0134999999999999998 	0.0050000000000000001 	0.00949999999999999976 	4 	
-1 	0.275000000000000022 	0.174999999999999989 	0.0899999999999999967 	0.231500000000000011 	0.096000000000000002 	0.0570000000000000021 	0.0704999999999999932 	5 	
-1 	0.520000000000000018 	0.400000000000000022 	0.130000000000000004 	0.582500000000000018 	0.233000000000000013 	0.13650000000000001 	0.179999999999999993 	10 	
-2 	0.589999999999999969 	0.455000000000000016 	0.160000000000000003 	1.09000000000000008 	0.5 	0.221500000000000002 	0.291999999999999982 	9 	
-0 	0.604999999999999982 	0.494999999999999996 	0.170000000000000012 	1.09149999999999991 	0.436499999999999999 	0.271500000000000019 	0.33500000000000002 	13 	
-2 	0.660000000000000031 	0.494999999999999996 	0.195000000000000007 	1.62749999999999995 	0.593999999999999972 	0.359499999999999986 	0.484999999999999987 	10 	
-2 	0.665000000000000036 	0.525000000000000022 	0.174999999999999989 	1.44300000000000006 	0.663499999999999979 	0.384500000000000008 	0.35299999999999998 	11 	
-1 	0.440000000000000002 	0.344999999999999973 	0.100000000000000006 	0.365999999999999992 	0.121999999999999997 	0.0904999999999999971 	0.119999999999999996 	13 	
-2 	0.400000000000000022 	0.315000000000000002 	0.104999999999999996 	0.286999999999999977 	0.113500000000000004 	0.0369999999999999982 	0.113000000000000003 	10 	
-1 	0.540000000000000036 	0.429999999999999993 	0.140000000000000013 	0.819500000000000006 	0.393500000000000016 	0.172499999999999987 	0.22950000000000001 	9 	
-2 	0.635000000000000009 	0.489999999999999991 	0.160000000000000003 	1.10099999999999998 	0.53400000000000003 	0.186499999999999999 	0.345499999999999974 	10 	
-1 	0.510000000000000009 	0.405000000000000027 	0.125 	0.679499999999999993 	0.346499999999999975 	0.139500000000000013 	0.181999999999999995 	8 	
-0 	0.635000000000000009 	0.489999999999999991 	0.174999999999999989 	1.24350000000000005 	0.580500000000000016 	0.313 	0.304999999999999993 	10 	
-2 	0.709999999999999964 	0.560000000000000053 	0.174999999999999989 	1.72399999999999998 	0.565999999999999948 	0.457500000000000018 	0.462500000000000022 	13 	
-0 	0.665000000000000036 	0.505000000000000004 	0.165000000000000008 	1.34899999999999998 	0.598500000000000032 	0.317500000000000004 	0.359999999999999987 	9 	
-1 	0.440000000000000002 	0.320000000000000007 	0.104999999999999996 	0.387500000000000011 	0.175499999999999989 	0.0739999999999999963 	0.119999999999999996 	9 	
-1 	0.33500000000000002 	0.25 	0.0800000000000000017 	0.169500000000000012 	0.0695000000000000062 	0.0439999999999999974 	0.0495000000000000023 	6 	
-0 	0.660000000000000031 	0.530000000000000027 	0.179999999999999993 	1.51750000000000007 	0.776499999999999968 	0.301999999999999991 	0.401000000000000023 	10 	
-2 	0.344999999999999973 	0.270000000000000018 	0.0899999999999999967 	0.195000000000000007 	0.0779999999999999999 	0.0454999999999999988 	0.0589999999999999969 	9 	
-2 	0.494999999999999996 	0.400000000000000022 	0.140000000000000013 	0.777499999999999969 	0.201500000000000012 	0.179999999999999993 	0.25 	15 	
-0 	0.609999999999999987 	0.469999999999999973 	0.195000000000000007 	1.27350000000000008 	0.468999999999999972 	0.331500000000000017 	0.39800000000000002 	12 	
-1 	0.54500000000000004 	0.429999999999999993 	0.149999999999999994 	0.728500000000000036 	0.301999999999999991 	0.131500000000000006 	0.254500000000000004 	10 	
-1 	0.400000000000000022 	0.299999999999999989 	0.110000000000000001 	0.315000000000000002 	0.109 	0.067000000000000004 	0.119999999999999996 	9 	
-1 	0.325000000000000011 	0.239999999999999991 	0.0700000000000000067 	0.151999999999999996 	0.0565000000000000016 	0.0304999999999999993 	0.0539999999999999994 	8 	
-2 	0.33500000000000002 	0.25 	0.0899999999999999967 	0.180999999999999994 	0.0754999999999999977 	0.0415000000000000022 	0.0599999999999999978 	7 	
-1 	0.375 	0.284999999999999976 	0.0899999999999999967 	0.254500000000000004 	0.118999999999999995 	0.0594999999999999973 	0.0675000000000000044 	6 	
-1 	0.450000000000000011 	0.354999999999999982 	0.119999999999999996 	0.411999999999999977 	0.114500000000000005 	0.0665000000000000036 	0.160000000000000003 	19 	
-1 	0.395000000000000018 	0.294999999999999984 	0.0899999999999999967 	0.302499999999999991 	0.142999999999999988 	0.0665000000000000036 	0.0764999999999999986 	5 	
-1 	0.375 	0.280000000000000027 	0.0850000000000000061 	0.315500000000000003 	0.187 	0.0459999999999999992 	0.067000000000000004 	7 	
-0 	0.635000000000000009 	0.494999999999999996 	0.174999999999999989 	1.21100000000000008 	0.706999999999999962 	0.27250000000000002 	0.323000000000000009 	9 	
-1 	0.325000000000000011 	0.200000000000000011 	0.0800000000000000017 	0.0995000000000000051 	0.0395000000000000004 	0.0224999999999999992 	0.0320000000000000007 	8 	
-1 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.644499999999999962 	0.344999999999999973 	0.128500000000000003 	0.200000000000000011 	8 	
-0 	0.400000000000000022 	0.299999999999999989 	0.115000000000000005 	0.302499999999999991 	0.133500000000000008 	0.0464999999999999997 	0.0934999999999999998 	8 	
-0 	0.564999999999999947 	0.450000000000000011 	0.14499999999999999 	0.849500000000000033 	0.421499999999999986 	0.168500000000000011 	0.225000000000000006 	8 	
-0 	0.619999999999999996 	0.479999999999999982 	0.179999999999999993 	1.22150000000000003 	0.581999999999999962 	0.269500000000000017 	0.313 	12 	
-2 	0.474999999999999978 	0.395000000000000018 	0.135000000000000009 	0.591999999999999971 	0.246499999999999997 	0.164500000000000007 	0.200000000000000011 	13 	
-0 	0.569999999999999951 	0.455000000000000016 	0.165000000000000008 	1.05950000000000011 	0.440000000000000002 	0.219500000000000001 	0.284999999999999976 	14 	
-0 	0.375 	0.28999999999999998 	0.115000000000000005 	0.270500000000000018 	0.0929999999999999993 	0.0660000000000000031 	0.0884999999999999953 	10 	
-1 	0.520000000000000018 	0.380000000000000004 	0.115000000000000005 	0.66449999999999998 	0.328500000000000014 	0.170000000000000012 	0.142499999999999988 	7 	
-1 	0.594999999999999973 	0.429999999999999993 	0.165000000000000008 	0.984500000000000042 	0.452500000000000013 	0.20699999999999999 	0.27250000000000002 	8 	
-2 	0.555000000000000049 	0.424999999999999989 	0.130000000000000004 	0.766499999999999959 	0.264000000000000012 	0.16800000000000001 	0.275000000000000022 	13 	
-2 	0.5 	0.380000000000000004 	0.154999999999999999 	0.660000000000000031 	0.265500000000000014 	0.13650000000000001 	0.214999999999999997 	19 	
-0 	0.665000000000000036 	0.525000000000000022 	0.209999999999999992 	1.64399999999999991 	0.817999999999999949 	0.339500000000000024 	0.427499999999999991 	10 	
-1 	0.130000000000000004 	0.100000000000000006 	0.0299999999999999989 	0.0129999999999999994 	0.00449999999999999966 	0.00300000000000000006 	0.00400000000000000008 	3 	
-0 	0.525000000000000022 	0.424999999999999989 	0.160000000000000003 	0.83550000000000002 	0.354499999999999982 	0.213499999999999995 	0.244999999999999996 	9 	
-0 	0.584999999999999964 	0.465000000000000024 	0.170000000000000012 	0.991500000000000048 	0.38650000000000001 	0.224000000000000005 	0.265000000000000013 	12 	
-2 	0.550000000000000044 	0.424999999999999989 	0.154999999999999999 	0.917499999999999982 	0.277500000000000024 	0.242999999999999994 	0.33500000000000002 	13 	
-1 	0.375 	0.28999999999999998 	0.0950000000000000011 	0.212999999999999995 	0.096000000000000002 	0.0410000000000000017 	0.0609999999999999987 	5 	
-1 	0.340000000000000024 	0.260000000000000009 	0.0899999999999999967 	0.178999999999999992 	0.0759999999999999981 	0.0524999999999999981 	0.0550000000000000003 	6 	
-1 	0.390000000000000013 	0.28999999999999998 	0.100000000000000006 	0.222500000000000003 	0.0950000000000000011 	0.0464999999999999997 	0.0729999999999999954 	7 	
-0 	0.505000000000000004 	0.390000000000000013 	0.174999999999999989 	0.691999999999999948 	0.267000000000000015 	0.149999999999999994 	0.214999999999999997 	12 	
-0 	0.550000000000000044 	0.434999999999999998 	0.140000000000000013 	0.744999999999999996 	0.346999999999999975 	0.173999999999999988 	0.226500000000000007 	9 	
-0 	0.645000000000000018 	0.510000000000000009 	0.179999999999999993 	1.61949999999999994 	0.781499999999999972 	0.322000000000000008 	0.467500000000000027 	12 	
-1 	0.315000000000000002 	0.239999999999999991 	0.0850000000000000061 	0.171500000000000014 	0.0709999999999999937 	0.0345000000000000029 	0.0534999999999999989 	7 	
-2 	0.510000000000000009 	0.395000000000000018 	0.14499999999999999 	0.61850000000000005 	0.215999999999999998 	0.138500000000000012 	0.239999999999999991 	12 	
-2 	0.604999999999999982 	0.469999999999999973 	0.165000000000000008 	1.23150000000000004 	0.602500000000000036 	0.262000000000000011 	0.292499999999999982 	11 	
-0 	0.455000000000000016 	0.349999999999999978 	0.119999999999999996 	0.455500000000000016 	0.194500000000000006 	0.104499999999999996 	0.137500000000000011 	7 	
-1 	0.225000000000000006 	0.165000000000000008 	0.0550000000000000003 	0.0589999999999999969 	0.0269999999999999997 	0.0125000000000000007 	0.0149999999999999994 	4 	
-2 	0.569999999999999951 	0.445000000000000007 	0.149999999999999994 	0.987500000000000044 	0.504000000000000004 	0.20699999999999999 	0.248999999999999999 	8 	
-0 	0.525000000000000022 	0.41499999999999998 	0.170000000000000012 	0.832500000000000018 	0.275500000000000023 	0.168500000000000011 	0.309999999999999998 	13 	
-1 	0.429999999999999993 	0.330000000000000016 	0.100000000000000006 	0.44900000000000001 	0.254000000000000004 	0.0825000000000000039 	0.0970000000000000029 	6 	
-2 	0.550000000000000044 	0.424999999999999989 	0.149999999999999994 	0.831500000000000017 	0.410999999999999976 	0.17649999999999999 	0.216499999999999998 	10 	
-1 	0.33500000000000002 	0.25 	0.0800000000000000017 	0.16700000000000001 	0.0675000000000000044 	0.0325000000000000011 	0.0575000000000000025 	6 	
-1 	0.380000000000000004 	0.275000000000000022 	0.100000000000000006 	0.225500000000000006 	0.0800000000000000017 	0.0490000000000000019 	0.0850000000000000061 	10 	
-1 	0.525000000000000022 	0.390000000000000013 	0.119999999999999996 	0.664000000000000035 	0.311499999999999999 	0.146999999999999992 	0.177999999999999992 	9 	
-1 	0.385000000000000009 	0.280000000000000027 	0.125 	0.243999999999999995 	0.101999999999999993 	0.0379999999999999991 	0.0850000000000000061 	6 	
-2 	0.204999999999999988 	0.154999999999999999 	0.0449999999999999983 	0.0425000000000000031 	0.0170000000000000012 	0.00549999999999999968 	0.0154999999999999999 	7 	
-1 	0.5 	0.375 	0.140000000000000013 	0.549499999999999988 	0.247999999999999998 	0.112000000000000002 	0.158500000000000002 	7 	
-2 	0.555000000000000049 	0.455000000000000016 	0.135000000000000009 	0.836999999999999966 	0.382000000000000006 	0.171000000000000013 	0.234999999999999987 	9 	
-0 	0.57999999999999996 	0.440000000000000002 	0.125 	0.785499999999999976 	0.362999999999999989 	0.195500000000000007 	0.195000000000000007 	11 	
-1 	0.515000000000000013 	0.400000000000000022 	0.125 	0.5625 	0.25 	0.1245 	0.170000000000000012 	7 	
-0 	0.505000000000000004 	0.380000000000000004 	0.130000000000000004 	0.692999999999999949 	0.391000000000000014 	0.119499999999999995 	0.151499999999999996 	8 	
-2 	0.619999999999999996 	0.469999999999999973 	0.149999999999999994 	1.30899999999999994 	0.586999999999999966 	0.440500000000000003 	0.325000000000000011 	9 	
-2 	0.614999999999999991 	0.494999999999999996 	0.154999999999999999 	1.28649999999999998 	0.434999999999999998 	0.292999999999999983 	0.324500000000000011 	11 	
-2 	0.469999999999999973 	0.349999999999999978 	0.100000000000000006 	0.47749999999999998 	0.188500000000000001 	0.0884999999999999953 	0.174999999999999989 	8 	
-0 	0.635000000000000009 	0.510000000000000009 	0.170000000000000012 	1.22350000000000003 	0.532000000000000028 	0.271000000000000019 	0.353999999999999981 	9 	
-0 	0.650000000000000022 	0.5 	0.165000000000000008 	1.14450000000000007 	0.484999999999999987 	0.217999999999999999 	0.364999999999999991 	12 	
-2 	0.270000000000000018 	0.195000000000000007 	0.0800000000000000017 	0.100000000000000006 	0.0384999999999999995 	0.0195 	0.0299999999999999989 	6 	
-2 	0.645000000000000018 	0.5 	0.174999999999999989 	1.51049999999999995 	0.673499999999999988 	0.3755 	0.377500000000000002 	12 	
-1 	0.5 	0.395000000000000018 	0.140000000000000013 	0.621500000000000052 	0.292499999999999982 	0.120499999999999996 	0.195000000000000007 	9 	
-0 	0.455000000000000016 	0.354999999999999982 	1.12999999999999989 	0.593999999999999972 	0.332000000000000017 	0.116000000000000006 	0.133500000000000008 	8 	
-2 	0.574999999999999956 	0.445000000000000007 	0.14499999999999999 	0.876000000000000001 	0.379500000000000004 	0.161500000000000005 	0.270000000000000018 	10 	
-2 	0.685000000000000053 	0.550000000000000044 	0.200000000000000011 	1.77249999999999996 	0.812999999999999945 	0.387000000000000011 	0.489999999999999991 	11 	
-2 	0.535000000000000031 	0.429999999999999993 	0.140000000000000013 	0.716500000000000026 	0.285499999999999976 	0.159500000000000003 	0.215499999999999997 	8 	
-2 	0.574999999999999956 	0.46000000000000002 	0.165000000000000008 	0.91549999999999998 	0.400500000000000023 	0.246499999999999997 	0.23849999999999999 	8 	
-1 	0.375 	0.28999999999999998 	0.100000000000000006 	0.219 	0.0924999999999999989 	0.0379999999999999991 	0.0749999999999999972 	6 	
-0 	0.465000000000000024 	0.375 	0.135000000000000009 	0.599999999999999978 	0.222500000000000003 	0.129000000000000004 	0.23000000000000001 	16 	
-1 	0.440000000000000002 	0.340000000000000024 	0.100000000000000006 	0.406999999999999973 	0.208999999999999991 	0.0734999999999999959 	0.102999999999999994 	7 	
-1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.197000000000000008 	0.081500000000000003 	0.0325000000000000011 	0.0650000000000000022 	6 	
-2 	0.574999999999999956 	0.450000000000000011 	0.184999999999999998 	0.925000000000000044 	0.342000000000000026 	0.197000000000000008 	0.349999999999999978 	12 	
-0 	0.594999999999999973 	0.455000000000000016 	0.154999999999999999 	1.0605 	0.513499999999999956 	0.216499999999999998 	0.299999999999999989 	12 	
-1 	0.390000000000000013 	0.299999999999999989 	0.0950000000000000011 	0.326500000000000012 	0.166500000000000009 	0.0575000000000000025 	0.0889999999999999958 	7 	
-0 	0.450000000000000011 	0.344999999999999973 	0.119999999999999996 	0.416499999999999981 	0.165500000000000008 	0.0950000000000000011 	0.135000000000000009 	9 	
-0 	0.680000000000000049 	0.510000000000000009 	0.200000000000000011 	1.60749999999999993 	0.713999999999999968 	0.339000000000000024 	0.470499999999999974 	11 	
-2 	0.550000000000000044 	0.46000000000000002 	0.174999999999999989 	0.868999999999999995 	0.315500000000000003 	0.182499999999999996 	0.320000000000000007 	10 	
-2 	0.699999999999999956 	0.574999999999999956 	0.190000000000000002 	2.27300000000000013 	1.09499999999999997 	0.417999999999999983 	0.638000000000000012 	12 	
-1 	0.390000000000000013 	0.309999999999999998 	0.100000000000000006 	0.301999999999999991 	0.116000000000000006 	0.0640000000000000013 	0.115000000000000005 	11 	
-0 	0.650000000000000022 	0.494999999999999996 	0.154999999999999999 	1.33699999999999997 	0.614999999999999991 	0.319500000000000006 	0.33500000000000002 	9 	
-0 	0.755000000000000004 	0.574999999999999956 	0.200000000000000011 	2.07299999999999995 	1.01350000000000007 	0.465500000000000025 	0.479999999999999982 	11 	
-2 	0.724999999999999978 	0.564999999999999947 	0.214999999999999997 	1.89100000000000001 	0.697500000000000009 	0.472499999999999976 	0.57999999999999996 	16 	
-0 	0.675000000000000044 	0.564999999999999947 	0.195000000000000007 	1.83749999999999991 	0.764499999999999957 	0.361499999999999988 	0.553000000000000047 	12 	
-0 	0.395000000000000018 	0.315000000000000002 	0.104999999999999996 	0.351499999999999979 	0.118499999999999994 	0.0909999999999999976 	0.119499999999999995 	16 	
-2 	0.560000000000000053 	0.409999999999999976 	0.165000000000000008 	0.930000000000000049 	0.350499999999999978 	0.236999999999999988 	0.299999999999999989 	13 	
-2 	0.530000000000000027 	0.409999999999999976 	0.140000000000000013 	0.68100000000000005 	0.309499999999999997 	0.141499999999999987 	0.183499999999999996 	6 	
-0 	0.594999999999999973 	0.429999999999999993 	0.209999999999999992 	1.52449999999999997 	0.653000000000000025 	0.396000000000000019 	0.409999999999999976 	11 	
-0 	0.594999999999999973 	0.469999999999999973 	0.25 	1.28299999999999992 	0.462000000000000022 	0.247499999999999998 	0.445000000000000007 	14 	
-0 	0.635000000000000009 	0.494999999999999996 	0.179999999999999993 	1.59600000000000009 	0.616999999999999993 	0.317000000000000004 	0.369999999999999996 	11 	
-1 	0.275000000000000022 	0.204999999999999988 	0.0800000000000000017 	0.096000000000000002 	0.0359999999999999973 	0.0184999999999999991 	0.0299999999999999989 	6 	
-2 	0.724999999999999978 	0.574999999999999956 	0.239999999999999991 	2.20999999999999996 	1.35099999999999998 	0.412999999999999978 	0.501499999999999946 	13 	
-0 	0.665000000000000036 	0.555000000000000049 	0.195000000000000007 	1.43849999999999989 	0.580999999999999961 	0.353999999999999981 	0.359999999999999987 	17 	
-0 	0.550000000000000044 	0.429999999999999993 	0.140000000000000013 	0.839999999999999969 	0.375 	0.217999999999999999 	0.194500000000000006 	8 	
-0 	0.434999999999999998 	0.395000000000000018 	0.104999999999999996 	0.36349999999999999 	0.13600000000000001 	0.0980000000000000038 	0.130000000000000004 	9 	
-2 	0.535000000000000031 	0.440000000000000002 	0.149999999999999994 	0.67649999999999999 	0.256000000000000005 	0.139000000000000012 	0.260000000000000009 	12 	
-0 	0.555000000000000049 	0.434999999999999998 	0.165000000000000008 	0.969999999999999973 	0.336000000000000021 	0.231500000000000011 	0.294999999999999984 	17 	
-1 	0.520000000000000018 	0.395000000000000018 	0.125 	0.580500000000000016 	0.244499999999999995 	0.145999999999999991 	0.165000000000000008 	9 	
-0 	0.465000000000000024 	0.349999999999999978 	0.115000000000000005 	0.420999999999999985 	0.1565 	0.0909999999999999976 	0.134500000000000008 	9 	
-1 	0.479999999999999982 	0.369999999999999996 	0.125 	0.473999999999999977 	0.178999999999999992 	0.103499999999999995 	0.174999999999999989 	9 	
-1 	0.655000000000000027 	0.515000000000000013 	0.14499999999999999 	1.25 	0.526499999999999968 	0.282999999999999974 	0.315000000000000002 	15 	
-2 	0.619999999999999996 	0.489999999999999991 	0.154999999999999999 	1.10000000000000009 	0.505000000000000004 	0.247499999999999998 	0.309999999999999998 	9 	
-1 	0.54500000000000004 	0.424999999999999989 	0.140000000000000013 	0.814500000000000002 	0.304999999999999993 	0.231000000000000011 	0.243999999999999995 	10 	
-1 	0.530000000000000027 	0.429999999999999993 	0.140000000000000013 	0.677000000000000046 	0.297999999999999987 	0.0965000000000000024 	0.23000000000000001 	8 	
-0 	0.515000000000000013 	0.375 	0.110000000000000001 	0.606500000000000039 	0.300499999999999989 	0.131000000000000005 	0.149999999999999994 	6 	
-0 	0.599999999999999978 	0.46000000000000002 	0.149999999999999994 	1.2350000000000001 	0.602500000000000036 	0.274000000000000021 	0.28999999999999998 	8 	
-2 	0.614999999999999991 	0.450000000000000011 	0.149999999999999994 	1.19799999999999995 	0.706999999999999962 	0.209499999999999992 	0.2505 	7 	
-1 	0.455000000000000016 	0.344999999999999973 	0.104999999999999996 	0.400500000000000023 	0.164000000000000007 	0.0754999999999999977 	0.126000000000000001 	8 	
-0 	0.525000000000000022 	0.390000000000000013 	0.135000000000000009 	0.600500000000000034 	0.226500000000000007 	0.131000000000000005 	0.209999999999999992 	16 	
-1 	0.455000000000000016 	0.359999999999999987 	0.115000000000000005 	0.457000000000000017 	0.208499999999999991 	0.0855000000000000066 	0.146999999999999992 	10 	
-1 	0.330000000000000016 	0.239999999999999991 	0.0749999999999999972 	0.163000000000000006 	0.0744999999999999968 	0.0330000000000000016 	0.048000000000000001 	6 	
-2 	0.54500000000000004 	0.440000000000000002 	0.119999999999999996 	0.856500000000000039 	0.347499999999999976 	0.171500000000000014 	0.239999999999999991 	12 	
-2 	0.465000000000000024 	0.375 	0.110000000000000001 	0.5 	0.209999999999999992 	0.113000000000000003 	0.150499999999999995 	8 	
-0 	0.614999999999999991 	0.46000000000000002 	0.149999999999999994 	1.02649999999999997 	0.493499999999999994 	0.201000000000000012 	0.274500000000000022 	10 	
-0 	0.530000000000000027 	0.440000000000000002 	0.135000000000000009 	0.783499999999999974 	0.313 	0.171500000000000014 	0.2185 	9 	
-2 	0.640000000000000013 	0.489999999999999991 	0.154999999999999999 	1.12850000000000006 	0.47699999999999998 	0.269000000000000017 	0.340000000000000024 	9 	
-1 	0.170000000000000012 	0.125 	0.0550000000000000003 	0.0235000000000000001 	0.00899999999999999932 	0.00549999999999999968 	0.00800000000000000017 	6 	
-2 	0.635000000000000009 	0.525000000000000022 	0.184999999999999998 	1.40650000000000008 	0.684000000000000052 	0.299999999999999989 	0.3745 	10 	
-0 	0.57999999999999996 	0.445000000000000007 	0.135000000000000009 	0.949999999999999956 	0.483999999999999986 	0.181999999999999995 	0.232500000000000012 	8 	
-2 	0.694999999999999951 	0.560000000000000053 	0.184999999999999998 	1.73999999999999999 	0.885000000000000009 	0.371499999999999997 	0.4375 	10 	
-0 	0.630000000000000004 	0.494999999999999996 	0.190000000000000002 	1.16549999999999998 	0.536000000000000032 	0.211499999999999994 	0.162500000000000006 	10 	
-2 	0.5 	0.385000000000000009 	0.14499999999999999 	0.761499999999999955 	0.245999999999999996 	0.195000000000000007 	0.203999999999999987 	14 	
-2 	0.604999999999999982 	0.479999999999999982 	0.170000000000000012 	1.1835 	0.581999999999999962 	0.236499999999999988 	0.317000000000000004 	10 	
-2 	0.349999999999999978 	0.265000000000000013 	0.110000000000000001 	0.296499999999999986 	0.13650000000000001 	0.0630000000000000004 	0.0850000000000000061 	7 	
-2 	0.515000000000000013 	0.380000000000000004 	0.135000000000000009 	0.661499999999999977 	0.287499999999999978 	0.209499999999999992 	0.154999999999999999 	10 	
-1 	0.320000000000000007 	0.234999999999999987 	0.0800000000000000017 	0.148499999999999993 	0.0640000000000000013 	0.0309999999999999998 	0.0449999999999999983 	6 	
-2 	0.729999999999999982 	0.584999999999999964 	0.225000000000000006 	2.23050000000000015 	1.23950000000000005 	0.421999999999999986 	0.562999999999999945 	14 	
-2 	0.645000000000000018 	0.520000000000000018 	0.174999999999999989 	1.56099999999999994 	0.708999999999999964 	0.355499999999999983 	0.400000000000000022 	8 	
-1 	0.325000000000000011 	0.25 	0.0800000000000000017 	0.173499999999999988 	0.0764999999999999986 	0.0345000000000000029 	0.0490000000000000019 	7 	
-2 	0.5 	0.405000000000000027 	0.154999999999999999 	0.77200000000000002 	0.345999999999999974 	0.153499999999999998 	0.244999999999999996 	12 	
-2 	0.589999999999999969 	0.469999999999999973 	0.14499999999999999 	0.923499999999999988 	0.454500000000000015 	0.172999999999999987 	0.254000000000000004 	9 	
-0 	0.455000000000000016 	0.369999999999999996 	0.104999999999999996 	0.492499999999999993 	0.215999999999999998 	0.1245 	0.135000000000000009 	9 	
-2 	0.640000000000000013 	0.584999999999999964 	0.195000000000000007 	1.64700000000000002 	0.722500000000000031 	0.331000000000000016 	0.470999999999999974 	12 	
-1 	0.46000000000000002 	0.349999999999999978 	0.119999999999999996 	0.48849999999999999 	0.193000000000000005 	0.104999999999999996 	0.154999999999999999 	11 	
-1 	0.494999999999999996 	0.380000000000000004 	0.130000000000000004 	0.512499999999999956 	0.2185 	0.116000000000000006 	0.160000000000000003 	7 	
-1 	0.354999999999999982 	0.270000000000000018 	0.104999999999999996 	0.271000000000000019 	0.142499999999999988 	0.0524999999999999981 	0.0734999999999999959 	9 	
-1 	0.540000000000000036 	0.419999999999999984 	0.140000000000000013 	0.726500000000000035 	0.320500000000000007 	0.14449999999999999 	0.229000000000000009 	9 	
-0 	0.729999999999999982 	0.555000000000000049 	0.179999999999999993 	1.6895 	0.655499999999999972 	0.196500000000000008 	0.493499999999999994 	10 	
-1 	0.465000000000000024 	0.364999999999999991 	0.115000000000000005 	0.467000000000000026 	0.231500000000000011 	0.0924999999999999989 	0.113000000000000003 	7 	
-2 	0.57999999999999996 	0.450000000000000011 	0.140000000000000013 	0.961500000000000021 	0.485999999999999988 	0.181499999999999995 	0.253000000000000003 	9 	
-2 	0.604999999999999982 	0.455000000000000016 	0.160000000000000003 	1.10349999999999993 	0.420999999999999985 	0.30149999999999999 	0.325000000000000011 	9 	
-0 	0.724999999999999978 	0.564999999999999947 	0.209999999999999992 	2.14250000000000007 	1.03000000000000003 	0.486999999999999988 	0.503000000000000003 	14 	
-0 	0.675000000000000044 	0.530000000000000027 	0.174999999999999989 	1.4464999999999999 	0.677499999999999991 	0.330000000000000016 	0.389000000000000012 	10 	
-0 	0.440000000000000002 	0.354999999999999982 	0.115000000000000005 	0.41499999999999998 	0.158500000000000002 	0.0924999999999999989 	0.131000000000000005 	11 	
-2 	0.719999999999999973 	0.550000000000000044 	0.204999999999999988 	2.16500000000000004 	1.10549999999999993 	0.525000000000000022 	0.404000000000000026 	10 	
-0 	0.655000000000000027 	0.525000000000000022 	0.190000000000000002 	1.35949999999999993 	0.563999999999999946 	0.321500000000000008 	0.398500000000000021 	10 	
-2 	0.560000000000000053 	0.41499999999999998 	0.14499999999999999 	0.85199999999999998 	0.429999999999999993 	0.188500000000000001 	0.204999999999999988 	8 	
-0 	0.465000000000000024 	0.380000000000000004 	0.135000000000000009 	0.578999999999999959 	0.20799999999999999 	0.1095 	0.220000000000000001 	14 	
-2 	0.650000000000000022 	0.520000000000000018 	0.209999999999999992 	1.6785000000000001 	0.666499999999999981 	0.307999999999999996 	0.46000000000000002 	11 	
-0 	0.619999999999999996 	0.489999999999999991 	0.160000000000000003 	1.05600000000000005 	0.492999999999999994 	0.243999999999999995 	0.27250000000000002 	9 	
-2 	0.540000000000000036 	0.419999999999999984 	0.119999999999999996 	0.811499999999999999 	0.392000000000000015 	0.14549999999999999 	0.223500000000000004 	9 	
-2 	0.630000000000000004 	0.494999999999999996 	0.160000000000000003 	1.09299999999999997 	0.496999999999999997 	0.221000000000000002 	0.315000000000000002 	12 	
-2 	0.614999999999999991 	0.474999999999999978 	0.174999999999999989 	1.10299999999999998 	0.463500000000000023 	0.309499999999999997 	0.27250000000000002 	10 	
-0 	0.645000000000000018 	0.484999999999999987 	0.149999999999999994 	1.15100000000000002 	0.593500000000000028 	0.231500000000000011 	0.292999999999999983 	12 	
-0 	0.380000000000000004 	0.28999999999999998 	0.104999999999999996 	0.257000000000000006 	0.0990000000000000047 	0.0509999999999999967 	0.0850000000000000061 	10 	
-2 	0.505000000000000004 	0.385000000000000009 	0.14499999999999999 	0.677499999999999991 	0.235999999999999988 	0.178999999999999992 	0.200000000000000011 	15 	
-1 	0.419999999999999984 	0.315000000000000002 	0.115000000000000005 	0.354999999999999982 	0.189500000000000002 	0.0650000000000000022 	0.086999999999999994 	6 	
-0 	0.599999999999999978 	0.479999999999999982 	0.179999999999999993 	1.0645 	0.449500000000000011 	0.245499999999999996 	0.325000000000000011 	10 	
-2 	0.564999999999999947 	0.455000000000000016 	0.184999999999999998 	0.92649999999999999 	0.353999999999999981 	0.157500000000000001 	0.375 	16 	
-2 	0.589999999999999969 	0.489999999999999991 	0.135000000000000009 	1.00800000000000001 	0.421999999999999986 	0.224500000000000005 	0.284999999999999976 	11 	
-2 	0.469999999999999973 	0.369999999999999996 	0.179999999999999993 	0.510000000000000009 	0.191500000000000004 	0.128500000000000003 	0.162500000000000006 	9 	
-1 	0.419999999999999984 	0.309999999999999998 	0.0950000000000000011 	0.279000000000000026 	0.1255 	0.0509999999999999967 	0.0879999999999999949 	6 	
-0 	0.625 	0.515000000000000013 	0.154999999999999999 	1.16349999999999998 	0.487499999999999989 	0.259000000000000008 	0.354999999999999982 	11 	
-0 	0.560000000000000053 	0.455000000000000016 	0.160000000000000003 	0.966999999999999971 	0.452500000000000013 	0.20699999999999999 	0.274000000000000021 	9 	
-1 	0.555000000000000049 	0.434999999999999998 	0.14499999999999999 	0.697500000000000009 	0.262000000000000011 	0.157500000000000001 	0.239999999999999991 	11 	
-2 	0.479999999999999982 	0.375 	0.115000000000000005 	0.67649999999999999 	0.320500000000000007 	0.106499999999999997 	0.170000000000000012 	6 	
-1 	0.510000000000000009 	0.400000000000000022 	0.125 	0.557499999999999996 	0.26150000000000001 	0.119499999999999995 	0.152499999999999997 	9 	
-1 	0.395000000000000018 	0.28999999999999998 	0.0950000000000000011 	0.299999999999999989 	0.158000000000000002 	0.0680000000000000049 	0.0779999999999999999 	7 	
-0 	0.614999999999999991 	0.474999999999999978 	0.170000000000000012 	1.05499999999999994 	0.543000000000000038 	0.245999999999999996 	0.234499999999999986 	9 	
-1 	0.390000000000000013 	0.284999999999999976 	0.100000000000000006 	0.281000000000000028 	0.127500000000000002 	0.0619999999999999996 	0.076999999999999999 	7 	
-2 	0.450000000000000011 	0.330000000000000016 	0.104999999999999996 	0.495499999999999996 	0.257500000000000007 	0.0820000000000000034 	0.129000000000000004 	8 	
-0 	0.515000000000000013 	0.41499999999999998 	0.130000000000000004 	0.764000000000000012 	0.276000000000000023 	0.196000000000000008 	0.25 	13 	
-2 	0.584999999999999964 	0.455000000000000016 	0.225000000000000006 	1.05499999999999994 	0.381500000000000006 	0.221000000000000002 	0.364999999999999991 	15 	
-1 	0.445000000000000007 	0.340000000000000024 	0.14499999999999999 	0.433999999999999997 	0.194500000000000006 	0.0904999999999999971 	0.130000000000000004 	7 	
-0 	0.550000000000000044 	0.434999999999999998 	0.170000000000000012 	0.884000000000000008 	0.287499999999999978 	0.164500000000000007 	0.280000000000000027 	14 	
-1 	0.635000000000000009 	0.505000000000000004 	0.190000000000000002 	1.33149999999999991 	0.580500000000000016 	0.252000000000000002 	0.434999999999999998 	17 	
-2 	0.635000000000000009 	0.515000000000000013 	0.170000000000000012 	1.27499999999999991 	0.509000000000000008 	0.285999999999999976 	0.340000000000000024 	16 	
-0 	0.584999999999999964 	0.434999999999999998 	0.140000000000000013 	0.695500000000000007 	0.308499999999999996 	0.129000000000000004 	0.224500000000000005 	8 	
-2 	0.660000000000000031 	0.530000000000000027 	0.195000000000000007 	1.55049999999999999 	0.650499999999999967 	0.329500000000000015 	0.494999999999999996 	10 	
-0 	0.614999999999999991 	0.520000000000000018 	0.149999999999999994 	1.34349999999999992 	0.629000000000000004 	0.260500000000000009 	0.344999999999999973 	10 	
-1 	0.520000000000000018 	0.409999999999999976 	0.110000000000000001 	0.518499999999999961 	0.216499999999999998 	0.091499999999999998 	0.183999999999999997 	8 	
-1 	0.535000000000000031 	0.419999999999999984 	0.14499999999999999 	0.926000000000000045 	0.39800000000000002 	0.196500000000000008 	0.25 	17 	
-1 	0.315000000000000002 	0.23000000000000001 	0.0899999999999999967 	0.128500000000000003 	0.0429999999999999966 	0.0400000000000000008 	0.0400000000000000008 	7 	
-0 	0.625 	0.484999999999999987 	0.174999999999999989 	1.3620000000000001 	0.67649999999999999 	0.26150000000000001 	0.370499999999999996 	10 	
-1 	0.424999999999999989 	0.315000000000000002 	0.0800000000000000017 	0.302999999999999992 	0.131000000000000005 	0.0585000000000000034 	0.0950000000000000011 	7 	
-2 	0.390000000000000013 	0.280000000000000027 	0.125 	0.563999999999999946 	0.303499999999999992 	0.0955000000000000016 	0.142999999999999988 	7 	
-2 	0.380000000000000004 	0.28999999999999998 	0.119999999999999996 	0.282999999999999974 	0.117499999999999993 	0.0655000000000000027 	0.0850000000000000061 	9 	
-2 	0.325000000000000011 	0.239999999999999991 	0.0749999999999999972 	0.154999999999999999 	0.0475000000000000006 	0.0354999999999999968 	0.0599999999999999978 	9 	
-2 	0.650000000000000022 	0.510000000000000009 	0.190000000000000002 	1.54200000000000004 	0.715500000000000025 	0.373499999999999999 	0.375 	9 	
-2 	0.645000000000000018 	0.484999999999999987 	0.214999999999999997 	1.51400000000000001 	0.546000000000000041 	0.26150000000000001 	0.635000000000000009 	16 	
-1 	0.5 	0.380000000000000004 	0.135000000000000009 	0.52849999999999997 	0.226000000000000006 	0.122999999999999998 	0.208999999999999991 	8 	
-0 	0.645000000000000018 	0.5 	0.200000000000000011 	1.4285000000000001 	0.639000000000000012 	0.304999999999999993 	0.359999999999999987 	11 	
-0 	0.614999999999999991 	0.494999999999999996 	0.165000000000000008 	1.19799999999999995 	0.541499999999999981 	0.286499999999999977 	0.318500000000000005 	10 	
-2 	0.445000000000000007 	0.320000000000000007 	0.119999999999999996 	0.413999999999999979 	0.19900000000000001 	0.0899999999999999967 	0.117000000000000007 	7 	
-0 	0.675000000000000044 	0.550000000000000044 	0.190000000000000002 	1.55099999999999993 	0.71050000000000002 	0.368499999999999994 	0.411999999999999977 	13 	
-0 	0.46000000000000002 	0.380000000000000004 	0.130000000000000004 	0.639000000000000012 	0.299999999999999989 	0.152499999999999997 	0.160000000000000003 	11 	
-0 	0.5 	0.380000000000000004 	0.154999999999999999 	0.655000000000000027 	0.240499999999999992 	0.142999999999999988 	0.204999999999999988 	17 	
-1 	0.315000000000000002 	0.23000000000000001 	0.0800000000000000017 	0.137500000000000011 	0.0544999999999999998 	0.0309999999999999998 	0.0444999999999999979 	5 	
-0 	0.709999999999999964 	0.5 	0.149999999999999994 	1.3165 	0.683499999999999996 	0.281499999999999972 	0.280000000000000027 	10 	
-0 	0.515000000000000013 	0.419999999999999984 	0.135000000000000009 	0.629499999999999948 	0.281499999999999972 	0.127000000000000002 	0.214999999999999997 	9 	
-2 	0.655000000000000027 	0.57999999999999996 	0.204999999999999988 	2.08049999999999979 	0.958999999999999964 	0.341500000000000026 	0.600999999999999979 	17 	
-1 	0.469999999999999973 	0.369999999999999996 	0.119999999999999996 	0.470499999999999974 	0.184499999999999997 	0.105499999999999997 	0.154999999999999999 	12 	
-2 	0.630000000000000004 	0.484999999999999987 	0.14499999999999999 	1.06200000000000006 	0.50649999999999995 	0.178499999999999992 	0.336500000000000021 	12 	
-2 	0.619999999999999996 	0.510000000000000009 	0.174999999999999989 	1.61499999999999999 	0.510499999999999954 	0.192000000000000004 	0.675000000000000044 	12 	
-0 	0.609999999999999987 	0.450000000000000011 	0.160000000000000003 	1.1359999999999999 	0.413999999999999979 	0.310999999999999999 	0.299999999999999989 	9 	
-0 	0.635000000000000009 	0.505000000000000004 	0.14499999999999999 	1.13450000000000006 	0.505000000000000004 	0.265500000000000014 	0.315000000000000002 	10 	
-1 	0.424999999999999989 	0.344999999999999973 	0.125 	0.424999999999999989 	0.160000000000000003 	0.0795000000000000012 	0.153999999999999998 	13 	
-1 	0.405000000000000027 	0.299999999999999989 	0.119999999999999996 	0.32400000000000001 	0.126500000000000001 	0.0700000000000000067 	0.110000000000000001 	7 	
-1 	0.214999999999999997 	0.170000000000000012 	0.0550000000000000003 	0.0604999999999999982 	0.0205000000000000009 	0.0140000000000000003 	0.0200000000000000004 	6 	
-1 	0.469999999999999973 	0.375 	0.104999999999999996 	0.441000000000000003 	0.16700000000000001 	0.0864999999999999936 	0.14499999999999999 	10 	
-2 	0.584999999999999964 	0.479999999999999982 	0.184999999999999998 	1.04000000000000004 	0.433999999999999997 	0.265000000000000013 	0.284999999999999976 	10 	
-1 	0.390000000000000013 	0.28999999999999998 	0.0899999999999999967 	0.262500000000000011 	0.117000000000000007 	0.0539999999999999994 	0.076999999999999999 	7 	
-2 	0.419999999999999984 	0.344999999999999973 	0.104999999999999996 	0.429999999999999993 	0.174999999999999989 	0.096000000000000002 	0.130000000000000004 	7 	
-0 	0.650000000000000022 	0.5 	0.160000000000000003 	1.38250000000000006 	0.701999999999999957 	0.303999999999999992 	0.319500000000000006 	9 	
-2 	0.569999999999999951 	0.450000000000000011 	0.140000000000000013 	0.79500000000000004 	0.338500000000000023 	0.147999999999999993 	0.244999999999999996 	9 	
-1 	0.540000000000000036 	0.424999999999999989 	0.140000000000000013 	0.741999999999999993 	0.320000000000000007 	0.139500000000000013 	0.25 	9 	
-1 	0.535000000000000031 	0.455000000000000016 	0.140000000000000013 	1.00150000000000006 	0.530000000000000027 	0.17649999999999999 	0.243999999999999995 	9 	
-2 	0.385000000000000009 	0.309999999999999998 	0.100000000000000006 	0.284499999999999975 	0.106499999999999997 	0.0749999999999999972 	0.100000000000000006 	11 	
-2 	0.574999999999999956 	0.445000000000000007 	0.160000000000000003 	0.838999999999999968 	0.400500000000000023 	0.198000000000000009 	0.23899999999999999 	9 	
-1 	0.510000000000000009 	0.405000000000000027 	0.135000000000000009 	0.769000000000000017 	0.365499999999999992 	0.158500000000000002 	0.179999999999999993 	7 	
-2 	0.719999999999999973 	0.564999999999999947 	0.200000000000000011 	2.10550000000000015 	1.0169999999999999 	0.362999999999999989 	0.493999999999999995 	12 	
-2 	0.685000000000000053 	0.535000000000000031 	0.154999999999999999 	1.38450000000000006 	0.661499999999999977 	0.214499999999999996 	0.407499999999999973 	10 	
-2 	0.515000000000000013 	0.400000000000000022 	0.140000000000000013 	0.633499999999999952 	0.287999999999999978 	0.14499999999999999 	0.16800000000000001 	9 	
-1 	0.344999999999999973 	0.265000000000000013 	0.100000000000000006 	0.245499999999999996 	0.111000000000000001 	0.0534999999999999989 	0.0650000000000000022 	7 	
-0 	0.469999999999999973 	0.364999999999999991 	0.119999999999999996 	0.543000000000000038 	0.22950000000000001 	0.149499999999999994 	0.149999999999999994 	9 	
-1 	0.614999999999999991 	0.489999999999999991 	0.154999999999999999 	0.988500000000000045 	0.41449999999999998 	0.195000000000000007 	0.344999999999999973 	13 	
-2 	0.660000000000000031 	0.54500000000000004 	0.184999999999999998 	1.32000000000000006 	0.530499999999999972 	0.263500000000000012 	0.455000000000000016 	16 	
-1 	0.380000000000000004 	0.28999999999999998 	0.0850000000000000061 	0.228500000000000009 	0.0879999999999999949 	0.0464999999999999997 	0.0749999999999999972 	7 	
-2 	0.569999999999999951 	0.440000000000000002 	0.0950000000000000011 	0.826999999999999957 	0.339500000000000024 	0.221500000000000002 	0.234999999999999987 	8 	
-1 	0.400000000000000022 	0.309999999999999998 	0.100000000000000006 	0.287499999999999978 	0.114500000000000005 	0.0635000000000000009 	0.0950000000000000011 	10 	
-0 	0.530000000000000027 	0.429999999999999993 	0.170000000000000012 	0.775000000000000022 	0.349999999999999978 	0.151999999999999996 	0.234999999999999987 	17 	
-0 	0.630000000000000004 	0.479999999999999982 	0.149999999999999994 	1.05249999999999999 	0.392000000000000015 	0.336000000000000021 	0.284999999999999976 	12 	
-2 	0.525000000000000022 	0.405000000000000027 	0.119999999999999996 	0.755499999999999949 	0.3755 	0.155499999999999999 	0.201000000000000012 	9 	
-0 	0.584999999999999964 	0.455000000000000016 	0.130000000000000004 	0.875499999999999945 	0.410999999999999976 	0.206499999999999989 	0.225000000000000006 	8 	
-2 	0.445000000000000007 	0.354999999999999982 	0.110000000000000001 	0.441500000000000004 	0.180499999999999994 	0.103499999999999995 	0.150499999999999995 	10 	
-2 	0.694999999999999951 	0.515000000000000013 	0.174999999999999989 	1.51649999999999996 	0.577999999999999958 	0.410499999999999976 	0.390000000000000013 	15 	
-1 	0.390000000000000013 	0.294999999999999984 	0.100000000000000006 	0.279000000000000026 	0.115500000000000005 	0.0589999999999999969 	0.0800000000000000017 	7 	
-0 	0.709999999999999964 	0.569999999999999951 	0.195000000000000007 	1.98049999999999993 	0.992500000000000049 	0.492499999999999993 	0.479999999999999982 	12 	
-2 	0.734999999999999987 	0.589999999999999969 	0.214999999999999997 	1.74700000000000011 	0.727500000000000036 	0.403000000000000025 	0.557000000000000051 	11 	
-0 	0.625 	0.5 	0.160000000000000003 	1.21700000000000008 	0.572500000000000009 	0.20699999999999999 	0.354999999999999982 	11 	
-0 	0.574999999999999956 	0.46000000000000002 	0.14499999999999999 	0.994500000000000051 	0.466000000000000025 	0.229000000000000009 	0.265000000000000013 	7 	
-0 	0.755000000000000004 	0.625 	0.209999999999999992 	2.50499999999999989 	1.1964999999999999 	0.513000000000000012 	0.678499999999999992 	11 	
-1 	0.46000000000000002 	0.349999999999999978 	0.100000000000000006 	0.470999999999999974 	0.252000000000000002 	0.076999999999999999 	0.122999999999999998 	8 	
-0 	0.564999999999999947 	0.450000000000000011 	0.135000000000000009 	0.988500000000000045 	0.387000000000000011 	0.149499999999999994 	0.309999999999999998 	12 	
-0 	0.564999999999999947 	0.440000000000000002 	0.154999999999999999 	0.939500000000000002 	0.427499999999999991 	0.213999999999999996 	0.270000000000000018 	12 	
-1 	0.650000000000000022 	0.520000000000000018 	0.149999999999999994 	1.23799999999999999 	0.549499999999999988 	0.295999999999999985 	0.330500000000000016 	10 	
-0 	0.635000000000000009 	0.505000000000000004 	0.154999999999999999 	1.28950000000000009 	0.593999999999999972 	0.314000000000000001 	0.344999999999999973 	11 	
-2 	0.450000000000000011 	0.33500000000000002 	0.125 	0.348999999999999977 	0.118999999999999995 	0.105499999999999997 	0.115000000000000005 	10 	
-0 	0.609999999999999987 	0.474999999999999978 	0.140000000000000013 	1.13300000000000001 	0.527499999999999969 	0.235499999999999987 	0.349999999999999978 	11 	
-2 	0.689999999999999947 	0.525000000000000022 	0.200000000000000011 	1.78249999999999997 	0.916499999999999981 	0.332500000000000018 	0.461000000000000021 	12 	
-2 	0.655000000000000027 	0.489999999999999991 	0.174999999999999989 	1.35850000000000004 	0.639499999999999957 	0.293999999999999984 	0.364999999999999991 	10 	
-2 	0.530000000000000027 	0.409999999999999976 	0.154999999999999999 	0.715500000000000025 	0.280500000000000027 	0.168500000000000011 	0.213999999999999996 	11 	
-1 	0.505000000000000004 	0.405000000000000027 	0.130000000000000004 	0.601500000000000035 	0.30149999999999999 	0.110000000000000001 	0.179999999999999993 	8 	
-1 	0.409999999999999976 	0.315000000000000002 	0.100000000000000006 	0.299999999999999989 	0.123999999999999999 	0.0575000000000000025 	0.100000000000000006 	8 	
-1 	0.484999999999999987 	0.380000000000000004 	0.119999999999999996 	0.472499999999999976 	0.20749999999999999 	0.107499999999999998 	0.146999999999999992 	6 	
-1 	0.450000000000000011 	0.354999999999999982 	0.110000000000000001 	0.458500000000000019 	0.194000000000000006 	0.067000000000000004 	0.140000000000000013 	8 	
-2 	0.599999999999999978 	0.489999999999999991 	0.209999999999999992 	1.98750000000000004 	1.00499999999999989 	0.418999999999999984 	0.490999999999999992 	10 	
-2 	0.530000000000000027 	0.419999999999999984 	0.165000000000000008 	0.894499999999999962 	0.319000000000000006 	0.23899999999999999 	0.244999999999999996 	11 	
-2 	0.550000000000000044 	0.450000000000000011 	0.149999999999999994 	1.01449999999999996 	0.406999999999999973 	0.201500000000000012 	0.287499999999999978 	10 	
-1 	0.465000000000000024 	0.380000000000000004 	0.130000000000000004 	0.454000000000000015 	0.189500000000000002 	0.0800000000000000017 	0.154999999999999999 	11 	
-2 	0.474999999999999978 	0.354999999999999982 	0.125 	0.462500000000000022 	0.185999999999999999 	0.106999999999999998 	0.14499999999999999 	9 	
-2 	0.594999999999999973 	0.489999999999999991 	0.184999999999999998 	1.18500000000000005 	0.481999999999999984 	0.201500000000000012 	0.360999999999999988 	10 	
-2 	0.275000000000000022 	0.204999999999999988 	0.0700000000000000067 	0.0940000000000000002 	0.033500000000000002 	0.0200000000000000004 	0.0325000000000000011 	5 	
-1 	0.349999999999999978 	0.265000000000000013 	0.0800000000000000017 	0.200000000000000011 	0.0899999999999999967 	0.0420000000000000026 	0.0599999999999999978 	7 	
-0 	0.685000000000000053 	0.540000000000000036 	0.160000000000000003 	1.66749999999999998 	0.832999999999999963 	0.377500000000000002 	0.474999999999999978 	11 	
-0 	0.5 	0.375 	0.140000000000000013 	0.603999999999999981 	0.241999999999999993 	0.141499999999999987 	0.178999999999999992 	15 	
-2 	0.699999999999999956 	0.550000000000000044 	0.200000000000000011 	1.52299999999999991 	0.692999999999999949 	0.305999999999999994 	0.440500000000000003 	13 	
-0 	0.589999999999999969 	0.450000000000000011 	0.160000000000000003 	0.900000000000000022 	0.357999999999999985 	0.156 	0.315000000000000002 	19 	
-1 	0.405000000000000027 	0.309999999999999998 	0.0950000000000000011 	0.342500000000000027 	0.178499999999999992 	0.0640000000000000013 	0.0855000000000000066 	8 	
-0 	0.619999999999999996 	0.479999999999999982 	0.170000000000000012 	1.10450000000000004 	0.535000000000000031 	0.25 	0.286999999999999977 	10 	
-1 	0.474999999999999978 	0.354999999999999982 	0.104999999999999996 	0.468000000000000027 	0.201000000000000012 	0.111500000000000002 	0.119999999999999996 	8 	
-1 	0.530000000000000027 	0.405000000000000027 	0.119999999999999996 	0.632000000000000006 	0.271500000000000019 	0.147999999999999993 	0.1875 	9 	
-1 	0.5 	0.375 	0.119999999999999996 	0.529000000000000026 	0.223500000000000004 	0.122999999999999998 	0.160000000000000003 	8 	
-2 	0.70499999999999996 	0.564999999999999947 	0.515000000000000013 	2.20999999999999996 	1.10749999999999993 	0.486499999999999988 	0.512000000000000011 	10 	
-2 	0.635000000000000009 	0.494999999999999996 	0.149999999999999994 	1.08099999999999996 	0.482499999999999984 	0.241999999999999993 	0.309999999999999998 	11 	
-0 	0.67000000000000004 	0.515000000000000013 	0.170000000000000012 	1.4265000000000001 	0.660499999999999976 	0.339500000000000024 	0.369999999999999996 	11 	
-0 	0.505000000000000004 	0.375 	0.179999999999999993 	0.567999999999999949 	0.232500000000000012 	0.149499999999999994 	0.170000000000000012 	12 	
-1 	0.385000000000000009 	0.280000000000000027 	0.0950000000000000011 	0.257000000000000006 	0.118999999999999995 	0.0589999999999999969 	0.0700000000000000067 	7 	
-0 	0.440000000000000002 	0.330000000000000016 	0.115000000000000005 	0.400500000000000023 	0.142999999999999988 	0.113000000000000003 	0.119999999999999996 	8 	
-2 	0.625 	0.465000000000000024 	0.140000000000000013 	1.19500000000000006 	0.482499999999999984 	0.204999999999999988 	0.400000000000000022 	13 	
-0 	0.650000000000000022 	0.510000000000000009 	0.170000000000000012 	1.56699999999999995 	0.724500000000000033 	0.348999999999999977 	0.391000000000000014 	10 	
-0 	0.550000000000000044 	0.429999999999999993 	0.149999999999999994 	0.839999999999999969 	0.395000000000000018 	0.195000000000000007 	0.223000000000000004 	8 	
-1 	0.484999999999999987 	0.369999999999999996 	0.130000000000000004 	0.458000000000000018 	0.180999999999999994 	0.113000000000000003 	0.13600000000000001 	10 	
-2 	0.640000000000000013 	0.510000000000000009 	0.190000000000000002 	1.61299999999999999 	0.621500000000000052 	0.360999999999999988 	0.469999999999999973 	14 	
-0 	0.540000000000000036 	0.474999999999999978 	0.154999999999999999 	0.928000000000000047 	0.394000000000000017 	0.194000000000000006 	0.260000000000000009 	11 	
-1 	0.530000000000000027 	0.429999999999999993 	0.135000000000000009 	0.625499999999999945 	0.244999999999999996 	0.14549999999999999 	0.213499999999999995 	10 	
-0 	0.57999999999999996 	0.465000000000000024 	0.14499999999999999 	0.986500000000000044 	0.469999999999999973 	0.215499999999999997 	0.25 	11 	
-1 	0.184999999999999998 	0.135000000000000009 	0.0400000000000000008 	0.0269999999999999997 	0.0105000000000000007 	0.00549999999999999968 	0.00899999999999999932 	5 	
-1 	0.474999999999999978 	0.375 	0.110000000000000001 	0.456000000000000016 	0.181999999999999995 	0.0990000000000000047 	0.160000000000000003 	9 	
-0 	0.455000000000000016 	0.354999999999999982 	0.119999999999999996 	0.449500000000000011 	0.176999999999999991 	0.103999999999999995 	0.149999999999999994 	9 	
-2 	0.505000000000000004 	0.395000000000000018 	0.135000000000000009 	0.591500000000000026 	0.287999999999999978 	0.131500000000000006 	0.184999999999999998 	12 	
-2 	0.550000000000000044 	0.424999999999999989 	0.160000000000000003 	0.793000000000000038 	0.343000000000000027 	0.203499999999999986 	0.214999999999999997 	9 	
-0 	0.650000000000000022 	0.515000000000000013 	0.195000000000000007 	1.40050000000000008 	0.519499999999999962 	0.359999999999999987 	0.440000000000000002 	13 	
-1 	0.429999999999999993 	0.325000000000000011 	0.0899999999999999967 	0.424999999999999989 	0.216999999999999998 	0.086999999999999994 	0.0950000000000000011 	7 	
-2 	0.645000000000000018 	0.510000000000000009 	0.154999999999999999 	1.53899999999999992 	0.640499999999999958 	0.358499999999999985 	0.429999999999999993 	11 	
-2 	0.574999999999999956 	0.469999999999999973 	0.140000000000000013 	0.837500000000000022 	0.348499999999999976 	0.173499999999999988 	0.239999999999999991 	11 	
-1 	0.614999999999999991 	0.474999999999999978 	0.130000000000000004 	0.842500000000000027 	0.35299999999999998 	0.191500000000000004 	0.251000000000000001 	8 	
-0 	0.619999999999999996 	0.474999999999999978 	0.174999999999999989 	1.01649999999999996 	0.435499999999999998 	0.213999999999999996 	0.325000000000000011 	10 	
-1 	0.625 	0.465000000000000024 	0.154999999999999999 	0.971999999999999975 	0.404000000000000026 	0.184499999999999997 	0.349999999999999978 	14 	
-1 	0.320000000000000007 	0.255000000000000004 	0.100000000000000006 	0.175499999999999989 	0.0729999999999999954 	0.0415000000000000022 	0.0650000000000000022 	7 	
-2 	0.484999999999999987 	0.369999999999999996 	0.140000000000000013 	0.572500000000000009 	0.203999999999999987 	0.141499999999999987 	0.174999999999999989 	10 	
-2 	0.589999999999999969 	0.440000000000000002 	0.149999999999999994 	0.955500000000000016 	0.365999999999999992 	0.242499999999999993 	0.294999999999999984 	11 	
-2 	0.550000000000000044 	0.450000000000000011 	0.154999999999999999 	0.78949999999999998 	0.343000000000000027 	0.159000000000000002 	0.25 	12 	
-0 	0.489999999999999991 	0.385000000000000009 	0.149999999999999994 	0.786499999999999977 	0.240999999999999992 	0.140000000000000013 	0.239999999999999991 	23 	
-0 	0.67000000000000004 	0.520000000000000018 	0.190000000000000002 	1.32000000000000006 	0.523499999999999965 	0.309499999999999997 	0.427499999999999991 	13 	
-0 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.828500000000000014 	0.361999999999999988 	0.165500000000000008 	0.235999999999999988 	10 	
-2 	0.555000000000000049 	0.400000000000000022 	0.130000000000000004 	0.707500000000000018 	0.332000000000000017 	0.158500000000000002 	0.179999999999999993 	7 	
-1 	0.405000000000000027 	0.309999999999999998 	0.0650000000000000022 	0.320500000000000007 	0.157500000000000001 	0.0660000000000000031 	0.0879999999999999949 	6 	
-1 	0.354999999999999982 	0.284999999999999976 	0.0950000000000000011 	0.227500000000000008 	0.0955000000000000016 	0.0475000000000000006 	0.0714999999999999941 	6 	
-0 	0.520000000000000018 	0.409999999999999976 	0.125 	0.69850000000000001 	0.294499999999999984 	0.162500000000000006 	0.214999999999999997 	10 	
-0 	0.645000000000000018 	0.520000000000000018 	0.170000000000000012 	1.19700000000000006 	0.526000000000000023 	0.292499999999999982 	0.317000000000000004 	11 	
-1 	0.375 	0.299999999999999989 	0.0749999999999999972 	0.143999999999999989 	0.0589999999999999969 	0.0299999999999999989 	0.0439999999999999974 	7 	
-1 	0.349999999999999978 	0.260000000000000009 	0.0850000000000000061 	0.173999999999999988 	0.0704999999999999932 	0.0345000000000000029 	0.0599999999999999978 	10 	
-2 	0.354999999999999982 	0.265000000000000013 	0.0850000000000000061 	0.201000000000000012 	0.0690000000000000058 	0.0529999999999999985 	0.0695000000000000062 	8 	
-1 	0.510000000000000009 	0.380000000000000004 	0.115000000000000005 	0.515499999999999958 	0.214999999999999997 	0.113500000000000004 	0.166000000000000009 	8 	
-2 	0.564999999999999947 	0.390000000000000013 	0.125 	0.743999999999999995 	0.35199999999999998 	0.130000000000000004 	0.168500000000000011 	11 	
-2 	0.609999999999999987 	0.474999999999999978 	0.174999999999999989 	1.02400000000000002 	0.408999999999999975 	0.26100000000000001 	0.322000000000000008 	9 	
-2 	0.619999999999999996 	0.5 	0.165000000000000008 	1.30699999999999994 	0.635499999999999954 	0.254500000000000004 	0.315000000000000002 	9 	
-0 	0.344999999999999973 	0.260000000000000009 	0.0899999999999999967 	0.20699999999999999 	0.0774999999999999994 	0.043499999999999997 	0.0764999999999999986 	10 	
-2 	0.505000000000000004 	0.390000000000000013 	0.119999999999999996 	0.653000000000000025 	0.331500000000000017 	0.138500000000000012 	0.16700000000000001 	9 	
-0 	0.369999999999999996 	0.275000000000000022 	0.100000000000000006 	0.222500000000000003 	0.0929999999999999993 	0.0259999999999999988 	0.0800000000000000017 	8 	
-2 	0.550000000000000044 	0.429999999999999993 	0.149999999999999994 	0.874500000000000055 	0.412999999999999978 	0.190500000000000003 	0.247999999999999998 	9 	
-2 	0.775000000000000022 	0.569999999999999951 	0.220000000000000001 	2.03200000000000003 	0.734999999999999987 	0.475499999999999978 	0.658499999999999974 	17 	
-2 	0.369999999999999996 	0.265000000000000013 	0.0749999999999999972 	0.213999999999999996 	0.0899999999999999967 	0.0509999999999999967 	0.0700000000000000067 	6 	
-2 	0.599999999999999978 	0.465000000000000024 	0.165000000000000008 	1.0475000000000001 	0.465000000000000024 	0.234499999999999986 	0.315000000000000002 	11 	
-0 	0.469999999999999973 	0.354999999999999982 	0.130000000000000004 	0.546499999999999986 	0.200500000000000012 	0.126000000000000001 	0.184999999999999998 	14 	
-2 	0.599999999999999978 	0.479999999999999982 	0.154999999999999999 	1.01400000000000001 	0.451000000000000012 	0.188500000000000001 	0.325000000000000011 	11 	
-0 	0.625 	0.489999999999999991 	0.165000000000000008 	1.127 	0.47699999999999998 	0.236499999999999988 	0.318500000000000005 	9 	
-2 	0.625 	0.474999999999999978 	0.174999999999999989 	1.34050000000000002 	0.656000000000000028 	0.282999999999999974 	0.337000000000000022 	10 	
-2 	0.469999999999999973 	0.369999999999999996 	0.104999999999999996 	0.466500000000000026 	0.202500000000000013 	0.101500000000000007 	0.154999999999999999 	10 	
-1 	0.429999999999999993 	0.325000000000000011 	0.110000000000000001 	0.367499999999999993 	0.135500000000000009 	0.0934999999999999998 	0.119999999999999996 	13 	
-1 	0.23000000000000001 	0.170000000000000012 	0.0500000000000000028 	0.0570000000000000021 	0.0259999999999999988 	0.0129999999999999994 	0.0160000000000000003 	5 	
-0 	0.484999999999999987 	0.395000000000000018 	0.160000000000000003 	0.660000000000000031 	0.247499999999999998 	0.128000000000000003 	0.234999999999999987 	14 	
-1 	0.375 	0.244999999999999996 	0.100000000000000006 	0.394000000000000017 	0.166000000000000009 	0.0909999999999999976 	0.112500000000000003 	6 	
-0 	0.625 	0.489999999999999991 	0.174999999999999989 	1.10749999999999993 	0.44850000000000001 	0.216499999999999998 	0.359499999999999986 	8 	
-2 	0.589999999999999969 	0.489999999999999991 	0.165000000000000008 	1.20700000000000007 	0.559000000000000052 	0.234999999999999987 	0.308999999999999997 	10 	
-0 	0.564999999999999947 	0.450000000000000011 	0.160000000000000003 	0.79500000000000004 	0.360499999999999987 	0.155499999999999999 	0.23000000000000001 	12 	
-0 	0.380000000000000004 	0.299999999999999989 	0.0899999999999999967 	0.321500000000000008 	0.154499999999999998 	0.0749999999999999972 	0.0950000000000000011 	9 	
-2 	0.614999999999999991 	0.469999999999999973 	0.149999999999999994 	1.08749999999999991 	0.497499999999999998 	0.282999999999999974 	0.268500000000000016 	9 	
-2 	0.540000000000000036 	0.434999999999999998 	0.140000000000000013 	0.734500000000000042 	0.330000000000000016 	0.159500000000000003 	0.212999999999999995 	9 	
-2 	0.530000000000000027 	0.41499999999999998 	0.174999999999999989 	0.739500000000000046 	0.26100000000000001 	0.139500000000000013 	0.264500000000000013 	17 	
-0 	0.724999999999999978 	0.530000000000000027 	0.190000000000000002 	1.73150000000000004 	0.82999999999999996 	0.39800000000000002 	0.405000000000000027 	11 	
-1 	0.5 	0.380000000000000004 	0.135000000000000009 	0.593999999999999972 	0.294499999999999984 	0.103999999999999995 	0.1565 	9 	
-0 	0.535000000000000031 	0.419999999999999984 	0.130000000000000004 	0.698999999999999955 	0.3125 	0.1565 	0.203499999999999986 	8 	
-1 	0.424999999999999989 	0.320000000000000007 	0.0850000000000000061 	0.262000000000000011 	0.123499999999999999 	0.067000000000000004 	0.072499999999999995 	8 	
-1 	0.525000000000000022 	0.400000000000000022 	0.125 	0.696500000000000008 	0.368999999999999995 	0.138500000000000012 	0.164000000000000007 	9 	
-2 	0.584999999999999964 	0.450000000000000011 	0.174999999999999989 	1.12749999999999995 	0.492499999999999993 	0.262000000000000011 	0.33500000000000002 	11 	
-1 	0.535000000000000031 	0.41499999999999998 	0.149999999999999994 	0.576500000000000012 	0.359499999999999986 	0.135000000000000009 	0.225000000000000006 	8 	
-2 	0.625 	0.455000000000000016 	0.170000000000000012 	1.08200000000000007 	0.495499999999999996 	0.234499999999999986 	0.315000000000000002 	9 	
-2 	0.589999999999999969 	0.46000000000000002 	0.154999999999999999 	0.906000000000000028 	0.327000000000000013 	0.148499999999999993 	0.33500000000000002 	15 	
-2 	0.505000000000000004 	0.400000000000000022 	0.135000000000000009 	0.722999999999999976 	0.377000000000000002 	0.148999999999999994 	0.177999999999999992 	7 	
-0 	0.474999999999999978 	0.380000000000000004 	0.14499999999999999 	0.569999999999999951 	0.16700000000000001 	0.117999999999999994 	0.187 	11 	
-2 	0.630000000000000004 	0.510000000000000009 	0.170000000000000012 	1.18849999999999989 	0.491499999999999992 	0.306499999999999995 	0.347999999999999976 	7 	
-0 	0.434999999999999998 	0.320000000000000007 	0.119999999999999996 	0.378500000000000003 	0.151999999999999996 	0.091499999999999998 	0.125 	11 	
-2 	0.594999999999999973 	0.484999999999999987 	0.149999999999999994 	1.08349999999999991 	0.530499999999999972 	0.231000000000000011 	0.276000000000000023 	8 	
-0 	0.400000000000000022 	0.325000000000000011 	0.119999999999999996 	0.318500000000000005 	0.134000000000000008 	0.0565000000000000016 	0.0950000000000000011 	8 	
-0 	0.530000000000000027 	0.385000000000000009 	0.125 	0.669499999999999984 	0.288999999999999979 	0.150999999999999995 	0.179999999999999993 	10 	
-2 	0.429999999999999993 	0.344999999999999973 	0.115000000000000005 	0.304499999999999993 	0.0924999999999999989 	0.0550000000000000003 	0.119999999999999996 	11 	
-0 	0.569999999999999951 	0.445000000000000007 	0.149999999999999994 	0.994999999999999996 	0.504000000000000004 	0.184999999999999998 	0.2505 	9 	
-2 	0.734999999999999987 	0.569999999999999951 	0.209999999999999992 	2.23550000000000004 	1.1705000000000001 	0.463000000000000023 	0.531499999999999972 	10 	
-0 	0.550000000000000044 	0.41499999999999998 	0.135000000000000009 	0.814500000000000002 	0.426999999999999991 	0.185499999999999998 	0.174999999999999989 	8 	
-0 	0.560000000000000053 	0.409999999999999976 	0.160000000000000003 	0.821500000000000008 	0.342000000000000026 	0.183999999999999997 	0.253000000000000003 	9 	
-0 	0.569999999999999951 	0.434999999999999998 	0.140000000000000013 	0.858500000000000041 	0.390500000000000014 	0.196000000000000008 	0.22950000000000001 	8 	
-0 	0.530000000000000027 	0.434999999999999998 	0.170000000000000012 	0.815500000000000003 	0.298499999999999988 	0.154999999999999999 	0.275000000000000022 	13 	
-2 	0.450000000000000011 	0.359999999999999987 	0.160000000000000003 	0.566999999999999948 	0.173999999999999988 	0.1245 	0.225000000000000006 	12 	
-1 	0.400000000000000022 	0.315000000000000002 	0.0899999999999999967 	0.324500000000000011 	0.150999999999999995 	0.0729999999999999954 	0.0879999999999999949 	8 	
-1 	0.434999999999999998 	0.325000000000000011 	0.104999999999999996 	0.33500000000000002 	0.13600000000000001 	0.0650000000000000022 	0.115000000000000005 	8 	
-0 	0.569999999999999951 	0.46000000000000002 	0.170000000000000012 	1.10000000000000009 	0.412499999999999978 	0.220500000000000002 	0.380000000000000004 	14 	
-1 	0.110000000000000001 	0.0899999999999999967 	0.0299999999999999989 	0.00800000000000000017 	0.00250000000000000005 	0.00200000000000000004 	0.00300000000000000006 	3 	
-2 	0.510000000000000009 	0.41499999999999998 	0.14499999999999999 	0.751000000000000001 	0.329500000000000015 	0.183499999999999996 	0.203000000000000014 	8 	
-1 	0.340000000000000024 	0.25 	0.0700000000000000067 	0.222500000000000003 	0.103999999999999995 	0.0425000000000000031 	0.0550000000000000003 	7 	
-2 	0.564999999999999947 	0.450000000000000011 	0.154999999999999999 	1.05950000000000011 	0.473499999999999976 	0.239999999999999991 	0.265000000000000013 	10 	
-1 	0.359999999999999987 	0.260000000000000009 	0.0800000000000000017 	0.179499999999999993 	0.0739999999999999963 	0.0315000000000000002 	0.0599999999999999978 	5 	
-0 	0.560000000000000053 	0.429999999999999993 	0.149999999999999994 	0.874500000000000055 	0.453000000000000014 	0.161000000000000004 	0.220000000000000001 	8 	
-0 	0.550000000000000044 	0.450000000000000011 	0.14499999999999999 	0.740999999999999992 	0.294999999999999984 	0.143499999999999989 	0.266500000000000015 	10 	
-0 	0.699999999999999956 	0.574999999999999956 	0.204999999999999988 	1.7975000000000001 	0.729500000000000037 	0.393500000000000016 	0.516499999999999959 	13 	
-2 	0.650000000000000022 	0.505000000000000004 	0.190000000000000002 	1.27400000000000002 	0.589999999999999969 	0.23000000000000001 	0.391000000000000014 	11 	
-2 	0.174999999999999989 	0.125 	0.0400000000000000008 	0.0240000000000000005 	0.00949999999999999976 	0.00600000000000000012 	0.0050000000000000001 	4 	
-2 	0.330000000000000016 	0.214999999999999997 	0.0749999999999999972 	0.114500000000000005 	0.0449999999999999983 	0.0264999999999999993 	0.0350000000000000033 	6 	
-1 	0.650000000000000022 	0.525000000000000022 	0.179999999999999993 	1.62599999999999989 	0.596999999999999975 	0.344499999999999973 	0.530000000000000027 	18 	
-2 	0.625 	0.474999999999999978 	0.160000000000000003 	1.08450000000000002 	0.500499999999999945 	0.235499999999999987 	0.310499999999999998 	10 	
-0 	0.450000000000000011 	0.380000000000000004 	0.165000000000000008 	0.816500000000000004 	0.25 	0.191500000000000004 	0.265000000000000013 	23 	
-0 	0.640000000000000013 	0.515000000000000013 	0.204999999999999988 	1.53350000000000009 	0.663499999999999979 	0.33450000000000002 	0.402500000000000024 	9 	
-2 	0.584999999999999964 	0.465000000000000024 	0.190000000000000002 	1.17100000000000004 	0.390500000000000014 	0.235499999999999987 	0.400000000000000022 	17 	
-2 	0.555000000000000049 	0.434999999999999998 	0.140000000000000013 	0.749500000000000055 	0.341000000000000025 	0.164500000000000007 	0.213999999999999996 	8 	
-0 	0.57999999999999996 	0.424999999999999989 	0.149999999999999994 	0.843999999999999972 	0.364499999999999991 	0.184999999999999998 	0.270500000000000018 	9 	
-0 	0.584999999999999964 	0.46000000000000002 	0.170000000000000012 	0.932499999999999996 	0.364999999999999991 	0.271000000000000019 	0.28999999999999998 	9 	
-2 	0.625 	0.479999999999999982 	0.170000000000000012 	1.35549999999999993 	0.671000000000000041 	0.268000000000000016 	0.338500000000000023 	10 	
-1 	0.57999999999999996 	0.440000000000000002 	0.14499999999999999 	0.79049999999999998 	0.35249999999999998 	0.164500000000000007 	0.241999999999999993 	10 	
-2 	0.375 	0.299999999999999989 	0.100000000000000006 	0.246499999999999997 	0.103999999999999995 	0.0475000000000000006 	0.0830000000000000043 	11 	
-0 	0.520000000000000018 	0.400000000000000022 	0.119999999999999996 	0.651499999999999968 	0.26100000000000001 	0.201500000000000012 	0.165000000000000008 	15 	
-1 	0.385000000000000009 	0.299999999999999989 	0.100000000000000006 	0.28949999999999998 	0.121499999999999997 	0.0630000000000000004 	0.0899999999999999967 	7 	
-0 	0.465000000000000024 	0.349999999999999978 	0.110000000000000001 	0.408499999999999974 	0.165000000000000008 	0.101999999999999993 	0.131000000000000005 	8 	
-0 	0.625 	0.515000000000000013 	0.160000000000000003 	1.26400000000000001 	0.571500000000000008 	0.326000000000000012 	0.321000000000000008 	9 	
-0 	0.604999999999999982 	0.489999999999999991 	0.149999999999999994 	1.13450000000000006 	0.526499999999999968 	0.264500000000000013 	0.294999999999999984 	9 	
-2 	0.714999999999999969 	0.564999999999999947 	0.174999999999999989 	1.9524999999999999 	0.764499999999999957 	0.418499999999999983 	0.413499999999999979 	10 	
-0 	0.450000000000000011 	0.354999999999999982 	0.104999999999999996 	0.522499999999999964 	0.236999999999999988 	0.116500000000000006 	0.14499999999999999 	8 	
-0 	0.589999999999999969 	0.46000000000000002 	0.14499999999999999 	0.990500000000000047 	0.453000000000000014 	0.220500000000000002 	0.275000000000000022 	8 	
-2 	0.630000000000000004 	0.484999999999999987 	0.154999999999999999 	1.27800000000000002 	0.637000000000000011 	0.275000000000000022 	0.309999999999999998 	8 	
-2 	0.359999999999999987 	0.294999999999999984 	0.100000000000000006 	0.210499999999999993 	0.0660000000000000031 	0.0524999999999999981 	0.0749999999999999972 	9 	
-2 	0.645000000000000018 	0.479999999999999982 	0.149999999999999994 	1.19199999999999995 	0.605500000000000038 	0.259500000000000008 	0.284999999999999976 	9 	
-2 	0.535000000000000031 	0.409999999999999976 	0.135000000000000009 	0.861999999999999988 	0.285499999999999976 	0.152499999999999997 	0.320000000000000007 	14 	
-0 	0.525000000000000022 	0.409999999999999976 	0.135000000000000009 	0.708500000000000019 	0.292999999999999983 	0.152499999999999997 	0.234999999999999987 	11 	
-0 	0.599999999999999978 	0.455000000000000016 	0.14499999999999999 	0.889499999999999957 	0.418999999999999984 	0.171500000000000014 	0.269000000000000017 	10 	
-0 	0.569999999999999951 	0.445000000000000007 	0.154999999999999999 	1.0169999999999999 	0.526499999999999968 	0.202500000000000013 	0.265000000000000013 	10 	
-1 	0.309999999999999998 	0.225000000000000006 	0.0500000000000000028 	0.14449999999999999 	0.0675000000000000044 	0.0384999999999999995 	0.0449999999999999983 	6 	
-0 	0.510000000000000009 	0.369999999999999996 	0.209999999999999992 	1.18300000000000005 	0.508000000000000007 	0.291999999999999982 	0.343000000000000027 	9 	
-0 	0.569999999999999951 	0.440000000000000002 	0.119999999999999996 	0.803000000000000047 	0.382000000000000006 	0.152499999999999997 	0.234000000000000014 	9 	
-2 	0.645000000000000018 	0.510000000000000009 	0.190000000000000002 	1.47449999999999992 	0.604999999999999982 	0.344999999999999973 	0.479999999999999982 	9 	
-2 	0.630000000000000004 	0.505000000000000004 	0.174999999999999989 	1.22100000000000009 	0.555000000000000049 	0.252000000000000002 	0.340000000000000024 	12 	
-2 	0.675000000000000044 	0.520000000000000018 	0.14499999999999999 	1.36450000000000005 	0.557000000000000051 	0.340500000000000025 	0.385000000000000009 	11 	
-0 	0.655000000000000027 	0.510000000000000009 	0.174999999999999989 	1.65250000000000008 	0.851500000000000035 	0.336500000000000021 	0.403000000000000025 	10 	
-2 	0.349999999999999978 	0.255000000000000004 	0.0800000000000000017 	0.191500000000000004 	0.0800000000000000017 	0.0384999999999999995 	0.0630000000000000004 	9 	
-0 	0.614999999999999991 	0.474999999999999978 	0.154999999999999999 	1.02699999999999991 	0.447000000000000008 	0.25 	0.284999999999999976 	9 	
-1 	0.33500000000000002 	0.260000000000000009 	0.0899999999999999967 	0.196500000000000008 	0.0874999999999999944 	0.0410000000000000017 	0.0560000000000000012 	7 	
-2 	0.57999999999999996 	0.450000000000000011 	0.14499999999999999 	1.00249999999999995 	0.547000000000000042 	0.197500000000000009 	0.22950000000000001 	8 	
-1 	0.450000000000000011 	0.359999999999999987 	0.130000000000000004 	0.47799999999999998 	0.191000000000000003 	0.127000000000000002 	0.137000000000000011 	7 	
-1 	0.46000000000000002 	0.354999999999999982 	0.140000000000000013 	0.493499999999999994 	0.215999999999999998 	0.133000000000000007 	0.115000000000000005 	13 	
-1 	0.520000000000000018 	0.380000000000000004 	0.140000000000000013 	0.525000000000000022 	0.177499999999999991 	0.115000000000000005 	0.184999999999999998 	11 	
-0 	0.665000000000000036 	0.510000000000000009 	0.174999999999999989 	1.38050000000000006 	0.675000000000000044 	0.298499999999999988 	0.325000000000000011 	10 	
-0 	0.599999999999999978 	0.474999999999999978 	0.149999999999999994 	1.12999999999999989 	0.574999999999999956 	0.196000000000000008 	0.304999999999999993 	9 	
-1 	0.494999999999999996 	0.375 	0.119999999999999996 	0.588999999999999968 	0.307499999999999996 	0.121499999999999997 	0.140500000000000014 	8 	
-1 	0.255000000000000004 	0.195000000000000007 	0.0550000000000000003 	0.072499999999999995 	0.028500000000000001 	0.0170000000000000012 	0.0210000000000000013 	4 	
-1 	0.429999999999999993 	0.325000000000000011 	0.100000000000000006 	0.364499999999999991 	0.157500000000000001 	0.0825000000000000039 	0.104999999999999996 	7 	
-2 	0.584999999999999964 	0.455000000000000016 	0.149999999999999994 	0.986999999999999988 	0.435499999999999998 	0.20749999999999999 	0.309999999999999998 	11 	
-0 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	1.10050000000000003 	0.541499999999999981 	0.166000000000000009 	0.265000000000000013 	8 	
-1 	0.354999999999999982 	0.270000000000000018 	0.0749999999999999972 	0.177499999999999991 	0.0790000000000000008 	0.0315000000000000002 	0.0539999999999999994 	6 	
-1 	0.244999999999999996 	0.195000000000000007 	0.0599999999999999978 	0.0950000000000000011 	0.0444999999999999979 	0.0245000000000000009 	0.0259999999999999988 	4 	
-0 	0.560000000000000053 	0.429999999999999993 	0.125 	0.802499999999999991 	0.313 	0.171500000000000014 	0.263000000000000012 	13 	
-0 	0.560000000000000053 	0.445000000000000007 	0.179999999999999993 	0.903000000000000025 	0.357499999999999984 	0.204499999999999987 	0.294999999999999984 	9 	
-1 	0.550000000000000044 	0.41499999999999998 	0.14499999999999999 	0.781499999999999972 	0.372999999999999998 	0.160000000000000003 	0.221500000000000002 	8 	
-0 	0.434999999999999998 	0.349999999999999978 	0.119999999999999996 	0.458500000000000019 	0.192000000000000004 	0.100000000000000006 	0.130000000000000004 	11 	
-1 	0.369999999999999996 	0.270000000000000018 	0.0950000000000000011 	0.232000000000000012 	0.132500000000000007 	0.0410000000000000017 	0.0614999999999999991 	6 	
-2 	0.589999999999999969 	0.455000000000000016 	0.154999999999999999 	0.885499999999999954 	0.388000000000000012 	0.188 	0.275000000000000022 	10 	
-0 	0.660000000000000031 	0.525000000000000022 	0.160000000000000003 	1.27699999999999991 	0.497499999999999998 	0.319000000000000006 	0.394000000000000017 	13 	
-0 	0.479999999999999982 	0.405000000000000027 	0.130000000000000004 	0.637499999999999956 	0.277000000000000024 	0.14449999999999999 	0.209999999999999992 	10 	
-2 	0.589999999999999969 	0.445000000000000007 	0.140000000000000013 	0.93100000000000005 	0.355999999999999983 	0.234000000000000014 	0.280000000000000027 	12 	
-0 	0.434999999999999998 	0.349999999999999978 	0.110000000000000001 	0.384000000000000008 	0.142999999999999988 	0.100500000000000006 	0.125 	13 	
-2 	0.57999999999999996 	0.489999999999999991 	0.130000000000000004 	1.13349999999999995 	0.585999999999999965 	0.256500000000000006 	0.236999999999999988 	9 	
-2 	0.625 	0.494999999999999996 	0.174999999999999989 	1.254 	0.581500000000000017 	0.285999999999999976 	0.318500000000000005 	9 	
-1 	0.270000000000000018 	0.195000000000000007 	0.0650000000000000022 	0.106499999999999997 	0.0475000000000000006 	0.0224999999999999992 	0.028500000000000001 	5 	
-0 	0.640000000000000013 	0.479999999999999982 	0.14499999999999999 	1.11450000000000005 	0.508000000000000007 	0.239999999999999991 	0.340000000000000024 	10 	
-0 	0.574999999999999956 	0.465000000000000024 	0.195000000000000007 	0.996500000000000052 	0.416999999999999982 	0.246999999999999997 	0.469999999999999973 	8 	
-1 	0.424999999999999989 	0.330000000000000016 	0.115000000000000005 	0.326500000000000012 	0.131500000000000006 	0.076999999999999999 	0.102999999999999994 	6 	
-2 	0.359999999999999987 	0.294999999999999984 	0.130000000000000004 	0.276500000000000024 	0.0894999999999999962 	0.0570000000000000021 	0.100500000000000006 	10 	
-1 	0.265000000000000013 	0.204999999999999988 	0.0700000000000000067 	0.105499999999999997 	0.0389999999999999999 	0.0410000000000000017 	0.0350000000000000033 	5 	
-1 	0.484999999999999987 	0.375 	0.130000000000000004 	0.602500000000000036 	0.293499999999999983 	0.128500000000000003 	0.160000000000000003 	7 	
-2 	0.645000000000000018 	0.515000000000000013 	0.174999999999999989 	1.61149999999999993 	0.674499999999999988 	0.384000000000000008 	0.385000000000000009 	14 	
-2 	0.604999999999999982 	0.474999999999999978 	0.174999999999999989 	1.25249999999999995 	0.557499999999999996 	0.305499999999999994 	0.343000000000000027 	9 	
-1 	0.574999999999999956 	0.455000000000000016 	0.179999999999999993 	0.852500000000000036 	0.30149999999999999 	0.182499999999999996 	0.299999999999999989 	13 	
-0 	0.515000000000000013 	0.395000000000000018 	0.165000000000000008 	0.75649999999999995 	0.190500000000000003 	0.170000000000000012 	0.320500000000000007 	10 	
-1 	0.489999999999999991 	0.364999999999999991 	0.125 	0.558499999999999996 	0.252000000000000002 	0.126000000000000001 	0.161500000000000005 	10 	
-0 	0.604999999999999982 	0.484999999999999987 	0.160000000000000003 	1.05649999999999999 	0.369999999999999996 	0.235499999999999987 	0.354999999999999982 	10 	
-1 	0.515000000000000013 	0.395000000000000018 	0.125 	0.663499999999999979 	0.320000000000000007 	0.140000000000000013 	0.170000000000000012 	8 	
-1 	0.550000000000000044 	0.450000000000000011 	0.130000000000000004 	0.804000000000000048 	0.337500000000000022 	0.140500000000000014 	0.23000000000000001 	6 	
-0 	0.604999999999999982 	0.515000000000000013 	0.170000000000000012 	1.28899999999999992 	0.599999999999999978 	0.294499999999999984 	0.331500000000000017 	9 	
-1 	0.440000000000000002 	0.340000000000000024 	0.100000000000000006 	0.379000000000000004 	0.172499999999999987 	0.081500000000000003 	0.101000000000000006 	7 	
-0 	0.530000000000000027 	0.455000000000000016 	0.165000000000000008 	0.980500000000000038 	0.315500000000000003 	0.281499999999999972 	0.296499999999999986 	11 	
-1 	0.28999999999999998 	0.214999999999999997 	0.0650000000000000022 	0.0985000000000000042 	0.0425000000000000031 	0.0210000000000000013 	0.0309999999999999998 	5 	
-1 	0.494999999999999996 	0.375 	0.119999999999999996 	0.61399999999999999 	0.285499999999999976 	0.13650000000000001 	0.161000000000000004 	8 	
-1 	0.369999999999999996 	0.275000000000000022 	0.100000000000000006 	0.220000000000000001 	0.0940000000000000002 	0.0449999999999999983 	0.0650000000000000022 	7 	
-0 	0.540000000000000036 	0.434999999999999998 	0.174999999999999989 	0.892000000000000015 	0.322000000000000008 	0.173999999999999988 	0.33500000000000002 	13 	
-0 	0.719999999999999973 	0.550000000000000044 	0.195000000000000007 	2.07299999999999995 	1.0714999999999999 	0.42649999999999999 	0.501499999999999946 	9 	
-1 	0.535000000000000031 	0.450000000000000011 	0.170000000000000012 	0.781000000000000028 	0.305499999999999994 	0.155499999999999999 	0.294999999999999984 	11 	
-2 	0.680000000000000049 	0.520000000000000018 	0.165000000000000008 	1.47750000000000004 	0.723999999999999977 	0.279000000000000026 	0.406000000000000028 	11 	
-0 	0.640000000000000013 	0.494999999999999996 	0.170000000000000012 	1.22649999999999992 	0.489999999999999991 	0.377000000000000002 	0.287499999999999978 	11 	
-1 	0.424999999999999989 	0.325000000000000011 	0.100000000000000006 	0.39800000000000002 	0.118499999999999994 	0.0645000000000000018 	0.0945000000000000007 	6 	
-2 	0.630000000000000004 	0.484999999999999987 	0.160000000000000003 	1.2430000000000001 	0.622999999999999998 	0.275000000000000022 	0.299999999999999989 	10 	
-1 	0.299999999999999989 	0.23000000000000001 	0.0800000000000000017 	0.127500000000000002 	0.043499999999999997 	0.0264999999999999993 	0.0400000000000000008 	8 	
-1 	0.320000000000000007 	0.225000000000000006 	0.0850000000000000061 	0.141499999999999987 	0.0675000000000000044 	0.0294999999999999984 	0.0405000000000000013 	6 	
-0 	0.614999999999999991 	0.525000000000000022 	0.154999999999999999 	1.03849999999999998 	0.426999999999999991 	0.231500000000000011 	0.344999999999999973 	11 	
-0 	0.665000000000000036 	0.5 	0.149999999999999994 	1.24750000000000005 	0.462500000000000022 	0.295499999999999985 	0.359499999999999986 	10 	
-0 	0.574999999999999956 	0.450000000000000011 	0.160000000000000003 	0.977500000000000036 	0.313500000000000001 	0.231000000000000011 	0.330000000000000016 	12 	
-0 	0.57999999999999996 	0.450000000000000011 	0.234999999999999987 	1.07099999999999995 	0.299999999999999989 	0.205999999999999989 	0.395000000000000018 	14 	
-0 	0.57999999999999996 	0.465000000000000024 	0.165000000000000008 	1.10149999999999992 	0.404000000000000026 	0.209499999999999992 	0.349999999999999978 	11 	
-1 	0.419999999999999984 	0.320000000000000007 	0.110000000000000001 	0.362499999999999989 	0.173999999999999988 	0.0635000000000000009 	0.104999999999999996 	7 	
-2 	0.564999999999999947 	0.455000000000000016 	0.154999999999999999 	0.935499999999999998 	0.420999999999999985 	0.182999999999999996 	0.260000000000000009 	11 	
-0 	0.625 	0.505000000000000004 	0.174999999999999989 	1.14999999999999991 	0.547499999999999987 	0.256000000000000005 	0.304499999999999993 	11 	
-2 	0.489999999999999991 	0.385000000000000009 	0.125 	0.649000000000000021 	0.320000000000000007 	0.123999999999999999 	0.169500000000000012 	8 	
-2 	0.594999999999999973 	0.46000000000000002 	0.170000000000000012 	1.12949999999999995 	0.569999999999999951 	0.255500000000000005 	0.265000000000000013 	10 	
-1 	0.550000000000000044 	0.434999999999999998 	0.140000000000000013 	0.799499999999999988 	0.294999999999999984 	0.190500000000000003 	0.237999999999999989 	10 	
-0 	0.560000000000000053 	0.424999999999999989 	0.125 	0.932000000000000051 	0.360999999999999988 	0.212999999999999995 	0.33500000000000002 	9 	
-2 	0.609999999999999987 	0.479999999999999982 	0.165000000000000008 	1.24350000000000005 	0.557499999999999996 	0.267500000000000016 	0.371999999999999997 	8 	
-1 	0.405000000000000027 	0.304999999999999993 	0.104999999999999996 	0.362499999999999989 	0.1565 	0.0704999999999999932 	0.125 	10 	
-2 	0.510000000000000009 	0.409999999999999976 	0.14499999999999999 	0.796000000000000041 	0.38650000000000001 	0.181499999999999995 	0.195500000000000007 	8 	
-0 	0.520000000000000018 	0.429999999999999993 	0.149999999999999994 	0.72799999999999998 	0.301999999999999991 	0.157500000000000001 	0.234999999999999987 	11 	
-0 	0.469999999999999973 	0.359999999999999987 	0.100000000000000006 	0.470499999999999974 	0.163500000000000006 	0.0889999999999999958 	0.138500000000000012 	8 	
-2 	0.699999999999999956 	0.530000000000000027 	0.190000000000000002 	1.31850000000000001 	0.548000000000000043 	0.233000000000000013 	0.419999999999999984 	18 	
-2 	0.655000000000000027 	0.535000000000000031 	0.204999999999999988 	1.64450000000000007 	0.730500000000000038 	0.359499999999999986 	0.46000000000000002 	13 	
-1 	0.465000000000000024 	0.359999999999999987 	0.104999999999999996 	0.497999999999999998 	0.213999999999999996 	0.116000000000000006 	0.140000000000000013 	15 	
-0 	0.604999999999999982 	0.494999999999999996 	0.170000000000000012 	1.23849999999999993 	0.528000000000000025 	0.246499999999999997 	0.390000000000000013 	14 	
-0 	0.535000000000000031 	0.434999999999999998 	0.160000000000000003 	0.810499999999999998 	0.315500000000000003 	0.179499999999999993 	0.239999999999999991 	10 	
-0 	0.479999999999999982 	0.364999999999999991 	0.135000000000000009 	0.639499999999999957 	0.294499999999999984 	0.113000000000000003 	0.174999999999999989 	8 	
-2 	0.599999999999999978 	0.474999999999999978 	0.170000000000000012 	1.13149999999999995 	0.508000000000000007 	0.27200000000000002 	0.308999999999999997 	10 	
-2 	0.489999999999999991 	0.380000000000000004 	0.135000000000000009 	0.541499999999999981 	0.217499999999999999 	0.0950000000000000011 	0.190000000000000002 	11 	
-0 	0.564999999999999947 	0.489999999999999991 	0.154999999999999999 	0.924499999999999988 	0.405000000000000027 	0.219500000000000001 	0.255000000000000004 	11 	
-0 	0.67000000000000004 	0.550000000000000044 	0.154999999999999999 	1.56600000000000006 	0.857999999999999985 	0.339000000000000024 	0.353999999999999981 	10 	
-2 	0.719999999999999973 	0.574999999999999956 	0.214999999999999997 	2.17300000000000004 	0.951500000000000012 	0.563999999999999946 	0.536499999999999977 	12 	
-0 	0.525000000000000022 	0.440000000000000002 	0.149999999999999994 	0.842500000000000027 	0.368499999999999994 	0.19850000000000001 	0.239999999999999991 	12 	
-0 	0.650000000000000022 	0.5 	0.184999999999999998 	1.4415 	0.740999999999999992 	0.295499999999999985 	0.341000000000000025 	9 	
-0 	0.619999999999999996 	0.479999999999999982 	0.165000000000000008 	1.01249999999999996 	0.532499999999999973 	0.436499999999999999 	0.32400000000000001 	10 	
-0 	0.719999999999999973 	0.574999999999999956 	0.170000000000000012 	1.9335 	0.913000000000000034 	0.389000000000000012 	0.510000000000000009 	13 	
-0 	0.594999999999999973 	0.465000000000000024 	0.154999999999999999 	1.02600000000000002 	0.464500000000000024 	0.112000000000000002 	0.304999999999999993 	12 	
-1 	0.41499999999999998 	0.309999999999999998 	0.110000000000000001 	0.296499999999999986 	0.122999999999999998 	0.0570000000000000021 	0.0995000000000000051 	10 	
-0 	0.645000000000000018 	0.525000000000000022 	0.200000000000000011 	1.44900000000000007 	0.600999999999999979 	0.256500000000000006 	0.505000000000000004 	13 	
-2 	0.635000000000000009 	0.494999999999999996 	0.195000000000000007 	1.17199999999999993 	0.445000000000000007 	0.311499999999999999 	0.347499999999999976 	11 	
-1 	0.619999999999999996 	0.484999999999999987 	0.170000000000000012 	1.20799999999999996 	0.480499999999999983 	0.304499999999999993 	0.330000000000000016 	15 	
-2 	0.479999999999999982 	0.375 	0.14499999999999999 	0.777000000000000024 	0.215999999999999998 	0.130000000000000004 	0.170000000000000012 	9 	
-2 	0.635000000000000009 	0.515000000000000013 	0.165000000000000008 	1.22900000000000009 	0.505499999999999949 	0.297499999999999987 	0.353499999999999981 	10 	
-1 	0.604999999999999982 	0.479999999999999982 	0.154999999999999999 	0.999500000000000055 	0.424999999999999989 	0.19850000000000001 	0.299999999999999989 	10 	
-1 	0.535000000000000031 	0.419999999999999984 	0.14499999999999999 	0.688500000000000001 	0.27300000000000002 	0.151499999999999996 	0.236999999999999988 	9 	
-1 	0.434999999999999998 	0.344999999999999973 	0.119999999999999996 	0.447500000000000009 	0.221000000000000002 	0.112000000000000002 	0.125 	7 	
-0 	0.675000000000000044 	0.525000000000000022 	0.154999999999999999 	1.47849999999999993 	0.628000000000000003 	0.340500000000000025 	0.419999999999999984 	9 	
-0 	0.489999999999999991 	0.385000000000000009 	0.125 	0.53949999999999998 	0.217499999999999999 	0.128000000000000003 	0.165000000000000008 	11 	
-2 	0.719999999999999973 	0.569999999999999951 	0.200000000000000011 	1.8274999999999999 	0.919000000000000039 	0.365999999999999992 	0.484999999999999987 	10 	
-0 	0.625 	0.484999999999999987 	0.174999999999999989 	1.37450000000000006 	0.733500000000000041 	0.271500000000000019 	0.332000000000000017 	9 	
-2 	0.489999999999999991 	0.465000000000000024 	0.125 	0.522499999999999964 	0.234999999999999987 	0.130000000000000004 	0.140999999999999986 	7 	
-1 	0.489999999999999991 	0.375 	0.125 	0.544499999999999984 	0.279000000000000026 	0.115000000000000005 	0.130000000000000004 	8 	
-2 	0.635000000000000009 	0.510000000000000009 	0.170000000000000012 	1.35549999999999993 	0.618999999999999995 	0.304999999999999993 	0.390000000000000013 	9 	
-0 	0.619999999999999996 	0.474999999999999978 	0.160000000000000003 	1.12949999999999995 	0.463000000000000023 	0.268500000000000016 	0.330000000000000016 	10 	
-2 	0.614999999999999991 	0.489999999999999991 	0.170000000000000012 	1.14500000000000002 	0.491499999999999992 	0.20799999999999999 	0.343000000000000027 	13 	
-2 	0.390000000000000013 	0.284999999999999976 	0.0950000000000000011 	0.271000000000000019 	0.110000000000000001 	0.0599999999999999978 	0.0800000000000000017 	8 	
-0 	0.609999999999999987 	0.484999999999999987 	0.209999999999999992 	1.34450000000000003 	0.535000000000000031 	0.220500000000000002 	0.515000000000000013 	20 	
-0 	0.604999999999999982 	0.484999999999999987 	0.160000000000000003 	1.20100000000000007 	0.416999999999999982 	0.287499999999999978 	0.380000000000000004 	9 	
-1 	0.369999999999999996 	0.28999999999999998 	0.0950000000000000011 	0.248999999999999999 	0.104499999999999996 	0.0580000000000000029 	0.067000000000000004 	6 	
-1 	0.400000000000000022 	0.309999999999999998 	0.100000000000000006 	0.305999999999999994 	0.130000000000000004 	0.0599999999999999978 	0.0940000000000000002 	6 	
-1 	0.315000000000000002 	0.234999999999999987 	0.0550000000000000003 	0.150999999999999995 	0.0650000000000000022 	0.0269999999999999997 	0.0389999999999999999 	6 	
-2 	0.530000000000000027 	0.434999999999999998 	0.135000000000000009 	0.736500000000000044 	0.327500000000000013 	0.131500000000000006 	0.220000000000000001 	12 	
-2 	0.450000000000000011 	0.320000000000000007 	0.100000000000000006 	0.381000000000000005 	0.170500000000000013 	0.0749999999999999972 	0.115000000000000005 	9 	
-2 	0.650000000000000022 	0.525000000000000022 	0.190000000000000002 	1.61250000000000004 	0.777000000000000024 	0.368499999999999994 	0.396500000000000019 	11 	
-0 	0.614999999999999991 	0.5 	0.165000000000000008 	1.32699999999999996 	0.599999999999999978 	0.30149999999999999 	0.354999999999999982 	10 	
-2 	0.46000000000000002 	0.380000000000000004 	0.135000000000000009 	0.481999999999999984 	0.20699999999999999 	0.122499999999999998 	0.14499999999999999 	10 	
-0 	0.734999999999999987 	0.599999999999999978 	0.220000000000000001 	2.55500000000000016 	1.13349999999999995 	0.440000000000000002 	0.599999999999999978 	11 	
-2 	0.589999999999999969 	0.469999999999999973 	0.179999999999999993 	1.12349999999999994 	0.420499999999999985 	0.280500000000000027 	0.359999999999999987 	13 	
-0 	0.584999999999999964 	0.434999999999999998 	0.174999999999999989 	0.981999999999999984 	0.405500000000000027 	0.2495 	0.270000000000000018 	10 	
-2 	0.689999999999999947 	0.505000000000000004 	0.200000000000000011 	1.87200000000000011 	0.893000000000000016 	0.401500000000000024 	0.479999999999999982 	10 	
-2 	0.474999999999999978 	0.385000000000000009 	0.14499999999999999 	0.617500000000000049 	0.234999999999999987 	0.107999999999999999 	0.214999999999999997 	14 	
-2 	0.665000000000000036 	0.525000000000000022 	0.160000000000000003 	1.36299999999999999 	0.629000000000000004 	0.279000000000000026 	0.340000000000000024 	8 	
-1 	0.28999999999999998 	0.209999999999999992 	0.0650000000000000022 	0.0970000000000000029 	0.0374999999999999986 	0.0219999999999999987 	0.0299999999999999989 	6 	
-2 	0.569999999999999951 	0.445000000000000007 	0.140000000000000013 	1.06349999999999989 	0.526499999999999968 	0.219500000000000001 	0.239999999999999991 	8 	
-1 	0.325000000000000011 	0.239999999999999991 	0.0749999999999999972 	0.187 	0.0825000000000000039 	0.0444999999999999979 	0.0500000000000000028 	6 	
-1 	0.515000000000000013 	0.349999999999999978 	0.104999999999999996 	0.474499999999999977 	0.212999999999999995 	0.122999999999999998 	0.127500000000000002 	10 	
-1 	0.239999999999999991 	0.174999999999999989 	0.0449999999999999983 	0.0700000000000000067 	0.0315000000000000002 	0.0235000000000000001 	0.0200000000000000004 	5 	
-0 	0.424999999999999989 	0.33500000000000002 	0.0950000000000000011 	0.322000000000000008 	0.120499999999999996 	0.0609999999999999987 	0.125 	10 	
-0 	0.604999999999999982 	0.484999999999999987 	0.165000000000000008 	1.01049999999999995 	0.434999999999999998 	0.208999999999999991 	0.299999999999999989 	19 	
-2 	0.515000000000000013 	0.424999999999999989 	0.14499999999999999 	0.936499999999999999 	0.496999999999999997 	0.180999999999999994 	0.2185 	8 	
-2 	0.564999999999999947 	0.440000000000000002 	0.174999999999999989 	0.902499999999999969 	0.309999999999999998 	0.193000000000000005 	0.325000000000000011 	14 	
-2 	0.604999999999999982 	0.489999999999999991 	0.179999999999999993 	1.22700000000000009 	0.479999999999999982 	0.286999999999999977 	0.349999999999999978 	18 	
-2 	0.675000000000000044 	0.525000000000000022 	0.160000000000000003 	1.28350000000000009 	0.571999999999999953 	0.275500000000000023 	0.354499999999999982 	13 	
-0 	0.70499999999999996 	0.569999999999999951 	0.179999999999999993 	1.53449999999999998 	0.959999999999999964 	0.419499999999999984 	0.429999999999999993 	12 	
-1 	0.57999999999999996 	0.445000000000000007 	0.149999999999999994 	0.886499999999999955 	0.383000000000000007 	0.208999999999999991 	0.255000000000000004 	11 	
-2 	0.619999999999999996 	0.469999999999999973 	0.135000000000000009 	1.01950000000000007 	0.531499999999999972 	0.200500000000000012 	0.247499999999999998 	8 	
-0 	0.724999999999999978 	0.560000000000000053 	0.209999999999999992 	2.14100000000000001 	0.650000000000000022 	0.39800000000000002 	1.00499999999999989 	18 	
-0 	0.550000000000000044 	0.440000000000000002 	0.149999999999999994 	0.894499999999999962 	0.314500000000000002 	0.150999999999999995 	0.320000000000000007 	19 	
-1 	0.589999999999999969 	0.469999999999999973 	0.14499999999999999 	0.973999999999999977 	0.453000000000000014 	0.235999999999999988 	0.288999999999999979 	8 	
-2 	0.450000000000000011 	0.349999999999999978 	0.130000000000000004 	0.46000000000000002 	0.173999999999999988 	0.111000000000000001 	0.135000000000000009 	8 	
-2 	0.67000000000000004 	0.525000000000000022 	0.195000000000000007 	1.44049999999999989 	0.659499999999999975 	0.267500000000000016 	0.424999999999999989 	9 	
-0 	0.450000000000000011 	0.359999999999999987 	0.125 	0.4995 	0.203499999999999986 	0.100000000000000006 	0.170000000000000012 	13 	
-2 	0.609999999999999987 	0.479999999999999982 	0.149999999999999994 	1.19999999999999996 	0.560000000000000053 	0.245499999999999996 	0.280000000000000027 	14 	
-1 	0.604999999999999982 	0.489999999999999991 	0.165000000000000008 	1.07099999999999995 	0.481999999999999984 	0.193500000000000005 	0.35199999999999998 	10 	
-2 	0.680000000000000049 	0.540000000000000036 	0.209999999999999992 	1.78849999999999998 	0.83450000000000002 	0.407999999999999974 	0.437 	13 	
-0 	0.520000000000000018 	0.395000000000000018 	0.179999999999999993 	0.640000000000000013 	0.158000000000000002 	0.110000000000000001 	0.244999999999999996 	22 	
-1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.204999999999999988 	0.0779999999999999999 	0.0485000000000000014 	0.0700000000000000067 	7 	
-0 	0.5 	0.385000000000000009 	0.104999999999999996 	0.497999999999999998 	0.179499999999999993 	0.1095 	0.170000000000000012 	17 	
-1 	0.550000000000000044 	0.424999999999999989 	0.149999999999999994 	0.766499999999999959 	0.339000000000000024 	0.17599999999999999 	0.209999999999999992 	8 	
-1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.313 	0.139000000000000012 	0.0625 	0.0965000000000000024 	7 	
-1 	0.479999999999999982 	0.369999999999999996 	0.119999999999999996 	0.514000000000000012 	0.20749999999999999 	0.131000000000000005 	0.154999999999999999 	13 	
-2 	0.455000000000000016 	0.340000000000000024 	0.135000000000000009 	0.462000000000000022 	0.16750000000000001 	0.158000000000000002 	0.119999999999999996 	9 	
-2 	0.584999999999999964 	0.429999999999999993 	0.160000000000000003 	0.95499999999999996 	0.362499999999999989 	0.17599999999999999 	0.270000000000000018 	11 	
-0 	0.569999999999999951 	0.429999999999999993 	0.160000000000000003 	0.811000000000000054 	0.387500000000000011 	0.159000000000000002 	0.228500000000000009 	9 	
-1 	0.474999999999999978 	0.375 	0.110000000000000001 	0.493999999999999995 	0.210999999999999993 	0.109 	0.154499999999999998 	8 	
-2 	0.584999999999999964 	0.465000000000000024 	0.165000000000000008 	0.885000000000000009 	0.402500000000000024 	0.162500000000000006 	0.274000000000000021 	10 	
-1 	0.395000000000000018 	0.299999999999999989 	0.0899999999999999967 	0.253000000000000003 	0.115500000000000005 	0.0500000000000000028 	0.0749999999999999972 	6 	
-1 	0.530000000000000027 	0.41499999999999998 	0.130000000000000004 	0.69399999999999995 	0.390500000000000014 	0.111000000000000001 	0.16700000000000001 	9 	
-1 	0.450000000000000011 	0.33500000000000002 	0.104999999999999996 	0.447000000000000008 	0.233500000000000013 	0.152999999999999997 	0.118999999999999995 	7 	
-0 	0.645000000000000018 	0.510000000000000009 	0.160000000000000003 	1.24150000000000005 	0.581500000000000017 	0.276000000000000023 	0.315000000000000002 	9 	
-0 	0.650000000000000022 	0.525000000000000022 	0.174999999999999989 	1.4225000000000001 	0.609999999999999987 	0.299499999999999988 	0.445000000000000007 	20 	
-1 	0.525000000000000022 	0.405000000000000027 	0.149999999999999994 	0.79500000000000004 	0.307499999999999996 	0.204999999999999988 	0.255000000000000004 	14 	
-1 	0.515000000000000013 	0.385000000000000009 	0.125 	0.571999999999999953 	0.236999999999999988 	0.143499999999999989 	0.165000000000000008 	7 	
-2 	0.665000000000000036 	0.535000000000000031 	0.225000000000000006 	2.1835 	0.753499999999999948 	0.391000000000000014 	0.885000000000000009 	27 	
-2 	0.584999999999999964 	0.46000000000000002 	0.14499999999999999 	0.933499999999999996 	0.47799999999999998 	0.182499999999999996 	0.234999999999999987 	9 	
-0 	0.589999999999999969 	0.474999999999999978 	0.160000000000000003 	1.10149999999999992 	0.47749999999999998 	0.255500000000000005 	0.294999999999999984 	13 	
-2 	0.699999999999999956 	0.564999999999999947 	0.174999999999999989 	1.85650000000000004 	0.844500000000000028 	0.393500000000000016 	0.540000000000000036 	10 	
-1 	0.299999999999999989 	0.23000000000000001 	0.0950000000000000011 	0.138500000000000012 	0.0560000000000000012 	0.0364999999999999977 	0.0369999999999999982 	6 	
-0 	0.680000000000000049 	0.560000000000000053 	0.195000000000000007 	1.77750000000000008 	0.860999999999999988 	0.322000000000000008 	0.41499999999999998 	11 	
-0 	0.535000000000000031 	0.400000000000000022 	0.149999999999999994 	0.804499999999999993 	0.33450000000000002 	0.212499999999999994 	0.209999999999999992 	13 	
-0 	0.689999999999999947 	0.550000000000000044 	0.200000000000000011 	1.56899999999999995 	0.687000000000000055 	0.367499999999999993 	0.46000000000000002 	12 	
-1 	0.589999999999999969 	0.450000000000000011 	0.14499999999999999 	1.02200000000000002 	0.427999999999999992 	0.268000000000000016 	0.265000000000000013 	10 	
-1 	0.564999999999999947 	0.429999999999999993 	0.149999999999999994 	0.821500000000000008 	0.332000000000000017 	0.168500000000000011 	0.28999999999999998 	11 	
-2 	0.594999999999999973 	0.479999999999999982 	0.165000000000000008 	1.26200000000000001 	0.483499999999999985 	0.282999999999999974 	0.409999999999999976 	17 	
-2 	0.380000000000000004 	0.234999999999999987 	0.100000000000000006 	0.258000000000000007 	0.105499999999999997 	0.0539999999999999994 	0.0800000000000000017 	7 	
-0 	0.719999999999999973 	0.550000000000000044 	0.179999999999999993 	1.52000000000000002 	0.637000000000000011 	0.325000000000000011 	0.434999999999999998 	10 	
-2 	0.699999999999999956 	0.535000000000000031 	0.160000000000000003 	1.72550000000000003 	0.630000000000000004 	0.263500000000000012 	0.540000000000000036 	19 	
-2 	0.630000000000000004 	0.479999999999999982 	0.14499999999999999 	1.01150000000000007 	0.423499999999999988 	0.236999999999999988 	0.304999999999999993 	12 	
-1 	0.520000000000000018 	0.405000000000000027 	0.140000000000000013 	0.577500000000000013 	0.200000000000000011 	0.14499999999999999 	0.178999999999999992 	11 	
-1 	0.469999999999999973 	0.349999999999999978 	0.135000000000000009 	0.566999999999999948 	0.231500000000000011 	0.146499999999999991 	0.152499999999999997 	11 	
-2 	0.734999999999999987 	0.589999999999999969 	0.204999999999999988 	2.08700000000000019 	0.90900000000000003 	0.473999999999999977 	0.625 	12 	
-1 	0.505000000000000004 	0.385000000000000009 	0.119999999999999996 	0.600500000000000034 	0.23899999999999999 	0.141999999999999987 	0.184999999999999998 	7 	
-1 	0.395000000000000018 	0.294999999999999984 	0.100000000000000006 	0.292999999999999983 	0.140000000000000013 	0.0619999999999999996 	0.0820000000000000034 	7 	
-2 	0.619999999999999996 	0.479999999999999982 	0.149999999999999994 	1.10149999999999992 	0.496499999999999997 	0.242999999999999994 	0.304999999999999993 	10 	
-1 	0.555000000000000049 	0.424999999999999989 	0.14499999999999999 	0.79049999999999998 	0.348499999999999976 	0.17649999999999999 	0.225000000000000006 	9 	
-1 	0.555000000000000049 	0.424999999999999989 	0.179999999999999993 	0.875 	0.369499999999999995 	0.200500000000000012 	0.255000000000000004 	11 	
-1 	0.184999999999999998 	0.375 	0.119999999999999996 	0.464500000000000024 	0.196000000000000008 	0.104499999999999996 	0.149999999999999994 	6 	
-2 	0.489999999999999991 	0.380000000000000004 	0.140000000000000013 	0.638499999999999956 	0.23050000000000001 	0.141999999999999987 	0.195000000000000007 	13 	
-0 	0.349999999999999978 	0.265000000000000013 	0.0899999999999999967 	0.185499999999999998 	0.0744999999999999968 	0.0415000000000000022 	0.0599999999999999978 	7 	
-0 	0.729999999999999982 	0.57999999999999996 	0.190000000000000002 	1.73750000000000004 	0.678499999999999992 	0.434499999999999997 	0.520000000000000018 	11 	
-1 	0.239999999999999991 	0.179999999999999993 	0.0550000000000000003 	0.0555000000000000007 	0.0235000000000000001 	0.0129999999999999994 	0.0179999999999999986 	4 	
-1 	0.604999999999999982 	0.469999999999999973 	0.14499999999999999 	0.802499999999999991 	0.379000000000000004 	0.226500000000000007 	0.220000000000000001 	9 	
-2 	0.550000000000000044 	0.419999999999999984 	0.160000000000000003 	1.34050000000000002 	0.632499999999999951 	0.310999999999999999 	0.343999999999999972 	10 	
-2 	0.520000000000000018 	0.400000000000000022 	0.104999999999999996 	0.871999999999999997 	0.451500000000000012 	0.161500000000000005 	0.19850000000000001 	9 	
-1 	0.54500000000000004 	0.419999999999999984 	0.125 	0.716999999999999971 	0.357999999999999985 	0.112000000000000002 	0.220000000000000001 	8 	
-2 	0.699999999999999956 	0.525000000000000022 	0.174999999999999989 	1.75849999999999995 	0.874500000000000055 	0.361499999999999988 	0.469999999999999973 	10 	
-0 	0.625 	0.489999999999999991 	0.154999999999999999 	1.11499999999999999 	0.483999999999999986 	0.277000000000000024 	0.309499999999999997 	9 	
-0 	0.67000000000000004 	0.540000000000000036 	0.195000000000000007 	1.61899999999999999 	0.739999999999999991 	0.330500000000000016 	0.465000000000000024 	11 	
-2 	0.689999999999999947 	0.550000000000000044 	0.200000000000000011 	1.84650000000000003 	0.731999999999999984 	0.471999999999999975 	0.569999999999999951 	19 	
-2 	0.619999999999999996 	0.494999999999999996 	0.179999999999999993 	1.25550000000000006 	0.576500000000000012 	0.254000000000000004 	0.354999999999999982 	12 	
-2 	0.604999999999999982 	0.489999999999999991 	0.14499999999999999 	1.30000000000000004 	0.517000000000000015 	0.328500000000000014 	0.309999999999999998 	14 	
-2 	0.680000000000000049 	0.530000000000000027 	0.179999999999999993 	1.52899999999999991 	0.763499999999999956 	0.311499999999999999 	0.402500000000000024 	11 	
-1 	0.569999999999999951 	0.445000000000000007 	0.154999999999999999 	0.866999999999999993 	0.370499999999999996 	0.170500000000000013 	0.280000000000000027 	9 	
-1 	0.525000000000000022 	0.41499999999999998 	0.119999999999999996 	0.595999999999999974 	0.280500000000000027 	0.119999999999999996 	0.169500000000000012 	9 	
-2 	0.625 	0.489999999999999991 	0.165000000000000008 	1.11650000000000005 	0.489499999999999991 	0.26150000000000001 	0.332500000000000018 	11 	
-1 	0.385000000000000009 	0.299999999999999989 	0.125 	0.343000000000000027 	0.170500000000000013 	0.0734999999999999959 	0.0810000000000000026 	7 	
-1 	0.309999999999999998 	0.214999999999999997 	0.0749999999999999972 	0.127500000000000002 	0.0565000000000000016 	0.0275000000000000001 	0.0359999999999999973 	7 	
-1 	0.429999999999999993 	0.340000000000000024 	0.100000000000000006 	0.340500000000000025 	0.139500000000000013 	0.0665000000000000036 	0.119999999999999996 	8 	
-2 	0.520000000000000018 	0.419999999999999984 	0.160000000000000003 	0.744999999999999996 	0.255000000000000004 	0.157000000000000001 	0.288499999999999979 	11 	
-1 	0.604999999999999982 	0.494999999999999996 	0.14499999999999999 	1.05400000000000005 	0.368999999999999995 	0.225500000000000006 	0.359999999999999987 	12 	
-2 	0.400000000000000022 	0.299999999999999989 	0.125 	0.416999999999999982 	0.191000000000000003 	0.0899999999999999967 	0.117499999999999993 	9 	
-0 	0.474999999999999978 	0.375 	0.125 	0.578500000000000014 	0.277500000000000024 	0.0850000000000000061 	0.154999999999999999 	10 	
-1 	0.41499999999999998 	0.315000000000000002 	0.104999999999999996 	0.330000000000000016 	0.140500000000000014 	0.0704999999999999932 	0.0950000000000000011 	6 	
-2 	0.625 	0.494999999999999996 	0.174999999999999989 	1.20750000000000002 	0.531000000000000028 	0.281000000000000028 	0.35249999999999998 	11 	
-2 	0.665000000000000036 	0.535000000000000031 	0.195000000000000007 	1.60600000000000009 	0.575500000000000012 	0.388000000000000012 	0.479999999999999982 	14 	
-1 	0.364999999999999991 	0.275000000000000022 	0.0850000000000000061 	0.223000000000000004 	0.0980000000000000038 	0.0374999999999999986 	0.0749999999999999972 	7 	
-2 	0.625 	0.489999999999999991 	0.165000000000000008 	1.20500000000000007 	0.51749999999999996 	0.310499999999999998 	0.346499999999999975 	10 	
-2 	0.54500000000000004 	0.41499999999999998 	0.140000000000000013 	0.819999999999999951 	0.461500000000000021 	0.127000000000000002 	0.217999999999999999 	9 	
-1 	0.569999999999999951 	0.440000000000000002 	0.130000000000000004 	0.766499999999999959 	0.346999999999999975 	0.178499999999999992 	0.202000000000000013 	10 	
-1 	0.515000000000000013 	0.390000000000000013 	0.140000000000000013 	0.555499999999999994 	0.200000000000000011 	0.113500000000000004 	0.223500000000000004 	12 	
-1 	0.630000000000000004 	0.505000000000000004 	0.179999999999999993 	1.27200000000000002 	0.602500000000000036 	0.294999999999999984 	0.315000000000000002 	11 	
-2 	0.645000000000000018 	0.525000000000000022 	0.160000000000000003 	1.50750000000000006 	0.745500000000000052 	0.244999999999999996 	0.432499999999999996 	11 	
-0 	0.555000000000000049 	0.455000000000000016 	0.179999999999999993 	0.957999999999999963 	0.295999999999999985 	0.195000000000000007 	0.390000000000000013 	14 	
-2 	0.640000000000000013 	0.520000000000000018 	0.200000000000000011 	1.40700000000000003 	0.565999999999999948 	0.303999999999999992 	0.455000000000000016 	17 	
-2 	0.535000000000000031 	0.419999999999999984 	0.14499999999999999 	0.791000000000000036 	0.330000000000000016 	0.189000000000000001 	0.25 	10 	
-0 	0.525000000000000022 	0.400000000000000022 	0.135000000000000009 	0.713999999999999968 	0.318000000000000005 	0.138000000000000012 	0.20799999999999999 	10 	
-1 	0.479999999999999982 	0.390000000000000013 	0.14499999999999999 	0.582500000000000018 	0.231500000000000011 	0.120999999999999996 	0.255000000000000004 	15 	
-2 	0.46000000000000002 	0.349999999999999978 	0.119999999999999996 	0.515000000000000013 	0.224000000000000005 	0.107999999999999999 	0.1565 	10 	
-1 	0.280000000000000027 	0.200000000000000011 	0.0749999999999999972 	0.122499999999999998 	0.0544999999999999998 	0.0114999999999999998 	0.0350000000000000033 	5 	
-0 	0.54500000000000004 	0.445000000000000007 	0.174999999999999989 	0.852500000000000036 	0.346499999999999975 	0.189000000000000001 	0.294999999999999984 	13 	
-0 	0.385000000000000009 	0.304999999999999993 	0.104999999999999996 	0.331500000000000017 	0.13650000000000001 	0.0744999999999999968 	0.100000000000000006 	7 	
-2 	0.569999999999999951 	0.474999999999999978 	0.195000000000000007 	1.02950000000000008 	0.463500000000000023 	0.190500000000000003 	0.304999999999999993 	18 	
-1 	0.359999999999999987 	0.270000000000000018 	0.0899999999999999967 	0.232000000000000012 	0.119999999999999996 	0.043499999999999997 	0.0560000000000000012 	8 	
-2 	0.505000000000000004 	0.409999999999999976 	0.125 	0.642000000000000015 	0.288999999999999979 	0.133000000000000007 	0.154999999999999999 	9 	
-2 	0.469999999999999973 	0.390000000000000013 	0.149999999999999994 	0.635499999999999954 	0.2185 	0.0884999999999999953 	0.255000000000000004 	9 	
-1 	0.474999999999999978 	0.359999999999999987 	0.14499999999999999 	0.632499999999999951 	0.282499999999999973 	0.137000000000000011 	0.190000000000000002 	8 	
-0 	0.594999999999999973 	0.465000000000000024 	0.140000000000000013 	1.11299999999999999 	0.51749999999999996 	0.243999999999999995 	0.304999999999999993 	12 	
-2 	0.440000000000000002 	0.349999999999999978 	0.110000000000000001 	0.458500000000000019 	0.200000000000000011 	0.0884999999999999953 	0.130000000000000004 	9 	
-2 	0.589999999999999969 	0.465000000000000024 	0.140000000000000013 	1.04600000000000004 	0.469499999999999973 	0.263000000000000012 	0.263000000000000012 	7 	
-2 	0.719999999999999973 	0.57999999999999996 	0.190000000000000002 	2.0884999999999998 	0.995500000000000052 	0.47799999999999998 	0.530499999999999972 	13 	
-1 	0.405000000000000027 	0.309999999999999998 	0.0899999999999999967 	0.312 	0.138000000000000012 	0.0599999999999999978 	0.086999999999999994 	8 	
-2 	0.270000000000000018 	0.195000000000000007 	0.0700000000000000067 	0.105999999999999997 	0.0464999999999999997 	0.0179999999999999986 	0.0359999999999999973 	7 	
-0 	0.640000000000000013 	0.5 	0.179999999999999993 	1.49950000000000006 	0.592999999999999972 	0.314000000000000001 	0.430999999999999994 	11 	
-0 	0.604999999999999982 	0.474999999999999978 	0.174999999999999989 	1.3819999999999999 	0.608999999999999986 	0.232500000000000012 	0.398500000000000021 	10 	
-0 	0.655000000000000027 	0.515000000000000013 	0.179999999999999993 	1.41199999999999992 	0.619500000000000051 	0.248499999999999999 	0.496999999999999997 	11 	
-1 	0.154999999999999999 	0.104999999999999996 	0.0500000000000000028 	0.0175000000000000017 	0.0050000000000000001 	0.00350000000000000007 	0.0050000000000000001 	4 	
-0 	0.594999999999999973 	0.434999999999999998 	0.149999999999999994 	0.900000000000000022 	0.417499999999999982 	0.170000000000000012 	0.265000000000000013 	8 	
-2 	0.609999999999999987 	0.479999999999999982 	0.165000000000000008 	1.24399999999999999 	0.634499999999999953 	0.257000000000000006 	0.304999999999999993 	12 	
-2 	0.619999999999999996 	0.489999999999999991 	0.160000000000000003 	1.03499999999999992 	0.440000000000000002 	0.252500000000000002 	0.284999999999999976 	11 	
-2 	0.530000000000000027 	0.41499999999999998 	0.140000000000000013 	0.723999999999999977 	0.310499999999999998 	0.16750000000000001 	0.204999999999999988 	10 	
-2 	0.569999999999999951 	0.450000000000000011 	0.154999999999999999 	1.19500000000000006 	0.5625 	0.256500000000000006 	0.294999999999999984 	10 	
-1 	0.344999999999999973 	0.275000000000000022 	0.0950000000000000011 	0.199500000000000011 	0.0754999999999999977 	0.0534999999999999989 	0.0700000000000000067 	6 	
-2 	0.770000000000000018 	0.599999999999999978 	0.214999999999999997 	2.19450000000000012 	1.0515000000000001 	0.481999999999999984 	0.583999999999999964 	10 	
-2 	0.594999999999999973 	0.455000000000000016 	0.149999999999999994 	1.04400000000000004 	0.518000000000000016 	0.220500000000000002 	0.270000000000000018 	9 	
-0 	0.424999999999999989 	0.330000000000000016 	0.115000000000000005 	0.406000000000000028 	0.163500000000000006 	0.0810000000000000026 	0.135500000000000009 	8 	
-2 	0.574999999999999956 	0.474999999999999978 	0.160000000000000003 	1.1140000000000001 	0.495499999999999996 	0.274500000000000022 	0.28999999999999998 	9 	
-0 	0.569999999999999951 	0.450000000000000011 	0.179999999999999993 	0.908000000000000029 	0.401500000000000024 	0.216999999999999998 	0.255000000000000004 	9 	
-0 	0.455000000000000016 	0.349999999999999978 	0.125 	0.44850000000000001 	0.158500000000000002 	0.101999999999999993 	0.133500000000000008 	16 	
-1 	0.494999999999999996 	0.380000000000000004 	0.119999999999999996 	0.512000000000000011 	0.233000000000000013 	0.120499999999999996 	0.13600000000000001 	7 	
-2 	0.560000000000000053 	0.46000000000000002 	0.234999999999999987 	0.839500000000000024 	0.332500000000000018 	0.157000000000000001 	0.304999999999999993 	12 	
-0 	0.530000000000000027 	0.395000000000000018 	0.14499999999999999 	0.775000000000000022 	0.307999999999999996 	0.169000000000000011 	0.255000000000000004 	7 	
-2 	0.569999999999999951 	0.405000000000000027 	0.160000000000000003 	0.924499999999999988 	0.344499999999999973 	0.2185 	0.294999999999999984 	19 	
-2 	0.614999999999999991 	0.474999999999999978 	0.170000000000000012 	1.18250000000000011 	0.473999999999999977 	0.28949999999999998 	0.239999999999999991 	11 	
-1 	0.450000000000000011 	0.344999999999999973 	0.135000000000000009 	0.443000000000000005 	0.197500000000000009 	0.0874999999999999944 	0.117499999999999993 	14 	
-2 	0.57999999999999996 	0.440000000000000002 	0.160000000000000003 	0.829500000000000015 	0.336500000000000021 	0.200500000000000012 	0.248499999999999999 	9 	
-2 	0.385000000000000009 	0.275000000000000022 	0.115000000000000005 	0.268500000000000016 	0.0975000000000000033 	0.0825000000000000039 	0.0850000000000000061 	8 	
-0 	0.604999999999999982 	0.450000000000000011 	0.195000000000000007 	1.09800000000000009 	0.480999999999999983 	0.28949999999999998 	0.315000000000000002 	13 	
-2 	0.599999999999999978 	0.450000000000000011 	0.160000000000000003 	1.1419999999999999 	0.539000000000000035 	0.225000000000000006 	0.306999999999999995 	10 	
-2 	0.734999999999999987 	0.589999999999999969 	0.225000000000000006 	1.75600000000000001 	0.637000000000000011 	0.340500000000000025 	0.57999999999999996 	21 	
-1 	0.25 	0.184999999999999998 	0.0650000000000000022 	0.0709999999999999937 	0.0269999999999999997 	0.0184999999999999991 	0.0224999999999999992 	5 	
-1 	0.530000000000000027 	0.41499999999999998 	0.110000000000000001 	0.574500000000000011 	0.252500000000000002 	0.123499999999999999 	0.189000000000000001 	9 	
-1 	0.505000000000000004 	0.390000000000000013 	0.149999999999999994 	0.685000000000000053 	0.361999999999999988 	0.131000000000000005 	0.156 	8 	
-0 	0.660000000000000031 	0.535000000000000031 	0.204999999999999988 	1.4415 	0.592500000000000027 	0.277500000000000024 	0.489999999999999991 	10 	
-1 	0.375 	0.280000000000000027 	0.0850000000000000061 	0.214499999999999996 	0.0855000000000000066 	0.0485000000000000014 	0.0719999999999999946 	7 	
-1 	0.5 	0.380000000000000004 	0.110000000000000001 	0.493999999999999995 	0.217999999999999999 	0.0899999999999999967 	0.132500000000000007 	7 	
-1 	0.560000000000000053 	0.434999999999999998 	0.135000000000000009 	0.719999999999999973 	0.329000000000000015 	0.102999999999999994 	0.251000000000000001 	11 	
-1 	0.184999999999999998 	0.130000000000000004 	0.0449999999999999983 	0.0290000000000000015 	0.0120000000000000002 	0.00749999999999999972 	0.00949999999999999976 	4 	
-2 	0.630000000000000004 	0.5 	0.174999999999999989 	1.26449999999999996 	0.563500000000000001 	0.306499999999999995 	0.342500000000000027 	10 	
-1 	0.140000000000000013 	0.104999999999999996 	0.0350000000000000033 	0.0140000000000000003 	0.00549999999999999968 	0.00250000000000000005 	0.00400000000000000008 	3 	
-2 	0.609999999999999987 	0.484999999999999987 	0.14499999999999999 	1.33050000000000002 	0.783000000000000029 	0.225500000000000006 	0.286499999999999977 	9 	
-1 	0.469999999999999973 	0.354999999999999982 	0.119999999999999996 	0.393000000000000016 	0.16700000000000001 	0.0884999999999999953 	0.115000000000000005 	8 	
-1 	0.284999999999999976 	0.209999999999999992 	0.0700000000000000067 	0.109 	0.0439999999999999974 	0.0264999999999999993 	0.0330000000000000016 	5 	
-1 	0.525000000000000022 	0.429999999999999993 	0.149999999999999994 	0.736500000000000044 	0.322500000000000009 	0.161000000000000004 	0.214999999999999997 	11 	
-1 	0.270000000000000018 	0.200000000000000011 	0.0700000000000000067 	0.100000000000000006 	0.0340000000000000024 	0.0245000000000000009 	0.0350000000000000033 	5 	
-0 	0.505000000000000004 	0.375 	0.115000000000000005 	0.589500000000000024 	0.263500000000000012 	0.119999999999999996 	0.16700000000000001 	10 	
-2 	0.630000000000000004 	0.484999999999999987 	0.165000000000000008 	1.2330000000000001 	0.656499999999999972 	0.231500000000000011 	0.303499999999999992 	10 	
-2 	0.630000000000000004 	0.484999999999999987 	0.174999999999999989 	1.30000000000000004 	0.433499999999999996 	0.294499999999999984 	0.46000000000000002 	23 	
-0 	0.67000000000000004 	0.584999999999999964 	0.160000000000000003 	1.30899999999999994 	0.544499999999999984 	0.294499999999999984 	0.412999999999999978 	10 	
-2 	0.494999999999999996 	0.385000000000000009 	0.135000000000000009 	0.633499999999999952 	0.200000000000000011 	0.122499999999999998 	0.260000000000000009 	14 	
-1 	0.325000000000000011 	0.25 	0.0550000000000000003 	0.166000000000000009 	0.0759999999999999981 	0.0509999999999999967 	0.0449999999999999983 	5 	
-1 	0.574999999999999956 	0.455000000000000016 	0.160000000000000003 	0.989500000000000046 	0.494999999999999996 	0.195000000000000007 	0.245999999999999996 	9 	
-2 	0.685000000000000053 	0.520000000000000018 	0.149999999999999994 	1.34299999999999997 	0.463500000000000023 	0.291999999999999982 	0.400000000000000022 	13 	
-2 	0.589999999999999969 	0.5 	0.149999999999999994 	1.1419999999999999 	0.484999999999999987 	0.265000000000000013 	0.344999999999999973 	9 	
-0 	0.520000000000000018 	0.405000000000000027 	0.125 	0.643499999999999961 	0.241499999999999992 	0.173499999999999988 	0.209999999999999992 	12 	
-0 	0.599999999999999978 	0.479999999999999982 	0.170000000000000012 	1.05600000000000005 	0.457500000000000018 	0.243499999999999994 	0.313500000000000001 	10 	
-0 	0.479999999999999982 	0.390000000000000013 	0.125 	0.690500000000000003 	0.219 	0.154999999999999999 	0.200000000000000011 	12 	
-0 	0.520000000000000018 	0.409999999999999976 	0.170000000000000012 	0.870500000000000052 	0.373499999999999999 	0.219 	0.25 	14 	
-1 	0.385000000000000009 	0.28999999999999998 	0.0899999999999999967 	0.236499999999999988 	0.100000000000000006 	0.0505000000000000032 	0.0759999999999999981 	8 	
-1 	0.560000000000000053 	0.440000000000000002 	0.140000000000000013 	0.824999999999999956 	0.402000000000000024 	0.139000000000000012 	0.244999999999999996 	10 	
-0 	0.469999999999999973 	0.364999999999999991 	0.104999999999999996 	0.420499999999999985 	0.163000000000000006 	0.103499999999999995 	0.140000000000000013 	9 	
-1 	0.445000000000000007 	0.330000000000000016 	0.100000000000000006 	0.437 	0.163000000000000006 	0.0754999999999999977 	0.170000000000000012 	13 	
-0 	0.540000000000000036 	0.424999999999999989 	0.160000000000000003 	0.945500000000000007 	0.367499999999999993 	0.200500000000000012 	0.294999999999999984 	9 	
-1 	0.359999999999999987 	0.275000000000000022 	0.0950000000000000011 	0.216999999999999998 	0.0840000000000000052 	0.043499999999999997 	0.0899999999999999967 	7 	
-1 	0.440000000000000002 	0.354999999999999982 	0.119999999999999996 	0.494999999999999996 	0.231000000000000011 	0.110000000000000001 	0.125 	7 	
-2 	0.614999999999999991 	0.510000000000000009 	0.149999999999999994 	1.29600000000000004 	0.54500000000000004 	0.331500000000000017 	0.320000000000000007 	9 	
-2 	0.57999999999999996 	0.445000000000000007 	0.135000000000000009 	0.813999999999999946 	0.377500000000000002 	0.191500000000000004 	0.220000000000000001 	9 	
-0 	0.650000000000000022 	0.510000000000000009 	0.184999999999999998 	1.375 	0.531000000000000028 	0.384000000000000008 	0.398500000000000021 	10 	
-2 	0.5 	0.400000000000000022 	0.125 	0.672499999999999987 	0.336000000000000021 	0.119999999999999996 	0.182499999999999996 	7 	
-2 	0.614999999999999991 	0.474999999999999978 	0.174999999999999989 	1.22399999999999998 	0.603500000000000036 	0.26100000000000001 	0.310999999999999999 	9 	
-2 	0.67000000000000004 	0.515000000000000013 	0.165000000000000008 	1.17349999999999999 	0.526000000000000023 	0.284999999999999976 	0.316000000000000003 	11 	
-1 	0.419999999999999984 	0.304999999999999993 	0.0899999999999999967 	0.328000000000000014 	0.16800000000000001 	0.0614999999999999991 	0.0820000000000000034 	6 	
-0 	0.655000000000000027 	0.46000000000000002 	0.160000000000000003 	1.49399999999999999 	0.689500000000000002 	0.331000000000000016 	0.182499999999999996 	9 	
-2 	0.609999999999999987 	0.489999999999999991 	0.149999999999999994 	1.10299999999999998 	0.424999999999999989 	0.202500000000000013 	0.359999999999999987 	23 	
-0 	0.680000000000000049 	0.57999999999999996 	0.200000000000000011 	1.78699999999999992 	0.584999999999999964 	0.453000000000000014 	0.599999999999999978 	19 	
-0 	0.41499999999999998 	0.325000000000000011 	0.104999999999999996 	0.380000000000000004 	0.159500000000000003 	0.0785000000000000003 	0.119999999999999996 	12 	
-1 	0.280000000000000027 	0.200000000000000011 	0.0650000000000000022 	0.0894999999999999962 	0.0359999999999999973 	0.0184999999999999991 	0.0299999999999999989 	7 	
-1 	0.429999999999999993 	0.340000000000000024 	0.119999999999999996 	0.357499999999999984 	0.150999999999999995 	0.0645000000000000018 	0.104499999999999996 	9 	
-2 	0.630000000000000004 	0.505000000000000004 	0.225000000000000006 	1.52499999999999991 	0.560000000000000053 	0.333500000000000019 	0.450000000000000011 	15 	
-1 	0.515000000000000013 	0.41499999999999998 	0.135000000000000009 	0.712500000000000022 	0.284999999999999976 	0.151999999999999996 	0.244999999999999996 	10 	
-0 	0.619999999999999996 	0.484999999999999987 	0.174999999999999989 	1.21550000000000002 	0.54500000000000004 	0.253000000000000003 	0.344999999999999973 	10 	
-1 	0.359999999999999987 	0.275000000000000022 	0.0749999999999999972 	0.220500000000000002 	0.0985000000000000042 	0.0439999999999999974 	0.0660000000000000031 	7 	
-2 	0.57999999999999996 	0.455000000000000016 	0.170000000000000012 	0.930000000000000049 	0.407999999999999974 	0.259000000000000008 	0.220000000000000001 	9 	
-2 	0.599999999999999978 	0.450000000000000011 	0.149999999999999994 	0.866500000000000048 	0.369499999999999995 	0.195500000000000007 	0.255000000000000004 	12 	
-2 	0.349999999999999978 	0.265000000000000013 	0.0899999999999999967 	0.177499999999999991 	0.0575000000000000025 	0.0420000000000000026 	0.0680000000000000049 	12 	
-2 	0.555000000000000049 	0.409999999999999976 	0.125 	0.598999999999999977 	0.234499999999999986 	0.146499999999999991 	0.194000000000000006 	8 	
-2 	0.520000000000000018 	0.450000000000000011 	0.149999999999999994 	0.895000000000000018 	0.361499999999999988 	0.185999999999999999 	0.234999999999999987 	14 	
-2 	0.709999999999999964 	0.574999999999999956 	0.214999999999999997 	2.0089999999999999 	0.989500000000000046 	0.447500000000000009 	0.502000000000000002 	11 	
-1 	0.409999999999999976 	0.309999999999999998 	0.0899999999999999967 	0.339000000000000024 	0.154999999999999999 	0.0695000000000000062 	0.0899999999999999967 	7 	
-2 	0.584999999999999964 	0.455000000000000016 	0.140000000000000013 	0.969999999999999973 	0.462000000000000022 	0.184999999999999998 	0.294999999999999984 	9 	
-1 	0.450000000000000011 	0.349999999999999978 	0.119999999999999996 	0.468000000000000027 	0.200500000000000012 	0.106499999999999997 	0.132500000000000007 	8 	
-1 	0.320000000000000007 	0.255000000000000004 	0.0850000000000000061 	0.174499999999999988 	0.0719999999999999946 	0.0330000000000000016 	0.0570000000000000021 	8 	
-0 	0.625 	0.489999999999999991 	0.154999999999999999 	1.20849999999999991 	0.465000000000000024 	0.162000000000000005 	0.410999999999999976 	11 	
-2 	0.604999999999999982 	0.474999999999999978 	0.149999999999999994 	1.14999999999999991 	0.574999999999999956 	0.232000000000000012 	0.296999999999999986 	10 	
-0 	0.614999999999999991 	0.469999999999999973 	0.174999999999999989 	1.29849999999999999 	0.513499999999999956 	0.343000000000000027 	0.320000000000000007 	14 	
-0 	0.67000000000000004 	0.520000000000000018 	0.149999999999999994 	1.40599999999999992 	0.519000000000000017 	0.347999999999999976 	0.369999999999999996 	13 	
-0 	0.660000000000000031 	0.525000000000000022 	0.200000000000000011 	1.46300000000000008 	0.652499999999999969 	0.299499999999999988 	0.421999999999999986 	11 	
-2 	0.650000000000000022 	0.5 	0.154999999999999999 	1.20199999999999996 	0.564999999999999947 	0.313500000000000001 	0.293999999999999984 	11 	
-0 	0.564999999999999947 	0.440000000000000002 	0.149999999999999994 	0.982999999999999985 	0.447500000000000009 	0.235499999999999987 	0.248499999999999999 	9 	
-1 	0.609999999999999987 	0.424999999999999989 	0.154999999999999999 	1.04849999999999999 	0.507000000000000006 	0.195500000000000007 	0.274000000000000021 	11 	
-1 	0.409999999999999976 	0.325000000000000011 	0.104999999999999996 	0.360999999999999988 	0.160500000000000004 	0.0665000000000000036 	0.102999999999999994 	8 	
-0 	0.515000000000000013 	0.424999999999999989 	0.135000000000000009 	0.711999999999999966 	0.266500000000000015 	0.160500000000000004 	0.25 	11 	
-2 	0.625 	0.515000000000000013 	0.165000000000000008 	1.21700000000000008 	0.667000000000000037 	0.206499999999999989 	0.311499999999999999 	10 	
-1 	0.255000000000000004 	0.184999999999999998 	0.0650000000000000022 	0.0739999999999999963 	0.0304999999999999993 	0.0165000000000000008 	0.0200000000000000004 	4 	
-1 	0.320000000000000007 	0.214999999999999997 	0.0950000000000000011 	0.304999999999999993 	0.140000000000000013 	0.067000000000000004 	0.0884999999999999953 	6 	
-2 	0.494999999999999996 	0.400000000000000022 	0.154999999999999999 	0.808499999999999996 	0.234499999999999986 	0.115500000000000005 	0.349999999999999978 	6 	
-0 	0.57999999999999996 	0.450000000000000011 	0.149999999999999994 	0.92000000000000004 	0.393000000000000016 	0.211999999999999994 	0.28949999999999998 	9 	
-2 	0.5 	0.390000000000000013 	0.130000000000000004 	0.708999999999999964 	0.275000000000000022 	0.16800000000000001 	0.179999999999999993 	11 	
-2 	0.57999999999999996 	0.465000000000000024 	0.149999999999999994 	0.906499999999999972 	0.370999999999999996 	0.196500000000000008 	0.28999999999999998 	8 	
-1 	0.165000000000000008 	0.119999999999999996 	0.0299999999999999989 	0.0214999999999999983 	0.00700000000000000015 	0.0050000000000000001 	0.0050000000000000001 	3 	
-2 	0.625 	0.505000000000000004 	0.184999999999999998 	1.15650000000000008 	0.520000000000000018 	0.240499999999999992 	0.353499999999999981 	10 	
-2 	0.535000000000000031 	0.434999999999999998 	0.149999999999999994 	0.724999999999999978 	0.269000000000000017 	0.138500000000000012 	0.25 	9 	
-2 	0.564999999999999947 	0.434999999999999998 	0.184999999999999998 	0.981500000000000039 	0.329000000000000015 	0.13600000000000001 	0.390000000000000013 	13 	
-1 	0.255000000000000004 	0.184999999999999998 	0.0700000000000000067 	0.0749999999999999972 	0.0280000000000000006 	0.0179999999999999986 	0.0250000000000000014 	6 	
-2 	0.650000000000000022 	0.525000000000000022 	0.174999999999999989 	1.47150000000000003 	0.675000000000000044 	0.315000000000000002 	0.399000000000000021 	11 	
-1 	0.255000000000000004 	0.184999999999999998 	0.0599999999999999978 	0.0879999999999999949 	0.0364999999999999977 	0.0210000000000000013 	0.0229999999999999996 	5 	
-0 	0.645000000000000018 	0.550000000000000044 	0.174999999999999989 	1.29150000000000009 	0.569999999999999951 	0.304499999999999993 	0.330000000000000016 	14 	
-0 	0.375 	0.270000000000000018 	0.135000000000000009 	0.596999999999999975 	0.27200000000000002 	0.131000000000000005 	0.16750000000000001 	7 	
-1 	0.375 	0.280000000000000027 	0.0800000000000000017 	0.226000000000000006 	0.104999999999999996 	0.0470000000000000001 	0.0650000000000000022 	6 	
-0 	0.619999999999999996 	0.484999999999999987 	0.154999999999999999 	1.1944999999999999 	0.510499999999999954 	0.271000000000000019 	0.35199999999999998 	9 	
-2 	0.5 	0.375 	0.149999999999999994 	0.63600000000000001 	0.253500000000000003 	0.14499999999999999 	0.190000000000000002 	10 	
-1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.1875 	0.0810000000000000026 	0.0420000000000000026 	0.0580000000000000029 	6 	
-2 	0.520000000000000018 	0.400000000000000022 	0.119999999999999996 	0.57999999999999996 	0.234000000000000014 	0.131500000000000006 	0.184999999999999998 	8 	
-2 	0.604999999999999982 	0.474999999999999978 	0.140000000000000013 	1.11749999999999994 	0.555000000000000049 	0.257000000000000006 	0.274000000000000021 	9 	
-0 	0.450000000000000011 	0.359999999999999987 	0.125 	0.50649999999999995 	0.222000000000000003 	0.104999999999999996 	0.160000000000000003 	10 	
-0 	0.630000000000000004 	0.494999999999999996 	0.200000000000000011 	1.42549999999999999 	0.65900000000000003 	0.336000000000000021 	0.380000000000000004 	11 	
-0 	0.369999999999999996 	0.275000000000000022 	0.0800000000000000017 	0.227000000000000007 	0.0929999999999999993 	0.0625 	0.0700000000000000067 	8 	
-0 	0.535000000000000031 	0.405000000000000027 	0.125 	0.927000000000000046 	0.260000000000000009 	0.142499999999999988 	0.344999999999999973 	16 	
-1 	0.445000000000000007 	0.330000000000000016 	0.110000000000000001 	0.357999999999999985 	0.152499999999999997 	0.067000000000000004 	0.118499999999999994 	8 	
-2 	0.650000000000000022 	0.515000000000000013 	0.174999999999999989 	1.46599999999999997 	0.677000000000000046 	0.304499999999999993 	0.400000000000000022 	10 	
-2 	0.645000000000000018 	0.510000000000000009 	0.160000000000000003 	1.1835 	0.55600000000000005 	0.23849999999999999 	0.344999999999999973 	11 	
-2 	0.555000000000000049 	0.445000000000000007 	0.135000000000000009 	0.835999999999999965 	0.336000000000000021 	0.162500000000000006 	0.275000000000000022 	13 	
-2 	0.174999999999999989 	0.135000000000000009 	0.0400000000000000008 	0.0304999999999999993 	0.0109999999999999994 	0.00749999999999999972 	0.0100000000000000002 	5 	
-1 	0.369999999999999996 	0.280000000000000027 	0.0899999999999999967 	0.233000000000000013 	0.0904999999999999971 	0.0544999999999999998 	0.0700000000000000067 	11 	
-1 	0.510000000000000009 	0.395000000000000018 	0.130000000000000004 	0.602500000000000036 	0.281000000000000028 	0.142999999999999988 	0.162000000000000005 	7 	
-0 	0.599999999999999978 	0.465000000000000024 	0.154999999999999999 	1.04000000000000004 	0.475499999999999978 	0.25 	0.280000000000000027 	11 	
-2 	0.440000000000000002 	0.320000000000000007 	0.119999999999999996 	0.456500000000000017 	0.243499999999999994 	0.0919999999999999984 	0.102499999999999994 	8 	
-2 	0.614999999999999991 	0.46000000000000002 	0.170000000000000012 	1.05649999999999999 	0.481499999999999984 	0.27200000000000002 	0.270000000000000018 	10 	
-2 	0.569999999999999951 	0.450000000000000011 	0.160000000000000003 	0.861500000000000044 	0.372499999999999998 	0.217499999999999999 	0.255000000000000004 	12 	
-1 	0.469999999999999973 	0.385000000000000009 	0.130000000000000004 	0.586999999999999966 	0.264000000000000012 	0.117000000000000007 	0.173999999999999988 	8 	
-0 	0.535000000000000031 	0.400000000000000022 	0.149999999999999994 	1.22399999999999998 	0.617999999999999994 	0.275000000000000022 	0.287499999999999978 	10 	
-0 	0.569999999999999951 	0.450000000000000011 	0.174999999999999989 	0.955500000000000016 	0.380000000000000004 	0.166500000000000009 	0.294999999999999984 	18 	
-2 	0.359999999999999987 	0.270000000000000018 	0.0899999999999999967 	0.222500000000000003 	0.0830000000000000043 	0.0529999999999999985 	0.0749999999999999972 	6 	
-2 	0.46000000000000002 	0.375 	0.135000000000000009 	0.493499999999999994 	0.185999999999999999 	0.0845000000000000057 	0.170000000000000012 	12 	
-0 	0.525000000000000022 	0.400000000000000022 	0.130000000000000004 	0.699500000000000011 	0.311499999999999999 	0.131000000000000005 	0.223000000000000004 	9 	
-2 	0.640000000000000013 	0.494999999999999996 	0.170000000000000012 	1.13900000000000001 	0.53949999999999998 	0.281999999999999973 	0.284999999999999976 	10 	
-1 	0.25 	0.190000000000000002 	0.0599999999999999978 	0.0764999999999999986 	0.0359999999999999973 	0.0114999999999999998 	0.0245000000000000009 	6 	
-0 	0.614999999999999991 	0.474999999999999978 	0.154999999999999999 	1.11499999999999999 	0.483999999999999986 	0.211499999999999994 	0.354999999999999982 	10 	
-0 	0.530000000000000027 	0.409999999999999976 	0.165000000000000008 	0.811499999999999999 	0.239999999999999991 	0.169000000000000011 	0.239999999999999991 	19 	
-1 	0.349999999999999978 	0.255000000000000004 	0.0899999999999999967 	0.178499999999999992 	0.0855000000000000066 	0.0304999999999999993 	0.0524999999999999981 	8 	
-2 	0.354999999999999982 	0.280000000000000027 	0.0950000000000000011 	0.245499999999999996 	0.0955000000000000016 	0.0619999999999999996 	0.0749999999999999972 	11 	
-2 	0.330000000000000016 	0.25 	0.0899999999999999967 	0.197000000000000008 	0.0850000000000000061 	0.0410000000000000017 	0.0604999999999999982 	10 	
-1 	0.380000000000000004 	0.280000000000000027 	0.0850000000000000061 	0.273500000000000021 	0.115000000000000005 	0.0609999999999999987 	0.0850000000000000061 	6 	
-2 	0.70499999999999996 	0.560000000000000053 	0.165000000000000008 	1.67500000000000004 	0.797000000000000042 	0.409499999999999975 	0.388000000000000012 	10 	
-2 	0.469999999999999973 	0.380000000000000004 	0.14499999999999999 	0.586500000000000021 	0.23849999999999999 	0.143999999999999989 	0.184999999999999998 	8 	
-2 	0.614999999999999991 	0.469999999999999973 	0.165000000000000008 	1.12799999999999989 	0.446500000000000008 	0.219500000000000001 	0.340000000000000024 	10 	
-2 	0.474999999999999978 	0.359999999999999987 	0.100000000000000006 	0.428499999999999992 	0.196500000000000008 	0.0990000000000000047 	0.112000000000000002 	7 	
-1 	0.315000000000000002 	0.234999999999999987 	0.0700000000000000067 	0.148999999999999994 	0.0580000000000000029 	0.0325000000000000011 	0.0470000000000000001 	7 	
-1 	0.555000000000000049 	0.434999999999999998 	0.140000000000000013 	0.765000000000000013 	0.394500000000000017 	0.149999999999999994 	0.205999999999999989 	8 	
-2 	0.440000000000000002 	0.340000000000000024 	0.104999999999999996 	0.402000000000000024 	0.130500000000000005 	0.0955000000000000016 	0.165000000000000008 	10 	
-1 	0.465000000000000024 	0.340000000000000024 	0.110000000000000001 	0.345999999999999974 	0.142499999999999988 	0.0729999999999999954 	0.113000000000000003 	11 	
-2 	0.484999999999999987 	0.369999999999999996 	0.154999999999999999 	0.967999999999999972 	0.418999999999999984 	0.245499999999999996 	0.236499999999999988 	9 	
-2 	0.604999999999999982 	0.474999999999999978 	0.174999999999999989 	1.20100000000000007 	0.53949999999999998 	0.275000000000000022 	0.308999999999999997 	10 	
-0 	0.530000000000000027 	0.41499999999999998 	0.149999999999999994 	0.777499999999999969 	0.236999999999999988 	0.141499999999999987 	0.330000000000000016 	20 	
-0 	0.604999999999999982 	0.469999999999999973 	0.160000000000000003 	1.08349999999999991 	0.54049999999999998 	0.221500000000000002 	0.275000000000000022 	12 	
-2 	0.625 	0.479999999999999982 	0.14499999999999999 	1.08499999999999996 	0.464500000000000024 	0.244499999999999995 	0.327000000000000013 	10 	
-2 	0.195000000000000007 	0.14499999999999999 	0.0500000000000000028 	0.0320000000000000007 	0.0100000000000000002 	0.00800000000000000017 	0.0120000000000000002 	4 	
-2 	0.525000000000000022 	0.424999999999999989 	0.119999999999999996 	0.701999999999999957 	0.333500000000000019 	0.146499999999999991 	0.220000000000000001 	12 	
-2 	0.560000000000000053 	0.450000000000000011 	0.14499999999999999 	0.894000000000000017 	0.388500000000000012 	0.209499999999999992 	0.264000000000000012 	9 	
-2 	0.755000000000000004 	0.57999999999999996 	0.204999999999999988 	2.00649999999999995 	0.829500000000000015 	0.401500000000000024 	0.594999999999999973 	10 	
-1 	0.494999999999999996 	0.385000000000000009 	0.125 	0.584999999999999964 	0.275500000000000023 	0.123499999999999999 	0.165000000000000008 	8 	
-0 	0.584999999999999964 	0.469999999999999973 	0.170000000000000012 	1.09899999999999998 	0.39750000000000002 	0.232500000000000012 	0.357999999999999985 	20 	
-1 	0.294999999999999984 	0.225000000000000006 	0.0800000000000000017 	0.123999999999999999 	0.0485000000000000014 	0.0320000000000000007 	0.0400000000000000008 	9 	
-1 	0.359999999999999987 	0.284999999999999976 	0.104999999999999996 	0.241499999999999992 	0.091499999999999998 	0.0570000000000000021 	0.0749999999999999972 	7 	
-0 	0.564999999999999947 	0.440000000000000002 	0.149999999999999994 	0.862999999999999989 	0.434999999999999998 	0.148999999999999994 	0.270000000000000018 	9 	
-0 	0.550000000000000044 	0.424999999999999989 	0.135000000000000009 	0.851500000000000035 	0.361999999999999988 	0.196000000000000008 	0.270000000000000018 	14 	
-2 	0.57999999999999996 	0.455000000000000016 	0.135000000000000009 	0.795499999999999985 	0.405000000000000027 	0.16700000000000001 	0.203999999999999987 	10 	
-2 	0.589999999999999969 	0.469999999999999973 	0.149999999999999994 	0.995500000000000052 	0.480999999999999983 	0.232000000000000012 	0.239999999999999991 	8 	
-2 	0.650000000000000022 	0.520000000000000018 	0.174999999999999989 	1.26550000000000007 	0.614999999999999991 	0.277500000000000024 	0.336000000000000021 	9 	
-0 	0.625 	0.484999999999999987 	0.200000000000000011 	1.37999999999999989 	0.58450000000000002 	0.301999999999999991 	0.401000000000000023 	9 	
-1 	0.405000000000000027 	0.309999999999999998 	0.110000000000000001 	0.910000000000000031 	0.415999999999999981 	0.20749999999999999 	0.0995000000000000051 	8 	
-1 	0.375 	0.28999999999999998 	0.140000000000000013 	0.299999999999999989 	0.140000000000000013 	0.0625 	0.0825000000000000039 	8 	
-2 	0.440000000000000002 	0.33500000000000002 	0.110000000000000001 	0.394000000000000017 	0.157000000000000001 	0.096000000000000002 	0.121999999999999997 	9 	
-2 	0.640000000000000013 	0.525000000000000022 	0.179999999999999993 	1.31349999999999989 	0.486499999999999988 	0.299499999999999988 	0.407499999999999973 	10 	
-0 	0.604999999999999982 	0.469999999999999973 	0.165000000000000008 	1.17749999999999999 	0.610999999999999988 	0.227500000000000008 	0.291999999999999982 	9 	
-2 	0.635000000000000009 	0.510000000000000009 	0.209999999999999992 	1.59800000000000009 	0.65349999999999997 	0.283499999999999974 	0.57999999999999996 	15 	
-1 	0.255000000000000004 	0.195000000000000007 	0.0700000000000000067 	0.0734999999999999959 	0.0254999999999999984 	0.0200000000000000004 	0.0250000000000000014 	6 	
-0 	0.67000000000000004 	0.505000000000000004 	0.204999999999999988 	1.36450000000000005 	0.60750000000000004 	0.302499999999999991 	0.35299999999999998 	9 	
-2 	0.614999999999999991 	0.469999999999999973 	0.14499999999999999 	1.02849999999999997 	0.443500000000000005 	0.282499999999999973 	0.284999999999999976 	11 	
-2 	0.660000000000000031 	0.505000000000000004 	0.200000000000000011 	1.63050000000000006 	0.486499999999999988 	0.296999999999999986 	0.609999999999999987 	18 	
-0 	0.619999999999999996 	0.474999999999999978 	0.149999999999999994 	0.954500000000000015 	0.455000000000000016 	0.186499999999999999 	0.277000000000000024 	9 	
-2 	0.41499999999999998 	0.315000000000000002 	0.115000000000000005 	0.389500000000000013 	0.201500000000000012 	0.0650000000000000022 	0.102999999999999994 	9 	
-2 	0.520000000000000018 	0.465000000000000024 	0.149999999999999994 	0.950500000000000012 	0.456000000000000016 	0.19900000000000001 	0.255000000000000004 	8 	
-0 	0.604999999999999982 	0.479999999999999982 	0.140000000000000013 	0.990999999999999992 	0.473499999999999976 	0.234499999999999986 	0.239999999999999991 	8 	
-2 	0.344999999999999973 	0.270000000000000018 	0.0950000000000000011 	0.197000000000000008 	0.0665000000000000036 	0.0500000000000000028 	0.0700000000000000067 	9 	
-0 	0.680000000000000049 	0.520000000000000018 	0.184999999999999998 	1.49399999999999999 	0.614999999999999991 	0.393500000000000016 	0.406000000000000028 	11 	
-2 	0.154999999999999999 	0.110000000000000001 	0.0400000000000000008 	0.0154999999999999999 	0.0064999999999999997 	0.00300000000000000006 	0.0050000000000000001 	3 	
-1 	0.455000000000000016 	0.33500000000000002 	0.104999999999999996 	0.421999999999999986 	0.229000000000000009 	0.0864999999999999936 	0.100000000000000006 	6 	
-0 	0.574999999999999956 	0.46000000000000002 	0.184999999999999998 	1.09400000000000008 	0.44850000000000001 	0.216999999999999998 	0.344999999999999973 	15 	
-1 	0.484999999999999987 	0.364999999999999991 	0.140000000000000013 	0.447500000000000009 	0.189500000000000002 	0.0924999999999999989 	0.23050000000000001 	8 	
-1 	0.419999999999999984 	0.325000000000000011 	0.115000000000000005 	0.353999999999999981 	0.162500000000000006 	0.0640000000000000013 	0.104999999999999996 	8 	
-0 	0.650000000000000022 	0.54500000000000004 	0.23000000000000001 	1.752 	0.560499999999999998 	0.28949999999999998 	0.814999999999999947 	16 	
-0 	0.640000000000000013 	0.510000000000000009 	0.165000000000000008 	1.48599999999999999 	0.759499999999999953 	0.332000000000000017 	0.321000000000000008 	8 	
-2 	0.619999999999999996 	0.489999999999999991 	0.170000000000000012 	1.21049999999999991 	0.518499999999999961 	0.255500000000000005 	0.33500000000000002 	13 	
-1 	0.455000000000000016 	0.344999999999999973 	0.110000000000000001 	0.433999999999999997 	0.20699999999999999 	0.0855000000000000066 	0.121499999999999997 	8 	
-2 	0.599999999999999978 	0.479999999999999982 	0.174999999999999989 	1.22900000000000009 	0.412499999999999978 	0.273500000000000021 	0.41499999999999998 	13 	
-1 	0.450000000000000011 	0.33500000000000002 	0.0950000000000000011 	0.350499999999999978 	0.161500000000000005 	0.0625 	0.118499999999999994 	7 	
-1 	0.484999999999999987 	0.375 	0.130000000000000004 	0.553499999999999992 	0.266000000000000014 	0.112000000000000002 	0.157000000000000001 	8 	
-1 	0.429999999999999993 	0.340000000000000024 	0 	0.427999999999999992 	0.206499999999999989 	0.0859999999999999931 	0.115000000000000005 	8 	
-0 	0.609999999999999987 	0.429999999999999993 	0.140000000000000013 	0.90900000000000003 	0.438 	0.200000000000000011 	0.220000000000000001 	8 	
-1 	0.510000000000000009 	0.455000000000000016 	0.135000000000000009 	0.685499999999999998 	0.287499999999999978 	0.153999999999999998 	0.203499999999999986 	9 	
-2 	0.655000000000000027 	0.540000000000000036 	0.214999999999999997 	1.84400000000000008 	0.742500000000000049 	0.327000000000000013 	0.584999999999999964 	22 	
-1 	0.525000000000000022 	0.409999999999999976 	0.174999999999999989 	0.873999999999999999 	0.358499999999999985 	0.20699999999999999 	0.204999999999999988 	18 	
-0 	0.569999999999999951 	0.424999999999999989 	0.130000000000000004 	0.782000000000000028 	0.369499999999999995 	0.174499999999999988 	0.196500000000000008 	8 	
-1 	0.349999999999999978 	0.25 	0.0700000000000000067 	0.179999999999999993 	0.0655000000000000027 	0.048000000000000001 	0.0539999999999999994 	6 	
-1 	0.429999999999999993 	0.340000000000000024 	0.110000000000000001 	0.364499999999999991 	0.159000000000000002 	0.0855000000000000066 	0.104999999999999996 	7 	
-2 	0.709999999999999964 	0.560000000000000053 	0.220000000000000001 	2.01500000000000012 	0.921499999999999986 	0.454000000000000015 	0.565999999999999948 	11 	
-1 	0.369999999999999996 	0.275000000000000022 	0.100000000000000006 	0.281499999999999972 	0.150499999999999995 	0.0505000000000000032 	0.0680000000000000049 	5 	
-1 	0.395000000000000018 	0.325000000000000011 	0.104999999999999996 	0.305999999999999994 	0.111000000000000001 	0.0734999999999999959 	0.0950000000000000011 	8 	
-0 	0.560000000000000053 	0.450000000000000011 	0.160000000000000003 	1.02350000000000008 	0.428999999999999992 	0.268000000000000016 	0.299999999999999989 	10 	
-0 	0.604999999999999982 	0.505000000000000004 	0.179999999999999993 	1.43399999999999994 	0.728500000000000036 	0.264000000000000012 	0.430999999999999994 	11 	
-0 	0.560000000000000053 	0.429999999999999993 	0.149999999999999994 	0.882499999999999951 	0.346499999999999975 	0.171999999999999986 	0.309999999999999998 	9 	
-0 	0.57999999999999996 	0.434999999999999998 	0.154999999999999999 	0.878499999999999948 	0.424999999999999989 	0.168500000000000011 	0.242499999999999993 	10 	
-0 	0.515000000000000013 	0.429999999999999993 	0.140000000000000013 	0.833999999999999964 	0.366999999999999993 	0.200000000000000011 	0.23000000000000001 	8 	
-1 	0.440000000000000002 	0.340000000000000024 	0.104999999999999996 	0.368999999999999995 	0.164000000000000007 	0.0800000000000000017 	0.101500000000000007 	5 	
-1 	0.505000000000000004 	0.395000000000000018 	0.104999999999999996 	0.551000000000000045 	0.247999999999999998 	0.102999999999999994 	0.171000000000000013 	8 	
-1 	0.434999999999999998 	0.330000000000000016 	0.104999999999999996 	0.33500000000000002 	0.156 	0.0555000000000000007 	0.104999999999999996 	8 	
-0 	0.28999999999999998 	0.209999999999999992 	0.0749999999999999972 	0.275000000000000022 	0.113000000000000003 	0.0675000000000000044 	0.0350000000000000033 	6 	
-0 	0.569999999999999951 	0.434999999999999998 	0.149999999999999994 	0.829500000000000015 	0.387500000000000011 	0.156 	0.244999999999999996 	10 	
-2 	0.154999999999999999 	0.115000000000000005 	0.0250000000000000014 	0.0240000000000000005 	0.00899999999999999932 	0.0050000000000000001 	0.00749999999999999972 	5 	
-0 	0.46000000000000002 	0.354999999999999982 	0.130000000000000004 	0.458000000000000018 	0.192000000000000004 	0.105499999999999997 	0.130000000000000004 	13 	
-1 	0.165000000000000008 	0.110000000000000001 	0.0200000000000000004 	0.0189999999999999995 	0.0064999999999999997 	0.00250000000000000005 	0.0050000000000000001 	4 	
-0 	0.655000000000000027 	0.515000000000000013 	0.170000000000000012 	1.52699999999999991 	0.848500000000000032 	0.263500000000000012 	0.331000000000000016 	11 	
-0 	0.469999999999999973 	0.375 	0.119999999999999996 	0.601500000000000035 	0.276500000000000024 	0.14549999999999999 	0.135000000000000009 	8 	
-2 	0.630000000000000004 	0.489999999999999991 	0.170000000000000012 	1.1745000000000001 	0.525499999999999967 	0.27300000000000002 	0.339000000000000024 	11 	
-0 	0.67000000000000004 	0.530000000000000027 	0.225000000000000006 	1.56150000000000011 	0.630000000000000004 	0.486999999999999988 	0.372499999999999998 	11 	
-1 	0.330000000000000016 	0.260000000000000009 	0.0800000000000000017 	0.190000000000000002 	0.0764999999999999986 	0.0384999999999999995 	0.0650000000000000022 	7 	
-0 	0.640000000000000013 	0.520000000000000018 	0.174999999999999989 	1.248 	0.424499999999999988 	0.259500000000000008 	0.479999999999999982 	12 	
-1 	0.434999999999999998 	0.33500000000000002 	0.100000000000000006 	0.329500000000000015 	0.129000000000000004 	0.0700000000000000067 	0.110000000000000001 	7 	
-2 	0.5 	0.380000000000000004 	0.135000000000000009 	0.583500000000000019 	0.22950000000000001 	0.126500000000000001 	0.179999999999999993 	12 	
-1 	0.41499999999999998 	0.325000000000000011 	0.110000000000000001 	0.316000000000000003 	0.138500000000000012 	0.0795000000000000012 	0.0924999999999999989 	8 	
-2 	0.5 	0.380000000000000004 	0.119999999999999996 	0.576500000000000012 	0.27300000000000002 	0.135000000000000009 	0.14499999999999999 	9 	
-0 	0.445000000000000007 	0.33500000000000002 	0.140000000000000013 	0.456500000000000017 	0.178499999999999992 	0.114000000000000004 	0.140000000000000013 	11 	
-2 	0.635000000000000009 	0.510000000000000009 	0.184999999999999998 	1.30800000000000005 	0.544000000000000039 	0.318000000000000005 	0.377000000000000002 	8 	
-1 	0.465000000000000024 	0.344999999999999973 	0.110000000000000001 	0.441500000000000004 	0.175499999999999989 	0.0904999999999999971 	0.119999999999999996 	7 	
-2 	0.530000000000000027 	0.409999999999999976 	0.165000000000000008 	0.731999999999999984 	0.189000000000000001 	0.170000000000000012 	0.309999999999999998 	11 	
-0 	0.520000000000000018 	0.409999999999999976 	0.115000000000000005 	0.807000000000000051 	0.285499999999999976 	0.178999999999999992 	0.234999999999999987 	12 	
-1 	0.0749999999999999972 	0.0550000000000000003 	0.0100000000000000002 	0.00200000000000000004 	0.00100000000000000002 	0.00050000000000000001 	0.00150000000000000003 	1 	
-1 	0.225000000000000006 	0.170000000000000012 	0.0500000000000000028 	0.0514999999999999972 	0.0189999999999999995 	0.0120000000000000002 	0.0170000000000000012 	4 	
-0 	0.660000000000000031 	0.505000000000000004 	0.184999999999999998 	1.52800000000000002 	0.689999999999999947 	0.302499999999999991 	0.441000000000000003 	11 	
-1 	0.320000000000000007 	0.204999999999999988 	0.0800000000000000017 	0.180999999999999994 	0.0879999999999999949 	0.0340000000000000024 	0.0495000000000000023 	5 	
-2 	0.369999999999999996 	0.284999999999999976 	0.100000000000000006 	0.228000000000000008 	0.0675000000000000044 	0.0675000000000000044 	0.0810000000000000026 	10 	
-0 	0.640000000000000013 	0.510000000000000009 	0.200000000000000011 	1.39050000000000007 	0.609999999999999987 	0.331500000000000017 	0.409999999999999976 	12 	
-2 	0.344999999999999973 	0.255000000000000004 	0.0899999999999999967 	0.200500000000000012 	0.0940000000000000002 	0.0294999999999999984 	0.0630000000000000004 	9 	
-1 	0.330000000000000016 	0.25 	0.0950000000000000011 	0.208499999999999991 	0.101999999999999993 	0.0395000000000000004 	0.0519999999999999976 	7 	
-2 	0.484999999999999987 	0.364999999999999991 	0.154999999999999999 	1.02899999999999991 	0.423499999999999988 	0.228500000000000009 	0.313 	8 	
-0 	0.780000000000000027 	0.630000000000000004 	0.214999999999999997 	2.65700000000000003 	1.48799999999999999 	0.498499999999999999 	0.585999999999999965 	11 	
-0 	0.54500000000000004 	0.429999999999999993 	0.140000000000000013 	0.831999999999999962 	0.435499999999999998 	0.170000000000000012 	0.201000000000000012 	9 	
-0 	0.369999999999999996 	0.284999999999999976 	0.104999999999999996 	0.270000000000000018 	0.112500000000000003 	0.0585000000000000034 	0.0835000000000000048 	9 	
-1 	0.54500000000000004 	0.429999999999999993 	0.149999999999999994 	0.741999999999999993 	0.35249999999999998 	0.158000000000000002 	0.20799999999999999 	10 	
-0 	0.699999999999999956 	0.550000000000000044 	0.170000000000000012 	1.68399999999999994 	0.753499999999999948 	0.326500000000000012 	0.320000000000000007 	11 	
-1 	0.294999999999999984 	0.23000000000000001 	0.0800000000000000017 	0.162500000000000006 	0.0650000000000000022 	0.0500000000000000028 	0.0384999999999999995 	5 	
-2 	0.535000000000000031 	0.405000000000000027 	0.174999999999999989 	1.27049999999999996 	0.548000000000000043 	0.326500000000000012 	0.337000000000000022 	13 	
-0 	0.41499999999999998 	0.340000000000000024 	0.130000000000000004 	0.367499999999999993 	0.145999999999999991 	0.0884999999999999953 	0.119999999999999996 	10 	
-1 	0.450000000000000011 	0.354999999999999982 	0.104999999999999996 	0.444500000000000006 	0.197000000000000008 	0.0929999999999999993 	0.133500000000000008 	8 	
-2 	0.685000000000000053 	0.510000000000000009 	0.165000000000000008 	1.54499999999999993 	0.686000000000000054 	0.377500000000000002 	0.405500000000000027 	10 	
-0 	0.739999999999999991 	0.599999999999999978 	0.195000000000000007 	1.97399999999999998 	0.597999999999999976 	0.408499999999999974 	0.709999999999999964 	16 	
-0 	0.614999999999999991 	0.479999999999999982 	0.160000000000000003 	1.25249999999999995 	0.584999999999999964 	0.259500000000000008 	0.330000000000000016 	8 	
-0 	0.574999999999999956 	0.424999999999999989 	0.149999999999999994 	0.876499999999999946 	0.455000000000000016 	0.179999999999999993 	0.228000000000000008 	8 	
-0 	0.614999999999999991 	0.515000000000000013 	0.170000000000000012 	1.1399999999999999 	0.430499999999999994 	0.224500000000000005 	0.419999999999999984 	16 	
-2 	0.489999999999999991 	0.390000000000000013 	0.140000000000000013 	0.706999999999999962 	0.279500000000000026 	0.2185 	0.179999999999999993 	13 	
-1 	0.434999999999999998 	0.330000000000000016 	0.110000000000000001 	0.412999999999999978 	0.205499999999999988 	0.096000000000000002 	0.096000000000000002 	6 	
-2 	0.660000000000000031 	0.505000000000000004 	0.165000000000000008 	1.37400000000000011 	0.588999999999999968 	0.350999999999999979 	0.344999999999999973 	10 	
-2 	0.440000000000000002 	0.325000000000000011 	0.115000000000000005 	0.390000000000000013 	0.163000000000000006 	0.086999999999999994 	0.113000000000000003 	7 	
-2 	0.75 	0.550000000000000044 	0.179999999999999993 	1.89300000000000002 	0.941999999999999948 	0.39700000000000002 	0.445000000000000007 	11 	
-2 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	0.919000000000000039 	0.433499999999999996 	0.17649999999999999 	0.262000000000000011 	9 	
-1 	0.525000000000000022 	0.400000000000000022 	0.140000000000000013 	0.600999999999999979 	0.262500000000000011 	0.128500000000000003 	0.183499999999999996 	9 	
-0 	0.650000000000000022 	0.489999999999999991 	0.154999999999999999 	1.12200000000000011 	0.54500000000000004 	0.228000000000000008 	0.305499999999999994 	9 	
-2 	0.584999999999999964 	0.474999999999999978 	0.149999999999999994 	1.06499999999999995 	0.531499999999999972 	0.19900000000000001 	0.288499999999999979 	10 	
-1 	0.564999999999999947 	0.440000000000000002 	0.174999999999999989 	0.873500000000000054 	0.413999999999999979 	0.209999999999999992 	0.209999999999999992 	11 	
-2 	0.574999999999999956 	0.474999999999999978 	0.14499999999999999 	0.856999999999999984 	0.366499999999999992 	0.172999999999999987 	0.269000000000000017 	9 	
-0 	0.530000000000000027 	0.424999999999999989 	0.170000000000000012 	0.948999999999999955 	0.348499999999999976 	0.239499999999999991 	0.278000000000000025 	17 	
-0 	0.609999999999999987 	0.469999999999999973 	0.14499999999999999 	1.15300000000000002 	0.403000000000000025 	0.295999999999999985 	0.320000000000000007 	14 	
-1 	0.364999999999999991 	0.255000000000000004 	0.0800000000000000017 	0.19850000000000001 	0.0785000000000000003 	0.0345000000000000029 	0.0529999999999999985 	5 	
-0 	0.609999999999999987 	0.46000000000000002 	0.154999999999999999 	0.956999999999999962 	0.425499999999999989 	0.197500000000000009 	0.265000000000000013 	8 	
-0 	0.455000000000000016 	0.349999999999999978 	0.140000000000000013 	0.518499999999999961 	0.221000000000000002 	0.126500000000000001 	0.135000000000000009 	10 	
-0 	0.505000000000000004 	0.400000000000000022 	0.125 	0.582999999999999963 	0.245999999999999996 	0.130000000000000004 	0.174999999999999989 	7 	
-1 	0.434999999999999998 	0.33500000000000002 	0.110000000000000001 	0.383000000000000007 	0.155499999999999999 	0.0675000000000000044 	0.135000000000000009 	12 	
-2 	0.625 	0.5 	0.195000000000000007 	1.36899999999999999 	0.587500000000000022 	0.2185 	0.369999999999999996 	17 	
-2 	0.450000000000000011 	0.33500000000000002 	0.140000000000000013 	0.462500000000000022 	0.164000000000000007 	0.0759999999999999981 	0.149999999999999994 	14 	
-2 	0.660000000000000031 	0.510000000000000009 	0.174999999999999989 	1.21799999999999997 	0.505499999999999949 	0.302999999999999992 	0.369999999999999996 	11 	
-1 	0.484999999999999987 	0.364999999999999991 	0.104999999999999996 	0.520499999999999963 	0.195000000000000007 	0.122999999999999998 	0.181999999999999995 	8 	
-2 	0.665000000000000036 	0.515000000000000013 	0.190000000000000002 	1.63850000000000007 	0.830999999999999961 	0.357499999999999984 	0.370999999999999996 	11 	
-1 	0.440000000000000002 	0.33500000000000002 	0.115000000000000005 	0.421499999999999986 	0.172999999999999987 	0.0764999999999999986 	0.113000000000000003 	7 	
-0 	0.57999999999999996 	0.46000000000000002 	0.174999999999999989 	1.16500000000000004 	0.650000000000000022 	0.220500000000000002 	0.305499999999999994 	9 	
-2 	0.280000000000000027 	0.204999999999999988 	0.100000000000000006 	0.116500000000000006 	0.0544999999999999998 	0.028500000000000001 	0.0299999999999999989 	5 	
-1 	0.515000000000000013 	0.380000000000000004 	0.119999999999999996 	0.625 	0.326500000000000012 	0.129500000000000004 	0.160000000000000003 	7 	
-1 	0.450000000000000011 	0.330000000000000016 	0.104999999999999996 	0.448000000000000009 	0.20799999999999999 	0.0889999999999999958 	0.119999999999999996 	9 	
-2 	0.734999999999999987 	0.584999999999999964 	0.184999999999999998 	2.12400000000000011 	0.951999999999999957 	0.550000000000000044 	0.5 	11 	
-0 	0.54500000000000004 	0.385000000000000009 	0.149999999999999994 	1.11850000000000005 	0.542499999999999982 	0.244499999999999995 	0.284499999999999975 	9 	
-1 	0.299999999999999989 	0.220000000000000001 	0.0800000000000000017 	0.1255 	0.0550000000000000003 	0.0264999999999999993 	0.0389999999999999999 	6 	
-2 	0.505000000000000004 	0.440000000000000002 	0.140000000000000013 	0.827500000000000013 	0.341500000000000026 	0.185499999999999998 	0.23899999999999999 	8 	
-1 	0.41499999999999998 	0.325000000000000011 	0.100000000000000006 	0.385000000000000009 	0.16700000000000001 	0.0800000000000000017 	0.125 	7 	
-0 	0.604999999999999982 	0.494999999999999996 	0.190000000000000002 	1.43700000000000006 	0.468999999999999972 	0.265500000000000014 	0.409999999999999976 	15 	
-0 	0.525000000000000022 	0.405000000000000027 	0.160000000000000003 	0.658000000000000029 	0.265500000000000014 	0.112500000000000003 	0.225000000000000006 	12 	
-2 	0.744999999999999996 	0.584999999999999964 	0.214999999999999997 	2.49900000000000011 	0.92649999999999999 	0.471999999999999975 	0.699999999999999956 	17 	
-1 	0.474999999999999978 	0.364999999999999991 	0.119999999999999996 	0.530000000000000027 	0.2505 	0.0975000000000000033 	0.162500000000000006 	10 	
-2 	0.694999999999999951 	0.54500000000000004 	0.184999999999999998 	1.5714999999999999 	0.66449999999999998 	0.383500000000000008 	0.450500000000000012 	13 	
-2 	0.57999999999999996 	0.455000000000000016 	0.195000000000000007 	1.85899999999999999 	0.944999999999999951 	0.42599999999999999 	0.441000000000000003 	9 	
-2 	0.564999999999999947 	0.469999999999999973 	0.195000000000000007 	1.1419999999999999 	0.387000000000000011 	0.258000000000000007 	0.349999999999999978 	17 	
-1 	0.280000000000000027 	0.209999999999999992 	0.0850000000000000061 	0.107499999999999998 	0.0415000000000000022 	0.0240000000000000005 	0.0340000000000000024 	5 	
-2 	0.540000000000000036 	0.41499999999999998 	0.130000000000000004 	0.824500000000000011 	0.27200000000000002 	0.226000000000000006 	0.239999999999999991 	13 	
-2 	0.685000000000000053 	0.520000000000000018 	0.149999999999999994 	1.37349999999999994 	0.718500000000000028 	0.292999999999999983 	0.320000000000000007 	11 	
-2 	0.375 	0.304999999999999993 	0.0899999999999999967 	0.324500000000000011 	0.139500000000000013 	0.0565000000000000016 	0.0950000000000000011 	5 	
-2 	0.67000000000000004 	0.540000000000000036 	0.174999999999999989 	1.48199999999999998 	0.73899999999999999 	0.292499999999999982 	0.364999999999999991 	10 	
-0 	0.645000000000000018 	0.505000000000000004 	0.165000000000000008 	1.31800000000000006 	0.550000000000000044 	0.30149999999999999 	0.33500000000000002 	11 	
-2 	0.655000000000000027 	0.520000000000000018 	0.170000000000000012 	1.14450000000000007 	0.530000000000000027 	0.223000000000000004 	0.347999999999999976 	9 	
-1 	0.515000000000000013 	0.359999999999999987 	0.125 	0.472499999999999976 	0.181499999999999995 	0.125 	0.138000000000000012 	9 	
-0 	0.635000000000000009 	0.489999999999999991 	0.154999999999999999 	1.14500000000000002 	0.47749999999999998 	0.303499999999999992 	0.315500000000000003 	9 	
-2 	0.57999999999999996 	0.465000000000000024 	0.160000000000000003 	1.03449999999999998 	0.315000000000000002 	0.260000000000000009 	0.36349999999999999 	12 	
-2 	0.419999999999999984 	0.340000000000000024 	0.125 	0.449500000000000011 	0.165000000000000008 	0.112500000000000003 	0.143999999999999989 	11 	
-1 	0.46000000000000002 	0.354999999999999982 	0.110000000000000001 	0.425499999999999989 	0.201500000000000012 	0.0810000000000000026 	0.130000000000000004 	7 	
-1 	0.149999999999999994 	0.100000000000000006 	0.0250000000000000014 	0.0149999999999999994 	0.00449999999999999966 	0.00400000000000000008 	0.0050000000000000001 	2 	
-0 	0.469999999999999973 	0.359999999999999987 	0.14499999999999999 	0.537000000000000033 	0.172499999999999987 	0.137500000000000011 	0.195000000000000007 	15 	
-0 	0.630000000000000004 	0.494999999999999996 	0.14499999999999999 	1.14700000000000002 	0.545499999999999985 	0.266000000000000014 	0.288499999999999979 	9 	
-0 	0.469999999999999973 	0.375 	0.125 	0.561499999999999999 	0.252000000000000002 	0.137000000000000011 	0.179999999999999993 	10 	
-2 	0.574999999999999956 	0.400000000000000022 	0.154999999999999999 	0.932499999999999996 	0.360499999999999987 	0.244499999999999995 	0.299999999999999989 	17 	
-0 	0.599999999999999978 	0.450000000000000011 	0.140000000000000013 	0.836999999999999966 	0.369999999999999996 	0.176999999999999991 	0.242499999999999993 	10 	
-2 	0.630000000000000004 	0.5 	0.184999999999999998 	1.3620000000000001 	0.578500000000000014 	0.3125 	0.384000000000000008 	10 	
-0 	0.530000000000000027 	0.405000000000000027 	0.149999999999999994 	0.889000000000000012 	0.405500000000000027 	0.227500000000000008 	0.214999999999999997 	8 	
-0 	0.469999999999999973 	0.359999999999999987 	0.130000000000000004 	0.471999999999999975 	0.181999999999999995 	0.114000000000000004 	0.149999999999999994 	10 	
-2 	0.5 	0.419999999999999984 	0.135000000000000009 	0.67649999999999999 	0.301999999999999991 	0.141499999999999987 	0.206499999999999989 	9 	
-2 	0.424999999999999989 	0.325000000000000011 	0.100000000000000006 	0.329500000000000015 	0.13650000000000001 	0.072499999999999995 	0.110000000000000001 	7 	
-2 	0.5 	0.400000000000000022 	0.130000000000000004 	0.66449999999999998 	0.258000000000000007 	0.133000000000000007 	0.239999999999999991 	12 	
-2 	0.650000000000000022 	0.515000000000000013 	0.174999999999999989 	1.48049999999999993 	0.529499999999999971 	0.27200000000000002 	0.525000000000000022 	20 	
-2 	0.694999999999999951 	0.540000000000000036 	0.195000000000000007 	1.69100000000000006 	0.768000000000000016 	0.362999999999999989 	0.475499999999999978 	11 	
-1 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.824500000000000011 	0.337500000000000022 	0.211499999999999994 	0.23899999999999999 	11 	
-2 	0.555000000000000049 	0.424999999999999989 	0.149999999999999994 	0.872999999999999998 	0.462500000000000022 	0.184499999999999997 	0.196500000000000008 	9 	
-2 	0.569999999999999951 	0.440000000000000002 	0.154999999999999999 	1.1160000000000001 	0.47749999999999998 	0.231500000000000011 	0.270000000000000018 	13 	
-0 	0.479999999999999982 	0.375 	0.104999999999999996 	0.525000000000000022 	0.2185 	0.119499999999999995 	0.154999999999999999 	12 	
-0 	0.655000000000000027 	0.520000000000000018 	0.200000000000000011 	1.5475000000000001 	0.712999999999999967 	0.314000000000000001 	0.466000000000000025 	9 	
-0 	0.550000000000000044 	0.424999999999999989 	0.140000000000000013 	0.951999999999999957 	0.489499999999999991 	0.194500000000000006 	0.2185 	7 	
-2 	0.555000000000000049 	0.405000000000000027 	0.190000000000000002 	1.40599999999999992 	0.611500000000000044 	0.342000000000000026 	0.389000000000000012 	10 	
-2 	0.569999999999999951 	0.434999999999999998 	0.14499999999999999 	0.905499999999999972 	0.392500000000000016 	0.235499999999999987 	0.275000000000000022 	10 	
-1 	0.505000000000000004 	0.380000000000000004 	0.135000000000000009 	0.538499999999999979 	0.264500000000000013 	0.0950000000000000011 	0.165000000000000008 	9 	
-0 	0.569999999999999951 	0.450000000000000011 	0.135000000000000009 	0.780499999999999972 	0.33450000000000002 	0.184999999999999998 	0.209999999999999992 	8 	
-2 	0.564999999999999947 	0.455000000000000016 	0.149999999999999994 	0.95950000000000002 	0.456500000000000017 	0.239499999999999991 	0.23000000000000001 	9 	
-2 	0.619999999999999996 	0.515000000000000013 	0.174999999999999989 	1.22100000000000009 	0.535000000000000031 	0.240999999999999992 	0.395000000000000018 	13 	
-0 	0.474999999999999978 	0.380000000000000004 	0.135000000000000009 	0.485999999999999988 	0.173499999999999988 	0.0700000000000000067 	0.184999999999999998 	7 	
-0 	0.574999999999999956 	0.46000000000000002 	0.160000000000000003 	1.10299999999999998 	0.538000000000000034 	0.221000000000000002 	0.248999999999999999 	9 	
-0 	0.550000000000000044 	0.429999999999999993 	0.154999999999999999 	0.785000000000000031 	0.288999999999999979 	0.227000000000000007 	0.233000000000000013 	11 	
-0 	0.650000000000000022 	0.5 	0.190000000000000002 	1.46399999999999997 	0.641499999999999959 	0.339000000000000024 	0.424499999999999988 	9 	
-0 	0.719999999999999973 	0.57999999999999996 	0.195000000000000007 	2.1030000000000002 	1.02649999999999997 	0.479999999999999982 	0.537499999999999978 	10 	
-0 	0.450000000000000011 	0.359999999999999987 	0.104999999999999996 	0.471499999999999975 	0.203499999999999986 	0.0934999999999999998 	0.148999999999999994 	9 	
-2 	0.564999999999999947 	0.424999999999999989 	0.135000000000000009 	0.811499999999999999 	0.341000000000000025 	0.16750000000000001 	0.255000000000000004 	15 	
-0 	0.510000000000000009 	0.390000000000000013 	0.135000000000000009 	0.633499999999999952 	0.231000000000000011 	0.178999999999999992 	0.200000000000000011 	9 	
-2 	0.599999999999999978 	0.474999999999999978 	0.174999999999999989 	1.1100000000000001 	0.510499999999999954 	0.256000000000000005 	0.284999999999999976 	9 	
-2 	0.564999999999999947 	0.450000000000000011 	0.160000000000000003 	0.895000000000000018 	0.41499999999999998 	0.195000000000000007 	0.245999999999999996 	9 	
-2 	0.655000000000000027 	0.530000000000000027 	0.174999999999999989 	1.26350000000000007 	0.485999999999999988 	0.263500000000000012 	0.41499999999999998 	15 	
-0 	0.380000000000000004 	0.325000000000000011 	0.110000000000000001 	0.310499999999999998 	0.119999999999999996 	0.0739999999999999963 	0.104999999999999996 	10 	
-0 	0.574999999999999956 	0.465000000000000024 	0.140000000000000013 	0.957999999999999963 	0.442000000000000004 	0.181499999999999995 	0.270500000000000018 	9 	
-1 	0.434999999999999998 	0.344999999999999973 	0.119999999999999996 	0.321500000000000008 	0.130000000000000004 	0.0560000000000000012 	0.118499999999999994 	7 	
-1 	0.479999999999999982 	0.349999999999999978 	0.135000000000000009 	0.546499999999999986 	0.273500000000000021 	0.0995000000000000051 	0.158000000000000002 	8 	
-1 	0.299999999999999989 	0.220000000000000001 	0.0650000000000000022 	0.123499999999999999 	0.0589999999999999969 	0.0259999999999999988 	0.0315000000000000002 	5 	
-0 	0.689999999999999947 	0.54500000000000004 	0.204999999999999988 	1.93300000000000005 	0.785499999999999976 	0.428999999999999992 	0.497999999999999998 	13 	
-1 	0.550000000000000044 	0.419999999999999984 	0.154999999999999999 	0.912000000000000033 	0.494999999999999996 	0.180499999999999994 	0.204999999999999988 	9 	
-1 	0.359999999999999987 	0.270000000000000018 	0.0850000000000000061 	0.2185 	0.106499999999999997 	0.0379999999999999991 	0.0619999999999999996 	6 	
-1 	0.309999999999999998 	0.234999999999999987 	0.0700000000000000067 	0.150999999999999995 	0.0630000000000000004 	0.0405000000000000013 	0.0449999999999999983 	6 	
-1 	0.609999999999999987 	0.474999999999999978 	0.149999999999999994 	0.966500000000000026 	0.41449999999999998 	0.200000000000000011 	0.344999999999999973 	10 	
-2 	0.555000000000000049 	0.450000000000000011 	0.14499999999999999 	0.915000000000000036 	0.400000000000000022 	0.245999999999999996 	0.284999999999999976 	11 	
-0 	0.594999999999999973 	0.465000000000000024 	0.149999999999999994 	1.02550000000000008 	0.411999999999999977 	0.274500000000000022 	0.288999999999999979 	11 	
-2 	0.640000000000000013 	0.574999999999999956 	0.174999999999999989 	1.45849999999999991 	0.625 	0.266000000000000014 	0.439500000000000002 	11 	
-2 	0.589999999999999969 	0.440000000000000002 	0.149999999999999994 	0.872500000000000053 	0.387000000000000011 	0.214999999999999997 	0.244999999999999996 	8 	
-0 	0.445000000000000007 	0.330000000000000016 	0.104999999999999996 	0.452500000000000013 	0.179999999999999993 	0.102999999999999994 	0.122999999999999998 	9 	
-2 	0.609999999999999987 	0.5 	0.239999999999999991 	1.6419999999999999 	0.532000000000000028 	0.33450000000000002 	0.689999999999999947 	18 	
-2 	0.645000000000000018 	0.515000000000000013 	0.160000000000000003 	1.18450000000000011 	0.506000000000000005 	0.310999999999999999 	0.33500000000000002 	9 	
-2 	0.515000000000000013 	0.419999999999999984 	0.140000000000000013 	0.769000000000000017 	0.2505 	0.153999999999999998 	0.28999999999999998 	13 	
-1 	0.419999999999999984 	0.320000000000000007 	0.100000000000000006 	0.340000000000000024 	0.174499999999999988 	0.0500000000000000028 	0.0945000000000000007 	8 	
-0 	0.505000000000000004 	0.409999999999999976 	0.149999999999999994 	0.644000000000000017 	0.284999999999999976 	0.14499999999999999 	0.209999999999999992 	11 	
-1 	0.225000000000000006 	0.160000000000000003 	0.0449999999999999983 	0.0464999999999999997 	0.0250000000000000014 	0.0149999999999999994 	0.0149999999999999994 	4 	
-2 	0.650000000000000022 	0.510000000000000009 	0.174999999999999989 	1.44599999999999995 	0.648499999999999965 	0.270500000000000018 	0.450000000000000011 	12 	
-0 	0.630000000000000004 	0.5 	0.170000000000000012 	1.31349999999999989 	0.559499999999999997 	0.267000000000000015 	0.400000000000000022 	20 	
-0 	0.574999999999999956 	0.469999999999999973 	0.165000000000000008 	0.868999999999999995 	0.434999999999999998 	0.197000000000000008 	0.237999999999999989 	9 	
-2 	0.540000000000000036 	0.419999999999999984 	0.135000000000000009 	0.807499999999999996 	0.348499999999999976 	0.179499999999999993 	0.234999999999999987 	11 	
-2 	0.489999999999999991 	0.380000000000000004 	0.140000000000000013 	0.760499999999999954 	0.244999999999999996 	0.16700000000000001 	0.184999999999999998 	10 	
-1 	0.505000000000000004 	0.400000000000000022 	0.125 	0.560499999999999998 	0.225500000000000006 	0.143499999999999989 	0.170000000000000012 	8 	
-2 	0.530000000000000027 	0.41499999999999998 	0.119999999999999996 	0.705999999999999961 	0.33550000000000002 	0.163500000000000006 	0.134500000000000008 	9 	
-2 	0.599999999999999978 	0.465000000000000024 	0.165000000000000008 	1.03800000000000003 	0.497499999999999998 	0.220500000000000002 	0.251000000000000001 	9 	
-2 	0.599999999999999978 	0.5 	0.160000000000000003 	1.0149999999999999 	0.399500000000000022 	0.173499999999999988 	0.330000000000000016 	19 	
-0 	0.709999999999999964 	0.560000000000000053 	0.179999999999999993 	1.65199999999999991 	0.734999999999999987 	0.381000000000000005 	0.452500000000000013 	11 	
-2 	0.719999999999999973 	0.564999999999999947 	0.14499999999999999 	1.18700000000000006 	0.690999999999999948 	0.194500000000000006 	0.268500000000000016 	8 	
-0 	0.489999999999999991 	0.369999999999999996 	0.140000000000000013 	0.584999999999999964 	0.242999999999999994 	0.115000000000000005 	0.195000000000000007 	10 	
-1 	0.440000000000000002 	0.330000000000000016 	0.110000000000000001 	0.370499999999999996 	0.154499999999999998 	0.0840000000000000052 	0.119999999999999996 	7 	
-0 	0.369999999999999996 	0.275000000000000022 	0.0850000000000000061 	0.240499999999999992 	0.103999999999999995 	0.0534999999999999989 	0.0700000000000000067 	5 	
-0 	0.54500000000000004 	0.440000000000000002 	0.149999999999999994 	0.947500000000000009 	0.365999999999999992 	0.23899999999999999 	0.275000000000000022 	8 	
-1 	0.275000000000000022 	0.204999999999999988 	0.0749999999999999972 	0.110500000000000001 	0.0449999999999999983 	0.028500000000000001 	0.0350000000000000033 	6 	
-0 	0.479999999999999982 	0.380000000000000004 	0.119999999999999996 	0.607999999999999985 	0.270500000000000018 	0.140500000000000014 	0.184999999999999998 	8 	
-1 	0.409999999999999976 	0.325000000000000011 	0.100000000000000006 	0.394000000000000017 	0.20799999999999999 	0.0655000000000000027 	0.105999999999999997 	6 	
-1 	0.5 	0.390000000000000013 	0.130000000000000004 	0.507499999999999951 	0.211499999999999994 	0.103999999999999995 	0.175499999999999989 	9 	
-2 	0.530000000000000027 	0.424999999999999989 	0.130000000000000004 	0.745500000000000052 	0.299499999999999988 	0.135500000000000009 	0.244999999999999996 	10 	
-0 	0.719999999999999973 	0.594999999999999973 	0.225000000000000006 	1.96900000000000008 	0.804499999999999993 	0.422999999999999987 	0.660000000000000031 	16 	
-0 	0.555000000000000049 	0.445000000000000007 	0.174999999999999989 	1.14650000000000007 	0.551000000000000045 	0.243999999999999995 	0.278500000000000025 	8 	
-1 	0.375 	0.284999999999999976 	0.0899999999999999967 	0.236999999999999988 	0.105999999999999997 	0.0395000000000000004 	0.0800000000000000017 	8 	
-1 	0.564999999999999947 	0.434999999999999998 	0.14499999999999999 	0.844500000000000028 	0.39750000000000002 	0.158000000000000002 	0.255000000000000004 	9 	
-2 	0.465000000000000024 	0.349999999999999978 	0.140000000000000013 	0.575500000000000012 	0.201500000000000012 	0.150499999999999995 	0.190000000000000002 	15 	
-1 	0.424999999999999989 	0.309999999999999998 	0.0950000000000000011 	0.307499999999999996 	0.139000000000000012 	0.0744999999999999968 	0.0929999999999999993 	7 	
-1 	0.46000000000000002 	0.369999999999999996 	0.119999999999999996 	0.533499999999999974 	0.264500000000000013 	0.107999999999999999 	0.134500000000000008 	6 	
-1 	0.494999999999999996 	0.375 	0.140000000000000013 	0.493999999999999995 	0.180999999999999994 	0.0975000000000000033 	0.191000000000000003 	8 	
-0 	0.5 	0.395000000000000018 	0.140000000000000013 	0.715500000000000025 	0.316500000000000004 	0.17599999999999999 	0.239999999999999991 	10 	
-0 	0.645000000000000018 	0.520000000000000018 	0.179999999999999993 	1.28499999999999992 	0.577500000000000013 	0.35199999999999998 	0.317000000000000004 	9 	
-1 	0.494999999999999996 	0.375 	0.115000000000000005 	0.507000000000000006 	0.240999999999999992 	0.102999999999999994 	0.149999999999999994 	8 	
-0 	0.584999999999999964 	0.450000000000000011 	0.160000000000000003 	0.904499999999999971 	0.405000000000000027 	0.221500000000000002 	0.233500000000000013 	8 	
-2 	0.5 	0.364999999999999991 	0.130000000000000004 	0.594500000000000028 	0.308999999999999997 	0.108499999999999999 	0.153499999999999998 	9 	
-1 	0.375 	0.275000000000000022 	0.0899999999999999967 	0.237999999999999989 	0.107499999999999998 	0.0544999999999999998 	0.0700000000000000067 	6 	
-1 	0.190000000000000002 	0.130000000000000004 	0.0299999999999999989 	0.0294999999999999984 	0.0154999999999999999 	0.0149999999999999994 	0.0100000000000000002 	6 	
-0 	0.54500000000000004 	0.440000000000000002 	0.174999999999999989 	0.774499999999999966 	0.298499999999999988 	0.1875 	0.265000000000000013 	11 	
-1 	0.359999999999999987 	0.270000000000000018 	0.0850000000000000061 	0.196000000000000008 	0.0874999999999999944 	0.0350000000000000033 	0.0640000000000000013 	4 	
-2 	0.619999999999999996 	0.479999999999999982 	0.165000000000000008 	1.07250000000000001 	0.481499999999999984 	0.234999999999999987 	0.312 	9 	
-0 	0.564999999999999947 	0.445000000000000007 	0.154999999999999999 	0.825999999999999956 	0.341000000000000025 	0.205499999999999988 	0.247499999999999998 	10 	
-1 	0.525000000000000022 	0.400000000000000022 	0.110000000000000001 	0.627499999999999947 	0.30149999999999999 	0.126000000000000001 	0.179999999999999993 	8 	
-1 	0.385000000000000009 	0.280000000000000027 	0.100000000000000006 	0.275500000000000023 	0.130500000000000005 	0.0609999999999999987 	0.072499999999999995 	8 	
-1 	0.434999999999999998 	0.330000000000000016 	0.110000000000000001 	0.380000000000000004 	0.151499999999999996 	0.0945000000000000007 	0.110000000000000001 	7 	
-0 	0.635000000000000009 	0.505000000000000004 	0.170000000000000012 	1.41500000000000004 	0.604999999999999982 	0.296999999999999986 	0.364999999999999991 	15 	
-0 	0.569999999999999951 	0.469999999999999973 	0.140000000000000013 	0.870999999999999996 	0.385000000000000009 	0.210999999999999993 	0.231500000000000011 	10 	
-0 	0.635000000000000009 	0.5 	0.149999999999999994 	1.37599999999999989 	0.649499999999999966 	0.360999999999999988 	0.309999999999999998 	10 	
-0 	0.719999999999999973 	0.574999999999999956 	0.179999999999999993 	1.6705000000000001 	0.731999999999999984 	0.360499999999999987 	0.501000000000000001 	12 	
-0 	0.619999999999999996 	0.540000000000000036 	0.165000000000000008 	1.13900000000000001 	0.4995 	0.243499999999999994 	0.356999999999999984 	11 	
-0 	0.589999999999999969 	0.46000000000000002 	0.160000000000000003 	1.01150000000000007 	0.445000000000000007 	0.26150000000000001 	0.256500000000000006 	8 	
-2 	0.564999999999999947 	0.429999999999999993 	0.149999999999999994 	0.830999999999999961 	0.424499999999999988 	0.173499999999999988 	0.219 	10 	
-1 	0.440000000000000002 	0.340000000000000024 	0.119999999999999996 	0.4995 	0.296499999999999986 	0.0945000000000000007 	0.118499999999999994 	6 	
-1 	0.41499999999999998 	0.315000000000000002 	0.100000000000000006 	0.364499999999999991 	0.17649999999999999 	0.0795000000000000012 	0.0950000000000000011 	8 	
-2 	0.525000000000000022 	0.364999999999999991 	0.170000000000000012 	0.96050000000000002 	0.438 	0.222500000000000003 	0.276000000000000023 	10 	
-0 	0.599999999999999978 	0.465000000000000024 	0.160000000000000003 	1.13300000000000001 	0.466000000000000025 	0.288499999999999979 	0.297999999999999987 	11 	
-2 	0.46000000000000002 	0.354999999999999982 	0.140000000000000013 	0.490999999999999992 	0.20699999999999999 	0.115000000000000005 	0.173999999999999988 	10 	
-2 	0.209999999999999992 	0.149999999999999994 	0.0500000000000000028 	0.0384999999999999995 	0.0154999999999999999 	0.00850000000000000061 	0.0100000000000000002 	3 	
-1 	0.510000000000000009 	0.400000000000000022 	0.14499999999999999 	0.577500000000000013 	0.231000000000000011 	0.142999999999999988 	0.176999999999999991 	9 	
-2 	0.660000000000000031 	0.5 	0.165000000000000008 	1.3194999999999999 	0.667000000000000037 	0.269000000000000017 	0.341000000000000025 	9 	
-2 	0.569999999999999951 	0.455000000000000016 	0.154999999999999999 	0.831999999999999962 	0.358499999999999985 	0.173999999999999988 	0.277000000000000024 	11 	
-0 	0.645000000000000018 	0.489999999999999991 	0.160000000000000003 	1.14399999999999991 	0.501499999999999946 	0.288999999999999979 	0.319000000000000006 	8 	
-2 	0.67000000000000004 	0.510000000000000009 	0.179999999999999993 	1.67999999999999994 	0.926000000000000045 	0.297499999999999987 	0.393500000000000016 	13 	
-1 	0.540000000000000036 	0.395000000000000018 	0.135000000000000009 	0.655499999999999972 	0.270500000000000018 	0.154999999999999999 	0.192000000000000004 	9 	
-0 	0.540000000000000036 	0.419999999999999984 	0.14499999999999999 	0.865500000000000047 	0.431499999999999995 	0.163000000000000006 	0.217499999999999999 	10 	
-0 	0.625 	0.489999999999999991 	0.200000000000000011 	1.38250000000000006 	0.589500000000000024 	0.284999999999999976 	0.381000000000000005 	11 	
-2 	0.680000000000000049 	0.515000000000000013 	0.160000000000000003 	1.23449999999999993 	0.617999999999999994 	0.262500000000000011 	0.325000000000000011 	11 	
-0 	0.609999999999999987 	0.484999999999999987 	0.149999999999999994 	1.24049999999999994 	0.602500000000000036 	0.291499999999999981 	0.308499999999999996 	12 	
-2 	0.650000000000000022 	0.520000000000000018 	0.154999999999999999 	1.3680000000000001 	0.61850000000000005 	0.287999999999999978 	0.364999999999999991 	9 	
-1 	0.25 	0.190000000000000002 	0.0650000000000000022 	0.0835000000000000048 	0.0389999999999999999 	0.0149999999999999994 	0.0250000000000000014 	5 	
-1 	0.535000000000000031 	0.400000000000000022 	0.135000000000000009 	0.775000000000000022 	0.367999999999999994 	0.20799999999999999 	0.205499999999999988 	8 	
-1 	0.354999999999999982 	0.255000000000000004 	0.0800000000000000017 	0.187 	0.0779999999999999999 	0.0505000000000000032 	0.0580000000000000029 	7 	
-0 	0.440000000000000002 	0.340000000000000024 	0.130000000000000004 	0.419499999999999984 	0.152999999999999997 	0.115500000000000005 	0.130000000000000004 	10 	
-0 	0.41499999999999998 	0.304999999999999993 	0.130000000000000004 	0.320000000000000007 	0.130500000000000005 	0.0754999999999999977 	0.104999999999999996 	8 	
-0 	0.635000000000000009 	0.505000000000000004 	0.165000000000000008 	1.25099999999999989 	0.576999999999999957 	0.227000000000000007 	0.382500000000000007 	11 	
-1 	0.275000000000000022 	0.200000000000000011 	0.0650000000000000022 	0.0919999999999999984 	0.0384999999999999995 	0.0235000000000000001 	0.0269999999999999997 	5 	
-1 	0.320000000000000007 	0.239999999999999991 	0.0749999999999999972 	0.173499999999999988 	0.0759999999999999981 	0.0354999999999999968 	0.0500000000000000028 	7 	
-1 	0.479999999999999982 	0.354999999999999982 	0.115000000000000005 	0.472499999999999976 	0.206499999999999989 	0.112000000000000002 	0.132000000000000006 	8 	
-1 	0.359999999999999987 	0.275000000000000022 	0.110000000000000001 	0.233500000000000013 	0.0950000000000000011 	0.0524999999999999981 	0.0850000000000000061 	10 	
-0 	0.569999999999999951 	0.46000000000000002 	0.135000000000000009 	0.979500000000000037 	0.39700000000000002 	0.252500000000000002 	0.265500000000000014 	9 	
-2 	0.699999999999999956 	0.54500000000000004 	0.184999999999999998 	1.61349999999999993 	0.75 	0.403500000000000025 	0.368499999999999994 	11 	
-0 	0.635000000000000009 	0.484999999999999987 	0.190000000000000002 	1.37650000000000006 	0.634000000000000008 	0.288499999999999979 	0.406000000000000028 	11 	
-1 	0.530000000000000027 	0.400000000000000022 	0.14499999999999999 	0.555000000000000049 	0.193500000000000005 	0.130500000000000005 	0.195000000000000007 	9 	
-1 	0.280000000000000027 	0.204999999999999988 	0.0700000000000000067 	0.101500000000000007 	0.0410000000000000017 	0.0299999999999999989 	0.0299999999999999989 	6 	
-2 	0.655000000000000027 	0.540000000000000036 	0.165000000000000008 	1.40300000000000002 	0.695500000000000007 	0.23849999999999999 	0.419999999999999984 	11 	
-2 	0.645000000000000018 	0.494999999999999996 	0.190000000000000002 	1.53899999999999992 	0.611500000000000044 	0.407999999999999974 	0.445000000000000007 	12 	
-0 	0.57999999999999996 	0.455000000000000016 	0.170000000000000012 	0.907499999999999973 	0.373999999999999999 	0.213499999999999995 	0.284999999999999976 	13 	
-0 	0.57999999999999996 	0.46000000000000002 	0.179999999999999993 	1.0515000000000001 	0.409499999999999975 	0.259500000000000008 	0.276000000000000023 	8 	
-2 	0.560000000000000053 	0.424999999999999989 	0.135000000000000009 	0.941500000000000004 	0.509000000000000008 	0.201500000000000012 	0.197500000000000009 	9 	
-2 	0.469999999999999973 	0.369999999999999996 	0.119999999999999996 	0.579500000000000015 	0.292999999999999983 	0.227000000000000007 	0.140000000000000013 	9 	
-1 	0.369999999999999996 	0.275000000000000022 	0.0899999999999999967 	0.206499999999999989 	0.096000000000000002 	0.0395000000000000004 	0.0580000000000000029 	7 	
-2 	0.525000000000000022 	0.409999999999999976 	0.130000000000000004 	0.6875 	0.343500000000000028 	0.149499999999999994 	0.17649999999999999 	9 	
-2 	0.54500000000000004 	0.450000000000000011 	0.149999999999999994 	0.879499999999999948 	0.387000000000000011 	0.149999999999999994 	0.262500000000000011 	11 	
-0 	0.429999999999999993 	0.325000000000000011 	0.119999999999999996 	0.445000000000000007 	0.165000000000000008 	0.0995000000000000051 	0.154999999999999999 	8 	
-1 	0.520000000000000018 	0.424999999999999989 	0.170000000000000012 	0.680499999999999994 	0.280000000000000027 	0.173999999999999988 	0.195000000000000007 	10 	
-0 	0.665000000000000036 	0.515000000000000013 	0.184999999999999998 	1.34050000000000002 	0.559499999999999997 	0.292999999999999983 	0.4375 	11 	
-0 	0.599999999999999978 	0.474999999999999978 	0.149999999999999994 	1.00750000000000006 	0.442500000000000004 	0.221000000000000002 	0.280000000000000027 	15 	
-1 	0.440000000000000002 	0.364999999999999991 	0.115000000000000005 	0.501000000000000001 	0.243499999999999994 	0.0840000000000000052 	0.146499999999999991 	9 	
-2 	0.385000000000000009 	0.284999999999999976 	0.104999999999999996 	0.29049999999999998 	0.121499999999999997 	0.0685000000000000053 	0.0874999999999999944 	12 	
-0 	0.594999999999999973 	0.479999999999999982 	0.160000000000000003 	1.20950000000000002 	0.522499999999999964 	0.295999999999999985 	0.320000000000000007 	8 	
-1 	0.530000000000000027 	0.429999999999999993 	0.160000000000000003 	0.724500000000000033 	0.321000000000000008 	0.127500000000000002 	0.239999999999999991 	9 	
-2 	0.574999999999999956 	0.424999999999999989 	0.140000000000000013 	0.863500000000000045 	0.393000000000000016 	0.227000000000000007 	0.200000000000000011 	11 	
-1 	0.325000000000000011 	0.25 	0.0700000000000000067 	0.174499999999999988 	0.0874999999999999944 	0.0354999999999999968 	0.0400000000000000008 	7 	
-1 	0.28999999999999998 	0.214999999999999997 	0.0599999999999999978 	0.111500000000000002 	0.0529999999999999985 	0.0184999999999999991 	0.0320000000000000007 	5 	
-0 	0.675000000000000044 	0.510000000000000009 	0.184999999999999998 	1.47300000000000009 	0.629499999999999948 	0.302499999999999991 	0.424499999999999988 	11 	
-1 	0.589999999999999969 	0.474999999999999978 	0.14499999999999999 	0.974500000000000033 	0.467500000000000027 	0.20699999999999999 	0.259000000000000008 	10 	
-0 	0.535000000000000031 	0.424999999999999989 	0.135000000000000009 	0.771000000000000019 	0.376500000000000001 	0.181499999999999995 	0.179499999999999993 	8 	
-2 	0.594999999999999973 	0.469999999999999973 	0.174999999999999989 	0.990999999999999992 	0.382000000000000006 	0.239499999999999991 	0.5 	12 	
-2 	0.474999999999999978 	0.375 	0.119999999999999996 	0.562999999999999945 	0.252500000000000002 	0.120499999999999996 	0.184999999999999998 	10 	
-1 	0.474999999999999978 	0.364999999999999991 	0.115000000000000005 	0.489999999999999991 	0.223000000000000004 	0.123499999999999999 	0.133500000000000008 	9 	
-1 	0.419999999999999984 	0.309999999999999998 	0.100000000000000006 	0.286499999999999977 	0.115000000000000005 	0.0734999999999999959 	0.0850000000000000061 	8 	
-1 	0.474999999999999978 	0.359999999999999987 	0.125 	0.490499999999999992 	0.204999999999999988 	0.130500000000000005 	0.125 	8 	
-2 	0.635000000000000009 	0.525000000000000022 	0.204999999999999988 	1.48399999999999999 	0.550000000000000044 	0.311499999999999999 	0.429999999999999993 	20 	
-0 	0.540000000000000036 	0.440000000000000002 	0.135000000000000009 	0.958999999999999964 	0.23849999999999999 	0.221000000000000002 	0.299999999999999989 	17 	
-1 	0.33500000000000002 	0.25 	0.0749999999999999972 	0.182499999999999996 	0.0704999999999999932 	0.0439999999999999974 	0.0550000000000000003 	7 	
-0 	0.660000000000000031 	0.525000000000000022 	0.179999999999999993 	1.59650000000000003 	0.776499999999999968 	0.39700000000000002 	0.360499999999999987 	10 	
-0 	0.489999999999999991 	0.380000000000000004 	0.125 	0.549000000000000044 	0.244999999999999996 	0.107499999999999998 	0.173999999999999988 	10 	
-2 	0.734999999999999987 	0.569999999999999951 	0.174999999999999989 	1.87999999999999989 	0.909499999999999975 	0.387000000000000011 	0.487999999999999989 	11 	
-0 	0.510000000000000009 	0.400000000000000022 	0.125 	0.54500000000000004 	0.26100000000000001 	0.115000000000000005 	0.138500000000000012 	6 	
-2 	0.564999999999999947 	0.440000000000000002 	0.174999999999999989 	1.12200000000000011 	0.393000000000000016 	0.200000000000000011 	0.375 	20 	
-1 	0.354999999999999982 	0.275000000000000022 	0.0850000000000000061 	0.220000000000000001 	0.0919999999999999984 	0.0599999999999999978 	0.149999999999999994 	8 	
-1 	0.309999999999999998 	0.225000000000000006 	0.0749999999999999972 	0.154999999999999999 	0.0650000000000000022 	0.0369999999999999982 	0.0364999999999999977 	6 	
-1 	0.354999999999999982 	0.280000000000000027 	0.0850000000000000061 	0.29049999999999998 	0.0950000000000000011 	0.0395000000000000004 	0.115000000000000005 	7 	
-0 	0.650000000000000022 	0.535000000000000031 	0.174999999999999989 	1.28950000000000009 	0.609500000000000042 	0.276500000000000024 	0.343999999999999972 	10 	
-0 	0.675000000000000044 	0.54500000000000004 	0.195000000000000007 	1.73449999999999993 	0.684499999999999997 	0.369499999999999995 	0.604999999999999982 	20 	
-1 	0.369999999999999996 	0.280000000000000027 	0.0950000000000000011 	0.286499999999999977 	0.150499999999999995 	0.0690000000000000058 	0.0795000000000000012 	7 	
-2 	0.599999999999999978 	0.450000000000000011 	0.195000000000000007 	1.34000000000000008 	0.616999999999999993 	0.325500000000000012 	0.360499999999999987 	10 	
-1 	0.560000000000000053 	0.455000000000000016 	0.14499999999999999 	0.973999999999999977 	0.547000000000000042 	0.161500000000000005 	0.234999999999999987 	9 	
-0 	0.635000000000000009 	0.494999999999999996 	0.195000000000000007 	1.29699999999999993 	0.55600000000000005 	0.298499999999999988 	0.369999999999999996 	11 	
-0 	0.609999999999999987 	0.484999999999999987 	0.165000000000000008 	1.09149999999999991 	0.393500000000000016 	0.243499999999999994 	0.330000000000000016 	18 	
-2 	0.614999999999999991 	0.469999999999999973 	0.154999999999999999 	1.19999999999999996 	0.508499999999999952 	0.320000000000000007 	0.291999999999999982 	8 	
-1 	0.349999999999999978 	0.234999999999999987 	0.0800000000000000017 	0.170000000000000012 	0.072499999999999995 	0.0464999999999999997 	0.0495000000000000023 	7 	
-0 	0.619999999999999996 	0.474999999999999978 	0.160000000000000003 	1.32450000000000001 	0.686499999999999999 	0.233000000000000013 	0.327500000000000013 	9 	
-1 	0.315000000000000002 	0.234999999999999987 	0.0749999999999999972 	0.148499999999999993 	0.0585000000000000034 	0.0374999999999999986 	0.0425000000000000031 	6 	
-2 	0.675000000000000044 	0.54500000000000004 	0.184999999999999998 	1.73750000000000004 	0.876000000000000001 	0.313500000000000001 	0.468999999999999972 	13 	
-0 	0.594999999999999973 	0.474999999999999978 	0.160000000000000003 	1.14050000000000007 	0.547000000000000042 	0.231000000000000011 	0.271000000000000019 	6 	
-2 	0.550000000000000044 	0.409999999999999976 	0.125 	0.760499999999999954 	0.2505 	0.163500000000000006 	0.195000000000000007 	14 	
-1 	0.550000000000000044 	0.405000000000000027 	0.149999999999999994 	0.675499999999999989 	0.30149999999999999 	0.146499999999999991 	0.209999999999999992 	10 	
-2 	0.584999999999999964 	0.455000000000000016 	0.125 	1.02699999999999991 	0.391000000000000014 	0.211999999999999994 	0.25 	17 	
-1 	0.540000000000000036 	0.429999999999999993 	0.170000000000000012 	0.835999999999999965 	0.372499999999999998 	0.181499999999999995 	0.239999999999999991 	9 	
-1 	0.494999999999999996 	0.390000000000000013 	0.125 	0.66549999999999998 	0.283999999999999975 	0.162000000000000005 	0.200000000000000011 	11 	
-1 	0.484999999999999987 	0.380000000000000004 	0.125 	0.521499999999999964 	0.221500000000000002 	0.117999999999999994 	0.160000000000000003 	8 	
-1 	0.41499999999999998 	0.315000000000000002 	0.0899999999999999967 	0.362499999999999989 	0.174999999999999989 	0.0835000000000000048 	0.0929999999999999993 	6 	
-2 	0.465000000000000024 	0.349999999999999978 	0.119999999999999996 	0.520499999999999963 	0.201500000000000012 	0.162500000000000006 	0.184999999999999998 	11 	
-0 	0.625 	0.489999999999999991 	0.14499999999999999 	0.92000000000000004 	0.437 	0.173499999999999988 	0.280000000000000027 	10 	
-1 	0.28999999999999998 	0.209999999999999992 	0.0700000000000000067 	0.111500000000000002 	0.048000000000000001 	0.0205000000000000009 	0.0299999999999999989 	5 	
-2 	0.574999999999999956 	0.450000000000000011 	0.135000000000000009 	0.921499999999999986 	0.353999999999999981 	0.208999999999999991 	0.236499999999999988 	9 	
-0 	0.550000000000000044 	0.465000000000000024 	0.179999999999999993 	1.21249999999999991 	0.324500000000000011 	0.204999999999999988 	0.525000000000000022 	27 	
-0 	0.57999999999999996 	0.434999999999999998 	0.149999999999999994 	0.838999999999999968 	0.348499999999999976 	0.20699999999999999 	0.192000000000000004 	7 	
-2 	0.589999999999999969 	0.5 	0.200000000000000011 	1.18700000000000006 	0.411999999999999977 	0.270500000000000018 	0.369999999999999996 	16 	
-2 	0.660000000000000031 	0.515000000000000013 	0.165000000000000008 	1.4464999999999999 	0.69399999999999995 	0.297999999999999987 	0.3755 	10 	
-0 	0.734999999999999987 	0.564999999999999947 	0.204999999999999988 	2.12749999999999995 	0.948999999999999955 	0.46000000000000002 	0.564999999999999947 	12 	
-0 	0.46000000000000002 	0.359999999999999987 	0.115000000000000005 	0.475499999999999978 	0.210499999999999993 	0.104999999999999996 	0.160000000000000003 	8 	
-0 	0.550000000000000044 	0.429999999999999993 	0.149999999999999994 	0.655000000000000027 	0.263500000000000012 	0.121999999999999997 	0.221000000000000002 	8 	
-1 	0.494999999999999996 	0.380000000000000004 	0.135000000000000009 	0.509499999999999953 	0.206499999999999989 	0.116500000000000006 	0.165000000000000008 	8 	
-1 	0.385000000000000009 	0.299999999999999989 	0.0950000000000000011 	0.301999999999999991 	0.151999999999999996 	0.0614999999999999991 	0.0734999999999999959 	7 	
-2 	0.604999999999999982 	0.489999999999999991 	0.179999999999999993 	1.16700000000000004 	0.457000000000000017 	0.28999999999999998 	0.3745 	9 	
-1 	0.395000000000000018 	0.299999999999999989 	0.119999999999999996 	0.299499999999999988 	0.126500000000000001 	0.0680000000000000049 	0.0894999999999999962 	8 	
-2 	0.655000000000000027 	0.589999999999999969 	0.200000000000000011 	1.5455000000000001 	0.654000000000000026 	0.376500000000000001 	0.41499999999999998 	11 	
-0 	0.699999999999999956 	0.555000000000000049 	0.220000000000000001 	1.66599999999999993 	0.64700000000000002 	0.428499999999999992 	0.455000000000000016 	11 	
-0 	0.594999999999999973 	0.46000000000000002 	0.140000000000000013 	1.00449999999999995 	0.465500000000000025 	0.209499999999999992 	0.251500000000000001 	9 	
-0 	0.560000000000000053 	0.434999999999999998 	0.149999999999999994 	0.871500000000000052 	0.475499999999999978 	0.183499999999999996 	0.183499999999999996 	9 	
-1 	0.41499999999999998 	0.33500000000000002 	0.100000000000000006 	0.357999999999999985 	0.169000000000000011 	0.067000000000000004 	0.104999999999999996 	7 	
-0 	0.635000000000000009 	0.535000000000000031 	0.190000000000000002 	1.24199999999999999 	0.575999999999999956 	0.247499999999999998 	0.390000000000000013 	14 	
-0 	0.41499999999999998 	0.325000000000000011 	0.115000000000000005 	0.345499999999999974 	0.140500000000000014 	0.0764999999999999986 	0.110000000000000001 	9 	
-2 	0.609999999999999987 	0.474999999999999978 	0.165000000000000008 	1.1160000000000001 	0.427999999999999992 	0.220500000000000002 	0.315000000000000002 	15 	
-2 	0.569999999999999951 	0.450000000000000011 	0.135000000000000009 	1.02000000000000002 	0.546000000000000041 	0.203999999999999987 	0.25 	9 	
-0 	0.645000000000000018 	0.520000000000000018 	0.209999999999999992 	1.5535000000000001 	0.615999999999999992 	0.365499999999999992 	0.473999999999999977 	16 	
-1 	0.225000000000000006 	0.170000000000000012 	0.0700000000000000067 	0.0565000000000000016 	0.0240000000000000005 	0.0129999999999999994 	0.0160000000000000003 	4 	
-2 	0.57999999999999996 	0.469999999999999973 	0.165000000000000008 	0.997500000000000053 	0.393500000000000016 	0.241999999999999993 	0.330000000000000016 	10 	
-2 	0.569999999999999951 	0.434999999999999998 	0.130000000000000004 	0.753499999999999948 	0.348999999999999977 	0.175499999999999989 	0.194000000000000006 	10 	
-0 	0.609999999999999987 	0.494999999999999996 	0.184999999999999998 	1.10850000000000004 	0.370499999999999996 	0.313500000000000001 	0.330000000000000016 	12 	
-0 	0.604999999999999982 	0.455000000000000016 	0.14499999999999999 	0.977500000000000036 	0.468000000000000027 	0.177499999999999991 	0.275000000000000022 	9 	
-2 	0.675000000000000044 	0.555000000000000049 	0.200000000000000011 	1.43849999999999989 	0.54500000000000004 	0.266500000000000015 	0.465000000000000024 	21 	
-0 	0.564999999999999947 	0.445000000000000007 	0.125 	0.830500000000000016 	0.313500000000000001 	0.178499999999999992 	0.23000000000000001 	11 	
-1 	0.160000000000000003 	0.110000000000000001 	0.0250000000000000014 	0.0195 	0.00749999999999999972 	0.0050000000000000001 	0.00600000000000000012 	4 	
-0 	0.645000000000000018 	0.5 	0.154999999999999999 	1.22049999999999992 	0.614500000000000046 	0.235999999999999988 	0.318500000000000005 	10 	
-1 	0.375 	0.275000000000000022 	0.0950000000000000011 	0.246499999999999997 	0.110000000000000001 	0.0415000000000000022 	0.0774999999999999994 	6 	
-1 	0.474999999999999978 	0.359999999999999987 	0.110000000000000001 	0.491999999999999993 	0.210999999999999993 	0.110000000000000001 	0.149999999999999994 	8 	
-1 	0.625 	0.484999999999999987 	0.149999999999999994 	1.04400000000000004 	0.438 	0.286499999999999977 	0.278000000000000025 	9 	
-2 	0.41499999999999998 	0.315000000000000002 	0.125 	0.388000000000000012 	0.0680000000000000049 	0.0899999999999999967 	0.125 	12 	
-1 	0.505000000000000004 	0.424999999999999989 	0.125 	0.611500000000000044 	0.244999999999999996 	0.137500000000000011 	0.200000000000000011 	9 	
-0 	0.604999999999999982 	0.469999999999999973 	0.154999999999999999 	0.973999999999999977 	0.393000000000000016 	0.224000000000000005 	0.33450000000000002 	9 	
-1 	0.445000000000000007 	0.330000000000000016 	0.119999999999999996 	0.346999999999999975 	0.119999999999999996 	0.0840000000000000052 	0.104999999999999996 	11 	
-1 	0.46000000000000002 	0.359999999999999987 	0.140000000000000013 	0.447000000000000008 	0.161000000000000004 	0.086999999999999994 	0.160000000000000003 	9 	
-1 	0.405000000000000027 	0.304999999999999993 	0.0899999999999999967 	0.282499999999999973 	0.114000000000000004 	0.0575000000000000025 	0.0950000000000000011 	7 	
-1 	0.354999999999999982 	0.280000000000000027 	0.100000000000000006 	0.227500000000000008 	0.0934999999999999998 	0.0454999999999999988 	0.0850000000000000061 	11 	
-2 	0.530000000000000027 	0.405000000000000027 	0.125 	0.651499999999999968 	0.271500000000000019 	0.160500000000000004 	0.185999999999999999 	7 	
-0 	0.625 	0.515000000000000013 	0.149999999999999994 	1.24150000000000005 	0.523499999999999965 	0.306499999999999995 	0.359999999999999987 	15 	
-2 	0.594999999999999973 	0.474999999999999978 	0.170000000000000012 	1.09650000000000003 	0.418999999999999984 	0.229000000000000009 	0.349999999999999978 	17 	
-2 	0.465000000000000024 	0.340000000000000024 	0.104999999999999996 	0.485999999999999988 	0.231000000000000011 	0.103499999999999995 	0.122499999999999998 	9 	
-1 	0.304999999999999993 	0.214999999999999997 	0.0650000000000000022 	0.107499999999999998 	0.0439999999999999974 	0.0205000000000000009 	0.0379999999999999991 	5 	
-1 	0.540000000000000036 	0.390000000000000013 	0.125 	0.625499999999999945 	0.252500000000000002 	0.158000000000000002 	0.190000000000000002 	8 	
-1 	0.5 	0.375 	0.140000000000000013 	0.559000000000000052 	0.237499999999999989 	0.135000000000000009 	0.169000000000000011 	9 	
-0 	0.530000000000000027 	0.405000000000000027 	0.130000000000000004 	0.635499999999999954 	0.263500000000000012 	0.1565 	0.184999999999999998 	9 	
-0 	0.465000000000000024 	0.349999999999999978 	0.125 	0.481999999999999984 	0.23000000000000001 	0.105999999999999997 	0.1095 	6 	
-0 	0.655000000000000027 	0.505000000000000004 	0.190000000000000002 	1.34850000000000003 	0.593500000000000028 	0.274500000000000022 	0.424999999999999989 	12 	
-0 	0.645000000000000018 	0.515000000000000013 	0.174999999999999989 	1.54600000000000004 	0.703500000000000014 	0.364999999999999991 	0.41499999999999998 	10 	
-2 	0.280000000000000027 	0.209999999999999992 	0.0800000000000000017 	0.108499999999999999 	0.0410000000000000017 	0.0264999999999999993 	0.0345000000000000029 	7 	
-0 	0.609999999999999987 	0.46000000000000002 	0.14499999999999999 	1.11850000000000005 	0.47799999999999998 	0.294499999999999984 	0.298499999999999988 	10 	
-1 	0.450000000000000011 	0.344999999999999973 	0.110000000000000001 	0.469999999999999973 	0.235499999999999987 	0.0855000000000000066 	0.113500000000000004 	7 	
-2 	0.694999999999999951 	0.550000000000000044 	0.214999999999999997 	1.95649999999999991 	0.712500000000000022 	0.541000000000000036 	0.589999999999999969 	14 	
-2 	0.349999999999999978 	0.255000000000000004 	0.0850000000000000061 	0.214499999999999996 	0.100000000000000006 	0.0464999999999999997 	0.0599999999999999978 	13 	
-0 	0.520000000000000018 	0.409999999999999976 	0.154999999999999999 	0.72699999999999998 	0.290999999999999981 	0.183499999999999996 	0.234999999999999987 	12 	
-2 	0.5 	0.369999999999999996 	0.149999999999999994 	1.06150000000000011 	0.493999999999999995 	0.223000000000000004 	0.295999999999999985 	9 	
-2 	0.724999999999999978 	0.550000000000000044 	0.200000000000000011 	1.51000000000000001 	0.873500000000000054 	0.42649999999999999 	0.508499999999999952 	9 	
-1 	0.25 	0.179999999999999993 	0.0650000000000000022 	0.0685000000000000053 	0.0245000000000000009 	0.0154999999999999999 	0.0224999999999999992 	5 	
-0 	0.619999999999999996 	0.469999999999999973 	0.225000000000000006 	1.11499999999999999 	0.378000000000000003 	0.214499999999999996 	0.359999999999999987 	15 	
-1 	0.530000000000000027 	0.41499999999999998 	0.14499999999999999 	0.94399999999999995 	0.384500000000000008 	0.184999999999999998 	0.265000000000000013 	21 	
-0 	0.619999999999999996 	0.515000000000000013 	0.154999999999999999 	1.3254999999999999 	0.668499999999999983 	0.260500000000000009 	0.33500000000000002 	12 	
-1 	0.419999999999999984 	0.315000000000000002 	0.100000000000000006 	0.343500000000000028 	0.157000000000000001 	0.0795000000000000012 	0.0899999999999999967 	6 	
-2 	0.54500000000000004 	0.434999999999999998 	0.14499999999999999 	0.938500000000000001 	0.368499999999999994 	0.1245 	0.344999999999999973 	11 	
-0 	0.614999999999999991 	0.455000000000000016 	0.14499999999999999 	1.11549999999999994 	0.504499999999999948 	0.237999999999999989 	0.315000000000000002 	10 	
-2 	0.505000000000000004 	0.400000000000000022 	0.130000000000000004 	0.764000000000000012 	0.303499999999999992 	0.189000000000000001 	0.217499999999999999 	11 	
-0 	0.604999999999999982 	0.455000000000000016 	0.14499999999999999 	0.861999999999999988 	0.334000000000000019 	0.19850000000000001 	0.299999999999999989 	9 	
-2 	0.625 	0.5 	0.140000000000000013 	1.09600000000000009 	0.544499999999999984 	0.216499999999999998 	0.294999999999999984 	10 	
-2 	0.619999999999999996 	0.494999999999999996 	0.195000000000000007 	1.51449999999999996 	0.578999999999999959 	0.345999999999999974 	0.519499999999999962 	15 	
-1 	0.419999999999999984 	0.320000000000000007 	0.110000000000000001 	0.308999999999999997 	0.115000000000000005 	0.0645000000000000018 	0.0945000000000000007 	6 	
-0 	0.569999999999999951 	0.450000000000000011 	0.170000000000000012 	1.09800000000000009 	0.413999999999999979 	0.187 	0.405000000000000027 	20 	
-0 	0.505000000000000004 	0.395000000000000018 	0.14499999999999999 	0.651499999999999968 	0.269500000000000017 	0.152999999999999997 	0.204999999999999988 	15 	
-0 	0.569999999999999951 	0.445000000000000007 	0.14499999999999999 	0.877499999999999947 	0.411999999999999977 	0.216999999999999998 	0.220000000000000001 	8 	
-0 	0.619999999999999996 	0.5 	0.149999999999999994 	1.29299999999999993 	0.595999999999999974 	0.313500000000000001 	0.353999999999999981 	10 	
-2 	0.395000000000000018 	0.294999999999999984 	0.115000000000000005 	0.316000000000000003 	0.120499999999999996 	0.0594999999999999973 	0.110500000000000001 	12 	
-2 	0.525000000000000022 	0.400000000000000022 	0.170000000000000012 	0.730500000000000038 	0.279000000000000026 	0.205499999999999988 	0.195000000000000007 	11 	
-0 	0.489999999999999991 	0.390000000000000013 	0.135000000000000009 	0.578500000000000014 	0.246499999999999997 	0.122999999999999998 	0.200000000000000011 	13 	
-2 	0.645000000000000018 	0.494999999999999996 	0.184999999999999998 	1.49350000000000005 	0.526499999999999968 	0.278500000000000025 	0.455000000000000016 	15 	
-1 	0.409999999999999976 	0.33500000000000002 	0.104999999999999996 	0.330500000000000016 	0.140500000000000014 	0.0640000000000000013 	0.104999999999999996 	7 	
-1 	0.505000000000000004 	0.405000000000000027 	0.130000000000000004 	0.598999999999999977 	0.224500000000000005 	0.117499999999999993 	0.225000000000000006 	11 	
-1 	0.505000000000000004 	0.354999999999999982 	0.125 	0.600999999999999979 	0.25 	0.120499999999999996 	0.184999999999999998 	8 	
-0 	0.70499999999999996 	0.550000000000000044 	0.170000000000000012 	1.21900000000000008 	0.639499999999999957 	0.235999999999999988 	0.30099999999999999 	9 	
-2 	0.33500000000000002 	0.234999999999999987 	0.0850000000000000061 	0.154499999999999998 	0.0660000000000000031 	0.0345000000000000029 	0.0449999999999999983 	6 	
-1 	0.434999999999999998 	0.340000000000000024 	0.110000000000000001 	0.379500000000000004 	0.149499999999999994 	0.0850000000000000061 	0.119999999999999996 	8 	
-1 	0.265000000000000013 	0.209999999999999992 	0.0599999999999999978 	0.0965000000000000024 	0.0425000000000000031 	0.0219999999999999987 	0.0299999999999999989 	5 	
-1 	0.179999999999999993 	0.125 	0.0350000000000000033 	0.0264999999999999993 	0.00949999999999999976 	0.00549999999999999968 	0.00850000000000000061 	4 	
-0 	0.655000000000000027 	0.505000000000000004 	0.195000000000000007 	1.44049999999999989 	0.687999999999999945 	0.380500000000000005 	0.362999999999999989 	11 	
-0 	0.625 	0.489999999999999991 	0.154999999999999999 	1.33000000000000007 	0.667499999999999982 	0.259000000000000008 	0.330000000000000016 	10 	
-2 	0.650000000000000022 	0.505000000000000004 	0.170000000000000012 	1.55950000000000011 	0.694999999999999951 	0.351499999999999979 	0.395000000000000018 	11 	
-1 	0.340000000000000024 	0.265000000000000013 	0.0800000000000000017 	0.201500000000000012 	0.0899999999999999967 	0.0475000000000000006 	0.0550000000000000003 	5 	
-0 	0.569999999999999951 	0.465000000000000024 	0.160000000000000003 	0.893499999999999961 	0.314500000000000002 	0.257500000000000007 	0.263000000000000012 	10 	
-0 	0.630000000000000004 	0.479999999999999982 	0.165000000000000008 	1.26150000000000007 	0.550499999999999989 	0.277000000000000024 	0.388500000000000012 	10 	
-2 	0.569999999999999951 	0.46000000000000002 	0.154999999999999999 	1.00049999999999994 	0.454000000000000015 	0.204999999999999988 	0.265000000000000013 	11 	
-2 	0.630000000000000004 	0.465000000000000024 	0.149999999999999994 	1.02699999999999991 	0.537000000000000033 	0.188 	0.17599999999999999 	8 	
-1 	0.465000000000000024 	0.369999999999999996 	0.115000000000000005 	0.53400000000000003 	0.26100000000000001 	0.0980000000000000038 	0.142999999999999988 	7 	
-2 	0.599999999999999978 	0.46000000000000002 	0.170000000000000012 	1.1805000000000001 	0.456000000000000016 	0.337000000000000022 	0.329000000000000015 	11 	
-0 	0.660000000000000031 	0.535000000000000031 	0.174999999999999989 	1.51750000000000007 	0.710999999999999965 	0.3125 	0.41499999999999998 	12 	
-1 	0.54500000000000004 	0.429999999999999993 	0.140000000000000013 	0.687000000000000055 	0.26150000000000001 	0.140500000000000014 	0.25 	9 	
-0 	0.655000000000000027 	0.520000000000000018 	0.190000000000000002 	1.4544999999999999 	0.599999999999999978 	0.38650000000000001 	0.383000000000000007 	10 	
-1 	0.5 	0.375 	0.119999999999999996 	0.542000000000000037 	0.214999999999999997 	0.116000000000000006 	0.170000000000000012 	9 	
-2 	0.680000000000000049 	0.535000000000000031 	0.184999999999999998 	1.60699999999999998 	0.724500000000000033 	0.321500000000000008 	0.497999999999999998 	12 	
-2 	0.665000000000000036 	0.520000000000000018 	0.174999999999999989 	1.37250000000000005 	0.605999999999999983 	0.320000000000000007 	0.395000000000000018 	12 	
-2 	0.609999999999999987 	0.479999999999999982 	0.140000000000000013 	1.0625 	0.516000000000000014 	0.225000000000000006 	0.291499999999999981 	11 	
-0 	0.630000000000000004 	0.5 	0.174999999999999989 	1.11050000000000004 	0.467000000000000026 	0.268000000000000016 	0.329000000000000015 	10 	
-1 	0.515000000000000013 	0.375 	0.140000000000000013 	0.650499999999999967 	0.2495 	0.140999999999999986 	0.221500000000000002 	10 	
-2 	0.57999999999999996 	0.469999999999999973 	0.165000000000000008 	0.927000000000000046 	0.321500000000000008 	0.19850000000000001 	0.315000000000000002 	11 	
-2 	0.599999999999999978 	0.510000000000000009 	0.184999999999999998 	1.28499999999999992 	0.609500000000000042 	0.274500000000000022 	0.315000000000000002 	9 	
-2 	0.560000000000000053 	0.419999999999999984 	0.149999999999999994 	0.875499999999999945 	0.440000000000000002 	0.196500000000000008 	0.231500000000000011 	8 	
-1 	0.385000000000000009 	0.28999999999999998 	0.0800000000000000017 	0.248499999999999999 	0.121999999999999997 	0.0495000000000000023 	0.0650000000000000022 	7 	
-1 	0.520000000000000018 	0.395000000000000018 	0.14499999999999999 	0.770000000000000018 	0.423999999999999988 	0.141999999999999987 	0.189500000000000002 	7 	
-2 	0.515000000000000013 	0.455000000000000016 	0.135000000000000009 	0.722500000000000031 	0.294999999999999984 	0.162500000000000006 	0.234999999999999987 	9 	
-1 	0.520000000000000018 	0.395000000000000018 	0.125 	0.663000000000000034 	0.300499999999999989 	0.131000000000000005 	0.190500000000000003 	9 	
-2 	0.589999999999999969 	0.405000000000000027 	0.149999999999999994 	0.85299999999999998 	0.326000000000000012 	0.26150000000000001 	0.244999999999999996 	9 	
-1 	0.325000000000000011 	0.239999999999999991 	0.0749999999999999972 	0.152499999999999997 	0.0719999999999999946 	0.0645000000000000018 	0.0429999999999999966 	6 	
-1 	0.67000000000000004 	0.484999999999999987 	0.174999999999999989 	1.25649999999999995 	0.535499999999999976 	0.322000000000000008 	0.38600000000000001 	9 	
-0 	0.594999999999999973 	0.469999999999999973 	0.154999999999999999 	1.121 	0.451500000000000012 	0.177999999999999992 	0.154999999999999999 	11 	
-1 	0.390000000000000013 	0.299999999999999989 	0.100000000000000006 	0.266500000000000015 	0.110500000000000001 	0.0589999999999999969 	0.0840000000000000052 	7 	
-0 	0.614999999999999991 	0.479999999999999982 	0.165000000000000008 	1.16149999999999998 	0.513000000000000012 	0.30099999999999999 	0.304999999999999993 	10 	
-0 	0.680000000000000049 	0.5 	0.184999999999999998 	1.7410000000000001 	0.766499999999999959 	0.325500000000000012 	0.468500000000000028 	12 	
-0 	0.70499999999999996 	0.560000000000000053 	0.204999999999999988 	2.38099999999999978 	0.991500000000000048 	0.500499999999999945 	0.623999999999999999 	10 	
-1 	0.424999999999999989 	0.340000000000000024 	0.100000000000000006 	0.351499999999999979 	0.162500000000000006 	0.0820000000000000034 	0.0940000000000000002 	7 	
-1 	0.284999999999999976 	0.220000000000000001 	0.0650000000000000022 	0.096000000000000002 	0.0405000000000000013 	0.0205000000000000009 	0.0299999999999999989 	5 	
-0 	0.405000000000000027 	0.309999999999999998 	0.119999999999999996 	0.309499999999999997 	0.138000000000000012 	0.0580000000000000029 	0.0950000000000000011 	13 	
-1 	0.564999999999999947 	0.419999999999999984 	0.154999999999999999 	0.742999999999999994 	0.309999999999999998 	0.185999999999999999 	0.231000000000000011 	9 	
-1 	0.5 	0.369999999999999996 	0.115000000000000005 	0.574500000000000011 	0.305999999999999994 	0.112000000000000002 	0.140999999999999986 	7 	
-0 	0.604999999999999982 	0.484999999999999987 	0.160000000000000003 	1.22199999999999998 	0.530000000000000027 	0.257500000000000007 	0.280000000000000027 	13 	
-2 	0.465000000000000024 	0.354999999999999982 	0.104999999999999996 	0.479499999999999982 	0.227000000000000007 	0.123999999999999999 	0.125 	8 	
-0 	0.694999999999999951 	0.560000000000000053 	0.220000000000000001 	1.83400000000000007 	0.845500000000000029 	0.421999999999999986 	0.455000000000000016 	11 	
-2 	0.640000000000000013 	0.494999999999999996 	0.165000000000000008 	1.30699999999999994 	0.678000000000000047 	0.291999999999999982 	0.266000000000000014 	11 	
-1 	0.440000000000000002 	0.344999999999999973 	0.130000000000000004 	0.449500000000000011 	0.208999999999999991 	0.0835000000000000048 	0.134000000000000008 	6 	
-1 	0.5 	0.385000000000000009 	0.119999999999999996 	0.560000000000000053 	0.283499999999999974 	0.102999999999999994 	0.135000000000000009 	8 	
-0 	0.67000000000000004 	0.550000000000000044 	0.190000000000000002 	1.39050000000000007 	0.542499999999999982 	0.303499999999999992 	0.400000000000000022 	12 	
-2 	0.589999999999999969 	0.434999999999999998 	0.165000000000000008 	0.976500000000000035 	0.452500000000000013 	0.239499999999999991 	0.234999999999999987 	9 	
-1 	0.380000000000000004 	0.28999999999999998 	0.100000000000000006 	0.236999999999999988 	0.107999999999999999 	0.0395000000000000004 	0.0820000000000000034 	6 	
-1 	0.265000000000000013 	0.200000000000000011 	0.0650000000000000022 	0.0975000000000000033 	0.0400000000000000008 	0.0205000000000000009 	0.0280000000000000006 	7 	
-1 	0.440000000000000002 	0.344999999999999973 	0.119999999999999996 	0.364999999999999991 	0.165500000000000008 	0.0830000000000000043 	0.110000000000000001 	7 	
-0 	0.57999999999999996 	0.440000000000000002 	0.179999999999999993 	0.853999999999999981 	0.366499999999999992 	0.163500000000000006 	0.244999999999999996 	12 	
-0 	0.589999999999999969 	0.440000000000000002 	0.140000000000000013 	1.0069999999999999 	0.47749999999999998 	0.210499999999999993 	0.292499999999999982 	9 	
-0 	0.630000000000000004 	0.505000000000000004 	0.165000000000000008 	1.06499999999999995 	0.45950000000000002 	0.215999999999999998 	0.315000000000000002 	12 	
-2 	0.630000000000000004 	0.530000000000000027 	0.179999999999999993 	1.27950000000000008 	0.617999999999999994 	0.256000000000000005 	0.315000000000000002 	9 	
-2 	0.569999999999999951 	0.434999999999999998 	0.170000000000000012 	0.872999999999999998 	0.382000000000000006 	0.182999999999999996 	0.270500000000000018 	10 	
-0 	0.474999999999999978 	0.349999999999999978 	0.115000000000000005 	0.452000000000000013 	0.171500000000000014 	0.0919999999999999984 	0.154999999999999999 	11 	
-1 	0.569999999999999951 	0.445000000000000007 	0.14499999999999999 	0.740500000000000047 	0.305999999999999994 	0.171999999999999986 	0.182499999999999996 	12 	
-1 	0.299999999999999989 	0.23000000000000001 	0.0749999999999999972 	0.127000000000000002 	0.0519999999999999976 	0.0299999999999999989 	0.0345000000000000029 	6 	
-2 	0.619999999999999996 	0.484999999999999987 	0.154999999999999999 	1.04899999999999993 	0.462000000000000022 	0.231000000000000011 	0.25 	10 	
-0 	0.569999999999999951 	0.440000000000000002 	0.140000000000000013 	0.953500000000000014 	0.378500000000000003 	0.201000000000000012 	0.304999999999999993 	17 	
-1 	0.25 	0.174999999999999989 	0.0599999999999999978 	0.0635000000000000009 	0.0275000000000000001 	0.00800000000000000017 	0.0200000000000000004 	4 	
-1 	0.284999999999999976 	0.214999999999999997 	0.0599999999999999978 	0.0934999999999999998 	0.0309999999999999998 	0.0229999999999999996 	0.0299999999999999989 	6 	
-1 	0.5 	0.375 	0.125 	0.569500000000000006 	0.259000000000000008 	0.123999999999999999 	0.157000000000000001 	7 	
-2 	0.359999999999999987 	0.270000000000000018 	0.100000000000000006 	0.216999999999999998 	0.0884999999999999953 	0.0495000000000000023 	0.0714999999999999941 	6 	
-0 	0.560000000000000053 	0.455000000000000016 	0.190000000000000002 	0.713999999999999968 	0.282999999999999974 	0.129000000000000004 	0.275000000000000022 	9 	
-2 	0.625 	0.489999999999999991 	0.174999999999999989 	1.33250000000000002 	0.570500000000000007 	0.271000000000000019 	0.405000000000000027 	10 	
-1 	0.46000000000000002 	0.349999999999999978 	0.104999999999999996 	0.370499999999999996 	0.157500000000000001 	0.076999999999999999 	0.114000000000000004 	9 	
-2 	0.400000000000000022 	0.320000000000000007 	0.0950000000000000011 	0.302999999999999992 	0.133500000000000008 	0.0599999999999999978 	0.100000000000000006 	7 	
-0 	0.70499999999999996 	0.540000000000000036 	0.204999999999999988 	1.7569999999999999 	0.826500000000000012 	0.416999999999999982 	0.461000000000000021 	9 	
-2 	0.630000000000000004 	0.510000000000000009 	0.174999999999999989 	1.34149999999999991 	0.657499999999999973 	0.262000000000000011 	0.375 	10 	
-2 	0.200000000000000011 	0.140000000000000013 	0.0550000000000000003 	0.0350000000000000033 	0.0145000000000000007 	0.00800000000000000017 	0.0100000000000000002 	5 	
-0 	0.599999999999999978 	0.46000000000000002 	0.14499999999999999 	0.932499999999999996 	0.398500000000000021 	0.224500000000000005 	0.247999999999999998 	8 	
-2 	0.525000000000000022 	0.424999999999999989 	0.190000000000000002 	0.871999999999999997 	0.462500000000000022 	0.172499999999999987 	0.19900000000000001 	9 	
-0 	0.694999999999999951 	0.550000000000000044 	0.154999999999999999 	1.84949999999999992 	0.767000000000000015 	0.442000000000000004 	0.417499999999999982 	10 	
-0 	0.609999999999999987 	0.484999999999999987 	0.165000000000000008 	1.08699999999999997 	0.425499999999999989 	0.232000000000000012 	0.380000000000000004 	11 	
-0 	0.440000000000000002 	0.344999999999999973 	0.170000000000000012 	0.408499999999999974 	0.149999999999999994 	0.0825000000000000039 	0.151499999999999996 	12 	
-2 	0.640000000000000013 	0.525000000000000022 	0.184999999999999998 	1.70700000000000007 	0.763000000000000012 	0.420499999999999985 	0.443500000000000005 	11 	
-1 	0.434999999999999998 	0.330000000000000016 	0.125 	0.406000000000000028 	0.168500000000000011 	0.105499999999999997 	0.096000000000000002 	12 	
-1 	0.309999999999999998 	0.23000000000000001 	0.0700000000000000067 	0.1245 	0.0505000000000000032 	0.0264999999999999993 	0.0379999999999999991 	6 	
-2 	0.619999999999999996 	0.484999999999999987 	0.204999999999999988 	1.21900000000000008 	0.387500000000000011 	0.2505 	0.385000000000000009 	14 	
-0 	0.550000000000000044 	0.424999999999999989 	0.125 	0.963999999999999968 	0.547499999999999987 	0.159000000000000002 	0.214999999999999997 	8 	
-0 	0.675000000000000044 	0.550000000000000044 	0.179999999999999993 	1.68849999999999989 	0.562000000000000055 	0.370499999999999996 	0.599999999999999978 	15 	
-2 	0.510000000000000009 	0.405000000000000027 	0.149999999999999994 	0.703500000000000014 	0.346999999999999975 	0.134000000000000008 	0.188500000000000001 	8 	
-0 	0.530000000000000027 	0.41499999999999998 	0.115000000000000005 	0.591500000000000026 	0.233000000000000013 	0.158500000000000002 	0.179999999999999993 	11 	
-0 	0.57999999999999996 	0.46000000000000002 	0.149999999999999994 	1.11549999999999994 	0.557499999999999996 	0.225500000000000006 	0.28999999999999998 	7 	
-0 	0.625 	0.525000000000000022 	0.214999999999999997 	1.57650000000000001 	0.511499999999999955 	0.259500000000000008 	0.665000000000000036 	16 	
-0 	0.604999999999999982 	0.484999999999999987 	0.165000000000000008 	0.951500000000000012 	0.453500000000000014 	0.193000000000000005 	0.276500000000000024 	11 	
-1 	0.325000000000000011 	0.225000000000000006 	0.0749999999999999972 	0.139000000000000012 	0.0565000000000000016 	0.0320000000000000007 	0.0899999999999999967 	6 	
-2 	0.429999999999999993 	0.349999999999999978 	0.110000000000000001 	0.406000000000000028 	0.16750000000000001 	0.0810000000000000026 	0.135000000000000009 	10 	
-1 	0.5 	0.380000000000000004 	0.125 	0.519000000000000017 	0.248499999999999999 	0.113500000000000004 	0.134000000000000008 	8 	
-0 	0.739999999999999991 	0.574999999999999956 	0.220000000000000001 	2.01200000000000001 	0.891499999999999959 	0.526499999999999968 	0.470999999999999974 	12 	
-0 	0.450000000000000011 	0.349999999999999978 	0.14499999999999999 	0.542499999999999982 	0.17649999999999999 	0.122999999999999998 	0.174999999999999989 	13 	
-0 	0.635000000000000009 	0.515000000000000013 	0.190000000000000002 	1.37149999999999994 	0.50649999999999995 	0.304999999999999993 	0.450000000000000011 	10 	
-0 	0.619999999999999996 	0.465000000000000024 	0.140000000000000013 	1.16050000000000009 	0.600500000000000034 	0.219500000000000001 	0.306999999999999995 	9 	
-2 	0.574999999999999956 	0.455000000000000016 	0.14499999999999999 	1.16500000000000004 	0.580999999999999961 	0.227500000000000008 	0.299999999999999989 	14 	
-0 	0.630000000000000004 	0.474999999999999978 	0.154999999999999999 	1.00049999999999994 	0.452000000000000013 	0.252000000000000002 	0.265000000000000013 	10 	
-2 	0.645000000000000018 	0.5 	0.174999999999999989 	1.28600000000000003 	0.564500000000000002 	0.287999999999999978 	0.38600000000000001 	12 	
-0 	0.660000000000000031 	0.520000000000000018 	0.200000000000000011 	1.67599999999999993 	0.673000000000000043 	0.480499999999999983 	0.450000000000000011 	17 	
-0 	0.584999999999999964 	0.474999999999999978 	0.184999999999999998 	0.958500000000000019 	0.41449999999999998 	0.161500000000000005 	0.330000000000000016 	11 	
-1 	0.255000000000000004 	0.190000000000000002 	0.0749999999999999972 	0.0864999999999999936 	0.0345000000000000029 	0.0205000000000000009 	0.0250000000000000014 	5 	
-2 	0.234999999999999987 	0.170000000000000012 	0.0550000000000000003 	0.0514999999999999972 	0.0179999999999999986 	0.0105000000000000007 	0.0195 	7 	
-2 	0.594999999999999973 	0.455000000000000016 	0.149999999999999994 	0.88600000000000001 	0.431499999999999995 	0.201000000000000012 	0.223000000000000004 	10 	
-0 	0.584999999999999964 	0.41499999999999998 	0.154999999999999999 	0.69850000000000001 	0.299999999999999989 	0.145999999999999991 	0.195000000000000007 	12 	
-0 	0.680000000000000049 	0.550000000000000044 	0.209999999999999992 	1.74449999999999994 	0.597500000000000031 	0.304999999999999993 	0.625 	17 	
-0 	0.599999999999999978 	0.46000000000000002 	0.154999999999999999 	0.973500000000000032 	0.426999999999999991 	0.204499999999999987 	0.299999999999999989 	8 	
-2 	0.46000000000000002 	0.375 	0.130000000000000004 	0.57350000000000001 	0.2505 	0.118999999999999995 	0.195000000000000007 	9 	
-1 	0.330000000000000016 	0.225000000000000006 	0.0749999999999999972 	0.187 	0.0945000000000000007 	0.0395000000000000004 	0.0425000000000000031 	7 	
-0 	0.709999999999999964 	0.564999999999999947 	0.195000000000000007 	1.81699999999999995 	0.785000000000000031 	0.491999999999999993 	0.489999999999999991 	11 	
-2 	0.719999999999999973 	0.564999999999999947 	0.190000000000000002 	2.08099999999999996 	1.08149999999999991 	0.430499999999999994 	0.503000000000000003 	11 	
-1 	0.364999999999999991 	0.270000000000000018 	0.0850000000000000061 	0.222500000000000003 	0.0934999999999999998 	0.0524999999999999981 	0.0660000000000000031 	7 	
-2 	0.540000000000000036 	0.419999999999999984 	0.190000000000000002 	0.685499999999999998 	0.292999999999999983 	0.163000000000000006 	0.380000000000000004 	10 	
-0 	0.560000000000000053 	0.440000000000000002 	0.154999999999999999 	0.640499999999999958 	0.336000000000000021 	0.17649999999999999 	0.244999999999999996 	8 	
-1 	0.380000000000000004 	0.270000000000000018 	0.0800000000000000017 	0.210499999999999993 	0.0864999999999999936 	0.0420000000000000026 	0.0700000000000000067 	8 	
-2 	0.419999999999999984 	0.325000000000000011 	0.115000000000000005 	0.288499999999999979 	0.100000000000000006 	0.0570000000000000021 	0.113500000000000004 	15 	
-0 	0.719999999999999973 	0.560000000000000053 	0.174999999999999989 	1.72649999999999992 	0.637000000000000011 	0.341500000000000026 	0.525000000000000022 	17 	
-2 	0.744999999999999996 	0.564999999999999947 	0.214999999999999997 	1.93100000000000005 	0.896000000000000019 	0.458500000000000019 	0.5 	11 	
-2 	0.604999999999999982 	0.474999999999999978 	0.190000000000000002 	1.12549999999999994 	0.589999999999999969 	0.246999999999999997 	0.260000000000000009 	10 	
-0 	0.54500000000000004 	0.41499999999999998 	0.200000000000000011 	1.3580000000000001 	0.566999999999999948 	0.318000000000000005 	0.403000000000000025 	10 	
-2 	0.650000000000000022 	0.525000000000000022 	0.184999999999999998 	1.48799999999999999 	0.665000000000000036 	0.337000000000000022 	0.378000000000000003 	11 	
-1 	0.530000000000000027 	0.380000000000000004 	0.125 	0.615999999999999992 	0.291999999999999982 	0.113000000000000003 	0.184999999999999998 	8 	
-2 	0.599999999999999978 	0.469999999999999973 	0.174999999999999989 	1.10499999999999998 	0.486499999999999988 	0.246999999999999997 	0.315000000000000002 	15 	
-0 	0.75 	0.550000000000000044 	0.195000000000000007 	1.83250000000000002 	0.82999999999999996 	0.365999999999999992 	0.440000000000000002 	11 	
-1 	0.424999999999999989 	0.340000000000000024 	0.100000000000000006 	0.370999999999999996 	0.149999999999999994 	0.0864999999999999936 	0.115000000000000005 	8 	
-2 	0.540000000000000036 	0.405000000000000027 	0.125 	0.891000000000000014 	0.481499999999999984 	0.191500000000000004 	0.202000000000000013 	9 	
-2 	0.515000000000000013 	0.434999999999999998 	0.14499999999999999 	0.88149999999999995 	0.291999999999999982 	0.205999999999999989 	0.255000000000000004 	10 	
-1 	0.434999999999999998 	0.344999999999999973 	0.115000000000000005 	0.417999999999999983 	0.222000000000000003 	0.0734999999999999959 	0.105999999999999997 	7 	
-0 	0.604999999999999982 	0.46000000000000002 	0.170000000000000012 	1.12200000000000011 	0.346999999999999975 	0.304499999999999993 	0.315000000000000002 	13 	
-2 	0.569999999999999951 	0.429999999999999993 	0.119999999999999996 	1.06150000000000011 	0.347999999999999976 	0.16700000000000001 	0.309999999999999998 	15 	
-2 	0.599999999999999978 	0.494999999999999996 	0.174999999999999989 	1.29000000000000004 	0.605999999999999983 	0.276000000000000023 	0.344499999999999973 	11 	
-2 	0.505000000000000004 	0.385000000000000009 	0.115000000000000005 	0.482499999999999984 	0.209999999999999992 	0.103499999999999995 	0.153499999999999998 	10 	
-1 	0.635000000000000009 	0.5 	0.165000000000000008 	1.4890000000000001 	0.714999999999999969 	0.344499999999999973 	0.361499999999999988 	13 	
-1 	0.46000000000000002 	0.354999999999999982 	0.110000000000000001 	0.435999999999999999 	0.197500000000000009 	0.096000000000000002 	0.125 	8 	
-0 	0.650000000000000022 	0.589999999999999969 	0.220000000000000001 	1.66199999999999992 	0.770000000000000018 	0.378000000000000003 	0.434999999999999998 	11 	
-0 	0.744999999999999996 	0.569999999999999951 	0.214999999999999997 	2.25 	1.15650000000000008 	0.446000000000000008 	0.558000000000000052 	9 	
-2 	0.574999999999999956 	0.440000000000000002 	0.160000000000000003 	0.961500000000000021 	0.482999999999999985 	0.166000000000000009 	0.275000000000000022 	13 	
-2 	0.574999999999999956 	0.479999999999999982 	0.149999999999999994 	0.946500000000000008 	0.435499999999999998 	0.260500000000000009 	0.2505 	9 	
-1 	0.344999999999999973 	0.255000000000000004 	0.0850000000000000061 	0.200500000000000012 	0.104999999999999996 	0.0369999999999999982 	0.0500000000000000028 	5 	
-1 	0.275000000000000022 	0.195000000000000007 	0.0899999999999999967 	0.112500000000000003 	0.0544999999999999998 	0.0294999999999999984 	0.0354999999999999968 	6 	
-1 	0.380000000000000004 	0.284999999999999976 	0.0950000000000000011 	0.242999999999999994 	0.0894999999999999962 	0.0665000000000000036 	0.0749999999999999972 	7 	
-0 	0.694999999999999951 	0.535000000000000031 	0.200000000000000011 	1.58549999999999991 	0.667000000000000037 	0.334000000000000019 	0.470999999999999974 	11 	
-1 	0.455000000000000016 	0.340000000000000024 	0.115000000000000005 	0.485999999999999988 	0.26100000000000001 	0.0655000000000000027 	0.131500000000000006 	8 	
-1 	0.390000000000000013 	0.309999999999999998 	0.104999999999999996 	0.266500000000000015 	0.118499999999999994 	0.0524999999999999981 	0.0810000000000000026 	8 	
-2 	0.665000000000000036 	0.525000000000000022 	0.154999999999999999 	1.35749999999999993 	0.532499999999999973 	0.304499999999999993 	0.44850000000000001 	10 	
-2 	0.714999999999999969 	0.535000000000000031 	0.190000000000000002 	1.67549999999999999 	0.889000000000000012 	0.313 	0.419999999999999984 	10 	
-2 	0.33500000000000002 	0.265000000000000013 	0.104999999999999996 	0.222000000000000003 	0.0934999999999999998 	0.0560000000000000012 	0.0749999999999999972 	7 	
-]
-;
-source = *0 ;
-fieldnames = []
-;
-deep_copy_memory_data = 1 ;
+verbosity = 2 ;
+tsource = *9 ->MemoryVMatrixNoSave(
+source = *10 ->TransposeVMatrix(
+source = *11 ->SubVMatrix(
+parent = *4  ;
+istart = 0 ;
+jstart = 0 ;
+fistart = -1 ;
+flength = -1 ;
+source = *4  ;
 writable = 0 ;
-length = 3132 ;
-width = 9 ;
-inputsize = 8 ;
+length = 150 ;
+width = 6 ;
+inputsize = 4 ;
 targetsize = 1 ;
 weightsize = 0 ;
 extrasize = 0 ;
 metadatadir = ""  )
 ;
 writable = 0 ;
-length = 9 ;
-width = 3132 ;
-inputsize = 3132 ;
+length = 6 ;
+width = 150 ;
+inputsize = 150 ;
 targetsize = 0 ;
 weightsize = 0 ;
 extrasize = 0 ;
@@ -3190,61 +98,61 @@
 ;
 deep_copy_memory_data = 1 ;
 writable = 0 ;
-length = 9 ;
-width = 3132 ;
-inputsize = 3132 ;
+length = 6 ;
+width = 150 ;
+inputsize = 150 ;
 targetsize = 0 ;
 weightsize = 0 ;
 extrasize = 0 ;
 metadatadir = ""  )
 ;
-next_id = 58 ;
-leave_register = 3132 [ 7 21 7 6 7 6 7 31 34 21 7 6 7 33 7 7 7 21 7 37 6 31 6 34 7 33 7 7 31 34 7 34 7 34 6 34 34 7 6 7 7 34 34 34 34 31 33 34 6 7 6 6 34 33 31 7 37 34 34 7 7 7 7 6 6 34 34 7 7 7 27 34 21 33 28 27 34 7 7 27 7 34 31 34 7 37 37 34 6 33 6 7 33 7 6 7 6 37 37 37 7 7 37 31 36 31 27 6 7 34 36 33 31 27 6 33 37 34 34 34 37 37 7 6 37 34 37 7 7 7 34 27 6 7 6 28 27 21 33 21 7 34 21 27 7 21 7 34 6 34 37 7 7 21 6 21 34 27 7 34 27 7 6 33 31 34 31 34 34 34 37 34 31 6 37 7 7 34 34 6 33 7 7 7 6 34 34 7 7 27 31 7 34 33 7 28 33 37 34 7 37 7 7 7 6 34 7 37 7 7 6 33 6 21 34 34 27 34 27 31 7 7 33 7 34 28 33 33 34 7 37 34 7 27 33 27 6 7 33 7 36 34 34 6 36 31 28 7 7 6 33 7 36 37 34 7 7 6 21 36 33 7 6 34 33 7 6 21 7 27 34 7 7 34 7 7 37 7 6 7 37 7 6 6 33 6 7 31 7 7 6 34 34 7 7 34 34 6 33 33 7 7 7 6 7 6 7 34 33 7 7 7 7 34 6 31 31 6 34 27 7 6 34 33 6 7 34 33 27 37 31 27 7 21 37 7 37 34 33 7 6 34 6 31 27 34 34 37 33 21 31 7 34 31 7 7 37 7 6 33 34 37 7 7 7 37 7 27 31 34 6 34 37 7 7 33 7 7 !
 27 31 34 7 6 6 31 7 36 7 6 34 33 31 21 33 34 33 7 37 33 7 37 37 34 6 7 7 33 33 7 37 37 21 6 7 7 7 37 34 31 33 27 34 33 6 7 7 36 34 34 34 36 7 33 37 33 34 21 27 31 21 7 7 7 33 6 34 34 31 7 31 7 31 37 7 33 7 7 34 34 33 28 27 7 33 7 33 7 7 31 27 7 37 31 7 36 7 34 6 37 31 7 33 6 33 31 21 34 7 37 37 31 7 31 33 34 7 7 7 34 31 7 36 34 7 7 33 33 37 7 21 31 7 34 34 7 34 7 33 7 31 7 33 7 31 7 7 7 7 6 6 31 7 6 34 7 34 7 33 7 6 31 7 6 33 33 34 27 31 31 6 27 7 34 7 7 31 6 7 27 31 37 21 7 37 36 6 31 34 31 33 33 6 6 7 7 34 37 31 34 37 34 28 36 34 7 27 37 7 7 33 7 7 33 7 7 27 34 36 7 7 7 21 37 27 7 7 34 37 6 7 7 34 6 31 33 31 7 7 37 31 7 31 7 7 7 7 34 7 21 28 34 7 7 34 7 37 7 34 7 37 6 27 7 34 34 6 7 7 36 7 7 34 33 21 31 33 34 33 6 33 21 6 7 7 7 33 7 7 6 34 34 28 34 7 31 36 6 7 27 7 31 37 6 7 31 7 33 33 34 7 7 7 7 36 7 34 7 33 37 37 7 27 6 7 34 7 6 34 7 7 33 34 37 7 34 6 28 27 7 33 37 6 7 7 31 7 21 37 7 6 7 7 33 7 36 31 34 34 7 7 21 34 31 31 7 7 34 6 31 7 33 36 33 7 27 7 37 31 7 37 34 7 34!
  6 31 33 21 37 37 34 6 7 33 7 33 33 21 7 33 34 6 34 37 7 21 27!
  6 7 33 
31 31 7 34 6 37 7 34 6 31 7 7 7 7 37 27 6 31 7 7 7 33 34 7 6 33 7 33 6 33 7 7 37 37 6 33 33 34 7 6 7 7 33 7 34 7 7 6 7 7 34 7 37 34 7 7 6 7 6 7 7 7 7 6 31 7 7 33 31 31 34 7 7 34 7 7 7 7 33 7 33 7 34 7 27 7 34 34 7 7 7 21 31 6 37 6 34 7 31 31 7 31 34 7 27 34 7 7 31 31 31 7 7 33 21 34 7 7 37 34 34 7 37 37 37 7 31 34 7 6 7 7 34 6 34 34 27 21 6 37 37 31 7 6 7 37 31 31 7 7 7 34 7 33 28 21 34 7 34 6 7 37 27 34 31 34 7 34 34 7 33 6 33 33 6 7 6 6 34 7 6 33 33 7 7 7 7 27 7 33 31 33 7 28 7 31 33 27 7 34 33 33 7 34 37 37 27 31 34 33 27 6 31 33 28 27 21 34 34 37 34 37 34 34 21 21 6 7 33 31 6 21 7 7 34 37 6 34 21 7 7 7 33 34 7 31 28 27 7 7 33 27 21 27 7 27 34 7 7 34 34 6 37 7 34 7 7 31 37 33 37 7 34 31 7 31 31 33 33 6 34 36 7 7 6 34 37 34 34 34 7 7 27 34 33 27 34 21 37 21 34 36 33 6 36 6 7 37 31 33 37 37 31 34 34 7 31 33 33 6 37 34 7 36 7 7 6 7 28 7 6 37 6 7 34 37 7 7 7 7 34 7 6 31 31 6 37 34 7 7 7 6 21 28 34 7 34 7 6 7 34 6 31 34 33 34 27 33 34 7 7 37 27 28 6 37 37 7 7 6 7 34 7 6 27 33 !
 7 7 34 21 37 37 34 34 6 28 34 27 7 7 6 33 7 7 7 7 6 31 7 6 31 31 34 7 31 34 7 6 7 37 33 31 34 6 31 37 7 6 7 28 33 34 6 37 33 33 6 7 7 34 7 7 34 31 7 6 34 27 33 7 37 34 7 34 33 34 27 34 7 27 21 34 34 7 7 36 21 6 34 7 21 7 37 33 7 31 34 31 37 7 6 7 7 27 7 27 33 31 31 34 31 7 31 7 27 34 7 7 6 7 34 31 31 21 33 7 33 34 7 6 34 7 34 27 33 6 7 7 7 36 33 7 31 33 21 27 27 37 6 6 27 37 34 33 7 31 33 27 34 31 31 7 34 37 21 27 34 37 6 28 27 7 7 34 31 37 34 7 33 6 6 27 31 7 7 31 34 34 33 7 27 31 37 37 34 33 7 37 31 34 34 31 34 7 7 34 7 28 33 27 27 7 33 6 34 6 33 27 31 33 27 6 6 33 37 27 34 7 7 7 7 37 33 7 7 7 34 6 7 6 31 37 7 34 7 31 6 33 37 7 27 33 31 37 31 7 7 34 37 31 31 37 31 34 31 7 33 27 34 31 37 6 28 6 6 31 27 7 36 31 34 7 37 34 7 27 33 7 33 21 7 7 7 31 34 27 33 37 31 7 33 7 7 31 33 7 7 7 21 33 37 7 6 7 34 7 6 34 37 31 27 34 6 21 7 31 34 33 7 7 7 37 34 7 33 37 7 31 6 6 7 7 28 34 7 7 6 37 28 31 33 21 21 7 7 34 27 36 34 7 33 7 7 37 6 27 34 7 33 34 34 34 37 34 34 7 27 7 33 6 31 37 7 !
 21 7 37 34 7 7 6 34 34 33 7 7 31 33 7 31 7 7 6 34 27 7 7 6 7 3!
 4 36 31 
7 28 28 7 6 37 34 7 37 7 31 7 33 33 6 33 33 6 21 7 7 7 31 37 31 6 34 7 34 6 7 7 6 21 27 6 7 37 21 28 7 7 7 34 34 31 7 37 7 37 33 21 21 6 7 34 7 33 7 37 7 7 31 31 7 37 34 37 6 33 33 21 6 7 7 34 37 6 7 37 34 34 31 33 7 33 37 34 34 7 7 21 6 34 37 27 7 21 7 33 36 27 34 7 31 6 7 31 6 37 7 7 7 31 7 7 31 34 37 37 7 7 7 34 6 7 7 33 34 27 34 7 7 37 34 7 34 21 34 7 6 34 6 36 34 37 7 6 6 6 7 7 6 34 6 37 7 33 34 37 33 7 7 33 31 37 27 6 37 31 31 6 6 7 37 37 27 6 36 33 7 6 33 31 7 33 6 7 7 7 6 7 37 37 7 7 34 34 7 34 34 6 34 37 7 31 27 37 33 7 36 7 6 31 34 7 7 27 31 28 7 34 27 28 27 7 31 7 27 21 34 6 27 34 7 7 31 31 7 7 7 34 34 37 37 7 33 33 7 36 7 6 37 7 33 37 34 6 34 33 27 7 37 34 7 7 6 27 34 6 37 33 7 27 7 7 7 37 27 7 33 34 27 34 7 34 34 33 36 27 33 37 34 33 34 7 36 7 34 31 33 7 7 34 33 37 7 7 7 33 7 7 37 31 7 31 21 34 37 27 34 7 37 6 34 7 7 7 6 34 21 36 34 34 7 27 7 36 6 33 37 28 7 34 21 34 34 7 7 6 7 33 7 7 34 37 37 33 7 33 7 27 34 7 6 7 34 21 7 37 7 37 31 37 33 7 34 7 27 27 34 33 28 !
 7 31 33 34 34 7 34 27 34 37 7 7 7 7 27 37 33 7 7 34 6 6 27 7 27 31 7 33 7 7 7 27 34 34 7 7 7 34 34 37 7 27 31 37 33 6 7 7 7 37 21 7 27 37 37 34 34 6 7 31 37 36 21 37 7 7 7 37 34 6 6 7 7 7 34 34 7 7 7 33 28 7 34 7 34 34 34 7 7 6 37 7 34 34 37 7 33 37 31 34 7 37 7 7 34 37 34 31 7 7 34 7 33 7 7 7 33 27 7 37 37 31 36 7 7 34 6 37 6 33 6 33 31 27 34 6 6 27 34 31 34 31 37 31 31 34 37 7 7 7 7 34 34 27 7 33 34 7 33 31 37 33 33 6 34 37 27 34 34 34 6 33 6 33 7 7 7 34 34 7 6 7 34 33 6 6 31 31 37 7 6 31 34 37 31 7 33 34 6 34 21 7 7 6 7 34 34 31 31 7 31 7 37 34 7 31 6 7 6 31 27 31 34 33 7 34 6 6 34 34 31 31 34 7 33 34 7 33 37 31 34 7 37 37 7 21 27 7 34 37 7 34 7 33 34 27 37 34 34 27 34 7 27 34 34 7 34 34 37 7 27 7 27 34 7 7 34 34 34 7 21 34 6 7 6 37 7 34 34 7 28 34 33 27 36 34 6 33 6 7 6 7 34 33 31 34 34 27 31 33 28 31 33 7 27 7 33 34 27 36 7 7 37 7 7 7 31 37 7 33 7 7 7 34 27 31 7 28 33 34 27 6 27 37 27 33 31 21 7 27 27 34 7 7 28 7 7 34 37 31 7 36 6 27 6 37 34 33 37 27 34 27 28 34 34 34 !
 31 7 34 7 6 7 31 31 7 7 7 34 21 7 34 33 37 36 34 27 31 21 37 3!
 7 31 7 6
 31 7 34 6 7 31 7 34 7 33 27 7 6 27 34 27 6 33 34 33 37 34 7 28 33 7 33 33 7 7 31 31 31 33 7 37 7 34 34 28 6 7 7 27 7 7 37 6 34 6 33 7 6 37 6 7 34 21 21 31 6 33 34 34 36 34 36 37 7 33 7 7 31 7 7 34 37 34 7 34 34 7 7 34 21 7 6 7 21 37 34 6 33 31 7 7 37 27 7 34 7 6 21 34 34 34 27 34 33 33 7 37 34 6 7 31 31 7 31 6 34 37 31 6 34 6 34 7 6 34 7 6 7 33 7 34 7 31 7 34 34 31 6 7 7 33 7 33 31 7 33 31 7 7 37 33 6 34 36 6 7 6 7 34 7 34 7 6 37 7 7 33 34 31 33 34 6 37 31 28 7 31 6 7 33 31 33 33 34 34 7 7 7 34 33 28 6 34 33 21 33 7 33 33 7 34 6 7 34 36 7 21 34 34 7 21 7 7 7 33 37 28 37 37 6 7 27 7 7 34 27 31 33 37 7 7 7 6 37 6 7 6 34 7 34 34 6 21 7 7 7 7 7 34 7 31 37 6 6 27 6 7 27 6 6 34 28 33 7 37 34 6 34 7 7 34 28 34 33 21 7 7 34 7 27 33 7 34 33 33 31 31 34 6 33 7 7 7 34 7 34 7 34 7 34 6 7 7 27 33 6 37 7 21 37 28 7 27 27 31 6 37 34 7 34 34 34 7 34 31 7 7 6 36 33 7 31 37 34 33 7 37 7 37 21 27 37 33 33 7 27 33 34 31 7 37 33 34 7 33 37 27 27 7 31 36 33 33 21 7 33 7 7 6 27 33 6 6 34 33 33 27!
  37 7 21 34 31 7 37 6 27 34 33 37 7 7 7 33 31 27 33 37 7 7 31 6 7 7 7 37 28 33 7 31 7 7 7 7 37 34 7 33 7 7 6 31 6 34 37 7 7 7 34 37 34 27 34 33 33 7 7 33 34 7 6 7 34 31 34 34 7 33 34 34 34 34 6 37 6 7 7 34 6 6 7 7 31 34 34 36 6 27 27 31 33 33 7 6 7 31 7 37 21 33 7 7 34 37 37 6 6 27 33 7 21 7 7 7 7 21 31 6 34 7 27 7 31 7 6 7 34 28 7 34 33 34 31 33 6 34 6 27 33 37 37 37 37 37 7 7 7 33 6 37 21 7 34 34 27 7 37 7 7 34 7 27 27 33 7 7 34 7 34 33 27 6 31 7 31 33 21 37 6 34 7 7 33 7 37 31 7 7 7 7 7 34 34 7 6 7 7 7 7 27 27 6 34 7 28 6 37 34 27 6 31 31 34 7 31 34 37 31 34 21 7 34 37 33 34 7 37 37 21 7 36 7 34 6 7 6 6 34 34 34 6 31 34 33 7 7 34 27 37 34 7 27 34 33 34 37 31 34 33 6 7 37 7 37 6 34 7 7 34 27 28 7 6 7 37 7 33 7 27 33 7 7 34 33 7 6 7 37 33 34 34 31 7 7 6 33 31 6 6 7 7 31 34 7 7 27 34 6 37 33 27 34 7 27 7 6 31 33 21 7 7 33 21 33 7 7 7 27 7 21 34 34 33 34 27 34 6 6 33 37 28 34 37 6 27 27 6 31 37 7 7 27 27 33 34 34 7 34 27 7 33 31 7 31 31 34 7 34 7 27 27 33 33 6 6 7 27 7 7 21 !
 27 7 ] ;
+next_id = 244 ;
+leave_register = 150 [ 165 106 64 97 79 105 97 151 99 166 109 58 135 105 153 91 79 106 136 166 91 135 70 121 70 106 166 99 165 75 67 88 97 72 109 64 105 105 165 72 136 79 60 108 165 118 54 154 121 117 97 81 105 88 28 97 99 121 94 79 79 96 165 109 109 96 135 55 54 118 153 109 96 151 55 58 90 63 87 96 105 150 87 87 64 93 135 117 150 121 118 78 70 73 81 78 105 91 153 106 78 97 166 69 91 88 63 121 67 54 69 109 108 121 97 93 64 153 96 99 70 136 121 153 69 60 153 87 99 120 88 88 97 64 109 64 153 121 99 96 109 90 117 121 97 55 105 91 58 151 ] ;
 writable = 0 ;
-length = 3132 ;
-width = 9 ;
-inputsize = 8 ;
+length = 150 ;
+width = 6 ;
+inputsize = 4 ;
 targetsize = 1 ;
 weightsize = 0 ;
 extrasize = 0 ;
 metadatadir = ""  )
 ;
-root = *7 ->RegressionTreeNode(
+root = *12 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *8 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *13 ->RegressionTreeLeave(
 id = 1 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 3132 ;
-weights_sum = 1.00000000000005396 ;
-targets_sum = 31044 ;
-weighted_targets_sum = 9.91187739463616069 ;
-weighted_squared_targets_sum = 108.97190293742257 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 150 ;
+weights_sum = 1.00000000000000244 ;
+targets_sum = 2401.03592000000026 ;
+weighted_targets_sum = 16.0069061333333345 ;
+weighted_squared_targets_sum = 9123.03187068025909 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *8  ;
-leave_output = 2 [ 9.911877394635626 1 ] ;
-leave_error = 3 [ 21.4531789022570862 0 2.00000000000010791 ] ;
-split_col = 7 ;
-split_balance = 536 ;
-split_feature_value = 0.194750000000000006 ;
-after_split_error = 15.4025990807452686 ;
+train_set = *8  ;
+leave = *13  ;
+leave_output = 2 [ 16.0069061333332954 1 ] ;
+leave_error = 3 [ 17733.6216534378291 0 2.00000000000000488 ] ;
+split_col = 1 ;
+split_balance = 10 ;
+split_feature_value = 0.036194999999999998 ;
+after_split_error = 6339.63754059466464 ;
 missing_node = *0 ;
-missing_leave = *9 ->RegressionTreeLeave(
+missing_leave = *14 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3256,42 +164,42 @@
 error = []
  )
 ;
-left_node = *10 ->RegressionTreeNode(
+left_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *11 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *16 ->RegressionTreeLeave(
 id = 3 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 1298 ;
-weights_sum = 0.414431673052358729 ;
-targets_sum = 10182 ;
-weighted_targets_sum = 3.25095785440610197 ;
-weighted_squared_targets_sum = 27.6296296296295161 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 70 ;
+weights_sum = 0.466666666666666063 ;
+targets_sum = -4527.80441000000064 ;
+weighted_targets_sum = -30.185362733333335 ;
+weighted_squared_targets_sum = 3339.66270165469314 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *11  ;
-leave_output = 2 [ 7.84437596302003826 1 ] ;
-leave_error = 3 [ 4.25578795947018751 0 0.828863346104717458 ] ;
-split_col = 7 ;
-split_balance = 618 ;
-split_feature_value = 0.0677500000000000047 ;
-after_split_error = 3.17040827833524297 ;
+train_set = *8  ;
+leave = *16  ;
+leave_output = 2 [ -64.6829201428572276 1 ] ;
+leave_error = 3 [ 2774.37058898262922 0 0.933333333333332127 ] ;
+split_col = 1 ;
+split_balance = 12 ;
+split_feature_value = -0.796699999999999964 ;
+after_split_error = 791.868290498600572 ;
 missing_node = *0 ;
-missing_leave = *12 ->RegressionTreeLeave(
+missing_leave = *17 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3303,42 +211,42 @@
 error = []
  )
 ;
-left_node = *13 ->RegressionTreeNode(
+left_node = *18 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *14 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *19 ->RegressionTreeLeave(
 id = 6 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 340 ;
-weights_sum = 0.108556832694763336 ;
-targets_sum = 2014 ;
-weighted_targets_sum = 0.643039591315452297 ;
-weighted_squared_targets_sum = 4.09195402298852073 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 29 ;
+weights_sum = 0.193333333333333218 ;
+targets_sum = -3465.00773000000072 ;
+weighted_targets_sum = -23.1000515333333354 ;
+weighted_squared_targets_sum = 2997.85468750103064 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *14  ;
-leave_output = 2 [ 5.9235294117647177 1 ] ;
-leave_error = 3 [ 0.565780181804550253 0 0.217113665389526672 ] ;
-split_col = 7 ;
-split_balance = 148 ;
-split_feature_value = 0.0264999999999999993 ;
-after_split_error = 0.390135822533501653 ;
+train_set = *8  ;
+leave = *19  ;
+leave_output = 2 [ -119.483025172413875 1 ] ;
+leave_error = 3 [ 475.58129731941159 0 0.386666666666666436 ] ;
+split_col = 1 ;
+split_balance = 9 ;
+split_feature_value = -1.31519999999999992 ;
+after_split_error = 133.463062402859748 ;
 missing_node = *0 ;
-missing_leave = *15 ->RegressionTreeLeave(
-id = 11 ;
+missing_leave = *20 ->RegressionTreeLeave(
+id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3350,18 +258,159 @@
 error = []
  )
 ;
+left_node = *21 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *22 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 10 ;
+weights_sum = 0.0666666666666666657 ;
+targets_sum = -1604.8422300000002 ;
+weighted_targets_sum = -10.698948200000002 ;
+weighted_squared_targets_sum = 1746.29511187725006 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *22  ;
+leave_output = 2 [ -160.484223000000043 1 ] ;
+leave_error = 3 [ 58.5654461660012444 0 0.133333333333333331 ] ;
+split_col = 1 ;
+split_balance = 4 ;
+split_feature_value = -1.76971500000000015 ;
+after_split_error = 16.8789931264006015 ;
+missing_node = *0 ;
+missing_leave = *23 ->RegressionTreeLeave(
+id = 29 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *24 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *25 ->RegressionTreeLeave(
+id = 30 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -562.481160000000045 ;
+weighted_targets_sum = -3.74987440000000039 ;
+weighted_squared_targets_sum = 704.078692073113416 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *25  ;
+leave_output = 2 [ -187.493720000000025 1 ] ;
+leave_error = 3 [ 2.00158256869046891 0 0.0400000000000000008 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = -1.65359500000000015 ;
+after_split_error = 0.00309428292258406845 ;
+missing_node = *0 ;
+missing_leave = *26 ->RegressionTreeLeave(
+id = 71 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *27 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *28 ->RegressionTreeLeave(
+id = 72 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -384.983659999999986 ;
+weighted_targets_sum = -2.56655773333333359 ;
+weighted_squared_targets_sum = 494.042942031446728 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *28  ;
+leave_output = 2 [ -192.491829999999993 1 ] ;
+leave_error = 3 [ 0.00309428292276214822 0 0.00309428292276214822 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = -2.05787500000000012 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *29 ->RegressionTreeLeave(
+id = 233 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
 left_node = *0 ;
-left_leave = *16 ->RegressionTreeLeave(
-id = 12 ;
+left_leave = *30 ->RegressionTreeLeave(
+id = 234 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.000319284802043429908 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00031928480204337765 ;
-weighted_squared_targets_sum = 0.000319284802043344257 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -192.832469999999972 ;
+weighted_targets_sum = -1.28554980000000008 ;
+weighted_squared_targets_sum = 247.895743242006034 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3369,17 +418,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *17 ->RegressionTreeLeave(
-id = 13 ;
+right_leave = *31 ->RegressionTreeLeave(
+id = 235 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 339 ;
-weights_sum = 0.108237547892719915 ;
-targets_sum = 2013 ;
-weighted_targets_sum = 0.642720306513410722 ;
-weighted_squared_targets_sum = 4.09163473818648615 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -192.151190000000014 ;
+weighted_targets_sum = -1.28100793333333351 ;
+weighted_squared_targets_sum = 246.147198789440694 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3387,43 +436,1108 @@
  )
  )
 ;
-left_leave = *14  ;
-right_node = *18 ->RegressionTreeNode(
+left_leave = *28  ;
+right_node = *32 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *19 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *33 ->RegressionTreeLeave(
+id = 73 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -177.497500000000002 ;
+weighted_targets_sum = -1.18331666666666679 ;
+weighted_squared_targets_sum = 210.035750041666688 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *33  ;
+leave_output = 2 [ -177.497500000000002 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 0 ;
+split_feature_value = 0 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *34 ->RegressionTreeLeave(
+id = 236 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *35 ->RegressionTreeLeave(
+id = 237 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -177.497500000000002 ;
+weighted_targets_sum = -1.18331666666666679 ;
+weighted_squared_targets_sum = 210.035750041666688 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *36 ->RegressionTreeLeave(
+id = 238 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *33   )
+;
+left_leave = *25  ;
+right_node = *37 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *38 ->RegressionTreeLeave(
+id = 31 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 7 ;
+weights_sum = 0.0466666666666666688 ;
+targets_sum = -1042.36106999999993 ;
+weighted_targets_sum = -6.94907379999999986 ;
+weighted_squared_targets_sum = 1042.21641980413688 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *38  ;
+leave_output = 2 [ -148.908724285714271 1 ] ;
+leave_error = 3 [ 14.8774105577123557 0 0.0933333333333333376 ] ;
+split_col = 3 ;
+split_balance = 5 ;
+split_feature_value = -4.74906000000000006 ;
+after_split_error = 3.5971516681086797 ;
+missing_node = *0 ;
+missing_leave = *39 ->RegressionTreeLeave(
+id = 74 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *40 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *41 ->RegressionTreeLeave(
+id = 75 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -175.837510000000009 ;
+weighted_targets_sum = -1.17225006666666687 ;
+weighted_squared_targets_sum = 206.1255328200007 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *41  ;
+leave_output = 2 [ -175.837510000000009 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 0 ;
+split_feature_value = 0 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *42 ->RegressionTreeLeave(
+id = 113 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *43 ->RegressionTreeLeave(
+id = 114 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -175.837510000000009 ;
+weighted_targets_sum = -1.17225006666666687 ;
+weighted_squared_targets_sum = 206.1255328200007 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *44 ->RegressionTreeLeave(
+id = 115 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *41  ;
+right_node = *45 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *46 ->RegressionTreeLeave(
+id = 76 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 6 ;
+weights_sum = 0.0400000000000000008 ;
+targets_sum = -866.523559999999975 ;
+weighted_targets_sum = -5.77682373333333388 ;
+weighted_squared_targets_sum = 836.090886984136091 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *46  ;
+leave_output = 2 [ -144.420593333333358 1 ] ;
+leave_error = 3 [ 3.59715166810845233 0 0.0800000000000000017 ] ;
+split_col = 1 ;
+split_balance = 0 ;
+split_feature_value = -1.43682500000000002 ;
+after_split_error = 0.529347535980104866 ;
+missing_node = *0 ;
+missing_leave = *47 ->RegressionTreeLeave(
+id = 116 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *48 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *49 ->RegressionTreeLeave(
+id = 117 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -451.839400000000012 ;
+weighted_targets_sum = -3.01226266666666698 ;
+weighted_squared_targets_sum = 453.827814718630691 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *49  ;
+leave_output = 2 [ -150.613133333333337 1 ] ;
+leave_error = 3 [ 0.282992137883448458 0 0.0400000000000000008 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = -1.51283000000000012 ;
+after_split_error = 0.00999469557589860447 ;
+missing_node = *0 ;
+missing_leave = *50 ->RegressionTreeLeave(
+id = 197 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *51 ->RegressionTreeLeave(
+id = 198 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -146.918560000000014 ;
+weighted_targets_sum = -0.979457066666666876 ;
+weighted_squared_targets_sum = 143.900421816490706 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *52 ->RegressionTreeLeave(
+id = 199 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -304.920839999999998 ;
+weighted_targets_sum = -2.0328056000000001 ;
+weighted_squared_targets_sum = 309.927392902140014 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *49  ;
+right_node = *53 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *54 ->RegressionTreeLeave(
+id = 118 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -414.68416000000002 ;
+weighted_targets_sum = -2.7645610666666669 ;
+weighted_squared_targets_sum = 382.2630722655054 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *54  ;
+leave_output = 2 [ -138.228053333333349 1 ] ;
+leave_error = 3 [ 0.246355398096885225 0 0.0400000000000000008 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = -2.44208500000000006 ;
+after_split_error = 0.183220482032616871 ;
+missing_node = *0 ;
+missing_leave = *55 ->RegressionTreeLeave(
+id = 200 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *56 ->RegressionTreeLeave(
+id = 201 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -136.49520000000004 ;
+weighted_targets_sum = -0.909967999999999999 ;
+weighted_squared_targets_sum = 124.206264153600031 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *57 ->RegressionTreeLeave(
+id = 202 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -278.188960000000009 ;
+weighted_targets_sum = -1.85459306666666679 ;
+weighted_squared_targets_sum = 258.056808111905355 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *54   )
+;
+right_leave = *46   )
+;
+right_leave = *38   )
+;
+left_leave = *22  ;
+right_node = *58 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *59 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 19 ;
+weights_sum = 0.126666666666666677 ;
+targets_sum = -1860.16549999999984 ;
+weighted_targets_sum = -12.4011033333333351 ;
+weighted_squared_targets_sum = 1251.55957562378012 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *59  ;
+leave_output = 2 [ -97.9034473684210553 1 ] ;
+leave_error = 3 [ 74.8976162368580418 0 0.253333333333333355 ] ;
+split_col = 1 ;
+split_balance = 7 ;
+split_feature_value = -1.16121499999999989 ;
+after_split_error = 21.5111374778552857 ;
+missing_node = *0 ;
+missing_leave = *60 ->RegressionTreeLeave(
+id = 32 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *61 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *62 ->RegressionTreeLeave(
+id = 33 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 6 ;
+weights_sum = 0.0400000000000000008 ;
+targets_sum = -715.629170000000045 ;
+weighted_targets_sum = -4.77086113333333373 ;
+weighted_squared_targets_sum = 570.045923418595407 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *62  ;
+leave_output = 2 [ -119.271528333333336 1 ] ;
+leave_error = 3 [ 2.03604915965981625 0 0.0800000000000000017 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = -1.10694000000000004 ;
+after_split_error = 0.221228036537741812 ;
+missing_node = *0 ;
+missing_leave = *63 ->RegressionTreeLeave(
+id = 53 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *64 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *65 ->RegressionTreeLeave(
+id = 54 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -372.10329999999999 ;
+weighted_targets_sum = -2.48068866666666699 ;
+weighted_squared_targets_sum = 307.693557698994709 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *65  ;
+leave_output = 2 [ -124.034433333333354 1 ] ;
+leave_error = 3 [ 0.00548930514478163101 0 0.00548930514478163101 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = -1.69603999999999999 ;
+after_split_error = 0.000219082922519264756 ;
+missing_node = *0 ;
+missing_leave = *66 ->RegressionTreeLeave(
+id = 239 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *67 ->RegressionTreeLeave(
+id = 240 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -124.200460000000007 ;
+weighted_targets_sum = -0.828003066666666898 ;
+weighted_squared_targets_sum = 102.838361761410681 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *68 ->RegressionTreeLeave(
+id = 241 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -247.902839999999998 ;
+weighted_targets_sum = -1.65268560000000009 ;
+weighted_squared_targets_sum = 204.855195937584028 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *65  ;
+right_node = *69 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *70 ->RegressionTreeLeave(
+id = 55 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -343.525869999999998 ;
+weighted_targets_sum = -2.29017246666666674 ;
+weighted_squared_targets_sum = 262.352365719600698 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *70  ;
+leave_output = 2 [ -114.508623333333333 1 ] ;
+leave_error = 3 [ 0.215738731392987493 0 0.0400000000000000008 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 0.509650000000000047 ;
+after_split_error = 0.128841484970660891 ;
+missing_node = *0 ;
+missing_leave = *71 ->RegressionTreeLeave(
+id = 242 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *72 ->RegressionTreeLeave(
+id = 243 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -113.352759999999989 ;
+weighted_targets_sum = -0.755685066666666683 ;
+weighted_squared_targets_sum = 85.65898799745068 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *73 ->RegressionTreeLeave(
+id = 244 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -230.173110000000008 ;
+weighted_targets_sum = -1.53448739999999995 ;
+weighted_squared_targets_sum = 176.693377722149989 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *70   )
+;
+left_leave = *62  ;
+right_node = *74 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *75 ->RegressionTreeLeave(
+id = 34 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 13 ;
+weights_sum = 0.0866666666666666696 ;
+targets_sum = -1144.53632999999968 ;
+weighted_targets_sum = -7.63024219999999964 ;
+weighted_squared_targets_sum = 681.513652205184826 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *75  ;
+leave_output = 2 [ -88.0412561538461489 1 ] ;
+leave_error = 3 [ 19.4750883181964802 0 0.173333333333333339 ] ;
+split_col = 2 ;
+split_balance = 7 ;
+split_feature_value = -6.68825000000000003 ;
+after_split_error = 5.81526685152111966 ;
+missing_node = *0 ;
+missing_leave = *76 ->RegressionTreeLeave(
+id = 56 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *77 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *78 ->RegressionTreeLeave(
+id = 57 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 10 ;
+weights_sum = 0.0666666666666666657 ;
+targets_sum = -929.035599999999931 ;
+weighted_targets_sum = -6.19357066666666611 ;
+weighted_squared_targets_sum = 577.502672231901329 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *78  ;
+leave_output = 2 [ -92.9035599999999988 1 ] ;
+leave_error = 3 [ 4.19581637398951024 0 0.133333333333333331 ] ;
+split_col = 1 ;
+split_balance = 6 ;
+split_feature_value = -1.04829499999999998 ;
+after_split_error = 2.15172959846818168 ;
+missing_node = *0 ;
+missing_leave = *79 ->RegressionTreeLeave(
+id = 107 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *80 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *81 ->RegressionTreeLeave(
+id = 108 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -201.468869999999981 ;
+weighted_targets_sum = -1.34312580000000015 ;
+weighted_squared_targets_sum = 135.305857542979339 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *81  ;
+leave_output = 2 [ -100.734435000000005 1 ] ;
+leave_error = 3 [ 0.0136778921126326503 0 0.0136778921126326503 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = -0.441430000000000045 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *82 ->RegressionTreeLeave(
+id = 227 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *83 ->RegressionTreeLeave(
+id = 228 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -100.018249999999981 ;
+weighted_targets_sum = -0.666788333333333205 ;
+weighted_squared_targets_sum = 66.6910022204166637 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *84 ->RegressionTreeLeave(
+id = 229 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -101.450620000000001 ;
+weighted_targets_sum = -0.67633746666666672 ;
+weighted_squared_targets_sum = 68.6148553225626756 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *81  ;
+right_node = *85 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *86 ->RegressionTreeLeave(
+id = 109 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 8 ;
+weights_sum = 0.0533333333333333368 ;
+targets_sum = -727.566730000000007 ;
+weighted_targets_sum = -4.85044486666666685 ;
+weighted_squared_targets_sum = 442.196814688922075 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *86  ;
+leave_output = 2 [ -90.9458412500000009 1 ] ;
+leave_error = 3 [ 2.13805170635598518 0 0.106666666666666674 ] ;
+split_col = 2 ;
+split_balance = 2 ;
+split_feature_value = -9.18992000000000075 ;
+after_split_error = 0.337813845131658352 ;
+missing_node = *0 ;
+missing_leave = *87 ->RegressionTreeLeave(
+id = 230 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *88 ->RegressionTreeLeave(
+id = 231 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = -91.9765299999998547 ;
+weighted_targets_sum = -0.613176866666666598 ;
+weighted_squared_targets_sum = 56.397880472272675 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *89 ->RegressionTreeLeave(
+id = 232 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 7 ;
+weights_sum = 0.0466666666666666688 ;
+targets_sum = -635.590200000000095 ;
+weighted_targets_sum = -4.23726800000000026 ;
+weighted_squared_targets_sum = 385.7989342166494 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *86   )
+;
+left_leave = *78  ;
+right_node = *90 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *91 ->RegressionTreeLeave(
+id = 58 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -215.500729999999976 ;
+weighted_targets_sum = -1.43667153333333331 ;
+weighted_squared_targets_sum = 104.010979973283341 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *91  ;
+leave_output = 2 [ -71.8335766666666586 1 ] ;
+leave_error = 3 [ 1.61945047753160898 0 0.0400000000000000008 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 2.85157499999999997 ;
+after_split_error = 0.0267389822939781974 ;
+missing_node = *0 ;
+missing_leave = *92 ->RegressionTreeLeave(
+id = 110 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *93 ->RegressionTreeLeave(
+id = 111 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -75.2941599999999909 ;
+weighted_targets_sum = -0.501961066666666733 ;
+weighted_squared_targets_sum = 37.7947368673706734 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *94 ->RegressionTreeLeave(
+id = 112 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -140.206569999999999 ;
+weighted_targets_sum = -0.934710466666666684 ;
+weighted_squared_targets_sum = 66.2162431059126675 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *91   )
+;
+right_leave = *75   )
+;
+right_leave = *59   )
+;
+left_leave = *19  ;
+right_node = *95 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *96 ->RegressionTreeLeave(
 id = 7 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 958 ;
-weights_sum = 0.305874840357600375 ;
-targets_sum = 8168 ;
-weighted_targets_sum = 2.60791826309066277 ;
-weighted_squared_targets_sum = 23.5376756066410842 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 41 ;
+weights_sum = 0.273333333333333095 ;
+targets_sum = -1062.7966799999997 ;
+weighted_targets_sum = -7.08531119999999959 ;
+weighted_squared_targets_sum = 341.808014153662668 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *19  ;
-leave_output = 2 [ 8.52609603340283684 1 ] ;
-leave_error = 3 [ 2.60462809652993066 0 0.61174968071520075 ] ;
-split_col = 7 ;
-split_balance = 186 ;
-split_feature_value = 0.119249999999999995 ;
-after_split_error = 2.34506637014711616 ;
+train_set = *8  ;
+leave = *96  ;
+leave_output = 2 [ -25.9218702439024611 1 ] ;
+leave_error = 3 [ 316.286993179187675 0 0.54666666666666619 ] ;
+split_col = 1 ;
+split_balance = 3 ;
+split_feature_value = -0.34517500000000001 ;
+after_split_error = 105.868276535697049 ;
 missing_node = *0 ;
-missing_leave = *20 ->RegressionTreeLeave(
-id = 14 ;
+missing_leave = *97 ->RegressionTreeLeave(
+id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3435,18 +1549,159 @@
 error = []
  )
 ;
+left_node = *98 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *99 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 19 ;
+weights_sum = 0.126666666666666677 ;
+targets_sum = -893.630649999999946 ;
+weighted_targets_sum = -5.95753766666666706 ;
+weighted_squared_targets_sum = 305.178639377724721 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *99  ;
+leave_output = 2 [ -47.0331921052631543 1 ] ;
+leave_error = 3 [ 49.9532516541003346 0 0.253333333333333355 ] ;
+split_col = 2 ;
+split_balance = 3 ;
+split_feature_value = -7.35407999999999973 ;
+after_split_error = 19.8796985770064367 ;
+missing_node = *0 ;
+missing_leave = *100 ->RegressionTreeLeave(
+id = 41 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *101 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *102 ->RegressionTreeLeave(
+id = 42 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 8 ;
+weights_sum = 0.0533333333333333368 ;
+targets_sum = -478.474159999999983 ;
+weighted_targets_sum = -3.1898277333333338 ;
+weighted_squared_targets_sum = 194.724798247468016 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *102  ;
+leave_output = 2 [ -59.809270000000005 1 ] ;
+leave_error = 3 [ 7.88706018209330217 0 0.106666666666666674 ] ;
+split_col = 1 ;
+split_balance = 2 ;
+split_feature_value = -0.581860000000000044 ;
+after_split_error = 0.868682218515694049 ;
+missing_node = *0 ;
+missing_leave = *103 ->RegressionTreeLeave(
+id = 77 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *104 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *105 ->RegressionTreeLeave(
+id = 78 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -210.84371999999999 ;
+weighted_targets_sum = -1.40562480000000001 ;
+weighted_squared_targets_sum = 99.065637851193344 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *105  ;
+leave_output = 2 [ -70.2812399999999968 1 ] ;
+leave_error = 3 [ 0.553167864882681992 0 0.0400000000000000008 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = -0.627214999999999967 ;
+after_split_error = 0.168978629290664761 ;
+missing_node = *0 ;
+missing_leave = *106 ->RegressionTreeLeave(
+id = 143 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
 left_node = *0 ;
-left_leave = *21 ->RegressionTreeLeave(
-id = 15 ;
+left_leave = *107 ->RegressionTreeLeave(
+id = 144 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.000319284802043416031 ;
-targets_sum = 8 ;
-weighted_targets_sum = 0.00255427841634700212 ;
-weighted_squared_targets_sum = 0.0204342273307801178 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -65.8983800000000031 ;
+weighted_targets_sum = -0.439322533333333376 ;
+weighted_squared_targets_sum = 28.9506432441626735 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3454,17 +1709,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *22 ->RegressionTreeLeave(
-id = 16 ;
+right_leave = *108 ->RegressionTreeLeave(
+id = 145 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 957 ;
-weights_sum = 0.305555555555556968 ;
-targets_sum = 8160 ;
-weighted_targets_sum = 2.60536398467430619 ;
-weighted_squared_targets_sum = 23.5172413793105299 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -144.945339999999987 ;
+weighted_targets_sum = -0.966302266666666632 ;
+weighted_squared_targets_sum = 70.1149946070306669 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3472,45 +1727,1244 @@
  )
  )
 ;
-right_leave = *19   )
+left_leave = *105  ;
+right_node = *109 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *110 ->RegressionTreeLeave(
+id = 79 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = -267.630440000000021 ;
+weighted_targets_sum = -1.78420293333333335 ;
+weighted_squared_targets_sum = 95.6591603962746717 ;
+loss_function_factor = 2 ;
+output = []
 ;
-left_leave = *11  ;
-right_node = *23 ->RegressionTreeNode(
+error = []
+ )
+;
+train_set = *8  ;
+leave = *110  ;
+leave_output = 2 [ -53.5260880000000014 1 ] ;
+leave_error = 3 [ 0.315514353633076561 0 0.0666666666666666657 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = -0.501179999999999959 ;
+after_split_error = 0.0204439538035167401 ;
+missing_node = *0 ;
+missing_leave = *111 ->RegressionTreeLeave(
+id = 146 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *112 ->RegressionTreeLeave(
+id = 147 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = -52.4257500000000078 ;
+weighted_targets_sum = -0.349504999999999844 ;
+weighted_squared_targets_sum = 18.3230617537499931 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *113 ->RegressionTreeLeave(
+id = 148 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = -215.204690000000028 ;
+weighted_targets_sum = -1.4346979333333334 ;
+weighted_squared_targets_sum = 77.336098642524675 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *110   )
+;
+left_leave = *102  ;
+right_node = *114 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *24 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *115 ->RegressionTreeLeave(
+id = 43 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 11 ;
+weights_sum = 0.0733333333333333337 ;
+targets_sum = -415.156490000000019 ;
+weighted_targets_sum = -2.76770993333333326 ;
+weighted_squared_targets_sum = 110.453841130256677 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *115  ;
+leave_output = 2 [ -37.7414990909090875 1 ] ;
+leave_error = 3 [ 11.9926383949132571 0 0.146666666666666667 ] ;
+split_col = 1 ;
+split_balance = 7 ;
+split_feature_value = -0.617779999999999996 ;
+after_split_error = 5.7658396809937118 ;
+missing_node = *0 ;
+missing_leave = *116 ->RegressionTreeLeave(
+id = 80 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *117 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *118 ->RegressionTreeLeave(
+id = 81 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -103.127129999999994 ;
+weighted_targets_sum = -0.687514200000000075 ;
+weighted_squared_targets_sum = 35.6753355603233402 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *118  ;
+leave_output = 2 [ -51.5635650000000041 1 ] ;
+leave_error = 3 [ 0.449304840400662542 0 0.0266666666666666684 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 2.03620000000000001 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *119 ->RegressionTreeLeave(
+id = 161 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *120 ->RegressionTreeLeave(
+id = 162 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -55.6683099999999911 ;
+weighted_targets_sum = -0.371122066666666695 ;
+weighted_squared_targets_sum = 20.6597382550406685 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *121 ->RegressionTreeLeave(
+id = 163 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -47.4588200000000029 ;
+weighted_targets_sum = -0.316392133333333381 ;
+weighted_squared_targets_sum = 15.0155973052826699 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *118  ;
+right_node = *122 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *123 ->RegressionTreeLeave(
+id = 82 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 9 ;
+weights_sum = 0.0600000000000000047 ;
+targets_sum = -312.029360000000054 ;
+weighted_targets_sum = -2.08019573333333341 ;
+weighted_squared_targets_sum = 74.7785055699333441 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *123  ;
+leave_output = 2 [ -34.6699288888888901 1 ] ;
+leave_error = 3 [ 5.31653484059320025 0 0.120000000000000009 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 0.591400000000000037 ;
+after_split_error = 1.59838525525966713 ;
+missing_node = *0 ;
+missing_leave = *124 ->RegressionTreeLeave(
+id = 164 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *125 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *126 ->RegressionTreeLeave(
+id = 165 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = -198.243249999999989 ;
+weighted_targets_sum = -1.32162166666666669 ;
+weighted_squared_targets_sum = 52.7188190578006726 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *126  ;
+leave_output = 2 [ -39.6486500000000035 1 ] ;
+leave_error = 3 [ 0.636608327434661891 0 0.0666666666666666657 ] ;
+split_col = 1 ;
+split_balance = 3 ;
+split_feature_value = -0.401110000000000022 ;
+after_split_error = 0.405305692174637633 ;
+missing_node = *0 ;
+missing_leave = *127 ->RegressionTreeLeave(
+id = 191 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *128 ->RegressionTreeLeave(
+id = 192 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = -35.9233100000000007 ;
+weighted_targets_sum = -0.23948873333333337 ;
+weighted_squared_targets_sum = 8.60322800904066831 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *129 ->RegressionTreeLeave(
+id = 193 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = -162.319939999999974 ;
+weighted_targets_sum = -1.08213293333333338 ;
+weighted_squared_targets_sum = 44.1155910487599954 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *126  ;
+right_node = *130 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *131 ->RegressionTreeLeave(
+id = 166 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = -113.786109999999994 ;
+weighted_targets_sum = -0.758574066666666713 ;
+weighted_squared_targets_sum = 22.059686512132668 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *131  ;
+leave_output = 2 [ -28.4465274999999984 1 ] ;
+leave_error = 3 [ 0.961776927825005234 0 0.0533333333333333368 ] ;
+split_col = 1 ;
+split_balance = 2 ;
+split_feature_value = -0.412640000000000007 ;
+after_split_error = 0.122110760935989035 ;
+missing_node = *0 ;
+missing_leave = *132 ->RegressionTreeLeave(
+id = 194 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *133 ->RegressionTreeLeave(
+id = 195 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -33.20474999999999 ;
+weighted_targets_sum = -0.221365000000000062 ;
+weighted_squared_targets_sum = 7.35036948375000065 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *134 ->RegressionTreeLeave(
+id = 196 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -80.5813599999999894 ;
+weighted_targets_sum = -0.537209066666666679 ;
+weighted_squared_targets_sum = 14.7093170283826673 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *131   )
+;
+right_leave = *123   )
+;
+right_leave = *115   )
+;
+left_leave = *99  ;
+right_node = *135 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *136 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 22 ;
+weights_sum = 0.14666666666666664 ;
+targets_sum = -169.166030000000035 ;
+weighted_targets_sum = -1.12777353333333341 ;
+weighted_squared_targets_sum = 36.6293747759380111 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *136  ;
+leave_output = 2 [ -7.68936500000000223 1 ] ;
+leave_error = 3 [ 55.9150248815966791 0 0.293333333333333279 ] ;
+split_col = 1 ;
+split_balance = 4 ;
+split_feature_value = -0.0821300000000000086 ;
+after_split_error = 26.8904632651653444 ;
+missing_node = *0 ;
+missing_leave = *137 ->RegressionTreeLeave(
+id = 44 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *138 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *139 ->RegressionTreeLeave(
+id = 45 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 13 ;
+weights_sum = 0.0866666666666666696 ;
+targets_sum = -207.557480000000027 ;
+weighted_targets_sum = -1.38371653333333322 ;
+weighted_squared_targets_sum = 30.1523790784066676 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *139  ;
+leave_output = 2 [ -15.965959999999999 1 ] ;
+leave_error = 3 [ 16.1200325117360102 0 0.173333333333333339 ] ;
+split_col = 3 ;
+split_balance = 3 ;
+split_feature_value = -1.15891499999999992 ;
+after_split_error = 9.19154126603875987 ;
+missing_node = *0 ;
+missing_leave = *140 ->RegressionTreeLeave(
+id = 83 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *141 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *142 ->RegressionTreeLeave(
+id = 84 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = -119.815830000000005 ;
+weighted_targets_sum = -0.798772200000000043 ;
+weighted_squared_targets_sum = 20.6369126752206711 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *142  ;
+leave_output = 2 [ -23.9631660000000011 1 ] ;
+leave_error = 3 [ 2.9916037008709373 0 0.0666666666666666657 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = -0.215794999999999987 ;
+after_split_error = 0.746847325971322307 ;
+missing_node = *0 ;
+missing_leave = *143 ->RegressionTreeLeave(
+id = 149 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *144 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *145 ->RegressionTreeLeave(
+id = 150 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -62.1399899999999974 ;
+weighted_targets_sum = -0.41426660000000004 ;
+weighted_squared_targets_sum = 12.9961512182033356 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *145  ;
+leave_output = 2 [ -31.0699950000000022 1 ] ;
+leave_error = 3 [ 0.2497800550726679 0 0.0266666666666666684 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = -1.68244499999999997 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *146 ->RegressionTreeLeave(
+id = 215 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *147 ->RegressionTreeLeave(
+id = 216 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -28.0094799999999964 ;
+weighted_targets_sum = -0.186729866666666688 ;
+weighted_squared_targets_sum = 5.23020646580266568 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *148 ->RegressionTreeLeave(
+id = 217 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -34.130510000000001 ;
+weighted_targets_sum = -0.227536733333333352 ;
+weighted_squared_targets_sum = 7.76594475240066817 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *145  ;
+right_node = *149 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *150 ->RegressionTreeLeave(
+id = 151 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -57.6758400000000009 ;
+weighted_targets_sum = -0.384505600000000058 ;
+weighted_squared_targets_sum = 7.64076145701733456 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *150  ;
+leave_output = 2 [ -19.2252800000000015 1 ] ;
+leave_error = 3 [ 0.497067270898664926 0 0.0400000000000000008 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = -0.10633999999999999 ;
+after_split_error = 0.00781290769066451431 ;
+missing_node = *0 ;
+missing_leave = *151 ->RegressionTreeLeave(
+id = 218 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *152 ->RegressionTreeLeave(
+id = 219 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -22.2395500000000013 ;
+weighted_targets_sum = -0.148263666666666655 ;
+weighted_squared_targets_sum = 3.29731722801666738 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *153 ->RegressionTreeLeave(
+id = 220 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -35.4362899999999996 ;
+weighted_targets_sum = -0.236241933333333348 ;
+weighted_squared_targets_sum = 4.34344422900066718 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *150   )
+;
+left_leave = *142  ;
+right_node = *154 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *155 ->RegressionTreeLeave(
+id = 85 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 8 ;
+weights_sum = 0.0533333333333333368 ;
+targets_sum = -87.7416499999999928 ;
+weighted_targets_sum = -0.584944333333333399 ;
+weighted_squared_targets_sum = 9.5154664031860019 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *155  ;
+leave_output = 2 [ -10.9677062500000009 1 ] ;
+leave_error = 3 [ 6.19993756516783456 0 0.106666666666666674 ] ;
+split_col = 3 ;
+split_balance = 6 ;
+split_feature_value = 3.08020000000000005 ;
+after_split_error = 2.37574213259581013 ;
+missing_node = *0 ;
+missing_leave = *156 ->RegressionTreeLeave(
+id = 152 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *157 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *158 ->RegressionTreeLeave(
+id = 153 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 7 ;
+weights_sum = 0.0466666666666666688 ;
+targets_sum = -92.6157499999999914 ;
+weighted_targets_sum = -0.617438333333333422 ;
+weighted_squared_targets_sum = 9.3570873977860014 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *158  ;
+leave_output = 2 [ -13.2308214285714296 1 ] ;
+leave_error = 3 [ 2.37574213259581013 0 0.0933333333333333376 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = -0.262695000000000012 ;
+after_split_error = 0.82728846988722271 ;
+missing_node = *0 ;
+missing_leave = *159 ->RegressionTreeLeave(
+id = 185 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *160 ->RegressionTreeLeave(
+id = 186 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = -10.5931799999999878 ;
+weighted_targets_sum = -0.0706212000000001061 ;
+weighted_squared_targets_sum = 0.748103083416000847 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *161 ->RegressionTreeLeave(
+id = 187 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 6 ;
+weights_sum = 0.0400000000000000008 ;
+targets_sum = -82.0225700000000018 ;
+weighted_targets_sum = -0.546817133333333372 ;
+weighted_squared_targets_sum = 8.60898431436999978 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *158  ;
+right_node = *162 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *163 ->RegressionTreeLeave(
+id = 154 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 4.87410000000000032 ;
+weighted_targets_sum = 0.032494000000000002 ;
+weighted_squared_targets_sum = 0.158379005400000022 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *163  ;
+leave_output = 2 [ 4.87410000000000032 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 0 ;
+split_feature_value = 0 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *164 ->RegressionTreeLeave(
+id = 188 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *165 ->RegressionTreeLeave(
+id = 189 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 4.87410000000000032 ;
+weighted_targets_sum = 0.032494000000000002 ;
+weighted_squared_targets_sum = 0.158379005400000022 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *166 ->RegressionTreeLeave(
+id = 190 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *163   )
+;
+right_leave = *155   )
+;
+left_leave = *139  ;
+right_node = *167 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *168 ->RegressionTreeLeave(
+id = 46 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 9 ;
+weights_sum = 0.0600000000000000047 ;
+targets_sum = 38.391449999999999 ;
+weighted_targets_sum = 0.255943000000000032 ;
+weighted_squared_targets_sum = 6.47699569753133275 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *168  ;
+leave_output = 2 [ 4.26571666666666705 1 ] ;
+leave_error = 3 [ 10.7704307534293324 0 0.120000000000000009 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = -0.0792649999999999744 ;
+after_split_error = 2.14048019403406808 ;
+missing_node = *0 ;
+missing_leave = *169 ->RegressionTreeLeave(
+id = 86 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *170 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *171 ->RegressionTreeLeave(
+id = 87 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = -20.8623900000000013 ;
+weighted_targets_sum = -0.139082600000000001 ;
+weighted_squared_targets_sum = 1.25263627282333334 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *171  ;
+leave_output = 2 [ -5.21559749999999944 1 ] ;
+leave_error = 3 [ 1.05447482393966685 0 0.0533333333333333368 ] ;
+split_col = 3 ;
+split_balance = 2 ;
+split_feature_value = -1.85971500000000001 ;
+after_split_error = 0.139893091254222196 ;
+missing_node = *0 ;
+missing_leave = *172 ->RegressionTreeLeave(
+id = 125 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *173 ->RegressionTreeLeave(
+id = 126 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -12.3881300000000003 ;
+weighted_targets_sum = -0.0825875333333333378 ;
+weighted_squared_targets_sum = 1.02310509931266669 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *174 ->RegressionTreeLeave(
+id = 127 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -8.47425999999999924 ;
+weighted_targets_sum = -0.0564950666666666698 ;
+weighted_squared_targets_sum = 0.229531173510666681 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *171  ;
+right_node = *175 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *176 ->RegressionTreeLeave(
+id = 88 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = 59.2538400000000038 ;
+weighted_targets_sum = 0.395025600000000032 ;
+weighted_squared_targets_sum = 5.22435942470800008 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *176  ;
+leave_output = 2 [ 11.8507680000000004 1 ] ;
+leave_error = 3 [ 1.08600537009439835 0 0.0666666666666666657 ] ;
+split_col = 3 ;
+split_balance = 3 ;
+split_feature_value = 2.88428500000000021 ;
+after_split_error = 0.734990923745331592 ;
+missing_node = *0 ;
+missing_leave = *177 ->RegressionTreeLeave(
+id = 128 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *178 ->RegressionTreeLeave(
+id = 129 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 10.0296400000000041 ;
+weighted_targets_sum = 0.0668642666666666996 ;
+weighted_squared_targets_sum = 0.670624523530667149 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *179 ->RegressionTreeLeave(
+id = 130 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = 49.2241999999999962 ;
+weighted_targets_sum = 0.328161333333333305 ;
+weighted_squared_targets_sum = 4.55373490117733315 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *176   )
+;
+right_leave = *168   )
+;
+right_leave = *136   )
+;
+right_leave = *96   )
+;
+left_leave = *16  ;
+right_node = *180 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *181 ->RegressionTreeLeave(
 id = 4 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 1834 ;
-weights_sum = 0.585568326947639717 ;
-targets_sum = 20862 ;
-weighted_targets_sum = 6.66091954022990951 ;
-weighted_squared_targets_sum = 81.3422733077908617 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 80 ;
+weights_sum = 0.533333333333332882 ;
+targets_sum = 6928.84032999999908 ;
+weighted_targets_sum = 46.1922688666666588 ;
+weighted_squared_targets_sum = 5783.36916902556004 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *24  ;
-leave_output = 2 [ 11.3751363140676069 1 ] ;
-leave_error = 3 [ 11.1468111212782066 0 1.17113665389527943 ] ;
-split_col = 7 ;
-split_balance = 1132 ;
-split_feature_value = 0.409499999999999975 ;
-after_split_error = 10.0808309880025568 ;
+train_set = *8  ;
+leave = *181  ;
+leave_output = 2 [ 86.6105041250000625 1 ] ;
+leave_error = 3 [ 3565.26695161203133 0 1.06666666666666576 ] ;
+split_col = 1 ;
+split_balance = 20 ;
+split_feature_value = 0.937115000000000031 ;
+after_split_error = 1048.37002708401451 ;
 missing_node = *0 ;
-missing_leave = *25 ->RegressionTreeLeave(
+missing_leave = *182 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3522,42 +2976,42 @@
 error = []
  )
 ;
-left_node = *26 ->RegressionTreeNode(
+left_node = *183 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *27 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *184 ->RegressionTreeLeave(
 id = 9 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 1483 ;
-weights_sum = 0.47349936143038901 ;
-targets_sum = 16181 ;
-weighted_targets_sum = 5.16634738186464926 ;
-weighted_squared_targets_sum = 59.951149425287305 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 50 ;
+weights_sum = 0.333333333333332982 ;
+targets_sum = 2449.19952999999987 ;
+weighted_targets_sum = 16.3279968666666662 ;
+weighted_squared_targets_sum = 1012.36507460954613 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *27  ;
-leave_output = 2 [ 10.9109912339853796 1 ] ;
-leave_error = 3 [ 7.1623568600776073 0 0.946998722860778019 ] ;
-split_col = 5 ;
-split_balance = 271 ;
-split_feature_value = 0.39975000000000005 ;
-after_split_error = 6.77385057026893733 ;
+train_set = *8  ;
+leave = *184  ;
+leave_output = 2 [ 48.9839906000000482 1 ] ;
+leave_error = 3 [ 425.10925915183185 0 0.666666666666665964 ] ;
+split_col = 1 ;
+split_balance = 6 ;
+split_feature_value = 0.432620000000000005 ;
+after_split_error = 121.842470789270692 ;
 missing_node = *0 ;
-missing_leave = *28 ->RegressionTreeLeave(
-id = 17 ;
+missing_leave = *185 ->RegressionTreeLeave(
+id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3569,42 +3023,42 @@
 error = []
  )
 ;
-left_node = *29 ->RegressionTreeNode(
+left_node = *186 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *30 ->RegressionTreeLeave(
-id = 18 ;
+verbosity = 2 ;
+leave_template = *187 ->RegressionTreeLeave(
+id = 12 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 606 ;
-weights_sum = 0.193486590038316253 ;
-targets_sum = 7079 ;
-weighted_targets_sum = 2.26021711366538325 ;
-weighted_squared_targets_sum = 28.4773307790548458 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 28 ;
+weights_sum = 0.186666666666666564 ;
+targets_sum = 842.195039999999949 ;
+weighted_targets_sum = 5.61463360000000122 ;
+weighted_squared_targets_sum = 189.51913846573197 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *30  ;
-leave_output = 2 [ 11.6815181518150233 1 ] ;
-leave_error = 3 [ 4.14912707745942111 0 0.386973180076632506 ] ;
-split_col = 7 ;
-split_balance = 68 ;
-split_feature_value = 0.254750000000000032 ;
-after_split_error = 3.62419886116776269 ;
+train_set = *8  ;
+leave = *187  ;
+leave_output = 2 [ 30.0783942857143103 1 ] ;
+leave_error = 3 [ 41.2799505502247399 0 0.373333333333333128 ] ;
+split_col = 1 ;
+split_balance = 2 ;
+split_feature_value = 0.26894499999999999 ;
+after_split_error = 22.4639602548301482 ;
 missing_node = *0 ;
-missing_leave = *31 ->RegressionTreeLeave(
-id = 29 ;
+missing_leave = *188 ->RegressionTreeLeave(
+id = 35 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3616,42 +3070,42 @@
 error = []
  )
 ;
-left_node = *32 ->RegressionTreeNode(
+left_node = *189 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *33 ->RegressionTreeLeave(
-id = 30 ;
+verbosity = 2 ;
+leave_template = *190 ->RegressionTreeLeave(
+id = 36 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 337 ;
-weights_sum = 0.107598978288633074 ;
-targets_sum = 3586 ;
-weighted_targets_sum = 1.14495530012771329 ;
-weighted_squared_targets_sum = 12.9355044699872739 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 15 ;
+weights_sum = 0.100000000000000006 ;
+targets_sum = 352.039670000000001 ;
+weighted_targets_sum = 2.34693113333333336 ;
+weighted_squared_targets_sum = 59.5814975016619925 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *33  ;
-leave_output = 2 [ 10.6409495548961743 1 ] ;
-leave_error = 3 [ 1.50418575743453542 0 0.215197956577266147 ] ;
-split_col = 5 ;
-split_balance = 235 ;
-split_feature_value = 0.242499999999999993 ;
-after_split_error = 1.21784951331944691 ;
+train_set = *8  ;
+leave = *190  ;
+leave_output = 2 [ 23.4693113333333336 1 ] ;
+leave_error = 3 [ 9.00128011113828741 0 0.200000000000000011 ] ;
+split_col = 3 ;
+split_balance = 3 ;
+split_feature_value = 0.287649999999999961 ;
+after_split_error = 4.55178643471853928 ;
 missing_node = *0 ;
-missing_leave = *34 ->RegressionTreeLeave(
-id = 35 ;
+missing_leave = *191 ->RegressionTreeLeave(
+id = 95 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3663,42 +3117,42 @@
 error = []
  )
 ;
-left_node = *35 ->RegressionTreeNode(
+left_node = *192 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *36 ->RegressionTreeLeave(
-id = 36 ;
+verbosity = 2 ;
+leave_template = *193 ->RegressionTreeLeave(
+id = 96 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 51 ;
-weights_sum = 0.0162835249042145545 ;
-targets_sum = 682 ;
-weighted_targets_sum = 0.217752234993614374 ;
-weighted_squared_targets_sum = 3.10472541507024236 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 6 ;
+weights_sum = 0.0400000000000000008 ;
+targets_sum = 106.155180000000001 ;
+weighted_targets_sum = 0.70770120000000003 ;
+weighted_squared_targets_sum = 13.2969076162293316 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *36  ;
-leave_output = 2 [ 13.3725490196078507 1 ] ;
-leave_error = 3 [ 0.385645956977931859 0 0.0325670498084291091 ] ;
-split_col = 4 ;
-split_balance = 15 ;
-split_feature_value = 0.639249999999999985 ;
-after_split_error = 0.273165886192706542 ;
+train_set = *8  ;
+leave = *193  ;
+leave_output = 2 [ 17.6925300000000014 1 ] ;
+leave_error = 3 [ 1.55176580838666167 0 0.0800000000000000017 ] ;
+split_col = 1 ;
+split_balance = 4 ;
+split_feature_value = 0.107589999999999991 ;
+after_split_error = 0.446338964225057477 ;
 missing_node = *0 ;
-missing_leave = *37 ->RegressionTreeLeave(
-id = 47 ;
+missing_leave = *194 ->RegressionTreeLeave(
+id = 179 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3711,17 +3165,17 @@
  )
 ;
 left_node = *0 ;
-left_leave = *38 ->RegressionTreeLeave(
-id = 48 ;
+left_leave = *195 ->RegressionTreeLeave(
+id = 180 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.00031928480204342297 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.00415070242656449383 ;
-weighted_squared_targets_sum = 0.0539591315453387121 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 21.1915699999999987 ;
+weighted_targets_sum = 0.141277133333333388 ;
+weighted_squared_targets_sum = 2.9938842604326652 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3729,17 +3183,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *39 ->RegressionTreeLeave(
-id = 49 ;
+right_leave = *196 ->RegressionTreeLeave(
+id = 181 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 50 ;
-weights_sum = 0.0159642401021711303 ;
-targets_sum = 669 ;
-weighted_targets_sum = 0.21360153256704989 ;
-weighted_squared_targets_sum = 3.05076628352490475 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = 84.9636100000000027 ;
+weighted_targets_sum = 0.56642406666666667 ;
+weighted_squared_targets_sum = 10.3030233557966664 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3747,43 +3201,130 @@
  )
  )
 ;
-left_leave = *36  ;
-right_node = *40 ->RegressionTreeNode(
+left_leave = *193  ;
+right_node = *197 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *41 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *198 ->RegressionTreeLeave(
+id = 97 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 9 ;
+weights_sum = 0.0600000000000000047 ;
+targets_sum = 245.884489999999971 ;
+weighted_targets_sum = 1.63922993333333356 ;
+weighted_squared_targets_sum = 46.2845898854326663 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *198  ;
+leave_output = 2 [ 27.320498888888892 1 ] ;
+leave_error = 3 [ 3.00002062633182565 0 0.120000000000000009 ] ;
+split_col = 3 ;
+split_balance = 3 ;
+split_feature_value = 2.14775000000000027 ;
+after_split_error = 1.60150674033000873 ;
+missing_node = *0 ;
+missing_leave = *199 ->RegressionTreeLeave(
+id = 182 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *200 ->RegressionTreeLeave(
+id = 183 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 30.6246100000000006 ;
+weighted_targets_sum = 0.204164066666666616 ;
+weighted_squared_targets_sum = 6.25244491768066712 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *201 ->RegressionTreeLeave(
+id = 184 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 8 ;
+weights_sum = 0.0533333333333333368 ;
+targets_sum = 215.25988000000001 ;
+weighted_targets_sum = 1.43506586666666669 ;
+weighted_squared_targets_sum = 40.0321449677520036 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *198   )
+;
+left_leave = *190  ;
+right_node = *202 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *203 ->RegressionTreeLeave(
 id = 37 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 286 ;
-weights_sum = 0.0913154533844186128 ;
-targets_sum = 2904 ;
-weighted_targets_sum = 0.927203065134099558 ;
-weighted_squared_targets_sum = 9.83077905491701109 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 13 ;
+weights_sum = 0.0866666666666666696 ;
+targets_sum = 490.155370000000005 ;
+weighted_targets_sum = 3.26770246666666697 ;
+weighted_squared_targets_sum = 129.937640964069999 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *41  ;
-leave_output = 2 [ 10.1538461538461853 1 ] ;
-leave_error = 3 [ 0.832203556341481798 0 0.182630906768837226 ] ;
+train_set = *8  ;
+leave = *203  ;
+leave_output = 2 [ 37.7042592307692317 1 ] ;
+leave_error = 3 [ 13.4626801436918626 0 0.173333333333333339 ] ;
 split_col = 3 ;
-split_balance = 176 ;
-split_feature_value = 0.152499999999999997 ;
-after_split_error = 0.76663404709379912 ;
+split_balance = 1 ;
+split_feature_value = -0.349470000000000003 ;
+after_split_error = 5.07508760070883014 ;
 missing_node = *0 ;
-missing_leave = *42 ->RegressionTreeLeave(
-id = 50 ;
+missing_leave = *204 ->RegressionTreeLeave(
+id = 98 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3795,18 +3336,65 @@
 error = []
  )
 ;
+left_node = *205 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *206 ->RegressionTreeLeave(
+id = 99 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 6 ;
+weights_sum = 0.0400000000000000008 ;
+targets_sum = 181.14364999999998 ;
+weighted_targets_sum = 1.20762433333333341 ;
+weighted_squared_targets_sum = 37.1207787902646729 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *206  ;
+leave_output = 2 [ 30.1906083333333335 1 ] ;
+leave_error = 3 [ 1.32373105759045329 0 0.0800000000000000017 ] ;
+split_col = 3 ;
+split_balance = 2 ;
+split_feature_value = -1.86292999999999997 ;
+after_split_error = 0.274094608278331597 ;
+missing_node = *0 ;
+missing_leave = *207 ->RegressionTreeLeave(
+id = 131 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
 left_node = *0 ;
-left_leave = *43 ->RegressionTreeLeave(
-id = 51 ;
+left_leave = *208 ->RegressionTreeLeave(
+id = 132 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.000319284802043429908 ;
-targets_sum = 9 ;
-weighted_targets_sum = 0.00287356321839083231 ;
-weighted_squared_targets_sum = 0.0258620689655178135 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 26.9559800000000003 ;
+weighted_targets_sum = 0.179706533333333446 ;
+weighted_squared_targets_sum = 4.84416571840267363 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3814,17 +3402,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *44 ->RegressionTreeLeave(
-id = 52 ;
+right_leave = *209 ->RegressionTreeLeave(
+id = 133 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 285 ;
-weights_sum = 0.090996168582375192 ;
-targets_sum = 2895 ;
-weighted_targets_sum = 0.924329501915708396 ;
-weighted_squared_targets_sum = 9.80491698595149153 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = 154.187669999999997 ;
+weighted_targets_sum = 1.02791779999999999 ;
+weighted_squared_targets_sum = 32.2766130718620019 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3832,45 +3420,266 @@
  )
  )
 ;
-right_leave = *41   )
+left_leave = *206  ;
+right_node = *210 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *211 ->RegressionTreeLeave(
+id = 100 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 7 ;
+weights_sum = 0.0466666666666666688 ;
+targets_sum = 309.011719999999968 ;
+weighted_targets_sum = 2.06007813333333356 ;
+weighted_squared_targets_sum = 92.8168621738053332 ;
+loss_function_factor = 2 ;
+output = []
 ;
-left_leave = *33  ;
-right_node = *45 ->RegressionTreeNode(
+error = []
+ )
+;
+train_set = *8  ;
+leave = *211  ;
+leave_output = 2 [ 44.1445314285714332 1 ] ;
+leave_error = 3 [ 3.75135654311842615 0 0.0933333333333333376 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 1.28273500000000018 ;
+after_split_error = 1.16191341589332642 ;
+missing_node = *0 ;
+missing_leave = *212 ->RegressionTreeLeave(
+id = 134 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *213 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *46 ->RegressionTreeLeave(
-id = 31 ;
+verbosity = 2 ;
+leave_template = *214 ->RegressionTreeLeave(
+id = 135 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 269 ;
-weights_sum = 0.0858876117496804592 ;
-targets_sum = 3493 ;
-weighted_targets_sum = 1.11526181353767462 ;
-weighted_squared_targets_sum = 15.5418263090677033 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = 158.331799999999987 ;
+weighted_targets_sum = 1.05554533333333356 ;
+weighted_squared_targets_sum = 42.328464357964009 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *46  ;
-leave_output = 2 [ 12.9851301115241906 1 ] ;
-leave_error = 3 [ 2.12001310373313379 0 0.171775223499360918 ] ;
-split_col = 7 ;
-split_balance = 233 ;
-split_feature_value = 0.364249999999999963 ;
-after_split_error = 1.97259889065841376 ;
+train_set = *8  ;
+leave = *214  ;
+leave_output = 2 [ 39.5829500000000039 1 ] ;
+leave_error = 3 [ 1.09373241179465586 0 0.0533333333333333368 ] ;
+split_col = 1 ;
+split_balance = 0 ;
+split_feature_value = 0.299314999999999998 ;
+after_split_error = 0.240273207153327617 ;
 missing_node = *0 ;
-missing_leave = *47 ->RegressionTreeLeave(
+missing_leave = *215 ->RegressionTreeLeave(
+id = 203 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *216 ->RegressionTreeLeave(
+id = 204 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = 46.3773499999999856 ;
+weighted_targets_sum = 0.309182333333333503 ;
+weighted_squared_targets_sum = 14.3390572868166686 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *217 ->RegressionTreeLeave(
+id = 205 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = 111.954450000000008 ;
+weighted_targets_sum = 0.74636300000000011 ;
+weighted_squared_targets_sum = 27.9894070711473368 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *214  ;
+right_node = *218 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *219 ->RegressionTreeLeave(
+id = 136 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = 150.679919999999981 ;
+weighted_targets_sum = 1.0045328 ;
+weighted_squared_targets_sum = 50.4883978158413385 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *219  ;
+leave_output = 2 [ 50.2266399999999962 1 ] ;
+leave_error = 3 [ 0.0681810040986781524 0 0.0400000000000000008 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 2.04712499999999986 ;
+after_split_error = 0.00392131309066329203 ;
+missing_node = *0 ;
+missing_leave = *220 ->RegressionTreeLeave(
+id = 206 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *221 ->RegressionTreeLeave(
+id = 207 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = 48.4341599999999914 ;
+weighted_targets_sum = 0.32289439999999997 ;
+weighted_squared_targets_sum = 15.6391190327040039 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *222 ->RegressionTreeLeave(
+id = 208 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = 102.24575999999999 ;
+weighted_targets_sum = 0.681638399999999978 ;
+weighted_squared_targets_sum = 34.8492787831373292 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *219   )
+;
+right_leave = *211   )
+;
+right_leave = *203   )
+;
+left_leave = *187  ;
+right_node = *223 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *224 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 22 ;
+weights_sum = 0.14666666666666664 ;
+targets_sum = 1607.00448999999981 ;
+weighted_targets_sum = 10.7133632666666685 ;
+weighted_squared_targets_sum = 822.845936143814129 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *224  ;
+leave_output = 2 [ 73.0456586363636688 1 ] ;
+leave_error = 3 [ 80.5625202390456536 0 0.293333333333333279 ] ;
+split_col = 1 ;
+split_balance = 6 ;
+split_feature_value = 0.756380000000000052 ;
+after_split_error = 32.2963206990872962 ;
+missing_node = *0 ;
+missing_leave = *225 ->RegressionTreeLeave(
 id = 38 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3882,18 +3691,112 @@
 error = []
  )
 ;
-left_node = *0 ;
-left_leave = *48 ->RegressionTreeLeave(
+left_node = *226 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *227 ->RegressionTreeLeave(
 id = 39 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 14 ;
+weights_sum = 0.0933333333333333376 ;
+targets_sum = 886.886090000000081 ;
+weighted_targets_sum = 5.91257393333333425 ;
+weighted_squared_targets_sum = 386.540563219147373 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *227  ;
+leave_output = 2 [ 63.3490064285714354 1 ] ;
+leave_error = 3 [ 23.969758214020132 0 0.186666666666666675 ] ;
+split_col = 3 ;
+split_balance = 10 ;
+split_feature_value = -2.19003499999999995 ;
+after_split_error = 9.66110139106137744 ;
+missing_node = *0 ;
+missing_leave = *228 ->RegressionTreeLeave(
+id = 59 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *229 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *230 ->RegressionTreeLeave(
+id = 60 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = 83.8064699999999903 ;
+weighted_targets_sum = 0.558709800000000034 ;
+weighted_squared_targets_sum = 23.4124120810993332 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *230  ;
+leave_output = 2 [ 41.9032350000000022 1 ] ;
+leave_error = 3 [ 0.00132806979265908537 0 0.00132806979265908537 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = -3.1773049999999996 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *231 ->RegressionTreeLeave(
+id = 101 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *232 ->RegressionTreeLeave(
+id = 102 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.000319284802043429908 ;
-targets_sum = 11 ;
-weighted_targets_sum = 0.00351213282247765483 ;
-weighted_squared_targets_sum = 0.0386334610472539378 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 41.6800699999999935 ;
+weighted_targets_sum = 0.277867133333333349 ;
+weighted_squared_targets_sum = 11.5815215680326666 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3901,17 +3804,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *49 ->RegressionTreeLeave(
-id = 40 ;
+right_leave = *233 ->RegressionTreeLeave(
+id = 103 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 268 ;
-weights_sum = 0.0855683269476370384 ;
-targets_sum = 3482 ;
-weighted_targets_sum = 1.11174968071519653 ;
-weighted_squared_targets_sum = 15.5031928480204844 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 42.1263999999999967 ;
+weighted_targets_sum = 0.280842666666666685 ;
+weighted_squared_targets_sum = 11.8308905130666666 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -3919,45 +3822,90 @@
  )
  )
 ;
-right_leave = *46   )
+left_leave = *230  ;
+right_node = *234 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *235 ->RegressionTreeLeave(
+id = 61 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 12 ;
+weights_sum = 0.0800000000000000017 ;
+targets_sum = 803.079619999999977 ;
+weighted_targets_sum = 5.35386413333333344 ;
+weighted_squared_targets_sum = 363.128151138048054 ;
+loss_function_factor = 2 ;
+output = []
 ;
-left_leave = *30  ;
-right_node = *50 ->RegressionTreeNode(
+error = []
+ )
+;
+train_set = *8  ;
+leave = *235  ;
+leave_output = 2 [ 66.92330166666666 1 ] ;
+leave_error = 3 [ 9.65977332126890786 0 0.160000000000000003 ] ;
+split_col = 3 ;
+split_balance = 4 ;
+split_feature_value = 2.82453999999999983 ;
+after_split_error = 2.91998118893811931 ;
+missing_node = *0 ;
+missing_leave = *236 ->RegressionTreeLeave(
+id = 104 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *237 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *51 ->RegressionTreeLeave(
-id = 19 ;
+verbosity = 2 ;
+leave_template = *238 ->RegressionTreeLeave(
+id = 105 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 877 ;
-weights_sum = 0.280012771392084414 ;
-targets_sum = 9102 ;
-weighted_targets_sum = 2.90613026819921494 ;
-weighted_squared_targets_sum = 31.4738186462324094 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 8 ;
+weights_sum = 0.0533333333333333368 ;
+targets_sum = 498.671859999999981 ;
+weighted_targets_sum = 3.32447906666666659 ;
+weighted_squared_targets_sum = 208.09516304127601 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *51  ;
-leave_output = 2 [ 10.378563283922297 1 ] ;
-leave_error = 3 [ 2.62472349280956019 0 0.560025542784168828 ] ;
-split_col = 7 ;
-split_balance = 233 ;
-split_feature_value = 0.292749999999999955 ;
-after_split_error = 2.30803457130376533 ;
+train_set = *8  ;
+leave = *238  ;
+leave_output = 2 [ 62.3339824999999976 1 ] ;
+leave_error = 3 [ 1.73428615611938763 0 0.106666666666666674 ] ;
+split_col = 3 ;
+split_balance = 6 ;
+split_feature_value = 2.35129499999999991 ;
+after_split_error = 1.20228931743125678 ;
 missing_node = *0 ;
-missing_leave = *52 ->RegressionTreeLeave(
-id = 32 ;
+missing_leave = *239 ->RegressionTreeLeave(
+id = 155 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -3969,42 +3917,80 @@
 error = []
  )
 ;
-left_node = *53 ->RegressionTreeNode(
+left_node = *0 ;
+left_leave = *240 ->RegressionTreeLeave(
+id = 156 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 62.9106799999999993 ;
+weighted_targets_sum = 0.419404533333332941 ;
+weighted_squared_targets_sum = 26.3850243870826695 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *241 ->RegressionTreeLeave(
+id = 157 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 7 ;
+weights_sum = 0.0466666666666666688 ;
+targets_sum = 435.761180000000024 ;
+weighted_targets_sum = 2.90507453333333343 ;
+weighted_squared_targets_sum = 181.710138654193344 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *238  ;
+right_node = *242 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *54 ->RegressionTreeLeave(
-id = 33 ;
+verbosity = 2 ;
+leave_template = *243 ->RegressionTreeLeave(
+id = 106 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 322 ;
-weights_sum = 0.102809706257981762 ;
-targets_sum = 3024 ;
-weighted_targets_sum = 0.965517241379309277 ;
-weighted_squared_targets_sum = 9.29885057471267551 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = 304.407759999999996 ;
+weighted_targets_sum = 2.02938506666666685 ;
+weighted_squared_targets_sum = 155.032988096772016 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *54  ;
-leave_output = 2 [ 9.39130434782610912 1 ] ;
-leave_error = 3 [ 0.462768615692195073 0 0.205619412515963523 ] ;
-split_col = 7 ;
-split_balance = 16 ;
-split_feature_value = 0.259750000000000036 ;
-after_split_error = 0.420542772062299286 ;
+train_set = *8  ;
+leave = *243  ;
+leave_output = 2 [ 76.101939999999999 1 ] ;
+leave_error = 3 [ 1.18569503281867772 0 0.0533333333333333368 ] ;
+split_col = 1 ;
+split_balance = 0 ;
+split_feature_value = 0.592494999999999994 ;
+after_split_error = 0.00355264134659952213 ;
 missing_node = *0 ;
-missing_leave = *55 ->RegressionTreeLeave(
-id = 41 ;
+missing_leave = *244 ->RegressionTreeLeave(
+id = 158 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -4017,17 +4003,17 @@
  )
 ;
 left_node = *0 ;
-left_leave = *56 ->RegressionTreeLeave(
-id = 42 ;
+left_leave = *245 ->RegressionTreeLeave(
+id = 159 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.000319284802043429908 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.00383141762452103227 ;
-weighted_squared_targets_sum = 0.0459770114942528868 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = 80.5889199999999875 ;
+weighted_targets_sum = 0.537259466666666796 ;
+weighted_squared_targets_sum = 43.2971601784426596 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4035,17 +4021,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *57 ->RegressionTreeLeave(
-id = 43 ;
+right_leave = *246 ->RegressionTreeLeave(
+id = 160 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 321 ;
-weights_sum = 0.102490421455938341 ;
-targets_sum = 3012 ;
-weighted_targets_sum = 0.961685823754788838 ;
-weighted_squared_targets_sum = 9.25287356321841159 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = 223.818840000000023 ;
+weighted_targets_sum = 1.49212560000000005 ;
+weighted_squared_targets_sum = 111.735827918329349 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4053,43 +4039,47 @@
  )
  )
 ;
-left_leave = *54  ;
-right_node = *58 ->RegressionTreeNode(
+right_leave = *243   )
+;
+right_leave = *235   )
+;
+left_leave = *227  ;
+right_node = *247 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *59 ->RegressionTreeLeave(
-id = 34 ;
+verbosity = 2 ;
+leave_template = *248 ->RegressionTreeLeave(
+id = 40 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 555 ;
-weights_sum = 0.177203065134101084 ;
-targets_sum = 6078 ;
-weighted_targets_sum = 1.94061302681991688 ;
-weighted_squared_targets_sum = 22.1749680715198565 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 8 ;
+weights_sum = 0.0533333333333333368 ;
+targets_sum = 720.118400000000065 ;
+weighted_targets_sum = 4.80078933333333335 ;
+weighted_squared_targets_sum = 436.305372924666642 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *59  ;
-leave_output = 2 [ 10.9513513513512244 1 ] ;
-leave_error = 3 [ 1.84526595561154205 0 0.354406130268202169 ] ;
-split_col = 5 ;
-split_balance = 437 ;
-split_feature_value = 0.444250000000000034 ;
-after_split_error = 1.69370165776774684 ;
+train_set = *8  ;
+leave = *248  ;
+leave_output = 2 [ 90.0147999999999939 1 ] ;
+leave_error = 3 [ 8.32656248506665619 0 0.106666666666666674 ] ;
+split_col = 2 ;
+split_balance = 4 ;
+split_feature_value = 5.85068500000000036 ;
+after_split_error = 2.39782196026289407 ;
 missing_node = *0 ;
-missing_leave = *60 ->RegressionTreeLeave(
-id = 44 ;
+missing_leave = *249 ->RegressionTreeLeave(
+id = 62 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -4101,18 +4091,65 @@
 error = []
  )
 ;
+left_node = *250 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *251 ->RegressionTreeLeave(
+id = 63 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = 154.203579999999988 ;
+weighted_targets_sum = 1.02802386666666679 ;
+weighted_squared_targets_sum = 79.2874180128546868 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *251  ;
+leave_output = 2 [ 77.1017900000000083 1 ] ;
+leave_error = 3 [ 0.0498754602666810209 0 0.0266666666666666684 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = -1.04825499999999994 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *252 ->RegressionTreeLeave(
+id = 167 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
 left_node = *0 ;
-left_leave = *61 ->RegressionTreeLeave(
-id = 45 ;
+left_leave = *253 ->RegressionTreeLeave(
+id = 168 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.000319284802043443786 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.00383141762452116627 ;
-weighted_squared_targets_sum = 0.0459770114942545938 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 75.7341899999999839 ;
+weighted_targets_sum = 0.504894600000000082 ;
+weighted_squared_targets_sum = 38.2377835663740129 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4120,17 +4157,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *62 ->RegressionTreeLeave(
-id = 46 ;
+right_leave = *254 ->RegressionTreeLeave(
+id = 169 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 554 ;
-weights_sum = 0.17688378033205765 ;
-targets_sum = 6066 ;
-weighted_targets_sum = 1.93678160919539266 ;
-weighted_squared_targets_sum = 22.1289910600255588 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 78.4693900000000042 ;
+weighted_targets_sum = 0.523129266666666704 ;
+weighted_squared_targets_sum = 41.0496344464806739 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4138,47 +4175,134 @@
  )
  )
 ;
-right_leave = *59   )
+left_leave = *251  ;
+right_node = *255 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *256 ->RegressionTreeLeave(
+id = 64 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 6 ;
+weights_sum = 0.0400000000000000008 ;
+targets_sum = 565.914819999999963 ;
+weighted_targets_sum = 3.77276546666666679 ;
+weighted_squared_targets_sum = 357.017954911812012 ;
+loss_function_factor = 2 ;
+output = []
 ;
-right_leave = *51   )
+error = []
+ )
 ;
-left_leave = *27  ;
-right_node = *63 ->RegressionTreeNode(
+train_set = *8  ;
+leave = *256  ;
+leave_output = 2 [ 94.3191366666666653 1 ] ;
+leave_error = 3 [ 2.34794649999640193 0 0.0800000000000000017 ] ;
+split_col = 3 ;
+split_balance = 2 ;
+split_feature_value = 0.139270000000000005 ;
+after_split_error = 1.53879167940953376 ;
+missing_node = *0 ;
+missing_leave = *257 ->RegressionTreeLeave(
+id = 170 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *258 ->RegressionTreeLeave(
+id = 171 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 96.6401699999999408 ;
+weighted_targets_sum = 0.644267800000000279 ;
+weighted_squared_targets_sum = 62.2621497175260004 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *259 ->RegressionTreeLeave(
+id = 172 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = 469.274649999999951 ;
+weighted_targets_sum = 3.12849766666666662 ;
+weighted_squared_targets_sum = 294.755805194285983 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *256   )
+;
+right_leave = *248   )
+;
+right_leave = *224   )
+;
+left_leave = *184  ;
+right_node = *260 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *64 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *261 ->RegressionTreeLeave(
 id = 10 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 351 ;
-weights_sum = 0.112068965517240965 ;
-targets_sum = 4681 ;
-weighted_targets_sum = 1.49457215836526025 ;
-weighted_squared_targets_sum = 21.3911238825032299 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 30 ;
+weights_sum = 0.199999999999999872 ;
+targets_sum = 4479.64080000000013 ;
+weighted_targets_sum = 29.8642719999999997 ;
+weighted_squared_targets_sum = 4771.0040944160155 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *64  ;
-leave_output = 2 [ 13.3361823361823717 1 ] ;
-leave_error = 3 [ 2.91847412792496907 0 0.22413793103448193 ] ;
-split_col = 5 ;
-split_balance = 143 ;
-split_feature_value = 0.588999999999999968 ;
-after_split_error = 2.50366757410772811 ;
+train_set = *8  ;
+leave = *261  ;
+leave_output = 2 [ 149.321360000000084 1 ] ;
+leave_error = 3 [ 623.260767932185786 0 0.399999999999999745 ] ;
+split_col = 1 ;
+split_balance = 8 ;
+split_feature_value = 1.44567500000000004 ;
+after_split_error = 238.043712468330853 ;
 missing_node = *0 ;
-missing_leave = *65 ->RegressionTreeLeave(
-id = 20 ;
+missing_leave = *262 ->RegressionTreeLeave(
+id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -4190,42 +4314,42 @@
 error = []
  )
 ;
-left_node = *66 ->RegressionTreeNode(
+left_node = *263 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *67 ->RegressionTreeLeave(
-id = 21 ;
+verbosity = 2 ;
+leave_template = *264 ->RegressionTreeLeave(
+id = 15 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 104 ;
-weights_sum = 0.0332056194125160201 ;
-targets_sum = 1605 ;
-weighted_targets_sum = 0.512452107279693592 ;
-weighted_squared_targets_sum = 8.37771392081737432 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 19 ;
+weights_sum = 0.126666666666666677 ;
+targets_sum = 2388.4680000000003 ;
+weighted_targets_sum = 15.9231200000000008 ;
+weighted_squared_targets_sum = 2035.13483177585226 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *67  ;
-leave_output = 2 [ 15.4326923076922853 1 ] ;
-leave_error = 3 [ 0.938396453482690629 0 0.0664112388250320401 ] ;
-split_col = 2 ;
-split_balance = 48 ;
-split_feature_value = 0.492499999999999993 ;
-after_split_error = 0.847781570785234084 ;
+train_set = *8  ;
+leave = *264  ;
+leave_output = 2 [ 125.708842105263159 1 ] ;
+leave_error = 3 [ 66.9157077453886728 0 0.253333333333333355 ] ;
+split_col = 1 ;
+split_balance = 5 ;
+split_feature_value = 1.2774350000000001 ;
+after_split_error = 24.1608341688744517 ;
 missing_node = *0 ;
-missing_leave = *68 ->RegressionTreeLeave(
+missing_leave = *265 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -4237,80 +4361,616 @@
 error = []
  )
 ;
-left_node = *0 ;
-left_leave = *69 ->RegressionTreeLeave(
+left_node = *266 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *267 ->RegressionTreeLeave(
 id = 24 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 12 ;
+weights_sum = 0.0800000000000000017 ;
+targets_sum = 1389.4405099999999 ;
+weighted_targets_sum = 9.26293673333333345 ;
+weighted_squared_targets_sum = 1080.890425285226 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *267  ;
+leave_output = 2 [ 115.786709166666668 1 ] ;
+leave_error = 3 [ 16.7309274270517392 0 0.160000000000000003 ] ;
+split_col = 2 ;
+split_balance = 8 ;
+split_feature_value = 16.504294999999999 ;
+after_split_error = 6.78331317461814542 ;
+missing_node = *0 ;
+missing_leave = *268 ->RegressionTreeLeave(
+id = 65 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *269 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *270 ->RegressionTreeLeave(
+id = 66 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 10 ;
+weights_sum = 0.0666666666666666657 ;
+targets_sum = 1122.60447999999997 ;
+weighted_targets_sum = 7.48402986666666781 ;
+weighted_squared_targets_sum = 843.26523310528944 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *270  ;
+leave_output = 2 [ 112.260448000000025 1 ] ;
+leave_error = 3 [ 6.20937485581771398 0 0.133333333333333331 ] ;
+split_col = 1 ;
+split_balance = 8 ;
+split_feature_value = 1.01285500000000006 ;
+after_split_error = 1.72287732250566372 ;
+missing_node = *0 ;
+missing_leave = *271 ->RegressionTreeLeave(
+id = 119 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *272 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *273 ->RegressionTreeLeave(
+id = 120 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.00031928480204342297 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.00415070242656454067 ;
-weighted_squared_targets_sum = 0.0539591315453373105 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 94.8581899999999933 ;
+weighted_targets_sum = 0.632387933333333319 ;
+weighted_squared_targets_sum = 59.9871747338406607 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
+train_set = *8  ;
+leave = *273  ;
+leave_output = 2 [ 94.8581899999999933 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 0 ;
+split_feature_value = 0 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *274 ->RegressionTreeLeave(
+id = 173 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *275 ->RegressionTreeLeave(
+id = 174 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 94.8581899999999933 ;
+weighted_targets_sum = 0.632387933333333319 ;
+weighted_squared_targets_sum = 59.9871747338406607 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
 right_node = *0 ;
-right_leave = *70 ->RegressionTreeLeave(
+right_leave = *276 ->RegressionTreeLeave(
+id = 175 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *273  ;
+right_node = *277 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *278 ->RegressionTreeLeave(
+id = 121 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 9 ;
+weights_sum = 0.0600000000000000047 ;
+targets_sum = 1027.74629000000004 ;
+weighted_targets_sum = 6.85164193333333404 ;
+weighted_squared_targets_sum = 783.278058371448765 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *278  ;
+leave_output = 2 [ 114.194032222222219 1 ] ;
+leave_error = 3 [ 1.72287732250623149 0 0.120000000000000009 ] ;
+split_col = 1 ;
+split_balance = 7 ;
+split_feature_value = 1.03719500000000009 ;
+after_split_error = 1.07265289472230152 ;
+missing_node = *0 ;
+missing_leave = *279 ->RegressionTreeLeave(
+id = 176 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *280 ->RegressionTreeLeave(
+id = 177 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 111.287989999999937 ;
+weighted_targets_sum = 0.741919933333333392 ;
+weighted_squared_targets_sum = 82.566778121600592 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *281 ->RegressionTreeLeave(
+id = 178 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 8 ;
+weights_sum = 0.0533333333333333368 ;
+targets_sum = 916.458299999999895 ;
+weighted_targets_sum = 6.10972200000000054 ;
+weighted_squared_targets_sum = 700.711280249848187 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *278   )
+;
+left_leave = *270  ;
+right_node = *282 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *283 ->RegressionTreeLeave(
+id = 67 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = 266.836029999999994 ;
+weighted_targets_sum = 1.77890686666666675 ;
+weighted_squared_targets_sum = 237.625192179936676 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *283  ;
+leave_output = 2 [ 133.418014999999997 1 ] ;
+leave_error = 3 [ 0.573938318800659264 0 0.0266666666666666684 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 4.0628700000000002 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *284 ->RegressionTreeLeave(
+id = 122 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *285 ->RegressionTreeLeave(
+id = 123 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 138.057269999999988 ;
+weighted_targets_sum = 0.920381800000000028 ;
+weighted_squared_targets_sum = 127.06539866568599 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *286 ->RegressionTreeLeave(
+id = 124 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 128.778760000000005 ;
+weighted_targets_sum = 0.858525066666666725 ;
+weighted_squared_targets_sum = 110.559793514250686 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *283   )
+;
+left_leave = *267  ;
+right_node = *287 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *288 ->RegressionTreeLeave(
 id = 25 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 103 ;
-weights_sum = 0.0328863346104725993 ;
-targets_sum = 1592 ;
-weighted_targets_sum = 0.508301404853129024 ;
-weighted_squared_targets_sum = 8.32375478927203361 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 7 ;
+weights_sum = 0.0466666666666666688 ;
+targets_sum = 999.027489999999943 ;
+weighted_targets_sum = 6.66018326666666649 ;
+weighted_squared_targets_sum = 954.244406490626147 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
+;
+train_set = *8  ;
+leave = *288  ;
+leave_output = 2 [ 142.718212857142845 1 ] ;
+leave_error = 3 [ 7.42990674182374722 0 0.0933333333333333376 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 0.852400000000000047 ;
+after_split_error = 0.383762277940444818 ;
+missing_node = *0 ;
+missing_leave = *289 ->RegressionTreeLeave(
+id = 68 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
  )
 ;
-left_leave = *67  ;
-right_node = *71 ->RegressionTreeNode(
+left_node = *290 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *72 ->RegressionTreeLeave(
-id = 22 ;
+verbosity = 2 ;
+leave_template = *291 ->RegressionTreeLeave(
+id = 69 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 247 ;
-weights_sum = 0.0788633461047252016 ;
-targets_sum = 3076 ;
-weighted_targets_sum = 0.982120051085567769 ;
-weighted_squared_targets_sum = 13.0134099616858769 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = 398.055919999999958 ;
+weighted_targets_sum = 2.65370613333333338 ;
+weighted_squared_targets_sum = 352.214078048060003 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *72  ;
-leave_output = 2 [ 12.4534412955465861 1 ] ;
-leave_error = 3 [ 1.56527112062509044 0 0.157726692209450403 ] ;
-split_col = 7 ;
-split_balance = 147 ;
-split_feature_value = 0.567999999999999949 ;
-after_split_error = 1.28062106566572131 ;
+train_set = *8  ;
+leave = *291  ;
+leave_output = 2 [ 132.685306666666662 1 ] ;
+leave_error = 3 [ 0.212531887024959398 0 0.0400000000000000008 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = 1.33162500000000006 ;
+after_split_error = 0.0504540736025838044 ;
 missing_node = *0 ;
-missing_leave = *73 ->RegressionTreeLeave(
+missing_leave = *292 ->RegressionTreeLeave(
+id = 137 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *293 ->RegressionTreeLeave(
+id = 138 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = 129.886429999999962 ;
+weighted_targets_sum = 0.865909533333333203 ;
+weighted_squared_targets_sum = 112.469897987632677 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *294 ->RegressionTreeLeave(
+id = 139 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = 268.169489999999996 ;
+weighted_targets_sum = 1.78779660000000007 ;
+weighted_squared_targets_sum = 239.74418006042734 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *291  ;
+right_node = *295 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *296 ->RegressionTreeLeave(
+id = 70 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = 600.971569999999929 ;
+weighted_targets_sum = 4.006477133333334 ;
+weighted_squared_targets_sum = 602.030328442565974 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *296  ;
+leave_output = 2 [ 150.242892500000011 1 ] ;
+leave_error = 3 [ 0.171230390915363406 0 0.0533333333333333368 ] ;
+split_col = 1 ;
+split_balance = 2 ;
+split_feature_value = 1.42736499999999999 ;
+after_split_error = 0.0180043344341506151 ;
+missing_node = *0 ;
+missing_leave = *297 ->RegressionTreeLeave(
+id = 140 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *298 ->RegressionTreeLeave(
+id = 141 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = 153.178699999999935 ;
+weighted_targets_sum = 1.02119133333333378 ;
+weighted_squared_targets_sum = 156.424760891266658 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *299 ->RegressionTreeLeave(
+id = 142 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = 447.792869999999994 ;
+weighted_targets_sum = 2.98528580000000021 ;
+weighted_squared_targets_sum = 445.605567551299316 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *296   )
+;
+right_leave = *288   )
+;
+left_leave = *264  ;
+right_node = *300 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *301 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 11 ;
+weights_sum = 0.0733333333333333337 ;
+targets_sum = 2091.17280000000028 ;
+weighted_targets_sum = 13.9411520000000007 ;
+weighted_squared_targets_sum = 2735.86926264016302 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *301  ;
+leave_output = 2 [ 190.106618181818192 1 ] ;
+leave_error = 3 [ 171.12800472294353 0 0.146666666666666667 ] ;
+split_col = 2 ;
+split_balance = 9 ;
+split_feature_value = 24.753779999999999 ;
+after_split_error = 32.3542885699819749 ;
+missing_node = *0 ;
+missing_leave = *302 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -4322,42 +4982,42 @@
 error = []
  )
 ;
-left_node = *74 ->RegressionTreeNode(
+left_node = *303 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *75 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *304 ->RegressionTreeLeave(
 id = 27 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 197 ;
-weights_sum = 0.0628991060025541615 ;
-targets_sum = 2320 ;
-weighted_targets_sum = 0.740740740740740256 ;
-weighted_squared_targets_sum = 9.12771392081738675 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 10 ;
+weights_sum = 0.0666666666666666657 ;
+targets_sum = 1803.79416999999989 ;
+weighted_targets_sum = 12.0252944666666668 ;
+weighted_squared_targets_sum = 2185.29274943564997 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *75  ;
-leave_output = 2 [ 11.7766497461929074 1 ] ;
-leave_error = 3 [ 0.808539328756404108 0 0.125798212005108323 ] ;
-split_col = 7 ;
-split_balance = 71 ;
-split_feature_value = 0.444500000000000006 ;
-after_split_error = 0.761926849363249947 ;
+train_set = *8  ;
+leave = *304  ;
+leave_output = 2 [ 180.379417000000018 1 ] ;
+leave_error = 3 [ 32.3542885699819749 0 0.133333333333333331 ] ;
+split_col = 1 ;
+split_balance = 4 ;
+split_feature_value = 1.77604000000000006 ;
+after_split_error = 6.9334155021134114 ;
 missing_node = *0 ;
-missing_leave = *76 ->RegressionTreeLeave(
-id = 53 ;
+missing_leave = *305 ->RegressionTreeLeave(
+id = 47 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -4369,18 +5029,112 @@
 error = []
  )
 ;
+left_node = *306 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *307 ->RegressionTreeLeave(
+id = 48 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 7 ;
+weights_sum = 0.0466666666666666688 ;
+targets_sum = 1199.38043000000016 ;
+weighted_targets_sum = 7.99586953333333383 ;
+weighted_squared_targets_sum = 1372.27835111403033 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *307  ;
+leave_output = 2 [ 171.340061428571431 1 ] ;
+leave_error = 3 [ 4.53114819570822469 0 0.0933333333333333376 ] ;
+split_col = 1 ;
+split_balance = 3 ;
+split_feature_value = 1.58981499999999998 ;
+after_split_error = 2.24276698607739045 ;
+missing_node = *0 ;
+missing_leave = *308 ->RegressionTreeLeave(
+id = 89 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *309 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *310 ->RegressionTreeLeave(
+id = 90 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = 327.021780000000035 ;
+weighted_targets_sum = 2.18014520000000012 ;
+weighted_squared_targets_sum = 357.150820013889415 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *310  ;
+leave_output = 2 [ 163.510889999999989 1 ] ;
+leave_error = 3 [ 1.34667606532281203 0 0.0266666666666666684 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 2.63016499999999986 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *311 ->RegressionTreeLeave(
+id = 209 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
 left_node = *0 ;
-left_leave = *77 ->RegressionTreeLeave(
-id = 54 ;
+left_leave = *312 ->RegressionTreeLeave(
+id = 210 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.000319284802043429908 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.00383141762452108301 ;
-weighted_squared_targets_sum = 0.045977011494252179 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 156.404530000000022 ;
+weighted_targets_sum = 1.04269686666666672 ;
+weighted_squared_targets_sum = 163.082513363472628 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4388,17 +5142,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *78 ->RegressionTreeLeave(
-id = 55 ;
+right_leave = *313 ->RegressionTreeLeave(
+id = 211 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 196 ;
-weights_sum = 0.0625798212005107407 ;
-targets_sum = 2308 ;
-weighted_targets_sum = 0.736909323116218484 ;
-weighted_squared_targets_sum = 9.08173690932313882 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 170.617250000000013 ;
+weighted_targets_sum = 1.13744833333333339 ;
+weighted_squared_targets_sum = 194.06830665041673 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4406,43 +5160,351 @@
  )
  )
 ;
-left_leave = *75  ;
-right_node = *79 ->RegressionTreeNode(
+left_leave = *310  ;
+right_node = *314 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-leave_template = *80 ->RegressionTreeLeave(
+verbosity = 2 ;
+leave_template = *315 ->RegressionTreeLeave(
+id = 91 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = 872.358650000000011 ;
+weighted_targets_sum = 5.81572433333333372 ;
+weighted_squared_targets_sum = 1015.12753110014091 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *315  ;
+leave_output = 2 [ 174.471730000000008 1 ] ;
+leave_error = 3 [ 0.896090920754980758 0 0.0666666666666666657 ] ;
+split_col = 2 ;
+split_balance = 3 ;
+split_feature_value = 20.3570750000000018 ;
+after_split_error = 0.551595038088242617 ;
+missing_node = *0 ;
+missing_leave = *316 ->RegressionTreeLeave(
+id = 212 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *317 ->RegressionTreeLeave(
+id = 213 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 170.817769999999967 ;
+weighted_targets_sum = 1.13878513333333364 ;
+weighted_squared_targets_sum = 194.524736985152657 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *318 ->RegressionTreeLeave(
+id = 214 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = 701.540880000000016 ;
+weighted_targets_sum = 4.67693920000000052 ;
+weighted_squared_targets_sum = 820.602794114988228 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *315   )
+;
+left_leave = *307  ;
+right_node = *319 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *320 ->RegressionTreeLeave(
+id = 49 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = 604.413739999999962 ;
+weighted_targets_sum = 4.02942493333333385 ;
+weighted_squared_targets_sum = 813.014398321619979 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *320  ;
+leave_output = 2 [ 201.471246666666701 1 ] ;
+leave_error = 3 [ 2.40226730640564057 0 0.0400000000000000008 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = 1.97313999999999989 ;
+after_split_error = 0.166055228482320016 ;
+missing_node = *0 ;
+missing_leave = *321 ->RegressionTreeLeave(
+id = 92 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *322 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *323 ->RegressionTreeLeave(
+id = 93 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = 392.368439999999964 ;
+weighted_targets_sum = 2.61578960000000027 ;
+weighted_squared_targets_sum = 513.259669974353301 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *323  ;
+leave_output = 2 [ 196.18422000000001 1 ] ;
+leave_error = 3 [ 0.166055228482390183 0 0.0266666666666666684 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 2.6831649999999998 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *324 ->RegressionTreeLeave(
+id = 221 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *325 ->RegressionTreeLeave(
+id = 222 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 193.688809999999961 ;
+weighted_targets_sum = 1.29125873333333341 ;
+weighted_squared_targets_sum = 250.102367461440622 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *326 ->RegressionTreeLeave(
+id = 223 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 198.679630000000003 ;
+weighted_targets_sum = 1.32453086666666686 ;
+weighted_squared_targets_sum = 263.157302512912679 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *323  ;
+right_node = *327 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *328 ->RegressionTreeLeave(
+id = 94 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 212.045299999999997 ;
+weighted_targets_sum = 1.41363533333333335 ;
+weighted_squared_targets_sum = 299.754728347266678 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *8  ;
+leave = *328  ;
+leave_output = 2 [ 212.045299999999997 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 0 ;
+split_feature_value = 0 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *329 ->RegressionTreeLeave(
+id = 224 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *330 ->RegressionTreeLeave(
+id = 225 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 212.045299999999997 ;
+weighted_targets_sum = 1.41363533333333335 ;
+weighted_squared_targets_sum = 299.754728347266678 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *331 ->RegressionTreeLeave(
+id = 226 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *328   )
+;
+right_leave = *320   )
+;
+left_leave = *304  ;
+right_node = *332 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *333 ->RegressionTreeLeave(
 id = 28 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 50 ;
-weights_sum = 0.0159642401021711303 ;
-targets_sum = 756 ;
-weighted_targets_sum = 0.241379310344827597 ;
-weighted_squared_targets_sum = 3.88569604086845288 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 287.378629999999987 ;
+weighted_targets_sum = 1.91585753333333342 ;
+weighted_squared_targets_sum = 550.576513204512594 ;
 loss_function_factor = 2 ;
 output = []
 ;
 error = []
  )
 ;
-train_set = *3  ;
-leave = *80  ;
-leave_output = 2 [ 15.1200000000000063 1 ] ;
-leave_error = 3 [ 0.472081736909316374 0 0.0319284802043422605 ] ;
-split_col = 5 ;
-split_balance = 6 ;
-split_feature_value = 0.770000000000000018 ;
-after_split_error = 0.329980842911872685 ;
+train_set = *8  ;
+leave = *333  ;
+leave_output = 2 [ 287.378629999999987 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 0 ;
+split_feature_value = 0 ;
+after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *81 ->RegressionTreeLeave(
-id = 56 ;
+missing_leave = *334 ->RegressionTreeLeave(
+id = 50 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 0 ;
 weights_sum = 0 ;
 targets_sum = 0 ;
@@ -4455,17 +5517,17 @@
  )
 ;
 left_node = *0 ;
-left_leave = *82 ->RegressionTreeLeave(
-id = 57 ;
+left_leave = *335 ->RegressionTreeLeave(
+id = 51 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
+verbosity = 2 ;
+train_set = *8  ;
 length = 1 ;
-weights_sum = 0.00031928480204342297 ;
-targets_sum = 19 ;
-weighted_targets_sum = 0.00606641123882503252 ;
-weighted_squared_targets_sum = 0.115261813537675867 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 287.378629999999987 ;
+weighted_targets_sum = 1.91585753333333342 ;
+weighted_squared_targets_sum = 550.576513204512594 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4473,17 +5535,17 @@
  )
 ;
 right_node = *0 ;
-right_leave = *83 ->RegressionTreeLeave(
-id = 58 ;
+right_leave = *336 ->RegressionTreeLeave(
+id = 52 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
-verbosity = 0 ;
-train_set = *3  ;
-length = 49 ;
-weights_sum = 0.015644955300127706 ;
-targets_sum = 737 ;
-weighted_targets_sum = 0.23531289910600256 ;
-weighted_squared_targets_sum = 3.7704342273307776 ;
+verbosity = 2 ;
+train_set = *8  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4491,36 +5553,104 @@
  )
  )
 ;
-right_leave = *80   )
+right_leave = *333   )
 ;
-right_leave = *72   )
+right_leave = *301   )
 ;
-right_leave = *64   )
+right_leave = *261   )
 ;
-right_leave = *24   )
+right_leave = *181   )
 ;
-priority_queue = *84 ->RegressionTreeQueue(
-verbosity = 0 ;
+priority_queue = *337 ->RegressionTreeQueue(
+verbosity = 2 ;
 maximum_number_of_nodes = 50 ;
-next_available_node = 10 ;
-nodes = 50 [ *18  *13  *45  *58  *79  *40  *53  *35  *74  *66  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
+next_available_node = 35 ;
+nodes = 50 [ *85  *90  *157  *242  *205  *197  *309  *149  *170  *130  *104  *277  *192  *48  *213  *144  *69  *282  *109  *314  *290  *218  *175  *237  *117  *250  *255  *125  *53  *295  *229  *322  *80  *27  *64  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
 ;
-first_leave = *8  ;
-first_leave_output = 2 [ 9.911877394635626 1 ] ;
-first_leave_error = 3 [ 21.4531789022570862 0 2.00000000000010791 ] ;
-split_cols = 9 [ 7 7 7 5 5 7 7 5 7 ] ;
+first_leave = *13  ;
+first_leave_output = 2 [ 16.0069061333332954 1 ] ;
+first_leave_error = 3 [ 17733.6216534378291 0 2.00000000000000488 ] ;
+split_cols = 40 [ 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 2 3 3 3 1 3 3 1 2 1 3 3 3 1 3 1 1 1 1 3 3 ] ;
 random_gen = *0 ;
 seed = 1827 ;
-stage = 10 ;
-n_examples = 3132 ;
-inputsize = 8 ;
+stage = 41 ;
+n_examples = 150 ;
+inputsize = 4 ;
 targetsize = 1 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 1 ;
-nstages = 10 ;
+nstages = 41 ;
 report_progress = 1 ;
-verbosity = 0 ;
+verbosity = 2 ;
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 0 ;
+save_stat_collectors = 1 ;
+save_learners = 0 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 0 ;
+call_forget_in_run = 1 ;
+save_test_costs = 0 ;
+save_test_names = 0 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 0 ;
+enforce_clean_expdir = 1  )
+;
+option_fields = 1 [ "nstages" ] ;
+dont_restart_upon_change = 1 [ "nstages" ] ;
+strategy = 1 [ *338 ->HyperOptimize(
+which_cost = "E[test2.E[mse]]" ;
+min_n_trials = 0 ;
+oracle = *339 ->EarlyStoppingOracle(
+option = "nstages" ;
+values = []
+;
+range = 3 [ 1 61 20 ] ;
+min_value = -3.40282000000000014e+38 ;
+max_value = 3.40282000000000014e+38 ;
+max_degradation = 3.40282000000000014e+38 ;
+relative_max_degradation = -1 ;
+min_improvement = -3.40282000000000014e+38 ;
+relative_min_improvement = -1 ;
+max_degraded_steps = 120 ;
+min_n_steps = 2  )
+;
+provide_tester_expdir = 1 ;
+sub_strategy = []
+;
+rerun_after_sub = 0 ;
+provide_sub_expdir = 1 ;
+splitter = *0  )
+] ;
+provide_strategy_expdir = 1 ;
+save_final_learner = 0 ;
+learner = *6  ;
+provide_learner_expdir = 1 ;
+expdir_append = "" ;
+forward_nstages = 0 ;
+random_gen = *0 ;
+stage = 1 ;
+n_examples = 200 ;
+inputsize = 4 ;
+targetsize = 1 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 0 ;
+nstages = 1 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -12,18 +12,18 @@
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
-nnonmissing_ = 1045 ;
-sumsquarew_ = 1045 ;
-sum_ = 4830.09365816082482 ;
-sumsquare_ = 139626.922815131547 ;
-sumcube_ = 6450152.61848150287 ;
-sumfourth_ = 381367387.208392203 ;
-min_ = 0.000221113583283008883 ;
-max_ = 96.9467455621295642 ;
-agmemin_ = 935 ;
-agemax_ = 998 ;
-first_ = 0.905069393717800819 ;
-last_ = 2.32896910316787231 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 179.545344601616335 ;
+sumsquare_ = 58456.8760552968452 ;
+sumcube_ = 2844509.12695805216 ;
+sumfourth_ = 247849777.418584049 ;
+min_ = 0 ;
+max_ = 128.059475553344555 ;
+agmemin_ = 120 ;
+agemax_ = 16 ;
+first_ = 13.8781581156000211 ;
+last_ = 9.08582363289999861 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -31,16 +31,16 @@
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
-nnonmissing_ = 1045 ;
-sumsquarew_ = 1045 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
 sum_ = 0 ;
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
 min_ = 1 ;
 max_ = 1 ;
-agmemin_ = 1044 ;
-agemax_ = 1044 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
 counts = {};
@@ -50,18 +50,18 @@
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
-nnonmissing_ = 1045 ;
-sumsquarew_ = 1045 ;
-sum_ = -9660.18731632164963 ;
-sumsquare_ = 558507.69126052619 ;
-sumcube_ = -51601220.9478520229 ;
-sumfourth_ = 6101878195.33427525 ;
-min_ = -192.893491124259128 ;
-max_ = 0.999557772833433944 ;
-agmemin_ = 998 ;
-agemax_ = 935 ;
-first_ = -0.810138787435601637 ;
-last_ = -3.65793820633574462 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -359.090689203232671 ;
+sumsquare_ = 233827.504221187381 ;
+sumcube_ = -22756073.0156644173 ;
+sumfourth_ = 3965596438.69734478 ;
+min_ = -255.11895110668911 ;
+max_ = 1 ;
+agmemin_ = 16 ;
+agemax_ = 120 ;
+first_ = -26.7563162312000422 ;
+last_ = -17.1716472657999972 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -69,18 +69,18 @@
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
-nnonmissing_ = 1045 ;
-sumsquarew_ = 1045 ;
-sum_ = -1617.89347877840351 ;
-sumsquare_ = 13163.6340431307199 ;
-sumcube_ = -110813.165274897692 ;
-sumfourth_ = 1200028.692267237 ;
-min_ = -18.6923076923076295 ;
-max_ = 0.970260223048381221 ;
-agmemin_ = 998 ;
-agemax_ = 935 ;
-first_ = -0.902702702702448789 ;
-last_ = -2.05219206680567368 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 192.890804746032558 ;
+sumsquare_ = 3592.51670061680807 ;
+sumcube_ = 2141.61368714041009 ;
+sumfourth_ = 201415.693085262843 ;
+min_ = -21.6326733333333436 ;
+max_ = 1 ;
+agmemin_ = 16 ;
+agemax_ = 120 ;
+first_ = -6.45068000000000552 ;
+last_ = -5.02853999999999957 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,4 @@
+mse	0
+base_confidence	0
+base_reward_l2	0
+base_reward_l1	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,2 @@
+out0	0
+out1	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -0,0 +1,100 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 4 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 10974.0895327640446 ;
+sumsquare_ = 76330936.5701396614 ;
+sumcube_ = 569842056881.908203 ;
+sumfourth_ = 4522877790823688 ;
+min_ = 0.650915482711108107 ;
+max_ = 8311.49117606439722 ;
+agmemin_ = 17 ;
+agemax_ = 35 ;
+first_ = 143.301489139599965 ;
+last_ = 138.08415022370167 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -21948.1790655280893 ;
+sumsquare_ = 305323746.280558646 ;
+sumcube_ = -4558736455055.26562 ;
+sumfourth_ = 72366044653179008 ;
+min_ = -16621.9823521287944 ;
+max_ = -0.301830965422216213 ;
+agmemin_ = 35 ;
+agemax_ = 17 ;
+first_ = -285.602978279199931 ;
+last_ = -275.16830044740334 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = -61.8945591190477131 ;
+sumsquare_ = 40932.6337231527941 ;
+sumcube_ = -4578700.6447631754 ;
+sumfourth_ = 688955792.048339963 ;
+min_ = -181.33475999999996 ;
+max_ = -0.61358666666666295 ;
+agmemin_ = 35 ;
+agemax_ = 17 ;
+first_ = -22.9417199999999966 ;
+last_ = -22.5018425000000093 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,29 +1,29 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 8 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
-nnonmissing_ = 3132 ;
-sumsquarew_ = 3132 ;
-sum_ = 10477.381224406141 ;
-sumsquare_ = 613374.62650835875 ;
-sumcube_ = 66193004.0028153807 ;
-sumfourth_ = 11773621014.3345871 ;
-min_ = 0.000221113583283008883 ;
-max_ = 296.643793965316775 ;
-agmemin_ = 3124 ;
-agemax_ = 2896 ;
-first_ = 2.17239290275085128 ;
-last_ = 2.32896910316787231 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 15.0751270796107963 ;
+max_ = 15.0751270796107963 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 15.0751270796107963 ;
+last_ = 15.0751270796107963 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -31,16 +31,16 @@
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
-nnonmissing_ = 3132 ;
-sumsquarew_ = 3132 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
 sum_ = 0 ;
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
 min_ = 1 ;
 max_ = 1 ;
-agmemin_ = 3131 ;
-agemax_ = 3131 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 1 ;
 last_ = 1 ;
 counts = {};
@@ -50,18 +50,18 @@
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
-nnonmissing_ = 3132 ;
-sumsquarew_ = 3132 ;
-sum_ = -20954.7624488122819 ;
-sumsquare_ = 2453498.506033435 ;
-sumcube_ = -529544032.022523046 ;
-sumfourth_ = 188377936229.353394 ;
-min_ = -592.287587930633549 ;
-max_ = 0.999557772833433944 ;
-agmemin_ = 2896 ;
-agemax_ = 3124 ;
-first_ = -3.34478580550170257 ;
-last_ = -3.65793820633574462 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = -29.1502541592215927 ;
+max_ = -29.1502541592215927 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = -29.1502541592215927 ;
+last_ = -29.1502541592215927 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -69,20 +69,96 @@
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
-nnonmissing_ = 3132 ;
-sumsquarew_ = 3132 ;
-sum_ = -1284.71671482389047 ;
-sumsquare_ = 34335.3286498939269 ;
-sumcube_ = -311184.047184544965 ;
-sumfourth_ = 4951313.62387488503 ;
-min_ = -33.4467005076141817 ;
-max_ = 0.970260223048381221 ;
-agmemin_ = 2896 ;
-agemax_ = 3124 ;
-first_ = -1.94780793319432632 ;
-last_ = -2.05219206680567368 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = -5.1647413016931214 ;
+max_ = -5.1647413016931214 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = -5.1647413016931214 ;
+last_ = -5.1647413016931214 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 362.783279794880855 ;
+max_ = 362.783279794880855 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 362.783279794880855 ;
+last_ = 362.783279794880855 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 1 ;
+last_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = -724.56655958976171 ;
+max_ = -724.56655958976171 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = -724.56655958976171 ;
+last_ = -724.56655958976171 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = -24.1796111823809525 ;
+max_ = -24.1796111823809525 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = -24.1796111823809525 ;
+last_ = -24.1796111823809525 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,40 +1,103 @@
-*5 -> PTester(
+*10 -> PTester(
     dataset = *1 -> AutoVMatrix(
-        inputsize = 8,
-        specification = "PLEARNDIR:examples/data/uci_mldb/abalone_all.vmat",
+        inputsize = 4,
+        specification = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat",
         targetsize = 1,
         weightsize = 0
         ),
     expdir = "expdir",
-    learner = *3 -> RegressionTree(
-        complexity_penalty_factor = 0.0,
-        compute_train_stats = 1,
-        forget_when_training_set_changes = 1,
-        leave_template = *2 -> RegressionTreeLeave( ),
-        loss_function_weight = 1,
-        maximum_number_of_nodes = 50,
-        nstages = 10,
+    learner = *8 -> HyperLearner(
+        dont_restart_upon_change = [ "nstages" ],
+        forget_when_training_set_changes = 0,
+        learner = *3 -> RegressionTree(
+            complexity_penalty_factor = 0.0,
+            compute_train_stats = 1,
+            forget_when_training_set_changes = 1,
+            leave_template = *2 -> RegressionTreeLeave( ),
+            loss_function_weight = 1,
+            maximum_number_of_nodes = 50,
+            nstages = 10,
+            report_progress = 1,
+            verbosity = 2
+            ),
+        nstages = 1,
+        option_fields = [ "nstages" ],
+        provide_learner_expdir = 1,
+        provide_strategy_expdir = 1,
         report_progress = 1,
-        verbosity = 0
+        save_final_learner = 0,
+        strategy = [
+            *5 -> HyperOptimize(
+                oracle = *4 -> EarlyStoppingOracle(
+                    max_degradation = 3.40282e+38,
+                    max_degraded_steps = 120,
+                    max_value = 3.40282e+38,
+                    min_improvement = -3.40282e+38,
+                    min_n_steps = 2,
+                    min_value = -3.40282e+38,
+                    option = "nstages",
+                    range = [
+                        1,
+                        61,
+                        20
+                        ],
+                    relative_max_degradation = -1,
+                    relative_min_improvement = -1
+                    ),
+                provide_tester_expdir = 1,
+                which_cost = "E[test2.E[mse]]"
+                )
+            ],
+        tester = *7 -> PTester(
+            provide_learner_expdir = 1,
+            report_stats = 1,
+            save_data_sets = 0,
+            save_initial_learners = 0,
+            save_initial_tester = 0,
+            save_learners = 0,
+            save_test_confidence = 0,
+            save_test_costs = 0,
+            save_test_names = 0,
+            save_test_outputs = 0,
+            splitter = *6 -> FractionSplitter(
+                splits = 1 3 [
+                        (0, 0.75),
+                        (0, 0.75),
+                        (0.75, 1)
+                        ]
+                ),
+            statnames = [
+                "E[test1.E[mse]]",
+                "E[test1.E[base_confidence]]",
+                "E[test1.E[base_reward_l2]]",
+                "E[test1.E[base_reward_l1]]",
+                "E[test2.E[mse]]",
+                "E[test2.E[base_confidence]]",
+                "E[test2.E[base_reward_l2]]",
+                "E[test2.E[base_reward_l1]]"
+                ]
+            ),
+        verbosity = 2
         ),
     provide_learner_expdir = 1,
     save_test_confidence = 1,
     save_test_costs = 1,
     save_test_outputs = 1,
-    splitter = *4 -> FractionSplitter(
-        splits = 1 2 [
+    splitter = *9 -> FractionSplitter(
+        splits = 1 3 [
+                (0, 1),
                 (0, 0.75),
                 (0.75, 1)
                 ]
         ),
     statnames = [
-        "E[train.E[mse]]",
-        "E[train.E[base_confidence]]",
-        "E[train.E[base_reward_l2]]",
-        "E[train.E[base_reward_l1]]",
         "E[test1.E[mse]]",
         "E[test1.E[base_confidence]]",
         "E[test1.E[base_reward_l2]]",
-        "E[test1.E[base_reward_l1]]"
+        "E[test1.E[base_reward_l1]]",
+        "E[test2.E[mse]]",
+        "E[test2.E[base_confidence]]",
+        "E[test2.E[base_reward_l2]]",
+        "E[test2.E[base_reward_l1]]"
         ]
     )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,8 +1,8 @@
-E[train.E[mse]]	0
-E[train.E[base_confidence]]	0
-E[train.E[base_reward_l2]]	0
-E[train.E[base_reward_l1]]	0
 E[test1.E[mse]]	0
 E[test1.E[base_confidence]]	0
 E[test1.E[base_reward_l2]]	0
 E[test1.E[base_reward_l1]]	0
+E[test2.E[mse]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8390"
-data                                          = PLEARNDIR:examples/data/uci_mldb/abalone_all.vmat
+__REVISION__ = "PL8398"
+data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,9 +1,9 @@
 splitnum	0
-train.E[mse]	0
-train.E[base_confidence]	0
-train.E[base_reward_l2]	0
-train.E[base_reward_l1]	0
 test1.E[mse]	0
 test1.E[base_confidence]	0
 test1.E[base_reward_l2]	0
 test1.E[base_reward_l1]	0
+test2.E[mse]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,28 +1,42 @@
 PTester(
 expdir = "PYTEST__PL_RegressionTree__RESULTS:expdir/" ;
 dataset = *1 ->AutoVMatrix(
-filename = "UCI_MLDB:abalone_all.vmat" ;
+filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat" ;
 load_in_memory = 0 ;
 writable = 0 ;
-length = 4177 ;
-width = 9 ;
-inputsize = 8 ;
+length = 200 ;
+width = 6 ;
+inputsize = 4 ;
 targetsize = 1 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = "UCI_MLDB:abalone_all.vmat.metadata/"  )
+metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat.metadata/"  )
 ;
 splitter = *2 ->FractionSplitter(
 round_to_closest = 0 ;
-splits = 1  2  [ 
-(0 , 0.75 )	(0.75 , 1 )	
+splits = 1  3  [ 
+(0 , 1 )	(0 , 0.75 )	(0.75 , 1 )	
 ]
  )
 ;
-statnames = 8 [ "E[train.E[mse]]" "E[train.E[base_confidence]]" "E[train.E[base_reward_l2]]" "E[train.E[base_reward_l1]]" "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" ] ;
+statnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
 statmask = []
 ;
-learner = *3 ->RegressionTree(
+learner = *3 ->HyperLearner(
+tester = *4 ->PTester(
+expdir = "" ;
+dataset = *0 ;
+splitter = *5 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 0.75 )	(0 , 0.75 )	(0.75 , 1 )	
+]
+ )
+;
+statnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *6 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 50 ;
@@ -30,7 +44,7 @@
 complexity_penalty_factor = 0 ;
 multiclass_outputs = []
 ;
-leave_template = *4 ->RegressionTreeLeave(
+leave_template = *7 ->RegressionTreeLeave(
 id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -67,7 +81,7 @@
 forget_when_training_set_changes = 1 ;
 nstages = 10 ;
 report_progress = 1 ;
-verbosity = 0 ;
+verbosity = 2 ;
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
@@ -75,6 +89,74 @@
 ;
 perf_evaluators = {};
 report_stats = 1 ;
+save_initial_tester = 0 ;
+save_stat_collectors = 1 ;
+save_learners = 0 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 0 ;
+call_forget_in_run = 1 ;
+save_test_costs = 0 ;
+save_test_names = 0 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 0 ;
+enforce_clean_expdir = 1  )
+;
+option_fields = 1 [ "nstages" ] ;
+dont_restart_upon_change = 1 [ "nstages" ] ;
+strategy = 1 [ *8 ->HyperOptimize(
+which_cost = "E[test2.E[mse]]" ;
+min_n_trials = 0 ;
+oracle = *9 ->EarlyStoppingOracle(
+option = "nstages" ;
+values = []
+;
+range = 3 [ 1 61 20 ] ;
+min_value = -3.40282000000000014e+38 ;
+max_value = 3.40282000000000014e+38 ;
+max_degradation = 3.40282000000000014e+38 ;
+relative_max_degradation = -1 ;
+min_improvement = -3.40282000000000014e+38 ;
+relative_min_improvement = -1 ;
+max_degraded_steps = 120 ;
+min_n_steps = 2  )
+;
+provide_tester_expdir = 1 ;
+sub_strategy = []
+;
+rerun_after_sub = 0 ;
+provide_sub_expdir = 1 ;
+splitter = *0  )
+] ;
+provide_strategy_expdir = 1 ;
+save_final_learner = 0 ;
+learner = *6  ;
+provide_learner_expdir = 1 ;
+expdir_append = "" ;
+forward_nstages = 0 ;
+random_gen = *0 ;
+stage = 0 ;
+n_examples = -1 ;
+inputsize = -1 ;
+targetsize = -1 ;
+weightsize = -1 ;
+forget_when_training_set_changes = 0 ;
+nstages = 1 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
 save_initial_tester = 1 ;
 save_stat_collectors = 1 ;
 save_learners = 1 ;
@@ -83,6 +165,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/train_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/train_cost_names.txt	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/train_cost_names.txt	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,4 +1,8 @@
-mse
-base_confidence
-base_reward_l2
-base_reward_l1
+E[test1.E[mse]]
+E[test1.E[base_confidence]]
+E[test1.E[base_reward_l2]]
+E[test1.E[base_reward_l1]]
+E[test2.E[mse]]
+E[test2.E[base_confidence]]
+E[test2.E[base_reward_l2]]
+E[test2.E[base_reward_l1]]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-01-23 17:01:39 UTC (rev 8406)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-01-23 17:18:21 UTC (rev 8407)
@@ -1,17 +1,26 @@
 import os.path
 from plearn.pyplearn import *
 
-plarg_defaults.data    = "PLEARNDIR:examples/data/uci_mldb/abalone_all.vmat"
+plarg_defaults.data    = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat"
 
 dataset = pl.AutoVMatrix(
     specification = plargs.data,
-    inputsize = 8,
+    inputsize = 4,
     targetsize = 1,
     weightsize = 0
     )
 
-#learner = pl.RegresssionTree()
-learner = pl.RegressionTree(
+learner = pl.HyperLearner(
+    option_fields = [ "nstages" ],
+    dont_restart_upon_change = [ "nstages" ] ,
+    provide_strategy_expdir = 1 ,
+    save_final_learner = 0 ,
+    provide_learner_expdir = 1 ,
+    forget_when_training_set_changes = 0 ,
+    nstages = 1 ,
+    report_progress = 1 ,
+    verbosity = 2 ,
+    learner = pl.RegressionTree(
         nstages = 10
         ,loss_function_weight = 1
 #        ,missing_is_valid = 0
@@ -19,22 +28,78 @@
         ,maximum_number_of_nodes = 50
         ,compute_train_stats = 1
         ,complexity_penalty_factor = 0.0
-        ,verbosity = 0
+        ,verbosity = 2
         ,report_progress = 1
         ,forget_when_training_set_changes = 1
 #        ,conf_rated_adaboost = 0
         ,leave_template = pl.RegressionTreeLeave( )
-        )
+        ),
+    tester = pl.PTester(
+    splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,0.75), (0,.75), (0.75,1) ])),
+    statnames = [ #'E[train.E[mse]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[mse]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]',
+                 'E[test2.E[mse]]',  'E[test2.E[base_confidence]]',  'E[test2.E[base_reward_l2]]',  'E[test2.E[base_reward_l1]]'],
+    save_test_outputs = 0 ,
+    report_stats = 1  ,
+    save_initial_tester = 0 ,
+    save_learners = 0 ,
+    save_initial_learners = 0  ,
+    save_data_sets = 0  ,
+    save_test_costs = 0  ,
+    provide_learner_expdir = 1  ,
+    save_test_confidence = 0  ,
+    save_test_names = 0,
+    ),
+    strategy = [
+#    HyperOptimize(
+#    which_cost = "E[test2.E[square_class_error]]" ,
+#    min_n_trials = 1 ,
+#    provide_tester_expdir = 1 ,
+#    rerun_after_sub = 0 ,
+#    provide_sub_expdir = 1 ,
+#    oracle =
+#    OptimizeOptionOracle(
+#    option = "learner.base_regressor_template.loss_function_weight" ,
+###    max_steps = 1 ,
+#    start_value = 1 ,
+#    min_value = .5 ,
+#    factor = 1.5 ,
+#    relative_precision = 0.1 ,
+#    start_direction = "up" ,
+#    ) , # end of OptimizeOptionOracle
+#    sub_strategy = [
+
+    pl.HyperOptimize(
+    which_cost = "E[test2.E[mse]]" ,
+    provide_tester_expdir = 1 ,
+    oracle = pl.EarlyStoppingOracle(
+    option = "nstages" ,
+    range = [ 1, 61, 20 ],
+    min_value = -3.40282e+38 ,
+    max_value = 3.40282e+38 ,
+    max_degradation = 3.40282e+38 ,
+    relative_max_degradation = -1 ,
+    min_improvement = -3.40282e+38 ,
+    relative_min_improvement = -1 ,
+    max_degraded_steps = 120 ,
+    min_n_steps = 2 
+    )  # end of EarlyStoppingOracle
+    )  # end of sub_strategy.HyperOptimize
+#    ]  # end of sub_strategy
+#    )  # end of strategy.HyperOptimize
+    ]  # end of HyperLearner strategy
+    )
 splitter = pl.FractionSplitter(
-    splits = TMat(1,2, [ (0,0.75),  (0.75,1) ])
+    splits = TMat(1,3, [ (0,1), (0,0.75), (0.75,1) ])
     )
 tester = pl.PTester(
     expdir = plargs.expdir,
     dataset = dataset,
     splitter = splitter,
     learner = learner,
-    statnames = ['E[train.E[mse]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
-                 'E[test1.E[mse]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]'],
+    statnames = [#'E[train.E[mse]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[mse]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]',
+                 'E[test2.E[mse]]',  'E[test2.E[base_confidence]]',  'E[test2.E[base_reward_l2]]',  'E[test2.E[base_reward_l1]]'],
     provide_learner_expdir = 1,
     save_test_costs = 1,
     save_test_outputs = 1,



From tihocan at mail.berlios.de  Thu Jan 24 18:13:19 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 24 Jan 2008 18:13:19 +0100
Subject: [Plearn-commits] r8408 - trunk/plearn/vmat
Message-ID: <200801241713.m0OHDJOB020820@sheep.berlios.de>

Author: tihocan
Date: 2008-01-24 18:13:19 +0100 (Thu, 24 Jan 2008)
New Revision: 8408

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Added getFieldIndex in the declared methods

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-01-23 17:18:21 UTC (rev 8407)
+++ trunk/plearn/vmat/VMatrix.cc	2008-01-24 17:13:19 UTC (rev 8408)
@@ -187,6 +187,13 @@
         rmm, "fieldNames", &VMatrix::fieldNames,
         (BodyDoc("Returns the field names.\n"),
          RetDoc ("TVec of field names.\n")));
+
+     declareMethod(
+        rmm, "getFieldIndex", &VMatrix::getFieldIndex,
+        (BodyDoc("Returns the index of a field.\n"),
+         ArgDoc ("fname_or_num",
+             "Field name or index (as a string) of the field.\n"),
+         RetDoc ("Index of the field.\n")));
     
     declareMethod(
         rmm, "appendRow", &VMatrix::appendRow,



From larocheh at mail.berlios.de  Thu Jan 24 21:51:19 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 24 Jan 2008 21:51:19 +0100
Subject: [Plearn-commits] r8409 - trunk/plearn_learners_experimental
Message-ID: <200801242051.m0OKpJ5B022248@sheep.berlios.de>

Author: larocheh
Date: 2008-01-24 21:51:19 +0100 (Thu, 24 Jan 2008)
New Revision: 8409

Modified:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
   trunk/plearn_learners_experimental/DiscriminativeRBM.h
Log:
Added the possibility to do Self-taught learning.


Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-24 17:13:19 UTC (rev 8408)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-24 20:51:19 UTC (rev 8409)
@@ -67,7 +67,10 @@
     n_classes( -1 ),
     target_weights_L1_penalty_factor( 0. ),
     target_weights_L2_penalty_factor( 0. ),
-    do_not_use_discriminative_learning( false )
+    do_not_use_discriminative_learning( false ),
+    unlabeled_class_index_begin( 0 ),
+    n_classes_at_test_time( -1 )
+
 {
     random_gen = new PRandom();
 }
@@ -140,6 +143,18 @@
                   OptionBase::buildoption,
                   "Indication that discriminative learning should not be used.\n");
 
+    declareOption(ol, "unlabeled_class_index_begin", 
+                  &DiscriminativeRBM::unlabeled_class_index_begin,
+                  OptionBase::buildoption,
+                  "The smallest index for the classes of the unlabeled data.\n");
+
+    declareOption(ol, "n_classes_at_test_time", 
+                  &DiscriminativeRBM::n_classes_at_test_time,
+                  OptionBase::buildoption,
+                  "The number of classes to discriminate from during test.\n"
+                  "The classes that will be discriminated are indexed\n"
+                  "from 0 to n_classes_at_test_time.\n");
+
     declareOption(ol, "classification_module",
                   &DiscriminativeRBM::classification_module,
                   OptionBase::learntoption,
@@ -193,9 +208,9 @@
     build_classification_cost();
 
     int current_index = 0;
-    cost_names.append("NLL");
-    nll_cost_index = current_index;
-    current_index++;
+    //cost_names.append("NLL");
+    //nll_cost_index = current_index;
+    //current_index++;
     
     cost_names.append("class_error");
     class_cost_index = current_index;
@@ -329,6 +344,62 @@
     joint_layer->sub_layers[1] = target_layer;
     joint_layer->random_gen = random_gen;
     joint_layer->build();
+
+    if( unlabeled_class_index_begin != 0 )
+    {
+        unlabeled_class_output.resize( n_classes - unlabeled_class_index_begin );
+        PP<RBMMultinomialLayer> sub_layer = new RBMMultinomialLayer();
+        sub_layer->bias = target_layer->bias.subVec(
+            unlabeled_class_index_begin,
+            n_classes - unlabeled_class_index_begin);
+        sub_layer->size = n_classes - unlabeled_class_index_begin;
+        sub_layer->random_gen = random_gen;
+        sub_layer->build();
+
+        PP<RBMMatrixConnection> sub_connection = new RBMMatrixConnection();
+        sub_connection->weights = last_to_target->weights.subMatColumns(
+            unlabeled_class_index_begin,
+            n_classes - unlabeled_class_index_begin);
+        sub_connection->up_size = hidden_layer->size;
+        sub_connection->down_size = n_classes - unlabeled_class_index_begin;
+        sub_connection->random_gen = random_gen;
+        sub_connection->build();
+
+        unlabeled_classification_module = new RBMClassificationModule();
+        unlabeled_classification_module->previous_to_last = connection;
+        unlabeled_classification_module->last_layer = hidden_layer;
+        unlabeled_classification_module->last_to_target = sub_connection;
+        unlabeled_classification_module->target_layer = sub_layer;
+        unlabeled_classification_module->random_gen = random_gen;
+        unlabeled_classification_module->build();
+    }
+
+    if( n_classes_at_test_time > 0 && n_classes_at_test_time != n_classes )
+    {
+        test_time_class_output.resize( n_classes_at_test_time ); 
+        PP<RBMMultinomialLayer> sub_layer = new RBMMultinomialLayer();
+        sub_layer->bias = target_layer->bias.subVec(
+            0, n_classes_at_test_time );
+        sub_layer->size = n_classes_at_test_time;
+        sub_layer->random_gen = random_gen;
+        sub_layer->build();
+
+        PP<RBMMatrixConnection> sub_connection = new RBMMatrixConnection();
+        sub_connection->weights = last_to_target->weights.subMatColumns(
+            0, n_classes_at_test_time );
+        sub_connection->up_size = hidden_layer->size;
+        sub_connection->down_size = n_classes_at_test_time;
+        sub_connection->random_gen = random_gen;
+        sub_connection->build();
+
+        test_time_classification_module = new RBMClassificationModule();
+        test_time_classification_module->previous_to_last = connection;
+        test_time_classification_module->last_layer = hidden_layer;
+        test_time_classification_module->last_to_target = sub_connection;
+        test_time_classification_module->target_layer = sub_layer;
+        test_time_classification_module->random_gen = random_gen;
+        test_time_classification_module->build();
+    }
 }
 
 ///////////
@@ -358,6 +429,8 @@
     deepCopyField(last_to_target_connection, copies);
     deepCopyField(joint_connection, copies);
     deepCopyField(target_layer, copies);
+    deepCopyField(unlabeled_classification_module, copies);
+    deepCopyField(test_time_classification_module, copies);
     deepCopyField(target_one_hot, copies);
     deepCopyField(disc_pos_down_val, copies);
     deepCopyField(disc_pos_up_val, copies);
@@ -373,6 +446,8 @@
     deepCopyField(semi_sup_neg_up_val, copies);
     deepCopyField(input_gradient, copies);
     deepCopyField(class_output, copies);
+    deepCopyField(unlabeled_class_output, copies);
+    deepCopyField(test_time_class_output, copies);
     deepCopyField(class_gradient, copies);
 }
 
@@ -382,7 +457,7 @@
 ////////////////
 int DiscriminativeRBM::outputsize() const
 {
-    return n_classes;
+    return n_classes_at_test_time > 0 ? n_classes_at_test_time : n_classes;
 }
 
 ////////////
@@ -562,8 +637,20 @@
             // Positive phase
 
             // Clamp visible units and sample from p(y|x)
-            classification_module->fprop( input,
-                                          class_output );
+            if( unlabeled_classification_module )
+            {
+                unlabeled_classification_module->fprop( input,
+                                                        unlabeled_class_output );
+                class_output.clear();
+                class_output.subVec( unlabeled_class_index_begin,
+                                     n_classes - unlabeled_class_index_begin )
+                    << unlabeled_class_output;
+            }
+            else
+            {
+                classification_module->fprop( input,
+                                              class_output );
+            }
             target_layer->setExpectation( class_output );
             target_layer->generateSample();            
             input_layer->sample << input ;
@@ -608,7 +695,7 @@
                                                     nll_cost );
 
             class_error =  ( argmax(class_output) == target_index ) ? 0: 1;  
-            train_costs[nll_cost_index] = nll_cost;
+            //train_costs[nll_cost_index] = nll_cost;
             train_costs[class_cost_index] = class_error;
 
             classification_cost->bpropUpdate( class_output, target, nll_cost,
@@ -664,7 +751,16 @@
 {
     // Compute the output from the input.
     output.resize(0);
-    classification_module->fprop( input, output );
+    if( test_time_classification_module )
+    {
+        test_time_classification_module->fprop( input,
+                                                output );
+    }
+    else
+    {
+        classification_module->fprop( input,
+                                      output );
+    }
 }
 
 
@@ -679,7 +775,7 @@
     if( !is_missing(target[0]) )
     {
         //classification_cost->fprop( output, target, costs[nll_cost_index] );
-        classification_cost->CostModule::fprop( output, target, costs[nll_cost_index] );
+        //classification_cost->CostModule::fprop( output, target, costs[nll_cost_index] );
         costs[class_cost_index] =
             (argmax(output) == (int) round(target[0]))? 0 : 1;
     }

Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.h
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-01-24 17:13:19 UTC (rev 8408)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-01-24 20:51:19 UTC (rev 8409)
@@ -106,6 +106,14 @@
     //! Indication that discriminative learning should not be used
     bool do_not_use_discriminative_learning;
 
+    //! The smallest index for the classes of the unlabeled data
+    int unlabeled_class_index_begin;
+
+    //! The number of classes to discriminate from during test.
+    //! The classes that will be discriminated are indexed 
+    //! from 0 to n_classes_at_test_time.
+    int n_classes_at_test_time;
+
     //#####  Public Learnt Options  ###########################################
     //! The module computing the probabilities of the different classes.
     PP<RBMClassificationModule> classification_module;
@@ -216,6 +224,12 @@
     //! (pointer to classification_module->target_layer)
     PP<RBMMultinomialLayer> target_layer;
 
+    //! Classification module for when unlabeled_class_index_begin != 0
+    PP<RBMClassificationModule> unlabeled_classification_module;
+
+    //! Classification module for when n_classes_at_test_time != n_classes
+    PP<RBMClassificationModule> test_time_classification_module;
+
     //! Temporary variables for Contrastive Divergence
     mutable Vec target_one_hot;
 
@@ -237,6 +251,8 @@
     //! Temporary variables for gradient descent
     mutable Vec input_gradient;
     mutable Vec class_output;
+    mutable Vec unlabeled_class_output;
+    mutable Vec test_time_class_output;
     mutable Vec class_gradient;
 
     //! Keeps the index of the NLL cost in train_costs



From ducharme at mail.berlios.de  Fri Jan 25 17:07:12 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 25 Jan 2008 17:07:12 +0100
Subject: [Plearn-commits] r8410 - trunk/python_modules/plearn/utilities
Message-ID: <200801251607.m0PG7CbM029467@sheep.berlios.de>

Author: ducharme
Date: 2008-01-25 17:07:11 +0100 (Fri, 25 Jan 2008)
New Revision: 8410

Modified:
   trunk/python_modules/plearn/utilities/options_dialog.py
Log:
Uniformite dans l'appel au logging.


Modified: trunk/python_modules/plearn/utilities/options_dialog.py
===================================================================
--- trunk/python_modules/plearn/utilities/options_dialog.py	2008-01-24 20:51:19 UTC (rev 8409)
+++ trunk/python_modules/plearn/utilities/options_dialog.py	2008-01-25 16:07:11 UTC (rev 8410)
@@ -51,7 +51,7 @@
     """
     logs_pos= -1
     for i in range(len(args)):
-        if args[i].startswith('--named-logging='):
+        if args[i].startswith('--enable-logging='):
             logs_pos= i
     logs= ['__NONE__']
     if logs_pos >= 0:



From ducharme at mail.berlios.de  Fri Jan 25 17:56:29 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 25 Jan 2008 17:56:29 +0100
Subject: [Plearn-commits] r8411 - trunk/plearn/io
Message-ID: <200801251656.m0PGuTvJ000122@sheep.berlios.de>

Author: ducharme
Date: 2008-01-25 17:56:29 +0100 (Fri, 25 Jan 2008)
New Revision: 8411

Modified:
   trunk/plearn/io/PyPLearnScript.cc
Log:
Uniformite dans l'appel au logging.


Modified: trunk/plearn/io/PyPLearnScript.cc
===================================================================
--- trunk/plearn/io/PyPLearnScript.cc	2008-01-25 16:07:11 UTC (rev 8410)
+++ trunk/plearn/io/PyPLearnScript.cc	2008-01-25 16:56:29 UTC (rev 8411)
@@ -140,7 +140,7 @@
         }
     }
 
-    final_args.push_back(string("--named-logging=")
+    final_args.push_back(string("--enable-logging=")
                          + join(PL_Log::instance().namedLogging(), ","));
     final_args.push_back(string("--verbosity=")
                          + tostring(PL_Log::instance().verbosity()))    ;



From nouiz at mail.berlios.de  Fri Jan 25 21:50:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Jan 2008 21:50:11 +0100
Subject: [Plearn-commits] r8412 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200801252050.m0PKoBIx002968@sheep.berlios.de>

Author: nouiz
Date: 2008-01-25 21:50:10 +0100 (Fri, 25 Jan 2008)
New Revision: 8412

Added:
   trunk/scripts/dbi
Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Added a small utility script in the path


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-01-25 16:56:29 UTC (rev 8411)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-01-25 20:50:10 UTC (rev 8412)
@@ -1230,7 +1230,7 @@
 
 def main():
     if len(sys.argv)!=2:
-        print "Usage:dbi.py {Condor|Cluster|Ssh|Local|bqtools} < joblist"
+        print "Usage:%s {Condor|Cluster|Ssh|Local|bqtools} < joblist"%(sys.argv[0])
         print "Where joblist is a file containing one exeperiement by line"
         sys.exit(0)
     DBI([ s[0:-1] for s in sys.stdin.readlines() ], sys.argv[1]).run()

Added: trunk/scripts/dbi
===================================================================
--- trunk/scripts/dbi	2008-01-25 16:56:29 UTC (rev 8411)
+++ trunk/scripts/dbi	2008-01-25 20:50:10 UTC (rev 8412)
@@ -0,0 +1,42 @@
+#!/usr/bin/env python
+
+# pymake
+# Copyright (C) 2001 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+import sys
+from plearn.parallel import dbi
+
+if __name__=='__main__':
+    try:
+        dbi.main()
+    except KeyboardInterrupt:
+        print 'Pymake interrupted by Ctrl-C'
+


Property changes on: trunk/scripts/dbi
___________________________________________________________________
Name: svn:executable
   + *



From nouiz at mail.berlios.de  Mon Jan 28 15:23:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jan 2008 15:23:21 +0100
Subject: [Plearn-commits] r8413 - trunk/plearn/vmat
Message-ID: <200801281423.m0SENLJm020295@sheep.berlios.de>

Author: nouiz
Date: 2008-01-28 15:23:20 +0100 (Mon, 28 Jan 2008)
New Revision: 8413

Modified:
   trunk/plearn/vmat/VMatrix.h
Log:
Corrected comment


Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-01-25 20:50:10 UTC (rev 8412)
+++ trunk/plearn/vmat/VMatrix.h	2008-01-28 14:23:20 UTC (rev 8413)
@@ -597,12 +597,15 @@
     { return getStats()[fieldnum]; }
 
     
-    //! Compare the stats of this VecStatsCollector with the target one.
-    //! @param target the VMat we compare again
-    //! @param stderror_threshold The threshold allowed for the standard error
-    //! @param missing_threshold The threshold allowed for the % of missing
-    //! @return If they are comparable with respect to the gived threshold,
-    //! we return true. Otherwise we return false.
+    /** Compare the stats of this VecStatsCollector with the target one.
+     * @param target the VMat we compare again
+     * @param stderror_threshold The threshold allowed for the standard error
+     * @param missing_threshold The threshold allowed for the % of missing
+     * @param sumdiff_stderr The sum of all variable difference of stderr
+     * @param sumdiff_missing The sum of all variable difference of missing
+     * @return If they are comparable with respect to the gived threshold,
+     * we return true. Otherwise we return false
+     */
     int compareStats(const VMat& target,
                      const real stderror_threshold = 2,
                      const real missing_threshold = 2,



From nouiz at mail.berlios.de  Mon Jan 28 15:40:28 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jan 2008 15:40:28 +0100
Subject: [Plearn-commits] r8414 - trunk/commands
Message-ID: <200801281440.m0SEeSnb021460@sheep.berlios.de>

Author: nouiz
Date: 2008-01-28 15:40:27 +0100 (Mon, 28 Jan 2008)
New Revision: 8414

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added an include


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-01-28 14:23:20 UTC (rev 8413)
+++ trunk/commands/plearn_noblas_inc.h	2008-01-28 14:40:27 UTC (rev 8414)
@@ -296,6 +296,7 @@
 #include <plearn/vmat/DatedJoinVMatrix.h>
 // #include <plearn/vmat/DictionaryVMatrix.h>
 #include <plearn/vmat/DisregardRowsVMatrix.h>
+#include <plearn/vmat/DichotomizeVMatrix.h>
 #include <plearn/vmat/BinaryNumbersVMatrix.h>
 #include <plearn/vmat/ExtractNNetParamsVMatrix.h>
 #include <plearn/vmat/FilteredVMatrix.h>



From nouiz at mail.berlios.de  Mon Jan 28 15:45:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jan 2008 15:45:49 +0100
Subject: [Plearn-commits] r8415 - trunk/commands
Message-ID: <200801281445.m0SEjnnu021792@sheep.berlios.de>

Author: nouiz
Date: 2008-01-28 15:45:48 +0100 (Mon, 28 Jan 2008)
New Revision: 8415

Modified:
   trunk/commands/
Log:
Added file to ignore



Property changes on: trunk/commands
___________________________________________________________________
Name: svn:ignore
   - OBJS
plearn
plearn_curses
plearn_full
plearn_lapack
plearn_light
plearn_noblas
plearn_tests
plearn.lnk
plearn_tests.lnk

   + OBJS
plearn
plearn_curses
plearn_full
plearn_lapack
plearn_light
plearn_noblas
plearn_python
plearn_tests
plearn.lnk
plearn_tests.lnk




From nouiz at mail.berlios.de  Mon Jan 28 15:49:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jan 2008 15:49:50 +0100
Subject: [Plearn-commits] r8416 - trunk/plearn_learners/meta
Message-ID: <200801281449.m0SEnoUY022000@sheep.berlios.de>

Author: nouiz
Date: 2008-01-28 15:49:50 +0100 (Mon, 28 Jan 2008)
New Revision: 8416

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
Should stop after caltulating the stats...


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-01-28 14:45:48 UTC (rev 8415)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-01-28 14:49:50 UTC (rev 8416)
@@ -506,33 +506,6 @@
             cout << "weak learner at stage " << stage 
                  << " has average loss = " << learners_error[stage] << endl;
 
-        // stopping criterion (in addition to n_stages)
-        if (early_stopping && learners_error[stage] >= target_error)
-        {
-            nstages = stage;
-            cout << 
-                "AdaBoost::train early stopping because learner's loss at stage " 
-                 << stage << " is " << learners_error[stage] << endl;       
-            break;
-        }
-
-        if(fast_exact_is_equal(learners_error[stage], 0))
-        {
-            cout << "AdaBoost::train found weak learner with 0 training "
-                 << "error at stage " 
-                 << stage << " is " << learners_error[stage] << endl;  
-
-            // Simulate infinite weight on new_weak_learner
-            weak_learners.resize(0);
-            weak_learners.push_back(new_weak_learner);
-            voting_weights.resize(0);
-            voting_weights.push_back(1);
-            sum_voting_weights = 1;
-            found_zero_error_weak_learner = true;
-            break;
-        }
-
-
         weak_learners.push_back(new_weak_learner);
 
         if (save_often && expdir!="")
@@ -705,6 +678,34 @@
                      << train_stats->getMean() << endl;
      
         }
+
+        if(fast_exact_is_equal(learners_error[stage], 0))
+        {
+            cout << "AdaBoost::train found weak learner with 0 training "
+                 << "error at stage " 
+                 << stage << " is " << learners_error[stage] << endl;  
+
+            // Simulate infinite weight on new_weak_learner
+            weak_learners.resize(0);
+            weak_learners.push_back(new_weak_learner);
+            voting_weights.resize(0);
+            voting_weights.push_back(1);
+            sum_voting_weights = 1;
+            found_zero_error_weak_learner = true;
+            break;
+        }
+
+        // stopping criterion (in addition to n_stages)
+        if (early_stopping && learners_error[stage] >= target_error)
+        {
+            nstages = stage;
+            cout << 
+                "AdaBoost::train early stopping because learner's loss at stage " 
+                 << stage << " is " << learners_error[stage] << endl;       
+            break;
+        }
+
+
     }
 }
 



From saintmlx at mail.berlios.de  Mon Jan 28 18:14:59 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 28 Jan 2008 18:14:59 +0100
Subject: [Plearn-commits] r8417 - trunk/plearn_learners/regressors
Message-ID: <200801281714.m0SHExqE002414@sheep.berlios.de>

Author: saintmlx
Date: 2008-01-28 18:14:58 +0100 (Mon, 28 Jan 2008)
New Revision: 8417

Modified:
   trunk/plearn_learners/regressors/WPLS.cc
Log:
- assert precision > 0 and cosmetic mods.


Modified: trunk/plearn_learners/regressors/WPLS.cc
===================================================================
--- trunk/plearn_learners/regressors/WPLS.cc	2008-01-28 14:49:50 UTC (rev 8416)
+++ trunk/plearn_learners/regressors/WPLS.cc	2008-01-28 17:14:58 UTC (rev 8417)
@@ -144,6 +144,16 @@
                   "If set to 1, then (the prediction of) the target will be included in the\n"
                   "output (after the score).");
 
+    declareOption(ol, "parent_filename", &WPLS::parent_filename, OptionBase::buildoption,
+                  "For hyper-parameter selection purposes: use incremental learning to speed-up process");
+    
+    declareOption(ol, "parent_sub", &WPLS::parent_sub, OptionBase::buildoption,
+                  "Tells which of the sublearners (of the combined learner) should be used.");
+    
+    declareOption(ol, "precision", &WPLS::precision, OptionBase::buildoption,
+                  "The precision to which we compute the eigenvectors.");
+
+
     // Learnt options.
 
     declareOption(ol, "B", &WPLS::B, OptionBase::learntoption,
@@ -161,15 +171,6 @@
     declareOption(ol, "p", &WPLS::p, OptionBase::learntoption,
                   "Used to store the input size.");
 
-    declareOption(ol, "parent_filename", &WPLS::parent_filename, OptionBase::buildoption,
-                  "For hyper-parameter selection purposes: use incremental learning to speed-up process");
-    
-    declareOption(ol, "parent_sub", &WPLS::parent_sub, OptionBase::buildoption,
-                  "Tells which of the sublearners (of the combined learner) should be used.");
-    
-    declareOption(ol, "precision", &WPLS::precision, OptionBase::buildoption,
-                  "The precision to which we compute the eigenvectors.");
-    
     declareOption(ol, "P", &WPLS::P, OptionBase::learntoption,
                   "Matrix that maps features to observed inputs: X = T.P' + E.");
 
@@ -209,6 +210,8 @@
 ////////////
 void WPLS::build_()
 {
+    PLASSERT(precision > 0);
+
     if (train_set) {
         this->m = train_set->targetsize();
         this->p = train_set->inputsize();



From nouiz at mail.berlios.de  Mon Jan 28 22:00:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jan 2008 22:00:30 +0100
Subject: [Plearn-commits] r8418 - trunk/python_modules/plearn/pytest
Message-ID: <200801282100.m0SL0UWv011956@sheep.berlios.de>

Author: nouiz
Date: 2008-01-28 22:00:29 +0100 (Mon, 28 Jan 2008)
New Revision: 8418

Modified:
   trunk/python_modules/plearn/pytest/tests.py
Log:
Added the print of the run time of each test


Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2008-01-28 17:14:58 UTC (rev 8417)
+++ trunk/python_modules/plearn/pytest/tests.py	2008-01-28 21:00:29 UTC (rev 8418)
@@ -32,6 +32,7 @@
 #  library, go to the PLearn Web site at www.plearn.org
 
 import os, shutil 
+from time import time
 
 from plearn.utilities import ppath 
 from plearn.utilities import pldiff
@@ -344,7 +345,7 @@
     pfileprg         = PLOption("__program__")
     ignored_files_re = PLOption([])
     disabled         = PLOption(False)
-    
+    runtime          = PLOption(None)
 
     #######  Class Variables and Methods  #########################################
     # NB: Class variables could now all be 'public', laziness is why it isn't so...
@@ -570,27 +571,30 @@
         statsHeader = TestStatus.summaryHeader()
 
         # Hackish hardcoded display summing to 80...
-        C = 6; S = len("** FAILED **")+2; H = len(statsHeader)+3
-        N = 80 - (C+S+H); 
-        def vpformat(c,n,s, h):
+        C = 6; S = len("** FAILED **")+2; H = len(statsHeader)+3; T = 11
+        N = 80 - (C+S+H+T); 
+        def vpformat(c, n, s, h, t):
             if len(n) < N:
-                logging.info( c.ljust(C)+n.ljust(N)+s.center(S)+h.center(H) )
+                logging.info( c.ljust(C)+n.ljust(N)+s.center(S)+h.center(H)
+                              +t.ljust(T))
             else:
                 logging.info( c.ljust(C)+n.ljust(N) )
-                logging.info( ((C+N)*" ")+s.center(S)+h.center(H) )
+                logging.info( ((C+N)*" ")+s.center(S)+h.center(H)+t.ljust(T))
         
         if self._status.isCompleted():
             if not self._logged_header:
-                logging.info(TestStatus.headerLegend(C+N+S+H)+'\n')
-                vpformat("N/%d"%self._test_count, "Test Name", "Status", statsHeader)
+                logging.info(TestStatus.headerLegend(C+N+S+H+T)+'\n')
+                vpformat("N/%d"%self._test_count, "Test Name", "Status",
+                         statsHeader,"Run time(s)")
                 self.__class__._logged_header = True
 
             if self._log_count%5 == 0:
-                logging.info("-"*(C+N+S+H))
+                logging.info("-"*(C+N+S+H+T))
                 
             self.__class__._log_count += 1
             vpformat(str(self._log_count), self.getName(),
-                     str(self._status), TestStatus.summary())
+                     str(self._status), TestStatus.summary(),
+                     "%10.4s"%self.runtime)
 
     def toBeNeglected(self):
         neglect = False
@@ -748,7 +752,11 @@
         ## Run the test from inside the test_results directory and return
         ## to the cwd
         pushd(test_results)
+        starttime = time()
         self.test.program.invoke(self.test.arguments, self.run_log)
+        endtime = time()
+        self.test.runtime = endtime-starttime
+
         self.clean_cwd()
         popd()
 



From tihocan at mail.berlios.de  Tue Jan 29 17:11:52 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 29 Jan 2008 17:11:52 +0100
Subject: [Plearn-commits] r8419 - trunk/python_modules/plearn/plotting
Message-ID: <200801291611.m0TGBqVF018310@sheep.berlios.de>

Author: tihocan
Date: 2008-01-29 17:11:52 +0100 (Tue, 29 Jan 2008)
New Revision: 8419

Modified:
   trunk/python_modules/plearn/plotting/
Log:
Ignoring .pyc files


Property changes on: trunk/python_modules/plearn/plotting
___________________________________________________________________
Name: svn:ignore
   + *.pyc




From tihocan at mail.berlios.de  Tue Jan 29 17:12:32 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 29 Jan 2008 17:12:32 +0100
Subject: [Plearn-commits] r8420 - trunk/plearn_learners/online
Message-ID: <200801291612.m0TGCWuU018356@sheep.berlios.de>

Author: tihocan
Date: 2008-01-29 17:12:32 +0100 (Tue, 29 Jan 2008)
New Revision: 8420

Modified:
   trunk/plearn_learners/online/IdentityModule.cc
Log:
Implemented deep copy

Modified: trunk/plearn_learners/online/IdentityModule.cc
===================================================================
--- trunk/plearn_learners/online/IdentityModule.cc	2008-01-29 16:11:52 UTC (rev 8419)
+++ trunk/plearn_learners/online/IdentityModule.cc	2008-01-29 16:12:32 UTC (rev 8420)
@@ -100,15 +100,6 @@
 void IdentityModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("IdentityModule::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
 ///////////



From tihocan at mail.berlios.de  Tue Jan 29 17:13:19 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 29 Jan 2008 17:13:19 +0100
Subject: [Plearn-commits] r8421 - trunk/plearn_learners/hyper
Message-ID: <200801291613.m0TGDJvd018427@sheep.berlios.de>

Author: tihocan
Date: 2008-01-29 17:13:19 +0100 (Tue, 29 Jan 2008)
New Revision: 8421

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
   trunk/plearn_learners/hyper/HyperOptimize.h
Log:
Added an option to always keep the best current learner saved (in a single file)

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-01-29 16:12:32 UTC (rev 8420)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-01-29 16:13:19 UTC (rev 8421)
@@ -44,9 +44,10 @@
 /*! \file HyperOptimize.cc */
 #include "HyperOptimize.h"
 #include "HyperLearner.h"
+#include <plearn/io/load_and_save.h>
+#include <plearn/base/stringutils.h>
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/vmat/MemoryVMatrix.h>
-#include <plearn/base/stringutils.h>
 
 namespace PLearn {
 using namespace std;
@@ -111,7 +112,8 @@
       min_n_trials(0),
       provide_tester_expdir(false),
       rerun_after_sub(false),
-      provide_sub_expdir(true)
+      provide_sub_expdir(true),
+      save_best_learner(false)
 { }
 
 
@@ -150,6 +152,12 @@
         "Should sub_strategy commands be provided an expdir");
 
     declareOption(
+        ol, "save_best_learner", &HyperOptimize::save_best_learner,
+        OptionBase::buildoption,
+        "If true, the best learner at any step will be saved in the\n"
+        "strategy expdir, as 'current_best_learner.psave'.");
+
+    declareOption(
         ol, "splitter", &HyperOptimize::splitter, OptionBase::buildoption,
         "If not specified, we'll use default splitter specified in the hyper-learner's tester option");
 
@@ -325,7 +333,7 @@
             for(int commandnum=0; commandnum<sub_strategy.length(); commandnum++)
             {
                 sub_strategy[commandnum]->setHyperLearner(hlearner);
-                if(expdir!="" && provide_sub_expdir)
+                if(!expdir.isEmpty() && provide_sub_expdir)
                     sub_strategy[commandnum]->setExperimentDirectory(
                         expdir / ("Trials"+tostring(trialnum)) / ("Step"+tostring(commandnum))
                         );
@@ -359,6 +367,10 @@
             CopiesMap copies;
             best_learner = NULL;
             best_learner = hlearner->getLearner()->deepCopy(copies);
+            if (save_best_learner && !expdir.isEmpty()) {
+                PLearn::save(expdir / "current_best_learner.psave",
+                             best_learner);
+            }
         }
         ++trialnum;
     }

Modified: trunk/plearn_learners/hyper/HyperOptimize.h
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.h	2008-01-29 16:12:32 UTC (rev 8420)
+++ trunk/plearn_learners/hyper/HyperOptimize.h	2008-01-29 16:13:19 UTC (rev 8421)
@@ -127,6 +127,7 @@
     TVec< PP<HyperCommand> > sub_strategy; //!< A possible sub-strategy to optimize other hyper parameters
     bool rerun_after_sub;
     bool provide_sub_expdir; // should sub_strategy be provided an expdir
+    bool save_best_learner;
     PP<Splitter> splitter;  // (if not specified, use default splitter specified in PTester)
 
     // ****************



From nouiz at mail.berlios.de  Tue Jan 29 18:09:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 29 Jan 2008 18:09:30 +0100
Subject: [Plearn-commits] r8422 - in trunk: plearn/math
 plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456
 plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0
 plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir
 plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0
 plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir
 plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir
 plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0
 plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir
 plearn_learners/regressors/test/KernelRidgeRegressor/!
 .pytest/PL_kernel_ridge_regressor/expected_results/expdir
 plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0
 plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir
 plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0
 plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_bad!
 ly_conditioned_linreg_script/expected_results/expdir plearn_le! arners/r
Message-ID: <200801291709.m0TH9UYO027348@sheep.berlios.de>

Author: nouiz
Date: 2008-01-29 18:09:27 +0100 (Tue, 29 Jan 2008)
New Revision: 8422

Modified:
   trunk/plearn/math/StatsCollector.cc
   trunk/plearn/math/StatsCollector.h
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/final_learner.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/train_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/global_stats.pmat
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_confidence.pmat
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_confidence.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_confidence.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/tester.psave
Log:
Added in StatsCollector the function isinteger() and isbinary().
Updated the test for this change.


Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn/math/StatsCollector.cc	2008-01-29 17:09:27 UTC (rev 8422)
@@ -169,6 +169,8 @@
       first_(MISSING_VALUE),
       last_(MISSING_VALUE),
       more_than_maxnvalues(false),
+      binary_(-1),
+      integer_(-1),
       sorted(false)
 {
     build_();
@@ -297,6 +299,22 @@
         "last encountered observation");
 
     declareOption(
+        ol, "binary_", &StatsCollector::binary_,
+        OptionBase::learntoption,
+        "1(true) if all seen value are binary. 0(false) otherwise"
+        "In the case where we would have reloaded and old version"
+        "we will calculate the result from the data in counts"
+        "If maxnvalues==0, we are in trouble as we can't recalculate it"
+        "So binary_==-1 and integer_==-1, but "
+        "if we do new update, it will contain the result of only the "
+        " new value if they change it for 0.");
+
+    declareOption(
+        ol, "integer_", &StatsCollector::integer_,
+        OptionBase::learntoption,
+        "as binary_, execpt for integer");
+
+    declareOption(
         ol, "counts", &StatsCollector::counts,
         OptionBase::learntoption,
         "Will contain up to 'maxnvalues' values and associated counts, as\n"
@@ -459,6 +477,9 @@
     for(map<real, StatsCollectorCounts>::iterator it= counts.begin();
         it != counts.end(); ++it)
         count_ids[it->second.id]= it->first;
+    
+    //In case we reload an old version
+    calculate_binary_integer();
 }
 
 ///////////
@@ -487,6 +508,8 @@
     agemin_ = MISSING_VALUE;
     agemax_ = MISSING_VALUE;
     first_ = last_ = MISSING_VALUE;
+    binary_ = -1;
+    integer_ = -1;
     more_than_maxnvalues = (maxnvalues == 0);
     approximate_counts.clear();
     sorted = false;
@@ -515,6 +538,8 @@
             min_ = max_ = first_ = last_ = val;
             agemin_ = 0;
             agemax_ = 0;
+            binary_  = true;
+            integer_ = true;
         }
         else if(val<min_) {
             min_ = val;
@@ -537,8 +562,12 @@
         sumsquare_ += sqval              * weight;
         sumcube_   += sqval*(val-first_) * weight;
         sumfourth_ += sqval*sqval        * weight;
-    
 
+        if(!(fast_exact_is_equal(val,0) ||fast_exact_is_equal(val,1)))
+            binary_ = false;
+        if(!fast_exact_is_equal(val,int(round(val))))
+            integer_ = false;
+            
         if (storeCounts())
         {
             // Also remembering statistics inside values ranges.
@@ -1372,7 +1401,6 @@
     return 2 * zpr1t();
 }
 
-
 void StatsCollector::merge(const StatsCollector& other)
 {
     if(storeCounts() && other.maxnvalues != -1)
@@ -1492,7 +1520,37 @@
     if (!approximate_counts.empty()) approximate_counts.clear();
 }
 
-
+void StatsCollector::calculate_binary_integer()
+{
+    if(maxnvalues!=0 && nnonmissing_>0)
+    {
+        binary_  = true;
+        integer_ = true;
+        for(map<real, StatsCollectorCounts>::iterator it = counts.begin();
+            it!=counts.end();it++)
+        {
+            if(it->second.n!=0)
+            {
+                if(!(fast_exact_is_equal(it->first,0)||
+                     fast_exact_is_equal(it->first,1)))
+                    binary_ = false;
+                if(!fast_exact_is_equal(int(round(it->first)),it->first)){
+                    integer_ = false;
+                    break;
+                }
+            }
+        }
+        if((binary_||integer_)&&more_than_maxnvalues)
+            PLWARNING("In StatsCollector::calculate_binary_integer() - "
+                      "Reloading an old StatsCollector. While recalculating data for isbinary() and isinteger(), we found a possible error case. The StatsCollector have more value then maxnvalues(%d), but we are still thinking it is a binary or an integer. This can be false.",maxnvalues);
+    }
+    else if(maxnvalues==0 && nnonmissing()>0 && -1==binary_ && -1==integer_)
+        PLWARNING("In StatsCollector::calculate_binary_integer() - "
+                  "Reloadind old StatsCollector with maxnvalues==0 and "
+                  "nnonmissing()>0. This cause trouble as we can't recompute"
+                  "the data for the function isbinary() and isinteger()"
+            );
+}
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/math/StatsCollector.h
===================================================================
--- trunk/plearn/math/StatsCollector.h	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn/math/StatsCollector.h	2008-01-29 17:09:27 UTC (rev 8422)
@@ -187,6 +187,8 @@
     real first_;           //!< first encountered nonmissing observation
     real last_;            //!< last encountered nonmissing observation
     bool more_than_maxnvalues;
+    int binary_;           //!< true if all seen variable are 0 or 1, -1 in some case
+    int integer_;          //!< true if all seen variable are integer, -1 in some case
 
     map<real, StatsCollectorCounts> counts; 
     map<int, real> count_ids;
@@ -208,6 +210,9 @@
     //! This does the actual building.
     void build_();
 
+    //! used to calculate binary_ and integer_ if we reload an old version
+    void calculate_binary_integer();
+
 protected: 
 
     //! Declares this class' options
@@ -366,6 +371,13 @@
     //! merge another StatsCollector into this one
     virtual void merge(const StatsCollector& other);
 
+    //! @return true if all value seen are binary, false otherwise and is not
+    //! defined in case where we reload an old version that have maxnvalues==0
+    bool isbinary(){return binary_;}
+
+    //! @return true if all value seen are integer, false otherwise and is not
+    //! defined in case where we reload an old version that have maxnvalues==0
+    bool isinteger(){return integer_;}
 };
 
 DECLARE_OBJECT_PTR(StatsCollector);

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -5,11 +5,11 @@
 use_last_eig = 1 ;
 given_mu = []
 ;
-mu = 4 [ -0.0868066666666664766 -0.062603333333333358 -1.50779199999992364 5.35029800000000844 ] ;
-eigenvalues = 2 [ 12666.493023310637 458.781023436977705 ] ;
+mu = 4 [ -0.0868066666666664488 -0.0626033333333333025 -1.50779199999991498 5.35029800000000844 ] ;
+eigenvalues = 2 [ 12666.4930233106425 458.781023436976511 ] ;
 eigenvectors = 2  4  [ 
--0.000988821712955273565 	-0.00880822163979268734 	-0.890970870803750148 	-0.453973947316771542 	
-0.0172789682695909733 	-0.00620286016779083519 	-0.453881809086822574 	0.890872754753085028 	
+-0.00098882171295527465 	-0.00880822163979269081 	-0.89097087080375037 	-0.453973947316771431 	
+0.0172789682695909386 	-0.00620286016779083085 	-0.453881809086822574 	0.890872754753085139 	
 ]
 ;
 outputs_def = "l" ;
@@ -17,7 +17,11 @@
 lower_bound = 0 ;
 upper_bound = 0 ;
 n_predicted = 4 ;
+random_gen = *2 ->PRandom(
 seed = 123456 ;
+fixed_seed = 0  )
+;
+seed = 123456 ;
 stage = 0 ;
 n_examples = 150 ;
 inputsize = 4 ;

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 48.8719180204965085 ;
-sumsquare_ = 104.363929133441701 ;
-sumcube_ = 318.950374924756545 ;
-sumfourth_ = 1177.77083991770928 ;
-min_ = 17.6042426640295098 ;
-max_ = 22.4027835362180134 ;
+sum_ = 48.8719180204967216 ;
+sumsquare_ = 104.363929133442653 ;
+sumcube_ = 318.950374924761547 ;
+sumfourth_ = 1177.77083991773657 ;
+min_ = 17.6042426640295062 ;
+max_ = 22.4027835362180525 ;
 agmemin_ = 36 ;
 agemax_ = 18 ;
-first_ = 17.6199119453043913 ;
-last_ = 17.6479526969470228 ;
+first_ = 17.6199119453043878 ;
+last_ = 17.6479526969470193 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 9.41750512464402334 ;
-sumsquare_ = 139.406100961583832 ;
-sumcube_ = 289.545736167176756 ;
-sumfourth_ = 1182.45789565907239 ;
-min_ = 17.600507769412193 ;
-max_ = 23.6898552948462466 ;
+sum_ = 9.41750512464414413 ;
+sumsquare_ = 139.406100961585082 ;
+sumcube_ = 289.545736167180053 ;
+sumfourth_ = 1182.45789565908922 ;
+min_ = 17.6005077694121894 ;
+max_ = 23.6898552948462715 ;
 agmemin_ = 95 ;
 agemax_ = 6 ;
 first_ = 18.5231445696105403 ;
 last_ = 19.4342267319090887 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = []

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,5 +1,6 @@
 DeepBeliefNet(
 cd_learning_rate = 0.0100000000000000002 ;
+cd_decrease_ct = 0 ;
 grad_learning_rate = 0.100000000000000006 ;
 grad_decrease_ct = 0 ;
 batch_size = 1 ;
@@ -11,6 +12,8 @@
 size = 2 ;
 learning_rate = 0.100000000000000006 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
@@ -30,11 +33,13 @@
 size = 2 ;
 learning_rate = 0.100000000000000006 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00419880748139610759 0.0029152184514807198 ] ;
+bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -43,16 +48,19 @@
 expdir = "" ;
 random_gen = *2   )
 ] ;
+i_output_layer = 1 ;
 connections = 1 [ *4 ->RBMMatrixConnection(
 weights = 2  2  [ 
 -0.222066209664342068 	0.582925561708350082 	
-0.516883916989072434 	-0.316272804129796303 	
+0.516883916989072545 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -71,14 +79,16 @@
 last_layer = *3  ;
 last_to_target = *6 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.603392333131359537 	0.370787384786479657 	
-0.0970862565219703239 	-0.668099599496062679 	
+-0.603392333131359426 	0.370787384786479657 	
+0.09708625652197031 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -96,11 +106,13 @@
 size = 2 ;
 learning_rate = 0.100000000000000006 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00487910443291655199 0.00487910443291657281 ] ;
+bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -10,6 +10,7 @@
 source = *0 ;
 fieldnames = []
 ;
+deep_copy_memory_data = 1 ;
 writable = 0 ;
 length = 2 ;
 width = 3 ;
@@ -31,6 +32,7 @@
 ;
 learner = *5 ->DeepBeliefNet(
 cd_learning_rate = 0.0100000000000000002 ;
+cd_decrease_ct = 0 ;
 grad_learning_rate = 0.100000000000000006 ;
 grad_decrease_ct = 0 ;
 batch_size = 1 ;
@@ -42,6 +44,8 @@
 size = 2 ;
 learning_rate = 0.100000000000000006 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
@@ -61,11 +65,13 @@
 size = 2 ;
 learning_rate = 0.100000000000000006 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00419880748139610759 0.0029152184514807198 ] ;
+bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -74,16 +80,19 @@
 expdir = "" ;
 random_gen = *7   )
 ] ;
+i_output_layer = 1 ;
 connections = 1 [ *9 ->RBMMatrixConnection(
 weights = 2  2  [ 
 -0.222066209664342068 	0.582925561708350082 	
-0.516883916989072434 	-0.316272804129796303 	
+0.516883916989072545 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -102,14 +111,16 @@
 last_layer = *8  ;
 last_to_target = *11 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.603392333131359537 	0.370787384786479657 	
-0.0970862565219703239 	-0.668099599496062679 	
+-0.603392333131359426 	0.370787384786479657 	
+0.09708625652197031 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0.100000000000000006 ;
@@ -127,11 +138,13 @@
 size = 2 ;
 learning_rate = 0.100000000000000006 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00487910443291655199 0.00487910443291657281 ] ;
+bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
@@ -209,6 +222,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 0 ;
 should_train = 1 ;
 should_test = 1 ;
@@ -250,6 +264,7 @@
 learner = *5  ;
 provide_learner_expdir = 0 ;
 expdir_append = "" ;
+forward_nstages = 0 ;
 random_gen = *0 ;
 stage = 1 ;
 n_examples = 2 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 0 ;
 first_ = 0.651261219307483818 ;
 last_ = 0.651261219307483818 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 0 ;
 first_ = 0.5 ;
 last_ = 0.5 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 0 ;
 first_ = 0.614096318051678525 ;
 last_ = 0.614096318051678525 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 0 ;
 first_ = 0 ;
 last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL7704"
+__REVISION__ = "PL8398"
 

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -9,6 +9,7 @@
 source = *0 ;
 fieldnames = []
 ;
+deep_copy_memory_data = 1 ;
 writable = 0 ;
 length = 2 ;
 width = 3 ;
@@ -40,6 +41,7 @@
 ;
 learner = *6 ->DeepBeliefNet(
 cd_learning_rate = 0.0100000000000000002 ;
+cd_decrease_ct = 0 ;
 grad_learning_rate = 0.100000000000000006 ;
 grad_decrease_ct = 0 ;
 batch_size = 1 ;
@@ -51,6 +53,8 @@
 size = 2 ;
 learning_rate = 0 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
@@ -70,6 +74,8 @@
 size = 2 ;
 learning_rate = 0 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
@@ -83,16 +89,19 @@
 expdir = "" ;
 random_gen = *8   )
 ] ;
+i_output_layer = 1 ;
 connections = 1 [ *10 ->RBMMatrixConnection(
 weights = 2  2  [ 
-0.211616747512905712 	0.215664750468485322 	
-0.59211590021607885 	0.667132771633056509 	
+0.211616747512905684 	0.21566475046848535 	
+0.59211590021607885 	0.667132771633056398 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0 ;
@@ -111,14 +120,16 @@
 last_layer = *9  ;
 last_to_target = *12 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.166376995329432731 	0.522222782976989097 	
-0.449826313170107572 	-0.266489754613600693 	
+-0.166376995329432703 	0.522222782976989097 	
+0.449826313170107683 	-0.266489754613600693 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
 down_size = 2 ;
 up_size = 2 ;
 learning_rate = 0 ;
@@ -136,6 +147,8 @@
 size = 2 ;
 learning_rate = 0 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
@@ -218,6 +231,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 0 ;
 should_train = 1 ;
 should_test = 1 ;
@@ -259,6 +273,7 @@
 learner = *6  ;
 provide_learner_expdir = 0 ;
 expdir_append = "" ;
+forward_nstages = 0 ;
 random_gen = *0 ;
 stage = 0 ;
 n_examples = -1 ;
@@ -284,6 +299,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -12,231 +12,231 @@
 weight_decay = 0.0100000000000000002 ;
 include_bias = 1 ;
 params = 225  2  [ 
-2.71513510622207166 	6.96796926050934751 	
--0.924570620987045566 	-5.50999860198858826 	
-2.74766209737875133 	16.2764471019206418 	
--0.897331116697557052 	24.1696624838867358 	
--0.257145462011077963 	-2.68785159119476535 	
--1.3282966539726655 	-18.5932956256296649 	
--5.49258347509019718 	-1.57002604298704584 	
-1.46070534954897835 	-3.53276434233635506 	
--1.038126171674709 	-1.2073115958693148 	
-1.02580172581497386 	-26.7245637328128929 	
-0.791254761045932176 	82.4469215737125865 	
-0.836570941006688051 	-16.4202630459276833 	
--1.58240826854525074 	-6.65940808938515527 	
--8.24967860415612364 	-3.49957089648365738 	
-0.9782411438628974 	-4.92116475768973771 	
-0.0717117261764370351 	-3.09067303050886988 	
--2.58531178640483761 	12.035458429525173 	
-3.211076444250756 	5.97223155883393897 	
-1.43705644733268212 	94.2974631733484614 	
-0.651016019176285488 	1.95403761231488304 	
--1.20978628334898786 	-7.36323983157156103 	
--0.676733526068380775 	-31.4356962338337986 	
--3.71894302925004316 	-5.30412867324711534 	
--2.6818491206821955 	-0.224631918494603705 	
--1.88827683784314537 	-11.2808510609389074 	
-1.56552507696732834 	-5.27853909851336311 	
--2.51225078969590498 	-3.22314543003026799 	
--8.82457968582814622 	-1.42245769348111262 	
--5.49712489654685843 	-14.7599950960045039 	
-8.47592427242672741 	8.11107085181684795 	
-2.34953305575425997 	4.63659562959487381 	
--1.64301101095052804 	11.4873476009648563 	
-0.580660833463284676 	13.1755650470951462 	
--0.650250885492095976 	1.90203710703754014 	
--3.87917543422272804 	5.03518774179621609 	
-1.70397379112897029 	1.34775554557823152 	
--2.78522303432040363 	-64.1292521132013178 	
-2.32700743878965444 	24.7436125596276284 	
--5.39566293426405519 	-9.82739376173207368 	
-12.9203582472648133 	5.29957416697557626 	
--10.7455769141731885 	89.9090483177333653 	
-4.31713435896750308 	-5.18586285076809528 	
-0.848047232583675648 	-2.17948525292697681 	
--9.46810320629667324 	-0.220748951447968828 	
-7.21414655874827737 	7.46866878382293553 	
--1.75858145600769644 	11.2871954305640294 	
-1.96560792644487359 	-6.52458829385446837 	
--2.79380808107613055 	-5.09955504191000042 	
--0.79905931023683674 	-12.925504941414971 	
--4.25437046157972976 	8.96615214816773154 	
--9.43622335236653065 	3.78339978190649218 	
-4.36697916817812093 	-0.68750387907888455 	
--2.18796130106569242 	-6.69802616230909642 	
-3.17982632559557965 	-1.4370316032819157 	
--4.31414536641474289 	-5.51667561668654649 	
--2.74519292766675793 	7.73168925235037285 	
-1.0537820640848472 	0.178110338215527242 	
-0.460217896787073477 	10.058544903984771 	
--0.0106331952212118993 	-19.6790725953963772 	
-0.424465188958454087 	-15.8551532141373048 	
-0.567218573772457435 	-2.69519041953220029 	
-4.45718670597664524 	5.62393864871897797 	
-2.19961229164894245 	-5.42279390073130685 	
--0.228762156047200543 	-0.831542078921280403 	
--2.40103186222691534 	15.8373411775187218 	
--1.34006398347233469 	-12.0359127014981127 	
-4.03810217513344138 	-2.91524838149503962 	
-0.593248196381594317 	0.218503040354157629 	
+2.7151351062236615 	6.96796926051022236 	
+-0.924570620989403458 	-5.50999860198525226 	
+2.74766209737707889 	16.2764471019291541 	
+-0.897331116702694054 	24.1696624839723277 	
+-0.257145462010700432 	-2.68785159119131878 	
+-1.3282966539723533 	-18.5932956256279311 	
+-5.49258347507660183 	-1.57002604296742554 	
+1.46070534955196196 	-3.53276434234297421 	
+-1.03812617167629262 	-1.20731159585204617 	
+1.02580172582147133 	-26.7245637329914061 	
+0.791254761045375288 	82.4469215737143202 	
+0.836570941010414737 	-16.420263045944921 	
+-1.58240826855158301 	-6.65940808940268703 	
+-8.24967860415081411 	-3.4995708964716874 	
+0.978241143861720674 	-4.92116475768050154 	
+0.0717117261694705244 	-3.09067303052573861 	
+-2.58531178640539849 	12.0354584295284699 	
+3.21107644425249639 	5.97223155883275147 	
+1.43705644733271809 	94.2974631733482198 	
+0.651016019177080518 	1.95403761232638362 	
+-1.20978628334706184 	-7.36323983158574702 	
+-0.676733526068720059 	-31.4356962338422257 	
+-3.71894302925004183 	-5.30412867323935266 	
+-2.68184912068139347 	-0.224631918495068694 	
+-1.88827683783783651 	-11.2808510609680628 	
+1.56552507696869569 	-5.27853909851396796 	
+-2.51225078969575177 	-3.22314543004070897 	
+-8.82457968581596752 	-1.42245769348021844 	
+-5.4971248965487387 	-14.759995096029046 	
+8.47592427242844337 	8.11107085181370557 	
+2.34953305575392202 	4.63659562958691041 	
+-1.64301101094937252 	11.4873476009575572 	
+0.580660833465362791 	13.1755650470871277 	
+-0.650250885491696851 	1.90203710703855133 	
+-3.87917543422287148 	5.03518774179589901 	
+1.7039737911280386 	1.34775554555850574 	
+-2.78522303432125629 	-64.1292521131980919 	
+2.32700743878955185 	24.743612559642397 	
+-5.39566293426395394 	-9.82739376173094037 	
+12.9203582472649128 	5.29957416698956862 	
+-10.7455769141728563 	89.9090483177386801 	
+4.31713435895606157 	-5.18586285077431786 	
+0.848047232584521082 	-2.17948525293295869 	
+-9.4681032062965258 	-0.22074895143857387 	
+7.21414655874796917 	7.46866878382939792 	
+-1.75858145601429361 	11.2871954305960074 	
+1.96560792644405802 	-6.52458829385219907 	
+-2.79380808107006473 	-5.09955504189952258 	
+-0.799059310236704512 	-12.9255049414569196 	
+-4.25437046158016585 	8.96615214816819339 	
+-9.43622335236651288 	3.78339978190638559 	
+4.36697916816686238 	-0.687503879096243442 	
+-2.18796130106503472 	-6.69802616230832459 	
+3.17982632559527989 	-1.43703160328240198 	
+-4.3141453664152607 	-5.51667561668627471 	
+-2.74519292766851297 	7.73168925236620286 	
+1.05378206408368125 	0.178110338223795794 	
+0.460217896787359693 	10.0585449039864159 	
+-0.0106331952235446964 	-19.6790725953361481 	
+0.424465188958869921 	-15.8551532141411631 	
+0.567218573767627854 	-2.6951904195179508 	
+4.45718670597547018 	5.62393864872558336 	
+2.19961229165357031 	-5.42279390073859879 	
+-0.228762156049838294 	-0.831542078930034179 	
+-2.4010318622261817 	15.8373411775190593 	
+-1.34006398346898026 	-12.0359127014414575 	
+4.0381021751345596 	-2.91524838149849552 	
+0.593248196395251171 	0.218503040318817232 	
 -0.894620455881181775 	1.01068418956454376 	
--0.905934974062951315 	-1.04508844319939009 	
-8.00428356105542704 	-1.26168634488815745 	
-0.619982738889116813 	54.1218393092184655 	
--0.705573849337669534 	-1.25381377885319911 	
-0.871066182005948986 	40.4927882352012602 	
--7.05074671248575058 	71.3212740911221061 	
--2.22324163214598158 	1.07345612751800834 	
--2.44557668836901909 	4.29373139551074523 	
-9.8216236808294628 	6.64093872401048912 	
--9.18335934248439933 	-3.97745257894915438 	
--0.836507465302499531 	-1.03211418641978869 	
--0.053497546963581534 	19.5696227138299683 	
-1.07568581176873801 	-5.10104965788098763 	
--0.51895274081205478 	15.8636310713846687 	
--2.84087926537020463 	-12.9191491848348825 	
-2.04826664350002785 	12.2408805586060261 	
-1.80375307384701089 	11.6859209915609643 	
--1.25621312029488119 	-2.02266034407121387 	
--0.0484853946924818577 	3.80498181684614956 	
--0.677242736032365844 	-42.6254775342041086 	
-1.5916269901049056 	-5.48763586146515259 	
--1.21972683978988194 	-1.50185397433043755 	
-1.24858166506403889 	4.87092530126864176 	
--7.04338199489057715 	17.3390887919734524 	
-3.47070361924793991 	1.42970456326488016 	
--4.26550633389315159 	-1.73980331015090517 	
-2.12547785989944504 	3.62923159122205607 	
-2.5325993386476946 	-38.0016204477384605 	
-2.21881274683430751 	-0.0138947589494949966 	
-3.14760002032383079 	-1.50792558449675762 	
-1.08730538645968133 	-5.46742192799871596 	
-0.805042144582074792 	-53.0054722631666593 	
--1.33448103050452405 	5.00293548641393393 	
--4.27305110186876824 	-0.418543428368392034 	
-1.70132006444656469 	-5.38749080327059016 	
--1.12532164909935517 	33.3451642986786609 	
--2.70415771236861247 	3.89572987273113291 	
-4.98777580381695707 	13.8821103996001245 	
-0.568511986632529109 	-6.88339363195469822 	
-2.78082104553195331 	-5.19833751432510915 	
--3.45195094389145929 	6.54989673235798353 	
--1.26923932125293937 	-82.4386151296812386 	
-6.7221655120617827 	-5.26765661015447861 	
--0.742712929107001374 	-2.30022073912255554 	
-1.55989131761590083 	-4.18345207234387129 	
-4.72158352753704147 	21.7128846359820926 	
--2.74322426786354923 	0.0600870120432775226 	
-5.66665271018074534 	-3.34386936425682135 	
--1.7405916077807202 	11.7068252411723854 	
-0.339841379787169384 	1.53151755701152847 	
--0.136156736944080564 	-12.0671867463559686 	
-2.99636501987962234 	1.68582324362498048 	
--1.96646003943842773 	-7.02906457262245166 	
--2.68253702359453339 	19.2149219172491286 	
-3.77166285715723992 	-15.0634217849763132 	
-1.10118597277121655 	11.2002484600206493 	
--0.21265096744765738 	0.824744814217108657 	
-3.73652792828045532 	-7.81218626521123305 	
--0.1517753718816853 	1.00699915467459311 	
-3.72484877108446399 	5.92264185066637783 	
--0.0755790292585322709 	-3.91733851844826209 	
--1.69431689893366721 	3.02479371219842008 	
-0.0753423487892740662 	14.9935199826461982 	
-2.23109480398673998 	-2.59140850098083764 	
-6.91777882726973914 	41.5668021912243546 	
-1.87173910300826329 	-16.2617861471825336 	
--2.61089444121170011 	-39.3944996228537363 	
+-0.905934974062951648 	-1.04508844319938632 	
+8.00428356105546257 	-1.26168634488796494 	
+0.619982738884564899 	54.1218393093347103 	
+-0.705573849334636627 	-1.2538137788610173 	
+0.871066182002223743 	40.4927882352060706 	
+-7.05074671248599127 	71.3212740911236835 	
+-2.22324163215964843 	1.0734561275325698 	
+-2.44557668837193853 	4.2937313954156906 	
+9.82162368083046999 	6.64093872400913376 	
+-9.18335934249402008 	-3.97745257891762716 	
+-0.836507465302499753 	-1.0321141864197878 	
+-0.0534975469631229703 	19.5696227138288812 	
+1.07568581177046618 	-5.10104965789005504 	
+-0.518952740814859759 	15.8636310713528932 	
+-2.84087926537195301 	-12.9191491848439277 	
+2.04826664350717103 	12.2408805584981177 	
+1.80375307384732975 	11.6859209915770084 	
+-1.25621312028974019 	-2.02266034406559303 	
+-0.0484853946913663472 	3.8049818168441556 	
+-0.677242736030050585 	-42.625477534183851 	
+1.59162699009028663 	-5.48763586147435767 	
+-1.21972683978848395 	-1.50185397433300194 	
+1.24858166506431711 	4.87092530127280199 	
+-7.04338199489003269 	17.3390887919741523 	
+3.47070361924823967 	1.42970456326448381 	
+-4.2655063338911301 	-1.73980331015853862 	
+2.12547785989874116 	3.62923159123009187 	
+2.53259933864800901 	-38.0016204477437114 	
+2.21881274683373908 	-0.0138947589423880184 	
+3.1476000203292771 	-1.50792558450150538 	
+1.08730538646035324 	-5.46742192799836069 	
+0.805042144582562846 	-53.0054722631629431 	
+-1.33448103050428291 	5.00293548641483543 	
+-4.27305110186990333 	-0.418543428366281389 	
+1.70132006444962425 	-5.38749080326675145 	
+-1.12532164909613996 	33.3451642986689407 	
+-2.70415771239980796 	3.8957298727264229 	
+4.98777580381757346 	13.8821103996191333 	
+0.568511986630580113 	-6.88339363194706877 	
+2.78082104553625609 	-5.19833751433356372 	
+-3.45195094389070523 	6.54989673235655356 	
+-1.26923932124764582 	-82.4386151297649405 	
+6.72216551206190616 	-5.26765661015598763 	
+-0.742712929107524844 	-2.30022073912311376 	
+1.55989131761279332 	-4.18345207235786098 	
+4.72158352754091215 	21.7128846360060805 	
+-2.74322426786452844 	0.0600870120549067896 	
+5.66665271018079864 	-3.34386936425720371 	
+-1.7405916077822694 	11.7068252411821998 	
+0.339841379787524656 	1.53151755701120851 	
+-0.136156736953116003 	-12.0671867463523572 	
+2.99636501990413606 	1.68582324363194558 	
+-1.96646003944210279 	-7.02906457252052252 	
+-2.68253702359536872 	19.2149219172472989 	
+3.77166285715782834 	-15.0634217849804468 	
+1.10118597276613972 	11.2002484600089556 	
+-0.212650967437634342 	0.824744814209136368 	
+3.73652792828373403 	-7.81218626519529735 	
+-0.151775371881687632 	1.00699915467459045 	
+3.72484877108447021 	5.92264185066602611 	
+-0.0755790292600937996 	-3.91733851844509173 	
+-1.69431689893300641 	3.02479371219719839 	
+0.0753423487888285753 	14.9935199826456476 	
+2.23109480398829207 	-2.59140850098041486 	
+6.91777882726934212 	41.5668021912209156 	
+1.87173910301644164 	-16.2617861474499854 	
+-2.610894441212122 	-39.3944996228497075 	
 -0.836329838029520878 	0.975444302819992171 	
-3.00474481016317796 	-21.1150530969481025 	
--8.62460330276778109 	3.04533586235480191 	
--11.9994315455979592 	-1.47919451474482977 	
-4.44069944340338374 	-18.2589018941275931 	
--2.27968651835906844 	1.02704086191163668 	
-6.31981383647014194 	1.54445289435077759 	
--5.26600895531925595 	-35.7977533157710113 	
--0.481694055896209672 	2.21195940922255518 	
-7.77036312147073716 	1.09313418809460505 	
--4.68097213985389882 	-6.20894287206248841 	
--1.0971415680584542 	-2.85536759176862898 	
-0.545700238407987648 	11.8862474186525926 	
-1.48867902407183705 	-5.48427538646079693 	
--6.1558764315134562 	6.74361516831379859 	
--0.34943300218334733 	-6.2328287440142951 	
--2.64624601969912154 	3.41977326768898227 	
--2.94286030596212944 	-2.75364076618103537 	
-5.73828756539082274 	-5.57310817627025834 	
--4.97225940962958202 	3.65509474643111654 	
--6.10148670149821548 	-3.5099263102813012 	
-1.55044181583261187 	14.6775034364680952 	
-0.782385337009910753 	25.481371320069055 	
--1.53273115443696883 	4.58635057373261112 	
-10.4172385843581061 	6.86633339432157452 	
--0.368394943949268971 	20.6264920696424987 	
-5.60810932982655341 	6.78678373501238852 	
-6.71791590246840631 	-5.80482782599815295 	
-0.388121780022244123 	-3.46219858678257397 	
-2.83447313289494707 	7.76167610817524611 	
--1.54630702794617703 	0.277670233006282186 	
-8.9837147693589845 	-0.793959411589270636 	
-2.70994943537632693 	14.8040061566573282 	
-6.68573003919279518 	0.0462762095885054375 	
--4.29261320816020042 	-2.95101472906158513 	
--4.54151110825627313 	16.1599843952082907 	
--5.15501226513589383 	6.64257668637242293 	
-1.90733106596579205 	2.8501923085260672 	
-1.43237792951475584 	11.0408267483489499 	
--1.36458349374178489 	-1.30153240656658253 	
--4.30599353659890038 	-10.9529742342083214 	
-0.0316284965238302559 	1.25596215070410944 	
--2.60581041885303533 	-6.14526724575781902 	
--0.312851078671457561 	4.8899176585817985 	
-5.29462188951335122 	-16.5770775166756401 	
--4.00703215573606908 	2.75362305989316081 	
-8.7281729037734479 	9.97806304735961547 	
-0.312638773494846978 	-2.14323866166241261 	
--1.81776211066742133 	-44.0518064486871594 	
-2.55609366479101618 	-4.74886921353842606 	
--0.833149682703483663 	11.5472059416640835 	
--1.51305776067499043 	-109.480058674132408 	
-0.96348561462821225 	-6.81413026705121183 	
--8.58496213584385792 	13.0940886747191065 	
-0.0915420976503914519 	5.86702156901551231 	
-4.79191376944235348 	-6.83937697733123517 	
-0.534268386340944978 	-12.4849809673482426 	
-2.17337508306810578 	-14.4118021194967199 	
-0.140164109164443162 	62.1647781456503097 	
-0.167570766919056896 	-24.6465822115656401 	
-1.23667489086769566 	1.84320815143338623 	
--0.440814885867638395 	-1.55400780863914356 	
-3.86906628870721292 	5.74016181018787464 	
--4.46332178439648342 	14.5109110420769749 	
-0.978359207199548941 	-0.176691538654043939 	
--0.253112600092994244 	11.6452773485220398 	
-9.90282705194954538 	7.59072425472390933 	
--1.48088213993055762 	-12.6227827040919376 	
--2.07209465738824505 	6.0289852325298714 	
-0.0851342612640778873 	65.9186373049947889 	
--4.36632696181617952 	-11.3811871241025226 	
--1.71368855874052861 	1.31764702088757302 	
-2.15686598960512166 	18.4587976762973902 	
-0.511354889660306289 	-43.8065302486330737 	
-3.79985826164594931 	2.1930211957444623 	
-2.2000165454424403 	-4.52230342001110674 	
--3.10342048881031829 	1.69054390982302927 	
-2.49515664287580075 	-12.6407876685394616 	
--2.05596705724929718 	-15.4018265269691028 	
-2.4535740130399093 	-17.9766881878857845 	
-14.0087664049954643 	-2.1415029657413216 	
--0.458146105055854791 	-94.286637940529971 	
--7.43642773069096918 	-0.417123011311428815 	
--0.401022533507097045 	12.8005812092002085 	
--0.954443961086675396 	-18.2505062496947659 	
--3.53127796312802422 	1.20073147588600682 	
--4.42051731554893923 	-59.221195467202314 	
-1.77815493364989341 	47.1410132671640412 	
--7.07469141009799074 	-2.82886733985023353 	
+3.00474481016258865 	-21.1150530969548456 	
+-8.6246033027724156 	3.04533586233992315 	
+-11.9994315456001406 	-1.47919451474942232 	
+4.44069944340081424 	-18.2589018941206973 	
+-2.27968651835379399 	1.02704086194237587 	
+6.31981383646970141 	1.54445289434102828 	
+-5.26600895531991675 	-35.797753315782785 	
+-0.481694055895810824 	2.2119594092289705 	
+7.77036312147051333 	1.09313418809397089 	
+-4.6809721398523001 	-6.20894287205482076 	
+-1.09714156805876417 	-2.85536759175997856 	
+0.54570023840243842 	11.8862474186002061 	
+1.48867902407078145 	-5.48427538647323942 	
+-6.15587643151411612 	6.74361516830442742 	
+-0.349433002185481179 	-6.23282874401665588 	
+-2.64624601969778839 	3.41977326768260959 	
+-2.94286030596189985 	-2.7536407661812845 	
+5.73828756539099682 	-5.5731081762760617 	
+-4.97225940963330615 	3.6550947464273591 	
+-6.10148670149677752 	-3.5099263102824203 	
+1.55044181583256746 	14.6775034364667842 	
+0.782385337009624093 	25.4813713200397167 	
+-1.53273115443669905 	4.58635057373372401 	
+10.4172385843634707 	6.8663333943362943 	
+-0.368394943953074205 	20.6264920696709204 	
+5.60810932980211341 	6.78678373498030751 	
+6.71791590246602066 	-5.80482782600071445 	
+0.388121780023255425 	-3.46219858678926329 	
+2.83447313288851621 	7.76167610821283915 	
+-1.54630702793932451 	0.277670233017959622 	
+8.98371476935886726 	-0.793959411588884501 	
+2.70994943537675281 	14.8040061565928838 	
+6.68573003918294617 	0.0462762095938224136 	
+-4.29261320816485004 	-2.95101472907809592 	
+-4.54151110825021309 	16.1599843952047344 	
+-5.15501226513595601 	6.64257668637025311 	
+1.9073310659665581 	2.85019230852685856 	
+1.43237792951583942 	11.0408267483431235 	
+-1.36458349374152954 	-1.30153240656581781 	
+-4.30599353659678563 	-10.9529742342064758 	
+0.0316284965242700083 	1.25596215072053163 	
+-2.60581041885561548 	-6.14526724575121097 	
+-0.312851078676800676 	4.88991765871333239 	
+5.29462188951449519 	-16.5770775166692452 	
+-4.00703215573595894 	2.75362305989386025 	
+8.72817290377258459 	9.97806304735738259 	
+0.312638773499881673 	-2.14323866156373199 	
+-1.81776211067225546 	-44.0518064486709733 	
+2.55609366479082256 	-4.74886921353495595 	
+-0.833149682701808447 	11.5472059416397173 	
+-1.51305776067360043 	-109.480058674138334 	
+0.963485614629060239 	-6.81413026706003766 	
+-8.58496213584483847 	13.0940886747311964 	
+0.0915420976403832215 	5.86702156899371197 	
+4.79191376947731396 	-6.83937697735480477 	
+0.534268386342521273 	-12.4849809673474379 	
+2.17337508306685523 	-14.4118021194800612 	
+0.140164109164259837 	62.16477814568497 	
+0.167570766918804098 	-24.6465822114029365 	
+1.23667489086611737 	1.84320815139914407 	
+-0.440814885865148109 	-1.5540078086496576 	
+3.86906628870645619 	5.74016181018549343 	
+-4.46332178439692751 	14.5109110420846736 	
+0.978359207200913406 	-0.176691538655378344 	
+-0.253112600078602312 	11.6452773486085555 	
+9.90282705194929491 	7.59072425472290746 	
+-1.48088213993077722 	-12.6227827040926677 	
+-2.07209465738966392 	6.02898523253392682 	
+0.0851342612633791129 	65.918637305018251 	
+-4.36632696181662183 	-11.3811871240997462 	
+-1.71368855874337322 	1.31764702087338548 	
+2.15686598960616616 	18.4587976763140915 	
+0.511354889667731349 	-43.8065302488140631 	
+3.7998582616395109 	2.19302119575025722 	
+2.20001654545043479 	-4.52230341999792085 	
+-3.10342048881173493 	1.69054390982317559 	
+2.49515664288340444 	-12.6407876685416056 	
+-2.05596705726674811 	-15.4018265269667936 	
+2.45357401304169231 	-17.9766881878872269 	
+14.0087664049932883 	-2.14150296573378363 	
+-0.458146105055885766 	-94.2866379405297295 	
+-7.43642773069070628 	-0.417123011304259772 	
+-0.401022533514366897 	12.800581209304859 	
+-0.954443961086527848 	-18.250506249697839 	
+-3.53127796312637265 	1.20073147588584428 	
+-4.42051731555071115 	-59.221195467169288 	
+1.77815493365281663 	47.1410132671280238 	
+-7.07469141008806623 	-2.82886733983377647 	
 ]
 ;
 training_inputs = *2 ->MemoryVMatrix(
@@ -471,6 +471,7 @@
 source = *0 ;
 fieldnames = []
 ;
+deep_copy_memory_data = 1 ;
 writable = 0 ;
 length = 225 ;
 width = 1 ;
@@ -480,6 +481,7 @@
 extrasize = 0 ;
 metadatadir = ""  )
 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
 n_examples = 225 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -233,6 +233,7 @@
 source = *0 ;
 fieldnames = []
 ;
+deep_copy_memory_data = 1 ;
 writable = 0 ;
 length = 225 ;
 width = 3 ;
@@ -260,231 +261,231 @@
 weight_decay = 0.0100000000000000002 ;
 include_bias = 1 ;
 params = 225  2  [ 
-2.71513510622207166 	6.96796926050934751 	
--0.924570620987045566 	-5.50999860198858826 	
-2.74766209737875133 	16.2764471019206418 	
--0.897331116697557052 	24.1696624838867358 	
--0.257145462011077963 	-2.68785159119476535 	
--1.3282966539726655 	-18.5932956256296649 	
--5.49258347509019718 	-1.57002604298704584 	
-1.46070534954897835 	-3.53276434233635506 	
--1.038126171674709 	-1.2073115958693148 	
-1.02580172581497386 	-26.7245637328128929 	
-0.791254761045932176 	82.4469215737125865 	
-0.836570941006688051 	-16.4202630459276833 	
--1.58240826854525074 	-6.65940808938515527 	
--8.24967860415612364 	-3.49957089648365738 	
-0.9782411438628974 	-4.92116475768973771 	
-0.0717117261764370351 	-3.09067303050886988 	
--2.58531178640483761 	12.035458429525173 	
-3.211076444250756 	5.97223155883393897 	
-1.43705644733268212 	94.2974631733484614 	
-0.651016019176285488 	1.95403761231488304 	
--1.20978628334898786 	-7.36323983157156103 	
--0.676733526068380775 	-31.4356962338337986 	
--3.71894302925004316 	-5.30412867324711534 	
--2.6818491206821955 	-0.224631918494603705 	
--1.88827683784314537 	-11.2808510609389074 	
-1.56552507696732834 	-5.27853909851336311 	
--2.51225078969590498 	-3.22314543003026799 	
--8.82457968582814622 	-1.42245769348111262 	
--5.49712489654685843 	-14.7599950960045039 	
-8.47592427242672741 	8.11107085181684795 	
-2.34953305575425997 	4.63659562959487381 	
--1.64301101095052804 	11.4873476009648563 	
-0.580660833463284676 	13.1755650470951462 	
--0.650250885492095976 	1.90203710703754014 	
--3.87917543422272804 	5.03518774179621609 	
-1.70397379112897029 	1.34775554557823152 	
--2.78522303432040363 	-64.1292521132013178 	
-2.32700743878965444 	24.7436125596276284 	
--5.39566293426405519 	-9.82739376173207368 	
-12.9203582472648133 	5.29957416697557626 	
--10.7455769141731885 	89.9090483177333653 	
-4.31713435896750308 	-5.18586285076809528 	
-0.848047232583675648 	-2.17948525292697681 	
--9.46810320629667324 	-0.220748951447968828 	
-7.21414655874827737 	7.46866878382293553 	
--1.75858145600769644 	11.2871954305640294 	
-1.96560792644487359 	-6.52458829385446837 	
--2.79380808107613055 	-5.09955504191000042 	
--0.79905931023683674 	-12.925504941414971 	
--4.25437046157972976 	8.96615214816773154 	
--9.43622335236653065 	3.78339978190649218 	
-4.36697916817812093 	-0.68750387907888455 	
--2.18796130106569242 	-6.69802616230909642 	
-3.17982632559557965 	-1.4370316032819157 	
--4.31414536641474289 	-5.51667561668654649 	
--2.74519292766675793 	7.73168925235037285 	
-1.0537820640848472 	0.178110338215527242 	
-0.460217896787073477 	10.058544903984771 	
--0.0106331952212118993 	-19.6790725953963772 	
-0.424465188958454087 	-15.8551532141373048 	
-0.567218573772457435 	-2.69519041953220029 	
-4.45718670597664524 	5.62393864871897797 	
-2.19961229164894245 	-5.42279390073130685 	
--0.228762156047200543 	-0.831542078921280403 	
--2.40103186222691534 	15.8373411775187218 	
--1.34006398347233469 	-12.0359127014981127 	
-4.03810217513344138 	-2.91524838149503962 	
-0.593248196381594317 	0.218503040354157629 	
+2.7151351062236615 	6.96796926051022236 	
+-0.924570620989403458 	-5.50999860198525226 	
+2.74766209737707889 	16.2764471019291541 	
+-0.897331116702694054 	24.1696624839723277 	
+-0.257145462010700432 	-2.68785159119131878 	
+-1.3282966539723533 	-18.5932956256279311 	
+-5.49258347507660183 	-1.57002604296742554 	
+1.46070534955196196 	-3.53276434234297421 	
+-1.03812617167629262 	-1.20731159585204617 	
+1.02580172582147133 	-26.7245637329914061 	
+0.791254761045375288 	82.4469215737143202 	
+0.836570941010414737 	-16.420263045944921 	
+-1.58240826855158301 	-6.65940808940268703 	
+-8.24967860415081411 	-3.4995708964716874 	
+0.978241143861720674 	-4.92116475768050154 	
+0.0717117261694705244 	-3.09067303052573861 	
+-2.58531178640539849 	12.0354584295284699 	
+3.21107644425249639 	5.97223155883275147 	
+1.43705644733271809 	94.2974631733482198 	
+0.651016019177080518 	1.95403761232638362 	
+-1.20978628334706184 	-7.36323983158574702 	
+-0.676733526068720059 	-31.4356962338422257 	
+-3.71894302925004183 	-5.30412867323935266 	
+-2.68184912068139347 	-0.224631918495068694 	
+-1.88827683783783651 	-11.2808510609680628 	
+1.56552507696869569 	-5.27853909851396796 	
+-2.51225078969575177 	-3.22314543004070897 	
+-8.82457968581596752 	-1.42245769348021844 	
+-5.4971248965487387 	-14.759995096029046 	
+8.47592427242844337 	8.11107085181370557 	
+2.34953305575392202 	4.63659562958691041 	
+-1.64301101094937252 	11.4873476009575572 	
+0.580660833465362791 	13.1755650470871277 	
+-0.650250885491696851 	1.90203710703855133 	
+-3.87917543422287148 	5.03518774179589901 	
+1.7039737911280386 	1.34775554555850574 	
+-2.78522303432125629 	-64.1292521131980919 	
+2.32700743878955185 	24.743612559642397 	
+-5.39566293426395394 	-9.82739376173094037 	
+12.9203582472649128 	5.29957416698956862 	
+-10.7455769141728563 	89.9090483177386801 	
+4.31713435895606157 	-5.18586285077431786 	
+0.848047232584521082 	-2.17948525293295869 	
+-9.4681032062965258 	-0.22074895143857387 	
+7.21414655874796917 	7.46866878382939792 	
+-1.75858145601429361 	11.2871954305960074 	
+1.96560792644405802 	-6.52458829385219907 	
+-2.79380808107006473 	-5.09955504189952258 	
+-0.799059310236704512 	-12.9255049414569196 	
+-4.25437046158016585 	8.96615214816819339 	
+-9.43622335236651288 	3.78339978190638559 	
+4.36697916816686238 	-0.687503879096243442 	
+-2.18796130106503472 	-6.69802616230832459 	
+3.17982632559527989 	-1.43703160328240198 	
+-4.3141453664152607 	-5.51667561668627471 	
+-2.74519292766851297 	7.73168925236620286 	
+1.05378206408368125 	0.178110338223795794 	
+0.460217896787359693 	10.0585449039864159 	
+-0.0106331952235446964 	-19.6790725953361481 	
+0.424465188958869921 	-15.8551532141411631 	
+0.567218573767627854 	-2.6951904195179508 	
+4.45718670597547018 	5.62393864872558336 	
+2.19961229165357031 	-5.42279390073859879 	
+-0.228762156049838294 	-0.831542078930034179 	
+-2.4010318622261817 	15.8373411775190593 	
+-1.34006398346898026 	-12.0359127014414575 	
+4.0381021751345596 	-2.91524838149849552 	
+0.593248196395251171 	0.218503040318817232 	
 -0.894620455881181775 	1.01068418956454376 	
--0.905934974062951315 	-1.04508844319939009 	
-8.00428356105542704 	-1.26168634488815745 	
-0.619982738889116813 	54.1218393092184655 	
--0.705573849337669534 	-1.25381377885319911 	
-0.871066182005948986 	40.4927882352012602 	
--7.05074671248575058 	71.3212740911221061 	
--2.22324163214598158 	1.07345612751800834 	
--2.44557668836901909 	4.29373139551074523 	
-9.8216236808294628 	6.64093872401048912 	
--9.18335934248439933 	-3.97745257894915438 	
--0.836507465302499531 	-1.03211418641978869 	
--0.053497546963581534 	19.5696227138299683 	
-1.07568581176873801 	-5.10104965788098763 	
--0.51895274081205478 	15.8636310713846687 	
--2.84087926537020463 	-12.9191491848348825 	
-2.04826664350002785 	12.2408805586060261 	
-1.80375307384701089 	11.6859209915609643 	
--1.25621312029488119 	-2.02266034407121387 	
--0.0484853946924818577 	3.80498181684614956 	
--0.677242736032365844 	-42.6254775342041086 	
-1.5916269901049056 	-5.48763586146515259 	
--1.21972683978988194 	-1.50185397433043755 	
-1.24858166506403889 	4.87092530126864176 	
--7.04338199489057715 	17.3390887919734524 	
-3.47070361924793991 	1.42970456326488016 	
--4.26550633389315159 	-1.73980331015090517 	
-2.12547785989944504 	3.62923159122205607 	
-2.5325993386476946 	-38.0016204477384605 	
-2.21881274683430751 	-0.0138947589494949966 	
-3.14760002032383079 	-1.50792558449675762 	
-1.08730538645968133 	-5.46742192799871596 	
-0.805042144582074792 	-53.0054722631666593 	
--1.33448103050452405 	5.00293548641393393 	
--4.27305110186876824 	-0.418543428368392034 	
-1.70132006444656469 	-5.38749080327059016 	
--1.12532164909935517 	33.3451642986786609 	
--2.70415771236861247 	3.89572987273113291 	
-4.98777580381695707 	13.8821103996001245 	
-0.568511986632529109 	-6.88339363195469822 	
-2.78082104553195331 	-5.19833751432510915 	
--3.45195094389145929 	6.54989673235798353 	
--1.26923932125293937 	-82.4386151296812386 	
-6.7221655120617827 	-5.26765661015447861 	
--0.742712929107001374 	-2.30022073912255554 	
-1.55989131761590083 	-4.18345207234387129 	
-4.72158352753704147 	21.7128846359820926 	
--2.74322426786354923 	0.0600870120432775226 	
-5.66665271018074534 	-3.34386936425682135 	
--1.7405916077807202 	11.7068252411723854 	
-0.339841379787169384 	1.53151755701152847 	
--0.136156736944080564 	-12.0671867463559686 	
-2.99636501987962234 	1.68582324362498048 	
--1.96646003943842773 	-7.02906457262245166 	
--2.68253702359453339 	19.2149219172491286 	
-3.77166285715723992 	-15.0634217849763132 	
-1.10118597277121655 	11.2002484600206493 	
--0.21265096744765738 	0.824744814217108657 	
-3.73652792828045532 	-7.81218626521123305 	
--0.1517753718816853 	1.00699915467459311 	
-3.72484877108446399 	5.92264185066637783 	
--0.0755790292585322709 	-3.91733851844826209 	
--1.69431689893366721 	3.02479371219842008 	
-0.0753423487892740662 	14.9935199826461982 	
-2.23109480398673998 	-2.59140850098083764 	
-6.91777882726973914 	41.5668021912243546 	
-1.87173910300826329 	-16.2617861471825336 	
--2.61089444121170011 	-39.3944996228537363 	
+-0.905934974062951648 	-1.04508844319938632 	
+8.00428356105546257 	-1.26168634488796494 	
+0.619982738884564899 	54.1218393093347103 	
+-0.705573849334636627 	-1.2538137788610173 	
+0.871066182002223743 	40.4927882352060706 	
+-7.05074671248599127 	71.3212740911236835 	
+-2.22324163215964843 	1.0734561275325698 	
+-2.44557668837193853 	4.2937313954156906 	
+9.82162368083046999 	6.64093872400913376 	
+-9.18335934249402008 	-3.97745257891762716 	
+-0.836507465302499753 	-1.0321141864197878 	
+-0.0534975469631229703 	19.5696227138288812 	
+1.07568581177046618 	-5.10104965789005504 	
+-0.518952740814859759 	15.8636310713528932 	
+-2.84087926537195301 	-12.9191491848439277 	
+2.04826664350717103 	12.2408805584981177 	
+1.80375307384732975 	11.6859209915770084 	
+-1.25621312028974019 	-2.02266034406559303 	
+-0.0484853946913663472 	3.8049818168441556 	
+-0.677242736030050585 	-42.625477534183851 	
+1.59162699009028663 	-5.48763586147435767 	
+-1.21972683978848395 	-1.50185397433300194 	
+1.24858166506431711 	4.87092530127280199 	
+-7.04338199489003269 	17.3390887919741523 	
+3.47070361924823967 	1.42970456326448381 	
+-4.2655063338911301 	-1.73980331015853862 	
+2.12547785989874116 	3.62923159123009187 	
+2.53259933864800901 	-38.0016204477437114 	
+2.21881274683373908 	-0.0138947589423880184 	
+3.1476000203292771 	-1.50792558450150538 	
+1.08730538646035324 	-5.46742192799836069 	
+0.805042144582562846 	-53.0054722631629431 	
+-1.33448103050428291 	5.00293548641483543 	
+-4.27305110186990333 	-0.418543428366281389 	
+1.70132006444962425 	-5.38749080326675145 	
+-1.12532164909613996 	33.3451642986689407 	
+-2.70415771239980796 	3.8957298727264229 	
+4.98777580381757346 	13.8821103996191333 	
+0.568511986630580113 	-6.88339363194706877 	
+2.78082104553625609 	-5.19833751433356372 	
+-3.45195094389070523 	6.54989673235655356 	
+-1.26923932124764582 	-82.4386151297649405 	
+6.72216551206190616 	-5.26765661015598763 	
+-0.742712929107524844 	-2.30022073912311376 	
+1.55989131761279332 	-4.18345207235786098 	
+4.72158352754091215 	21.7128846360060805 	
+-2.74322426786452844 	0.0600870120549067896 	
+5.66665271018079864 	-3.34386936425720371 	
+-1.7405916077822694 	11.7068252411821998 	
+0.339841379787524656 	1.53151755701120851 	
+-0.136156736953116003 	-12.0671867463523572 	
+2.99636501990413606 	1.68582324363194558 	
+-1.96646003944210279 	-7.02906457252052252 	
+-2.68253702359536872 	19.2149219172472989 	
+3.77166285715782834 	-15.0634217849804468 	
+1.10118597276613972 	11.2002484600089556 	
+-0.212650967437634342 	0.824744814209136368 	
+3.73652792828373403 	-7.81218626519529735 	
+-0.151775371881687632 	1.00699915467459045 	
+3.72484877108447021 	5.92264185066602611 	
+-0.0755790292600937996 	-3.91733851844509173 	
+-1.69431689893300641 	3.02479371219719839 	
+0.0753423487888285753 	14.9935199826456476 	
+2.23109480398829207 	-2.59140850098041486 	
+6.91777882726934212 	41.5668021912209156 	
+1.87173910301644164 	-16.2617861474499854 	
+-2.610894441212122 	-39.3944996228497075 	
 -0.836329838029520878 	0.975444302819992171 	
-3.00474481016317796 	-21.1150530969481025 	
--8.62460330276778109 	3.04533586235480191 	
--11.9994315455979592 	-1.47919451474482977 	
-4.44069944340338374 	-18.2589018941275931 	
--2.27968651835906844 	1.02704086191163668 	
-6.31981383647014194 	1.54445289435077759 	
--5.26600895531925595 	-35.7977533157710113 	
--0.481694055896209672 	2.21195940922255518 	
-7.77036312147073716 	1.09313418809460505 	
--4.68097213985389882 	-6.20894287206248841 	
--1.0971415680584542 	-2.85536759176862898 	
-0.545700238407987648 	11.8862474186525926 	
-1.48867902407183705 	-5.48427538646079693 	
--6.1558764315134562 	6.74361516831379859 	
--0.34943300218334733 	-6.2328287440142951 	
--2.64624601969912154 	3.41977326768898227 	
--2.94286030596212944 	-2.75364076618103537 	
-5.73828756539082274 	-5.57310817627025834 	
--4.97225940962958202 	3.65509474643111654 	
--6.10148670149821548 	-3.5099263102813012 	
-1.55044181583261187 	14.6775034364680952 	
-0.782385337009910753 	25.481371320069055 	
--1.53273115443696883 	4.58635057373261112 	
-10.4172385843581061 	6.86633339432157452 	
--0.368394943949268971 	20.6264920696424987 	
-5.60810932982655341 	6.78678373501238852 	
-6.71791590246840631 	-5.80482782599815295 	
-0.388121780022244123 	-3.46219858678257397 	
-2.83447313289494707 	7.76167610817524611 	
--1.54630702794617703 	0.277670233006282186 	
-8.9837147693589845 	-0.793959411589270636 	
-2.70994943537632693 	14.8040061566573282 	
-6.68573003919279518 	0.0462762095885054375 	
--4.29261320816020042 	-2.95101472906158513 	
--4.54151110825627313 	16.1599843952082907 	
--5.15501226513589383 	6.64257668637242293 	
-1.90733106596579205 	2.8501923085260672 	
-1.43237792951475584 	11.0408267483489499 	
--1.36458349374178489 	-1.30153240656658253 	
--4.30599353659890038 	-10.9529742342083214 	
-0.0316284965238302559 	1.25596215070410944 	
--2.60581041885303533 	-6.14526724575781902 	
--0.312851078671457561 	4.8899176585817985 	
-5.29462188951335122 	-16.5770775166756401 	
--4.00703215573606908 	2.75362305989316081 	
-8.7281729037734479 	9.97806304735961547 	
-0.312638773494846978 	-2.14323866166241261 	
--1.81776211066742133 	-44.0518064486871594 	
-2.55609366479101618 	-4.74886921353842606 	
--0.833149682703483663 	11.5472059416640835 	
--1.51305776067499043 	-109.480058674132408 	
-0.96348561462821225 	-6.81413026705121183 	
--8.58496213584385792 	13.0940886747191065 	
-0.0915420976503914519 	5.86702156901551231 	
-4.79191376944235348 	-6.83937697733123517 	
-0.534268386340944978 	-12.4849809673482426 	
-2.17337508306810578 	-14.4118021194967199 	
-0.140164109164443162 	62.1647781456503097 	
-0.167570766919056896 	-24.6465822115656401 	
-1.23667489086769566 	1.84320815143338623 	
--0.440814885867638395 	-1.55400780863914356 	
-3.86906628870721292 	5.74016181018787464 	
--4.46332178439648342 	14.5109110420769749 	
-0.978359207199548941 	-0.176691538654043939 	
--0.253112600092994244 	11.6452773485220398 	
-9.90282705194954538 	7.59072425472390933 	
--1.48088213993055762 	-12.6227827040919376 	
--2.07209465738824505 	6.0289852325298714 	
-0.0851342612640778873 	65.9186373049947889 	
--4.36632696181617952 	-11.3811871241025226 	
--1.71368855874052861 	1.31764702088757302 	
-2.15686598960512166 	18.4587976762973902 	
-0.511354889660306289 	-43.8065302486330737 	
-3.79985826164594931 	2.1930211957444623 	
-2.2000165454424403 	-4.52230342001110674 	
--3.10342048881031829 	1.69054390982302927 	
-2.49515664287580075 	-12.6407876685394616 	
--2.05596705724929718 	-15.4018265269691028 	
-2.4535740130399093 	-17.9766881878857845 	
-14.0087664049954643 	-2.1415029657413216 	
--0.458146105055854791 	-94.286637940529971 	
--7.43642773069096918 	-0.417123011311428815 	
--0.401022533507097045 	12.8005812092002085 	
--0.954443961086675396 	-18.2505062496947659 	
--3.53127796312802422 	1.20073147588600682 	
--4.42051731554893923 	-59.221195467202314 	
-1.77815493364989341 	47.1410132671640412 	
--7.07469141009799074 	-2.82886733985023353 	
+3.00474481016258865 	-21.1150530969548456 	
+-8.6246033027724156 	3.04533586233992315 	
+-11.9994315456001406 	-1.47919451474942232 	
+4.44069944340081424 	-18.2589018941206973 	
+-2.27968651835379399 	1.02704086194237587 	
+6.31981383646970141 	1.54445289434102828 	
+-5.26600895531991675 	-35.797753315782785 	
+-0.481694055895810824 	2.2119594092289705 	
+7.77036312147051333 	1.09313418809397089 	
+-4.6809721398523001 	-6.20894287205482076 	
+-1.09714156805876417 	-2.85536759175997856 	
+0.54570023840243842 	11.8862474186002061 	
+1.48867902407078145 	-5.48427538647323942 	
+-6.15587643151411612 	6.74361516830442742 	
+-0.349433002185481179 	-6.23282874401665588 	
+-2.64624601969778839 	3.41977326768260959 	
+-2.94286030596189985 	-2.7536407661812845 	
+5.73828756539099682 	-5.5731081762760617 	
+-4.97225940963330615 	3.6550947464273591 	
+-6.10148670149677752 	-3.5099263102824203 	
+1.55044181583256746 	14.6775034364667842 	
+0.782385337009624093 	25.4813713200397167 	
+-1.53273115443669905 	4.58635057373372401 	
+10.4172385843634707 	6.8663333943362943 	
+-0.368394943953074205 	20.6264920696709204 	
+5.60810932980211341 	6.78678373498030751 	
+6.71791590246602066 	-5.80482782600071445 	
+0.388121780023255425 	-3.46219858678926329 	
+2.83447313288851621 	7.76167610821283915 	
+-1.54630702793932451 	0.277670233017959622 	
+8.98371476935886726 	-0.793959411588884501 	
+2.70994943537675281 	14.8040061565928838 	
+6.68573003918294617 	0.0462762095938224136 	
+-4.29261320816485004 	-2.95101472907809592 	
+-4.54151110825021309 	16.1599843952047344 	
+-5.15501226513595601 	6.64257668637025311 	
+1.9073310659665581 	2.85019230852685856 	
+1.43237792951583942 	11.0408267483431235 	
+-1.36458349374152954 	-1.30153240656581781 	
+-4.30599353659678563 	-10.9529742342064758 	
+0.0316284965242700083 	1.25596215072053163 	
+-2.60581041885561548 	-6.14526724575121097 	
+-0.312851078676800676 	4.88991765871333239 	
+5.29462188951449519 	-16.5770775166692452 	
+-4.00703215573595894 	2.75362305989386025 	
+8.72817290377258459 	9.97806304735738259 	
+0.312638773499881673 	-2.14323866156373199 	
+-1.81776211067225546 	-44.0518064486709733 	
+2.55609366479082256 	-4.74886921353495595 	
+-0.833149682701808447 	11.5472059416397173 	
+-1.51305776067360043 	-109.480058674138334 	
+0.963485614629060239 	-6.81413026706003766 	
+-8.58496213584483847 	13.0940886747311964 	
+0.0915420976403832215 	5.86702156899371197 	
+4.79191376947731396 	-6.83937697735480477 	
+0.534268386342521273 	-12.4849809673474379 	
+2.17337508306685523 	-14.4118021194800612 	
+0.140164109164259837 	62.16477814568497 	
+0.167570766918804098 	-24.6465822114029365 	
+1.23667489086611737 	1.84320815139914407 	
+-0.440814885865148109 	-1.5540078086496576 	
+3.86906628870645619 	5.74016181018549343 	
+-4.46332178439692751 	14.5109110420846736 	
+0.978359207200913406 	-0.176691538655378344 	
+-0.253112600078602312 	11.6452773486085555 	
+9.90282705194929491 	7.59072425472290746 	
+-1.48088213993077722 	-12.6227827040926677 	
+-2.07209465738966392 	6.02898523253392682 	
+0.0851342612633791129 	65.918637305018251 	
+-4.36632696181662183 	-11.3811871240997462 	
+-1.71368855874337322 	1.31764702087338548 	
+2.15686598960616616 	18.4587976763140915 	
+0.511354889667731349 	-43.8065302488140631 	
+3.7998582616395109 	2.19302119575025722 	
+2.20001654545043479 	-4.52230341999792085 	
+-3.10342048881173493 	1.69054390982317559 	
+2.49515664288340444 	-12.6407876685416056 	
+-2.05596705726674811 	-15.4018265269667936 	
+2.45357401304169231 	-17.9766881878872269 	
+14.0087664049932883 	-2.14150296573378363 	
+-0.458146105055885766 	-94.2866379405297295 	
+-7.43642773069070628 	-0.417123011304259772 	
+-0.401022533514366897 	12.800581209304859 	
+-0.954443961086527848 	-18.250506249697839 	
+-3.53127796312637265 	1.20073147588584428 	
+-4.42051731555071115 	-59.221195467169288 	
+1.77815493365281663 	47.1410132671280238 	
+-7.07469141008806623 	-2.82886733983377647 	
 ]
 ;
 training_inputs = *6 ->MemoryVMatrix(
@@ -719,6 +720,7 @@
 source = *0 ;
 fieldnames = []
 ;
+deep_copy_memory_data = 1 ;
 writable = 0 ;
 length = 225 ;
 width = 1 ;
@@ -728,6 +730,7 @@
 extrasize = 0 ;
 metadatadir = ""  )
 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
 n_examples = 225 ;
@@ -753,6 +756,7 @@
 save_test_outputs = 0 ;
 call_forget_in_run = 1 ;
 save_test_costs = 0 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
@@ -801,6 +805,8 @@
 learner = *4  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;
+forward_nstages = 0 ;
+random_gen = *0 ;
 stage = 1 ;
 n_examples = 225 ;
 inputsize = 1 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,3 +1,3 @@
 MATRIX 75 1 DOUBLE LITTLE_ENDIAN                               
-_hXBK?????$,?l??ia?????5m? ??)=0??b???o?y[??????i???x?>V???H???????E%v????<8dl??h???o??xe?????vS???????'???f6?x??????x????????T??-j???y?z?)	2?V??
-?kb????A??^x???(R?T??oi?H??????]?\w??????pP???,)U[??)??y:???0????O?\???uQ?~?m?????:?v????????5q?{????t??pv?c?T?D?^&z?z?R?3A????y*u?up?P??P???G?A??????E??1????g??mn??tWm?Oa?(??@?Q?????G???z???K??g?Z?7???La??????[y*(Xr?A/y?IB???<?rmx??e?x'?????u9K????B{????5r??x???$i?????????????|??^s??cx?g??6y???D???????????o:??H??[???????=??f+??????Ym;???????W?????b????%/???J91? r????~cP???*?????*??!e????C???????????
\ No newline at end of file
+?hXBK???6?$,?l??a?????5m? ???=0??b???o?y[????????i??x?>V?^??H???????E%v??XK<8dl?~`???o?I?e?????vS???y???'???/4?x???????x???^G????T??\j???y?l?)	2?V?b?kb???u?@??^x???(R?T???a?H???|?]?\w?2r???pP?jZ,)U[?????y:????q???O??<???uQ?\?m???????v??????V?5q?{?Ja???t?3?v?c?T???^&z?z???4A????=*u?up?=H??P?????A?????*?E??1??D3g??mn??rWm?Oa?2??@?Q????G???x???K????Z?7????X??????
+??????????o:???H??[???e???=??:8??????~m;???????W??????b????%/???w?81? r????~cP?M*?????J??!e???~4C?????%??????
\ No newline at end of file

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 75 ;
 sumsquarew_ = 75 ;
-sum_ = 8.32876645342359012 ;
-sumsquare_ = 7.8668222754000583 ;
-sumcube_ = 10.9548142657038454 ;
-sumfourth_ = 17.4914867139879391 ;
-min_ = 0.000403386661488822746 ;
-max_ = 1.79622562497000327 ;
+sum_ = 8.32876645342360256 ;
+sumsquare_ = 7.8668222754001027 ;
+sumcube_ = 10.95481426570392 ;
+sumfourth_ = 17.4914867139880847 ;
+min_ = 0.0004033866614889374 ;
+max_ = 1.79622562497000371 ;
 agmemin_ = 13 ;
 agemax_ = 8 ;
-first_ = 0.0133271392087507996 ;
-last_ = 0.0703378652394482801 ;
+first_ = 0.0133271392087509453 ;
+last_ = 0.0703378652394524712 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 300 ;
 sumsquarew_ = 300 ;
-sum_ = 19.3910908077377719 ;
-sumsquare_ = 13.9110920621987386 ;
-sumcube_ = 15.8389851220146944 ;
-sumfourth_ = 22.1170742798113977 ;
-min_ = 3.99687001153608957e-05 ;
-max_ = 1.79622562497000327 ;
+sum_ = 19.3910908077380881 ;
+sumsquare_ = 13.9110920621991241 ;
+sumcube_ = 15.8389851220151474 ;
+sumfourth_ = 22.1170742798119626 ;
+min_ = 3.99687001152846492e-05 ;
+max_ = 1.79622562497000371 ;
 agmemin_ = 232 ;
 agemax_ = 8 ;
-first_ = 0.00559245542604580592 ;
-last_ = 0.0703378652394482801 ;
+first_ = 0.00559245542604559776 ;
+last_ = 0.0703378652394524712 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = nan ;
 first_ = nan ;
 last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -37,12 +39,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.052180113517651637 ;
-max_ = 0.052180113517651637 ;
+min_ = 0.0521801135176526432 ;
+max_ = 0.0521801135176526432 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.052180113517651637 ;
-last_ = 0.052180113517651637 ;
+first_ = 0.0521801135176526432 ;
+last_ = 0.0521801135176526432 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL7398"
+__REVISION__ = "PL8398"
 DataOpt.data                                  = PLEARNDIR:examples/data/test_suite/sin_signcos_1x_2y.amat
 HyperKRR.kfold                                = 10
 HyperKRR.lambda_list                          = 1e-8,1e-6,1e-4,1e-2,1e0

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -47,6 +47,7 @@
 ]
 ;
 training_inputs = *0 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
 n_examples = -1 ;
@@ -72,6 +73,7 @@
 save_test_outputs = 0 ;
 call_forget_in_run = 1 ;
 save_test_costs = 0 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
@@ -120,6 +122,8 @@
 learner = *5  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;
+forward_nstages = 0 ;
+random_gen = *0 ;
 stage = 0 ;
 n_examples = -1 ;
 inputsize = -1 ;
@@ -144,6 +148,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -12,231 +12,231 @@
 weight_decay = 1.00000000000000002e-08 ;
 include_bias = 1 ;
 params = 225  2  [ 
-1366616.76869139355 	-6185964.89130615257 	
-849446.462325189379 	3337888.51939160004 	
-10184.5537716248291 	13716774.6870256364 	
--997.650785046500118 	-2108.90240737419799 	
--3248959.72653274005 	528148.782342329156 	
--1943097.68024857552 	-4901102.64686471969 	
--1312429.9126792301 	10398196.8188508581 	
--158794.058776748658 	-1386582.14899571799 	
--1962.6410987352383 	-7054.31950725431034 	
--572903.619195229956 	7147677.50085736625 	
-3755531.68937593233 	68129807.3667383641 	
-390299.559027336538 	3403200.42786649195 	
--1608441.83044905681 	-7861453.46051791031 	
--7287794.00923610292 	-1053642.94474513293 	
-1592719.34113233862 	-2551914.16670501884 	
--321929.414245234919 	2427258.14508416271 	
--1600265.36663510208 	6529643.91745474748 	
-1917518.03298101691 	8173398.79621593282 	
--50.0911550445243279 	1580.45032349210237 	
-2750475.90512487944 	7382194.00645548198 	
-264291.456274428871 	-1569562.63893406698 	
-331819.272982495371 	-5391371.79910762887 	
--4401561.05349927396 	-4677960.47644133586 	
--1082838.3290902304 	636277.829709192039 	
--1741818.78390260786 	-11587605.2968365438 	
-300302.451371876406 	-3858730.31795606297 	
--26622.7234530920541 	212868.344704747753 	
-666528.240721351001 	3212431.98968670657 	
--1099.60880165658682 	5095.49231101872829 	
-6684811.54933989234 	-1312542.71212858148 	
-2841737.0537566198 	9900044.66552052088 	
-256.791053833973422 	927.919899973866109 	
--747260.287556492724 	1765149.18080786197 	
-1787299.59581780853 	-2930370.66858883994 	
--4761642.46401395556 	-857422.759164088173 	
-1861104.19019156229 	-13840896.5967209041 	
-341116.802705410169 	-43141078.0743555352 	
-169608.087400039891 	-2797993.33357280539 	
--3481163.38401352149 	-1140348.05882267887 	
-12707300.2583892494 	-8763708.02537456714 	
--6975599.85001251288 	84985235.194521144 	
-804670.708787796204 	-4425492.76176093053 	
-953.103327609957773 	3466.98951959306669 	
--9397758.51891290583 	358007.044675878773 	
-6651586.35805763677 	6155402.90434635524 	
--2767027.6851156042 	-1443320.7631305547 	
--136.030844553251939 	-492.326757368743472 	
--3171958.00430362998 	1182975.79363097087 	
-1044513.77211671858 	20813344.3670466878 	
--4450957.88235075772 	9143841.99734943733 	
--7535595.58832198288 	5149952.58349180594 	
-749046.921188037028 	639031.878722379333 	
--2270569.334879498 	-6509706.08528705314 	
--716733.018778344383 	-16732609.8245362248 	
--5326827.88161650114 	-3383004.02813892299 	
--4958709.41615742445 	10219858.9617376775 	
-1402658.47472910746 	-6585633.927737833 	
-1726906.45002966304 	-6898981.73086728156 	
-412095.614195419243 	26193948.5870998167 	
-396524.437640503107 	-8979638.71408112347 	
--361415.67939353385 	-901901.59727373533 	
-5314130.40951517411 	9291094.7646400705 	
-4602934.24297516141 	-5946574.54280858487 	
--1070874.35669219703 	-30368.1909077826131 	
--263755.43672070204 	4156245.92873147735 	
-207.569616159466619 	484.269676083718821 	
-3074588.03519257531 	-2857510.94609070988 	
-1878676.89029815234 	-9811182.40645078197 	
+1366617.64288142417 	-6185966.42974149343 	
+849444.24850008823 	3337894.27500708215 	
+10182.7344990859001 	13716780.1884523034 	
+-997.650812257653911 	-2108.90314516664284 	
+-3248959.25093092956 	528152.402155514341 	
+-1943098.5018205368 	-4901103.5240961574 	
+-1312430.93940421287 	10398242.2345557716 	
+-158794.219702221249 	-1386584.94755934994 	
+-1962.64217521145042 	-7054.34295134580861 	
+-572906.919031283469 	7147676.00999723002 	
+3755531.23993762024 	68129807.598218292 	
+390301.69630502857 	3403201.26822880143 	
+-1608449.83081890712 	-7861451.71293165628 	
+-7287789.21068003681 	-1053643.76947448542 	
+1592718.23068070132 	-2551911.05277541373 	
+-321936.415265785181 	2427260.80256103165 	
+-1600266.67012444371 	6529648.04702275153 	
+1917519.42252595699 	8173396.13618640509 	
+-50.0911411813181786 	1580.45027172356254 	
+2750477.07776657119 	7382188.09886426199 	
+264293.764395976614 	-1569564.75455413456 	
+331819.579904250393 	-5391370.84535927698 	
+-4401560.04846040066 	-4677956.53513474762 	
+-1082837.19426669646 	636278.046444773907 	
+-1741812.14755935082 	-11587697.8482306562 	
+300303.60766241973 	-3858727.96783664078 	
+-26622.7193839248393 	212868.425504128536 	
+666528.15836237045 	3212421.69095607894 	
+-1099.60855265407508 	5095.49152712575597 	
+6684812.96044983342 	-1312544.95324070775 	
+2841736.6681027608 	9900043.35509689525 	
+256.791190743367622 	927.922970047414537 	
+-747257.641239341698 	1765146.99592787703 	
+1787300.11144392518 	-2930367.3656045692 	
+-4761642.62588602025 	-857422.029823462013 	
+1861103.87846565293 	-13840853.2794568371 	
+341116.086864745361 	-43141073.9513364583 	
+169606.265286486479 	-2797991.44352930924 	
+-3481163.31524779601 	-1140349.08526240964 	
+12707299.4785184041 	-8763707.46899035573 	
+-6975599.70754752122 	84985235.9055105001 	
+804671.225336260977 	-4425519.99369032402 	
+953.103858197436011 	3467.00093304487064 	
+-9397757.13732322119 	358010.360709724715 	
+6651586.54894186091 	6155406.35204745736 	
+-2767029.25241871504 	-1443275.77305316809 	
+-136.0309170520745 	-492.328388584458082 	
+-3171951.88933444768 	1182970.67210857128 	
+1044516.53955397825 	20813342.7160588577 	
+-4450957.92652958538 	9143841.89595435187 	
+-7535595.39281961601 	5149951.23592957109 	
+749048.58687946538 	639030.197572621983 	
+-2270569.25416577794 	-6509705.7799350014 	
+-716733.276295285672 	-16732608.9797926135 	
+-5326827.83259753045 	-3383004.60104250815 	
+-4958711.29914016835 	10219859.6197844204 	
+1402656.28198464261 	-6585635.32592584379 	
+1726907.71518324153 	-6898982.10842956696 	
+412096.171447259432 	26193976.1834124364 	
+396524.838631341292 	-8979640.05077948608 	
+-361417.872880718845 	-901904.263679497177 	
+5314130.71166251414 	9291096.93145320192 	
+4602939.2972927466 	-5946569.75966465473 	
+-1070877.2526368103 	-30372.0137253452231 	
+-263756.248004556925 	4156247.10297207767 	
+207.569698338024494 	484.27154318392121 	
+3074588.16341048665 	-2857511.89732546825 	
+1878677.76353513682 	-9811199.33801080659 	
 -0.903566651404326682 	1.02079102125227728 	
--0.922725599764326221 	-0.804165380466485669 	
-7926619.95202558581 	-1072946.92199276714 	
--417583.86867909634 	7775047.79883481003 	
--337996.771954034164 	15439.5829109386341 	
-1346323.44783454621 	10194330.6781451628 	
--867881.586292344145 	23695584.3018980399 	
--2028976.93054823088 	1656679.72875180421 	
--1572757.54866653238 	-14416031.021050252 	
-9497215.48082085699 	4589621.75002842583 	
--5683888.06716533843 	782707.8245822998 	
--0.841998910710579196 	-1.03485911528526775 	
-1259123.10628452338 	5412015.43995862547 	
--446617.124349066929 	132223.309307438787 	
--137.35184030357334 	-278.712857774059046 	
--619.491822707103438 	3195.4795886396937 	
-1330745.39480344485 	1115601.24691671995 	
--14985.633487573954 	-10429997.2341286689 	
--190575.136063572951 	-191610.758138739388 	
--388325.410311483894 	-6956949.21786825825 	
-1816359.53549249028 	-11960425.5674961712 	
-1564797.01975456439 	-5860708.85307790432 	
--1723630.25670568668 	-3000658.77547918353 	
-2064.51781319754582 	-14088.9484015295693 	
--4956111.72499638982 	5659955.24393226113 	
-5328355.23387095612 	730739.06183062389 	
--699048.62137132755 	-5046443.15819535311 	
-450.333638903209248 	-2039.44030913536631 	
-973347.651858710218 	-6241064.55017109588 	
-641436.685829726863 	5386883.81526193954 	
-4740628.47782350704 	-6290723.0028706314 	
-307054.179486245499 	2402056.85077607399 	
-559239.019541763235 	-29018740.2155409977 	
--3126070.65637546265 	-1025530.98060100572 	
--155801.197085267399 	7898600.1400711108 	
-2406986.86380285257 	2335537.62154969573 	
--1032554.23450594023 	-1378606.53044761368 	
--2563211.45629337151 	822053.624572880915 	
-3369257.50764141418 	-10725727.8998395819 	
--1353375.88423969341 	-7612242.98617786448 	
-2441604.27984183747 	2049619.67142971256 	
--2156838.11880350718 	-9427032.68944941275 	
--1028588.7884617839 	-52982472.9460312426 	
-2380355.83032485517 	5145850.16324428655 	
-564300.304213029798 	-2083487.10712761804 	
--275925.519923126558 	-1743839.14941733656 	
-1094.4732375089884 	-5271.22198148798361 	
--2042891.91918126494 	4524054.7454075953 	
-7990948.31450272165 	-2976369.24887729483 	
--71.5312240788905456 	-106.090387459362432 	
-633354.92175690853 	3850344.41100039519 	
--847378.378119752742 	1112659.00903518172 	
-2676589.46392424125 	-749047.764716492034 	
--126273.250347111491 	7571038.39657037426 	
--347857.496533141588 	-11015882.8807118721 	
--905134.531059579225 	11168991.5850313175 	
--1842762.1192507362 	5183038.11359634344 	
-975200.470001525246 	-4445813.75277888402 	
--173843.513534353551 	-8670399.88006355986 	
--0.152911067266690709 	1.01158735435002023 	
-877162.454537640326 	4775498.34942457918 	
--1135309.39143074118 	3560111.25226351805 	
--1501079.13008318446 	-5252699.32413829863 	
--1932771.07761389413 	34420359.973832652 	
-7226.5994643689919 	32894.1630781643107 	
-1593571.16456751968 	7942382.19208099786 	
-8459.68269193453489 	18698.9797352975365 	
--179210.432993102091 	4384113.01121255755 	
+-0.922725597620292715 	-0.804165388485015842 	
+7926620.47078567371 	-1072946.90073528094 	
+-417586.208741464652 	7775044.97806266695 	
+-337994.877034423 	15435.1598010049311 	
+1346321.44400652917 	10194328.2724584825 	
+-867881.201871286379 	23695583.3681646772 	
+-2028987.86288331356 	1656680.81330526434 	
+-1572757.55327662663 	-14416038.8355148416 	
+9497217.31052252837 	4589619.18962512258 	
+-5683908.85908158217 	782712.841746510938 	
+-0.841998910671586276 	-1.03485911398832098 	
+1259122.14729367523 	5412015.07015024312 	
+-446615.948707438773 	132223.213054942898 	
+-137.351888542633105 	-278.713955679563924 	
+-619.491672951553028 	3195.47930048364742 	
+1330745.48027885496 	1115568.92483212822 	
+-14988.8883451377824 	-10429993.3474617321 	
+-190575.585663602746 	-191610.417572159495 	
+-388325.395389857877 	-6956949.691152527 	
+1816358.87709198357 	-11960420.3641861938 	
+1564788.74810412456 	-5860830.2852299558 	
+-1723628.63695709663 	-3000660.17918722052 	
+2064.51730713777988 	-14088.9501808491405 	
+-4956112.19170907885 	5659954.73396471422 	
+5328355.26487070508 	730737.378458630526 	
+-699049.404278120957 	-5046443.62873419095 	
+450.333538694964773 	-2039.43996912528405 	
+973347.09636970365 	-6241064.42226643208 	
+641437.400624387199 	5386885.79083444644 	
+4740632.78317914344 	-6290721.24202487059 	
+307053.96077811101 	2402058.05174675025 	
+559238.996101363911 	-29018739.1815036573 	
+-3126070.23386024404 	-1025531.67732546001 	
+-155800.504112387163 	7898599.93988365401 	
+2406989.3552889633 	2335533.17448321078 	
+-1032551.52307158627 	-1378603.43171242927 	
+-2563224.67637358001 	822055.417948087445 	
+3369258.39517004276 	-10725716.4975315444 	
+-1353378.66484166984 	-7612244.4403631445 	
+2441609.20822470728 	2049621.60606737132 	
+-2156837.44837383181 	-9427032.79202610441 	
+-1028588.50049963885 	-52982534.5317424908 	
+2380355.88954142528 	5145849.75306271855 	
+564300.109707692405 	-2083486.48498249683 	
+-275925.423310168204 	-1743836.54160688445 	
+1094.47298298226019 	-5271.22128525976768 	
+-2042892.33928718162 	4524048.14191954955 	
+7990948.17243123427 	-2976370.59203438088 	
+-71.5312363966548475 	-106.090672005438734 	
+633355.065646149684 	3850345.56549233152 	
+-847384.371787322802 	1112655.35433097999 	
+2676602.9696628293 	-749049.683018399402 	
+-126274.196393867504 	7571030.7711941367 	
+-347858.14183634083 	-11015882.3636266142 	
+-905134.11931091093 	11168994.2430636305 	
+-1842770.71995336795 	5183040.33398779575 	
+975208.912378598121 	-4445812.14661667589 	
+-173835.032043901854 	-8670407.22276088782 	
+-0.152911067315477017 	1.01158735453246518 	
+877161.827474033227 	4775498.97741680965 	
+-1135309.61295715417 	3560109.44932497106 	
+-1501078.42041809531 	-5252698.4913652651 	
+-1932771.08531409409 	34420359.2112229913 	
+7226.60422565710996 	32894.2542506348691 	
+1593570.69090277399 	7942380.89848313946 	
+8459.68289850585643 	18698.9854681869074 	
+-179210.303848969255 	4384112.38927208353 	
 -0.84469312796288476 	0.985198735996204666 	
-2213909.14917233214 	-1585910.21855751169 	
--7598271.24430005904 	1404511.04082059651 	
--10596431.8961382378 	-7833024.0052961316 	
--1032883.18330968637 	-4446594.3214858193 	
-127168.252115407173 	-193761.910577517614 	
-6252961.50505916495 	1939103.81118396064 	
--346903.0889608867 	-1973122.19387586415 	
-38.8083069909251108 	124.622361090508662 	
-7187069.29777966347 	-4620466.04599134065 	
--4206065.60676611029 	4857056.76551134605 	
--609543.897762163309 	4953932.75617051497 	
--69245.7733973851427 	-2286970.93046659091 	
-1795418.06003572722 	1682406.66947836312 	
--5514178.24620093498 	-1913659.99175527785 	
-933431.739659197745 	-3883185.15834952379 	
-2465585.72659639083 	9818734.38561884128 	
--3840386.09479657514 	-2690264.24444791023 	
-6706622.50180042442 	1697671.4527333742 	
--3617541.87569450727 	5949665.83224923164 	
--6191834.37702970486 	-911315.658814185299 	
-986529.297328819521 	6511189.04347078409 	
--2345424.75511409529 	7525958.96167184971 	
-661231.714253665064 	12388301.3896347061 	
-10710651.1682039425 	7251964.84472675808 	
-284.985640539585063 	699.391862733024141 	
--919728.240818873048 	552996.488032467198 	
-3745530.74291117769 	-7319155.01500570681 	
--20.441926306111597 	-58.1069559765113084 	
-1538602.31356622931 	2598008.02185010305 	
--790499.452385343495 	-4506589.62143105175 	
-7099038.66958017461 	-417628.756644672772 	
-558962.223774768412 	-12475745.1652629729 	
--599028.389531427063 	-2314389.61348295677 	
--2356709.31597649446 	-6595912.28064572997 	
--4324414.98677539453 	9107240.27722355351 	
--4611642.11950800382 	6001460.90574251954 	
-1090839.72956573125 	10707990.1497143246 	
-2522704.52002362767 	-3308361.78308354132 	
--1821734.70080943499 	-9767993.70651571453 	
--3970368.9095407608 	-4529263.21456202958 	
-445352.430356294441 	3484777.56158052804 	
--1608445.61350843939 	-2098591.40584415942 	
-861662.808744097012 	-11973633.9091349617 	
-4818289.41085499711 	4032521.08733894257 	
--3115649.6514784256 	-937814.225446133758 	
-2238247.79614611994 	16425039.6453627963 	
-455739.717257110693 	-1762485.37458749139 	
--2284832.60310314177 	-14792666.9594637621 	
-577478.785007526283 	-4729072.54814479407 	
-1863.46811013348042 	6666.7634574112617 	
-2427682.06752805458 	-110334877.543169826 	
-44.8472138550197315 	67.3130567963881532 	
--7990540.57389935479 	3543786.15082896827 	
-345676.613834777672 	1018549.38455951412 	
-6378531.99577932432 	-1245178.92397588305 	
-64.2668503017845723 	101.851279866970572 	
--873.851786078105988 	-3132.56562317622365 	
--367442.756262125389 	34184864.1671724021 	
--7736.07549782182468 	-17251.6142373168113 	
--125.927173872006009 	-330.578506420377721 	
-287936.965238874138 	-4149507.23117405549 	
--71856.372490478112 	-7543641.72015177179 	
--2529018.94605846051 	-16852251.2351096533 	
-340829.439237291575 	403562.77959225449 	
--498549.810092130792 	14552195.477159258 	
-339333.16505398514 	-21020151.0038519874 	
-1050692.19254508428 	-5970472.41092107631 	
--2478761.44948981423 	-4045000.20913781319 	
--519950.015120307507 	49437456.9018368945 	
--2401259.68627850339 	4867094.82043668535 	
--332791.087348784902 	-2550526.78852387052 	
-227507.728691458644 	1461747.38533827779 	
-268337.713860568474 	-9188238.11257815734 	
-3629516.69618111663 	11406975.0401876196 	
-1034892.54185595678 	-8745978.28239533491 	
--6753.87162032543529 	-30293.892295221387 	
-1714822.43494585901 	18545934.1622730419 	
--2010161.35843832395 	-3447009.04323048145 	
-2074572.47689777007 	-1377294.50835124776 	
-14179867.5865770541 	-1291203.50932988478 	
-50.1794248016306312 	-1576.47570399897336 	
--8302577.63904379122 	410995.064018334495 	
--1295784.74769848166 	-2836197.2607189212 	
-237420.968742528319 	2112338.64849909302 	
--3597610.06835860293 	1595798.22147221654 	
--2113748.73927465128 	-37968957.9276739657 	
-1274900.38314836798 	15914317.4766592756 	
--1153260.24299119809 	-493473.209634491068 	
+2213909.53081213729 	-1585910.50469392631 	
+-7598277.18223414384 	1404519.52004569955 	
+-10596433.6865027398 	-7833029.18329287879 	
+-1032882.65720343066 	-4446587.03905070573 	
+127168.829378520328 	-193762.000750631501 	
+6252958.96402912587 	1939098.81434288155 	
+-346902.920441658993 	-1973121.30937733268 	
+38.808326100647939 	124.622793534354699 	
+7187069.58780155331 	-4620464.60971987899 	
+-4206060.97278586403 	4857053.54710425436 	
+-609543.913832061575 	4953936.66203057393 	
+-69243.9172869074391 	-2287045.67764599761 	
+1795415.78037901432 	1682402.29950779025 	
+-5514178.23187299538 	-1913663.30630124127 	
+933429.789760548389 	-3883186.19376715319 	
+2465586.0210751649 	9818730.55265720002 	
+-3840386.11247398658 	-2690263.7400822062 	
+6706624.7304463461 	1697675.37932518846 	
+-3617544.90939237643 	5949671.35782151483 	
+-6191833.15474079549 	-911317.178794218111 	
+986529.415163152386 	6511189.33402219508 	
+-2345425.87924914481 	7525951.44637239818 	
+661232.231545119197 	12388301.9537591413 	
+10710659.2269515321 	7251961.9632122973 	
+284.985646053052449 	699.392025115782758 	
+-919731.970199437928 	552997.23234785872 	
+3745529.20431050053 	-7319157.64963697921 	
+-20.441926594557458 	-58.1069654953719166 	
+1538601.2349063647 	2598032.70322503103 	
+-790494.933982362621 	-4506597.31227191817 	
+7099038.27351487894 	-417627.52379791002 	
+558967.768952595652 	-12475747.4189345017 	
+-599028.375427415129 	-2314378.82470187172 	
+-2356721.72409138223 	-6595912.27468967717 	
+-4324409.43722983636 	9107243.29053750075 	
+-4611643.65632884018 	6001457.86403452791 	
+1090840.26071550907 	10707991.0073271729 	
+2522705.22206004616 	-3308359.87631005188 	
+-1821734.66851421096 	-9767994.5200019218 	
+-3970365.20712225186 	-4529261.76262150612 	
+445352.845524480857 	3484784.71361653041 	
+-1608449.05329672131 	-2098591.41101606144 	
+861667.756433022092 	-11973630.98161906 	
+4818290.43474329356 	4032524.81276691984 	
+-3115649.81108444137 	-937816.361507824156 	
+2238247.76197976526 	16425033.7757122871 	
+455742.664148917189 	-1762394.49981897045 	
+-2284834.34325662209 	-14792669.2332068644 	
+577478.78658536938 	-4729075.9719802551 	
+1863.46912122347658 	6666.78570107817904 	
+2427682.78758014785 	-110334881.60770689 	
+44.8472196534564134 	67.3131920654455627 	
+-7990542.82013130281 	3543787.8596567451 	
+345677.836237427371 	1018534.80538976414 	
+6378555.72046477348 	-1245184.48215829441 	
+64.2668662502234156 	101.851645112453255 	
+-873.852257041853363 	-3132.57606538450227 	
+-367445.677368487522 	34184938.8992745951 	
+-7736.07568290028394 	-17251.6193976762734 	
+-125.927228394181398 	-330.579743435666273 	
+287939.829166949377 	-4149505.60568079352 	
+-71857.1383847622492 	-7543640.97152774036 	
+-2529019.27821710939 	-16852246.5363857411 	
+340830.596103781136 	403560.88732724922 	
+-498541.860307599185 	14552316.639100749 	
+339333.225815137324 	-21020151.1692593694 	
+1050691.66554240696 	-5970472.02000350133 	
+-2478763.87634560419 	-4044999.50127253402 	
+-519949.676857746148 	49437449.500458546 	
+-2401260.31671311194 	4867098.09354826901 	
+-332791.393199397135 	-2550532.05323082209 	
+227507.621212484693 	1461746.15333868167 	
+268339.223802988359 	-9188231.49896172993 	
+3629511.79692190886 	11406974.3608692978 	
+1034895.25846352486 	-8745968.69177344441 	
+-6753.8760257467502 	-30293.9771700261663 	
+1714824.35382728046 	18545971.4143232405 	
+-2010171.66051860107 	-3447085.86079623271 	
+2074572.57985985605 	-1377294.12777965562 	
+14179864.9756228495 	-1291200.79082937492 	
+50.1794111342545932 	-1576.47565295215645 	
+-8302577.78617179953 	410998.184646853537 	
+-1295784.74016834167 	-2836163.1001902502 	
+237421.445122343051 	2112337.33389551984 	
+-3597607.81524020201 	1595798.25639176718 	
+-2113751.16806406993 	-37968954.8972309381 	
+1274902.39815011993 	15914285.1767959557 	
+-1153262.77103033056 	-493469.523152813024 	
 ]
 ;
 training_inputs = *3 ->MemoryVMatrix(
@@ -471,6 +471,7 @@
 source = *0 ;
 fieldnames = []
 ;
+deep_copy_memory_data = 1 ;
 writable = 0 ;
 length = 225 ;
 width = 1 ;
@@ -480,6 +481,7 @@
 extrasize = 0 ;
 metadatadir = ""  )
 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
 n_examples = 225 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 75 ;
 sumsquarew_ = 75 ;
-sum_ = 144.002796681173749 ;
-sumsquare_ = 10604.7679530276328 ;
-sumcube_ = 1024173.84013462893 ;
-sumfourth_ = 102322137.326348484 ;
-min_ = 5.4718757392551664e-06 ;
-max_ = 100.558698546447715 ;
+sum_ = 144.002796667722663 ;
+sumsquare_ = 10604.7679523845018 ;
+sumcube_ = 1024173.84012045246 ;
+sumfourth_ = 102322137.326550901 ;
+min_ = 5.47190448166269372e-06 ;
+max_ = 100.558698546327321 ;
 agmemin_ = 38 ;
 agemax_ = 28 ;
-first_ = 0.0159720327578714814 ;
-last_ = 0.000951494976113702773 ;
+first_ = 0.0159720324702003133 ;
+last_ = 0.000951494981345345583 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_costs.pmat	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_costs.pmat	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,10 +1,12 @@
 MATRIX 300 1 DOUBLE LITTLE_ENDIAN                              
-/?}Upp?_??I?oS??4`W=D???Bj???>E?s?d?Q?????U?f?]#?
-????????)?j???7>?EPy?u?'?5??????M??9S??C?x?_z??4?A?5v?*?????M???&?C?[EB?r?\??	??|???x??.?=?????ky???JM?0???>??g?0?<??p?,?	???$?Z??????????X???^t?M?>Wzj???Q?=???qW'>????bs??????????I??{?={IG?18?
--?_?MS?r???-c???z?????a??????%?_?I??C?NJ?U????"Qf??????D??'dY?`?j?}?5>{0?v??
-zU???????B-?O?/0cf??=??m???R?vj?3G<??WT?k;.????????J??i????xs?R`????????c?hOp??_Am??@??	?r????q??t????R????*x??????????e????yv??N?B?)w???????EU??j\?$Y&???=|t|*??\?C?v??o?? ???u??<T?.???<N?L?5z???>??x????	?>@R,????p~?O???T???{F??4???????S?gQ????????j?H3a????<?TB9Ki???&???>??\	;?=XSC?3>???t?3????xG??J??/???>9????s?P%??????N-?$n? 0V???S????!??U>???]?.w?1??\?g?O"??5Ce????????=d?5+?Wp??C??h?7?jy???|47C??1K??????
-4/??Q???n?c?y?????mR?????Wq3???E?<?G????????U?2*|x?>w?1h?P?3?	?0'???F~?????J,?xUj?v/}?>??]??m4?Y#G2??(>?=?0{/d?)y???}?`mK>?|=O^?X????q?)?!3#_PI?J?	-|w?2????????R??>????????h??I?]??`?$@E???~???7?6+?</p?
-ao5?|????8??g=??'?K?7?bT??????Lq??>P??y=b???+>?U?)t???!?tE?0c>???????=G????X\??????Ow?ljc;?????3r>B?>R{??????93z?????????"n?xD???ob??r?E?I???l?Y!h?M*????E???v?C?,??*?!N???rp??X?????????? ???y>???~?????]lZ?G>??QD?}
-??	?]?:?l????,?Z?`	[(k?L??\????u??e?w????Ru^?6p8?????4]?8????J?4??>L???T?e?N????r'?
-???}?i?%@?W??i???Q??,?8????7 7???Z4.0???W?9?;???Nn+?#Y@??,Q??y??l?g!#y??^??"????y??????0???j?|?aNi????@?e??b???????	??@??OFk???2@?##
-??5v??F?:??$@??HLm??+;?*R@??aZ?c?????q???aO?c?P?1????[??*??}?1?g??-O?
\ No newline at end of file
+Lb#KVpp?J0?/?oS???d>D???????>~??ae?Q??atCV?f?t?~????:??)??H%7>?4=??u??X?????`'??9S?QG???_z?|n0??5v?|9?~??M?'??+?C??0$D?r??=*??|?L???.?=;???ky?l??P?0?????g?
+???p?1?????$??D?????n??\?X?{?g?M?>E5b???Q??u.?qW'>Cn??cs??Q?U?????I?{?=?L?,8?C]??MS?%#??-c?J?"??????YN????A??]?I????'J?U?.???Pf??-E2L?D?????;?`?uN\6>?XB???JJ8???1?Mv?O??7?^q??=??u???R??+??F<??.?D?:.???|+???????i?[?Jxs?gr?????_?2iOp?kL?Cm???t??	?r?o/G???t?????????M?????????Ok???~B{v??m????)w??????p???j\?,wej???=????\???rr?o?? ???u??<2X????<Y???5z??i9??x??~
+?>?<?????M??O?????"/?{F???q??????iQ???8?a???j???}????<??T?Ki??A????>? M?;?=?0??3>v???3?HM^xG???????>??X???s????t??????]#%n???~??S???????U>d?{t?.w?}?}(?g?<? ?5Ce???????=|??7?Wp???>*?h?X???jy?'0?	97C????g?????)r??Q?'8?Hc?y???z}?mR????xPq3??/j#L?G?v at y??????O?*|x??p?k?P?x?
+0'???Q??????~T%zUj?|???>???:?m4?a????(>???x/d?GF
+\\?a??l8???.?_?m?f????8T?SK???F?b?8?????y???{+p?? ?jXQ?Iz?~#??????}?5??W?*?????CV
+?b?",?3??4>G?e?|???]?V?g=@[??K???Y?S??????Lq?S??y=?xy??+>???z|???<????0c>5??????=?~???X\??a??Ow???
+??????,UB?>?K?????n?????????4"n???+??ob?2???I????u!h??ziI??E??hI6B?,?vc?
+????_4??y>2?t??????$\?Z?Z?U??QD?Z???????7+???=???M|?eo?H$?O?k,?q?=?M?-??`Y????????I?L2???St>??$???p??Z???m??y??M??V??El?%J??6L?(LV2?l7_?^H?????\\???^?L???zBN????)F8?O??[[Zv9r??n?????????6????????????V?u
+?`???Eq?4K???u???????<??Y6Q??\r@??u???gM.?;?VZ?r-???h7g???????AgFz??????z???????M???A?????}w?*q?Sn
+??;??9?l???j??Z????r?L???wh????yS?e?w?z<w?Su^??????????-?d7???~??i??>
+;?S?T?f??????_s????????f?%@?gg?????7k#?8?T???7??J??;?????0?;??3???#Y@??HU??y??PB"#y????b?"???g3?????????j???vug??G???e???q^??????H?@??&??O??2 at 3?????T???Z???:dIo?????:T??4?????#?P~G???c?? ?t?U???M?(????k??5v???PF$?$@?K9m?????'*R@5<???c????m?o??
\ No newline at end of file

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 300 ;
 sumsquarew_ = 300 ;
-sum_ = 148.80923780255759 ;
-sumsquare_ = 10610.6708408309078 ;
-sumcube_ = 1024556.67164685845 ;
-sumfourth_ = 102371139.874690771 ;
+sum_ = 148.809237760624569 ;
+sumsquare_ = 10610.670840229488 ;
+sumcube_ = 1024556.67161457823 ;
+sumfourth_ = 102371139.872506976 ;
 min_ = 1.04669084620911105e-16 ;
-max_ = 100.558698546447715 ;
+max_ = 100.558698546327321 ;
 agmemin_ = 172 ;
 agemax_ = 28 ;
-first_ = 0.00401338269720593124 ;
-last_ = 0.000951494976113702773 ;
+first_ = 0.0040133829918081558 ;
+last_ = 0.000951494981345345583 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 225 ;
 sumsquarew_ = 225 ;
-sum_ = 3.90954236683399037 ;
-sumsquare_ = 2.44800399839949234 ;
-sumcube_ = 2.31347596784497256 ;
-sumfourth_ = 2.50165040817058326 ;
+sum_ = 3.90954238202241511 ;
+sumsquare_ = 2.44800420917268058 ;
+sumcube_ = 2.31347641771492718 ;
+sumfourth_ = 2.50165118855559721 ;
 min_ = 1.04669084620911105e-16 ;
-max_ = 1.21796824903727607 ;
+max_ = 1.21796836701509115 ;
 agmemin_ = 97 ;
 agemax_ = 37 ;
-first_ = 0.00401338269720593124 ;
-last_ = 0.000157352772999446117 ;
+first_ = 0.0040133829918081558 ;
+last_ = 0.00015735279796383564 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/metainfos.txt	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/metainfos.txt	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL7398"
+__REVISION__ = "PL8398"
 data                                          = PLEARNDIR:examples/data/test_suite/sin_signcos_1x_2y.amat
 sigma                                         = 1.0

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/tester.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/tester.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -39,6 +39,7 @@
 ]
 ;
 training_inputs = *0 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
 n_examples = -1 ;
@@ -64,6 +65,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -4,14 +4,15 @@
 weight_decay = 0 ;
 output_learned_weights = 0 ;
 weights = 3  2  [ 
-5.67364640948039956 	9.88961990926600798 	
-10.7829822118754084 	17.900656890875144 	
-99.7615196252840377 	47.6879647530819 	
+5.67364640948040133 	9.88961990926600976 	
+10.7829822118754173 	17.900656890875144 	
+99.7615196252840661 	47.6879647530819142 	
 ]
 ;
-AIC = 6.23533365254734484 ;
-BIC = 6.35575906431119542 ;
-resid_variance = 2 [ 22.6711187811875661 458.169739725179056 ] ;
+AIC = 6.23533365254733773 ;
+BIC = 6.35575906431118831 ;
+resid_variance = 2 [ 22.6711187811828125 458.169739725178715 ] ;
+random_gen = *0 ;
 stage = 0 ;
 n_examples = 150 ;
 inputsize = 2 ;

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_confidence.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_costs.pmat	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_costs.pmat	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,2 +1,2 @@
 MATRIX 50 5 DOUBLE LITTLE_ENDIAN                               
-;I?/??@;I?/??@6[N??@z??Ll@X?_??.@?s|?%??@?s|?%??@6[N??@z??Ll@X?_??.@	'????9@	'????9 at 6[N??@z??Ll@X?_??.@????P@????P at 6[N??@z??Ll@X?_??.@??9P{@??9P{@6[N??@z??Ll@X?_??.@?k?b???@?k?b???@6[N??@z??Ll@X?_??.@U??KBP at U??KBP at 6[N??@z??Ll@X?_??.@?O??GI@?O??GI at 6[N??@z??Ll@X?_??.@?;a6???@?;a6???@6[N??@z??Ll@X?_??.@??lk1e?@??lk1e?@6[N??@z??Ll@X?_??.@.&??9?Z at .&??9?Z at 6[N??@z??Ll@X?_??.@???????@???????@6[N??@z??Ll@X?_??.@?Q+Z?Tb@?Q+Z?Tb at 6[N??@z??Ll@X?_??.@???4g@???4g at 6[N??@z??Ll@X?_??.@6P?^3??@6P?^3??@6[N??@z??Ll@X?_??.@'???5?@'???5?@6[N??@z??Ll@X?_??.@bT8??@bT8??@6[N??@z??Ll@X?_??.@?/??y>"@?/??y>"@6[N??@z??Ll@X?_??.@,`?J?D@,`?J?D at 6[N??@z??Ll@X?_??.@:?(???@:?(???@6[N??@z??Ll@X?_??.@P47r?D at P47r?D at 6[N??@z??Ll@X?_??.@??6iVz@??6iVz at 6[N??@z??Ll@X?_??.@n?BO?v at n?BO?v at 6[N??@z??Ll@X?_??.@0K6??r at 0K6??r at 6[N??@z??Ll@X?_??.@P?G?@P?G?@6[N??@z??!
 Ll@X?_??.@#y!???@#y!???@6[N??@z??Ll@X?_??.@?(?s?@?(?s?@6[N??@z??Ll@X?_??.@ND???@ND???@6[N??@z??Ll@X?_??.@?K??v?t@?K??v?t at 6[N??@z??Ll@X?_??.@???aDb@???aDb at 6[N??@z??Ll@X?_??.@????!?u@????!?u at 6[N??@z??Ll@X?_??.@BTI??@BTI??@6[N??@z??Ll@X?_??.@2???T@2???T at 6[N??@z??Ll@X?_??.@? yD?+@? yD?+ at 6[N??@z??Ll@X?_??.@???FA?@???FA?@6[N??@z??Ll@X?_??.@?????A@?????A at 6[N??@z??Ll@X?_??.@:?x?b?@:?x?b?@6[N??@z??Ll@X?_??.@????
\ No newline at end of file
+9I?/??@9I?/??@.[N??@r??Ll@P?_??.@?s|?%??@?s|?%??@.[N??@r??Ll@P?_??.@'????9@'????9 at .[N??@r??Ll@P?_??.@????P@????P at .[N??@r??Ll@P?_??.@??9P{@??9P{@.[N??@r??Ll@P?_??.@?k?b???@?k?b???@.[N??@r??Ll@P?_??.@W??KBP at W??KBP at .[N??@r??Ll@P?_??.@?O??GI@?O??GI at .[N??@r??Ll@P?_??.@?;a6???@?;a6???@.[N??@r??Ll@P?_??.@??lk1e?@??lk1e?@.[N??@r??Ll@P?_??.@5&??9?Z at 5&??9?Z at .[N??@r??Ll@P?_??.@???????@???????@.[N??@r??Ll@P?_??.@?Q+Z?Tb@?Q+Z?Tb at .[N??@r??Ll@P?_??.@???4g@???4g at .[N??@r??Ll@P?_??.@3P?^3??@3P?^3??@.[N??@r??Ll@P?_??.@'???5?@'???5?@.[N??@r??Ll@P?_??.@WT8??@WT8??@.[N??@r??Ll@P?_??.@0??y>"@0??y>"@.[N??@r??Ll@P?_??.@?`?J?D@?`?J?D at .[N??@r??Ll@P?_??.@=?(???@=?(???@.[N??@r??Ll@P?_??.@547r?D at 547r?D at .[N??@r??Ll@P?_??.@??6iVz@??6iVz at .[N??@r??Ll@P?_??.@f?BO?v at f?BO?v at .[N??@r??Ll@P?_??.@+K6??r at +K6??r at .[N??@r??Ll@P?_??.@_P?G?@_P?G?@.[N??@r??!
 Ll@P?_??.@+y!???@+y!???@.[N??@r??Ll@P?_??.@?'?s?@?'?s?@.[N??@r??Ll@P?_??.@ND???@ND???@.[N??@r??Ll@P?_??.@?K??v?t@?K??v?t at .[N??@r??Ll@P?_??.@???aDb@???aDb at .[N??@r??Ll@P?_??.@????!?u@????!?u at .[N??@r??Ll@P?_??.@'TI??@'TI??@.[N??@r??Ll@P?_??.@2???T@2???T at .[N??@r??Ll@P?_??.@ yD?+@ yD?+ at .[N??@r??Ll@P?_??.@???FA?@???FA?@.[N??@r??Ll@P?_??.@?????A@?????A at .[N??@r??Ll@P?_??.@:?x?b?@:?x?b?@.[N??@r??Ll@P?_??.@????
\ No newline at end of file

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -16,14 +16,16 @@
 sumsquarew_ = 50 ;
 sum_ = -4802.11772097781613 ;
 sumsquare_ = 11114205.6583226975 ;
-sumcube_ = 912252339.820317268 ;
-sumfourth_ = 4363965068449.31055 ;
-min_ = 4.49614059257518761 ;
+sumcube_ = 912252339.820317984 ;
+sumfourth_ = 4363965068449.31104 ;
+min_ = 4.49614059257518672 ;
 max_ = 1506.91426994253925 ;
 agmemin_ = 25 ;
 agemax_ = 41 ;
 first_ = 564.628509411856726 ;
-last_ = 13.7380273402321649 ;
+last_ = 13.7380273402321524 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -35,14 +37,16 @@
 sumsquarew_ = 50 ;
 sum_ = -4802.11772097781613 ;
 sumsquare_ = 11114205.6583226975 ;
-sumcube_ = 912252339.820317268 ;
-sumfourth_ = 4363965068449.31055 ;
-min_ = 4.49614059257518761 ;
+sumcube_ = 912252339.820317984 ;
+sumfourth_ = 4363965068449.31104 ;
+min_ = 4.49614059257518672 ;
 max_ = 1506.91426994253925 ;
 agmemin_ = 25 ;
 agemax_ = 41 ;
 first_ = 564.628509411856726 ;
-last_ = 13.7380273402321649 ;
+last_ = 13.7380273402321524 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 49 ;
 first_ = 6.23533365254733773 ;
 last_ = 6.23533365254733773 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 49 ;
 first_ = 6.35575906431118831 ;
 last_ = 6.35575906431118831 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -100,6 +108,8 @@
 agemax_ = 49 ;
 first_ = 6.29554635842926302 ;
 last_ = 6.29554635842926302 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 0 ;
 first_ = 471.224041336225355 ;
 last_ = 471.224041336225355 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 0 ;
 first_ = 471.224041336225355 ;
 last_ = 471.224041336225355 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 0 ;
 first_ = 6.23533365254733773 ;
 last_ = 6.23533365254733773 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 0 ;
 first_ = 6.35575906431118831 ;
 last_ = 6.35575906431118831 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -100,6 +108,8 @@
 agemax_ = 0 ;
 first_ = 6.29554635842926302 ;
 last_ = 6.29554635842926302 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/metainfos.txt	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/metainfos.txt	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL7398"
+__REVISION__ = "PL8398"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_2x_2y.amat

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/tester.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/tester.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -34,6 +34,7 @@
 BIC = nan ;
 resid_variance = []
 ;
+random_gen = *0 ;
 stage = 0 ;
 n_examples = -1 ;
 inputsize = -1 ;
@@ -58,6 +59,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 95 ;
 first_ = 2696.74734765470976 ;
 last_ = 1462.79140675898816 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 10 ;
 first_ = -5392.49469530941951 ;
 last_ = -2924.58281351797632 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 10 ;
 first_ = -102.860432266666592 ;
 last_ = -75.4929122666665933 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 15 ;
 first_ = 2228.12756426559145 ;
 last_ = 9063.38769912142379 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 49 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 20 ;
 first_ = -4455.25512853118289 ;
 last_ = -18125.7753982428476 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 20 ;
 first_ = -93.4060922666665903 ;
 last_ = -189.403652266666597 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 95 ;
 first_ = 2696.74734765470976 ;
 last_ = 1462.79140675898816 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 10 ;
 first_ = -5392.49469530941951 ;
 last_ = -2924.58281351797632 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 10 ;
 first_ = -102.860432266666592 ;
 last_ = -75.4929122666665933 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 102 ;
 first_ = 3.3058115703008113 ;
 last_ = 39.3579314881000286 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 120 ;
 first_ = -5.61162314060162259 ;
 last_ = -77.7158629762000572 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 120 ;
 first_ = -2.63637818181817352 ;
 last_ = -11.5471800000000044 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 35 ;
 first_ = 231.958382832400019 ;
 last_ = 187.926810649600071 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 49 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 45 ;
 first_ = -462.916765664800039 ;
 last_ = -374.853621299200142 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 45 ;
 first_ = -29.4603600000000014 ;
 last_ = -26.4172800000000052 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 102 ;
 first_ = 3.3058115703008113 ;
 last_ = 39.3579314881000286 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 120 ;
 first_ = -5.61162314060162259 ;
 last_ = -77.7158629762000572 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 120 ;
 first_ = -2.63637818181817352 ;
 last_ = -11.5471800000000044 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 16 ;
 first_ = 13.8781581156000211 ;
 last_ = 9.08582363289999861 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 120 ;
 first_ = -26.7563162312000422 ;
 last_ = -17.1716472657999972 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 120 ;
 first_ = -6.45068000000000552 ;
 last_ = -5.02853999999999957 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 35 ;
 first_ = 143.301489139599965 ;
 last_ = 138.08415022370167 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 49 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 17 ;
 first_ = -285.602978279199931 ;
 last_ = -275.16830044740334 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 17 ;
 first_ = -22.9417199999999966 ;
 last_ = -22.5018425000000093 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 16 ;
 first_ = 13.8781581156000211 ;
 last_ = 9.08582363289999861 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 120 ;
 first_ = -26.7563162312000422 ;
 last_ = -17.1716472657999972 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 120 ;
 first_ = -6.45068000000000552 ;
 last_ = -5.02853999999999957 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 16 ;
 first_ = 13.8781581156000211 ;
 last_ = 9.08582363289999861 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 149 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 120 ;
 first_ = -26.7563162312000422 ;
 last_ = -17.1716472657999972 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 120 ;
 first_ = -6.45068000000000552 ;
 last_ = -5.02853999999999957 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 35 ;
 first_ = 143.301489139599965 ;
 last_ = 138.08415022370167 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 49 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 17 ;
 first_ = -285.602978279199931 ;
 last_ = -275.16830044740334 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 17 ;
 first_ = -22.9417199999999966 ;
 last_ = -22.5018425000000093 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 0 ;
 first_ = 15.0751270796107963 ;
 last_ = 15.0751270796107963 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 0 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 0 ;
 first_ = -29.1502541592215927 ;
 last_ = -29.1502541592215927 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 0 ;
 first_ = -5.1647413016931214 ;
 last_ = -5.1647413016931214 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -100,6 +108,8 @@
 agemax_ = 0 ;
 first_ = 362.783279794880855 ;
 last_ = 362.783279794880855 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -119,6 +129,8 @@
 agemax_ = 0 ;
 first_ = 1 ;
 last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -138,6 +150,8 @@
 agemax_ = 0 ;
 first_ = -724.56655958976171 ;
 last_ = -724.56655958976171 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -157,6 +171,8 @@
 agemax_ = 0 ;
 first_ = -24.1796111823809525 ;
 last_ = -24.1796111823809525 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -4,16 +4,17 @@
 weight_decay = 0 ;
 output_learned_weights = 0 ;
 weights = 5  2  [ 
-5.13619795675571744 	11.1385630516838638 	
--6792.65035113731574 	-259513.234118045773 	
--13406.9293024818126 	-522479.686988846515 	
-1347.39643895091672 	52368.1545918183474 	
-32.717423747049466 	-1153.07130460664916 	
+5.13619797550028867 	11.1385637816564618 	
+-6792.60659839147775 	-259511.530268940318 	
+-13406.8419427241715 	-522476.284966421546 	
+1347.38770786956138 	52367.8145801943028 	
+32.7173751554060317 	-1153.07319706610338 	
 ]
 ;
-AIC = 6.09268868403331076 ;
-BIC = 6.2933977036397275 ;
-resid_variance = 2 [ 21.7608079288492036 378.956575834844273 ] ;
+AIC = 6.09268861370523496 ;
+BIC = 6.2933976333116517 ;
+resid_variance = 2 [ 21.7608079465732374 378.95664299527175 ] ;
+random_gen = *0 ;
 stage = 0 ;
 n_examples = 150 ;
 inputsize = 4 ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_confidence.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_costs.pmat	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_costs.pmat	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,6 +1,2 @@
 MATRIX 50 5 DOUBLE LITTLE_ENDIAN                               
-?A???@?A???@??J??^@?j?rp,@?p??@2=??(_?@2=??(_?@??J??^@?j?rp,@?p??@T?????@T?????@??J??^@?j?rp,@?p??@
-pZ?
-4Z@
-pZ?
-4Z@??J??^@?j?rp,@?p??@???lG@???lG@??J??^@?j?rp,@?p??@?I?OU?@?I?OU?@??J??^@?j?rp,@?p??@??/?~?v@??/?~?v@??J??^@?j?rp,@?p??@qO,??v@qO,??v@??J??^@?j?rp,@?p??@?i????@?i????@??J??^@?j?rp,@?p??@?A?!?g@?A?!?g@??J??^@?j?rp,@?p??@V+SJ?@V+SJ?@??J??^@?j?rp,@?p??@,uOr
\ No newline at end of file
+???i???@???i???@`M???^@{0?mp,@?????@Y?y?+_?@Y?y?+_?@`M???^@{0?mp,@?????@???i???@???i???@`M???^@{0?mp,@?????@Ap/?4Z at Ap/?4Z@`M???^@{0?mp,@?????@?FlG@?FlG@`M???^@{0?mp,@?????@?t?ER?@?t?ER?@`M???^@{0?mp,@?????@???|?v@???|?v@`M???^@{0?mp,@?????@[vh??v@[vh??v@`M???^@{0?mp,@?????@M0?]???@M0?]???@`M???^@{0?mp,@?????@?T?D?g@?T?D?g@`M???^@{0?mp,@?????@?n?gJ?@?n?gJ?@`M???^@{0?mp,@?????@??P???@??P???@`M???^@{0?mp,@?????@??8Q??q@??8Q??q@`M???^@{0?mp,@?????@{?2}?X?@{?2}?X?@`M???^@{0?mp,@?????@S?td??#@S?td??#@`M???^@{0?mp,@?????@v?Fu.`@v?Fu.`@`M???^@{0?mp,@?????@?y"?7]@?y"?7]@`M???^@{0?mp,@?????@??f,^@??f,^@`M???^@{0?mp,@?????@6?m??Y at 6?m??Y@`M???^@{0?mp,@?????@C&H?NIV at C&H?NIV@`M???^@{0?mp,@?????@?!???@!@?!???@!@`M???^@{0?mp,@?????@?oP:??Z@?oP:??Z@`M???^@{0?mp,@?????@Y?D?/iJ at Y?D?/iJ@`M???^@{0?mp,@?????@v.?I?9@v.?I?9@`M???^@{0?mp,@?????@Jc??Ok at Jc??Ok@`M???^@{0?m!
 p,@?????@j?5<k{@j?5<k{@`M???^@{0?mp,@?????@I^?????I^?????`M???^@{0?mp,@?????@?J?X??@?J?X??@`M???^@{0?mp,@?????@??????@??????@`M???^@{0?mp,@?????@??Ib?a@??Ib?a@`M???^@{0?mp,@?????@+
\ No newline at end of file

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -61031.3713111458346 ;
-sumsquare_ = 88238920.3267626911 ;
-sumcube_ = -124602455703.620605 ;
-sumfourth_ = 187859983108665.156 ;
-min_ = 1.23064138227748932 ;
-max_ = 2757.59234913165938 ;
+sum_ = -61031.3713111039688 ;
+sumsquare_ = 88238920.3266784102 ;
+sumcube_ = -124602455703.431458 ;
+sumfourth_ = 187859983108331.781 ;
+min_ = 1.23064138231520048 ;
+max_ = 2757.59234913323962 ;
 agmemin_ = 23 ;
 agemax_ = 1 ;
-first_ = 1651.65665408713994 ;
-last_ = 892.793588571159603 ;
+first_ = 1651.65665408652944 ;
+last_ = 892.793588570034785 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -33,16 +35,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -61031.3713111458346 ;
-sumsquare_ = 88238920.3267626911 ;
-sumcube_ = -124602455703.620605 ;
-sumfourth_ = 187859983108665.156 ;
-min_ = 1.23064138227748932 ;
-max_ = 2757.59234913165938 ;
+sum_ = -61031.3713111039688 ;
+sumsquare_ = 88238920.3266784102 ;
+sumcube_ = -124602455703.431458 ;
+sumfourth_ = 187859983108331.781 ;
+min_ = 1.23064138231520048 ;
+max_ = 2757.59234913323962 ;
 agmemin_ = 23 ;
 agemax_ = 1 ;
-first_ = 1651.65665408713994 ;
-last_ = 892.793588571159603 ;
+first_ = 1651.65665408652944 ;
+last_ = 892.793588570034785 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -56,12 +60,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.09268869187651152 ;
-max_ = 6.09268869187651152 ;
+min_ = 6.09268861370523496 ;
+max_ = 6.09268861370523496 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 6.09268869187651152 ;
-last_ = 6.09268869187651152 ;
+first_ = 6.09268861370523496 ;
+last_ = 6.09268861370523496 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -75,12 +81,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.29339771148292826 ;
-max_ = 6.29339771148292826 ;
+min_ = 6.2933976333116517 ;
+max_ = 6.2933976333116517 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 6.29339771148292826 ;
-last_ = 6.29339771148292826 ;
+first_ = 6.2933976333116517 ;
+last_ = 6.2933976333116517 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -94,12 +102,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.19304320167971944 ;
-max_ = 6.19304320167971944 ;
+min_ = 6.19304312350844377 ;
+max_ = 6.19304312350844377 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 6.19304320167971944 ;
-last_ = 6.19304320167971944 ;
+first_ = 6.19304312350844377 ;
+last_ = 6.19304312350844377 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -18,12 +18,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 387.360335203449551 ;
-max_ = 387.360335203449551 ;
+min_ = 387.36030492299858 ;
+max_ = 387.36030492299858 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 387.360335203449551 ;
-last_ = 387.360335203449551 ;
+first_ = 387.36030492299858 ;
+last_ = 387.36030492299858 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -37,12 +39,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 387.360335203449551 ;
-max_ = 387.360335203449551 ;
+min_ = 387.36030492299858 ;
+max_ = 387.36030492299858 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 387.360335203449551 ;
-last_ = 387.360335203449551 ;
+first_ = 387.36030492299858 ;
+last_ = 387.36030492299858 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -56,12 +60,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.09268869187651152 ;
-max_ = 6.09268869187651152 ;
+min_ = 6.09268861370523496 ;
+max_ = 6.09268861370523496 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 6.09268869187651152 ;
-last_ = 6.09268869187651152 ;
+first_ = 6.09268861370523496 ;
+last_ = 6.09268861370523496 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -75,12 +81,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.29339771148292826 ;
-max_ = 6.29339771148292826 ;
+min_ = 6.2933976333116517 ;
+max_ = 6.2933976333116517 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 6.29339771148292826 ;
-last_ = 6.29339771148292826 ;
+first_ = 6.2933976333116517 ;
+last_ = 6.2933976333116517 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -94,12 +102,14 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.19304320167971944 ;
-max_ = 6.19304320167971944 ;
+min_ = 6.19304312350844377 ;
+max_ = 6.19304312350844377 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 6.19304320167971944 ;
-last_ = 6.19304320167971944 ;
+first_ = 6.19304312350844377 ;
+last_ = 6.19304312350844377 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/metainfos.txt	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/metainfos.txt	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL7398"
+__REVISION__ = "PL8398"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.amat

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/tester.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/tester.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -34,6 +34,7 @@
 BIC = nan ;
 resid_variance = []
 ;
+random_gen = *0 ;
 stage = 0 ;
 n_examples = -1 ;
 inputsize = -1 ;
@@ -58,6 +59,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/final_learner.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/final_learner.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -7,13 +7,14 @@
 horizon = -1 ;
 normalize_warning = 0 ;
 impute_missing = 0 ;
-mu = 4 [ 0.0497776000000000815 0.104000866666666622 1.28890380000000127 0.203573999999999089 ] ;
-eigenvals = 2 [ 118.672460110071 2.49715033832618793 ] ;
+mu = 4 [ 0.0497776000000000537 0.104000866666666733 1.2889038000000006 0.203573999999998978 ] ;
+eigenvals = 2 [ 118.672460110070972 2.49715033832620659 ] ;
 eigenvecs = 2  4  [ 
--0.0447323220438456681 	-0.0758632537261853673 	-0.982297988853390924 	-0.165331325504956606 	
-0.533389571247938066 	-0.280198984783957883 	-0.135043923445805569 	0.786604877273427316 	
+-0.0447323220438456681 	-0.0758632537261853951 	-0.982297988853390702 	-0.165331325504956606 	
+0.533389571247937955 	-0.280198984783957827 	-0.135043923445805542 	0.786604877273427316 	
 ]
 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 1 ;
 n_examples = 150 ;
@@ -36,14 +37,15 @@
 weight_decay = 0 ;
 output_learned_weights = 0 ;
 weights = 3  2  [ 
-16.0069061333333629 	17.6304524666666858 	
--87.2554510136208989 	-52.8943537258218299 	
--35.9391743888709172 	-3.77263769686746775 	
+16.0069061333333558 	17.6304524666666822 	
+-87.2554510136209274 	-52.8943537258218441 	
+-35.9391743888710806 	-3.77263769686748285 	
 ]
 ;
 AIC = 6.05769163403522626 ;
 BIC = 6.17811704579907683 ;
-resid_variance = 2 [ 21.4698224052260969 381.110381493487864 ] ;
+resid_variance = 2 [ 21.4698224052197624 381.110381493486273 ] ;
+random_gen = *0 ;
 stage = 0 ;
 n_examples = 150 ;
 inputsize = 2 ;
@@ -67,6 +69,7 @@
 put_raw_input = 0 ;
 share_learner = 0 ;
 nsep = 1 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 1 ;
 n_examples = 150 ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_confidence.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -14,16 +14,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -49625.3815839320014 ;
-sumsquare_ = 61057483.567084454 ;
-sumcube_ = -73001204564.4328918 ;
-sumfourth_ = 92944320862419.2344 ;
-min_ = 1.86363769529453394 ;
+sum_ = -49625.381583931965 ;
+sumsquare_ = 61057483.5670844391 ;
+sumcube_ = -73001204564.4328003 ;
+sumfourth_ = 92944320862419.2031 ;
+min_ = 1.86363769529458145 ;
 max_ = 2256.46039029148506 ;
 agmemin_ = 23 ;
 agemax_ = 1 ;
-first_ = 1412.74238652622353 ;
-last_ = 782.35445173265191 ;
+first_ = 1412.7423865262233 ;
+last_ = 782.354451732651683 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -33,16 +35,18 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -49625.3815839320014 ;
-sumsquare_ = 61057483.567084454 ;
-sumcube_ = -73001204564.4328918 ;
-sumfourth_ = 92944320862419.2344 ;
-min_ = 1.86363769529453394 ;
+sum_ = -49625.381583931965 ;
+sumsquare_ = 61057483.5670844391 ;
+sumcube_ = -73001204564.4328003 ;
+sumfourth_ = 92944320862419.2031 ;
+min_ = 1.86363769529458145 ;
 max_ = 2256.46039029148506 ;
 agmemin_ = 23 ;
 agemax_ = 1 ;
-first_ = 1412.74238652622353 ;
-last_ = 782.35445173265191 ;
+first_ = 1412.7423865262233 ;
+last_ = 782.354451732651683 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 49 ;
 first_ = 6.05769163403522626 ;
 last_ = 6.05769163403522626 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 49 ;
 first_ = 6.17811704579907683 ;
 last_ = 6.17811704579907683 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -100,6 +108,8 @@
 agemax_ = 49 ;
 first_ = 6.11790433991715155 ;
 last_ = 6.11790433991715155 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/train_stats.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/train_stats.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -24,6 +24,8 @@
 agemax_ = 0 ;
 first_ = 394.528599820739259 ;
 last_ = 394.528599820739259 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -43,6 +45,8 @@
 agemax_ = 0 ;
 first_ = 394.528599820739259 ;
 last_ = 394.528599820739259 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -62,6 +66,8 @@
 agemax_ = 0 ;
 first_ = 6.05769163403522626 ;
 last_ = 6.05769163403522626 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -81,6 +87,8 @@
 agemax_ = 0 ;
 first_ = 6.17811704579907683 ;
 last_ = 6.17811704579907683 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -100,6 +108,8 @@
 agemax_ = 0 ;
 first_ = 6.11790433991715155 ;
 last_ = 6.11790433991715155 ;
+binary_ = 0 ;
+integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/metainfos.txt	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/metainfos.txt	2008-01-29 17:09:27 UTC (rev 8422)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL7398"
+__REVISION__ = "PL8398"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.amat

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/tester.psave	2008-01-29 16:13:19 UTC (rev 8421)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/tester.psave	2008-01-29 17:09:27 UTC (rev 8422)
@@ -38,6 +38,7 @@
 eigenvecs = 0  0  [ 
 ]
 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
 n_examples = -1 ;
@@ -66,6 +67,7 @@
 BIC = nan ;
 resid_variance = []
 ;
+random_gen = *0 ;
 stage = 0 ;
 n_examples = -1 ;
 inputsize = -1 ;
@@ -89,6 +91,7 @@
 put_raw_input = 0 ;
 share_learner = 0 ;
 nsep = 1 ;
+random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
 n_examples = -1 ;
@@ -114,6 +117,7 @@
 save_test_outputs = 1 ;
 call_forget_in_run = 1 ;
 save_test_costs = 1 ;
+save_test_names = 1 ;
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;



From tihocan at mail.berlios.de  Tue Jan 29 19:52:12 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 29 Jan 2008 19:52:12 +0100
Subject: [Plearn-commits] r8423 - trunk/plearn/math
Message-ID: <200801291852.m0TIqC2u028399@sheep.berlios.de>

Author: tihocan
Date: 2008-01-29 19:52:12 +0100 (Tue, 29 Jan 2008)
New Revision: 8423

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
No need to write fieldname in error message since it will be empty

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2008-01-29 17:09:27 UTC (rev 8422)
+++ trunk/plearn/math/VecStatsCollector.cc	2008-01-29 18:52:12 UTC (rev 8423)
@@ -251,10 +251,10 @@
     in.smartReadUntilNext("[", statname);
     string fieldname;
     in.smartReadUntilNext("]", fieldname);
-    if(fieldname.size()==0)
+    if(fieldname.empty())
         PLERROR("In VecStatsCollector::getStat - the stat asked is invalid."
-                "Parsed stat name '%s' and field name '%d'",
-                statname.c_str(),fieldname.c_str());
+                "Parsed stat name '%s' with an empty field name.",
+                statname.c_str());
     int fieldnum = getFieldNum(fieldname);
     if(fieldnum<0)
         PLERROR("In VecStatsCollector::getStat invalid fieldname: %s;\n"



From saintmlx at mail.berlios.de  Tue Jan 29 20:10:06 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 29 Jan 2008 20:10:06 +0100
Subject: [Plearn-commits] r8424 - trunk/python_modules/plearn/pymake
Message-ID: <200801291910.m0TJA6bm030481@sheep.berlios.de>

Author: saintmlx
Date: 2008-01-29 20:10:06 +0100 (Tue, 29 Jan 2008)
New Revision: 8424

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
- added help text for -pyso option



Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-01-29 18:52:12 UTC (rev 8423)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-01-29 19:10:06 UTC (rev 8424)
@@ -89,6 +89,8 @@
         on ia64. Allows to compile on .pymake/linux-x86_64.hosts,
         linux-ia64.hosts and linux-i386.hosts.
   -so:  create a shared object (.so) instead of an executable file.
+  -pyso: same as -so, but for python extension modules (does not add
+         'lib' before the .so name.)
   -static: produce a statically linked executable.
 
 Options that will not affect the final compiled file:



From ducharme at mail.berlios.de  Tue Jan 29 20:33:44 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 29 Jan 2008 20:33:44 +0100
Subject: [Plearn-commits] r8425 - trunk/plearn_learners/regressors
Message-ID: <200801291933.m0TJXinl000448@sheep.berlios.de>

Author: ducharme
Date: 2008-01-29 20:33:44 +0100 (Tue, 29 Jan 2008)
New Revision: 8425

Modified:
   trunk/plearn_learners/regressors/WPLS.cc
Log:


Modified: trunk/plearn_learners/regressors/WPLS.cc
===================================================================
--- trunk/plearn_learners/regressors/WPLS.cc	2008-01-29 19:10:06 UTC (rev 8424)
+++ trunk/plearn_learners/regressors/WPLS.cc	2008-01-29 19:33:44 UTC (rev 8425)
@@ -386,7 +386,8 @@
     stddev.resize(p+m);
     Vec input(p+m), target(p+m);
     real weight;
-    real sum_wi, sum_wi2;
+    real sum_wi = 0.0;
+    real sum_wi2 = 0.0;
     Vec sum_wixi(p+m), sum_wixi2(p+m);
     sum_wixi.fill(0.0);
     sum_wixi2.fill(0.0);



From nouiz at mail.berlios.de  Tue Jan 29 20:43:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 29 Jan 2008 20:43:20 +0100
Subject: [Plearn-commits] r8426 - in trunk/plearn_learners/regressors: .
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0
Message-ID: <200801291943.m0TJhK8m001591@sheep.berlios.de>

Author: nouiz
Date: 2008-01-29 20:43:19 +0100 (Tue, 29 Jan 2008)
New Revision: 8426

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
Log:
correctly init some variable to fix a pytest faillure at apstat.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-01-29 19:33:44 UTC (rev 8425)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-01-29 19:43:19 UTC (rev 8426)
@@ -53,6 +53,13 @@
     );
 
 RegressionTreeNode::RegressionTreeNode():
+    missing_is_valid(0),
+    loss_function_weight(1),
+    verbosity(0),
+    split_col(-1),
+    split_balance(INT_MAX),
+    split_feature_value(REAL_MAX),
+    after_split_error(REAL_MAX),
     dummy_int(0)
 {
     build();

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-01-29 19:33:44 UTC (rev 8425)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-01-29 19:43:19 UTC (rev 8426)
@@ -463,9 +463,9 @@
 leave_output = 2 [ -177.497500000000002 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
-split_balance = 0 ;
-split_feature_value = 0 ;
-after_split_error = 0 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
 missing_leave = *34 ->RegressionTreeLeave(
 id = 236 ;
@@ -597,9 +597,9 @@
 leave_output = 2 [ -175.837510000000009 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
-split_balance = 0 ;
-split_feature_value = 0 ;
-after_split_error = 0 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
 missing_leave = *42 ->RegressionTreeLeave(
 id = 113 ;
@@ -2641,9 +2641,9 @@
 leave_output = 2 [ 4.87410000000000032 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
-split_balance = 0 ;
-split_feature_value = 0 ;
-after_split_error = 0 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
 missing_leave = *164 ->RegressionTreeLeave(
 id = 188 ;
@@ -4481,9 +4481,9 @@
 leave_output = 2 [ 94.8581899999999933 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
-split_balance = 0 ;
-split_feature_value = 0 ;
-after_split_error = 0 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
 missing_leave = *274 ->RegressionTreeLeave(
 id = 173 ;
@@ -5406,9 +5406,9 @@
 leave_output = 2 [ 212.045299999999997 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
-split_balance = 0 ;
-split_feature_value = 0 ;
-after_split_error = 0 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
 missing_leave = *329 ->RegressionTreeLeave(
 id = 224 ;
@@ -5495,9 +5495,9 @@
 leave_output = 2 [ 287.378629999999987 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
-split_balance = 0 ;
-split_feature_value = 0 ;
-after_split_error = 0 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
 missing_leave = *334 ->RegressionTreeLeave(
 id = 50 ;



From nouiz at mail.berlios.de  Tue Jan 29 22:25:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 29 Jan 2008 22:25:19 +0100
Subject: [Plearn-commits] r8427 - trunk/plearn/vmat
Message-ID: <200801292125.m0TLPJ9D021280@sheep.berlios.de>

Author: nouiz
Date: 2008-01-29 22:25:17 +0100 (Tue, 29 Jan 2008)
New Revision: 8427

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/GaussianizeVMatrix.h
Log:
Added an build option 'gaussianize_binary' in GaussianizeVMatrix.  When false binary variable are not gaussianized.
The default value is false. This change the default behavior!!! All pytest are working, so if you use it in your experiment, check them...


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-01-29 19:43:19 UTC (rev 8426)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-01-29 21:25:17 UTC (rev 8427)
@@ -80,6 +80,7 @@
     gaussianize_target(false),
     gaussianize_weight(false),
     gaussianize_extra(false),
+    gaussianize_binary(false),
     threshold_ratio(10)
 {}
 
@@ -113,6 +114,11 @@
                   OptionBase::buildoption,
         "Whether or not to Gaussianize the extra part.");
 
+    declareOption(ol, "gaussianize_binary",
+                  &GaussianizeVMatrix::gaussianize_binary,
+                  OptionBase::buildoption,
+        "Whether or not to Gaussianize binary variable.");
+
     declareOption(ol, "train_source", &GaussianizeVMatrix::train_source,
                                       OptionBase::buildoption,
         "An optional VMat that will be used instead of 'source' to compute\n"
@@ -187,6 +193,8 @@
         StatsCollector& stat = stats[j];
         if (fast_exact_is_equal(stat.stddev(), 0))
             continue;
+        if (!gaussianize_binary && stat.isbinary())
+            continue;
         if ((stat.max() - stat.min()) > threshold_ratio * stat.stddev()) {
             features_to_gaussianize.append(j);
             values.append(Vec());

Modified: trunk/plearn/vmat/GaussianizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.h	2008-01-29 19:43:19 UTC (rev 8426)
+++ trunk/plearn/vmat/GaussianizeVMatrix.h	2008-01-29 21:25:17 UTC (rev 8427)
@@ -68,6 +68,7 @@
     bool gaussianize_target;
     bool gaussianize_weight;
     bool gaussianize_extra;
+    bool gaussianize_binary;
     real threshold_ratio;
     VMat train_source;
 



From tihocan at mail.berlios.de  Wed Jan 30 17:23:54 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 30 Jan 2008 17:23:54 +0100
Subject: [Plearn-commits] r8428 - trunk/plearn/ker
Message-ID: <200801301623.m0UGNsjD013701@sheep.berlios.de>

Author: tihocan
Date: 2008-01-30 17:23:53 +0100 (Wed, 30 Jan 2008)
New Revision: 8428

Modified:
   trunk/plearn/ker/GeodesicDistanceKernel.cc
   trunk/plearn/ker/GeodesicDistanceKernel.h
Log:
Implemented proper build mechanism in constructor

Modified: trunk/plearn/ker/GeodesicDistanceKernel.cc
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.cc	2008-01-29 21:25:17 UTC (rev 8427)
+++ trunk/plearn/ker/GeodesicDistanceKernel.cc	2008-01-30 16:23:53 UTC (rev 8428)
@@ -63,14 +63,17 @@
 GeodesicDistanceKernel::GeodesicDistanceKernel(
         Ker the_distance_kernel, int the_knn,
         const PPath& the_geodesic_file, bool the_pow_distance,
-        const string& the_method):
+        const string& the_method,
+        bool call_build_):
+    inherited(true, call_build_),
     geodesic_file(the_geodesic_file),
     knn(the_knn),
     pow_distance(the_pow_distance),
     shortest_algo(the_method)
 {
     distance_kernel = the_distance_kernel;
-    build();
+    if (call_build_)
+        build_();
 }
 
 PLEARN_IMPLEMENT_OBJECT(GeodesicDistanceKernel,

Modified: trunk/plearn/ker/GeodesicDistanceKernel.h
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.h	2008-01-29 21:25:17 UTC (rev 8427)
+++ trunk/plearn/ker/GeodesicDistanceKernel.h	2008-01-30 16:23:53 UTC (rev 8428)
@@ -89,7 +89,8 @@
     GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn = 10,
                            const PPath& the_geodesic_file = "",
                            bool the_pow_distance = false,
-                           const string& the_method = "floyd");
+                           const string& the_method = "floyd",
+                           bool call_build_ = true);
 
     // ******************
     // * Kernel methods *



From tihocan at mail.berlios.de  Wed Jan 30 17:24:50 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 30 Jan 2008 17:24:50 +0100
Subject: [Plearn-commits] r8429 - trunk/plearn_learners/online
Message-ID: <200801301624.m0UGOop8013837@sheep.berlios.de>

Author: tihocan
Date: 2008-01-30 17:24:49 +0100 (Wed, 30 Jan 2008)
New Revision: 8429

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
Log:
Added option to have a decreasing L2 penalty

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-01-30 16:23:53 UTC (rev 8428)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-01-30 16:24:49 UTC (rev 8429)
@@ -54,7 +54,9 @@
     gibbs_ma_increment(0.1),
     gibbs_initial_ma_coefficient(0.1),
     L1_penalty_factor(0),
-    L2_penalty_factor(0)
+    L2_penalty_factor(0),
+    L2_decrease_constant(0),
+    L2_n_updates(0)
 {
 }
 
@@ -100,7 +102,19 @@
                   "minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 "
                   "during training.\n");
 
+    declareOption(ol, "L2_decrease_constant", 
+                  &RBMMatrixConnection::L2_decrease_constant,
+                  OptionBase::buildoption,
+        "The L2 penalty is divided by (1 + t*L2_decrease_constant) where\n"
+        "'t' is the number of times this penalty has been used to modify\n"
+        "the weights.",
+        OptionBase::advanced_level);
 
+    declareOption(ol, "L2_n_updates", 
+                  &RBMMatrixConnection::L2_n_updates,
+                  OptionBase::learntoption,
+        "Number of times that weights have been changed by the L2 penalty\n"
+        "update rule.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -816,7 +830,8 @@
 void RBMMatrixConnection::applyWeightPenalty()
 {
     real delta_L1 = learning_rate * L1_penalty_factor;
-    real delta_L2 = learning_rate * L2_penalty_factor;
+    real delta_L2 = learning_rate * L2_penalty_factor
+                                  / (1 + L2_decrease_constant * L2_n_updates);
     for( int i=0; i<up_size; i++)
     {
         real* w_ = weights[i];
@@ -836,6 +851,8 @@
             }
         }
     }
+    if (delta_L2 > 0)
+        L2_n_updates++;
 }
 
 // Adds penalty (decay) gradient
@@ -890,6 +907,7 @@
 
         random_gen->fill_random_uniform( weights, -d, d );
     }
+    L2_n_updates = 0;
 }
 
 

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2008-01-30 16:23:53 UTC (rev 8428)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2008-01-30 16:24:49 UTC (rev 8429)
@@ -74,6 +74,9 @@
     //! Optional (default=0) factor of L2 regularization term
     real L2_penalty_factor;
 
+    real L2_decrease_constant;
+    int L2_n_updates;
+
     //! Matrix containing unit-to-unit weights (output_size ? input_size)
     Mat weights;
 



From tihocan at mail.berlios.de  Wed Jan 30 17:25:04 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 30 Jan 2008 17:25:04 +0100
Subject: [Plearn-commits] r8430 - trunk/plearn_learners/hyper
Message-ID: <200801301625.m0UGP4Rr013887@sheep.berlios.de>

Author: tihocan
Date: 2008-01-30 17:25:03 +0100 (Wed, 30 Jan 2008)
New Revision: 8430

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
Log:
Using perr instead of cerr

Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-01-30 16:24:49 UTC (rev 8429)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-01-30 16:25:03 UTC (rev 8430)
@@ -218,7 +218,7 @@
             }
 
             if(verbosity>0)
-                cerr<<"HyperLearner: starting the optimization"<<endl;
+                perr<<"HyperLearner: starting the optimization"<<endl;
 
             results = strategy[commandnum]->optimize();
         }



From tihocan at mail.berlios.de  Wed Jan 30 17:25:34 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 30 Jan 2008 17:25:34 +0100
Subject: [Plearn-commits] r8431 - trunk/plearn_learners/unsupervised
Message-ID: <200801301625.m0UGPY9U013918@sheep.berlios.de>

Author: tihocan
Date: 2008-01-30 17:25:33 +0100 (Wed, 30 Jan 2008)
New Revision: 8431

Modified:
   trunk/plearn_learners/unsupervised/KernelProjection.cc
Log:
Using pout instead of cout, and removed some annoying output

Modified: trunk/plearn_learners/unsupervised/KernelProjection.cc
===================================================================
--- trunk/plearn_learners/unsupervised/KernelProjection.cc	2008-01-30 16:25:03 UTC (rev 8430)
+++ trunk/plearn_learners/unsupervised/KernelProjection.cc	2008-01-30 16:25:33 UTC (rev 8431)
@@ -192,7 +192,7 @@
     costs[0] = abs(k_x_x - fs_norm);
     if (k_x_x - fs_norm < -1e-5) {
         // TODO Remove this later after making sure it didn't happen.
-        cout << "Negative error: " << k_x_x - fs_norm << " (k_x_x = " << k_x_x << ", fs_norm = " << fs_norm << ")" << endl;
+        perr << "Negative error: " << k_x_x - fs_norm << " (k_x_x = " << k_x_x << ", fs_norm = " << fs_norm << ")" << endl;
     }
 }                                
 
@@ -244,9 +244,11 @@
 void KernelProjection::forget()
 {
     stage = 0;
-    cout << "forget: n_comp_kept = " << n_comp_kept << endl;
+    if (verbosity > 1)
+        pout << "forget: n_comp_kept = " << n_comp_kept << endl;
     n_comp_kept = n_comp;
-    cout << "forget: n_comp_kept = " << n_comp_kept << endl;
+    if (verbosity > 1)
+        pout << "forget: n_comp_kept = " << n_comp_kept << endl;
     n_examples = 0;
     first_output = true;
     last_input.resize(0);
@@ -340,7 +342,7 @@
     kernel->computeGramMatrix(gram);
     time_for_gram = clock() - time_for_gram;
     if (verbosity >= 3) {
-        cout << flush;
+        pout << flush;
     }
     // (2) Compute its eigenvectors and eigenvalues.
     eigenVecOfSymmMat(gram, n_comp + ignore_n_first, eigenvalues, eigenvectors);



From tihocan at mail.berlios.de  Thu Jan 31 17:40:18 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 17:40:18 +0100
Subject: [Plearn-commits] r8432 - trunk/plearn/var
Message-ID: <200801311640.m0VGeIhn024495@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 17:40:18 +0100 (Thu, 31 Jan 2008)
New Revision: 8432

Modified:
   trunk/plearn/var/TimesConstantVariable.h
Log:
declareOptions should be protected

Modified: trunk/plearn/var/TimesConstantVariable.h
===================================================================
--- trunk/plearn/var/TimesConstantVariable.h	2008-01-30 16:25:33 UTC (rev 8431)
+++ trunk/plearn/var/TimesConstantVariable.h	2008-01-31 16:40:18 UTC (rev 8432)
@@ -69,7 +69,6 @@
 
     PLEARN_DECLARE_OBJECT(TimesConstantVariable);
 
-    static void declareOptions(OptionList &ol);
 
     virtual string info() const
     { return string("TimesConstant (* ")+tostring(cst)+")"; }
@@ -79,6 +78,10 @@
     virtual void bprop();
     virtual void symbolicBprop();
     virtual void rfprop();
+
+protected:
+
+    static void declareOptions(OptionList &ol);
 };
 
 DECLARE_OBJECT_PTR(TimesConstantVariable);



From tihocan at mail.berlios.de  Thu Jan 31 17:41:32 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 17:41:32 +0100
Subject: [Plearn-commits] r8433 - trunk/plearn/var
Message-ID: <200801311641.m0VGfWhE024695@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 17:41:32 +0100 (Thu, 31 Jan 2008)
New Revision: 8433

Modified:
   trunk/plearn/var/ColumnIndexVariable.cc
Log:
- Better help
- Added a safety check at build time


Modified: trunk/plearn/var/ColumnIndexVariable.cc
===================================================================
--- trunk/plearn/var/ColumnIndexVariable.cc	2008-01-31 16:40:18 UTC (rev 8432)
+++ trunk/plearn/var/ColumnIndexVariable.cc	2008-01-31 16:41:32 UTC (rev 8433)
@@ -49,39 +49,59 @@
 
 /** ColumnIndexVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(ColumnIndexVariable,
-                        "Return a row vector with the elements indexed in each column",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+    ColumnIndexVariable,
+    "Return a row vector with the elements indexed in each column.",
+    "The first input is a matrix of size NxM.\n"
+    "The second input is a vector of size M, with elements in {0, ..., N-1}.\n"
+    "The result is a row vector of size M, where the i-th element is equal\n"
+    "to input1(input2[i], i)."
+);
 
 ColumnIndexVariable::ColumnIndexVariable(Variable *input1, Variable *input2)
-    : inherited(input1, input2, 1/*input2->length()*/, input1->width())
+    : inherited(input1, input2, 1, input1->width())
 {
     build_();
 }
 
-void
-ColumnIndexVariable::build()
+///////////
+// build //
+///////////
+void ColumnIndexVariable::build()
 {
     inherited::build();
     build_();
 }
 
-void
-ColumnIndexVariable::build_()
+////////////
+// build_ //
+////////////
+void ColumnIndexVariable::build_()
 {
     if (input2 && !input2->isVec())
-        PLERROR("In ColumnIndexVariable: input2 must be a vector variable representing the indexs of input1");
+        PLERROR("In ColumnIndexVariable::build_ - input2 must be a vector "
+                "variable representing the indices of input1");
+    if (input1 && input2 && input1->width() != input2->size())
+        PLERROR("In ColumnIndexVariable::build_ - input1's width (%d) "
+                "should be equal to input2's size (%s)",
+                input1->width(), input2->size());
 }
 
+///////////////////
+// recomputeSize //
+///////////////////
 void ColumnIndexVariable::recomputeSize(int& l, int& w) const
 {
-    l = 1; /*input2->length()*/
+    l = 1;
     if (input1)
         w = input1->width();
     else
         w = 0;
 }
 
+///////////
+// fprop //
+///////////
 void ColumnIndexVariable::fprop()
 {
     for (int i=0; i<input2->size(); i++)



From tihocan at mail.berlios.de  Thu Jan 31 17:43:10 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 17:43:10 +0100
Subject: [Plearn-commits] r8434 - trunk/plearn/vmat
Message-ID: <200801311643.m0VGhAfb024937@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 17:43:10 +0100 (Thu, 31 Jan 2008)
New Revision: 8434

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
Added missing initialization of reorder_fieldspec_from_headers (set to false by default)


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-01-31 16:41:32 UTC (rev 8433)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-01-31 16:43:10 UTC (rev 8434)
@@ -49,15 +49,15 @@
 
 char TextFilesVMatrix::buf[50000];
 
-TextFilesVMatrix::TextFilesVMatrix()
-    : idxfile(0),
-      delimiter("\t"),
-      quote_delimiter(""),
-      auto_build_map(false),
-      auto_extend_map(true),
-      build_vmatrix_stringmap(false)
-{
-}
+TextFilesVMatrix::TextFilesVMatrix():
+    idxfile(0),
+    delimiter("\t"),
+    quote_delimiter(""),
+    auto_build_map(false),
+    auto_extend_map(true),
+    build_vmatrix_stringmap(false),
+    reorder_fieldspec_from_headers(false)
+{}
 
 PLEARN_IMPLEMENT_OBJECT(
     TextFilesVMatrix,
@@ -835,8 +835,9 @@
     declareOption(ol, "reorder_fieldspec_from_headers", 
                   &TextFilesVMatrix::reorder_fieldspec_from_headers,
                   OptionBase::buildoption,
-                  "If true, will reorder the fieldspec in the order gived "
-                  "by the field names taken from txtfilenames");
+                  "If true, will reorder the fieldspec in the order given "
+                  "by the field names taken from txtfilenames.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }



From tihocan at mail.berlios.de  Thu Jan 31 17:43:45 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 17:43:45 +0100
Subject: [Plearn-commits] r8435 - trunk/plearn/vmat
Message-ID: <200801311643.m0VGhjjR025056@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 17:43:44 +0100 (Thu, 31 Jan 2008)
New Revision: 8435

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.h
Log:
Removed duplicated help - better keep help up-to-date in a single place

Modified: trunk/plearn/vmat/TextFilesVMatrix.h
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.h	2008-01-31 16:43:10 UTC (rev 8434)
+++ trunk/plearn/vmat/TextFilesVMatrix.h	2008-01-31 16:43:44 UTC (rev 8435)
@@ -126,8 +126,6 @@
     //! specific stringmap
     bool build_vmatrix_stringmap;
 
-    //! If true, will reorder the fieldspec in the order gived
-    //! by the field names taken from txtfilenames
     bool reorder_fieldspec_from_headers;
 
     // ****************



From tihocan at mail.berlios.de  Thu Jan 31 17:44:38 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 17:44:38 +0100
Subject: [Plearn-commits] r8436 - trunk/plearn_learners/online
Message-ID: <200801311644.m0VGicfc025253@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 17:44:38 +0100 (Thu, 31 Jan 2008)
New Revision: 8436

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
Log:
Added new decrease mechanism for the L2 penalty


Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-01-31 16:43:44 UTC (rev 8435)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-01-31 16:44:38 UTC (rev 8436)
@@ -56,6 +56,8 @@
     L1_penalty_factor(0),
     L2_penalty_factor(0),
     L2_decrease_constant(0),
+    L2_shift(100),
+    L2_decrease_type("one_over_t"),
     L2_n_updates(0)
 {
 }
@@ -105,17 +107,34 @@
     declareOption(ol, "L2_decrease_constant", 
                   &RBMMatrixConnection::L2_decrease_constant,
                   OptionBase::buildoption,
-        "The L2 penalty is divided by (1 + t*L2_decrease_constant) where\n"
-        "'t' is the number of times this penalty has been used to modify\n"
-        "the weights.",
+        "Parameter of the L2 penalty decrease (see L2_decrease_type).",
         OptionBase::advanced_level);
 
+    declareOption(ol, "L2_shift", 
+                  &RBMMatrixConnection::L2_shift,
+                  OptionBase::buildoption,
+        "Parameter of the L2 penalty decrease (see L2_decrease_type).",
+        OptionBase::advanced_level);
+
+    declareOption(ol, "L2_decrease_type", 
+                  &RBMMatrixConnection::L2_decrease_type,
+                  OptionBase::buildoption,
+        "The kind of L2 decrease that is being applied. The decrease\n"
+        "consists in scaling the L2 penalty by a factor that depends on the\n"
+        "number 't' of times this penalty has been used to modify the\n"
+        "weights of the connection. It can be one of:\n"
+        " - 'one_over_t': 1 / (1 + t * L2_decrease_constant)\n"
+        " - 'sigmoid_like': sigmoid(L2_shift - t * L2_decrease_constant)",
+        OptionBase::advanced_level);
+
     declareOption(ol, "L2_n_updates", 
                   &RBMMatrixConnection::L2_n_updates,
                   OptionBase::learntoption,
         "Number of times that weights have been changed by the L2 penalty\n"
         "update rule.");
 
+
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -826,12 +845,21 @@
         addWeightPenalty(weights, weights_gradient);
 }
 
-// Applies penalty (decay) on weights
+////////////////////////
+// applyWeightPenalty //
+////////////////////////
 void RBMMatrixConnection::applyWeightPenalty()
 {
+    // Apply penalty (decay) on weights.
     real delta_L1 = learning_rate * L1_penalty_factor;
-    real delta_L2 = learning_rate * L2_penalty_factor
-                                  / (1 + L2_decrease_constant * L2_n_updates);
+    real delta_L2 = learning_rate * L2_penalty_factor;
+    if (L2_decrease_type == "one_over_t")
+        delta_L2 /= (1 + L2_decrease_constant * L2_n_updates);
+    else if (L2_decrease_type == "sigmoid_like")
+        delta_L2 *= sigmoid(L2_shift - L2_decrease_constant * L2_n_updates);
+    else
+        PLERROR("In RBMMatrixConnection::applyWeightPenalty - Invalid value "
+                "for L2_decrease_type: %s", L2_decrease_type.c_str());
     for( int i=0; i<up_size; i++)
     {
         real* w_ = weights[i];
@@ -855,11 +883,16 @@
         L2_n_updates++;
 }
 
-// Adds penalty (decay) gradient
+//////////////////////
+// addWeightPenalty //
+//////////////////////
 void RBMMatrixConnection::addWeightPenalty(Mat weights, Mat weight_gradients)
 {
+    // Add penalty (decay) gradient.
     real delta_L1 = L1_penalty_factor;
     real delta_L2 = L2_penalty_factor;
+    PLASSERT_MSG( is_equal(L2_decrease_constant, 0) && is_equal(L2_shift, 100),
+                  "L2 decrease not implemented in this method" );
     for( int i=0; i<weights.length(); i++)
     {
         real* w_ = weights[i];

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2008-01-31 16:43:44 UTC (rev 8435)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2008-01-31 16:44:38 UTC (rev 8436)
@@ -75,6 +75,8 @@
     real L2_penalty_factor;
 
     real L2_decrease_constant;
+    real L2_shift;
+    string L2_decrease_type;
     int L2_n_updates;
 
     //! Matrix containing unit-to-unit weights (output_size ? input_size)



From tihocan at mail.berlios.de  Thu Jan 31 17:45:21 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 17:45:21 +0100
Subject: [Plearn-commits] r8437 - trunk/plearn_learners/generic
Message-ID: <200801311645.m0VGjLXn025473@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 17:45:20 +0100 (Thu, 31 Jan 2008)
New Revision: 8437

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
It is better to use isEmpty() than == '' on a PPath

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-01-31 16:44:38 UTC (rev 8436)
+++ trunk/plearn_learners/generic/NNet.cc	2008-01-31 16:45:20 UTC (rev 8437)
@@ -1061,9 +1061,9 @@
         pb = new ProgressBar("Training " + classname() + " from stage " + tostring(stage) + " to " + tostring(nstages), nstages-stage);
 
 
-    //Open/create vmat to save train costs at each epoch
+    // Open/create vmat to save train costs at each epoch.
     VMat costs_per_epoch= 0;
-    if(expdir != "")
+    if(!expdir.isEmpty())
     {
         PPath cpe_path= expdir / "NNet_train_costs.pmat";
         if(isfile(cpe_path))



From tihocan at mail.berlios.de  Thu Jan 31 17:46:22 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 17:46:22 +0100
Subject: [Plearn-commits] r8438 - trunk/plearn/vmat
Message-ID: <200801311646.m0VGkMTA025630@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 17:46:22 +0100 (Thu, 31 Jan 2008)
New Revision: 8438

Modified:
   trunk/plearn/vmat/MemoryVMatrix.cc
   trunk/plearn/vmat/MemoryVMatrix.h
Log:
Modified convenience constructor to use a proper build mechanism


Modified: trunk/plearn/vmat/MemoryVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MemoryVMatrix.cc	2008-01-31 16:45:20 UTC (rev 8437)
+++ trunk/plearn/vmat/MemoryVMatrix.cc	2008-01-31 16:46:22 UTC (rev 8438)
@@ -47,12 +47,16 @@
 
 /** MemoryVMatrix **/
 
-PLEARN_IMPLEMENT_OBJECT(MemoryVMatrix,
-                        "A VMatrix whose data is stored in memory.",
-                        "The data can either be given directly by a Mat, or by another VMat that\n"
-                        "will be precomputed in memory at build time.\n"
-    );
+PLEARN_IMPLEMENT_OBJECT(
+    MemoryVMatrix,
+    "A VMatrix whose data is stored in memory.",
+    "The data can either be given directly by a Mat, or by another VMat that\n"
+    "will be precomputed in memory at build time.\n"
+);
 
+///////////////////
+// MemoryVMatrix //
+///////////////////
 MemoryVMatrix::MemoryVMatrix()
     : synch_data(true),
       data(Mat()),
@@ -82,16 +86,14 @@
     defineSizes(the_data.width(), 0, 0);
 }
 
-MemoryVMatrix::MemoryVMatrix(VMat the_source)
-    : inherited(the_source->length(), the_source->width()),
-      memory_data(the_source->toMat()),
-      synch_data(false),
-      source(the_source),
-      deep_copy_memory_data(true)
-
+MemoryVMatrix::MemoryVMatrix(VMat the_source, bool call_build_):
+    inherited(the_source->length(), the_source->width(), call_build_),
+    synch_data(false),
+    source(the_source),
+    deep_copy_memory_data(true)
 {
-    copySizesFrom(the_source);
-    setMetaInfoFrom(the_source);
+    if (call_build_)
+        build_();
 }
 
 ////////////////////

Modified: trunk/plearn/vmat/MemoryVMatrix.h
===================================================================
--- trunk/plearn/vmat/MemoryVMatrix.h	2008-01-31 16:45:20 UTC (rev 8437)
+++ trunk/plearn/vmat/MemoryVMatrix.h	2008-01-31 16:46:22 UTC (rev 8438)
@@ -90,7 +90,7 @@
     MemoryVMatrix();
     MemoryVMatrix(const Mat& the_data);
     MemoryVMatrix(int l, int w);
-    MemoryVMatrix(VMat the_source);
+    MemoryVMatrix(VMat the_source, bool call_build_ = true);
     virtual real get(int i, int j) const;
     virtual void getSubRow(int i, int j, Vec v) const;
     virtual void getRow(int i, Vec v) const;



From tihocan at mail.berlios.de  Thu Jan 31 17:48:51 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 17:48:51 +0100
Subject: [Plearn-commits] r8439 - trunk/plearn/vmat
Message-ID: <200801311648.m0VGmpXT025950@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 17:48:50 +0100 (Thu, 31 Jan 2008)
New Revision: 8439

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
- Fixed some indent and comments
- getPrecomputedStatsFromFile now systematically locks the metadatadir to ensure that the same stats are never computed twice at the same time


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-01-31 16:46:22 UTC (rev 8438)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-01-31 16:48:50 UTC (rev 8439)
@@ -178,10 +178,8 @@
                 TVec<int>(col, col + the_source->extrasize() - 1, 1));
     col += the_source->extrasize();
 
-    the_source->lockMetaDataDir();
     TVec<StatsCollector> stats = the_source->
         getPrecomputedStatsFromFile("stats_gaussianizeVMatrix.psave", -1, true);
-    the_source->unlockMetaDataDir();
 
     // See which dimensions violate the Gaussian assumption and will be
     // actually Gaussianized, and store the corresponding list of values.

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-01-31 16:46:22 UTC (rev 8438)
+++ trunk/plearn/vmat/VMatrix.cc	2008-01-31 16:48:50 UTC (rev 8439)
@@ -1398,18 +1398,21 @@
     return field_stats;
 }
 
-/////////////////////////
-// getPrecomputedStats //
-/////////////////////////
-TVec<StatsCollector> VMatrix::getPrecomputedStatsFromFile(const string filename, const int maxnvalues, bool progress_bar) const
+/////////////////////////////////
+// getPrecomputedStatsFromFile //
+/////////////////////////////////
+TVec<StatsCollector> VMatrix::getPrecomputedStatsFromFile(
+        const string& filename, int maxnvalues, bool progress_bar) const
 {
     TVec<StatsCollector> stats;
     PPath metadatadir = getMetaDataDir();
-    PPath statsfile =  metadatadir + filename;
+    PPath statsfile =  metadatadir / filename;
+    lockMetaDataDir();
     if (isfile(statsfile) && getMtime()<mtime(statsfile))
     {
-        if(getMtime()==0)
-            PLWARNING("Warning: using a saved stat file (%s) but mtime is 0.\n(cannot be sure file is up to date)",
+        if(getMtime() == 0)
+            PLWARNING("Warning: using a saved stat file (%s) but mtime is 0"
+                      "(cannot be sure file is up to date).",
                       statsfile.absolute().c_str());
         PLearn::load(statsfile, stats);
     }
@@ -1420,9 +1423,13 @@
         if(!metadatadir.isEmpty())
             PLearn::save(statsfile, stats);
     }
+    unlockMetaDataDir();
     return stats;
 }
 
+/////////////////////
+// remote_getStats //
+/////////////////////
 TVec<PP<StatsCollector> > VMatrix::remote_getStats() const
 {
     if(field_p_stats.isEmpty())
@@ -1887,6 +1894,9 @@
         }
 }
 
+///////////////
+// getRowVec //
+///////////////
 Vec VMatrix::getRowVec(int i) const
 {
     Vec v(width());
@@ -1894,6 +1904,9 @@
     return v;
 }
 
+////////////////
+// appendRows //
+////////////////
 void VMatrix::appendRows(Mat rows)
 {
     for(int i=0; i<rows.length(); i++)
@@ -1901,14 +1914,19 @@
 }
 
 
-int VMatrix:: compareStats(const VMat& target,
-                           const real stderror_threshold,
-                           const real missing_threshold,
-                           real* sumdiff_stderr,
-                           real* sumdiff_missing) const
+//////////////////
+// compareStats //
+//////////////////
+int VMatrix::compareStats(const VMat& target,
+                          const real stderror_threshold,
+                          const real missing_threshold,
+                          real* sumdiff_stderr,
+                          real* sumdiff_missing) const
 {
     if(target->width()!=width())
-        PLERROR("In VecStatsCollector:: compareStats() - this vmatris have width %d witch differ from the target width of %d", width(), target->width());
+        PLERROR("In VecStatsCollector:: compareStats() - This VMatrix has "
+                "width %d which differs from the target width of %d",
+                width(), target->width());
 
     int nbdiff            = 0;
     real sumdiff_stderr_  = 0;

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-01-31 16:46:22 UTC (rev 8438)
+++ trunk/plearn/vmat/VMatrix.h	2008-01-31 16:48:50 UTC (rev 8439)
@@ -584,27 +584,32 @@
     void loadStats(const PPath& filename);
 
     /**
-     *  Returns the unconditonal statistics for all fields from the stats.psave
-     *  file (if the file does not exist, a default version is automatically
-     *  created).
+     * Returns the unconditional statistics for all fields from the
+     * stats.psave file (if the file does not exist, a default version is
+     * automatically created).
      */
     TVec<StatsCollector> getStats() const;
-    TVec<StatsCollector> getPrecomputedStatsFromFile(const string filename, const int maxnvalues, bool progress_bar) const;
 
+    //! Generic function to obtain the statistics from a given file in the
+    //! metadatadir. If this file does not exist, statistics are computed and
+    //! saved in this file.
+    TVec<StatsCollector> getPrecomputedStatsFromFile(const string& filename,
+                                                     int maxnvalues,
+                                                     bool progress_bar) const;
+
     TVec<PP<StatsCollector> > remote_getStats() const;
 
     StatsCollector& getStats(int fieldnum) const
     { return getStats()[fieldnum]; }
 
     
-    /** Compare the stats of this VecStatsCollector with the target one.
-     * @param target the VMat we compare again
+    /** Compare the stats of this VMat with the target one.
+     * @param target The VMat we compare against
      * @param stderror_threshold The threshold allowed for the standard error
      * @param missing_threshold The threshold allowed for the % of missing
-     * @param sumdiff_stderr The sum of all variable difference of stderr
-     * @param sumdiff_missing The sum of all variable difference of missing
-     * @return If they are comparable with respect to the gived threshold,
-     * we return true. Otherwise we return false
+     * @param sumdiff_stderr The sum of all variable differences of stderr
+     * @param sumdiff_missing The sum of all variable differences of missing
+     * @return The number of differences that were found
      */
     int compareStats(const VMat& target,
                      const real stderror_threshold = 2,



From nouiz at mail.berlios.de  Thu Jan 31 19:00:17 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jan 2008 19:00:17 +0100
Subject: [Plearn-commits] r8440 - in trunk/plearn_learners/regressors: .
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0
Message-ID: <200801311800.m0VI0HoT013361@sheep.berlios.de>

Author: nouiz
Date: 2008-01-31 19:00:16 +0100 (Thu, 31 Jan 2008)
New Revision: 8440

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
Log:
correctly initialize variable in RegressionTreeLeave.cc and RegressionTreeRegisters.cc. Now all class for RegresionTree should be correctly initialized.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-01-31 16:48:50 UTC (rev 8439)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-01-31 18:00:16 UTC (rev 8440)
@@ -50,7 +50,17 @@
                         "of the samples in the leave.\n"
     );
 
-RegressionTreeLeave::RegressionTreeLeave()
+RegressionTreeLeave::RegressionTreeLeave():
+    id(-1),
+    missing_leave(false),
+    loss_function_weight(0),
+    verbosity(0),
+    length(0),
+    weights_sum(0),
+    targets_sum(0),
+    weighted_targets_sum(0),
+    weighted_squared_targets_sum(0),
+    loss_function_factor(1)
 {
     build();
 }
@@ -120,7 +130,8 @@
     targets_sum = 0.0;
     weighted_targets_sum = 0.0;
     weighted_squared_targets_sum = 0.0; 
-    if (loss_function_weight != 0.0) loss_function_factor = 2.0 / pow(loss_function_weight, 2);
+    if (loss_function_weight != 0.0) 
+        loss_function_factor = 2.0 / pow(loss_function_weight, 2);
     else loss_function_factor = 1.0;
 }
 

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-01-31 16:48:50 UTC (rev 8439)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-01-31 18:00:16 UTC (rev 8440)
@@ -55,7 +55,10 @@
                         "It is also used to maintain the next available leave id.\n"
     );
 
-RegressionTreeRegisters::RegressionTreeRegisters()
+RegressionTreeRegisters::RegressionTreeRegisters():
+    report_progress(0),
+    verbosity(0),
+    next_id(0)
 {
     build();
 }

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-01-31 16:48:50 UTC (rev 8439)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-01-31 18:00:16 UTC (rev 8440)
@@ -48,7 +48,7 @@
 multiclass_outputs = []
 ;
 leave_template = *7 ->RegressionTreeLeave(
-id = 0 ;
+id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
@@ -5632,6 +5632,7 @@
 ;
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
+save_best_learner = 0 ;
 splitter = *0  )
 ] ;
 provide_strategy_expdir = 1 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-01-31 16:48:50 UTC (rev 8439)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-01-31 18:00:16 UTC (rev 8440)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8398"
+__REVISION__ = "PL8439"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-01-31 16:48:50 UTC (rev 8439)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-01-31 18:00:16 UTC (rev 8440)
@@ -45,7 +45,7 @@
 multiclass_outputs = []
 ;
 leave_template = *7 ->RegressionTreeLeave(
-id = 0 ;
+id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -55,7 +55,7 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 0 ;
+loss_function_factor = 1 ;
 output = []
 ;
 error = []
@@ -132,6 +132,7 @@
 ;
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
+save_best_learner = 0 ;
 splitter = *0  )
 ] ;
 provide_strategy_expdir = 1 ;



From tihocan at mail.berlios.de  Thu Jan 31 19:46:06 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 31 Jan 2008 19:46:06 +0100
Subject: [Plearn-commits] r8441 - trunk/plearn_learners/online
Message-ID: <200801311846.m0VIk6MR025035@sheep.berlios.de>

Author: tihocan
Date: 2008-01-31 19:46:06 +0100 (Thu, 31 Jan 2008)
New Revision: 8441

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.cc
Log:
Small change in the L2 penalty decrease function to make it easier to set hyperparameters

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-01-31 18:00:16 UTC (rev 8440)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-01-31 18:46:06 UTC (rev 8441)
@@ -124,7 +124,7 @@
         "number 't' of times this penalty has been used to modify the\n"
         "weights of the connection. It can be one of:\n"
         " - 'one_over_t': 1 / (1 + t * L2_decrease_constant)\n"
-        " - 'sigmoid_like': sigmoid(L2_shift - t * L2_decrease_constant)",
+        " - 'sigmoid_like': sigmoid((L2_shift - t) * L2_decrease_constant)",
         OptionBase::advanced_level);
 
     declareOption(ol, "L2_n_updates", 
@@ -856,7 +856,7 @@
     if (L2_decrease_type == "one_over_t")
         delta_L2 /= (1 + L2_decrease_constant * L2_n_updates);
     else if (L2_decrease_type == "sigmoid_like")
-        delta_L2 *= sigmoid(L2_shift - L2_decrease_constant * L2_n_updates);
+        delta_L2 *= sigmoid((L2_shift - L2_n_updates) * L2_decrease_constant);
     else
         PLERROR("In RBMMatrixConnection::applyWeightPenalty - Invalid value "
                 "for L2_decrease_type: %s", L2_decrease_type.c_str());



