<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r10314 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2010-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r10314%20-%20trunk/plearn_learners/online&In-Reply-To=%3C201001212230.o0LMU7mh002289%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   
   <LINK REL="Next"  HREF="003755.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r10314 - trunk/plearn_learners/online</H1>
    <B>islaja at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r10314%20-%20trunk/plearn_learners/online&In-Reply-To=%3C201001212230.o0LMU7mh002289%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r10314 - trunk/plearn_learners/online">islaja at mail.berlios.de
       </A><BR>
    <I>Thu Jan 21 23:30:07 CET 2010</I>
    <P><UL>
        
        <LI>Next message: <A HREF="003755.html">[Plearn-commits] r10315 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3754">[ date ]</a>
              <a href="thread.html#3754">[ thread ]</a>
              <a href="subject.html#3754">[ subject ]</a>
              <a href="author.html#3754">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: islaja
Date: 2010-01-21 23:30:06 +0100 (Thu, 21 Jan 2010)
New Revision: 10314

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
-New option for pepper salt noise:
    -'pep_salt_zero_centered' indicates the pepper and salt value : if==0 then pepper value=0 and salt value=1. if==x pepper then pepper value=-x and salt value=x.
   
-Train basic auto-encoders (AE) with noisy example:
    -Use 'noisy'. = 0 : no noisy example
                  = 1 : noisy example for unsup. pre-training
                  = 2 : noisy example for unsup. pre-training and supervised fine-tuning.

-Option to save the learner just after the pre-train
     -Use 'learnerExpdir' and 'save_learner_before_fine_tuning'.

-Option keep_online_representations&quot;,
    -Use 'keep_online_representations' to keep trace of the representations obtained during the pre-train.

- Possibility to use separated encoding and decoding layers. Until now, decoding layers were the same as the encoding layers, so that when the bias of a decoding layer was updated, the bias of the associated encoding layer was also updated.
        -Use 'reconstruction_layers'

-Added the possibility to use the 'computeOutputs' function even during the pre-training to enhance efficiency. 

-Modifications about fantasizeKTime.
    -New parameter: 'alwaysFromSrcImg', indicates if each encodage-decodage phase (fantasize phase) is always from the source image or if we use the output of the preceding fantasize phase.
    -New function 'fantasizeKTimeOnMultiSrcImg', will call fantasizeKTime for each images found in the matrix 'srcImg' received as a parameter.

-New pre-training variante: Stack Renoising Auto-Encoder (SRAE)
     -Use 'renoising'. Same as Stacked Denoising Auto-Encoder (SDAE) except that the RAE compare its output vector with a _second corrupted version of the input_.

-Addition to a pre-training variante: deMissing stacked auto-encoder (SMAE)
    - Added a second way to include missing data state: 'one_if_missing', a binary vector is used to specify if (1) the associated input is missing or (0) not.




Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-12-10 20:02:50 UTC (rev 10313)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2010-01-21 22:30:06 UTC (rev 10314)
@@ -42,6 +42,7 @@
 #include &quot;StackedAutoassociatorsNet.h&quot;
 #include &lt;plearn/io/pl_log.h&gt;
 #include &lt;plearn/sys/Profiler.h&gt;
+#include &lt;plearn/io/load_and_save.h&gt;
 
 #define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
 
@@ -70,11 +71,14 @@
     missing_data_method( &quot;binomial_complementary&quot;),
     corrupted_data_weight( 1 ),
     data_weight( 1 ),
-    fraction_of_masked_inputs( 0 ),
-    probability_of_masked_inputs( 0 ),
-    probability_of_masked_target( 0 ),
+    fraction_of_masked_inputs( 0. ),
+    probability_of_masked_inputs( 0. ),
+    probability_of_masked_target( 0. ),
     mask_with_mean( false ),
     mask_with_pepper_salt( false ),
+    pep_salt_zero_centered( 0. ),
+    renoising( false ),
+    noisy( 0 ),
     prob_salt_noise( 0.5 ),
     gaussian_std( 1. ),
     binary_sampling_noise_parameter( 1. ),
@@ -85,6 +89,9 @@
     mask_input_layer_only( false ),
     mask_input_layer_only_in_unsupervised_fine_tuning( false ),
     train_stats_window( -1 ),
+    learnerExpdir(&quot;&quot;),
+    save_learner_before_fine_tuning( false ),
+    keep_online_representations( false ),
     n_layers( 0 ),
     unsupervised_stage( 0 ),
     minibatch_size( 0 ),
@@ -155,6 +162,12 @@
                   &quot;subsequent elements should be the hidden layers. The\n&quot;
                   &quot;output layer should not be included in layers.\n&quot;);
 
+    declareOption(ol, &quot;reconstruction_layers&quot;, &amp;StackedAutoassociatorsNet::reconstruction_layers,
+                  OptionBase::buildoption,
+                  &quot;The reconstruction layers in the network (if different than encodage layers.\n&quot;
+                  &quot;The first element of this vector should be the layer for the input layer reconstruction and the\n&quot;
+                  &quot;subsequent elements should be the layer for the reconstruction of hidden layers.\n&quot;);
+
     declareOption(ol, &quot;connections&quot;, &amp;StackedAutoassociatorsNet::connections,
                   OptionBase::buildoption,
                   &quot;The weights of the connections between the layers&quot;);
@@ -263,7 +276,7 @@
                   &quot;Method used to fill the double_input vector for missing_data noise type.&quot;
                   &quot;Choose among:\n&quot;
                   &quot; - \&quot;binomial_complementary\&quot;\n&quot;
-                  &quot; - \&quot;none\&quot;\n&quot;
+                  &quot; - \&quot;one_if_missing\&quot;&quot;
         );
 
     declareOption(ol, &quot;corrupted_data_weight&quot;,
@@ -315,6 +328,38 @@
                   &quot;0 or 1 according to prob_salt_noise.\n&quot;
         );
 
+    declareOption(ol, &quot;pep_salt_zero_centered&quot;,
+                  &amp;StackedAutoassociatorsNet::pep_salt_zero_centered,
+                  OptionBase::buildoption,
+                  &quot; Indicate if the mask is zero centered (&gt;0) or not (==0). &quot;
+                  &quot; If equal 0 (not centered)&quot;
+                  &quot; then pepVal is 0 and saltVal is 1.&quot;
+                  &quot; If is greater than 0 (centered),&quot;
+                  &quot; then pepVal is -pep_salt_zero_centered and &quot;
+                  &quot; saltVal is pep_salt_zero_centered.\n&quot;
+        );
+
+    declareOption(ol, &quot;renoising&quot;,
+                  &amp;StackedAutoassociatorsNet::renoising,
+                  OptionBase::buildoption,
+                  &quot;Indication that the autoassociator will try to&quot;
+                  &quot;'reconstruct' _another_ corrupted version of the input&quot;
+                  &quot;(instead of the input itself),&quot;
+                  &quot;from an initial encoded corrupted version of the input.\n&quot;
+        );
+
+    declareOption(ol, &quot;noisy&quot;,
+                  &amp;StackedAutoassociatorsNet::noisy, 
+                  OptionBase::buildoption,
+                  &quot;Indication that example are corrupted before using them for a particular training.&quot;
+                  &quot;Note that the original example are used for any test.&quot;
+                  &quot;Choose among:\n&quot;
+                  &quot;0 : no example noisy\n&quot;
+                  &quot;1 : noisy applied before unsup. pre-training (basic autoassociator will be used (no denoising).\n&quot;
+                  &quot;2 : noisy applied before unsup. pre-training and before supervised fine-tuning.\n&quot; 
+        );
+
+
     declareOption(ol, &quot;prob_salt_noise&quot;,
                   &amp;StackedAutoassociatorsNet::prob_salt_noise,
                   OptionBase::buildoption,
@@ -376,6 +421,27 @@
                   &quot;The number of samples to use to compute training stats.\n&quot;
                   &quot;-1 (default) means the number of training samples.\n&quot;);
 
+
+    declareOption(ol, &quot;learnerExpdir&quot;,
+                  &amp;StackedAutoassociatorsNet::learnerExpdir,
+                  OptionBase::buildoption,
+                  &quot;Experiment directory where the learner will be save\n&quot;
+                  &quot;if save_learner_before_fine_tuning is true.&quot;
+        );
+
+    declareOption(ol, &quot;save_learner_before_fine_tuning&quot;,
+                  &amp;StackedAutoassociatorsNet::save_learner_before_fine_tuning,
+                  OptionBase::buildoption,
+                  &quot;Saves the learner before the supervised fine-tuning.&quot;
+        );
+
+    declareOption(ol, &quot;keep_online_representations&quot;,
+                  &amp;StackedAutoassociatorsNet::keep_online_representations,
+                  OptionBase::buildoption,
+                  &quot;Keep trace of the representations obtained during an &quot;
+                  &quot;unsupervised training phase.\n&quot;
+        );
+    
     declareOption(ol, &quot;greedy_stages&quot;,
                   &amp;StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -419,8 +485,10 @@
     declareMethod(
         rmm, &quot;fantasizeKTime&quot;,
         &amp;StackedAutoassociatorsNet::fantasizeKTime,
-        (BodyDoc(&quot;On a trained learner, computes a codage-decodage (fantasize) through a specified number of hidden layer.&quot;),
-         ArgDoc (&quot;kTime&quot;, &quot;Number of time we want to fantasize (next source image will be the last fantasize Image, and so on for kTime.)&quot;),
+        (BodyDoc(&quot;On a trained learner, computes a codage-decodage phase (fantasize phase) through a specified number of hidden layer. From one specified source image.&quot;),
+         ArgDoc (&quot;kTime&quot;, &quot;Number of time we want to fantasize. \n&quot; 
+                 &quot;Next input image will again be the source Image (if alwaysFromSrcImg is True) \n&quot;
+                 &quot;or next input image will be the last fantasize image (if alwaysFromSrcImg is False), and so on for kTime.)&quot;),
          ArgDoc (&quot;srcImg&quot;, &quot;Source image vector (should have same width as raws layer)&quot;),
          ArgDoc (&quot;sampling&quot;, &quot;Vector of bool indicating whether or not a sampling will be done for each hidden layer\n&quot;
                 &quot;during decodage. Its width indicates how many hidden layer will be used.)\n&quot;
@@ -430,7 +498,43 @@
                 &quot;(according to the one used during the learning stage)\n&quot;
                 &quot;for each layer. (should have same width as sampling or be empty if unuseful.\n&quot;
                 &quot;Smaller element of the vector correspond to lower layer&quot;),
-         RetDoc (&quot;Corresponding fantasize image (will have same width as srcImg)&quot;)));
+         ArgDoc (&quot;alwaysFromSrcImg&quot;, &quot;Booleen indicating whether each encode-decode \n&quot;
+                &quot;steps are done from the source image (sets to True) or \n&quot;
+                &quot;if the next input image is the preceding fantasize image obtained (sets to False). &quot;),
+         RetDoc (&quot;Fantasize images obtained for each kTime.&quot;)));
+    
+    declareMethod(
+        rmm, &quot;fantasizeKTimeOnMultiSrcImg&quot;,
+        &amp;StackedAutoassociatorsNet::fantasizeKTimeOnMultiSrcImg,
+        (BodyDoc(&quot;Call the 'fantasizeKTime' function for each source images found in the matrix 'srcImg'.&quot;),
+         ArgDoc (&quot;kTime&quot;, &quot;Number of time we want to fantasize for each source images. \n&quot;
+                 &quot;Next input image will again be the source Image (if alwaysFromSrcImg is True) \n&quot;
+                 &quot;or next input image will be the last fantasize image (if alwaysFromSrcImg is False), and so on for kTime.)&quot;),
+         ArgDoc (&quot;srcImg&quot;, &quot;Source images matrix (should have same width as raws layer)&quot;),
+         ArgDoc (&quot;sampling&quot;, &quot;Vector of bool indicating whether or not a sampling will be done for each hidden layer\n&quot;
+                &quot;during decodage. Its width indicates how many hidden layer will be used.)\n&quot;
+                &quot; (should have same width as maskNoiseFractOrProb)\n&quot;
+                &quot;smaller element of the vector correspond to lower layer&quot;),
+         ArgDoc (&quot;maskNoiseFractOrProb&quot;, &quot;Vector of noise fraction or probability\n&quot;
+                &quot;(according to the one used during the learning stage)\n&quot;
+                &quot;for each layer. (should have same width as sampling or be empty if unuseful.\n&quot;
+                &quot;Smaller element of the vector correspond to lower layer&quot;),
+         ArgDoc (&quot;alwaysFromSrcImg&quot;, &quot;Booleen indicating whether each encode-decode \n&quot;
+                &quot;steps are done from the source image (sets to True) or \n&quot;
+                &quot;if the next input image is the preceding fantasize image obtained (sets to False). &quot;),
+         RetDoc (&quot;For each source images, fantasize images obtained for each kTime.&quot;)));
+    
+    declareMethod(
+        rmm, &quot;getTrainRepresentations&quot;, &amp;StackedAutoassociatorsNet::getTrainRepresentations,
+        (BodyDoc(&quot;Returns the representations obtained during last pre-training of the current layer.\n&quot;),
+         RetDoc (&quot;Current train representations&quot;)));
+
+    declareMethod(
+        rmm, &quot;remote_setCurrentlyTrainedLayer&quot;, &amp;StackedAutoassociatorsNet::remote_setCurrentlyTrainedLayer,
+        (BodyDoc(&quot;Modify current_trained_layer.\n&quot;),
+        ArgDoc (&quot;input&quot;, &quot;Matrix of inputs.&quot;),
+        RetDoc (&quot;Outputs from each hidden layers.&quot;)));
+   
 }
 
 void StackedAutoassociatorsNet::build_()
@@ -517,6 +621,21 @@
                     &quot;corrupted inputs only works with masking noise in online setting,&quot;
                     &quot;in the non-minibatch case.\n&quot;);
 
+        if( renoising &amp;&amp; noisy &gt; 0 )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_()&quot;
+                    &quot; - \n&quot;
+                    &quot;cannot use renoising and noisy at the same time.\n&quot;);
+
+        if( renoising &amp;&amp; noise_type == &quot;missing_data&quot; )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_()&quot;
+                    &quot; - \n&quot;
+                    &quot;cannot use renoising with missing data.\n&quot;);
+
+        if( noisy &gt; 0 &amp;&amp; noise_type == &quot;missing_data&quot;)
+            PLERROR(&quot;StackedAutoassociatorsNet::build_()&quot;
+                    &quot; - \n&quot;
+                    &quot;cannot use noisy with missing data.\n&quot;);
+
         if( !online )
         {
             if( greedy_stages.length() == 0)
@@ -576,23 +695,23 @@
                 &quot;compute_all_test_costs option is not implemented for\n&quot;
                 &quot;reconstruct_hidden option.&quot;);
 
-    if( noise_type == &quot;missing_data&quot;)
+    if( noise_type == &quot;missing_data&quot; || renoising || noisy &gt; 0 )
     {
         if( correlation_connections.length() !=0 )
             PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
-                    &quot;Missing data is not implemented with correlation_connections.\n&quot;);
+                    &quot;Missing data, renoising and noisy are not implemented with correlation_connections.\n&quot;);
     
         if( direct_connections.length() !=0 )
             PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
-                    &quot;Missing data is not implemented with direct_connections.\n&quot;);
+                    &quot;Missing data, renoising and noisy are not implemented with direct_connections.\n&quot;);
         
         if( reconstruct_hidden )
             PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
-                    &quot;Missing data is not implemented with reconstruct_hidden.\n&quot;);
+                    &quot;Missing data, renoising and noisy are not implemented with reconstruct_hidden.\n&quot;);
 
         if( online ) 
             PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
-                    &quot;Missing data is not implemented in the online setting.\n&quot;);
+                    &quot;Missing data, renoising and noisy are not implemented in the online setting.\n&quot;);
     }
 
     if(correlation_connections.length() != 0)
@@ -635,9 +754,26 @@
     expectation_gradients.resize( n_layers );
     expectation_gradients_m.resize( n_layers );
 
+    // If not defined, reconstruction_layers will  
+    // simply point to the layers vector. 
+    if( reconstruction_layers.length() == 0 )
+        reconstruction_layers = layers;
+    else
+        if( reconstruction_layers.length() != layers.length()-1 &amp;&amp; 
+            reconstruction_layers.length() != layers.length() )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() &quot;
+                     &quot;- \n&quot;
+                    &quot;reconstruction_layers should have a length of layers.length-1 or layers.length, i.e: %d\n.&quot;,
+                     layers.length()-1);
 
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
+        if( layers[i]-&gt;size != reconstruction_layers[i]-&gt;size )
+            PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() &quot;
+                     &quot;- \n&quot;
+                    &quot;layers[%i] should have same size of reconstruction_layers[%i], i.e: %d.\n&quot;,
+                    i, i, layers[i]-&gt;size);
+
         if( noise_type == &quot;missing_data&quot;)
         {
             if( layers[i]-&gt;size * 2 != connections[i]-&gt;down_size )
@@ -765,6 +901,12 @@
             layers[i]-&gt;random_gen = random_gen;
             layers[i]-&gt;forget();
         }
+        
+        if( !(reconstruction_layers[i]-&gt;random_gen) )
+        {
+            reconstruction_layers[i]-&gt;random_gen = random_gen;
+            reconstruction_layers[i]-&gt;forget();
+        }
 
         if( !(connections[i]-&gt;random_gen) )
         {
@@ -794,6 +936,8 @@
     expectation_gradients[n_layers-1].resize( layers[n_layers-1]-&gt;size );
 
     reconstruction_weights.resize( layers[0]-&gt;size );
+    // Will be correctly resized if keep_online_representations == True
+    train_representations.resize( 1 );
 
     // For denoising autoencoders
     doubled_expectations.resize( n_layers-1 );
@@ -803,6 +947,9 @@
     
     if( (noise_type == &quot;masking_noise&quot; || noise_type == &quot;missing_data&quot;) &amp;&amp; fraction_of_masked_inputs &gt; 0 )
         autoassociator_expectation_indices.resize( n_layers-1 );
+    
+    if( renoising || noisy &gt; 0 )
+       second_corrupted_autoassociator_expectations.resize( n_layers-1 );
 
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
@@ -826,6 +973,9 @@
             for( int j=0 ; j &lt; autoassociator_expectation_indices[i].length() ; j++ )
                 autoassociator_expectation_indices[i][j] = j;
         }
+
+        if( renoising || noisy &gt; 0 )
+            second_corrupted_autoassociator_expectations[i].resize( layers[i]-&gt;size );
     }
 
     if(greedy_target_connections.length() != 0)
@@ -935,6 +1085,7 @@
     // Public options
     deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
+    deepCopyField(reconstruction_layers, copies);
     deepCopyField(connections, copies);
     deepCopyField(reconstruction_connections, copies);
     deepCopyField(correlation_connections, copies);
@@ -994,6 +1145,7 @@
     deepCopyField(final_cost_gradient, copies);
     deepCopyField(final_cost_gradients, copies);
     deepCopyField(corrupted_autoassociator_expectations, copies);
+    deepCopyField(second_corrupted_autoassociator_expectations, copies);
     deepCopyField(reconstruction_weights, copies);
     deepCopyField(binary_masks, copies);
     deepCopyField(tmp_mask, copies);
@@ -1033,6 +1185,7 @@
 
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
+        reconstruction_layers[i]-&gt;forget();
         connections[i]-&gt;forget();
         reconstruction_connections[i]-&gt;forget();
     }
@@ -1174,6 +1327,7 @@
             train_costs.fill(MISSING_VALUE);
             lr = greedy_learning_rate;
             layers[i]-&gt;setLearningRate( lr );
+            reconstruction_layers[i]-&gt;setLearningRate( lr );
             connections[i]-&gt;setLearningRate( lr );
             reconstruction_connections[i]-&gt;setLearningRate( lr );
             if(correlation_connections.length() != 0)
@@ -1218,6 +1372,14 @@
                 direct_and_reconstruction_activations.resize(layers[i]-&gt;size);
                 direct_and_reconstruction_activation_gradients.resize(layers[i]-&gt;size);
             }
+
+            if( keep_online_representations )
+            {
+                train_representations.resize(end_stage-(*this_stage));
+                train_representations.clear();
+            }
+            int greedyBatchSize = end_stage - (*this_stage);
+            string old_noise_type = noise_type;
             for( ; *this_stage&lt;end_stage ; (*this_stage)++ )
             {
                 if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
@@ -1225,6 +1387,7 @@
                     lr = greedy_learning_rate/(1 + greedy_decrease_ct
                                                * (*this_stage));
                     layers[i]-&gt;setLearningRate( lr );
+                    reconstruction_layers[i]-&gt;setLearningRate( lr );
                     connections[i]-&gt;setLearningRate( lr );
                     reconstruction_connections[i]-&gt;setLearningRate( lr );
                     layers[i+1]-&gt;setLearningRate( lr );
@@ -1242,9 +1405,24 @@
                     if( greedy_target_connections.length() &amp;&amp; greedy_target_connections[i] )
                         greedy_target_connections[i]-&gt;setLearningRate( lr );
                 }
+                int train_representations_i = 0;
                 sample = *this_stage % nsamples;
                 train_set-&gt;getExample(sample, input, target, weight);
-                greedyStep( input, target, i, train_costs );
+                if( keep_online_representations )
+                {
+                    train_representations_i = greedyBatchSize - (end_stage-(*this_stage));
+                    train_representations[train_representations_i].resize(layers[i+1]-&gt;size);
+                }
+                if( noisy &gt;= 1 )
+                {
+                    corrupt_input( input, second_corrupted_autoassociator_expectations[0], 0 );
+                    noise_type = &quot;none&quot;;
+                    greedyStep( second_corrupted_autoassociator_expectations[0], target, i, train_costs, train_representations[train_representations_i]);
+                    noise_type = old_noise_type;
+                }
+                else
+                    greedyStep( input, target, i, train_costs, train_representations[train_representations_i]);
+                
                 train_stats-&gt;update( train_costs );
 
                 if( pb )
@@ -1295,6 +1473,7 @@
 
             setLearningRate( unsupervised_fine_tuning_learning_rate );
             train_costs.fill(MISSING_VALUE);
+            string old_noise_type = noise_type;
             for( ; unsupervised_stage&lt;unsupervised_nstages ; unsupervised_stage++ )
             {
                 sample = unsupervised_stage % nsamples;
@@ -1305,7 +1484,15 @@
                            * unsupervised_stage ) );
 
                 train_set-&gt;getExample( sample, input, target, weight );
-                unsupervisedFineTuningStep( input, target, train_costs );
+                if( noisy &gt;= 1)
+                {
+                    corrupt_input( input, second_corrupted_autoassociator_expectations[0], 0 );
+                    noise_type = &quot;none&quot;;
+                    unsupervisedFineTuningStep(second_corrupted_autoassociator_expectations[0], target, train_costs );
+                    noise_type = old_noise_type;
+                }
+                else
+                    unsupervisedFineTuningStep( input, target, train_costs );
                 train_stats-&gt;update( train_costs );
 
                 if( pb )
@@ -1314,6 +1501,16 @@
             Profiler::pl_profile_end(&quot;StackedAutoassociatorsNet::train unsupervised&quot;);
         }
 
+        if( save_learner_before_fine_tuning )
+        {
+            if( learnerExpdir == &quot;&quot; )
+                PLWARNING(&quot;StackedAutoassociatorsNet::train() - \n&quot;
+                    &quot;cannot save model before fine-tuning because\n&quot;
+                    &quot;no experiment directory has been set.&quot;);
+            else
+                PLearn::save(learnerExpdir + &quot;/learner_before_finetuning.psave&quot;,*this);
+        }
+
         /***** fine-tuning by gradient descent *****/
         if( stage &lt; nstages )
         {
@@ -1341,7 +1538,13 @@
                                      / (1. + fine_tuning_decrease_ct * stage ) );
 
                 train_set-&gt;getExample( sample, input, target, weight );
-                fineTuningStep( input, target, train_costs );
+                if( noisy &gt;= 2)
+                {
+                    corrupt_input( input, second_corrupted_autoassociator_expectations[0], 0 );
+                    fineTuningStep( second_corrupted_autoassociator_expectations[0], target, train_costs );
+                }
+                else
+                    fineTuningStep( input, target, train_costs );
                 train_stats-&gt;update( train_costs );
 
                 if( pb )
@@ -1468,10 +1671,19 @@
                 PLERROR(&quot;In StackedAutoassociatorsNet::corrupt_input():&quot; 
                         &quot; fraction_of_masked_inputs and probability_of_masked_inputs can't be both &gt; 0&quot;);
             if( mask_with_pepper_salt )
+            {
+                real pepVal = 0;
+                real saltVal = 1;
+                if( pep_salt_zero_centered&gt;0. )
+                {
+                    pepVal = -pep_salt_zero_centered;
+                    saltVal = pep_salt_zero_centered;
+                }
                 for( int j=0 ; j &lt;input.length() ; j++)
+                {
                     if( random_gen-&gt;uniform_sample() &lt; probability_of_masked_inputs )
                     {
-                        corrupted_input[ j ] = random_gen-&gt;binomial_sample(prob_salt_noise);
+                        corrupted_input[ j ] = random_gen-&gt;bounded_sample(prob_salt_noise,pepVal,saltVal);
                         reconstruction_weights[j] = corrupted_data_weight;
                     }
                     else
@@ -1479,8 +1691,12 @@
                         corrupted_input[ j ] = input[ j ];  
                         reconstruction_weights[j] = data_weight;
                     }
+                }
+            }       
             else if( mask_with_mean )
+            {
                 for( int j=0 ; j &lt;input.length() ; j++)
+                {
                     if( random_gen-&gt;uniform_sample() &lt; probability_of_masked_inputs )
                     {                    
                         corrupted_input[ j ] = expectation_means[layer][ j ];
@@ -1492,8 +1708,12 @@
                         corrupted_input[ j ] = input[ j ];
                         reconstruction_weights[j] = data_weight;
                     }
+                }
+            }
             else
+            {
                 for( int j=0 ; j &lt;input.length() ; j++)
+                {
                     if( random_gen-&gt;uniform_sample() &lt; probability_of_masked_inputs )
                     {
                         corrupted_input[ j ] = 0;   
@@ -1505,56 +1725,73 @@
                         corrupted_input[ j ] = input[ j ];
                         reconstruction_weights[j] = data_weight;
                     }
+                }
+            }
         }
         else
         {
             corrupted_input &lt;&lt; input;
             reconstruction_weights.fill(data_weight);
-            if( fraction_of_masked_inputs != 0 ) 
+            if( fraction_of_masked_inputs &gt; 0. ) 
             {
                 random_gen-&gt;shuffleElements(autoassociator_expectation_indices[layer]);
                 if( mask_with_pepper_salt )
+                {
+                    real pepVal = 0;
+                    real saltVal = 1;
+                    if( pep_salt_zero_centered&gt;0. )
+                    {
+                        pepVal = -pep_salt_zero_centered;
+                        saltVal = pep_salt_zero_centered;
+                    }
                     for( int j=0 ; j &lt; round(fraction_of_masked_inputs*input.length()) ; j++)
                     {
-                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] = random_gen-&gt;binomial_sample(prob_salt_noise);
+                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] = random_gen-&gt;bounded_sample(prob_salt_noise,pepVal,saltVal);
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                     }
+                }   
                 else if( mask_with_mean )
+                {
                     for( int j=0 ; j &lt; round(fraction_of_masked_inputs*input.length()) ; j++)
                     {
                         corrupted_input[ autoassociator_expectation_indices[layer][j] ] = expectation_means[layer][autoassociator_expectation_indices[layer][j]];
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                         binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
                     }
+                }
                 else
+                {
                     for( int j=0 ; j &lt; round(fraction_of_masked_inputs*input.length()) ; j++)
                     {
                         corrupted_input[ autoassociator_expectation_indices[layer][j] ] = 0;
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                         binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
                     }
+                }
             }
         }
     }
     else if( noise_type == &quot;binary_sampling&quot; )
-    {
         for( int i=0; i&lt;corrupted_input.length(); i++ )
             corrupted_input[i] = random_gen-&gt;binomial_sample((input[i]-0.5)*binary_sampling_noise_parameter+0.5);
-    }
     else if( noise_type == &quot;gaussian&quot; )
-    {
         for( int i=0; i&lt;corrupted_input.length(); i++ )
-            corrupted_input[i] = input[i] +
-                random_gen-&gt;gaussian_01() * gaussian_std;
-    }
+            corrupted_input[i] = input[i] + random_gen-&gt;gaussian_01() * gaussian_std;
     else if( noise_type == &quot;missing_data&quot;)
     {
         // The entry input is the doubled one according to missing_data_method
         int original_input_length = input.length() / 2;
         reconstruction_weights.resize(original_input_length);
    
-        if(missing_data_method == &quot;binomial_complementary&quot;)
+        if(missing_data_method == &quot;binomial_complementary&quot; || 
+           missing_data_method == &quot;one_if_missing&quot;)
         {
+            int down_missing_value = 0;
+            int up_missing_value = 0;
+        
+            if(missing_data_method == &quot;one_if_missing&quot;)
+                up_missing_value = 1;
+
             if( probability_of_masked_inputs &gt; 0 )
             {
                 if( fraction_of_masked_inputs &gt; 0 )
@@ -1563,8 +1800,8 @@
                 for( int j=0 ; j&lt;original_input_length ; j++ )
                     if( random_gen-&gt;uniform_sample() &lt; probability_of_masked_inputs )
                     {
-                        corrupted_input[ j*2 ] = 0;
-                        corrupted_input[ j*2+1 ] = 0;
+                        corrupted_input[ j*2 ] = down_missing_value;
+                        corrupted_input[ j*2+1 ] = up_missing_value;
                         reconstruction_weights[j] = corrupted_data_weight;
                     }
                     else
@@ -1578,13 +1815,13 @@
             {
                 corrupted_input &lt;&lt; input;
                 reconstruction_weights.fill(data_weight);
-                if( fraction_of_masked_inputs != 0 )
+                if( fraction_of_masked_inputs &gt; 0. )
                 {
                     random_gen-&gt;shuffleElements(autoassociator_expectation_indices[layer]);
                     for( int j=0 ; j &lt; round(fraction_of_masked_inputs*original_input_length) ; j++)
                     {
-                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 ] = 0;
-                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 + 1 ] = 0;
+                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 ] = down_missing_value;
+                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 + 1 ] = up_missing_value;
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                     }
                 }
@@ -1602,34 +1839,37 @@
 }
 
 
+// ***** binomial_complementary ******
+// doubled_input[2*i] = input[i] and
+// doubled input[2*i+1] = 1-input[i]
+// If input is gradient that we have to double for backpropagation
+// (double_grad==true), then:
+// doubled_input[2*i] = input[i] and
+// oubled input[2*i+1] = -input[i]
+// ********** one_if_missing *********
+// doubled_input[2*i] = input[i] and
+// doubled input[2*i+1] = 0 (gradian or not)
 void StackedAutoassociatorsNet::double_input(const Vec&amp; input, Vec&amp; doubled_input, bool double_grad) const
 {
     if( noise_type == &quot;missing_data&quot; )
     {
-        if( missing_data_method == &quot;binomial_complementary&quot; )
+        doubled_input.resize(input.length()*2);
+        for( int i=0; i&lt;input.size(); i++ )
         {
-            // doubled_input[2*i] = input[i] and
-            // doubled input[2*i+1] = 1-input[i]
-            // If input is gradient that we have to double for backpropagation,
-            // (double_grad==true), then:
-            // doubled_input[2*i] = input[i] and
-            // doubled input[2*i+1] = -input[i] 
-            doubled_input.resize(input.length()*2);
-            for( int i=0; i&lt;input.size(); i++ )
+            doubled_input[i*2] = input[i];
+            if( missing_data_method == &quot;binomial_complementary&quot;)
             {
-                doubled_input[i*2] = input[i];
-                // double gradient before backpropagate 
-                // during the pre-training
                 if( double_grad )
                     doubled_input[i*2+1] = - input[i];
-                // double input of a layer
                 else
                     doubled_input[i*2+1] = 1 - input[i];
             }
+            else if( missing_data_method == &quot;one_if_missing&quot; )
+                doubled_input[i*2+1] = 0;
+            else
+                PLERROR(&quot;In StackedAutoassociatorsNet::double_input(): &quot;
+                &quot;missing_data_method %s not valid&quot;,missing_data_method.c_str());
         }
-        else
-            PLERROR(&quot;In StackedAutoassociatorsNet::double_input(): &quot;
-                    &quot;missing_data_method %s not valid&quot;,missing_data_method.c_str());
     }
     else
     {
@@ -1638,21 +1878,26 @@
     }
 }
 
+// ***** binomial_complementary *****
+// divided_input[i] = input[2*i] - input[2*i+1]
+// even if input is the doubled_gradient
+// ********** one_if_missing *********
+// divided_input[i] = input[2*i]
 void StackedAutoassociatorsNet::divide_input(const Vec&amp; input, Vec&amp; divided_input) const
 {
     if( noise_type == &quot;missing_data&quot; )
     {
-        if( missing_data_method == &quot;binomial_complementary&quot; )
+        divided_input.resize(input.length()/2);
+        for( int i=0; i&lt;divided_input.size(); i++ )  
         {
-            // divided_input[i] = input[2*i] - input[2*i+1]
-            // even if input is the doubled_gradient
-            divided_input.resize(input.length()/2);
-            for( int i=0; i&lt;divided_input.size(); i++)  
+            if( missing_data_method == &quot;binomial_complementary&quot; )
                 divided_input[i] = input[i*2] - input[i*2+1];
+            else if( missing_data_method == &quot;one_if_missing&quot; )
+                divided_input[i] = input[i*2];
+            else
+                PLERROR(&quot;In StackedAutoassociatorsNet::divide_input(): &quot;
+                        &quot;missing_data_method %s not valid&quot;, missing_data_method.c_str());
         }
-        else
-            PLERROR(&quot;In StackedAutoassociatorsNet::divide_input(): &quot;
-                    &quot;missing_data_method %s not valid&quot;, missing_data_method.c_str());
     }
     else
     {
@@ -1663,12 +1908,13 @@
 
 
 void StackedAutoassociatorsNet::greedyStep(const Vec&amp; input, const Vec&amp; target,
-                                           int index, Vec train_costs)
+                                           int index, Vec train_costs, Vec&amp; representation)
 {
     Profiler::pl_profile_start(&quot;StackedAutoassociatorsNet::greedyStep&quot;);
     PLASSERT( index &lt; n_layers );
 
     expectations[0] &lt;&lt; input;
+
     if(correlation_connections.length() != 0)
     {
         for( int i=0 ; i&lt;index + 1; i++ )
@@ -1685,7 +1931,7 @@
             if( i == index &amp;&amp; greedy_target_connections.length() &amp;&amp; greedy_target_connections[i] )
             {
                 target_vec.clear();
-                if( probability_of_masked_target == 0 ||
+                if( probability_of_masked_target == 0. ||
                     random_gen-&gt;uniform_sample() &gt;= probability_of_masked_target )
                     target_vec[(int)target[0]] = 1;
 
@@ -1720,7 +1966,7 @@
             if( i == index &amp;&amp; greedy_target_connections.length() &amp;&amp; greedy_target_connections[i] )
             {
                 target_vec.clear();
-                if( probability_of_masked_target == 0 ||
+                if( probability_of_masked_target == 0. ||
                     random_gen-&gt;uniform_sample() &gt;= probability_of_masked_target )
                     target_vec[(int)target[0]] = 1;
 
@@ -1730,6 +1976,8 @@
             }
 
             layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+            if( keep_online_representations )
+                representation &lt;&lt; expectations[i+1];
         }
     }
 
@@ -1775,19 +2023,19 @@
         direct_and_reconstruction_activations += direct_activations;
         direct_and_reconstruction_activations += reconstruction_activations;
 
-        layers[ index ]-&gt;fprop( direct_and_reconstruction_activations,
-                                layers[ index ]-&gt;expectation);
+        reconstruction_layers[ index ]-&gt;fprop( direct_and_reconstruction_activations,
+                                reconstruction_layers[ index ]-&gt;expectation);
 
-        layers[ index ]-&gt;activation &lt;&lt; direct_and_reconstruction_activations;
-        layers[ index ]-&gt;activation += layers[ index ]-&gt;bias;
-        //layers[ index ]-&gt;expectation_is_up_to_date = true;  // Won't work for certain RBMLayers
-        layers[ index ]-&gt;setExpectationByRef( layers[ index ]-&gt;expectation );
-        train_costs[index] = layers[ index ]-&gt;fpropNLL(expectations[index]);
+        reconstruction_layers[ index ]-&gt;activation &lt;&lt; direct_and_reconstruction_activations;
+        reconstruction_layers[ index ]-&gt;activation += reconstruction_layers[ index ]-&gt;bias;
+        //reconstruction_layers[ index ]-&gt;expectation_is_up_to_date = true;  // Won't work for certain RBMLayers
+        reconstruction_layers[ index ]-&gt;setExpectationByRef( reconstruction_layers[ index ]-&gt;expectation );
+        train_costs[index] = reconstruction_layers[ index ]-&gt;fpropNLL(expectations[index]);
 
-        layers[ index ]-&gt;bpropNLL(expectations[index], train_costs[index],
+        reconstruction_layers[ index ]-&gt;bpropNLL(expectations[index], train_costs[index],
                                   direct_and_reconstruction_activation_gradients);
 
-        layers[ index ]-&gt;update(direct_and_reconstruction_activation_gradients);
+        reconstruction_layers[ index ]-&gt;update(direct_and_reconstruction_activation_gradients);
 
         direct_connections[ index ]-&gt;bpropUpdate(
             corrupted_autoassociator_expectations[index],
@@ -1804,25 +2052,33 @@
     else
     {
         Vec divided_reconstruction_activations(reconstruction_activations.size());
-        Vec divided_reconstruction_activation_gradients(layers[ index ]-&gt;size);
+        Vec divided_reconstruction_activation_gradients(reconstruction_layers[ index ]-&gt;size);
         
         divide_input(reconstruction_activations, divided_reconstruction_activations);
 
-        layers[ index ]-&gt;fprop( divided_reconstruction_activations,
-                                layers[ index ]-&gt;expectation);
-        layers[ index ]-&gt;activation &lt;&lt; divided_reconstruction_activations;
-        layers[ index ]-&gt;activation += layers[ index ]-&gt;bias;
-        //layers[ index ]-&gt;expectation_is_up_to_date = true;
-        layers[ index ]-&gt;setExpectationByRef( layers[ index ]-&gt;expectation );
+        reconstruction_layers[ index ]-&gt;fprop( divided_reconstruction_activations,
+                               reconstruction_layers[ index ]-&gt;expectation);
+        reconstruction_layers[ index ]-&gt;activation &lt;&lt; divided_reconstruction_activations;
+        reconstruction_layers[ index ]-&gt;activation += reconstruction_layers[ index ]-&gt;bias;
+        //reconstruction_layers[ index ]-&gt;expectation_is_up_to_date = true;
+        reconstruction_layers[ index ]-&gt;setExpectationByRef( reconstruction_layers[ index ]-&gt;expectation );
         real rec_err;
 
         // If we want to compute reconstruction error according to reconstruction weights.
-        //   rec_err = layers[ index ]-&gt;fpropNLL(expectations[index], reconstruction_weights);
-        
-        rec_err = layers[ index ]-&gt;fpropNLL(expectations[index]);
-
+        //   rec_err = reconstruction_layers[ index ]-&gt;fpropNLL(expectations[index], reconstruction_weights);
+       
+        if( renoising )
+        {
+            corrupt_input( expectations[index], second_corrupted_autoassociator_expectations[index], index );
+            rec_err = reconstruction_layers[ index ]-&gt;fpropNLL(second_corrupted_autoassociator_expectations[index]);
+            reconstruction_layers[ index ]-&gt;bpropNLL(second_corrupted_autoassociator_expectations[index], rec_err, divided_reconstruction_activation_gradients);
+        }
+        else
+        {
+            rec_err = reconstruction_layers[ index ]-&gt;fpropNLL(expectations[index]);
+            reconstruction_layers[ index ]-&gt;bpropNLL(expectations[index], rec_err, divided_reconstruction_activation_gradients);
+        }
         train_costs[index] = rec_err;
-        layers[ index ]-&gt;bpropNLL(expectations[index], rec_err, divided_reconstruction_activation_gradients);
 
         // apply reconstruction weights which can be different for corrupted
         // (or missing) and non corrupted data. 
@@ -1836,7 +2092,7 @@
         if(reconstruct_hidden)
         {
             Profiler::pl_profile_start(&quot;StackedAutoassociatorsNet::greedyStep reconstruct_hidden&quot;);
-            connections[ index ]-&gt;fprop( layers[ index ]-&gt;expectation,
+            connections[ index ]-&gt;fprop( reconstruction_layers[ index ]-&gt;expectation,
                                          hidden_reconstruction_activations );
             layers[ index+1 ]-&gt;fprop( hidden_reconstruction_activations,
                 layers[ index+1 ]-&gt;expectation );
@@ -1853,29 +2109,29 @@
 
             Profiler::pl_profile_start(&quot;StackedAutoassociatorsNet::greedyStep reconstruct_hidden connection bprop&quot;);
             connections[ index ]-&gt;bpropUpdate(
-                layers[ index ]-&gt;expectation,
+                reconstruction_layers[ index ]-&gt;expectation,
                 hidden_reconstruction_activations,
                 reconstruction_expectation_gradients_from_hid_rec,
                 hidden_reconstruction_activation_gradients);
             Profiler::pl_profile_end(&quot;StackedAutoassociatorsNet::greedyStep reconstruct_hidden connection bprop&quot;);
 
-            layers[ index ]-&gt;bpropUpdate(
+            reconstruction_layers[ index ]-&gt;bpropUpdate(
                 reconstruction_activations,
-                layers[ index ]-&gt;expectation,
+                reconstruction_layers[ index ]-&gt;expectation,
                 reconstruction_activation_gradients_from_hid_rec,
                 reconstruction_expectation_gradients_from_hid_rec);
             Profiler::pl_profile_end(&quot;StackedAutoassociatorsNet::greedyStep reconstruct_hidden&quot;);
         }
 
-        layers[ index ]-&gt;update(divided_reconstruction_activation_gradients);
+        reconstruction_layers[ index ]-&gt;update(divided_reconstruction_activation_gradients);
         
         if(reconstruct_hidden)
             reconstruction_activation_gradients +=
                 reconstruction_activation_gradients_from_hid_rec;
 
         // // This is a bad update! Propagates gradient through sigmoid again!
-        // layers[ index ]-&gt;bpropUpdate( reconstruction_activations,
-        //                                   layers[ index ]-&gt;expectation,
+        // reconstruction_layers[ index ]-&gt;bpropUpdate( reconstruction_activations,
+        //                                   reconstruction_layers[ index ]-&gt;expectation,
         //                                   reconstruction_activation_gradients,
         //                                   reconstruction_expectation_gradients);
         reconstruction_connections[ index ]-&gt;bpropUpdate(
@@ -2250,7 +2506,7 @@
             if( greedy_target_connections.length() &amp;&amp; greedy_target_connections[i] )
             {
                 targets_vec[i].clear();
-                if( probability_of_masked_target == 0 ||
+                if( probability_of_masked_target == 0. ||
                     random_gen-&gt;uniform_sample() &gt;= probability_of_masked_target )
                     targets_vec[i][(int)target[0]] = 1;
 
@@ -2280,7 +2536,7 @@
             if( greedy_target_connections.length() &amp;&amp; greedy_target_connections[i] )
             {
                 targets_vec[i].clear();
-                if( probability_of_masked_target == 0 ||
+                if( probability_of_masked_target == 0. ||
                     random_gen-&gt;uniform_sample() &gt;= probability_of_masked_target )
                     targets_vec[i][(int)target[0]] = 1;
 
@@ -2304,6 +2560,7 @@
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
         layers[i]-&gt;setLearningRate( lr );
+        reconstruction_layers[i]-&gt;setLearningRate( lr );
         connections[i]-&gt;setLearningRate( lr );
         reconstruction_connections[i]-&gt;setLearningRate( lr );
         if(correlation_layers.length() != 0)
@@ -2327,20 +2584,20 @@
             expectations[ i ],
             reconstruction_activations);
 
-        layers[ i-1 ]-&gt;fprop( reconstruction_activations,
-                              layers[ i-1 ]-&gt;expectation);
+        reconstruction_layers[ i-1 ]-&gt;fprop( reconstruction_activations,
+                              reconstruction_layers[ i-1 ]-&gt;expectation);
 
-        layers[ i-1 ]-&gt;activation &lt;&lt; reconstruction_activations;
-        layers[ i-1 ]-&gt;activation += layers[ i-1 ]-&gt;bias;
-        //layers[ i-1 ]-&gt;expectation_is_up_to_date = true;
-        layers[ i-1 ]-&gt;setExpectationByRef( layers[ i-1 ]-&gt;expectation );
-        real rec_err = layers[ i-1 ]-&gt;fpropNLL( expectations[i-1] );
+        reconstruction_layers[ i-1 ]-&gt;activation &lt;&lt; reconstruction_activations;
+        reconstruction_layers[ i-1 ]-&gt;activation += reconstruction_layers[ i-1 ]-&gt;bias;
+        //reconstruction_layers[ i-1 ]-&gt;expectation_is_up_to_date = true;
+        reconstruction_layers[ i-1 ]-&gt;setExpectationByRef( reconstruction_layers[ i-1 ]-&gt;expectation );
+        real rec_err = reconstruction_layers[ i-1 ]-&gt;fpropNLL( expectations[i-1] );
         train_costs[i-1] = rec_err;
 
-        layers[ i-1 ]-&gt;bpropNLL(expectations[i-1], rec_err,
+        reconstruction_layers[ i-1 ]-&gt;bpropNLL(expectations[i-1], rec_err,
                                   reconstruction_activation_gradients);
 
-        layers[ i-1 ]-&gt;update(reconstruction_activation_gradients);
+        reconstruction_layers[ i-1 ]-&gt;update(reconstruction_activation_gradients);
 
         reconstruction_connections[ i-1 ]-&gt;bpropUpdate(
             expectations[ i ],
@@ -2651,6 +2908,7 @@
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
         layers[i]-&gt;setLearningRate( lr );
+        reconstruction_layers[i]-&gt;setLearningRate( lr );
         connections[i]-&gt;setLearningRate( lr );
         reconstruction_connections[i]-&gt;setLearningRate( lr );
         if(correlation_layers.length() != 0)
@@ -2668,22 +2926,22 @@
             expectations_m[ i ],
             reconstruction_activations_m);
 
-        layers[ i-1 ]-&gt;activations.resize(mbatch_size, layers[i-1]-&gt;size);
-        layers[ i-1 ]-&gt;activations &lt;&lt; reconstruction_activations_m;
-        layers[ i-1 ]-&gt;activations += layers[ i-1 ]-&gt;bias;
+        reconstruction_layers[ i-1 ]-&gt;activations.resize(mbatch_size,reconstruction_layers[i-1]-&gt;size);
+        reconstruction_layers[ i-1 ]-&gt;activations &lt;&lt; reconstruction_activations_m;
+        reconstruction_layers[ i-1 ]-&gt;activations += reconstruction_layers[ i-1 ]-&gt;bias;
 
-        Mat layer_exp = layers[i-1]-&gt;getExpectations();
-        layers[ i-1 ]-&gt;fprop(reconstruction_activations_m,
+        Mat layer_exp = reconstruction_layers[i-1]-&gt;getExpectations();
+        reconstruction_layers[ i-1 ]-&gt;fprop(reconstruction_activations_m,
                              layer_exp);
-        layers[ i-1 ]-&gt;setExpectationsByRef(layer_exp);
+        reconstruction_layers[ i-1 ]-&gt;setExpectationsByRef(layer_exp);
 
-        layers[ i-1 ]-&gt;fpropNLL(expectations_m[i-1],
+        reconstruction_layers[ i-1 ]-&gt;fpropNLL(expectations_m[i-1],
                                 train_costs.column(i-1));
 
-        layers[ i-1 ]-&gt;bpropNLL(expectations_m[i-1], train_costs.column(i-1),
+        reconstruction_layers[ i-1 ]-&gt;bpropNLL(expectations_m[i-1], train_costs.column(i-1),
                                 reconstruction_activation_gradients_m);
 
-        layers[ i-1 ]-&gt;update(reconstruction_activation_gradients_m);
+        reconstruction_layers[ i-1 ]-&gt;update(reconstruction_activation_gradients_m);
 
         reconstruction_connections[ i-1 ]-&gt;bpropUpdate(
             expectations_m[ i ],
@@ -2892,26 +3150,33 @@
 void StackedAutoassociatorsNet::computeOutputs(const Mat&amp; input, Mat&amp; output) const
 {
     if(correlation_connections.length() != 0
-       || currently_trained_layer!=n_layers
        || compute_all_test_costs
        || noise_type == &quot;missing_data&quot;){
         inherited::computeOutputs(input, output);
     }else{
         Profiler::pl_profile_start(&quot;StackedAutoassociatorsNet::computeOutputs&quot;);
-        PLCHECK(correlation_connections.length() == 0);
-        PLCHECK(currently_trained_layer == n_layers);
-        PLCHECK(!compute_all_test_costs);
 
         expectations_m[0].resize(input.length(), inputsize());
         Mat m = expectations_m[0];
         m&lt;&lt;input;
+
         for(int i=0 ; i&lt;currently_trained_layer-1 ; i++ )
         {
             connections[i]-&gt;fprop( expectations_m[i], activations_m[i+1] );
             layers[i+1]-&gt;fprop(activations_m[i+1],expectations_m[i+1]);
         }
-        final_module-&gt;fprop( expectations_m[ currently_trained_layer - 1],
+        if(currently_trained_layer &lt; n_layers)
+        {
+            connections[currently_trained_layer-1]-&gt;fprop( expectations_m[currently_trained_layer-1],
+                 activations_m[currently_trained_layer] );
+            layers[currently_trained_layer]-&gt;fprop(activations_m[currently_trained_layer],
+                 output);
+        }
+        else
+        {
+            final_module-&gt;fprop( expectations_m[ currently_trained_layer - 1],
                              output );
+        }
         Profiler::pl_profile_end(&quot;StackedAutoassociatorsNet::computeOutputs&quot;);
     }
 }
@@ -2920,15 +3185,11 @@
                                                        Mat&amp; output, Mat&amp; costs) const
 {
     if(correlation_connections.length() != 0 
-       || currently_trained_layer!=n_layers
        || compute_all_test_costs
        || noise_type == &quot;missing_data&quot;){
         inherited::computeOutputsAndCosts(input, target, output, costs);
     }else{
         Profiler::pl_profile_start(&quot;StackedAutoassociatorsNet::computeOutputsAndCosts&quot;);
-        PLCHECK(correlation_connections.length() == 0);
-        PLCHECK(currently_trained_layer == n_layers);
-        PLCHECK(!compute_all_test_costs);
 
         int n=input.length();
         PLASSERT(target.length()==n);
@@ -2970,15 +3231,15 @@
                 reconstruction_activations += direct_activations;
             }
 
-            layers[ i ]-&gt;fprop( reconstruction_activations,
-                                layers[ i ]-&gt;expectation);
+            reconstruction_layers[ i ]-&gt;fprop( reconstruction_activations,
+                                reconstruction_layers[ i ]-&gt;expectation);
 
-            layers[ i ]-&gt;activation &lt;&lt; reconstruction_activations;
-            layers[ i ]-&gt;activation += layers[ i ]-&gt;bias;
-            //layers[ i ]-&gt;expectation_is_up_to_date = true;
-            layers[ i ]-&gt;setExpectationByRef( layers[ i ]-&gt;expectation );
+            reconstruction_layers[ i ]-&gt;activation &lt;&lt; reconstruction_activations;
+            reconstruction_layers[ i ]-&gt;activation += reconstruction_layers[ i ]-&gt;bias;
+            //reconstruction_layers[ i ]-&gt;expectation_is_up_to_date = true;
+            reconstruction_layers[ i ]-&gt;setExpectationByRef( reconstruction_layers[ i ]-&gt;expectation );
 
-            costs[i] = layers[ i ]-&gt;fpropNLL(expectations[ i ]);
+            costs[i] = reconstruction_layers[ i ]-&gt;fpropNLL(expectations[ i ]);
 
             if( partial_costs &amp;&amp; partial_costs[i])
             {
@@ -3007,27 +3268,26 @@
         Vec divided_reconstruction_activations(reconstruction_activations.size());
         divide_input(reconstruction_activations, divided_reconstruction_activations);
 
-        layers[ currently_trained_layer-1 ]-&gt;fprop(
+        reconstruction_layers[ currently_trained_layer-1 ]-&gt;fprop(
           divided_reconstruction_activations,
-          layers[ currently_trained_layer-1 ]-&gt;expectation);
+          reconstruction_layers[ currently_trained_layer-1 ]-&gt;expectation);
         
-        layers[ currently_trained_layer-1 ]-&gt;activation &lt;&lt;
+        reconstruction_layers[ currently_trained_layer-1 ]-&gt;activation &lt;&lt;
             divided_reconstruction_activations;
+        reconstruction_layers[ currently_trained_layer-1 ]-&gt;activation += 
+            reconstruction_layers[ currently_trained_layer-1 ]-&gt;bias;
+        //reconstruction_layers[ currently_trained_layer-1 ]-&gt;expectation_is_up_to_date = true;
+        reconstruction_layers[ currently_trained_layer-1 ]-&gt;setExpectationByRef(
+            reconstruction_layers[ currently_trained_layer-1 ]-&gt;expectation );
 
-        layers[ currently_trained_layer-1 ]-&gt;activation += 
-            layers[ currently_trained_layer-1 ]-&gt;bias;
-        //layers[ currently_trained_layer-1 ]-&gt;expectation_is_up_to_date = true;
-        layers[ currently_trained_layer-1 ]-&gt;setExpectationByRef(
-            layers[ currently_trained_layer-1 ]-&gt;expectation );
-
         costs[ currently_trained_layer-1 ] =
-            layers[ currently_trained_layer-1 ]-&gt;fpropNLL(
+            reconstruction_layers[ currently_trained_layer-1 ]-&gt;fpropNLL(
                 expectations[ currently_trained_layer-1 ]);
 
         if(reconstruct_hidden)
         {
             connections[ currently_trained_layer-1 ]-&gt;fprop(
-                layers[ currently_trained_layer-1 ]-&gt;expectation,
+                reconstruction_layers[ currently_trained_layer-1 ]-&gt;expectation,
                 hidden_reconstruction_activations );
             layers[ currently_trained_layer ]-&gt;fprop(
                 hidden_reconstruction_activations,
@@ -3117,6 +3377,7 @@
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
         layers[i]-&gt;setLearningRate( the_learning_rate );
+        reconstruction_layers[i]-&gt;setLearningRate( the_learning_rate );
         connections[i]-&gt;setLearningRate( the_learning_rate );
         if(correlation_layers.length() != 0)
         {
@@ -3139,8 +3400,24 @@
     final_module-&gt;setLearningRate( the_learning_rate );
 }
 
-Vec StackedAutoassociatorsNet::fantasizeKTime(int KTime, const Vec&amp; srcImg, const Vec&amp; sample, const Vec&amp; maskNoiseFractOrProb)
+TVec&lt;Vec&gt; StackedAutoassociatorsNet::fantasizeKTimeOnMultiSrcImg(const int KTime, const Mat&amp; srcImg, const Vec&amp; sample, const Vec&amp; maskNoiseFractOrProb, bool alwaysFromSrcImg)
 {
+    int n=srcImg.length();
+    TVec&lt;Vec&gt; output(0);
+
+    for( int i=0; i&lt;n; i++ )
+    {
+        const Vec img_i = srcImg(i);
+        TVec&lt;Vec&gt; outputTmp;  
+        outputTmp = fantasizeKTime(KTime, img_i, sample, maskNoiseFractOrProb, alwaysFromSrcImg);
+        output = concat(output, outputTmp);        
+    }
+
+    return output;
+}
+
+TVec&lt;Vec&gt; StackedAutoassociatorsNet::fantasizeKTime(const int KTime, const Vec&amp; srcImg, const Vec&amp; sample, const Vec&amp; maskNoiseFractOrProb, bool alwaysFromSrcImg)
+{
     bool bFractOrProbUseful=false;
 
     // Noise type that needs fraction_of_masked_inputs or prob_masked_inputs
@@ -3179,7 +3456,7 @@
 
     if(bFractOrProbUseful)
     {
-        if(old_prob_masked_inputs != 0)
+        if(old_prob_masked_inputs &gt; 0.)
             bFraction_masked_input = false;
         else
             if(autoassociator_expectation_indices.size() == 0)
@@ -3188,12 +3465,17 @@
                 autoassociator_expectation_indices_temp_initialized = true;
             }
     }
+    
+    TVec&lt;Vec&gt; fantaImagesObtained(KTime+1);
 
+    fantaImagesObtained[0].resize(srcImg.size());
+    fantaImagesObtained[0] &lt;&lt; srcImg;
     expectations[0] &lt;&lt; srcImg;
-
+    
     // Do fantasize k time.
     for( int k=0 ; k&lt;KTime ; k++ )
     {
+        fantaImagesObtained[k+1].resize(srcImg.size());
         for( int i=0 ; i&lt;n_hlayers_used; i++ )
         {
             // Initialisation made only at the first loop.
@@ -3229,7 +3511,7 @@
         for( int i=n_hlayers_used-1 ; i&gt;=0; i-- )
         {
             // Binomial sample
-            if(sample[i])
+            if( sample[i] == 1 )
                 for( int j=0; j&lt;expectations[i+1].size(); j++ )
                     expectations[i+1][j] =
                     random_gen-&gt;binomial_sample(expectations[i+1][j]);
@@ -3241,8 +3523,11 @@
             Vec divided_reconstruction_activations(reconstruction_activations.size());
             divide_input(reconstruction_activations, divided_reconstruction_activations);
 
-            layers[i]-&gt;fprop(divided_reconstruction_activations, expectations[i]);
+            reconstruction_layers[i]-&gt;fprop(divided_reconstruction_activations, expectations[i]);
         }
+        fantaImagesObtained[k+1] &lt;&lt; expectations[0];
+        if( alwaysFromSrcImg )
+            expectations[0] &lt;&lt; srcImg;
     }
 
     if(bFractOrProbUseful)
@@ -3254,10 +3539,9 @@
     mask_input_layer_only = old_mask_input_layer_only;
     nb_corrupted_layer = old_nb_corrupted_layer;
 
-    return expectations[0];
+    return fantaImagesObtained;
 }
 
-
 } // end of namespace PLearn
 
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	
	<LI>Next message: <A HREF="003755.html">[Plearn-commits] r10315 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3754">[ date ]</a>
              <a href="thread.html#3754">[ thread ]</a>
              <a href="subject.html#3754">[ subject ]</a>
              <a href="author.html#3754">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
