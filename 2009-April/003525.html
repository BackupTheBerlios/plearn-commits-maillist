<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r10085 - trunk/plearn_learners/regressors
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2009-April/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r10085%20-%20trunk/plearn_learners/regressors&In-Reply-To=%3C200904042034.n34KYvvg017813%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003524.html">
   <LINK REL="Next"  HREF="003526.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r10085 - trunk/plearn_learners/regressors</H1>
    <B>chapados at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r10085%20-%20trunk/plearn_learners/regressors&In-Reply-To=%3C200904042034.n34KYvvg017813%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r10085 - trunk/plearn_learners/regressors">chapados at mail.berlios.de
       </A><BR>
    <I>Sat Apr  4 22:34:57 CEST 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="003524.html">[Plearn-commits] r10084 - trunk/plearn/ker
</A></li>
        <LI>Next message: <A HREF="003526.html">[Plearn-commits] r10086 - trunk/plearn/var
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3525">[ date ]</a>
              <a href="thread.html#3525">[ thread ]</a>
              <a href="subject.html#3525">[ subject ]</a>
              <a href="author.html#3525">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: chapados
Date: 2009-04-04 22:34:56 +0200 (Sat, 04 Apr 2009)
New Revision: 10085

Modified:
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.h
Log:
Implemented the Projected-Process sparse approximation in GaussianProcess -- this allows much larger training sets to be used

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-04-04 20:34:02 UTC (rev 10084)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-04-04 20:34:56 UTC (rev 10085)
@@ -2,7 +2,7 @@
 
 // GaussianProcessRegressor.cc
 //
-// Copyright (C) 2006-2007 Nicolas Chapados 
+// Copyright (C) 2006-2009 Nicolas Chapados 
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -52,12 +52,16 @@
 #include &lt;plearn/opt/Optimizer.h&gt;
 #include &lt;plearn/io/pl_log.h&gt;
 
+#ifdef USE_BLAS_SPECIALISATIONS
+#include &lt;plearn/math/plapack.h&gt;
+#endif
+
 namespace PLearn {
 using namespace std;
 
 PLEARN_IMPLEMENT_OBJECT(
     GaussianProcessRegressor,
-    &quot;Kernelized version of linear ridge regression.&quot;,
+    &quot;Implements Gaussian Process Regression (GPR) with an arbitrary kernel&quot;,
     &quot;Given a kernel K(x,y) = phi(x)'phi(y), where phi(x) is the projection of a\n&quot;
     &quot;vector x into feature space, this class implements a version of the ridge\n&quot;
     &quot;estimator, giving the prediction at x as\n&quot;
@@ -100,6 +104,18 @@
     &quot;number of training examples (due to the matrix inversion).  When saving the\n&quot;
     &quot;learner, the training set inputs must be saved, along with an additional\n&quot;
     &quot;matrix of length number-of-training-examples, and width number-of-targets.\n&quot;
+    &quot;\n&quot;
+    &quot;To alleviate the computational bottleneck of the exact method, the sparse\n&quot;
+    &quot;approximation method of Projected Process is also available.  This method\n&quot;
+    &quot;requires identifying M datapoints in the training set called the active\n&quot;
+    &quot;set, although it makes use of all N training points for computing the\n&quot;
+    &quot;likelihood.  The computational complexity of the approach is then O(NM^2).\n&quot;
+    &quot;Note that in the current implementation, hyperparameter optimization is\n&quot;
+    &quot;performed using ONLY the active set (called the \&quot;Subset of Data\&quot; method in\n&quot;
+    &quot;the Rasmussen &amp; Williams book).  Making use of the full set of datapoints\n&quot;
+    &quot;is more computationally expensive and would require substantial updates to\n&quot;
+    &quot;the PLearn Kernel class (to efficiently support asymmetric kernel-matrix\n&quot;
+    &quot;gradient).  This may come later.\n&quot;
     );
 
 GaussianProcessRegressor::GaussianProcessRegressor() 
@@ -107,7 +123,8 @@
       m_include_bias(true),
       m_compute_confidence(false),
       m_confidence_epsilon(1e-8),
-      m_save_gram_matrix(false)
+      m_save_gram_matrix(false),
+      m_solution_algorithm(&quot;exact&quot;)
 { }
 
 
@@ -189,14 +206,39 @@
         &quot;It is saved in the current expdir under the names 'gram_matrix_N.pmat'\n&quot;
         &quot;where N is an increasing counter.\n&quot;);
 
+    declareOption(
+        ol, &quot;solution_algorithm&quot;, &amp;GaussianProcessRegressor::m_solution_algorithm,
+        OptionBase::buildoption,
+        &quot;Solution algorithm used for the regression.  If \&quot;exact\&quot;, use the exact\n&quot;
+        &quot;Gaussian process solution (requires O(N^3) computation).  If\n&quot;
+        &quot;\&quot;projected-process\&quot;, use the PP approximation, which requires O(MN^2)\n&quot;
+        &quot;computation, where M is given by the size of the active training\n&quot;
+        &quot;examples specified by the \&quot;active-set\&quot; option.  Default=\&quot;exact\&quot;.\n&quot;);
 
+    declareOption(
+        ol, &quot;active_set_indices&quot;, &amp;GaussianProcessRegressor::m_active_set_indices,
+        OptionBase::buildoption,
+        &quot;If a sparse approximation algorithm is used (e.g. projected process),\n&quot;
+        &quot;this specifies the indices of the training-set examples which should be\n&quot;
+        &quot;considered to be part of the active set.  Note that these indices must\n&quot;
+        &quot;be SORTED IN INCREASING ORDER and should not contain duplicates.\n&quot;);
+
+    
     //#####  Learnt Options  ##################################################
 
     declareOption(
         ol, &quot;alpha&quot;, &amp;GaussianProcessRegressor::m_alpha,
         OptionBase::learntoption,
-        &quot;Vector of learned parameters, determined from the equation\n&quot;
-        &quot;    (M + lambda I)^-1 y&quot;);
+        &quot;Matrix of learned parameters, determined from the equation\n&quot;
+        &quot;\n&quot;
+        &quot;  (K + lambda I)^-1 y\n&quot;
+        &quot;\n&quot;
+        &quot;(don't forget that y can be a matrix for multivariate output problems)\n&quot;
+        &quot;\n&quot;
+        &quot;In the case of the projected-process approximation, this contains\n&quot;
+        &quot;the result of the equiation\n&quot;
+        &quot;\n&quot;
+        &quot;  (lambda K_mm + K_mn K_nm)^-1 K_mn y\n&quot;);
 
     declareOption(
         ol, &quot;gram_inverse&quot;, &amp;GaussianProcessRegressor::m_gram_inverse,
@@ -204,9 +246,18 @@
         &quot;Inverse of the Gram matrix, used to compute confidence intervals (must\n&quot;
         &quot;be saved since the confidence intervals are obtained from the equation\n&quot;
         &quot;\n&quot;
-        &quot;  sigma^2 = k(x,x) - k(x)'(M + lambda I)^-1 k(x)\n&quot;);
+        &quot;  sigma^2 = k(x,x) - k(x)'(K + lambda I)^-1 k(x)\n&quot;
+        &quot;\n&quot;
+        &quot;An adjustment similar to 'alpha' is made for the projected-process\n&quot;
+        &quot;approximation.\n&quot;);
 
     declareOption(
+        ol, &quot;subgram_inverse&quot;, &amp;GaussianProcessRegressor::m_subgram_inverse,
+        OptionBase::learntoption,
+        &quot;Inverse of the sub-Gram matrix, i.e. K_mm^-1.  Used only with the\n&quot;
+        &quot;projected-process approximation.\n&quot;);
+    
+    declareOption(
         ol, &quot;target_mean&quot;, &amp;GaussianProcessRegressor::m_target_mean,
         OptionBase::learntoption,
         &quot;Mean of the targets, if the option 'include_bias' is true&quot;);
@@ -215,7 +266,9 @@
         ol, &quot;training_inputs&quot;, &amp;GaussianProcessRegressor::m_training_inputs,
         OptionBase::learntoption,
         &quot;Saved version of the training set, which must be kept along for\n&quot;
-        &quot;carrying out kernel evaluations with the test point&quot;);
+        &quot;carrying out kernel evaluations with the test point.  If using the\n&quot;
+        &quot;projected-process approximation, only the inputs in the active set are\n&quot;
+        &quot;saved.&quot;);
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -244,6 +297,16 @@
 
     if (m_confidence_epsilon &lt; 0)
         PLERROR(&quot;GaussianProcessRegressor::build_: 'confidence_epsilon' must be non-negative&quot;);
+
+    // Cache solution algorithm in quick form
+    if (m_solution_algorithm == &quot;exact&quot;)
+        m_algorithm_enum = AlgoExact;
+    else if (m_solution_algorithm == &quot;projected-process&quot;)
+        m_algorithm_enum = AlgoProjectedProcess;
+    else
+        PLERROR(&quot;GaussianProcessRegressor::build_: the option solution_algorithm=='%s' &quot;
+                &quot;is not supported.  Value must be in {'exact', 'projected-process'}&quot;,
+                m_solution_algorithm.c_str());
 }
 
 // ### Nothing to add here, simply calls build_
@@ -261,8 +324,10 @@
     deepCopyField(m_kernel,                     copies);
     deepCopyField(m_hyperparameters,            copies);
     deepCopyField(m_optimizer,                  copies);
+    deepCopyField(m_active_set_indices,         copies);
     deepCopyField(m_alpha,                      copies);
     deepCopyField(m_gram_inverse,               copies);
+    deepCopyField(m_subgram_inverse,            copies);
     deepCopyField(m_target_mean,                copies);
     deepCopyField(m_training_inputs,            copies);
     deepCopyField(m_kernel_evaluations,         copies);
@@ -322,9 +387,29 @@
     if (!initTrain())
         return;
 
+    // If we use the projected process approximation, make sure that the
+    // active-set indices are specified and that they are sorted in increasing
+    // order
+    if (m_algorithm_enum == AlgoProjectedProcess) {
+        if (m_active_set_indices.size() == 0)
+            PLERROR(&quot;GaussianProcessRegressor::train: with the projected-process &quot;
+                    &quot;approximation, the active_set_indices option must be specified.&quot;);
+        int last_index = -1;
+        for (int i=0, n=m_active_set_indices.size() ; i&lt;n ; ++i) {
+            int cur_index = m_active_set_indices[i];
+            if (cur_index &lt;= last_index)
+                PLERROR(&quot;GaussianProcessRegressor::train: the option active_set_indices &quot;
+                        &quot;must be sorted and should not contain duplicates; at index %d, &quot;
+                        &quot;encounted value %d whereas previous value was %d.&quot;,
+                        i, cur_index, last_index);
+            last_index = cur_index;
+        }
+    }
+    
     PLASSERT( m_kernel );
     if (! train_set || ! m_training_inputs)
         PLERROR(&quot;GaussianProcessRegressor::train: the training set must be specified&quot;);
+    int activelength= m_active_set_indices.size();
     int trainlength = train_set-&gt;length();
     int inputsize   = train_set-&gt;inputsize() ;
     int targetsize  = train_set-&gt;targetsize();
@@ -344,10 +429,25 @@
         targets -= m_target_mean;
     }
 
+    // Determine the subset of training inputs and targets to use depending on
+    // the training algorithm
+    Mat sub_training_inputs;
+    Mat sub_training_targets;
+    if (m_algorithm_enum == AlgoExact) {
+        sub_training_inputs = m_training_inputs;
+        sub_training_targets= targets;
+    }
+    else if (m_algorithm_enum == AlgoProjectedProcess) {
+        sub_training_inputs .resize(activelength, inputsize);
+        sub_training_targets.resize(activelength, targetsize);
+        selectRows(m_training_inputs, m_active_set_indices, sub_training_inputs);
+        selectRows(targets,           m_active_set_indices, sub_training_targets);
+    }
+    
     // Optimize hyperparameters
     VarArray hyperparam_vars;
-    PP&lt;GaussianProcessNLLVariable&gt; nll = hyperOptimize(m_training_inputs, targets,
-                                                       hyperparam_vars);
+    PP&lt;GaussianProcessNLLVariable&gt; nll =
+        hyperOptimize(sub_training_inputs, sub_training_targets, hyperparam_vars);
     PLASSERT( nll );
     
     // Compute parameters.  Be careful to also propagate through the
@@ -355,23 +455,34 @@
     // into their respective kernels.
     hyperparam_vars.fprop();
     nll-&gt;fprop();
-    m_alpha = nll-&gt;alpha();
-    m_gram_inverse = nll-&gt;gramInverse();
+    if (m_algorithm_enum == AlgoExact) {
+        m_alpha = nll-&gt;alpha();
+        m_gram_inverse = nll-&gt;gramInverse();
+    }
+    else if (m_algorithm_enum == AlgoProjectedProcess) {
+        trainProjectedProcess(m_training_inputs, sub_training_inputs, targets);
 
-    // Compute train MSE, as 1/(2N) * dot(z,z), with z=K*alpha - y
-    // *** FIXME: MSE IS NOT COMPUTED CORRECTLY SINCE GRAM MATRIX IS
-    // OVERWRITTEN BY CHOLESKY DECOMPOSITION ***
-    Mat residuals(m_alpha.length(), m_alpha.width());
-    product(residuals, nll-&gt;gram(), m_alpha);
-    residuals -= targets;
-    real mse = dot(residuals, residuals) / (2 * trainlength);
+        // Full training set no longer required from now on
+        m_training_inputs = sub_training_inputs;
+        m_kernel-&gt;setDataForKernelMatrix(m_training_inputs);
+    }
+
+    if (getTrainStatsCollector()) {
+        // Compute train statistics by running a test over the training set.
+        // This works uniformly for all solution algorithms, albeit with some
+        // performance hit.
+        PP&lt;VecStatsCollector&gt; test_stats = new VecStatsCollector;
+        test(getTrainingSet(), test_stats);
     
-    // And accumulate some statistics
-    Vec costs(2);
-    costs[0] = nll-&gt;value[0];
-    costs[1] = mse;
-    getTrainStatsCollector()-&gt;update(costs);
-    MODULE_LOG &lt;&lt; &quot;Train NLL: &quot; &lt;&lt; costs[0] &lt;&lt; endl;
+        // And accumulate some statistics.  Note: the NLL corresponds to the
+        // subset-of-data version if the projected-process approximation is
+        // used.  It is the exact NLL if the exact algorithm is used.
+        Vec costs(3);
+        costs.subVec(0,2) &lt;&lt; test_stats-&gt;getMean();
+        costs[2] = nll-&gt;value[0];
+        getTrainStatsCollector()-&gt;update(costs);
+    }
+    MODULE_LOG &lt;&lt; &quot;Train marginal NLL (subset-of-data): &quot; &lt;&lt; nll-&gt;value[0] &lt;&lt; endl;
 }
 
 
@@ -400,7 +511,9 @@
     
     m_kernel-&gt;evaluate_all_i_x(input, kernel_evaluations);
 
-    // Finally compute k(x,x_i) * (M + \lambda I)^-1 y
+    // Finally compute k(x,x_i) * (K + \lambda I)^-1 y.
+    // This expression does not change depending on whether we are using
+    // the exact algorithm or the projected-process approximation.
     product(Mat(1, output.size(), output),
             Mat(1, kernel_evaluations.size(), kernel_evaluations),
             m_alpha);
@@ -461,15 +574,29 @@
         return false;
     }
 
-    // BIG-BIG assumption: assume that computeOutput has just been called and
-    // that m_kernel_evaluations contains the right stuff.
+    // BIG assumption: assume that computeOutput has just been called and that
+    // m_kernel_evaluations contains the right stuff.
     PLASSERT( m_kernel &amp;&amp; m_gram_inverse.isNotNull() );
     real base_sigma_sq = m_kernel(input, input);
     m_gram_inverse_product.resize(m_kernel_evaluations.size());
-    product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
-    real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
-    real sigma = sqrt(max(real(0.), base_sigma_sq - sigma_reductor + m_confidence_epsilon));
 
+    real sigma;
+    if (m_algorithm_enum == AlgoExact) {
+        product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
+        real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
+        sigma = sqrt(max(real(0.),
+                         base_sigma_sq - sigma_reductor + m_confidence_epsilon));
+    }
+    else if (m_algorithm_enum == AlgoProjectedProcess) {
+        // From R&amp;W eq. (8.27).
+        product(m_gram_inverse_product, m_subgram_inverse, m_kernel_evaluations);
+        productScaleAcc(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations,
+                        -1.0, 1.0);
+        real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
+        sigma = sqrt(max(real(0.),
+                         base_sigma_sq - sigma_reductor + m_confidence_epsilon));
+    }
+
     // two-tailed
     const real multiplier = gauss_01_quantile((1+probability)/2);
     real half_width = multiplier * sigma;
@@ -516,7 +643,7 @@
         has_missings = has_missings || inputs(i).hasMissing();
     }
 
-    // If any missings found in the inputs, don't bother with with computing a
+    // If any missings found in the inputs, don't bother with computing a
     // covariance matrix
     if (has_missings) {
         covmat.fill(MISSING_VALUE);
@@ -527,34 +654,42 @@
     // less lifted from Kernel.cc ==&gt; must see with Olivier how to better
     // factor this code
     Mat&amp; K = covmat;
-    PLASSERT( K.width() == N &amp;&amp; K.length() == N );
-    const int mod = K.mod();
-    real Kij;
-    real* Ki;
-    real* Kji;
-    for (int i=0 ; i&lt;N ; ++i) {
-        Ki  = K[i];
-        Kji = &amp;K[0][i];
-        const Vec&amp; cur_input_i = inputs(i);
-        for (int j=0 ; j&lt;=i ; ++j, Kji += mod) {
-            Kij = m_kernel-&gt;evaluate(cur_input_i, inputs(j));
-            *Ki++ = Kij;
-            if (j&lt;i)
-                *Kji = Kij;    // Assume symmetry, checked at build
-        }
-    }
-
-    // The predictive covariance matrix is (c.f. Rasmussen and Williams):
+    Vec self_cov(N);
+    m_kernel-&gt;computeTestGramMatrix(inputs, K, self_cov);
+    
+    // The predictive covariance matrix is for the exact cast(c.f. Rasmussen
+    // and Williams):
     //
     //    cov(f*) = K(X*,X*) - K(X*,X) [K(X,X) + sigma*I]^-1 K(X,X*)
     //
     // where X are the training inputs, and X* are the test inputs.
+    //
+    // For the projected process case, it is:
+    //
+    //    cov(f*) = K(X*,X*) - K(X*,X_m) K_mm^-1 K(X*,X_m)
+    //               + sigma^2 K(X*,X_m) (sigma^2 K_mm + K_mn K_nm)^-1 K(X*,X_m)
+    //
+    // Note that all sigma^2's have been absorbed into their respective
+    // cached terms, and in particular in this context sigma^2 is emphatically
+    // not equal to the weight decay.
     m_gram_inv_traintest_product.resize(T,N);
     m_sigma_reductor.resize(N,N);
-    productTranspose(m_gram_inv_traintest_product, m_gram_inverse,
-                     m_gram_traintest_inputs);
-    product(m_sigma_reductor, m_gram_traintest_inputs,
-            m_gram_inv_traintest_product);
+
+    if (m_algorithm_enum == AlgoExact) {    
+        productTranspose(m_gram_inv_traintest_product, m_gram_inverse,
+                         m_gram_traintest_inputs);
+        product(m_sigma_reductor, m_gram_traintest_inputs,
+                m_gram_inv_traintest_product);
+    }
+    else if (m_algorithm_enum == AlgoProjectedProcess) {
+        productTranspose(m_gram_inv_traintest_product, m_subgram_inverse,
+                         m_gram_traintest_inputs);
+        productTransposeScaleAcc(m_gram_inv_traintest_product, m_gram_inverse,
+                                 m_gram_traintest_inputs, -1.0, 1.0);
+        product(m_sigma_reductor, m_gram_traintest_inputs,
+                m_gram_inv_traintest_product);
+    }
+    
     covmat -= m_sigma_reductor;
 
     // As a preventive measure, never output negative variance, even though
@@ -577,9 +712,10 @@
 
 TVec&lt;string&gt; GaussianProcessRegressor::getTrainCostNames() const
 {
-    TVec&lt;string&gt; c(2);
-    c[0] = &quot;nmll&quot;;
+    TVec&lt;string&gt; c(3);
+    c[0] = &quot;nll&quot;;
     c[1] = &quot;mse&quot;;
+    c[2] = &quot;marginal-nll&quot;;
     return c;
 }
 
@@ -669,6 +805,106 @@
     return nll;
 }
 
+
+//#####  trainProjectedProcess (LAPACK)  ######################################
+
+void GaussianProcessRegressor::trainProjectedProcess(
+    const Mat&amp; all_training_inputs, const Mat&amp; sub_training_inputs,
+    const Mat&amp; all_training_targets)
+{
+    PLASSERT( m_kernel );
+    const int activelength= m_active_set_indices.length();
+    const int trainlength = all_training_inputs.length();
+    const int targetsize  = all_training_targets.width();
+    
+    // The RHS matrix (when solving the linear system Gram*Params=RHS) is made
+    // up of two parts: the regression targets themselves, and the identity
+    // matrix if we requested them (for confidence intervals).  After solving
+    // the linear system, set the gram-inverse appropriately.  To interface
+    // nicely with LAPACK, we store this in a transposed format.
+    int rhs_width = targetsize + (m_compute_confidence? activelength : 0);
+    Mat tmp_rhs(rhs_width, activelength);
+    if (m_compute_confidence) {
+        Mat rhs_identity = tmp_rhs.subMatRows(targetsize, activelength);
+        identityMatrix(rhs_identity);
+    }
+
+    // We always need to solve K_mm^-1.  Prepare the RHS with the identity
+    // matrix to be ready to solve with a Cholesky decomposition.
+    m_subgram_inverse.resize(activelength, activelength);
+    Mat gram_cholesky(activelength, activelength);
+    identityMatrix(m_subgram_inverse);
+    
+    // Compute Gram Matrix and add weight decay to diagonal.  This is done in a
+    // few steps: (1) K_mm (using the active-set only), (2) then separately
+    // compute K_mn (active-set by all examples), (3) computing the covariance
+    // matrix of K_mn to give an m x m matrix, (4) and finally add them up.
+    // cf. R&amp;W p. 179, eq. 8.26 :: (sigma_n^2 K_mm + K_mn K_nm)
+    m_kernel-&gt;setDataForKernelMatrix(all_training_inputs);
+    Mat gram(activelength, activelength);
+    Mat asym_gram(activelength, trainlength);
+    Vec self_cov(activelength);
+    m_kernel-&gt;computeTestGramMatrix(sub_training_inputs, asym_gram, self_cov);
+    // Note: asym_gram contains K_mn without any sampling noise.
+
+    // DBG_MODULE_LOG &lt;&lt; &quot;Asym_gram =\n&quot; &lt;&lt; asym_gram &lt;&lt; endl;
+    
+    // Obtain K_mm, also without self-noise.  Add some jitter as per
+    // the Rasmussen &amp; Williams code
+    selectColumns(asym_gram, m_active_set_indices, gram);
+    real jitter = m_weight_decay * trace(gram);
+    addToDiagonal(gram, jitter);
+
+    // DBG_MODULE_LOG &lt;&lt; &quot;Kmm =\n&quot; &lt;&lt; gram &lt;&lt; endl;
+    
+    // Obtain an estimate of the EFFECTIVE sampling noise from the
+    // difference between self_cov and the diagonal of gram
+    Vec sigma_sq = self_cov - diag(gram);
+    double sigma_sq_est = mean(sigma_sq);
+    // DBG_MODULE_LOG &lt;&lt; &quot;Sigma^2 estimate = &quot; &lt;&lt; sigma_sq_est &lt;&lt; endl;
+
+    // Before clobbering K_mm, compute its inverse.
+    gram_cholesky &lt;&lt; gram;
+    lapackCholeskyDecompositionInPlace(gram_cholesky);
+    lapackCholeskySolveInPlace(gram_cholesky, m_subgram_inverse,
+                               true /* column-major */);
+    
+    gram *= sigma_sq_est;                            // sigma_n^2 K_mm
+    productTransposeAcc(gram, asym_gram, asym_gram); // Inner part of eq. 8.26
+
+    // DBG_MODULE_LOG &lt;&lt; &quot;Gram =\n&quot; &lt;&lt; gram &lt;&lt; endl;
+    
+    // Dump a fragment of the Gram Matrix to the debug log
+    DBG_MODULE_LOG &lt;&lt; &quot;Projected-process Gram fragment: &quot;
+                   &lt;&lt; gram(0,0) &lt;&lt; ' '
+                   &lt;&lt; gram(1,0) &lt;&lt; ' '
+                   &lt;&lt; gram(1,1) &lt;&lt; endl;
+
+    // The RHS should contain (K_mn*y)' = y'*K_mn'.  Compute it.
+    Mat targets_submat = tmp_rhs.subMatRows(0, targetsize);
+    transposeTransposeProduct(targets_submat, all_training_targets, asym_gram);
+    // DBG_MODULE_LOG &lt;&lt; &quot;Projected RHS =\n&quot; &lt;&lt; targets_submat &lt;&lt; endl;
+    
+    // Compute Cholesky decomposition and solve the linear system.  LAPACK
+    // solves in-place, but luckily we don't need either the Gram and RHS
+    // matrices after solving.
+    lapackCholeskyDecompositionInPlace(gram);
+    lapackCholeskySolveInPlace(gram, tmp_rhs, true /* column-major */);
+
+    // Transpose final result.  LAPACK solved in-place for tmp_rhs.
+    m_alpha.resize(tmp_rhs.width(), tmp_rhs.length());
+    transpose(tmp_rhs, m_alpha);
+    if (m_compute_confidence) {
+        m_gram_inverse = m_alpha.subMatColumns(targetsize, activelength);
+        m_alpha        = m_alpha.subMatColumns(0, targetsize);
+
+        // Absorb sigma^2 into gram_inverse as per eq. 8.27 of R&amp;W
+        m_gram_inverse *= sigma_sq_est;
+    }
+}
+
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2009-04-04 20:34:02 UTC (rev 10084)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2009-04-04 20:34:56 UTC (rev 10085)
@@ -2,7 +2,7 @@
 
 // GaussianProcessRegressor.h
 //
-// Copyright (C) 2006 Nicolas Chapados 
+// Copyright (C) 2006--2009 Nicolas Chapados 
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -98,6 +98,18 @@
  *  number of training examples (due to the matrix inversion).  When saving the
  *  learner, the training set inputs must be saved, along with an additional
  *  matrix of length number-of-training-examples, and width number-of-targets.
+ *
+ *  To alleviate the computational bottleneck of the exact method, the sparse
+ *  approximation method of Projected Process is also available.  This method
+ *  requires identifying M datapoints in the training set called the active
+ *  set, although it makes use of all N training points for computing the
+ *  likelihood.  The computational complexity of the approach is then O(NM^2).
+ *  Note that in the current implementation, hyperparameter optimization is
+ *  performed using ONLY the active set (called the &quot;Subset of Data&quot; method in
+ *  the Rasmussen &amp; Williams book).  Making use of the full set of datapoints
+ *  is more computationally expensive and would require substantial updates to
+ *  the PLearn Kernel class (to efficiently support asymmetric kernel-matrix
+ *  gradient).  This may come later.
  */
 class GaussianProcessRegressor : public PLearner
 {
@@ -179,7 +191,24 @@
      */
     bool m_save_gram_matrix;
 
+    /**
+     *  Solution algorithm used for the regression.  If &quot;exact&quot;, use the exact
+     *  Gaussian process solution (requires O(N^3) computation).  If
+     *  &quot;projected-process&quot;, use the PP approximation, which requires O(MN^2)
+     *  computation, where M is given by the size of the active training
+     *  examples specified by the &quot;active-set&quot; option.  Default=&quot;exact&quot;.
+     */
+    string m_solution_algorithm;
 
+    /**
+     *  If a sparse approximation algorithm is used (e.g. projected process),
+     *  this specifies the indices of the training-set examples which should be
+     *  considered to be part of the active set.  Note that these indices must
+     *  be SORTED IN INCREASING ORDER and should not contain duplicates.
+     */
+    TVec&lt;int&gt; m_active_set_indices;
+    
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -262,15 +291,26 @@
     PP&lt;GaussianProcessNLLVariable&gt; hyperOptimize(
         const Mat&amp; inputs, const Mat&amp; targets, VarArray&amp; hyperparam_vars);
 
+    /// Update the parameters required for the Projected Process approximation,
+    /// assuming hyperparameters have already been optimized.
+    void trainProjectedProcess(const Mat&amp; all_training_inputs,
+                               const Mat&amp; sub_training_inputs,
+                               const Mat&amp; all_training_targets);
+    
 protected:
     //#####  Protected Options  ###############################################
 
     /**
      *  Matrix of learned parameters, determined from the equation
      *
-     *    (M + lambda I)^-1 y
+     *    (K + lambda I)^-1 y
      *
      *  (don't forget that y can be a matrix for multivariate output problems)
+     *
+     *  In the case of the projected-process approximation, this contains
+     *  the result of the equiation
+     *
+     *    (lambda K_mm + K_mn K_nm)^-1 K_mn y
      */
     Mat m_alpha;
 
@@ -278,15 +318,26 @@
      *  Inverse of the Gram matrix, used to compute confidence intervals (must
      *  be saved since the confidence intervals are obtained from the equation
      *
-     *    sigma^2 = k(x,x) - k(x)'(M + lambda I)^-1 k(x)
+     *    sigma^2 = k(x,x) - k(x)'(K + lambda I)^-1 k(x)
+     *
+     *  An adjustment similar to 'alpha' is made for the projected-process
+     *  approximation.
      */
     Mat m_gram_inverse;
 
+    /**
+     *  Inverse of the sub-Gram matrix, i.e. K_mm^-1.  Used only with the
+     *  projected-process approximation.
+     */
+    Mat m_subgram_inverse;
+    
     /// Mean of the targets, if the option 'include_bias' is true
     Vec m_target_mean;
     
     /// Saved version of the training set inputs, which must be kept along for
-    /// carrying out kernel evaluations with the test point
+    /// carrying out kernel evaluations with the test point.  If using the
+    /// projected-process approximation, only the inputs in the active set are
+    /// saved.
     Mat m_training_inputs;
 
     /// Buffer for kernel evaluations at test time
@@ -307,6 +358,13 @@
 
     //! Buffer to hold the sigma reductor for m_gram_inverse_product
     mutable Mat m_sigma_reductor;
+
+    //! Solution algorithm in enum form to avoid lengthy string-compare
+    //! each time we want to compute a confidence interval
+    enum {
+        AlgoExact,
+        AlgoProjectedProcess
+    } m_algorithm_enum;
     
 private: 
     /// This does the actual building. 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003524.html">[Plearn-commits] r10084 - trunk/plearn/ker
</A></li>
	<LI>Next message: <A HREF="003526.html">[Plearn-commits] r10086 - trunk/plearn/var
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3525">[ date ]</a>
              <a href="thread.html#3525">[ thread ]</a>
              <a href="subject.html#3525">[ subject ]</a>
              <a href="author.html#3525">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
