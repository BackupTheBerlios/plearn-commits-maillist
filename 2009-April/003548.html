<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r10108 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2009-April/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r10108%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200904100203.n3A231ul026284%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003547.html">
   <LINK REL="Next"  HREF="003549.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r10108 - trunk/plearn_learners_experimental</H1>
    <B>laulysta at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r10108%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200904100203.n3A231ul026284%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r10108 - trunk/plearn_learners_experimental">laulysta at mail.berlios.de
       </A><BR>
    <I>Fri Apr 10 04:03:01 CEST 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="003547.html">[Plearn-commits] r10107 - trunk/plearn_learners/regressors
</A></li>
        <LI>Next message: <A HREF="003549.html">[Plearn-commits] r10109 - in	trunk/plearn_learners/regressors/test/GaussianProcessRegressor: .	.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3548">[ date ]</a>
              <a href="thread.html#3548">[ thread ]</a>
              <a href="subject.html#3548">[ subject ]</a>
              <a href="author.html#3548">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: laulysta
Date: 2009-04-10 04:03:00 +0200 (Fri, 10 Apr 2009)
New Revision: 10108

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
auto encoder and expressive timing


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-04-09 17:26:43 UTC (rev 10107)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-04-10 02:03:00 UTC (rev 10108)
@@ -90,7 +90,9 @@
     current_learning_rate(0),
     nb_stage_reconstruction(0),
     nb_stage_target(0),
-    noise(false)
+    noise(false),
+    L1_penalty_factor(0),
+    L2_penalty_factor(0)
 {
     random_gen = new PRandom();
 }
@@ -270,6 +272,23 @@
                   OptionBase::learntoption,
                   &quot;The nomber of stage for de target&quot;);
 
+    declareOption(ol, &quot;L1_penalty_factor&quot;,
+                  &amp;DenoisingRecurrentNet::L1_penalty_factor,
+                  OptionBase::buildoption,
+                  &quot;Optional (default=0) factor of L1 regularization term, i.e.\n&quot;
+                  &quot;minimize L1_penalty_factor * sum_{ij} |weights(i,j)| &quot;
+                  &quot;during training.\n&quot;);
+
+    declareOption(ol, &quot;L2_penalty_factor&quot;,
+                  &amp;DenoisingRecurrentNet::L2_penalty_factor,
+                  OptionBase::buildoption,
+                  &quot;Optional (default=0) factor of L2 regularization term, i.e.\n&quot;
+                  &quot;minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 &quot;
+                  &quot;during training.\n&quot;);
+                  
+                  
+                  
+
  /*
     declareOption(ol, &quot;&quot;, &amp;DenoisingRecurrentNet::,
                   OptionBase::learntoption,
@@ -379,9 +398,9 @@
             }
         }
 
-        if( tar_layer != target_layers.length() )
-            PLERROR(&quot;DenoisingRecurrentNet::build_(): target layers &quot;
-                    &quot;does not cover all targets.&quot;);
+        //if( tar_layer != target_layers.length() )
+        //    PLERROR(&quot;DenoisingRecurrentNet::build_(): target layers &quot;
+        //            &quot;does not cover all targets.&quot;);
 
 
         // Building weights and layers
@@ -510,6 +529,7 @@
     deepCopyField( mean_encoded_vec, copies);
     deepCopyField( input_reconstruction_bias, copies);
     deepCopyField( hidden_reconstruction_bias, copies);
+    deepCopyField( hidden_reconstruction_bias2, copies);
 
     // Protected fields
     deepCopyField( data, copies);
@@ -620,7 +640,7 @@
         for(int i=0; i&lt;nseq; i++)
         {
             getSequence(i, seq);
-            encodeSequenceAndPopulateLists(seq);
+            encodeSequenceAndPopulateLists(seq, false);
             if(i==0)
             {
                 mean_encoded_vec.resize(encoded_seq.width());
@@ -723,25 +743,25 @@
 
                 
                 
-                noise = false;
+                
                 getSequence(i, seq);
-                encodeSequenceAndPopulateLists(seq);
+                encodeSequenceAndPopulateLists(seq, false);
+
+                
               
-                bool corrupt_input = false;//input_noise_prob!=0 &amp;&amp; (noisy_recurrent_lr!=0 || input_reconstruction_lr!=0);
+                //bool corrupt_input = false;//input_noise_prob!=0 &amp;&amp; (noisy_recurrent_lr!=0 || input_reconstruction_lr!=0);
 
-                clean_encoded_seq.resize(encoded_seq.length(), encoded_seq.width());
-                clean_encoded_seq &lt;&lt; encoded_seq;
+                //clean_encoded_seq.resize(encoded_seq.length(), encoded_seq.width());
+                //clean_encoded_seq &lt;&lt; encoded_seq;
 
-                if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
-                    inject_zero_forcing_noise(encoded_seq, input_noise_prob);
+                //if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
+                      //  inject_zero_forcing_noise(encoded_seq, input_noise_prob);
 
                 // recurrent no noise phase
                 if(stage&gt;=nb_stage_reconstruction){
                     if(recurrent_lr!=0)
                     {
                         
-                        if(noise) // need to recover the clean sequence                        
-                            encoded_seq &lt;&lt; clean_encoded_seq;                  
                         setLearningRate( recurrent_lr );                    
                         recurrentFprop(train_costs, train_n_items);
                         recurrentUpdate(0,0,1, prediction_cost_weight,1, train_costs, train_n_items );
@@ -751,20 +771,38 @@
 
                 if(stage&lt;nb_stage_reconstruction || nb_stage_reconstruction == 0 ){
 
+                    // greedy phase hidden
+                    if(hidden_reconstruction_lr!=0){
+                        setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
+                        recurrentFprop(train_costs, train_n_items, true);
+                        //recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
+                        recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
+                    }
 
+                    /*if(recurrent_lr!=0)
+                    {                 
+                        setLearningRate( recurrent_lr );                    
+                        recurrentFprop(train_costs, train_n_items);
+                        //recurrentUpdate(0,0,1, prediction_cost_weight,0, train_costs, train_n_items );
+                        recurrentUpdate(0,0,0, prediction_cost_weight,0, train_costs, train_n_items );
+                        
+                        }*/
+                    
                     // greedy phase input
                     if(input_reconstruction_lr!=0){
+                        if (noise)
+                            encodeSequenceAndPopulateLists(seq, true);
                         setLearningRate( input_reconstruction_lr );
-                        recurrentFprop(train_costs, train_n_items);
-                        recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0,1, train_costs, train_n_items );
+                        recurrentFprop(train_costs, train_n_items, false);
+                        if (noise)
+                            encodeSequenceAndPopulateLists(seq, false);
+                        //recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0,1, train_costs, train_n_items );
+                        recurrentUpdate(input_reconstruction_cost_weight, 0, 0, 0,1, train_costs, train_n_items );
                     }
                     
-                    // greedy phase hidden
-                    if(hidden_reconstruction_lr!=0){
-                        setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
-                        recurrentFprop(train_costs, train_n_items);
-                        recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
-                    }
+                    
+                    
+                    
                 }
 
                 // recurrent no noise phase
@@ -838,10 +876,12 @@
 
 
 //! does encoding if needed and populates the list.
-void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq) const
+void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq, bool doNoise) const
 {
     if(encoding==&quot;raw_masked_supervised&quot;) // old already encoded format (for backward testing)
-        splitRawMaskedSupervisedSequence(seq);
+        splitRawMaskedSupervisedSequence(seq, doNoise);
+    else if(encoding==&quot;generic&quot;)
+        encode_artificialData(seq);
     else
         encodeAndCreateSupervisedSequence(seq);
 }
@@ -882,7 +922,7 @@
 
 
 // For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
-void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq) const
+void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq, bool doNoise) const
 {
     int l = seq.length();
     resize_lists(l);
@@ -893,7 +933,7 @@
     Mat mask_part = seq.subMatColumns(inputsize_without_masks, targetsize());
     Mat target_part = seq.subMatColumns(inputsize_without_masks+targetsize(), targetsize());
 
-    if(noise)
+    if(doNoise)
         inject_zero_forcing_noise(input_part, input_noise_prob);
 
     for(int i=0; i&lt;l; i++)
@@ -915,7 +955,54 @@
     encoded_seq &lt;&lt; input_part;
 }
 
+void DenoisingRecurrentNet::encode_artificialData(Mat seq) const
+{
+    int l = seq.length();
+    int theInputsize = inputsize();
+    int theTargetsize = targetsize();
+    resize_lists(l);
+    //int inputsize_without_masks = inputsize-targetsize;
+    Mat input_part;
+    input_part.resize(seq.length(),theInputsize);
+    input_part &lt;&lt; seq.subMatColumns(0,theInputsize);
+    //Mat mask_part = seq.subMatColumns(inputsize, targetsize);
+    Mat target_part = seq.subMatColumns(theInputsize, theTargetsize);
 
+    //if(doNoise)
+    //    inject_zero_forcing_noise(input_part, input_noise_prob);
+
+    for(int i=0; i&lt;l; i++)
+        input_list[i] = input_part(i);
+
+    int ntargets = target_layers.length();
+    targets_list.resize(ntargets);
+    //masks_list.resize(ntargets);
+    int startcol = 0; // starting column of next target in target_part and mask_part
+    for(int k=0; k&lt;ntargets; k++)
+    {
+        int targsize = target_layers[k]-&gt;size;
+        targets_list[k] = target_part.subMatColumns(startcol, targsize);
+        //masks_list[k] = mask_part.subMatColumns(startcol, targsize);
+        startcol += targsize;
+    }
+
+    encoded_seq.resize(input_part.length(), input_part.width());
+    encoded_seq &lt;&lt; input_part;
+
+
+    /*int l = sequence.length();
+ 
+    // reserve one extra bit to mean repetition
+    encoded_sequence.resize(l, 1);
+    encoded_sequence.clear();
+
+    for(int i=0; i&lt;l; i++)
+    {
+        int number = int(sequence(i,0));
+        encoded_sequence(i,0) = number;        
+        }    */
+}    
+
 void DenoisingRecurrentNet::resize_lists(int l) const
 {
     input_list.resize(l);
@@ -987,7 +1074,7 @@
 }
 
 // fprop accumulates costs in costs and counts in n_items
-void DenoisingRecurrentNet::recurrentFprop(Vec train_costs, Vec train_n_items) const
+void DenoisingRecurrentNet::recurrentFprop(Vec train_costs, Vec train_n_items, bool useDynamicConnections) const
 {
     int l = input_list.length();
     int ntargets = target_layers.length();
@@ -996,14 +1083,14 @@
     {
         Vec hidden_act_no_bias_i = hidden_act_no_bias_list(i);
         input_connections-&gt;fprop( input_list[i], hidden_act_no_bias_i);
-
-        if( i &gt; 0 &amp;&amp; dynamic_connections )
-        {
-            Vec hidden_i_prev = hidden_list(i-1);
-            dynamic_connections-&gt;fprop(hidden_i_prev,dynamic_act_no_bias_contribution );
-            hidden_act_no_bias_i += dynamic_act_no_bias_contribution;
+        if(useDynamicConnections){
+            if( i &gt; 0 &amp;&amp; dynamic_connections )
+            {
+                Vec hidden_i_prev = hidden_list(i-1);
+                dynamic_connections-&gt;fprop(hidden_i_prev,dynamic_act_no_bias_contribution );
+                hidden_act_no_bias_i += dynamic_act_no_bias_contribution;
+            }
         }
-        
         Vec hidden_i = hidden_list(i);
         hidden_layer-&gt;fprop( hidden_act_no_bias_i, 
                              hidden_i);
@@ -1048,8 +1135,8 @@
             }
         }
     }
-    if(noise)
-        inject_zero_forcing_noise(hidden_list, input_noise_prob);
+    //if(noise)
+    //  inject_zero_forcing_noise(hidden_list, input_noise_prob);
 }
 
 
@@ -1130,7 +1217,8 @@
                                                   int&amp; down_size,
                                                   int&amp; up_size,
                                                   real&amp; lr,
-                                                  bool accumulate)
+                                                  bool accumulate,
+                                                  bool using_penalty_factor)
 {
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
@@ -1156,7 +1244,8 @@
     //externalProductScaleAcc( weights, output_gradient, input, -lr );
     externalProductScaleAcc( acc_weights_gr, output_gradient, input, -lr );
     
-   
+    if((!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) &amp;&amp; using_penalty_factor)
+        applyWeightPenalty(weights, acc_weights_gr, down_size, up_size, lr);
 }
 
 void DenoisingRecurrentNet::bpropUpdateHiddenLayer(const Vec&amp; input, 
@@ -1194,6 +1283,45 @@
     //applyBiasDecay();
 }
 
+void DenoisingRecurrentNet::applyWeightPenalty(Mat&amp; weights, Mat&amp; acc_weights_gr, int&amp; down_size, int&amp; up_size, real&amp; lr)
+{
+    // Apply penalty (decay) on weights.
+    real delta_L1 = lr * L1_penalty_factor;
+    real delta_L2 = lr * L2_penalty_factor;
+    /*if (L2_decrease_type == &quot;one_over_t&quot;)
+        delta_L2 /= (1 + L2_decrease_constant * L2_n_updates);
+    else if (L2_decrease_type == &quot;sigmoid_like&quot;)
+        delta_L2 *= sigmoid((L2_shift - L2_n_updates) * L2_decrease_constant);
+    else
+        PLERROR(&quot;In RBMMatrixConnection::applyWeightPenalty - Invalid value &quot;
+                &quot;for L2_decrease_type: %s&quot;, L2_decrease_type.c_str());
+    */
+    for( int i=0; i&lt;up_size; i++)
+    {
+        real* w_ = weights[i];
+        real* a_w_g = acc_weights_gr[i];
+        for( int j=0; j&lt;down_size; j++ )
+        {
+            if( delta_L2 != 0. ){
+                //w_[j] *= (1 - delta_L2);
+                a_w_g[j] -= w_[j]*delta_L2;
+            }
+
+            if( delta_L1 != 0. )
+            {
+                if( w_[j] &gt; delta_L1 )
+                    a_w_g[j] -= delta_L1;
+                else if( w_[j] &lt; -delta_L1 )
+                    a_w_g[j] += delta_L1;
+                else
+                    a_w_g[j] = 0.;
+            }
+        }
+    }
+    /*if (delta_L2 &gt; 0)
+      L2_n_updates++;*/
+}
+
 double DenoisingRecurrentNet::fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat&amp; reconstruction_weights, Mat&amp; acc_weights_gr, Vec&amp; input_reconstruction_bias, Vec&amp; input_reconstruction_prob, 
                                                                        Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
 {
@@ -1232,7 +1360,7 @@
     }
 
     double result_cost = 0;
-    if(encoding==&quot;raw_masked_supervised&quot;) // complicated input format... consider it's squared error
+    if(encoding==&quot;raw_masked_supervised&quot; || encoding==&quot;generic&quot;) // complicated input format... consider it's squared error
     {
         double r = 0;
         double neg_log_cost = 0; // neg log softmax
@@ -1294,22 +1422,47 @@
 }
 
 
-double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Mat&amp; acc_weights_gr, Vec&amp; reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec&amp; reconstruction_prob, 
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec theInput, Vec hidden, Mat reconstruction_weights, Mat&amp; acc_weights_gr, Vec&amp; reconstruction_bias, Vec&amp; reconstruction_bias2, Vec hidden_reconstruction_activation_grad, Vec&amp; reconstruction_prob, 
                                                                  Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
 {
     // set appropriate sizes
     int fullhiddenlength = hidden_target.length();
     Vec reconstruction_activation;
+    Vec hidden_input_noise;
+    Vec hidden_fprop_noise;
+    Vec hidden_act_no_bias;
+    Vec dynamic_act_no_bias_contribution;
     if(reconstruction_bias.length()==0)
     {
         reconstruction_bias.resize(fullhiddenlength);
         reconstruction_bias.clear();
     }
+    if(reconstruction_bias2.length()==0)
+    {
+        reconstruction_bias2.resize(fullhiddenlength);
+        reconstruction_bias2.clear();
+    }
     reconstruction_activation.resize(fullhiddenlength);
     reconstruction_prob.resize(fullhiddenlength);
 
+    hidden_fprop_noise.resize(fullhiddenlength);
+    hidden_input_noise.resize(fullhiddenlength);
+    hidden_act_no_bias.resize(fullhiddenlength);
+    dynamic_act_no_bias_contribution.resize(fullhiddenlength);
+
+    //input_connections-&gt;fprop( theInput, hidden_act_no_bias);
+    hidden_input_noise &lt;&lt; hidden_target;
+    inject_zero_forcing_noise(hidden_input_noise, input_noise_prob);
+    dynamic_connections-&gt;fprop(hidden_input_noise, dynamic_act_no_bias_contribution );
+    hidden_act_no_bias += dynamic_act_no_bias_contribution;
+    //hidden_layer-&gt;fprop( hidden_act_no_bias, hidden_fprop_noise);
+    hidden_act_no_bias += reconstruction_bias2;
+    for( int j=0 ; j&lt;fullhiddenlength ; j++ )
+        hidden_fprop_noise[j] = fastsigmoid(hidden_act_no_bias[j] );
+
     // predict (denoised) input_reconstruction 
-    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden_fprop_noise); //dynamic matrice tied
+    //transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
     //product(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice not tied
     reconstruction_activation += reconstruction_bias;
 
@@ -1334,7 +1487,8 @@
     //externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
     //externalProductScaleAcc(acc_weights_gr, hidden_reconstruction_activation_grad, hidden, -lr); //dynamic matrice not tied
                 
-    
+    //update bias2
+    multiplyAcc(reconstruction_bias2, hidden_gradient, -lr);
     /********************************************************************************/
     // Vec hidden_reconstruction_activation_grad;
     /*hidden_reconstruction_activation_grad.clear();
@@ -1601,7 +1755,8 @@
                                               target_connections[tar]-&gt;down_size,
                                               target_connections[tar]-&gt;up_size,
                                               target_connections[tar]-&gt;learning_rate,
-                                              true);
+                                              true,
+                                              false);
                     }
                     else{
                         //target_connections[tar]-&gt;bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),hidden_gradient, bias_gradient,true);
@@ -1614,7 +1769,8 @@
                                               target_connections[tar]-&gt;down_size,
                                               target_connections[tar]-&gt;up_size,
                                               target_connections[tar]-&gt;learning_rate,
-                                              true);
+                                              true,
+                                              false);
                     }
                 }
             }
@@ -1637,11 +1793,11 @@
             if(input_reconstruction_weight!=0)
             {
                 //Mat reconstruction_weights = getInputConnectionsWeightMatrix();
-                Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
+                //Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
                 
-                train_costs[4] += fpropUpdateInputReconstructionFromHidden(hidden_list(i), inputWeights, acc_input_connections_gr, input_reconstruction_bias, input_reconstruction_prob, 
-                                                                           clean_input, hidden_gradient, input_reconstruction_weight, current_learning_rate);
-                train_n_items[4]++;
+                train_costs[train_costs.length()-2] += fpropUpdateInputReconstructionFromHidden(hidden_list(i), inputWeights, acc_input_connections_gr, input_reconstruction_bias, input_reconstruction_prob, 
+                                                                           input_list[i], hidden_gradient, input_reconstruction_weight, current_learning_rate);
+                train_n_items[train_costs.length()-2]++;
             }
             
             
@@ -1659,9 +1815,9 @@
                     
                     //truc stan
                     //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                    train_costs[5] += fpropHiddenReconstructionFromLastHidden(hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                    train_costs[train_costs.length()-1] += fpropHiddenReconstructionFromLastHidden(input_list[i], hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_bias2, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
                     //fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconsWeights, acc_reconstruction_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                    train_n_items[5]++;
+                    train_n_items[train_costs.length()-1]++;
                 }
                 
                 
@@ -1673,69 +1829,87 @@
                     
                 }
                 
+                
+                
+                
+               
                 bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
                                        hidden_list(i),
                                        hidden_temporal_gradient, 
                                        hidden_gradient,
                                        hidden_layer-&gt;bias, 
                                        hidden_layer-&gt;learning_rate );
-                //Dynamic
-                bpropUpdateConnection(hidden_list(i-1),
-                                      hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
-                                      hidden_gradient, 
-                                      hidden_temporal_gradient, 
-                                      dynamicWeights,
-                                      acc_dynamic_connections_gr,
-                                      dynamic_connections-&gt;down_size,
-                                      dynamic_connections-&gt;up_size,
-                                      dynamic_connections-&gt;learning_rate,
-                                      false);
                 
-                /*if(hidden_reconstruction_weight!=0)
-                  {
-                  // update bias
-                  multiplyAcc(hidden_reconstruction_bias, hidden_reconstruction_activation_grad, -current_learning_rate);
-                  // update weight
-                  externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -current_learning_rate);
-                  
-                  }*/
                 
                 //input
-                bpropUpdateConnection(input_list[i],
-                                      hidden_act_no_bias_list(i), 
-                                      visi_bias_gradient, 
-                                      hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
-                                      inputWeights,
-                                      acc_input_connections_gr,
-                                      input_connections-&gt;down_size,
-                                      input_connections-&gt;up_size,
-                                      input_connections-&gt;learning_rate,
-                                      false);
+                if(hidden_reconstruction_weight==0)
+                {
+                   
+                    
+                    bpropUpdateConnection(input_list[i],
+                                          hidden_act_no_bias_list(i), 
+                                          visi_bias_gradient, 
+                                          hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                          inputWeights,
+                                          acc_input_connections_gr,
+                                          input_connections-&gt;down_size,
+                                          input_connections-&gt;up_size,
+                                          input_connections-&gt;learning_rate,
+                                          false,
+                                          true);
+                }
                 
+                //Dynamic
+                if(input_reconstruction_weight==0)
+                {
+                    /*bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
+                                       hidden_list(i),
+                                       hidden_temporal_gradient, 
+                                       hidden_gradient,
+                                       hidden_layer-&gt;bias, 
+                                       hidden_layer-&gt;learning_rate );*/
+
+                    bpropUpdateConnection(hidden_list(i-1),
+                                          hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
+                                          hidden_gradient, 
+                                          hidden_temporal_gradient, 
+                                          dynamicWeights,
+                                          acc_dynamic_connections_gr,
+                                          dynamic_connections-&gt;down_size,
+                                          dynamic_connections-&gt;up_size,
+                                          dynamic_connections-&gt;learning_rate,
+                                          false,
+                                          false);
+                }
+                
                 hidden_temporal_gradient &lt;&lt; hidden_gradient; 
                 //if(hidden_reconstruction_weight!=0)
                 //    hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
             }
             else
             {
-                bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
-                                       hidden_list(i),
-                                       hidden_temporal_gradient, // Not really temporal gradient, but this is the final iteration...
-                                       hidden_gradient,
-                                       hidden_layer-&gt;bias, 
-                                       hidden_layer-&gt;learning_rate );
-                
-                //input
-                bpropUpdateConnection(input_list[i],
-                                      hidden_act_no_bias_list(i), 
-                                      visi_bias_gradient, 
-                                      hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
-                                      inputWeights,
-                                      acc_input_connections_gr,
-                                      input_connections-&gt;down_size,
-                                      input_connections-&gt;up_size,
-                                      input_connections-&gt;learning_rate,
-                                      false);
+                if(input_reconstruction_weight==0)
+                {
+                    bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
+                                           hidden_list(i),
+                                           hidden_temporal_gradient, // Not really temporal gradient, but this is the final iteration...
+                                           hidden_gradient,
+                                           hidden_layer-&gt;bias, 
+                                           hidden_layer-&gt;learning_rate );
+                    
+                    //input
+                    bpropUpdateConnection(input_list[i],
+                                          hidden_act_no_bias_list(i), 
+                                          visi_bias_gradient, 
+                                          hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                          inputWeights,
+                                          acc_input_connections_gr,
+                                          input_connections-&gt;down_size,
+                                          input_connections-&gt;up_size,
+                                          input_connections-&gt;learning_rate,
+                                          false,
+                                          true);
+                }
             }
         }
     }
@@ -1842,7 +2016,7 @@
     else if(encoding==&quot;raw_masked_supervised&quot;)
         PLERROR(&quot;raw_masked_supervised means already encoded! You shouldnt have landed here!!!&quot;);
     else if(encoding==&quot;generic&quot;)
-        PLERROR(&quot;generic encoding not yet implemented&quot;);
+        PLERROR(&quot;generic means already encoded! You shouldnt have landed here!!!&quot;);
     else
         PLERROR(&quot;unsupported encoding: %s&quot;,encoding.c_str());
 }
@@ -2005,8 +2179,8 @@
             encoded_sequence(k++,nnotes) = 1;            
     }    
 }
-    
 
+
 // input noise injection
 void DenoisingRecurrentNet::inject_zero_forcing_noise(Mat sequence, double noise_prob) const
 {
@@ -2022,6 +2196,19 @@
     }
 }
 
+// input noise injection
+void DenoisingRecurrentNet::inject_zero_forcing_noise(Vec sequence, double noise_prob) const
+{
+    
+    real* p = sequence.data();
+    int n = sequence.size();
+    while(n--)
+    {
+        if(*p!=real(0.) &amp;&amp; random_gen-&gt;uniform_sample()&lt;noise_prob)
+            *p = real(0.);
+        ++p;
+    }
+}
 
 void DenoisingRecurrentNet::clamp_units(const Vec layer_vector,
                                              PP&lt;RBMLayer&gt; layer,
@@ -2165,7 +2352,7 @@
         int seqlen = end-start; // target_prediction_list[0].length();
         seq.resize(seqlen, w);
         testset-&gt;getMat(start,0,seq);
-        encodeSequenceAndPopulateLists(seq);
+        encodeSequenceAndPopulateLists(seq, false);
 
         if(input_window_size==0)
             unconditionalFprop(costs, n_items);

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-04-09 17:26:43 UTC (rev 10107)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-04-10 02:03:00 UTC (rev 10108)
@@ -70,6 +70,7 @@
 
     ////! Number of epochs for rbm phase
     //int rbm_nstages;
+    //int nCost;
 
     //! The training weights of each target layers
     Vec target_layers_weights;
@@ -126,6 +127,12 @@
     string encoding;
 
     bool noise;
+
+    //! Optional (default=0) factor of L1 regularization term
+    real L1_penalty_factor;
+
+    //! Optional (default=0) factor of L2 regularization term
+    real L2_penalty_factor;
     
     //! Input window size
     int input_window_size;
@@ -154,6 +161,8 @@
 
     // learnt bias for hidden reconstruction
     Vec hidden_reconstruction_bias;
+
+    Vec hidden_reconstruction_bias2;
     
     double prediction_cost_weight;
     double input_reconstruction_cost_weight;
@@ -181,8 +190,9 @@
                                                   bool use_silence, int octav_nbits, int duration_nbits=20);
     
     static void encode_onehot_timeframe(Mat sequence, Mat&amp; encoded_sequence, 
-                                        int prepend_zero_rows, bool use_silence=false);    
+                                        int prepend_zero_rows, bool use_silence=false); 
 
+
     static int duration_to_number_of_timeframes(int duration);
     static int getDurationBit(int duration);
 
@@ -190,6 +200,9 @@
     // input noise injection
     void inject_zero_forcing_noise(Mat sequence, double noise_prob) const;
 
+    // noise injection
+    void inject_zero_forcing_noise(Vec sequence, double noise_prob) const;
+
     inline static Vec getInputWindow(Mat sequence, int startpos, int winsize)
     { return sequence.subMatRows(startpos, winsize).toVec(); }
           
@@ -414,17 +427,19 @@
     // note: the following functions are declared const because they have
     // to be called by test (which is const). Similarly, the members they 
     // manipulate are all declared mutable.
-    void recurrentFprop(Vec train_costs, Vec train_n_items) const;
+    void recurrentFprop(Vec train_costs, Vec train_n_items, bool useDynamicConnections=true) const;
 
     //! does encoding if needed and populates the list.
-    void encodeSequenceAndPopulateLists(Mat seq) const;
+    void encodeSequenceAndPopulateLists(Mat seq, bool doNoise) const;
 
     //! encodes seq, then populates: inputslist, targets_list, masks_list
     void encodeAndCreateSupervisedSequence(Mat seq) const;
 
     //! For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
-    void splitRawMaskedSupervisedSequence(Mat seq) const;
+    void splitRawMaskedSupervisedSequence(Mat seq, bool doNoise) const;
 
+    void encode_artificialData(Mat seq) const; 
+
     void resize_lists(int l) const;
 
     void trainUnconditionalPredictor();
@@ -451,7 +466,8 @@
                                int&amp; down_size,
                                int&amp; up_size,
                                real&amp; lr,
-                               bool accumulate);
+                               bool accumulate,
+                               bool using_penalty_factor);
 
     void bpropUpdateHiddenLayer(const Vec&amp; input, 
                                 const Vec&amp; output,
@@ -460,6 +476,9 @@
                                 Vec&amp; bias,
                                 real&amp; lr);
 
+
+    void applyWeightPenalty(Mat&amp; weights, Mat&amp; acc_weights_gr, int&amp; down_size, int&amp; up_size, real&amp; lr);
+
     //! Builds input_reconstruction_prob from hidden (using reconstruction_weights which is  nhidden x ninputs, and input_reconstruction_bias)
     //! then backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
     //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
@@ -479,7 +498,7 @@
     void updateInputReconstructionFromHidden(Vec hidden, Mat&amp; reconstruction_weights, Mat&amp; acc_weights_gr, Vec&amp; input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
-    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Mat&amp; acc_weights_gr, Vec&amp; reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec&amp; reconstruction_prob, 
+    double fpropHiddenReconstructionFromLastHidden(Vec theInput, Vec hidden, Mat reconstruction_weights, Mat&amp; acc_weights_gr, Vec&amp; reconstruction_bias, Vec&amp; reconstruction_bias2, Vec hidden_reconstruction_activation_grad, Vec&amp; reconstruction_prob, 
                                                                           Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
     
     double fpropHiddenSymmetricDynamicMatrix(Vec hidden, Mat reconstruction_weights, Vec&amp; reconstruction_prob, 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003547.html">[Plearn-commits] r10107 - trunk/plearn_learners/regressors
</A></li>
	<LI>Next message: <A HREF="003549.html">[Plearn-commits] r10109 - in	trunk/plearn_learners/regressors/test/GaussianProcessRegressor: .	.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3548">[ date ]</a>
              <a href="thread.html#3548">[ thread ]</a>
              <a href="subject.html#3548">[ subject ]</a>
              <a href="author.html#3548">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
