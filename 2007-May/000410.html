<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6961 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6961%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705011816.l41IGL0J000822%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000409.html">
   <LINK REL="Next"  HREF="000411.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6961 - trunk/plearn_learners/online</H1>
    <B>tihocan at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6961%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705011816.l41IGL0J000822%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6961 - trunk/plearn_learners/online">tihocan at mail.berlios.de
       </A><BR>
    <I>Tue May  1 20:16:21 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000409.html">[Plearn-commits] r6960 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000411.html">[Plearn-commits] r6962 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#410">[ date ]</a>
              <a href="thread.html#410">[ thread ]</a>
              <a href="subject.html#410">[ subject ]</a>
              <a href="author.html#410">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: tihocan
Date: 2007-05-01 20:16:20 +0200 (Tue, 01 May 2007)
New Revision: 6961

Modified:
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
Log:
- Implemented mini-batch versions of fprop and bpropUpdate
- Fixed L2 gradient descent in bpropUpdate, so that the update takes into account the previous value of the weight, and not the updated one


Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-05-01 17:59:28 UTC (rev 6960)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-05-01 18:16:20 UTC (rev 6961)
@@ -42,6 +42,7 @@
 
 
 #include &quot;GradNNetLayerModule.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
 
 namespace PLearn {
 using namespace std;
@@ -58,6 +59,9 @@
     &quot;from a uniform distribution.\n&quot;
     );
 
+/////////////////////////
+// GradNNetLayerModule //
+/////////////////////////
 GradNNetLayerModule::GradNNetLayerModule():
     start_learning_rate( .001 ),
     decrease_constant( 0. ),
@@ -65,10 +69,11 @@
     L1_penalty_factor( 0. ),
     L2_penalty_factor( 0. ),
     step_number( 0 )
-{
-}
+{}
 
-// Applies linear transformation
+///////////
+// fprop //
+///////////
 void GradNNetLayerModule::fprop(const Vec&amp; input, Vec&amp; output) const
 {
     PLASSERT_MSG( input.size() == input_size,
@@ -76,10 +81,26 @@
 
     output.resize( output_size );
 
+    // Applies linear transformation
     for( int i=0 ; i&lt;output_size ; i++ )
         output[i] = dot( weights(i), input ) + bias[i];
 }
 
+void GradNNetLayerModule::fprop(const Mat&amp; inputs, Mat&amp; outputs)
+{
+    PLASSERT( inputs.width() == input_size );
+    int n = inputs.length();
+    outputs.resize(n, output_size);
+    productTranspose(outputs, inputs, weights);
+
+    // Add bias.
+    resizeOnes(n);
+    externalProductScaleAcc(outputs, ones, bias, 1);
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
 // We are not using blas routines anymore, because we would iterate several
 // times over the weight matrix.
 void GradNNetLayerModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
@@ -111,6 +132,9 @@
 
         for( int j=0; j&lt;input_size; j++ )
         {
+            if( delta_L2 &gt; 0. )
+                w_[j] *= (1 - delta_L2);
+
             w_[j] -= input[j] * lr_og_i;
 
             if( delta_L1 &gt; 0. )
@@ -123,8 +147,6 @@
                     w_[j] = 0.;
             }
 
-            if( delta_L2 &gt; 0. )
-                w_[j] *= (1 - delta_L2);
         }
     }
     step_number++;
@@ -175,6 +197,10 @@
         for( int j=0; j&lt;input_size; j++ )
         {
             input_gradient[j] += w_[j] * og_i;
+
+            if( delta_L2 &gt; 0. )
+                w_[j] *= (1 - delta_L2);
+
             w_[j] -= input[j] * lr_og_i;
 
             if( delta_L1 &gt; 0. )
@@ -187,14 +213,80 @@
                     w_[j] = 0.;
             }
 
-            if( delta_L2 &gt; 0. )
-                w_[j] *= (1 - delta_L2);
         }
     }
     step_number++;
 }
 
-// Update
+void GradNNetLayerModule::bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+        Mat&amp; input_gradients,
+        const Mat&amp; output_gradients,
+        bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( outputs.width() == output_size );
+    PLASSERT( output_gradients.width() == output_size );
+
+    int n = inputs.length();
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size &amp;&amp;
+                input_gradients.length() == n,
+                &quot;Cannot resize input_gradients and accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradients.resize(n, input_size);
+        input_gradients.fill(0);
+    }
+
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
+    real avg_lr = learning_rate / n; // To obtain an average on a mini-batch.
+
+    // With L2 regularization, weights are scaled by a coefficient equal to
+    // 1 - learning rate * penalty.
+    real l2_scaling =
+        L2_penalty_factor &gt; 0 ? 1 - learning_rate * L2_penalty_factor
+                              : 1;
+    PLASSERT_MSG(l2_scaling &gt; 0, &quot;Learning rate too large&quot;);
+
+    // Compute input gradient.
+    productAcc(input_gradients, output_gradients, weights);
+
+    // Update bias.
+    resizeOnes(n);
+    productScaleAcc(bias, output_gradients, true, ones, -avg_lr, 1);
+
+    // Update weights.
+    productScaleAcc(weights, output_gradients, true, inputs, false,
+            -avg_lr, l2_scaling);
+
+    // Apply L1 penalty if needed (note: this is not very efficient).
+    if (L1_penalty_factor &gt; 0) {
+        real delta_L1 = learning_rate * L1_penalty_factor;
+        for( int i=0; i&lt;output_size; i++ )
+        {
+            real* w_ = weights[i];
+            for( int j=0; j&lt;input_size; j++ )
+            {
+                real&amp; w_ij = w_[j];
+                if( w_ij &gt; delta_L1 )
+                    w_ij -= delta_L1;
+                else if( w_ij &lt; -delta_L1 )
+                    w_ij += delta_L1;
+                else
+                    w_ij = 0.;
+            }
+        }
+    }
+    step_number += n;
+}
+    
+
+//////////////////
+// bbpropUpdate //
+//////////////////
 void GradNNetLayerModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                                        const Vec&amp; output_gradient,
                                        const Vec&amp; output_diag_hessian)
@@ -218,6 +310,9 @@
 }
 */
 
+////////////
+// forget //
+////////////
 // Forget the bias and reinitialize the weights
 void GradNNetLayerModule::forget()
 {
@@ -258,25 +353,35 @@
 {
     start_learning_rate = dynamic_learning_rate;
     step_number = 0;
-    // learning_rate will automaticly be set in bpropUpdate()
+    // learning_rate will automatically be set in bpropUpdate()
 }
 
+///////////
+// build //
+///////////
 void GradNNetLayerModule::build()
 {
     inherited::build();
     build_();
 }
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void GradNNetLayerModule::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(init_weights, copies);
-    deepCopyField(init_bias, copies);
-    deepCopyField(weights, copies);
-    deepCopyField(bias, copies);
+    deepCopyField(init_bias,    copies);
+    deepCopyField(weights,      copies);
+    deepCopyField(bias,         copies);
+    deepCopyField(ones,         copies);
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void GradNNetLayerModule::declareOptions(OptionList&amp; ol)
 {
     declareOption(ol, &quot;start_learning_rate&quot;,
@@ -330,6 +435,9 @@
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void GradNNetLayerModule::build_()
 {
     if( input_size &lt; 0 ) // has not been initialized
@@ -351,6 +459,17 @@
     }
 }
 
+////////////////
+// resizeOnes //
+////////////////
+void GradNNetLayerModule::resizeOnes(int n)
+{
+    if (ones.length() &lt; n) {
+        ones.resize(n);
+        ones.fill(1);
+    } else if (ones.length() &gt; n)
+        ones.resize(n);
+}
 
 
 

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2007-05-01 17:59:28 UTC (rev 6960)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2007-05-01 18:16:20 UTC (rev 6961)
@@ -108,6 +108,9 @@
 
     virtual void fprop(const Vec&amp; input, Vec&amp; output) const;
 
+    //! Overridden.
+    virtual void fprop(const Mat&amp; inputs, Mat&amp; outputs);
+
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              const Vec&amp; output_gradient);
 
@@ -115,6 +118,11 @@
                              Vec&amp; input_gradient, const Vec&amp; output_gradient,
                              bool accumulate=false);
 
+    virtual void bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                             Mat&amp; input_gradients,
+                             const Mat&amp; output_gradients,
+                             bool accumulate = false);
+    
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                               const Vec&amp; output_gradient,
                               const Vec&amp; output_diag_hessian);
@@ -144,6 +152,10 @@
     virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
 
 protected:
+
+    //! A vector filled with all ones.
+    Vec ones;
+    
     //#####  Protected Options  ###############################################
 
 protected:
@@ -158,6 +170,9 @@
     //! This does the actual building.
     void build_();
 
+    //! Resize vector 'ones'.
+    void resizeOnes(int n);
+
 private:
     //#####  Private Data Members  ############################################
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000409.html">[Plearn-commits] r6960 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000411.html">[Plearn-commits] r6962 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#410">[ date ]</a>
              <a href="thread.html#410">[ thread ]</a>
              <a href="subject.html#410">[ subject ]</a>
              <a href="author.html#410">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
