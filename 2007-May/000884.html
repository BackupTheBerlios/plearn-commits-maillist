<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7435 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7435%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705301725.l4UHPgcL020920%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000883.html">
   <LINK REL="Next"  HREF="000885.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7435 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7435%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705301725.l4UHPgcL020920%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7435 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Wed May 30 19:25:42 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000883.html">[Plearn-commits] r7434 - in trunk/python_modules/plearn: . learners	learners/autolr pyext
</A></li>
        <LI>Next message: <A HREF="000885.html">[Plearn-commits] r7436 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#884">[ date ]</a>
              <a href="thread.html#884">[ thread ]</a>
              <a href="subject.html#884">[ subject ]</a>
              <a href="author.html#884">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-05-30 19:25:42 +0200 (Wed, 30 May 2007)
New Revision: 7435

Modified:
   trunk/plearn_learners/online/RBMConnection.cc
   trunk/plearn_learners/online/RBMConnection.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Added the possibility to give an RBM's weights as a port, using the petiteCulotteOlivier*() functions of RBMConnection...


Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2007-05-30 17:02:28 UTC (rev 7434)
+++ trunk/plearn_learners/online/RBMConnection.cc	2007-05-30 17:25:42 UTC (rev 7435)
@@ -281,6 +281,48 @@
     computeProduct( 0, output_size, output );
 }
 
+void RBMConnection::getAllWeights(Mat&amp; rbm_weights) const
+{
+    PLERROR(&quot;In RBMConnection::getAllWeights(): not implemented&quot;);
+}
+
+void RBMConnection::setAllWeights(const Mat&amp; rbm_weights) 
+{
+    PLERROR(&quot;In RBMConnection::setAllWeights(): not implemented&quot;);
+}
+
+void RBMConnection::petiteCulotteOlivierUpdate(
+    const Vec&amp; input, const Mat&amp; rbm_weights,
+    const Vec&amp; output,
+    Vec&amp; input_gradient, 
+    Mat&amp; rbm_weights_gradient,
+    const Vec&amp; output_gradient,
+    bool accumulate)
+{
+    PLERROR(&quot;In RBMConnection::bpropUpdate(): not implemented&quot;);
+}
+
+ 
+/////////////
+// bpropCD //
+/////////////
+void RBMConnection::petiteCulotteOlivierCD(Mat&amp; weights_gradient,
+                                           bool accumulate)
+{
+    PLERROR(&quot;In RBMConnection::petiteCulotteOlivierCD(): not implemented&quot;);
+}
+
+void RBMConnection::petiteCulotteOlivierCD( const Vec&amp; pos_down_values, 
+                                            const Vec&amp; pos_up_values,   
+                                            const Vec&amp; neg_down_values, 
+                                            const Vec&amp; neg_up_values, 
+                                            Mat&amp; weights_gradient,
+                                            bool accumulate)
+{
+    PLERROR(&quot;In RBMConnection::petiteCulotteOlivierCD(): not implemented&quot;);
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2007-05-30 17:02:28 UTC (rev 7434)
+++ trunk/plearn_learners/online/RBMConnection.h	2007-05-30 17:25:42 UTC (rev 7435)
@@ -188,7 +188,35 @@
     //! given the input, compute the output (possibly resize it  appropriately)
     virtual void fprop(const Vec&amp; input, Vec&amp; output) const;
 
+    //! provide the internal weight values (not a copy)
+    virtual void getAllWeights(Mat&amp; rbm_weights) const;
 
+    //! set the internal weight values to rbm_weights (not a copy)
+    virtual void setAllWeights(const Mat&amp; rbm_weights);
+
+    //! back-propagates the output gradient to the input and the weights
+    //! (the weights are not updated)
+    virtual void petiteCulotteOlivierUpdate(
+        const Vec&amp; input, const Mat&amp; rbm_weights,
+        const Vec&amp; output,
+        Vec&amp; input_gradient, Mat&amp; rbm_weights_gradient,
+        const Vec&amp; output_gradient,
+        bool accumulate = false);
+    
+    //! Computes the contrastive divergence gradient with respect to the weights
+    //! It should be noted that bpropCD does not call clearstats().
+    virtual void petiteCulotteOlivierCD(Mat&amp; weights_gradient, 
+                                        bool accumulate = false);
+    
+    //! Computes the contrastive divergence gradient with respect to the weights
+    //! given the positive and negative phase values.
+    virtual void petiteCulotteOlivierCD(const Vec&amp; pos_down_values,
+                                        const Vec&amp; pos_up_values,
+                                        const Vec&amp; neg_down_values,
+                                        const Vec&amp; neg_up_values,
+                                        Mat&amp; weights_gradient,
+                                        bool accumulate = false);
+    
     //! return the number of parameters
     virtual int nParameters() const = 0;
 

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-05-30 17:02:28 UTC (rev 7434)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-05-30 17:25:42 UTC (rev 7435)
@@ -486,14 +486,21 @@
 }
 
 ///////////////////
+// getAllWeights //
+///////////////////
+void RBMMatrixConnection::getAllWeights(Mat&amp; rbm_weights) const
+{
+    rbm_weights = weights;
+}
+
+///////////////////
 // setAllWeights //
 ///////////////////
 void RBMMatrixConnection::setAllWeights(const Mat&amp; rbm_weights)
 {
-    weights &lt;&lt; rbm_weights;
+    weights = rbm_weights;
 }
 
-
 /////////////////
 // bpropUpdate //
 /////////////////
@@ -556,12 +563,13 @@
             -learning_rate / inputs.length(), 1.);
 }
 
-void RBMMatrixConnection::bpropUpdate(const Vec&amp; input, const Mat&amp; rbm_weights,
-                                      const Vec&amp; output,
-                                      Vec&amp; input_gradient, 
-                                      Mat&amp; rbm_weights_gradient,
-                                      const Vec&amp; output_gradient,
-                                      bool accumulate)
+void RBMMatrixConnection::petiteCulotteOlivierUpdate(
+    const Vec&amp; input, const Mat&amp; rbm_weights,
+    const Vec&amp; output,
+    Vec&amp; input_gradient, 
+    Mat&amp; rbm_weights_gradient,
+    const Vec&amp; output_gradient,
+    bool accumulate)
 {
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
@@ -598,7 +606,8 @@
 /////////////
 // bpropCD //
 /////////////
-void RBMMatrixConnection::bpropCD(Mat&amp; weights_gradient)
+void RBMMatrixConnection::petiteCulotteOlivierCD(Mat&amp; weights_gradient,
+                                                 bool accumulate)
 {
     int l = weights_gradient.length();
     int w = weights_gradient.width();
@@ -610,18 +619,29 @@
     int wps_mod = weights_pos_stats.mod();
     int wns_mod = weights_neg_stats.mod();
     
-    for( int i=0 ; i&lt;l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
-        for( int j=0 ; j&lt;w ; j++ )
-            w_i[j] = wps_i[j]/pos_count - wns_i[j]/neg_count;
+    if(accumulate)
+    {
+        for( int i=0 ; i&lt;l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
+            for( int j=0 ; j&lt;w ; j++ )
+                w_i[j] += wps_i[j]/pos_count - wns_i[j]/neg_count;
+    }
+    else
+    {
+        for( int i=0 ; i&lt;l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
+            for( int j=0 ; j&lt;w ; j++ )
+                w_i[j] = wps_i[j]/pos_count - wns_i[j]/neg_count;
+    }
 }
 
 // Instead of using the statistics, we assume we have only one markov chain
 // runned and we update the parameters from the first 4 values of the chain
-void RBMMatrixConnection::bpropCD( const Vec&amp; pos_down_values, // v_0
-                                   const Vec&amp; pos_up_values,   // h_0
-                                   const Vec&amp; neg_down_values, // v_1
-                                   const Vec&amp; neg_up_values, // h_1
-                                   Mat&amp; weights_gradient) 
+void RBMMatrixConnection::petiteCulotteOlivierCD( 
+    const Vec&amp; pos_down_values, // v_0
+    const Vec&amp; pos_up_values,   // h_0
+    const Vec&amp; neg_down_values, // v_1
+    const Vec&amp; neg_up_values, // h_1
+    Mat&amp; weights_gradient,
+    bool accumulate) 
 {
     int l = weights.length();
     int w = weights.width();
@@ -637,9 +657,18 @@
     real* ndv = neg_down_values.data();
     int w_mod = weights_gradient.mod();
 
-    for( int i=0 ; i&lt;l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
-        for( int j=0 ; j&lt;w ; j++ )
-            w_i[j] =  *puv_i * pdv[j] - *nuv_i * ndv[j] ;
+    if(accumulate)
+    {
+        for( int i=0 ; i&lt;l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
+            for( int j=0 ; j&lt;w ; j++ )
+                w_i[j] +=  *puv_i * pdv[j] - *nuv_i * ndv[j] ;
+    }
+    else
+    {
+        for( int i=0 ; i&lt;l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
+            for( int j=0 ; j&lt;w ; j++ )
+                w_i[j] =  *puv_i * pdv[j] - *nuv_i * ndv[j] ;
+    }
 }
 
 ////////////

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-05-30 17:02:28 UTC (rev 7434)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-05-30 17:25:42 UTC (rev 7435)
@@ -188,7 +188,10 @@
     virtual void fprop(const Vec&amp; input, const Mat&amp; rbm_weights,
                        Vec&amp; output) const;
 
-    //! set the internal weight values to rbm_weights (copy)
+    //! provide the internal weight values (not a copy)
+    virtual void getAllWeights(Mat&amp; rbm_weights) const;
+
+    //! set the internal weight values to rbm_weights (not a copy)
     virtual void setAllWeights(const Mat&amp; rbm_weights);
 
     //! this version allows to obtain the input gradient as well
@@ -205,23 +208,26 @@
 
     //! back-propagates the output gradient to the input and the weights
     //! (the weights are not updated)
-    virtual void bpropUpdate(const Vec&amp; input, const Mat&amp; rbm_weights,
-                             const Vec&amp; output,
-                             Vec&amp; input_gradient, Mat&amp; rbm_weights_gradient,
-                             const Vec&amp; output_gradient,
-                             bool accumulate = false);
-
+    virtual void petiteCulotteOlivierUpdate(
+        const Vec&amp; input, const Mat&amp; rbm_weights,
+        const Vec&amp; output,
+        Vec&amp; input_gradient, Mat&amp; rbm_weights_gradient,
+        const Vec&amp; output_gradient,
+        bool accumulate = false);
+    
     //! Computes the contrastive divergence gradient with respect to the weights
     //! It should be noted that bpropCD does not call clearstats().
-    virtual void bpropCD(Mat&amp; weights_gradient);
+    virtual void petiteCulotteOlivierCD(Mat&amp; weights_gradient,
+                                        bool accumulate = false);
     
     //! Computes the contrastive divergence gradient with respect to the weights
     //! given the positive and negative phase values.
-    virtual void bpropCD(const Vec&amp; pos_down_values,
-                         const Vec&amp; pos_up_values,
-                         const Vec&amp; neg_down_values,
-                         const Vec&amp; neg_up_values,
-                         Mat&amp; weights_gradient);
+    virtual void petiteCulotteOlivierCD(const Vec&amp; pos_down_values,
+                                        const Vec&amp; pos_up_values,
+                                        const Vec&amp; neg_down_values,
+                                        const Vec&amp; neg_up_values,
+                                        Mat&amp; weights_gradient,
+                                        bool accumulate = false);
 
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-05-30 17:02:28 UTC (rev 7434)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-05-30 17:25:42 UTC (rev 7435)
@@ -244,6 +244,7 @@
     addPortName(&quot;hidden_sample&quot;);
     addPortName(&quot;energy&quot;);
     addPortName(&quot;hidden_bias&quot;); 
+    addPortName(&quot;weights&quot;); 
     addPortName(&quot;neg_log_likelihood&quot;);
     if(reconstruction_connection)
     {
@@ -392,10 +393,34 @@
 //////////////////////////////
 void RBMModule::computeHiddenActivations(const Mat&amp; visible)
 {
-    connection-&gt;setAsDownInputs(visible);
-    hidden_layer-&gt;getAllActivations(connection, 0, true);
-    if (hidden_bias &amp;&amp; !hidden_bias-&gt;isEmpty())
-        hidden_layer-&gt;activations += *hidden_bias;
+    if(weights &amp;&amp; !weights-&gt;isEmpty())
+    {
+        Mat old_weights;
+        Vec old_activation;
+        connection-&gt;getAllWeights(old_weights);
+        old_activation = hidden_layer-&gt;activation;
+        int up = connection-&gt;up_size;
+        int down = connection-&gt;down_size;
+        PLASSERT( weights-&gt;width() == up * down  );
+        for(int i=0; i&lt;visible.length(); i++)
+        {
+            connection-&gt;setAllWeights(Mat(up, down, (*weights)(i)));
+            connection-&gt;setAsDownInput(visible(i));
+            hidden_layer-&gt;activation = hidden_layer-&gt;activations(i);
+            hidden_layer-&gt;getAllActivations(connection, 0, false);
+            if (hidden_bias &amp;&amp; !hidden_bias-&gt;isEmpty())
+                hidden_layer-&gt;activation += (*hidden_bias)(i);
+        }
+        connection-&gt;setAllWeights(old_weights);
+        hidden_layer-&gt;activation = old_activation;
+    }
+    else
+    {
+        connection-&gt;setAsDownInputs(visible);
+        hidden_layer-&gt;getAllActivations(connection, 0, true);
+        if (hidden_bias &amp;&amp; !hidden_bias-&gt;isEmpty())
+            hidden_layer-&gt;activations += *hidden_bias;
+    }
 }
 
 ///////////////////////////////////////////
@@ -431,8 +456,30 @@
     }
     else
     {
-        connection-&gt;setAsUpInputs(hidden);
-        visible_layer-&gt;getAllActivations(connection, 0, true);
+        if(weights &amp;&amp; !weights-&gt;isEmpty())
+        {
+            Mat old_weights;
+            Vec old_activation;
+            connection-&gt;getAllWeights(old_weights);
+            old_activation = visible_layer-&gt;activation;
+            int up = connection-&gt;up_size;
+            int down = connection-&gt;down_size;
+            PLASSERT( weights-&gt;width() == up * down  );
+            for(int i=0; i&lt;hidden.length(); i++)
+            {
+                connection-&gt;setAllWeights(Mat(up,down,(*weights)(i)));
+                connection-&gt;setAsUpInput(hidden(i));
+                visible_layer-&gt;activation = visible_layer-&gt;activations(i);
+                visible_layer-&gt;getAllActivations(connection, 0, false);
+            }
+            connection-&gt;setAllWeights(old_weights);
+            visible_layer-&gt;activation = old_activation;
+        }
+        else
+        {
+            connection-&gt;setAsUpInputs(hidden);
+            visible_layer-&gt;getAllActivations(connection, 0, true);
+        }
     }
 }
 
@@ -481,6 +528,7 @@
     Mat* energy = ports_value[getPortIndex(&quot;energy&quot;)];
     Mat* neg_log_likelihood = ports_value[getPortIndex(&quot;neg_log_likelihood&quot;)];
     hidden_bias = ports_value[getPortIndex(&quot;hidden_bias&quot;)];
+    weights = ports_value[getPortIndex(&quot;weights&quot;)];
     Mat* visible_reconstruction = 0;
     Mat* visible_reconstruction_activations = 0;
     Mat* reconstruction_error = 0;
@@ -867,7 +915,9 @@
     Mat* hidden = ports_value[getPortIndex(&quot;hidden.state&quot;)];
     hidden_act = ports_value[getPortIndex(&quot;hidden_activations.state&quot;)];
     Mat* reconstruction_error_grad = 0;
-    Mat* hidden_bias_grad = ports_gradient[getPortIndex(&quot;hidden_bias&quot;)];    
+    Mat* hidden_bias_grad = ports_gradient[getPortIndex(&quot;hidden_bias&quot;)];
+    weights = ports_value[getPortIndex(&quot;weights&quot;)]; 
+    Mat* weights_grad = ports_gradient[getPortIndex(&quot;weights&quot;)];    
 
     if(reconstruction_connection)
         reconstruction_error_grad = 
@@ -909,9 +959,40 @@
                 store_visible_grad = &amp;visible_exp_grad;
             }
             store_visible_grad-&gt;resize(mbs,visible_layer-&gt;size);
-            connection-&gt;bpropUpdate(
+            
+            if (weights_grad)
+            {
+                int up = connection-&gt;up_size;
+                int down = connection-&gt;down_size;
+                PLASSERT( weights &amp;&amp; !weights-&gt;isEmpty() &amp;&amp;
+                          weights_grad-&gt;isEmpty() &amp;&amp;
+                          weights_grad-&gt;width() == up * down );
+                weights_grad-&gt;resize(mbs, up * down);
+                Mat w, wg;
+                Vec v,h,vg,hg;
+                for(int i=0; i&lt;mbs; i++)
+                {
+                    w = Mat(up, down,(*weights)(i));
+                    wg = Mat(up, down,(*weights_grad)(i));
+                    v = (*visible)(i);
+                    h = (*hidden_act)(i);
+                    vg = (*store_visible_grad)(i);
+                    hg = hidden_act_grad(i);
+                    connection-&gt;petiteCulotteOlivierUpdate(
+                        v,
+                        w,
+                        h,
+                        vg,
+                        wg,
+                        hg,true);
+                }
+            }
+            else
+            {
+                connection-&gt;bpropUpdate(
                     *visible, *hidden_act, *store_visible_grad,
                     hidden_act_grad, true);
+            }
             partition_function_is_stale = true;
         }
     } 
@@ -961,9 +1042,38 @@
             }
             // Perform update.
             visible_layer-&gt;update(*visible, *negative_phase_visible_samples);
-            connection-&gt;update(*visible, *hidden,
-                               *negative_phase_visible_samples,
-                               *negative_phase_hidden_expectations);
+            if (weights_grad)
+            {
+                int up = connection-&gt;up_size;
+                int down = connection-&gt;down_size;
+                PLASSERT( weights &amp;&amp; !weights-&gt;isEmpty() &amp;&amp;
+                          weights_grad-&gt;isEmpty() &amp;&amp;
+                          weights_grad-&gt;width() == up * down );
+                weights_grad-&gt;resize(mbs, up * down);
+                    
+                Mat wg;
+                Vec vp, hp, vn, hn;
+                for(int i=0; i&lt;mbs; i++)
+                {
+                    vp = (*visible)(i);
+                    hp = (*hidden)(i);
+                    vn = (*negative_phase_visible_samples)(i);
+                    hn = (*negative_phase_hidden_expectations)(i);
+                    wg = Mat(up, down,(*weights_grad)(i));
+                    connection-&gt;petiteCulotteOlivierCD(
+                        vp, hp,
+                        vn,
+                        hn,
+                        wg,
+                        true);
+                }
+            }
+            else
+            {
+                connection-&gt;update(*visible, *hidden,
+                                   *negative_phase_visible_samples,
+                                   *negative_phase_hidden_expectations);
+            }
             hidden_layer-&gt;update(*hidden, *negative_phase_hidden_expectations);
             if (hidden_bias_grad)
             {
@@ -993,6 +1103,9 @@
                   reconstruction_error);
         int mbs = reconstruction_error_grad-&gt;length();
 
+        PLCHECK_MSG( weights, &quot;In RBMModule::bpropAccUpdate(): reconstruction cost &quot;
+                     &quot;for conditional weights is not implemented&quot;);
+
         // Backprop reconstruction gradient
 
         // Must change visible_layer's expectation

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-05-30 17:02:28 UTC (rev 7434)
+++ trunk/plearn_learners/online/RBMModule.h	2007-05-30 17:25:42 UTC (rev 7435)
@@ -215,6 +215,7 @@
 protected:
 
     Mat* hidden_bias;
+    Mat* weights;
 
     //! Used to store gradient w.r.t. expectations of the hidden layer.
     Mat hidden_exp_grad;
@@ -235,7 +236,7 @@
     Mat hidden_exp_store;
     Mat hidden_act_store;
     Mat* hidden_act;
-    bool hidden_activations_are_computed;
+    bool hidden_activations_are_computed;    
 
     //! List of port names.
     TVec&lt;string&gt; ports;


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000883.html">[Plearn-commits] r7434 - in trunk/python_modules/plearn: . learners	learners/autolr pyext
</A></li>
	<LI>Next message: <A HREF="000885.html">[Plearn-commits] r7436 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#884">[ date ]</a>
              <a href="thread.html#884">[ thread ]</a>
              <a href="subject.html#884">[ subject ]</a>
              <a href="author.html#884">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
