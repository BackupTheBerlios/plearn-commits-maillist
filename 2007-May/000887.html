<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7438 - in trunk/python_modules/plearn: .	learners/autolr
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7438%20-%20in%20trunk/python_modules/plearn%3A%20.%0A%09learners/autolr&In-Reply-To=%3C200705301856.l4UIuC6h027699%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000886.html">
   <LINK REL="Next"  HREF="000888.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7438 - in trunk/python_modules/plearn: .	learners/autolr</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7438%20-%20in%20trunk/python_modules/plearn%3A%20.%0A%09learners/autolr&In-Reply-To=%3C200705301856.l4UIuC6h027699%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7438 - in trunk/python_modules/plearn: .	learners/autolr">yoshua at mail.berlios.de
       </A><BR>
    <I>Wed May 30 20:56:12 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000886.html">[Plearn-commits] r7437 - trunk/plearn_learners_experimental
</A></li>
        <LI>Next message: <A HREF="000888.html">[Plearn-commits] r7439 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#887">[ date ]</a>
              <a href="thread.html#887">[ thread ]</a>
              <a href="subject.html#887">[ subject ]</a>
              <a href="author.html#887">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-05-30 20:56:12 +0200 (Wed, 30 May 2007)
New Revision: 7438

Added:
   trunk/python_modules/plearn/bridge.py
   trunk/python_modules/plearn/bridgemode.py
Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Added bridge and bridgemode to allow different behaviors
depending on whether using plearn server or plearn-as-python-extension


Added: trunk/python_modules/plearn/bridge.py
===================================================================
--- trunk/python_modules/plearn/bridge.py	2007-05-30 18:06:02 UTC (rev 7437)
+++ trunk/python_modules/plearn/bridge.py	2007-05-30 18:56:12 UTC (rev 7438)
@@ -0,0 +1,21 @@
+
+from plearn.bridgemode import *
+
+# whether we use the plearn server or python extension bridge
+# between PLearn and python
+#
+if useserver:
+    from plearn.pyplearn import *
+    from plearn.io.server import *
+    import time
+    server_command = 'plearn_curses server'
+    serv = launch_plearn_server(command = server_command)
+    time.sleep(0.5) # give some time to the server to get born and well alive
+else:
+    from plearn.pyext import *
+# from gdb do the following to see PLearn symbols AFTER pyext has been loaded
+# add-symbol-file ~/PLearn/python_modules/plearn/pyext/libplextdbg.so
+
+if interactive:
+    from pylab import *
+    

Added: trunk/python_modules/plearn/bridgemode.py
===================================================================
--- trunk/python_modules/plearn/bridgemode.py	2007-05-30 18:06:02 UTC (rev 7437)
+++ trunk/python_modules/plearn/bridgemode.py	2007-05-30 18:56:12 UTC (rev 7438)
@@ -0,0 +1,10 @@
+
+# whether we use the plearn server or python extension bridge
+# between PLearn and python
+
+useserver = False
+
+# whether we plot error curve graphs interactively (pylab)
+#
+
+interactive = False

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-05-30 18:06:02 UTC (rev 7437)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-05-30 18:56:12 UTC (rev 7438)
@@ -1,8 +1,12 @@
 from math import *
+from numarray import *
+from plearn.bridge import *
 
 # YET TO BE DOCUMENTED AND CLEANED UP CODE
 
-def deepcopy(plearnobject): # ugly way to copy until done properly with PLearn's deepcopy
+
+# ugly way to copy until done properly with PLearn's deepcopy
+def deepcopy(plearnobject):
     # actually not a deep-copy, only copy options
     if useserver:
         raise NotImplementedError
@@ -16,7 +20,8 @@
                         stages,         # corresponding list of stages associated with each above learning rate
                         trainset,testsets,expdir,
                         cost_to_select_best=0,
-                        selected_costnames = False):
+                        selected_costnames = False,
+                        logfile=False):
     costnames = learner.getTestCostNames()
     if not selected_costnames:
         # use all cost names if not user-provided
@@ -41,12 +46,14 @@
         for j in range(0,n_tests):
             ts = pl.VecStatsCollector()
             learner.test(testsets[j],ts,0,0)
-            print &gt;&gt;logfile, &quot;At stage &quot;,learner.stage,&quot; test&quot; + str(j+1),&quot;: &quot;,
+            if logfile:
+                print &gt;&gt;logfile, &quot;At stage &quot;,learner.stage,&quot; test&quot; + str(j+1),&quot;: &quot;,
             for k in range(0,n_costs):
                 err = ts.getStat(&quot;E[&quot;+str(k)+&quot;]&quot;)
                 results[i,j*n_costs+k+1]=err
                 costname = costnames[cost_indices[k]]
-                print &gt;&gt;logfile, costname, &quot;=&quot;, err,
+                if logfile:
+                    print &gt;&gt;logfile, costname, &quot;=&quot;, err,
                 if k==cost_to_select_best and j==0 and err &lt; best_err:
                     best_err = err
                     learner.save(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;,&quot;plearn_ascii&quot;)
@@ -54,7 +61,8 @@
                     plot(results[0:i+1,0],results[0:i+1,
                          j*n_costs+k+1],colors[k%7]+styles[j%15],
                          label='test'+str(j+1)+':'+costname)
-            print &gt;&gt;logfile
+            if logfile:
+                print &gt;&gt;logfile
         if interactive and i==0:
             legend()
     return (['stage']+selected_costnames,results)
@@ -65,6 +73,7 @@
                       cost_to_select_best=0,
                       initial_lr=0.01,
                       lr_steps=exp(log(10)/2)):
+
     log_initial_lr=log(initial_lr)
     log_steps=log(lr_steps)
 
@@ -103,7 +112,6 @@
                 best=bottom
     return (lr(best),perfs)
 
-    
 def train_adapting_lr(learner,
                       epoch,nstages,
                       trainset,testsets,expdir,
@@ -113,7 +121,26 @@
                       cost_to_select_best=0,
                       save_best=False,
                       selected_costnames = False,
-                      lr_steps=exp(log(10)/2)):
+                      min_epochs_to_delete = 2,
+                      lr_steps=exp(log(10)/2),
+                      logfile=False):
+
+    min_epochs_to_delete = max(1,min_epochs_to_delete) # although 1 is probably too small
+        
+    # used within the dominates function to predict future value of the error
+    def dominates(c1,c2,current_t):
+        &quot;&quot;&quot;c1 has a lower last error than c2, but
+           will c2 eventually cross c1? if yes return False o/w return True&quot;&quot;&quot;
+        start_t = max(all_start[c1],all_start[c2])
+        delta_t = current_t+1-start_t
+        curve1 = all_results[c1][start_t:current_t+1,cost_to_select_best]
+        curve2 = all_results[c2][start_t:current_t+1,cost_to_select_best]
+        # wait to have at least 3 points in curve =2 epochs since split between the candidates
+        if delta_t-1&lt;min_epochs_to_delete or curve1[-1]&gt;=curve2[-1]:
+            return False
+        return curve1[-1]-curve1[-2]  &lt; curve2[-1]-curve2[-2]
+        #return all_last_err[j]&gt;all_last_err[i] and all_slope[j]&gt;=all_slope[i]
+    
     costnames = learner.getTestCostNames()
     if not selected_costnames:
         # use all cost names if not user-provided
@@ -128,7 +155,7 @@
     all_results = [zeros([n_epochs,2+n_tests*n_costs],Float32)]
     all_candidates = [learner]
     all_last_err = [best_err]
-    all_slope = [0]
+    #all_slope = [0]
     all_lr = [initial_lr]
     all_start = [0]
     actives = [0]
@@ -138,7 +165,8 @@
         styles=['-', '--', '-.', ':', '.', ',', 'o', '^', 'v', '&lt;', '&gt;', 's', '+', 'x', 'D']
     initial_stage=learner.stage
     for (s,i) in zip(range(epoch,nstages+epoch,epoch),range(n_epochs)):
-        print &gt;&gt;logfile, &quot;At stage &quot;, initial_stage+s
+        if logfile:
+            print &gt;&gt;logfile, &quot;At stage &quot;, initial_stage+s
         for active in actives:
             candidate = all_candidates[active]
             results = all_results[active]
@@ -149,16 +177,19 @@
             candidate.train()
             results[i,0] = candidate.stage
             results[i,1] = all_lr[active]
-            print &gt;&gt;logfile, &quot;candidate &quot;,active,&quot;:&quot;,
+            if logfile:
+                print &gt;&gt;logfile, &quot;candidate &quot;,active,&quot;:&quot;,
             for j in range(0,n_tests):
                 ts = pl.VecStatsCollector()
                 candidate.test(testsets[j],ts,0,0)
-                print &gt;&gt;logfile, &quot; test&quot; + str(j+1),&quot;: &quot;,
+                if logfile:
+                    print &gt;&gt;logfile, &quot; test&quot; + str(j+1),&quot;: &quot;,
                 for k in range(0,n_costs):
                     err = ts.getStat(&quot;E[&quot;+str(k)+&quot;]&quot;)
                     results[i,j*n_costs+k+2]=err
                     costname = costnames[cost_indices[k]]
-                    print &gt;&gt;logfile, costname, &quot;=&quot;, err,
+                    if logfile:
+                        print &gt;&gt;logfile, costname, &quot;=&quot;, err,
                     if k==cost_to_select_best and j==0:
                         if interactive:
                             start = all_start[active]
@@ -167,28 +198,31 @@
                                  label='candidate'+str(active)+':'+costname)
                             if active==0 and i==0:
                                 legend()
-                        if s&gt;epoch:
-                                all_slope[active]=0.5*all_slope[active]+0.5*(err-all_last_err[active])
+                        #if s&gt;epoch:
+                        #        all_slope[active]=0.5*all_slope[active]+0.5*(err-all_last_err[active])
                         all_last_err[active]=err
                         if err &lt; best_err:
                             best_err = err
                             best_active = active
                             if save_best:
                                 candidate.save(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;,&quot;plearn_binary&quot;)
-            print &gt;&gt;logfile
-            logfile.flush()
+            if logfile:
+                print &gt;&gt;logfile
+                logfile.flush()
         if previous_best_err &gt; best_err:
             previous_best_err = best_err
-            print &gt;&gt;logfile,&quot;BEST to now is candidate &quot;,best_active,&quot; with err=&quot;,best_err
+            if logfile:
+                print &gt;&gt;logfile,&quot;BEST to now is candidate &quot;,best_active,&quot; with err=&quot;,best_err
         if s%(epoch*nskip)==0 and s&lt;nstages:
             best_active = argmin(all_last_err)
             # remove candidates that are worse and have higher slope
-            best_slope = all_slope[best_active]
+            #best_slope = all_slope[best_active]
             best_last = all_last_err[best_active]
             ndeleted = 0
             for (a,i) in zip(actives,range(len(actives))):
-                if a!=best_active and all_last_err[a]&gt;best_last and all_slope[a]&gt;=best_slope:
-                    print &gt;&gt;logfile,&quot;REMOVE candidate &quot;,a
+                if a!=best_active and dominates(best_active,a,i):
+                    if logfile:
+                        print &gt;&gt;logfile,&quot;REMOVE candidate &quot;,a
                     all_candidates[a]=None # hopefully this destroys the candidate
                     del actives[i-ndeleted]
                     ndeleted+=1
@@ -199,11 +233,12 @@
             all_candidates.append(new_candidate)
             all_results.append(all_results[best_active].copy())
             all_last_err.append(best_last)
-            all_slope.append(best_slope)
+            #all_slope.append(best_slope)
             all_lr.append(all_lr[best_active]/lr_steps) # always try a smaller learning rate
             all_start.append(i)
-            print &gt;&gt;logfile,&quot;CREATE candidate &quot;, new_i, &quot; from &quot;,best_active,&quot;at epoch &quot;,s,&quot; with lr=&quot;,all_lr[new_i]
-            logfile.flush()
+            if logfile:
+                print &gt;&gt;logfile,&quot;CREATE candidate &quot;, new_i, &quot; from &quot;,best_active,&quot;at epoch &quot;,s,&quot; with lr=&quot;,all_lr[new_i]
+                logfile.flush()
     if save_best:
         final_model = loadObject(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;)
     else:


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000886.html">[Plearn-commits] r7437 - trunk/plearn_learners_experimental
</A></li>
	<LI>Next message: <A HREF="000888.html">[Plearn-commits] r7439 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#887">[ date ]</a>
              <a href="thread.html#887">[ thread ]</a>
              <a href="subject.html#887">[ subject ]</a>
              <a href="author.html#887">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
