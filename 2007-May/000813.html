<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7364 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7364%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200705272208.l4RM89eV005523%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000812.html">
   <LINK REL="Next"  HREF="000814.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7364 - trunk/plearn_learners_experimental</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7364%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200705272208.l4RM89eV005523%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7364 - trunk/plearn_learners_experimental">larocheh at mail.berlios.de
       </A><BR>
    <I>Mon May 28 00:08:09 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000812.html">[Plearn-commits] r7363 - trunk/plearn_learners/generic
</A></li>
        <LI>Next message: <A HREF="000814.html">[Plearn-commits] r7365 - in	trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest:	PL_badly_conditioned_linreg_script/expected_results	PL_badly_conditioned_linreg_script/expected_results/expdir	PL_badly_conditioned_linreg_script/expected_results/expdir/Split0	PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_confidence.pmat.metadata	PL_stacked_pca_regression_script/expected_results	PL_stacked_pca_regression_script/expected_results/expdir	PL_stacked_pca_regression_script/expected_results/expdir/Split0	PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_confidence.pmat.metadata
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#813">[ date ]</a>
              <a href="thread.html#813">[ thread ]</a>
              <a href="subject.html#813">[ subject ]</a>
              <a href="author.html#813">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-05-28 00:08:09 +0200 (Mon, 28 May 2007)
New Revision: 7364

Modified:
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h
Log:
Added a model that uses a shared learned representation of the input (which is simply the hidden layer of a neural net).


Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-05-27 21:22:33 UTC (rev 7363)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2007-05-27 22:08:09 UTC (rev 7364)
@@ -63,14 +63,17 @@
 //#include &lt;plearn/var/PowVariable.h&gt;
 #include &lt;plearn/var/ProductTransposeVariable.h&gt;
 #include &lt;plearn/var/ProductVariable.h&gt;
+#include &lt;plearn/var/ReshapeVariable.h&gt;
 #include &lt;plearn/var/SigmoidVariable.h&gt;
 #include &lt;plearn/var/SoftmaxVariable.h&gt;
 #include &lt;plearn/var/SumVariable.h&gt;
 #include &lt;plearn/var/SumAbsVariable.h&gt;
 #include &lt;plearn/var/SumOfVariable.h&gt;
 #include &lt;plearn/var/SumSquareVariable.h&gt;
+#include &lt;plearn/var/TanhVariable.h&gt;
+#include &lt;plearn/var/TimesVariable.h&gt;
 #include &lt;plearn/var/TransposeVariable.h&gt;
-//#include &lt;plearn/var/TransposeProductVariable.h&gt;
+#include &lt;plearn/var/TransposeProductVariable.h&gt;
 #include &lt;plearn/var/VarRowsVariable.h&gt;
 #include &lt;plearn/var/Var_operators.h&gt;
 #include &lt;plearn/var/Var_utils.h&gt;
@@ -97,56 +100,79 @@
       dont_consider_train_targets(false),
       use_bias_in_weights_prediction(false),
       multi_target_classifier(false),
-      sigma_min(1e-5)
+      sigma_min(1e-5),
+      nhidden(-1)
 {
     random_gen = new PRandom();
 }
 
 void LinearInductiveTransferClassifier::declareOptions(OptionList&amp; ol)
 {
-    declareOption(ol, &quot;optimizer&quot;, &amp;LinearInductiveTransferClassifier::optimizer, OptionBase::buildoption,
+    declareOption(ol, &quot;optimizer&quot;, &amp;LinearInductiveTransferClassifier::optimizer, 
+                  OptionBase::buildoption,
                   &quot;Optimizer of the discriminative classifier&quot;);
-    declareOption(ol, &quot;batch_size&quot;, &amp;LinearInductiveTransferClassifier::batch_size, OptionBase::buildoption, 
+    declareOption(ol, &quot;batch_size&quot;, &amp;LinearInductiveTransferClassifier::batch_size,
+                  OptionBase::buildoption, 
                   &quot;How many samples to use to estimate the avergage gradient before updating the weights\n&quot;
                   &quot;0 is equivalent to specifying training_set-&gt;length() \n&quot;);
-    declareOption(ol, &quot;weight_decay&quot;, &amp;LinearInductiveTransferClassifier::weight_decay, OptionBase::buildoption, 
+    declareOption(ol, &quot;weight_decay&quot;, 
+                  &amp;LinearInductiveTransferClassifier::weight_decay, 
+                  OptionBase::buildoption, 
                   &quot;Global weight decay for all layers\n&quot;);
-    declareOption(ol, &quot;model_type&quot;, &amp;LinearInductiveTransferClassifier::model_type, OptionBase::buildoption, 
+    declareOption(ol, &quot;model_type&quot;, &amp;LinearInductiveTransferClassifier::model_type,
+                  OptionBase::buildoption, 
                   &quot;Model type. Choose between:\n&quot;
-                  &quot; - \&quot;discriminative\&quot;             (multiclass classifier)\n&quot;
-                  &quot; - \&quot;discriminative_1_vs_all\&quot;    (1 vs all multitask classier)\n&quot;
-                  &quot; - \&quot;generative\&quot;                 (gaussian input)\n&quot;
-                  &quot; - \&quot;generative_0-1\&quot;             ([0,1] input)\n&quot;
+                  &quot; - \&quot;discriminative\&quot;               (multiclass classifier)\n&quot;
+                  &quot; - \&quot;discriminative_1_vs_all\&quot;      (1 vs all multitask classier)\n&quot;
+                  &quot; - \&quot;generative\&quot;                   (gaussian input)\n&quot;
+                  &quot; - \&quot;generative_0-1\&quot;               ([0,1] input)\n&quot;
+                  &quot; - \&quot;nnet_discriminative_1_vs_all\&quot; ([0,1] input)\n&quot;
         );
-    declareOption(ol, &quot;penalty_type&quot;, &amp;LinearInductiveTransferClassifier::penalty_type,
+    declareOption(ol, &quot;penalty_type&quot;, 
+                  &amp;LinearInductiveTransferClassifier::penalty_type,
                   OptionBase::buildoption,
                   &quot;Penalty to use on the weights (for weight and bias decay).\n&quot;
                   &quot;Can be any of:\n&quot;
                   &quot;  - \&quot;L1\&quot;: L1 norm,\n&quot;
                   &quot;  - \&quot;L1_square\&quot;: square of the L1 norm,\n&quot;
                   &quot;  - \&quot;L2_square\&quot; (default): square of the L2 norm.\n&quot;);
-    declareOption(ol, &quot;initialization_method&quot;, &amp;LinearInductiveTransferClassifier::initialization_method, OptionBase::buildoption, 
+    declareOption(ol, &quot;initialization_method&quot;, 
+                  &amp;LinearInductiveTransferClassifier::initialization_method, 
+                  OptionBase::buildoption, 
                   &quot;The method used to initialize the weights:\n&quot;
                   &quot; - \&quot;normal_linear\&quot;  = a normal law with variance 1/n_inputs\n&quot;
                   &quot; - \&quot;normal_sqrt\&quot;    = a normal law with variance 1/sqrt(n_inputs)\n&quot;
                   &quot; - \&quot;uniform_linear\&quot; = a uniform law in [-1/n_inputs, 1/n_inputs]\n&quot;
                   &quot; - \&quot;uniform_sqrt\&quot;   = a uniform law in [-1/sqrt(n_inputs), 1/sqrt(n_inputs)]\n&quot;
                   &quot; - \&quot;zero\&quot;           = all weights are set to 0\n&quot;);    
-    declareOption(ol, &quot;paramsvalues&quot;, &amp;LinearInductiveTransferClassifier::paramsvalues, OptionBase::learntoption, 
+    declareOption(ol, &quot;paramsvalues&quot;, 
+                  &amp;LinearInductiveTransferClassifier::paramsvalues, 
+                  OptionBase::learntoption, 
                   &quot;The learned parameters\n&quot;);
-    declareOption(ol, &quot;class_reps&quot;, &amp;LinearInductiveTransferClassifier::class_reps, OptionBase::buildoption, 
+    declareOption(ol, &quot;class_reps&quot;, &amp;LinearInductiveTransferClassifier::class_reps,
+                  OptionBase::buildoption, 
                   &quot;Class vector representations\n&quot;);
-    declareOption(ol, &quot;dont_consider_train_targets&quot;, &amp;LinearInductiveTransferClassifier::dont_consider_train_targets, OptionBase::buildoption, 
+    declareOption(ol, &quot;dont_consider_train_targets&quot;, 
+                  &amp;LinearInductiveTransferClassifier::dont_consider_train_targets, 
+                  OptionBase::buildoption, 
                   &quot;Indication that the targets seen in the training set\n&quot;
                   &quot;should not be considered when tagging a new set\n&quot;);
-    declareOption(ol, &quot;use_bias_in_weights_prediction&quot;, &amp;LinearInductiveTransferClassifier::use_bias_in_weights_prediction, OptionBase::buildoption, 
+    declareOption(ol, &quot;use_bias_in_weights_prediction&quot;, 
+                  &amp;LinearInductiveTransferClassifier::use_bias_in_weights_prediction, 
+                  OptionBase::buildoption, 
                   &quot;Indication that a bias should be used for weights prediction\n&quot;);
-    declareOption(ol, &quot;multi_target_classifier&quot;, &amp;LinearInductiveTransferClassifier::multi_target_classifier, OptionBase::buildoption, 
+    declareOption(ol, &quot;multi_target_classifier&quot;, 
+                  &amp;LinearInductiveTransferClassifier::multi_target_classifier, 
+                  OptionBase::buildoption, 
                   &quot;Indication that the classifier works with multiple targets,\n&quot;
                   &quot;possibly ON simulatneously.\n&quot;);
-    declareOption(ol, &quot;sigma_min&quot;, &amp;LinearInductiveTransferClassifier::sigma_min, OptionBase::buildoption, 
+    declareOption(ol, &quot;sigma_min&quot;, &amp;LinearInductiveTransferClassifier::sigma_min, 
+                  OptionBase::buildoption, 
                   &quot;Minimum variance for all coordinates, which is added\n&quot;
                   &quot;to the maximum likelihood estimates.\n&quot;);
+    declareOption(ol, &quot;nhidden&quot;, &amp;LinearInductiveTransferClassifier::nhidden, 
+                  OptionBase::buildoption, 
+                  &quot;Number of hidden units for neural network.&quot;);
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -192,19 +218,49 @@
         {
             class_reps_to_use = class_reps;
         }
+                
 
-        A = Var(inputsize_,class_reps_to_use.width());
-        //fillWeights(A,false);        
-        A-&gt;value.fill(0);
-        s = Var(1,inputsize_,&quot;sigma_square&quot;);
-        s-&gt;value.fill(1);
-        params.push_back(A);
-        params.push_back(s);
+        if(model_type == &quot;nnet_discriminative_1_vs_all&quot;)
+        {
+            if(nhidden &lt;= 0)
+                PLERROR(&quot;In LinearInductiveTransferClassifier::build_(): nhidden &quot;
+                        &quot;must be &gt; 0.&quot;);
+//            Ws.resize(nhidden); 
+//            As.resize(nhidden);
+//            s_hids.resize(nhidden);
+//            s = Var(1,nhidden,&quot;sigma_square&quot;);
+//            for(int i=0; i&lt;Ws.length(); i++)
+//            {
+//                Ws[i] = Var(inputsize_,class_reps_to_use.width());
+//                As[i] = Var(1,class_reps_to_use.width());
+//                s_hids[i] = Var(1,inputsize_);
+//            }
+            W = Var(inputsize_+1,nhidden,&quot;hidden_weights&quot;);
+            A = Var(nhidden,class_reps_to_use.width());
+            s = Var(1,nhidden,&quot;sigma_square&quot;);
+            params.push_back(W);
+            params.push_back(A);
+            params.push_back(s);
+//            params.append(Ws);
+//            params.append(As);
+//            params.append(s);
+//            params.append(s_hids);
+//            A = vconcat(As);
+        }
+        else
+        {
+            A = Var(inputsize_,class_reps_to_use.width());
+            s = Var(1,inputsize_,&quot;sigma_square&quot;);
+            //fillWeights(A,false);     
+            params.push_back(A);
+            params.push_back(s);        
+        }
         
+
         class_reps_var = new SourceVariable(class_reps_to_use);
         Var weights = productTranspose(A,class_reps_var);
         if(model_type == &quot;discriminative&quot; || model_type == &quot;discriminative_1_vs_all&quot;)
-        {
+        { 
             weights =vconcat(-product(exp(s),square(weights)) &amp; weights); // Making sure that the scaling factor is going to be positive
             output = affine_transform(input, weights);
         }
@@ -216,12 +272,35 @@
         }
         else if(model_type == &quot;generative&quot;)
         {
-            weights =vconcat(-columnSum(square(weights)/transpose(duplicateRow(s,noutputs))) &amp; 2*weights/transpose(duplicateRow(s,noutputs)));
+            weights = vconcat(-columnSum(square(weights)/transpose(duplicateRow(s,noutputs))) &amp; 2*weights/transpose(duplicateRow(s,noutputs)));
             if(targetsize() == 1)
                 output = affine_transform(input, weights);
             else
                 output = exp(affine_transform(input, weights) - duplicateRow(dot(transpose(input)/s,input),noutputs))+REAL_EPSILON;
         }
+        else if(model_type == &quot;nnet_discriminative_1_vs_all&quot;)
+        {
+            //hidden_neurons.resize(nhidden);
+            //Var weights;
+            //for(int i=0; i&lt;nhidden; i++)
+            //{
+            //    weights = productTranspose(Ws[i],class_reps_var);
+            //    weights = vconcat(-product(exp(s_hids[i]),square(weights)) 
+            //                      &amp; weights); 
+            //    hidden_neurons[i] = tanh(affine_transform(input, weights));
+            //}
+            //
+            //weights = productTranspose(A,class_reps_var);
+            //output = -transpose(product(exp(s),square(weights)));
+            //
+            //for(int i=0; i&lt;nhidden; i++)
+            //{
+            //    output = output + times(productTranspose(class_reps_var,As[i]),
+            //                   hidden_neurons[i]);
+            //}
+            weights =vconcat(-product(exp(s),square(weights)) &amp; weights); // Making sure that the scaling factor is going to be positive
+            output = affine_transform(tanh(affine_transform(input,W)), weights);
+        }
         else
             PLERROR(&quot;In LinearInductiveTransferClassifier::build_(): model_type %s is not valid&quot;, model_type.c_str());
 
@@ -297,7 +376,7 @@
         }
 
         // Build costs
-        if(model_type == &quot;discriminative&quot; || model_type == &quot;discriminative_1_vs_all&quot; || model_type == &quot;generative_0-1&quot;)
+        if(model_type == &quot;discriminative&quot; || model_type == &quot;discriminative_1_vs_all&quot; || model_type == &quot;generative_0-1&quot; || model_type == &quot;nnet_discriminative_1_vs_all&quot;)
         {
             if(model_type == &quot;discriminative&quot;)
             {
@@ -312,7 +391,8 @@
                 new_costs[0] = neg_log_pi(new_output,new_target);
                 new_costs[1] = classification_loss(new_output, new_target);
             }
-            if(model_type == &quot;discriminative_1_vs_all&quot;)
+            if(model_type == &quot;discriminative_1_vs_all&quot; 
+               || model_type == &quot;nnet_discriminative_1_vs_all&quot;)
             {
                 costs.resize(2);
                 new_costs.resize(2);
@@ -481,6 +561,12 @@
     varDeepCopyField(s, copies);
     varDeepCopyField(class_reps_var, copies);
 
+    deepCopyField(W, copies);
+    //deepCopyField(As, copies);
+    //deepCopyField(Ws, copies);
+    //deepCopyField(s_hids, copies);
+    //deepCopyField(hidden_neurons, copies);
+
     varDeepCopyField(input, copies);
     varDeepCopyField(output, copies);
     varDeepCopyField(sup_output, copies);
@@ -510,9 +596,27 @@
         optimizer-&gt;reset();
     stage = 0;
     
-    fillWeights(A,false);
-    s-&gt;value.fill(1);
- 
+    if(model_type == &quot;nnet_discriminative_1_vs_all&quot;)
+    {
+//        for(int i=0; i&lt;Ws.length(); i++)
+//        {
+//            fillWeights(Ws[i],false,1./(inputsize_*class_reps.width()));
+//            fillWeights(As[i],false,1./(nhidden*class_reps.width()));
+//            s_hids[i]-&gt;value.fill(1);
+//        }
+        fillWeights(W,true);
+        fillWeights(A,false,1./(nhidden*class_reps.width()));
+        s-&gt;value.fill(1);
+    }
+    else
+    {
+        //A = Var(inputsize_,class_reps_to_use.width());
+        A-&gt;value.fill(0);
+        s-&gt;value.fill(1);
+    }
+
+    // Might need to recompute proppaths (if number of task representations changed
+    // for instance)
     build();
 }
     
@@ -529,7 +633,7 @@
     if(f.isNull()) // Net has not been properly built yet (because build was called before the learner had a proper training set)
         build();
     
-    if(model_type == &quot;discriminative&quot; || model_type == &quot;discriminative_1_vs_all&quot; || model_type == &quot;generative_0-1&quot;)
+    if(model_type == &quot;discriminative&quot; || model_type == &quot;discriminative_1_vs_all&quot; || model_type == &quot;generative_0-1&quot; || model_type == &quot;nnet_discriminative_1_vs_all&quot;)
     {
         // number of samples seen by optimizer before each optimizer update
         int nsamples = batch_size&gt;0 ? batch_size : l;
@@ -752,7 +856,7 @@
 TVec&lt;string&gt; LinearInductiveTransferClassifier::getTestCostNames() const
 {
     TVec&lt;string&gt; costs_str;
-    if(model_type == &quot;discriminative&quot; || model_type == &quot;discriminative_1_vs_all&quot; || model_type == &quot;generative_0-1&quot;)
+    if(model_type == &quot;discriminative&quot; || model_type == &quot;discriminative_1_vs_all&quot; || model_type == &quot;generative_0-1&quot; || model_type == &quot;nnet_discriminative_1_vs_all&quot;)
     {
         if(model_type == &quot;discriminative&quot; || model_type == &quot;generative_0-1&quot;)
         {
@@ -760,7 +864,8 @@
             costs_str[0] = &quot;NLL&quot;;
             costs_str[1] = &quot;class_error&quot;;
         }
-        if(model_type == &quot;discriminative_1_vs_all&quot;)
+        if(model_type == &quot;discriminative_1_vs_all&quot; 
+           || model_type == &quot;nnet_discriminative_1_vs_all&quot;)
         {
             costs_str.resize(1);
             costs_str[0] = &quot;cross_entropy&quot;;
@@ -815,29 +920,47 @@
     penalties.resize(0);  // prevents penalties from being added twice by consecutive builds
     if(weight_decay &gt; 0)
     {
-        penalties.append(affine_transform_weight_penalty(A, weight_decay, (use_bias_in_weights_prediction ? 0 : weight_decay), penalty_type));
+        if(model_type == &quot;nnet_discriminative_1_vs_all&quot;)
+        {
+            //for(int i=0; i&lt;Ws.length(); i++)
+            //{
+            //    penalties.append(affine_transform_weight_penalty(Ws[i], weight_decay, weight_decay, penalty_type));
+            //}
+            penalties.append(affine_transform_weight_penalty(W, weight_decay, 0, penalty_type));
+        }
+        
+        penalties.append(affine_transform_weight_penalty(A, weight_decay, weight_decay, penalty_type));
     }
 }
 
-void LinearInductiveTransferClassifier::fillWeights(const Var&amp; weights, bool fill_first_row, real fill_with_this) {
+void LinearInductiveTransferClassifier::fillWeights(const Var&amp; weights, 
+                                                    bool zero_first_row, 
+                                                    real scale_with_this) {
     if (initialization_method == &quot;zero&quot;) {
         weights-&gt;value-&gt;clear();
         return;
     }
     real delta;
-    int is = weights.length();
-    if (fill_first_row)
-        is--; // -1 to get the same result as before.
-    if (initialization_method.find(&quot;linear&quot;) != string::npos)
-        delta = 1.0 / real(is);
+    if(scale_with_this &lt; 0)
+    {
+        int is = weights.length();
+        if (zero_first_row)
+            is--; // -1 to get the same result as before.
+        if (initialization_method.find(&quot;linear&quot;) != string::npos)
+            delta = 1.0 / real(is);
+        else
+            delta = 1.0 / sqrt(real(is));
+    }
     else
-        delta = 1.0 / sqrt(real(is));
+        delta = scale_with_this;
+
     if (initialization_method.find(&quot;normal&quot;) != string::npos)
         random_gen-&gt;fill_random_normal(weights-&gt;value, 0, delta);
     else
         random_gen-&gt;fill_random_uniform(weights-&gt;value, -delta, delta);
-    if (fill_first_row)
-        weights-&gt;matValue(0).fill(fill_with_this);
+
+    if(zero_first_row)
+        weights-&gt;matValue(0).clear();
 }
 
 void LinearInductiveTransferClassifier::buildFuncs(const Var&amp; the_input, const Var&amp; the_output, const Var&amp; the_target, const Var&amp; the_sampleweight){

Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h	2007-05-27 21:22:33 UTC (rev 7363)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.h	2007-05-27 22:08:09 UTC (rev 7364)
@@ -89,6 +89,8 @@
     //! Minimum variance for all coordinates, which is added
     //! to the maximum likelihood estimates.
     real sigma_min;
+    //! Number of hidden units for neural network
+    int nhidden;
 
 public:
     //#####  Public Member Functions  #########################################
@@ -212,6 +214,17 @@
     //! Function: output &amp; target -&gt; cost
     mutable Func sup_output_and_target_to_cost; 
 
+    // Neural networks variables
+    //! Parameters for hidden to output layer weights prediction
+    VarArray As;
+    //! Parameters for input to hidden layer weights prediction
+    VarArray Ws;
+    //! Input to hidden layer weights 
+    Var W;
+    //! Scale parameter for input to hidden layer weights prediction
+    VarArray s_hids;
+    //! Hidden layer neurons
+    VarArray hidden_neurons;
     
 protected:
     //#####  Protected Member Functions  ######################################
@@ -239,7 +252,8 @@
     //! Fill a matrix of weights according to the 'initialization_method' specified.
     //! The 'clear_first_row' boolean indicates whether we should fill the first
     //! row with zeros.
-    void fillWeights(const Var&amp; weights, bool fill_first_row, real fill_with_this=0);
+    void fillWeights(const Var&amp; weights, bool zero_first_row, 
+                     real scale_with_this=-1);
 
     //! Fill the costs penalties.
     virtual void buildPenalties();


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000812.html">[Plearn-commits] r7363 - trunk/plearn_learners/generic
</A></li>
	<LI>Next message: <A HREF="000814.html">[Plearn-commits] r7365 - in	trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest:	PL_badly_conditioned_linreg_script/expected_results	PL_badly_conditioned_linreg_script/expected_results/expdir	PL_badly_conditioned_linreg_script/expected_results/expdir/Split0	PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_confidence.pmat.metadata	PL_stacked_pca_regression_script/expected_results	PL_stacked_pca_regression_script/expected_results/expdir	PL_stacked_pca_regression_script/expected_results/expdir/Split0	PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_confidence.pmat.metadata
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#813">[ date ]</a>
              <a href="thread.html#813">[ thread ]</a>
              <a href="subject.html#813">[ subject ]</a>
              <a href="author.html#813">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
