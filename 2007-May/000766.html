<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7317 - trunk/python_modules/plearn/var
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7317%20-%20trunk/python_modules/plearn/var&In-Reply-To=%3C200705242023.l4OKNCpu005688%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000765.html">
   <LINK REL="Next"  HREF="000767.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7317 - trunk/python_modules/plearn/var</H1>
    <B>plearner at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7317%20-%20trunk/python_modules/plearn/var&In-Reply-To=%3C200705242023.l4OKNCpu005688%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7317 - trunk/python_modules/plearn/var">plearner at mail.berlios.de
       </A><BR>
    <I>Thu May 24 22:23:12 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000765.html">[Plearn-commits] r7316 - trunk/plearn_learners/distributions
</A></li>
        <LI>Next message: <A HREF="000767.html">[Plearn-commits] r7318 -	trunk/plearn_learners/distributions/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#766">[ date ]</a>
              <a href="thread.html#766">[ thread ]</a>
              <a href="subject.html#766">[ subject ]</a>
              <a href="author.html#766">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: plearner
Date: 2007-05-24 22:23:11 +0200 (Thu, 24 May 2007)
New Revision: 7317

Modified:
   trunk/python_modules/plearn/var/Var.py
Log:
Additions/changes to the (still under development) Var module


Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-05-24 20:12:43 UTC (rev 7316)
+++ trunk/python_modules/plearn/var/Var.py	2007-05-24 20:23:11 UTC (rev 7317)
@@ -56,6 +56,9 @@
     def sigmoid(self):
         return Var(pl.SigmoidVariable(input=self.v))
 
+    def tanh(self):
+        return Var(pl.TanhVariable(input=self.v))
+
     def plearn_repr( self, indent_level=0, inner_repr=plearn_repr ):
         # asking for plearn_repr could be to send specification over
         # to another prg so that will open the .pmat
@@ -65,21 +68,49 @@
     def probabilityPairs(self, min, max):
         return Var(pl.ProbabilityPairsVariable(input=self.v, min=min, max=max))
 
-    def matrixProduct(self, W):
-        return Var(pl.ProductVariable(self.v, W))
+    def transpose(self):
+        return Var(pl.TransposeVariable(input=self.v))
 
-    def matrixTransposeProduct(self, W):
-        return Var(pl.TransposeProductVariable(self.v, W))
+    def matrixProduct(self, other):
+        return self.matrixProduct_A_B(other)
+    
+    def matrixProduct_A_B(self, other):
+        return Var(pl.ProductVariable(input1=self.v, input2=other.v))
 
+    def matrixProduct_At_B(self, other):
+        return Var(pl.TransposeProductVariable(input1=self.v, input2=other.v))
+
+    def matrixProduct_A_Bt(self, other):
+        return Var(pl.ProductTransposeVariable(input1=self.v, input2=other.v))
+
+    def matrixProduct_At_Bt(self, other):
+        return other.matrixProduct(self).transpose()         
+
     def affineTransform(self, W):
         return Var(pl.AffineTransformVariable(self.v, W))
 
+    # TODO: verifier si NegCrossEntropySigmoidVariable est bien ce qui est utilise par Hugo
+    def negCrossEntropySigmoid(self, other, regularizer=0., ignore_missing=False):
+        return Var(pl.NegCrossEntropySigmoidVariable(input1=self.v, input2=other.v, regularizer=regularizer, ignore_missing=ignore_missing))
+        
     def doubleProduct(self, W, M):
         return Var(pl.DoubleProductVariable(varray=[self.v, W, M]))
 
-    def dot(self, input):
-        return Var(pl.DotProductVariable(input1=self, input2=input))
+    def logSoftMax(self):
+        return Var(pl.LogSoftmaxVariable(input=self.v))
 
+    def getElement(self, index):
+        &quot;&quot;&quot;Returns a Var that selects an element from the current Var
+        index must be another scalar variable whose value will be
+        interpreted as an integer index of the element to select.&quot;&quot;&quot;
+        return Var(pl.VarElementVariable(input1=self.v, input2=index.v))
+
+    def __getitem__(self, index):
+        return self.getElement(index)
+
+    def dot(self, other):
+        return Var(pl.DotProductVariable(input1=self.v, input2=other.v))
+
     def multiMax(self, igs, computation_type):        
         return Var(pl.MultiMaxVariable(input=self.v, groupsize=igs, computation_type=PLChar(computation_type)))
 
@@ -95,61 +126,96 @@
     def square(self):
         return Var(pl.SquareVariable(input=self.v))
 
-    def __add__(self, other):
+    def add(self, other):
         return Var(pl.PlusVariable(input1=self.v, input2=other.v))
 
+    def __add__(self, other):
+        return self.add(other)
+
     def __sub__(self, other):
         return Var(pl.MinusVariable(input1=self.v,input2=other.v))
 
+    def neg(self):
+        return Var(pl.NegateElementsVariable(input=self.v))
+
     def __neg__(self):
-        return Var(pl.NegateElementsVariable(input=self.v))
+        return self.neg()
     
-    
 
-    
+###################################################    
 # RLayer stands for reconstruciton hidden layer
 
-def addSigmoidRLayer(input, iw, ow):
-    &quot;&quot;&quot;Returns a triple (hidden, reconstruciton_cost, params)&quot;&quot;&quot;
-    W = Var(1+iw,ow)
-    hidden = input.affineTransform(W).sigmoid()
-    cost = -hidden.matrixTransposeProduct(W).log_sigmoid().dot(input)
-    return (hidden, cost, W)
+def addSigmoidTiedRLayer(input, iw, ow, add_bias=True, basename=&quot;&quot;):
+    &quot;&quot;&quot;This assumes the input is a (1,iw) matrix,
+    and will produce an output that will be a (1, ow) matrix.
+    It will create parameter W(ow,iw)
+    and an optional parameter b(1,ow)
+    Then output = sigmoid(input.W^T + b)
+    
+    Returns a triple (hidden, reconstruciton_cost)&quot;&quot;&quot;
+    W = Var(ow,iw,&quot;uniform&quot;, -1./iw, varname=basename+'_W')
+    
+    if add_bias:
+        b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
+        hidden = input.matrixProduct_A_Bt(W).add(b).sigmoid()
+        br = Var(1,iw,&quot;fill&quot;,0, varname=basename+'_br')
+        cost = hidden.matrixProduct(W).add(br).negCrossEntropySigmoid(input)
+    else:
+        hidden = input.matrixProduct_A_Bt(W).sigmoid()
+        cost = hidden.matrixProduct(W).negCrossEntropySigmoid(input)
+    return hidden, cost
 
-
-
-
 def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
     &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output&quot;&quot;&quot;
-    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1/iw, 1/iw, False, varname=basename+&quot;_M&quot;)
-    W = Var(ogs, iw, &quot;uniform&quot;, -1/iw, 1/iw, False, varname=basename+&quot;_W&quot;)
+    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_M&quot;)
+    W = Var(ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_W&quot;)
     hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
     cost = -hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs).dot(input)            
-    return hidden, cost, (W,M)
+    return hidden, cost
 
-def addMultiSoftMaxDoubleProductNotTiedRLayer(input, iw, igs, ow, ogs):
-    return 1,2,3
+def addMultiSoftMaxDoubleProductRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
+    &quot;&quot;&quot;iw is the input's width
+    igs is the input's group size
+    ow and ogs analog but for output&quot;&quot;&quot;
+    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_M&quot;)
+    W = Var(ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_W&quot;)
+    hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
+    Mr = Var(iw/igs, ow, &quot;uniform&quot;, -1./ow, 1./ow, False, varname=basename+&quot;_Mr&quot;)
+    Wr = Var(igs, ow, &quot;uniform&quot;, -1./ow, 1./ow, False, varname=basename+&quot;_Wr&quot;)
+    cost = -hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs).dot(input)       
+    return hidden, cost
 
-def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs):
-    W = Var(iw, ow)
-    hidden = input.matrixProduct(W).multiSoftMax(ogs)
-    cost = -hidden.matrixTransposeProduct(W).multiLogSoftMax(igs).dot(input)
-    return hidden, cost, W
+def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
+    W = Var(ow, iw, &quot;uniform&quot;, -1./iw, varname=basename+'_W')
+    hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
+    cost = -hidden.matrixProduct(W).multiLogSoftMax(igs).dot(input)
+    return hidden, cost
 
-def addMultiSoftMaxSimpleProductNotTiedRLayer(input, iw, igs, ow, ogs):
-    W = Var(iw,ow)
-    hidden = input.matrixProduct(W).multiSoftMax(ogs)
-    Wr = Var(ow, iw)
-    cost = -hidden.matrixProduct(Wr).multiLogSoftMax(igs)
-    return hidden, cost, (W, Wr)
+def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs):
+    W = Var(ow, iw, &quot;uniform&quot;, -1./iw, varname=basename+'_W')
+    hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
+    Wr = Var(ow, iw, &quot;uniform&quot;, -1./ow, varname=basename+'_Wr')
+    cost = -hidden.matrixProduct(Wr).multiLogSoftMax(igs).dot(input)
+    return hidden, cost
 
+#################################
+# These build supervised layers
 
+def addClassificationNegLogSoftmaxLayer(activation, target):
+    &quot;&quot;&quot;Assumes that target is a scalar variable containing a class number between 0 and m-1,
+    and activation is a m-dimensional vector of class scores (reals).
+    Returns a pair of output, cost variables where:
+      output = log(sotmax(activation))
+      cost = -output[target]
+      &quot;&quot;&quot;
+    output = activation.logSoftMax()
+    cost = -output[target]
+    return output, cost
 
 
 
-
 ## def addMultiSoftmaxRLayer(input, ing, igs, ong, ogs, tied=True, use_double_product=True):
 ##     &quot;&quot;&quot;ing is the input's number of groups
 ##     igs is the input's group size


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000765.html">[Plearn-commits] r7316 - trunk/plearn_learners/distributions
</A></li>
	<LI>Next message: <A HREF="000767.html">[Plearn-commits] r7318 -	trunk/plearn_learners/distributions/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#766">[ date ]</a>
              <a href="thread.html#766">[ thread ]</a>
              <a href="subject.html#766">[ subject ]</a>
              <a href="author.html#766">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
