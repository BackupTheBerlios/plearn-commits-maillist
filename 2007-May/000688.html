<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7239 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7239%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705221748.l4MHmVSl024579%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000687.html">
   <LINK REL="Next"  HREF="000689.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7239 - trunk/plearn_learners/online</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7239%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705221748.l4MHmVSl024579%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7239 - trunk/plearn_learners/online">larocheh at mail.berlios.de
       </A><BR>
    <I>Tue May 22 19:48:31 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000687.html">[Plearn-commits] r7238 - trunk/scripts
</A></li>
        <LI>Next message: <A HREF="000689.html">[Plearn-commits] r7240 - trunk/scripts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#688">[ date ]</a>
              <a href="thread.html#688">[ thread ]</a>
              <a href="subject.html#688">[ subject ]</a>
              <a href="author.html#688">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-05-22 19:48:25 +0200 (Tue, 22 May 2007)
New Revision: 7239

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnection.h
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
Log:
Modification to include reconstruction cost in RBMModule. Modifications to RBMMatrixConnection where also made to be able to deal with conditional weight connections.   


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-05-22 17:48:25 UTC (rev 7239)
@@ -365,6 +365,20 @@
     }
 }
 
+void RBMBinomialLayer::bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                                Mat&amp; bias_gradients)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+
+    substract(targets,expectations,bias_gradients);
+}
+
 void RBMBinomialLayer::declareOptions(OptionList&amp; ol)
 {
 /*

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-05-22 17:48:25 UTC (rev 7239)
@@ -114,6 +114,8 @@
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient);
+    virtual void bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                          Mat&amp; bias_gradients);
 
     //! compute bias' unit_values
     virtual real energy(const Vec&amp; unit_values) const;

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-05-22 17:48:25 UTC (rev 7239)
@@ -322,6 +322,12 @@
     PLERROR(&quot;In RBMLayer::bpropNLL(): not implemented&quot;);
 }
 
+void RBMLayer::bpropNLL(const Mat&amp; targets,  const Mat&amp; costs_column, 
+                        Mat&amp; bias_gradients)
+{
+    PLERROR(&quot;In RBMLayer::bpropNLL(): not implemented&quot;);
+}
+
 ////////////////////////
 // accumulatePosStats //
 ////////////////////////

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-05-22 17:48:25 UTC (rev 7239)
@@ -195,6 +195,8 @@
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient);
+    virtual void bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                          Mat&amp; bias_gradients);
 
     //! Accumulates positive phase statistics
     virtual void accumulatePosStats( const Vec&amp; pos_values );
@@ -247,6 +249,7 @@
 
     //! Computes the contrastive divergence gradient with respect to the bias
     //! (or activations, which is equivalent).
+    //! It should be noted that bpropCD does not call clearstats().
     virtual void bpropCD(Vec&amp; bias_gradient);
 
     //! Computes the contrastive divergence gradient with respect to the bias

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-05-22 17:48:25 UTC (rev 7239)
@@ -60,7 +60,7 @@
 {
     declareOption(ol, &quot;weights&quot;, &amp;RBMMatrixConnection::weights,
                   OptionBase::learntoption,
-                  &quot;Matrix containing unit-to-unit weights (up_size &#215;&quot;
+                  &quot;Matrix containing unit-to-unit weights (up_size  x&quot;
                   &quot; down_size)&quot;);
 
     declareOption(ol, &quot;gibbs_ma_schedule&quot;, &amp;RBMMatrixConnection::gibbs_ma_schedule,
@@ -476,6 +476,24 @@
     }
 }
 
+///////////
+// fprop //
+///////////
+void RBMMatrixConnection::fprop(const Vec&amp; input, const Mat&amp; rbm_weights, 
+                          Vec&amp; output) const
+{
+    product( output, rbm_weights, input );
+}
+
+///////////////////
+// setAllWeights //
+///////////////////
+void RBMMatrixConnection::setAllWeights(const Mat&amp; rbm_weights)
+{
+    weights &lt;&lt; rbm_weights;
+}
+
+
 /////////////////
 // bpropUpdate //
 /////////////////
@@ -537,7 +555,93 @@
     productScaleAcc(weights, output_gradients, true, inputs, false,
             -learning_rate / inputs.length(), 1.);
 }
+
+void RBMMatrixConnection::bpropUpdate(const Vec&amp; input, const Mat&amp; rbm_weights,
+                                      const Vec&amp; output,
+                                      Vec&amp; input_gradient, 
+                                      Mat&amp; rbm_weights_gradient,
+                                      const Vec&amp; output_gradient,
+                                      bool accumulate)
+{
+    PLASSERT( input.size() == down_size );
+    PLASSERT( output.size() == up_size );
+    PLASSERT( output_gradient.size() == up_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+
+        // input_gradient += rbm_weights' * output_gradient
+        transposeProductAcc( input_gradient, rbm_weights, output_gradient );
+
+        // rbm_weights_gradient += output_gradient' * input
+        externalProductAcc( rbm_weights_gradient, output_gradient, 
+                            input);
+
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+
+        // input_gradient = rbm_weights' * output_gradient
+        transposeProduct( input_gradient, rbm_weights, output_gradient );
+
+        // rbm_weights_gradient = output_gradient' * input
+        externalProduct( rbm_weights_gradient, output_gradient, 
+                         input);
+    }
+
+}
+
  
+/////////////
+// bpropCD //
+/////////////
+void RBMMatrixConnection::bpropCD(Mat&amp; weights_gradient)
+{
+    int l = weights_gradient.length();
+    int w = weights_gradient.width();
+
+    real* w_i = weights_gradient.data();
+    real* wps_i = weights_pos_stats.data();
+    real* wns_i = weights_neg_stats.data();
+    int w_mod = weights_gradient.mod();
+    int wps_mod = weights_pos_stats.mod();
+    int wns_mod = weights_neg_stats.mod();
+    
+    for( int i=0 ; i&lt;l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
+        for( int j=0 ; j&lt;w ; j++ )
+            w_i[j] = wps_i[j]/pos_count - wns_i[j]/neg_count;
+}
+
+// Instead of using the statistics, we assume we have only one markov chain
+// runned and we update the parameters from the first 4 values of the chain
+void RBMMatrixConnection::bpropCD( const Vec&amp; pos_down_values, // v_0
+                                   const Vec&amp; pos_up_values,   // h_0
+                                   const Vec&amp; neg_down_values, // v_1
+                                   const Vec&amp; neg_up_values, // h_1
+                                   Mat&amp; weights_gradient) 
+{
+    int l = weights.length();
+    int w = weights.width();
+    PLASSERT( pos_up_values.length() == l );
+    PLASSERT( neg_up_values.length() == l );
+    PLASSERT( pos_down_values.length() == w );
+    PLASSERT( neg_down_values.length() == w );
+
+    real* w_i = weights_gradient.data();
+    real* puv_i = pos_up_values.data();
+    real* nuv_i = neg_up_values.data();
+    real* pdv = pos_down_values.data();
+    real* ndv = neg_down_values.data();
+    int w_mod = weights_gradient.mod();
+
+    for( int i=0 ; i&lt;l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
+        for( int j=0 ; j&lt;w ; j++ )
+            w_i[j] =  *puv_i * pdv[j] - *nuv_i * ndv[j] ;
+}
+
 ////////////
 // forget //
 ////////////

Modified: trunk/plearn_learners/online/RBMMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.h	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMMatrixConnection.h	2007-05-22 17:48:25 UTC (rev 7239)
@@ -68,7 +68,7 @@
 
     //#####  Learned Options  #################################################
 
-    //! Matrix containing unit-to-unit weights (output_size &#215; input_size)
+    //! Matrix containing unit-to-unit weights (output_size &#195;&#151; input_size)
     Mat weights;
 
     //! used for Gibbs chain methods only
@@ -183,6 +183,14 @@
     // virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
     //                          const Vec&amp; output_gradient);
 
+    //! given the input and the conneciton weights, 
+    //! compute the output (possibly resize it  appropriately)
+    virtual void fprop(const Vec&amp; input, const Mat&amp; rbm_weights,
+                       Vec&amp; output) const;
+
+    //! set the internal weight values to rbm_weights (copy)
+    virtual void setAllWeights(const Mat&amp; rbm_weights);
+
     //! this version allows to obtain the input gradient as well
     //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
@@ -195,6 +203,26 @@
                              const Mat&amp; output_gradients,
                              bool accumulate = false);
 
+    //! back-propagates the output gradient to the input and the weights
+    //! (the weights are not updated)
+    virtual void bpropUpdate(const Vec&amp; input, const Mat&amp; rbm_weights,
+                             const Vec&amp; output,
+                             Vec&amp; input_gradient, Mat&amp; rbm_weights_gradient,
+                             const Vec&amp; output_gradient,
+                             bool accumulate = false);
+
+    //! Computes the contrastive divergence gradient with respect to the weights
+    //! It should be noted that bpropCD does not call clearstats().
+    virtual void bpropCD(Mat&amp; weights_gradient);
+    
+    //! Computes the contrastive divergence gradient with respect to the weights
+    //! given the positive and negative phase values.
+    virtual void bpropCD(const Vec&amp; pos_down_values,
+                         const Vec&amp; pos_up_values,
+                         const Vec&amp; neg_down_values,
+                         const Vec&amp; neg_up_values,
+                         Mat&amp; weights_gradient);
+
     //! reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-05-22 17:48:25 UTC (rev 7239)
@@ -58,7 +58,8 @@
 ///////////////
 RBMModule::RBMModule():
     cd_learning_rate(0),
-    grad_learning_rate(0)
+    grad_learning_rate(0),
+    n_Gibbs_steps_CD(1)
 {
 }
 
@@ -84,6 +85,11 @@
                   OptionBase::buildoption,
         &quot;Connection between the visible and hidden layers.&quot;);
 
+    declareOption(ol, &quot;reconstruction_connection&quot;, 
+                  &amp;RBMModule::reconstruction_connection,
+                  OptionBase::buildoption,
+        &quot;Reconstuction connection between the hidden and visible layers.&quot;);
+
     declareOption(ol, &quot;grad_learning_rate&quot;, &amp;RBMModule::grad_learning_rate,
                   OptionBase::buildoption,
         &quot;Learning rate for the gradient descent step.&quot;);
@@ -92,6 +98,12 @@
                   OptionBase::buildoption,
         &quot;Learning rate for the constrastive divergence step.&quot;);
 
+    declareOption(ol, &quot;n_Gibbs_steps_CD&quot;, 
+                  &amp;RBMModule::n_Gibbs_steps_CD,
+                  OptionBase::buildoption,
+                  &quot;Number of Gibbs sampling steps in negative phase of &quot;
+                  &quot;contrastive divergence.&quot;);
+
     declareOption(ol, &quot;min_n_Gibbs_steps&quot;, &amp;RBMModule::min_n_Gibbs_steps,
                   OptionBase::buildoption,
                   &quot;Used in generative mode (when visible_sample or hidden_sample is requested)\n&quot;
@@ -129,9 +141,15 @@
     PLASSERT( ports_gradient.length() == nPorts() );
     Mat* visible_grad = ports_gradient[0];
     Mat* hidden_grad = ports_gradient[1];
+    Mat* reconstruction_error_grad = 0;
+    if(reconstruction_connection)
+        reconstruction_error_grad = ports_gradient[8];
+
+    bool bprop_performed = false;
     if (hidden_grad &amp;&amp; !hidden_grad-&gt;isEmpty() &amp;&amp;
             (!visible_grad || visible_grad-&gt;isEmpty()))
     {
+        bprop_performed = true;
         if (grad_learning_rate &gt; 0) {
             setAllLearningRates(grad_learning_rate);
             Mat* hidden_out = ports_value[1];
@@ -169,25 +187,96 @@
             PLASSERT( visible_exp &amp;&amp; hidden_exp );
             // Generate hidden samples.
             hidden_layer-&gt;setExpectations(*hidden_exp);
-            hidden_layer-&gt;generateSamples();
-            // Generate visible samples.
-            connection-&gt;setAsUpInputs(hidden_layer-&gt;samples);
-            visible_layer-&gt;getAllActivations(connection, 0, true);
-            visible_layer-&gt;generateSamples();
-            // (Negative phase) compute corresponding hidden expectations.
-            connection-&gt;setAsDownInputs(visible_layer-&gt;samples);
-            hidden_layer-&gt;getAllActivations(connection, 0, true);
-            hidden_layer-&gt;computeExpectations();
-            // Perform update.
-            visible_layer-&gt;update(*visible_exp, visible_layer-&gt;samples);
-            connection-&gt;update(*visible_exp, *hidden_exp,
-                               visible_layer-&gt;samples,
-                               hidden_layer-&gt;getExpectations());
-            hidden_layer-&gt;update(*hidden_exp, hidden_layer-&gt;getExpectations());
+            for( int i=0; i&lt;n_Gibbs_steps_CD; i++)
+            {
+                hidden_layer-&gt;generateSamples();
+                // (Negative phase) Generate visible samples.
+                connection-&gt;setAsUpInputs(hidden_layer-&gt;samples);
+                visible_layer-&gt;getAllActivations(connection, 0, true);
+                visible_layer-&gt;generateSamples();
+                // compute corresponding hidden expectations.
+                connection-&gt;setAsDownInputs(visible_layer-&gt;samples);
+                hidden_layer-&gt;getAllActivations(connection, 0, true);
+                hidden_layer-&gt;computeExpectations();
+            }
+             // Perform update.
+             visible_layer-&gt;update(*visible_exp, visible_layer-&gt;samples);
+             connection-&gt;update(*visible_exp, *hidden_exp,
+                                visible_layer-&gt;samples,
+                                hidden_layer-&gt;getExpectations());
+             hidden_layer-&gt;update(*hidden_exp, hidden_layer-&gt;getExpectations());
         }
-    } else
-        PLERROR(&quot;In RBMModule::bpropAccUpdate - Only hidden -&gt; visible &quot;
-                &quot;back propagation is currently implemented&quot;);
+    } 
+
+    if (reconstruction_error_grad &amp;&amp; !reconstruction_error_grad-&gt;isEmpty()
+        &amp;&amp; ( !visible_grad || visible_grad-&gt;isEmpty() ) ) {
+        bprop_performed = true;
+        setAllLearningRates(grad_learning_rate);
+        PLASSERT( reconstruction_connection != 0 );
+        // Perform gradient descent on Autoassociator reconstruction cost
+        PLASSERT( ports_value.length() == nPorts() );
+        Mat* visible_exp = ports_value[0];
+        Mat* hidden_exp = ports_value[1];
+        Mat* hidden_act = ports_value[2];
+        Mat* visible_reconstruction = ports_value[6];
+        Mat* visible_reconstruction_activations = ports_value[7];
+        Mat* reconstruction_error = ports_value[8];
+        PLASSERT( hidden_exp != 0 );
+        PLASSERT( visible_exp  &amp;&amp; hidden_act &amp;&amp;
+                  visible_reconstruction &amp;&amp; visible_reconstruction_activations &amp;&amp;
+                  reconstruction_error);
+        // Backprop reconstruction gradient
+
+        // Must change visible_layer's expectation
+        visible_layer-&gt;getExpectations() &lt;&lt; *visible_reconstruction;
+        visible_layer-&gt;bpropNLL(*visible_exp,*reconstruction_error,
+                                visible_act_grad);
+
+        // Combine with incoming gradient
+        PLASSERT( (*reconstruction_error_grad).width() == 1 );
+        real* m_i = visible_act_grad.data();
+        real* vv;
+        for(int i=0; i&lt;visible_act_grad.length(); 
+            i++, m_i+=visible_act_grad.mod())
+        {
+            vv = (*reconstruction_error_grad).data();
+            for(int j=0; j&lt;visible_act_grad.width(); 
+                j++, vv += (*reconstruction_error_grad).mod())
+                m_i[j] *= *vv;
+        }
+
+        // Visible bias update
+        columnSum(visible_act_grad,visible_bias_grad);
+        visible_layer-&gt;update(visible_bias_grad);
+        // Reconstruction connection update
+        reconstruction_connection-&gt;bpropUpdate(
+            *hidden_exp, *visible_reconstruction_activations,
+            hidden_exp_grad, visible_act_grad, false);
+        // Hidden layer bias update
+        hidden_layer-&gt;bpropUpdate(*hidden_act,
+                                  *hidden_exp, hidden_act_grad,
+                                  hidden_exp_grad, false);
+        // Connection update
+        if(visible_grad)
+        {
+            PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
+            connection-&gt;bpropUpdate(
+                *visible_exp, *hidden_act,
+                *visible_grad, hidden_act_grad, true);
+        }
+        else
+        {
+            visible_exp_grad.resize(reconstruction_error_grad-&gt;length(),
+                                       visible_layer-&gt;size);        
+            connection-&gt;bpropUpdate(
+                *visible_exp, *hidden_act,
+                visible_exp_grad, hidden_act_grad, true);
+        }
+    }
+
+    if(!bprop_performed)
+        PLERROR(&quot;In RBMModule::bpropAccUpdate - could not perform back &quot;
+                &quot;propagation given gradient ports&quot;);
 }
 
 ////////////
@@ -198,9 +287,11 @@
     PLASSERT( cd_learning_rate &gt;= 0 &amp;&amp; grad_learning_rate &gt;= 0 );
     if (fast_exact_is_equal(cd_learning_rate, 0) &amp;&amp;
         fast_exact_is_equal(grad_learning_rate, 0) )
-        PLWARNING(&quot;In RBMModule::build_ - Both 'cd_learning_rate' and &quot;
-                &quot;'grad_learning_rate' are set to 0, the RBM will not learn &quot;
-                &quot;much&quot;);
+        PLWARNING(&quot;In RBMModule::build_ - 'cd_learning_rate' and &quot;
+                  &quot;'grad_learning_rate' are set to 0, the RBM will not learn &quot;
+                  &quot;much&quot;);
+    if(visible_layer)
+        visible_bias_grad.resize(visible_layer-&gt;size);
 
     // buid port_sizes
     port_sizes.resize(nPorts(), 2);
@@ -238,6 +329,13 @@
     deepCopyField(connection,       copies);
     deepCopyField(hidden_act_grad,  copies);
     deepCopyField(visible_exp_grad, copies);
+    deepCopyField(reconstruction_connection, copies);
+
+    deepCopyField(hidden_exp_grad, copies);
+    deepCopyField(hidden_act_grad, copies);
+    deepCopyField(visible_exp_grad, copies);
+    deepCopyField(visible_act_grad, copies);
+    deepCopyField(visible_bias_grad, copies);
 }
 
 ///////////
@@ -257,8 +355,19 @@
     Mat* visible_sample = ports_value[3];
     Mat* hidden_sample = ports_value[4];
     Mat* energy = ports_value[5];
+    Mat* visible_reconstruction = 0;
+    Mat* visible_reconstruction_activations = 0;
+    Mat* reconstruction_error = 0;
+    if(reconstruction_connection)
+    {
+        visible_reconstruction = ports_value[6];
+        visible_reconstruction_activations = ports_value[7];
+        reconstruction_error = ports_value[8];
+    }
+    bool hidden_expectations_are_computed=false;
     bool hidden_activations_are_computed=false;
     bool visible_activations_are_computed=false;
+
     if (energy) 
     {
         PLASSERT_MSG( energy-&gt;isEmpty(), 
@@ -331,7 +440,7 @@
             hidden_layer-&gt;getAllActivations(connection, 0, true);
             hidden_activations_are_computed=true;
         }
-        if (hidden_act) {
+        if (hidden_activations_are_computed &amp;&amp; hidden_act) {
             // Also store hidden layer activations.
             PLASSERT( hidden_act-&gt;isEmpty() );
             const Mat&amp; to_store = hidden_layer-&gt;activations;
@@ -341,17 +450,18 @@
         if (hidden) {
             PLASSERT( hidden-&gt;isEmpty() );
             hidden_layer-&gt;computeExpectations();
+            hidden_expectations_are_computed=true;
             Mat&amp; hidden_out = hidden_layer-&gt;getExpectations();
             hidden-&gt;resize(hidden_out.length(), hidden_out.width());
             *hidden &lt;&lt; hidden_out;
         }
         // Since we return below, the other ports must be unused.
-        PLASSERT( !visible_sample &amp;&amp; !hidden_sample );
-        return;
+        //PLASSERT( !visible_sample &amp;&amp; !hidden_sample );
     } 
     if ((visible_sample &amp;&amp; visible_sample-&gt;isEmpty()) // it is asked to sample the visible units
         || (hidden_sample &amp;&amp; hidden_sample-&gt;isEmpty())) // or to sample the hidden units
     {
+        PLWARNING(&quot;In RBMModule::fprop - sampling in RBMModule has not been tested&quot;);
         if (hidden &amp;&amp; !hidden-&gt;isEmpty()) // sample visible conditionally on hidden
         {
             connection-&gt;setAsUpInputs(*hidden);
@@ -394,10 +504,63 @@
                 *hidden_sample &lt;&lt; hidden_layer-&gt;samples;
             }
         }        
-    } 
+    }
+    if ( visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp; 
+         ( ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() ) || 
+           ( visible_reconstruction_activations &amp;&amp; 
+             visible_reconstruction_activations-&gt;isEmpty() ) ||
+           ( reconstruction_error &amp;&amp; reconstruction_error-&gt;isEmpty() ) ) ) 
+    {        
+        // Autoassociator reconstruction cost
+        PLASSERT( ports_value.length() == nPorts() );
+        if (!hidden_activations_are_computed)
+        {
+            connection-&gt;setAsDownInputs(*visible); 
+            hidden_layer-&gt;getAllActivations(connection, 0, true);
+            hidden_activations_are_computed=true;
+        }
+        if(!hidden_expectations_are_computed)
+        {
+            hidden_layer-&gt;computeExpectations();
+            hidden_expectations_are_computed=true;
+        }
+        // Don't need to verify is they are asked in a port, this was done previously
+        
+        reconstruction_connection-&gt;setAsDownInputs(hidden_layer-&gt;getExpectations());
+        visible_layer-&gt;getAllActivations(
+            reconstruction_connection, 0, true);
+        if(visible_reconstruction_activations)
+        {
+            PLASSERT( visible_reconstruction_activations-&gt;isEmpty() );
+            const Mat&amp; to_store = visible_layer-&gt;activations;
+            visible_reconstruction_activations-&gt;resize(to_store.length(), 
+                                                       to_store.width());
+            *visible_reconstruction_activations &lt;&lt; to_store;
+        }
+        if (visible_reconstruction || reconstruction_error)
+        {        
+            visible_layer-&gt;computeExpectations();
+            if(visible_reconstruction)
+            {
+                PLASSERT( visible_reconstruction-&gt;isEmpty() );
+                const Mat&amp; to_store = visible_layer-&gt;getExpectations();
+                visible_reconstruction-&gt;resize(to_store.length(), 
+                                                           to_store.width());
+                *visible_reconstruction &lt;&lt; to_store;
+            }
+            if(reconstruction_error)
+            {
+                PLASSERT( reconstruction_error-&gt;isEmpty() );
+                reconstruction_error-&gt;resize(visible-&gt;length(),1);
+                visible_layer-&gt;fpropNLL(*visible,
+                                        *reconstruction_error);
+            }
+        }
+    }
+
     // Remark: the code above probably has not been tested, since the PLERROR
     // will systematically be reached.
-    PLERROR(&quot;In RBMModule::fprop - Unknown port configuration&quot;);
+    //PLERROR(&quot;In RBMModule::fprop - Unknown port configuration&quot;);
 }
 
 ////////////
@@ -439,6 +602,12 @@
         ports.append(&quot;visible_sample&quot;);
         ports.append(&quot;hidden_sample&quot;);
         ports.append(&quot;energy&quot;);
+        if(reconstruction_connection)
+        {
+            ports.append(&quot;visible_reconstruction.state&quot;);
+            ports.append(&quot;visible_reconstruction_activations.state&quot;);
+            ports.append(&quot;reconstruction_error.state&quot;);
+        }
     }
     return ports;
 }
@@ -448,6 +617,7 @@
 ///////////////////
 const TMat&lt;int&gt;&amp; RBMModule::getPortSizes()
 {
+    //port_sizes(3,1) = 1;
     return port_sizes;
 }
 
@@ -468,6 +638,8 @@
     hidden_layer-&gt;setLearningRate(lr);
     visible_layer-&gt;setLearningRate(lr);
     connection-&gt;setLearningRate(lr);
+    if(reconstruction_connection)
+        reconstruction_connection-&gt;setLearningRate(lr);
 }
 
 /////////////////////

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMModule.h	2007-05-22 17:48:25 UTC (rev 7239)
@@ -66,9 +66,15 @@
     PP&lt;RBMLayer&gt; hidden_layer;
     PP&lt;RBMLayer&gt; visible_layer;
     PP&lt;RBMConnection&gt; connection;
+    PP&lt;RBMConnection&gt; reconstruction_connection;
 
     real cd_learning_rate;
     real grad_learning_rate;
+
+    //! Number of Gibbs sampling steps in negative phase 
+    //! of contrastive divergence.
+    int n_Gibbs_steps_CD;
+
     //! used to generate samples from the RBM
     int min_n_Gibbs_steps; 
     int n_Gibbs_steps_per_generated_sample;
@@ -198,12 +204,21 @@
 
 protected:
 
+    //! Used to store gradient w.r.t. expectations of the hidden layer.
+    Mat hidden_exp_grad;
+
     //! Used to store gradient w.r.t. activations of the hidden layer.
     Mat hidden_act_grad;
 
     //! Used to store gradient w.r.t. expectations of the visible layer.
     Mat visible_exp_grad;
 
+    //! Used to store gradient w.r.t. activations of the visible layer.
+    Mat visible_act_grad;
+
+    //! Used to store gradient w.r.t. bias of visible layer
+    Vec visible_bias_grad;
+
     //#####  Protected Member Functions  ######################################
 
     //! Forward the given learning rate to all elements of this module.

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-05-22 17:48:25 UTC (rev 7239)
@@ -295,6 +295,33 @@
     return ret;
 }
 
+void RBMMultinomialLayer::fpropNLL(const Mat&amp; targets, const Mat&amp; costs_column)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+
+    real target_i, expectation_i;
+    for (int k=0;k&lt;batch_size;k++) // loop over minibatch
+    {
+        real nll = 0;
+        //real* activation = activations[k];
+        real* expectation = expectations[k];
+        real* target = targets[k];
+        for( int i=0 ; i&lt;size ; i++ )
+        {
+            target_i = target[i];
+            expectation_i = expectation[i];
+            if(!fast_exact_is_equal(target_i,0.0))
+                nll -= target_i * pl_log(expectation_i);
+        }
+        costs_column(k,0) = nll;
+    }
+}
+
 void RBMMultinomialLayer::bpropNLL(const Vec&amp; target, real nll,
                                    Vec&amp; bias_gradient)
 {
@@ -311,6 +338,28 @@
         biasg[i] = tar[i] - sum_tar * exp[i];
 }
 
+void RBMMultinomialLayer::bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                                Mat&amp; bias_gradients)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+
+    for (int k=0;k&lt;batch_size;k++) // loop over minibatch
+    {        
+        real sum_tar = sum( targets(k) );
+        real* exp = expectations[k];
+        real* tar = targets[k];
+        real* biasg = bias_gradients[k];
+        for( int i=0 ; i&lt;size ; i++ )
+            biasg[i] = tar[i] - sum_tar * exp[i];
+    }
+}
+
 void RBMMultinomialLayer::declareOptions(OptionList&amp; ol)
 {
 /*

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-05-22 17:22:26 UTC (rev 7238)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-05-22 17:48:25 UTC (rev 7239)
@@ -108,10 +108,13 @@
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec&amp; target);
+    virtual void fpropNLL(const Mat&amp; targets, const Mat&amp; costs_column);
 
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient);
+    virtual void bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                          Mat&amp; bias_gradients);
 
     //#####  PLearn::Object Protocol  #########################################
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000687.html">[Plearn-commits] r7238 - trunk/scripts
</A></li>
	<LI>Next message: <A HREF="000689.html">[Plearn-commits] r7240 - trunk/scripts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#688">[ date ]</a>
              <a href="thread.html#688">[ thread ]</a>
              <a href="subject.html#688">[ subject ]</a>
              <a href="author.html#688">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
