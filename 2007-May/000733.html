<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7284 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7284%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705241411.l4OEBjot026660%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000732.html">
   <LINK REL="Next"  HREF="000734.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7284 - trunk/plearn_learners/online</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7284%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705241411.l4OEBjot026660%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7284 - trunk/plearn_learners/online">yoshua at mail.berlios.de
       </A><BR>
    <I>Thu May 24 16:11:45 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000732.html">[Plearn-commits] r7283 - trunk/scripts
</A></li>
        <LI>Next message: <A HREF="000734.html">[Plearn-commits] r7285 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#733">[ date ]</a>
              <a href="thread.html#733">[ thread ]</a>
              <a href="subject.html#733">[ subject ]</a>
              <a href="author.html#733">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-05-24 16:11:44 +0200 (Thu, 24 May 2007)
New Revision: 7284

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/RBMModule.cc
Log:
Rearranged a few things in RBMModule


Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-05-24 13:58:45 UTC (rev 7283)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-05-24 14:11:44 UTC (rev 7284)
@@ -50,11 +50,15 @@
     OnlineLearningModule,
     &quot;Learn to map inputs to outputs, online, using caller-provided gradients.&quot;,
     &quot;This pure virtual class (i.e. an interface) can basically do two things:\n&quot;
-    &quot;  * map an input to an output\n&quot;
+    &quot;  * map its inputs to its outputs\n&quot;
     &quot;  * modify itself when told in what direction the output should have \n&quot;
     &quot;    changed (i.e. output gradient),  while optionally giving back the \n&quot;
     &quot;    information about how the input should also have changed \n&quot;
     &quot;    (i.e. input gradient)\n&quot;
+    &quot;The main methods are fprop methods (which maps inputs to outputs) and bprop\n&quot;
+    &quot;methods (which map outputs gradients into inputs gradients and update internal\n&quot;
+    &quot;parameters). Changes to options should not occur between an fprop and\n&quot;
+    &quot;the corresponding bprop.\n&quot;
     );
 
 //////////////////////////

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-05-24 13:58:45 UTC (rev 7283)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-05-24 14:11:44 UTC (rev 7284)
@@ -136,163 +136,6 @@
     inherited::declareOptions(ol);
 }
 
-////////////////////
-// bpropAccUpdate //
-////////////////////
-void RBMModule::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
-                               const TVec&lt;Mat*&gt;&amp; ports_gradient)
-{
-    PLASSERT( ports_gradient.length() == nPorts() );
-    Mat* visible_grad = ports_gradient[0];
-    Mat* hidden_grad = ports_gradient[1];
-    Mat* reconstruction_error_grad = 0;
-    
-    if(reconstruction_connection)
-        reconstruction_error_grad = ports_gradient[8];
-
-    bool bprop_performed = false;
-    if (hidden_grad &amp;&amp; !hidden_grad-&gt;isEmpty() &amp;&amp;
-        (!visible_grad || visible_grad-&gt;isEmpty()))
-    {
-        bprop_performed = true;
-        if (grad_learning_rate &gt; 0) {
-            setAllLearningRates(grad_learning_rate);
-            Mat* hidden_out = ports_value[1];
-            Mat* hidden_act = ports_value[2];
-            PLASSERT( hidden_out &amp;&amp; hidden_act );
-            // Compute gradient w.r.t. activations of the hidden layer.
-            hidden_layer-&gt;bpropUpdate(
-                    *hidden_act, *hidden_out, hidden_act_grad, *hidden_grad,
-                    false);
-            Mat* visible_out = ports_value[0];
-            // Compute gradient w.r.t. expectations of the visible layer (=
-            // inputs).
-            Mat* store_visible_grad = NULL;
-            if (visible_grad) {
-                PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
-                store_visible_grad = visible_grad;
-            } else {
-                // We do not actually need to store the gradient, but since it
-                // is required in bpropUpdate, we provide a dummy matrix to
-                // store it.
-                store_visible_grad = &amp;visible_exp_grad;
-            }
-            store_visible_grad-&gt;resize(hidden_grad-&gt;length(),
-                                       visible_layer-&gt;size);
-            connection-&gt;bpropUpdate(
-                    *visible_out, *hidden_act, *store_visible_grad,
-                    hidden_act_grad, true);
-        }
-        if (cd_learning_rate &gt; 0) {
-            setAllLearningRates(cd_learning_rate);
-            // Perform a step of contrastive divergence.
-            PLASSERT( ports_value.length() == nPorts() );
-            Mat* visible_exp = ports_value[0];
-            Mat* hidden_exp = ports_value[1];
-            Mat* negative_phase_visible_samples = 
-                compute_contrastive_divergence?ports_value[reconstruction_connection?10:7]:0;
-            Mat* negative_phase_hidden_expectations = 
-                compute_contrastive_divergence?ports_value[reconstruction_connection?11:8]:0;
-            PLASSERT( visible_exp &amp;&amp; hidden_exp );
-            if (!(negative_phase_visible_samples &amp;&amp; negative_phase_hidden_expectations))
-            {
-                // Generate hidden samples.
-                hidden_layer-&gt;setExpectations(*hidden_exp);
-                for( int i=0; i&lt;n_Gibbs_steps_CD; i++)
-                {
-                    hidden_layer-&gt;generateSamples();
-                    // (Negative phase) Generate visible samples.
-                    connection-&gt;setAsUpInputs(hidden_layer-&gt;samples);
-                    visible_layer-&gt;getAllActivations(connection, 0, true);
-                    visible_layer-&gt;generateSamples();
-                    // compute corresponding hidden expectations.
-                    connection-&gt;setAsDownInputs(visible_layer-&gt;samples);
-                    hidden_layer-&gt;getAllActivations(connection, 0, true);
-                    hidden_layer-&gt;computeExpectations();
-                }
-                negative_phase_visible_samples = &amp;(visible_layer-&gt;samples);
-                negative_phase_hidden_expectations = &amp;(hidden_layer-&gt;getExpectations());
-            }
-            // Perform update.
-            visible_layer-&gt;update(*visible_exp, *negative_phase_visible_samples);
-            connection-&gt;update(*visible_exp, *hidden_exp,
-                               *negative_phase_visible_samples,
-                               *negative_phase_hidden_expectations);
-            hidden_layer-&gt;update(*hidden_exp, *negative_phase_hidden_expectations);
-        }
-    } 
-
-    if (reconstruction_error_grad &amp;&amp; !reconstruction_error_grad-&gt;isEmpty()
-        &amp;&amp; ( !visible_grad || visible_grad-&gt;isEmpty() ) ) {
-        bprop_performed = true;
-        setAllLearningRates(grad_learning_rate);
-        PLASSERT( reconstruction_connection != 0 );
-        // Perform gradient descent on Autoassociator reconstruction cost
-        PLASSERT( ports_value.length() == nPorts() );
-        Mat* visible_exp = ports_value[0];
-        Mat* hidden_exp = ports_value[1];
-        Mat* hidden_act = ports_value[2];
-        Mat* visible_reconstruction = ports_value[6];
-        Mat* visible_reconstruction_activations = ports_value[7];
-        Mat* reconstruction_error = ports_value[8];
-        PLASSERT( hidden_exp != 0 );
-        PLASSERT( visible_exp  &amp;&amp; hidden_act &amp;&amp;
-                  visible_reconstruction &amp;&amp; visible_reconstruction_activations &amp;&amp;
-                  reconstruction_error);
-        // Backprop reconstruction gradient
-
-        // Must change visible_layer's expectation
-        visible_layer-&gt;getExpectations() &lt;&lt; *visible_reconstruction;
-        visible_layer-&gt;bpropNLL(*visible_exp,*reconstruction_error,
-                                visible_act_grad);
-
-        // Combine with incoming gradient
-        PLASSERT( (*reconstruction_error_grad).width() == 1 );
-        real* m_i = visible_act_grad.data();
-        real* vv;
-        for(int i=0; i&lt;visible_act_grad.length(); 
-            i++, m_i+=visible_act_grad.mod())
-        {
-            vv = (*reconstruction_error_grad).data();
-            for(int j=0; j&lt;visible_act_grad.width(); 
-                j++, vv += (*reconstruction_error_grad).mod())
-                m_i[j] *= *vv;
-        }
-
-        // Visible bias update
-        columnSum(visible_act_grad,visible_bias_grad);
-        visible_layer-&gt;update(visible_bias_grad);
-        // Reconstruction connection update
-        reconstruction_connection-&gt;bpropUpdate(
-            *hidden_exp, *visible_reconstruction_activations,
-            hidden_exp_grad, visible_act_grad, false);
-        // Hidden layer bias update
-        hidden_layer-&gt;bpropUpdate(*hidden_act,
-                                  *hidden_exp, hidden_act_grad,
-                                  hidden_exp_grad, false);
-        // Connection update
-        if(visible_grad)
-        {
-            PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
-            connection-&gt;bpropUpdate(
-                *visible_exp, *hidden_act,
-                *visible_grad, hidden_act_grad, true);
-        }
-        else
-        {
-            visible_exp_grad.resize(reconstruction_error_grad-&gt;length(),
-                                       visible_layer-&gt;size);        
-            connection-&gt;bpropUpdate(
-                *visible_exp, *hidden_act,
-                visible_exp_grad, hidden_act_grad, true);
-        }
-    }
-
-    if(!bprop_performed)
-        PLERROR(&quot;In RBMModule::bpropAccUpdate - could not perform back &quot;
-                &quot;propagation given gradient ports&quot;);
-}
-
 ////////////
 // build_ //
 ////////////
@@ -434,6 +277,14 @@
     bool visible_activations_are_computed=false;
     bool found_a_valid_configuration = false;
 
+    if (visible &amp;&amp; !visible-&gt;isEmpty())
+    // when an input is provided, that would restart the chain for unconditional sampling, from that example
+    {
+        Gibbs_step = 0; 
+        visible_layer-&gt;setExpectations(*visible);
+    }
+
+    // compute the energy if asked
     if (energy) 
     {
         PLASSERT_MSG( energy-&gt;isEmpty(), 
@@ -528,6 +379,56 @@
         //PLASSERT( !visible_sample &amp;&amp; !hidden_sample );
         found_a_valid_configuration = true;
     } 
+    if ((visible_sample &amp;&amp; visible_sample-&gt;isEmpty()) // it is asked to sample the visible units
+        || (hidden_sample &amp;&amp; hidden_sample-&gt;isEmpty())) // or to sample the hidden units
+    {
+        PLWARNING(&quot;In RBMModule::fprop - sampling in RBMModule has not been tested&quot;);
+        if (hidden &amp;&amp; !hidden-&gt;isEmpty()) // sample visible conditionally on hidden
+        {
+            connection-&gt;setAsUpInputs(*hidden);
+            visible_layer-&gt;getAllActivations(connection, 0, true);
+            visible_layer-&gt;generateSamples();
+            visible_sample-&gt;resize(visible_layer-&gt;samples.length(),visible_layer-&gt;samples.width());
+            *visible_sample &lt;&lt; visible_layer-&gt;samples;
+            Gibbs_step = 0; // that would restart the chain for unconditional sampling
+        }
+        else if (visible &amp;&amp; !visible-&gt;isEmpty()) // if an input is provided, sample hidden conditionally
+        {
+            connection-&gt;setAsDownInputs(visible_layer-&gt;samples);
+            hidden_layer-&gt;getAllActivations(connection, 0, true);
+            hidden_layer-&gt;generateSamples();
+            Gibbs_step = 0; // that would restart the chain for unconditional sampling
+        }
+        else // sample unconditionally: Gibbs sample after k steps
+        {
+            // the visible_layer-&gt;expectations contain the &quot;state&quot; from which we
+            // start or continue the chain
+            int min_n = min(Gibbs_step+n_Gibbs_steps_per_generated_sample,
+                            min_n_Gibbs_steps);
+            for (;Gibbs_step&lt;min_n;Gibbs_step++)
+            {
+                connection-&gt;setAsDownInputs(visible_layer-&gt;samples);
+                hidden_layer-&gt;getAllActivations(connection, 0, true);
+                hidden_layer-&gt;generateSamples();
+                connection-&gt;setAsUpInputs(hidden_layer-&gt;samples);
+                visible_layer-&gt;getAllActivations(connection, 0, true);
+                visible_layer-&gt;generateSamples();
+            }
+        }
+        if (visible_sample &amp;&amp; visible_sample-&gt;isEmpty()) // provide sample of the visible units
+        {
+            visible_sample-&gt;resize(visible_layer-&gt;samples.length(),
+                                   visible_layer-&gt;samples.width());
+            *visible_sample &lt;&lt; visible_layer-&gt;samples;
+        }
+        if (hidden_sample &amp;&amp; hidden_sample-&gt;isEmpty()) // provide sample of the hidden units
+        {
+            hidden_sample-&gt;resize(hidden_layer-&gt;samples.length(),
+                                  hidden_layer-&gt;samples.width());
+            *hidden_sample &lt;&lt; hidden_layer-&gt;samples;
+        }
+        found_a_valid_configuration = true;
+    }
     if (contrastive_divergence)
     {
         PLASSERT_MSG( contrastive_divergence-&gt;isEmpty(), 
@@ -535,17 +436,16 @@
         if (visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp; (!hidden || hidden-&gt;isEmpty()))
         {
             int mbs = visible-&gt;length();
-            if (!hidden_activations_are_computed)
+            if (!hidden_activations_are_computed) // it must be because neither hidden nor hidden_act were asked
             {
                 connection-&gt;setAsDownInputs(*visible); 
                 hidden_layer-&gt;getAllActivations(connection, 0, true);
                 hidden_activations_are_computed=true;
             }
-            if (!hidden_expectations_are_computed)
+            if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
             {
                 hidden_layer-&gt;computeExpectations();
                 hidden_expectations_are_computed=true;
-                // save hidden_expectations in hidde port
             }
             // perform negative phase
             for( int i=0; i&lt;n_Gibbs_steps_CD; i++)
@@ -578,54 +478,6 @@
                     &quot;only possible if only visible are provided in input).\n&quot;);
         found_a_valid_configuration = true;
     }
-    if ((visible_sample &amp;&amp; visible_sample-&gt;isEmpty()) // it is asked to sample the visible units
-        || (hidden_sample &amp;&amp; hidden_sample-&gt;isEmpty())) // or to sample the hidden units
-    {
-        PLWARNING(&quot;In RBMModule::fprop - sampling in RBMModule has not been tested&quot;);
-        if (hidden &amp;&amp; !hidden-&gt;isEmpty()) // sample visible conditionally on hidden
-        {
-            connection-&gt;setAsUpInputs(*hidden);
-            visible_layer-&gt;getAllActivations(connection, 0, true);
-            visible_layer-&gt;generateSamples();
-            visible_sample-&gt;resize(visible_layer-&gt;samples.length(),visible_layer-&gt;samples.width());
-            *visible_sample &lt;&lt; visible_layer-&gt;samples;
-        }
-        else // sample unconditionally: Gibbs sample after k steps
-        {
-            if (visible &amp;&amp; !visible-&gt;isEmpty()) // if an input is provided, start the chain from it
-            {
-                Gibbs_step = 0;
-                visible_layer-&gt;samples &lt;&lt; *visible;
-            }
-            // else we might want to restore as a separate &quot;state&quot; the last visible_layer-&gt;samples,
-            // (copied elsewhere) in case the RBM is used to something else than sampling 
-            // in between calls to this fprop-for-sampling.
-            int min_n = min(Gibbs_step+n_Gibbs_steps_per_generated_sample,
-                            min_n_Gibbs_steps);
-            for (;Gibbs_step&lt;min_n;Gibbs_step++)
-            {
-                connection-&gt;setAsDownInputs(visible_layer-&gt;samples);
-                hidden_layer-&gt;getAllActivations(connection, 0, true);
-                hidden_layer-&gt;generateSamples();
-                connection-&gt;setAsUpInputs(hidden_layer-&gt;samples);
-                visible_layer-&gt;getAllActivations(connection, 0, true);
-                visible_layer-&gt;generateSamples();
-            }
-            if (visible_sample &amp;&amp; visible_sample-&gt;isEmpty()) // provide sample of the visible units
-            {
-                visible_sample-&gt;resize(visible_layer-&gt;samples.length(),
-                                       visible_layer-&gt;samples.width());
-                *visible_sample &lt;&lt; visible_layer-&gt;samples;
-            }
-            if (hidden_sample &amp;&amp; hidden_sample-&gt;isEmpty()) // provide sample of the hidden units
-            {
-                hidden_sample-&gt;resize(hidden_layer-&gt;samples.length(),
-                                      hidden_layer-&gt;samples.width());
-                *hidden_sample &lt;&lt; hidden_layer-&gt;samples;
-            }
-        }        
-        found_a_valid_configuration = true;
-    }
     if ( visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp; 
          ( ( visible_reconstruction &amp;&amp; visible_reconstruction-&gt;isEmpty() ) || 
            ( visible_reconstruction_activations &amp;&amp; 
@@ -689,6 +541,178 @@
         PLERROR(&quot;In RBMModule::fprop - Unknown port configuration&quot;);
 }
 
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void RBMModule::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                               const TVec&lt;Mat*&gt;&amp; ports_gradient)
+{
+    PLASSERT( ports_gradient.length() == nPorts() );
+    Mat* visible_grad = ports_gradient[0];
+    Mat* hidden_grad = ports_gradient[1];
+    Mat* visible = ports_value[0];
+    Mat* hidden = ports_value[1];
+    Mat* hidden_act = ports_value[2];
+    Mat* reconstruction_error_grad = 0;
+    
+    if(reconstruction_connection)
+        reconstruction_error_grad = ports_gradient[8];
+
+    bool bprop_performed = false;
+    if (visible &amp;&amp; !visible-&gt;isEmpty() &amp;&amp; 
+        (hidden_grad &amp;&amp; !hidden_grad-&gt;isEmpty() &amp;&amp;
+         (!visible_grad || visible_grad-&gt;isEmpty())
+         || cd_learning_rate&gt;0))
+    {
+        bprop_performed = true;
+        int mbs = visible-&gt;length();
+        if (grad_learning_rate &gt; 0) {
+            setAllLearningRates(grad_learning_rate);
+            PLASSERT( hidden &amp;&amp; hidden_act );
+            // Compute gradient w.r.t. activations of the hidden layer.
+            hidden_layer-&gt;bpropUpdate(
+                    *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
+                    false);
+            Mat* visible_out = ports_value[0];
+            // Compute gradient w.r.t. expectations of the visible layer (=
+            // inputs).
+            Mat* store_visible_grad = NULL;
+            if (visible_grad) {
+                PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
+                store_visible_grad = visible_grad;
+            } else {
+                // We do not actually need to store the gradient, but since it
+                // is required in bpropUpdate, we provide a dummy matrix to
+                // store it.
+                store_visible_grad = &amp;visible_exp_grad;
+            }
+            store_visible_grad-&gt;resize(mbs,visible_layer-&gt;size);
+            connection-&gt;bpropUpdate(
+                    *visible_out, *hidden_act, *store_visible_grad,
+                    hidden_act_grad, true);
+        }
+        if (cd_learning_rate &gt; 0) {
+            setAllLearningRates(cd_learning_rate);
+            // Perform a step of contrastive divergence.
+            PLASSERT( ports_value.length() == nPorts() );
+            Mat* negative_phase_visible_samples = 
+                compute_contrastive_divergence?ports_value[reconstruction_connection?10:7]:0;
+            Mat* negative_phase_hidden_expectations = 
+                compute_contrastive_divergence?ports_value[reconstruction_connection?11:8]:0;
+            PLASSERT( visible &amp;&amp; hidden );
+            if (!negative_phase_visible_samples || negative_phase_visible_samples-&gt;isEmpty())
+            {
+                // Generate hidden samples.
+                hidden_layer-&gt;setExpectations(*hidden);
+                for( int i=0; i&lt;n_Gibbs_steps_CD; i++)
+                {
+                    hidden_layer-&gt;generateSamples();
+                    // (Negative phase) Generate visible samples.
+                    connection-&gt;setAsUpInputs(hidden_layer-&gt;samples);
+                    visible_layer-&gt;getAllActivations(connection, 0, true);
+                    visible_layer-&gt;generateSamples();
+                    // compute corresponding hidden expectations.
+                    connection-&gt;setAsDownInputs(visible_layer-&gt;samples);
+                    hidden_layer-&gt;getAllActivations(connection, 0, true);
+                    hidden_layer-&gt;computeExpectations();
+                }
+                if (!negative_phase_hidden_expectations)
+                    negative_phase_hidden_expectations = &amp;(hidden_layer-&gt;getExpectations());
+                else
+                {
+                    PLASSERT(negative_phase_hidden_expectations-&gt;isEmpty());
+                    negative_phase_hidden_expectations-&gt;resize(mbs,hidden_layer-&gt;size);
+                    *negative_phase_hidden_expectations &lt;&lt; hidden_layer-&gt;getExpectations();
+                }
+                if (!negative_phase_visible_samples)
+                    negative_phase_visible_samples = &amp;(visible_layer-&gt;samples);
+                else
+                {
+                    PLASSERT(negative_phase_visible_samples-&gt;isEmpty());
+                    negative_phase_visible_samples-&gt;resize(mbs,visible_layer-&gt;size);
+                    *negative_phase_visible_samples &lt;&lt; visible_layer-&gt;samples;
+                }
+            }
+            // Perform update.
+            visible_layer-&gt;update(*visible, *negative_phase_visible_samples);
+            connection-&gt;update(*visible, *hidden,
+                               *negative_phase_visible_samples,
+                               *negative_phase_hidden_expectations);
+            hidden_layer-&gt;update(*hidden, *negative_phase_hidden_expectations);
+        }
+    } 
+
+    if (reconstruction_error_grad &amp;&amp; !reconstruction_error_grad-&gt;isEmpty()
+        &amp;&amp; ( !visible_grad || visible_grad-&gt;isEmpty() ) ) {
+        bprop_performed = true;
+        setAllLearningRates(grad_learning_rate);
+        PLASSERT( reconstruction_connection != 0 );
+        // Perform gradient descent on Autoassociator reconstruction cost
+        PLASSERT( ports_value.length() == nPorts() );
+        Mat* hidden = ports_value[1];
+        Mat* hidden_act = ports_value[2];
+        Mat* visible_reconstruction = ports_value[6];
+        Mat* visible_reconstruction_activations = ports_value[7];
+        Mat* reconstruction_error = ports_value[8];
+        PLASSERT( hidden != 0 );
+        PLASSERT( visible  &amp;&amp; hidden_act &amp;&amp;
+                  visible_reconstruction &amp;&amp; visible_reconstruction_activations &amp;&amp;
+                  reconstruction_error);
+        int mbs = reconstruction_error_grad-&gt;length();
+
+        // Backprop reconstruction gradient
+
+        // Must change visible_layer's expectation
+        visible_layer-&gt;getExpectations() &lt;&lt; *visible_reconstruction;
+        visible_layer-&gt;bpropNLL(*visible,*reconstruction_error,
+                                visible_act_grad);
+
+        // Combine with incoming gradient
+        PLASSERT( (*reconstruction_error_grad).width() == 1 );
+        real* m_i = visible_act_grad.data();
+        real* vv;
+        for(int i=0; i&lt;visible_act_grad.length(); 
+            i++, m_i+=visible_act_grad.mod())
+        {
+            vv = (*reconstruction_error_grad).data();
+            for(int j=0; j&lt;visible_act_grad.width(); 
+                j++, vv += (*reconstruction_error_grad).mod())
+                m_i[j] *= *vv;
+        }
+
+        // Visible bias update
+        columnSum(visible_act_grad,visible_bias_grad);
+        visible_layer-&gt;update(visible_bias_grad);
+        // Reconstruction connection update
+        reconstruction_connection-&gt;bpropUpdate(
+            *hidden, *visible_reconstruction_activations,
+            hidden_exp_grad, visible_act_grad, false);
+        // Hidden layer bias update
+        hidden_layer-&gt;bpropUpdate(*hidden_act,
+                                  *hidden, hidden_act_grad,
+                                  hidden_exp_grad, false);
+        // Connection update
+        if(visible_grad)
+        {
+            PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
+            connection-&gt;bpropUpdate(
+                *visible, *hidden_act,
+                *visible_grad, hidden_act_grad, true);
+        }
+        else
+        {
+            visible_exp_grad.resize(mbs,visible_layer-&gt;size);        
+            connection-&gt;bpropUpdate(
+                *visible, *hidden_act,
+                visible_exp_grad, hidden_act_grad, true);
+        }
+    }
+
+    if(!bprop_performed)
+        PLERROR(&quot;In RBMModule::bpropAccUpdate - could not perform back &quot;
+                &quot;propagation given gradient ports&quot;);
+}
+
 ////////////
 // forget //
 ////////////


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000732.html">[Plearn-commits] r7283 - trunk/scripts
</A></li>
	<LI>Next message: <A HREF="000734.html">[Plearn-commits] r7285 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#733">[ date ]</a>
              <a href="thread.html#733">[ thread ]</a>
              <a href="subject.html#733">[ subject ]</a>
              <a href="author.html#733">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
