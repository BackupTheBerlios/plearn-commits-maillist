<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7129 - in trunk: plearn/var/EXPERIMENTAL	python_modules/plearn/var
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7129%20-%20in%20trunk%3A%20plearn/var/EXPERIMENTAL%0A%09python_modules/plearn/var&In-Reply-To=%3C200705161752.l4GHqaHE013714%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000577.html">
   <LINK REL="Next"  HREF="000579.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7129 - in trunk: plearn/var/EXPERIMENTAL	python_modules/plearn/var</H1>
    <B>simonl at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7129%20-%20in%20trunk%3A%20plearn/var/EXPERIMENTAL%0A%09python_modules/plearn/var&In-Reply-To=%3C200705161752.l4GHqaHE013714%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7129 - in trunk: plearn/var/EXPERIMENTAL	python_modules/plearn/var">simonl at mail.berlios.de
       </A><BR>
    <I>Wed May 16 19:52:36 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000577.html">[Plearn-commits] r7128 - trunk/plearn/sys
</A></li>
        <LI>Next message: <A HREF="000579.html">[Plearn-commits] r7130 - in trunk/plearn_learners: generic online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#578">[ date ]</a>
              <a href="thread.html#578">[ thread ]</a>
              <a href="subject.html#578">[ subject ]</a>
              <a href="author.html#578">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: simonl
Date: 2007-05-16 19:52:35 +0200 (Wed, 16 May 2007)
New Revision: 7129

Modified:
   trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc
   trunk/python_modules/plearn/var/Var.py
Log:
Minor improvements  to Var.py


Modified: trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc	2007-05-16 16:28:30 UTC (rev 7128)
+++ trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc	2007-05-16 17:52:35 UTC (rev 7129)
@@ -188,6 +188,7 @@
 } // end of namespace PLearn
 
 
+
 /*
   Local Variables:
   mode:c++

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-05-16 16:28:30 UTC (rev 7128)
+++ trunk/python_modules/plearn/var/Var.py	2007-05-16 17:52:35 UTC (rev 7129)
@@ -59,9 +59,14 @@
         return Var(pl.ProbabilityPairsVariable(input=self.v, min=min, max=max))
 
     def matrixProduct(self, W):
-        pass
-        # return Var(pl.MatrixProductVariable(self.v, W)
+        return Var(pl.ProductVariable(self.v, W))
 
+    def matrixTransposeProduct(self, W):
+        return Var(pl.TransposeProductVariable(self.v, W))
+
+    def affineTransform(self, W):
+        return Var(pl.AffineTransformVariable(self.v, W))
+
     def doubleProduct(self, W, M):
         return Var(pl.DoubleProductVariable(varray=[self.v, W, M]))
 
@@ -88,45 +93,78 @@
     
 # RLayer stands for reconstruciton hidden layer
 
-def addSigmoidRLayer(input, nh):
+def addSigmoidRLayer(input, iw, ow):
     &quot;&quot;&quot;Returns a triple (hidden, reconstruciton_cost, params)&quot;&quot;&quot;
-    ni = input.size()
-    W = Var(ni,nh)
-    hidden = input.matrixProduct(W).sigmoid()
+    W = Var(1+iw,ow)
+    hidden = input.affineTransform(W).sigmoid()
     cost = -hidden.matrixTransposeProduct(W).log_sigmoid().dot(input)
     return (hidden, cost, W)
 
-def addMultiSoftmaxRLayer(input, ing, igs, ong, ogs, tighed=True, use_double_product=True):
-    &quot;&quot;&quot;ing is the input's number of groups
+
+
+
+def addMultiSoftMaxDoubleProductTighedRLayer(input, iw, igs, ow, ogs):
+    &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
-    ong is the output's number of groups
-    ogs is the output's group size
-    If tighed is true we will use the transpose of the recognition weight matrix for reconstruction
-    If use_double_product is false we'll use a regular weight matrix.
-    If it's true, we'll use the decomposition as a 
-    Returns a triple (hidden, params, reonstruction_cost)&quot;&quot;&quot;
-    if not use_double_product:
-        W = Var(ing*igs,ong*ogs)
-        hidden = input.matrixProduct(W).multiSoftMax(ogs)
-        if tighed:
-            cost = -hidden.matrixTransposeProduct(W).multiLogSoftMax(igs).dot(input)
-            return hidden, cost, W
-        else:
-            Wr = Var(ong*ogs, ing*igs)
-            cost = -hidden.matrixProduct(Wr).multiLogSoftMax(igs)
-            return hidden, cost, (W, Wr)
+    ow and ogs analog but for output&quot;&quot;&quot;
+    M = Var(ow/ogs, iw, 'U', -1/iw, 1/iw, False)
+    W = Var(ogs, iw, 'U', -1/iw, 1/iw, False)
+    hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
+    cost = -hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs).dot(input)            
+    return hidden, cost, (W,M)
 
-    else: # use double product
-       # M = Var(ing*igs, ong, 'U', -1/ing/igs, 1/ing/igs, False)
-       # W = Var(ing*igs, ogs, 'U', -1/ing/igs, 1/ing/igs, False)
-        M = Var(ong, ing*igs, 'U', -1/ing/igs, 1/ing/igs, False)
-        W = Var(ogs, ing*igs, 'U', -1/ing/igs, 1/ing/igs, False)
-        hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
-        if tighed:
-            cost = -hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs).dot(input)            
-            return hidden, cost, (W,M)
-        else:
-            Mr = Var()
-            Wr = Var()
+def addMultiSoftMaxDoubleProductNotTighedRLayer(input, iw, igs, ow, ogs):
+    return 1,2,3
+
+def addMultiSoftMaxSimpleProductTighedRLayer(input, iw, igs, ow, ogs):
+    W = Var(iw, ow)
+    hidden = input.matrixProduct(W).multiSoftMax(ogs)
+    cost = -hidden.matrixTransposeProduct(W).multiLogSoftMax(igs).dot(input)
+    return hidden, cost, W
+
+def addMultiSoftMaxSimpleProductNotTighedRLayer(input, iw, igs, ow, ogs):
+    W = Var(iw,ow)
+    hidden = input.matrixProduct(W).multiSoftMax(ogs)
+    Wr = Var(ow, iw)
+    cost = -hidden.matrixProduct(Wr).multiLogSoftMax(igs)
+    return hidden, cost, (W, Wr)
+
+
+
+
+
+
+## def addMultiSoftmaxRLayer(input, ing, igs, ong, ogs, tighed=True, use_double_product=True):
+##     &quot;&quot;&quot;ing is the input's number of groups
+##     igs is the input's group size
+##     ong is the output's number of groups
+##     ogs is the output's group size
+##     If tighed is true we will use the transpose of the recognition weight matrix for reconstruction
+##     If use_double_product is false we'll use a regular weight matrix.
+##     If it's true, we'll use the decomposition as a 
+##     Returns a triple (hidden, params, reonstruction_cost)&quot;&quot;&quot;
+##     if not use_double_product:
+##         W = Var(ing*igs,ong*ogs)
+##         hidden = input.matrixProduct(W).multiSoftMax(ogs)
+##         if tighed:
+##             cost = -hidden.matrixTransposeProduct(W).multiLogSoftMax(igs).dot(input)
+##             return hidden, cost, W
+##         else:
+##             Wr = Var(ong*ogs, ing*igs)
+##             cost = -hidden.matrixProduct(Wr).multiLogSoftMax(igs)
+##             return hidden, cost, (W, Wr)
+
+##     else: # use double product
+##        # M = Var(ing*igs, ong, 'U', -1/ing/igs, 1/ing/igs, False)
+##        # W = Var(ing*igs, ogs, 'U', -1/ing/igs, 1/ing/igs, False)
+##         M = Var(ong, ing*igs, 'U', -1/ing/igs, 1/ing/igs, False)
+##         W = Var(ogs, ing*igs, 'U', -1/ing/igs, 1/ing/igs, False)
+##         hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
+##         if tighed:
+##             cost = -hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs).dot(input)            
+##             return hidden, cost, (W,M)
+##         else:
+##             Mr = Var()
+##             Wr = Var()
             
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000577.html">[Plearn-commits] r7128 - trunk/plearn/sys
</A></li>
	<LI>Next message: <A HREF="000579.html">[Plearn-commits] r7130 - in trunk/plearn_learners: generic online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#578">[ date ]</a>
              <a href="thread.html#578">[ thread ]</a>
              <a href="subject.html#578">[ subject ]</a>
              <a href="author.html#578">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
