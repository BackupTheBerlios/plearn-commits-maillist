<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6995 - trunk/plearn/ker
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6995%20-%20trunk/plearn/ker&In-Reply-To=%3C200705040216.l442GKMu023980%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000443.html">
   <LINK REL="Next"  HREF="000445.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6995 - trunk/plearn/ker</H1>
    <B>chapados at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6995%20-%20trunk/plearn/ker&In-Reply-To=%3C200705040216.l442GKMu023980%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6995 - trunk/plearn/ker">chapados at mail.berlios.de
       </A><BR>
    <I>Fri May  4 04:16:20 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000443.html">[Plearn-commits] r6994 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000445.html">[Plearn-commits] r6996 - trunk/commands
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#444">[ date ]</a>
              <a href="thread.html#444">[ thread ]</a>
              <a href="subject.html#444">[ subject ]</a>
              <a href="author.html#444">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: chapados
Date: 2007-05-04 04:16:17 +0200 (Fri, 04 May 2007)
New Revision: 6995

Added:
   trunk/plearn/ker/LinearARDKernel.cc
   trunk/plearn/ker/LinearARDKernel.h
Modified:
   trunk/plearn/ker/SummationKernel.cc
Log:
Added linear (dot-product) kernel with automatic-relevance determination -- useful for benchmarks

Added: trunk/plearn/ker/LinearARDKernel.cc
===================================================================
--- trunk/plearn/ker/LinearARDKernel.cc	2007-05-04 01:06:20 UTC (rev 6994)
+++ trunk/plearn/ker/LinearARDKernel.cc	2007-05-04 02:16:17 UTC (rev 6995)
@@ -0,0 +1,358 @@
+// -*- C++ -*-
+
+// LinearARDKernel.cc
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file LinearARDKernel.cc */
+
+
+#include &quot;LinearARDKernel.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    LinearARDKernel,
+    &quot;Linear kernel that can be used for Automatic Relevance Determination&quot;,
+    &quot;This is a simple linear (dot-product) kernel that provides a different\n&quot;
+    &quot;length-scale parameter for each input variable.  When used in conjunction\n&quot;
+    &quot;with GaussianProcessRegressor it yields a Bayesian linear regression model\n&quot;
+    &quot;with a non-isotropic prior.  (It is not a particularly efficient way of\n&quot;
+    &quot;performing linear regression, but can be useful as a benchmark against\n&quot;
+    &quot;other kernels).\n&quot;
+    &quot;\n&quot;
+    &quot;This kernel function is specified as:\n&quot;
+    &quot;\n&quot;
+    &quot;  k(x,y) = sf * (sum_i x_i * y_i / w_i) * k_kron(x,y)\n&quot;
+    &quot;\n&quot;
+    &quot;where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +\n&quot;
+    &quot;isp_input_sigma[i]), and k_kron(x,y) is the result of the\n&quot;
+    &quot;KroneckerBaseKernel evaluation, or 1.0 if there are no Kronecker terms.\n&quot;
+    &quot;Note that since the Kronecker terms are incorporated multiplicatively, the\n&quot;
+    &quot;very presence of the term associated to this kernel can be gated by the\n&quot;
+    &quot;value of some input variable(s) (that are incorporated within one or more\n&quot;
+    &quot;Kronecker terms).\n&quot;
+    &quot;\n&quot;
+    &quot;For best results, especially with moderately noisy data, IT IS IMPERATIVE\n&quot;
+    &quot;to use whis kernel within a SummationKernel in conjunction with an\n&quot;
+    &quot;IIDNoiseKernel, as follows (e.g. within a GaussianProcessRegressor):\n&quot;
+    &quot;\n&quot;
+    &quot;    kernel = SummationKernel(terms = [ LinearARDKernel(),\n&quot;
+    &quot;                                       IIDNoiseKernel() ] )\n&quot;
+    &quot;\n&quot;
+    &quot;Note that to make its operations more robust when used with unconstrained\n&quot;
+    &quot;optimization of hyperparameters, all hyperparameters of this kernel are\n&quot;
+    &quot;specified in the inverse softplus domain.  See IIDNoiseKernel for more\n&quot;
+    &quot;explanations.\n&quot;
+    );
+
+
+LinearARDKernel::LinearARDKernel()
+{ }
+
+
+//#####  declareOptions  ######################################################
+
+void LinearARDKernel::declareOptions(OptionList&amp; ol)
+{
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+
+//#####  build  ###############################################################
+
+void LinearARDKernel::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+
+//#####  build_  ##############################################################
+
+void LinearARDKernel::build_()
+{
+    // Ensure that we multiply in Kronecker terms
+    inherited::m_default_value = 1.0;
+}
+
+
+//#####  evaluate  ############################################################
+
+real LinearARDKernel::evaluate(const Vec&amp; x1, const Vec&amp; x2) const
+{
+    PLASSERT( x1.size() == x2.size() );
+    PLASSERT( !m_isp_input_sigma.size() || x1.size() == m_isp_input_sigma.size() );
+
+    real gating_term = inherited::evaluate(x1,x2);
+    if (fast_is_equal(gating_term, 0.0) || x1.size() == 0)
+        return 0.0;
+    
+    real the_dot    = 0.0;
+    if (m_isp_input_sigma.size() &gt; 0) {
+        const real* px1 = x1.data();
+        const real* px2 = x2.data();
+        const real* pinpsig = m_isp_input_sigma.data();
+        for (int i=0, n=x1.size() ; i&lt;n ; ++i) {
+            the_dot += (*px1++ * *px2++) / softplus(m_isp_global_sigma + *pinpsig++);
+        }
+    }
+    else {
+        real global_sigma = softplus(m_isp_global_sigma);
+        the_dot = dot(x1, x2) / global_sigma;
+    }
+
+    // Gate by Kronecker term
+    return softplus(m_isp_signal_sigma) * the_dot * gating_term;
+}
+
+
+//#####  computeGramMatrix  ###################################################
+
+void LinearARDKernel::computeGramMatrix(Mat K) const
+{
+    PLASSERT( !m_isp_input_sigma.size() || dataInputsize() == m_isp_input_sigma.size() );
+    PLASSERT( K.size() == 0 || m_data_cache.size() &gt; 0 );  // Ensure data cached OK
+
+    // Compute Kronecker gram matrix
+    inherited::computeGramMatrix(K);
+
+    // Precompute some terms. Make sure that the input sigmas don't get too
+    // small
+    real sf = softplus(m_isp_signal_sigma);
+    m_input_sigma.resize(dataInputsize());
+    softplusFloor(m_isp_global_sigma, 1e-6);
+    m_input_sigma.fill(m_isp_global_sigma);  // Still in ISP domain
+    for (int i=0, n=m_input_sigma.size() ; i&lt;n ; ++i) {
+        if (m_isp_input_sigma.size() &gt; 0) {
+            softplusFloor(m_isp_input_sigma[i], 1e-6);
+            m_input_sigma[i] += m_isp_input_sigma[i];
+        }
+        m_input_sigma[i] = softplus(m_input_sigma[i]);
+    }
+
+    // Compute Gram Matrix
+    int  l = data-&gt;length();
+    int  m = K.mod();
+    int  n = dataInputsize();
+    int  cache_mod = m_data_cache.mod();
+
+    real *data_start = &amp;m_data_cache(0,0);
+    real *Ki = K[0];                         // Start of current row
+    real *Kij;                               // Current element along row
+    real *input_sigma_data = m_input_sigma.data();
+    real *xi = data_start;
+    
+    for (int i=0 ; i&lt;l ; ++i, xi += cache_mod, Ki+=m)
+    {
+        Kij = Ki;
+        real *xj = data_start;
+
+        for (int j=0; j&lt;=i; ++j, xj += cache_mod) {
+            // Kernel evaluation per se
+            real *x1 = xi;
+            real *x2 = xj;
+            real *p_inpsigma = input_sigma_data;
+            real the_dot = 0.0;
+            int  k = n;
+
+            // Use Duff's device to unroll the following loop:
+            //     while (k--) {
+            //         the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+            //     }
+            switch (k % 8) {
+            case 0: do { the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+            case 7:      the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+            case 6:      the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+            case 5:      the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+            case 4:      the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+            case 3:      the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+            case 2:      the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+            case 1:      the_dot += (*x1++ * *x2++) / *p_inpsigma++;
+                       } while((k -= 8) &gt; 0);
+            }
+
+            // Multiplicatively update kernel matrix (already pre-filled with
+            // Kronecker terms, or 1.0 if no Kronecker terms, as per build_).
+            real Kij_cur = *Kij * sf * the_dot;
+            *Kij++ = Kij_cur;
+        }
+    }
+    if (cache_gram_matrix) {
+        gram_matrix.resize(l,l);
+        gram_matrix &lt;&lt; K;
+        gram_matrix_is_cached = true;
+    }
+}
+
+
+//#####  computeGramMatrixDerivative  #########################################
+
+void LinearARDKernel::computeGramMatrixDerivative(
+    Mat&amp; KD, const string&amp; kernel_param, real epsilon) const
+{
+    static const string ISS(&quot;isp_signal_sigma&quot;);
+    static const string IGS(&quot;isp_global_sigma&quot;);
+    static const string IIS(&quot;isp_input_sigma[&quot;);
+
+    if (kernel_param == ISS) {
+        computeGramMatrixDerivIspSignalSigma(KD);
+        
+        // computeGramMatrixDerivNV&lt;
+        //     LinearARDKernel,
+        //     &amp;LinearARDKernel::derivIspSignalSigma&gt;(KD, this, -1);
+    }
+    else if (kernel_param == IGS) {
+        computeGramMatrixDerivNV&lt;
+            LinearARDKernel,
+            &amp;LinearARDKernel::derivIspGlobalSigma&gt;(KD, this, -1);
+    }
+    else if (string_begins_with(kernel_param, IIS) &amp;&amp;
+             kernel_param[kernel_param.size()-1] == ']')
+    {
+        int arg = tolong(kernel_param.substr(
+                             IIS.size(), kernel_param.size() - IIS.size() - 1));
+        PLASSERT( arg &lt; m_isp_input_sigma.size() );
+
+        computeGramMatrixDerivIspInputSigma(KD, arg);
+
+    }
+    else
+        inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
+}
+
+
+//#####  derivIspSignalSigma  #################################################
+
+real LinearARDKernel::derivIspSignalSigma(int i, int j, int arg, real K) const
+{
+    // (No longer used; see computeGramMatrixDerivIspInputSigma below)
+    return K*sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+
+//#####  derivIspGlobalSigma  #################################################
+
+real LinearARDKernel::derivIspGlobalSigma(int i, int j, int arg, real K) const
+{
+    if (fast_is_equal(K,0.))
+        return 0.;
+
+    return - K * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
+}
+
+
+//#####  computeGramMatrixDerivIspSignalSigma  ################################
+
+void LinearARDKernel::computeGramMatrixDerivIspSignalSigma(Mat&amp; KD) const
+{
+    int l = data-&gt;length();
+    KD.resize(l,l);
+    PLASSERT_MSG(
+        gram_matrix.width() == l &amp;&amp; gram_matrix.length() == l,
+        &quot;To compute the derivative with respect to 'isp_signal_sigma', the\n&quot;
+        &quot;Gram matrix must be precomputed and cached in LinearARDKernel.&quot;);
+    
+    KD &lt;&lt; gram_matrix;
+    KD *= sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+
+//#####  computeGramMatrixDerivIspInputSigma  #################################
+
+void LinearARDKernel::computeGramMatrixDerivIspInputSigma(Mat&amp; KD, int arg) const
+{
+    // Precompute some terms
+    real signal_sigma    = softplus(m_isp_signal_sigma);
+    real input_sigma_arg = m_input_sigma[arg];
+    real input_sigma_sq  = input_sigma_arg * input_sigma_arg;
+    real input_sigmoid   = sigmoid(m_isp_global_sigma + m_isp_input_sigma[arg]);
+    
+    // Compute Gram Matrix derivative w.r.t. isp_input_sigma[arg]
+    int  l = data-&gt;length();
+    PLASSERT_MSG(
+        gram_matrix.width() == l &amp;&amp; gram_matrix.length() == l,
+        &quot;To compute the derivative with respect to 'isp_input_sigma[i]', the\n&quot;
+        &quot;Gram matrix must be precomputed and cached in LinearARDKernel.&quot;);
+
+    // Variables that walk over the data matrix
+    int  cache_mod = m_data_cache.mod();
+    real *data_start = &amp;m_data_cache(0,0);
+    real *xi = data_start+arg;               // Iterator on data rows
+
+    // Variables that walk over the kernel derivative matrix (KD)
+    KD.resize(l,l);
+    real* KDi = KD.data();                   // Start of row i
+    real* KDij;                              // Current element on row i
+    int   KD_mod = KD.mod();
+
+    // Iterate on rows of derivative matrix
+    for (int i=0 ; i&lt;l ; ++i, xi += cache_mod, KDi += KD_mod)
+    {
+        KDij = KDi;
+        real *xj  = data_start+arg;           // Inner iterator on data rows
+
+        // Iterate on columns of derivative matrix
+        for (int j=0 ; j &lt;= i ; ++j, xj += cache_mod)
+        {
+            // Set into derivative matrix
+            *KDij++ = - signal_sigma * (*xi * *xj) * input_sigmoid / input_sigma_sq;
+        }
+    }
+}
+
+
+//#####  makeDeepCopyFromShallowCopy  #########################################
+
+void LinearARDKernel::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/LinearARDKernel.h
===================================================================
--- trunk/plearn/ker/LinearARDKernel.h	2007-05-04 01:06:20 UTC (rev 6994)
+++ trunk/plearn/ker/LinearARDKernel.h	2007-05-04 02:16:17 UTC (rev 6995)
@@ -0,0 +1,161 @@
+// -*- C++ -*-
+
+// LinearARDKernel.h
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file LinearARDKernel.h */
+
+
+#ifndef LinearARDKernel_INC
+#define LinearARDKernel_INC
+
+#include &lt;plearn/ker/ARDBaseKernel.h&gt;
+
+namespace PLearn {
+
+/**
+ *  Linear kernel that can be used for Automatic Relevance Determination
+ *
+ *  This is a simple linear (dot-product) kernel that provides a different
+ *  length-scale parameter for each input variable.  When used in conjunction
+ *  with GaussianProcessRegressor it yields a Bayesian linear regression model
+ *  with a non-isotropic prior.  (It is not a particularly efficient way of
+ *  performing linear regression, but can be useful as a benchmark against
+ *  other kernels).
+ *
+ *  This kernel function is specified as:
+ *
+ *    k(x,y) = sf * (sum_i x_i * y_i / w_i) * k_kron(x,y)
+ *
+ *  where sf is softplus(isp_signal_sigma), w_i is softplus(isp_global_sigma +
+ *  isp_input_sigma[i]), and k_kron(x,y) is the result of the
+ *  KroneckerBaseKernel evaluation, or 1.0 if there are no Kronecker terms.
+ *  Note that since the Kronecker terms are incorporated multiplicatively, the
+ *  very presence of the term associated to this kernel can be gated by the
+ *  value of some input variable(s) (that are incorporated within one or more
+ *  Kronecker terms).
+ *
+ *  For best results, especially with moderately noisy data, IT IS IMPERATIVE
+ *  to use whis kernel within a SummationKernel in conjunction with an
+ *  IIDNoiseKernel, as follows (e.g. within a GaussianProcessRegressor):
+ *
+ *      kernel = SummationKernel(terms = [ LinearARDKernel(),
+ *                                         IIDNoiseKernel() ] )
+ *
+ *  Note that to make its operations more robust when used with unconstrained
+ *  optimization of hyperparameters, all hyperparameters of this kernel are
+ *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
+ *  explanations.
+ */
+class LinearARDKernel : public ARDBaseKernel
+{
+    typedef ARDBaseKernel inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    // (No new options other than those inherited)
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    LinearARDKernel();
+
+
+    //#####  Kernel Member Functions  #########################################
+
+    //! Compute K(x1,x2).
+    virtual real evaluate(const Vec&amp; x1, const Vec&amp; x2) const;
+
+    //! Compute the Gram Matrix.
+    virtual void computeGramMatrix(Mat K) const;
+    
+    //! Directly compute the derivative with respect to hyperparameters
+    //! (Faster than finite differences...)
+    virtual void computeGramMatrixDerivative(Mat&amp; KD, const string&amp; kernel_param,
+                                             real epsilon=1e-6) const;
+    
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(LinearARDKernel);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+    //! Derivative function with respect to isp_signal_sigma
+    real derivIspSignalSigma(int i, int j, int arg, real K) const;
+
+    //! Derivative function with respect to isp_global_sigma
+    real derivIspGlobalSigma(int i, int j, int arg, real K) const;
+    
+    // Compute derivative w.r.t. isp_signal_sigma for WHOLE MATRIX
+    void computeGramMatrixDerivIspSignalSigma(Mat&amp; KD) const;
+    
+    // Compute derivative w.r.t. isp_input_sigma[arg] for WHOLE MATRIX
+    void computeGramMatrixDerivIspInputSigma(Mat&amp; KD, int arg) const;
+    
+private:
+    //! This does the actual building.
+    void build_();
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LinearARDKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-05-04 01:06:20 UTC (rev 6994)
+++ trunk/plearn/ker/SummationKernel.cc	2007-05-04 02:16:17 UTC (rev 6995)
@@ -264,10 +264,10 @@
     // Mat KD1;
     // Kernel::computeGramMatrixDerivative(KD1, kernel_param, epsilon);
     // cerr &lt;&lt; &quot;Kernel hyperparameter: &quot; &lt;&lt; kernel_param &lt;&lt; endl;
-    // cerr &lt;&lt; &quot;Analytic derivative (200th row):&quot; &lt;&lt; endl
-    //      &lt;&lt; KD(200) &lt;&lt; endl
+    // cerr &lt;&lt; &quot;Analytic derivative (15th row):&quot; &lt;&lt; endl
+    //      &lt;&lt; KD(15) &lt;&lt; endl
     //      &lt;&lt; &quot;Finite differences:&quot; &lt;&lt; endl
-    //      &lt;&lt; KD1(200) &lt;&lt; endl;
+    //      &lt;&lt; KD1(15) &lt;&lt; endl;
 }
 
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000443.html">[Plearn-commits] r6994 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000445.html">[Plearn-commits] r6996 - trunk/commands
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#444">[ date ]</a>
              <a href="thread.html#444">[ thread ]</a>
              <a href="subject.html#444">[ subject ]</a>
              <a href="author.html#444">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
