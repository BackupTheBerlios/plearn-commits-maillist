<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6980 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6980%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705030156.l431uRpQ002920%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000428.html">
   <LINK REL="Next"  HREF="000430.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6980 - trunk/plearn_learners/online</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6980%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705030156.l431uRpQ002920%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6980 - trunk/plearn_learners/online">yoshua at mail.berlios.de
       </A><BR>
    <I>Thu May  3 03:56:27 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000428.html">[Plearn-commits] r6979 - trunk/plearn/math
</A></li>
        <LI>Next message: <A HREF="000430.html">[Plearn-commits] r6981 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#429">[ date ]</a>
              <a href="thread.html#429">[ thread ]</a>
              <a href="subject.html#429">[ subject ]</a>
              <a href="author.html#429">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-05-03 03:56:25 +0200 (Thu, 03 May 2007)
New Revision: 6980

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
Log:
handling reconstruction error in DeepBeliefNet


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-05-03 01:07:17 UTC (rev 6979)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-05-03 01:56:25 UTC (rev 6980)
@@ -76,7 +76,7 @@
     final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
     class_cost_index( -1 ),
-    recons_cost_index( -1 )
+    reconstruction_cost_index( -1 )
 {
     random_gen = new PRandom( seed_ );
 }
@@ -137,7 +137,7 @@
     declareOption(ol, &quot;reconstruct_layerwise&quot;,
                   &amp;DeepBeliefNet::reconstruct_layerwise,
                   OptionBase::buildoption,
-                  &quot;Minimize reconstruction error of each layer as an auto-encoder.\n&quot;
+                  &quot;Compute reconstruction error of each layer as an auto-encoder.\n&quot;
                   &quot;This is done using cross-entropy between actual and reconstructed.\n&quot;
                   &quot;This option automatically adds the following cost names:\n&quot;
                   &quot;   layerwise_reconstruction_error (sum over all layers)\n&quot;
@@ -304,6 +304,7 @@
         build_final_cost();
 
     // build_costs(); /* ? */
+    reconstruction_costs.resize(reconstruct_layerwise?n_layers-1:0);
 }
 
 //////////////////////////////////
@@ -635,7 +636,8 @@
             }
     }
 
-    recons_cost_index = train_cost_names.find(&quot;recons_error&quot;);
+    if ( reconstruct_layerwise)
+        reconstruction_cost_index = train_cost_names.find(&quot;layerwise_reconstruction_error&quot;);
 
     class_cost_index = train_cost_names.find(&quot;class_error&quot;);
 
@@ -650,8 +652,6 @@
 
     PP&lt;ProgressBar&gt; pb;
 
-    real train_recons_error = 0.0;
-
     // clear stats of previous epoch
     train_stats-&gt;forget();
 
@@ -686,6 +686,7 @@
             MODULE_LOG &lt;&lt; &quot;Training connection weights between layers &quot; &lt;&lt; i
                        &lt;&lt; &quot; and &quot; &lt;&lt; i+1 &lt;&lt; endl;
 
+            int first_stage = stage;
             int end_stage = min( training_schedule[i], nstages );
 
             MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
@@ -695,7 +696,7 @@
             if( report_progress &amp;&amp; stage &lt; end_stage )
                 pb = new ProgressBar( &quot;Training layer &quot;+tostring(i)
                                       +&quot; of &quot;+classname(),
-                                      end_stage - stage );
+                                      end_stage - first_stage);
 
             layers[i]-&gt;setLearningRate( cd_learning_rate );
             connections[i]-&gt;setLearningRate( cd_learning_rate );
@@ -709,7 +710,14 @@
                     if (batch_size &gt; 1 || minibatch_hack) {
                         train_set-&gt;getExamples(sample_start, minibatch_size,
                                 inputs, targets, weights, NULL, true);
-                        greedyStep( inputs, targets, i );
+                        if (reconstruct_layerwise)
+                        {
+                            train_costs_m.fill(MISSING_VALUE);
+                            train_costs_m.column(reconstruction_cost_index).clear();
+                        }
+                        greedyStep( inputs, targets, i , train_costs_m);
+                        for (int k = 0; k &lt; minibatch_size; k++)
+                            train_stats-&gt;update(train_costs_m(k));
                     } else {
                         train_set-&gt;getExample(sample_start, input, target, weight);
                         greedyStep( input, target, i );
@@ -717,10 +725,10 @@
 
                 }
                 if( pb )
-                    if( i == 0 )
-                        pb-&gt;update( stage + 1 );
+                    pb-&gt;update( stage - first_stage + 1 );
+/*                    if( i == 0 )
                     else
-                        pb-&gt;update( stage - training_schedule[i-1] + 1 );
+                    pb-&gt;update( stage - training_schedule[i-1] + 1 ); */
             }
         }
 
@@ -761,51 +769,7 @@
             }
         }
 
-        /**** compute reconstruction error*****/
-        PP&lt;RBMLayer&gt; down_layer = get_pointer(layers[0]) ;
-        PP&lt;RBMLayer&gt; up_layer =  get_pointer(layers[1]) ;
-        PP&lt;RBMConnection&gt; parameters = get_pointer(connections[0]);
 
-        // TODO Do we really want to systematically compute this reconstruction
-        // error?
-
-        if (batch_size == 1) {
-        for(int train_index = 0 ; train_index &lt; nsamples ; train_index++)
-        {
-
-            PLASSERT_MSG(batch_size == 1, &quot;Not implemented for mini-batches&quot;);
-
-            train_set-&gt;getExample( train_index, input, target, weight );
-
-            down_layer-&gt;expectation &lt;&lt; input;
-
-            // up
-            parameters-&gt;setAsDownInput( down_layer-&gt;expectation );
-            up_layer-&gt;getAllActivations( parameters );
-            up_layer-&gt;generateSample();
-
-            // down
-            parameters-&gt;setAsUpInput( up_layer-&gt;sample );
-
-            down_layer-&gt;getAllActivations( parameters );
-            down_layer-&gt;computeExpectation();
-            down_layer-&gt;generateSample();
-
-            //    result += powdistance( input, down_layer-&gt;expectation );
-
-            for( int i=0 ; i&lt;input.size() ; i++ )
-                train_recons_error += (input[i] - down_layer-&gt;expectation[i])
-                    * (input[i] - down_layer-&gt;expectation[i]);
-
-        }
-
-        train_recons_error /= nsamples ;
-        } else {
-            // Currently do not compute reconstruction error with mini-batches.
-            train_recons_error = MISSING_VALUE;
-        }
-
-
         /***** fine-tuning by gradient descent *****/
         if( stage &gt;= nstages )
             return;
@@ -860,11 +824,6 @@
         }
     }
 
-    //update the reconstruction error
-    train_costs.fill( MISSING_VALUE );
-    train_costs[recons_cost_index] = train_recons_error;
-    train_stats-&gt;update( train_costs ) ;
-
     train_stats-&gt;finalize();
 
 }
@@ -1137,7 +1096,7 @@
 /////////////////
 // greedySteps //
 /////////////////
-void DeepBeliefNet::greedyStep( const Mat&amp; inputs, const Mat&amp; targets, int index )
+void DeepBeliefNet::greedyStep( const Mat&amp; inputs, const Mat&amp; targets, int index, Mat&amp; train_costs_m )
 {
     PLASSERT( index &lt; n_layers );
 
@@ -1188,6 +1147,18 @@
                                connections[ index ],
                                layers[ index+1 ],
                                index, true);
+
+    if (reconstruct_layerwise)
+    {
+        connections[index]-&gt;setAsUpInputs(layers[index+1]-&gt;getExpectations());
+        layers[index]-&gt;getAllActivations(connections[index], 0, true);
+        for (int i=0;i&lt;inputs.length();i++)
+        {
+            real rc = train_costs_m(i,reconstruction_cost_index+index+1)
+                = layers[index]-&gt;fpropNLL( inputs(i) ); 
+            train_costs_m(i,reconstruction_cost_index) += rc;
+        }
+    }
 }
 
 /////////////////////
@@ -1655,11 +1626,21 @@
 
     // fprop
     layers[0]-&gt;expectation &lt;&lt; input;
+    reconstruction_costs[0]=0;
     for( int i=0 ; i&lt;n_layers-2 ; i++ )
     {
         connections[i]-&gt;setAsDownInput( layers[i]-&gt;expectation );
         layers[i+1]-&gt;getAllActivations( connections[i] );
         layers[i+1]-&gt;computeExpectation();
+
+        if (reconstruct_layerwise)
+        {
+            Vec layer_input = layers[i]-&gt;expectation.copy();
+            connections[i]-&gt;setAsUpInputs(layers[i+1]-&gt;getExpectations());
+            layers[i]-&gt;getAllActivations(connections[i]);
+            real rc = reconstruction_costs[i+1] = layers[i]-&gt;fpropNLL( layer_input ); 
+            reconstruction_costs[0] += rc;
+        }
     }
 
 
@@ -1741,6 +1722,10 @@
             }
     }
 
+    if (reconstruct_layerwise)
+        costs.subVec(reconstruction_cost_index,reconstruction_costs.length()) 
+            &lt;&lt; reconstruction_costs;
+
 }
 
 TVec&lt;string&gt; DeepBeliefNet::getTestCostNames() const
@@ -1769,14 +1754,20 @@
                     cost_names.append( names[j] + &quot;_&quot; + tostring(i+1) );
             }
 
+    if (reconstruct_layerwise)
+    {
+        reconstruction_cost_index = cost_names.length();
+        cost_names.append(&quot;layerwise_reconstruction_error&quot;);
+        for (int i=0;i&lt;n_layers-1;i++)
+            cost_names.append(&quot;layer&quot;+tostring(i+1)+&quot;_reconstruction_error&quot;);
+    }
+
     return cost_names;
 }
 
 TVec&lt;string&gt; DeepBeliefNet::getTrainCostNames() const
 {
     TVec&lt;string&gt; cost_names = getTestCostNames() ;
-
-    cost_names.append(&quot;recons_error&quot;);
     return cost_names;
 }
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-05-03 01:07:17 UTC (rev 6979)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-05-03 01:56:25 UTC (rev 6980)
@@ -223,7 +223,7 @@
     void greedyStep( const Vec&amp; input, const Vec&amp; target, int index );
 
     //! Greedy step with mini-batches.
-    void greedyStep(const Mat&amp; inputs, const Mat&amp; targets, int index);
+    void greedyStep(const Mat&amp; inputs, const Mat&amp; targets, int index, Mat&amp; train_costs_m);
 
     void jointGreedyStep( const Vec&amp; input, const Vec&amp; target );
 
@@ -346,11 +346,13 @@
     TVec&lt;int&gt; final_cost_indices;
 
     //! Keeps the index of the reconstruction cost in train_costs
-    int recons_cost_index;
+    mutable int reconstruction_cost_index;
 
     //! indices of the partial costs in train_costs
     TVec&lt;TVec&lt;int&gt; &gt; partial_cost_indices;
 
+    Vec reconstruction_costs;
+
 protected:
     //#####  Protected Member Functions  ######################################
 

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-05-03 01:07:17 UTC (rev 6979)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-05-03 01:56:25 UTC (rev 6980)
@@ -287,6 +287,18 @@
     return ret;
 }
 
+real RBMBinomialLayer::fpropNLL(const Mat&amp; target)
+{
+    computeExpectations();
+
+    PLASSERT( target.width() == input_size );
+
+    real total_nll=0;
+    for (int i=0;i&lt;target.length();i++)
+        total_nll += fpropNLL(target(i));
+    return total_nll;
+}
+
 void RBMBinomialLayer::bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient)
 {
     computeExpectation();

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-05-03 01:07:17 UTC (rev 6979)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-05-03 01:56:25 UTC (rev 6980)
@@ -106,6 +106,7 @@
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec&amp; target);
+    virtual real fpropNLL(const Mat&amp; target);
 
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-05-03 01:07:17 UTC (rev 6979)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-05-03 01:56:25 UTC (rev 6980)
@@ -263,6 +263,12 @@
     return REAL_MAX;
 }
 
+real RBMLayer::fpropNLL(const Mat&amp; target)
+{
+    PLERROR(&quot;In RBMLayer::fpropNLL(): not implemented&quot;);
+    return REAL_MAX;
+}
+
 void RBMLayer::bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient)
 {
     PLERROR(&quot;In RBMLayer::bpropNLL(): not implemented&quot;);

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-05-03 01:07:17 UTC (rev 6979)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-05-03 01:56:25 UTC (rev 6980)
@@ -172,6 +172,7 @@
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec&amp; target);
+    virtual real fpropNLL(const Mat&amp; target);
 
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000428.html">[Plearn-commits] r6979 - trunk/plearn/math
</A></li>
	<LI>Next message: <A HREF="000430.html">[Plearn-commits] r6981 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#429">[ date ]</a>
              <a href="thread.html#429">[ thread ]</a>
              <a href="subject.html#429">[ subject ]</a>
              <a href="author.html#429">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
