<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7098 - trunk/plearn_learners/online/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7098%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200705151504.l4FF4fFt019725%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000546.html">
   <LINK REL="Next"  HREF="000548.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7098 - trunk/plearn_learners/online/EXPERIMENTAL</H1>
    <B>tihocan at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7098%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200705151504.l4FF4fFt019725%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7098 - trunk/plearn_learners/online/EXPERIMENTAL">tihocan at mail.berlios.de
       </A><BR>
    <I>Tue May 15 17:04:41 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000546.html">[Plearn-commits] r7097 - trunk/python_modules/plearn/pyplearn
</A></li>
        <LI>Next message: <A HREF="000548.html">[Plearn-commits] r7099 - trunk/plearn_learners/online/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#547">[ date ]</a>
              <a href="thread.html#547">[ thread ]</a>
              <a href="subject.html#547">[ subject ]</a>
              <a href="author.html#547">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: tihocan
Date: 2007-05-15 17:04:40 +0200 (Tue, 15 May 2007)
New Revision: 7098

Added:
   trunk/plearn_learners/online/EXPERIMENTAL/RBMModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/RBMModule.h
Log:
Module that encapsulates a whole RBM

Added: trunk/plearn_learners/online/EXPERIMENTAL/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/RBMModule.cc	2007-05-15 15:03:51 UTC (rev 7097)
+++ trunk/plearn_learners/online/EXPERIMENTAL/RBMModule.cc	2007-05-15 15:04:40 UTC (rev 7098)
@@ -0,0 +1,340 @@
+// -*- C++ -*-
+
+// RBMModule.cc
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file RBMModule.cc */
+
+
+
+#include &quot;RBMModule.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMModule,
+    &quot;A Restricted Boltzmann Machine.&quot;,
+    &quot;&quot;
+);
+
+///////////////
+// RBMModule //
+///////////////
+RBMModule::RBMModule():
+    cd_learning_rate(0),
+    grad_learning_rate(0)
+{
+}
+
+void RBMModule::declareOptions(OptionList&amp; ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the &quot;flags&quot; of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, &quot;visible_layer&quot;, &amp;RBMModule::visible_layer,
+                  OptionBase::buildoption,
+        &quot;Visible layer of the RBM.&quot;);
+
+    declareOption(ol, &quot;hidden_layer&quot;, &amp;RBMModule::hidden_layer,
+                  OptionBase::buildoption,
+        &quot;Hidden layer of the RBM.&quot;);
+
+    declareOption(ol, &quot;connection&quot;, &amp;RBMModule::connection,
+                  OptionBase::buildoption,
+        &quot;Connection between the visible and hidden layers.&quot;);
+
+    declareOption(ol, &quot;grad_learning_rate&quot;, &amp;RBMModule::grad_learning_rate,
+                  OptionBase::buildoption,
+        &quot;Learning rate for the gradient descent step.&quot;);
+
+    declareOption(ol, &quot;cd_learning_rate&quot;, &amp;RBMModule::cd_learning_rate,
+                  OptionBase::buildoption,
+        &quot;Learning rate for the constrastive divergence step.&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void RBMModule::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                               const TVec&lt;Mat*&gt;&amp; ports_gradient)
+{
+    PLASSERT( ports_gradient.length() == nPorts() );
+    Mat* visible_grad = ports_gradient[0];
+    Mat* hidden_grad = ports_gradient[1];
+    if (hidden_grad &amp;&amp; !hidden_grad-&gt;isEmpty() &amp;&amp;
+            (!visible_grad || visible_grad-&gt;isEmpty()))
+    {
+        if (cd_learning_rate &gt; 0) {
+            PLERROR(&quot;In RBMModule::bpropAccUpdate - Contrastive Divergence &quot;
+                    &quot;not implemented yet&quot;);
+        }
+        if (grad_learning_rate &gt; 0) {
+            hidden_layer-&gt;setLearningRate(grad_learning_rate);
+            visible_layer-&gt;setLearningRate(grad_learning_rate);
+            connection-&gt;setLearningRate(grad_learning_rate);
+            Mat* hidden_out = ports_value[1];
+            Mat* hidden_act = ports_value[2];
+            PLASSERT( hidden_out &amp;&amp; hidden_act );
+            // Compute gradient w.r.t. activations of the hidden layer.
+            hidden_layer-&gt;bpropUpdate(
+                    *hidden_act, *hidden_out, hidden_act_grad, *hidden_grad,
+                    false);
+            Mat* visible_out = ports_value[0];
+            // Compute gradient w.r.t. expectations of the visible layer (=
+            // inputs).
+            Mat* store_visible_grad = NULL;
+            if (visible_grad) {
+                PLASSERT( visible_grad-&gt;width() == visible_layer-&gt;size );
+                store_visible_grad = visible_grad;
+            } else {
+                // We do not actually need to store the gradient, but since it
+                // is required in bpropUpdate, we provide a dummy matrix to
+                // store it.
+                store_visible_grad = &amp;visible_exp_grad;
+            }
+            store_visible_grad-&gt;resize(hidden_grad-&gt;length(),
+                                       visible_layer-&gt;size);
+            connection-&gt;bpropUpdate(
+                    *visible_out, *hidden_act, *store_visible_grad,
+                    hidden_act_grad, true);
+        }
+    } else
+        PLERROR(&quot;In RBMModule::bpropAccUpdate - Only hidden -&gt; visible &quot;
+                &quot;back propagation is currently implemented&quot;);
+}
+
+////////////
+// build_ //
+////////////
+void RBMModule::build_()
+{
+    PLASSERT( cd_learning_rate &gt;= 0 &amp;&amp; grad_learning_rate &gt;= 0 );
+    if (fast_exact_is_equal(cd_learning_rate, 0) &amp;&amp;
+        fast_exact_is_equal(grad_learning_rate, 0) )
+        PLWARNING(&quot;In RBMModule::build_ - Both 'cd_learning_rate' and &quot;
+                &quot;'grad_learning_rate' are set to 0, the RBM will not learn &quot;
+                &quot;much&quot;);
+}
+
+///////////
+// build //
+///////////
+void RBMModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void RBMModule::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR(&quot;RBMModule::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+}
+
+///////////
+// fprop //
+///////////
+void RBMModule::fprop(const Vec&amp; input, Vec&amp; output) const
+{
+    PLERROR(&quot;In RBMModule::fprop - Not implemented&quot;);
+}
+
+void RBMModule::fprop(const TVec&lt;Mat*&gt;&amp; ports_value)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    Mat* visible = ports_value[0];
+    Mat* hidden = ports_value[1];
+    Mat* hidden_act = ports_value[2];
+    if ( visible &amp;&amp; hidden &amp;&amp;
+         !visible-&gt;isEmpty() &amp;&amp; hidden-&gt;isEmpty() )
+    {
+        visible_layer-&gt;setExpectations(*visible);
+        connection-&gt;setAsDownInputs(visible_layer-&gt;getExpectations());
+        hidden_layer-&gt;getAllActivations(connection, 0, true);
+        if (hidden_act) {
+            // Also store hidden layer activations.
+            PLASSERT( hidden_act-&gt;isEmpty() );
+            const Mat&amp; to_store = hidden_layer-&gt;activations;
+            hidden_act-&gt;resize(to_store.length(), to_store.width());
+            *hidden_act &lt;&lt; to_store;
+        }
+        hidden_layer-&gt;computeExpectations();
+        Mat&amp; hidden_out = hidden_layer-&gt;getExpectations();
+        hidden-&gt;resize(hidden_out.length(), hidden_out.width());
+        *hidden &lt;&lt; hidden_out;
+    } else {
+        PLERROR(&quot;In RBMModule::fprop - Only bottom-up fprop is currently &quot;
+                &quot;implemented&quot; );
+    }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+/* THIS METHOD IS OPTIONAL
+void RBMModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                               Vec&amp; input_gradient,
+                               const Vec&amp; output_gradient,
+                               bool accumulate)
+{
+}
+*/
+
+/* THIS METHOD IS OPTIONAL
+void RBMModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                               const Vec&amp; output_gradient)
+{
+}
+*/
+
+//////////////////
+// bbpropUpdate //
+//////////////////
+/* THIS METHOD IS OPTIONAL
+void RBMModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                Vec&amp; input_gradient,
+                                const Vec&amp; output_gradient,
+                                Vec&amp; input_diag_hessian,
+                                const Vec&amp; output_diag_hessian,
+                                bool accumulate)
+{
+}
+*/
+
+/* THIS METHOD IS OPTIONAL
+void RBMModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                const Vec&amp; output_gradient,
+                                const Vec&amp; output_diag_hessian)
+{
+}
+*/
+
+////////////
+// forget //
+////////////
+void RBMModule::forget()
+{
+}
+
+//////////////
+// finalize //
+//////////////
+/* THIS METHOD IS OPTIONAL
+void RBMModule::finalize()
+{
+}
+*/
+
+//////////////
+// getPorts //
+//////////////
+const TVec&lt;string&gt;&amp; RBMModule::getPorts()
+{
+    static TVec&lt;string&gt; ports;
+    if (ports.isEmpty()) {
+        ports.append(&quot;visible&quot;);
+        ports.append(&quot;hidden&quot;);
+        ports.append(&quot;hidden_activations&quot;);
+    }
+    return ports;
+}
+
+///////////////////
+// getPortsSizes //
+///////////////////
+const TMat&lt;int&gt;&amp; RBMModule::getPortSizes() {
+    port_sizes.resize(2, 2);
+    port_sizes.fill(-1);
+    if (visible_layer)
+        port_sizes(0, 1) = visible_layer-&gt;size;
+    if (hidden_layer) {
+        port_sizes(1, 1) = hidden_layer-&gt;size;
+        port_sizes(2, 1) = hidden_layer-&gt;size;
+    }
+    return port_sizes;
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+bool RBMModule::bpropDoesNothing()
+{
+}
+*/
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+/* OPTIONAL
+void RBMModule::setLearningRate(real dynamic_learning_rate)
+{
+}
+*/
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/RBMModule.h	2007-05-15 15:03:51 UTC (rev 7097)
+++ trunk/plearn_learners/online/EXPERIMENTAL/RBMModule.h	2007-05-15 15:04:40 UTC (rev 7098)
@@ -0,0 +1,238 @@
+// -*- C++ -*-
+
+// RBMModule.h
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file RBMModule.h */
+
+
+#ifndef RBMModule_INC
+#define RBMModule_INC
+
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn_learners/online/RBMConnection.h&gt;
+#include &lt;plearn_learners/online/RBMLayer.h&gt;
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class RBMModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    PP&lt;RBMLayer&gt; hidden_layer;
+    PP&lt;RBMLayer&gt; visible_layer;
+    PP&lt;RBMConnection&gt; connection;
+
+    real cd_learning_rate;
+    real grad_learning_rate;
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    RBMModule();
+
+    // Your other public member functions go here
+
+    //! given the input, compute the output (possibly resize it  appropriately)
+    virtual void fprop(const Vec&amp; input, Vec&amp; output) const;
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient,
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
+    */
+
+    /* Optional
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             const Vec&amp; output_gradient);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                              Vec&amp; input_gradient,
+                              const Vec&amp; output_gradient,
+                              Vec&amp; input_diag_hessian,
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                              const Vec&amp; output_gradient,
+                              const Vec&amp; output_diag_hessian);
+    */
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    /* Optional
+       Default implementation prints a warning and does nothing
+    //! If this class has a learning rate (or something close to it), set it.
+    //! If not, you can redefine this method to get rid of the warning.
+    virtual void setLearningRate(real dynamic_learning_rate);
+    */
+
+    //! Overridden.
+    virtual void fprop(const TVec&lt;Mat*&gt;&amp; ports_value);
+
+    //! Overridden.
+    virtual void bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                                const TVec&lt;Mat*&gt;&amp; ports_gradient);
+
+    //! The ports are &quot;visible&quot;, &quot;hidden&quot; and &quot;hidden_activations&quot;.
+    virtual const TVec&lt;string&gt;&amp; getPorts();
+
+    //! The ports' sizes are given by the corresponding RBM layers.
+    virtual const TMat&lt;int&gt;&amp; getPortSizes();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(RBMModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+
+protected:
+
+    //! Used to store gradient w.r.t. activations of the hidden layer.
+    Mat hidden_act_grad;
+
+    //! Used to store gradient w.r.t. expectations of the visible layer.
+    Mat visible_exp_grad;
+
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000546.html">[Plearn-commits] r7097 - trunk/python_modules/plearn/pyplearn
</A></li>
	<LI>Next message: <A HREF="000548.html">[Plearn-commits] r7099 - trunk/plearn_learners/online/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#547">[ date ]</a>
              <a href="thread.html#547">[ thread ]</a>
              <a href="subject.html#547">[ subject ]</a>
              <a href="author.html#547">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
