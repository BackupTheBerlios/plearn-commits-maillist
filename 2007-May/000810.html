<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7361 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7361%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705271811.l4RIBvTc022475%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000809.html">
   <LINK REL="Next"  HREF="000811.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7361 - trunk/plearn_learners/online</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7361%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705271811.l4RIBvTc022475%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7361 - trunk/plearn_learners/online">yoshua at mail.berlios.de
       </A><BR>
    <I>Sun May 27 20:11:57 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000809.html">[Plearn-commits] r7360 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000811.html">[Plearn-commits] r7362 - in trunk/plearn_learners: distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_1_diagonal_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_1_diagonal_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_1_diagonal_5_2_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_1_diagonal_5_2_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_5_diagonal_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_5_diagonal_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_5_diagonal_5_2_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_5_diagonal_5_2_123456_10/Split0/test1_outputs.pmat.metadata di! stributions/test/.pytest/PL_GaussMix_Diagonal_Missing/expected_results/expdir_1_diagonal_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal_Missing/expected_results/expdir_1_diagonal_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_Diagonal_Missing/expected_results/expdir_5_diagonal_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal_Missing/expected_results/expdir_5_diagonal_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_General/expected_results/expdir_1_general_-1_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_General/expected_results/expdir_1_general_-1_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_General/expected_results/expdir_1_general_-1_5_2_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_General/expected_results/expdir_1_general_-1_5_2_123456_10/Split0/test1_outputs.pmat.metadata distributi! ons/test/.pytest/PL_GaussMix_General/expected_results/expdir_1! _general
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#810">[ date ]</a>
              <a href="thread.html#810">[ thread ]</a>
              <a href="subject.html#810">[ subject ]</a>
              <a href="author.html#810">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-05-27 20:11:56 +0200 (Sun, 27 May 2007)
New Revision: 7361

Modified:
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Added 'neg_log_likelihood' port in RBMModule, and hack
OnlineLearningModule::during_training to avoid unnecessary computations.


Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2007-05-27 12:04:57 UTC (rev 7360)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2007-05-27 18:11:56 UTC (rev 7361)
@@ -275,6 +275,7 @@
     if (!initTrain())
         return;
 
+    OnlineLearningModule::during_training=true;
     if (stage == 0) {
         // Perform training set-dependent initialization here.
         if (batch_size == 0)
@@ -310,6 +311,7 @@
         PLWARNING(&quot;In ModuleLearner::train - The network was trained for &quot;
                 &quot;only %d stages (instead of nstages = %d, which is not a &quot;
                 &quot;multiple of batch_size = %d&quot;, stage, nstages, batch_size);
+    OnlineLearningModule::during_training=false;
 }
 
 //////////////////

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-05-27 12:04:57 UTC (rev 7360)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-05-27 18:11:56 UTC (rev 7361)
@@ -61,6 +61,8 @@
     &quot;the corresponding bprop.\n&quot;
     );
 
+bool OnlineLearningModule::during_training=false;
+
 //////////////////////////
 // OnlineLearningModule //
 //////////////////////////

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-05-27 12:04:57 UTC (rev 7360)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-05-27 18:11:56 UTC (rev 7361)
@@ -65,6 +65,9 @@
     typedef Object inherited;
 
 public:
+    // BICK UGLY HACK to let modules know if we are performing training of testing
+    static bool during_training;
+
     //#####  Public Build Options  ############################################
 
     //! input size

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-05-27 12:04:57 UTC (rev 7360)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-05-27 18:11:56 UTC (rev 7361)
@@ -51,9 +51,38 @@
     &quot;A Restricted Boltzmann Machine.&quot;,
     &quot;An RBM contains a 'visible_layer', a 'hidden_layer' (both instances of a subclass\n&quot;
     &quot;of RBMLayer) and a 'connection' (an instance of a subclass of RBMConnection).\n&quot;
-    &quot;It has two ports: the 'visible' port and the 'hidden' port.\n&quot;
+    &quot;It always has the following ports: \n&quot;
+    &quot;  - 'visible' : expectations of the visible (normally input) layer\n&quot;
+    &quot;  - 'hidden.state' : expectations of the hidden (normally output) layer\n&quot;
+    &quot;  - 'hidden_activations.state' : activations of hidden units (given visible)\n&quot;
+    &quot;  - 'visible_sample' : random sample obtained on visible units\n&quot;
+    &quot;  - 'hidden_sample' : random sample obtained on hidden units\n&quot;
+    &quot;  - 'energy' : energy of the joint (visible,hidden) pair or free-energy\n&quot;
+    &quot;               of the visible (if given) or of the hidden (if given).\n&quot;
+    &quot;  - 'hidden_bias' : externally controlled bias on the hidden units,\n&quot;
+    &quot;                    used to implement conditional RBMs\n&quot;
+    &quot;  - 'neg_log_likelihood' : USE WITH CARE, this is the exact negative log-likelihood\n&quot;
+    &quot;    of the RBM. Computing it requires re-computing the partition function (which must\n&quot;
+    &quot;    be recomputed if the parameters have changed) and takes O(2^{min(n_hidden,n_visible)})\n&quot;
+    &quot;    computations of the free-energy.\n&quot;
+    &quot;An RBM also has other ports that exist only if some options are set.\n&quot;
+    &quot;If reconstruction_connection is given, then it has\n&quot;
+    &quot;  - 'visible_reconstruction_activations.state' : the deterministic reconstruction of the\n&quot;
+    &quot;     visible activations through the conditional expectations of the hidden given the visible.\n&quot;
+    &quot;  - 'visible_reconstruction.state' : the deterministic reconstruction of the visible\n&quot;
+    &quot;     values (expectations) through the conditional expectations of hidden | visible.\n&quot;
+    &quot;  - 'reconstruction_error.state' : the auto-associator reconstruction error (NLL)\n&quot;
+    &quot;    obtained by matching the visible_reconstruction with the given visible.\n&quot;
+    &quot;If compute_contrastive_divergence is true, then the RBM also has these ports\n&quot;
+    &quot;  - 'contrastive_divergence' : the quantity minimized by contrastive-divergence training.\n&quot;
+    &quot;  - 'negative_phase_visible_samples.state' : the negative phase stochastic reconstruction\n&quot;
+    &quot;    of the visible units, only provided to avoid recomputing them in bpropUpdate.\n&quot;
+    &quot;  - 'negative_phase_hidden_expectations.state' : the negative phase hidden units\n&quot;
+    &quot;    expected values, only provided to avoid recomputing them in bpropUpdate.\n&quot;
+    &quot;\n&quot;
     &quot;The RBM can be trained by gradient descent (wrt to gradients provided on\n&quot;
-    &quot;the 'hidden' port) or by contrastive divergence.\n&quot;
+    &quot;the 'hidden.state' port or on the 'reconstruction_error.state' port)\n&quot;
+    &quot;if grad_learning_rate&gt;0 or by contrastive divergence, if cd_learning_rate&gt;0.\n&quot;
 );
 
 ///////////////
@@ -66,7 +95,10 @@
     n_Gibbs_steps_CD(1),
     min_n_Gibbs_steps(1),
     n_Gibbs_steps_per_generated_sample(1),
+    compute_log_likelihood(false),
     Gibbs_step(0),
+    log_partition_function(0),
+    partition_function_is_stale(true),
     hidden_bias(0)
 {
 }
@@ -132,6 +164,13 @@
                   &quot;consecutive generated samples that are produced in output of the fprop method.\n&quot;
                   &quot;By default this is equal to min_n_Gibbs_steps.\n&quot;);
 
+    declareOption(ol, &quot;compute_log_likelihood&quot;,
+                  &amp;RBMModule::compute_log_likelihood,
+                  OptionBase::buildoption,
+                  &quot;Whether to compute the RBM generative model's log-likelihood\n&quot;
+                  &quot;(on the neg_log_likelihood port). If false then the neg_log_likelihood\n&quot;
+                  &quot;port just computes the input visible's free energy.\n&quot;);
+
     declareOption(ol, &quot;Gibbs_step&quot;, 
                   &amp;RBMModule::Gibbs_step,
                   OptionBase::learntoption,
@@ -140,6 +179,20 @@
                   &quot;Keeps track of the number of steps that have been ran since the beginning\n&quot;
                   &quot;of the chain.\n&quot;);
 
+    declareOption(ol, &quot;log_partition_function&quot;, 
+                  &amp;RBMModule::log_partition_function,
+                  OptionBase::learntoption,
+                  &quot;log(Z) = log(sum_{h,x} exp(-energy(h,x))\n&quot;
+                  &quot;only computed if compute_log_likelihood is true and\n&quot;
+                  &quot;the neg_log_likelihood port is requested.\n&quot;);
+
+    declareOption(ol, &quot;partition_function_is_stale&quot;, 
+                  &amp;RBMModule::log_partition_function,
+                  OptionBase::learntoption,
+                  &quot;Whether parameters have changed since the last computation\n&quot;
+                  &quot;of the log_partition_function (to know if it should be recomputed\n&quot;
+                  &quot;when the neg_log_likelihood port is requested.\n&quot;);
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -188,6 +241,7 @@
     addportname(&quot;hidden_sample&quot;);
     addportname(&quot;energy&quot;);
     addportname(&quot;hidden_bias&quot;); 
+    addportname(&quot;neg_log_likelihood&quot;);
     if(reconstruction_connection)
     {
         addportname(&quot;visible_reconstruction.state&quot;);
@@ -211,8 +265,10 @@
         port_sizes(portname2index(&quot;hidden.state&quot;), 1) = hidden_layer-&gt;size;
         port_sizes(portname2index(&quot;hidden_activations.state&quot;), 1) = hidden_layer-&gt;size; 
         port_sizes(portname2index(&quot;hidden_sample&quot;), 1) = hidden_layer-&gt;size; 
+        port_sizes(portname2index(&quot;hidden_bias&quot;),1) = hidden_layer-&gt;size;
     }
     port_sizes(portname2index(&quot;energy&quot;),1) = 1;
+    port_sizes(portname2index(&quot;neg_log_likelihood&quot;),1) = 1;
     if(reconstruction_connection)
     {
         if (visible_layer) {
@@ -260,7 +316,10 @@
     deepCopyField(visible_exp_grad, copies);
     deepCopyField(visible_act_grad, copies);
     deepCopyField(visible_bias_grad, copies);
+    deepCopyField(hidden_exp_store, copies);
+    deepCopyField(hidden_act_store, copies);
 
+    deepCopyField(ports, copies);
     deepCopyField(portname_to_index, copies);
 }
 
@@ -280,10 +339,11 @@
     PLASSERT( connection );
     Mat* visible = ports_value[portname2index(&quot;visible&quot;)]; 
     Mat* hidden = ports_value[portname2index(&quot;hidden.state&quot;)];
-    Mat* hidden_act = ports_value[portname2index(&quot;hidden_activations.state&quot;)];
+    hidden_act = ports_value[portname2index(&quot;hidden_activations.state&quot;)];
     Mat* visible_sample = ports_value[portname2index(&quot;visible_sample&quot;)];
     Mat* hidden_sample = ports_value[portname2index(&quot;hidden_sample&quot;)];
     Mat* energy = ports_value[portname2index(&quot;energy&quot;)];
+    Mat* neg_log_likelihood = ports_value[portname2index(&quot;neg_log_likelihood&quot;)];
     hidden_bias = ports_value[portname2index(&quot;hidden_bias&quot;)];
     Mat* visible_reconstruction = 0;
     Mat* visible_reconstruction_activations = 0;
@@ -303,6 +363,9 @@
     if (compute_contrastive_divergence)
     {
         contrastive_divergence = ports_value[portname2index(&quot;contrastive_divergence&quot;)]; 
+        if (!contrastive_divergence || !contrastive_divergence-&gt;isEmpty())
+            PLERROR(&quot;RBMModule: when option compute_contrastive_divergence=true\n&quot;
+                    &quot;the contrastive_divergence port should be provided, as an output.\n&quot;);
         negative_phase_visible_samples = 
             ports_value[portname2index(&quot;negative_phase_visible_samples.state&quot;)];
         negative_phase_hidden_expectations = 
@@ -310,8 +373,7 @@
     }
 
     bool hidden_expectations_are_computed=false;
-    bool hidden_activations_are_computed=false;
-    bool visible_activations_are_computed=false;
+    hidden_activations_are_computed=false;
     bool found_a_valid_configuration = false;
 
     if (visible &amp;&amp; !visible-&gt;isEmpty())
@@ -326,43 +388,21 @@
     {
         PLASSERT_MSG( energy-&gt;isEmpty(), 
                       &quot;RBMModule: the energy port can only be an output port\n&quot; );
-        if (visible &amp;&amp; !visible-&gt;isEmpty())
+        if (visible &amp;&amp; !visible-&gt;isEmpty()
+            &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty()) 
         {
-            int mbs = visible-&gt;length();
-            if (hidden &amp;&amp; !hidden-&gt;isEmpty()) 
-                // FULLY OBSERVED CASE
-                // we know x and h: energy(h,x) = b'x + c'h + h'Wx
-                //  = visible_layer-&gt;energy(x) + hidden_layer-&gt;energy(h) + dot(h,hidden_layer-&gt;activation)
-            {
-                energy-&gt;resize(mbs,1);
-                energy-&gt;clear();
-                for (int i=0;i&lt;mbs;i++)
-                {
-                    computeHiddenActivations(*visible);
-                    hidden_activations_are_computed=true;
-                    (*energy)(i,0) = visible_layer-&gt;energy((*visible)(i)) + 
-                        hidden_layer-&gt;energy((*hidden)(i)) + 
-                        dot((*hidden)(i),hidden_layer-&gt;activations(i));
-                }
-            } else 
-                // FREE-ENERGY(visible) CASE
-                // we know x: free energy = -log sum_h e^{-energy(h,x)}
-                //                        = b'x + sum_i log sigmoid(-c_i - W_i'x) .... FOR BINOMIAL HIDDEN LAYER
-                //                        = visible_layer-&gt;energy(x) + sum_i log hidden_layer-&gt;expectation[i]
-                // or more robustly,      = visible_layer-&gt;energy(x) - sum_i softplus(-hidden_layer-&gt;activation[i])
-            {
-                PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
-                energy-&gt;resize(mbs,1);
-                energy-&gt;clear();
-                for (int i=0;i&lt;mbs;i++)
-                {
-                    computeHiddenActivations(*visible);
-                    hidden_activations_are_computed=true;
-                    (*energy)(i,0) = visible_layer-&gt;energy((*visible)(i));
-                    for (int j=0;j&lt;hidden_layer-&gt;size;j++)
-                        (*energy)(i,0) += softplus(hidden_layer-&gt;activations(i,j));
-                }
-            }
+            // FULLY OBSERVED CASE
+            // we know x and h: energy(h,x) = b'x + c'h + h'Wx
+            //  = visible_layer-&gt;energy(x) + hidden_layer-&gt;energy(h) + dot(h,hidden_layer-&gt;activation)
+            computeEnergy(*visible,*hidden,*energy);
+        } else if (visible &amp;&amp; !visible-&gt;isEmpty())
+        {
+            // FREE-ENERGY(visible) CASE
+            // we know x: free energy = -log sum_h e^{-energy(h,x)}
+            //                        = b'x + sum_i log sigmoid(-c_i - W_i'x) .... FOR BINOMIAL HIDDEN LAYER
+            //                        = visible_layer-&gt;energy(x) + sum_i log hidden_layer-&gt;expectation[i]
+            // or more robustly,      = visible_layer-&gt;energy(x) - sum_i softplus(-hidden_layer-&gt;activation[i])
+            computeFreeEnergyOfVisible(*visible,*energy);
         }
         else if (hidden &amp;&amp; !hidden-&gt;isEmpty())
             // FREE-ENERGY(hidden) CASE
@@ -371,24 +411,101 @@
             //                        = hidden_layer-&gt;energy(h) + sum_i log visible_layer-&gt;expectation[i]
             // or more robustly,      = hidden_layer-&gt;energy(h) - sum_i softplus(-visible_layer-&gt;activation[i])
         {
-            PLASSERT(visible_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
-            int mbs = hidden-&gt;length();
-            energy-&gt;resize(mbs,1);
-            energy-&gt;clear();
-            for (int i=0;i&lt;mbs;i++)
-            {
-                computeVisibleActivations(*hidden);
-                visible_activations_are_computed=true; 
-                (*energy)(i,0) = hidden_layer-&gt;energy((*hidden)(i));
-                for (int j=0;j&lt;visible_layer-&gt;size;j++)
-                    (*energy)(i,0) += softplus(visible_layer-&gt;activations(i,j));
-            }
+            computeFreeEnergyOfHidden(*hidden,*energy);
         }
         else 
             PLERROR(&quot;RBMModule: unknown configuration to compute energy (currently\n&quot;
                     &quot;only possible if at least visible or hidden are provided).\n&quot;);
         found_a_valid_configuration = true;
     }
+    if (neg_log_likelihood &amp;&amp; neg_log_likelihood-&gt;isEmpty() &amp;&amp; compute_log_likelihood)
+    {
+        if (partition_function_is_stale &amp;&amp; !during_training)
+        {
+            // recompute partition function
+            if (hidden_layer-&gt;size &gt; visible_layer-&gt;size)
+                // do it by log-summing minus-free-energy of visible configurations
+            {
+                PLASSERT(visible_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+                // assuming a binary input we sum over all bit configurations
+                int n_configurations = 1;
+                n_configurations &lt;&lt; visible_layer-&gt;size; // = 2^{visible_layer-&gt;size}
+                Mat inputs = visible_layer-&gt;getExpectations();
+                inputs.resize(1,visible_layer-&gt;size);
+                Vec input = inputs(0);
+                // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+                // AT ONCE IN A 'MINIBATCH'
+                Mat free_energy(1,1);
+                log_partition_function = 0;
+                for (int c=0;c&lt;n_configurations;c++)
+                {
+                    // convert integer c into a bit-wise visible representation
+                    int x=c;
+                    for (int i=0;i&lt;visible_layer-&gt;size;i++)
+                    {
+                        input[i]= x &amp; 1; // take least significant bit
+                        x &gt;&gt;= 1; // and shift right (divide by 2)
+                    }
+                    computeFreeEnergyOfVisible(inputs,free_energy,false);
+                    if (c==0)
+                        log_partition_function = -free_energy(0,0);
+                    else
+                        log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+                }
+            }
+            else
+                // do it by summing free-energy of hidden configurations
+            {
+                PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+                // assuming a binary hidden we sum over all bit configurations
+                int n_configurations = 1;
+                n_configurations &lt;&lt; hidden_layer-&gt;size; // = 2^{hidden_layer-&gt;size}
+                Mat&amp; inputs = hidden_layer-&gt;getExpectations();
+                inputs.resize(1,hidden_layer-&gt;size);
+                Vec input = inputs(0);
+                // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+                // AT ONCE IN A 'MINIBATCH'
+                Mat free_energy(1,1);
+                log_partition_function = 0;
+                for (int c=0;c&lt;n_configurations;c++)
+                {
+                    // convert integer c into a bit-wise hidden representation
+                    int x=c;
+                    for (int i=0;i&lt;hidden_layer-&gt;size;i++)
+                    {
+                        input[i]= x &amp; 1; // take least significant bit
+                        x &gt;&gt;= 1; // and shift right (divide by 2)
+                    }
+                    computeFreeEnergyOfHidden(inputs,free_energy);
+                    if (c==0)
+                        log_partition_function = -free_energy(0,0);
+                    else
+                        log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+                }
+            }
+            partition_function_is_stale=false;
+        }
+        if (visible &amp;&amp; !visible-&gt;isEmpty()
+            &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
+        {
+            // neg-log-likelihood(visible,hidden) = energy(visible,visible) + log(partition_function)
+            computeEnergy(*visible,*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (visible &amp;&amp; !visible-&gt;isEmpty()) 
+        {
+            // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
+            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (hidden &amp;&amp; !hidden-&gt;isEmpty())
+        {
+            // neg-log-likelihood(hidden) = free_energy(hidden) + log(partition_function)
+            computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else PLERROR(&quot;RBMModule: neg_log_likelihood currently computable only of the visible as inputs&quot;);
+    }
     // REGULAR FPROP
     // we are given the visible units and we want to compute the hidden
     // activation and/or the hidden expectation
@@ -397,17 +514,7 @@
           (hidden_act &amp;&amp; hidden_act-&gt;isEmpty())) )
     {
         if (!hidden_activations_are_computed)
-        {
-            computeHiddenActivations(*visible);
-            hidden_activations_are_computed=true;
-        }
-        if (hidden_activations_are_computed &amp;&amp; hidden_act) {
-            // Also store hidden layer activations.
-            PLASSERT( hidden_act-&gt;isEmpty() );
-            const Mat&amp; to_store = hidden_layer-&gt;activations;
-            hidden_act-&gt;resize(to_store.length(), to_store.width());
-            *hidden_act &lt;&lt; to_store;
-        }
+            computePositivePhaseHiddenActivations(*visible);
         if (hidden) {
             PLASSERT( hidden-&gt;isEmpty() );
             hidden_layer-&gt;computeExpectations();
@@ -472,15 +579,22 @@
         {
             int mbs = visible-&gt;length();
             Mat&amp; hidden_expectations = hidden_layer-&gt;getExpectations();
+            Mat* h=0;
+            Mat* h_act=0;
             if (!hidden_activations_are_computed) // it must be because neither hidden nor hidden_act were asked
             {
                 PLASSERT(!hidden_act);
-                computeHiddenActivations(*visible);
-                hidden_activations_are_computed=true;
+                computePositivePhaseHiddenActivations(*visible);
+                
                 // we need to save the hidden activations somewhere
                 hidden_act_store.resize(mbs,hidden_layer-&gt;size);
                 hidden_act_store &lt;&lt; hidden_layer-&gt;activations;
-                hidden_act = &amp;hidden_act_store;
+                h_act = &amp;hidden_act_store;
+            } else 
+            {
+                // hidden_act must have been computed above if they were requested on port
+                PLASSERT(hidden_act &amp;&amp; !hidden_act-&gt;isEmpty()); 
+                h_act = hidden_act;
             }
             if (!hidden_expectations_are_computed) // it must be because hidden outputs were not asked
             {
@@ -490,7 +604,12 @@
                 // we need to save the hidden expectations somewhere
                 hidden_exp_store.resize(mbs,hidden_layer-&gt;size);
                 hidden_exp_store &lt;&lt; hidden_expectations;
-                hidden = &amp;hidden_exp_store;
+                h = &amp;hidden_exp_store;
+            } else
+            {
+                // hidden exp. must have been computed above if they were requested on port
+                PLASSERT(hidden &amp;&amp; !hidden-&gt;isEmpty());
+                h = hidden;
             }
             // perform negative phase
             for( int i=0; i&lt;n_Gibbs_steps_CD; i++)
@@ -499,8 +618,10 @@
                 // (Negative phase) Generate visible samples.
                 sampleVisibleGivenHidden(hidden_layer-&gt;samples);
                 // compute corresponding hidden expectations.
-                sampleHiddenGivenVisible(visible_layer-&gt;samples);
+                computeHiddenActivations(visible_layer-&gt;samples);
             }
+            PLASSERT(negative_phase_visible_samples);
+            PLASSERT(negative_phase_hidden_expectations);
             negative_phase_visible_samples-&gt;resize(mbs,visible_layer-&gt;size);
             *negative_phase_visible_samples &lt;&lt; visible_layer-&gt;samples;
             negative_phase_hidden_expectations-&gt;resize(hidden_expectations.length(),
@@ -510,9 +631,9 @@
             // compute the energy (again for now only in the binomial case)
             PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
 
-            // note that hidden_act and hidden may point to hidden_act_store and hidden_exp_store
-            PLASSERT(!hidden_act-&gt;isEmpty()); 
-            PLASSERT(!hidden-&gt;isEmpty());
+            // note that h_act and h may point to hidden_act_store and hidden_exp_store
+            PLASSERT(h_act &amp;&amp; !h_act-&gt;isEmpty()); 
+            PLASSERT(h &amp;&amp; !h-&gt;isEmpty());
 
             contrastive_divergence-&gt;resize(hidden_expectations.length(),1);
             // compute contrastive divergence itself
@@ -521,8 +642,8 @@
                 (*contrastive_divergence)(i,0) = 
                     // positive phase energy
                     visible_layer-&gt;energy((*visible)(i))
-                    + hidden_layer-&gt;energy((*hidden)(i))
-                    + dot((*hidden)(i),(*hidden_act)(i))
+                    + hidden_layer-&gt;energy((*h)(i))
+                    + dot((*h)(i),(*h_act)(i))
                     // minus
                     - 
                     // negative phase energy
@@ -546,20 +667,13 @@
         // Autoassociator reconstruction cost
         PLASSERT( ports_value.length() == nPorts() );
         if (!hidden_activations_are_computed)
-        {
-            computeHiddenActivations(*visible); 
-            hidden_activations_are_computed=true;
-        }
+            computePositivePhaseHiddenActivations(*visible); 
         if(!hidden_expectations_are_computed)
         {
             hidden_layer-&gt;computeExpectations();
             hidden_expectations_are_computed=true;
         }
-        if (hidden_activations_are_computed &amp;&amp; hidden_act &amp;&amp; hidden_act-&gt;isEmpty()) {
-            const Mat&amp; to_store = hidden_layer-&gt;activations;
-            hidden_act-&gt;resize(to_store.length(), to_store.width());
-            *hidden_act &lt;&lt; to_store;
-        }
+
         // Don't need to verify if they are asked in a port, this was done previously
         
         computeVisibleActivations(hidden_layer-&gt;getExpectations(),true);
@@ -652,6 +766,7 @@
             connection-&gt;bpropUpdate(
                     *visible, *hidden_act, *store_visible_grad,
                     hidden_act_grad, true);
+            partition_function_is_stale = true;
         }
     } 
 
@@ -713,6 +828,7 @@
                 *hidden_bias_grad += *hidden;
                 *hidden_bias_grad -= *negative_phase_hidden_expectations;
             }
+            partition_function_is_stale = true;
     }
 
     if (reconstruction_error_grad &amp;&amp; !reconstruction_error_grad-&gt;isEmpty()
@@ -782,6 +898,7 @@
                 *visible, *hidden_act,
                 visible_exp_grad, hidden_act_grad, true);
         }
+        partition_function_is_stale = true;
     }
 
     checkProp(ports_gradient);

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-05-27 12:04:57 UTC (rev 7360)
+++ trunk/plearn_learners/online/RBMModule.h	2007-05-27 18:11:56 UTC (rev 7361)
@@ -82,9 +82,13 @@
     int min_n_Gibbs_steps; 
     int n_Gibbs_steps_per_generated_sample;
 
+    bool compute_log_likelihood;
+
     //#####  Public Learnt Options  ############################################
     //! used to generate samples from the RBM
     int Gibbs_step;
+    real log_partition_function;
+    bool partition_function_is_stale;
 
 public:
     //#####  Public Member Functions  #########################################
@@ -205,6 +209,8 @@
 
 protected:
 
+    Mat* hidden_bias;
+
     //! Used to store gradient w.r.t. expectations of the hidden layer.
     Mat hidden_exp_grad;
 
@@ -223,6 +229,8 @@
     //! Used to cache the hidden layer expectations and activations
     Mat hidden_exp_store;
     Mat hidden_act_store;
+    Mat* hidden_act;
+    bool hidden_activations_are_computed;
 
     //! names of the ports
     TVec&lt;string&gt; ports;
@@ -243,19 +251,26 @@
     //! Declares the class options.
     static void declareOptions(OptionList&amp; ol);
 
-    Mat* hidden_bias;
-
-    void computeHiddenActivations(Mat&amp; visible) {
+    void computeHiddenActivations(const Mat&amp; visible) {
         connection-&gt;setAsDownInputs(visible);
         hidden_layer-&gt;getAllActivations(connection, 0, true);
         if (hidden_bias &amp;&amp; !hidden_bias-&gt;isEmpty())
             hidden_layer-&gt;activations += *hidden_bias;
     }
-    void sampleHiddenGivenVisible(Mat&amp; visible) {
+    void computePositivePhaseHiddenActivations(const Mat&amp; visible) {
         computeHiddenActivations(visible);
+        hidden_activations_are_computed=true;
+        if (hidden_act &amp;&amp; hidden_act-&gt;isEmpty())
+        {
+            hidden_act-&gt;resize(visible.length(),hidden_layer-&gt;size);
+            *hidden_act &lt;&lt; hidden_layer-&gt;activations;
+        }
+    }
+    void sampleHiddenGivenVisible(const Mat&amp; visible) {
+        computeHiddenActivations(visible);
         hidden_layer-&gt;generateSamples();
     }
-    void computeVisibleActivations(Mat&amp; hidden, bool using_reconstruction_connection=false) {
+    void computeVisibleActivations(const Mat&amp; hidden, bool using_reconstruction_connection=false) {
         if (using_reconstruction_connection)
         {
             reconstruction_connection-&gt;setAsDownInputs(hidden);
@@ -267,11 +282,53 @@
             visible_layer-&gt;getAllActivations(connection, 0, true);
         }
     }
-    void sampleVisibleGivenHidden(Mat&amp; hidden) {
+    void sampleVisibleGivenHidden(const Mat&amp; hidden) {
         computeVisibleActivations(hidden);
         visible_layer-&gt;generateSamples();
     }
-
+    void computeFreeEnergyOfVisible(const Mat&amp; visible, Mat&amp; energy, bool positive_phase=true) {
+        int mbs=visible.length();
+        if (energy.isEmpty())
+            energy.resize(mbs,1);
+        PLASSERT(hidden_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+        if (positive_phase)
+            computePositivePhaseHiddenActivations(visible);
+        else
+            computeHiddenActivations(visible);
+        for (int i=0;i&lt;mbs;i++)
+        {
+            energy(i,0) = visible_layer-&gt;energy(visible(i));
+            for (int j=0;j&lt;hidden_layer-&gt;size;j++)
+                energy(i,0) += softplus(hidden_layer-&gt;activations(i,j));
+        }
+    }
+    void computeFreeEnergyOfHidden(const Mat&amp; hidden, Mat&amp; energy) {
+        int mbs=hidden.length();
+        if (energy.isEmpty())
+            energy.resize(mbs,1);
+        PLASSERT(visible_layer-&gt;classname()==&quot;RBMBinomialLayer&quot;);
+        computeVisibleActivations(hidden);
+        for (int i=0;i&lt;mbs;i++)
+        {
+            energy(i,0) = hidden_layer-&gt;energy(hidden(i));
+            for (int j=0;j&lt;visible_layer-&gt;size;j++)
+                energy(i,0) += softplus(visible_layer-&gt;activations(i,j));
+        }
+    }
+    void computeEnergy(const Mat&amp; visible, const Mat&amp; hidden, Mat&amp; energy, 
+                       bool positive_phase=true) 
+    {
+        int mbs=hidden.length();
+        energy.resize(mbs,1);
+        if (positive_phase)
+            computePositivePhaseHiddenActivations(visible);
+        else
+            computeHiddenActivations(visible);
+        for (int i=0;i&lt;mbs;i++)
+            energy(i,0) = visible_layer-&gt;energy(visible(i)) + 
+                hidden_layer-&gt;energy(hidden(i)) + 
+                dot(hidden(i),hidden_layer-&gt;activations(i));
+    }
 private:
     //#####  Private Member Functions  ########################################
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000809.html">[Plearn-commits] r7360 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000811.html">[Plearn-commits] r7362 - in trunk/plearn_learners: distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_1_diagonal_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_1_diagonal_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_1_diagonal_5_2_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_1_diagonal_5_2_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_5_diagonal_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_5_diagonal_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_5_diagonal_5_2_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal/expected_results/expdir_5_diagonal_5_2_123456_10/Split0/test1_outputs.pmat.metadata di! stributions/test/.pytest/PL_GaussMix_Diagonal_Missing/expected_results/expdir_1_diagonal_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal_Missing/expected_results/expdir_1_diagonal_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_Diagonal_Missing/expected_results/expdir_5_diagonal_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_Diagonal_Missing/expected_results/expdir_5_diagonal_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_General/expected_results/expdir_1_general_-1_5_0_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_General/expected_results/expdir_1_general_-1_5_0_123456_10/Split0/test1_outputs.pmat.metadata distributions/test/.pytest/PL_GaussMix_General/expected_results/expdir_1_general_-1_5_2_123456_10/Split0 distributions/test/.pytest/PL_GaussMix_General/expected_results/expdir_1_general_-1_5_2_123456_10/Split0/test1_outputs.pmat.metadata distributi! ons/test/.pytest/PL_GaussMix_General/expected_results/expdir_1! _general
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#810">[ date ]</a>
              <a href="thread.html#810">[ thread ]</a>
              <a href="subject.html#810">[ subject ]</a>
              <a href="author.html#810">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
