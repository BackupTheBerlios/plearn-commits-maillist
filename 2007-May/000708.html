<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7259 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL python_modules/plearn/var
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7259%20-%20in%20trunk%3A%20plearn/var/EXPERIMENTAL%0A%09plearn_learners/generic/EXPERIMENTAL%20python_modules/plearn/var&In-Reply-To=%3C200705231508.l4NF8is1013710%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000707.html">
   <LINK REL="Next"  HREF="000709.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7259 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL python_modules/plearn/var</H1>
    <B>simonl at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7259%20-%20in%20trunk%3A%20plearn/var/EXPERIMENTAL%0A%09plearn_learners/generic/EXPERIMENTAL%20python_modules/plearn/var&In-Reply-To=%3C200705231508.l4NF8is1013710%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7259 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL python_modules/plearn/var">simonl at mail.berlios.de
       </A><BR>
    <I>Wed May 23 17:08:44 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000707.html">[Plearn-commits] r7258 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000709.html">[Plearn-commits] r7260 - in trunk: commands plearn_learners/online	plearn_learners/online/test	plearn_learners/online/test/MaxSubsampling2DModule	plearn_learners/online/test/MaxSubsampling2DModule/.pytest	plearn_learners/online/test/MaxSubsampling2DModule/.pytest/PL_max_subsampling	plearn_learners/online/test/MaxSubsampling2DModule/.pytest/PL_max_subsampling/expected_results
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#708">[ date ]</a>
              <a href="thread.html#708">[ thread ]</a>
              <a href="subject.html#708">[ subject ]</a>
              <a href="author.html#708">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: simonl
Date: 2007-05-23 17:08:43 +0200 (Wed, 23 May 2007)
New Revision: 7259

Modified:
   trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc
   trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.h
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/var/Var.py
Log:
some ameliorations...


Modified: trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc	2007-05-23 14:49:30 UTC (rev 7258)
+++ trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc	2007-05-23 15:08:43 UTC (rev 7259)
@@ -46,26 +46,27 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     TransposedDoubleProductVariable,
-    &quot;Let W, M and H be the inputs and nw the length of W. Then output(n,k) = sum_i{ sum_j { W(i,k)*M(j,k)*H(n,i+j*nw) } }&quot;,
+    &quot;Let H, W and M be the inputs and nw the length of W. Then output(n,k) = sum_i{ sum_j { W(i,k)*M(j,k)*H(n,i+j*nw) } }&quot;,
     &quot;MULTI LINE\nHELP FOR USERS&quot;
     );
 
 TransposedDoubleProductVariable::TransposedDoubleProductVariable()
 {}
 
-TransposedDoubleProductVariable::TransposedDoubleProductVariable(Var w, Var m, Var h)
-    : inherited(w &amp; m &amp; h, h.length(), w.width())
+TransposedDoubleProductVariable::TransposedDoubleProductVariable(Var h, Var w, Var m)
+    : inherited(h &amp; w &amp; m, h.length(), w.width())
 {
     build_();
 }
 
 
 void TransposedDoubleProductVariable::recomputeSize(int&amp; l, int&amp; w) const
-{
-    
+{    
         if (varray.size() &gt; 0) {
-              l = varray[2].length(); // the computed length of this Var
-             w = varray[0].width(); // the computed width
+            // l = varH()-&gt;length(); // the computed length of this Var
+            //w = varW()-&gt;width(); // the computed width
+            l = varray[0].length();
+            w = varray[1].width();
         } else
             l = w = 0;
 }
@@ -117,7 +118,6 @@
                     m_grad(j,k) += x_grad(n,k)*w(i,k)*h(n,i+j*nw);
                     h_grad(n,i+j*nw) += x_grad(n,k)*w(i,k)*m(j,k);
                 }
-
 }
 
 // ### You can implement these methods:
@@ -184,7 +184,7 @@
     // ### called.
 
     if (varH().width() != varW().length()*varM().length())
-        PLERROR(&quot;The width of matrix H incompatible with lengths of matrix W and M in TranposedDoubleProductVariable&quot;);
+        PLERROR(&quot;The width of matrix H is incompatible with the lengths of matrix W and M in TranposedDoubleProductVariable&quot;);
     if (varM().width() != varW().width())
         PLERROR(&quot;Matrix W and M must have the same width in TranposedDoubleProduct&quot;);
 

Modified: trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.h	2007-05-23 14:49:30 UTC (rev 7258)
+++ trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.h	2007-05-23 15:08:43 UTC (rev 7259)
@@ -73,15 +73,16 @@
     TransposedDoubleProductVariable();
 
     //! Constructor initializing from input variables
-    TransposedDoubleProductVariable(Var w, Var m, Var h);
+    TransposedDoubleProductVariable(Var h, Var w, Var m);
 
 
     //easy access to input
-    Var&amp; varW() { return varray[0]; }
-    Var&amp; varM() { return varray[1]; } 
-    Var&amp; varH() { return varray[2]; }
+    Var&amp; varH() { return varray[0]; }
+    Var&amp; varW() { return varray[1]; }
+    Var&amp; varM() { return varray[2]; } 
 
 
+
     // Your other public member functions go here
 
     //#####  PLearn::Variable methods #########################################

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-05-23 14:49:30 UTC (rev 7258)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-05-23 15:08:43 UTC (rev 7259)
@@ -110,6 +110,10 @@
                   OptionBase::buildoption,
                   &quot;&quot;);
 
+    declareOption(ol, &quot;good_improvement_rate&quot;, &amp;DeepReconstructorNet::good_improvement_rate,
+                  OptionBase::buildoption,
+                  &quot;&quot;);
+
     
     
     // Now call the parent class' declareOptions
@@ -142,10 +146,11 @@
     int n_rec_costs = reconstruction_costs.length();
     for(int k=0; k&lt;n_rec_costs; k++)
         fullcost = fullcost + reconstruction_costs[k];
-    displayVarGraph(fullcost);
+    //displayVarGraph(fullcost);
     Var input = layers[0];
     Func f(input&amp;target, fullcost);
     parameters = f-&gt;parameters;
+    outmat.resize(n_rec_costs);
 }
 
 // ### Nothing to add here, simply calls build_
@@ -227,13 +232,14 @@
     VMat dset = train_set.subMatColumns(0,insize);
 
     // hack...
-    dset = dset.subMatRows(0,100);
+    dset = dset.subMatRows(0,10);
     for(int k=0; k&lt;nreconstructions; k++)
     {
         trainHiddenLayer(k, dset);
         int width = layers[k+1].width();
-        outmat[k] = new FileVMatrix(outmatfname+tostring(k),0,width);
-        buildHiddenLayerOutputs(0, dset, outmat[k]);
+        outmat[k] = new FileVMatrix(outmatfname+tostring(k)+&quot;.pmat&quot;,0,width);
+        outmat[k]-&gt;defineSizes(width,0);
+        buildHiddenLayerOutputs(k, dset, outmat[k]);
         dset = outmat[k];
     }    
 
@@ -277,7 +283,7 @@
     perr &lt;&lt; &quot;*** Training layer &quot; &lt;&lt; which_input_layer &lt;&lt; &quot; for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
     perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
     Func f(layers[which_input_layer], reconstruction_costs[which_input_layer]);
-    displayVarGraph(reconstruction_costs[which_input_layer]);
+    //displayVarGraph(reconstruction_costs[which_input_layer]);
     // displayVarGraph(fproppath,true, 333, &quot;ffpp&quot;, false);
     Var totalcost = sumOf(inputs, f, minibatch_size);
     VarArray params = totalcost-&gt;parents();
@@ -285,7 +291,8 @@
     reconstruction_optimizer-&gt;reset();
     VecStatsCollector st;
     real prev_mean = -1;
-    for(int n=0; n&lt;nepochs; n++)
+    real relative_improvement = good_improvement_rate;
+    for(int n=0; n&lt;nepochs &amp;&amp; relative_improvement &gt;= good_improvement_rate; n++)
     {
         st.forget();
         reconstruction_optimizer-&gt;nstages = l/minibatch_size;
@@ -294,7 +301,10 @@
         real m = s.mean();
         perr &lt;&lt; &quot;Epoch &quot; &lt;&lt; n &lt;&lt; &quot; mean error: &quot; &lt;&lt; m &lt;&lt; &quot; +- &quot; &lt;&lt; s.stderror() &lt;&lt; endl;
         if(prev_mean&gt;0)
-            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; ((prev_mean-m)/prev_mean)*100 &lt;&lt; endl;
+        {
+            relative_improvement = ((prev_mean-m)/prev_mean)*100;
+            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; relative_improvement &lt;&lt; &quot; %&quot;&lt;&lt; endl;
+        }
         prev_mean = m;
     }
 }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-05-23 14:49:30 UTC (rev 7258)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-05-23 15:08:43 UTC (rev 7259)
@@ -76,6 +76,8 @@
 
     TVec&lt;int&gt; training_schedule;
 
+    real good_improvement_rate;
+
     // layers[0] is the input variable
     // last layer is final output layer
     TVec&lt;Var&gt; layers;
@@ -157,9 +159,11 @@
 
 
     virtual void initializeParams(bool set_seed=true);
-   
 
+    //! Returns the matValue of the parameter variable with the given name
+    Mat getParameterValue(const string&amp; varname);
 
+
     // *** SUBCLASS WRITING: ***
     // While in general not necessary, in case of particular needs
     // (efficiency concerns for ex) you may also want to overload

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-05-23 14:49:30 UTC (rev 7258)
+++ trunk/python_modules/plearn/var/Var.py	2007-05-23 15:08:43 UTC (rev 7259)
@@ -36,7 +36,7 @@
     
 class Var:
 
-    def __init__(self, l, w=1, random_type=&quot;none&quot;, random_a=0., random_b=1., random_clear_first_row=False):
+    def __init__(self, l, w=1, random_type=&quot;none&quot;, random_a=0., random_b=1., random_clear_first_row=False, varname=&quot;&quot;):
         if isinstance(l, Var):
             self.v = l.v
         elif isinstance(l,int):
@@ -45,7 +45,8 @@
                                        random_type=random_type,
                                        random_a=random_a,
                                        random_b=random_b,
-                                       random_clear_first_row=random_clear_first_row)
+                                       random_clear_first_row=random_clear_first_row,
+                                       varname=varname)
         else: # assume parameter l is a pl. plvar
             self.v = l
 
@@ -89,7 +90,7 @@
         return self.multiMax(igs, 'L')
 
     def transposeDoubleProduct(self, W, M):
-        return Var(pl.TransposedDoubleProductVariable(varray=[W, M, self.v]))
+        return Var(pl.TransposedDoubleProductVariable(varray=[self.v, W, M]))
 
     def square(self):
         return Var(pl.SquareVariable(input=self.v))
@@ -118,12 +119,12 @@
 
 
 
-def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs):
+def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
     &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output&quot;&quot;&quot;
-    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1/iw, 1/iw, False)
-    W = Var(ogs, iw, &quot;uniform&quot;, -1/iw, 1/iw, False)
+    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1/iw, 1/iw, False, varname=basename+&quot;_M&quot;)
+    W = Var(ogs, iw, &quot;uniform&quot;, -1/iw, 1/iw, False, varname=basename+&quot;_W&quot;)
     hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
     cost = -hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs).dot(input)            
     return hidden, cost, (W,M)


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000707.html">[Plearn-commits] r7258 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000709.html">[Plearn-commits] r7260 - in trunk: commands plearn_learners/online	plearn_learners/online/test	plearn_learners/online/test/MaxSubsampling2DModule	plearn_learners/online/test/MaxSubsampling2DModule/.pytest	plearn_learners/online/test/MaxSubsampling2DModule/.pytest/PL_max_subsampling	plearn_learners/online/test/MaxSubsampling2DModule/.pytest/PL_max_subsampling/expected_results
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#708">[ date ]</a>
              <a href="thread.html#708">[ thread ]</a>
              <a href="subject.html#708">[ subject ]</a>
              <a href="author.html#708">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
