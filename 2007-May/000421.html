<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6972 - trunk/plearn_learners_experimental/netflix
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6972%20-%20trunk/plearn_learners_experimental/netflix&In-Reply-To=%3C200705021744.l42HivOZ022548%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000420.html">
   <LINK REL="Next"  HREF="000422.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6972 - trunk/plearn_learners_experimental/netflix</H1>
    <B>manzagop at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6972%20-%20trunk/plearn_learners_experimental/netflix&In-Reply-To=%3C200705021744.l42HivOZ022548%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6972 - trunk/plearn_learners_experimental/netflix">manzagop at mail.berlios.de
       </A><BR>
    <I>Wed May  2 19:44:57 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000420.html">[Plearn-commits] r6971 - trunk/plearn/math
</A></li>
        <LI>Next message: <A HREF="000422.html">[Plearn-commits] r6973 - trunk/plearn_learners_experimental/netflix
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#421">[ date ]</a>
              <a href="thread.html#421">[ thread ]</a>
              <a href="subject.html#421">[ subject ]</a>
              <a href="author.html#421">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: manzagop
Date: 2007-05-02 19:44:57 +0200 (Wed, 02 May 2007)
New Revision: 6972

Modified:
   trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc
   trunk/plearn_learners_experimental/netflix/NxProfileLearner.h
Log:
Added possibility of:
- L1 and L2 regularization
- natural gradient descent


Modified: trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc
===================================================================
--- trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc	2007-05-02 16:46:44 UTC (rev 6971)
+++ trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc	2007-05-02 17:44:57 UTC (rev 6972)
@@ -50,6 +50,8 @@
 NxProfileLearner::NxProfileLearner()    :   profile_dim(1),
                                             slr(0.0),
                                             dc(0.0),
+                                            L1_penalty_factor(0.0),
+                                            L2_penalty_factor(0.0),
                                             n_films(17770),
                                             n_users(480189)
 /* ### Initialize all fields to their default value here */
@@ -87,7 +89,29 @@
                   OptionBase::buildoption,
                   &quot;Learning rate decrease constant.&quot;);
 
+    declareOption(ol, &quot;L1_penalty_factor&quot;,
+                  &amp;NxProfileLearner::L1_penalty_factor,
+                  OptionBase::buildoption,
+                  &quot;Optional (default=0) factor of L1 regularization term, i.e.\n&quot;
+                  &quot;minimize L1_penalty_factor * sum_{ij} |weights(i,j)| during training.\n&quot;);
+    declareOption(ol, &quot;L2_penalty_factor&quot;,
+                  &amp;NxProfileLearner::L2_penalty_factor,
+                  OptionBase::buildoption,
+                  &quot;Optional (default=0) factor of L2 regularization term, i.e.\n&quot;
+                  &quot;minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 during training.\n&quot;);
 
+    declareOption(ol, &quot;ngest_films&quot;,
+                  &amp;NxProfileLearner::ngest_films,
+                  OptionBase::buildoption,
+                  &quot;Optional NatGradEstimator object for the gradients on the parameters OF ALL USERS!\n&quot;
+                  &quot;NOT A TEMPLATE!&quot;);
+
+    declareOption(ol, &quot;ngest_users&quot;,
+                  &amp;NxProfileLearner::ngest_users,
+                  OptionBase::buildoption,
+                  &quot;Optional NatGradEstimator object for the gradients on the parameters OF ALL FILMS!\n&quot;
+                  &quot;NOT A TEMPLATE!&quot;);
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -110,6 +134,16 @@
 
     cout &lt;&lt; &quot;build()&quot; &lt;&lt; endl;
 
+    if( L1_penalty_factor &lt; 0. )
+        PLWARNING(&quot;NxProfileLearner::build:\n&quot;
+                    &quot;L1_penalty_factor is negative!\n&quot;, slr);
+    if( L2_penalty_factor &lt; 0. )
+        PLWARNING(&quot;NxProfileLearner::build:\n&quot;
+                    &quot;L2_penalty_factor is negative!\n&quot;, slr);
+    if( (slr*L2_penalty_factor) &gt; 1. )
+        PLWARNING(&quot;NxProfileLearner::build:\n&quot;
+                    &quot;slr = %f is too large for L2_penalty_factor!\n&quot;, slr);
+
     f_profiles.resize(n_films, profile_dim);     //! matrix of film profiles (n_films*profile_dim)
     u_profiles.resize(n_users, profile_dim);     //! matrix of user profiles (n_users*profile_dim)
 
@@ -128,6 +162,8 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
+    deepCopyField(ngest_films, copies);
+    deepCopyField(ngest_users, copies);
     deepCopyField(f_profiles, copies);
     deepCopyField(u_profiles, copies);
 
@@ -179,6 +215,19 @@
     target.resize(targetsize());  // the train_set's targetsize()
     real weight, error, lr;
 
+    static Vec f_grad, f_natgrad; 
+    static Vec u_grad, u_natgrad;
+    f_grad.resize(profile_dim);
+    f_natgrad.resize(profile_dim);
+    u_grad.resize(profile_dim);
+    u_natgrad.resize(profile_dim);
+
+    // indexes for natural gradient estimator calls
+    // Since calls must have continuous 't's, and we exceed the int limit
+    // we need to do some hacking
+    static int ngf_idx=0;
+    static int ngu_idx=0;
+
     // This generic PLearner method does a number of standard stuff useful for
     // (almost) any learner, and return 'false' if no training should take
     // place. See PLearner.h for more details.
@@ -195,37 +244,83 @@
         PP&lt;ProgressBar&gt; pb;
         if( report_progress )
             pb = new ProgressBar( &quot;Training &quot;+classname(), nsamples);
-        
+
+        // TODO In case minibarches are used, remember to modify in the case of
+        // natural gradient updates.        
         lr = slr/(1.0 + stage*dc);
+        real L1_delta = lr * L1_penalty_factor;
+        real L2_scaling = 1. - lr * L2_penalty_factor;
 
         for(int i=0; i&lt;nsamples; i++)   {
             train_set-&gt;getExample(i, input, target, weight);
 
             PLASSERT( (input[0]&gt;=0) &amp;&amp; (input[0]&lt;n_films) &amp;&amp; (input[1]&gt;=0) &amp;&amp; (input[1]&lt;n_users) );
 
-            // save a function call by not using the functions
-            // We're using squared error cost, but dropping the 2 and taking the
-            // negative already
+            // Save a function call by not using the functions (computeOutput,
+            // etc.). Also, we're using squared error cost, but dropping the 2 and taking
+            // the negative already.
             error = target[0] - dot( f_profiles((int)input[0]), u_profiles((int)input[1]) );
 
-        /*cout &lt;&lt; &quot; f &quot; &lt;&lt; filmProfileID &lt;&lt; &quot; &quot; &lt;&lt; f_profiles(filmProfileID) &lt;&lt; endl;
-        cout &lt;&lt; &quot; u &quot; &lt;&lt; userProfileID &lt;&lt; &quot; &quot; &lt;&lt; u_profiles(userProfileID) &lt;&lt; endl;
-        cout &lt;&lt; &quot;error &quot; &lt;&lt; error &lt;&lt; endl;*/
+            // WHAT FOLLOWS SHOULD PROBABLY BE MADE MORE EFFICIENT
+            // examples:
+            // - for normal gradient multiply lr and error (2 scalars) 
+            // before multiplying the vector
+            // - Consider operating on the vector's elements
+            //- Use the updated film parameters in the user's update
 
-            // Not quite exact. Should do exact (copy the f_profiles entry)? 
-            // Or perhaps alternate based on stage parity?
-            f_profiles((int)input[0]) += lr * error * u_profiles((int)input[1]);
-            u_profiles((int)input[1]) += lr * error * f_profiles((int)input[0]);
+            // the gradients
+            f_grad = error * u_profiles((int)input[1]);
+            u_grad = error * f_profiles((int)input[0]);
 
-            // TODO add regularization
-    /*        real delta_L2 = learning_rate * L2_penalty_factor;
-            if( delta_L2 &gt; 1 )
-                PLWARNING(&quot;GradNNetLayerModule::bpropUpdate:\n&quot;
-                      &quot;learning rate = %f is too large!\n&quot;, learning_rate);
+            // Update the parameters
+            if( !ngest_films )  {
+                f_profiles((int)input[0]) += lr * f_grad;
+            }   else    {
+                (*ngest_films)( ngf_idx, f_grad, f_natgrad );
+                // do index shananigans
+                ngf_idx++;
+                ngf_idx = ngf_idx%ngest_films-&gt;cov_minibatch_size + ngest_films-&gt;cov_minibatch_size;
+                ngest_films-&gt;previous_t = ngf_idx-1;
+                // perform parameter update
+                f_profiles((int)input[0]) += lr * f_natgrad;
+            }
 
-              if( delta_L2 &gt; 0. )
-                    w_[j] *= (1 - delta_L2);
-*/
+            if( !ngest_users )  {
+                u_profiles((int)input[1]) += lr * u_grad;
+            }   else    {
+                (*ngest_users)( ngu_idx, u_grad, u_natgrad );
+                // do index shananigans
+                ngu_idx++;
+                ngu_idx = ngu_idx%ngest_users-&gt;cov_minibatch_size + ngest_users-&gt;cov_minibatch_size;
+                ngest_users-&gt;previous_t = ngu_idx-1;
+                // perform parameter update
+                u_profiles((int)input[1]) += lr * u_natgrad;
+            }
+
+            // L1 regularization
+            for( int d=0; d&lt;profile_dim; d++ )  {
+                // films
+                if( f_profiles((int)input[0], d) &gt; L1_delta )
+                    f_profiles((int)input[0], d) -= L1_delta;
+                else if( f_profiles((int)input[0], d) &lt; -L1_delta )
+                    f_profiles((int)input[0], d) += L1_delta;
+                else
+                    f_profiles((int)input[0], d) = 0.;
+                // users
+                if( u_profiles((int)input[1], d) &gt; L1_delta )
+                    u_profiles((int)input[1], d) -= L1_delta;
+                else if( u_profiles((int)input[1], d) &lt; -L1_delta )
+                    u_profiles((int)input[1], d) += L1_delta;
+                else
+                    u_profiles((int)input[1], d) = 0.;
+            }
+
+            // L2 regularization
+            if( L2_penalty_factor != 0. )    {
+                f_profiles((int)input[0]) *= L2_scaling;
+                u_profiles((int)input[1]) *= L2_scaling;
+            }
+
             //train_stats-&gt;update(train_costs)
             if( pb )
                 pb-&gt;update(i);

Modified: trunk/plearn_learners_experimental/netflix/NxProfileLearner.h
===================================================================
--- trunk/plearn_learners_experimental/netflix/NxProfileLearner.h	2007-05-02 16:46:44 UTC (rev 6971)
+++ trunk/plearn_learners_experimental/netflix/NxProfileLearner.h	2007-05-02 17:44:57 UTC (rev 6972)
@@ -41,6 +41,7 @@
 #define NxProfileLearner_INC
 
 #include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn_learners/generic/NatGradEstimator.h&gt;
 
 namespace PLearn {
 
@@ -68,6 +69,22 @@
     real slr;           //! start learning rate
     real dc;            //! learning rate decrease constant
 
+    real L1_penalty_factor;
+    real L2_penalty_factor;
+
+    //! *Natural gradient* 
+    //! For now we use 2 estimators: 1 for films and one for users.
+    //! We also treat each user/film gradient as if it were on the same parameters,
+    //! ie, feed all user gradients to the same estimator.
+
+    //! Natural gradient estimator for the film parameters.
+    //! (if 0 then do not use natural gradient)
+    PP&lt;NatGradEstimator&gt; ngest_films;
+
+    //! Natural gradient estimator for the user parameters.
+    //! (if 0 then do not use natural gradient)
+    PP&lt;NatGradEstimator&gt; ngest_users;
+
     //#####  Public NOT Options  ##############################################
 
     int n_films;


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000420.html">[Plearn-commits] r6971 - trunk/plearn/math
</A></li>
	<LI>Next message: <A HREF="000422.html">[Plearn-commits] r6973 - trunk/plearn_learners_experimental/netflix
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#421">[ date ]</a>
              <a href="thread.html#421">[ thread ]</a>
              <a href="subject.html#421">[ subject ]</a>
              <a href="author.html#421">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
