<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7011 - in trunk/plearn_learners/online: .	test/DeepBeliefNet	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7011%20-%20in%20trunk/plearn_learners/online%3A%20.%0A%09test/DeepBeliefNet%0A%09test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results%0A%09test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir%0A%09test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0%0A%09test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir&In-Reply-To=%3C200705050108.l45180Tg019241%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000459.html">
   <LINK REL="Next"  HREF="000461.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7011 - in trunk/plearn_learners/online: .	test/DeepBeliefNet	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7011%20-%20in%20trunk/plearn_learners/online%3A%20.%0A%09test/DeepBeliefNet%0A%09test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results%0A%09test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir%0A%09test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0%0A%09test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir&In-Reply-To=%3C200705050108.l45180Tg019241%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7011 - in trunk/plearn_learners/online: .	test/DeepBeliefNet	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0	test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir">lamblin at mail.berlios.de
       </A><BR>
    <I>Sat May  5 03:08:00 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000459.html">[Plearn-commits] r7010 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="000461.html">[Plearn-commits] r7012 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#460">[ date ]</a>
              <a href="thread.html#460">[ thread ]</a>
              <a href="subject.html#460">[ subject ]</a>
              <a href="author.html#460">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-05-05 03:07:59 +0200 (Sat, 05 May 2007)
New Revision: 7011

Removed:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat1results.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat1results.pmat.metadata/
Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/SimpleRBM_test.pyplearn
Log:
Do not tie nstages to be equal to sum(training_schedule).
Now, we will train until stage == min(nstages, sum(training_schedule)).
Update the test.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-05-04 19:40:29 UTC (rev 7010)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-05-05 01:07:59 UTC (rev 7011)
@@ -121,9 +121,10 @@
                   OptionBase::buildoption,
                   &quot;Number of examples to use during each phase of learning:\n&quot;
                   &quot;first the greedy phases, and then the fine-tuning phase.\n&quot;
+                  &quot;However, the learning will stop as soon as we reach nstages.\n&quot;
                   &quot;For example for 2 hidden layers, with 1000 examples in each\n&quot;
                   &quot;greedy phase, and 500 in the fine-tuning phase, this option\n&quot;
-                  &quot;should be [1000 1000 500], and nstages will be set to 2500.\n&quot;
+                  &quot;should be [1000 1000 500], and nstages should be at least 2500.\n&quot;
                   &quot;When online = true, this vector is ignored and should be empty.\n&quot;);
 
     declareOption(ol, &quot;use_classification_cost&quot;,
@@ -141,8 +142,8 @@
                   &quot;This is done using cross-entropy between actual and reconstructed.\n&quot;
                   &quot;This option automatically adds the following cost names:\n&quot;
                   &quot;   layerwise_reconstruction_error (sum over all layers)\n&quot;
-                  &quot;   layer1_reconstruction_error (only layer 1)\n&quot;
-                  &quot;   layer2_reconstruction_error (only layer 2)\n&quot;
+                  &quot;   layer0.reconstruction_error (only layers[0])\n&quot;
+                  &quot;   layer1.reconstruction_error (only layers[1])\n&quot;
                   &quot;   etc.\n&quot;);
 
     declareOption(ol, &quot;layers&quot;, &amp;DeepBeliefNet::layers,
@@ -300,9 +301,6 @@
             cumulative_schedule[i+1] = cumulative_schedule[i] +
                 training_schedule[i];
         }
-
-        // nstages is the total number of previously seen examples
-        nstages = cumulative_schedule[n_layers];
     }
 
     build_layers_and_connections();
@@ -688,13 +686,11 @@
 
     if (!online)
     {
-        // Enforce values of cumulative_schedule and nstages,
-        // because build_() might not be called if we change training_schedule
-        // inside a HyperLearner
+        // Enforce value of cumulative_schedule because build_() might
+        // not be called if we change training_schedule inside a HyperLearner
         for( int i=0 ; i&lt;n_layers ; i++ )
             cumulative_schedule[i+1] = cumulative_schedule[i] +
                 training_schedule[i];
-        nstages = cumulative_schedule[n_layers];
     }
 
     MODULE_LOG &lt;&lt; &quot;  training_schedule = &quot; &lt;&lt; training_schedule &lt;&lt; endl;
@@ -718,16 +714,6 @@
         optimized_costs.resize(minibatch_size);
     }
 
-    /* Why is it here???????
-       it's copy-pasted from build_()!!!!
-    layers[n_layers-1]-&gt;random_gen = random_gen;
-    int last_layer_size = layers[n_layers-1]-&gt;size;
-    PLASSERT_MSG(last_layer_size &gt;= 0,
-                 &quot;Size of last layer must be non-negative&quot;);
-    activation_gradients[n_layers-1].resize(last_layer_size);
-    expectation_gradients[n_layers-1].resize(last_layer_size);
-    */
-
     Vec input( inputsize() );
     Vec target( targetsize() );
     real weight; // unused
@@ -787,16 +773,17 @@
             if( use_classification_cost &amp;&amp; i == n_layers-2 )
                 break; // we will do a joint supervised learning instead
 
+            int end_stage = min(cumulative_schedule[i+1], nstages);
+            if( stage &gt;= end_stage )
+                continue;
+
             MODULE_LOG &lt;&lt; &quot;Training connection weights between layers &quot; &lt;&lt; i
                        &lt;&lt; &quot; and &quot; &lt;&lt; i+1 &lt;&lt; endl;
-
-            int end_stage = cumulative_schedule[i+1];
-
             MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  cd_learning_rate = &quot; &lt;&lt; cd_learning_rate &lt;&lt; endl;
 
-            if( report_progress &amp;&amp; stage &lt; end_stage )
+            if( report_progress )
                 pb = new ProgressBar( &quot;Training layer &quot;+tostring(i)
                                       +&quot; of &quot;+classname(),
                                       end_stage - stage );
@@ -829,20 +816,18 @@
         }
 
         // possible supervised part
-        if( use_classification_cost )
+        int end_stage = min(cumulative_schedule[n_layers-1], nstages);
+        if( use_classification_cost &amp;&amp; (stage &lt; end_stage) )
         {
             PLASSERT_MSG(batch_size == 1, &quot;'use_classification_cost' code not &quot;
                     &quot;verified with mini-batch learning yet&quot;);
 
             MODULE_LOG &lt;&lt; &quot;Training the classification module&quot; &lt;&lt; endl;
-
-            int end_stage = cumulative_schedule[n_layers-1];
-
             MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  cd_learning_rate = &quot; &lt;&lt; cd_learning_rate &lt;&lt; endl;
 
-            if( report_progress &amp;&amp; stage &lt; end_stage )
+            if( report_progress )
                 pb = new ProgressBar( &quot;Training the classification module&quot;,
                                       end_stage - stage );
 
@@ -866,24 +851,25 @@
 
 
         /***** fine-tuning by gradient descent *****/
-        if( stage &gt;= nstages )
+        end_stage = min(cumulative_schedule[n_layers], nstages);
+        if( stage &gt;= end_stage )
             return;
         MODULE_LOG &lt;&lt; &quot;Fine-tuning all parameters, by gradient descent&quot; &lt;&lt; endl;
         MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
-        MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
         MODULE_LOG &lt;&lt; &quot;  grad_learning_rate = &quot; &lt;&lt; grad_learning_rate &lt;&lt; endl;
 
         int init_stage = stage;
-        if( report_progress &amp;&amp; stage &lt; nstages )
+        if( report_progress )
             pb = new ProgressBar( &quot;Fine-tuning parameters of all layers of &quot;
                                   + classname(),
-                                  nstages - init_stage );
+                                  end_stage - init_stage );
 
         setLearningRate( grad_learning_rate );
 
         train_stats-&gt;forget();
         bool update_stats = false;
-        for( ; stage&lt;nstages ; stage++ )
+        for( ; stage&lt;end_stage ; stage++ )
         {
             initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
             // Update every 'minibatch_size' samples.
@@ -891,7 +877,7 @@
                 int sample_start = stage % nsamples;
                 // Only update train statistics for the last 'epoch', i.e. last
                 // 'nsamples' seen.
-                update_stats = update_stats || stage &gt;= nstages - nsamples;
+                update_stats = update_stats || stage &gt;= end_stage - nsamples;
 
                 if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
                     setLearningRate( grad_learning_rate

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-05-04 19:40:29 UTC (rev 7010)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-05-05 01:07:59 UTC (rev 7011)
@@ -88,7 +88,12 @@
     int n_classes;
 
     //! Number of examples to use during each phase of learning:
-    //! first the greedy phases, and then the gradient descent
+    //! first the greedy phases, and then the fine-tuning phase.
+    //! However, the learning will stop as soon as we reach nstages.
+    //! For example for 2 hidden layers, with 1000 examples in each
+    //! greedy phase, and 500 in the fine-tuning phase, this option
+    //! should be [1000 1000 500], and nstages should be at least 2500.
+    //! When online = true, this vector is ignored and should be empty.
     TVec&lt;int&gt; training_schedule;
 
     //! If the first cost function is the NLL in classification,
@@ -100,8 +105,8 @@
     //! This is done using cross-entropy between actual and reconstructed.
     //! This option automatically adds the following cost names:
     //! layerwise_reconstruction_error (sum over all layers)
-    //!   layer1_reconstruction_error (only layer 1)
-    //!   layer2_reconstruction_error (only layer 2)
+    //!   layer0.reconstruction_error (only layers[0])
+    //!   layer1.reconstruction_error (only layers[1])
     //!   etc.
     bool reconstruct_layerwise;
 

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log	2007-05-04 19:40:29 UTC (rev 7010)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log	2007-05-05 01:07:59 UTC (rev 7011)
@@ -9,148 +9,128 @@
 [DeepBeliefNet] build_classification_cost() called
 [RBMClassificationModule] build_() called
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 0 0 
-[DeepBeliefNet]   cumulative_schedule = 0 0 0 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 0, target nstages = 0
  WARNING: In PLearner::initTrain (called by 'DeepBeliefNet') - The learner is already trained
 [DeepBeliefNet] train() aborted
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 2 0 
-[DeepBeliefNet]   cumulative_schedule = 0 2 2 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 0, target nstages = 2
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 0
 [DeepBeliefNet]   end_stage = 2
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 4 0 
-[DeepBeliefNet]   cumulative_schedule = 0 4 4 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 2, target nstages = 4
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 2
 [DeepBeliefNet]   end_stage = 4
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 6 0 
-[DeepBeliefNet]   cumulative_schedule = 0 6 6 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 4, target nstages = 6
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 4
 [DeepBeliefNet]   end_stage = 6
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 8 0 
-[DeepBeliefNet]   cumulative_schedule = 0 8 8 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 6, target nstages = 8
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 6
 [DeepBeliefNet]   end_stage = 8
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 10 0 
-[DeepBeliefNet]   cumulative_schedule = 0 10 10 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 8, target nstages = 10
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 8
 [DeepBeliefNet]   end_stage = 10
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 12 0 
-[DeepBeliefNet]   cumulative_schedule = 0 12 12 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 10, target nstages = 12
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 10
 [DeepBeliefNet]   end_stage = 12
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 14 0 
-[DeepBeliefNet]   cumulative_schedule = 0 14 14 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 12, target nstages = 14
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 12
 [DeepBeliefNet]   end_stage = 14
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 16 0 
-[DeepBeliefNet]   cumulative_schedule = 0 16 16 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 14, target nstages = 16
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 14
 [DeepBeliefNet]   end_stage = 16
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 18 0 
-[DeepBeliefNet]   cumulative_schedule = 0 18 18 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 16, target nstages = 18
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 16
 [DeepBeliefNet]   end_stage = 18
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 20 0 
-[DeepBeliefNet]   cumulative_schedule = 0 20 20 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 18, target nstages = 20
 [DeepBeliefNet] Training the classification module
 [DeepBeliefNet]   stage = 18
 [DeepBeliefNet]   end_stage = 20
 [DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 18 2 
-[DeepBeliefNet]   cumulative_schedule = 0 18 20 
-[DeepBeliefNet] stage = 18, target nstages = 20
-[DeepBeliefNet] Training the classification module
-[DeepBeliefNet]   stage = 18
-[DeepBeliefNet]   end_stage = 18
-[DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
-[DeepBeliefNet] Fine-tuning all parameters, by gradient descent
-[DeepBeliefNet]   stage = 18
-[DeepBeliefNet]   nstages = 20
-[DeepBeliefNet]   grad_learning_rate = 0.100000000000000006
-[DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 18 4 
-[DeepBeliefNet]   cumulative_schedule = 0 18 22 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 20, target nstages = 22
-[DeepBeliefNet] Training the classification module
-[DeepBeliefNet]   stage = 20
-[DeepBeliefNet]   end_stage = 18
-[DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] Fine-tuning all parameters, by gradient descent
 [DeepBeliefNet]   stage = 20
-[DeepBeliefNet]   nstages = 22
+[DeepBeliefNet]   end_stage = 22
 [DeepBeliefNet]   grad_learning_rate = 0.100000000000000006
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 18 6 
-[DeepBeliefNet]   cumulative_schedule = 0 18 24 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 22, target nstages = 24
-[DeepBeliefNet] Training the classification module
-[DeepBeliefNet]   stage = 22
-[DeepBeliefNet]   end_stage = 18
-[DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] Fine-tuning all parameters, by gradient descent
 [DeepBeliefNet]   stage = 22
-[DeepBeliefNet]   nstages = 24
+[DeepBeliefNet]   end_stage = 24
 [DeepBeliefNet]   grad_learning_rate = 0.100000000000000006
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 18 8 
-[DeepBeliefNet]   cumulative_schedule = 0 18 26 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 24, target nstages = 26
-[DeepBeliefNet] Training the classification module
-[DeepBeliefNet]   stage = 24
-[DeepBeliefNet]   end_stage = 18
-[DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] Fine-tuning all parameters, by gradient descent
 [DeepBeliefNet]   stage = 24
-[DeepBeliefNet]   nstages = 26
+[DeepBeliefNet]   end_stage = 26
 [DeepBeliefNet]   grad_learning_rate = 0.100000000000000006
 [DeepBeliefNet] train() called 
-[DeepBeliefNet]   training_schedule = 18 10 
-[DeepBeliefNet]   cumulative_schedule = 0 18 28 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
 [DeepBeliefNet] stage = 26, target nstages = 28
-[DeepBeliefNet] Training the classification module
-[DeepBeliefNet]   stage = 26
-[DeepBeliefNet]   end_stage = 18
-[DeepBeliefNet]   cd_learning_rate = 0.0100000000000000002
 [DeepBeliefNet] Fine-tuning all parameters, by gradient descent
 [DeepBeliefNet]   stage = 26
-[DeepBeliefNet]   nstages = 28
+[DeepBeliefNet]   end_stage = 28
 [DeepBeliefNet]   grad_learning_rate = 0.100000000000000006
+[DeepBeliefNet] train() called 
+[DeepBeliefNet]   training_schedule = 20 10 
+[DeepBeliefNet]   cumulative_schedule = 0 20 30 
+[DeepBeliefNet] stage = 28, target nstages = 30
+[DeepBeliefNet] Fine-tuning all parameters, by gradient descent
+[DeepBeliefNet]   stage = 28
+[DeepBeliefNet]   end_stage = 30
+[DeepBeliefNet]   grad_learning_rate = 0.100000000000000006

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Deleted: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat1results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2007-05-04 19:40:29 UTC (rev 7010)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2007-05-05 01:07:59 UTC (rev 7011)
@@ -3,7 +3,7 @@
 grad_learning_rate = 0.100000000000000006 ;
 batch_size = 1 ;
 n_classes = 2 ;
-training_schedule = 2 [ 18 10 ] ;
+training_schedule = 2 [ 20 10 ] ;
 use_classification_cost = 1 ;
 reconstruct_layerwise = 0 ;
 layers = 2 [ *1 -&gt;RBMBinomialLayer(
@@ -119,13 +119,13 @@
 ]
 ] ;
 seed = 1827 ;
-stage = 28 ;
+stage = 30 ;
 n_examples = 2 ;
 inputsize = 2 ;
 targetsize = 1 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 28 ;
+nstages = 30 ;
 report_progress = 1 ;
 verbosity = 1 ;
 nservers = 0 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2007-05-04 19:40:29 UTC (rev 7010)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2007-05-05 01:07:59 UTC (rev 7011)
@@ -34,7 +34,7 @@
 grad_learning_rate = 0.100000000000000006 ;
 batch_size = 1 ;
 n_classes = 2 ;
-training_schedule = 2 [ 18 10 ] ;
+training_schedule = 2 [ 20 10 ] ;
 use_classification_cost = 1 ;
 reconstruct_layerwise = 0 ;
 layers = 2 [ *6 -&gt;RBMBinomialLayer(
@@ -150,13 +150,13 @@
 ]
 ] ;
 seed = 1827 ;
-stage = 28 ;
+stage = 30 ;
 n_examples = 2 ;
 inputsize = 2 ;
 targetsize = 1 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 28 ;
+nstages = 30 ;
 report_progress = 1 ;
 verbosity = 1 ;
 nservers = 0 ;
@@ -183,22 +183,22 @@
 enforce_clean_expdir = 1  )
 ;
 option_fields = 1 [ &quot;nstages&quot; ] ;
-dont_restart_upon_change = 2 [ &quot;training_schedule[0]&quot; &quot;training_schedule[1]&quot; ] ;
-strategy = 2 [ *14 -&gt;HyperOptimize(
+dont_restart_upon_change = 1 [ &quot;nstages&quot; ] ;
+strategy = 1 [ *14 -&gt;HyperOptimize(
 which_cost = &quot;2&quot; ;
 min_n_trials = 0 ;
 oracle = *15 -&gt;EarlyStoppingOracle(
-option = &quot;training_schedule[0]&quot; ;
+option = &quot;nstages&quot; ;
 values = []
 ;
-range = 3 [ 0 21 2 ] ;
+range = 3 [ 0 31 2 ] ;
 min_value = -3.4028234663852886e+38 ;
 max_value = 3.4028234663852886e+38 ;
 max_degradation = 3.4028234663852886e+38 ;
 relative_max_degradation = -1 ;
 min_improvement = -3.4028234663852886e+38 ;
 relative_min_improvement = -1 ;
-max_degraded_steps = 21 ;
+max_degraded_steps = 31 ;
 min_n_steps = 0  )
 ;
 provide_tester_expdir = 0 ;
@@ -207,29 +207,6 @@
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
 splitter = *0  )
-*16 -&gt;HyperOptimize(
-which_cost = &quot;2&quot; ;
-min_n_trials = 0 ;
-oracle = *17 -&gt;EarlyStoppingOracle(
-option = &quot;training_schedule[1]&quot; ;
-values = []
-;
-range = 3 [ 2 11 2 ] ;
-min_value = -3.4028234663852886e+38 ;
-max_value = 3.4028234663852886e+38 ;
-max_degradation = 3.4028234663852886e+38 ;
-relative_max_degradation = -1 ;
-min_improvement = -3.4028234663852886e+38 ;
-relative_min_improvement = -1 ;
-max_degraded_steps = 11 ;
-min_n_steps = 0  )
-;
-provide_tester_expdir = 0 ;
-sub_strategy = []
-;
-rerun_after_sub = 0 ;
-provide_sub_expdir = 1 ;
-splitter = *0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 1 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/experiment.plearn	2007-05-04 19:40:29 UTC (rev 7010)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/experiment.plearn	2007-05-05 01:07:59 UTC (rev 7011)
@@ -1,4 +1,4 @@
-*14 -&gt; PTester(
+*12 -&gt; PTester(
     dataset = *1 -&gt; MemoryVMatrix(
         data = 2 3 [
                 1,
@@ -13,11 +13,8 @@
         weightsize = 0
         ),
     expdir = &quot;expdir&quot;,
-    learner = *12 -&gt; HyperLearner(
-        dont_restart_upon_change = [
-            &quot;training_schedule[0]&quot;,
-            &quot;training_schedule[1]&quot;
-            ],
+    learner = *10 -&gt; HyperLearner(
+        dont_restart_upon_change = [ &quot;nstages&quot; ],
         learner = *5 -&gt; DeepBeliefNet(
             cd_learning_rate = 0.01,
             connections = [
@@ -33,10 +30,11 @@
                 *4 -&gt; RBMBinomialLayer( size = 2 )
                 ],
             n_classes = 2,
+            nstages = 0,
             report_progress = 1,
             training_schedule = [
-                0,
-                0
+                20,
+                10
                 ],
             use_classification_cost = 1
             ),
@@ -46,34 +44,22 @@
         strategy = [
             *7 -&gt; HyperOptimize(
                 oracle = *6 -&gt; EarlyStoppingOracle(
-                    max_degraded_steps = 21,
-                    option = &quot;training_schedule[0]&quot;,
+                    max_degraded_steps = 31,
+                    option = &quot;nstages&quot;,
                     range = [
                         0,
-                        21,
+                        31,
                         2
                         ]
                     ),
                 which_cost = 2
-                ),
-            *9 -&gt; HyperOptimize(
-                oracle = *8 -&gt; EarlyStoppingOracle(
-                    max_degraded_steps = 11,
-                    option = &quot;training_schedule[1]&quot;,
-                    range = [
-                        2,
-                        11,
-                        2
-                        ]
-                    ),
-                which_cost = 2
                 )
             ],
-        tester = *11 -&gt; PTester(
+        tester = *9 -&gt; PTester(
             save_initial_tester = 1,
             save_test_costs = 1,
             save_test_outputs = 1,
-            splitter = *10 -&gt; FractionSplitter(
+            splitter = *8 -&gt; FractionSplitter(
                 splits = 1 2 [
                         (0, 1),
                         (0, 1)
@@ -92,7 +78,7 @@
     save_learners = 1,
     save_test_costs = 1,
     save_test_outputs = 1,
-    splitter = *13 -&gt; NoSplitSplitter( ),
+    splitter = *11 -&gt; NoSplitSplitter( ),
     statnames = [
         &quot;E[train.E[E[train.E[NLL]]]]&quot;,
         &quot;E[train.E[E[train.E[class_error]]]]&quot;,

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2007-05-04 19:40:29 UTC (rev 7010)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2007-05-05 01:07:59 UTC (rev 7011)
@@ -43,7 +43,7 @@
 grad_learning_rate = 0.100000000000000006 ;
 batch_size = 1 ;
 n_classes = 2 ;
-training_schedule = 2 [ 0 0 ] ;
+training_schedule = 2 [ 20 10 ] ;
 use_classification_cost = 1 ;
 reconstruct_layerwise = 0 ;
 layers = 2 [ *7 -&gt;RBMBinomialLayer(
@@ -192,22 +192,22 @@
 enforce_clean_expdir = 1  )
 ;
 option_fields = 1 [ &quot;nstages&quot; ] ;
-dont_restart_upon_change = 2 [ &quot;training_schedule[0]&quot; &quot;training_schedule[1]&quot; ] ;
-strategy = 2 [ *15 -&gt;HyperOptimize(
+dont_restart_upon_change = 1 [ &quot;nstages&quot; ] ;
+strategy = 1 [ *15 -&gt;HyperOptimize(
 which_cost = &quot;2&quot; ;
 min_n_trials = 0 ;
 oracle = *16 -&gt;EarlyStoppingOracle(
-option = &quot;training_schedule[0]&quot; ;
+option = &quot;nstages&quot; ;
 values = []
 ;
-range = 3 [ 0 21 2 ] ;
+range = 3 [ 0 31 2 ] ;
 min_value = -3.4028234663852886e+38 ;
 max_value = 3.4028234663852886e+38 ;
 max_degradation = 3.4028234663852886e+38 ;
 relative_max_degradation = -1 ;
 min_improvement = -3.4028234663852886e+38 ;
 relative_min_improvement = -1 ;
-max_degraded_steps = 21 ;
+max_degraded_steps = 31 ;
 min_n_steps = 0  )
 ;
 provide_tester_expdir = 0 ;
@@ -216,29 +216,6 @@
 rerun_after_sub = 0 ;
 provide_sub_expdir = 1 ;
 splitter = *0  )
-*17 -&gt;HyperOptimize(
-which_cost = &quot;2&quot; ;
-min_n_trials = 0 ;
-oracle = *18 -&gt;EarlyStoppingOracle(
-option = &quot;training_schedule[1]&quot; ;
-values = []
-;
-range = 3 [ 2 11 2 ] ;
-min_value = -3.4028234663852886e+38 ;
-max_value = 3.4028234663852886e+38 ;
-max_degradation = 3.4028234663852886e+38 ;
-relative_max_degradation = -1 ;
-min_improvement = -3.4028234663852886e+38 ;
-relative_min_improvement = -1 ;
-max_degraded_steps = 11 ;
-min_n_steps = 0  )
-;
-provide_tester_expdir = 0 ;
-sub_strategy = []
-;
-rerun_after_sub = 0 ;
-provide_sub_expdir = 1 ;
-splitter = *0  )
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 1 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/SimpleRBM_test.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/SimpleRBM_test.pyplearn	2007-05-04 19:40:29 UTC (rev 7010)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/SimpleRBM_test.pyplearn	2007-05-05 01:07:59 UTC (rev 7011)
@@ -34,7 +34,8 @@
 learner = pl.DeepBeliefNet(
         cd_learning_rate = 0.01,
         grad_learning_rate = 0.1,
-        training_schedule = [ 0, 0 ],
+        training_schedule = [ n_epochs_cd, n_epochs_grad ],
+        nstages = 0,
         n_classes = n_classes,
         use_classification_cost = 1,
         layers = layers,
@@ -62,27 +63,16 @@
         tester = tester,
         learner = learner,
         option_fields = [ 'nstages' ],
-        dont_restart_upon_change = [
-            'training_schedule[0]',
-            'training_schedule[1]'
-            ],
+        dont_restart_upon_change = [ 'nstages' ],
         provide_strategy_expdir = 1,
         strategy = [
             pl.HyperOptimize(
                 which_cost = 2,
                 oracle = pl.EarlyStoppingOracle(
-                    option = 'training_schedule[0]',
-                    range = [ 0, n_epochs_cd+1, 2 ],
-                    max_degraded_steps = n_epochs_cd+1
+                    option = 'nstages',
+                    range = [ 0, n_epochs_cd+n_epochs_grad+1, 2 ],
+                    max_degraded_steps = n_epochs_cd+n_epochs_grad+1
                     )
-                ),
-            pl.HyperOptimize(
-                which_cost = 2,
-                oracle = pl.EarlyStoppingOracle(
-                    option = 'training_schedule[1]',
-                    range = [ 2, n_epochs_grad+1, 2 ],
-                    max_degraded_steps = n_epochs_grad+1
-                    )
                 )
             ],
         report_progress = 1


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000459.html">[Plearn-commits] r7010 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="000461.html">[Plearn-commits] r7012 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#460">[ date ]</a>
              <a href="thread.html#460">[ thread ]</a>
              <a href="subject.html#460">[ subject ]</a>
              <a href="author.html#460">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
