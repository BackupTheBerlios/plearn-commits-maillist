<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7153 -	trunk/plearn_learners/distributions/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7153%20-%0A%09trunk/plearn_learners/distributions/EXPERIMENTAL&In-Reply-To=%3C200705171829.l4HITIm4010401%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000601.html">
   <LINK REL="Next"  HREF="000603.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7153 -	trunk/plearn_learners/distributions/EXPERIMENTAL</H1>
    <B>lysiane at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7153%20-%0A%09trunk/plearn_learners/distributions/EXPERIMENTAL&In-Reply-To=%3C200705171829.l4HITIm4010401%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7153 -	trunk/plearn_learners/distributions/EXPERIMENTAL">lysiane at mail.berlios.de
       </A><BR>
    <I>Thu May 17 20:29:18 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000601.html">[Plearn-commits] r7152 - trunk/python_modules/plearn/parallel
</A></li>
        <LI>Next message: <A HREF="000603.html">[Plearn-commits] r7154 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#602">[ date ]</a>
              <a href="thread.html#602">[ thread ]</a>
              <a href="subject.html#602">[ subject ]</a>
              <a href="author.html#602">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lysiane
Date: 2007-05-17 20:29:16 +0200 (Thu, 17 May 2007)
New Revision: 7153

Added:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
Initial version of TransformationLearner


Added: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-05-17 18:10:51 UTC (rev 7152)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-05-17 18:29:16 UTC (rev 7153)
@@ -0,0 +1,1262 @@
+// -*- C++ -*-
+
+// TransformationLearner.cc
+//
+//version 5
+// Copyright (C) 2007 Lysiane Bouchard
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Lysiane Bouchard
+
+/*! \file TransformationLearner.cc */
+
+
+#include &quot;TransformationLearner.h&quot;
+
+//C++ 
+#include &lt;math.h&gt;
+
+
+//Plearn
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &lt;plearn/math/PRandom.h&gt;
+#include &lt;plearn/math/plapack.h&gt;
+
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    TransformationLearner,
+    &quot;ONE LINE DESCRIPTION&quot;,
+    &quot;MULTI-LINE \nHELP&quot;);
+
+//TO TEST : OK
+TransformationLearner::TransformationLearner():
+/* ### Initialize all fields to their default value here */
+    seed(1827),
+    transformFamily(TRANSFORM_FAMILY_LINEAR),
+    noiseVariance(1.0),
+    transformsVariance(0.5),
+    nbTransforms(5),
+    nbNeighbors(5),
+    epsilonInitWeight(0.01)
+{
+    // ...
+    //pout &lt;&lt; &quot;TransformationLearner()&quot; &lt;&lt; endl;
+    random_gen = new PRandom();
+    
+
+    // ### You may (or not) want to call build_() to finish building the object
+    // ### (doing so assumes the parent classes' build_() have been called too
+    // ### in the parent classes' constructors, something that you must ensure)
+
+    // ### If this learner needs to generate random numbers, uncomment the
+    // ### line below to enable the use of the inherited PRandom object.
+    // random_gen = new PRandom();
+}
+
+
+//TO TEST
+void TransformationLearner::declareOptions(OptionList&amp; ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the &quot;flags&quot; of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, &quot;myoption&quot;, &amp;TransformationLearner::myoption,
+    //               OptionBase::buildoption,
+    //               &quot;Help text describing this option&quot;);
+    // ...
+    
+    declareOption(ol, 
+                  &quot;seed&quot;, 
+                  &amp;TransformationLearner::seed, 
+                  OptionBase::buildoption,
+                  &quot;seed of the random generator&quot;);
+    declareOption(ol, 
+                  &quot;transformFamily&quot;, 
+                  &amp;TransformationLearner::transformFamily, 
+                  OptionBase::buildoption,
+                  &quot;transformation function Family&quot;);
+    declareOption(ol, 
+                  &quot;noiseVariance&quot;, 
+                  &amp;TransformationLearner::noiseVariance, 
+                  OptionBase::buildoption,
+                  &quot;variance on the noise r.v. (normaly distributed with mean 0)&quot;);
+    declareOption(ol,
+                  &quot;transformsVariance&quot;, 
+                  &amp;TransformationLearner::transformsVariance, 
+                  OptionBase::buildoption,
+                  &quot;variance on the transformation parameters r.vs&quot;
+                  &quot;(normaly distributed with mean 0)&quot;);
+    declareOption(ol,
+                  &quot;nbTransforms&quot;, 
+                  &amp;TransformationLearner::nbTransforms, 
+                  OptionBase::buildoption,
+                  &quot;number of transformations&quot;);
+    declareOption(ol,
+                  &quot;nbNeighbors&quot;, 
+                  &amp;TransformationLearner::nbNeighbors, 
+                  OptionBase::buildoption,
+                  &quot;number of neighbors&quot;);
+    declareOption(ol,
+                  &quot;epsilonInitWeight&quot;, 
+                  &amp;TransformationLearner::epsilonInitWeight, 
+                  OptionBase::buildoption,
+                  &quot;smallest amount of weight we can give to a choosen \n&quot;
+                  &quot;generation candidate at initialization of the \n &quot;
+                  &quot;generation set&quot;);
+
+    declareOption(ol,
+                  &quot;transformDistribution&quot;, 
+                  &amp;TransformationLearner::transformDistribution, 
+                  OptionBase::buildoption,
+                  &quot;a multinomial distribution for the transformations\n&quot;
+                  &quot;i.e. p(kth transformation) = transformDistribution[k] \n &quot;);
+
+    declareOption(ol,
+                  &quot;inputSpaceDim&quot;, 
+                  &amp;TransformationLearner::inputSpaceDim, 
+                  OptionBase::learntoption,
+                  &quot;dimension of the training set input space&quot;);
+    declareOption(ol,
+                  &quot;nbGenerationCandidatesPerTarget&quot;, 
+                  &amp;TransformationLearner::nbGenerationCandidatesPerTarget, 
+                  OptionBase::learntoption,
+                  &quot;number of generation candidates per target&quot;); 
+    declareOption(ol,
+                  &quot;nbGenerationCandidates&quot;, 
+                  &amp;TransformationLearner::nbGenerationCandidates, 
+                  OptionBase::learntoption,
+                  &quot;number of generation candidates in the generation set&quot;);
+    declareOption(ol,
+                  &quot;nbTrainingInput&quot;, 
+                  &amp;TransformationLearner::nbTrainingInput, 
+                  OptionBase::learntoption,
+                  &quot;number of input given in the training set&quot;);  
+    declareOption(ol,
+                  &quot;trainsformsSet&quot;, 
+                  &amp;TransformationLearner::transformsSet, 
+                  OptionBase::learntoption,
+                  &quot;set of transformations&quot;);
+    declareOption(ol,
+                  &quot;transforms&quot;, 
+                  &amp;TransformationLearner::transforms, 
+                  OptionBase::learntoption,
+                  &quot;views on the transformation set&quot;);
+    
+    declareOption(ol,
+                  &quot;generationSet&quot;, 
+                  &amp;TransformationLearner::generationSet, 
+                  OptionBase::learntoption,
+                  &quot;set of generation candidates&quot;
+                  &quot;i.e. triples (target, neighbor, transformation)&quot;);
+    declareOption(ol,
+                  &quot;lambda&quot;, 
+                  &amp;TransformationLearner::lambda, 
+                  OptionBase::learntoption,
+                  &quot;weight decay&quot;);
+
+    declareOption(ol,
+                  &quot;noiseVarianceFactor&quot;, 
+                  &amp;TransformationLearner::noiseVarianceFactor, 
+                  OptionBase::learntoption,
+                  &quot;factor used in computation of generation weights&quot;
+                  &quot; 1/(2*noise variance)&quot;);
+
+    declareOption(ol,
+                  &quot;noiseStDev&quot;, 
+                  &amp;TransformationLearner::noiseStDev, 
+                  OptionBase::learntoption,
+                  &quot;standard deviation on noise distribution&quot;);
+    declareOption(ol,
+                  &quot;transformsStDev&quot;, 
+                  &amp;TransformationLearner::transformsStDev, 
+                  OptionBase::learntoption,
+                  &quot;standard deviation on transformation parameters&quot;);
+
+    // Now call the parent class' declareOptions
+    
+
+
+    inherited::declareOptions(ol);
+}
+
+
+//TO TEST
+void TransformationLearner::declareMethods(RemoteMethodMap&amp; rmm)
+{
+    declareMethod(rmm, &quot;largeEStepA&quot;, &amp;TransformationLearner::largeEStepA,
+                  (BodyDoc(&quot;Performs a large update of the generation set (expectation step)&quot;  
+                           &quot;For each target, we take the best generation candidates among all the possibilities \n&quot;)));
+    
+    declareMethod(rmm, &quot;initEStep&quot;, &amp;TransformationLearner::initEStep,
+                  (BodyDoc(&quot;Initialization of the generation set (expectation step)\n&quot; 
+                           &quot;For each possible couple (target,neighbor), we take the best transformations to form \n&quot; 
+                           &quot;the generation candidates&quot;)));
+    
+    declareMethod(rmm, &quot;smallEStep&quot;,&amp;TransformationLearner::smallEStep,
+                  (BodyDoc(&quot;Update of the generation set (expectation step) \n&quot;
+                           &quot;we update the weights of the generation candidates while keeping them fixed&quot;)));
+    
+    declareMethod(rmm, &quot;MStep&quot;, &amp;TransformationLearner::MStep,
+                  (BodyDoc(&quot;Updating the transformation parameters (maximization step)\n&quot;)));
+
+    declareMethod(rmm, &quot;largeEStepB&quot;, &amp;TransformationLearner::largeEStepB,
+                  (BodyDoc(&quot;Performs a large update of the&quot;)));
+ 
+    declareMethod(rmm, &quot;returnReproductionSources&quot;, &amp;TransformationLearner::returnReproductionSources,
+                  (BodyDoc(&quot;Returns the generation candidates associated to a specific target &quot;),
+                   ArgDoc (&quot;targetIdx&quot;, &quot;Index of the target data point in the training set&quot;),
+                   RetDoc (&quot;A vector of tuples (target index, neighbor index, transformation index, weight )&quot;)));
+
+    declareMethod(rmm, &quot;returnReproductions&quot;, &amp;TransformationLearner::returnReproductions,
+                  (BodyDoc(&quot;Computes the reproductions of the target from his generation candidates &quot;),
+                   ArgDoc(&quot;targetIdx&quot;,&quot;Index of the target data point in the training set&quot;),
+                   RetDoc(&quot;A matrix of data points (reproductions of the target)&quot;)));
+
+    declareMethod(rmm, &quot;returnTransform&quot;, &amp;TransformationLearner::returnTransform,
+                  (BodyDoc(&quot;Returns the parameters of a transformation&quot;),
+                   ArgDoc(&quot;transformIdx&quot;,&quot; Index of the transformation&quot;),
+                   RetDoc(&quot;a dXd matrix, d = dimension of input space&quot;)));
+    declareMethod(rmm, &quot;returnAllTransforms&quot;,&amp;TransformationLearner::returnAllTransforms,
+                  (BodyDoc(&quot;Returns all the transformation parameters&quot;),
+                   RetDoc(&quot;a kdXd matrix,  k = nb transformations \n&quot; 
+                          &quot;                d = dimension of input space&quot;)));
+    declareMethod(rmm, 
+                  &quot;returnGeneratedSamplesFrom&quot;, 
+                  &amp;TransformationLearner::returnGeneratedSamplesFrom,
+                  (BodyDoc(&quot;returns samples data point generated from\n&quot;
+                           &quot;a center data point&quot;),
+                   ArgDoc(&quot;Vec center, int n&quot;,&quot; center data point&quot;),
+                   ArgDoc(&quot;int n&quot;, &quot; number of sample data points to generate&quot;),
+                   RetDoc(&quot;a nXd matrix, the center is included in the dataset&quot;)));
+    declareMethod(rmm,
+                  &quot;returnGeneratedDataSet&quot;,
+                  &amp;TransformationLearner::returnGeneratedDataSet,
+                  (BodyDoc(&quot;returns a data set with respect to the current\n&quot;
+                           &quot;distribution paramaters\n&quot;
+                           &quot;We use a tree generation process (see createDataSet)\n&quot;
+                           &quot;i.e.: each new data point is used to generate a fix number of data points &quot;),
+                   ArgDoc(&quot;Vec root&quot;,&quot;initial data point&quot;),
+                   ArgDoc(&quot;int nbGenerations&quot;,&quot;deepness of the tree&quot;),
+                   ArgDoc(&quot;int GenerationLen&quot;,&quot;number of child for interior nodes&quot;),
+                   RetDoc(&quot;a nXd matrix, where n = number of nodes in the tree (root = part of the dataSet)&quot;)));
+    
+    declareMethod(rmm,
+                  &quot;returnSequentiallyGeneratedDataSet&quot;,
+                  &amp;TransformationLearner::returnSequentiallyGeneratedDataSet,
+                  (BodyDoc(&quot;returns a data set with respect to the current\n&quot;
+                           &quot;distribution parameters\n&quot;
+                           &quot;We use a sequential generation process\n&quot;
+                           &quot;i.e: each new data point is used to generate the next data point&quot;),
+                   ArgDoc(&quot;Vec root&quot;,&quot;initial data point&quot;),
+                   ArgDoc(&quot;int n&quot;,&quot;number of data points to generate&quot;),  
+                   RetDoc(&quot;a nXd matrix (the root is included in the data set)&quot;)));
+    
+    inherited::declareMethods(rmm);
+}
+
+
+
+//TO TEST
+//do the building operations related to the generation process
+//warning: we suppose the transformation parameters are set 
+void TransformationLearner::generatorInit(){
+    
+    inputSpaceDim = transformsSet.width();
+    
+
+}
+
+
+//TO TEST
+//do the building operations related to training
+//warning: we suppose the training set has been transmitted
+//         before calling the method
+void TransformationLearner::trainInit(){
+   
+    //DIMENSION VARIABLES
+    
+    //dimension of the input space
+    inputSpaceDim = train_set-&gt;inputsize();
+      
+    //number of samples given in the training set
+    nbTrainingInput = train_set-&gt;length();
+
+    
+    //number of generation candidates related to a specific target in the 
+    //generation set.   
+    nbGenerationCandidatesPerTarget = nbNeighbors * nbTransforms;
+
+   //total number of generation candidates in the generation set
+    nbGenerationCandidates = nbTrainingInput * nbGenerationCandidatesPerTarget;
+
+    
+    //LEARNED MODEL PARAMETERS
+    
+    //set of transformations (represented as a single matrix)
+    transformsSet = Mat(nbTransforms * inputSpaceDim, inputSpaceDim);
+    
+    //view on the set of transformations (vector)
+    //    each transformation = one matrix 
+    transforms.resize(nbTransforms);
+    for(int k = 0; k&lt; nbTransforms; k++){
+        transforms[k] = transformsSet.subMatRows(k * inputSpaceDim, inputSpaceDim);       
+    }
+    
+     //generation set and weights of the entries in the generation set
+    generationSet.resize(nbGenerationCandidates);
+
+    //OTHER VARIABLES
+    
+    //weight decay
+    lambda = noiseVariance/transformsVariance;
+   
+    
+
+    //factor used in the computation of the generation weights
+    noiseVarianceFactor = 1/(2*noiseVariance);
+    
+
+    //to store a view on the generation set 
+    //   (entries related to a specific target)
+    targetGenerationSet.resize(nbGenerationCandidatesPerTarget);
+
+    //Storage space used in the update of the transformation parameters
+    B_C = Mat(2 * nbTransforms * inputSpaceDim , inputSpaceDim);
+
+    B.resize(nbTransforms);
+    C.resize(nbTransforms);
+    for(int k=0; k&lt;nbTransforms; k++){
+        B[k]= B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
+    }
+    for(int k= nbTransforms ; k&lt;2*nbTransforms ; k++){
+        C[(k % nbTransforms)] = B_C.subMatRows(k*inputSpaceDim, inputSpaceDim);
+    }
+    
+    
+    target.resize(inputSpaceDim);
+    neighbor.resize(inputSpaceDim);
+
+}
+
+
+//TO TEST 
+void TransformationLearner::build_(){
+
+  
+    if(transformDistribution.length() == 0){
+        transformDistribution.resize(nbTransforms);
+        transformDistribution.fill(1.0/nbTransforms);
+    }
+    else{
+        PLASSERT(transformDistribution.length() == nbTransforms);
+        real sum =0;
+        for(int i=0; i&lt;nbTransforms; i++){
+            sum += transformDistribution[i];
+        }
+        PLASSERT(sum == 1);  
+    }
+    
+   
+
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+//TO TEST
+// ### Nothing to add here, simply calls build_
+void TransformationLearner::build()
+{
+
+    // pout &lt;&lt; &quot;build()&quot; &lt;&lt; endl;
+    inherited::build();
+    build_(); 
+}
+
+
+void TransformationLearner::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR(&quot;TransformationLearner::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+}
+
+/******** LEARNING MODULE ******************************************************/
+
+
+//TO DO
+int TransformationLearner::outputsize() const
+{
+    return 0;
+    // Compute and return the size of this learner's output (which typically
+    // may depend on its inputsize(), targetsize() and set options).
+}
+
+
+//TO DO
+void TransformationLearner::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+}
+
+//TO DO
+void TransformationLearner::train()
+{
+
+    trainInit();
+
+    // The role of the train method is to bring the learner up to
+    // stage==nstages, updating train_stats with training costs measured
+    // on-line in the process.
+
+    /* TYPICAL CODE:
+
+    static Vec input;  // static so we don't reallocate memory each time...
+    static Vec target; // (but be careful that static means shared!)
+    input.resize(inputsize());    // the train_set's inputsize()
+    target.resize(targetsize());  // the train_set's targetsize()
+    real weight;
+
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    if (!initTrain())
+        return;
+
+    while(stage&lt;nstages)
+    {
+        // clear statistics of previous epoch
+        train_stats-&gt;forget();
+
+        //... train for 1 stage, and update train_stats,
+        // using train_set-&gt;getExample(input, target, weight)
+        // and train_stats-&gt;update(train_costs)
+
+        ++stage;
+        train_stats-&gt;finalize(); // finalize statistics for this epoch
+    }
+    */
+}
+
+//TO DO
+void TransformationLearner::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    // Compute the output from the input.
+    // int nout = outputsize();
+    // output.resize(nout);
+    // ...
+}
+
+//TO DO
+void TransformationLearner::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+// Compute the costs from *already* computed output.
+// ...
+}
+
+//TO DO
+TVec&lt;string&gt; TransformationLearner::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+    // ..
+    
+    return TVec&lt;string&gt;();
+}
+
+//TO DO
+TVec&lt;string&gt; TransformationLearner::getTrainCostNames() const
+{
+    // Return the names of the objective costs that the train method computes
+    // and for which it updates the VecStatsCollector train_stats
+    // (these may or may not be exactly the same as what's returned by
+    // getTestCostNames).
+    // ...
+    return TVec&lt;string&gt;();
+}
+
+ /*********GENERATION MODULE *****************************************/
+
+    //The object is the representation of a learned distribution
+    //Are are methods to ensure the &quot;generative behavior&quot; of the object
+    //(Once the distribution is learned, we might be able to generate
+    // samples from it)
+
+
+
+// TO TEST
+//Chooses the transformation parameters using a 
+//normal distribution with mean 0 and variance &quot;transformsVariance&quot;
+//(call generatorBuild() after)
+void TransformationLearner::buildTransformationParametersNormal(){
+    transformsSet.resize(nbTransforms*inputSpaceDim, inputSpaceDim);
+    transforms.resize(nbTransforms);
+    for(int t=0; t&lt;nbTransforms ; t++){
+        random_gen-&gt;fill_random_normal(transforms[t], 0 , transformsStDev);
+    }
+}
+
+
+//TO TEST
+//set the transformation parameters to the specified values
+//(call generatorBuild() after)
+void TransformationLearner::setTransformationParameters(TVec&lt;Mat&gt; &amp; transforms_){
+     
+    PLASSERT(transforms_.length() == nbTransforms);
+    inputSpaceDim = transforms_[0].width();
+    
+    int nbRows = inputSpaceDim*nbTransforms;
+    transformsSet = Mat(nbRows,inputSpaceDim);
+    transforms.resize(nbTransforms);
+  
+    int rowIdx = 0;
+    for(int t=0; t&lt;nbTransforms; t++){
+        transformsSet.subMatRows(rowIdx,inputSpaceDim) &lt;&lt; transforms_[t];
+        transforms[t]= transformsSet.subMatRows(rowIdx,inputSpaceDim);
+        rowIdx += inputSpaceDim;
+    }
+}
+
+//TO TEST
+//creates a data set
+//
+//     Consists in building a tree of deepness d = &quot;nbGenerations&quot; and
+//     constant branch factor n = &quot;generationLength&quot;
+//
+//            0      1        2     ...         
+//  
+//            r - child1  - child1  ...       
+//                        - child2  ...
+//                            ...   ...
+//                        - childn  ...
+//
+//              - child2  - child1  ...
+//                        - child2  ...
+//                            ...   ...
+//                        - childn  ...
+//                     ...
+//             - childn   - child1  ...
+//                        - child2  ...
+//                            ...   ...
+//                        - childn  ... 
+//
+
+// all the childs are choosen following the same process:
+// 1) choose a transformation  
+// 2) apply the transformation to the parent
+// 3) add noise to the result 
+void TransformationLearner::createDataSet(Vec &amp; root,
+                                          int nbGenerations,
+                                          int generationLength,
+                                          Mat &amp; dataPoints){
+   
+    PLASSERT(root.length() == inputSpaceDim);
+ 
+    //we look at the length of the given matrix dataPoint ;.  
+    int nbDataPoints = int(pow(1.0*nbGenerations,1.0*generationLength)) + 1;
+    dataPoints.resize(nbDataPoints,inputSpaceDim);
+    
+    //root = first element in the matrix dataPoints
+    dataPoints(0) &lt;&lt; root;
+  
+    //generate the other data points 
+    int centerIdx=0 ;
+    for(int dataIdx=1; dataIdx &lt; nbDataPoints ; dataIdx+=generationLength){
+
+        Vec v = dataPoints(centerIdx);
+        Mat m = dataPoints.subMatRows(dataIdx, generationLength);
+        batchGenerateFrom(v,m); 
+        centerIdx ++ ;
+    }
+}
+
+
+//TO TEST
+//create a dataset using the same tree generation process as
+//createDataSet, except the number of child per parent is fixed to 1,
+//   root -&gt; 1st point -&gt; 2nd point ... -&gt; nth point 
+void TransformationLearner::createDataSetSequentially(Vec &amp; root,
+                                                      int n,
+                                                      Mat &amp; dataPoints){
+    createDataSet(root, n-1, 1, dataPoints);
+}
+
+
+//TO TEST
+//Select a transformation randomly (with respect ot our transformation
+//distribution)
+int TransformationLearner::pickTransformIdx(){
+     return random_gen-&gt;multinomial_sample(transformDistribution);
+}
+
+  
+//here is the generation process for a given center data point 
+//  1) choose a transformation
+//  2) apply it on the center data point
+//  3) add noise
+
+//TO TEST
+//generates a sample data point  from a  given center data point 
+void  TransformationLearner::generateFrom(Vec &amp; center, Vec &amp; sample){
+    int transformIdx = pickTransformIdx();
+    generateFrom(center, sample, transformIdx);
+}
+
+//TO TEST
+//generates a sample data point from a given center data point
+void TransformationLearner::generateFrom(Vec &amp; center,
+                                         Vec &amp; sample, 
+                                         int transformIdx){
+    int d = center.length();
+    PLASSERT(d == inputSpaceDim);
+    
+    sample.resize(inputSpaceDim);
+    
+    //apply the transformation
+    applyTransformationOn(transformIdx,center,sample);
+    
+    //add noise
+    for(int i=0; i&lt;d; i++){
+        sample[i] += random_gen-&gt;gaussian_mu_sigma(0, noiseStDev);
+    } 
+}
+
+//TO TEST
+//fill the matrix &quot;samples&quot; with sample data points obtained from
+// a given center data point.
+void TransformationLearner::batchGenerateFrom(Vec &amp; center, Mat &amp; samples){
+
+    PLASSERT(center.length() ==inputSpaceDim);
+    PLASSERT(samples.width()==inputSpaceDim);
+    int l = samples.length();
+    for(int i=0; i&lt;l; i++)
+    {
+        Vec v = samples(i);
+        generateFrom(center, v);
+    }
+}
+
+
+
+//-------------EXTERNAL ACCESS ---------------------------
+
+//TO TEST
+//Returns a copy of the generation candidates associated to a given target
+TVec&lt;GenerationCandidate&gt; TransformationLearner::returnReproductionSources
+(int targetIdx){
+    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
+    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
+    return generationSet.subVec(startIdx, endIdx).copy();
+}
+
+//TO TEST
+//Returns the parameters of a given transformation
+Mat TransformationLearner::returnTransform(int transformIdx){
+    return transforms[transformIdx].copy();   
+}
+
+//TO TEST
+//Returns all the transformation parameters
+Mat TransformationLearner::returnAllTransforms(){
+    return transformsSet;
+}
+
+
+//TO TEST
+//From the subset ofgeneration candidate associated to the target,
+//builds and returns the corresponding subset of generated data points . 
+Mat TransformationLearner::returnReproductions(int targetIdx){
+    Mat reproductions = Mat(nbGenerationCandidatesPerTarget,inputSpaceDim);
+    int candidateIdx = targetIdx*nbGenerationCandidatesPerTarget;
+    int neighborIdx, transformIdx;
+    for(int i=0; i&lt;nbGenerationCandidatesPerTarget; i++){
+        neighborIdx = generationSet[candidateIdx].neighborIdx;
+        transformIdx= generationSet[candidateIdx].transformIdx;
+        getNeighborFromTrainingSet(neighborIdx);
+        Vec v = reproductions(i);
+        applyTransformationOn(transformIdx, neighbor, v);
+        candidateIdx ++;
+    }
+    return reproductions;
+}
+
+
+//TO TEST
+//Generates n samples from center and returns them
+//    (generation process = 1) choose a transformation,
+//                          2) apply it on center
+//                          3) add noise)
+Mat TransformationLearner::returnGeneratedSamplesFrom(Vec center, int n){
+    int d = center.length();
+    PLASSERT(d == inputSpaceDim);
+    Mat m = Mat(n,d);
+    batchGenerateFrom(center, m);
+    return m;
+}
+
+//TO TEST
+//Generates a data set and returns it
+//(tree generation process: see createDataSet for more details)
+Mat TransformationLearner::returnGeneratedDataSet(Vec root,
+                                                  int nbGenerations,
+                                                  int generationLength){
+ 
+    int n = int(pow(1.0*nbGenerations, 1.0*generationLength)) + 1;
+    int d = root.length();
+    PLASSERT(d == inputSpaceDim);
+
+    Mat dataSet = Mat(n,d);
+    createDataSet(root,nbGenerations,generationLength,dataSet);
+    return dataSet;
+}
+
+//TO TEST
+//Generates a data set and returns it
+//(sequential generation process: see createDataSetSequentially for more details)
+Mat TransformationLearner::returnSequentiallyGeneratedDataSet(Vec root,int n){ 
+    return returnGeneratedDataSet(root, n-1,1);
+}
+
+
+// ----------GENERAL USE--------------------------------------------------
+
+
+
+//REFERENCE OPERATIONS ON GENERATION SET AND TRAINING SET  
+
+
+//TO TEST
+// stores a view on the subset of generation set related to the specified
+// target (into the variable &quot;targetGenerationSet&quot; )
+void TransformationLearner::getViewOnTargetGenerationCandidates(int targetIdx){
+    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
+    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
+    targetGenerationSet = generationSet.subVec(startIdx, 
+                                               endIdx);
+    
+}
+
+
+
+
+
+// stores the &quot;targetIdx&quot;th input in the training set into the variable
+// &quot;target&quot;
+void TransformationLearner::getTargetFromTrainingSet(int targetIdx){
+    Vec v;
+    real w;
+    train_set-&gt;getExample(targetIdx,target,v,w);
+
+    //TO TEST : OK
+}
+
+// stores the &quot;neighborIdx&quot;th input in the training set into the variable
+// &quot;neighbor&quot; 
+void TransformationLearner::getNeighborFromTrainingSet(int neighborIdx){
+    Vec v;
+    real w;
+    train_set-&gt;getExample(neighborIdx,neighbor,v,w);
+    
+    //TO TEST : OK
+}
+
+
+//OPERATIONS RELATED TO GENERATION WEIGHTS
+
+//normalizes the generation weights related to a given target. 
+void TransformationLearner::normalizeTargetGenerationWeights(int targetIdx, 
+                                                             real totalWeight){
+    real w;
+    int startIdx = targetIdx * nbGenerationCandidatesPerTarget;
+    int endIdx = startIdx + nbGenerationCandidatesPerTarget;
+    for(int candidateIdx =startIdx; candidateIdx&lt;endIdx; candidateIdx++){
+        w = generationSet[candidateIdx].weight;
+        generationSet[candidateIdx].weight =  DIV_weights(w,totalWeight);
+    }
+
+    //TO TEST : OK
+}
+    
+//returns a random positive weight 
+real TransformationLearner::randomPositiveGenerationWeight(){
+    return  random_gen-&gt;uniform_sample() + epsilonInitWeight;
+
+    //TO TEST : OK
+}
+  
+
+//arithmetic operations on  generation weights
+real TransformationLearner::DIV_weights(real numWeight,    //DIVISION
+                                        real denomWeight){ 
+    return numWeight/denomWeight;
+
+    //TO TEST : OK
+}
+real TransformationLearner::MULT_INVERSE_weight(real weight){//MULTIPLICATIVE INVERSE
+    return -weight;
+
+    //TO TEST : OK
+}
+real TransformationLearner::MULT_weights(real weight1, real weight2){ //MULTIPLICATION
+    return weight1*weight2;
+    
+    //TO TEST : OK
+}
+real TransformationLearner::SUM_weights(real weight1, real weight2){ //SUM
+    return weight1 + weight2;
+
+    //TO TEST : OK
+} 
+
+//TO TEST
+//update/compute the weight of a generation candidate with
+//the actual transformation parameters
+real TransformationLearner::updateGenerationWeight(int candidateIdx){
+    
+    GenerationCandidate * gc = &amp; generationSet[candidateIdx];
+    
+    real w = computeGenerationWeight(gc-&gt;targetIdx,
+                                     gc-&gt;neighborIdx,
+                                     gc-&gt;transformIdx);
+    pout &lt;&lt; &quot;weigth:&quot;&lt;&lt; w &lt;&lt;endl;
+    gc-&gt;weight = w;
+    return w;
+}
+
+//TO TEST
+real TransformationLearner::computeGenerationWeight(GenerationCandidate &amp; gc){
+    return computeGenerationWeight(gc.targetIdx,
+                                   gc.neighborIdx,
+                                   gc.transformIdx);
+
+   
+}
+
+//TO TEST
+real TransformationLearner::computeGenerationWeight(int targetIdx, 
+                                                    int neighborIdx, 
+                                                    int transformIdx){
+  
+
+    getTargetFromTrainingSet(targetIdx);
+    getNeighborFromTrainingSet(neighborIdx);
+    Vec predictedTarget ;
+    predictedTarget.resize(inputSpaceDim);
+    applyTransformationOn(transformIdx, neighbor, predictedTarget);
+    return exp(noiseVarianceFactor * powdistance(target, predictedTarget));
+
+    
+}
+
+
+//applies &quot;transformIdx&quot;th transformation on data point &quot;src&quot;
+void TransformationLearner::applyTransformationOn(int transformIdx, Vec &amp; src, Vec &amp;  dst){
+    if(transformFamily == TRANSFORM_FAMILY_LINEAR){
+        Mat m  = transforms[transformIdx];
+        product(dst,m,src);
+    }
+    else{
+        Mat m = transforms[transformIdx];
+        product(dst, m, src);
+        dst += src;
+    }
+}
+
+//-------- INITIAL E STEP --------------------------------------------
+
+//initialization of the generation set 
+void TransformationLearner::initEStep(){
+    priority_queue&lt; pair&lt; real,int &gt; &gt; pq = priority_queue&lt; pair&lt; real,int &gt; &gt;();
+    
+    real totalWeight;
+    int candidateIdx=0,targetStartIdx, neighborIdx;
+    
+    //for each point in the training set i.e. for each target point,
+    for(int targetIdx = 0; targetIdx &lt; nbTrainingInput ;targetIdx++){
+    
+        //finds the nearest neighbors and keep them in a priority queue 
+        findNearestNeighbors(targetIdx, pq);
+        
+        //expands those neighbors in the dataset:
+        //(i.e. for each neighbor, creates one entry per transformation and 
+        //assignsit a positive random weight)
+        
+        totalWeight =0;
+        targetStartIdx = candidateIdx;
+        for(int k = 0; k &lt; nbNeighbors; k++){
+            neighborIdx = pq.top().second;
+            pq.pop();
+            totalWeight =SUM_weights(expandTargetNeighborPairInGenerationSet(targetIdx, 
+                                                                             neighborIdx,
+                                                                             candidateIdx),
+                                     totalWeight);
+            candidateIdx += nbTransforms;
+        }
+        //normalizes the  weights of all the entries created for the target 
+        //point
+        normalizeTargetGenerationWeights(targetIdx,totalWeight);
+    }
+
+    //TO TEST
+}
+    
+//auxialiary function of &quot;initEStep&quot; . 
+//    for a given pair (target, neighbor), creates all the associated 
+//    generation candidates (entries) in the data set. 
+//returns the total weight of the generation candidates created
+real TransformationLearner::expandTargetNeighborPairInGenerationSet(int targetIdx,
+                                                                    int neighborIdx,
+                                                                    int candidateIdx){
+    real weight, totalWeight = 0;  
+    for(int transformIdx=0; transformIdx&lt;nbTransforms; transformIdx ++){
+        //choose a random positive weight
+        weight = randomPositiveGenerationWeight(); 
+        totalWeight = SUM_weights(weight,totalWeight);
+        generationSet[candidateIdx] = GenerationCandidate(targetIdx, 
+                                                          neighborIdx,
+                                                          transformIdx,
+                                                          weight);
+    
+        candidateIdx ++;
+    }
+    return totalWeight;    
+
+    //TO TEST
+}
+
+//auxiliary function of initEStep
+//    keeps the nearest neighbors for a given target point in a priority
+//    queue.
+void TransformationLearner::findNearestNeighbors (int targetIdx,
+                                                  priority_queue&lt; pair&lt; real, int &gt; &gt; &amp; pq){
+    
+    //we want an empty queue
+    PLASSERT(pq.empty()); 
+  
+    //capture the target from his index in the training set
+    getTargetFromTrainingSet(targetIdx);
+     
+    //for each potential neighbor,
+    real dist;    
+    for(int i=0; i&lt;nbTrainingInput; i++){
+        if(i != targetIdx){ //(the target cannot be his own neighbor)
+            //computes the distance to the target
+            getNeighborFromTrainingSet(i);
+            dist = powdistance(target, neighbor); 
+            //if the distance is among &quot;nbNeighbors&quot; smallest distances seen,
+            //keep it until to see a closer neighbor. 
+            if(int(pq.size()) &lt; nbNeighbors){
+                pq.push(pair&lt;real,int&gt;(dist,i));
+            }
+            else if (dist &lt; pq.top().first){
+                pq.pop();
+                pq.push(pair&lt;real,int&gt;(dist,i));
+            }
+        }
+    }    
+
+    //TO TEST
+}
+
+
+//-------- LARGE E STEP : VERSION A --------------------------------
+
+//full update of the generation set
+//for each target, keeps the top km most probable &lt;neighbor, transformation&gt; 
+//pairs (k = nb neighbors, m= nb transformations)
+void TransformationLearner::largeEStepA(){
+    
+    priority_queue&lt; GenerationCandidate &gt; pq =  priority_queue&lt; GenerationCandidate &gt;();
+    real totalWeight=0;
+    int candidateIdx=0;
+    
+    //for each point in the training set i.e. for each target point,
+    for(int targetIdx = 0; targetIdx &lt; nbTrainingInput ; targetIdx++){
+        
+        //finds the best weighted triples and keep them in a priority queue 
+        findBestTargetCandidates(targetIdx, pq);
+        
+        //store those triples in the dataset:
+        totalWeight = 0;
+        for(int k=0; k &lt; nbGenerationCandidatesPerTarget; k++){
+            generationSet[candidateIdx] = pq.top(); 
+            totalWeight = SUM_weights(pq.top().weight, totalWeight);
+            pq.pop();         
+            candidateIdx ++;
+        }
+        
+        //normalizes the  weights of all the entries created for the 
+        //target point;
+        normalizeTargetGenerationWeights(targetIdx,totalWeight);
+    }
+
+    //TO TEST
+}
+
+//auxiliary function of largeEStepA()
+//   for a given target, keeps the top km most probable neighbors,
+//   transformation pairs in a priority queue 
+//   (k = nb neighbors, m = nb transformations)
+void  TransformationLearner::findBestTargetCandidates
+(int targetIdx,
+ priority_queue&lt; GenerationCandidate &gt; &amp; pq){
+    
+    //we want an empty queue
+    PLASSERT(pq.empty()); 
+    real weight;
+
+    //for each potential neighbor
+    for(int neighborIdx=0; neighborIdx&lt;nbTrainingInput; neighborIdx++){
+        if(neighborIdx != targetIdx){
+            for(int transformIdx=0; transformIdx&lt;nbTransforms; transformIdx++){
+                weight = computeGenerationWeight(targetIdx, 
+                                                 neighborIdx, 
+                                                 transformIdx);
+                
+                //if the weight is among &quot;nbEntries&quot; biggest weight seen,
+                //keep it until to see a bigger neighbor. 
+                if(int(pq.size()) &lt; nbGenerationCandidatesPerTarget){
+                    pq.push(GenerationCandidate(targetIdx,
+                                                neighborIdx,
+                                                transformIdx,
+                                                weight));  
+                }
+                else if (weight &gt; pq.top().weight){
+                    pq.pop();
+                    pq.push(GenerationCandidate(targetIdx,
+                                                neighborIdx,
+                                                transformIdx,
+                                                weight));
+                }
+            }
+        }     
+    }
+
+    //TO TEST
+}
+
+
+//-------- LARGE E STEP : VERSION B --------------------------------
+
+//full update of the generation set
+//   for each given pair (target, transformation), find the best
+//   weighted neighbors  
+void  TransformationLearner::largeEStepB(){
+    priority_queue&lt; GenerationCandidate &gt; pq;
+    
+  real totalWeight=0 , weight;
+  int candidateIdx=0 ;
+  
+  //for each point in the training set i.e. for each target point,
+  for(int targetIdx =0; targetIdx&lt;nbTrainingInput ;targetIdx++){
+  
+      totalWeight = 0;
+      for(int transformIdx=0; transformIdx &lt; nbTransforms; transformIdx ++){
+          //finds the best weighted triples   them in a priority queue 
+          findBestWeightedNeighbors(targetIdx,transformIdx, pq);
+          
+          //store those neighbors in the dataset
+          for(int k=0; k&lt;nbNeighbors; k++){
+              generationSet[candidateIdx] = pq.top();
+              weight = pq.top().weight;
+              totalWeight = SUM_weights( weight, totalWeight);
+              pq.pop();
+              candidateIdx ++;
+          }
+      }
+      //normalizes the  weights of all the entries created for the target 
+      //point;
+      normalizeTargetGenerationWeights(targetIdx,totalWeight);
+  }
+
+  // TO TEST
+}
+
+    
+//auxiliary function of largeEStepB()
+//   for a given target x and a given transformationt , keeps the best
+//   weighted triples (x, neighbor, t) in a priority queue .
+void  TransformationLearner::findBestWeightedNeighbors
+(int targetIdx,
+ int transformIdx,
+ priority_queue&lt; GenerationCandidate &gt; &amp; pq){
+ 
+    //we want an empty queue
+    PLASSERT(pq.empty()); 
+
+    real weight; 
+    
+    //for each potential neighbor
+    for(int neighborIdx=0; neighborIdx&lt;nbTrainingInput; neighborIdx++){
+        if(neighborIdx != targetIdx){ //(the target cannot be his own neighbor)
+            
+	  weight=  computeGenerationWeight(targetIdx, 
+                                           neighborIdx, 
+                                           transformIdx);
+	  //if the weight of the triple is among the &quot;nbNeighbors&quot; biggest 
+	  //seen,keep it until see a bigger weight. 
+            if(int(pq.size()) &lt; nbNeighbors){
+                pq.push(GenerationCandidate(targetIdx,
+                                            neighborIdx, 
+                                            transformIdx,
+                                            weight));
+            }
+            else if (weight &gt;  pq.top().weight){
+                pq.pop();
+                pq.push(GenerationCandidate(targetIdx,
+                                            neighborIdx,
+                                            transformIdx,
+                                            weight));
+            }
+        }
+    } 
+
+    //TO TEST
+}
+
+
+//-------- SMALL E STEP --------------------------------------------- 
+
+
+//updating the weights while keeping the candidate neighbor set fixed
+void TransformationLearner::smallEStep(){
+    
+    int candidateIdx =0, startCandidateIdx=0;
+    int startTargetIdx = generationSet[startCandidateIdx].targetIdx;
+    int  targetIdx;
+    real totalWeight = 0;
+  
+    while(candidateIdx &lt; nbGenerationCandidates){
+    
+        totalWeight = SUM_weights(updateGenerationWeight(candidateIdx),
+                                  totalWeight);
+        candidateIdx ++;
+    
+        targetIdx = generationSet[candidateIdx].targetIdx; 
+    
+        if(candidateIdx &gt; nbGenerationCandidates || targetIdx != startTargetIdx){
+            normalizeTargetGenerationWeights(startTargetIdx, totalWeight);
+            totalWeight = 0;
+            startTargetIdx = targetIdx;
+        }
+    }    
+
+    //TO TEST
+}
+    
+
+//-------- M STEP ---------------------------------------------   
+    
+
+//updating the transformation parameters
+void TransformationLearner::MStep(){
+    //set the m dXd matrices Ck and Bk , k in{1, ...,m} to 0.
+    B_C.clear();
+  
+    for(int idx=0 ; idx&lt;nbGenerationCandidates ; idx++){
+    
+        //catch a view on the next entry of our dataset, that is, a  triple:
+        //(target_idx, neighbor_idx, transformation_idx)
+        GenerationCandidate * gc = &amp;generationSet[idx];
+        
+        real w = gc-&gt;weight;
+  
+        //catch the target and neighbor points from the training set
+        getTargetFromTrainingSet(gc-&gt;targetIdx);
+        getNeighborFromTrainingSet(gc-&gt;neighborIdx);
+        
+        int t = gc-&gt;transformIdx;
+        
+        externalProductScaleAcc(C[t], target, target, w);
+        if(transformFamily == TRANSFORM_FAMILY_LINEAR){
+            externalProductScaleAcc(B[t], target, neighbor,w);
+        }
+        else
+            externalProductScaleAcc(B[t], target, (target - neighbor), w); 
+    }
+    for(int t=0; t&lt;nbTransforms; t++){
+        addToDiagonal(C[t],lambda);
+        transforms[t] &lt;&lt; solveLinearSystem(C[t], B[t]);
+    }
+
+    //TO TEST
+
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-05-17 18:10:51 UTC (rev 7152)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-05-17 18:29:16 UTC (rev 7153)
@@ -0,0 +1,596 @@
+// -*- C++ -*-
+
+// TransformationLearner.h
+//
+//version 5 
+//
+// Copyright (C) 2007 Lysiane Bouchard
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Lysiane Bouchard
+
+/*! \file TransformationLearner.h */
+
+
+#ifndef TransformationLearner_INC
+#define TransformationLearner_INC
+
+//C++
+#include &lt;utility&gt;
+#include &lt;queue&gt;
+
+//Plearn
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &lt;plearn/math/PRandom.h&gt;
+#include &lt;plearn/base/tuple.h&gt;
+
+
+#define TRANSFORM_FAMILY_LINEAR 0
+#define TRANSFORM_FAMILY_LINEAR_INCREMENT 1
+
+
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+
+/***** GENERATION CANDIDATE ***********************************************/
+
+
+//Generate Candidate objects are basically 4-tuples with the following format: 
+//         nKm x4 matrix 
+//    ------C1----------|---- C2------------|-- C3-----------|-- C4-----------|
+//    index i  in the   | index j in the    | index t of a   | positive weight
+//    training set of a | training set of a | transformation | 
+//    target point      | a neighbour       |                |
+//                      | candidate         |                | 
+
+
+class GenerationCandidate
+{
+public:
+    int targetIdx, neighborIdx, transformIdx;
+    real weight;
+
+    GenerationCandidate(int targetIdx_=-1, int neighborIdx_=-1, int transformIdx_=-1, real weight_=0){
+        targetIdx =  targetIdx_;
+        neighborIdx =  neighborIdx_;
+        transformIdx =  transformIdx_ ;
+        weight =  weight_;            
+    }
+};
+inline bool operator&lt;(const GenerationCandidate&amp; o1 , const GenerationCandidate&amp; o2)
+{
+    // we need the inverse comparison for the priority queue
+    return o2.weight&gt;o1.weight;
+}
+
+inline bool operator==(const GenerationCandidate&amp; o1, const GenerationCandidate&amp; o2)
+{
+        return o1.weight==o2.weight;
+}
+
+
+
+inline PStream&amp; operator&lt;&lt;(PStream&amp; out, const GenerationCandidate&amp; x)
+    {
+        out &lt;&lt; tuple&lt;int, int, int, real&gt;(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight);
+        return out;
+    }
+
+inline PStream&amp; operator&gt;&gt;(PStream&amp; in, GenerationCandidate&amp; x)
+    {
+        tuple&lt;int, int, int, real&gt; t;
+        in &gt;&gt; t;
+        tie(x.targetIdx, x.neighborIdx, x.transformIdx, x.weight) = t;
+        return in;
+    }
+
+/********* END , GENERATION CANDIDATE *************************************/
+
+
+class TransformationLearner : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! for random generators
+    long seed; 
+    
+    //! set the family of transformation functions we are interested in
+    int transformFamily ;
+    
+    //HYPER-PARAMETERS OF THE ALGORITHM
+    
+    //!variance of the NOISE, considered here as a random variable normaly 
+    //!distributed, with mean 0
+    real noiseVariance;
+    
+    //!variance on the transformation parameters. (prior distribution = normal with
+    //!mean 0).
+    real transformsVariance;
+
+    //number of transformations
+    int nbTransforms;
+    
+    //number of neighbors
+    int nbNeighbors;
+
+    //minimum random weight to give to a  chosen generation candidate at
+    //initialization step
+    real epsilonInitWeight;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    TransformationLearner();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+    //                                    Vec&amp; output, Vec&amp; costs) const;
+    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
+    //                               Vec&amp; costs) const;
+    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    /*********GENERATION MODULE *****************************************/
+
+    //The object is the representation of a learned distribution
+    //Are are methods to ensure the &quot;generative behavior&quot; of the object
+    //(Once the distribution is learned, we might be able to generate
+    // samples from it)
+
+    //Chooses the transformation parameters using a 
+    //normal distribution with mean 0 and variance &quot;transformsVariance&quot;
+    //(call generatorBuild() after)
+    void buildTransformationParametersNormal();
+   
+    //set the transformation parameters to the specified values
+    //(call generatorBuild() after)
+    void setTransformationParameters(TVec&lt;Mat&gt; &amp; transforms);
+
+    //creates a data set
+    //
+    //     Consists in building a tree of deepness d = &quot;nbGenerations&quot; and
+    //     constant branch factor n = &quot;generationLength&quot;
+    //
+    //            0      1        2     ...         
+    //  
+    //            r - child1  - child1  ...       
+    //                        - child2  ...
+    //                            ...   ...
+    //                        - childn  ...
+    //
+    //              - child2  - child1  ...
+    //                        - child2  ...
+    //                            ...   ...
+    //                        - childn  ...
+    //                     ...
+    //             - childn   - child1  ...
+    //                        - child2  ...
+    //                            ...   ...
+    //                        - childn  ... 
+    //
+
+   // all the childs are choosen following the same process:
+   // 1) choose a transformation  
+   // 2) apply the transformation to the parent
+   // 3) add noise to the result 
+    void createDataSet(Vec &amp; root,
+                       int nbGenerations,
+                       int generationLength,
+                       Mat &amp; dataPoints);
+    
+    //create a dataset using the same tree generation process as
+    //createDataSet, except the number of child per parent is fixed to 1,
+    //   root -&gt; 1st point -&gt; 2nd point ... -&gt; nth point 
+    void createDataSetSequentially(Vec &amp; root,
+                                   int n,
+                                   Mat &amp; dataPoints);
+
+    //Select a transformation randomly (with respect ot our transformation
+    //distribution)
+    int pickTransformIdx();
+    
+    
+    //here is the generation process for a given center data point 
+    //  1) choose a transformation
+    //  2) apply it on the center data point
+    //  3) add noise
+
+    //generates a sample data point  from a  given center data point 
+    void generateFrom(Vec &amp; center, Vec &amp; sample);
+    //generates a sample data point from a given center data point
+    void generateFrom(Vec &amp; center, Vec &amp; sample, int transformIdx);
+    //fill the matrix &quot;samples&quot; with sample data points obtained from
+    // a given center data point.
+    void batchGenerateFrom( Vec &amp; center, Mat &amp; samples); 
+  
+   
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(TransformationLearner);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+
+    //---EXTERIOR ACCESS ON LEARNED VARIABLES -------------------------------
+    
+    //returns all the entries in the generation set with the 
+    //TARGET_IDX field fixed to &quot;targetIdx&quot;
+    //those entries represents ways to reproduct the 
+    //&quot;targetIdx&quot;th data point in the training set
+    TVec&lt;GenerationCandidate&gt; returnReproductionSources(int targetIdx); 
+  
+    //returns the reproductions of the &quot;targetIdx&quot;th data point in the
+    //training set
+    //(one reproduction by reproduction source)
+    Mat returnReproductions(int targetIdx);
+
+    //returns the parameter of the &quot;transformIdx&quot;th transformation
+    Mat returnTransform(int transformIdx);
+
+    
+    //returns the paramter of each transformation
+    //(as an tdXd matrix, t = number of transformation,
+    //                    d = dimension of input space)
+    Mat returnAllTransforms();
+    
+    //Generates n samples from center and returns them
+    //    (generation process = 1) choose a transformation,
+    //                          2) apply it on center
+    //                          3) add noise)
+    Mat returnGeneratedSamplesFrom(Vec center, int n);
+    
+    //Creates a data set and returns it
+    //(see createDataSet for more details on the generation process)
+    Mat returnGeneratedDataSet(Vec root,
+                               int nbGenerations,
+                               int generationLength);
+
+    //Generates a data set and returns it
+    //(sequential generation process: see createDataSetSequentially for more details)
+    Mat returnSequentiallyGeneratedDataSet(Vec root,int n);
+
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+public:    
+    
+    //DIMENSION VARIABLES 
+  
+    //dimension of the input space;
+    int inputSpaceDim;
+    
+    //number of generation candidates related to a specific target in the 
+    //generation set. 
+    int nbGenerationCandidatesPerTarget;
+
+    //total number of generation candidates in the generation set
+    int nbGenerationCandidates;
+
+    //number of samples given in the training set
+    int nbTrainingInput;
+
+    //multinomial probability ditribution for the transformations
+    //(i.e. probability of kth transformation = transformDistribution[k])
+    Vec transformDistribution;
+    
+
+    //LEARNED MODEL PARAMETERS
+
+    //set of transformations:
+    //mdxd matrix :  -where m = number of transformation,
+    //                      d = dimensionality of the input space
+    //               -rows kd to kd + d (exclusively) = sub-matrix = parameters of the
+    //                                                               kth transformation
+    //                                                               (0&lt;=k&lt;m)
+    Mat transformsSet ; 
+    TVec&lt; Mat &gt; transforms; //views on sub-matrices of the matrix transformsSet 
+    
+    //a generationSet D : 
+    //implemented as a vector of &quot;GenerationCandidate&quot; objects.
+    TVec&lt; GenerationCandidate &gt; generationSet; 
+
+
+    //OTHER VARIABLES
+
+    //the weight decay
+    real lambda;
+    
+
+    //factor used in the computation of generation weights :
+    // 1/2(noiseVariance)y
+    real noiseVarianceFactor;
+
+    //standard deviation for the noise distribution, and transformation
+    //parameters distributions:
+    real noiseStDev,transformsStDev;
+
+    //will be used to store a view on the generation set:
+    //that is, all the entries related to a specific target . 
+    TVec&lt;GenerationCandidate&gt;  targetGenerationSet; 
+    
+    //Storage space that will be used to update the transformation
+    //parameters. It represents a set of sub-matrices. There are exactly 2 
+    //sub-matrices by transformation.   
+    Mat B_C ;
+
+    //Vectors of matrices that will be used to update the transformation 
+    //parameters. Each matrix is a view on a sub-matrix in B_C. 
+    TVec&lt;Mat&gt; C , B ;
+
+    //to retrieve easily an input point from the training set 
+    Vec target, neighbor;
+
+    
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList&amp; ol);
+
+
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap&amp; rmm);
+
+    //general building operations 
+    void build_();
+    
+    //do the building operations related to training
+    //warning: we suppose the training set has been transmitted
+    //         before calling the method
+    void trainInit();
+    //do the building operations related to the generation process
+    //warning: we suppose the transformation parameters are set 
+    void generatorInit();
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+ 
+
+public:
+
+    // ----------GENERAL USE--------------------------------------------------
+    
+
+    //REFERENCE OPERATIONS ON GENERATION SET AND TRAINING SET  
+
+    // stores a view on the subset of generation set related to the specified
+    // target (into the variable &quot;targetGenerationSet&quot; )
+    void getViewOnTargetGenerationCandidates(int targetIdx);
+    // stores the &quot;targetIdx&quot;th input in the training set into the variable
+    // &quot;target&quot;
+    void getTargetFromTrainingSet(int targetIdx);
+    // stores the &quot;neighborIdx&quot;th input in the training set into the variable
+    // &quot;neighbor&quot; 
+    void getNeighborFromTrainingSet(int neighborIdx);
+    
+    
+    //OPERATIONS RELATED TO GENERATION WEIGHTS
+    
+    //normalizes the generation weights related to a given target. 
+    void normalizeTargetGenerationWeights(int targetIdx, real totalWeight);
+    
+    //returns a random positive weight 
+    real randomPositiveGenerationWeight();
+  
+    //arithmetic operations on  generation weights
+    real DIV_weights(real numWeight, real denomWeight); //DIVISION
+    real MULT_INVERSE_weight(real weight);//MULTIPLICATIVE INVERSE
+    real MULT_weights(real weight1, real weight2); //MULTIPLICATION
+    real SUM_weights(real weight1, real weight2); //SUM 
+    
+    //update/compute the weight of a generation candidate with
+    //the actual transformation parameters
+    real updateGenerationWeight(int candidateIdx);
+    real computeGenerationWeight(GenerationCandidate &amp; gc);
+    real computeGenerationWeight(int targetIdx, 
+                                 int neighborIdx, 
+                                 int transformIdx);
+    
+    //applies &quot;transformIdx&quot;th transformation on data point &quot;src&quot;
+    void applyTransformationOn(int transformIdx, Vec &amp; src , Vec &amp; dst);
+  
+
+   //-------- INITIAL E STEP --------------------------------------------
+
+    //initialization of the generation set 
+    void initEStep();
+    
+    //auxialiary function of &quot;initEStep&quot; . 
+    //    for a given pair (target, neighbor), creates all the associated 
+    //    generation candidates (entries) in the data set. 
+    //returns the total weight of the generation candidates created
+    real expandTargetNeighborPairInGenerationSet(int targetIdx,
+                                                 int neighborIdx,
+                                                 int candidateStartIdx);
+    
+    //auxiliary function of initEStep
+    //    keeps the nearest neighbors for a given target point in a priority
+    //    queue.
+    void findNearestNeighbors(int targetIdx,
+                              priority_queue&lt; pair&lt; real, int &gt; &gt; &amp; pq);
+    
+
+    //-------- LARGE E STEP : VERSION A --------------------------------
+
+    //full update of the generation set
+    //for each target, keeps the top km most probable &lt;neighbor, transformation&gt; 
+    //pairs (k = nb neighbors, m= nb transformations)
+    void largeEStepA();
+
+    //auxiliary function of largeEStepA()
+    //   for a given target, keeps the top km most probable neighbors,
+    //   transformation pairs in a priority queue 
+    //   (k = nb neighbors, m = nb transformations)
+    void findBestTargetCandidates
+    (int targetIdx,
+     priority_queue&lt; GenerationCandidate &gt; &amp; pq);
+    
+
+    //-------- LARGE E STEP : VERSION B --------------------------------
+
+    //full update of the generation set
+    //   for each given pair (target, transformation), find the best
+    //   weighted neighbors  
+    void largeEStepB();
+
+    
+    //auxiliary function of largeEStepB()
+    //   for a given target x and a given transformationt , keeps the best
+    //   weighted triples (x, neighbor, t) in a priority queue .
+    void findBestWeightedNeighbors
+    (int targetIdx,
+     int transformIdx,
+     priority_queue&lt; GenerationCandidate &gt; &amp; pq);
+
+
+
+    //-------- SMALL E STEP --------------------------------------------- 
+
+    
+    //updating the weights while keeping the candidate neighbor set fixed
+    void smallEStep();
+    
+
+    //-------- M STEP ---------------------------------------------   
+    
+
+    //updating the transformation parameters
+    void MStep();
+
+
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(TransformationLearner);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000601.html">[Plearn-commits] r7152 - trunk/python_modules/plearn/parallel
</A></li>
	<LI>Next message: <A HREF="000603.html">[Plearn-commits] r7154 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#602">[ date ]</a>
              <a href="thread.html#602">[ thread ]</a>
              <a href="subject.html#602">[ subject ]</a>
              <a href="author.html#602">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
