<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7151 - in trunk/plearn_learners/online: .	EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7151%20-%20in%20trunk/plearn_learners/online%3A%20.%0A%09EXPERIMENTAL&In-Reply-To=%3C200705171800.l4HI0HKh007638%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000599.html">
   <LINK REL="Next"  HREF="000601.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7151 - in trunk/plearn_learners/online: .	EXPERIMENTAL</H1>
    <B>tihocan at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7151%20-%20in%20trunk/plearn_learners/online%3A%20.%0A%09EXPERIMENTAL&In-Reply-To=%3C200705171800.l4HI0HKh007638%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7151 - in trunk/plearn_learners/online: .	EXPERIMENTAL">tihocan at mail.berlios.de
       </A><BR>
    <I>Thu May 17 20:00:17 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000599.html">[Plearn-commits] r7150 - trunk/plearn_learners/online/EXPERIMENTAL
</A></li>
        <LI>Next message: <A HREF="000601.html">[Plearn-commits] r7152 - trunk/python_modules/plearn/parallel
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#600">[ date ]</a>
              <a href="thread.html#600">[ thread ]</a>
              <a href="subject.html#600">[ subject ]</a>
              <a href="author.html#600">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: tihocan
Date: 2007-05-17 20:00:16 +0200 (Thu, 17 May 2007)
New Revision: 7151

Added:
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/ModuleLearner.h
Removed:
   trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc
   trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h
Log:
Moving LearningNetwork to the main repository, and renaming it as ModuleLearner at the same time, since there is no notion of network in it anymore

Deleted: trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc	2007-05-17 17:57:01 UTC (rev 7150)
+++ trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc	2007-05-17 18:00:16 UTC (rev 7151)
@@ -1,393 +0,0 @@
-// -*- C++ -*-
-
-// LearningNetwork.cc
-//
-// Copyright (C) 2007 Olivier Delalleau
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Olivier Delalleau
-
-/*! \file LearningNetwork.cc */
-
-
-#define PL_LOG_MODULE_NAME &quot;LearningNetwork&quot;
-#include &lt;plearn/io/pl_log.h&gt;
-
-#include &quot;LearningNetwork.h&quot;
-#include &lt;plearn_learners/online/NullModule.h&gt;
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    LearningNetwork,
-    &quot;Flexible network structure that can be optimized to learn some task(s).&quot;,
-    &quot;This network is made of several blocks, called 'modules' (deriving from\n&quot;
-    &quot;the OnlineLearningModule class), connected together so as to be able to\n&quot;
-    &quot;propagate information through the network.\n&quot;
-    &quot;Typically, during training, (input, target) pairs are fed to the\n&quot;
-    &quot;network and a cost is optimized. The trained network can later be used\n&quot;
-    &quot;to make predictions on new test points.\n&quot;
-);
-
-/////////////////////
-// LearningNetwork //
-/////////////////////
-LearningNetwork::LearningNetwork():
-    batch_size(1),
-    mbatch_size(-1)
-{
-    random_gen = new PRandom();
-}
-
-////////////////////
-// declareOptions //
-////////////////////
-void LearningNetwork::declareOptions(OptionList&amp; ol)
-{
-    declareOption(ol, &quot;module&quot;, &amp;LearningNetwork::module,
-                  OptionBase::buildoption,
-       &quot;The module being optimized. This module should typically have some\n&quot;
-       &quot;ports named 'input', 'target', 'weight', 'output' and 'cost'.&quot;);
-
-    declareOption(ol, &quot;batch_size&quot;, &amp;LearningNetwork::batch_size,
-                  OptionBase::buildoption,
-       &quot;Number of samples fed to the network at each iteration of learning.\n&quot;
-       &quot;Use '0' for full batch learning.&quot;);
-
-    declareOption(ol, &quot;mbatch_size&quot;, &amp;LearningNetwork::mbatch_size,
-                  OptionBase::learntoption,
-       &quot;Same as 'batch_size', except when 'batch_size' is set to 0, this\n&quot;
-       &quot;option takes the value of the size of the training set.&quot;);
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-////////////
-// build_ //
-////////////
-void LearningNetwork::build_()
-{
-    if (!module)
-        // Cannot do anything without an underlying module.
-        return;
-
-    // Forward random number generator to underlying module.
-    if (!module-&gt;random_gen) {
-        module-&gt;random_gen = random_gen;
-        // Currently we call forget, since it ensures the module is
-        // correctly initialized and also will propagate the random number
-        // generator to its own sub-modules. However, this is not very
-        // intuitive, and a better solution may be found.
-        module-&gt;forget();
-    }
-
-    // Create a new NetworkModule that connects the ports of the underlying
-    // module to simple MatrixModules that will provide/store data.
-    const TVec&lt;string&gt;&amp; ports = module-&gt;getPorts();
-    TVec&lt; PP&lt;OnlineLearningModule&gt; &gt; all_modules;
-    all_modules.append(module);
-    TVec&lt; PP&lt;NetworkConnection&gt; &gt; all_connections;
-
-    if (ports.find(&quot;input&quot;) &gt;= 0) {
-        store_inputs = new MatrixModule(&quot;store_inputs&quot;, true);
-        all_modules.append(get_pointer(store_inputs));
-        all_connections.append(new NetworkConnection(
-                    get_pointer(store_inputs), &quot;data&quot;,
-                    module, &quot;input&quot;, false));
-    } else
-        store_inputs = NULL;
-
-    if (ports.find(&quot;target&quot;) &gt;= 0) {
-        store_targets = new MatrixModule(&quot;store_targets&quot;, true);
-        all_modules.append(get_pointer(store_targets));
-        all_connections.append(new NetworkConnection(
-                    get_pointer(store_targets), &quot;data&quot;,
-                    module, &quot;target&quot;, false));
-    } else
-        store_targets = NULL;
-
-    if (ports.find(&quot;weight&quot;) &gt;= 0) {
-        store_weights = new MatrixModule(&quot;store_weights&quot;, true);
-        all_modules.append(get_pointer(store_weights));
-        all_connections.append(new NetworkConnection(
-                    get_pointer(store_weights), &quot;data&quot;,
-                    module, &quot;weight&quot;, false));
-    } else
-        store_weights = NULL;
-
-    if (ports.find(&quot;output&quot;) &gt;= 0) {
-        store_outputs = new MatrixModule(&quot;store_outputs&quot;, true);
-        all_modules.append(get_pointer(store_outputs));
-        all_connections.append(new NetworkConnection(
-                    module, &quot;output&quot;,
-                    get_pointer(store_outputs), &quot;data&quot;, false));
-    } else
-        store_outputs = NULL;
-
-    if (ports.find(&quot;cost&quot;) &gt;= 0) {
-        store_costs = new MatrixModule(&quot;store_costs&quot;, true);
-        all_modules.append(get_pointer(store_costs));
-        // Note that this is the only connection that propagates the gradient.
-        all_connections.append(new NetworkConnection(
-                    module, &quot;cost&quot;,
-                    get_pointer(store_costs), &quot;data&quot;, true));
-    } else
-        store_costs = NULL;
-
-    network = new NetworkModule();
-    network-&gt;modules = all_modules;
-    network-&gt;connections = all_connections;
-    network-&gt;build();
-
-    // Initialize the list of null pointers to provided for forward and
-    // backward propagation.
-    null_pointers.resize(module-&gt;nPorts());
-    null_pointers.fill(NULL);
-}
-
-///////////
-// build //
-///////////
-void LearningNetwork::build()
-{
-    inherited::build();
-    build_();
-}
-
-/////////////////////////////////
-// makeDeepCopyFromShallowCopy //
-/////////////////////////////////
-void LearningNetwork::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR(&quot;LearningNetwork::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
-}
-
-////////////////
-// outputsize //
-////////////////
-int LearningNetwork::outputsize() const
-{
-    PLASSERT( module &amp;&amp; store_outputs );
-    return module-&gt;getPortWidth(&quot;output&quot;);
-}
-
-////////////
-// forget //
-////////////
-void LearningNetwork::forget()
-{
-    inherited::forget();
-
-    if (module)
-        module-&gt;forget();
-
-    mbatch_size = -1;
-}
-
-///////////
-// train //
-///////////
-void LearningNetwork::train()
-{
-    if (!initTrain())
-        return;
-
-    if (stage == 0) {
-        // Perform training set-dependent initialization here.
-        if (batch_size == 0)
-            mbatch_size = train_set-&gt;length();
-        else
-            mbatch_size = batch_size;
-        if (train_set-&gt;weightsize() &gt;= 1 &amp;&amp; !store_weights)
-            PLWARNING(&quot;In LearningNetwork::train - The training set contains &quot;
-                    &quot;weights, but the network is not using them&quot;);
-    }
-
-    Mat inputs, targets;
-    Vec weights;
-    PP&lt;ProgressBar&gt; pb = NULL;
-
-    int stage_init = stage;
-    if (report_progress)
-        pb = new ProgressBar( &quot;Training &quot; + classname(), nstages - stage);
-
-    while (stage + mbatch_size &lt;= nstages) {
-        // Obtain training samples.
-        int sample_start = stage % train_set-&gt;length();
-        train_set-&gt;getExamples(sample_start, mbatch_size, inputs, targets,
-                weights, NULL, true);
-        // Perform a training step.
-        trainingStep(inputs, targets, weights);
-        // Handle training progress.
-        stage += mbatch_size;
-        if (report_progress)
-            pb-&gt;update(stage - stage_init);
-    }
-    if (stage != nstages)
-        PLWARNING(&quot;In LearningNetwork::train - The network was trained for &quot;
-                &quot;only %d stages (instead of nstages = %d, which is not a &quot;
-                &quot;multiple of batch_size = %d&quot;, stage, nstages, batch_size);
-}
-
-//////////////////
-// trainingStep //
-//////////////////
-void LearningNetwork::trainingStep(const Mat&amp; inputs, const Mat&amp; targets,
-                      const Vec&amp; weights)
-{
-    // Fill in the provided batch values (only if they are actually used by the
-    // network).
-    if (store_inputs)
-        store_inputs-&gt;setData(inputs);
-    if (store_targets)
-        store_targets-&gt;setData(targets);
-    if (store_weights)
-        store_weights-&gt;setData(weights.toMat(weights.length(), 1));
-
-    // Forward propagation.
-    network-&gt;fprop(null_pointers);
-
-    // Initialize cost gradients to 1.
-    // Note that we may not need to re-do it at every iteration, but this is so
-    // cheap it should not impact performance.
-    if (store_costs)
-        store_costs-&gt;setGradientTo(1);
-
-    // Backpropagation.
-    network-&gt;bpropAccUpdate(null_pointers, null_pointers);
-}
-
-///////////////////////////
-// computeOutputAndCosts //
-///////////////////////////
-void LearningNetwork::computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
-                                            Vec&amp; output, Vec&amp; costs) const
-{
-    static Mat one;
-    if (store_inputs)
-        store_inputs-&gt;setData(input.toMat(1, input.length()));
-    if (store_targets)
-        store_targets-&gt;setData(target.toMat(1, target.length()));
-    if (store_weights) {
-        if (one.isEmpty()) {
-            one.resize(1, 1);
-            one(0, 0) = 1;
-        }
-        store_weights-&gt;setData(one);
-    }
-
-    // Forward propagation.
-    network-&gt;fprop(null_pointers);
-
-    // Store output.
-    PLASSERT( store_outputs );
-    const Mat&amp; net_out = store_outputs-&gt;getData();
-    PLASSERT( net_out.length() == 1 );
-    output.resize(net_out.width());
-    output &lt;&lt; net_out;
-
-    // Store costs.
-    PLASSERT( store_costs );
-    const Mat&amp; net_cost = store_costs-&gt;getData();
-    PLASSERT( net_cost.length() == 1 );
-    costs.resize(net_cost.width());
-    costs &lt;&lt; net_cost;
-}
-
-///////////////////
-// computeOutput //
-///////////////////
-void LearningNetwork::computeOutput(const Vec&amp; input, Vec&amp; output) const
-{
-    // Unefficient implementation.
-    Vec target(targetsize(), MISSING_VALUE);
-    Vec costs;
-    computeOutputAndCosts(input, target, output, costs);
-}
-
-/////////////////////////////
-// computeCostsFromOutputs //
-/////////////////////////////
-void LearningNetwork::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
-                                           const Vec&amp; target, Vec&amp; costs) const
-{
-    // Unefficient implementation (recompute the output too).
-    Vec the_output;
-    computeOutputAndCosts(input, target, the_output, costs);
-#ifdef BOUNDCHECK
-    // Ensure the computed output is the same as the one provided in this
-    // method.
-    PLASSERT( output.length() == the_output.length() );
-    for (int i = 0; i &lt; output.length(); i++) {
-        PLASSERT( fast_exact_is_equal(output[i], the_output[i]) );
-    }
-#endif
-}
-
-//////////////////////
-// getTestCostNames //
-//////////////////////
-TVec&lt;string&gt; LearningNetwork::getTestCostNames() const
-{
-    if (!store_costs)
-        return TVec&lt;string&gt;();
-    else
-        return module-&gt;getPortDescription(&quot;cost&quot;);
-}
-
-///////////////////////
-// getTrainCostNames //
-///////////////////////
-TVec&lt;string&gt; LearningNetwork::getTrainCostNames() const
-{
-    // No training cost computed.
-    return TVec&lt;string&gt;();
-}
-
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:&quot;stroustrup&quot;
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h	2007-05-17 17:57:01 UTC (rev 7150)
+++ trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h	2007-05-17 18:00:16 UTC (rev 7151)
@@ -1,217 +0,0 @@
-// -*- C++ -*-
-
-// LearningNetwork.h
-//
-// Copyright (C) 2007 Olivier Delalleau
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Olivier Delalleau
-
-/*! \file LearningNetwork.h */
-
-
-#ifndef LearningNetwork_INC
-#define LearningNetwork_INC
-
-#include &lt;plearn_learners/generic/PLearner.h&gt;
-#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
-#include &lt;plearn_learners/online/EXPERIMENTAL/MatrixModule.h&gt;
-#include &lt;plearn_learners/online/NetworkConnection.h&gt;
-#include &lt;plearn_learners/online/NetworkModule.h&gt;
-
-namespace PLearn {
-
-/**
- * The first sentence should be a BRIEF DESCRIPTION of what the class does.
- * Place the rest of the class programmer documentation here.  Doxygen supports
- * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
- *
- * @todo Write class to-do's here if there are any.
- *
- * @deprecated Write deprecated stuff here if there is any.  Indicate what else
- * should be used instead.
- */
-class LearningNetwork : public PLearner
-{
-    typedef PLearner inherited;
-
-public:
-    //#####  Public Build Options  ############################################
-
-    PP&lt;OnlineLearningModule&gt; module;
-
-    int batch_size;
-
-public:
-    //#####  Public Member Functions  #########################################
-
-    //! Default constructor
-    LearningNetwork();
-
-    //#####  PLearner Member Functions  #######################################
-
-    //! Returns the size of this learner's output, (which typically
-    //! may depend on its inputsize(), targetsize() and set options).
-    virtual int outputsize() const;
-
-    //! (Re-)initializes the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!).
-    virtual void forget();
-
-    //! The role of the train method is to bring the learner up to
-    //! stage==nstages, updating the train_stats collector with training costs
-    //! measured on-line in the process.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void train();
-
-    //! Computes the output from the input.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
-
-    //! Computes the costs from already computed output.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
-                                         const Vec&amp; target, Vec&amp; costs) const;
-
-    //! Returns the names of the costs computed by computeCostsFromOutpus (and
-    //! thus the test method).
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual TVec&lt;std::string&gt; getTestCostNames() const;
-
-    //! Returns the names of the objective costs that the train method computes
-    //! and  for which it updates the VecStatsCollector train_stats.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
-
-
-    // *** SUBCLASS WRITING: ***
-    // While in general not necessary, in case of particular needs
-    // (efficiency concerns for ex) you may also want to overload
-    // some of the following methods:
-    virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
-                                       Vec&amp; output, Vec&amp; costs) const;
-    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
-    //                               Vec&amp; costs) const;
-    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
-    //                   VMat testoutputs=0, VMat testcosts=0) const;
-    // virtual int nTestCosts() const;
-    // virtual int nTrainCosts() const;
-    // virtual void resetInternalState();
-    // virtual bool isStatefulLearner() const;
-
-
-    //#####  PLearn::Object Protocol  #########################################
-
-    // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
-    PLEARN_DECLARE_OBJECT(LearningNetwork);
-
-    // Simply calls inherited::build() then build_()
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
-
-protected:
-
-    //! Simple module used to initialize the network's inputs.
-    PP&lt;MatrixModule&gt; store_inputs;
-
-    //! Simple module used to initialize the network's targets.
-    PP&lt;MatrixModule&gt; store_targets;
-
-    //! Simple module used to initialize the network's weights.
-    PP&lt;MatrixModule&gt; store_weights;
-
-    //! Simple module that will contain the network's outputs at the end of a
-    //! fprop step.
-    PP&lt;MatrixModule&gt; store_outputs;
-
-    //! Simple module that will contain the network's costs at the end of a
-    //! fprop step.
-    PP&lt;MatrixModule&gt; store_costs;
-
-    //! The network consisting of the optimized module and the additional
-    //! modules described above.
-    PP&lt;NetworkModule&gt; network;
-
-    //! The list of (null) pointers to matrices being given as argument to the
-    //! network's fprop and bpropAccUpdate methods.
-    TVec&lt;Mat*&gt; null_pointers;
-
-    //#####  Protected Options  ###############################################
-
-    int mbatch_size;
-
-protected:
-    //#####  Protected Member Functions  ######################################
-
-    //! Declares the class options.
-    static void declareOptions(OptionList&amp; ol);
-
-    //! Perform one training step for the given batch inputs, targets and
-    //! weights.
-    void trainingStep(const Mat&amp; inputs, const Mat&amp; targets,
-                      const Vec&amp; weights);
-
-private:
-    //#####  Private Member Functions  ########################################
-
-    //! This does the actual building.
-    void build_();
-
-private:
-    //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(LearningNetwork);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:&quot;stroustrup&quot;
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: trunk/plearn_learners/online/ModuleLearner.cc (from rev 7150, trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc)
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc	2007-05-17 17:57:01 UTC (rev 7150)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2007-05-17 18:00:16 UTC (rev 7151)
@@ -0,0 +1,393 @@
+// -*- C++ -*-
+
+// ModuleLearner.cc
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file ModuleLearner.cc */
+
+
+#define PL_LOG_MODULE_NAME &quot;ModuleLearner&quot;
+#include &lt;plearn/io/pl_log.h&gt;
+
+#include &quot;ModuleLearner.h&quot;
+#include &lt;plearn_learners/online/NullModule.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ModuleLearner,
+    &quot;Flexible network structure that can be optimized to learn some task(s).&quot;,
+    &quot;This network is made of several blocks, called 'modules' (deriving from\n&quot;
+    &quot;the OnlineLearningModule class), connected together so as to be able to\n&quot;
+    &quot;propagate information through the network.\n&quot;
+    &quot;Typically, during training, (input, target) pairs are fed to the\n&quot;
+    &quot;network and a cost is optimized. The trained network can later be used\n&quot;
+    &quot;to make predictions on new test points.\n&quot;
+);
+
+/////////////////////
+// ModuleLearner //
+/////////////////////
+ModuleLearner::ModuleLearner():
+    batch_size(1),
+    mbatch_size(-1)
+{
+    random_gen = new PRandom();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void ModuleLearner::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;module&quot;, &amp;ModuleLearner::module,
+                  OptionBase::buildoption,
+       &quot;The module being optimized. This module should typically have some\n&quot;
+       &quot;ports named 'input', 'target', 'weight', 'output' and 'cost'.&quot;);
+
+    declareOption(ol, &quot;batch_size&quot;, &amp;ModuleLearner::batch_size,
+                  OptionBase::buildoption,
+       &quot;Number of samples fed to the network at each iteration of learning.\n&quot;
+       &quot;Use '0' for full batch learning.&quot;);
+
+    declareOption(ol, &quot;mbatch_size&quot;, &amp;ModuleLearner::mbatch_size,
+                  OptionBase::learntoption,
+       &quot;Same as 'batch_size', except when 'batch_size' is set to 0, this\n&quot;
+       &quot;option takes the value of the size of the training set.&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void ModuleLearner::build_()
+{
+    if (!module)
+        // Cannot do anything without an underlying module.
+        return;
+
+    // Forward random number generator to underlying module.
+    if (!module-&gt;random_gen) {
+        module-&gt;random_gen = random_gen;
+        // Currently we call forget, since it ensures the module is
+        // correctly initialized and also will propagate the random number
+        // generator to its own sub-modules. However, this is not very
+        // intuitive, and a better solution may be found.
+        module-&gt;forget();
+    }
+
+    // Create a new NetworkModule that connects the ports of the underlying
+    // module to simple MatrixModules that will provide/store data.
+    const TVec&lt;string&gt;&amp; ports = module-&gt;getPorts();
+    TVec&lt; PP&lt;OnlineLearningModule&gt; &gt; all_modules;
+    all_modules.append(module);
+    TVec&lt; PP&lt;NetworkConnection&gt; &gt; all_connections;
+
+    if (ports.find(&quot;input&quot;) &gt;= 0) {
+        store_inputs = new MatrixModule(&quot;store_inputs&quot;, true);
+        all_modules.append(get_pointer(store_inputs));
+        all_connections.append(new NetworkConnection(
+                    get_pointer(store_inputs), &quot;data&quot;,
+                    module, &quot;input&quot;, false));
+    } else
+        store_inputs = NULL;
+
+    if (ports.find(&quot;target&quot;) &gt;= 0) {
+        store_targets = new MatrixModule(&quot;store_targets&quot;, true);
+        all_modules.append(get_pointer(store_targets));
+        all_connections.append(new NetworkConnection(
+                    get_pointer(store_targets), &quot;data&quot;,
+                    module, &quot;target&quot;, false));
+    } else
+        store_targets = NULL;
+
+    if (ports.find(&quot;weight&quot;) &gt;= 0) {
+        store_weights = new MatrixModule(&quot;store_weights&quot;, true);
+        all_modules.append(get_pointer(store_weights));
+        all_connections.append(new NetworkConnection(
+                    get_pointer(store_weights), &quot;data&quot;,
+                    module, &quot;weight&quot;, false));
+    } else
+        store_weights = NULL;
+
+    if (ports.find(&quot;output&quot;) &gt;= 0) {
+        store_outputs = new MatrixModule(&quot;store_outputs&quot;, true);
+        all_modules.append(get_pointer(store_outputs));
+        all_connections.append(new NetworkConnection(
+                    module, &quot;output&quot;,
+                    get_pointer(store_outputs), &quot;data&quot;, false));
+    } else
+        store_outputs = NULL;
+
+    if (ports.find(&quot;cost&quot;) &gt;= 0) {
+        store_costs = new MatrixModule(&quot;store_costs&quot;, true);
+        all_modules.append(get_pointer(store_costs));
+        // Note that this is the only connection that propagates the gradient.
+        all_connections.append(new NetworkConnection(
+                    module, &quot;cost&quot;,
+                    get_pointer(store_costs), &quot;data&quot;, true));
+    } else
+        store_costs = NULL;
+
+    network = new NetworkModule();
+    network-&gt;modules = all_modules;
+    network-&gt;connections = all_connections;
+    network-&gt;build();
+
+    // Initialize the list of null pointers to provided for forward and
+    // backward propagation.
+    null_pointers.resize(module-&gt;nPorts());
+    null_pointers.fill(NULL);
+}
+
+///////////
+// build //
+///////////
+void ModuleLearner::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void ModuleLearner::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR(&quot;ModuleLearner::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+}
+
+////////////////
+// outputsize //
+////////////////
+int ModuleLearner::outputsize() const
+{
+    PLASSERT( module &amp;&amp; store_outputs );
+    return module-&gt;getPortWidth(&quot;output&quot;);
+}
+
+////////////
+// forget //
+////////////
+void ModuleLearner::forget()
+{
+    inherited::forget();
+
+    if (module)
+        module-&gt;forget();
+
+    mbatch_size = -1;
+}
+
+///////////
+// train //
+///////////
+void ModuleLearner::train()
+{
+    if (!initTrain())
+        return;
+
+    if (stage == 0) {
+        // Perform training set-dependent initialization here.
+        if (batch_size == 0)
+            mbatch_size = train_set-&gt;length();
+        else
+            mbatch_size = batch_size;
+        if (train_set-&gt;weightsize() &gt;= 1 &amp;&amp; !store_weights)
+            PLWARNING(&quot;In ModuleLearner::train - The training set contains &quot;
+                    &quot;weights, but the network is not using them&quot;);
+    }
+
+    Mat inputs, targets;
+    Vec weights;
+    PP&lt;ProgressBar&gt; pb = NULL;
+
+    int stage_init = stage;
+    if (report_progress)
+        pb = new ProgressBar( &quot;Training &quot; + classname(), nstages - stage);
+
+    while (stage + mbatch_size &lt;= nstages) {
+        // Obtain training samples.
+        int sample_start = stage % train_set-&gt;length();
+        train_set-&gt;getExamples(sample_start, mbatch_size, inputs, targets,
+                weights, NULL, true);
+        // Perform a training step.
+        trainingStep(inputs, targets, weights);
+        // Handle training progress.
+        stage += mbatch_size;
+        if (report_progress)
+            pb-&gt;update(stage - stage_init);
+    }
+    if (stage != nstages)
+        PLWARNING(&quot;In ModuleLearner::train - The network was trained for &quot;
+                &quot;only %d stages (instead of nstages = %d, which is not a &quot;
+                &quot;multiple of batch_size = %d&quot;, stage, nstages, batch_size);
+}
+
+//////////////////
+// trainingStep //
+//////////////////
+void ModuleLearner::trainingStep(const Mat&amp; inputs, const Mat&amp; targets,
+                      const Vec&amp; weights)
+{
+    // Fill in the provided batch values (only if they are actually used by the
+    // network).
+    if (store_inputs)
+        store_inputs-&gt;setData(inputs);
+    if (store_targets)
+        store_targets-&gt;setData(targets);
+    if (store_weights)
+        store_weights-&gt;setData(weights.toMat(weights.length(), 1));
+
+    // Forward propagation.
+    network-&gt;fprop(null_pointers);
+
+    // Initialize cost gradients to 1.
+    // Note that we may not need to re-do it at every iteration, but this is so
+    // cheap it should not impact performance.
+    if (store_costs)
+        store_costs-&gt;setGradientTo(1);
+
+    // Backpropagation.
+    network-&gt;bpropAccUpdate(null_pointers, null_pointers);
+}
+
+///////////////////////////
+// computeOutputAndCosts //
+///////////////////////////
+void ModuleLearner::computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+                                            Vec&amp; output, Vec&amp; costs) const
+{
+    static Mat one;
+    if (store_inputs)
+        store_inputs-&gt;setData(input.toMat(1, input.length()));
+    if (store_targets)
+        store_targets-&gt;setData(target.toMat(1, target.length()));
+    if (store_weights) {
+        if (one.isEmpty()) {
+            one.resize(1, 1);
+            one(0, 0) = 1;
+        }
+        store_weights-&gt;setData(one);
+    }
+
+    // Forward propagation.
+    network-&gt;fprop(null_pointers);
+
+    // Store output.
+    PLASSERT( store_outputs );
+    const Mat&amp; net_out = store_outputs-&gt;getData();
+    PLASSERT( net_out.length() == 1 );
+    output.resize(net_out.width());
+    output &lt;&lt; net_out;
+
+    // Store costs.
+    PLASSERT( store_costs );
+    const Mat&amp; net_cost = store_costs-&gt;getData();
+    PLASSERT( net_cost.length() == 1 );
+    costs.resize(net_cost.width());
+    costs &lt;&lt; net_cost;
+}
+
+///////////////////
+// computeOutput //
+///////////////////
+void ModuleLearner::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    // Unefficient implementation.
+    Vec target(targetsize(), MISSING_VALUE);
+    Vec costs;
+    computeOutputAndCosts(input, target, output, costs);
+}
+
+/////////////////////////////
+// computeCostsFromOutputs //
+/////////////////////////////
+void ModuleLearner::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+    // Unefficient implementation (recompute the output too).
+    Vec the_output;
+    computeOutputAndCosts(input, target, the_output, costs);
+#ifdef BOUNDCHECK
+    // Ensure the computed output is the same as the one provided in this
+    // method.
+    PLASSERT( output.length() == the_output.length() );
+    for (int i = 0; i &lt; output.length(); i++) {
+        PLASSERT( fast_exact_is_equal(output[i], the_output[i]) );
+    }
+#endif
+}
+
+//////////////////////
+// getTestCostNames //
+//////////////////////
+TVec&lt;string&gt; ModuleLearner::getTestCostNames() const
+{
+    if (!store_costs)
+        return TVec&lt;string&gt;();
+    else
+        return module-&gt;getPortDescription(&quot;cost&quot;);
+}
+
+///////////////////////
+// getTrainCostNames //
+///////////////////////
+TVec&lt;string&gt; ModuleLearner::getTrainCostNames() const
+{
+    // No training cost computed.
+    return TVec&lt;string&gt;();
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: trunk/plearn_learners/online/ModuleLearner.h (from rev 7150, trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h)
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h	2007-05-17 17:57:01 UTC (rev 7150)
+++ trunk/plearn_learners/online/ModuleLearner.h	2007-05-17 18:00:16 UTC (rev 7151)
@@ -0,0 +1,217 @@
+// -*- C++ -*-
+
+// ModuleLearner.h
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file ModuleLearner.h */
+
+
+#ifndef ModuleLearner_INC
+#define ModuleLearner_INC
+
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn_learners/online/EXPERIMENTAL/MatrixModule.h&gt;
+#include &lt;plearn_learners/online/NetworkConnection.h&gt;
+#include &lt;plearn_learners/online/NetworkModule.h&gt;
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class ModuleLearner : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    PP&lt;OnlineLearningModule&gt; module;
+
+    int batch_size;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ModuleLearner();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+                                       Vec&amp; output, Vec&amp; costs) const;
+    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
+    //                               Vec&amp; costs) const;
+    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(ModuleLearner);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+
+    //! Simple module used to initialize the network's inputs.
+    PP&lt;MatrixModule&gt; store_inputs;
+
+    //! Simple module used to initialize the network's targets.
+    PP&lt;MatrixModule&gt; store_targets;
+
+    //! Simple module used to initialize the network's weights.
+    PP&lt;MatrixModule&gt; store_weights;
+
+    //! Simple module that will contain the network's outputs at the end of a
+    //! fprop step.
+    PP&lt;MatrixModule&gt; store_outputs;
+
+    //! Simple module that will contain the network's costs at the end of a
+    //! fprop step.
+    PP&lt;MatrixModule&gt; store_costs;
+
+    //! The network consisting of the optimized module and the additional
+    //! modules described above.
+    PP&lt;NetworkModule&gt; network;
+
+    //! The list of (null) pointers to matrices being given as argument to the
+    //! network's fprop and bpropAccUpdate methods.
+    TVec&lt;Mat*&gt; null_pointers;
+
+    //#####  Protected Options  ###############################################
+
+    int mbatch_size;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+    //! Perform one training step for the given batch inputs, targets and
+    //! weights.
+    void trainingStep(const Mat&amp; inputs, const Mat&amp; targets,
+                      const Vec&amp; weights);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ModuleLearner);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000599.html">[Plearn-commits] r7150 - trunk/plearn_learners/online/EXPERIMENTAL
</A></li>
	<LI>Next message: <A HREF="000601.html">[Plearn-commits] r7152 - trunk/python_modules/plearn/parallel
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#600">[ date ]</a>
              <a href="thread.html#600">[ thread ]</a>
              <a href="subject.html#600">[ subject ]</a>
              <a href="author.html#600">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
