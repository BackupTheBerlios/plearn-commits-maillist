<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7060 - trunk/plearn_learners/online/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7060%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200705111523.l4BFNWde024682%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000508.html">
   <LINK REL="Next"  HREF="000510.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7060 - trunk/plearn_learners/online/EXPERIMENTAL</H1>
    <B>tihocan at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7060%20-%20trunk/plearn_learners/online/EXPERIMENTAL&In-Reply-To=%3C200705111523.l4BFNWde024682%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7060 - trunk/plearn_learners/online/EXPERIMENTAL">tihocan at mail.berlios.de
       </A><BR>
    <I>Fri May 11 17:23:32 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000508.html">[Plearn-commits] r7059 - trunk/python_modules/plearn/parallel
</A></li>
        <LI>Next message: <A HREF="000510.html">[Plearn-commits] r7061 - trunk/plearn_learners/online/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#509">[ date ]</a>
              <a href="thread.html#509">[ thread ]</a>
              <a href="subject.html#509">[ subject ]</a>
              <a href="author.html#509">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: tihocan
Date: 2007-05-11 17:23:29 +0200 (Fri, 11 May 2007)
New Revision: 7060

Added:
   trunk/plearn_learners/online/EXPERIMENTAL/IdendityModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/IdendityModule.h
   trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc
   trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h
   trunk/plearn_learners/online/EXPERIMENTAL/MatrixModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/MatrixModule.h
   trunk/plearn_learners/online/EXPERIMENTAL/NetworkConnection.cc
   trunk/plearn_learners/online/EXPERIMENTAL/NetworkConnection.h
Log:
Draft experimental version of new classes for flexible learning networks of OnlineLearningModules

Added: trunk/plearn_learners/online/EXPERIMENTAL/IdendityModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/IdendityModule.cc	2007-05-11 14:21:02 UTC (rev 7059)
+++ trunk/plearn_learners/online/EXPERIMENTAL/IdendityModule.cc	2007-05-11 15:23:29 UTC (rev 7060)
@@ -0,0 +1,236 @@
+// -*- C++ -*-
+
+// IdentityModule.cc
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file IdentityModule.cc */
+
+
+
+#include &quot;IdentityModule.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    IdentityModule,
+    &quot;Module whose output is a single matrix.&quot;,
+    &quot;&quot;
+);
+
+IdentityModule::IdentityModule(bool call_build_):
+    inherited(call_build_)
+{
+    if (call_build_)
+        build_();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void IdentityModule::declareOptions(OptionList&amp; ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the &quot;flags&quot; of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, &quot;data&quot;, &amp;IdentityModule::data,
+                  OptionBase::buildoption,
+        &quot;The matrix seen by this module.&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void IdentityModule::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+// ### Nothing to add here, simply calls build_
+void IdentityModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void IdentityModule::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR(&quot;IdentityModule::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+}
+
+///////////
+// fprop //
+///////////
+void IdentityModule::fprop(const Vec&amp; input, Vec&amp; output) const
+{
+    PLERROR(&quot;In IdentityModule::fprop - Not implemented&quot;);
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+/* THIS METHOD IS OPTIONAL
+void IdentityModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                               Vec&amp; input_gradient,
+                               const Vec&amp; output_gradient,
+                               bool accumulate)
+{
+}
+*/
+
+/* THIS METHOD IS OPTIONAL
+void IdentityModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                               const Vec&amp; output_gradient)
+{
+}
+*/
+
+//////////////////
+// bbpropUpdate //
+//////////////////
+/* THIS METHOD IS OPTIONAL
+void IdentityModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                Vec&amp; input_gradient,
+                                const Vec&amp; output_gradient,
+                                Vec&amp; input_diag_hessian,
+                                const Vec&amp; output_diag_hessian,
+                                bool accumulate)
+{
+}
+*/
+
+/* THIS METHOD IS OPTIONAL
+void IdentityModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                const Vec&amp; output_gradient,
+                                const Vec&amp; output_diag_hessian)
+{
+}
+*/
+
+////////////
+// forget //
+////////////
+void IdentityModule::forget()
+{
+    // Nothing to forget.
+}
+
+//////////////
+// finalize //
+//////////////
+/* THIS METHOD IS OPTIONAL
+void IdentityModule::finalize()
+{
+}
+*/
+
+/////////////
+// getData //
+/////////////
+Mat&amp; IdentityModule::getData()
+{
+    return this-&gt;data;
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+bool IdentityModule::bpropDoesNothing()
+{
+}
+*/
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+/* OPTIONAL
+void IdentityModule::setLearningRate(real dynamic_learning_rate)
+{
+}
+*/
+
+/////////////
+// setData //
+/////////////
+void IdentityModule::setData(const Mat&amp; the_data)
+{
+    this-&gt;data = the_data;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/IdendityModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/IdendityModule.h	2007-05-11 14:21:02 UTC (rev 7059)
+++ trunk/plearn_learners/online/EXPERIMENTAL/IdendityModule.h	2007-05-11 15:23:29 UTC (rev 7060)
@@ -0,0 +1,209 @@
+// -*- C++ -*-
+
+// IdentityModule.h
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file IdentityModule.h */
+
+
+#ifndef IdentityModule_INC
+#define IdentityModule_INC
+
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class IdentityModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    IdentityModule(bool call_build_ = false);
+
+    // Your other public member functions go here
+
+    //! given the input, compute the output (possibly resize it  appropriately)
+    virtual void fprop(const Vec&amp; input, Vec&amp; output) const;
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient,
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
+    */
+
+    /* Optional
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             const Vec&amp; output_gradient);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                              Vec&amp; input_gradient,
+                              const Vec&amp; output_gradient,
+                              Vec&amp; input_diag_hessian,
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                              const Vec&amp; output_gradient,
+                              const Vec&amp; output_diag_hessian);
+    */
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    /* Optional
+       Default implementation prints a warning and does nothing
+    //! If this class has a learning rate (or something close to it), set it.
+    //! If not, you can redefine this method to get rid of the warning.
+    virtual void setLearningRate(real dynamic_learning_rate);
+    */
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(IdentityModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(IdentityModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc	2007-05-11 14:21:02 UTC (rev 7059)
+++ trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.cc	2007-05-11 15:23:29 UTC (rev 7060)
@@ -0,0 +1,573 @@
+// -*- C++ -*-
+
+// LearningNetwork.cc
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file LearningNetwork.cc */
+
+
+#define PL_LOG_MODULE_NAME &quot;LearningNetwork&quot;
+#include &lt;plearn/io/pl_log.h&gt;
+
+#include &quot;LearningNetwork.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    LearningNetwork,
+    &quot;Flexible network structure that can be optimized to learn some task(s).&quot;,
+    &quot;This network is made of several blocks, called 'modules' (deriving from\n&quot;
+    &quot;the OnlineLearningModule class), connected together so as to be able to\n&quot;
+    &quot;propagate information through the network.\n&quot;
+    &quot;Typically, during training, (input, target) pairs are fed to the\n&quot;
+    &quot;network and a cost is optimized. The trained network can later be used\n&quot;
+    &quot;to make predictions on new test points.\n&quot;
+);
+
+/////////////////////
+// LearningNetwork //
+/////////////////////
+LearningNetwork::LearningNetwork():
+    batch_size(1),
+    mbatch_size(-1)
+{
+    random_gen = new PRandom();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void LearningNetwork::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;modules&quot;, &amp;LearningNetwork::modules,
+                  OptionBase::buildoption,
+       &quot;List of modules contained in the network.&quot;);
+
+    declareOption(ol, &quot;connections&quot;, &amp;LearningNetwork::connections,
+                  OptionBase::buildoption,
+       &quot;List of connections between modules.&quot;);
+
+    declareOption(ol, &quot;input_module&quot;, &amp;LearningNetwork::input_module,
+                  OptionBase::buildoption,
+        &quot;Module that uses the samples' inputs.&quot;);
+
+    declareOption(ol, &quot;input_port&quot;, &amp;LearningNetwork::input_port,
+                  OptionBase::buildoption,
+       &quot;Port of 'input_module' that is filled with the samples' inputs.&quot;);
+
+    declareOption(ol, &quot;target_module&quot;, &amp;LearningNetwork::target_module,
+                  OptionBase::buildoption,
+        &quot;Module that uses the samples' targets.&quot;);
+
+    declareOption(ol, &quot;target_port&quot;, &amp;LearningNetwork::target_port,
+                  OptionBase::buildoption,
+       &quot;Port of 'target_module' that is filled with the samples' targets.&quot;);
+
+    declareOption(ol, &quot;weight_module&quot;, &amp;LearningNetwork::weight_module,
+                  OptionBase::buildoption,
+        &quot;Module that uses the samples' weights.&quot;);
+
+    declareOption(ol, &quot;weight_port&quot;, &amp;LearningNetwork::weight_port,
+                  OptionBase::buildoption,
+       &quot;Port of 'weight_module' that is filled with the samples' weights.&quot;);
+
+    declareOption(ol, &quot;output_module&quot;, &amp;LearningNetwork::output_module,
+                  OptionBase::buildoption,
+        &quot;Module that computes the network's output.&quot;);
+
+    declareOption(ol, &quot;output_port&quot;, &amp;LearningNetwork::output_port,
+                  OptionBase::buildoption,
+       &quot;Port of 'output_module' that contains the network's output&quot;);
+
+    declareOption(ol, &quot;cost_module&quot;, &amp;LearningNetwork::cost_module,
+                  OptionBase::buildoption,
+        &quot;Module that computes the network's cost.&quot;);
+
+    declareOption(ol, &quot;cost_port&quot;, &amp;LearningNetwork::cost_port,
+                  OptionBase::buildoption,
+       &quot;Port of 'cost_module' that contains the network's cost&quot;);
+
+    declareOption(ol, &quot;batch_size&quot;, &amp;LearningNetwork::batch_size,
+                  OptionBase::buildoption,
+       &quot;Number of samples fed to the network at each iteration of learning.\n&quot;
+       &quot;Use '0' for full batch learning.&quot;);
+
+    declareOption(ol, &quot;mbatch_size&quot;, &amp;LearningNetwork::mbatch_size,
+                  OptionBase::learntoption,
+       &quot;Same as 'batch_size', except when 'batch_size' is set to 0, this\n&quot;
+       &quot;option takes the value of the size of the training set.&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void LearningNetwork::build_()
+{
+    // Forward random number generator to underlying modules.
+    for (int i = 0; i &lt; modules.length(); i++)
+        if (!modules[i]-&gt;random_gen) {
+            modules[i]-&gt;random_gen = random_gen;
+            // Currently we call forget, since it ensures the module is
+            // correctly initialized and also will propagate the random number
+            // generator to its own sub-modules. However, this is not very
+            // intuitive, and a better solution may be found.
+            modules[i]-&gt;forget();
+        }
+
+    // Initialize 'all_modules' and 'all_connections' with copies of
+    // respectively 'modules' and 'connections'.
+    all_modules.resize(modules.length());
+    all_modules &lt;&lt; modules;
+    all_connections.resize(connections.length());
+    all_connections &lt;&lt; connections;
+    
+    // Add connections corresponding to the input, target, weight, output and
+    // cost data.
+    if (input_module) {
+        store_inputs = new MatrixModule(true);
+        all_modules.append(get_pointer(store_inputs));
+        all_connections.append(new NetworkConnection(
+                    get_pointer(store_inputs), &quot;data&quot;,
+                    input_module, input_port));
+    }
+    if (target_module) {
+        store_targets = new MatrixModule(true);
+        all_modules.append(get_pointer(store_targets));
+        all_connections.append(new NetworkConnection(
+                    get_pointer(store_targets), &quot;data&quot;,
+                    target_module, target_port));
+    }
+    if (weight_module) {
+        store_weights = new MatrixModule(true);
+        all_modules.append(get_pointer(store_weights));
+        all_connections.append(new NetworkConnection(
+                    get_pointer(store_weights), &quot;data&quot;,
+                    weight_module, weight_port));
+    }
+    if (output_module) {
+        store_outputs = new MatrixModule(true);
+        all_modules.append(get_pointer(store_outputs));
+        all_connections.append(new NetworkConnection(
+                    output_module, output_port,
+                    get_pointer(store_outputs), &quot;data&quot;));
+    }
+    if (cost_module) {
+        store_costs = new MatrixModule(true);
+        all_modules.append(get_pointer(store_costs));
+        all_connections.append(new NetworkConnection(
+                    cost_module, cost_port,
+                    get_pointer(store_costs), &quot;data&quot;));
+    }
+
+    // Construct fprop and bprop paths from the list of modules and
+    // connections.
+    // First preprocess some convenience data structures from the list of
+    // connections.
+    // 'module_to_index' maps module pointers to their corresponding index in
+    // the 'all_modules' list.
+    map&lt;const OnlineLearningModule*, int&gt; module_to_index;
+    for (int i = 0; i &lt; all_modules.length(); i++)
+        module_to_index[all_modules[i]] = i;
+    // The i-th element of 'in_connections' maps each port in the i-th module
+    // to the connection that has it as destination (there may be only one).
+    TVec&lt; map&lt;string, PP&lt;NetworkConnection&gt; &gt; &gt; in_connections;
+    // The i-th element of 'out_connections' maps each port in the i-th module
+    // to the connections that have it as source (there may be many).
+    TVec&lt; map&lt;string, TVec&lt; PP&lt;NetworkConnection&gt; &gt; &gt; &gt; out_connections;
+    // The 'inputs_needed' vector contains the number of inputs that must be
+    // fed to a module before it can compute a fprop.
+    TVec&lt;int&gt; inputs_needed(all_modules.length(), 0);
+    // The 'compute_input_of' list gives, for each module M, the indices of
+    // other modules that take an output of M as input.
+    TVec&lt; TVec&lt;int&gt; &gt; compute_input_of(all_modules.length());
+    for (int i = 0; i &lt; connections.length(); i++) {
+        PP&lt;NetworkConnection&gt; connection = connections[i];
+        int src = module_to_index[connection-&gt;src_module];
+        int dest = module_to_index[connection-&gt;dest_module];
+        inputs_needed[dest]++;
+        compute_input_of[src].append(dest);
+        map&lt;string, PP&lt;NetworkConnection&gt; &gt;&amp; in_conn = in_connections[dest];
+        if (in_conn.find(connection-&gt;dest_port) != in_conn.end())
+            PLERROR(&quot;In LearningNetwork::build_ - A port may have only one &quot;
+                    &quot;incoming connection&quot;);
+        in_conn[connection-&gt;dest_port] = connection;
+        out_connections[src][connection-&gt;src_port].append(connection);
+    }
+
+    // The fprop and bprop paths can now be computed.
+    fprop_path.resize(0);
+    bprop_path.resize(all_modules.length());
+    bprop_path.fill(-1);
+    TVec&lt;bool&gt; is_done(all_modules.length(), false);
+    fprop_data.resize(0);
+    all_mats.resize(0);
+    // A vector that stores the index of a module in the fprop path.
+    TVec&lt;int&gt; module_index_to_path_index(all_modules.length(), -1);
+    while (is_done.find(false) &gt;= 0) {
+        for (int i = 0; i &lt; all_modules.length(); i++) {
+            if (!is_done[i] &amp;&amp; inputs_needed[i] == 0) {
+                for (int j = 0; j &lt; compute_input_of[i].length(); j++)
+                    inputs_needed[compute_input_of[i][j]]--;
+                // Compute the list of matrices that must be provided to this
+                // module when doing a fprop and bprop.
+                TVec&lt;string&gt; ports = all_modules[i]-&gt;getPorts();
+                map&lt;string, PP&lt;NetworkConnection&gt; &gt;&amp; in_conn =
+                    in_connections[i];
+                map&lt;string, TVec&lt; PP&lt;NetworkConnection&gt; &gt; &gt;&amp; out_conn =
+                    out_connections[i];
+                TVec&lt;int&gt; fprop_tores;
+                TVec&lt;int&gt; bprop_tores;
+                TVec&lt;Mat*&gt; fprop_mats;
+                TVec&lt;Mat*&gt; bprop_mats;
+                for (int j = 0; j &lt; ports.length(); j++) {
+                    if (in_conn.find(ports[j]) != in_conn.end()) {
+                        // This port has an incoming connection: it is thus an
+                        // input, and the corresponding matrices for storing
+                        // its value and gradient are found by looking at the
+                        // source port of the connection.
+                        PP&lt;NetworkConnection&gt; conn = in_conn[ports[j]];
+                        int src_mod = module_to_index[conn-&gt;src_module];
+                        int path_index = module_index_to_path_index[src_mod];
+                        int port_index = conn-&gt;src_module-&gt;getPortIndex(
+                                conn-&gt;src_port);
+                        fprop_mats.append(fprop_data[path_index][port_index]);
+                        bprop_mats.append(bprop_data[path_index][port_index]);
+                        bprop_tores.append(j);
+                        PLASSERT( out_conn.find(ports[j]) == out_conn.end() );
+                    } else if (out_conn.find(ports[j]) != out_conn.end()) {
+                        // This port has (at least) one outgoing connection: it
+                        // is thus an output, and it must be provided with
+                        // matrices to store its value and gradient.
+                        all_mats.append(Mat());
+                        Mat* new_mat = &amp;all_mats.lastElement();
+                        fprop_mats.append(new_mat);
+                        all_mats.append(Mat());
+                        new_mat = &amp;all_mats.lastElement();
+                        bprop_mats.append(new_mat);
+                        fprop_tores.append(j);
+                    } else {
+                        // This port is not used (we do not provide its value,
+                        // and we do not care about obtaining it).
+                        fprop_mats.append(NULL);
+                        bprop_mats.append(NULL);
+                    }
+                }
+                module_index_to_path_index[i] = fprop_path.length();
+                // Update fprop path.
+                fprop_data.append(fprop_mats);
+                fprop_toresize.append(fprop_tores);
+                fprop_path.append(i);
+                // Update bprop path.
+                bprop_data.append(bprop_mats);
+                bprop_toresize.append(bprop_tores);
+                bprop_path[bprop_path.length() - fprop_path.length()] = i;
+
+                is_done[i] = true;
+            }
+        }
+    }
+    PLASSERT( module_index_to_path_index.find(-1) == -1 );
+}
+
+///////////
+// build //
+///////////
+void LearningNetwork::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void LearningNetwork::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(output_module, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR(&quot;LearningNetwork::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+}
+
+////////////////
+// outputsize //
+////////////////
+int LearningNetwork::outputsize() const
+{
+    PLASSERT( output_module );
+    return output_module-&gt;getPortSize(output_port);
+}
+
+////////////
+// forget //
+////////////
+void LearningNetwork::forget()
+{
+    inherited::forget();
+
+    // All modules should forget too.
+    for (int i = 0; i &lt; modules.length(); i++)
+        modules[i]-&gt;forget();
+
+    mbatch_size = -1;
+}
+
+///////////
+// train //
+///////////
+void LearningNetwork::train()
+{
+    if (!initTrain())
+        return;
+
+    if (stage == 0) {
+        // Perform training set-dependent initialization here.
+        if (batch_size == 0)
+            mbatch_size = train_set-&gt;length();
+        else
+            mbatch_size = batch_size;
+        if (train_set-&gt;weightsize() &gt;= 1 &amp;&amp; !weight_module)
+            PLWARNING(&quot;In LearningNetwork::train - The training set contains &quot;
+                    &quot;weights, but the network is not using them&quot;);
+    }
+
+    Mat inputs, targets;
+    Vec weights;
+    PP&lt;ProgressBar&gt; pb = NULL;
+
+    int stage_init = stage;
+    if (report_progress)
+        pb = new ProgressBar( &quot;Training &quot; + classname(), nstages - stage);
+
+    while (stage + mbatch_size &lt;= nstages) {
+        // Obtain training samples.
+        int sample_start = stage % train_set-&gt;length();
+        train_set-&gt;getExamples(sample_start, mbatch_size, inputs, targets,
+                weights, NULL, true);
+        // Perform a training step.
+        trainingStep(inputs, targets, weights);
+        // Handle training progress.
+        stage += mbatch_size;
+        if (report_progress)
+            pb-&gt;update(stage - stage_init);
+    }
+    if (stage != nstages)
+        PLWARNING(&quot;In LearningNetwork::train - The network was trained for &quot;
+                &quot;only %d stages (instead of nstages = %d, which is not a &quot;
+                &quot;multiple of batch_size = %d&quot;, stage, nstages, batch_size);
+}
+
+//////////////////
+// trainingStep //
+//////////////////
+void LearningNetwork::trainingStep(const Mat&amp; inputs, const Mat&amp; targets,
+                      const Vec&amp; weights)
+{
+    // Fill in the provided batch values (only if they are actually used by the
+    // network).
+    if (input_module)
+        store_inputs-&gt;setData(inputs);
+    if (target_module)
+        store_targets-&gt;setData(targets);
+    if (weight_module)
+        store_weights-&gt;setData(weights.toMat(weights.length(), 1));
+
+    // Propagate up.
+    for (int i = 0; i &lt; fprop_path.length(); i++) {
+        PP&lt;OnlineLearningModule&gt; module = all_modules[fprop_path[i]];
+        DBG_MODULE_LOG &lt;&lt; &quot;FPROP: &quot; &lt;&lt; module-&gt;classname() &lt;&lt; endl;
+        // First resize some data matrices, so that the outputs are properly
+        // computed.
+        const TVec&lt;int&gt;&amp; toresize = fprop_toresize[i];
+        for (int j = 0; j &lt; toresize.length(); j++) {
+            DBG_MODULE_LOG &lt;&lt; &quot;  out = &quot; &lt;&lt; module-&gt;getPortName(toresize[j])
+                           &lt;&lt; endl;
+            fprop_data[i][toresize[j]]-&gt;resize(0, 0);
+        }
+        module-&gt;fprop(fprop_data[i]);
+    }
+
+    // Clear gradients.
+    PLASSERT( fprop_path.length() == bprop_path.length() );
+    for (int i = 0; i &lt; bprop_path.length(); i++) {
+        const TVec&lt;int&gt;&amp; toresize = bprop_toresize[i];
+        const TVec&lt;Mat*&gt;&amp; f_data = fprop_data[fprop_data.length() - 1 - i];
+        for (int j = 0; j &lt; toresize.length(); j++) {
+            if (j == 0) {
+                DBG_MODULE_LOG &lt;&lt; &quot;CLEAR: &quot; &lt;&lt;
+                    all_modules[bprop_path[i]]-&gt;classname() &lt;&lt; endl;
+            }
+            int mat_idx = toresize[j];
+            DBG_MODULE_LOG &lt;&lt; &quot;  grad = &quot; &lt;&lt;
+                all_modules[bprop_path[i]]-&gt;getPortName(mat_idx) &lt;&lt; endl;
+            Mat* mat_toresize = bprop_data[i][mat_idx];
+            Mat* mat_tpl = f_data[mat_idx];
+            mat_toresize-&gt;resize(mat_tpl-&gt;length(), mat_tpl-&gt;width());
+            mat_toresize-&gt;fill(0);
+        }
+    }
+
+    // Backpropagate gradient to optimize parameters.
+    for (int i = 0; i &lt; bprop_path.length(); i++) {
+        PP&lt;OnlineLearningModule&gt; module = all_modules[bprop_path[i]];
+        DBG_MODULE_LOG &lt;&lt; &quot;BPROP: &quot; &lt;&lt; module-&gt;classname() &lt;&lt; endl;
+        // First resize some gradient matrices, so that the gradient is
+        // properly computed.
+        const TVec&lt;int&gt;&amp; toresize = bprop_toresize[i];
+        for (int j = 0; j &lt; toresize.length(); j++) {
+            int mat_idx = toresize[j];
+            DBG_MODULE_LOG &lt;&lt; &quot;  grad = &quot; &lt;&lt; module-&gt;getPortName(mat_idx)
+                           &lt;&lt; endl;
+            Mat* mat_toresize = bprop_data[i][mat_idx];
+            PLASSERT( mat_toresize-&gt;width() &gt; 0 );
+            mat_toresize-&gt;resize(0, mat_toresize-&gt;width());
+        }
+        // Then perform the bpropUpdate step.
+        module-&gt;bpropAccUpdate(
+                fprop_data[fprop_data.length() - 1 - i], bprop_data[i]);
+    }
+}
+
+///////////////////////////
+// computeOutputAndCosts //
+///////////////////////////
+void LearningNetwork::computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+                                            Vec&amp; output, Vec&amp; costs) const
+{
+    if (input_module)
+        store_inputs-&gt;setData(input.toMat(1, input.length()));
+    if (target_module)
+        store_targets-&gt;setData(target.toMat(1, target.length()));
+    if (weight_module)
+        // Should fill store_weights with 1, but this is not a priority.
+        PLERROR(&quot;In LearningNetwork::computeOutputAndCosts - Not implemented &quot;
+                &quot;with 'weight_module'&quot;);
+
+    // Propagate up.
+    for (int i = 0; i &lt; fprop_path.length(); i++)
+        all_modules[fprop_path[i]]-&gt;fprop(fprop_data[i]);
+
+    // Store output.
+    PLASSERT( store_outputs );
+    const Mat&amp; net_out = store_outputs-&gt;getData();
+    PLASSERT( net_out.length() == 1 );
+    output.resize(net_out.width());
+    output &lt;&lt; net_out;
+
+    // Store costs.
+    PLASSERT( store_costs );
+    const Mat&amp; net_cost = store_costs-&gt;getData();
+    PLASSERT( net_cost.length() == 1 );
+    costs.resize(net_cost.width());
+    costs &lt;&lt; net_cost;
+}
+
+///////////////////
+// computeOutput //
+///////////////////
+void LearningNetwork::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    // Unefficient implementation.
+    Vec target(targetsize(), MISSING_VALUE);
+    Vec costs;
+    computeOutputAndCosts(input, target, output, costs);
+}
+
+/////////////////////////////
+// computeCostsFromOutputs //
+/////////////////////////////
+void LearningNetwork::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+    // Unefficient implementation (recompute the output too).
+    Vec the_output;
+    computeOutputAndCosts(input, target, the_output, costs);
+#ifdef BOUNDCHECK
+    // Ensure the computed output is the same as the one provided in this
+    // method.
+    PLASSERT( output.length() == the_output.length() );
+    for (int i = 0; i &lt; output.length(); i++) {
+        PLASSERT( fast_exact_is_equal(output[i], the_output[i]) );
+    }
+#endif
+}
+
+//////////////////////
+// getTestCostNames //
+//////////////////////
+TVec&lt;string&gt; LearningNetwork::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+    // ...
+    PLASSERT( false );
+    return TVec&lt;string&gt;();
+}
+
+///////////////////////
+// getTrainCostNames //
+///////////////////////
+TVec&lt;string&gt; LearningNetwork::getTrainCostNames() const
+{
+    // Return the names of the objective costs that the train method computes
+    // and for which it updates the VecStatsCollector train_stats
+    // (these may or may not be exactly the same as what's returned by
+    // getTestCostNames).
+    // ...
+    PLASSERT( false );
+    return TVec&lt;string&gt;();
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h	2007-05-11 14:21:02 UTC (rev 7059)
+++ trunk/plearn_learners/online/EXPERIMENTAL/LearningNetwork.h	2007-05-11 15:23:29 UTC (rev 7060)
@@ -0,0 +1,263 @@
+// -*- C++ -*-
+
+// LearningNetwork.h
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file LearningNetwork.h */
+
+
+#ifndef LearningNetwork_INC
+#define LearningNetwork_INC
+
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn_learners/online/EXPERIMENTAL/MatrixModule.h&gt;
+#include &lt;plearn_learners/online/EXPERIMENTAL/NetworkConnection.h&gt;
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class LearningNetwork : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    TVec&lt; PP&lt;OnlineLearningModule&gt; &gt; modules;
+    TVec&lt; PP&lt;NetworkConnection&gt; &gt; connections;
+
+    PP&lt;OnlineLearningModule&gt; input_module;
+    PP&lt;OnlineLearningModule&gt; target_module;
+    PP&lt;OnlineLearningModule&gt; weight_module;
+    PP&lt;OnlineLearningModule&gt; output_module;
+    PP&lt;OnlineLearningModule&gt; cost_module;
+
+    string input_port;
+    string target_port;
+    string weight_port;
+    string output_port;
+    string cost_port;
+
+    int batch_size;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    LearningNetwork();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+                                       Vec&amp; output, Vec&amp; costs) const;
+    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
+    //                               Vec&amp; costs) const;
+    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(LearningNetwork);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+
+    //! Simple module used to initialize the network's inputs.
+    PP&lt;MatrixModule&gt; store_inputs;
+
+    //! Simple module used to initialize the network's targets.
+    PP&lt;MatrixModule&gt; store_targets;
+
+    //! Simple module used to initialize the network's weights.
+    PP&lt;MatrixModule&gt; store_weights;
+
+    //! Simple module that will contain the network's outputs at the end of a
+    //! fprop step.
+    PP&lt;MatrixModule&gt; store_outputs;
+
+    //! Simple module that will contain the network's costs at the end of a
+    //! fprop step.
+    PP&lt;MatrixModule&gt; store_costs;
+
+    //! Contains all modules (i.e. those in the 'modules' list, and the modules
+    //! storing the inputs, targets, etc).
+    TVec&lt; PP&lt;OnlineLearningModule&gt; &gt; all_modules;
+
+    //! Contains all connections (i.e. those in the 'connections' list, and
+    //! those added to connect the modules storing the inputs, targets, etc).
+    TVec&lt; PP&lt;NetworkConnection&gt; &gt; all_connections;
+
+    //! Ordered list of modules used when doing a fprop (the integer values
+    //! correspond to indices in 'all_modules').
+    TVec&lt;int&gt; fprop_path;
+
+    //! Ordered list of modules used when doing a bprop (the integer values
+    //! correspond to indices in 'all_modules').
+    TVec&lt;int&gt; bprop_path;
+
+    //! The i-th element is the list of Mat* pointers being provided to the
+    //! i-th module in a fprop step.
+    TVec&lt; TVec&lt;Mat*&gt; &gt; fprop_data;
+
+    //! The i-the elment is the list of matrices that need to be resized to
+    //! empty matrices prior to calling fprop() on the i-th module in a fprop
+    //! step.
+    //! The resizing is needed to ensure we correctly compute the desired
+    //! outputs.
+    TVec&lt; TVec&lt;int&gt; &gt; fprop_toresize;
+    
+    //! The i-th element is the list of Mat* pointers being provided to the
+    //! i-th module in a bprop step.
+    TVec&lt; TVec&lt;Mat*&gt; &gt; bprop_data;
+
+    //! The i-th element is the list of matrices that need to be resized to
+    //! empty matrices prior to calling bpropUpdate() on the i-th module in a
+    //! bprop step.
+    //! The resizing is needed to ensure we correctly compute the desired
+    //! gradients.
+    TVec&lt; TVec&lt;int&gt; &gt; bprop_toresize;
+
+    //! A list of all matrices used to store the various computation results in
+    //! the network (i.e. the outputs of each module).
+    TVec&lt;Mat&gt; all_mats;
+    
+    //#####  Protected Options  ###############################################
+
+    int mbatch_size;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+    //! Perform one training step for the given batch inputs, targets and
+    //! weights.
+    void trainingStep(const Mat&amp; inputs, const Mat&amp; targets,
+                      const Vec&amp; weights);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LearningNetwork);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/MatrixModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/MatrixModule.cc	2007-05-11 14:21:02 UTC (rev 7059)
+++ trunk/plearn_learners/online/EXPERIMENTAL/MatrixModule.cc	2007-05-11 15:23:29 UTC (rev 7060)
@@ -0,0 +1,236 @@
+// -*- C++ -*-
+
+// MatrixModule.cc
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file MatrixModule.cc */
+
+
+
+#include &quot;MatrixModule.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    MatrixModule,
+    &quot;Module that sees a single matrix.&quot;,
+    &quot;&quot;
+);
+
+MatrixModule::MatrixModule(bool call_build_):
+    inherited(call_build_)
+{
+    if (call_build_)
+        build_();
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void MatrixModule::declareOptions(OptionList&amp; ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the &quot;flags&quot; of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, &quot;data&quot;, &amp;MatrixModule::data,
+                  OptionBase::buildoption,
+        &quot;The matrix seen by this module.&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void MatrixModule::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+// ### Nothing to add here, simply calls build_
+void MatrixModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void MatrixModule::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR(&quot;MatrixModule::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+}
+
+///////////
+// fprop //
+///////////
+void MatrixModule::fprop(const Vec&amp; input, Vec&amp; output) const
+{
+    PLERROR(&quot;In MatrixModule::fprop - Not implemented&quot;);
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+/* THIS METHOD IS OPTIONAL
+void MatrixModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                               Vec&amp; input_gradient,
+                               const Vec&amp; output_gradient,
+                               bool accumulate)
+{
+}
+*/
+
+/* THIS METHOD IS OPTIONAL
+void MatrixModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                               const Vec&amp; output_gradient)
+{
+}
+*/
+
+//////////////////
+// bbpropUpdate //
+//////////////////
+/* THIS METHOD IS OPTIONAL
+void MatrixModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                Vec&amp; input_gradient,
+                                const Vec&amp; output_gradient,
+                                Vec&amp; input_diag_hessian,
+                                const Vec&amp; output_diag_hessian,
+                                bool accumulate)
+{
+}
+*/
+
+/* THIS METHOD IS OPTIONAL
+void MatrixModule::bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                const Vec&amp; output_gradient,
+                                const Vec&amp; output_diag_hessian)
+{
+}
+*/
+
+////////////
+// forget //
+////////////
+void MatrixModule::forget()
+{
+    // Nothing to forget.
+}
+
+//////////////
+// finalize //
+//////////////
+/* THIS METHOD IS OPTIONAL
+void MatrixModule::finalize()
+{
+}
+*/
+
+/////////////
+// getData //
+/////////////
+Mat&amp; MatrixModule::getData()
+{
+    return this-&gt;data;
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+bool MatrixModule::bpropDoesNothing()
+{
+}
+*/
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+/* OPTIONAL
+void MatrixModule::setLearningRate(real dynamic_learning_rate)
+{
+}
+*/
+
+/////////////
+// setData //
+/////////////
+void MatrixModule::setData(const Mat&amp; the_data)
+{
+    this-&gt;data = the_data;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/MatrixModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/MatrixModule.h	2007-05-11 14:21:02 UTC (rev 7059)
+++ trunk/plearn_learners/online/EXPERIMENTAL/MatrixModule.h	2007-05-11 15:23:29 UTC (rev 7060)
@@ -0,0 +1,218 @@
+// -*- C++ -*-
+
+// MatrixModule.h
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file MatrixModule.h */
+
+
+#ifndef MatrixModule_INC
+#define MatrixModule_INC
+
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class MatrixModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    Mat data;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    MatrixModule(bool call_build_ = false);
+
+    // Your other public member functions go here
+
+    //! Make this module see the provided 'the_data' matrix. Note that no copy
+    //! is made, so 'the_data' should not be modified afterwards.
+    void setData(const Mat&amp; the_data);
+
+    //! Return the current matrix viewed by this module.
+    Mat&amp; getData();
+
+    //! given the input, compute the output (possibly resize it  appropriately)
+    virtual void fprop(const Vec&amp; input, Vec&amp; output) const;
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient,
+                             const Vec&amp; output_gradient,
+                             bool accumulate=false);
+    */
+
+    /* Optional
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             const Vec&amp; output_gradient);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                              Vec&amp; input_gradient,
+                              const Vec&amp; output_gradient,
+                              Vec&amp; input_diag_hessian,
+                              const Vec&amp; output_diag_hessian,
+                              bool accumulate=false);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                              const Vec&amp; output_gradient,
+                              const Vec&amp; output_diag_hessian);
+    */
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    /* Optional
+       Default implementation prints a warning and does nothing
+    //! If this class has a learning rate (or something close to it), set it.
+    //! If not, you can redefine this method to get rid of the warning.
+    virtual void setLearningRate(real dynamic_learning_rate);
+    */
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(MatrixModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(MatrixModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/NetworkConnection.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/NetworkConnection.cc	2007-05-11 14:21:02 UTC (rev 7059)
+++ trunk/plearn_learners/online/EXPERIMENTAL/NetworkConnection.cc	2007-05-11 15:23:29 UTC (rev 7060)
@@ -0,0 +1,151 @@
+// -*- C++ -*-
+
+// NetworkConnection.cc
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file NetworkConnection.cc */
+
+
+#include &quot;NetworkConnection.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    NetworkConnection,
+    &quot;A connexion between modules in a LearningNetwork.&quot;,
+    &quot;A connexion indicates links between ports between two modules\n&quot;
+    &quot;inheriting from OnlineLearningModule.\n&quot;
+);
+
+NetworkConnection::NetworkConnection()
+{}
+
+///////////////////////
+// NetworkConnection //
+///////////////////////
+NetworkConnection::NetworkConnection(
+        PP&lt;OnlineLearningModule&gt; the_src_module,
+        const string&amp; the_src_port,
+        PP&lt;OnlineLearningModule&gt; the_dest_module,
+        const string&amp; dest_port,
+        bool call_build_):
+
+    inherited(call_build_),
+    src_module(the_src_module),
+    src_port(the_src_port),
+    dest_module(the_dest_module),
+    dest_port(the_dest_port)
+{
+    if (call_build_)
+        build_();
+}
+
+///////////
+// build //
+///////////
+void NetworkConnection::build()
+{
+    inherited::build();
+    build_();
+}
+
+void NetworkConnection::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR(&quot;NetworkConnection::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void NetworkConnection::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;src_module&quot;, &amp;NetworkConnection::src_module,
+                  OptionBase::buildoption,
+        &quot;Source module.&quot;);
+
+    declareOption(ol, &quot;src_port&quot;, &amp;NetworkConnection::src_port,
+                  OptionBase::buildoption,
+        &quot;Source module's port.&quot;);
+
+    declareOption(ol, &quot;dest_module&quot;, &amp;NetworkConnection::dest_module,
+                  OptionBase::buildoption,
+        &quot;Destination module.&quot;);
+
+    declareOption(ol, &quot;dest_port&quot;, &amp;NetworkConnection::dest_port,
+                  OptionBase::buildoption,
+        &quot;Destination module's port.&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void NetworkConnection::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/NetworkConnection.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/NetworkConnection.h	2007-05-11 14:21:02 UTC (rev 7059)
+++ trunk/plearn_learners/online/EXPERIMENTAL/NetworkConnection.h	2007-05-11 15:23:29 UTC (rev 7060)
@@ -0,0 +1,142 @@
+// -*- C++ -*-
+
+// NetworkConnection.h
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file NetworkConnection.h */
+
+
+#ifndef NetworkConnection_INC
+#define NetworkConnection_INC
+
+#include &lt;plearn/base/Object.h&gt;
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class NetworkConnection : public Object
+{
+    typedef Object inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    PP&lt;OnlineLearningModule&gt; src_module;
+    string src_port;
+    PP&lt;OnlineLearningModule&gt; dest_module;
+    string dest_port;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    NetworkConnection();
+
+    NetworkConnection(PP&lt;OnlineLearningModule&gt; the_src_module,
+                      const string&amp; the_src_port,
+                      PP&lt;OnlineLearningModule&gt; the_dest_module,
+                      const string&amp; dest_port,
+                      bool call_build_ = true);
+
+    // Your other public member functions go here
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(NetworkConnection);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(NetworkConnection);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000508.html">[Plearn-commits] r7059 - trunk/python_modules/plearn/parallel
</A></li>
	<LI>Next message: <A HREF="000510.html">[Plearn-commits] r7061 - trunk/plearn_learners/online/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#509">[ date ]</a>
              <a href="thread.html#509">[ thread ]</a>
              <a href="subject.html#509">[ subject ]</a>
              <a href="author.html#509">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
