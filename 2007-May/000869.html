<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7420 - in trunk: plearn/var	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/var scripts
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7420%20-%20in%20trunk%3A%20plearn/var%0A%09plearn_learners/generic/EXPERIMENTAL%0A%09python_modules/plearn/var%20scripts&In-Reply-To=%3C200705292006.l4TK6V48005563%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000868.html">
   <LINK REL="Next"  HREF="000870.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7420 - in trunk: plearn/var	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/var scripts</H1>
    <B>simonl at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7420%20-%20in%20trunk%3A%20plearn/var%0A%09plearn_learners/generic/EXPERIMENTAL%0A%09python_modules/plearn/var%20scripts&In-Reply-To=%3C200705292006.l4TK6V48005563%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7420 - in trunk: plearn/var	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/var scripts">simonl at mail.berlios.de
       </A><BR>
    <I>Tue May 29 22:06:31 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000868.html">[Plearn-commits] r7419 - trunk/commands
</A></li>
        <LI>Next message: <A HREF="000870.html">[Plearn-commits] r7421 - in trunk/plearn_learners/online/test: .	ModuleTester ModuleTester/.pytest	ModuleTester/.pytest/PL_ModuleTester_BasicCosts	ModuleTester/.pytest/PL_ModuleTester_BasicCosts/expected_results
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#869">[ date ]</a>
              <a href="thread.html#869">[ thread ]</a>
              <a href="subject.html#869">[ subject ]</a>
              <a href="author.html#869">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: simonl
Date: 2007-05-29 22:06:30 +0200 (Tue, 29 May 2007)
New Revision: 7420

Modified:
   trunk/plearn/var/PlusVariable.cc
   trunk/plearn/var/ProductTransposeVariable.cc
   trunk/plearn/var/VarElementVariable.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/var/Var.py
   trunk/scripts/pyplot
Log:
DeepReconstructorNet improved, some methods added to Var.py
and corrections were made in ProductTransposeVariable, PlusVariable and VarElementVariable


Modified: trunk/plearn/var/PlusVariable.cc
===================================================================
--- trunk/plearn/var/PlusVariable.cc	2007-05-29 20:02:16 UTC (rev 7419)
+++ trunk/plearn/var/PlusVariable.cc	2007-05-29 20:06:30 UTC (rev 7420)
@@ -1,3 +1,4 @@
+
 // -*- C++ -*-
 
 // PLearn (A C++ Machine Learning Library)
@@ -69,7 +70,7 @@
 {
     if (input1 &amp;&amp; input2) {
         if(input1-&gt;size() != input2-&gt;size())
-            PLERROR(&quot;In PlusVariable: input1 and input2 must have exactly the same size&quot;);
+            PLERROR(&quot;In PlusVariable: input1 and input2 must have exactly the same size (%ix%i != %ix%i)&quot;, input1-&gt;length(), input1-&gt;width(), input2-&gt;length(), input2-&gt;width());
     }
 }
 

Modified: trunk/plearn/var/ProductTransposeVariable.cc
===================================================================
--- trunk/plearn/var/ProductTransposeVariable.cc	2007-05-29 20:02:16 UTC (rev 7419)
+++ trunk/plearn/var/ProductTransposeVariable.cc	2007-05-29 20:06:30 UTC (rev 7420)
@@ -84,8 +84,10 @@
 void ProductTransposeVariable::recomputeSize(int&amp; l, int&amp; w) const
 {
     if (input1 &amp;&amp; input2) {
+        if (input1-&gt;width() != input2-&gt;width())
+            PLERROR(&quot;In ProductVariable: the size of m1 and m2 are not compatible for a matrix product&quot;);
         l = input1-&gt;length();
-        w = input2-&gt;width();
+        w = input2-&gt;length();
     } else
         l = w = 0;
 }

Modified: trunk/plearn/var/VarElementVariable.cc
===================================================================
--- trunk/plearn/var/VarElementVariable.cc	2007-05-29 20:02:16 UTC (rev 7419)
+++ trunk/plearn/var/VarElementVariable.cc	2007-05-29 20:06:30 UTC (rev 7420)
@@ -82,7 +82,7 @@
     {
         int k = int(input2-&gt;valuedata[0]);
 #ifdef BOUNDCHECK
-        if (k &gt;= input1-&gt;length())
+        if (k &gt;= input1-&gt;length() &amp;&amp; k&gt;= input1-&gt;width()) // we want to allow acces both to row vectors and column vectors.
             PLERROR(&quot;VarElementVariable::fprop() - k = %d is out of range (size is %d)&quot;, k, input1-&gt;length());
 #endif
         valuedata[0] = input1-&gt;valuedata[k];

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-05-29 20:02:16 UTC (rev 7419)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-05-29 20:06:30 UTC (rev 7420)
@@ -38,8 +38,10 @@
 
 
 #include &quot;DeepReconstructorNet.h&quot;
-#include &lt;plearn/display/DisplayUtils.h&gt; 
+#include &lt;plearn/display/DisplayUtils.h&gt;
 #include &lt;plearn/var/Var_operators.h&gt;
+#include &lt;plearn/vmat/ConcatColumnsVMatrix.h&gt;
+#include &lt;plearn/var/ConcatColumnsVariable.h&gt;
 
 namespace PLearn {
 using namespace std;
@@ -50,17 +52,10 @@
    &quot;MULTI-LINE \nHELP&quot;);
 
 DeepReconstructorNet::DeepReconstructorNet()
-/* ### Initialize all fields to their default value here */
+    :good_improvement_rate(-1e10),
+     fine_tuning_improvement_rate(-1e10),
+     minibatch_size(1)
 {
-    // ...
-
-    // ### You may (or not) want to call build_() to finish building the object
-    // ### (doing so assumes the parent classes' build_() have been called too
-    // ### in the parent classes' constructors, something that you must ensure)
-
-    // ### If this learner needs to generate random numbers, uncomment the
-    // ### line below to enable the use of the inherited PRandom object.
-    // random_gen = new PRandom();
 }
 
 void DeepReconstructorNet::declareOptions(OptionList&amp; ol)
@@ -94,10 +89,18 @@
                   OptionBase::buildoption,
                   &quot;&quot;);
 
-    declareOption(ol, &quot;supervised_cost&quot;, &amp;DeepReconstructorNet::supervised_cost,
+    declareOption(ol, &quot;supervised_costs&quot;, &amp;DeepReconstructorNet::supervised_costs,
                   OptionBase::buildoption,
                   &quot;&quot;);
 
+    declareOption(ol, &quot;supervised_costvec&quot;, &amp;DeepReconstructorNet::supervised_costvec,
+                  OptionBase::learntoption,
+                  &quot;&quot;);
+
+    declareOption(ol, &quot;supervised_costs_names&quot;, &amp;DeepReconstructorNet::supervised_costs_names,
+                  OptionBase::buildoption,
+                  &quot;&quot;);
+
     declareOption(ol, &quot;minibatch_size&quot;, &amp;DeepReconstructorNet::minibatch_size,
                   OptionBase::buildoption,
                   &quot;&quot;);
@@ -114,6 +117,10 @@
                   OptionBase::buildoption,
                   &quot;&quot;);
 
+    declareOption(ol, &quot;fine_tuning_improvement_rate&quot;, &amp;DeepReconstructorNet::fine_tuning_improvement_rate,
+                  OptionBase::buildoption,
+                  &quot;&quot;);
+
     
     
     // Now call the parent class' declareOptions
@@ -165,9 +172,19 @@
     compute_output = Func(layers[0], layers[nlayers-1]);
     nout = layers[nlayers-1]-&gt;size();
 
-    if(supervised_cost.isNull())
+    output_and_target_to_cost = Func(layers[nlayers-1]&amp;target, supervised_costvec); 
+
+
+    if(supervised_costs.isNull())
         PLERROR(&quot;You must provide a supervised_cost&quot;);
-    fullcost = supervised_cost;
+
+    supervised_costvec = hconcat(supervised_costs);
+
+
+    fullcost = supervised_costs[0];
+    for(int i=1; i&lt;supervised_costs.length(); i++)
+        fullcost = fullcost + supervised_costs[i];
+    
     int n_rec_costs = reconstruction_costs.length();
     for(int k=0; k&lt;n_rec_costs; k++)
         fullcost = fullcost + reconstruction_costs[k];
@@ -202,17 +219,24 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
+    deepCopyField(training_schedule, copies);
+    deepCopyField(layers, copies);
+    deepCopyField(reconstruction_costs, copies);
+    deepCopyField(reconstruction_optimizer, copies);
+    deepCopyField(target, copies);
+    deepCopyField(supervised_costs, copies);
+    deepCopyField(supervised_costvec, copies);
+    deepCopyField(fullcost, copies);    
+    deepCopyField(parameters,copies);
+    deepCopyField(supervised_optimizer, copies);
+    deepCopyField(fine_tuning_optimizer, copies);
 
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR(&quot;DeepReconstructorNet::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+    deepCopyField(compute_layer, copies);
+    deepCopyField(compute_output, copies);
+    deepCopyField(output_and_target_to_cost, copies);
+    deepCopyField(outmat, copies);
 }
 
-
 int DeepReconstructorNet::outputsize() const
 {
     // Compute and return the size of this learner's output (which typically
@@ -254,20 +278,30 @@
 
     int nreconstructions = reconstruction_costs.length();
     int insize = train_set-&gt;inputsize();
-    VMat dset = train_set.subMatColumns(0,insize);
+    VMat inputs = train_set.subMatColumns(0,insize);
+    VMat targets = train_set.subMatColumns(insize, train_set-&gt;targetsize());
+    VMat dset = inputs;
 
-    // hack...
-    dset = dset.subMatRows(0,10);
+    bool must_train_supervised_layer = (training_schedule[training_schedule.length()-2]&gt;0);
+
     for(int k=0; k&lt;nreconstructions; k++)
     {
         trainHiddenLayer(k, dset);
-        int width = layers[k+1].width();
-        outmat[k] = new FileVMatrix(outmatfname+tostring(k)+&quot;.pmat&quot;,0,width);
-        outmat[k]-&gt;defineSizes(width,0);
-        buildHiddenLayerOutputs(k, dset, outmat[k]);
-        dset = outmat[k];
-    }    
+        // 'if' is a hack to avoid precomputing last hidden layer if not needed
+        if(k&lt;nreconstructions-1 ||  must_train_supervised_layer) 
+        { 
+            int width = layers[k+1].width();
+            outmat[k] = new FileVMatrix(outmatfname+tostring(k+1)+&quot;.pmat&quot;,0,width);
+            outmat[k]-&gt;defineSizes(width,0);
+            buildHiddenLayerOutputs(k, dset, outmat[k]);
+            dset = outmat[k];
+        }
+    }
 
+    if(must_train_supervised_layer)
+        trainSupervisedLayer(dset, targets);
+
+    fineTuning();
     /*
     while(stage&lt;nstages)
     {        
@@ -300,12 +334,93 @@
     }
 }
 
+void DeepReconstructorNet::fineTuning()
+{
+    int l = train_set-&gt;length();
+    int nepochs = training_schedule[training_schedule.length()-1];
+    perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** Performing fine tuning for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
+
+    Func f(layers[0]&amp;target, supervised_costvec);
+    Var totalcost = sumOf(train_set, f, minibatch_size);
+    // displayVarGraph(supervised_costvec);
+    // displayVarGraph(totalcost);
+
+    VarArray params = totalcost-&gt;parents();
+    supervised_optimizer-&gt;setToOptimize(params, totalcost);
+    supervised_optimizer-&gt;reset();
+    VecStatsCollector st;
+    real prev_mean = -1;
+    real relative_improvement = fine_tuning_improvement_rate;
+    for(int n=0; n&lt;nepochs &amp;&amp; relative_improvement &gt;= fine_tuning_improvement_rate; n++)
+    {
+        st.forget();
+        supervised_optimizer-&gt;nstages = l/minibatch_size;
+        supervised_optimizer-&gt;optimizeN(st);
+        const StatsCollector&amp; s = st.getStats(0);
+        real m = s.mean();
+        perr &lt;&lt; &quot;Epoch &quot; &lt;&lt; n+1 &lt;&lt; &quot; mean error: &quot; &lt;&lt; m &lt;&lt; &quot; +- &quot; &lt;&lt; s.stderror() &lt;&lt; endl;
+        if(prev_mean&gt;0)
+        {
+            relative_improvement = ((prev_mean-m)/prev_mean)*100;
+            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; relative_improvement &lt;&lt; &quot; %&quot;&lt;&lt; endl;
+        }
+        prev_mean = m;
+    }
+}
+
+
+void DeepReconstructorNet::trainSupervisedLayer(VMat inputs, VMat targets)
+{
+    int l = inputs-&gt;length();
+    int last_hidden_layer = layers.length()-2;
+    int nepochs = training_schedule[training_schedule.length()-2];
+    perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** Training only supervised layer for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
+
+    Func f(layers[last_hidden_layer]&amp;target, supervised_costvec);
+    // displayVarGraph(supervised_costvec);
+    VMat inputs_targets = hconcat(inputs, targets);
+    inputs_targets-&gt;defineSizes(inputs.width(),targets.width());
+
+    Var totalcost = sumOf(inputs_targets, f, minibatch_size);
+    // displayVarGraph(totalcost);
+
+    VarArray params = totalcost-&gt;parents();
+    supervised_optimizer-&gt;setToOptimize(params, totalcost);
+    supervised_optimizer-&gt;reset();
+    VecStatsCollector st;
+    real prev_mean = -1;
+    real relative_improvement = good_improvement_rate;
+    for(int n=0; n&lt;nepochs &amp;&amp; relative_improvement &gt;= good_improvement_rate; n++)
+    {
+        st.forget();
+        supervised_optimizer-&gt;nstages = l/minibatch_size;
+        supervised_optimizer-&gt;optimizeN(st);
+        const StatsCollector&amp; s = st.getStats(0);
+        real m = s.mean();
+        perr &lt;&lt; &quot;Epoch &quot; &lt;&lt; n+1 &lt;&lt; &quot; Mean costs: &quot; &lt;&lt; st.getMean() &lt;&lt; &quot; stderr &quot; &lt;&lt; st.getStdError() &lt;&lt; endl;
+        perr &lt;&lt; &quot;mean error: &quot; &lt;&lt; m &lt;&lt; &quot; +- &quot; &lt;&lt; s.stderror() &lt;&lt; endl;
+        if(prev_mean&gt;0)
+        {
+            relative_improvement = ((prev_mean-m)/prev_mean)*100;
+            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; relative_improvement &lt;&lt; &quot; %&quot;&lt;&lt; endl;
+        }
+        prev_mean = m;
+        //displayVarGraph(supervised_costvec, true);
+
+    }
+    
+}
+
 void DeepReconstructorNet::trainHiddenLayer(int which_input_layer, VMat inputs)
 {
     int l = inputs-&gt;length();
     int nepochs = training_schedule[which_input_layer];
     perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
-    perr &lt;&lt; &quot;*** Training layer &quot; &lt;&lt; which_input_layer &lt;&lt; &quot; for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** Training layer &quot; &lt;&lt; which_input_layer+1 &lt;&lt; &quot; for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
     perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
     Func f(layers[which_input_layer], reconstruction_costs[which_input_layer]);
     //displayVarGraph(reconstruction_costs[which_input_layer]);
@@ -324,7 +439,7 @@
         reconstruction_optimizer-&gt;optimizeN(st);
         const StatsCollector&amp; s = st.getStats(0);
         real m = s.mean();
-        perr &lt;&lt; &quot;Epoch &quot; &lt;&lt; n &lt;&lt; &quot; mean error: &quot; &lt;&lt; m &lt;&lt; &quot; +- &quot; &lt;&lt; s.stderror() &lt;&lt; endl;
+        perr &lt;&lt; &quot;Epoch &quot; &lt;&lt; n+1 &lt;&lt; &quot; mean error: &quot; &lt;&lt; m &lt;&lt; &quot; +- &quot; &lt;&lt; s.stderror() &lt;&lt; endl;
         if(prev_mean&gt;0)
         {
             relative_improvement = ((prev_mean-m)/prev_mean)*100;
@@ -344,33 +459,18 @@
 void DeepReconstructorNet::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
                                            const Vec&amp; target, Vec&amp; costs) const
 {
-// Compute the costs from *already* computed output.
-// ...
+    costs.resize(supervised_costs_names.length());
+    output_and_target_to_cost-&gt;fprop(output&amp;target, costs);
 }
 
 TVec&lt;string&gt; DeepReconstructorNet::getTestCostNames() const
 {
-    // Return the names of the costs computed by computeCostsFromOutputs
-    // (these may or may not be exactly the same as what's returned by
-    // getTrainCostNames).
-    // ...
-    
-    //TODO : retourner la bonne chose ici !
-    TVec&lt;string&gt; todo(0);
-    return todo;
+    return supervised_costs_names;
 }
 
 TVec&lt;string&gt; DeepReconstructorNet::getTrainCostNames() const
-{
-    // Return the names of the objective costs that the train method computes
-    // and for which it updates the VecStatsCollector train_stats
-    // (these may or may not be exactly the same as what's returned by
-    // getTestCostNames).
-    // ...
-
-    //TODO : retourner la bonne chose ici !
-    TVec&lt;string&gt; todo(0);
-    return todo;
+{ 
+    return supervised_costs_names;
 }
 
 Mat DeepReconstructorNet::getParameterValue(const string&amp; varname)
@@ -402,7 +502,7 @@
 
 } // end of namespace PLearn
 
-
+
 /*
   Local Variables:
   mode:c++
@@ -413,4 +513,4 @@
   fill-column:79
   End:
 */
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :50

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-05-29 20:02:16 UTC (rev 7419)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-05-29 20:06:30 UTC (rev 7420)
@@ -77,6 +77,7 @@
     TVec&lt;int&gt; training_schedule;
 
     real good_improvement_rate;
+    real fine_tuning_improvement_rate;
 
     // layers[0] is the input variable
     // last layer is final output layer
@@ -89,8 +90,12 @@
 
 
     Var target;
-    Var supervised_cost;
+    //TVec&lt;Var&gt; supervised_costs;
+    VarArray supervised_costs;
+    Var supervised_costvec; // hconcat(supervised_costs)
 
+    TVec&lt;string&gt; supervised_costs_names;
+
     Var fullcost;
     
     VarArray parameters;
@@ -107,6 +112,7 @@
 
     TVec&lt;Func&gt; compute_layer;
     Func compute_output;
+    Func output_and_target_to_cost;
     TVec&lt;VMat&gt; outmat;
     
 
@@ -169,6 +175,11 @@
     //! Returns a list of the parameters
     TVec&lt;Mat&gt; listParameter();
 
+    void fineTuning();
+
+    void trainSupervisedLayer(VMat inputs, VMat targets);
+    
+
     // *** SUBCLASS WRITING: ***
     // While in general not necessary, in case of particular needs
     // (efficiency concerns for ex) you may also want to overload

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-05-29 20:02:16 UTC (rev 7419)
+++ trunk/python_modules/plearn/var/Var.py	2007-05-29 20:06:30 UTC (rev 7420)
@@ -91,7 +91,7 @@
         return Var(pl.AffineTransformVariable(self.v, W))
 
     # TODO: verifier si NegCrossEntropySigmoidVariable est bien ce qui est utilise par Hugo
-    def negCrossEntropySigmoid(self, other, regularizer=0., ignore_missing=False):
+    def negCrossEntropySigmoid(self, other, regularizer=0, ignore_missing=False):
         return Var(pl.NegCrossEntropySigmoidVariable(input1=self.v, input2=other.v, regularizer=regularizer, ignore_missing=ignore_missing))
         
     def doubleProduct(self, W, M):
@@ -130,6 +130,9 @@
     def add(self, other):
         return Var(pl.PlusVariable(input1=self.v, input2=other.v))
 
+    def classificationLoss(self, class_index):
+        return Var(pl.ClassificationLossVariable(input1=self.v, input2=class_index.v))
+    
     def __add__(self, other):
         return self.add(other)
 
@@ -146,6 +149,7 @@
 ###################################################    
 # RLayer stands for reconstruciton hidden layer
 
+
 def addSigmoidTiedRLayer(input, iw, ow, add_bias=True, basename=&quot;&quot;):
     &quot;&quot;&quot;This assumes the input is a (1,iw) matrix,
     and will produce an output that will be a (1, ow) matrix.
@@ -154,11 +158,11 @@
     Then output = sigmoid(input.W^T + b)
     
     Returns a triple (hidden, reconstruciton_cost)&quot;&quot;&quot;
-    W = Var(ow,iw,&quot;uniform&quot;, -1./iw, varname=basename+'_W')
+    W = Var(ow,iw,&quot;uniform&quot;, -1./sqrt(iw), 1./sqrt(iw), varname=basename+'_W')
     
     if add_bias:
-        b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
-        hidden = input.matrixProduct_A_Bt(W).add(b).sigmoid()
+        b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')        
+        hidden = input.matrixProduct_A_Bt(W).add(b).sigmoid()        
         br = Var(1,iw,&quot;fill&quot;,0, varname=basename+'_br')
         cost = hidden.matrixProduct(W).add(br).negCrossEntropySigmoid(input)
     else:
@@ -191,21 +195,38 @@
     return hidden, cost
 
 def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
-    W = Var(ow, iw, &quot;uniform&quot;, -1./iw, varname=basename+'_W')
+    W = Var(ow, iw, &quot;uniform&quot;, -1./iw, 1./iw, varname=basename+'_W')
     hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
     cost = -hidden.matrixProduct(W).multiLogSoftMax(igs).dot(input)
     return hidden, cost
 
 def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs):
-    W = Var(ow, iw, &quot;uniform&quot;, -1./iw, varname=basename+'_W')
+    W = Var(ow, iw, &quot;uniform&quot;, -1./iw, 1./iw, varname=basename+'_W')
     hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
-    Wr = Var(ow, iw, &quot;uniform&quot;, -1./ow, varname=basename+'_Wr')
+    Wr = Var(ow, iw, &quot;uniform&quot;, -1./ow, 1./ow, varname=basename+'_Wr')
     cost = -hidden.matrixProduct(Wr).multiLogSoftMax(igs).dot(input)
     return hidden, cost
 
 #################################
 # These build supervised layers
 
+def addLinearLayer(input, iw, ow, add_bias=True, basename=&quot;&quot;):
+    &quot;&quot;&quot;This assumes the input is a (1,iw) matrix,
+    and will produce an output that will be a (1, ow) matrix.
+    It will create parameter W(ow,iw)
+    and an optional parameter b(1,ow)
+    Then output = input.W^T + b    
+    Returns the output layer&quot;&quot;&quot;
+    W = Var(ow,iw,&quot;uniform&quot;, -1./iw, 1./iw, varname=basename+'_W')
+    
+    if add_bias:
+        b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
+        out = input.matrixProduct_A_Bt(W).add(b)
+    else:
+        out = input.matrixProduct_A_Bt(W)
+    return out
+
+
 def addClassificationNegLogSoftmaxLayer(activation, target):
     &quot;&quot;&quot;Assumes that target is a scalar variable containing a class number between 0 and m-1,
     and activation is a m-dimensional vector of class scores (reals).

Modified: trunk/scripts/pyplot
===================================================================
--- trunk/scripts/pyplot	2007-05-29 20:02:16 UTC (rev 7419)
+++ trunk/scripts/pyplot	2007-05-29 20:06:30 UTC (rev 7420)
@@ -113,22 +113,11 @@
         pl.RegularGridVMatrix(coordstart=[pair[0] for pair in bbox],
                               coordend=[pair[1] for pair in bbox],
                               dimensions = nsamples))
-
-    # cummagnitude = transform_magnitude_into_covered_percentage(xymagnitude)
-    # print &quot;Sum of magnitudes = &quot;, cummagnitude
-
-    # contour_xymagnitude(xymagnitude)
-    contourf_xymagnitude(xymagnitude)
-    plot_2d_points(data,'ko')
-
-    figure()
+    transform_magnitude_into_covered_percentage(xymagnitude)
+    # print 'xymagnitude.shape = ',xymagnitude.shape
     imshow_xymagnitude(xymagnitude)
     plot_2d_points(data,'ko')
-
     show()
-
-    maxmagnitude = divide_by_max_magnitude(xymagnitude)
-    print &quot;Dividing by max magnitude = &quot;, maxmagnitude
     surfplot_xymagnitude(xymagnitude)
     raw_input()
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000868.html">[Plearn-commits] r7419 - trunk/commands
</A></li>
	<LI>Next message: <A HREF="000870.html">[Plearn-commits] r7421 - in trunk/plearn_learners/online/test: .	ModuleTester ModuleTester/.pytest	ModuleTester/.pytest/PL_ModuleTester_BasicCosts	ModuleTester/.pytest/PL_ModuleTester_BasicCosts/expected_results
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#869">[ date ]</a>
              <a href="thread.html#869">[ thread ]</a>
              <a href="subject.html#869">[ subject ]</a>
              <a href="author.html#869">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
