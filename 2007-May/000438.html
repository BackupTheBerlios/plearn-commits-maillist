<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6989 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6989%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705032153.l43Lrshg026920%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000437.html">
   <LINK REL="Next"  HREF="000439.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6989 - trunk/plearn_learners/online</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6989%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705032153.l43Lrshg026920%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6989 - trunk/plearn_learners/online">yoshua at mail.berlios.de
       </A><BR>
    <I>Thu May  3 23:53:54 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000437.html">[Plearn-commits] r6988 - trunk/plearn/math
</A></li>
        <LI>Next message: <A HREF="000439.html">[Plearn-commits] r6990 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#438">[ date ]</a>
              <a href="thread.html#438">[ thread ]</a>
              <a href="subject.html#438">[ subject ]</a>
              <a href="author.html#438">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-05-03 23:53:53 +0200 (Thu, 03 May 2007)
New Revision: 6989

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
Log:
Still trying to make layerwise_reconstruction_error work


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-05-03 21:53:32 UTC (rev 6988)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-05-03 21:53:53 UTC (rev 6989)
@@ -717,11 +717,7 @@
                     if (batch_size &gt; 1 || minibatch_hack) {
                         train_set-&gt;getExamples(sample_start, minibatch_size,
                                 inputs, targets, weights, NULL, true);
-                        if (reconstruct_layerwise)
-                        {
-                            train_costs_m.fill(MISSING_VALUE);
-                            train_costs_m.column(reconstruction_cost_index).clear();
-                        }
+                        train_costs_m.fill(MISSING_VALUE);
                         greedyStep( inputs, targets, i , train_costs_m);
                         for (int k = 0; k &lt; minibatch_size; k++)
                             train_stats-&gt;update(train_costs_m(k));
@@ -813,6 +809,7 @@
                 if (minibatch_size &gt; 1 || minibatch_hack) {
                     train_set-&gt;getExamples(sample_start, minibatch_size, inputs,
                             targets, weights, NULL, true);
+                    train_costs_m.fill(MISSING_VALUE);
                     fineTuningStep(inputs, targets, train_costs_m);
                 } else {
                     train_set-&gt;getExample( sample_start, input, target, weight );
@@ -875,22 +872,6 @@
         }
         else
             expectation_gradients[i+1].clear();
-/*
-        if( reconstruct_layerwise )
-        {
-            // layer_input, reconstruction_cost_indices
-            layer_input.resize(layers[i]-&gt;size);
-            layer_input &lt;&lt; layers[i]-&gt;expectation; // fpropNLL writes in expectation
-            connections[i]-&gt;setAsUpInput( layers[i+1]-&gt;expectation );
-            layers[i]-&gt;getAllActivations( connections[i] );
-            real rc = train_costs[reconstruction_cost_indices[i+1]] 
-                = layer[i]-&gt;fpropNLL( layer_input ); // or use a NLLCostModule::fprop
-            train_costs[reconstruction_cost_indices[0]] +=
-                train_costs[reconstruction_cost_indices[i+1]];
-            ... layers[i]-&gt;bpropNLL( layer_input, rc, bias_gradient );
-            layers[i]-&gt;expectation &lt;&lt; layer_input;
-        }
-*/
     }
 
     // top layer may be connected to a final_module followed by a
@@ -1087,22 +1068,6 @@
         }
         else
             expectations_gradients[i+1].clear();
-/* TODO Remove if not used? Or implement if we want to do it?
-        if( reconstruct_layerwise )
-        {
-            // layer_input, reconstruction_cost_indices
-            layer_input.resize(layers[i]-&gt;size);
-            layer_input &lt;&lt; layers[i]-&gt;expectation; // fpropNLL writes in expectation
-            connections[i]-&gt;setAsUpInput( layers[i+1]-&gt;expectation );
-            layers[i]-&gt;getAllActivations( connections[i] );
-            real rc = train_costs[reconstruction_cost_indices[i+1]] 
-                = layer[i]-&gt;fpropNLL( layer_input ); // or use a NLLCostModule::fprop
-            train_costs[reconstruction_cost_indices[0]] +=
-                train_costs[reconstruction_cost_indices[i+1]];
-            ... layers[i]-&gt;bpropNLL( layer_input, rc, bias_gradient );
-            layers[i]-&gt;expectation &lt;&lt; layer_input;
-        }
-*/
     }
 
     // top layer may be connected to a final_module followed by a
@@ -1384,12 +1349,13 @@
     {
         connections[index]-&gt;setAsUpInputs(layers[index+1]-&gt;getExpectations());
         layers[index]-&gt;getAllActivations(connections[index], 0, true);
-        for (int i=0;i&lt;inputs.length();i++)
-        {
-            real rc = train_costs_m(i,reconstruction_cost_index+index+1)
-                = layers[index]-&gt;fpropNLL( inputs(i) ); 
-            train_costs_m(i,reconstruction_cost_index) += rc;
-        }
+        layers[index]-&gt;computeExpectations();
+        layers[index]-&gt;fpropNLL(inputs, train_costs_m.column(reconstruction_cost_index+index+1));
+        Mat rc = train_costs_m.column(reconstruction_cost_index);
+        if (rc.hasMissing())
+            rc &lt;&lt; train_costs_m.column(reconstruction_cost_index+index+1);
+        else
+            rc += train_costs_m.column(reconstruction_cost_index+index+1);
     }
 }
 
@@ -1682,6 +1648,26 @@
                                        expectations_gradients[i-1],
                                        activations_gradients[i] );
     }
+
+    // do it AFTER the bprop to avoid interfering with activations used in bprop
+    // (and do not worry that the weights have changed a bit)
+    if (reconstruct_layerwise)
+    {
+        Mat rc = train_costs.column(reconstruction_cost_index);
+        rc.clear();
+        for( int index=0 ; index&lt;n_layers-2 ; index++ )
+        {
+            layer_inputs.resize(minibatch_size,layers[index]-&gt;size);
+            layer_inputs &lt;&lt; layers[index]-&gt;getExpectations();
+            connections[index]-&gt;setAsUpInputs(layers[index+1]-&gt;getExpectations());
+            layers[index]-&gt;getAllActivations(connections[index], 0, true);
+            layers[index]-&gt;computeExpectations();
+            layers[index]-&gt;fpropNLL(layer_inputs, train_costs.column(reconstruction_cost_index+index+1));
+            rc += train_costs.column(reconstruction_cost_index+index+1);
+        }
+    }
+
+
 }
 
 ///////////////////////////////
@@ -1870,9 +1856,11 @@
 
         if (reconstruct_layerwise)
         {
-            Vec layer_input = layers[i]-&gt;expectation.copy();
-            connections[i]-&gt;setAsUpInputs(layers[i+1]-&gt;getExpectations());
+            layer_input.resize(layers[i]-&gt;size);
+            layer_input &lt;&lt; layers[i]-&gt;expectation;
+            connections[i]-&gt;setAsUpInput(layers[i+1]-&gt;expectation);
             layers[i]-&gt;getAllActivations(connections[i]);
+            layers[i]-&gt;computeExpectation();
             real rc = reconstruction_costs[i+1] = layers[i]-&gt;fpropNLL( layer_input ); 
             reconstruction_costs[0] += rc;
         }

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-05-03 21:53:32 UTC (rev 6988)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-05-03 21:53:53 UTC (rev 6989)
@@ -359,6 +359,9 @@
 
     Vec reconstruction_costs;
 
+    mutable Vec layer_input;
+    mutable Mat layer_inputs;
+
 protected:
     //#####  Protected Member Functions  ######################################
 

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-05-03 21:53:32 UTC (rev 6988)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-05-03 21:53:53 UTC (rev 6989)
@@ -287,16 +287,26 @@
     return ret;
 }
 
-real RBMBinomialLayer::fpropNLL(const Mat&amp; target)
+void RBMBinomialLayer::fpropNLL(const Mat&amp; targets, Mat costs_column)
 {
     computeExpectations();
 
-    PLASSERT( target.width() == input_size );
+    PLASSERT( targets.width() == input_size );
 
-    real total_nll=0;
-    for (int i=0;i&lt;target.length();i++)
-        total_nll += fpropNLL(target(i));
-    return total_nll;
+    for (int k=0;k&lt;targets.length();k++) // loop over minibatch
+    {
+        real nll = 0;
+        real* output = expectations[k];
+        real* target = targets[k];
+        for( int i=0 ; i&lt;size ; i++ ) // loop over outputs
+        {
+            if(!fast_exact_is_equal(target[i],0.0))
+                nll -= target[i] * pl_log(output[i]);
+            if(!fast_exact_is_equal(target[i],1.0))
+                nll -= (1-target[i]) * pl_log(1-output[i]);
+        }
+        costs_column(k,0) = nll;
+    }
 }
 
 void RBMBinomialLayer::bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient)

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-05-03 21:53:32 UTC (rev 6988)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-05-03 21:53:53 UTC (rev 6989)
@@ -106,7 +106,7 @@
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec&amp; target);
-    virtual real fpropNLL(const Mat&amp; target);
+    virtual void fpropNLL(const Mat&amp; target, Mat costs_column);
 
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-05-03 21:53:32 UTC (rev 6988)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-05-03 21:53:53 UTC (rev 6989)
@@ -263,10 +263,9 @@
     return REAL_MAX;
 }
 
-real RBMLayer::fpropNLL(const Mat&amp; target)
+void RBMLayer::fpropNLL(const Mat&amp; target, Mat costs_column)
 {
     PLERROR(&quot;In RBMLayer::fpropNLL(): not implemented&quot;);
-    return REAL_MAX;
 }
 
 void RBMLayer::bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient)

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-05-03 21:53:32 UTC (rev 6988)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-05-03 21:53:53 UTC (rev 6989)
@@ -172,7 +172,7 @@
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec&amp; target);
-    virtual real fpropNLL(const Mat&amp; target);
+    virtual void fpropNLL(const Mat&amp; target, Mat costs_column);
 
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000437.html">[Plearn-commits] r6988 - trunk/plearn/math
</A></li>
	<LI>Next message: <A HREF="000439.html">[Plearn-commits] r6990 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#438">[ date ]</a>
              <a href="thread.html#438">[ thread ]</a>
              <a href="subject.html#438">[ subject ]</a>
              <a href="author.html#438">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
