<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6986 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6986%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705031510.l43FAbaW011906%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000434.html">
   <LINK REL="Next"  HREF="000436.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6986 - trunk/plearn_learners/online</H1>
    <B>tihocan at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6986%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200705031510.l43FAbaW011906%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6986 - trunk/plearn_learners/online">tihocan at mail.berlios.de
       </A><BR>
    <I>Thu May  3 17:10:37 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000434.html">[Plearn-commits] r6985 - trunk/plearn/math
</A></li>
        <LI>Next message: <A HREF="000436.html">[Plearn-commits] r6987 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#435">[ date ]</a>
              <a href="thread.html#435">[ thread ]</a>
              <a href="subject.html#435">[ subject ]</a>
              <a href="author.html#435">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: tihocan
Date: 2007-05-03 17:10:36 +0200 (Thu, 03 May 2007)
New Revision: 6986

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Added code for mini-batch version of the 'online' mode

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-05-03 05:17:49 UTC (rev 6985)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-05-03 15:10:36 UTC (rev 6986)
@@ -510,6 +510,8 @@
     deepCopyField(final_cost_gradients,     copies);
     deepCopyField(save_layer_activation,    copies);
     deepCopyField(save_layer_expectation,   copies);
+    deepCopyField(save_layer_activations,   copies);
+    deepCopyField(save_layer_expectations,  copies);
     deepCopyField(pos_down_val,             copies);
     deepCopyField(pos_up_val,               copies);
     deepCopyField(cd_neg_up_vals,           copies);
@@ -656,26 +658,31 @@
     train_stats-&gt;forget();
 
     if (online)
-        // train all layers simultaneously AND fine-tuning as well!
     {
-        PLASSERT_MSG(batch_size == 1, &quot;Online mode not implemented for &quot;
-                &quot;mini-batch learning yet&quot;);
-
+        // Train all layers simultaneously AND fine-tuning as well!
         if( report_progress &amp;&amp; stage &lt; nstages )
             pb = new ProgressBar( &quot;Training &quot;+classname(),
                                   nstages - stage );
 
         for( ; stage&lt;nstages; stage++)
         {
-            int sample = stage % nsamples;
-            train_set-&gt;getExample(sample, input, target, weight);
-            onlineStep( input, target, train_costs );
-            train_stats-&gt;update( train_costs );
+            // Do a step every 'minibatch_size' examples.
+            if (stage % minibatch_size == 0) {
+                int sample_start = stage % nsamples;
+                if (batch_size &gt; 1 || minibatch_hack) {
+                    train_set-&gt;getExamples(sample_start, minibatch_size,
+                            inputs, targets, weights, NULL, true);
+                    onlineStep( inputs, targets, train_costs_m );
+                } else {
+                    train_set-&gt;getExample(sample_start, input, target, weight);
+                    onlineStep( input, target, train_costs );
+                }
+            }
             if( pb )
                 pb-&gt;update( stage + 1 );
         }
     }
-    else // by stages
+    else // Greedy learning, one layer at a time.
     {
         /***** initial greedy training *****/
         for( int i=0 ; i&lt;n_layers-1 ; i++ )
@@ -834,7 +841,7 @@
 void DeepBeliefNet::onlineStep( const Vec&amp; input, const Vec&amp; target,
                                 Vec&amp; train_costs)
 {
-    PLASSERT_MSG(batch_size == 1, &quot;Not implemented for mini-batches&quot;);
+    PLASSERT(batch_size == 1);
 
     TVec&lt;Vec&gt; cost;
     if (partial_costs)
@@ -842,7 +849,7 @@
 
     layers[0]-&gt;expectation &lt;&lt; input;
     // FORWARD PHASE
-    Vec layer_input;
+    //Vec layer_input;
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
         // mean-field fprop from layer i to layer i+1
@@ -1040,6 +1047,232 @@
 
 }
 
+void DeepBeliefNet::onlineStep(const Mat&amp; inputs, const Mat&amp; targets,
+                               Mat&amp; train_costs)
+{
+    // TODO Can we avoid this memory allocation?
+    TVec&lt;Mat&gt; cost;
+    Vec optimized_cost;
+    if (partial_costs) {
+        cost.resize(n_layers-1);
+        optimized_cost.resize(inputs.length());
+    }
+
+    layers[0]-&gt;setExpectations(inputs);
+    // FORWARD PHASE
+    //Vec layer_input;
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        // mean-field fprop from layer i to layer i+1
+        connections[i]-&gt;setAsDownInputs( layers[i]-&gt;getExpectations() );
+        // this does the actual matrix-vector computation
+        layers[i+1]-&gt;getAllActivations( connections[i] );
+        layers[i+1]-&gt;computeExpectations();
+
+        // propagate into local cost associated to output of layer i+1
+        if( partial_costs &amp;&amp; partial_costs[ i ] )
+        {
+            partial_costs[ i ]-&gt;fprop( layers[ i+1 ]-&gt;getExpectations(),
+                                       targets, cost[i] );
+
+            // Backward pass
+            // first time we set these gradients: do not accumulate
+            optimized_cost &lt;&lt; cost[i].column(0); // TODO Can we optimize?
+            partial_costs[ i ]-&gt;bpropUpdate( layers[ i+1 ]-&gt;getExpectations(),
+                                             targets, optimized_cost,
+                                             expectations_gradients[ i+1 ] );
+
+            for (int k = 0; k &lt; inputs.length(); k++)
+                for (int j=0;j&lt;partial_cost_indices[i].length();j++)
+                    train_costs(k, partial_cost_indices[i][j]) = cost[i](k, j);
+        }
+        else
+            expectations_gradients[i+1].clear();
+/* TODO Remove if not used? Or implement if we want to do it?
+        if( reconstruct_layerwise )
+        {
+            // layer_input, reconstruction_cost_indices
+            layer_input.resize(layers[i]-&gt;size);
+            layer_input &lt;&lt; layers[i]-&gt;expectation; // fpropNLL writes in expectation
+            connections[i]-&gt;setAsUpInput( layers[i+1]-&gt;expectation );
+            layers[i]-&gt;getAllActivations( connections[i] );
+            real rc = train_costs[reconstruction_cost_indices[i+1]] 
+                = layer[i]-&gt;fpropNLL( layer_input ); // or use a NLLCostModule::fprop
+            train_costs[reconstruction_cost_indices[0]] +=
+                train_costs[reconstruction_cost_indices[i+1]];
+            ... layers[i]-&gt;bpropNLL( layer_input, rc, bias_gradient );
+            layers[i]-&gt;expectation &lt;&lt; layer_input;
+        }
+*/
+    }
+
+    // top layer may be connected to a final_module followed by a
+    // final_cost and / or may be used to predict class probabilities
+    // through a joint classification_module
+
+    if ( final_cost )
+    {
+        if( final_module )
+        {
+                final_module-&gt;fprop( layers[ n_layers-1 ]-&gt;getExpectations(),
+                        final_cost_inputs );
+                final_cost-&gt;fprop( final_cost_inputs, targets,
+                        final_cost_values );
+                optimized_cost &lt;&lt; final_cost_values.column(0); // TODO optimize
+                final_cost-&gt;bpropUpdate( final_cost_inputs, targets,
+                        optimized_cost,
+                        final_cost_gradients );
+
+                final_module-&gt;bpropUpdate(
+                        layers[ n_layers-1 ]-&gt;getExpectations(),
+                        final_cost_inputs,
+                        expectations_gradients[ n_layers-1 ],
+                        final_cost_gradients, true );
+        }
+        else
+        {
+                final_cost-&gt;fprop( layers[ n_layers-1 ]-&gt;getExpectations(),
+                        targets,
+                        final_cost_values );
+                optimized_cost &lt;&lt; final_cost_values.column(0); // TODO optimize
+                final_cost-&gt;bpropUpdate( layers[n_layers-1]-&gt;getExpectations(),
+                        targets, optimized_cost,
+                        expectations_gradients[n_layers-1],
+                        true);
+        }
+
+        for (int k = 0; k &lt; inputs.length(); k++)
+            for (int j=0;j&lt;final_cost_indices.length();j++)
+                train_costs(k, final_cost_indices[j]) = final_cost_values(k, j);
+    }
+
+    if ( final_cost || (partial_costs &amp;&amp; partial_costs[n_layers-2]) )
+    {
+        layers[n_layers-1]-&gt;setLearningRate( grad_learning_rate );
+        connections[n_layers-2]-&gt;setLearningRate( grad_learning_rate );
+
+        layers[ n_layers-1 ]-&gt;bpropUpdate(
+                layers[ n_layers-1 ]-&gt;activations,
+                layers[ n_layers-1 ]-&gt;getExpectations(),
+                activations_gradients[ n_layers-1 ],
+                expectations_gradients[ n_layers-1 ],
+                false);
+
+        connections[ n_layers-2 ]-&gt;bpropUpdate(
+                layers[ n_layers-2 ]-&gt;getExpectations(),
+                layers[ n_layers-1 ]-&gt;activations,
+                expectations_gradients[ n_layers-2 ],
+                activations_gradients[ n_layers-1 ],
+                true);
+        // accumulate into expectations_gradients[n_layers-2]
+        // because a partial cost may have already put a gradient there
+    }
+
+    if( use_classification_cost )
+    {
+        PLERROR(&quot;In DeepBeliefNet::onlineStep - 'use_classification_cost' not &quot;
+                &quot;implemented for mini-batches&quot;);
+
+        /*
+        classification_module-&gt;fprop( layers[ n_layers-2 ]-&gt;expectation,
+                                      class_output );
+        real nll_cost;
+
+        // This doesn't work. gcc bug?
+        // classification_cost-&gt;fprop( class_output, target, cost );
+        classification_cost-&gt;CostModule::fprop( class_output, target,
+                                                nll_cost );
+
+        real class_error =
+            ( argmax(class_output) == (int) round(target[0]) ) ? 0: 1;
+
+        train_costs[nll_cost_index] = nll_cost;
+        train_costs[class_cost_index] = class_error;
+
+        classification_cost-&gt;bpropUpdate( class_output, target, nll_cost,
+                                          class_gradient );
+
+        classification_module-&gt;bpropUpdate( layers[ n_layers-2 ]-&gt;expectation,
+                                            class_output,
+                                            expectation_gradients[n_layers-2],
+                                            class_gradient,
+                                            true );
+        if( top_layer_joint_cd )
+        {
+            // set the input of the joint layer
+            Vec target_exp = classification_module-&gt;target_layer-&gt;expectation;
+            fill_one_hot( target_exp, (int) round(target[0]), real(0.), real(1.) );
+
+            joint_layer-&gt;setLearningRate( cd_learning_rate );
+            layers[ n_layers-1 ]-&gt;setLearningRate( cd_learning_rate );
+            classification_module-&gt;joint_connection-&gt;setLearningRate(
+                cd_learning_rate );
+
+            save_layer_activation.resize(layers[ n_layers-2 ]-&gt;size);
+            save_layer_activation &lt;&lt; layers[ n_layers-2 ]-&gt;activation;
+            save_layer_expectation.resize(layers[ n_layers-2 ]-&gt;size);
+            save_layer_expectation &lt;&lt; layers[ n_layers-2 ]-&gt;expectation;
+
+            contrastiveDivergenceStep(
+                get_pointer(joint_layer),
+                get_pointer(classification_module-&gt;joint_connection),
+                layers[ n_layers-1 ], n_layers-2);
+
+            layers[ n_layers-2 ]-&gt;activation &lt;&lt; save_layer_activation;
+            layers[ n_layers-2 ]-&gt;expectation &lt;&lt; save_layer_expectation;
+        }
+        */
+    }
+
+
+    // DOWNWARD PHASE (the downward phase for top layer is already done above)
+    for( int i=n_layers-3 ; i&gt;=0 ; i-- )
+    {
+        connections[ i ]-&gt;setLearningRate( grad_learning_rate );
+        layers[ i+1 ]-&gt;setLearningRate( grad_learning_rate );
+
+        layers[i+1]-&gt;bpropUpdate( layers[i+1]-&gt;activations,
+                                  layers[i+1]-&gt;getExpectations(),
+                                  activations_gradients[i+1],
+                                  expectations_gradients[i+1] );
+
+        connections[i]-&gt;bpropUpdate( layers[i]-&gt;getExpectations(),
+                                     layers[i+1]-&gt;activations,
+                                     expectations_gradients[i],
+                                     activations_gradients[i+1],
+                                     true);
+
+        // N.B. the contrastiveDivergenceStep changes the activation and
+        // expectation fields of top layer of the RBM, so it must be
+        // done last
+        layers[i]-&gt;setLearningRate( cd_learning_rate );
+        layers[i+1]-&gt;setLearningRate( cd_learning_rate );
+        connections[i]-&gt;setLearningRate( cd_learning_rate );
+
+        if( i &gt; 0 )
+        {
+            const Mat&amp; source_act = layers[i]-&gt;activations;
+            save_layer_activations.resize(source_act.length(),
+                                          source_act.width());
+            save_layer_activation &lt;&lt; source_act;
+            const Mat&amp; source_exp = layers[i]-&gt;getExpectations();
+            save_layer_expectation.resize(source_exp.length(),
+                                          source_exp.width());
+            save_layer_expectation &lt;&lt; source_exp;
+        }
+        contrastiveDivergenceStep( layers[ i ],
+                                   connections[ i ],
+                                   layers[ i+1 ] ,
+                                   i, true);
+        if( i &gt; 0 )
+        {
+            layers[i]-&gt;activations &lt;&lt; save_layer_activations;
+            layers[i]-&gt;getExpectations() &lt;&lt; save_layer_expectations;
+        }
+    }
+
+}
+
 ////////////////
 // greedyStep //
 ////////////////

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-05-03 05:17:49 UTC (rev 6985)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-05-03 15:10:36 UTC (rev 6986)
@@ -220,6 +220,8 @@
 
     void onlineStep( const Vec&amp; input, const Vec&amp; target, Vec&amp; train_costs );
 
+    void onlineStep( const Mat&amp; inputs, const Mat&amp; targets, Mat&amp; train_costs );
+
     void greedyStep( const Vec&amp; input, const Vec&amp; target, int index );
 
     //! Greedy step with mini-batches.
@@ -313,9 +315,13 @@
     //! buffers bottom layer activation during onlineStep 
     mutable Vec save_layer_activation;
 
+    Mat save_layer_activations; //!&lt; For mini-batches.
+
     //! buffers bottom layer expectation during onlineStep 
     mutable Vec save_layer_expectation;
 
+    Mat save_layer_expectations; //!&lt; For mini-batches.
+
     //! Does final_module exist and have a &quot;learning_rate&quot; option
     bool final_module_has_learning_rate;
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000434.html">[Plearn-commits] r6985 - trunk/plearn/math
</A></li>
	<LI>Next message: <A HREF="000436.html">[Plearn-commits] r6987 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#435">[ date ]</a>
              <a href="thread.html#435">[ thread ]</a>
              <a href="subject.html#435">[ subject ]</a>
              <a href="author.html#435">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
