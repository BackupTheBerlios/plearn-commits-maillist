<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7434 - in trunk/python_modules/plearn: . learners	learners/autolr pyext
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7434%20-%20in%20trunk/python_modules/plearn%3A%20.%20learners%0A%09learners/autolr%20pyext&In-Reply-To=%3C200705301702.l4UH2j5u023539%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000882.html">
   <LINK REL="Next"  HREF="000884.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7434 - in trunk/python_modules/plearn: . learners	learners/autolr pyext</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7434%20-%20in%20trunk/python_modules/plearn%3A%20.%20learners%0A%09learners/autolr%20pyext&In-Reply-To=%3C200705301702.l4UH2j5u023539%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7434 - in trunk/python_modules/plearn: . learners	learners/autolr pyext">yoshua at mail.berlios.de
       </A><BR>
    <I>Wed May 30 19:02:45 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000882.html">[Plearn-commits] r7433 - in	trunk/plearn_learners/online/test/ModuleTester: .	.pytest/PL_ModuleTester_Costs/expected_results
</A></li>
        <LI>Next message: <A HREF="000884.html">[Plearn-commits] r7435 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#883">[ date ]</a>
              <a href="thread.html#883">[ thread ]</a>
              <a href="subject.html#883">[ subject ]</a>
              <a href="author.html#883">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-05-30 19:02:28 +0200 (Wed, 30 May 2007)
New Revision: 7434

Added:
   trunk/python_modules/plearn/learners/
   trunk/python_modules/plearn/learners/__init__.py
   trunk/python_modules/plearn/learners/autolr/
   trunk/python_modules/plearn/learners/autolr/__init__.py
Modified:
   trunk/python_modules/plearn/pyext/__init__.py
Log:
Automatic learning rate adaptation code added in plearn.learners.autolr


Added: trunk/python_modules/plearn/learners/__init__.py
===================================================================

Added: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-05-30 16:47:40 UTC (rev 7433)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-05-30 17:02:28 UTC (rev 7434)
@@ -0,0 +1,212 @@
+from math import *
+
+# YET TO BE DOCUMENTED AND CLEANED UP CODE
+
+def deepcopy(plearnobject): # ugly way to copy until done properly with PLearn's deepcopy
+    # actually not a deep-copy, only copy options
+    if useserver:
+        raise NotImplementedError
+    else:
+        return newObject(str(plearnobject))
+
+
+def train_with_schedule(learner,
+                        lr_options, # e.g. [&quot;module.modules[0].cd_learning_rate&quot; &quot;module.modules[1].cd_learning_rate&quot;]
+                        learning_rates, # list of learning rates to try
+                        stages,         # corresponding list of stages associated with each above learning rate
+                        trainset,testsets,expdir,
+                        cost_to_select_best=0,
+                        selected_costnames = False):
+    costnames = learner.getTestCostNames()
+    if not selected_costnames:
+        # use all cost names if not user-provided
+        selected_costnames=costnames
+    cost_indices = [costnames.index(name) for name in selected_costnames]
+    learner.setTrainingSet(trainset,False)
+    n_train = len(learning_rates)
+    n_tests = len(testsets)
+    n_costs = len(costnames)
+    results = zeros([n_train,1+n_tests*n_costs],Float32)
+    best_err = 1e10
+    if interactive:
+        clf()
+    colors=&quot;bgrcmyk&quot;
+    styles=['-', '--', '-.', ':', '.', ',', 'o', '^', 'v', '&lt;', '&gt;', 's', '+', 'x', 'D']
+    for i in range(n_train):
+        learner.nstages = stages[i]
+        for lr_option in lr_options:
+            learner.changeOptions({lr_option:str(learning_rates[i])})
+        learner.train()
+        results[i,0] = learner.stage
+        for j in range(0,n_tests):
+            ts = pl.VecStatsCollector()
+            learner.test(testsets[j],ts,0,0)
+            print &gt;&gt;logfile, &quot;At stage &quot;,learner.stage,&quot; test&quot; + str(j+1),&quot;: &quot;,
+            for k in range(0,n_costs):
+                err = ts.getStat(&quot;E[&quot;+str(k)+&quot;]&quot;)
+                results[i,j*n_costs+k+1]=err
+                costname = costnames[cost_indices[k]]
+                print &gt;&gt;logfile, costname, &quot;=&quot;, err,
+                if k==cost_to_select_best and j==0 and err &lt; best_err:
+                    best_err = err
+                    learner.save(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;,&quot;plearn_ascii&quot;)
+                if interactive:
+                    plot(results[0:i+1,0],results[0:i+1,
+                         j*n_costs+k+1],colors[k%7]+styles[j%15],
+                         label='test'+str(j+1)+':'+costname)
+            print &gt;&gt;logfile
+        if interactive and i==0:
+            legend()
+    return (['stage']+selected_costnames,results)
+
+
+def choose_initial_lr(learner,trainset,testset,lr_option,
+                      nstages=10000,
+                      cost_to_select_best=0,
+                      initial_lr=0.01,
+                      lr_steps=exp(log(10)/2)):
+    log_initial_lr=log(initial_lr)
+    log_steps=log(lr_steps)
+
+    def lr(i):
+        return exp(log_initial_lr+i*log_steps)
+
+    def perf(i):
+        learner.setTrainingSet(trainset,True) # call forget()
+        learner.changeOptions({lr_option:str(lr(i))})
+        learner.nstages = nstages
+        learner.train()
+        ts = pl.VecStatsCollector()
+        learner.test(testset,ts,0,0)
+        return ts.getStat(&quot;E[&quot;+str(cost_to_select_best)+&quot;]&quot;)
+
+    perfs = {}
+    top = 1
+    perfs[top]=perf(top)
+    bottom = 0
+    perfs[bottom]=perf(bottom)
+    if perfs[bottom]&lt;perfs[top]:
+        best=bottom
+    else:
+        best=top
+    
+    while top-bottom&lt;2 or (best==top or best==bottom):
+        if best==top:
+            top+=1
+            perfs[top]=perf(top)
+            if perfs[top]&lt;perfs[best]:
+                best=top
+        if best==bottom:
+            bottom-=1
+            perfs[bottom]=perf(bottom)
+            if perfs[bottom]&lt;perfs[best]:
+                best=bottom
+    return (lr(best),perfs)
+
+    
+def train_adapting_lr(learner,
+                      epoch,nstages,
+                      trainset,testsets,expdir,
+                      lr_options,
+                      initial_lr=0.1,
+                      nskip=2,
+                      cost_to_select_best=0,
+                      save_best=False,
+                      selected_costnames = False,
+                      lr_steps=exp(log(10)/2)):
+    costnames = learner.getTestCostNames()
+    if not selected_costnames:
+        # use all cost names if not user-provided
+        selected_costnames=costnames
+    cost_indices = [costnames.index(name) for name in selected_costnames]
+    n_tests = len(testsets)
+    n_costs = len(costnames)
+    best_err = 1e10
+    previous_best_err = best_err
+    best_active = -1
+    n_epochs=nstages/epoch
+    all_results = [zeros([n_epochs,2+n_tests*n_costs],Float32)]
+    all_candidates = [learner]
+    all_last_err = [best_err]
+    all_slope = [0]
+    all_lr = [initial_lr]
+    all_start = [0]
+    actives = [0]
+    if interactive:
+        clf()
+        colors=&quot;bgrcmyk&quot;
+        styles=['-', '--', '-.', ':', '.', ',', 'o', '^', 'v', '&lt;', '&gt;', 's', '+', 'x', 'D']
+    initial_stage=learner.stage
+    for (s,i) in zip(range(epoch,nstages+epoch,epoch),range(n_epochs)):
+        print &gt;&gt;logfile, &quot;At stage &quot;, initial_stage+s
+        for active in actives:
+            candidate = all_candidates[active]
+            results = all_results[active]
+            candidate.nstages = initial_stage+s
+            for lr_option in lr_options:
+                candidate.changeOptions({lr_option:str(all_lr[active])})
+            candidate.setTrainingSet(trainset,False)
+            candidate.train()
+            results[i,0] = candidate.stage
+            results[i,1] = all_lr[active]
+            print &gt;&gt;logfile, &quot;candidate &quot;,active,&quot;:&quot;,
+            for j in range(0,n_tests):
+                ts = pl.VecStatsCollector()
+                candidate.test(testsets[j],ts,0,0)
+                print &gt;&gt;logfile, &quot; test&quot; + str(j+1),&quot;: &quot;,
+                for k in range(0,n_costs):
+                    err = ts.getStat(&quot;E[&quot;+str(k)+&quot;]&quot;)
+                    results[i,j*n_costs+k+2]=err
+                    costname = costnames[cost_indices[k]]
+                    print &gt;&gt;logfile, costname, &quot;=&quot;, err,
+                    if k==cost_to_select_best and j==0:
+                        if interactive:
+                            start = all_start[active]
+                            plot(results[start:i+1,0],
+                                 results[start:i+1,j*n_costs+k+2],colors[active%7]+styles[j%15],
+                                 label='candidate'+str(active)+':'+costname)
+                            if active==0 and i==0:
+                                legend()
+                        if s&gt;epoch:
+                                all_slope[active]=0.5*all_slope[active]+0.5*(err-all_last_err[active])
+                        all_last_err[active]=err
+                        if err &lt; best_err:
+                            best_err = err
+                            best_active = active
+                            if save_best:
+                                candidate.save(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;,&quot;plearn_binary&quot;)
+            print &gt;&gt;logfile
+            logfile.flush()
+        if previous_best_err &gt; best_err:
+            previous_best_err = best_err
+            print &gt;&gt;logfile,&quot;BEST to now is candidate &quot;,best_active,&quot; with err=&quot;,best_err
+        if s%(epoch*nskip)==0 and s&lt;nstages:
+            best_active = argmin(all_last_err)
+            # remove candidates that are worse and have higher slope
+            best_slope = all_slope[best_active]
+            best_last = all_last_err[best_active]
+            ndeleted = 0
+            for (a,i) in zip(actives,range(len(actives))):
+                if a!=best_active and all_last_err[a]&gt;best_last and all_slope[a]&gt;=best_slope:
+                    print &gt;&gt;logfile,&quot;REMOVE candidate &quot;,a
+                    all_candidates[a]=None # hopefully this destroys the candidate
+                    del actives[i-ndeleted]
+                    ndeleted+=1
+            # add a candidate with slightly lower learning rate than best_active, starting from it
+            new_candidate = deepcopy(all_candidates[best_active])
+            new_i = len(all_candidates)
+            actives.append(new_i)
+            all_candidates.append(new_candidate)
+            all_results.append(all_results[best_active].copy())
+            all_last_err.append(best_last)
+            all_slope.append(best_slope)
+            all_lr.append(all_lr[best_active]/lr_steps) # always try a smaller learning rate
+            all_start.append(i)
+            print &gt;&gt;logfile,&quot;CREATE candidate &quot;, new_i, &quot; from &quot;,best_active,&quot;at epoch &quot;,s,&quot; with lr=&quot;,all_lr[new_i]
+            logfile.flush()
+    if save_best:
+        final_model = loadObject(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;)
+    else:
+        final_model = all_candidates[best_active]
+    return (final_model,all_results[best_active],all_results,all_last_err,all_start)
+

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2007-05-30 16:47:40 UTC (rev 7433)
+++ trunk/python_modules/plearn/pyext/__init__.py	2007-05-30 17:02:28 UTC (rev 7434)
@@ -30,13 +30,9 @@
 #  This file is part of the PLearn library. For more information on the PLearn
 #  library, go to the PLearn Web site at www.plearn.org
 
-if globals().has_key('plextdbg'):
-   if plextdbg:
-      from plearn.pyext.plextdbg import *
-   else:
-      from plearn.pyext.plextopt import *
-else:
-   from plearn.pyext.plext import *
+import plearn.plextmode
+from plearn.pyext.plext import *
+   
 from plearn.pyplearn.plargs import *
 import cgitb
 cgitb.enable(format='PLearn')


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000882.html">[Plearn-commits] r7433 - in	trunk/plearn_learners/online/test/ModuleTester: .	.pytest/PL_ModuleTester_Costs/expected_results
</A></li>
	<LI>Next message: <A HREF="000884.html">[Plearn-commits] r7435 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#883">[ date ]</a>
              <a href="thread.html#883">[ thread ]</a>
              <a href="subject.html#883">[ subject ]</a>
              <a href="author.html#883">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
