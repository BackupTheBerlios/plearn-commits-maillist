<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7530 - trunk/python_modules/plearn/learners/autolr
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7530%20-%20trunk/python_modules/plearn/learners/autolr&In-Reply-To=%3C200706042210.l54MADGQ013912%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000978.html">
   <LINK REL="Next"  HREF="000980.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7530 - trunk/python_modules/plearn/learners/autolr</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7530%20-%20trunk/python_modules/plearn/learners/autolr&In-Reply-To=%3C200706042210.l54MADGQ013912%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7530 - trunk/python_modules/plearn/learners/autolr">yoshua at mail.berlios.de
       </A><BR>
    <I>Tue Jun  5 00:10:13 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000978.html">[Plearn-commits] r7529 - trunk/python_modules/plearn/pyplearn
</A></li>
        <LI>Next message: <A HREF="000980.html">[Plearn-commits] r7531 - trunk/python_modules/plearn/learners/autolr
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#979">[ date ]</a>
              <a href="thread.html#979">[ thread ]</a>
              <a href="subject.html#979">[ subject ]</a>
              <a href="author.html#979">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-06-05 00:10:13 +0200 (Tue, 05 Jun 2007)
New Revision: 7530

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
added save every x epoch option in autolr


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 21:22:42 UTC (rev 7529)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-06-04 22:10:13 UTC (rev 7530)
@@ -223,11 +223,12 @@
                       nskip=2,   # Number of epochs after which we add/remove candidates
                       cost_to_select_best=0, # Index of cost being optimized
                       return_best_model=False, # o/w return final model
-                      save_best=False, # for paranoids: save best model each time an improvement is found
+                      save_best=False, # for paranoids: save best model every save_best epochs, or not at all (if = False)
                       selected_costnames = False,
                       min_epochs_to_delete = 2,
                       lr_steps=exp(log(10)/2), # Scaling coefficient when modifying learning rates
                       logfile=False,
+                      min_lr=1e-6, # do not try to go below this learning rate
                       keep_lr=2): # Learning rate interval for heuristic
 
     min_epochs_to_delete = max(1,min_epochs_to_delete) # although 1 is probably too small
@@ -256,7 +257,8 @@
         for a in actives:
             if all_lr[a]==c2lr and all_last_err[a]&lt;=c2err:
                 return True # throw it away if worse than other actives of same lr
-            if a!=c2 and abs(log(all_lr[a]/c2lr))&lt;keep_lr*log(lr_steps):
+            # say that it is alone if there are no other actives with nearby and greater lr
+            if a!=c2 and all_lr[a]&gt;c2lr and abs(log(all_lr[a]/c2lr))&lt;keep_lr*log(lr_steps):
                 alone=False
         c1lr=all_lr[c1]
         if alone and c2lr&gt;c1lr: # and slope2&lt;0: 
@@ -361,11 +363,11 @@
                             best_early_stop = stage
                             if return_best_model:
                                 best_candidate = deepcopy(candidate)
-                            if save_best: 
-                                candidate.save(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;,&quot;plearn_binary&quot;)
             if logfile:
                 print &gt;&gt;logfile
                 logfile.flush()
+        if save_best and t%save_best==0:
+            all_candidates[best_active].save(expdir+&quot;/&quot;+&quot;best_learner.psave&quot;,&quot;plearn_binary&quot;)
         if previous_best_err &gt; best_err:
             previous_best_err = best_err
             if logfile:
@@ -397,18 +399,19 @@
                     del actives[j-ndeleted]
                     ndeleted+=1
             # add a candidate with slightly lower learning rate than best_active, starting from it
-            new_candidate = deepcopy(all_candidates[best_active])
-            new_a = len(all_candidates)
-            actives.append(new_a)
-            all_candidates.append(new_candidate)
-            all_results.append(all_results[best_active].copy())
-            all_last_err.append(best_last)
-            #all_slope.append(best_slope)
-            all_lr.append(all_lr[best_active]/lr_steps) # always try a smaller learning rate
-            all_start.append(t)
-            if logfile:
-                print &gt;&gt;logfile,&quot;CREATE candidate &quot;, new_a, &quot; from &quot;,best_active,&quot;at epoch &quot;,s,&quot; with lr=&quot;,all_lr[new_a]
-                logfile.flush()
+            new_lr=all_lr[best_active]/lr_steps # only try a smaller learning rate
+            if new_lr&gt;=min_lr:
+                all_lr.append(new_lr)
+                new_candidate = deepcopy(all_candidates[best_active])
+                new_a = len(all_candidates)
+                actives.append(new_a)
+                all_candidates.append(new_candidate)
+                all_results.append(all_results[best_active].copy())
+                all_last_err.append(best_last)
+                all_start.append(t)
+                if logfile:
+                    print &gt;&gt;logfile,&quot;CREATE candidate &quot;, new_a, &quot; from &quot;,best_active,&quot;at epoch &quot;,s,&quot; with lr=&quot;,all_lr[new_a]
+                    logfile.flush()
     if return_best_model:
         final_model = best_candidate
     else:


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000978.html">[Plearn-commits] r7529 - trunk/python_modules/plearn/pyplearn
</A></li>
	<LI>Next message: <A HREF="000980.html">[Plearn-commits] r7531 - trunk/python_modules/plearn/learners/autolr
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#979">[ date ]</a>
              <a href="thread.html#979">[ thread ]</a>
              <a href="subject.html#979">[ subject ]</a>
              <a href="author.html#979">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
