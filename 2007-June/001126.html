<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7678 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7678%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200706292159.l5TLxD8m011637%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001125.html">
   <LINK REL="Next"  HREF="001127.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7678 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7678%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200706292159.l5TLxD8m011637%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7678 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Fri Jun 29 23:59:13 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001125.html">[Plearn-commits] r7677 - tags
</A></li>
        <LI>Next message: <A HREF="001127.html">[Plearn-commits] r7679 - in trunk/plearn_learners/online/test: DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0 DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0 DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester ModuleLearner/.pytest/PL_ModuleLearn! er_Greedy/expected_results/expdir-tester/Split0 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1126">[ date ]</a>
              <a href="thread.html#1126">[ thread ]</a>
              <a href="subject.html#1126">[ subject ]</a>
              <a href="author.html#1126">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-06-29 23:59:11 +0200 (Fri, 29 Jun 2007)
New Revision: 7678

Modified:
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMClassificationModule.cc
   trunk/plearn_learners/online/RBMConv2DConnection.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
Log:
Change convention for energy sign in RBMs.
Now, it is:
  E = -h' b - h' W v - c' v


Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -94,7 +94,6 @@
     cost.resize( output_size );
 
     if( input.hasMissing() )
-        // TODO: should we put something else? infinity?
         cost[0] = MISSING_VALUE;
     else
     {
@@ -153,10 +152,7 @@
         for( int i=0; i&lt;batch_size; i++ )
         {
             if( (*prediction)(i).hasMissing() )
-            {
-                // TODO: should we put something else? infinity?
                 (*cost)(i,0) = MISSING_VALUE;
-            }
             else
             {
 #ifdef BOUNDCHECK
@@ -168,8 +164,8 @@
                 if (!is_equal( sum((*prediction)(i)), 1., 1., 1e-5, 1e-5 ))
                     PLERROR(&quot;In NLLCostModule::fprop - Elements of&quot;
                             &quot; \&quot;prediction\&quot; should sum to 1&quot;
-                            &quot; (found a sum = %f)&quot;,
-                            sum((*prediction)(i)));
+                            &quot; (found a sum = %f at row %d)&quot;,
+                            sum((*prediction)(i)), i);
 #endif
                 int target_i = (int) round( (*target)(i,0) );
                 PLASSERT( is_equal( (*target)(i, 0), target_i ) );

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -115,10 +115,10 @@
 
     if (use_fast_approximations)
         for( int i=0 ; i&lt;size ; i++ )
-            expectation[i] = fastsigmoid( -activation[i] );
+            expectation[i] = fastsigmoid( activation[i] );
     else
         for( int i=0 ; i&lt;size ; i++ )
-            expectation[i] = sigmoid( -activation[i] );
+            expectation[i] = sigmoid( activation[i] );
 
     expectation_is_up_to_date = true;
 }
@@ -136,11 +136,11 @@
     if (use_fast_approximations)
         for (int k = 0; k &lt; batch_size; k++)
             for (int i = 0 ; i &lt; size ; i++)
-                expectations(k, i) = fastsigmoid(-activations(k, i));
+                expectations(k, i) = fastsigmoid(activations(k, i));
     else
         for (int k = 0; k &lt; batch_size; k++)
             for (int i = 0 ; i &lt; size ; i++)
-                expectations(k, i) = sigmoid(-activations(k, i));
+                expectations(k, i) = sigmoid(activations(k, i));
 
     expectations_are_up_to_date = true;
 }
@@ -155,10 +155,10 @@
 
     if (use_fast_approximations)
         for( int i=0 ; i&lt;size ; i++ )
-            output[i] = fastsigmoid( -input[i] - bias[i] );
+            output[i] = fastsigmoid( input[i] + bias[i] );
     else
         for( int i=0 ; i&lt;size ; i++ )
-            output[i] = sigmoid( -input[i] - bias[i] );
+            output[i] = sigmoid( input[i] + bias[i] );
 }
 
 void RBMBinomialLayer::fprop( const Mat&amp; inputs, Mat&amp; outputs ) const
@@ -170,11 +170,11 @@
     if (use_fast_approximations)
         for( int k = 0; k &lt; mbatch_size; k++ )
             for( int i = 0; i &lt; size; i++ )
-                outputs(k,i) = fastsigmoid( -inputs(k,i) - bias[i] );
+                outputs(k,i) = fastsigmoid( inputs(k,i) + bias[i] );
     else
         for( int k = 0; k &lt; mbatch_size; k++ )
             for( int i = 0; i &lt; size; i++ )
-                outputs(k,i) = sigmoid( -inputs(k,i) - bias[i] );
+                outputs(k,i) = sigmoid( inputs(k,i) + bias[i] );
 }
 
 void RBMBinomialLayer::fprop( const Vec&amp; input, const Vec&amp; rbm_bias,
@@ -186,10 +186,10 @@
 
     if (use_fast_approximations)
         for( int i=0 ; i&lt;size ; i++ )
-            output[i] = fastsigmoid( -input[i] - rbm_bias[i]);
+            output[i] = fastsigmoid( input[i] + rbm_bias[i]);
     else
         for( int i=0 ; i&lt;size ; i++ )
-            output[i] = sigmoid( -input[i] - rbm_bias[i]);
+            output[i] = sigmoid( input[i] + rbm_bias[i]);
 }
 
 /////////////////
@@ -221,7 +221,7 @@
     for( int i=0 ; i&lt;size ; i++ )
     {
         real output_i = output[i];
-        real in_grad_i = - output_i * (1-output_i) * output_gradient[i];
+        real in_grad_i = output_i * (1-output_i) * output_gradient[i];
         input_gradient[i] += in_grad_i;
 
         if( momentum == 0. )
@@ -278,7 +278,7 @@
         for( int i=0 ; i&lt;size ; i++ )
         {
             real output_i = outputs(j, i);
-            real in_grad_i = -output_i * (1-output_i) * output_gradients(j, i);
+            real in_grad_i = output_i * (1-output_i) * output_gradients(j, i);
             input_gradients(j, i) += in_grad_i;
 
             if( momentum == 0. )
@@ -317,7 +317,7 @@
     for( int i=0 ; i&lt;size ; i++ )
     {
         real output_i = output[i];
-        input_gradient[i] = - output_i * (1-output_i) * output_gradient[i];
+        input_gradient[i] = output_i * (1-output_i) * output_gradient[i];
     }
 
     rbm_bias_gradient &lt;&lt; input_gradient;
@@ -338,16 +338,15 @@
                 // nll -= target[i] * pl_log(expectations[i]); 
                 // but it is numerically unstable, so use instead
                 // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                // but note that expectation = sigmoid(-activation)
-                ret += target_i * tabulated_softplus(activation_i);
+                ret += target_i * tabulated_softplus(-activation_i);
             if(!fast_exact_is_equal(target_i,1.0))
                 // ret -= (1-target_i) * pl_log(1-expectation_i);
                 // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
                 //                         = log(1/(1+exp(x)))
                 //                         = -log(1+exp(x)) = -softplus(x)
-                ret += (1-target_i) * tabulated_softplus(-activation_i);
+                ret += (1-target_i) * tabulated_softplus(activation_i);
         }
-    }else{
+    } else {
         for( int i=0 ; i&lt;size ; i++ )
         {
             target_i = target[i];
@@ -356,14 +355,13 @@
                 // nll -= target[i] * pl_log(expectations[i]); 
                 // but it is numerically unstable, so use instead
                 // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                // but note that expectation = sigmoid(-activation)
-                ret += target_i * softplus(activation_i);
+                ret += target_i * softplus(-activation_i);
             if(!fast_exact_is_equal(target_i,1.0))
                 // ret -= (1-target_i) * pl_log(1-expectation_i);
                 // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
                 //                         = log(1/(1+exp(x)))
                 //                         = -log(1+exp(x)) = -softplus(x)
-                ret += (1-target_i) * softplus(-activation_i);
+                ret += (1-target_i) * softplus(activation_i);
         }
     }
     return ret;
@@ -390,33 +388,30 @@
                     // nll -= target[i] * pl_log(expectations[i]); 
                     // but it is numerically unstable, so use instead
                     // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                    // but note that expectation = sigmoid(-activation)
-                    nll += target[i] * tabulated_softplus(activation[i]);
+                    nll += target[i] * tabulated_softplus(-activation[i]);
                 if(!fast_exact_is_equal(target[i],1.0))
                     // nll -= (1-target[i]) * pl_log(1-output[i]);
                     // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
                     //                         = log(1/(1+exp(x)))
                     //                         = -log(1+exp(x))
                     //                         = -softplus(x)
-                    nll += (1-target[i]) * tabulated_softplus(-activation[i]);
+                    nll += (1-target[i]) * tabulated_softplus(activation[i]);
             }
-        }else{
+        } else {
             for( int i=0 ; i&lt;size ; i++ ) // loop over outputs
             {
                 if(!fast_exact_is_equal(target[i],0.0))
                     // nll -= target[i] * pl_log(expectations[i]); 
                     // but it is numerically unstable, so use instead
                     // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                    // but note that expectation = sigmoid(-activation)
-                    nll += target[i] * softplus(activation[i]);
+                    nll += target[i] * softplus(-activation[i]);
                 if(!fast_exact_is_equal(target[i],1.0))
                     // nll -= (1-target[i]) * pl_log(1-output[i]);
                     // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
                     //                         = log(1/(1+exp(x)))
                     //                         = -log(1+exp(x))
                     //                         = -softplus(x)
-                    nll += (1-target[i]) * softplus(-activation[i]);
-                
+                    nll += (1-target[i]) * softplus(activation[i]);
             }
         }
         costs_column(k,0) = nll;
@@ -430,10 +425,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    for( int i=0 ; i&lt;size ; i++ )
-    {
-        bias_gradient[i] = target[i]-expectation[i];
-    }
+    // bias_gradient = target - expectation
+    substract(target, expectation, bias_gradient);
 }
 
 void RBMBinomialLayer::bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
@@ -447,7 +440,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    substract(targets,expectations,bias_gradients);
+    // bias_gradients = targets - expectations
+    substract(targets, expectations, bias_gradients);
 }
 
 void RBMBinomialLayer::declareOptions(OptionList&amp; ol)
@@ -479,7 +473,7 @@
 
 real RBMBinomialLayer::energy(const Vec&amp; unit_values) const
 {
-    return dot(unit_values,bias);
+    return -dot(unit_values, bias);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-06-29 21:59:11 UTC (rev 7678)
@@ -117,7 +117,7 @@
     virtual void bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
                           Mat&amp; bias_gradients);
 
-    //! compute bias' unit_values
+    //! compute -bias' unit_values
     virtual real energy(const Vec&amp; unit_values) const;
 
     //#####  PLearn::Object Protocol  #########################################

Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -197,7 +197,7 @@
     last_layer-&gt;getAllActivations( previous_to_last );
 
     // target_layer-&gt;activation =
-    //      bias - sum_j softplus(-(W_ji + last_layer-&gt;activation[j]))
+    //      bias + sum_j softplus(W_ji + last_layer-&gt;activation[j])
     Vec target_act = target_layer-&gt;activation;
     for( int i=0 ; i&lt;output_size ; i++ )
     {
@@ -210,7 +210,7 @@
         for( int j=0 ; j&lt;last_size ; j++, w+=m )
         {
             // *w = weights(j,i)
-            target_act[i] -= softplus( -(*w + last_act[j]) );
+            target_act[i] += softplus(*w + last_act[j]);
         }
     }
 
@@ -268,7 +268,7 @@
         for( int k=0 ; k&lt;output_size ; k++ )
         {
             // dC/d( w_ik + target_act_i )
-            real d_z = d_target_act[k]*(sigmoid(-w[k] - last_act[i]));
+            real d_z = d_target_act[k]*(sigmoid(w[k] + last_act[i]));
             w[k] -= last_to_target-&gt;learning_rate * d_z;
 
             d_last_act[i] += d_z;

Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -218,10 +218,10 @@
 void RBMConv2DConnection::update()
 {
     // updates parameters
-    // kernel -= learning_rate * (kernel_pos_stats/pos_count
+    // kernel += learning_rate * (kernel_pos_stats/pos_count
     //                              - kernel_neg_stats/neg_count)
-    real pos_factor = -learning_rate / pos_count;
-    real neg_factor = learning_rate / neg_count;
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = -learning_rate / neg_count;
 
     real* k_i = kernel.data();
     real* kps_i = kernel_pos_stats.data();
@@ -279,7 +279,7 @@
      *   for j=0 to up_image_width:
      *     for l=0 to kernel_length:
      *       for m=0 to kernel_width:
-     *         kernel_neg_stats(l,m) -= learning_rate *
+     *         kernel_neg_stats(l,m) += learning_rate *
      *           ( pos_down_image(step1*i+l,step2*j+m) * pos_up_image(i,j)
      *             - neg_down_image(step1*i+l,step2*j+m) * neg_up_image(i,j) )
      */
@@ -315,7 +315,7 @@
                                                ndv2+=down_image_width )
                     for( int m=0; m&lt;kernel_width; m++ )
                         k[m] += learning_rate *
-                            (ndv2[m] * nuv_ij - pdv2[m] * puv_ij);
+                            (pdv2[m] * puv_ij - ndv2[m] * nuv_ij);
             }
         }
     }
@@ -348,7 +348,7 @@
                                                pdv2+=down_image_width,
                                                ndv2+=down_image_width )
                     for( int m=0; m&lt;kernel_width; m++ )
-                        kinc[m] += ndv2[m] * nuv_ij - pdv2[m] * puv_ij;
+                        kinc[m] += pdv2[m] * puv_ij - ndv2[m] * nuv_ij;
             }
         }
         multiplyAcc( kernel, kernel_inc, learning_rate );
@@ -376,7 +376,7 @@
      *   for j=0 to up_image_width:
      *     for l=0 to kernel_length:
      *       for m=0 to kernel_width:
-     *         kernel_neg_stats(l,m) -= learning_rate *
+     *         kernel_neg_stats(l,m) += learning_rate *
      *           ( pos_down_image(step1*i+l,step2*j+m) * pos_up_image(i,j)
      *             - neg_down_image(step1*i+l,step2*j+m) * neg_up_image(i,j) )
      */
@@ -415,7 +415,7 @@
                                                    ndv2+=down_image_width )
                         for( int m=0; m&lt;kernel_width; m++ )
                             k[m] += norm_lr *
-                                (ndv2[m] * nuv_ij - pdv2[m] * puv_ij);
+                                (pdv2[m] * puv_ij - ndv2[m] * nuv_ij);
                 }
             }
         }

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -154,14 +154,14 @@
         real a_i = quad_coeff[0];
         for( int i=0 ; i&lt;size ; i++ )
         {
-            expectation[i] = - activation[i] / (2 * a_i * a_i);
+            expectation[i] = activation[i] / (2 * a_i * a_i);
         }
     }
     else
         for( int i=0 ; i&lt;size ; i++ )
         {
             real a_i = quad_coeff[i];
-            expectation[i] = - activation[i] / (2 * a_i * a_i);
+            expectation[i] = activation[i] / (2 * a_i * a_i);
         }
 
     expectation_is_up_to_date = true;
@@ -181,7 +181,7 @@
         for (int k = 0; k &lt; batch_size; k++)
             for (int i = 0 ; i &lt; size ; i++)
                 {
-                    expectations(k, i) = -activations(k, i) / (2 * a_i * a_i) ;
+                    expectations(k, i) = activations(k, i) / (2 * a_i * a_i) ;
                 }
     }
     else
@@ -189,7 +189,7 @@
             for (int i = 0 ; i &lt; size ; i++)
                 {
                     real a_i = quad_coeff[i];
-                    expectations(k, i) = -activations(k, i) / (2 * a_i * a_i) ;
+                    expectations(k, i) = activations(k, i) / (2 * a_i * a_i) ;
                 }
     expectations_are_up_to_date = true;
 }
@@ -220,14 +220,14 @@
         real a_i = quad_coeff[0];
         for( int i=0 ; i&lt;size ; i++ )
         {
-            output[i] = - (input[i] + bias[i]) / (2 * a_i * a_i);
+            output[i] = (input[i] + bias[i]) / (2 * a_i * a_i);
         }
     }
     else
         for( int i=0 ; i&lt;size ; i++ )
         {
             real a_i = quad_coeff[i];
-            output[i] = - (input[i] + bias[i]) / (2 * a_i * a_i);
+            output[i] = (input[i] + bias[i]) / (2 * a_i * a_i);
         }
 }
 
@@ -263,7 +263,7 @@
     {
         if(!share_quad_coeff)
             a_i = quad_coeff[i];
-        real in_grad_i = - output_gradient[i] / (2 * a_i * a_i);
+        real in_grad_i = output_gradient[i] / (2 * a_i * a_i);
         input_gradient[i] += in_grad_i;
 
         if( momentum == 0. )
@@ -275,9 +275,9 @@
                coefficient during the gradient descent phase.
 
             // update the quadratic coefficient:
-            // a_i += learning_rate * out_grad_i * (b_i + input_i) / a_i^3
-            // (or a_i += 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
-            a_i += two_lr * in_grad_i * (bias[i] + input[i])
+            // a_i -= learning_rate * out_grad_i * (b_i + input_i) / a_i^3
+            // (or a_i -= 2 * learning_rate * in_grad_i * (b_i + input_i) / a_i
+            a_i -= two_lr * in_grad_i * (bias[i] + input[i])
                                                     / a_i;
             if( a_i &lt; min_quad_coeff )
                 a_i = min_quad_coeff;
@@ -292,11 +292,11 @@
 
             /*
             // The update rule becomes:
-            // a_inc_i = momentum * a_i_inc + learning_rate * out_grad_i
+            // a_inc_i = momentum * a_i_inc - learning_rate * out_grad_i
             //                                  * (b_i + input_i) / a_i^3
             // a_i += a_inc_i
-            quad_coeff_inc[i] += momentum * quad_coeff_inc[i]
-                + two_lr * in_grad_i * (bias[i] + input[i])
+            quad_coeff_inc[i] = momentum * quad_coeff_inc[i]
+                - two_lr * in_grad_i * (bias[i] + input[i])
                                          / a_i;
             a_i += quad_coeff_inc[i];
             if( a_i &lt; min_quad_coeff )
@@ -622,13 +622,13 @@
             for(register int i=0; i&lt;size; i++)
             {
                 tmp = a[0]*v[i];
-                en += tmp*tmp + b[i]*v[i];
+                en += tmp*tmp - b[i]*v[i];
             }
         else
             for(register int i=0; i&lt;size; i++)
             {
                 tmp = a[i]*v[i];
-                en += tmp*tmp + b[i]*v[i];
+                en += tmp*tmp - b[i]*v[i];
             }
     }
     return en;
@@ -713,10 +713,8 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    for( int i=0 ; i&lt;size ; i++ )
-    {
-        bias_gradient[i] = target[i]-expectation[i];
-    }
+    // bias_gradient = target - expectation
+    substract(target, expectation, bias_gradient);
 }
 
 void RBMGaussianLayer::bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
@@ -730,7 +728,8 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    substract(targets,expectations,bias_gradients);
+    // bias_gradients = targets - expectations
+    substract(targets, expectations, bias_gradients);
 }
 
 

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-06-29 21:59:11 UTC (rev 7678)
@@ -48,7 +48,6 @@
 /**
  * Layer in an RBM formed with Gaussian units
  *
- * @todo: yes
  */
 class RBMGaussianLayer: public RBMLayer
 {
@@ -58,9 +57,9 @@
     //#####  Public Build Options  ############################################
 
     real min_quad_coeff;
-    
+
     bool share_quad_coeff;
-    
+
     //! Number of units when share_quad_coeff is False
     //! or 1 when share_quad_coeff is True
     int size_quad_coeff;
@@ -75,8 +74,10 @@
     //! Constructor from the number of units in the multinomial
     RBMGaussianLayer( int the_size, real the_learning_rate=0. );
 
-    //! Constructor from the number of units in the multinomial, with an aditional option
-    RBMGaussianLayer( int the_size, real the_learning_rate=0., bool do_share_quad_coeff=false );
+    //! Constructor from the number of units in the multinomial,
+    //! with an aditional option
+    RBMGaussianLayer( int the_size, real the_learning_rate=0.,
+                      bool do_share_quad_coeff=false );
 
     //! compute a sample, and update the sample field
     virtual void generateSample() ;

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -284,10 +284,8 @@
     This-&gt;activation &lt;&lt; input;
     This-&gt;activation += bias;
     This-&gt;expectation_is_up_to_date = false;
-    This-&gt;expectations_are_up_to_date = false;
 
-    PLERROR(&quot;In RBMLayer::fprop - The code seems buggy (no expectation seems&quot;
-            &quot; to be computed), someone should check this out&quot;);
+    This-&gt;computeExpectation();
 
     output &lt;&lt; This-&gt;expectation;
 }
@@ -363,10 +361,10 @@
 ////////////
 void RBMLayer::update()
 {
-    // bias -= learning_rate * (bias_pos_stats/pos_count
+    // bias += learning_rate * (bias_pos_stats/pos_count
     //                          - bias_neg_stats/neg_count)
-    real pos_factor = -learning_rate / pos_count;
-    real neg_factor = learning_rate / neg_count;
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = -learning_rate / neg_count;
 
     real* b = bias.data();
     real* bps = bias_pos_stats.data();
@@ -385,7 +383,7 @@
 
         // The update rule becomes:
         // bias_inc = momentum * bias_inc
-        //              - learning_rate * (bias_pos_stats/pos_count
+        //              + learning_rate * (bias_pos_stats/pos_count
         //                                  - bias_neg_stats/neg_count)
         // bias += bias_inc
         real* binc = bias_inc.data();
@@ -425,7 +423,7 @@
 
 void RBMLayer::update( const Vec&amp; pos_values, const Vec&amp; neg_values)
 {
-    // bias -= learning_rate * (pos_values - neg_values)
+    // bias += learning_rate * (pos_values - neg_values)
     real* b = bias.data();
     real* pv = pos_values.data();
     real* nv = neg_values.data();
@@ -433,7 +431,7 @@
     if( momentum == 0. )
     {
         for( int i=0 ; i&lt;size ; i++ )
-            b[i] += learning_rate * ( nv[i] - pv[i] );
+            b[i] += learning_rate * ( pv[i] - nv[i] );
     }
     else
     {
@@ -441,7 +439,7 @@
         real* binc = bias_inc.data();
         for( int i=0 ; i&lt;size ; i++ )
         {
-            binc[i] = momentum*binc[i] + learning_rate*( nv[i] - pv[i] );
+            binc[i] = momentum*binc[i] + learning_rate*( pv[i] - nv[i] );
             b[i] += binc[i];
         }
     }
@@ -449,7 +447,7 @@
 
 void RBMLayer::update( const Mat&amp; pos_values, const Mat&amp; neg_values)
 {
-    // bias -= learning_rate * (pos_values - neg_values)
+    // bias += learning_rate * (pos_values - neg_values)
 
     int n = pos_values.length();
     PLASSERT( neg_values.length() == n );
@@ -466,8 +464,8 @@
 
     if( momentum == 0. )
     {
-        transposeProductScaleAcc(bias, pos_values, ones, -avg_lr, real(1));
-        transposeProductScaleAcc(bias, neg_values, ones,  avg_lr, real(1));
+        transposeProductScaleAcc(bias, pos_values, ones,  avg_lr, real(1));
+        transposeProductScaleAcc(bias, neg_values, ones, -avg_lr, real(1));
     }
     else
     {
@@ -477,7 +475,7 @@
         real* binc = bias_inc.data();
         for( int i=0 ; i&lt;size ; i++ )
         {
-            binc[i] = momentum*binc[i] + learning_rate*( nv[i] - pv[i] );
+            binc[i] = momentum*binc[i] + learning_rate*( pv[i] - nv[i] );
             b[i] += binc[i];
         }
         */
@@ -513,15 +511,17 @@
                           bias_neg_stats);
     neg_count++;
 
-    // delta w = -lrate * ( sumoverrows(pos_values)
+    // delta w = lrate * ( sumoverrows(pos_values)
     //                   - ( background_gibbs_update_ratio*neg_stats
     //                      +(1-background_gibbs_update_ratio)
     //                       * sumoverrows(cd_neg_values) ) )
     columnSum(pos_values,tmp);
-    multiplyAcc(bias, tmp, -learning_rate*normalize_factor);
-    multiplyAcc(bias,bias_neg_stats,learning_rate*background_gibbs_update_ratio);
+    multiplyAcc(bias, tmp, learning_rate*normalize_factor);
+    multiplyAcc(bias, bias_neg_stats,
+                -learning_rate*background_gibbs_update_ratio);
     columnSum(cd_neg_values, tmp);
-    multiplyAcc(bias, tmp, learning_rate*(1-background_gibbs_update_ratio)*normalize_factor);
+    multiplyAcc(bias, tmp,
+                -learning_rate*(1-background_gibbs_update_ratio)*normalize_factor);
 }
 
 /////////////////
@@ -541,7 +541,7 @@
     real normalize_factor=1.0/minibatch_size;
     columnSum(gibbs_neg_values,tmp);
     if (neg_count==0)
-        multiply(tmp,normalize_factor,bias_neg_stats);
+        multiply(tmp, normalize_factor, bias_neg_stats);
     else // bias_neg_stats &lt;-- tmp*(1-gibbs_chain_statistics_forgetting_factor)/minibatch_size 
         //                    +gibbs_chain_statistics_forgetting_factor*bias_neg_stats
         multiplyScaledAdd(tmp,gibbs_ma_coefficient,
@@ -560,10 +560,10 @@
         gibbs_ma_coefficient = sigmoid(gibbs_ma_increment + inverse_sigmoid(gibbs_ma_coefficient));
 
 
-    // delta w = -lrate * ( meanoverrows(pos_values) - neg_stats ) 
+    // delta w = lrate * ( meanoverrows(pos_values) - neg_stats ) 
     columnSum(pos_values,tmp);
-    multiplyAcc(bias, tmp, -learning_rate*normalize_factor);
-    multiplyAcc(bias,bias_neg_stats,learning_rate);
+    multiplyAcc(bias, tmp, learning_rate*normalize_factor);
+    multiplyAcc(bias, bias_neg_stats, -learning_rate);
 }
 
 ////////////////
@@ -602,27 +602,27 @@
 /////////////
 void RBMLayer::bpropCD(Vec&amp; bias_gradient)
 {
-    // grad = bias_pos_stats/pos_count - bias_neg_stats/neg_count
+    // grad = -bias_pos_stats/pos_count + bias_neg_stats/neg_count
 
     real* bg = bias_gradient.data();
     real* bps = bias_pos_stats.data();
     real* bns = bias_neg_stats.data();
 
     for( int i=0 ; i&lt;size ; i++ )
-        bg[i] = bps[i]/pos_count - bns[i]/neg_count;
+        bg[i] = -bps[i]/pos_count + bns[i]/neg_count;
 }
 
 void RBMLayer::bpropCD(const Vec&amp; pos_values, const Vec&amp; neg_values,
                        Vec&amp; bias_gradient)
 {
-    // grad = bias_pos_stats/pos_count - bias_neg_stats/neg_count
+    // grad = -bias_pos_stats/pos_count + bias_neg_stats/neg_count
 
     real* bg = bias_gradient.data();
     real* bps = pos_values.data();
     real* bns = neg_values.data();
 
     for( int i=0 ; i&lt;size ; i++ )
-        bg[i] = bps[i] - bns[i];
+        bg[i] = -bps[i] + bns[i];
 }
 
 real RBMLayer::energy(const Vec&amp; unit_values) const

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-06-29 21:59:11 UTC (rev 7678)
@@ -52,7 +52,6 @@
 /**
  * Virtual class for a layer in an RBM.
  *
- * @todo: yes
  */
 class RBMLayer: public OnlineLearningModule
 {
@@ -262,10 +261,10 @@
     //! (or activations, which is equivalent), given the positive and
     //! negative phase values.
     virtual void bpropCD(const Vec&amp; pos_values, const Vec&amp; neg_values,
-                    Vec&amp; bias_gradient);
+                         Vec&amp; bias_gradient);
 
     virtual real energy(const Vec&amp; unit_values) const;
-    
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -173,10 +173,10 @@
 void RBMMatrixConnection::update()
 {
     // updates parameters
-    //weights -= learning_rate * (weights_pos_stats/pos_count
+    //weights += learning_rate * (weights_pos_stats/pos_count
     //                              - weights_neg_stats/neg_count)
-    real pos_factor = -learning_rate / pos_count;
-    real neg_factor = learning_rate / neg_count;
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = -learning_rate / neg_count;
 
     int l = weights.length();
     int w = weights.width();
@@ -227,9 +227,9 @@
                                   const Vec&amp; neg_down_values, // v_1
                                   const Vec&amp; neg_up_values )  // h_1
 {
-    // weights -= learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // weights += learning_rate * ( h_0 v_0' - h_1 v_1' );
     // or:
-    // weights[i][j] += learning_rate * (h_1[i] v_1[j] - h_0[i] v_0[j]);
+    // weights[i][j] += learning_rate * (h_0[i] v_0[j] - h_1[i] v_1[j]);
 
     int l = weights.length();
     int w = weights.width();
@@ -249,7 +249,7 @@
     {
         for( int i=0 ; i&lt;l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
             for( int j=0 ; j&lt;w ; j++ )
-                w_i[j] += learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                w_i[j] += learning_rate * (*puv_i * pdv[j] - *nuv_i * ndv[j]);
     }
     else
     {
@@ -268,7 +268,7 @@
             for( int j=0 ; j&lt;w ; j++ )
             {
                 winc_i[j] = momentum * winc_i[j]
-                    + learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                    + learning_rate * (*puv_i * pdv[j] - *nuv_i * ndv[j]);
                 w_i[j] += winc_i[j];
             }
     }
@@ -279,9 +279,9 @@
                                   const Mat&amp; neg_down_values, // v_1
                                   const Mat&amp; neg_up_values )  // h_1
 {
-    // weights -= learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // weights += learning_rate * ( h_0 v_0' - h_1 v_1' );
     // or:
-    // weights[i][j] += learning_rate * (h_1[i] v_1[j] - h_0[i] v_0[j]);
+    // weights[i][j] += learning_rate * (h_0[i] v_0[j] - h_1[i] v_1[j]);
 
     PLASSERT( pos_up_values.width() == weights.length() );
     PLASSERT( neg_up_values.width() == weights.length() );
@@ -294,10 +294,10 @@
         real avg_lr = learning_rate / pos_down_values.length();
 
         transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
-                                 -avg_lr, real(1));
+                                 avg_lr, real(1));
 
         transposeProductScaleAcc(weights, neg_up_values, neg_down_values,
-                                 avg_lr, real(1));
+                                 -avg_lr, real(1));
     }
     else
     {
@@ -308,7 +308,7 @@
 
         // The update rule becomes:
         // weights_inc = momentum * weights_inc
-        //               - learning_rate * ( h_0 v_0' - h_1 v_1' );
+        //               + learning_rate * ( h_0 v_0' - h_1 v_1' );
         // weights += weights_inc;
 
         real* winc_i = weights_inc.data();
@@ -318,7 +318,7 @@
             for( int j=0 ; j&lt;w ; j++ )
             {
                 winc_i[j] = momentum * winc_i[j]
-                    + learning_rate * (*nuv_i * ndv[j] - *puv_i * pdv[j]);
+                    + learning_rate * (*puv_i * pdv[j] - *nuv_i * ndv[j]);
                 w_i[j] += winc_i[j];
             }
          */
@@ -350,16 +350,16 @@
                                  gibbs_ma_coefficient);
     neg_count++;
 
-    // delta w = -lrate * ( pos_up_values'*pos_down_values
+    // delta w = lrate * ( pos_up_values'*pos_down_values
     //                   - ( background_gibbs_update_ratio*neg_stats
     //                      +(1-background_gibbs_update_ratio)
     //                       * cd_neg_up_values'*cd_neg_down_values/minibatch_size))
     transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
-                             -learning_rate*normalize_factor, real(1));
+                             learning_rate*normalize_factor, real(1));
     multiplyAcc(weights, weights_neg_stats,
-                learning_rate*background_gibbs_update_ratio);
+                -learning_rate*background_gibbs_update_ratio);
     transposeProductScaleAcc(weights, cd_neg_up_values, cd_neg_down_values,
-        learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,
+        -learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,
         real(1));
 }
 
@@ -399,10 +399,10 @@
         cout &lt;&lt; &quot;new coefficient = &quot; &lt;&lt; gibbs_ma_coefficient &lt;&lt; &quot; at example &quot; &lt;&lt; neg_count*minibatch_size &lt;&lt; endl;
     }
 
-    // delta w = -lrate * ( pos_up_values'*pos_down_values/minibatch_size - neg_stats )
+    // delta w = lrate * ( pos_up_values'*pos_down_values/minibatch_size - neg_stats )
     transposeProductScaleAcc(weights, pos_up_values, pos_down_values,
-                             -learning_rate*normalize_factor, real(1));
-    multiplyAcc(weights, weights_neg_stats,learning_rate);
+                             learning_rate*normalize_factor, real(1));
+    multiplyAcc(weights, weights_neg_stats, -learning_rate);
 }
 
 ////////////////
@@ -552,9 +552,9 @@
 }
 
 void RBMMatrixConnection::bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
-                             Mat&amp; input_gradients,
-                             const Mat&amp; output_gradients,
-                             bool accumulate)
+                                      Mat&amp; input_gradients,
+                                      const Mat&amp; output_gradients,
+                                      bool accumulate)
 {
     PLASSERT( inputs.width() == down_size );
     PLASSERT( outputs.width() == up_size );
@@ -641,13 +641,13 @@
     {
         for( int i=0 ; i&lt;l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
             for( int j=0 ; j&lt;w ; j++ )
-                w_i[j] += wps_i[j]/pos_count - wns_i[j]/neg_count;
+                w_i[j] += wns_i[j]/pos_count - wps_i[j]/neg_count;
     }
     else
     {
         for( int i=0 ; i&lt;l ; i++, w_i+=w_mod, wps_i+=wps_mod, wns_i+=wns_mod )
             for( int j=0 ; j&lt;w ; j++ )
-                w_i[j] = wps_i[j]/pos_count - wns_i[j]/neg_count;
+                w_i[j] = wns_i[j]/pos_count - wps_i[j]/neg_count;
     }
 }
 
@@ -679,13 +679,13 @@
     {
         for( int i=0 ; i&lt;l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
             for( int j=0 ; j&lt;w ; j++ )
-                w_i[j] +=  *puv_i * pdv[j] - *nuv_i * ndv[j] ;
+                w_i[j] +=  *nuv_i * ndv[j] - *puv_i * pdv[j] ;
     }
     else
     {
         for( int i=0 ; i&lt;l ; i++, w_i += w_mod, puv_i++, nuv_i++ )
             for( int j=0 ; j&lt;w ; j++ )
-                w_i[j] =  *puv_i * pdv[j] - *nuv_i * ndv[j] ;
+                w_i[j] =  *nuv_i * ndv[j] - *puv_i * pdv[j] ;
     }
 }
 

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -368,6 +368,12 @@
 ///////////////////
 // computeEnergy //
 ///////////////////
+// FULLY OBSERVED CASE
+// we know x and h:
+// energy(h,x) = -b'x - c'h - h'Wx
+//  = visible_layer-&gt;energy(x) + hidden_layer-&gt;energy(h)
+//      - dot(h, hidden_layer-&gt;activation-c)
+//  = visible_layer-&gt;energy(x) - dot(h, hidden_layer-&gt;activation)
 void RBMModule::computeEnergy(const Mat&amp; visible, const Mat&amp; hidden,
                               Mat&amp; energy, bool positive_phase)
 {
@@ -383,13 +389,20 @@
     }
     PLASSERT( hidden_activations );
     for (int i=0;i&lt;mbs;i++)
-        energy(i,0) = visible_layer-&gt;energy(visible(i)) + 
-            dot(hidden(i), (*hidden_activations)(i));
+        energy(i,0) = visible_layer-&gt;energy(visible(i))
+            - dot(hidden(i), (*hidden_activations)(i));
+            // Why not: + hidden_layer-&gt;energy(hidden(i)) ?
 }
 
 ///////////////////////////////
 // computeFreeEnergyOfHidden //
 ///////////////////////////////
+// FREE-ENERGY(hidden) CASE
+// we know h:
+// free energy = -log sum_x e^{-energy(h,x)}
+//  = -c'h - sum_i log sigmoid(b_i + W_{.i}'h) .... FOR BINOMIAL INPUT LAYER
+// or more robustly,
+//  = hidden_layer-&gt;energy(h) - sum_i softplus(visible_layer-&gt;activation[i])
 void RBMModule::computeFreeEnergyOfHidden(const Mat&amp; hidden, Mat&amp; energy)
 {
     int mbs=hidden.length();
@@ -405,16 +418,22 @@
         energy(i,0) = hidden_layer-&gt;energy(hidden(i));
         if (use_fast_approximations)
             for (int j=0;j&lt;visible_layer-&gt;size;j++)
-                energy(i,0) -= tabulated_softplus(-visible_layer-&gt;activations(i,j));
+                energy(i,0) -= tabulated_softplus(visible_layer-&gt;activations(i,j));
         else
             for (int j=0;j&lt;visible_layer-&gt;size;j++)
-                energy(i,0) -= softplus(-visible_layer-&gt;activations(i,j));
+                energy(i,0) -= softplus(visible_layer-&gt;activations(i,j));
     }
 }
 
 ////////////////////////////////
 // computeFreeEnergyOfVisible //
 ////////////////////////////////
+// FREE-ENERGY(visible) CASE
+// we know x:
+// free energy = -log sum_h e^{-energy(h,x)}
+//  = -b'x - sum_i log sigmoid(c_i + W_i'x) .... FOR BINOMIAL HIDDEN LAYER
+// or more robustly,
+//  = visible_layer-&gt;energy(x) - sum_i softplus(hidden_layer-&gt;activation[i])
 void RBMModule::computeFreeEnergyOfVisible(const Mat&amp; visible, Mat&amp; energy,
                                            bool positive_phase)
 {
@@ -441,10 +460,10 @@
         energy(i,0) = visible_layer-&gt;energy(visible(i));
         if (use_fast_approximations)
             for (int j=0;j&lt;hidden_layer-&gt;size;j++)
-                energy(i,0) -= tabulated_softplus(-(*hidden_activations)(i,j));
+                energy(i,0) -= tabulated_softplus((*hidden_activations)(i,j));
         else
             for (int j=0;j&lt;hidden_layer-&gt;size;j++)
-                energy(i,0) -= softplus(-(*hidden_activations)(i,j));
+                energy(i,0) -= softplus((*hidden_activations)(i,j));
     }
 }
 
@@ -586,7 +605,7 @@
     PLASSERT( visible_layer );
     PLASSERT( hidden_layer );
     PLASSERT( connection );
-        
+
     Mat* visible = ports_value[getPortIndex(&quot;visible&quot;)]; 
     Mat* hidden = ports_value[getPortIndex(&quot;hidden.state&quot;)];
     hidden_act = ports_value[getPortIndex(&quot;hidden_activations.state&quot;)];
@@ -647,32 +666,23 @@
         PLASSERT_MSG( energy-&gt;isEmpty(), 
                       &quot;RBMModule: the energy port can only be an output port\n&quot; );
         if (visible &amp;&amp; !visible-&gt;isEmpty()
-            &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty()) 
+            &amp;&amp; hidden &amp;&amp; !hidden-&gt;isEmpty())
         {
-            // FULLY OBSERVED CASE
-            // we know x and h: energy(h,x) = b'x + c'h + h'Wx
-            //  = visible_layer-&gt;energy(x) + hidden_layer-&gt;energy(h) + dot(h,hidden_layer-&gt;activation-c)
-            //  = visible_layer-&gt;energy(x) + dot(h,hidden_layer-&gt;activation)
-            computeEnergy(*visible,*hidden,*energy);
-        } else if (visible &amp;&amp; !visible-&gt;isEmpty())
+            computeEnergy(*visible, *hidden, *energy);
+        }
+        else if (visible &amp;&amp; !visible-&gt;isEmpty())
         {
-            // FREE-ENERGY(visible) CASE
-            // we know x: free energy = -log sum_h e^{-energy(h,x)}
-            //                        = b'x + sum_i log sigmoid(c_i + W_i'x) .... FOR BINOMIAL HIDDEN LAYER
-            // or more robustly,      = visible_layer-&gt;energy(x) - sum_i softplus(-hidden_layer-&gt;activation[i])
             computeFreeEnergyOfVisible(*visible,*energy);
         }
         else if (hidden &amp;&amp; !hidden-&gt;isEmpty())
-            // FREE-ENERGY(hidden) CASE
-            // we know h: free energy = -log sum_x e^{-energy(h,x)}
-            //                        = c'h + sum_i log sigmoid(b_i + W_{.i}'h) .... FOR BINOMIAL INPUT LAYER
-            // or more robustly,      = hidden_layer-&gt;energy(h) - sum_i softplus(-visible_layer-&gt;activation[i])
         {
             computeFreeEnergyOfHidden(*hidden,*energy);
         }
-        else 
+        else
+        {
             PLERROR(&quot;RBMModule: unknown configuration to compute energy (currently\n&quot;
                     &quot;only possible if at least visible or hidden are provided).\n&quot;);
+        }
         found_a_valid_configuration = true;
     }
     if (neg_log_likelihood &amp;&amp; neg_log_likelihood-&gt;isEmpty() &amp;&amp; compute_log_likelihood)
@@ -937,12 +947,12 @@
                 (*contrastive_divergence)(i,0) = 
                     // positive phase energy
                     visible_layer-&gt;energy((*visible)(i))
-                    + dot((*h)(i),(*h_act)(i))
+                    - dot((*h)(i),(*h_act)(i))
                     // minus
                     - 
                     // negative phase energy
                     (visible_layer-&gt;energy(visible_layer-&gt;samples(i))
-                     + dot(hidden_expectations(i),hidden_layer-&gt;activations(i)));
+                     - dot(hidden_expectations(i),hidden_layer-&gt;activations(i)));
             }
         }
         else
@@ -1004,7 +1014,7 @@
     }
     
 
-	
+
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;
@@ -1017,15 +1027,16 @@
     {
         /*
         if (visible)
-        cout &lt;&lt; &quot;visible_empty : &quot;&lt;&lt; (bool) visible-&gt;isEmpty() &lt;&lt; endl;
+            cout &lt;&lt; &quot;visible_empty : &quot;&lt;&lt; (bool) visible-&gt;isEmpty() &lt;&lt; endl;
         if (hidden)
-        cout &lt;&lt; &quot;hidden_empty : &quot;&lt;&lt; (bool) hidden-&gt;isEmpty() &lt;&lt; endl;
+            cout &lt;&lt; &quot;hidden_empty : &quot;&lt;&lt; (bool) hidden-&gt;isEmpty() &lt;&lt; endl;
         if (visible_sample)
-        cout &lt;&lt; &quot;visible_sample_empty : &quot;&lt;&lt; (bool) visible_sample-&gt;isEmpty() &lt;&lt; endl;
+            cout &lt;&lt; &quot;visible_sample_empty : &quot;&lt;&lt; (bool) visible_sample-&gt;isEmpty() &lt;&lt; endl;
         if (hidden_sample)
-        cout &lt;&lt; &quot;hidden_sample_empty : &quot;&lt;&lt; (bool) hidden_sample-&gt;isEmpty() &lt;&lt; endl;
+            cout &lt;&lt; &quot;hidden_sample_empty : &quot;&lt;&lt; (bool) hidden_sample-&gt;isEmpty() &lt;&lt; endl;
         if (visible_expectation)
-        cout &lt;&lt; &quot;visible_expectation_empty : &quot;&lt;&lt; (bool) visible_expectation-&gt;isEmpty() &lt;&lt; endl;
+            cout &lt;&lt; &quot;visible_expectation_empty : &quot;&lt;&lt; (bool) visible_expectation-&gt;isEmpty() &lt;&lt; endl;
+
         */
         PLERROR(&quot;In RBMModule::fprop - Unknown port configuration for module %s&quot;, name.c_str());
     }
@@ -1314,46 +1325,47 @@
                 store_weights_grad.clear();
                 weights_g = &amp; store_weights_grad;
             }
-                PLASSERT( connection-&gt;classname() == &quot;RBMMatrixConnection&quot; &amp;&amp;
-                          visible_layer-&gt;classname() == &quot;RBMBinomialLayer&quot; &amp;&amp;
-                          hidden_layer-&gt;classname() == &quot;RBMBinomialLayer&quot; );
+            PLASSERT( connection-&gt;classname() == &quot;RBMMatrixConnection&quot; &amp;&amp;
+                      visible_layer-&gt;classname() == &quot;RBMBinomialLayer&quot; &amp;&amp;
+                      hidden_layer-&gt;classname() == &quot;RBMBinomialLayer&quot; );
 
+            for (int k = 0; k &lt; mbs; k++) {
+                int idx = 0;
+                for (int i = 0; i &lt; up; i++) {
+                    real p_i_p = (*hidden)(k, i);
+                    real a_i_p = (*hidden_act)(k, i);
+                    real p_i_n =
+                        (*negative_phase_hidden_expectations)(k, i);
+                    real a_i_n =
+                        (*negative_phase_hidden_activations)(k, i);
+
+                    real scale_p = 1 + (1 - p_i_p) * a_i_p;
+                    real scale_n = 1 + (1 - p_i_n) * a_i_n;
+                    for (int j = 0; j &lt; down; j++, idx++) {
+                        // Weight 'idx' is the (i,j)-th element in the
+                        // 'weights' matrix.
+                        real v_j_p = (*visible)(k, j);
+                        real v_j_n =
+                            (*negative_phase_visible_samples)(k, j);
+                        (*weights_g)(k, idx) +=
+                            p_i_n * v_j_n * scale_n     // Negative phase.
+                            -(p_i_p * v_j_p * scale_p); // Positive phase.
+                    }
+                }
+            }
+            if (!standard_cd_grad) {
+                // Update connection manually.
+                Mat&amp; weights = ((RBMMatrixConnection*)
+                                get_pointer(connection))-&gt;weights;
+                real lr = cd_learning_rate / mbs;
                 for (int k = 0; k &lt; mbs; k++) {
                     int idx = 0;
-                    for (int i = 0; i &lt; up; i++) {
-                        real p_i_p = (*hidden)(k, i);
-                        real a_i_p = (*hidden_act)(k, i);
-                        real p_i_n =
-                            (*negative_phase_hidden_expectations)(k, i);
-                        real a_i_n =
-                            (*negative_phase_hidden_activations)(k, i);
-                        real scale_p = 1 - (1 - p_i_p) * a_i_p;
-                        real scale_n = 1 - (1 - p_i_n) * a_i_n;
-                        for (int j = 0; j &lt; down; j++, idx++) {
-                            // Weight 'idx' is the (i,j)-th element in the
-                            // 'weights' matrix.
-                            real v_j_p = (*visible)(k, j);
-                            real v_j_n =
-                                (*negative_phase_visible_samples)(k, j);
-                            (*weights_g)(k, idx) +=
-                                p_i_p * v_j_p * scale_p   // Positive phase.
-                             - (p_i_n * v_j_n * scale_n); // Negative phase.
-                        }
-                    }
+                    for (int i = 0; i &lt; up; i++)
+                        for (int j = 0; j &lt; down; j++, idx++)
+                            weights(i, j) -= lr * (*weights_g)(k, idx);
                 }
-                if (!standard_cd_grad) {
-                    // Update connection manually.
-                    Mat&amp; weights = ((RBMMatrixConnection*)
-                            get_pointer(connection))-&gt;weights;
-                    real lr = cd_learning_rate / mbs;
-                    for (int k = 0; k &lt; mbs; k++) {
-                        int idx = 0;
-                        for (int i = 0; i &lt; up; i++)
-                            for (int j = 0; j &lt; down; j++, idx++)
-                                weights(i, j) -= lr * (*weights_g)(k, idx);
-                    }
-                    connection_update_is_done = true;
-                }
+                connection_update_is_done = true;
+            }
         }
         if (!connection_update_is_done)
             // Perform standard update of the connection.
@@ -1388,9 +1400,9 @@
                     real p_i_n = (*negative_phase_hidden_expectations)(k, i);
                     real a_i_n = (*negative_phase_hidden_activations)(k, i);
                     (*hidden_bias_g)(k, i) +=
-                        standard_cd_bias_grad ? p_i_p - p_i_n :
-                        - p_i_p * (1 - p_i_p) * a_i_p + p_i_p    // Pos. phase
-                     -( - p_i_n * (1 - p_i_n) * a_i_n + p_i_n ); // Neg. phase
+                        standard_cd_bias_grad ? p_i_n - p_i_p :
+                        p_i_n * (1 - p_i_n) * a_i_n + p_i_n     // Neg. phase
+                     -( p_i_p * (1 - p_i_p) * a_i_p + p_i_p );  // Pos. phase
 
                 }
             }

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -1,5 +1,5 @@
+// -*- C++ -*-
 
-
 // RBMMultinomialLayer.cc
 //
 // Copyright (C) 2006 Pascal Lamblin &amp; Dan Popovici
@@ -67,25 +67,7 @@
     bias_pos_stats.resize( the_size );
     bias_neg_stats.resize( the_size );
 }
-/*
-//! Uses &quot;rbmp&quot; to obtain the activations of unit &quot;i&quot; of this layer.
-//! This activation vector is computed by the &quot;i+offset&quot;-th unit of &quot;rbmp&quot;
-void RBMMultinomialLayer::getUnitActivations( int i, PP&lt;RBMParameters&gt; rbmp,
-                                              int offset )
-{
-    Vec activation = activations.subVec( i, 1 );
-    rbmp-&gt;computeUnitActivations( i+offset, 1, activation );
-    expectation_is_up_to_date = false;
-}
 
-void RBMMultinomialLayer::getAllActivations( PP&lt;RBMParameters&gt; rbmp,
-                                             int offset )
-{
-    rbmp-&gt;computeUnitActivations( offset, size, activations );
-    expectation_is_up_to_date = false;
-}
-*/
-
 void RBMMultinomialLayer::generateSample()
 {
     PLASSERT_MSG(random_gen,
@@ -121,7 +103,7 @@
         return;
 
     // expectation = softmax(-activation)
-    softmaxMinus(activation, expectation);
+    softmax(activation, expectation);
     expectation_is_up_to_date = true;
 }
 
@@ -135,7 +117,7 @@
 
     // expectation = softmax(-activation)
     for (int k = 0; k &lt; batch_size; k++)
-        softmaxMinus(activations(k), expectations(k));
+        softmax(activations(k), expectations(k));
 
     expectations_are_up_to_date = true;
 }
@@ -147,7 +129,7 @@
     output.resize( output_size );
 
     // inefficient
-    softmaxMinus( input+bias, output );
+    softmax( input+bias, output );
 }
 
 ///////////
@@ -161,7 +143,7 @@
     output.resize( output_size );
 
     // inefficient
-    softmaxMinus( input+rbm_bias, output );
+    softmax( input+rbm_bias, output );
 }
 
 /////////////////
@@ -191,7 +173,7 @@
         bias_inc.resize( size );
 
     // input_gradient[i] =
-    //      (output_gradient . output - output_gradient[i] ) output[i]
+    //      (output_gradient[i] - output_gradient . output) output[i]
     real outg_dot_out = dot( output_gradient, output );
     real* out = output.data();
     real* outg = output_gradient.data();
@@ -201,7 +183,7 @@
 
     for( int i=0 ; i&lt;size ; i++ )
     {
-        real ing_i = (outg_dot_out - outg[i]) * out[i];
+        real ing_i = (outg[i] - outg_dot_out) * out[i];
         ing[i] += ing_i;
 
         if( momentum == 0. )
@@ -252,7 +234,7 @@
     // TODO see if we can have a speed-up by reorganizing the different steps
 
     // input_gradients[k][i] =
-    //   (output_gradients[k].outputs[k]-output_gradients[k][i]) outputs[k][i]
+    //   (output_gradients[k][i]-output_gradients[k].outputs[k]) outputs[k][i]
     for( int k=0; k&lt;mbatch_size; k++ )
     {
         real outg_dot_out = dot( output_gradients(k), outputs(k) );
@@ -264,7 +246,7 @@
 
         for( int i=0 ; i&lt;size ; i++ )
         {
-            real ing_ki = (outg_dot_out - outg[i]) * out[i];
+            real ing_ki = (outg[i] - outg_dot_out) * out[i];
             ing[i] += ing_ki;
 
             if( momentum == 0. )
@@ -305,7 +287,7 @@
     real* outg = output_gradient.data();
     real* ing = input_gradient.data();
     for( int i=0 ; i&lt;size ; i++ )
-        ing[i] = (outg_dot_out - outg[i]) * out[i];
+        ing[i] = (outg[i] - outg_dot_out) * out[i];
 
     rbm_bias_gradient &lt;&lt; input_gradient;
 }
@@ -319,17 +301,31 @@
 
     PLASSERT( target.size() == input_size );
 
-    real ret = 0;
+#ifdef BOUNDCHECK
+    if (!target.hasMissing())
+    {
+        PLASSERT_MSG( min(target) &gt;= 0.,
+                      &quot;Elements of \&quot;target\&quot; should be positive&quot; );
+        // Ensure the distribution probabilities sum to 1. We relax a
+        // bit the default tolerance as probabilities using
+        // exponentials could suffer numerical imprecisions.
+        if (!is_equal( sum(target), 1., 1., 1e-5, 1e-5 ))
+            PLERROR(&quot;In RBMMultinomialLayer::fpropNLL - Elements of \&quot;target\&quot;&quot;
+                    &quot; should sum to 1 (found a sum = %f)&quot;,
+                    sum(target));
+    }
+#endif
 
+    real nll = 0;
     real target_i, expectation_i;
-    for( int i=0 ; i&lt;size ; i++ )
+    for (int i=0; i&lt;size; i++)
     {
         target_i = target[i];
         expectation_i = expectation[i];
-        if(!fast_exact_is_equal(target_i,0.0))
-            ret -= target_i * pl_log(expectation_i);
+        if(!fast_exact_is_equal(target_i, 0.0))
+            nll -= target_i * pl_log(expectation_i);
     }
-    return ret;
+    return nll;
 }
 
 void RBMMultinomialLayer::fpropNLL(const Mat&amp; targets, const Mat&amp; costs_column)
@@ -342,20 +338,34 @@
     PLASSERT( costs_column.length() == batch_size );
 
     real target_i, expectation_i;
-    for (int k=0;k&lt;batch_size;k++) // loop over minibatch
+    for (int k=0; k&lt;batch_size; k++) // loop over minibatch
     {
+#ifdef BOUNDCHECK
+        if (!targets(k).hasMissing())
+        {
+            PLASSERT_MSG( min(targets(k)) &gt;= 0.,
+                          &quot;Elements of \&quot;targets\&quot; should be positive&quot; );
+            // Ensure the distribution probabilities sum to 1. We relax a
+            // bit the default tolerance as probabilities using
+            // exponentials could suffer numerical imprecisions.
+            if (!is_equal( sum(targets(k)), 1., 1., 1e-5, 1e-5 ))
+                PLERROR(&quot;In RBMMultinomialLayer::fpropNLL - Elements of&quot;
+                        &quot; \&quot;target\&quot; should sum to 1 (found a sum = %f at row&quot;
+                        &quot; %d)&quot;,
+                        sum(targets(k)), k);
+        }
+#endif
         real nll = 0;
-        //real* activation = activations[k];
         real* expectation = expectations[k];
         real* target = targets[k];
-        for( int i=0 ; i&lt;size ; i++ )
+        for(int i=0; i&lt;size; i++)
         {
             target_i = target[i];
             expectation_i = expectation[i];
-            if(!fast_exact_is_equal(target_i,0.0))
+            if(!fast_exact_is_equal(target_i, 0.0))
                 nll -= target_i * pl_log(expectation_i);
         }
-        costs_column(k,0) = nll;
+        costs_column(k, 0) = nll;
     }
 }
 
@@ -367,16 +377,11 @@
     PLASSERT( target.size() == input_size );
     bias_gradient.resize( size );
 
-    real sum_tar = sum( target );
-    real* exp = expectation.data();
-    real* tar = target.data();
-    real* biasg = bias_gradient.data();
-    for( int i=0 ; i&lt;size ; i++ )
-        biasg[i] = tar[i] - sum_tar * exp[i];
+    substract(target, expectation, bias_gradient);
 }
 
 void RBMMultinomialLayer::bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
-                                Mat&amp; bias_gradients)
+                                   Mat&amp; bias_gradients)
 {
     computeExpectations();
 
@@ -386,15 +391,7 @@
     PLASSERT( costs_column.length() == batch_size );
     bias_gradients.resize( batch_size, size );
 
-    for (int k=0;k&lt;batch_size;k++) // loop over minibatch
-    {        
-        real sum_tar = sum( targets(k) );
-        real* exp = expectations[k];
-        real* tar = targets[k];
-        real* biasg = bias_gradients[k];
-        for( int i=0 ; i&lt;size ; i++ )
-            biasg[i] = tar[i] - sum_tar * exp[i];
-    }
+    substract(targets, expectations, bias_gradients);
 }
 
 void RBMMultinomialLayer::declareOptions(OptionList&amp; ol)

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-06-29 21:49:44 UTC (rev 7677)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-06-29 21:59:11 UTC (rev 7678)
@@ -67,38 +67,18 @@
     bias_neg_stats.resize( the_size );
 }
 
-/*
-//! Uses &quot;rbmp&quot; to obtain the activations of unit &quot;i&quot; of this layer.
-//! This activation vector is computed by the &quot;i+offset&quot;-th unit of &quot;rbmp&quot;
-void RBMTruncExpLayer::getUnitActivations( int i, PP&lt;RBMParameters&gt; rbmp,
-                                           int offset )
-{
-    Vec activation = activations.subVec( i, 1 );
-    rbmp-&gt;computeUnitActivations( i+offset, 1, activation );
-    expectation_is_up_to_date = false;
-}
-
-//! Uses &quot;rbmp&quot; to obtain the activations of all units in this layer.
-//! Unit 0 of this layer corresponds to unit &quot;offset&quot; of &quot;rbmp&quot;.
-void RBMTruncExpLayer::getAllActivations( PP&lt;RBMParameters&gt; rbmp, int offset )
-{
-    rbmp-&gt;computeUnitActivations( offset, size, activations );
-    expectation_is_up_to_date = false;
-}
-*/
-
 void RBMTruncExpLayer::generateSample()
 {
     PLASSERT_MSG(random_gen,
                  &quot;random_gen should be initialized before generating samples&quot;);
 
     /* The cumulative is :
-     * C(U) = P(u&lt;U | x) = (1 - exp(-U a)) / (1 - exp(-a)) if 0 &lt; U &lt; 1,
+     * C(U) = P(u&lt;U | x) = (1 - exp(U a)) / (1 - exp(a)) if 0 &lt; U &lt; 1,
      *        0 if U &lt;= 0 and
      *        1 if 1 &lt;= U
      *
      * And the inverse, if 0 &lt;= s &lt;=1:
-     * C^{-1}(s) = - log(1 - s*(1 - exp(-a)) / a
+     * C^{-1}(s) = log(1 - s*(1 - exp(a)) / a
      */
 
     for( int i=0 ; i&lt;size ; i++ )
@@ -107,11 +87,11 @@
         real a_i = activation[i];
 
         // Polynomial approximation to avoid numerical instability if a ~ 0
-        // C^{-1}(s) ~ s + (-s + s^2)/2 * a + O(a^2)
+        // C^{-1}(s) ~ s + (s - s^2)/2 * a + O(a^2)
         if( fabs( a_i ) &lt;= 1e-5 )
-            sample[i] = s + a_i*( s*(-1 + s)/2 );
+            sample[i] = s + a_i*( s*(1 - s)/2 );
         else
-            sample[i] = - logadd( pl_log( 1-s ), pl_log(s) - a_i ) / a_i;
+            sample[i] = logadd( pl_log( 1-s ), pl_log(s) + a_i ) / a_i;
     }
 }
 
@@ -121,7 +101,7 @@
         return;
 
     /* Conditional expectation:
-     * E[u|x] = 1/(1-exp(a)) + 1/a
+     * E[u|x] = 1/(1-exp(-a)) - 1/a
      */
 
     for( int i=0 ; i&lt;size ; i++ )
@@ -129,11 +109,11 @@
         real a_i = activation[i];
 
         // Polynomial approximation to avoid numerical instability
-        // f(a) = 1/2 - a/12 + a^3/720 + O(a^5)
+        // f(a) = 1/2 + a/12 - a^3/720 + O(a^5)
         if( fabs( a_i ) &lt;= 0.01 )
-            expectation[i] = 0.5 - a_i*(1./12. + a_i*a_i/720.);
+            expectation[i] = 0.5 + a_i*(1./12. - a_i*a_i/720.);
         else
-            expectation[i] = 1/(1-exp(a_i)) + 1/a_i;
+            expectation[i] = 1/(1-exp(-a_i)) - 1/a_i;
     }
 
     expectation_is_up_to_date = true;
@@ -150,12 +130,12 @@
         real a_i = input[i] + bias[i];
 
         // Polynomial approximation to avoid numerical instability
-        // f(a) = 1/(1-exp(a) + 1/a
-        // f(a) = 1/2 - a/12 + a^3/720 + O(a^5)
+        // f(a) = 1/(1-exp(-a) - 1/a
+        // f(a) = 1/2 + a/12 - a^3/720 + O(a^5)
         if( fabs( a_i ) &lt;= 0.01 )
-            output[i] = 0.5 - a_i*(1./12. +a_i*a_i/720.);
+            output[i] = 0.5 + a_i*(1./12. - a_i*a_i/720.);
         else
-            output[i] = 1/(1-exp(a_i)) + 1/a_i;
+            output[i] = 1/(1-exp(-a_i)) - 1/a_i;
     }
 }
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001125.html">[Plearn-commits] r7677 - tags
</A></li>
	<LI>Next message: <A HREF="001127.html">[Plearn-commits] r7679 - in trunk/plearn_learners/online/test: DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-0/Split0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-1-1/Split0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-0/Split0 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1 DeepBeliefNet/.pytest/PL_DBN_Mini-batch/expected_results/expdir-dbn-3-1/Split0 DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0 DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/expdir-tester ModuleLearner/.pytest/PL_ModuleLearn! er_Greedy/expected_results/expdir-tester/Split0 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2 ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1126">[ date ]</a>
              <a href="thread.html#1126">[ thread ]</a>
              <a href="subject.html#1126">[ subject ]</a>
              <a href="author.html#1126">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
