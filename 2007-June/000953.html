<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7504 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/plotting python_modules/plearn/var
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7504%20-%20in%20trunk%3A%20plearn/var/EXPERIMENTAL%0A%09plearn_learners/generic/EXPERIMENTAL%0A%09python_modules/plearn/plotting%20python_modules/plearn/var&In-Reply-To=%3C200706012205.l51M5Zvm016209%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000952.html">
   <LINK REL="Next"  HREF="000954.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7504 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/plotting python_modules/plearn/var</H1>
    <B>simonl at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7504%20-%20in%20trunk%3A%20plearn/var/EXPERIMENTAL%0A%09plearn_learners/generic/EXPERIMENTAL%0A%09python_modules/plearn/plotting%20python_modules/plearn/var&In-Reply-To=%3C200706012205.l51M5Zvm016209%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7504 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/plotting python_modules/plearn/var">simonl at mail.berlios.de
       </A><BR>
    <I>Sat Jun  2 00:05:35 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000952.html">[Plearn-commits] r7503 - trunk/plearn/var
</A></li>
        <LI>Next message: <A HREF="000954.html">[Plearn-commits] r7505 - trunk/python_modules/plearn/pyext
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#953">[ date ]</a>
              <a href="thread.html#953">[ thread ]</a>
              <a href="subject.html#953">[ subject ]</a>
              <a href="author.html#953">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: simonl
Date: 2007-06-02 00:05:34 +0200 (Sat, 02 Jun 2007)
New Revision: 7504

Added:
   trunk/python_modules/plearn/plotting/netplot.py
Modified:
   trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/var/Var.py
Log:
Cool new stuff for plotting networks and DeepReconstructorNet


Modified: trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-01 22:05:34 UTC (rev 7504)
@@ -56,7 +56,7 @@
 
 ProbabilityPairsVariable::ProbabilityPairsVariable(Variable* input, real min, real max)
     : inherited(input, input-&gt;length(), input-&gt;width()*2),
-      max(max),min(min)
+      min(min),max(max)
 
 {
     build_();

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-01 22:05:34 UTC (rev 7504)
@@ -42,6 +42,7 @@
 #include &lt;plearn/var/Var_operators.h&gt;
 #include &lt;plearn/vmat/ConcatColumnsVMatrix.h&gt;
 #include &lt;plearn/var/ConcatColumnsVariable.h&gt;
+#include &lt;plearn/io/load_and_save.h&gt;
 
 namespace PLearn {
 using namespace std;
@@ -52,7 +53,8 @@
    &quot;MULTI-LINE \nHELP&quot;);
 
 DeepReconstructorNet::DeepReconstructorNet()
-    :good_improvement_rate(-1e10),
+    :supervised_nepochs(0),
+     good_improvement_rate(-1e10),
      fine_tuning_improvement_rate(-1e10),
      minibatch_size(1)
 {
@@ -81,6 +83,10 @@
                   OptionBase::buildoption,
                   &quot;recontruction_costs[k] is the reconstruction cost for layer[k]&quot;);
 
+    declareOption(ol, &quot;reconstructed_layers&quot;, &amp;DeepReconstructorNet::reconstructed_layers,
+                  OptionBase::buildoption,
+                  &quot;reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]&quot;);
+
     declareOption(ol, &quot;reconstruction_optimizer&quot;, &amp;DeepReconstructorNet::reconstruction_optimizer,
                   OptionBase::buildoption,
                   &quot;&quot;);
@@ -121,6 +127,10 @@
                   OptionBase::buildoption,
                   &quot;&quot;);
 
+    declareOption(ol, &quot;supervised_nepochs&quot;, &amp;DeepReconstructorNet::supervised_nepochs,
+                  OptionBase::buildoption,
+                  &quot;&quot;);
+
     
     
     // Now call the parent class' declareOptions
@@ -150,6 +160,20 @@
                   &amp;DeepReconstructorNet::listParameter,
                   (BodyDoc(&quot;Returns a list of the parameters&quot;),
                    RetDoc(&quot;Returns a list of the names&quot;)));
+
+    declareMethod(rmm,
+                  &quot;computeRepresentations&quot;,
+                  &amp;DeepReconstructorNet::computeRepresentations,
+                  (BodyDoc(&quot;Compute the representation of each hidden layer&quot;),
+                   ArgDoc(&quot;input&quot;, &quot;the input&quot;),
+                   RetDoc(&quot;The representations&quot;)));
+
+    declareMethod(rmm,
+                  &quot;computeReconstructions&quot;,
+                  &amp;DeepReconstructorNet::computeReconstructions,
+                  (BodyDoc(&quot;Compute the reconstructions of the input of each hidden layer&quot;),
+                   ArgDoc(&quot;input&quot;, &quot;the input&quot;),
+                   RetDoc(&quot;The reconstructions&quot;)));
 }
 
 void DeepReconstructorNet::build_()
@@ -222,11 +246,13 @@
     deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
     deepCopyField(reconstruction_costs, copies);
+    deepCopyField(reconstructed_layers, copies);
     deepCopyField(reconstruction_optimizer, copies);
-    deepCopyField(target, copies);
+    varDeepCopyField(target, copies);
     deepCopyField(supervised_costs, copies);
-    deepCopyField(supervised_costvec, copies);
-    deepCopyField(fullcost, copies);    
+    varDeepCopyField(supervised_costvec, copies);
+    deepCopyField(supervised_costs_names, copies);
+    varDeepCopyField(fullcost, copies);    
     deepCopyField(parameters,copies);
     deepCopyField(supervised_optimizer, copies);
     deepCopyField(fine_tuning_optimizer, copies);
@@ -274,34 +300,48 @@
     if (!initTrain())
         return;
 
-    PPath outmatfname = expdir/&quot;outmat&quot;;
+    while(stage&lt;nstages)
+    {
+        if(stage&lt;1)
+        {
+            PPath outmatfname = expdir/&quot;outmat&quot;;
 
-    int nreconstructions = reconstruction_costs.length();
-    int insize = train_set-&gt;inputsize();
-    VMat inputs = train_set.subMatColumns(0,insize);
-    VMat targets = train_set.subMatColumns(insize, train_set-&gt;targetsize());
-    VMat dset = inputs;
+            int nreconstructions = reconstruction_costs.length();
+            int insize = train_set-&gt;inputsize();
+            VMat inputs = train_set.subMatColumns(0,insize);
+            VMat targets = train_set.subMatColumns(insize, train_set-&gt;targetsize());
+            VMat dset = inputs;
 
-    bool must_train_supervised_layer = (training_schedule[training_schedule.length()-2]&gt;0);
+            bool must_train_supervised_layer = (training_schedule[training_schedule.length()-2]&gt;0);
+            
+            PLearn::save(expdir/&quot;learner_0.psave&quot;, this);
+            for(int k=0; k&lt;nreconstructions; k++)
+            {
+                trainHiddenLayer(k, dset);
+                PLearn::save(expdir/&quot;learner_&quot;+tostring(k+1)+&quot;.psave&quot;, this);
+                // 'if' is a hack to avoid precomputing last hidden layer if not needed
+                if(k&lt;nreconstructions-1 ||  must_train_supervised_layer) 
+                { 
+                    int width = layers[k+1].width();
+                    outmat[k] = new FileVMatrix(outmatfname+tostring(k+1)+&quot;.pmat&quot;,0,width);
+                    outmat[k]-&gt;defineSizes(width,0);
+                    buildHiddenLayerOutputs(k, dset, outmat[k]);
+                    dset = outmat[k];
+                }
+            }
 
-    for(int k=0; k&lt;nreconstructions; k++)
-    {
-        trainHiddenLayer(k, dset);
-        // 'if' is a hack to avoid precomputing last hidden layer if not needed
-        if(k&lt;nreconstructions-1 ||  must_train_supervised_layer) 
-        { 
-            int width = layers[k+1].width();
-            outmat[k] = new FileVMatrix(outmatfname+tostring(k+1)+&quot;.pmat&quot;,0,width);
-            outmat[k]-&gt;defineSizes(width,0);
-            buildHiddenLayerOutputs(k, dset, outmat[k]);
-            dset = outmat[k];
+            if(must_train_supervised_layer)
+                trainSupervisedLayer(dset, targets);
         }
+        else
+        {
+            pout &lt;&lt; &quot;Fine tuning stage &quot; &lt;&lt; stage+1 &lt;&lt; endl;
+            prepareForFineTuning();            
+            fineTuningFor1Epoch();
+        }
+        ++stage;
+        train_stats-&gt;finalize(); // finalize statistics for this epoch
     }
-
-    if(must_train_supervised_layer)
-        trainSupervisedLayer(dset, targets);
-
-    fineTuning();
     /*
     while(stage&lt;nstages)
     {        
@@ -334,14 +374,8 @@
     }
 }
 
-void DeepReconstructorNet::fineTuning()
+void DeepReconstructorNet::prepareForFineTuning()
 {
-    int l = train_set-&gt;length();
-    int nepochs = training_schedule[training_schedule.length()-1];
-    perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
-    perr &lt;&lt; &quot;*** Performing fine tuning for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
-    perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
-
     Func f(layers[0]&amp;target, supervised_costvec);
     Var totalcost = sumOf(train_set, f, minibatch_size);
     // displayVarGraph(supervised_costvec);
@@ -349,7 +383,69 @@
 
     VarArray params = totalcost-&gt;parents();
     supervised_optimizer-&gt;setToOptimize(params, totalcost);
+}
+
+
+TVec&lt;Mat&gt; DeepReconstructorNet::computeRepresentations(Mat input)
+{
+    int nlayers = layers.length();
+    TVec&lt;Mat&gt; representations(nlayers);
+    VarArray proppath = propagationPath(layers[0],layers[nlayers-1]);
+    layers[0]-&gt;matValue &lt;&lt; input;
+    proppath.fprop();
+    for(int k=0; k&lt;nlayers; k++)
+        representations[k] = layers[k]-&gt;matValue.copy();
+    return representations;
+}
+
+void DeepReconstructorNet::reconstructInputFromLayer(int layer)
+{
+    for(int k=layer; k&gt;0; k--)
+    {
+        VarArray proppath = propagationPath(layers[k],reconstructed_layers[k-1]);
+        proppath.fprop();
+        reconstructed_layers[k-1]-&gt;matValue &gt;&gt; layers[k-1]-&gt;matValue;
+    }
+}
+
+TVec&lt;Mat&gt; DeepReconstructorNet::computeReconstructions(Mat input)
+{
+    int nlayers = layers.length();
+    VarArray proppath = propagationPath(layers[0],layers[nlayers-1]);
+    layers[0]-&gt;matValue &lt;&lt; input;
+    proppath.fprop();
+
+    TVec&lt;Mat&gt; reconstructions(nlayers-2);
+    for(int k=1; k&lt;nlayers-1; k++)
+    {
+        reconstructInputFromLayer(k);
+        reconstructions[k-1] = layers[0]-&gt;matValue.copy();
+    }
+    return reconstructions;
+}
+
+
+void DeepReconstructorNet::fineTuningFor1Epoch()
+{
+    if(train_stats.isNull())
+        train_stats = new VecStatsCollector();
+
+    int l = train_set-&gt;length();
     supervised_optimizer-&gt;reset();
+    supervised_optimizer-&gt;nstages = l/minibatch_size;
+    supervised_optimizer-&gt;optimizeN(*train_stats);
+}
+
+void DeepReconstructorNet::fineTuningFullOld()
+{
+    prepareForFineTuning();
+
+    int l = train_set-&gt;length();
+    int nepochs = nstages;
+    perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** Performing fine tuning for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
+
     VecStatsCollector st;
     real prev_mean = -1;
     real relative_improvement = fine_tuning_improvement_rate;
@@ -375,7 +471,7 @@
 {
     int l = inputs-&gt;length();
     int last_hidden_layer = layers.length()-2;
-    int nepochs = training_schedule[training_schedule.length()-2];
+    int nepochs = supervised_nepochs;
     perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
     perr &lt;&lt; &quot;*** Training only supervised layer for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
     perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-01 22:05:34 UTC (rev 7504)
@@ -75,17 +75,21 @@
     //! input layer).
 
     TVec&lt;int&gt; training_schedule;
+    int supervised_nepochs;    
 
     real good_improvement_rate;
     real fine_tuning_improvement_rate;
 
     // layers[0] is the input variable
     // last layer is final output layer
-    TVec&lt;Var&gt; layers;
+    VarArray layers;
 
     // reconstruction_costs[k] is the reconstruction cost for layers[k]
-    TVec&lt;Var&gt; reconstruction_costs;
+    VarArray reconstruction_costs;
 
+    // reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]
+    VarArray reconstructed_layers;
+
     PP&lt;Optimizer&gt; reconstruction_optimizer;
 
 
@@ -175,9 +179,16 @@
     //! Returns a list of the parameters
     TVec&lt;Mat&gt; listParameter();
 
-    void fineTuning();
+    void prepareForFineTuning();
+    void fineTuningFor1Epoch();
+    void fineTuningFullOld();
 
     void trainSupervisedLayer(VMat inputs, VMat targets);
+
+    TVec&lt;Mat&gt; computeRepresentations(Mat input);
+    void reconstructInputFromLayer(int layer);
+    TVec&lt;Mat&gt; computeReconstructions(Mat input);
+
     
 
     // *** SUBCLASS WRITING: ***

Added: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-06-01 22:05:34 UTC (rev 7504)
@@ -0,0 +1,445 @@
+from pylab import *
+
+
+#################
+### constants ###
+#################
+
+defaultColorMap = cm.jet
+defaultInterpolation = 'nearest'
+
+########################
+### plotting methods ###
+########################
+
+def setPlotParams(titre='', color_bar=True, disable_ticks=False):
+    title(titre)
+    if color_bar:
+        colorbar()
+    if disable_ticks:
+        xticks([],[])
+        yticks([],[])
+
+def findMinMax(matrix):
+    M = matrix
+    mi = M[0][0]
+    ma = M[0][0]
+    for row in M:
+        for el in row:
+            if el &gt; ma:
+                ma = el
+            if el &lt; mi:
+                mi = el
+    return mi,ma
+
+def customColorBar(min, max, (x,y,width,height) = (0.9,0.1,0.1,0.8), nb_ticks = 50., color_map = defaultColorMap):
+    axes((x, y, width,height))
+    cbarh = arange(min, max,  (max-min)/50.)
+    cbar = vecToVerticalMatrix(cbarh)
+    cbarh_str = []
+    for el in cbarh:
+        cbarh_str.append(str(el)[0:5])
+    yticks(arange(len(cbarh)),cbarh_str)
+    xticks([],[])                              
+    imshow(cbar, cmap = color_map, vmin=min, vmax=max)
+
+
+
+
+def plotLayer1Old(W, M, width):
+    '''plots each row of W and M as if they where matrix (width is the width of one of these matrix)'''
+
+    #some calculations for plotting
+
+    nbPlotStyles = 3 # (normal and two times 1 sur 2)
+    subPlotHeight = nbPlotStyles*2
+    subPlotWidth = max(len(W), len(M))
+    axesHeight = 1./float(subPlotHeight)
+    axesWidth = 1./float(subPlotWidth)
+
+    colorMap = defaultColorMap
+    
+    matrices = [W,M]
+    names = [&quot;W&quot;, &quot;M&quot;]
+
+    #THE plotting
+
+    for i, matrix in enumerate(matrices):
+        for j, row in enumerate(matrix):
+            
+            #normal
+        
+            #subplot(subPlotHeight, subPlotWidth,(i%2)*subPlotWidth + j + 1)
+            axes((j*axesWidth, i*axesHeight, axesWidth, axesHeight))
+            imshow(rowToMatrix(row, width), interpolation=&quot;nearest&quot;, cmap = colorMap)
+            setPlotParams(names[i%2] + &quot;_&quot; + str(i) + &quot;_&quot; + str(j), False, True)
+            
+            #1 sur 2 -
+        
+            #subplot(subPlotHeight, subPlotWidth, (2+i%2)*subPlotWidth + j + 1)
+            axes((j*axesWidth, (i+2)*axesHeight, axesWidth, axesHeight))
+            imshow(rowToMatrix(toMinusRow(row),width), interpolation=&quot;nearest&quot;, cmap = colorMap)
+            setPlotParams(names[i%2] + &quot;_&quot; + str(i) + &quot;_&quot; + str(j), False, True)
+            
+            #1 sur 2 +
+
+            #subplot(subPlotHeight, subPlotWidth, (4+i%2)*subPlotWidth + j + 1)
+            axes((j*axesWidth, (i+4)*axesHeight, axesWidth, axesHeight))
+            imshow(rowToMatrix(toPlusRow(row),width), interpolation=&quot;nearest&quot;, cmap = colorMap)
+            setPlotParams(names[i%2] + &quot;_&quot; + str(i) + &quot;_&quot; + str(j), False, True)
+
+def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows_before = None):
+
+    
+    #some calculations for plotting
+
+    #hack
+    if length == -1:
+        length = len(M)
+
+    mWidth = float(len(M[0]))
+    mHeight = float(len(M))
+    sbi = space_between_images
+    plotHeight = mHeight/mWidth*plotWidth
+    cbw = .01 # color bar width
+
+    colorMap = defaultColorMap
+
+    mi,ma = findMinMax(M)
+    
+    ma = max(abs(mi),abs(ma))
+    mi = -ma
+            
+
+    #THE plotting
+    
+    x,y = sbi,sbi
+    
+    for i in arange(start,length):
+    
+        #normal
+        row = M[i]
+        
+        axes((x, y, plotWidth, plotHeight))
+        imshow(rowToMatrix(row, width), interpolation=&quot;nearest&quot;, cmap = colorMap, vmin = mi, vmax = ma)
+        setPlotParams('row_' + str(i), False, True)
+
+        x = x + plotWidth + sbi
+        if x + plotWidth +cbw &gt; 1:
+            x = sbi
+            y = y + plotHeight + sbi
+        if y + plotHeight &gt; 1:
+            # images that follows would be out of the figure...
+            break
+
+    #custom color bar
+    customColorBar(mi,ma,(1.-cbw-sbi, sbi, sbi, 1.-2.*cbw))    
+        
+
+        #1 sur 2 -
+        
+        #subplot(subPlotHeight, subPlotWidth, (2+i%2)*subPlotWidth + j + 1)
+        #axes((i*axesWidth, 2*axesHeight, axesWidth, axesHeight))
+        #imshow(rowToMatrix(toMinusRow(row),width), interpolation=&quot;nearest&quot;, cmap = colorMap)
+        #setPlotParams(str(i), False, True)
+        
+        #1 sur 2 +
+        
+        #subplot(subPlotHeight, subPlotWidth, (4+i%2)*subPlotWidth + i + 1)
+        #axes((i*axesWidth, 4*axesHeight, axesWidth, axesHeight))
+        #imshow(rowToMatrix(toPlusRow(row),width), interpolation=&quot;nearest&quot;, cmap = colorMap)
+        # setPlotParams(str(i), False, True)
+            
+
+def plotCharOLD(char, layers=[], space_between_layers = 5):
+    '''plots a caracter and hidden layers
+    '''  
+    #some 'plotting' consts
+    nbLayers = len(layers)
+    print 'nbLayers', nbLayers
+    totalLayersWidth = 0
+    for layer in layers:
+        totalLayersWidth += len(layer[0]) + space_between_layers
+    totalLayersWidth += space_between_layers
+    print 'totalLayersWidth', totalLayersWidth
+    
+    unit = .9/totalLayersWidth
+    print 'unit', unit
+    sbl = space_between_layers*unit
+    print 'sbl', sbl
+    plotCharWidth = .099
+    plotCharHeight = plotCharWidth
+    #plotLayerWidth = unit
+    plotCharBottom = (1.-plotCharHeight)/2.
+    
+    #plot of the char
+    axes((sbl,          
+          plotCharBottom,
+          plotCharWidth,
+          plotCharHeight))
+    imshow(char, interpolation=&quot;nearest&quot;, cmap = defaultColorMap)
+    setPlotParams(&quot;&quot;, True, True)
+    print 'char ok'
+    #plots of the layers
+
+
+    x,y=sbl,1.-2*len(layers[0])
+    axes((x,y,len(layers[0][0]), len(layers[0])))
+    imshow(layer, interpolation = &quot;nearest&quot;, cmap = defaultColorMap)
+    print 'layer[0] ok'
+    
+    k=1
+    x+=len(layers[0][0])
+    
+    while k &lt; nbLayers:
+
+       
+        
+        plotLayerHeight = unit*len(layers[k])
+        plotLayerWidth = unit*len(layers[k][0])
+        if plotLayerHeight &gt; 1:
+            plotLayerHeight = 1.-2.*sbl
+            
+        plotLayerBottom = (1.-plotLayerHeight)/2.
+
+        print 'k',k
+        print 'x',x
+        print 'plotLayerBottom', plotLayerBottom
+        print 'plotLayerWidth', plotLayer
+        axes((x,plotLayerBottom, plotLayerWidth, plotLayerHeight))
+        imshow(layers[k], interpolation=&quot;nearest&quot;, cmap = defaultColorMap)
+        setPlotParams(&quot;&quot;,True,True)
+        x += plotLayerWidth
+        k += 1
+
+def plotMatrices(matrices, same_color_bar = False, space_between_matrices = 5):
+    '''plot matrices from left to right
+    '''
+    colorMap = cm.gray
+    nbMatrices = len(matrices)
+    print 'plotting ' + str(nbMatrices) + ' matrices'
+
+    totalWidth = 0
+    maxHeight = 0
+    
+    for matrix in matrices:
+        if len(matrix) &gt; maxHeight:
+            maxHeight = len(matrix)
+        totalWidth += len(matrix[0])
+
+
+    unit = min(1./((nbMatrices+1)*space_between_matrices + totalWidth), 1./(maxHeight-2.*space_between_matrices))
+    sbm = space_between_matrices*unit
+
+    x=sbm
+    for matrix in matrices:
+        
+        h = len(matrix)*unit
+        w = len(matrix[0])*unit
+        if h&gt;1 :
+            h = 1.-2*sbm
+        bottom = (1.-h)/2.
+
+        axes(( x,bottom, w,h))
+        imshow(matrix, interpolation = 'nearest', cmap = colorMap)
+        colorbar()
+        x += w+sbm
+
+
+def truncate_imshow(mat, max_height_or_width = 200, width_height_ratio = 1, space_between_submatrices=5):
+
+    matWidth = float(len(mat[0]))
+    matHeight = float(len(mat))
+    
+    mhow = max_height_or_width
+
+    s = float(space_between_submatrices)
+    r = float(width_height_ratio)
+        
+    if (matWidth &gt; mhow and matHeight &lt; mhow) or (matWidth &lt; mhow and matHeight &gt; mhow):
+        
+        if matWidth &gt; matHeight:
+            n = (s + sqrt(s*s + 4.*(matHeight+s)*matWidth/r))/(2.*(matHeight+s))
+        else :
+            n = (s + sqrt(s*s + 4.*(matWidth+s)*matHeight/r))/(2.*(matWidth_s))        
+        newMat = truncateMatrix(mat, n)
+    else:
+        n=1.
+        newMat = [mat]
+    
+    
+    height = len(newMat[0])
+    width = len(newMat[0][0])
+    
+    #on met les submatrices de bas en haut
+    if width&gt;height :
+        
+        #somes plotting constants
+        unit = min(1./(n*height + (n+1)*s), 1./(width+2*s))
+        plotHeight = height*unit
+        plotWidth = width*unit
+        sbs = s*unit        
+        x,y=sbs,sbs
+    
+        for i,matrix in enumerate(newMat):
+            axes((x,y,plotWidth, plotHeight))
+            imshow(matrix, interpolation = defaultInterpolation, cmap = defaultColorMap)
+            setPlotParams(&quot;&quot;,False,True)
+            y = y + plotHeight + sbs
+    
+    #on met les matrices de gauche a droite     
+    else :
+
+        
+        #somes plotting constants
+        unit = min(1./(n*width + (n+1)*s), 1./(height+2*s))
+        plotHeight = height*unit
+        plotWidth = width*unit
+        sbs = s*unit
+        x,y=sbs,sbs
+    
+        for i,matrix in enumerate(newMat):
+            axes((x,y,plotWidth, plotHeight))            
+            imshow(matrix, interpolation = defaultInterpolation, cmap = defaultColorMap)
+            setPlotParams(&quot;&quot;,False,True)
+            x = x + plotWidth + sbs
+
+    #custom colorBar
+    mi,ma = findMinMax(mat)
+    
+    ma = max(abs(mi),abs(ma))
+    mi = -ma
+    customColorBar(mi,ma)
+    
+
+
+################################################
+### somes methods for matrix transformations ###
+################################################
+
+def toPlusRow(row):
+    '''[1,2,3,4,5,6]-&gt;[2,4,6]
+    '''
+    row2 = []
+    for i in arange(1,len(row),2):
+        row2.append(row[i])
+    return row2
+
+def toMinusRow(row):
+    '''[1,2,3,4,5,6]-&gt;[1,3,5]
+    '''
+    row2 = []
+        
+    for i in arange(0,len(row),2):
+        row2.append(row[i])
+    return row2
+
+def rowToMatrix(row, width, validate_size = True, fill_value = 0.):
+    '''change a row [1,2,3,4,5,6] into a matrix [[1,2],[3,4],[5,6]] if width = 2
+       or [1,2,3,4,5,6] -&gt; [[1,2,3,4],[5,6,fill_value,fill_value]] if width = 4 and validate_size = False
+    '''
+    if len(row)%width != 0 and validate_size:
+        raise Exception, &quot;dimensions does not fit ( &quot; + str(width) + &quot; does not divide &quot; + str(len(row)) + &quot;)&quot;
+            
+    m = []
+    k = 0
+    a=-1
+    for i,e in enumerate(row):        
+        if(i%width == 0):        
+            a=a+1
+            m.append([])
+        m[a].append(e)
+
+    # we finish by filling fill the last elements of the last row
+    if len(m)&gt;=2:
+        for i in arange(len(m[-1]),len(m[-2])):
+            m[-1].append(fill_value)
+    
+    return m
+
+def vecToVerticalMatrix(vec):
+    '''ex : [1,2,3,4,5] --&gt; [[1],[2],[3],[4],[5]]
+    '''
+    mat = []
+    for elem in vec:
+        mat.append([elem])
+    return mat
+
+def truncateMatrixOld(mat, maxHeight=10, maxWidth=10):
+    
+    width = len(mat[0])
+    height = len(mat)
+
+    #si notre matrice est trop haute (seulement), on va la couper en morceaux
+    if width &gt; maxWidth and height &lt;= maxHeight:        
+        truncMat = []
+        a = -1
+        for indCol in arange(len(mat[0])):
+            if indCol % maxWidth == 0:               
+                truncMat.append([])
+                a=a+1
+                for l in arange(height):#on ajoute des lignes
+                    truncMat[a].append([])                
+            for l in arange(height):#on remplie les lignes
+                truncMat[a][l].append(mat[l][indCol])            
+            
+        return truncMat
+    #si notre matrice est trop large   (seulement)    
+    elif height &gt; maxHeight and width &lt;= maxWidth:
+        truncMat = [[]]
+        a,l=0,0
+        for ligne in mat:
+            if(l &gt;= maxHeight):
+                truncMat.append([])
+                a=a+1
+                l=0
+            truncMat[a].append(ligne)
+            l=l+1
+            
+        return truncMat
+    elif height &gt; maxHeight and width &gt; maxWidth:
+        print 'matrice trop grosse... rien a faire..'
+        truncMat = [mat]
+    else:
+        truncMat = [mat]
+
+    return truncMat
+
+def truncateMatrix(mat, n=10.):
+        
+    width = len(mat[0])
+    height = len(mat)
+
+    #si notre matrice est trop haute (seulement), on va la couper en morceaux
+    if width &gt; height:
+        maxWidth = int(width/n)
+        truncMat = []
+        a = -1
+        for indCol in arange(len(mat[0])):
+            if indCol % maxWidth == 0:               
+                truncMat.append([])
+                a=a+1
+                for l in arange(height):#on ajoute des lignes
+                    truncMat[a].append([])                
+            for l in arange(height):#on remplie les lignes
+                truncMat[a][l].append(mat[l][indCol])            
+            
+        return truncMat
+    #si notre matrice est trop large   (seulement)    
+    else: # height &gt;= width
+        maxHeight = int(height/n)
+        truncMat = [[]]
+        a,l=0,0
+        for ligne in mat:
+            if(l &gt;= maxHeight):
+                truncMat.append([])
+                a=a+1
+                l=0
+            truncMat[a].append(ligne)
+            l=l+1
+            
+    return truncMat
+
+      
+        
+    

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-06-01 22:04:49 UTC (rev 7503)
+++ trunk/python_modules/plearn/var/Var.py	2007-06-01 22:05:34 UTC (rev 7504)
@@ -54,6 +54,9 @@
     def _serial_number(self):
         return self.v._serial_number()
 
+    def exp(self):
+        return Var(pl.ExpVariable(input=self.v))
+
     def sigmoid(self):
         return Var(pl.SigmoidVariable(input=self.v))
 
@@ -157,19 +160,22 @@
     and an optional parameter b(1,ow)
     Then output = sigmoid(input.W^T + b)
     
-    Returns a triple (hidden, reconstruciton_cost)&quot;&quot;&quot;
+    Returns a triple (hidden, reconstruciton_cost, reconstructed_input)&quot;&quot;&quot;
     W = Var(ow,iw,&quot;uniform&quot;, -1./sqrt(iw), 1./sqrt(iw), varname=basename+'_W')
     
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')        
         hidden = input.matrixProduct_A_Bt(W).add(b).sigmoid()        
         br = Var(1,iw,&quot;fill&quot;,0, varname=basename+'_br')
-        cost = hidden.matrixProduct(W).add(br).negCrossEntropySigmoid(input)
+        reconstr_activation = hidden.matrixProduct(W).add(br)
     else:
         hidden = input.matrixProduct_A_Bt(W).sigmoid()
-        cost = hidden.matrixProduct(W).negCrossEntropySigmoid(input)
-    return hidden, cost
+        reconstr_activation = hidden.matrixProduct(W)
 
+    reconstructed_input = reconstr_activation.sigmoid()
+    cost = reconstr_activation.negCrossEntropySigmoid(input)
+    return hidden, cost, reconstructed_input
+
 def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
     &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
@@ -179,8 +185,10 @@
     M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./sqrt(iw), 1./sqrt(iw), False, varname=basename+&quot;_M&quot;)
     W = Var(ogs, iw, &quot;uniform&quot;, -1./sqrt(iw), 1./sqrt(iw), False, varname=basename+&quot;_W&quot;)
     hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
-    cost = -hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs).dot(input)            
-    return hidden, cost
+    log_reconstructed = hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs)
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxDoubleProductRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
     &quot;&quot;&quot;iw is the input's width
@@ -191,21 +199,28 @@
     hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
     Mr = Var(iw/igs, ow, &quot;uniform&quot;, -1./ow, 1./ow, False, varname=basename+&quot;_Mr&quot;)
     Wr = Var(igs, ow, &quot;uniform&quot;, -1./ow, 1./ow, False, varname=basename+&quot;_Wr&quot;)
-    cost = -hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs).dot(input)       
-    return hidden, cost
+    # TODO: a repenser s'il faut un transpose ou non
+    log_reconstructed = hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs)
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
     W = Var(ow, iw, &quot;uniform&quot;, -1./iw, 1./iw, varname=basename+'_W')
     hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
-    cost = -hidden.matrixProduct(W).multiLogSoftMax(igs).dot(input)
-    return hidden, cost
+    log_reconstructed = hidden.matrixProduct(W).multiLogSoftMax(igs)
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs):
     W = Var(ow, iw, &quot;uniform&quot;, -1./iw, 1./iw, varname=basename+'_W')
     hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
     Wr = Var(ow, iw, &quot;uniform&quot;, -1./ow, 1./ow, varname=basename+'_Wr')
-    cost = -hidden.matrixProduct(Wr).multiLogSoftMax(igs).dot(input)
-    return hidden, cost
+    log_reconstructed = hidden.matrixProduct(Wr).multiLogSoftMax(igs)
+    reconstructed_input = log_reconstructed.exp()
+    cost = log_reconstructed.dot(input).neg()
+    return hidden, cost, reconstructed_input
 
 #################################
 # These build supervised layers


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000952.html">[Plearn-commits] r7503 - trunk/plearn/var
</A></li>
	<LI>Next message: <A HREF="000954.html">[Plearn-commits] r7505 - trunk/python_modules/plearn/pyext
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#953">[ date ]</a>
              <a href="thread.html#953">[ thread ]</a>
              <a href="subject.html#953">[ subject ]</a>
              <a href="author.html#953">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
