<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7519 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL python_modules/plearn/var
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7519%20-%20in%20trunk%3A%20plearn/var/EXPERIMENTAL%0A%09plearn_learners/generic/EXPERIMENTAL%20python_modules/plearn/var&In-Reply-To=%3C200706041633.l54GX4VQ009110%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000967.html">
   <LINK REL="Next"  HREF="000969.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7519 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL python_modules/plearn/var</H1>
    <B>plearner at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7519%20-%20in%20trunk%3A%20plearn/var/EXPERIMENTAL%0A%09plearn_learners/generic/EXPERIMENTAL%20python_modules/plearn/var&In-Reply-To=%3C200706041633.l54GX4VQ009110%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7519 - in trunk: plearn/var/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL python_modules/plearn/var">plearner at mail.berlios.de
       </A><BR>
    <I>Mon Jun  4 18:33:04 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000967.html">[Plearn-commits] r7518 - in trunk/python_modules/plearn/learners: .	autolr online
</A></li>
        <LI>Next message: <A HREF="000969.html">[Plearn-commits] r7520 - in trunk/plearn: io math
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#968">[ date ]</a>
              <a href="thread.html#968">[ thread ]</a>
              <a href="subject.html#968">[ subject ]</a>
              <a href="author.html#968">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: plearner
Date: 2007-06-04 18:33:02 +0200 (Mon, 04 Jun 2007)
New Revision: 7519

Modified:
   trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc
   trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
   trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/var/Var.py
Log:
Fixed deepCopy in new variables and modified options in DeepReconstructorNet to allow specifying a minimum number of stages.


Modified: trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc	2007-06-04 16:33:02 UTC (rev 7519)
@@ -142,9 +142,6 @@
 
     // ### If you want to deepCopy a Var field:
     // varDeepCopyField(somevariable, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR(&quot;DoubleProductVariable::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
 }
 
 void DoubleProductVariable::declareOptions(OptionList&amp; ol)

Modified: trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.cc	2007-06-04 16:33:02 UTC (rev 7519)
@@ -133,9 +133,6 @@
 
     // ### If you want to deepCopy a Var field:
     // varDeepCopyField(somevariable, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR(&quot;ProbabilityPairsVariable::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
 }
 
 void ProbabilityPairsVariable::declareOptions(OptionList&amp; ol)

Modified: trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.cc	2007-06-04 16:33:02 UTC (rev 7519)
@@ -145,9 +145,6 @@
 
     // ### If you want to deepCopy a Var field:
     // varDeepCopyField(somevariable, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR(&quot;TransposedDoubleProductVariable::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
 }
 
 void TransposedDoubleProductVariable::declareOptions(OptionList&amp; ol)

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-04 16:33:02 UTC (rev 7519)
@@ -53,9 +53,8 @@
    &quot;MULTI-LINE \nHELP&quot;);
 
 DeepReconstructorNet::DeepReconstructorNet()
-    :supervised_nepochs(0),
-     good_improvement_rate(-1e10),
-     fine_tuning_improvement_rate(-1e10),
+    :supervised_nepochs(pair&lt;int,int&gt;(0,0)),
+     supervised_min_improvement_rate(-10000),
      minibatch_size(1)
 {
 }
@@ -71,10 +70,26 @@
     // ### (OptionBase::buildoption | OptionBase::nosave)
 
     
-    declareOption(ol, &quot;training_schedule&quot;, &amp;DeepReconstructorNet::training_schedule,
+    declareOption(ol, &quot;unsupervised_nepochs&quot;, &amp;DeepReconstructorNet::unsupervised_nepochs,
                   OptionBase::buildoption,
-                  &quot;training_schedule[k] conatins the number of epochs for the training of the hidden layer taking layer k as input (k=0 corresponds to input layer).&quot;);
+                  &quot;unsupervised_nepochs[k] contains a pair of integers giving the minimum and\n&quot;
+                  &quot;maximum number of epochs for the training of layer k+1 (taking layer k&quot;
+                  &quot;as input). Thus k=0 corresponds to the training of the first hidden layer.&quot;);
 
+    declareOption(ol, &quot;unsupervised_min_improvement_rate&quot;, &amp;DeepReconstructorNet::unsupervised_min_improvement_rate,
+                  OptionBase::buildoption,
+                  &quot;unsupervised_min_improvement_rate[k] should contain the minimum required relative improvement rate\n&quot;
+                  &quot;for the training of layer k+1 (taking input from layer k.)&quot;);
+
+    declareOption(ol, &quot;supervised_nepochs&quot;, &amp;DeepReconstructorNet::supervised_nepochs,
+                  OptionBase::buildoption,
+                  &quot;&quot;);
+
+    declareOption(ol, &quot;supervised_min_improvement_rate&quot;, &amp;DeepReconstructorNet::supervised_min_improvement_rate,
+                  OptionBase::buildoption,
+                  &quot;supervised_min_improvement_rate contains the minimum required relative improvement rate\n&quot;
+                  &quot;for the training of the supervised layer.&quot;);
+
     declareOption(ol, &quot;layers&quot;, &amp;DeepReconstructorNet::layers,
                   OptionBase::buildoption,
                   &quot;layers[0] is the input variable ; last layer is final output layer&quot;);
@@ -119,20 +134,7 @@
                   OptionBase::buildoption,
                   &quot;&quot;);
 
-    declareOption(ol, &quot;good_improvement_rate&quot;, &amp;DeepReconstructorNet::good_improvement_rate,
-                  OptionBase::buildoption,
-                  &quot;&quot;);
-
-    declareOption(ol, &quot;fine_tuning_improvement_rate&quot;, &amp;DeepReconstructorNet::fine_tuning_improvement_rate,
-                  OptionBase::buildoption,
-                  &quot;&quot;);
-
-    declareOption(ol, &quot;supervised_nepochs&quot;, &amp;DeepReconstructorNet::supervised_nepochs,
-                  OptionBase::buildoption,
-                  &quot;&quot;);
-
     
-    
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -243,7 +245,11 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(training_schedule, copies);
+    deepCopyField(unsupervised_nepochs, copies);
+    deepCopyField(unsupervised_min_improvement_rate, copies);
+    deepCopyField(supervised_nepochs, copies);
+    deepCopyField(supervised_min_improvement_rate, copies);
+
     deepCopyField(layers, copies);
     deepCopyField(reconstruction_costs, copies);
     deepCopyField(reconstructed_layers, copies);
@@ -312,13 +318,13 @@
             VMat targets = train_set.subMatColumns(insize, train_set-&gt;targetsize());
             VMat dset = inputs;
 
-            bool must_train_supervised_layer = (training_schedule[training_schedule.length()-2]&gt;0);
+            bool must_train_supervised_layer = supervised_nepochs.second&gt;0;
             
-            PLearn::save(expdir/&quot;learner_0.psave&quot;, this);
+            PLearn::save(expdir/&quot;learner.psave&quot;, this);
             for(int k=0; k&lt;nreconstructions; k++)
             {
                 trainHiddenLayer(k, dset);
-                PLearn::save(expdir/&quot;learner_&quot;+tostring(k+1)+&quot;.psave&quot;, this);
+                PLearn::save(expdir/&quot;learner.psave&quot;, this);
                 // 'if' is a hack to avoid precomputing last hidden layer if not needed
                 if(k&lt;nreconstructions-1 ||  must_train_supervised_layer) 
                 { 
@@ -331,12 +337,19 @@
             }
 
             if(must_train_supervised_layer)
+            {
                 trainSupervisedLayer(dset, targets);
+                PLearn::save(expdir/&quot;learner.psave&quot;, this);
+            }
+            perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
+            perr &lt;&lt; &quot;****      Now performing fine tuning     ****&quot; &lt;&lt; endl;
+            perr &lt;&lt; &quot;********************************************* \n&quot; &lt;&lt; endl;
+
         }
         else
         {
-            pout &lt;&lt; &quot;Fine tuning stage &quot; &lt;&lt; stage+1 &lt;&lt; endl;
-            prepareForFineTuning();            
+            perr &lt;&lt; &quot;+++ Fine tuning stage &quot; &lt;&lt; stage+1 &lt;&lt; &quot; **&quot; &lt;&lt; endl;
+            prepareForFineTuning();
             fineTuningFor1Epoch();
         }
         ++stage;
@@ -436,6 +449,7 @@
     supervised_optimizer-&gt;optimizeN(*train_stats);
 }
 
+/*
 void DeepReconstructorNet::fineTuningFullOld()
 {
     prepareForFineTuning();
@@ -443,7 +457,7 @@
     int l = train_set-&gt;length();
     int nepochs = nstages;
     perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
-    perr &lt;&lt; &quot;*** Performing fine tuning for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** Performing fine tuning for max. &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
     perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
 
     VecStatsCollector st;
@@ -465,15 +479,17 @@
         prev_mean = m;
     }
 }
+*/
 
-
 void DeepReconstructorNet::trainSupervisedLayer(VMat inputs, VMat targets)
 {
     int l = inputs-&gt;length();
+    pair&lt;int,int&gt; nepochs = supervised_nepochs;
+    real min_improvement = supervised_min_improvement_rate;
+
     int last_hidden_layer = layers.length()-2;
-    int nepochs = supervised_nepochs;
     perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
-    perr &lt;&lt; &quot;*** Training only supervised layer for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** Training only supervised layer for max. &quot; &lt;&lt; nepochs.second &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
     perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
 
     Func f(layers[last_hidden_layer]&amp;target, supervised_costvec);
@@ -489,8 +505,8 @@
     supervised_optimizer-&gt;reset();
     VecStatsCollector st;
     real prev_mean = -1;
-    real relative_improvement = good_improvement_rate;
-    for(int n=0; n&lt;nepochs &amp;&amp; relative_improvement &gt;= good_improvement_rate; n++)
+    real relative_improvement = 1000;
+    for(int n=0; n&lt;nepochs.first || (n&lt;nepochs.second &amp;&amp; relative_improvement &gt;= min_improvement); n++)
     {
         st.forget();
         supervised_optimizer-&gt;nstages = l/minibatch_size;
@@ -501,8 +517,8 @@
         perr &lt;&lt; &quot;mean error: &quot; &lt;&lt; m &lt;&lt; &quot; +- &quot; &lt;&lt; s.stderror() &lt;&lt; endl;
         if(prev_mean&gt;0)
         {
-            relative_improvement = ((prev_mean-m)/prev_mean)*100;
-            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; relative_improvement &lt;&lt; &quot; %&quot;&lt;&lt; endl;
+            relative_improvement = (prev_mean-m)/prev_mean;
+            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; relative_improvement*100 &lt;&lt; &quot; %&quot;&lt;&lt; endl;
         }
         prev_mean = m;
         //displayVarGraph(supervised_costvec, true);
@@ -514,9 +530,12 @@
 void DeepReconstructorNet::trainHiddenLayer(int which_input_layer, VMat inputs)
 {
     int l = inputs-&gt;length();
-    int nepochs = training_schedule[which_input_layer];
+    pair&lt;int,int&gt; nepochs = unsupervised_nepochs[which_input_layer];
+    real min_improvement = -10000;
+    if(unsupervised_min_improvement_rate.length()!=0)
+        min_improvement = unsupervised_min_improvement_rate[which_input_layer];
     perr &lt;&lt; &quot;\n\n*********************************************&quot; &lt;&lt; endl;
-    perr &lt;&lt; &quot;*** Training layer &quot; &lt;&lt; which_input_layer+1 &lt;&lt; &quot; for &quot; &lt;&lt; nepochs &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
+    perr &lt;&lt; &quot;*** Training (unsupervised) layer &quot; &lt;&lt; which_input_layer+1 &lt;&lt; &quot; for max. &quot; &lt;&lt; nepochs.second &lt;&lt; &quot; epochs &quot; &lt;&lt; endl;
     perr &lt;&lt; &quot;*** each epoch has &quot; &lt;&lt; l &lt;&lt; &quot; examples and &quot; &lt;&lt; l/minibatch_size &lt;&lt; &quot; optimizer stages (updates)&quot; &lt;&lt; endl;
     Func f(layers[which_input_layer], reconstruction_costs[which_input_layer]);
     //displayVarGraph(reconstruction_costs[which_input_layer]);
@@ -525,22 +544,39 @@
     VarArray params = totalcost-&gt;parents();
     reconstruction_optimizer-&gt;setToOptimize(params, totalcost);
     reconstruction_optimizer-&gt;reset();
+
+    TVec&lt;string&gt; colnames(4);
+    colnames[0] = &quot;nepochs&quot;;
+    colnames[1] = &quot;reconstr_cost&quot;;
+    colnames[2] = &quot;stderror&quot;;
+    colnames[3] = &quot;relative_improvement&quot;;
+    VMat training_curve = new FileVMatrix(expdir/&quot;training_costs_layer_&quot;+tostring(which_input_layer+1)+&quot;.pmat&quot;,0,colnames);
+    Vec costrow(4);
+
     VecStatsCollector st;
     real prev_mean = -1;
-    real relative_improvement = good_improvement_rate;
-    for(int n=0; n&lt;nepochs &amp;&amp; relative_improvement &gt;= good_improvement_rate; n++)
+    real relative_improvement = 1000;
+    for(int n=0; n&lt;nepochs.first || (n&lt;nepochs.second &amp;&amp; relative_improvement &gt;= min_improvement); n++)
     {
         st.forget();
         reconstruction_optimizer-&gt;nstages = l/minibatch_size;
         reconstruction_optimizer-&gt;optimizeN(st);
         const StatsCollector&amp; s = st.getStats(0);
         real m = s.mean();
+        real er = s.stderror();
         perr &lt;&lt; &quot;Epoch &quot; &lt;&lt; n+1 &lt;&lt; &quot; mean error: &quot; &lt;&lt; m &lt;&lt; &quot; +- &quot; &lt;&lt; s.stderror() &lt;&lt; endl;
         if(prev_mean&gt;0)
         {
-            relative_improvement = ((prev_mean-m)/prev_mean)*100;
-            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; relative_improvement &lt;&lt; &quot; %&quot;&lt;&lt; endl;
+            relative_improvement = (prev_mean-m)/prev_mean;
+            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; relative_improvement*100 &lt;&lt; &quot; %&quot;&lt;&lt; endl;
         }
+        costrow[0] = n+1;
+        costrow[1] = m;
+        costrow[2] = er;
+        costrow[3] = relative_improvement*100;
+        training_curve-&gt;appendRow(costrow);
+        training_curve-&gt;flush();
+
         prev_mean = m;
     }
 }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-04 16:33:02 UTC (rev 7519)
@@ -70,16 +70,12 @@
     //! Start your comments with Doxygen-compatible comments such as //!
     
 
-    //! training_schedule[k] conatins the number of nstages to run the optimizer for the
-    //! training of the hidden layer taking layer k as input (k=0 corresponds to
-    //! input layer).
+    TVec&lt; pair&lt;int,int&gt; &gt; unsupervised_nepochs;
+    Vec unsupervised_min_improvement_rate;
 
-    TVec&lt;int&gt; training_schedule;
-    int supervised_nepochs;    
+    pair&lt;int,int&gt; supervised_nepochs;    
+    real supervised_min_improvement_rate;
 
-    real good_improvement_rate;
-    real fine_tuning_improvement_rate;
-
     // layers[0] is the input variable
     // last layer is final output layer
     VarArray layers;
@@ -181,7 +177,7 @@
 
     void prepareForFineTuning();
     void fineTuningFor1Epoch();
-    void fineTuningFullOld();
+    // void fineTuningFullOld();
 
     void trainSupervisedLayer(VMat inputs, VMat targets);
 

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-06-04 15:09:41 UTC (rev 7518)
+++ trunk/python_modules/plearn/var/Var.py	2007-06-04 16:33:02 UTC (rev 7519)
@@ -69,8 +69,8 @@
         # So we make sure data is flushed to disk.
         return self.v.plearn_repr(indent_level, inner_repr)
 
-    def probabilityPairs(self, min, max):
-        return Var(pl.ProbabilityPairsVariable(input=self.v, min=min, max=max))
+    def probabilityPairs(self, min, max, varname=&quot;&quot;):
+        return Var(pl.ProbabilityPairsVariable(input=self.v, min=min, max=max, varname=varname))
 
     def transpose(self):
         return Var(pl.TransposeVariable(input=self.v))
@@ -176,16 +176,22 @@
     cost = reconstr_activation.negCrossEntropySigmoid(input)
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, basename=&quot;&quot;):
+def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, constrain_mask=False, basename=&quot;&quot;):
     &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output&quot;&quot;&quot;
-    #M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_M&quot;)
-    #W = Var(ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_W&quot;)
-    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./sqrt(iw), 1./sqrt(iw), False, varname=basename+&quot;_M&quot;)
-    W = Var(ogs, iw, &quot;uniform&quot;, -1./sqrt(iw), 1./sqrt(iw), False, varname=basename+&quot;_W&quot;)
-    hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
-    log_reconstructed = hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs)
+    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_M&quot;)
+    if constrain_mask:
+        M = M.sigmoid()
+    W = Var(ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_W&quot;)
+    if add_bias:
+        b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
+        hidden = input.doubleProduct(W,M).add(b).multiSoftMax(ogs)
+        br = Var(1,iw,&quot;fill&quot;,0, varname=basename+'_br')
+        log_reconstructed = hidden.transposeDoubleProduct(W,M).add(br).multiLogSoftMax(igs)
+    else:
+        hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
+        log_reconstructed = hidden.transposeDoubleProduct(W,M).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000967.html">[Plearn-commits] r7518 - in trunk/python_modules/plearn/learners: .	autolr online
</A></li>
	<LI>Next message: <A HREF="000969.html">[Plearn-commits] r7520 - in trunk/plearn: io math
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#968">[ date ]</a>
              <a href="thread.html#968">[ thread ]</a>
              <a href="subject.html#968">[ subject ]</a>
              <a href="author.html#968">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
