<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7608 - in trunk:	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/plotting python_modules/plearn/var	scripts scripts/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7608%20-%20in%20trunk%3A%0A%09plearn_learners/generic/EXPERIMENTAL%0A%09python_modules/plearn/plotting%20python_modules/plearn/var%0A%09scripts%20scripts/EXPERIMENTAL&In-Reply-To=%3C200706192020.l5JKK4K6000117%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001055.html">
   <LINK REL="Next"  HREF="001057.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7608 - in trunk:	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/plotting python_modules/plearn/var	scripts scripts/EXPERIMENTAL</H1>
    <B>simonl at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7608%20-%20in%20trunk%3A%0A%09plearn_learners/generic/EXPERIMENTAL%0A%09python_modules/plearn/plotting%20python_modules/plearn/var%0A%09scripts%20scripts/EXPERIMENTAL&In-Reply-To=%3C200706192020.l5JKK4K6000117%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7608 - in trunk:	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/plotting python_modules/plearn/var	scripts scripts/EXPERIMENTAL">simonl at mail.berlios.de
       </A><BR>
    <I>Tue Jun 19 22:20:04 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001055.html">[Plearn-commits] r7607 - trunk/python_modules/plearn/pyplearn
</A></li>
        <LI>Next message: <A HREF="001057.html">[Plearn-commits] r7609 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1056">[ date ]</a>
              <a href="thread.html#1056">[ thread ]</a>
              <a href="subject.html#1056">[ subject ]</a>
              <a href="author.html#1056">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: simonl
Date: 2007-06-19 22:20:02 +0200 (Tue, 19 Jun 2007)
New Revision: 7608

Added:
   trunk/scripts/EXPERIMENTAL/
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/var/Var.py
Log:
Many improvements to deepreconstructorenet stuff


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2007-06-19 20:20:02 UTC (rev 7608)
@@ -102,6 +102,10 @@
                   OptionBase::buildoption,
                   &quot;reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]&quot;);
 
+    declareOption(ol, &quot;reconstruction_optimizers&quot;, &amp;DeepReconstructorNet::reconstruction_optimizers,
+                  OptionBase::buildoption,
+                  &quot;&quot;);
+
     declareOption(ol, &quot;reconstruction_optimizer&quot;, &amp;DeepReconstructorNet::reconstruction_optimizer,
                   OptionBase::buildoption,
                   &quot;&quot;);
@@ -153,7 +157,16 @@
                    ArgDoc(&quot;varname&quot;, &quot;name of the variable searched for&quot;),
                    RetDoc(&quot;Returns the value of the parameter as a Mat&quot;)));
 
+    declareMethod(rmm,
+                  &quot;getParameterRow&quot;,
+                  &amp;DeepReconstructorNet::getParameterRow,
+                  (BodyDoc(&quot;Returns the matValue of the parameter variable with the given name&quot;),
+                   ArgDoc(&quot;varname&quot;, &quot;name of the variable searched for&quot;),
+                   ArgDoc(&quot;n&quot;, &quot;row number&quot;),
+                   RetDoc(&quot;Returns the nth row of the value of the parameter as a Mat&quot;)));
 
+
+
     declareMethod(rmm,
                   &quot;listParameterNames&quot;,
                   &amp;DeepReconstructorNet::listParameterNames,
@@ -179,6 +192,35 @@
                   (BodyDoc(&quot;Compute the reconstructions of the input of each hidden layer&quot;),
                    ArgDoc(&quot;input&quot;, &quot;the input&quot;),
                    RetDoc(&quot;The reconstructions&quot;)));
+
+    declareMethod(rmm,
+                   &quot;getMatValue&quot;,
+                   &amp;DeepReconstructorNet::getMatValue,
+                   (BodyDoc(&quot;&quot;),
+                    ArgDoc(&quot;layer&quot;, &quot;no of the layer&quot;),
+                    RetDoc(&quot;the matValue&quot;)));
+
+    declareMethod(rmm,
+                   &quot;setMatValue&quot;,
+                   &amp;DeepReconstructorNet::setMatValue,
+                   (BodyDoc(&quot;&quot;),
+                    ArgDoc(&quot;layer&quot;, &quot;no of the layer&quot;),
+                    ArgDoc(&quot;values&quot;, &quot;the values&quot;)));
+
+    declareMethod(rmm,
+                   &quot;fpropOneLayer&quot;,
+                   &amp;DeepReconstructorNet::fpropOneLayer,
+                   (BodyDoc(&quot;&quot;),
+                    ArgDoc(&quot;layer&quot;, &quot;no of the layer&quot;),
+                    RetDoc(&quot;&quot;)));
+
+
+    declareMethod(rmm,
+                   &quot;reconstructOneLayer&quot;,
+                   &amp;DeepReconstructorNet::reconstructOneLayer,
+                   (BodyDoc(&quot;&quot;),
+                    ArgDoc(&quot;layer&quot;, &quot;no of the layer&quot;),
+                    RetDoc(&quot;&quot;)));
 }
 
 void DeepReconstructorNet::build_()
@@ -256,6 +298,7 @@
     deepCopyField(layers, copies);
     deepCopyField(reconstruction_costs, copies);
     deepCopyField(reconstructed_layers, copies);
+    deepCopyField(reconstruction_optimizers, copies);
     deepCopyField(reconstruction_optimizer, copies);
     varDeepCopyField(target, copies);
     deepCopyField(supervised_costs, copies);
@@ -562,8 +605,17 @@
     //displayFunction(f,false,false, 333, &quot;train_func&quot;);
     Var totalcost = sumOf(inputs, f, minibatch_size);
     VarArray params = totalcost-&gt;parents();
-    reconstruction_optimizer-&gt;setToOptimize(params, totalcost);
-    reconstruction_optimizer-&gt;reset();
+    
+    if ( reconstruction_optimizers.size() !=0 )
+    {
+        reconstruction_optimizers[which_input_layer]-&gt;setToOptimize(params, totalcost);
+        reconstruction_optimizers[which_input_layer]-&gt;reset();    
+    }
+    else 
+    {
+        reconstruction_optimizer-&gt;setToOptimize(params, totalcost);
+        reconstruction_optimizer-&gt;reset();    
+    }
 
     TVec&lt;string&gt; colnames(4);
     colnames[0] = &quot;nepochs&quot;;
@@ -579,8 +631,16 @@
     for(int n=0; n&lt;nepochs.first || (n&lt;nepochs.second &amp;&amp; relative_improvement &gt;= min_improvement); n++)
     {
         st.forget();
-        reconstruction_optimizer-&gt;nstages = l/minibatch_size;
-        reconstruction_optimizer-&gt;optimizeN(st);
+        if ( reconstruction_optimizers.size() !=0 )
+        {
+            reconstruction_optimizers[which_input_layer]-&gt;nstages = l/minibatch_size;
+            reconstruction_optimizers[which_input_layer]-&gt;optimizeN(st);
+        }
+        else 
+        {
+            reconstruction_optimizer-&gt;nstages = l/minibatch_size;
+            reconstruction_optimizer-&gt;optimizeN(st);
+        }        
         const StatsCollector&amp; s = st.getStats(0);
         real m = s.mean();
         real er = s.stderror();
@@ -634,6 +694,16 @@
     return Mat(0,0);
 }
 
+
+Vec DeepReconstructorNet::getParameterRow(const string&amp; varname, int n)
+{
+    for(int i=0; i&lt;parameters.length(); i++)
+        if(parameters[i]-&gt;getName() == varname)
+            return parameters[i]-&gt;matValue(n);
+    PLERROR(&quot;There is no parameter  named %s&quot;, varname.c_str());
+    return Vec(0);
+}
+
 TVec&lt;string&gt; DeepReconstructorNet::listParameterNames()
 {
     TVec&lt;string&gt; nameListe(0);
@@ -652,6 +722,31 @@
 }
 
 
+Mat DeepReconstructorNet::getMatValue(int layer)
+{
+    return layers[layer]-&gt;matValue;
+}
+
+void DeepReconstructorNet::setMatValue(int layer, Mat values)
+{
+    layers[layer]-&gt;matValue &lt;&lt; values;
+}
+
+Mat DeepReconstructorNet::fpropOneLayer(int layer)
+{
+    VarArray proppath = propagationPath( layers[layer], layers[layer+1] );
+    proppath.fprop();
+    return getMatValue(layer+1);
+}
+
+Mat DeepReconstructorNet::reconstructOneLayer(int layer)
+{
+    VarArray proppath = propagationPath(layers[layer],reconstructed_layers[layer-1]);
+    proppath.fprop();       
+    return reconstructed_layers[layer-1]-&gt;matValue;
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2007-06-19 20:20:02 UTC (rev 7608)
@@ -86,6 +86,10 @@
     // reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]
     VarArray reconstructed_layers;
 
+    // optimizers if we use different ones for each layer
+    TVec&lt; PP&lt;Optimizer&gt; &gt; reconstruction_optimizers;
+    
+    // if we use always the same optimizer
     PP&lt;Optimizer&gt; reconstruction_optimizer;
 
 
@@ -172,6 +176,9 @@
     //! Returns the matValue of the parameter variable with the given name
     Mat getParameterValue(const string&amp; varname);
 
+    //! Returns the nth row of the matValue of the parameter variable with the given name
+    Vec getParameterRow(const string&amp; varname, int n);
+
     //! Returns a list of the names of the parameters (in the same order as in listParameter)
     TVec&lt;string&gt; listParameterNames();
 
@@ -188,6 +195,12 @@
     void reconstructInputFromLayer(int layer);
     TVec&lt;Mat&gt; computeReconstructions(Mat input);
 
+    Mat getMatValue(int layer);
+    void setMatValue(int layer, Mat values);
+    Mat fpropOneLayer(int layer);
+    Mat reconstructOneLayer(int layer);
+       
+
     
 
     // *** SUBCLASS WRITING: ***

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-06-19 20:20:02 UTC (rev 7608)
@@ -1,4 +1,5 @@
 from pylab import *
+from numarray import *
 
 
 #################
@@ -149,61 +150,65 @@
     #custom color bar
     customColorBar(mi,ma,(1.-cbw-sbi, sbi, sbi, 1.-2.*cbw))
     return toReturn
-        
 
-        #1 sur 2 -
-        
-        #subplot(subPlotHeight, subPlotWidth, (2+i%2)*subPlotWidth + j + 1)
-        #axes((i*axesWidth, 2*axesHeight, axesWidth, axesHeight))
-        #imshow(rowToMatrix(toMinusRow(row),width), interpolation=&quot;nearest&quot;, cmap = colorMap)
-        #setPlotParams(str(i), False, True)
-        
-        #1 sur 2 +
-        
-        #subplot(subPlotHeight, subPlotWidth, (4+i%2)*subPlotWidth + i + 1)
-        #axes((i*axesWidth, 4*axesHeight, axesWidth, axesHeight))
-        #imshow(rowToMatrix(toPlusRow(row),width), interpolation=&quot;nearest&quot;, cmap = colorMap)
-        # setPlotParams(str(i), False, True)
-            
 
 
 
-def plotMatrices(matrices, same_color_bar = False, space_between_matrices = 5):
+def plotMatrices(matrices, names = None, ticks = False, same_color_bar = False, space_between_matrices = 5):
     '''plot matrices from left to right
     TODO : same_color_bar does nothing !!
     '''
+    
     colorMap = cm.gray
     nbMatrices = len(matrices)
-    #print 'plotting ' + str(nbMatrices) + ' matrices'
+    print 'plotting ' + str(nbMatrices) + ' matrices'
 
     totalWidth = 0
     maxHeight = 0
     
     for matrix in matrices:
-        if len(matrix) &gt; maxHeight:
-            maxHeight = len(matrix)
-        totalWidth += len(matrix[0])
+        #print matrix.info()
+        if matrix.shape[0] &gt; maxHeight:
+            maxHeight = matrix.shape[0]
+        totalWidth += matrix.shape[1]
+    print maxHeight, totalWidth
+    
 
+    #to prevent a little bug   
+    space_between_matrices = min(space_between_matrices, maxHeight, totalWidth)
 
-    unit = min(1./((nbMatrices+1)*space_between_matrices + totalWidth), 1./(maxHeight-2.*space_between_matrices))
+    unit = min(1./((nbMatrices+1)*space_between_matrices + totalWidth), 1./(maxHeight+2.*space_between_matrices))
     sbm = space_between_matrices*unit
 
+
     x=sbm
     the_axes = []
-    for matrix in matrices:
+    
+    if names != None:
+        if len(names) != len(matrices):
+            raise Exception, &quot;nb of matrices and nb of names must be equals in plotMatrices()&quot;
+    else:
+        names = ['']*len(matrices)
         
-        h = len(matrix)*unit
-        w = len(matrix[0])*unit
+    for matrix,name in zip(matrices,names):
+        
+        h = matrix.shape[0]*unit
+        w = matrix.shape[1]*unit
+
         if h&gt;1 :
             h = 1.-2*sbm
         bottom = (1.-h)/2.
 
+        #print x,bottom, w, h
         temp = axes(( x,bottom, w,h))
         the_axes.append(temp)
         imshow(matrix, interpolation = 'nearest', cmap = colorMap)
+        title(name)       
+        if ticks == False:
+            xticks([],[])
+            yticks([],[])
         colorbar()
         x += w+sbm
-
     return the_axes
 
 

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/python_modules/plearn/var/Var.py	2007-06-19 20:20:02 UTC (rev 7608)
@@ -164,7 +164,8 @@
     Then output = sigmoid(input.W^T + b)
     
     Returns a triple (hidden, reconstruciton_cost, reconstructed_input)&quot;&quot;&quot;
-    W = Var(ow,iw,&quot;uniform&quot;, -1./sqrt(iw), 1./sqrt(iw), varname=basename+'_W')
+    ra = 1./max(iw,ow)
+    W = Var(ow,iw,&quot;uniform&quot;, -ra, 1./ra, varname=basename+'_W')
     
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')        
@@ -183,10 +184,12 @@
     &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output&quot;&quot;&quot;
-    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_M&quot;)
+    ra = 1./max(iw,ow)
+    sqra = sqrt(ra)
+    M = Var(ow/ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_M&quot;)
     if constrain_mask:
         M = M.sigmoid()
-    W = Var(ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_W&quot;)
+    W = Var(ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_W&quot;)
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
         hidden = input.doubleProduct(W,M).add(b).multiSoftMax(ogs)
@@ -203,11 +206,13 @@
     &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output&quot;&quot;&quot;
-    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_M&quot;)
-    W = Var(ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_W&quot;)
+    ra = 1./max(iw,ow)
+    sqra = sqrt(ra)
+    M = Var(ow/ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_M&quot;)
+    W = Var(ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_W&quot;)
     hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
-    Mr = Var(iw/igs, ow, &quot;uniform&quot;, -1./ow, 1./ow, False, varname=basename+&quot;_Mr&quot;)
-    Wr = Var(igs, ow, &quot;uniform&quot;, -1./ow, 1./ow, False, varname=basename+&quot;_Wr&quot;)
+    Mr = Var(iw/igs, ow, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_Mr&quot;)
+    Wr = Var(igs, ow, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_Wr&quot;)
     # TODO: a repenser s'il faut un transpose ou non
     log_reconstructed = hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
@@ -218,9 +223,11 @@
     &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output&quot;&quot;&quot;
-    M = Var(ow/ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_M&quot;)
-    W = Var(ogs, iw, &quot;uniform&quot;, -1./iw, 1./iw, False, varname=basename+&quot;_W&quot;)
-    Wr = Var(ow, iw, &quot;uniform&quot;, -1./ow, 1./ow, varname=basename+'_Wr')
+    ra = 1./max(iw,ow)
+    sqra = sqrt(ra)
+    M = Var(ow/ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_M&quot;)
+    W = Var(ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_W&quot;)
+    Wr = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_Wr')
 
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
@@ -236,7 +243,8 @@
     return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=&quot;&quot;):
-    W = Var(ow, iw, &quot;uniform&quot;, -1./iw, 1./iw, varname=basename+'_W')
+    ra = 1./max(iw,ow)
+    W = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_W')
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
         hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)
@@ -250,8 +258,9 @@
     return hidden, cost, reconstructed_input
 
 def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=&quot;&quot;):
-    W = Var(ow, iw, &quot;uniform&quot;, -1./iw, 1./iw, varname=basename+'_W')
-    Wr = Var(ow, iw, &quot;uniform&quot;, -1./ow, 1./ow, varname=basename+'_Wr')
+    ra = 1./max(iw,ow)
+    W = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_W')
+    Wr = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_Wr')
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
         hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)

Added: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-06-19 20:05:26 UTC (rev 7607)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-06-19 20:20:02 UTC (rev 7608)
@@ -0,0 +1,789 @@
+#!/usr/bin/env python
+
+import sys
+from pylab import *
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting.netplot import *
+from numarray import *
+from numarray.random_array import *
+import numpy.random
+
+
+################
+### methods ###
+################
+
+def print_usage_and_exit():
+    print &quot;Usage: drnPlot &lt;task&gt; &lt;file&gt; [&lt;other arguments&gt;]&quot;,    
+    &quot;drnPlot plotSingleMatrix x.psave &quot;,
+    &quot;drnPlot plotEachRow learner.psave chars.pmat&quot;
+    &quot;drnPlot plotRepAndRec learner.psave chars.pmat&quot;
+    sys.exit()
+
+
+def appendMatrixToFile(file, matrix, matrix_name=&quot;&quot;):
+    file.write(&quot;\n\n&quot; + matrix_name + ' ('+ str(len(matrix)) + 'x' + str(len(matrix[0])) + ')\n\n')
+    for i, row in enumerate(matrix):
+        file.write('[')
+        for j, el in enumerate(row):
+            file.write(str(el) + ', ')
+        file.write(']\n')
+        
+
+class HiddenLayer:
+    
+    def __init__(self,hidden_Layer, groupsize):
+        self.hidden_layer = hidden_Layer
+        self.groupsize = groupsize
+
+        self.max_height = 150
+
+        #if it's too tall, we do this little tweak
+        gs = self.groupsize
+        height = self.hidden_layer.size()/gs
+        self.nbgroups = 1
+        while height &gt; self.max_height:
+            self.nbgroups+=1
+            while (self.hidden_layer.size()/gs)%self.nbgroups != 0 :
+                self.nbgroups+=1                       
+            height = self.hidden_layer.size()/gs/self.nbgroups
+        print 'nbgroups', self.nbgroups
+        
+    def getMatrix(self):        
+        return reshape(self.hidden_layer, (-1,self.groupsize*self.nbgroups))
+    
+    def matrixToLayer(self, x, y):
+        gs = self.groupsize
+        x,y = self.correctXY(x,y)
+        return x + gs*self.nbgroups*y
+
+    def correctXY(self,x,y):
+        return int(x + .4), int(y + .3)        
+
+    def getElement(self, x, y):
+        n = self.matrixToLayer(x,y)
+        return self.hidden_layer[n]
+
+    def setElement(self, x,y, value):
+        n = self.matrixToLayer(x,y)
+        self.hidden_layer[n] = value
+
+    def getRow(self,n):
+        return self.hidden_layer[n*self.groupsize:(n+1)*self.groupsize]
+
+    def setRow(self,n,row):
+        c=0
+        for i in arange(n*self.groupsize,(n+1)*self.groupsize):
+           self.hidden_layer[i] = row[c]
+           c+=1
+
+
+
+class InteractiveRepRecPlotter:
+    
+    def __init__(self, learner, vmat, image_width=28, char_indice=0):
+        '''constructor'''
+        self.current = char_indice-1#-1 because it's the first time
+        self.learner = learner
+        self.vmat = vmat
+        self.char = -1#the char we're looking for, -1 for anyone (it can be -1,0,1,2,3,4,5,6,7,8,9)
+        self.image_width = image_width
+
+        self.fig_rec = 0
+        self.fig_rep = 1
+        figure(self.fig_rec)
+        figure(self.fig_rep)
+
+        #self.current_fig = None
+        #self.current_axes = None
+        self.current_hl = None#current hidden layer
+
+        #plotting constants
+        self.interpolation = 'nearest'
+        self.cmap = cm.gray
+
+        self.plotNext()#starting with a plot...
+        self.__linkEvents()
+               
+
+
+    def size(self):
+        return len(self.originals_hl)
+        
+
+    ###
+    ### getting char from vmat
+    ###
+    
+    def __rowToClassInput(self, row):
+        '''we put the last element of row in self.classe, the rest in self.input'''
+        self.classe = row[-1:][0]
+        self.input = row[:-1]
+
+    def __getNextChar(self):
+        '''get next input row from the vmat'''        
+        while True:
+            self.current+=1
+            raw_input = vmat.getRow(self.current)
+            classe = int(raw_input[-1:])
+            if classe == self.char or self.char == -1:
+                break            
+        self.__rowToClassInput(raw_input)
+
+    def __getPrevChar(self):
+        '''get next input row from the vmat'''        
+        while True:
+            self.current-=1
+            raw_input = vmat.getRow(self.current)
+            classe = int(raw_input[-1:])
+            if classe == self.char or self.char == -1:
+                break            
+        self.__rowToClassInput(raw_input)
+
+    ###
+    ### computings
+    ###
+
+    def __computeRepresentation(self):
+        
+        # we convert list to tmat
+        imagetmat = TMat([self.input])
+        
+        #representation
+        print 'computing representations...'
+        raw_rep = learner.computeRepresentations(imagetmat)        
+        print '...done.'
+
+        try:
+            #groupsizes = self.learner.groupsizes[1:]
+            groupsizes = list(self.learner.getOption('group_sizes')[1:])
+        except:
+            groupsizes = [10,20,40]
+        if groupsizes == []:
+            groupsizes = [10, 20, 40]
+        groupsizes.insert(0, self.image_width)
+        groupsizes.append(1)
+
+        print groupsizes
+
+        self.hidden_layers = []
+
+        for gs,el in zip(groupsizes,raw_rep):
+            self.hidden_layers.append(HiddenLayer(el[0], gs))
+
+    def __computeReconstructions(self):
+
+        imagetmat = TMat([self.input])
+        print 'computing reconstructions...'
+        rec = learner.computeReconstructions(imagetmat)
+        print '...done.'
+
+        matrices = [rowToMatrix(self.input, self.image_width)]
+        for el in rec:
+            row = el[0]
+            matrices.append(rowToMatrix(row,self.image_width))
+
+        self.reconstructions = matrices
+       
+
+    ###
+    ### events
+    ###
+
+        
+    def __changeChar(self, event):
+        char = event.key
+        if char in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']:
+            self.char = int(char)
+            print 'now plotting only this digit :', char
+        elif char == '.':
+            self.char = -1
+            print 'now plotting any digit'
+        elif char == 'right':
+            self.plotNext()
+        elif char == 'left':
+            self.plotPrev()
+        elif char == '':
+            pass            
+
+
+    def __repCommands(self,event):
+
+        #met a jour self.current_hl
+        self.__findCurrentLayer(event)
+        
+        char = event.key        
+        i = self.current_hl
+               
+        if i != -1:
+
+            #commun
+
+            hl1 = self.hidden_layers[i]
+            if i &gt;  0:
+                hl0 = self.hidden_layers[i-1]
+            if i &lt; self.size()-1:
+                hl2 = self.hidden_layers[i+1]
+
+            hl = hl1
+                        
+            # fprop -- f
+
+            if char == 'f':                
+                print 'fproping...'
+                #update                
+                self.learner.setMatValue(i-1, reshape(hl0.hidden_layer, (1,-1)))
+                #fprop
+                row = self.learner.fpropOneLayer(i-1)[0]
+                #print
+                hl1.hidden_layer = row
+                self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                print '...done'
+
+            # big-fprop -- F
+
+            elif char == 'F':
+                print 'big-fproping...'                
+                for k in arange(i-1, self.size()-1):
+                    print 'k',k
+                    self.learner.setMatValue(k, reshape(self.hidden_layers[k].hidden_layer, (1,-1)))
+                    row = self.learner.fpropOneLayer(k)[0]
+                    self.hidden_layers[k+1].hidden_layer = row
+                    self.rep_axes[k+1].imshow(self.hidden_layers[k+1].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                    draw()
+                print '...done'
+                
+            # reconstruction -- r
+                
+            elif char == 'r':
+                print 'reconstructing...'
+                self.learner.setMatValue(i+1, reshape(hl2.hidden_layer, (1,-1)))
+                #reconstruct
+                row = self.learner.reconstructOneLayer(i+1)[0]
+                #HACK
+                if len(row) == 28*28*2:
+                    print 'hacking...'
+                    row = array(toMinusRow(row))
+                #print the new layer
+                hl1.hidden_layer = row
+                self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)                 
+                draw()
+                print '...done'
+
+            # big-reconstruction -- R
+
+            elif char == 'R':
+                print 'big-reconstructing...'
+                for k in arange(i+1, 0, -1):
+                    self.learner.setMatValue(k, reshape(self.hidden_layers[k].hidden_layer, (1,-1)))
+                    row = self.learner.reconstructOneLayer(k)[0]
+                    #HACK
+                    if len(row) == 28*28*2:
+                        print 'hacking...'
+                        row = array(toMinusRow(row))
+                    self.hidden_layers[k-1].hidden_layer = row
+                    self.rep_axes[k-1].imshow(self.hidden_layers[k-1].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                    draw()
+                print '...done'                
+
+            # max -- m
+
+            elif char == 'm':
+                print 'maximum...'
+
+                for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
+
+                    row = hl.getRow(n)
+
+                    #finding out the max
+                    indmax = 0
+                    for el in arange(1,len(row)):
+                        if row[el] &gt; row[indmax]:                            
+                            indmax = el                    
+
+                    #set max = 1, other = 0
+                    for el in arange(len(row)):
+                        if el == indmax:
+                            row[el] = 1.
+                        else:
+                           row[el] = 0.
+
+                    hl.setRow(n,row)
+                            
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                
+                print '...done'
+
+            # sampling -- s
+
+            elif char == 's':
+                print 'sampling...'
+                for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
+                    print 'sum', hl.getRow(n).sum()
+                    multi = numpy.random.multinomial(1,hl.getRow(n))                    
+                    hl.setRow(n,multi)                            
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                print '...done'
+                
+            # set pixel -- z,x,c,v,b
+
+            elif char in ['z', 'x', 'c', 'v', 'b']:
+                
+                x,y = event.xdata, event.ydata
+                
+                if char == 'z':
+                    hl.setElement(x,y,0.)
+                elif char == 'x':
+                    hl.setElement(x,y,.25)
+                elif char == 'c':
+                    hl.setElement(x,y,.5)
+                elif char == 'v':
+                    hl.setElement(x,y,.75)
+                elif char == 'b':
+                    hl.setElement(x,y,1.)
+
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                                            
+            # infos -- ' '
+
+            elif char == ' ':
+                x,y = event.xdata, event.ydata
+                print
+                print 'Position', hl.matrixToLayer(x,y), '(', x, y, ')'
+                print 'Value', hl.getElement(x,y)                
+
+            # plot W and M -- w
+
+            elif char == 'w':
+
+                prefixe = 'Layer' + str(i)
+                nameW = prefixe + '_W'
+                nameWr = nameW + 'r'
+                nameM = prefixe + '_M'
+                nameMr = nameM + 'r'
+                nameB = prefixe + '_b'
+                nameBr = nameB + 'r'
+             
+                listNames = learner.listParameterNames()
+
+                matricesToPlot = []
+                namesToPlot = []
+
+                x,y = hl.correctXY(event.xdata,event.ydata)
+                n = hl.matrixToLayer(x, y)
+                
+                
+                if nameW in listNames and nameM not in listNames:
+                    
+                    row = learner.getParameterRow(nameW,n)
+                    
+                    #HACK !!!
+                    print 'just hacked...'
+                    if i==1 and len(row) == 28*28*2:
+                        row = array(toMinusRow(list(row)))
+                    #END OF HACK
+
+                    matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                    namesToPlot.append(nameW)
+
+                    if nameWr in listNames:
+
+                        row = learner.getParameterRow(nameWr,n)
+                    
+                        #HACK !!!
+                        print 'just hacked...'
+                        if i==1 and len(row) == 28*28*2:
+                            row = array(toMinusRow(list(row)))
+                        #END OF HACK 
+                        
+                        matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                        namesToPlot.append(nameWr)
+
+                    figure(3)
+                    clf()
+                    plotMatrices(matricesToPlot, namesToPlot)
+                    draw()
+
+                if nameW in listNames and nameM in listNames:
+
+                    rowW = learner.getParameterRow(nameW,x)
+                    rowM = learner.getParameterRow(nameM,y)
+                    
+                    #HACK !!!
+                    #print 'just hacked...'
+                    #if i==1 and len(row) == 28*28*2:
+                    #    row = array(toMinusRow(row))
+                    #END OF HACK
+                                      
+                    rowW = reshape(rowW, (-1,self.hidden_layers[i-1].groupsize))
+                    rowM = reshape(rowM,(-1,self.hidden_layers[i-1].groupsize))
+
+                    #TODO: rajouter les deux cas  : juste un Wr et lautre : Mr ET Wr
+
+                    produit = rowW*rowM                   
+
+                    figure(3)
+                    clf()                  
+                    plotMatrices([rowW,rowM,produit], [nameW,nameM, 'term-to-term product'])
+                    draw()
+
+                #BIAS
+
+                if nameB in listNames:
+
+                    row = learner.getParameterValue(nameB)
+                    print nameB,row.shape
+
+                    print 'i',i
+                    matricesToPlot = [reshape(row, (-1, self.hidden_layers[i].groupsize))]
+                                           
+                    namesToPlot = [nameB]
+
+                    if nameBr in listNames:
+
+                        row = learner.getParameterValue(nameBr)
+                        print nameBr,row.shape
+
+                        #HACK !!!
+                        print i, row.shape[1]
+                        if i==1 and row.shape[1] == 28*28*2:
+                            row = array(toMinusRow(list(row[0])))
+                            print 'just hacked...'
+                        #END OF HACK
+
+                        matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                        
+                        namesToPlot.append(nameBr)
+
+                    figure(4)
+                    clf()
+                    plotMatrices(matricesToPlot, namesToPlot)
+                    draw()
+                
+
+            # back to Original -- o
+            
+            elif char == 'o':
+                print 'getting original layer...'                
+                self.hidden_layers[i].hidden_layer = copy.copy(self.originals_hl[i].hidden_layer)
+                self.rep_axes[i].imshow(self.hidden_layers[i].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                print '...done'
+                
+       
+
+        # big-back to Original -- O
+        if char == 'O':
+            print 'getting all original layers...'
+            for k in arange(0,self.size()):
+                self.hidden_layers[k].hidden_layer = copy.copy(self.originals_hl[k].hidden_layer)
+                self.rep_axes[k].imshow(self.hidden_layers[k].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+            print '...done'
+        
+
+
+    def __clicked(self, event):
+            
+        self.__findCurrentLayer(event)
+        
+        print 'current hidden layer is now', self.current_hl
+
+        if event.key == 'control' and self.current_hl != -1:
+            
+            hidden_layer = self.hidden_layers[self.current_hl]
+            
+            n = hidden_layer.matrixToLayer(x,y)
+           # print 'n', n
+            hidden_layer.hidden_layer[n] = 1 - hidden_layer.hidden_layer[n]
+            axes.imshow(hidden_layer.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)        
+            draw()
+
+        if event.button == 3:
+            print 'Layer', self.current_hl, ', position (',x,y,'), value', xself.hidden_layers[self.current_hl].getElement(x,y)
+
+    def __findCurrentLayer(self, event):
+
+        fig = event.canvas.figure.number
+        axes = event.inaxes
+
+        self.current_hl = -1
+        if axes != None:
+            figure(fig)
+            
+            #we find to which hidden layer corresponds our axes
+            for i,a in enumerate(self.rep_axes):                
+                if a == axes:
+                    self.current_hl = i
+    
+
+    def __linkEvents(self):
+
+        figure(self.fig_rep)
+        connect('key_press_event', self.__changeChar)
+        #connect('button_press_event', self.__clicked)
+        connect('key_press_event', self.__repCommands)
+
+        figure(self.fig_rec)
+        connect('key_press_event', self.__changeChar)        
+        #connect('button_press_event', self.__clicked)        
+        
+    ###
+    ### plotting
+    ###
+
+    def __plotReconstructions(self):
+        print 'plotting reconstructions...'
+        figure(self.fig_rec)
+        clf()
+        plotMatrices(self.reconstructions)
+        draw()
+        print '...done.'
+
+    def __plotRepresentations(self):
+        print 'plotting representations...'
+        figure(self.fig_rep)
+        clf()
+        temp = []
+        for x in self.hidden_layers:
+            temp.append( x.getMatrix() )        
+        draw()
+
+        self.rep_axes = plotMatrices(temp)
+        print '...done.'
+
+    def __computeAndPlot(self):        
+        self.__computeRepresentation()
+        self.__computeReconstructions()
+        self.__plotReconstructions()
+        self.__plotRepresentations()
+
+    def plotNext(self):
+        self.__getNextChar()
+        self.__computeAndPlot()
+
+        #saving &quot;original&quot; matrices
+        #print 'copying originals'
+        self.originals_hl = copy.deepcopy(self.hidden_layers)
+        
+        print 'a', self.classe, 'was plotted'
+        
+
+    def plotPrev(self):
+        self.__getPrevChar()
+        self.__computeAndPlot()
+
+        #saving &quot;original&quot; matrices
+        #print 'copying originals'
+        self.originals_hl = copy.deepcopy(self.hidden_layers)
+        
+        print 'a', self.classe, 'was plotted'
+    
+            
+    def plotOld(self):
+
+        print 'entered in plot method'
+               
+
+        #reconstruction
+        print 'computing reconstructions...'
+        rec = learner.computeReconstructions(imagetmat)
+        
+        print 'executing some matrix manipulations...'
+        row = raw_rep[0][0]
+        
+        image = rowToMatrix(row,28)
+        
+        listDeMatrices = [image]        
+        for el in raw_rep[1:]:
+            listDeMatrices.append(rowToMatrix(el[0],max(len(el[0])/100.,1), False))
+
+        listDeMatrices2 = [image]
+        for el in rec:
+            row = el[0]
+            listDeMatrices2.append(rowToMatrix(row,28))
+        
+    
+        print 'plotting'
+        #plotting
+        figure(self.nofig)
+        clf()
+        plotMatrices(listDeMatrices)        
+        draw()
+
+        figure(self.nofig+1)
+        clf()
+        plotMatrices(listDeMatrices2)
+        draw()       
+
+        self.i+=1
+
+        if self.log_file != &quot;&quot;:
+            file = open(self.log_file,'a')
+
+            file.write(&quot;\n\n\n\n\n\n--------------------------------------------------------------------&quot;)
+            file.write(&quot;---------- REP ------------------------------------------------------\n&quot;)
+
+
+            appendMatrixToFile(file, image, &quot;input&quot;)
+            
+            for i,mat in enumerate(raw_rep[1:]):
+                appendMatrixToFile(file, mat, 'rep of hidden layer ' + str(i+1))
+
+
+            file.write(&quot;\n\n\n---------- REC ------------------------------------------------------\n&quot;)            
+            for i, mat in enumerate(rec):
+                appendMatrixToFile(file,  rowToMatrix(mat[0],28), 'rec of hidden layer ' + str(i+1))
+            
+
+
+class EachRowPlotter:
+    
+    def __init__(self, matrix, width = 28, plot_width = .1, space_between_images = .01, do_to_rows = None, i=0, nofig=0):
+        self.matrix = matrix
+        self.i = i
+        self.nofig = nofig
+        self.do_to_rows = do_to_rows
+        self.last_element = -1
+        self.plot_width = plot_width
+        self.sbi = space_between_images
+        self.width = width
+
+    def plotNext(self, event):
+        clf()
+        if self.last_element &gt; 0:
+            self.last_element = plotLayer1(matrix, self.width, self.plot_width, self.last_element+1, -1, self.sbi, doToRow)
+        else:
+            self.last_element = plotLayer1(matrix, self.width, self.plot_width, 0, -1, self.sbi, doToRow)
+        draw()
+
+    def plot(self):        
+        print 'Plotting matrix ' +  matrixName + ' (' + str(len(matrix)) + 'x' + str(len(matrix[0])) + ')'        
+        self.last_element = plotLayer1(matrix,self.width, self.plot_width, 0, -1, self.sbi, self.do_to_rows)                   
+        connect(&quot;button_press_event&quot;,self.plotNext)
+        
+    
+
+        
+
+############
+### main ###
+############
+
+server_command = &quot;slearn server&quot;
+serv = launch_plearn_server(command = server_command)
+
+#print &quot;Press Enter to continue&quot;
+#raw_input()
+
+if len(sys.argv)&lt;2:
+    print_usage_and_exit()
+
+task = sys.argv[1]
+
+def openVMat(vmatspec):
+    if vmatspec.endswith(&quot;.amat&quot;) or vmatspec.endswith(&quot;.pmat&quot;):
+        vmat = serv.new('AutoVMatrix(specification =&quot;'+vmatspec+'&quot;);')
+    else:
+        vmat = serv.load(vmatspec)
+    return vmat
+
+if task == 'plotEachRow':
+
+    psave = sys.argv[2]
+    
+    learner = serv.load(psave)
+        
+    matrices = learner.listParameter() 
+    names = learner.listParameterNames()
+    
+    #doToRow = None
+    doToRow = toPlusRow
+    #doToRow = toMinusRow
+
+    matrixName = ''
+    while matrixName != 'exit':
+        print
+        print 'Matrix list :'
+        print names
+
+        print
+        matrixName = raw_input('Choose a matrix to be plotted (or \'exit\')&gt;&gt;&gt;')
+        
+        if matrixName in names:
+
+            matrix = learner.getParameterValue(matrixName)
+            #matrix = rand(500,28*28*2)
+            plotter = EachRowPlotter(matrix, 28, .1, .01, doToRow)
+            plotter.plot()
+            show()          
+            
+        elif matrixName != 'exit':
+            print
+            print 'This matrix does not exist !'
+
+
+elif task == 'plotRepAndRec':
+    
+    psave = sys.argv[2]
+    datafname = sys.argv[3]
+    #test = sys.argv[5]
+    #print test
+   
+    #loading learner
+    learner = serv.load(psave)
+    
+    #taking an input
+    vmat = openVMat(datafname)
+    
+    matrix_plot = InteractiveRepRecPlotter(learner, vmat)
+    
+    show()
+
+    
+elif task == 'plotSingleMatrix':
+
+    psave = sys.argv[2]
+
+    learner = serv.load(psave)
+    
+    nameList = learner.listParameterNames()
+
+    matrixName = ''
+    while matrixName != 'exit':
+        print
+        print 'Matrix list :'
+        print nameList
+
+        print
+        matrixName = raw_input('Choose a matrix to be plotted (or \'exit\')&gt;&gt;&gt;')
+        
+        if matrixName in nameList:
+            matrix = learner.getParameterValue(matrixName)
+            figure(0)
+            #cadre = .05
+            #axes((cadre,cadre,1.-2*cadre,1-2*cadre))
+            #imshow(matrix, interpolation = defaultInterpolation, cmap = defaultColorMap)
+            #setPlotParams(matrixName, True, True)            
+            #figure(1, figsize=(1000,1000), dpi=40)
+            truncate_imshow(matrix)
+            show()
+            
+        elif matrixName != 'exit':
+            print
+            print 'This matrix does not exist !'
+
+elif task == 'test':
+    #jutilise cet endroit pour faire des tests
+
+    pass
+
+
+
+else:
+    print_usage_and_exit()


Property changes on: trunk/scripts/EXPERIMENTAL/deepnetplot.py
___________________________________________________________________
Name: svn:executable
   + *


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001055.html">[Plearn-commits] r7607 - trunk/python_modules/plearn/pyplearn
</A></li>
	<LI>Next message: <A HREF="001057.html">[Plearn-commits] r7609 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1056">[ date ]</a>
              <a href="thread.html#1056">[ thread ]</a>
              <a href="subject.html#1056">[ subject ]</a>
              <a href="author.html#1056">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
