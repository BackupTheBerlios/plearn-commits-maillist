<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8182 - trunk/plearn_learners/generic/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-October/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8182%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200710151556.l9FFuk36015857%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001629.html">
   <LINK REL="Next"  HREF="001631.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8182 - trunk/plearn_learners/generic/EXPERIMENTAL</H1>
    <B>manzagop at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8182%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200710151556.l9FFuk36015857%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8182 - trunk/plearn_learners/generic/EXPERIMENTAL">manzagop at mail.berlios.de
       </A><BR>
    <I>Mon Oct 15 17:56:46 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001629.html">[Plearn-commits] r8181 -	branches/cgi-desjardin/plearn_learners/second_iteration
</A></li>
        <LI>Next message: <A HREF="001631.html">[Plearn-commits] r8183 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1630">[ date ]</a>
              <a href="thread.html#1630">[ thread ]</a>
              <a href="subject.html#1630">[ subject ]</a>
              <a href="author.html#1630">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: manzagop
Date: 2007-10-15 17:56:46 +0200 (Mon, 15 Oct 2007)
New Revision: 8182

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
Removed code related to the &quot;pv&quot; gradient technique. This code is now in PvGradNNet.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-10-12 20:25:06 UTC (rev 8181)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-10-15 15:56:46 UTC (rev 8182)
@@ -74,18 +74,6 @@
       target_stdev_activation(3), // 2.5% of the time we are above 1
       //corr_profiling_start(0), 
       //corr_profiling_end(0),
-      use_pvgrad(false),
-      // Next 5 values inspired those used in the '94 rprop tech report
-      // but we are in stochastic case
-      pv_initial_stepsize(1e-1),
-      pv_min_stepsize(1e-6),
-      pv_max_stepsize(50.0),
-      pv_acceleration(1.2),
-      pv_deceleration(0.5),
-      pv_min_samples(2),
-      pv_required_confidence(0.80),
-      pv_random_sample_step(false),
-      pv_gradstats(new VecStatsCollector()),
       n_layers(-1),
       cumulative_training_time(0)
 {
@@ -284,57 +272,6 @@
     //              &quot;Stage to end the profiling of the gradients' and the\n&quot;
     //              &quot;natural gradients' correlations.\n&quot;);
 
-    declareOption(ol, &quot;use_pvgrad&quot;,
-                  &amp;NatGradNNet::use_pvgrad,
-                  OptionBase::buildoption,
-                  &quot;Use Pascal Vincent's gradient technique.\n&quot;
-                  &quot;All options specific to this technique start with pv_...\n&quot;
-                  &quot;This is currently very experimental. Current code is \n&quot;
-                  &quot;NOT YET optimised for speed (nor supports minibatch).&quot;);
-
-    declareOption(ol, &quot;pv_initial_stepsize&quot;,
-                  &amp;NatGradNNet::pv_initial_stepsize,
-                  OptionBase::buildoption,
-                  &quot;Initial size of steps in parameter space&quot;);
-
-    declareOption(ol, &quot;pv_min_stepsize&quot;,
-                  &amp;NatGradNNet::pv_min_stepsize,
-                  OptionBase::buildoption,
-                  &quot;Minimal size of steps in parameter space&quot;);
-
-    declareOption(ol, &quot;pv_max_stepsize&quot;,
-                  &amp;NatGradNNet::pv_max_stepsize,
-                  OptionBase::buildoption,
-                  &quot;Maximal size of steps in parameter space&quot;);
-
-    declareOption(ol, &quot;pv_acceleration&quot;,
-                  &amp;NatGradNNet::pv_acceleration,
-                  OptionBase::buildoption,
-                  &quot;Coefficient by which to multiply the step sizes.&quot;);
-
-    declareOption(ol, &quot;pv_deceleration&quot;,
-                  &amp;NatGradNNet::pv_deceleration,
-                  OptionBase::buildoption,
-                  &quot;Coefficient by which to multiply the step sizes.&quot;);
-
-    declareOption(ol, &quot;pv_min_samples&quot;,
-                  &amp;NatGradNNet::pv_min_samples,
-                  OptionBase::buildoption,
-                  &quot;PV's minimum number of samples to estimate gradient sign.\n&quot;
-                  &quot;This should at least be 2.&quot;);
-
-    declareOption(ol, &quot;pv_required_confidence&quot;,
-                  &amp;NatGradNNet::pv_required_confidence,
-                  OptionBase::buildoption,
-                  &quot;Minimum required confidence (probability of being positive or negative) for taking a step.&quot;);
-
-    declareOption(ol, &quot;pv_random_sample_step&quot;,
-                  &amp;NatGradNNet::pv_random_sample_step,
-                  OptionBase::buildoption,
-                  &quot;If this is set to true, then we will randomly choose the step sign\n&quot;
-                  &quot;for each parameter based on the estimated probability of it being\n&quot;
-                  &quot;positive or negative.&quot;);
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -367,9 +304,6 @@
     if( output_layer_L1_penalty_factor &lt; 0. )
         PLWARNING(&quot;NatGradNNet::build_ - output_layer_L1_penalty_factor is negative!\n&quot;);
 
-    if(use_pvgrad &amp;&amp; minibatch_size!=1)
-        PLERROR(&quot;PV's gradient technique (triggered by use_pvgrad): support for minibatch not yet implemented (must have minibatch_size=1)&quot;);
-    
     while (hidden_layer_sizes.length()&gt;0 &amp;&amp; hidden_layer_sizes[hidden_layer_sizes.length()-1]==0)
         hidden_layer_sizes.resize(hidden_layer_sizes.length()-1);
     n_layers = hidden_layer_sizes.length()+2;
@@ -560,12 +494,6 @@
     deepCopyField(group_params_delta, copies);
     deepCopyField(layer_params_delta, copies);
 
-    deepCopyField(pv_gradstats, copies);
-    deepCopyField(pv_all_stepsizes, copies);
-    deepCopyField(pv_all_stepsigns, copies);
-    deepCopyField(pv_all_intstepsigns, copies);
-    //deepCopyField(pv_all_nsamples, copies); // *stat*
-
 /*
     deepCopyField(, copies);
 */
@@ -597,34 +525,6 @@
     if (params_averaging_coeff!=1.0)
         all_mparams &lt;&lt; all_params;
     
-    if(use_pvgrad)
-    {
-        pv_gradstats-&gt;forget();
-        int n = all_params.length();
-        pv_all_stepsizes.resize(n);
-        pv_all_stepsizes.fill(pv_initial_stepsize);
-        pv_all_stepsigns.resize(n);
-        pv_all_stepsigns.fill(true);    // TODO should be init to undetermined
-        pv_all_intstepsigns.resize(n);
-        pv_all_intstepsigns.fill(0);
-        //pv_all_nsamples.resize(n);    // *stat*
-
-        // Get some structure on the previous Vecs
-        pv_layer_stepsizes.resize(n_layers-1);
-        pv_layer_stepsigns.resize(n_layers-1);
-        pv_layer_intstepsigns.resize(n_layers-1);
-        //pv_layer_nsamples.resize(n_layers-1); // *stat*
-        for (int i=0,p=0;i&lt;n_layers-1;i++)
-        {
-            int np=layer_sizes[i+1]*(1+layer_sizes[i]);
-            pv_layer_stepsizes[i]=pv_all_stepsizes.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-            pv_layer_stepsigns[i]=pv_all_stepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-            pv_layer_intstepsigns[i]=pv_all_intstepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-            //pv_layer_nsamples[i]=pv_all_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);   // *stat*
-            p+=np;
-        }
-    }
-
     // *stat*
     /*if( pa_gradstats.length() == 0 )    {
         pa_gradstats.resize(noutputs);
@@ -673,7 +573,7 @@
     Vec costs = costs_plus_time.subVec(0,train_costs.width());
     int nsamples = train_set-&gt;length();
 
-    // *stat* - Need some stats for pvgrad analysis
+    // *stat* - Need some stats for grad analysis
     //sum_gradient_norms = 0.0;
     //all_params_cum_gradient.fill(0.0);
     
@@ -730,7 +630,7 @@
     //    ng_corrprof-&gt;printAndReset();
     //}
 
-    // *stat* - Need some stats for pvgrad analysis
+    // *stat* - Need some stats for grad analysis
     // The SGrad stats include the learning rate.
     //cout &lt;&lt; &quot;sum_gradient_norms &quot; &lt;&lt; sum_gradient_norms 
     //     &lt;&lt; &quot; norm(all_params_cum_gradient,2.0) &quot; &lt;&lt; norm(all_params_cum_gradient,2.0) &lt;&lt; endl;
@@ -813,13 +713,8 @@
             }
         }
         // compute gradient on parameters, possibly update them
-        if (use_pvgrad)
+        if (full_natgrad || params_natgrad_template || params_natgrad_per_input_template) 
         {
-            productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
-                            neuron_extended_outputs_per_layer[i-1],false,1,0);
-        }
-        else if (full_natgrad || params_natgrad_template || params_natgrad_per_input_template) 
-        {
 //alternate
             if( params_natgrad_per_input_template &amp;&amp; i==1 ){ // parameters are transposed
                 Profiler::pl_profile_start(&quot;ProducScaleAccOnlineStep&quot;);
@@ -857,12 +752,8 @@
                             1,0);*/
         }
     }
-    if (use_pvgrad)
+    if (full_natgrad)
     {
-        pvGradUpdate();
-    }
-    else if (full_natgrad)
-    {
         (*full_natgrad)(t/minibatch_size,all_params_gradient,all_params_delta); // compute update direction by natural gradient
         if (output_layer_lrate_scale!=1.0)
             layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate
@@ -934,126 +825,6 @@
 */
 }
 
-void NatGradNNet::paGradUpdate()
-{
-
-}
-
-void NatGradNNet::pvGradUpdate()
-{
-    int np = all_params_gradient.length();
-    if(pv_all_stepsizes.length()==0)
-    {
-        pv_all_stepsizes.resize(np);
-        pv_all_stepsizes.fill(pv_initial_stepsize);
-        pv_all_stepsigns.resize(np);
-        pv_all_stepsigns.fill(true);
-        pv_all_intstepsigns.resize(np);
-        pv_all_intstepsigns.fill(0);
-        //pv_all_nsamples.resize(np);   // *stat*
-    }
-    pv_gradstats-&gt;update(all_params_gradient);
-
-    // *stat* - Need some stats for pvgrad analysis
-    //real gradient_squared_sum = 0.0;
-
-    for(int k=0; k&lt;np; k++)
-    {
-        StatsCollector&amp; st = pv_gradstats-&gt;getStats(k);
-        int ns = (int)st.nnonmissing();
-        if(ns&gt;pv_min_samples)
-        {
-            real m = st.mean();
-            real e = st.stderror();
-
-            // test to see if solve numerical problems
-            if( fabs(m) &lt; 1e-15 || e &lt; 1e-15 )  {
-                cout &lt;&lt; &quot;small mean and error ratio.&quot; &lt;&lt; endl;
-                continue;
-            }
-
-            real prob_pos = gauss_01_cum(m/e);
-            real prob_neg = 1.-prob_pos;
-            if(!pv_random_sample_step)
-            {
-                // We adapt the stepsize before taking the step
-                // gradient is positive
-                if(prob_pos&gt;=pv_required_confidence)
-                {
-                    pv_all_stepsizes[k] *= (pv_all_stepsigns[k]?pv_acceleration:pv_deceleration);
-                    if( pv_all_stepsizes[k] &gt; pv_max_stepsize )
-                        pv_all_stepsizes[k] = pv_max_stepsize;
-                    else if( pv_all_stepsizes[k] &lt; pv_min_stepsize )
-                        pv_all_stepsizes[k] = pv_min_stepsize;
-                    all_params[k] -= pv_all_stepsizes[k];
-                    pv_all_stepsigns[k] = true;
-                    pv_all_intstepsigns[k] = -1;
-                    st.forget();
-
-                    // *stat* - Need some stats for pvgrad analysis
-                    //gradient_squared_sum += pv_all_stepsizes[k]*pv_all_stepsizes[k];
-                    //all_params_cum_gradient[k] -= pv_all_stepsizes[k];
-
-                }
-                // gradient is negative
-                else if(prob_neg&gt;=pv_required_confidence)
-                {
-                    pv_all_stepsizes[k] *= ((!pv_all_stepsigns[k])?pv_acceleration:pv_deceleration);
-                    if( pv_all_stepsizes[k] &gt; pv_max_stepsize )
-                        pv_all_stepsizes[k] = pv_max_stepsize;
-                    else if( pv_all_stepsizes[k] &lt; pv_min_stepsize )
-                        pv_all_stepsizes[k] = pv_min_stepsize;
-                    all_params[k] += pv_all_stepsizes[k];
-                    pv_all_stepsigns[k] = false;
-                    pv_all_intstepsigns[k] = 1;
-                    st.forget();
-
-                    // *stat* - Need some stats for pvgrad analysis
-                    //gradient_squared_sum += pv_all_stepsizes[k]*pv_all_stepsizes[k];
-                    //all_params_cum_gradient[k] += pv_all_stepsizes[k];
-                
-                }   
-                // no step
-                else    {
-                    pv_all_intstepsigns[k] = 0;
-                }
-            }
-            else  // random sample update direction (sign)
-            {
-                bool ispos = (random_gen-&gt;binomial_sample(prob_pos)&gt;0);
-                if(ispos) // picked positive
-                    all_params[k] += pv_all_stepsizes[k];
-                else  // picked negative
-                    all_params[k] -= pv_all_stepsizes[k];
-                pv_all_stepsizes[k] *= (pv_all_stepsigns[k]==ispos) ?pv_acceleration :pv_deceleration;
-                pv_all_stepsigns[k] = ispos;
-                st.forget();
-            }
-        }
-        //pv_all_nsamples[k] = ns; // *stat*
-    }
-
-    // *stat* - Need some stats for pvgrad analysis
-    //sum_gradient_norms += sqrt( gradient_squared_sum );
-
-    // Ouput for profiling: step sizes and number of samples
-    // horribly inefficient!
-/*    ofstream fd_ss;
-    fd_ss.open(&quot;step_sizes.txt&quot;, ios::app);
-    fd_ss &lt;&lt; pv_layer_stepsizes[0](0) &lt;&lt; &quot; &quot; &lt;&lt; pv_layer_stepsizes[1](0) &lt;&lt; endl;
-    fd_ss.close();
-    ofstream fd_ns;
-    fd_ns.open(&quot;nsamples.txt&quot;, ios::app);
-    fd_ns &lt;&lt; pv_layer_nsamples[0](0) &lt;&lt; &quot; &quot; &lt;&lt; pv_layer_nsamples[1](0) &lt;&lt; endl;
-    fd_ns.close();
-    ofstream fd_ssgn;
-    fd_ssgn.open(&quot;step_signs.txt&quot;, ios::app);
-    fd_ssgn &lt;&lt; pv_layer_intstepsigns[0](0) &lt;&lt; &quot; &quot; &lt;&lt; pv_layer_intstepsigns[1](0) &lt;&lt; endl;
-    fd_ssgn.close();
-*/
-
-}
-
 void NatGradNNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
 {
     Profiler::pl_profile_start(&quot;NatGradNNet::computeOutput&quot;);

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-10-12 20:25:06 UTC (rev 8181)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-10-15 15:56:46 UTC (rev 8182)
@@ -163,55 +163,10 @@
     // *stat* end
 
 public:
-    //*************************************************************
-    //*** Members used for Pascal Vincent's gradient technique  ***
-
-    //! Use Pascal's gradient 
-    bool use_pvgrad;
-
-    //! Initial size of steps in parameter space
-    real pv_initial_stepsize;
-
-    //! Bounds for the step sizes
-    real pv_min_stepsize, pv_max_stepsize;
-
-    //! Coefficients by which to multiply the step sizes  
-    real pv_acceleration, pv_deceleration;
-
-    //! PV's gradient minimum number of samples to estimate confidence
-    int pv_min_samples;
-
-    //! Minimum required confidence (probability of being positive or negative) for taking a step. 
-    real pv_required_confidence;
-
-    //! If this is set to true, then we will randomly choose the step sign for
-    // each parameter based on the estimated probability of it being positive or
-    // negative.
-    bool pv_random_sample_step;
-
-protected:
-    //! accumulated statistics of gradients on each parameter.
-    PP&lt;VecStatsCollector&gt; pv_gradstats;
-
-    //! The step size (absolute value) to be taken for each parameter.
-    Vec pv_all_stepsizes;
-    TVec&lt;Mat&gt; pv_layer_stepsizes;
-
-    //! Indicates whether the previous step was positive (true) or negative (false)
-    TVec&lt;bool&gt; pv_all_stepsigns;
-    TVec&lt; TMat&lt;bool&gt; &gt; pv_layer_stepsigns;
-
-    //! Temporary add-on. Allows an undetermined signed value (zero).
-    TVec&lt;int&gt; pv_all_intstepsigns;
-    TVec&lt; TMat&lt;int&gt; &gt; pv_layer_intstepsigns;
-
-
-public:
     //#####  Public Member Functions  #########################################
 
     NatGradNNet();
 
-
     //#####  PLearner Member Functions  #######################################
 
     //! Returns the size of this learner's output, (which typically
@@ -323,13 +278,7 @@
     //! and write into neuron_gradients_per_layer[n_layers-2], gradient on pre-non-linearity activation
     void fbpropLoss(const Mat&amp; output, const Mat&amp; target, const Vec&amp; example_weights, Mat&amp; train_costs) const;
 
-    //! gradient computation and weight update in Pascal Vincent's gradient technique
-    void pvGradUpdate();
 
-    //! a related idea 
-    void paGradUpdate();
-
-
 private:
     //#####  Private Member Functions  ########################################
 
@@ -358,8 +307,6 @@
     Mat targets; // one target row per example in a minibatch
     Vec example_weights; // one element per example in a minibatch
     Mat train_costs; // one row per example in a minibatch
-
-
     
 };
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001629.html">[Plearn-commits] r8181 -	branches/cgi-desjardin/plearn_learners/second_iteration
</A></li>
	<LI>Next message: <A HREF="001631.html">[Plearn-commits] r8183 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1630">[ date ]</a>
              <a href="thread.html#1630">[ thread ]</a>
              <a href="subject.html#1630">[ subject ]</a>
              <a href="author.html#1630">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
