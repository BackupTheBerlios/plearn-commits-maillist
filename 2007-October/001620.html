<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8172 - trunk/plearn_learners/distributions
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-October/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8172%20-%20trunk/plearn_learners/distributions&In-Reply-To=%3C200710102241.l9AMfYYM025208%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001619.html">
   <LINK REL="Next"  HREF="001621.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8172 - trunk/plearn_learners/distributions</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8172%20-%20trunk/plearn_learners/distributions&In-Reply-To=%3C200710102241.l9AMfYYM025208%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8172 - trunk/plearn_learners/distributions">larocheh at mail.berlios.de
       </A><BR>
    <I>Thu Oct 11 00:41:34 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001619.html">[Plearn-commits] r8171 - trunk/plearn/vmat
</A></li>
        <LI>Next message: <A HREF="001621.html">[Plearn-commits] r8173 - trunk/plearn_learners/distributions
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1620">[ date ]</a>
              <a href="thread.html#1620">[ thread ]</a>
              <a href="subject.html#1620">[ subject ]</a>
              <a href="author.html#1620">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-10-11 00:41:33 +0200 (Thu, 11 Oct 2007)
New Revision: 8172

Modified:
   trunk/plearn_learners/distributions/NonLocalManifoldParzen.cc
   trunk/plearn_learners/distributions/NonLocalManifoldParzen.h
Log:


Modified: trunk/plearn_learners/distributions/NonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners/distributions/NonLocalManifoldParzen.cc	2007-10-10 22:33:15 UTC (rev 8171)
+++ trunk/plearn_learners/distributions/NonLocalManifoldParzen.cc	2007-10-10 22:41:33 UTC (rev 8172)
@@ -43,263 +43,192 @@
 
 #include &quot;NonLocalManifoldParzen.h&quot;
 #include &lt;plearn/display/DisplayUtils.h&gt;
-#include &lt;plearn/vmat/LocalNeighborsDifferencesVMatrix.h&gt;
-//#include &lt;plearn/vmat/RandomNeighborsDifferencesVMatrix.h&gt;
-#include &lt;plearn/var/ProductVariable.h&gt;
-#include &lt;plearn/var/PlusVariable.h&gt;
-#include &lt;plearn/var/SoftplusVariable.h&gt;
-#include &lt;plearn/var/SumAbsVariable.h&gt;
-#include &lt;plearn/var/SumSquareVariable.h&gt;
-#include &lt;plearn/var/VarRowVariable.h&gt;
-#include &lt;plearn/var/SourceVariable.h&gt;
-#include &lt;plearn/var/Var_operators.h&gt;
-//#include &lt;plearn/var/DiagonalGaussianVariable.h&gt;
-#include &lt;plearn/vmat/ConcatColumnsVMatrix.h&gt;
-#include &lt;plearn/var/SumOfVariable.h&gt;
-#include &lt;plearn/var/RowOfVariable.h&gt;
-//#include &lt;plearn/var/RowPowNormVariable.h&gt;
-#include &lt;plearn/var/SumVariable.h&gt;
-#include &lt;plearn/var/TanhVariable.h&gt;
-#include &lt;plearn/var/NllGeneralGaussianVariable.h&gt;
-#include &lt;plearn/var/DiagonalizedFactorsProductVariable.h&gt;
 #include &lt;plearn/math/plapack.h&gt;
+#include &lt;plearn/var/AffineTransformVariable.h&gt;
+#include &lt;plearn/var/AffineTransformWeightPenalty.h&gt;
 #include &lt;plearn/var/ColumnSumVariable.h&gt;
-#include &lt;plearn/vmat/VMat_basic_stats.h&gt;
-#include &lt;plearn/vmat/ConcatRowsVMatrix.h&gt;
-#include &lt;plearn/vmat/SubVMatrix.h&gt;
-#include &lt;plearn/var/PDistributionVariable.h&gt;
-#include &lt;plearn_learners/distributions/UniformDistribution.h&gt;
-#include &lt;plearn_learners/distributions/GaussianDistribution.h&gt;
-#include &lt;plearn/display/DisplayUtils.h&gt;
-#include &lt;plearn/opt/GradientOptimizer.h&gt;
-#include &lt;plearn/var/TransposeVariable.h&gt;
-#include &lt;plearn/var/Var_utils.h&gt;
-#include &lt;plearn/var/ConcatRowsVariable.h&gt;
-#include &lt;plearn/var/RowSumVariable.h&gt;
-#include &lt;plearn/var/ThresholdBpropVariable.h&gt;
+#include &lt;plearn/var/NllGeneralGaussianVariable.h&gt;
 #include &lt;plearn/var/NoBpropVariable.h&gt;
 #include &lt;plearn/var/ReshapeVariable.h&gt;
+#include &lt;plearn/var/SourceVariable.h&gt;
 #include &lt;plearn/var/SquareVariable.h&gt;
-#include &lt;plearn/var/ExpVariable.h&gt;
-#include &lt;plearn/io/load_and_save.h&gt;
-#include &lt;plearn/vmat/VMat_computeNearestNeighbors.h&gt;
-#include &lt;plearn/vmat/FractionSplitter.h&gt;
-#include &lt;plearn/vmat/RepeatSplitter.h&gt;
-#include &lt;plearn/var/FNetLayerVariable.h&gt;
-#include &lt;plearn/vmat/MemoryVMatrix.h&gt;
+#include &lt;plearn/var/SumOfVariable.h&gt;
+#include &lt;plearn/var/TanhVariable.h&gt;
+#include &lt;plearn/var/ThresholdBpropVariable.h&gt;
+#include &lt;plearn/var/Var_operators.h&gt;
+#include &lt;plearn/vmat/AppendNeighborsVMatrix.h&gt;
+#include &lt;plearn/vmat/ConcatColumnsVMatrix.h&gt;
 
+
 namespace PLearn {
 using namespace std;
 
 
 NonLocalManifoldParzen::NonLocalManifoldParzen()
-    :  //weight_embedding(1),
-    curpos(0),
-    weight_decay(0), penalty_type(&quot;L2_square&quot;),
-//noise_grad_factor(0.01),noise(0), noise_type(&quot;gaussian&quot;), omit_last(0),
-learn_mu(true),
-//magnified_version(false),
-reference_set(0), sigma_init(0.1), sigma_min(0.00001), nneighbors(5), nneighbors_density(-1), mu_nneighbors(2), ncomponents(1), sigma_threshold_factor(1), variances_transfer_function(&quot;softplus&quot;), architecture_type(&quot;single_neural_network&quot;),
-    n_hidden_units(-1), batch_size(1), svd_threshold(1e-8), rw_n_step(1000), rw_size_step(0.01), rw_ith_component(0), rw_file_name(&quot;random_walk_&quot;), rw_save_every(100), store_prediction(false), optstage_per_lstage(-1), save_every(-1)
+    :  
+    reference_set(0), 
+    ncomponents(1), 
+    nneighbors(5), 
+    nneighbors_density(-1), 
+    store_prediction(false),
+    learn_mu(false),
+    sigma_init(0.1), 
+    sigma_min(0.00001), 
+    mu_nneighbors(2), 
+    sigma_threshold_factor(-1), 
+    svd_threshold(1e-8), 
+    nhidden(10), 
+    weight_decay(0),
+    penalty_type(&quot;L2_square&quot;),
+    batch_size(1)
 {
 }
 
-PLEARN_IMPLEMENT_OBJECT(NonLocalManifoldParzen, &quot;Non-Local version of Manifold Parzen Windows&quot;,
-                        &quot;Manifold Parzen Windows density model, where the parameters of\n&quot;
-                        &quot;the gaussians in the mixture are predicted by a neural network.&quot;
+PLEARN_IMPLEMENT_OBJECT(NonLocalManifoldParzen, 
+                        &quot;Non-Local version of Manifold Parzen Windows&quot;,
+                        &quot;Manifold Parzen Windows density model, where the\n&quot;
+                        &quot;parameters of the kernel for each training point\n&quot;
+                        &quot;are predicted by a neural network.\n&quot;
     );
 
 
 void NonLocalManifoldParzen::declareOptions(OptionList&amp; ol)
 {
 
-//  declareOption(ol, &quot;weight_embedding&quot;, &amp;NonLocalManifoldParzen::weight_embedding, OptionBase::buildoption,
-//                &quot;Embedding penalty weight\n&quot;);
-
-    declareOption(ol, &quot;weight_decay&quot;, &amp;NonLocalManifoldParzen::weight_decay, OptionBase::buildoption,
-                  &quot;Global weight decay for all layers\n&quot;);
-
-    declareOption(ol, &quot;penalty_type&quot;, &amp;NonLocalManifoldParzen::penalty_type,
-                  OptionBase::buildoption,
-                  &quot;Penalty to use on the weights (for weight and bias decay).\n&quot;
-                  &quot;Can be any of:\n&quot;
-                  &quot;  - \&quot;L1\&quot;: L1 norm,\n&quot;
-                  &quot;  - \&quot;L1_square\&quot;: square of the L1 norm,\n&quot;
-                  &quot;  - \&quot;L2_square\&quot; (default): square of the L2 norm.\n&quot;);
-
-//  declareOption(ol, &quot;omit_last&quot;, &amp;NonLocalManifoldParzen::omit_last, OptionBase::buildoption,
-//		&quot;Number of training examples at the end of trainin set to ignore in the training.\n&quot;
-//		);
-
-    declareOption(ol, &quot;learn_mu&quot;, &amp;NonLocalManifoldParzen::learn_mu, OptionBase::buildoption,
-                  &quot;Indication that mu should be learned.\n&quot;
+    declareOption(ol, &quot;parameters&quot;, &amp;NonLocalManifoldParzen::parameters, 
+                  OptionBase::learntoption,
+                  &quot;Parameters of the tangent_predictor function.\n&quot;
         );
 
-    declareOption(ol, &quot;nneighbors&quot;, &amp;NonLocalManifoldParzen::nneighbors, OptionBase::buildoption,
-                  &quot;Number of nearest neighbors to consider for gradient descent.\n&quot;
+    declareOption(ol, &quot;reference_set&quot;, &amp;NonLocalManifoldParzen::reference_set, 
+                  OptionBase::learntoption,
+                  &quot;Reference points for density computation.\n&quot;
         );
 
-    declareOption(ol, &quot;nneighbors_density&quot;, &amp;NonLocalManifoldParzen::nneighbors_density, OptionBase::buildoption,
-                  &quot;Number of nearest neighbors to consider for p(x) density estimation.\n&quot;
+    declareOption(ol, &quot;ncomponents&quot;, &amp;NonLocalManifoldParzen::ncomponents, 
+                  OptionBase::buildoption,
+                  &quot;Number of \&quot;principal components\&quot; to predict\n&quot;
+                  &quot;for kernel parameters prediction.\n&quot;
         );
 
-    declareOption(ol, &quot;mu_nneighbors&quot;, &amp;NonLocalManifoldParzen::mu_nneighbors, OptionBase::buildoption,
-                  &quot;Number of nearest neighbors to learn the mus (if &lt; 0, mu_nneighbors = nneighbors).\n&quot;
+    declareOption(ol, &quot;nneighbors&quot;, &amp;NonLocalManifoldParzen::nneighbors, 
+                  OptionBase::buildoption,
+                  &quot;Number of nearest neighbors to consider in training procedure.\n&quot;
         );
 
-    declareOption(ol, &quot;ncomponents&quot;, &amp;NonLocalManifoldParzen::ncomponents, OptionBase::buildoption,
-                  &quot;Number of tangent vectors to predict.\n&quot;
+    declareOption(ol, &quot;nneighbors_density&quot;, 
+                  &amp;NonLocalManifoldParzen::nneighbors_density, 
+                  OptionBase::buildoption,
+                  &quot;Number of nearest neighbors to consider for\n&quot;
+                  &quot;p(x) density estimation.\n&quot;
         );
 
-    declareOption(ol, &quot;sigma_threshold_factor&quot;, &amp;NonLocalManifoldParzen::sigma_threshold_factor, OptionBase::buildoption,
-                  &quot;Threshold factor of the gradient on the sigma noise. \n&quot;
-        );
-
-    declareOption(ol, &quot;optimizer&quot;, &amp;NonLocalManifoldParzen::optimizer, OptionBase::buildoption,
-                  &quot;Optimizer that optimizes the cost function.\n&quot;
-        );
-		
-    declareOption(ol, &quot;variances_transfer_function&quot;, &amp;NonLocalManifoldParzen::variances_transfer_function,
+    declareOption(ol, &quot;store_prediction&quot;, 
+                  &amp;NonLocalManifoldParzen::store_prediction, 
                   OptionBase::buildoption,
-                  &quot;Type of output transfer function for predicted variances, to force them to be &gt;0:\n&quot;
-                  &quot;  square : take the square\n&quot;
-                  &quot;  exp : apply the exponential\n&quot;
-                  &quot;  softplus : apply the function log(1+exp(.))\n&quot;
+                  &quot;Indication that the predicted parameters should be stored.\n&quot;
+                  &quot;This may make testing faster. Note that the predictions are\n&quot;
+                  &quot;stored after the last training stage\n&quot;
         );
-		
-    declareOption(ol, &quot;architecture_type&quot;, &amp;NonLocalManifoldParzen::architecture_type, OptionBase::buildoption,
-                  &quot;For pre-defined tangent_predictor types: \n&quot;
-                  &quot;   single_neural_network : prediction = b + W*tanh(c + V*x), where W has n_hidden_units columns\n&quot;
-                  &quot;                          where the resulting vector is viewed as a ncomponents by n matrix\n&quot;
-                  &quot;   embedding_neural_network: prediction[k,i] = d(e[k])/d(x[i), where e(x) is an ordinary neural\n&quot;
-                  &quot;                             network representing the embedding function (see output_type option)\n&quot;
-                  &quot;where (b,W,c,V) are parameters to be optimized.\n&quot;
-        );
 
-    declareOption(ol, &quot;n_hidden_units&quot;, &amp;NonLocalManifoldParzen::n_hidden_units, OptionBase::buildoption,
-                  &quot;Number of hidden units (if architecture_type is some kind of neural network)\n&quot;
+
+    declareOption(ol, &quot;paramsvalues&quot;, 
+                  &amp;NonLocalManifoldParzen::paramsvalues, 
+                  OptionBase::learntoption,
+                  &quot;The learned parameter vector.\n&quot;
         );
 
-    declareOption(ol, &quot;hidden_layer&quot;, &amp;NonLocalManifoldParzen::hidden_layer, OptionBase::buildoption,
-                  &quot;A user-specified NAry Var that computes the output of the first hidden layer\n&quot;
-                  &quot;from the network input vector and a set of parameters. Its first argument should\n&quot;
-                  &quot;be the network input and the remaining arguments the tunable parameters.\n&quot;);
+    // ** Gaussian kernel options
 
-    declareOption(ol, &quot;batch_size&quot;, &amp;NonLocalManifoldParzen::batch_size, OptionBase::buildoption,
-                  &quot;    how many samples to use to estimate the average gradient before updating the weights\n&quot;
-                  &quot;    0 is equivalent to specifying training_set-&gt;length() \n&quot;);
-
-    declareOption(ol, &quot;svd_threshold&quot;, &amp;NonLocalManifoldParzen::svd_threshold, OptionBase::buildoption,
-                  &quot;Threshold to accept singular values of F in solving for linear combination weights on tangent subspace.\n&quot;
+    declareOption(ol, &quot;learn_mu&quot;, &amp;NonLocalManifoldParzen::learn_mu, 
+                  OptionBase::buildoption,
+                  &quot;Indication that the deviation from the training point\n&quot;
+                  &quot;in a Gaussian kernel (called mu) should be learned.\n&quot;
         );
 
-    declareOption(ol, &quot;parameters&quot;, &amp;NonLocalManifoldParzen::parameters, OptionBase::learntoption,
-                  &quot;Parameters of the tangent_predictor function.\n&quot;
+    declareOption(ol, &quot;sigma_init&quot;, &amp;NonLocalManifoldParzen::sigma_init, 
+                  OptionBase::buildoption,
+                  &quot;Initial minimum value for sigma noise.\n&quot;
         );
 
-    declareOption(ol, &quot;shared_parameters&quot;, &amp;NonLocalManifoldParzen::shared_parameters, OptionBase::buildoption,
-                  &quot;Parameters of another NonLocalManifoldParzen estimator to share with this current object.\n&quot;
+    declareOption(ol, &quot;sigma_min&quot;, &amp;NonLocalManifoldParzen::sigma_min, 
+                  OptionBase::buildoption,
+                  &quot;The minimum value for sigma noise.\n&quot;
         );
 
-    declareOption(ol, &quot;L&quot;, &amp;NonLocalManifoldParzen::L, OptionBase::learntoption,
-                  &quot;Number of gaussians.\n&quot;
+    declareOption(ol, &quot;mu_nneighbors&quot;, &amp;NonLocalManifoldParzen::mu_nneighbors, 
+                  OptionBase::buildoption,
+                  &quot;Number of nearest neighbors to learn the mus \n&quot;
+                  &quot;(if &lt; 0, mu_nneighbors = nneighbors).\n&quot;
         );
 
-//  declareOption(ol, &quot;Us&quot;, &amp;NonLocalManifoldParzen::Us, OptionBase::learntoption,
-//		&quot;The U matrices for the reference set.\n&quot;
-//		);
-
-    declareOption(ol, &quot;mus&quot;, &amp;NonLocalManifoldParzen::mus, OptionBase::learntoption,
-                  &quot;The mu vectors for the reference set.\n&quot;
+    declareOption(ol, &quot;sigma_threshold_factor&quot;, 
+                  &amp;NonLocalManifoldParzen::sigma_threshold_factor, 
+                  OptionBase::buildoption,
+                  &quot;Threshold factor of the gradient on the sigma noise\n&quot;
+                  &quot;parameter of the Gaussian kernel. If &lt; 0, then\n&quot;
+                  &quot;no threshold is used.&quot;
         );
 
-//  declareOption(ol, &quot;sms&quot;, &amp;NonLocalManifoldParzen::sms, OptionBase::learntoption,
-//		&quot;The sm values for the reference set.\n&quot;
-//                );
-
-    declareOption(ol, &quot;sns&quot;, &amp;NonLocalManifoldParzen::sns, OptionBase::learntoption,
-                  &quot;The sn values for the reference set.\n&quot;
+    declareOption(ol, &quot;svd_threshold&quot;, 
+                  &amp;NonLocalManifoldParzen::svd_threshold, OptionBase::buildoption,
+                  &quot;Threshold to accept singular values of F in solving for\n&quot;
+                  &quot;linear combination weights on tangent subspace.\n&quot;
         );
 
-    declareOption(ol, &quot;sms&quot;, &amp;NonLocalManifoldParzen::sms, OptionBase::learntoption,
-                  &quot;The sm values for the reference set.\n&quot;
-        );
+    // ** Neural network predictor **
 
-    declareOption(ol, &quot;Fs&quot;, &amp;NonLocalManifoldParzen::Fs, OptionBase::learntoption,
-                  &quot;The F values for the reference set.\n&quot;
+    declareOption(ol, &quot;nhidden&quot;, 
+                  &amp;NonLocalManifoldParzen::nhidden, OptionBase::buildoption,
+                  &quot;Number of hidden units of the neural network.\n&quot;
         );
 
-    declareOption(ol, &quot;sigma_min&quot;, &amp;NonLocalManifoldParzen::sigma_min, OptionBase::buildoption,
-                  &quot;The minimum value for sigma noise.\n&quot;
-        );
+    declareOption(ol, &quot;weight_decay&quot;, &amp;NonLocalManifoldParzen::weight_decay, 
+                  OptionBase::buildoption,
+                  &quot;Global weight decay for all layers.\n&quot;);
 
-    declareOption(ol, &quot;sigma_init&quot;, &amp;NonLocalManifoldParzen::sigma_init, OptionBase::buildoption,
-                  &quot;Initial minimum value for sigma noise.\n&quot;
-        );
+    declareOption(ol, &quot;penalty_type&quot;, &amp;NonLocalManifoldParzen::penalty_type,
+                  OptionBase::buildoption,
+                  &quot;Penalty to use on the weights (for weight and bias decay).\n&quot;
+                  &quot;Can be any of:\n&quot;
+                  &quot;  - \&quot;L1\&quot;: L1 norm,\n&quot;
+                  &quot;  - \&quot;L2_square\&quot; (default): square of the L2 norm.\n&quot;);
 
-    declareOption(ol, &quot;rw_n_step&quot;, &amp;NonLocalManifoldParzen::rw_n_step, OptionBase::buildoption,
-                  &quot;Number of steps in the random walk (for compute output).\n&quot;
+    declareOption(ol, &quot;optimizer&quot;, &amp;NonLocalManifoldParzen::optimizer, 
+                  OptionBase::buildoption,
+                  &quot;Optimizer that optimizes the cost function.\n&quot;
         );
 
-    declareOption(ol, &quot;rw_size_step&quot;, &amp;NonLocalManifoldParzen::rw_size_step, OptionBase::buildoption,
-                  &quot;Size of the steps in the random walk (for compute output).\n&quot;
-        );
+    declareOption(ol, &quot;batch_size&quot;, 
+                  &amp;NonLocalManifoldParzen::batch_size, OptionBase::buildoption,
+                  &quot;How many samples to use to estimate the average gradient\n&quot;
+                  &quot;before updating the weights. If &lt;= 0, is equivalent to\n&quot;
+                  &quot;specifying training_set-&gt;length() \n&quot;);
 
-    declareOption(ol, &quot;rw_ith_component&quot;, &amp;NonLocalManifoldParzen::rw_ith_component, OptionBase::buildoption,
-                  &quot;Which principal component to follow.\n&quot;
-        );
 
-    declareOption(ol, &quot;rw_save_every&quot;, &amp;NonLocalManifoldParzen::rw_save_every, OptionBase::buildoption,
-                  &quot;Number of iterations between savings of random walk results.\n&quot;
-        );
-    declareOption(ol, &quot;rw_file_name&quot;, &amp;NonLocalManifoldParzen::rw_file_name, OptionBase::buildoption,
-                  &quot;File name for the random walk saves.\n&quot;
-        );
+    // ** Stored outputs of neural network
 
-    declareOption(ol, &quot;store_prediction&quot;, &amp;NonLocalManifoldParzen::store_prediction, OptionBase::buildoption,
-                  &quot;Indication that the predicted parameters should be stored.\n&quot;
-                  &quot;This may make testing faster. Note that the predictions are\n&quot;
-                  &quot;stored after the last training stage, so if the predictor is\n&quot;
-                  &quot;modified later on (e.g. if the parameters of the predictor are shared), then\n&quot;
-                  &quot;this option might give different testing results.\n&quot;
+    declareOption(ol, &quot;mus&quot;, 
+                  &amp;NonLocalManifoldParzen::mus, OptionBase::learntoption,
+                  &quot;The stored mu vectors for the reference set.\n&quot;
         );
 
-    declareOption(ol, &quot;optstage_per_lstage&quot;, &amp;NonLocalManifoldParzen::optstage_per_lstage, OptionBase::buildoption,
-                  &quot;Number of optimizer stages. If &lt; 0, then it is determined as a function of\n&quot;
-                  &quot;the training set length and of the batch size.\n&quot;
+    declareOption(ol, &quot;sns&quot;, &amp;NonLocalManifoldParzen::sns, 
+                  OptionBase::learntoption,
+                  &quot;The stored sigma noise values for the reference set.\n&quot;
         );
 
-    declareOption(ol, &quot;save_every&quot;, &amp;NonLocalManifoldParzen::save_every, OptionBase::buildoption,
-                  &quot;Number of iterations since the last save after which the parameters must be saved.\n&quot;
-                  &quot;If &lt; 0, then the parameters are saved at the end of train().\n&quot;
+    declareOption(ol, &quot;sms&quot;, &amp;NonLocalManifoldParzen::sms, 
+                  OptionBase::learntoption,
+                  &quot;The stored sigma manifold values for the reference set.\n&quot;
         );
 
-
-//  declareOption(ol, &quot;noise&quot;, &amp;NonLocalManifoldParzen::noise, OptionBase::buildoption,
-//		&quot;Noise parameter for the training data. For uniform noise, this gives the half the length \n&quot; &quot;of the uniform window (centered around the origin), and for gaussian noise, this gives the variance of the noise in all directions.\n&quot;
-//                );
-
-//  declareOption(ol, &quot;noise_type&quot;, &amp;NonLocalManifoldParzen::noise_type, OptionBase::buildoption,
-//		&quot;Type of the noise (\&quot;uniform\&quot; or \&quot;gaussian\&quot;).\n&quot;
-//                );
-
-//  declareOption(ol, &quot;noise_grad_factor&quot;, &amp;NonLocalManifoldParzen::noise_grad_factor, OptionBase::buildoption,
-//		&quot;Gradient factor to apply to the noise signal error.\n&quot;
-//                );
-
-//  declareOption(ol, &quot;magnified_version&quot;, &amp;NonLocalManifoldParzen::magnified_version, OptionBase::buildoption,
-//		&quot;Indication that, when computing the log density, the magnified estimation should be used.\n&quot;
-//                );
-
-    declareOption(ol, &quot;reference_set&quot;, &amp;NonLocalManifoldParzen::reference_set, OptionBase::learntoption,
-                  &quot;Reference points for density computation.\n&quot;
+    declareOption(ol, &quot;Fs&quot;, &amp;NonLocalManifoldParzen::Fs, OptionBase::learntoption,
+                  &quot;The storaged \&quot;principal components\&quot; (F) values for\n&quot;
+                  &quot;the reference set.\n&quot;
         );
 
-    declareOption(ol, &quot;curpos&quot;, &amp;NonLocalManifoldParzen::curpos, OptionBase::learntoption,
-                  &quot;Position of the current example in the training set.\n&quot;
-        );
 
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -307,356 +236,149 @@
 void NonLocalManifoldParzen::build_()
 {
 
-    n = PLearner::inputsize_;
-
-    if (n&gt;0)
+    if (inputsize_&gt;0)
     {
+        if (nhidden &lt;= 0) 
+            PLERROR(&quot;NonLocalManifoldParzen::Number of hidden units &quot;
+                    &quot;should be positive, now %d\n&quot;,nhidden);
 
-        VarArray params;
-
-        int sp_index = 0;
-
         Var log_n_examples(1,1,&quot;log(n_examples)&quot;);
         if(train_set)
         {
             L = train_set-&gt;length();
-            reference_set = train_set; // Maybe things could be changed here to make access faster!
+            reference_set = train_set; 
         }
 
         log_L= pl_log((real) L);
+        parameters.resize(0);
+        
+        // Neural network prediction of principal components
 
-        {
+        x = Var(inputsize_);
+        x-&gt;setName(&quot;x&quot;);
 
-            x = Var(n);
-            Var a; // outputs of hidden layer
+        W = Var(nhidden+1,inputsize_,&quot;W&quot;);
+        parameters.append(W);
 
-            if (hidden_layer) // user-specified hidden layer Var
-            {
-                if(shared_parameters.size() != 0)
-                    PLERROR(&quot;In NonLocalManifoldParzen:build_(): shared parameters is not implemented for user-specified hidden layer Var&quot;);
-                NaryVariable* layer_var = dynamic_cast&lt;NaryVariable*&gt;((Variable*)hidden_layer);
-                if (!layer_var)
-                    PLERROR(&quot;In NonLocalManifoldParzen::build - 'hidden_layer' should be &quot;
-                            &quot;from a subclass of NaryVariable&quot;);
-                if (layer_var-&gt;varray.size() &lt; 1)
-                    layer_var-&gt;varray.resize(1);
-                layer_var-&gt;varray[0] = transpose(x);
-                layer_var-&gt;build(); // make sure everything is consistent and finish the build
-                if (layer_var-&gt;varray.size()&lt;2)
-                    PLERROR(&quot;In NonLocalManifoldParzen::build - 'hidden_layer' should have parameters&quot;);
-                for (int i=1;i&lt;layer_var-&gt;varray.size();i++)
-                    params.append(layer_var-&gt;varray[i]);
-                a = transpose(layer_var);
-                n_hidden_units = layer_var-&gt;width();
-            }
-            else // standard hidden layer
-            {
-                if (n_hidden_units &lt;= 0)
-                    PLERROR(&quot;NonLocalManifoldParzen::Number of hidden units should be positive, now %d\n&quot;,n_hidden_units);
-                if(shared_parameters.size() != 0)
-                {
-                    c = shared_parameters[sp_index++];
-                    V = shared_parameters[sp_index++];
-                }
-                else
-                {
-                    c = Var(n_hidden_units,1,&quot;c &quot;);
-                    V = Var(n_hidden_units,n,&quot;V &quot;);
-                }
-                params.append(c);
-                params.append(V);
-                a = tanh(c + product(V,x));
-            }
+        Var a; // outputs of hidden layer
+        a = affine_transform(x,W);
+        a-&gt;setName(&quot;a&quot;);
 
-            if(shared_parameters != 0)
-            {
-                muV = shared_parameters[sp_index++];
-                snV = shared_parameters[sp_index++];
-                snb = shared_parameters[sp_index++];
-            }
-            else
-            {
-                muV = Var(n,n_hidden_units,&quot;muV &quot;);
-                snV = Var(1,n_hidden_units,&quot;snV &quot;);
-                snb = Var(1,1,&quot;snB &quot;);
-            }
-            params.append(muV);
-            params.append(snV);
-            params.append(snb);
+        V = Var(ncomponents*(inputsize_+1),nhidden,&quot;V&quot;);
+        parameters.append(V);
 
-            if(architecture_type == &quot;embedding_neural_network&quot;)
-            {
-                if(shared_parameters.size() != 0)
-                    W = shared_parameters[sp_index++];
-                else
-                    W = Var(ncomponents,n_hidden_units,&quot;W &quot;);
-                tangent_plane = diagonalized_factors_product(W,1-a*a,V);
-                embedding = product(W,a);
-                output_embedding = Func(x,embedding);
-                params.append(W);
-            }
-            else if(architecture_type == &quot;single_neural_network&quot;)
-            {
-                if(shared_parameters.size() != 0)
-                {
-                    b = shared_parameters[sp_index++];
-                    W = shared_parameters[sp_index++];
-                }
-                else
-                {
-                    b = Var(ncomponents*n,1,&quot;b&quot;);
-                    W = Var(ncomponents*n,n_hidden_units,&quot;W &quot;);
-                }
-                tangent_plane = reshape(b + product(W,a),ncomponents,n);
-                params.append(b);
-                params.append(W);
-            }
-            else
-                PLERROR(&quot;NonLocalManifoldParzen::build_, unknown architecture_type option %s&quot;,
-                        architecture_type.c_str());
-            if(learn_mu)
-                mu = product(muV,a);
-            else
-            {
-                mu = new SourceVariable(n,1);
-                mu-&gt;value.fill(0);
-                mu_nneighbors = 0;
-            }
-            min_sig = new SourceVariable(1,1);
-            min_sig-&gt;value[0] = sigma_min;
-            min_sig-&gt;setName(&quot;min_sig&quot;);
-            if(shared_parameters.size() != 0)
-                init_sig = shared_parameters[sp_index++];
-            else
-            {
-                init_sig = Var(1,1);
-                init_sig-&gt;setName(&quot;init_sig&quot;);
-            }
-            params.append(init_sig);
+        // TODO: instead, make NllGeneralGaussianVariable use vector... (DONE)
+        //components = reshape(affine_transform(V,a),ncomponents,n);
+        components = affine_transform(V,a);
+        components-&gt;setName(&quot;components&quot;);
 
-            if(variances_transfer_function == &quot;softplus&quot;) sn = softplus(snb + product(snV,a))  + min_sig + softplus(init_sig);
-            else if(variances_transfer_function == &quot;square&quot;) sn = square(snb + product(snV,a)) + min_sig + square(init_sig);
-            else if(variances_transfer_function == &quot;exp&quot;) sn = exp(snb + product(snV,a)) + min_sig + exp(init_sig);
-            else PLERROR(&quot;In NonLocalManifoldParzen::build_ : unknown variances_transfer_function option %s &quot;, variances_transfer_function.c_str());
+        // Gaussian kernel parameters prediction
 
+        muV = Var(inputsize_+1,nhidden,&quot;muV&quot;);
+        snV = Var(2,nhidden,&quot;snV&quot;);
+    
+        parameters.append(muV);
+        parameters.append(snV);
 
+        if(learn_mu)
+            mu = affine_transform(muV,a);
+        else
+        {
+            mu = new SourceVariable(inputsize_,1);
+            mu-&gt;value.clear();
+        }
+        mu-&gt;setName(&quot;mu&quot;);
 
-            if(sigma_threshold_factor &gt; 0)
-            {
-                sn = threshold_bprop(sn,sigma_threshold_factor);
-            }
+        min_sig = new SourceVariable(1,1);
+        min_sig-&gt;value[0] = sigma_min;
+        min_sig-&gt;setName(&quot;min_sig&quot;);
+        init_sig = Var(1,1);
+        init_sig-&gt;setName(&quot;init_sig&quot;);
+        parameters.append(init_sig);
 
-            /*
-              if(noise &gt; 0)
-              {
-              if(noise_type == &quot;uniform&quot;)
-              {
-              PP&lt;UniformDistribution&gt; temp = new UniformDistribution();
-              Vec lower_noise(n);
-              Vec upper_noise(n);
-              for(int i=0; i&lt;n; i++)
-              {
-              lower_noise[i] = -1*noise;
-              upper_noise[i] = noise;
-              }
-              temp-&gt;min = lower_noise;
-              temp-&gt;max = upper_noise;
-              dist = temp;
-              }
-              else if(noise_type == &quot;gaussian&quot;)
-              {
-              PP&lt;GaussianDistribution&gt; temp = new GaussianDistribution();
-              Vec mu(n); mu.clear();
-              Vec eig_values(n);
-              Mat eig_vectors(n,n); eig_vectors.clear();
-              for(int i=0; i&lt;n; i++)
-              {
-              eig_values[i] = noise; // maybe should be adjusted to the sigma noiseat the input
-              eig_vectors(i,i) = 1.0;
-              }
-              temp-&gt;mu = mu;
-              temp-&gt;eigenvalues = eig_values;
-              temp-&gt;eigenvectors = eig_vectors;
-              dist = temp;
-              }
-              else PLERROR(&quot;In GaussianContinuumDistribution::build_() : noise_type %c not defined&quot;,noise_type.c_str());
-              noise_var = new PDistributionVariable(x,dist);
-              for(int k=0; k&lt;ncomponents; k++)
-              {
-              Var index_var = new SourceVariable(1,1);
-              index_var-&gt;value[0] = k;
-              Var f_k = new VarRowVariable(tangent_plane,index_var);
-              noise_var = noise_var - product(f_k,noise_var)* transpose(f_k)/pownorm(f_k,2);
-              }
+        sn = square(affine_transform(snV,a)) + min_sig + square(init_sig);
+        sn-&gt;setName(&quot;sn&quot;);
+        
+        if(sigma_threshold_factor &gt; 0)
+            sn = threshold_bprop(sn,sigma_threshold_factor);
 
-              noise_var = no_bprop(noise_var);
-              noise_var-&gt;setName(noise_type);
-              }
-              else
-              {
-
-              noise_var = new SourceVariable(n,1);
-              noise_var-&gt;setName(&quot;no noise&quot;);
-              for(int i=0; i&lt;n; i++)
-              noise_var-&gt;value[i] = 0;
-              }
-            */
-
-            // Path for noisy mu
-            //Var a_noisy = tanh(c + product(V,x+noise_var));
-            //mu_noisy = no_bprop(product(muV,a_noisy),noise_grad_factor);
-
-
-            tangent_plane-&gt;setName(&quot;tangent_plane &quot;);
-            mu-&gt;setName(&quot;mu &quot;);
-            sn-&gt;setName(&quot;sn &quot;);
-            a-&gt;setName(&quot;a &quot;);
-            if(architecture_type == &quot;embedding_neural_network&quot;)
-                embedding-&gt;setName(&quot;embedding &quot;);
-            x-&gt;setName(&quot;x &quot;);
-
-            predictor = Func(x, params , tangent_plane &amp; mu &amp; sn );
-        }
-
-        if(shared_parameters.size() == 0)
-        {
-            if (parameters.size()&gt;0 &amp;&amp; parameters.nelems() == predictor-&gt;parameters.nelems())
-                predictor-&gt;parameters.copyValuesFrom(parameters);
-            parameters.resize(predictor-&gt;parameters.size());
-            for (int i=0;i&lt;parameters.size();i++)
-                parameters[i] = predictor-&gt;parameters[i];
-        }
+        predictor = Func(x, parameters , components &amp; mu &amp; sn );
+    
         Var target_index = Var(1,1);
         target_index-&gt;setName(&quot;target_index&quot;);
         Var neighbor_indexes = Var(nneighbors,1);
         neighbor_indexes-&gt;setName(&quot;neighbor_indexes&quot;);
-        Var random_index = Var(1,1);
-        random_index-&gt;setName(&quot;neighbor_index&quot;);
-        /*
-        // The following variables are discarded to
-        // make the gradient computation faster
-        // and more stable in nlmp_general_gaussian
-        log_p_x = Var(L,1);
-        log_p_x-&gt;setName(&quot;log_p_x&quot;);
 
-        // Initialisation hack for nlmp_general_gaussian hack
-        for(int i=0; i&lt;log_p_x.length(); i++)
-        log_p_x-&gt;value[i] = MISSING_VALUE;
-
-        log_p_target = new VarRowsVariable(log_p_x,target_index);
-        log_p_target-&gt;value[0] = log(1.0/L);
-        log_p_target-&gt;setName(&quot;log_p_target&quot;);
-        log_p_neighbors =new VarRowsVariable(log_p_x,neighbor_indexes);
-        log_p_neighbors-&gt;setName(&quot;log_p_neighbors&quot;);
-        */
-
-        tangent_targets = Var(nneighbors,n);
+        tangent_targets = Var(nneighbors,inputsize_);
         if(mu_nneighbors &lt; 0 ) mu_nneighbors = nneighbors;
 
-        // compute - sum_{neighbors of x} log ( P(neighbor|x) ) according to semi-spherical model
         Var nll;
-        //if(noise &lt;= 0)
-        nll = nll_general_gaussian(tangent_plane, mu, sn, tangent_targets, log_L, mu_nneighbors,0,0); // + log_n_examples;
-        //else
-        //nll = nll_general_gaussian(tangent_plane, mu, sn, tangent_targets, log_L, mu_nneighbors,noise_var,mu_noisy); // + log_n_examples;
+        nll = nll_general_gaussian(components, mu, sn, tangent_targets, 
+                                   log_L, learn_mu, mu_nneighbors); 
 
         Var knn = new SourceVariable(1,1);
         knn-&gt;setName(&quot;knn&quot;);
         knn-&gt;value[0] = nneighbors;
         sum_nll = new ColumnSumVariable(nll) / knn;
-        /*
-          if(architecture_type == &quot;embedding_neural_network&quot;)
-          {
-          // Notes: - seulement prendre le plus proche voisin d'un voisin random
-          //        - il va peut-&#234;tre falloir utiliser des fonctions de distances diff&#233;rentes
-          //        - peut-&#234;tre utiliser les directions principales apprises!
-          //        - peut-&#234;tre utiliser les distances dans l'espace initial pour pond&#233;rer!
-          //        - il va peut-&#234;tre falloir mettre un poids diff&#233;rent sur ce nouveau co&#251;t
-          //        - utiliser ici une VarRowsVariable(...)
-          //        - question: est-ce que je devrais faire une bprop partout, juste sur embedding
-          //          juste sur neighbor et random, ... ?
 
-          //Var nearest_emb = product(W, tanh(c + product(V,rowOf(reference_set,neighbor_indexes))));
-          Var random_emb = product(W, tanh(c + product(V,rowOf(reference_set,random_index))));
-
-          //Var nearest_emb_diff = nearest_emb - embedding;
-          //Var random_emb_diff = random_emb - embedding;
-          //sum_nll += weight_embedding * (sum(square(nearest_emb_diff)) - sum(square(random_emb_diff)));
-          sum_nll += weight_embedding * diagonal_gaussian(random_emb,embedding,no_bprop(rowPowNorm(tangent_plane,2)));
-          }
-        */
+        // Weight decay penalty
         if(weight_decay &gt; 0 )
         {
-            if(penalty_type == &quot;L1_square&quot;) sum_nll += (square(sumabs(W))+ square(sumabs(V)) + square(sumabs(muV)) + square(sumabs(snV)))*weight_decay;
-            else if(penalty_type == &quot;L1&quot;) sum_nll += (sumabs(W)+ sumabs(V) + sumabs(muV) + sumabs(snV))*weight_decay;
-            else if(penalty_type == &quot;L2_square&quot;) sum_nll += (sumsquare(W)+ sumsquare(V) + sumsquare(muV) + sumsquare(snV))*weight_decay;
-            else PLERROR(&quot;In NonLocalManifoldParzen::build_(): penalty_type %s not recognized&quot;, penalty_type.c_str());
+            sum_nll += affine_transform_weight_penalty(
+                W,weight_decay,0,penalty_type) + 
+                affine_transform_weight_penalty(
+                V,weight_decay,0,penalty_type) + 
+                affine_transform_weight_penalty(
+                muV,weight_decay,0,penalty_type) + 
+                affine_transform_weight_penalty(
+                snV,weight_decay,0,penalty_type);
         }
-        /*
-          if(architecture_type == &quot;embedding_neural_network&quot;)
-          {
-          Var random_diff = Var(n,1);
-          cost_of_one_example = Func(x &amp; tangent_targets &amp; target_index &amp; neighbor_indexes &amp; random_diff &amp; random_index, predictor-&gt;parameters, sum_nll);
-          }
-          else
-        */
-        cost_of_one_example = Func(x &amp; tangent_targets &amp; target_index &amp; neighbor_indexes, predictor-&gt;parameters, sum_nll);
 
-        if(nneighbors_density &gt;= L || nneighbors_density &lt; 0) nneighbors_density = L;
+        cost_of_one_example = Func(x &amp; tangent_targets &amp; target_index &amp; 
+                                   neighbor_indexes, parameters, sum_nll);
 
-        t_row.resize(n);
-        Ut_svd.resize(n,n);
+        if(nneighbors_density &gt;= L || nneighbors_density &lt; 0) 
+            nneighbors_density = L;
+
+        // Output storage variables
+        t_row.resize(inputsize_);
+        Ut_svd.resize(inputsize_,inputsize_);
         V_svd.resize(ncomponents,ncomponents);
-        F.resize(tangent_plane-&gt;length(),tangent_plane-&gt;width());
-        z.resize(n);
-        x_minus_neighbor.resize(n);
-        neighbor_row.resize(n);
-        // log_density and Kernel methods
-        U_temp.resize(ncomponents,n);
-        mu_temp.resize(n);
+        F.resize(components-&gt;length(),components-&gt;width());
+        z.resize(inputsize_);
+        x_minus_neighbor.resize(inputsize_);
+        neighbor_row.resize(inputsize_);
+
+        // log_density and Kernel methods variables
+        U_temp.resize(ncomponents,inputsize_);
+        mu_temp.resize(inputsize_);
         sm_temp.resize(ncomponents);
         sn_temp.resize(1);
-        diff.resize(n);
+        diff.resize(inputsize_);
 
-        mus.resize(L, n);
+        mus.resize(L, inputsize_);
         sns.resize(L);
         sms.resize(L,ncomponents);
         Fs.resize(L);
         for(int i=0; i&lt;L; i++)
         {
-            Fs[i].resize(ncomponents,n);
+            Fs[i].resize(ncomponents,inputsize_);
         }
+
+        if(paramsvalues.length() == parameters.nelems())
+            parameters &lt;&lt; paramsvalues;
+        else
+        {
+            paramsvalues.resize(parameters.nelems());
+            initializeParams();
+            if(optimizer)
+                optimizer-&gt;reset();
+        }
+        parameters.makeSharedValue(paramsvalues);
     }
 
 }
 
-/*
-  void NonLocalManifoldParzen::update_reference_set_parameters()
-  {
-  // Compute Us, mus, sms, sns
-  Us.resize(L);
-  mus.resize(L, n);
-  sms.resize(L,ncomponents);
-  sns.resize(L);
-
-  for(int t=0; t&lt;L; t++)
-  {
-  Us[t].resize(ncomponents,n);
-  reference_set-&gt;getRow(t,t_row);
-  predictor-&gt;fprop(t_row, F.toVec() &amp; mus(t) &amp; sns.subVec(t,1));
-
-  // N.B. this is the SVD of F'
-  lapackSVD(F, Ut_svd, S_svd, V_svd,'A',1.5);
-  for (int k=0;k&lt;ncomponents;k++)
-  {
-  sms(t,k) = mypow(S_svd[k],2);
-  Us[t](k) &lt;&lt; Ut_svd(k);
-  }
-  }
-
-  }
-*/
-
 void NonLocalManifoldParzen::knn(const VMat&amp; vm, const Vec&amp; x, const int&amp; k, TVec&lt;int&gt;&amp; neighbors, bool sortk) const
 {
     int n = vm-&gt;length();
@@ -674,18 +396,6 @@
         neighbors[i] = int(distances(i,1));
     }
 
-
-    /*
-      for (int i = 0, j=0; i &lt; k  &amp;&amp; j&lt;n; j++)
-      {
-      real d = distances(j,0);
-      if (include_current_point || d&gt;0)  //Ouach, caca!!!
-      {
-      neighbors[i] = int(distances(j,1));
-      i++;
-      }
-      }
-    */
 }
 
 // ### Nothing to add here, simply calls build_
@@ -704,64 +414,65 @@
 #endif
 
 void NonLocalManifoldParzen::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
-{  inherited::makeDeepCopyFromShallowCopy(copies);
+{  
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
- deepCopyField(cost_of_one_example, copies);
- varDeepCopyField(x, copies);
- varDeepCopyField(b, copies);
- varDeepCopyField(W, copies);
- varDeepCopyField(c, copies);
- varDeepCopyField(V, copies);
- varDeepCopyField(muV, copies);
- varDeepCopyField(snV, copies);
- varDeepCopyField(snb, copies);
- varDeepCopyField(tangent_targets, copies);
- varDeepCopyField(tangent_plane, copies);
- varDeepCopyField(mu, copies);
- varDeepCopyField(sn, copies);
- varDeepCopyField(sum_nll, copies);
- varDeepCopyField(min_sig, copies);
- varDeepCopyField(init_sig, copies);
- varDeepCopyField(embedding, copies);
- deepCopyField(output_embedding, copies);
- deepCopyField(predictor, copies);
+    // Protected
 
- deepCopyField(U_temp,copies);
- deepCopyField(F, copies);
- deepCopyField(distances,copies);
- deepCopyField(mu_temp,copies);
- deepCopyField(sm_temp,copies);
- deepCopyField(sn_temp,copies);
- deepCopyField(diff,copies);
- deepCopyField(z,copies);
- deepCopyField(x_minus_neighbor,copies);
- deepCopyField(t_row,copies);
- deepCopyField(neighbor_row,copies);
- deepCopyField(log_gauss,copies);
- deepCopyField(t_dist,copies);
- deepCopyField(t_nn,copies);
- deepCopyField(Ut_svd, copies);
- deepCopyField(V_svd, copies);
- deepCopyField(S_svd, copies);
+    deepCopyField(cost_of_one_example, copies);
+    varDeepCopyField(x, copies);
+    varDeepCopyField(W, copies);
+    varDeepCopyField(V, copies);
+    varDeepCopyField(muV, copies);
+    varDeepCopyField(snV, copies);
+    varDeepCopyField(tangent_targets, copies);
+    varDeepCopyField(components, copies);
+    varDeepCopyField(mu, copies);
+    varDeepCopyField(sn, copies);
+    varDeepCopyField(sum_nll, copies);
+    varDeepCopyField(min_sig, copies);
+    varDeepCopyField(init_sig, copies);
+    deepCopyField(predictor, copies);
+    deepCopyField(U_temp,copies);
+    deepCopyField(F, copies);
+    deepCopyField(distances,copies);
+    deepCopyField(mu_temp,copies);
+    deepCopyField(sm_temp,copies);
+    deepCopyField(sn_temp,copies);
+    deepCopyField(diff,copies);
+    deepCopyField(z,copies);
+    deepCopyField(x_minus_neighbor,copies);
+    deepCopyField(t_row,copies);
+    deepCopyField(neighbor_row,copies);
+    deepCopyField(log_gauss,copies);
+    deepCopyField(t_dist,copies);
+    deepCopyField(t_nn,copies);
+    deepCopyField(Ut_svd, copies);
+    deepCopyField(V_svd, copies);
+    deepCopyField(S_svd, copies);
+    deepCopyField(mus, copies);
+    deepCopyField(sns, copies);
+    deepCopyField(sms, copies);
+    deepCopyField(Fs, copies);
+    deepCopyField(train_set_with_targets, copies);
+    deepCopyField(targets_vmat, copies);
+    varDeepCopyField(totalcost, copies);
+    deepCopyField(paramsvalues, copies);
+    
+    // Public
 
- deepCopyField(mus, copies);
- deepCopyField(sns, copies);
- deepCopyField(sms, copies);
- deepCopyField(Fs, copies);
+    deepCopyField(parameters, copies);    
+    deepCopyField(reference_set,copies);
+    deepCopyField(optimizer, copies);
 
- deepCopyField(parameters, copies);
- deepCopyField(shared_parameters, copies);
-
- deepCopyField(reference_set,copies);
- varDeepCopyField(hidden_layer, copies);
- deepCopyField(optimizer, copies);
-
 }
 
 
 void NonLocalManifoldParzen::forget()
 {
+    inherited::forget();
     if (train_set) initializeParams();
+    if(optimizer) optimizer-&gt;reset();
     stage = 0;
 }
 
@@ -772,14 +483,6 @@
     // except for sn...
     bool flag = (nstages == stage);
 
-    if(store_prediction &amp;&amp; flag)
-    {
-        for(int i=0; i&lt;L; i++)
-        {
-            sns[i] += sigma_min - min_sig-&gt;value[0];
-        }
-    }
-
     // Update sigma_min, in case it was changed,
     // e.g. using an HyperLearner
     min_sig-&gt;value[0] = sigma_min;
@@ -787,51 +490,28 @@
     // Set train_stats if not already done.
     if (!train_stats)
         train_stats = new VecStatsCollector();
-    /*
-    VMat train_set_with_targets;
-    VMat targets_vmat;
-    Var totalcost;
-    int nsamples;
-    */
+
     if (!cost_of_one_example)
         PLERROR(&quot;NonLocalManifoldParzen::train: build has not been run after setTrainingSet!&quot;);
-    /*
-      if(stage==0)
-      {
-      train_set = new SubVMatrix(train_set,0,0,train_set.length()-omit_last,train_set.width());
-      }
-    */
-    /*
-      if(architecture_type == &quot;embedding_neural_network&quot;)
-      targets_vmat = hconcat(local_neighbors_differences(train_set, nneighbors, false, true),random_neighbors_differences(train_set,1,false,true));
-      else*/
 
     if(stage == 0)
     {
-        targets_vmat = local_neighbors_differences(train_set, nneighbors, false, true);
-
-        train_set_with_targets = hconcat(train_set, targets_vmat);
-        train_set_with_targets-&gt;defineSizes(inputsize()+ inputsize()*nneighbors+1+nneighbors /*+ (architecture_type == &quot;embedding_neural_network&quot; ? inputsize()+1:0)*/,0);
+        targets_vmat = append_neighbors(
+            train_set, nneighbors, true);
         nsamples = batch_size&gt;0 ? batch_size : train_set-&gt;length();
 
         totalcost = meanOf(train_set_with_targets, cost_of_one_example, nsamples);
 
         if(optimizer)
         {
-            if(shared_parameters.size()!=0)
-                optimizer-&gt;setToOptimize(shared_parameters, totalcost);
-            else
-                optimizer-&gt;setToOptimize(parameters, totalcost);
+            optimizer-&gt;setToOptimize(parameters, totalcost);
             optimizer-&gt;build();
         }
         else PLERROR(&quot;NonLocalManifoldParzen::train can't train without setting an optimizer first!&quot;);
     }
 
-    dynamic_cast&lt;SumOfVariable*&gt;( (Variable*) totalcost)-&gt;curpos = curpos;
+    int optstage_per_lstage = train_set-&gt;length()/nsamples;
 
-    // number of optimizer stages corresponding to one learner stage (one epoch)
-    if(optstage_per_lstage &lt; 0) optstage_per_lstage = train_set-&gt;length()/nsamples;
-
     PP&lt;ProgressBar&gt; pb;
     if(report_progress&gt;0)
         pb = new ProgressBar(&quot;Training NonLocalManifoldParzen from stage &quot; + tostring(stage) + &quot; to &quot; + tostring(nstages), nstages-stage);
@@ -850,31 +530,13 @@
         if(verbosity&gt;2)
             cout &lt;&lt; &quot;Epoch &quot; &lt;&lt; stage &lt;&lt; &quot; train objective: &quot; &lt;&lt; train_stats-&gt;getMean() &lt;&lt; endl;
         ++stage;
-        if(stage%save_every == 0 &amp;&amp; store_prediction &amp;&amp; !flag)
-        {
-            for(int t=0; t&lt;L;t++)
-            {
-                reference_set-&gt;getRow(t,neighbor_row);
-                predictor-&gt;fprop(neighbor_row, F.toVec() &amp; mus(t) &amp; sns.subVec(t,1));
-                // N.B. this is the SVD of F'
-                lapackSVD(F, Ut_svd, S_svd, V_svd,'A',1.5);
-                for (int k=0;k&lt;ncomponents;k++)
-                {
-                    sms(t,k) = mypow(S_svd[k],2);
-                    Fs[t](k) &lt;&lt; Ut_svd(k);
-                }
-
-            }
-        }
-
         if(pb)
             pb-&gt;update(stage-initial_stage);
-
     }
     if(verbosity&gt;1)
         cout &lt;&lt; &quot;EPOCH &quot; &lt;&lt; stage &lt;&lt; &quot; train objective: &quot; &lt;&lt; train_stats-&gt;getMean() &lt;&lt; endl;
 
-    if(save_every &lt; 0 &amp;&amp; store_prediction &amp;&amp; !flag)
+    if(store_prediction &amp;&amp; !flag)
     {
         for(int t=0; t&lt;L;t++)
         {
@@ -887,10 +549,9 @@
                 sms(t,k) = mypow(S_svd[k],2);
                 Fs[t](k) &lt;&lt; Ut_svd(k);
             }
-
+            sns[t] += sigma_min - min_sig-&gt;value[0];
         }
     }
-    curpos = dynamic_cast&lt;SumOfVariable*&gt;( (Variable*) totalcost)-&gt;curpos;
 }
 
 //////////////////////
@@ -898,55 +559,17 @@
 //////////////////////
 void NonLocalManifoldParzen::initializeParams()
 {
-    resetGenerator(seed_);
-
-    if (architecture_type==&quot;embedding_neural_network&quot;)
-    {
-        real delta = 1.0 / sqrt(real(inputsize()));
-        random_gen-&gt;fill_random_uniform(V-&gt;value, -delta, delta);
-        delta = 1.0 / real(n_hidden_units);
-        random_gen-&gt;fill_random_uniform(W-&gt;matValue, -delta, delta);
-        c-&gt;value.clear();
-        snb-&gt;value.clear();
-        random_gen-&gt;fill_random_uniform(snV-&gt;matValue, -delta, delta);
-        random_gen-&gt;fill_random_uniform(muV-&gt;matValue, -delta, delta);
-        //min_sig-&gt;value[0] = sigma_init;
-        //min_d-&gt;value.fill(diff_init);
-        if(variances_transfer_function == &quot;softplus&quot;) {
-            init_sig-&gt;value[0] = pl_log(exp(sigma_init)-1); }
-        else if(variances_transfer_function == &quot;square&quot;) { init_sig-&gt;value[0] = sqrt(sigma_init);}
-        else if(variances_transfer_function == &quot;exp&quot;) {
-            init_sig-&gt;value[0] = pl_log(sigma_init); }
-    }
-    else if (architecture_type==&quot;single_neural_network&quot;)
-    {
-        real delta = 1.0 / sqrt(real(inputsize()));
-        if (!hidden_layer)
-           random_gen-&gt;fill_random_uniform(V-&gt;value, -delta, delta);
-        delta = 1.0 / real(n_hidden_units);
-        random_gen-&gt;fill_random_uniform(W-&gt;matValue, -delta, delta);
-        if (!hidden_layer) c-&gt;value.clear();
-        snb-&gt;value.clear();
-        random_gen-&gt;fill_random_uniform(snV-&gt;matValue, -delta, delta);
-        random_gen-&gt;fill_random_uniform(muV-&gt;matValue, -delta, delta);
-        b-&gt;value.clear();
-        //min_sig-&gt;value[0] = sigma_init;
-        //min_d-&gt;value.fill(diff_init);
-        if(variances_transfer_function == &quot;softplus&quot;) {
-            init_sig-&gt;value[0] = pl_log(exp(sigma_init)-1); }
-        else if(variances_transfer_function == &quot;square&quot;) { init_sig-&gt;value[0] = sqrt(sigma_init);}
-        else if(variances_transfer_function == &quot;exp&quot;) {
-            init_sig-&gt;value[0] = pl_log(sigma_init);}
-    }
-    else PLERROR(&quot;other types not handled yet!&quot;);
-
-    /*
-      for(int i=0; i&lt;log_p_x.length(); i++)
-      //p_x-&gt;value[i] = log(1.0/p_x.length());
-      log_p_x-&gt;value[i] = MISSING_VALUE;
-    */
-    if(optimizer)
-        optimizer-&gt;reset();
+    real delta = 1.0 / sqrt(real(inputsize_));
+    random_gen-&gt;fill_random_uniform(W-&gt;value, -delta, delta);
+    delta = 1.0 / real(nhidden);
+    random_gen-&gt;fill_random_uniform(V-&gt;matValue, -delta, delta);
+    random_gen-&gt;fill_random_uniform(snV-&gt;matValue, -delta, delta);
+    random_gen-&gt;fill_random_uniform(muV-&gt;matValue, -delta, delta);
+    W-&gt;matValue(0).clear();
+    V-&gt;matValue(0).clear();
+    muV-&gt;matValue(0).clear();
+    snV-&gt;matValue(0).clear();
+    init_sig-&gt;value[0] = sqrt(sigma_init);
 }
 
 /////////////////
@@ -970,38 +593,12 @@
         }
     }
 
-
     min_sig-&gt;value[0] = sigma_min;
 
-/*
-  if(magnified_version)
-  {
-  predictor-&gt;fprop(x, F.toVec() &amp; mu_temp &amp; sn_temp);
-
-  // N.B. this is the SVD of F'
-  lapackSVD(F, Ut_svd, S_svd, V_svd,'A',1.5);
-  for (int k=0;k&lt;ncomponents;k++)
-  {
-  sm_temp[k] = mypow(S_svd[k],2);
-  F(k) &lt;&lt; Ut_svd(k);
-  }
-
-  mahal = -0.5*pownorm(mu_temp)/sn_temp[0];
-  norm_term = - n/2.0 * Log2Pi - 0.5*(n-ncomponents)*log(sn_temp[0]);
-  for(int k=0; k&lt;ncomponents; k++)
-  {
-  mahal -= square(dot(F(k), mu_temp))*(0.5/(sm_temp[k]+sn_temp[0]) - 0.5/sn_temp[0]);
-  norm_term -= 0.5*log(sm_temp[k]+sn_temp[0]);
-  }
-
-  ret = mahal + norm_term + log((real)nneighbors) - log((real)L);
-  }
-  else
-  {*/
     if(nneighbors_density != L)
     {
         // Fetching nearest neighbors for density estimation.
-        knn(reference_set,x,nneighbors_density,t_nn,bool(0));
+        knn(reference_set,x,nneighbors_density,t_nn,0);
         log_gauss.resize(t_nn.length());
         for(int neighbor=0; neighbor&lt;t_nn.length(); neighbor++)
         {
@@ -1019,27 +616,29 @@
             }
             else
             {
-                mu_temp &lt;&lt; mus(t_nn[neighbor]);
+                if(learn_mu)
+                    mu_temp &lt;&lt; mus(t_nn[neighbor]);
                 sn_temp[0] = sns[t_nn[neighbor]];
                 sm_temp &lt;&lt; sms(t_nn[neighbor]);
                 U_temp &lt;&lt; Fs[t_nn[neighbor]];
             }
-            substract(t_row,neighbor_row,x_minus_neighbor);
-            //substract(x_minus_neighbor,mus(t_nn[neighbor]),z);
-            substract(x_minus_neighbor,mu_temp,z);
-
-            //mahal = -0.5*pownorm(z)/sns[t_nn[neighbor]];
-            //norm_term = - n/2.0 * Log2Pi - log_L - 0.5*(n-ncomponents)*log(sns[t_nn[neighbor]]);
-
+            if(learn_mu)
+            {
+                substract(t_row,neighbor_row,x_minus_neighbor);
+                substract(x_minus_neighbor,mu_temp,z);
+            }
+            else
+                substract(t_row,neighbor_row,z);
+                
             mahal = -0.5*pownorm(z)/sn_temp[0];
-            norm_term = - n/2.0 * Log2Pi - log_L - 0.5*(n-ncomponents)*pl_log(sn_temp[0]);
+            norm_term = - inputsize_/2.0 * Log2Pi 
+                - log_L - 0.5*(inputsize_-ncomponents)*pl_log(sn_temp[0]);
 
 
             for(int k=0; k&lt;ncomponents; k++)
             {
-                //mahal -= square(dot(z,Us[t_nn[neighbor]](k)))*(0.5/(sms(t_nn[neighbor],k)+sns[t_nn[neighbor]]) - 0.5/sns[t_nn[neighbor]]); // Pourrait &#234;tre acc&#233;l&#233;r&#233;!
-                //norm_term -= 0.5*log(sms(t_nn[neighbor],k)+sns[t_nn[neighbor]]);
-                mahal -= square(dot(z,U_temp(k)))*(0.5/(sm_temp[k]+sn_temp[0]) - 0.5/sn_temp[0]);
+                mahal -= square(dot(z,U_temp(k)))*(0.5/(sm_temp[k]+sn_temp[0]) 
+                                                   - 0.5/sn_temp[0]);
                 norm_term -= 0.5*pl_log(sm_temp[k]+sn_temp[0]);
             }
 
@@ -1067,28 +666,29 @@
             }
             else
             {
-                mu_temp &lt;&lt; mus(t);
+                if(learn_mu)
+                    mu_temp &lt;&lt; mus(t);
                 sn_temp[0] = sns[t];
                 sm_temp &lt;&lt; sms(t);
                 U_temp &lt;&lt; Fs[t];
             }
 
-            substract(t_row,neighbor_row,x_minus_neighbor);
-            //substract(x_minus_neighbor,mus(t),z);
-            substract(x_minus_neighbor,mu_temp,z);
+            if(learn_mu)
+            {
+                substract(t_row,neighbor_row,x_minus_neighbor);
+                substract(x_minus_neighbor,mu_temp,z);
+            }
+            else
+                substract(t_row,neighbor_row,z);
 
-            //mahal = -0.5*pownorm(z)/sns[t];
-            //norm_term = - n/2.0 * Log2Pi - log_L - 0.5*(n-ncomponents)*log(sns[t]);
-
             mahal = -0.5*pownorm(z)/sn_temp[0];
-            norm_term = - n/2.0 * Log2Pi - log_L - 0.5*(n-ncomponents)*pl_log(sn_temp[0]);
+            norm_term = - inputsize_/2.0 * Log2Pi - log_L 
+                - 0.5*(inputsize_-ncomponents)*pl_log(sn_temp[0]);
 
             for(int k=0; k&lt;ncomponents; k++)
             {
-                //mahal -= square(dot(z,Us[t](k)))*(0.5/(sms(t,k)+sns[t]) - 0.5/sns[t]); // Pourrait &#234;tre acc&#233;l&#233;r&#233;!
-                //norm_term -= 0.5*log(sms(t,k)+sns[t]);
-
-                mahal -= square(dot(z,U_temp(k)))*(0.5/(sm_temp[k]+sn_temp[0]) - 0.5/sn_temp[0]);
+                mahal -= square(dot(z,U_temp(k)))*(0.5/(sm_temp[k]+sn_temp[0]) 
+                                                   - 0.5/sn_temp[0]);
                 norm_term -= 0.5*pl_log(sm_temp[k]+sn_temp[0]);
             }
 
@@ -1096,24 +696,11 @@
         }
     }
     ret = logadd(log_gauss);
-    //}
 
     return ret;
 }
 
-/*
-  Mat NonLocalManifoldParzen::getEigenvectors(int j) const {
-  {
-  return Us[j];
-  }
 
-  Vec NonLocalManifoldParzen::getTrainPoint(int j) const {
-  Vec ret(reference_set-&gt;width());
-  reference_set-&gt;getRow(j,ret);
-  return ret;
-  }
-*/
-
 ///////////////////
 // computeOutput //
 ///////////////////
@@ -1121,10 +708,7 @@
 {
     switch(outputs_def[0])
     {
-    case 'm':
-        output_embedding(input);
-        output &lt;&lt; embedding-&gt;value;
-        break;
+        /*
     case 'r':
     {
         string fsave = &quot;&quot;;
@@ -1198,7 +782,9 @@
         output &lt;&lt; F.toVec();
         break;
     }
+        */
     default:
+        
         inherited::computeOutput(input,output);
     }
 }
@@ -1210,6 +796,7 @@
 {
     switch(outputs_def[0])
     {
+        /*
     case 'm':
         return ncomponents;
         break;
@@ -1217,39 +804,12 @@
         return n;
     case 't':
         return ncomponents*n;
+        */
     default:
         return inherited::outputsize();
     }
 }
 
-real NonLocalManifoldParzen::evaluate(Vec x1,Vec x2,real scale)
-{
-    real ret;
-
-    // Update sigma_min, in case it was changed,
-    // e.g. using an HyperLearner
-    min_sig-&gt;value[0] = sigma_min;
-
-    predictor-&gt;fprop(x2, F.toVec() &amp; mu_temp &amp; sn_temp);
-
-    // N.B. this is the SVD of F'
-    lapackSVD(F, Ut_svd, S_svd, V_svd,'A',1.5);
-    for (int k=0;k&lt;ncomponents;k++)
-    {
-        sm_temp[k] = mypow(S_svd[k],2);
-        F(k) &lt;&lt; Ut_svd(k);
-    }
-
-    diff = x1 - x2;
-    diff -= mu_temp;
-    ret = scale * pownorm(diff)/sn_temp[0];
-    for (int k = 0; k &lt; ncomponents ; k++) {
-        ret += scale * (1.0 /( sm_temp[k] + sn_temp[0]) - 1.0/sn_temp[0]) * square(dot(F(k), diff));
-    }
-    return ret;
-
-}
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/distributions/NonLocalManifoldParzen.h
===================================================================
--- trunk/plearn_learners/distributions/NonLocalManifoldParzen.h	2007-10-10 22:33:15 UTC (rev 8171)
+++ trunk/plearn_learners/distributions/NonLocalManifoldParzen.h	2007-10-10 22:41:33 UTC (rev 8172)
@@ -68,46 +68,34 @@
     // * protected options *
     // *********************
 
-    // ### declare protected option fields (such as learnt parameters) here
-
-
-    // NON-OPTION FIELDS
-    //! Input size
-    int n;
+    //! Number of gaussians
+    int L;
+    //! Logarithm of number of gaussians
+    real log_L;
     //! Cost of one example
     Func cost_of_one_example;
     //! Input vector
     Var x;
     //! Parameters of the neural network
-    Var b, W, c, V, muV, snV, snb;
+    Var W, V, muV, snV;
     //! Tangent vector targets
     Var tangent_targets;
     //! Tangent vectors spanning the tangent plane, given by
     //! the neural network
-    Var tangent_plane;
+    Var components;
     //! Mean of the gaussian
     Var mu;
     //! Sigma^2_noise of the gaussian
     Var sn;
-    //Var mu_noisy, noise_var;
-    //PP&lt;PDistribution&gt; dist;
     //! Sum of NLL cost
     Var sum_nll;
     //! Mininum value of sigma^2_noise
     Var min_sig;
     //! Initial (approximate) value of sigma^2_noise
     Var init_sig;
-    //! Embedding computed by the (embedding) neural network
-    Var embedding;
-    //! Function to output the embedding
-    Func output_embedding;
     //! Predictor of the parameters of the gaussian at x
     Func predictor; // predicts everything about the gaussian
 
-    //TVec&lt; Mat &gt; Us;
-    //Mat mus,sms;
-    //Vec sns;
-
     //! log_density and Kernel methods' temporary variables
     mutable Mat U_temp, F, distances;
     //! log_density and Kernel methods' temporary variables
@@ -132,101 +120,66 @@
     //! Predictions for F
     TVec&lt;Mat&gt; Fs;
 
-    //! Position of the current example in
-    //! the training set
-    int curpos;
-
+    //! Training set concatenated with nearest neighbor targets
     VMat train_set_with_targets;
+    //! Nearest neighbor differences targets
     VMat targets_vmat;
+    //! Total cost Var
     Var totalcost;
+    //! Batch size
     int nsamples;
 
+    //! Parameter values
+    Vec paramsvalues;
+
 public:
 
     // ************************
     // * public build options *
     // ************************
 
-    // ### declare public option fields (such as build options) here
+    // ** General parameters **
 
     //! Parameters of the model
-    //! It is put here so that these parameters can
-    //! be shared by different NonLocalManifoldParzen
-    //! objects (eventually, should use the &quot;friend class&quot;
-    //! principal instead)
     VarArray parameters;
+    //! Reference set of points in the gaussian mixture
+    VMat reference_set;
+    //! Number of reduced dimensions (number of tangent vectors to compute)
+    int ncomponents;
+    //! Number of neighbors used for gradient descent
+    int nneighbors;
+    //! Number of neighbors for the p(x) density estimation
+    int nneighbors_density;
+    //! Indication that the predicted parameters should be stored
+    bool store_prediction;
+    
+    // ** Gaussian kernel options **
 
-
-    // Embedding penalty weight
-    //real weight_embedding;
-
-    //! Weight decay for all weights
-    real weight_decay;
-    //! Penalty type to use on the weights
-    string penalty_type;
-
-    //real noise_grad_factor;
-    //real noise;
-    //string noise_type;
-    //int omit_last;
-    //bool magnified_version;
-
     //! Indication that the mean of the gaussians should be learned
     bool learn_mu;
-    //! Reference set of points in the gaussian mixture
-    VMat reference_set;
     //! Initial (approximate) value of sigma^2_noise
     real sigma_init;
     //! Minimum value of sigma^2_noise
     real sigma_min;
-    //! Number of gaussians
-    int L;
-    //! User specified hidden layer NaryVariable subclass
-    Var hidden_layer;
-    //! Logarithm of number of gaussians
-    real log_L;
-    //! Number of neighbors used for gradient descent
-    int nneighbors;
-    //! Number of neighbors for the p(x) density estimation
-    int nneighbors_density;
     //! Number of neighbors to learn the mus
     int mu_nneighbors;
-    //! Number of reduced dimensions (number of tangent vectors to compute)
-    int ncomponents;
     //! Threshold applied on the update rule for sigma^2_noise
     real sigma_threshold_factor;
-    //! Variance transfer function (&quot;square&quot;, &quot;exp&quot; or &quot;softplus&quot;)
-    string variances_transfer_function;
+    //! SVD threshold on the eigen values
+    real svd_threshold;
+
+    // ** Neural network predictor option **
+
+    //! Number of hidden units
+    int nhidden;
+    //! Weight decay for all weights
+    real weight_decay;
+    //! Penalty type to use on the weights
+    string penalty_type;
     //! Optimizer of the neural network
     PP&lt;Optimizer&gt; optimizer;
-    //! Architecture type of the neural network (&quot;single_neural_network&quot; or &quot;embedding_neural_nework&quot;)
-    string architecture_type;
-    //! Number of hidden units
-    int n_hidden_units;
     //! Batch size of the gradient-based optimization
     int batch_size;
-    //! SVD threshold on the eigen values
-    real svd_threshold;
-    //! Number of steps in the random walk
-    int rw_n_step;
-    //! Size of the step
-    real rw_size_step;
-    //! Which principal component to follow
-    int rw_ith_component;
-    //! File name for the rw saves
-    string rw_file_name;
-    //! Number of iterations between rw saves
-    int rw_save_every;
-    //! Indication that the predicted parameters should be stored
-    bool store_prediction;
-    //! Parameters to share
-    VarArray shared_parameters;
-    //! Number of optimizer stages
-    int optstage_per_lstage;
-    //! Number of iterations since the last save
-    //! after which the parameters
-    //! must be saved
-    int save_every;
 
     // ****************
     // * Constructors *
@@ -235,7 +188,6 @@
     //! Default constructor.
     NonLocalManifoldParzen();
 
-
     // ********************
     // * PLearner methods *
     // ********************
@@ -247,6 +199,9 @@
 
     //void update_reference_set_parameters();
 
+    //! Finds nearest neighbors of &quot;x&quot; in set &quot;vm&quot; and 
+    //! puts their indices in &quot;neighbors&quot;. The neighbors
+    //! can be sorted if &quot;sortk&quot; is true
     void knn(const VMat&amp; vm, const Vec&amp; x, const int&amp; k, TVec&lt;int&gt;&amp; neighbors, bool sortk) const;
 
 protected:
@@ -284,9 +239,6 @@
     //! Return log of probability density log(p(y)).
     virtual real log_density(const Vec&amp; x) const;
 
-    //! Return log density of ith point in reference_set
-    real log_density(int i);
-
     //! The role of the train method is to bring the learner up to stage==nstages,
     //! updating the train_stats collector with training costs measured on-line in the process.
     virtual void train();
@@ -330,7 +282,6 @@
 
     //Mat getEigenvectors(int j) const;
     //Vec getTrainPoint(int j) const;
-    real evaluate(const Vec x1,const Vec x2,real scale=1);
 };
 
 // Declares a few other classes and functions related to this class.


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001619.html">[Plearn-commits] r8171 - trunk/plearn/vmat
</A></li>
	<LI>Next message: <A HREF="001621.html">[Plearn-commits] r8173 - trunk/plearn_learners/distributions
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1620">[ date ]</a>
              <a href="thread.html#1620">[ thread ]</a>
              <a href="subject.html#1620">[ subject ]</a>
              <a href="author.html#1620">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
