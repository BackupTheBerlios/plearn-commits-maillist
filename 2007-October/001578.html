<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8130 - in trunk: plearn/ker	plearn_learners/unsupervised
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-October/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8130%20-%20in%20trunk%3A%20plearn/ker%0A%09plearn_learners/unsupervised&In-Reply-To=%3C200710022059.l92KxbBe019858%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001577.html">
   <LINK REL="Next"  HREF="001579.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8130 - in trunk: plearn/ker	plearn_learners/unsupervised</H1>
    <B>louradou at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8130%20-%20in%20trunk%3A%20plearn/ker%0A%09plearn_learners/unsupervised&In-Reply-To=%3C200710022059.l92KxbBe019858%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8130 - in trunk: plearn/ker	plearn_learners/unsupervised">louradou at mail.berlios.de
       </A><BR>
    <I>Tue Oct  2 22:59:37 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001577.html">[Plearn-commits] r8129 - trunk/plearn/math
</A></li>
        <LI>Next message: <A HREF="001579.html">[Plearn-commits] r8131 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1578">[ date ]</a>
              <a href="thread.html#1578">[ thread ]</a>
              <a href="subject.html#1578">[ subject ]</a>
              <a href="author.html#1578">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: louradou
Date: 2007-10-02 22:59:36 +0200 (Tue, 02 Oct 2007)
New Revision: 8130

Modified:
   trunk/plearn/ker/GeodesicDistanceKernel.cc
   trunk/plearn/ker/GeodesicDistanceKernel.h
   trunk/plearn/ker/Kernel.cc
   trunk/plearn/ker/Kernel.h
   trunk/plearn/ker/VMatKernel.cc
   trunk/plearn/ker/VMatKernel.h
   trunk/plearn_learners/unsupervised/Isomap.cc
   trunk/plearn_learners/unsupervised/Isomap.h
   trunk/plearn_learners/unsupervised/KernelProjection.cc
Log:
some debugging and improvement with kernel functions



Modified: trunk/plearn/ker/GeodesicDistanceKernel.cc
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/GeodesicDistanceKernel.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -71,6 +71,19 @@
     build();
 }
 
+GeodesicDistanceKernel::GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn,
+                                               string the_geodesic_file, bool the_pow_distance,
+					       string the_method)
+    : geodesic_file(the_geodesic_file),
+      knn(the_knn),
+      pow_distance(the_pow_distance),
+      shortest_algo(the_method)
+{
+    distance_kernel = the_distance_kernel;
+    build();
+}
+
+
 PLEARN_IMPLEMENT_OBJECT(GeodesicDistanceKernel,
                         &quot;Computes the geodesic distance based on k nearest neighbors.&quot;,
                         &quot;&quot;

Modified: trunk/plearn/ker/GeodesicDistanceKernel.h
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.h	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/GeodesicDistanceKernel.h	2007-10-02 20:59:36 UTC (rev 8130)
@@ -88,6 +88,9 @@
     //! Convenient constructor.
     GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn = 10,
                            string the_geodesic_file = &quot;&quot;, bool the_pow_distance = false);
+    GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn = 10,
+                           string the_geodesic_file = &quot;&quot;, bool the_pow_distance = false,
+			   string the_method = &quot;floyd&quot;);
 
     // ******************
     // * Kernel methods *

Modified: trunk/plearn/ker/Kernel.cc
===================================================================
--- trunk/plearn/ker/Kernel.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/Kernel.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -48,11 +48,11 @@
 #include &lt;plearn/base/tostring.h&gt;
 #include &lt;plearn/base/ProgressBar.h&gt;
 #include &lt;plearn/math/TMat_maths.h&gt;
+#include &lt;plearn/base/RemoteDeclareMethod.h&gt;
 
 namespace PLearn {
 using namespace std;
 
-using namespace std;
   
 PLEARN_IMPLEMENT_ABSTRACT_OBJECT(Kernel,
                                  &quot;A Kernel is a real-valued function K(x,y).&quot;,
@@ -112,6 +112,21 @@
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void Kernel::declareMethods(RemoteMethodMap&amp; rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, &quot;returnComputedGramMatrix&quot;, &amp;Kernel::returnComputedGramMatrix,
+        (BodyDoc(&quot;\n&quot;),
+         RetDoc (&quot;Returns the Gram Matrix&quot;)));
+}
+
 ///////////
 // build //
 ///////////
@@ -370,6 +385,15 @@
     }
 }
 
+Mat Kernel::returnComputedGramMatrix() const
+{
+    if (!data)
+        PLERROR(&quot;Kernel::returnComputedGramMatrix should be called only after setDataForKernelMatrix&quot;);
+    int l=data.length();
+    Mat K(l,l);
+    computeGramMatrix(K);
+    return K;
+}
 /////////////////////////////
 // computeSparseGramMatrix //
 /////////////////////////////

Modified: trunk/plearn/ker/Kernel.h
===================================================================
--- trunk/plearn/ker/Kernel.h	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/Kernel.h	2007-10-02 20:59:36 UTC (rev 8130)
@@ -45,7 +45,6 @@
 
 #include &lt;plearn/base/Object.h&gt;
 #include &lt;plearn/vmat/VMat.h&gt;
-//#include &quot;PLMPI.h&quot;
 
 namespace PLearn {
 using namespace std;
@@ -74,6 +73,9 @@
     int n_examples;
 
     static void declareOptions(OptionList&amp; ol);
+    
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap&amp; rmm);
 
 public:
 
@@ -148,6 +150,7 @@
 
     //! Call evaluate_i_j to fill each of the entries (i,j) of symmetric matrix K.
     virtual void computeGramMatrix(Mat K) const;
+    virtual Mat returnComputedGramMatrix() const;
 
     /**
      *  Fill K[i] with the non-zero elements of row i of the Gram matrix.

Modified: trunk/plearn/ker/VMatKernel.cc
===================================================================
--- trunk/plearn/ker/VMatKernel.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/VMatKernel.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -2,7 +2,7 @@
 
 // VMatKernel.cc
 //
-// Copyright (C) 2005 Benoit Cromp 
+// Copyright (C) 2005 Benoit Cromp
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -36,7 +36,7 @@
  * $Id$ 
  ******************************************************* */
 
-// Authors: Benoit Cromp
+// Authors: Benoit Cromp, Jerome Louradour
 
 /*! \file VMatKernel.cc */
 
@@ -49,8 +49,8 @@
 //////////////////
 // VMatKernel //
 //////////////////
+// ### Initialize all fields to their default value here
 VMatKernel::VMatKernel() 
-/* ### Initialize all fields to their default value here */
 {
     // ...
 
@@ -69,20 +69,14 @@
 ////////////////////
 void VMatKernel::declareOptions(OptionList&amp; ol)
 {
-    // ### Declare all of this object's options here
-    // ### For the &quot;flags&quot; of each option, you should typically specify  
-    // ### one of OptionBase::buildoption, OptionBase::learntoption or 
-    // ### OptionBase::tuningoption. Another possible flag to be combined with
-    // ### is OptionBase::nosave
+    declareOption(ol,&quot;source&quot;,&amp;VMatKernel::source,
+                  OptionBase::buildoption,
+        &quot;Gram matrix&quot;);
 
-    // ### ex:
-    // declareOption(ol, &quot;myoption&quot;, &amp;VMatKernel::myoption, OptionBase::buildoption,
-    //               &quot;Help text describing this option&quot;);
-    // ...
+    declareOption(ol,&quot;train_indices&quot;,&amp;VMatKernel::train_indices,
+                  OptionBase::learntoption,
+        &quot;List of (real)indices corresponding to training samples&quot;);
 
-    declareOption(ol,&quot;source&quot;,&amp;VMatKernel::source,OptionBase::buildoption,
-                  &quot;Gram matrix&quot;);
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -102,39 +96,109 @@
 ////////////
 void VMatKernel::build_()
 {
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation. 
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of all serialised options.
-    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot; options have been modified.
-    // ### You should assume that the parent class' build_() has already been called.
+    PLASSERT( !(source) || ( source-&gt;length() == source-&gt;width() ) );
+    if ( !specify_dataset )
+        train_indices.resize(0);
 }
 
 //////////////
 // evaluate //
 //////////////
-real VMatKernel::evaluate(const Vec&amp; x1, const Vec&amp; x2) const {
-    PLASSERT( source );
+real VMatKernel::evaluate(const Vec&amp; x1, const Vec&amp; x2) const
+{
     PLASSERT( x1.size()==1 &amp;&amp; x2.size()==1 );
-    return source-&gt;get(int(x1[0]),int(x2[0]));
+    return evaluate( x1[0], x2[0] );
 }
 
-void VMatKernel::computeGramMatrix(Mat K) const
+real VMatKernel::evaluate(real x1, real x2) const
 {
+    PLASSERT( fabs(x1-(real)((int)x1)) &lt; 0.1 );
+    PLASSERT( fabs(x2-(real)((int)x2)) &lt; 0.1 );
+    return evaluate( int(x1), int(x2) );
+}
+
+real VMatKernel::evaluate(int x1, int x2) const
+{
     PLASSERT( source );
-    K &lt;&lt; source-&gt;toMat();
+    PLASSERT( x1 &gt;= 0 );
+    PLASSERT( x1 &lt; source-&gt;length() );
+    PLASSERT( x2 &gt;= 0 );
+    PLASSERT( x2 &lt; source-&gt;width() );
+    return source-&gt;get( x1, x2);
 }
 
-
 //////////////////
 // evaluate_i_j //
 //////////////////
-real VMatKernel::evaluate_i_j(int i, int j) const {
+real VMatKernel::evaluate_i_j(int i, int j) const
+{
     PLASSERT( source );
-    return source-&gt;get(i,j);
+    if( train_indices.length() == 0 )
+        return evaluate( i, j );
+    PLASSERT( i &gt;= 0 );
+    PLASSERT( i &lt; n_examples );
+    PLASSERT( j &gt;= 0 );
+    PLASSERT( j &lt; n_examples );
+    return evaluate( train_indices[i], train_indices[j] );
 }
 
+//////////////////
+// evaluate_i_x //
+//////////////////
+real VMatKernel::evaluate_i_x(int i, const Vec&amp; x, real squared_norm_of_x) const
+{
+    if( train_indices.length() == 0 )
+        return evaluate( i, (int)x[0] );
+    PLASSERT( i &gt;= 0 );
+    PLASSERT( i &lt; n_examples );
+    PLASSERT( x.size() == 1 );
+    return evaluate( train_indices[i], x[0] );
+}
+
+//////////////////
+// evaluate_x_i //
+//////////////////
+real VMatKernel::evaluate_x_i(const Vec&amp; x, int i, real squared_norm_of_x) const
+{
+//    if( is_symmetric )
+//        return evaluate_i_x( i, x, squared_norm_of_x);
+    if( train_indices.length() == 0 )
+        return evaluate( (int)x[0], i);
+    PLASSERT( i &gt;= 0 );
+    PLASSERT( i &lt; n_examples );
+    PLASSERT( x.size() == 1 );
+    return evaluate( x[0], train_indices[i]);
+}
+
+///////////////////////
+// computeGramMatrix //
+///////////////////////
+void VMatKernel::computeGramMatrix(Mat K) const
+{
+    PLASSERT( source );
+    if( train_indices.length() &gt; 0 )
+    {
+        K.resize(n_examples, n_examples);
+        if( is_symmetric )
+            for(int i = 0; i &lt; n_examples; i++ )
+	    {
+	        K(i,i) = evaluate( train_indices[i], train_indices[i] );
+                for(int j = 0; j &lt; i; j++ )
+                {
+	            K(i,j) = evaluate( train_indices[i], train_indices[j] );
+		    K(j,i) = K(i,j);
+	        }
+            }
+        else
+            for(int i = 0; i &lt; n_examples; i++ )
+                for(int j = 0; j &lt; n_examples; j++ )
+	            K(i,j) = evaluate( train_indices[i], train_indices[j] );
+    }
+    else
+        K &lt;&lt; source-&gt;toMat();
+}
+
+
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////
@@ -142,27 +206,60 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    // ### Call deepCopyField on all &quot;pointer-like&quot; fields 
-    // ### that you wish to be deepCopied rather than 
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR(&quot;VMatKernel::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+    deepCopyField(source, copies);
+    deepCopyField(train_indices, copies);
 }
 
-/* ### This method will be overridden if computations need to be done,
-   ### or to forward the call to another object.
-   ### In this case, be careful that it may be called BEFORE the build_()
-   ### method has been called, if the 'specify_dataset' option is used.
 ////////////////////////////
 // setDataForKernelMatrix //
 ////////////////////////////
-void VMatKernel::setDataForKernelMatrix(VMat the_data) {
+void VMatKernel::setDataForKernelMatrix(VMat the_data)
+{
+    inherited::setDataForKernelMatrix(the_data);
+
+    if( n_examples &gt; 1 )
+    {
+        PLASSERT( data_inputsize == 1 );
+        train_indices.resize(n_examples);
+        for(int i = 0; i &lt; n_examples; i++)
+        {
+            PLASSERT( the_data-&gt;get(i,0) &gt;= 0 );
+            PLASSERT( !(source) || ( the_data-&gt;get(i,0) &lt; (real)source-&gt;width() ) );
+            train_indices[i] = the_data-&gt;get(i,0);
+        }
+    }
+    else
+    {
+	PLASSERT( source );
+        PLWARNING(&quot;in VMatKernel::setDataForKernelMatrix: all values in the VMatKernel source are taken into acount for training&quot;);
+	n_examples = source-&gt;width();
+	train_indices.resize(0);
+    }
 }
-*/
 
+////////////////////////////
+// addDataForKernelMatrix //
+////////////////////////////
+void VMatKernel::addDataForKernelMatrix(const Vec&amp; newRow)
+{
+    PLASSERT( newRow.size() == 1 );
+    inherited::addDataForKernelMatrix( newRow );
+    if( train_indices.length() == 0 )
+    {
+        PLASSERT( source );
+	n_examples = source-&gt;width();
+        train_indices.resize( n_examples );
+        for(int i = 0; i &lt; n_examples; i++)
+	    train_indices[i] = (real)i;
+    }
+    PLASSERT( newRow[0] &gt; 0 );
+    PLASSERT( !(source) || ( newRow[0] &lt; source-&gt;width() ) );
+    train_indices.resize( n_examples + 1 );
+    train_indices[ n_examples ] = newRow[0];
+    n_examples += 1;
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/ker/VMatKernel.h
===================================================================
--- trunk/plearn/ker/VMatKernel.h	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/VMatKernel.h	2007-10-02 20:59:36 UTC (rev 8130)
@@ -44,10 +44,11 @@
 #ifndef VMatKernel_INC
 #define VMatKernel_INC
 
-#include &lt;plearn/ker/Kernel.h&gt;
+#include &quot;Kernel.h&quot;
 #include &lt;plearn/vmat/VMat.h&gt;
 
 namespace PLearn {
+using namespace std;
 
 class VMatKernel: public Kernel
 {
@@ -62,8 +63,11 @@
     // * Protected options *
     // *********************
 
-    // ### declare protected option fields (such as learnt parameters) here
-    // ...
+    // *************************
+    // * Public learnt options *
+    // *************************
+
+    Vec train_indices;
     
 public:
 
@@ -73,6 +77,7 @@
 
     VMat source;
 
+
     // ### declare public option fields (such as build options) here
     // ...
 
@@ -124,25 +129,20 @@
 
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec&amp; x1, const Vec&amp; x2) const;
+    virtual real evaluate(real x1, real x2) const;
+    virtual real evaluate(int x1, int x2) const;
 
-    //! Overridden for efficiency.
-    virtual real evaluate_i_j(int i, int j) const;
-    virtual void computeGramMatrix(Mat K) const;
-
-    // *** SUBCLASS WRITING: ***
-    // While in general not necessary, in case of particular needs 
-    // (efficiency concerns for ex) you may also want to overload
-    // some of the following methods:
-    // virtual real evaluate_i_x(int i, const Vec&amp; x, real squared_norm_of_x=-1) const;
-    // virtual real evaluate_x_i(const Vec&amp; x, int i, real squared_norm_of_x=-1) const;
-    // virtual real evaluate_i_x_again(int i, const Vec&amp; x, real squared_norm_of_x=-1, bool first_time = false) const;
-    // virtual real evaluate_x_i_again(const Vec&amp; x, int i, real squared_norm_of_x=-1, bool first_time = false) const;
-    // virtual void setDataForKernelMatrix(VMat the_data);
-    // virtual void addDataForKernelMatrix(const Vec&amp; newRow);
+    //! Overridden methods
+    virtual void setDataForKernelMatrix(VMat the_data);
+    virtual void addDataForKernelMatrix(const Vec&amp; newRow);
+    virtual real evaluate_i_j(int i, int j) const; //! Compute K(xi,xj) on training samples
+    virtual real evaluate_i_x(int i, const Vec&amp; x, real squared_norm_of_x=-1) const;
+    virtual real evaluate_x_i(const Vec&amp; x, int i, real squared_norm_of_x=-1) const;
+    virtual void computeGramMatrix(Mat K) const;   //! Overridden for more efficiency
+    
+    //! Maybe later someone wants to implement a special behaviours with these functions
     // virtual void setParameters(Vec paramvec);
     // virtual Vec getParameters() const;
-  
-
 };
 
 // Declares a few other classes and functions related to this class.

Modified: trunk/plearn_learners/unsupervised/Isomap.cc
===================================================================
--- trunk/plearn_learners/unsupervised/Isomap.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn_learners/unsupervised/Isomap.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -52,11 +52,13 @@
 ////////////
 Isomap::Isomap() 
     : geodesic_file(&quot;&quot;),
+      geodesic_method(&quot;djikstra&quot;),
       knn(10)
 {
     kernel_is_distance = true;
     // Default distance kernel is the classical Euclidean distance.
     distance_kernel = new DistanceKernel(2);
+    min_eigenvalue = 0;
 }
 
 PLEARN_IMPLEMENT_OBJECT(Isomap,
@@ -93,6 +95,10 @@
     declareOption(ol, &quot;geodesic_file&quot;, &amp;Isomap::geodesic_file, OptionBase::buildoption,
                   &quot;If provided, the geodesic distances will be saved in this file in binary format.&quot;);
 
+    declareOption(ol, &quot;geodesic_method&quot;, &amp;Isomap::geodesic_method, OptionBase::buildoption,
+                  &quot;'floyd' or 'djikstra': the method to compute the geodesic distances.&quot;
+		  &quot;(cf. GeodesicDistanceKernel)&quot;);
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 
@@ -130,7 +136,7 @@
     if (distance_kernel &amp;&amp;
         (!kpca_kernel ||
          (dynamic_cast&lt;GeodesicDistanceKernel*&gt;((Kernel*) kpca_kernel))-&gt;distance_kernel != distance_kernel)) {
-        this-&gt;kpca_kernel = new GeodesicDistanceKernel(distance_kernel, knn, geodesic_file, true);
+        this-&gt;kpca_kernel = new GeodesicDistanceKernel(distance_kernel, knn, geodesic_file, true, geodesic_method);
         // We have modified the KPCA kernel, we must rebuild the KPCA.
         inherited::build();
     }

Modified: trunk/plearn_learners/unsupervised/Isomap.h
===================================================================
--- trunk/plearn_learners/unsupervised/Isomap.h	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn_learners/unsupervised/Isomap.h	2007-10-02 20:59:36 UTC (rev 8130)
@@ -74,6 +74,7 @@
 
     Ker distance_kernel;
     string geodesic_file;
+    string geodesic_method;
     int knn;
 
     // ****************

Modified: trunk/plearn_learners/unsupervised/KernelProjection.cc
===================================================================
--- trunk/plearn_learners/unsupervised/KernelProjection.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn_learners/unsupervised/KernelProjection.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -201,6 +201,7 @@
 ///////////////////
 void KernelProjection::computeOutput(const Vec&amp; input, Vec&amp; output) const
 {
+    PLASSERT( outputsize() &gt; 0 );
     static real* result_ptr;
     if (first_output) {
         // Initialize k_x_xi, used_eigenvectors and result correctly.
@@ -243,7 +244,9 @@
 void KernelProjection::forget()
 {
     stage = 0;
+    cout &lt;&lt; &quot;forget: n_comp_kept = &quot; &lt;&lt; n_comp_kept &lt;&lt; endl;
     n_comp_kept = n_comp;
+    cout &lt;&lt; &quot;forget: n_comp_kept = &quot; &lt;&lt; n_comp_kept &lt;&lt; endl;
     n_examples = 0;
     first_output = true;
     last_input.resize(0);
@@ -338,7 +341,6 @@
     time_for_gram = clock() - time_for_gram;
     if (verbosity &gt;= 3) {
         cout &lt;&lt; flush;
-        cout &lt;&lt; &quot;Time to compute the Gram matrix: &quot; &lt;&lt; real(time_for_gram) / real(CLOCKS_PER_SEC) &lt;&lt; endl;
     }
     // (2) Compute its eigenvectors and eigenvalues.
     eigenVecOfSymmMat(gram, n_comp + ignore_n_first, eigenvalues, eigenvectors);
@@ -346,12 +348,14 @@
         eigenvalues = eigenvalues.subVec(ignore_n_first, eigenvalues.length() - ignore_n_first);
         eigenvectors = eigenvectors.subMatRows(ignore_n_first, eigenvectors.length() - ignore_n_first);
     }
+    
     n_comp_kept = eigenvalues.length(); // Could be different of n_comp.
     // (3) Discard low eigenvalues.
     int p = 0;
     while (p &lt; n_comp_kept &amp;&amp; eigenvalues[p] &gt; min_eigenvalue)
         p++;
     n_comp_kept = p;
+
     // (4) Optionally remove the discarded components.
     if (free_extra_components) {
         eigenvalues.resize(n_comp_kept);


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001577.html">[Plearn-commits] r8129 - trunk/plearn/math
</A></li>
	<LI>Next message: <A HREF="001579.html">[Plearn-commits] r8131 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1578">[ date ]</a>
              <a href="thread.html#1578">[ thread ]</a>
              <a href="subject.html#1578">[ subject ]</a>
              <a href="author.html#1578">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
