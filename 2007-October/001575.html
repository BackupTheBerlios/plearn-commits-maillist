<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8127 - trunk/plearn_learners/generic/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-October/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8127%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200710011901.l91J1pI6010346%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001574.html">
   <LINK REL="Next"  HREF="001576.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8127 - trunk/plearn_learners/generic/EXPERIMENTAL</H1>
    <B>manzagop at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8127%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200710011901.l91J1pI6010346%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8127 - trunk/plearn_learners/generic/EXPERIMENTAL">manzagop at mail.berlios.de
       </A><BR>
    <I>Mon Oct  1 21:01:51 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001574.html">[Plearn-commits] r8126 - trunk
</A></li>
        <LI>Next message: <A HREF="001576.html">[Plearn-commits] r8128 - trunk/plearn/vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1575">[ date ]</a>
              <a href="thread.html#1575">[ thread ]</a>
              <a href="subject.html#1575">[ subject ]</a>
              <a href="author.html#1575">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: manzagop
Date: 2007-10-01 21:01:51 +0200 (Mon, 01 Oct 2007)
New Revision: 8127

Added:
   trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h
Log:
mNNet stands for (minimal?,minibatch?) NNet. It's a lean concentrate
of the current NatGradNNet implementation of a neural net removing
all bells and whistles. Think about subclassing it for your needs.


Added: trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc	2007-10-01 18:34:35 UTC (rev 8126)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc	2007-10-01 19:01:51 UTC (rev 8127)
@@ -0,0 +1,569 @@
+// -*- C++ -*-
+
+// mNNet.cc
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio, PAM
+
+/*! \file mNNet.cc */
+
+#include &quot;mNNet.h&quot;
+//#include &lt;plearn/math/pl_erf.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    mNNet,
+    &quot;Multi-layer neural network based on matrix-matrix multiplications&quot;,
+    &quot;This is a LEAN neural network. No bells, no whistles.\n&quot;
+    );
+
+mNNet::mNNet()
+    : noutputs(-1),
+      init_lrate(0.0),
+      lrate_decay(0.0),
+      minibatch_size(1),
+      output_type(&quot;NLL&quot;),
+      output_layer_L1_penalty_factor(0.0),
+      n_layers(-1),
+      cumulative_training_time(0.0)
+{
+    random_gen = new PRandom();
+}
+
+void mNNet::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;noutputs&quot;, &amp;mNNet::noutputs,
+                  OptionBase::buildoption,
+                  &quot;Number of outputs of the neural network, which can be derived from output_type and targetsize_\n&quot;);
+
+    declareOption(ol, &quot;hidden_layer_sizes&quot;, &amp;mNNet::hidden_layer_sizes,
+                  OptionBase::buildoption,
+                  &quot;Defines the architecture of the multi-layer neural network by\n&quot;
+                  &quot;specifying the number of hidden units in each hidden layer.\n&quot;);
+
+    declareOption(ol, &quot;init_lrate&quot;, &amp;mNNet::init_lrate,
+                  OptionBase::buildoption,
+                  &quot;Initial learning rate\n&quot;);
+
+    declareOption(ol, &quot;lrate_decay&quot;, &amp;mNNet::lrate_decay,
+                  OptionBase::buildoption,
+                  &quot;Learning rate decay factor\n&quot;);
+
+    // TODO Why this dependance on test_minibatch_size?
+    declareOption(ol, &quot;minibatch_size&quot;, &amp;mNNet::minibatch_size,
+                  OptionBase::buildoption,
+                  &quot;Update the parameters only so often (number of examples).\n&quot;
+                  &quot;Must be greater or equal to test_minibatch_size\n&quot;);
+
+    declareOption(ol, &quot;output_type&quot;, 
+                  &amp;mNNet::output_type,
+                  OptionBase::buildoption,
+                  &quot;type of output cost: 'cross_entropy' for binary classification,\n&quot;
+                  &quot;'NLL' for classification problems, or 'MSE' for regression.\n&quot;);
+
+    declareOption(ol, &quot;output_layer_L1_penalty_factor&quot;,
+                  &amp;mNNet::output_layer_L1_penalty_factor,
+                  OptionBase::buildoption,
+                  &quot;Optional (default=0) factor of L1 regularization term, i.e.\n&quot;
+                  &quot;minimize L1_penalty_factor * sum_{ij} |weights(i,j)| during training.\n&quot;
+                  &quot;Gets multiplied by the learning rate. Only on output layer!!&quot;);
+
+    declareOption(ol, &quot;n_layers&quot;, &amp;mNNet::n_layers,
+                  OptionBase::learntoption,
+                  &quot;Number of layers of weights plus 1 (ie. 3 for a neural net with one hidden layer).\n&quot;
+                  &quot;Needs not be specified explicitly (derived from hidden_layer_sizes).\n&quot;);
+
+    declareOption(ol, &quot;layer_sizes&quot;, &amp;mNNet::layer_sizes,
+                  OptionBase::learntoption,
+                  &quot;Derived from hidden_layer_sizes, inputsize_ and noutputs\n&quot;);
+
+    declareOption(ol, &quot;layer_params&quot;, &amp;mNNet::layer_params,
+                  OptionBase::learntoption,
+                  &quot;Parameters used while training, for each layer, organized as follows: layer_params[i] \n&quot;
+                  &quot;is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)\n&quot;
+                  &quot;containing the neuron biases in its first column.\n&quot;);
+
+    declareOption(ol, &quot;cumulative_training_time&quot;, &amp;mNNet::cumulative_training_time,
+                  OptionBase::learntoption,
+                  &quot;Cumulative training time since age=0, in seconds.\n&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+// TODO - reloading an object will not work! layer_params will juste get lost.
+void mNNet::build_()
+{
+    // *** Sanity checks ***
+
+    if (!train_set)
+        return;
+    if (output_type==&quot;MSE&quot;)
+    {
+        if (noutputs&lt;0) noutputs = targetsize_;
+        else PLASSERT_MSG(noutputs==targetsize_,&quot;mNNet: noutputs should be -1 or match data's targetsize&quot;);
+    }
+    else if (output_type==&quot;NLL&quot;)
+    {
+        // TODO add a check on noutput's value
+        if (noutputs&lt;0)
+            PLERROR(&quot;mNNet: if output_type=NLL (classification), one \n&quot;
+                    &quot;should provide noutputs = number of classes, or possibly\n&quot;
+                    &quot;1 when 2 classes\n&quot;);
+    }
+    else if (output_type==&quot;cross_entropy&quot;)
+    {
+        if(noutputs!=1)
+            PLERROR(&quot;mNNet: if output_type=cross_entropy, then \n&quot;
+                    &quot;noutputs should be 1.\n&quot;);
+    }
+    else PLERROR(&quot;mNNet: output_type should be cross_entropy, NLL or MSE\n&quot;);
+
+    if( output_layer_L1_penalty_factor &lt; 0. )
+        PLWARNING(&quot;mNNet::build_ - output_layer_L1_penalty_factor is negative!\n&quot;);
+
+    // *** Determine topology ***
+    inputsize_ = train_set-&gt;inputsize();
+    while (hidden_layer_sizes.length()&gt;0 &amp;&amp; hidden_layer_sizes[hidden_layer_sizes.length()-1]==0)
+        hidden_layer_sizes.resize(hidden_layer_sizes.length()-1);
+    n_layers = hidden_layer_sizes.length()+2; 
+    layer_sizes.resize(n_layers);
+    layer_sizes.subVec(1,n_layers-2) &lt;&lt; hidden_layer_sizes;
+    layer_sizes[0]=inputsize_;
+    layer_sizes[n_layers-1]=noutputs;
+
+    // *** Allocate memory for params and gradients ***
+    int n_params=0;
+    int n_neurons=0;
+    for (int i=0;i&lt;n_layers-1;i++)    {
+        n_neurons+=layer_sizes[i+1];
+        n_params+=layer_sizes[i+1]*(1+layer_sizes[i]);
+    }
+    all_params.resize(n_params);
+    all_params_gradient.resize(n_params);
+
+    // *** Set handles ***
+    layer_params.resize(n_layers-1);
+    layer_params_gradient.resize(n_layers-1);
+    biases.resize(n_layers-1);
+    weights.resize(n_layers-1);
+
+    for (int i=0,p=0;i&lt;n_layers-1;i++)    {
+        int np=layer_sizes[i+1]*(1+layer_sizes[i]);
+        layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        biases[i]=layer_params[i].subMatColumns(0,1);
+        weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+        layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        p+=np;
+    }
+
+    // *** Allocate memory for outputs and gradients on neurons ***
+    neuron_extended_outputs.resize(minibatch_size,layer_sizes[0]+1+n_neurons+n_layers);
+    neuron_gradients.resize(minibatch_size,n_neurons);
+
+    // *** Set handles and biases ***
+    neuron_outputs_per_layer.resize(n_layers); // layer 0 = input, layer n_layers-1 = output
+    neuron_extended_outputs_per_layer.resize(n_layers); // layer 0 = input, layer n_layers-1 = output
+    neuron_gradients_per_layer.resize(n_layers); // layer 0 not used
+
+    int k=0, kk=0;
+    for (int i=0;i&lt;n_layers;i++)
+    {
+        neuron_extended_outputs_per_layer[i] = neuron_extended_outputs.subMatColumns(k,1+layer_sizes[i]);
+        neuron_extended_outputs_per_layer[i].column(0).fill(1.0); // for biases
+        neuron_outputs_per_layer[i]=neuron_extended_outputs_per_layer[i].subMatColumns(1,layer_sizes[i]);
+        k+=1+layer_sizes[i];
+        if(i&gt;0) {
+            neuron_gradients_per_layer[i] = neuron_gradients.subMatColumns(kk,layer_sizes[i]);
+            kk+=layer_sizes[i];
+        }
+    }
+
+}
+
+// ### Nothing to add here, simply calls build_
+void mNNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void mNNet::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(hidden_layer_sizes, copies);
+    deepCopyField(layer_sizes, copies);
+    deepCopyField(all_params, copies);
+    deepCopyField(biases, copies);
+    deepCopyField(weights, copies);
+    deepCopyField(layer_params, copies);
+    deepCopyField(all_params_gradient, copies);
+    deepCopyField(layer_params_gradient, copies);
+    deepCopyField(neuron_gradients, copies);
+    deepCopyField(neuron_gradients_per_layer, copies);
+    deepCopyField(neuron_extended_outputs, copies);
+    deepCopyField(neuron_extended_outputs_per_layer, copies);
+    deepCopyField(neuron_outputs_per_layer, copies);
+    deepCopyField(targets, copies);
+    deepCopyField(example_weights, copies);
+    deepCopyField(train_costs, copies);
+}
+
+
+int mNNet::outputsize() const
+{
+    return noutputs;
+}
+
+void mNNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    inherited::forget();
+    for (int i=0;i&lt;n_layers-1;i++)
+    {
+        real delta = 1/sqrt(real(layer_sizes[i]));
+        random_gen-&gt;fill_random_uniform(weights[i],-delta,delta);
+        biases[i].clear();
+    }
+    stage = 0;
+    cumulative_training_time=0.0;
+}
+
+void mNNet::train()
+{
+
+    if (inputsize_&lt;0)
+        build();
+    if(!train_set)
+        PLERROR(&quot;In NNet::train, you did not setTrainingSet&quot;);
+    if(!train_stats)
+        setTrainStatsCollector(new VecStatsCollector());
+
+    targets.resize(minibatch_size,targetsize());  // the train_set's targetsize()
+    example_weights.resize(minibatch_size);
+
+    TVec&lt;string&gt; train_cost_names = getTrainCostNames() ;
+    train_costs.resize(minibatch_size,train_cost_names.length()-2); 
+    train_costs.fill(MISSING_VALUE) ;
+    Vec costs_plus_time(train_costs.width()+2);
+    costs_plus_time[train_costs.width()] = MISSING_VALUE;
+    costs_plus_time[train_costs.width()+1] = MISSING_VALUE;
+    Vec costs = costs_plus_time.subVec(0,train_costs.width());
+
+    train_stats-&gt;forget();
+
+    int b, sample, nsamples;
+    nsamples = train_set-&gt;length();
+    Vec input,target;   // TODO discard these variables.
+
+    Profiler::reset(&quot;training&quot;);
+    Profiler::start(&quot;training&quot;);
+
+    for( ; stage&lt;nstages; stage++)
+    {
+        sample = stage % nsamples;
+        b = stage % minibatch_size;
+        input = neuron_outputs_per_layer[0](b);
+        target = targets(b);
+        train_set-&gt;getExample(sample, input, target, example_weights[b]);
+        if (b+1==minibatch_size) // TODO do also special end-case || stage+1==nstages)
+        {
+            onlineStep(stage, targets, train_costs, example_weights );
+            for (int i=0;i&lt;minibatch_size;i++)  {
+                costs &lt;&lt; train_costs(b);    // TODO Is the copy necessary? Might be
+                                            // better to waste some memory in
+                                            // train_costs instead
+                train_stats-&gt;update( costs_plus_time );
+            }
+        }
+    }
+
+    Profiler::end(&quot;training&quot;);
+    if (verbosity&gt;0)
+        Profiler::report(cout);
+    // Take care of the timing stats.
+    const Profiler::Stats&amp; stats = Profiler::getStats(&quot;training&quot;);
+    costs.fill(MISSING_VALUE);
+    real ticksPerSec = Profiler::ticksPerSecond();
+    real cpu_time = (stats.user_duration+stats.system_duration)/ticksPerSec;
+    cumulative_training_time += cpu_time;
+    costs_plus_time[train_costs.width()] = cpu_time;
+    costs_plus_time[train_costs.width()+1] = cumulative_training_time;
+    train_stats-&gt;update( costs_plus_time );
+    train_stats-&gt;finalize(); // finalize statistics for this epoch
+}
+
+void mNNet::onlineStep(int t, const Mat&amp; targets,
+                             Mat&amp; train_costs, Vec example_weights)
+{
+    PLASSERT(targets.length()==minibatch_size &amp;&amp; train_costs.length()==minibatch_size &amp;&amp; example_weights.length()==minibatch_size);
+
+    fpropNet(minibatch_size);
+    fbpropLoss(neuron_outputs_per_layer[n_layers-1],targets,example_weights,train_costs);
+
+    // mean gradient over minibatch_size examples has less variance
+    // can afford larger learning rate (divide by sqrt(minibatch) 
+    // instead of minibatch)
+    real lrate = init_lrate/(1 + t*lrate_decay);
+    lrate /= sqrt(real(minibatch_size));
+
+    for (int i=n_layers-1;i&gt;0;i--)
+    {
+        // here neuron_gradients_per_layer[i] contains the gradient on activations (weighted sums)
+        //      (minibatch_size x layer_size[i])
+        Mat previous_neurons_gradient = neuron_gradients_per_layer[i-1];
+        Mat next_neurons_gradient = neuron_gradients_per_layer[i];
+        Mat previous_neurons_output = neuron_outputs_per_layer[i-1];
+
+        if (i&gt;1) // if not first hidden layer then compute gradient on previous layer
+        {
+            // propagate gradients
+            productScaleAcc(previous_neurons_gradient,next_neurons_gradient,false,
+                            weights[i-1],false,1,0);
+            // propagate through tanh non-linearity
+            // TODO IN NEED OF OPTIMIZATION
+            for (int j=0;j&lt;previous_neurons_gradient.length();j++)
+            {
+                real* grad = previous_neurons_gradient[j];
+                real* out = previous_neurons_output[j];
+                for (int k=0;k&lt;previous_neurons_gradient.width();k++,out++)
+                    grad[k] *= (1 - *out * *out); // gradient through tanh derivative
+            }
+        }
+        // compute gradient on parameters and update them in one go (more efficient)
+        productScaleAcc(layer_params[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,
+                            -lrate,1);
+    }
+
+    // Output layer L1 regularization
+    if( output_layer_L1_penalty_factor != 0. )    {
+        real L1_delta = lrate * output_layer_L1_penalty_factor;
+        real* m_i = layer_params[n_layers-2].data();
+        for(int i=0; i&lt;layer_params[n_layers-2].length(); i++,m_i+=layer_params[n_layers-2].mod())  {
+            for(int j=0; j&lt;layer_params[n_layers-2].width(); j++)   {
+                if( m_i[j] &gt; L1_delta )
+                    m_i[j] -= L1_delta;
+                else if( m_i[j] &lt; -L1_delta )
+                    m_i[j] += L1_delta;
+                else
+                    m_i[j] = 0.;
+            }
+        }
+    }
+
+}
+
+void mNNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    neuron_outputs_per_layer[0](0) &lt;&lt; input;
+    fpropNet(1);
+    output &lt;&lt; neuron_outputs_per_layer[n_layers-1](0);
+}
+
+//! compute (pre-final-non-linearity) network top-layer output given input
+void mNNet::fpropNet(int n_examples) const
+{
+    PLASSERT_MSG(n_examples&lt;=minibatch_size,&quot;mNNet::fpropNet: nb input vectors treated should be &lt;= minibatch_size\n&quot;);
+    for (int i=0;i&lt;n_layers-1;i++)
+    {
+        Mat prev_layer = neuron_extended_outputs_per_layer[i];
+        Mat next_layer = neuron_outputs_per_layer[i+1];
+        if (n_examples!=minibatch_size) {
+            prev_layer = prev_layer.subMatRows(0,n_examples);
+            next_layer = next_layer.subMatRows(0,n_examples);
+        }
+
+        // try to use BLAS for the expensive operation
+        productScaleAcc(next_layer, prev_layer, false, layer_params[i], true, 1, 0);
+
+        // compute layer's output non-linearity
+        if (i+1&lt;n_layers-1) {
+            for (int k=0;k&lt;n_examples;k++)  {
+                Vec L=next_layer(k);
+                compute_tanh(L,L);
+            }
+        }   else if (output_type==&quot;NLL&quot;)    {
+            for (int k=0;k&lt;n_examples;k++)  {
+                Vec L=next_layer(k);
+                log_softmax(L,L);
+            }
+        }   else if (output_type==&quot;cross_entropy&quot;)  {
+            for (int k=0;k&lt;n_examples;k++)  {
+                Vec L=next_layer(k);
+                log_sigmoid(L,L);
+            }
+         }
+    }
+}
+
+//! compute train costs given the (pre-final-non-linearity) network top-layer output
+void mNNet::fbpropLoss(const Mat&amp; output, const Mat&amp; target, const Vec&amp; example_weight, Mat&amp; costs) const
+{
+    int n_examples = output.length();
+    Mat out_grad = neuron_gradients_per_layer[n_layers-1];
+    if (n_examples!=minibatch_size)
+        out_grad = out_grad.subMatRows(0,n_examples);
+    int target_class;
+    Vec outp, grad;
+    if (output_type==&quot;NLL&quot;) {
+        for (int i=0;i&lt;n_examples;i++)  {
+            target_class = int(round(target(i,0)));
+            #ifdef BOUNDCHECK
+            if(target_class&gt;=noutputs)
+                PLERROR(&quot;In mNNet::fbpropLoss one target value %d is higher then allowed by nout %d&quot;,
+                        target_class, noutputs);
+            #endif          
+            outp = output(i);
+            grad = out_grad(i);
+            exp(outp,grad); // map log-prob to prob
+            costs(i,0) = -outp[target_class];
+            costs(i,1) = (target_class == argmax(outp))?0:1;
+            grad[target_class]-=1;
+            if (example_weight[i]!=1.0)
+                costs(i,0) *= example_weight[i];
+        }
+    }
+    else if(output_type==&quot;cross_entropy&quot;)   {
+        for (int i=0;i&lt;n_examples;i++)  {
+            target_class = int(round(target(i,0)));
+            outp = output(i);
+            grad = out_grad(i);
+            exp(outp,grad); // map log-prob to prob
+            if( target_class == 1 ) {
+                costs(i,0) = - outp[0];
+                costs(i,1) = (grad[0]&gt;0.5)?0:1;
+            }   else    {
+                costs(i,0) = - pl_log( 1.0 - grad[0] );
+                costs(i,1) = (grad[0]&gt;0.5)?1:0;
+            }
+            grad[0] -= (real)target_class; // ?
+            if (example_weight[i]!=1.0)
+                costs(i,0) *= example_weight[i];
+        }
+    }
+    else // if (output_type==&quot;MSE&quot;)
+    {
+        substract(output,target,out_grad);
+        for (int i=0;i&lt;n_examples;i++)  {
+            costs(i,0) = pownorm(out_grad(i));
+            if (example_weight[i]!=1.0) {
+                out_grad(i) *= example_weight[i];
+                costs(i,0) *= example_weight[i];
+            }
+        }
+    }
+}
+
+void mNNet::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+    Vec w(1);
+    w[0]=1;
+    Mat outputM = output.toMat(1,output.length());
+    Mat targetM = target.toMat(1,output.length());
+    Mat costsM = costs.toMat(1,costs.length());
+    fbpropLoss(outputM,targetM,w,costsM);
+}
+
+void mNNet::computeOutputs(const Mat&amp; input, Mat&amp; output) const
+{
+    PLASSERT(test_minibatch_size&lt;=minibatch_size);
+    neuron_outputs_per_layer[0].subMat(0,0,input.length(),input.width()) &lt;&lt; input;
+    fpropNet(input.length());
+    output &lt;&lt; neuron_outputs_per_layer[n_layers-1].subMat(0,0,output.length(),output.width());
+}
+void mNNet::computeOutputsAndCosts(const Mat&amp; input, const Mat&amp; target, 
+                                      Mat&amp; output, Mat&amp; costs) const
+{//TODO
+    int n=input.length();
+    PLASSERT(target.length()==n);
+    output.resize(n,outputsize());
+    costs.resize(n,nTestCosts());
+    computeOutputs(input,output);
+
+    Vec w(n);
+    w.fill(1);
+    fbpropLoss(output,target,w,costs);
+}
+
+TVec&lt;string&gt; mNNet::getTestCostNames() const
+{
+    TVec&lt;string&gt; costs;
+    if (output_type==&quot;NLL&quot;)
+    {
+        costs.resize(3);
+        costs[0]=&quot;NLL&quot;;
+        costs[1]=&quot;class_error&quot;;
+    }
+    else if (output_type==&quot;cross_entropy&quot;)  {
+        costs.resize(3);
+        costs[0]=&quot;cross_entropy&quot;;
+        costs[1]=&quot;class_error&quot;;
+    }
+    else if (output_type==&quot;MSE&quot;)
+    {
+        costs.resize(1);
+        costs[0]=&quot;MSE&quot;;
+    }
+    return costs;
+}
+
+TVec&lt;string&gt; mNNet::getTrainCostNames() const
+{
+    TVec&lt;string&gt; costs = getTestCostNames();
+    costs.append(&quot;train_seconds&quot;);
+    costs.append(&quot;cum_train_seconds&quot;);
+    return costs;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h	2007-10-01 18:34:35 UTC (rev 8126)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h	2007-10-01 19:01:51 UTC (rev 8127)
@@ -0,0 +1,232 @@
+// -*- C++ -*-
+// mNNet.h
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio, PAM
+
+/*! \file mNNet.h */
+
+
+#ifndef mNNet_INC
+#define mNNet_INC
+
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+
+namespace PLearn {
+
+/**
+ * Multi-layer neural network based on matrix-matrix multiplications.
+ */
+class mNNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! number of outputs to the network
+    int noutputs;
+
+    //! sizes of hidden layers
+    TVec&lt;int&gt; hidden_layer_sizes;
+
+    //! initial learning rate
+    real init_lrate;
+
+    //! learning rate decay factor
+    real lrate_decay;
+
+    //! update the parameters only so often
+    int minibatch_size;
+
+    //! type of output cost: &quot;NLL&quot; for classification problems, &quot;MSE&quot; for regression
+    string output_type;
+
+    //! L1 penalty applied to the output layer's parameters
+    real output_layer_L1_penalty_factor;
+
+public:
+    //#####  Public Not Build Options  ########################################
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    mNNet();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+    virtual void computeOutputs(const Mat&amp; input, Mat&amp; output) const;
+    virtual void computeOutputsAndCosts(const Mat&amp; input, const Mat&amp; target, 
+                                        Mat&amp; output, Mat&amp; costs) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods: computeOutputAndCosts(),
+    // computeCostsOnly(), test(), nTestCosts(), nTrainCosts(),
+    // resetInternalState(), isStatefulLearner()
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(mNNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+
+    //! number of layers of weights (2 for a neural net with one hidden layer)
+    int n_layers;
+
+    //! layer sizes (derived from hidden_layer_sizes, inputsize_ and outputsize_)
+    TVec&lt;int&gt; layer_sizes;
+
+    //! All the parameters in one vector
+    Vec all_params;
+    //! Alternate access to the params - one matrix per layer
+    TVec&lt;Mat&gt; biases;
+    TVec&lt;Mat&gt; weights;
+    //! Alternate access to the params - one matrix of dimension
+    //! layer_sizes[i+1] x (layer_sizes[i]+1) per layer
+    //! (neuron biases in the first column)
+    TVec&lt;Mat&gt; layer_params;
+
+    //! Gradient structures - reflect the parameter structures 
+    Vec all_params_gradient; 
+    TVec&lt;Mat&gt; layer_params_gradient;
+
+    //! Outputs of the neurons
+    Mat neuron_gradients;   // one row per example of a minibatch, has
+                            // concatenation of layer 0, layer 1, ... gradients.
+    TVec&lt;Mat&gt; neuron_gradients_per_layer;   // pointing into neuron_gradients
+                                            // (one row per example of a minibatch)
+
+    //! Gradients on the neurons - same structure as for outputs
+    mutable Mat neuron_extended_outputs;
+    mutable TVec&lt;Mat&gt; neuron_extended_outputs_per_layer;    // with 1's in the
+                                                            // first pseudo-neuron, for doing biases
+    mutable TVec&lt;Mat&gt; neuron_outputs_per_layer;  
+
+    Mat targets; // one target row per example in a minibatch
+    Vec example_weights; // one element per example in a minibatch
+    Mat train_costs; // one row per example in a minibatch
+
+    //! Holds training time, an additional cost
+    real cumulative_training_time;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+    //! One minibatch training step
+    virtual void onlineStep(int t, const Mat&amp; targets, Mat&amp; train_costs, Vec example_weights);
+
+    //! compute a minibatch of size n_examples network top-layer output given layer 0 output (= network input)
+    //! (note that log-probabilities are computed for classification tasks, output_type=NLL)
+    virtual void fpropNet(int n_examples) const;
+
+    //! compute train costs given the network top-layer output
+    //! and write into neuron_gradients_per_layer[n_layers-2], gradient on pre-non-linearity activation
+    virtual void fbpropLoss(const Mat&amp; output, const Mat&amp; target, const Vec&amp; example_weights, Mat&amp; train_costs) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(mNNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001574.html">[Plearn-commits] r8126 - trunk
</A></li>
	<LI>Next message: <A HREF="001576.html">[Plearn-commits] r8128 - trunk/plearn/vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1575">[ date ]</a>
              <a href="thread.html#1575">[ thread ]</a>
              <a href="subject.html#1575">[ subject ]</a>
              <a href="author.html#1575">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
