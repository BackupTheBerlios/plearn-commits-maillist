<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8156 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-October/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8156%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200710091313.l99DD9gq031347%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001603.html">
   <LINK REL="Next"  HREF="001605.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8156 - trunk/plearn_learners_experimental</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8156%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200710091313.l99DD9gq031347%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8156 - trunk/plearn_learners_experimental">larocheh at mail.berlios.de
       </A><BR>
    <I>Tue Oct  9 15:13:09 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001603.html">[Plearn-commits] r8155 - trunk/python_modules/plearn/learners
</A></li>
        <LI>Next message: <A HREF="001605.html">[Plearn-commits] r8157 - in trunk/plearn: base vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1604">[ date ]</a>
              <a href="thread.html#1604">[ thread ]</a>
              <a href="subject.html#1604">[ subject ]</a>
              <a href="author.html#1604">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2007-10-09 15:13:07 +0200 (Tue, 09 Oct 2007)
New Revision: 8156

Added:
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h
Log:
Badly named deep network, which focuses on features that are related to the target...


Added: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-06 20:07:56 UTC (rev 8155)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-09 13:13:07 UTC (rev 8156)
@@ -0,0 +1,1155 @@
+// -*- C++ -*-
+
+// StackedFocusedAutoassociatorsNet.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedFocusedAutoassociatorsNet.cc */
+
+
+#define PL_LOG_MODULE_NAME &quot;StackedFocusedAutoassociatorsNet&quot;
+#include &lt;plearn/io/pl_log.h&gt;
+
+#include &quot;StackedFocusedAutoassociatorsNet.h&quot;
+#include &lt;plearn/vmat/VMat_computeNearestNeighbors.h&gt;
+#include &lt;plearn_learners/online/RBMMixedLayer.h&gt;
+#include &lt;plearn_learners/online/RBMMixedConnection.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    StackedFocusedAutoassociatorsNet,
+    &quot;Neural net, trained layer-wise in a greedy but focused fashion using autoassociators/RBMs and a supervised non-parametric gradient.&quot;,
+    &quot;It is highly inspired by the StackedFocusedAutoassociators class,\n&quot;
+    &quot;and can use use the same RBMLayer and RBMConnection components.\n&quot;
+    );
+
+StackedFocusedAutoassociatorsNet::StackedFocusedAutoassociatorsNet() :
+    cd_learning_rate( 0. ),
+    cd_decrease_ct( 0. ),
+    greedy_learning_rate( 0. ),
+    greedy_decrease_ct( 0. ),
+    supervised_greedy_learning_rate( 0. ),
+    supervised_greedy_decrease_ct( 0. ),
+    fine_tuning_learning_rate( 0. ),
+    fine_tuning_decrease_ct( 0. ),
+    k_neighbors( 1 ),
+    n_classes( -1 ),
+    n_layers( 0 ),
+    train_set_representations_up_to_date(false),
+    currently_trained_layer( 0 )
+{
+    // random_gen will be initialized in PLearner::build_()
+    random_gen = new PRandom();
+    nstages = 0;
+}
+
+void StackedFocusedAutoassociatorsNet::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;cd_learning_rate&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::cd_learning_rate,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used during the RBM &quot;
+                  &quot;contrastive divergence training&quot;);
+
+    declareOption(ol, &quot;cd_decrease_ct&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::cd_decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;The decrease constant of the learning rate used during &quot;
+                  &quot;the RBMs contrastive\n&quot;
+                  &quot;divergence training. When a hidden layer has finished &quot;
+                  &quot;its training,\n&quot;
+                  &quot;the learning rate is reset to it's initial value.\n&quot;);
+
+    declareOption(ol, &quot;greedy_learning_rate&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::greedy_learning_rate,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used during the autoassociator &quot;
+                  &quot;gradient descent training&quot;);
+
+    declareOption(ol, &quot;greedy_decrease_ct&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::greedy_decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;The decrease constant of the learning rate used during &quot;
+                  &quot;the autoassociator\n&quot;
+                  &quot;gradient descent training. When a hidden layer has finished &quot;
+                  &quot;its training,\n&quot;
+                  &quot;the learning rate is reset to it's initial value.\n&quot;);
+
+    declareOption(ol, &quot;supervised_greedy_learning_rate&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::supervised_greedy_learning_rate,
+                  OptionBase::buildoption,
+                  &quot;Supervised, non-parametric, greedy learning rate&quot;);
+
+    declareOption(ol, &quot;supervised_greedy_decrease_ct&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::supervised_greedy_decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;Supervised, non-parametric, greedy decrease constant&quot;);
+
+    declareOption(ol, &quot;fine_tuning_learning_rate&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::fine_tuning_learning_rate,
+                  OptionBase::buildoption,
+                  &quot;The learning rate used during the fine tuning gradient descent&quot;);
+
+    declareOption(ol, &quot;fine_tuning_decrease_ct&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::fine_tuning_decrease_ct,
+                  OptionBase::buildoption,
+                  &quot;The decrease constant of the learning rate used during &quot;
+                  &quot;fine tuning\n&quot;
+                  &quot;gradient descent.\n&quot;);
+
+    declareOption(ol, &quot;training_schedule&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::training_schedule,
+                  OptionBase::buildoption,
+                  &quot;Number of examples to use during each phase of greedy pre-training.\n&quot;
+                  &quot;The number of fine-tunig steps is defined by nstages.\n&quot;
+        );
+
+    declareOption(ol, &quot;layers&quot;, &amp;StackedFocusedAutoassociatorsNet::layers,
+                  OptionBase::buildoption,
+                  &quot;The layers of units in the network. The first element\n&quot;
+                  &quot;of this vector should be the input layer and the\n&quot;
+                  &quot;subsequent elements should be the hidden layers. The\n&quot;
+                  &quot;output layer should not be included in layers.\n&quot;);
+
+    declareOption(ol, &quot;connections&quot;, &amp;StackedFocusedAutoassociatorsNet::connections,
+                  OptionBase::buildoption,
+                  &quot;The weights of the connections between the layers&quot;);
+
+    declareOption(ol, &quot;reconstruction_connections&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::reconstruction_connections,
+                  OptionBase::buildoption,
+                  &quot;The reconstruction weights of the autoassociators&quot;);
+
+    declareOption(ol, &quot;unsupervised_layers&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::unsupervised_layers,
+                  OptionBase::buildoption,
+                  &quot;Additional units for greedy unsupervised learning&quot;);
+
+    declareOption(ol, &quot;unsupervised_connections&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::unsupervised_connections,
+                  OptionBase::buildoption,
+                  &quot;Additional connections for greedy unsupervised learning&quot;);
+
+    declareOption(ol, &quot;k_neighbors&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::k_neighbors,
+                  OptionBase::buildoption,
+                  &quot;Number of good nearest neighbors to attract and bad nearest &quot;
+                  &quot;neighbors to repel.&quot;);
+
+    declareOption(ol, &quot;n_classes&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::n_classes,
+                  OptionBase::buildoption,
+                  &quot;Number of classes.&quot;);
+
+    declareOption(ol, &quot;greedy_stages&quot;, 
+                  &amp;StackedFocusedAutoassociatorsNet::greedy_stages,
+                  OptionBase::learntoption,
+                  &quot;Number of training samples seen in the different greedy &quot;
+                  &quot;phases.\n&quot;
+        );
+
+    declareOption(ol, &quot;n_layers&quot;, &amp;StackedFocusedAutoassociatorsNet::n_layers,
+                  OptionBase::learntoption,
+                  &quot;Number of layers&quot;
+        );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void StackedFocusedAutoassociatorsNet::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    MODULE_LOG &lt;&lt; &quot;build_() called&quot; &lt;&lt; endl;
+
+    if(inputsize_ &gt; 0 &amp;&amp; targetsize_ &gt; 0)
+    {
+        // Initialize some learnt variables
+        n_layers = layers.length();
+        
+        train_set_representations_up_to_date = false;
+
+        if( n_classes &lt;= 0 )
+            PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_() - \n&quot;
+                    &quot;n_classes should be &gt; 0.\n&quot;);
+        test_votes.resize(n_classes);
+
+        if( k_neighbors &lt;= 0 )
+            PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_() - \n&quot;
+                    &quot;k_neighbors should be &gt; 0.\n&quot;);
+        test_nearest_neighbors_indices.resize(k_neighbors);
+
+        if( weightsize_ &gt; 0 )
+            PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_() - \n&quot;
+                    &quot;usage of weighted samples (weight size &gt; 0) is not\n&quot;
+                    &quot;implemented yet.\n&quot;);
+
+        if( training_schedule.length() != n_layers-1 )        
+            PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_() - \n&quot;
+                    &quot;training_schedule should have %d elements.\n&quot;,
+                    n_layers-1);
+        
+        if(greedy_stages.length() == 0)
+        {
+            greedy_stages.resize(n_layers-1);
+            greedy_stages.clear();
+        }
+
+        if(stage &gt; 0)
+            currently_trained_layer = n_layers;
+        else
+        {            
+            currently_trained_layer = n_layers-1;
+            while(currently_trained_layer&gt;1
+                  &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
+                currently_trained_layer--;
+        }
+
+        build_layers_and_connections();
+    }
+}
+
+void StackedFocusedAutoassociatorsNet::build_layers_and_connections()
+{
+    MODULE_LOG &lt;&lt; &quot;build_layers_and_connections() called&quot; &lt;&lt; endl;
+
+    if( connections.length() != n_layers-1 )
+        PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;there should be %d connections.\n&quot;,
+                n_layers-1);
+
+    if( reconstruction_connections.length() != n_layers-1 )
+        PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;there should be %d reconstruction connections.\n&quot;,
+                n_layers-1);
+
+    if(unsupervised_layers.length() != n_layers-2 
+       &amp;&amp; unsupervised_layers.length() != 0)
+        PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;there should be either 0 of %d unsupervised_layers.\n&quot;,
+                n_layers-2);
+        
+    if(unsupervised_connections.length() != n_layers-2 
+       &amp;&amp; unsupervised_connections.length() != 0)
+        PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;there should be either 0 of %d unsupervised_connections.\n&quot;,
+                n_layers-2);
+        
+    if(unsupervised_connections.length() != unsupervised_layers.length())
+        PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;there should be as many unsupervised_connections and &quot;
+                &quot;unsupervised_layers.\n&quot;);
+        
+
+    if(layers[0]-&gt;size != inputsize_)
+        PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
+                &quot;layers[0] should have a size of %d.\n&quot;,
+                inputsize_);
+    
+
+    activations.resize( n_layers );
+    expectations.resize( n_layers );
+    activation_gradients.resize( n_layers );
+    expectation_gradients.resize( n_layers );
+
+    greedy_layers.resize(n_layers-1);
+    greedy_connections.resize(n_layers-1);
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        if( layers[i]-&gt;size != connections[i]-&gt;down_size )
+            PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() &quot;
+                    &quot;- \n&quot;
+                    &quot;connections[%i] should have a down_size of %d.\n&quot;,
+                    i, layers[i]-&gt;size);
+
+        if( connections[i]-&gt;up_size != layers[i+1]-&gt;size )
+            PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() &quot;
+                    &quot;- \n&quot;
+                    &quot;connections[%i] should have a up_size of %d.\n&quot;,
+                    i, layers[i+1]-&gt;size);
+
+        if(unsupervised_layers.length() != 0 &amp;&amp;
+           unsupervised_connections.length() != 0 &amp;&amp; 
+           unsupervised_layers[i] &amp;&amp; unsupervised_connections[i])
+        {
+            if( layers[i]-&gt;size != 
+                unsupervised_connections[i]-&gt;down_size )
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() &quot;
+                        &quot;- \n&quot;
+                        &quot;connections[%i] should have a down_size of %d.\n&quot;,
+                        i, unsupervised_layers[i]-&gt;size);
+            
+            if( unsupervised_connections[i]-&gt;up_size != 
+                unsupervised_layers[i]-&gt;size )
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() &quot;
+                        &quot;- \n&quot;
+                        &quot;connections[%i] should have a up_size of %d.\n&quot;,
+                        i, unsupervised_layers[i+1]-&gt;size);
+            
+            if( layers[i+1]-&gt;size + unsupervised_layers[i]-&gt;size != 
+                reconstruction_connections[i]-&gt;down_size )
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() &quot;
+                        &quot;- \n&quot;
+                        &quot;recontruction_connections[%i] should have a down_size of &quot;
+                        &quot;%d.\n&quot;,
+                        i, layers[i+1]-&gt;size + unsupervised_layers[i]-&gt;size);
+            
+            if( reconstruction_connections[i]-&gt;up_size != 
+                layers[i]-&gt;size )
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() &quot;
+                        &quot;- \n&quot;
+                        &quot;recontruction_connections[%i] should have a up_size of &quot;
+                        &quot;%d.\n&quot;,
+                        i, layers[i]-&gt;size);
+
+            if( !(unsupervised_layers[i]-&gt;random_gen) )
+            {
+                unsupervised_layers[i]-&gt;random_gen = random_gen;
+                unsupervised_layers[i]-&gt;forget();
+            }
+            
+            if( !(unsupervised_connections[i]-&gt;random_gen) )
+            {
+                unsupervised_connections[i]-&gt;random_gen = random_gen;
+                unsupervised_connections[i]-&gt;forget();
+            }
+
+            PP&lt;RBMMixedLayer&gt; greedy_layer = new RBMMixedLayer();
+            greedy_layer-&gt;sub_layers.resize(2);
+            greedy_layer-&gt;sub_layers[0] = layers[i+1];
+            greedy_layer-&gt;sub_layers[1] = unsupervised_layers[i];
+            greedy_layer-&gt;build();
+
+            PP&lt;RBMMixedConnection&gt; greedy_connection = new RBMMixedConnection();
+            greedy_connection-&gt;sub_connections.resize(2,1);
+            greedy_connection-&gt;sub_connections(1,0) = connections[i];
+            greedy_connection-&gt;sub_connections(2,0) = unsupervised_connections[i];
+            greedy_connection-&gt;build();
+            
+            greedy_layers[i] = greedy_layer;
+            greedy_connections[i] = greedy_connection;
+        }
+        else
+        {
+            if( layers[i+1]-&gt;size != reconstruction_connections[i]-&gt;down_size )
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() &quot;
+                        &quot;- \n&quot;
+                        &quot;recontruction_connections[%i] should have a down_size of &quot;
+                        &quot;%d.\n&quot;,
+                        i, layers[i+1]-&gt;size);
+            
+            if( reconstruction_connections[i]-&gt;up_size != layers[i]-&gt;size )
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::build_layers_and_connections() &quot;
+                        &quot;- \n&quot;
+                        &quot;recontruction_connections[%i] should have a up_size of &quot;
+                        &quot;%d.\n&quot;,
+                        i, layers[i]-&gt;size);
+ 
+            greedy_layers[i] = layers[i+1];
+            greedy_connections[i] = connections[i];
+        }
+
+        if( !(layers[i]-&gt;random_gen) )
+        {
+            layers[i]-&gt;random_gen = random_gen;
+            layers[i]-&gt;forget();
+        }
+
+        if( !(connections[i]-&gt;random_gen) )
+        {
+            connections[i]-&gt;random_gen = random_gen;
+            connections[i]-&gt;forget();
+        }
+
+        if( !(reconstruction_connections[i]-&gt;random_gen) )
+        {
+            reconstruction_connections[i]-&gt;random_gen = random_gen;
+            reconstruction_connections[i]-&gt;forget();
+        }        
+
+        activations[i].resize( layers[i]-&gt;size );
+        expectations[i].resize( layers[i]-&gt;size );
+        activation_gradients[i].resize( layers[i]-&gt;size );
+        expectation_gradients[i].resize( layers[i]-&gt;size );
+    }
+
+    if( !(layers[n_layers-1]-&gt;random_gen) )
+    {
+        layers[n_layers-1]-&gt;random_gen = random_gen;
+        layers[n_layers-1]-&gt;forget();
+    }
+    activations[n_layers-1].resize( layers[n_layers-1]-&gt;size );
+    expectations[n_layers-1].resize( layers[n_layers-1]-&gt;size );
+    activation_gradients[n_layers-1].resize( layers[n_layers-1]-&gt;size );
+    expectation_gradients[n_layers-1].resize( layers[n_layers-1]-&gt;size );
+}
+
+// ### Nothing to add here, simply calls build_
+void StackedFocusedAutoassociatorsNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void StackedFocusedAutoassociatorsNet::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(, copies);
+
+    // Public options
+    deepCopyField(training_schedule, copies);
+    deepCopyField(layers, copies);
+    deepCopyField(connections, copies);
+    deepCopyField(reconstruction_connections, copies);
+    deepCopyField(unsupervised_layers, copies);
+    deepCopyField(unsupervised_connections, copies);
+
+    // Protected options
+    deepCopyField(activations, copies);
+    deepCopyField(expectations, copies);
+    deepCopyField(activation_gradients, copies);
+    deepCopyField(expectation_gradients, copies);
+    deepCopyField(greedy_activation, copies);
+    deepCopyField(greedy_expectation, copies);
+    deepCopyField(greedy_activation_gradient, copies);
+    deepCopyField(greedy_expectation_gradient, copies);
+    deepCopyField(reconstruction_activations, copies);
+    deepCopyField(reconstruction_activation_gradients, copies);
+    deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(greedy_layers, copies);
+    deepCopyField(greedy_connections, copies);
+    deepCopyField(similar_example_representation, copies);
+    deepCopyField(dissimilar_example_representation, copies);
+    deepCopyField(input_representation, copies);
+    deepCopyField(previous_input_representation, copies);
+    deepCopyField(dissimilar_gradient_contribution, copies);
+    deepCopyField(pos_down_val, copies);
+    deepCopyField(pos_up_val, copies);
+    deepCopyField(neg_down_val, copies);
+    deepCopyField(neg_up_val, copies);
+    deepCopyField(class_datasets, copies);
+    deepCopyField(other_classes_proportions, copies);
+    deepCopyField(nearest_neighbors_indices, copies);
+    deepCopyField(test_nearest_neighbors_indices, copies);
+    deepCopyField(test_votes, copies);
+    deepCopyField(train_set_representations, copies);
+    deepCopyField(train_set_representations_vmat, copies);
+    deepCopyField(train_set_targets, copies);
+    deepCopyField(greedy_stages, copies);
+}
+
+
+int StackedFocusedAutoassociatorsNet::outputsize() const
+{
+    //if(currently_trained_layer &lt; n_layers)
+    //    return layers[currently_trained_layer]-&gt;size;
+    //return layers[n_layers-1]-&gt;size;
+    return n_classes;
+}
+
+void StackedFocusedAutoassociatorsNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+
+    train_set_representations_up_to_date = false;
+
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        connections[i]-&gt;forget();
+        reconstruction_connections[i]-&gt;forget();
+    }
+    
+    stage = 0;
+    greedy_stages.clear();
+}
+
+void StackedFocusedAutoassociatorsNet::train()
+{
+    MODULE_LOG &lt;&lt; &quot;train() called &quot; &lt;&lt; endl;
+    MODULE_LOG &lt;&lt; &quot;  training_schedule = &quot; &lt;&lt; training_schedule &lt;&lt; endl;
+
+    Vec input( inputsize() );
+    Vec similar_example( inputsize() );
+    Vec dissimilar_example( inputsize() );
+    Vec target( targetsize() );
+    Vec target2( targetsize() );
+    real weight; // unused
+    real weight2; // unused
+
+    Vec similar_example_index(1);
+
+    TVec&lt;string&gt; train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set-&gt;length();
+    int sample;
+
+    PP&lt;ProgressBar&gt; pb;
+
+    // clear stats of previous epoch
+    train_stats-&gt;forget();
+
+    int init_stage;
+
+    /***** initial greedy training *****/
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        MODULE_LOG &lt;&lt; &quot;Training connection weights between layers &quot; &lt;&lt; i
+            &lt;&lt; &quot; and &quot; &lt;&lt; i+1 &lt;&lt; endl;
+
+        int end_stage = training_schedule[i];
+        int* this_stage = greedy_stages.subVec(i,1).data();
+        init_stage = *this_stage;
+
+        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; *this_stage &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  end_stage = &quot; &lt;&lt; end_stage &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  greedy_learning_rate = &quot; &lt;&lt; greedy_learning_rate &lt;&lt; endl;
+
+        if( report_progress &amp;&amp; *this_stage &lt; end_stage )
+            pb = new ProgressBar( &quot;Training layer &quot;+tostring(i)
+                                  +&quot; of &quot;+classname(),
+                                  end_stage - init_stage );
+
+        train_costs.fill(MISSING_VALUE);
+        reconstruction_activations.resize(layers[i]-&gt;size);
+        reconstruction_activation_gradients.resize(layers[i]-&gt;size);
+        reconstruction_expectation_gradients.resize(layers[i]-&gt;size);
+
+        similar_example_representation.resize(layers[i+1]-&gt;size);
+        dissimilar_example_representation.resize(layers[i+1]-&gt;size);
+        dissimilar_gradient_contribution.resize(layers[i+1]-&gt;size);
+        input_representation.resize(layers[i+1]-&gt;size);
+
+        greedy_activation.resize(greedy_layers[i]-&gt;size);
+        greedy_expectation.resize(greedy_layers[i]-&gt;size);
+        greedy_activation_gradient.resize(greedy_layers[i]-&gt;size);
+        greedy_expectation_gradient.resize(greedy_layers[i]-&gt;size);
+
+        pos_down_val.resize(layers[i]-&gt;size);
+        pos_up_val.resize(greedy_layers[i]-&gt;size);
+        neg_down_val.resize(layers[i]-&gt;size);
+        neg_up_val.resize(greedy_layers[i]-&gt;size);
+
+        for( ; *this_stage&lt;end_stage ; (*this_stage)++ )
+        {
+            
+            sample = *this_stage % nsamples;
+            train_set-&gt;getExample(sample, input, target, weight);
+            // Find similar example
+
+            int sim_index = random_gen-&gt;uniform_multinomial_sample(k_neighbors);
+            train_set-&gt;getExample(nearest_neighbors_indices(sample,sim_index),
+                                  similar_example, target2, weight2);
+
+            if(round(target[0]) != round(target2[0]))
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::train(): similar&quot;
+                    &quot; example is not from same class!&quot;);
+
+            // Find dissimilar example
+
+            int dissim_class_index = random_gen-&gt;multinomial_sample(
+                other_classes_proportions(round(target[0])));
+
+            int dissim_index = random_gen-&gt;uniform_multinomial_sample(
+                class_datasets[dissim_class_index]-&gt;length());
+
+            class_datasets[dissim_class_index]-&gt;getExample(dissim_index,
+                                  dissimilar_example, target2, weight2);
+
+            if(round(target[0]) == round(target2[0]))
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::train(): dissimilar&quot;
+                    &quot; example is from same class!&quot;);
+
+            greedyStep( input, target, i, train_costs, *this_stage,
+                        similar_example, dissimilar_example);
+            train_stats-&gt;update( train_costs );
+
+            if( pb )
+                pb-&gt;update( *this_stage - init_stage + 1 );
+        }
+    }
+
+    /***** fine-tuning by gradient descent *****/
+    if( stage &lt; nstages )
+    {
+
+        MODULE_LOG &lt;&lt; &quot;Fine-tuning all parameters, by gradient descent&quot; &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
+        MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot; &lt;&lt; 
+            fine_tuning_learning_rate &lt;&lt; endl;
+
+        init_stage = stage;
+        if( report_progress &amp;&amp; stage &lt; nstages )
+            pb = new ProgressBar( &quot;Fine-tuning parameters of all layers of &quot;
+                                  + classname(),
+                                  nstages - init_stage );
+
+        setLearningRate( fine_tuning_learning_rate );
+        train_costs.fill(MISSING_VALUE);
+
+        similar_example_representation.resize(
+            layers[n_layers-1]-&gt;size);
+        dissimilar_example_representation.resize(
+            layers[n_layers-1]-&gt;size);
+        dissimilar_gradient_contribution.resize(
+            layers[n_layers-1]-&gt;size);
+        similar_example.resize(inputsize());
+        dissimilar_example.resize(inputsize());
+
+        for( ; stage&lt;nstages ; stage++ )
+        {
+            sample = stage % nsamples;
+            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                setLearningRate( fine_tuning_learning_rate
+                                 / (1. + fine_tuning_decrease_ct * stage ) );
+
+            train_set-&gt;getExample( sample, input, target, weight );
+
+            // Find similar example
+
+            int sim_index = random_gen-&gt;uniform_multinomial_sample(k_neighbors);
+            train_set-&gt;getExample(nearest_neighbors_indices(sample,sim_index),
+                                  similar_example, target2, weight2);
+
+            if(round(target[0]) != round(target2[0]))
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::train(): similar&quot;
+                    &quot; example is not from same class!&quot;);
+
+            // Find dissimilar example
+
+            int dissim_class_index = random_gen-&gt;multinomial_sample(
+                other_classes_proportions(round(target[0])));
+
+            int dissim_index = random_gen-&gt;uniform_multinomial_sample(
+                class_datasets[dissim_class_index]-&gt;length());
+
+            class_datasets[dissim_class_index]-&gt;getExample(dissim_index,
+                                  dissimilar_example, target2, weight2);
+
+            if(round(target[0]) == round(target2[0]))
+                PLERROR(&quot;StackedFocusedAutoassociatorsNet::train(): dissimilar&quot;
+                    &quot; example is from same class!&quot;);
+
+            fineTuningStep( input, target, train_costs, 
+                            similar_example, dissimilar_example);
+            train_stats-&gt;update( train_costs );
+
+            if( pb )
+                pb-&gt;update( stage - init_stage + 1 );
+        }
+    }
+    
+    train_stats-&gt;finalize();
+    MODULE_LOG &lt;&lt; &quot;  train costs = &quot; &lt;&lt; train_stats-&gt;getMean() &lt;&lt; endl;
+
+    // Update currently_trained_layer
+    if(stage &gt; 0)
+        currently_trained_layer = n_layers;
+    else
+    {            
+        currently_trained_layer = n_layers-1;
+        while(currently_trained_layer&gt;1 
+              &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
+            currently_trained_layer--;
+    }
+}
+
+void StackedFocusedAutoassociatorsNet::greedyStep( 
+    const Vec&amp; input, const Vec&amp; target, int index, 
+    Vec train_costs, int this_stage, Vec similar_example, Vec dissimilar_example )
+{
+    PLASSERT( index &lt; n_layers );
+    real lr;
+    train_set_representations_up_to_date = false;
+
+    // Get similar example representation
+    
+    computeRepresentation(similar_example, similar_example_representation, 
+                          index+1);
+
+    // Get dissimilar example representation
+
+    computeRepresentation(dissimilar_example, dissimilar_example_representation, 
+                          index+1);
+
+    // Get example representation
+
+    computeRepresentation(input, previous_input_representation, 
+                          index);
+    greedy_connections[index]-&gt;fprop(previous_input_representation,
+                                     greedy_activation);
+    greedy_layers[index]-&gt;fprop(greedy_activation,
+                                greedy_expectation);
+    input_representation &lt;&lt; greedy_expectation.subVec(0,layers[index+1]-&gt;size);
+
+    // Autoassociator learning
+
+    if( !fast_exact_is_equal( greedy_learning_rate, 0 ) )
+    {
+        if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+            lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                       * this_stage); 
+        else
+            lr = greedy_learning_rate;
+
+        layers[index]-&gt;setLearningRate( lr );
+        greedy_connections[index]-&gt;setLearningRate( lr );
+        reconstruction_connections[index]-&gt;setLearningRate( lr );
+        greedy_layers[index]-&gt;setLearningRate( lr );
+
+        reconstruction_connections[ index ]-&gt;fprop( greedy_expectation,
+                                                    reconstruction_activations);
+        layers[ index ]-&gt;fprop( reconstruction_activations,
+                                layers[ index ]-&gt;expectation);
+        
+        layers[ index ]-&gt;activation &lt;&lt; reconstruction_activations;
+        layers[ index ]-&gt;expectation_is_up_to_date = true;
+        real rec_err = layers[ index ]-&gt;fpropNLL(previous_input_representation);
+        train_costs[index] = rec_err;
+        
+        layers[ index ]-&gt;bpropNLL(previous_input_representation, rec_err,
+                                  reconstruction_activation_gradients);
+    }
+
+    // Compute supervised gradient
+    
+    // Similar example contribution
+    substract(input_representation,similar_example_representation,
+              expectation_gradients[index+1]);
+    expectation_gradients[index+1] *= 4/layers[index+1]-&gt;size;
+    
+    // Dissimilar example contribution
+    real dist = sqrt(powdistance(input_representation,
+                                 dissimilar_example_representation,
+                                 2));
+    
+    substract(input_representation,dissimilar_example_representation,
+              dissimilar_gradient_contribution);
+
+    dissimilar_gradient_contribution *= -5.54*
+        safeexp(-2.77*dist/layers[index+1]-&gt;size)/dist;
+    
+    expectation_gradients[index+1] += dissimilar_gradient_contribution;
+
+    // RBM learning
+    if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
+    {
+        greedy_layers[index]-&gt;expectation &lt;&lt; greedy_expectation;
+        greedy_layers[index]-&gt;expectation_is_up_to_date = true;
+        greedy_layers[index]-&gt;generateSample();
+        
+        // accumulate positive stats using the expectation
+        // we deep-copy because the value will change during negative phase
+        pos_down_val = expectations[index];
+        pos_up_val = greedy_layers[index]-&gt;expectation;
+        
+        // down propagation, starting from a sample of layers[index+1]
+        greedy_connections[index]-&gt;setAsUpInput( greedy_layers[index]-&gt;sample );
+        
+        layers[index]-&gt;getAllActivations( greedy_connections[index] );
+        layers[index]-&gt;computeExpectation();
+        layers[index]-&gt;generateSample();
+        
+        // negative phase
+        greedy_connections[index]-&gt;setAsDownInput( layers[index]-&gt;sample );
+        greedy_layers[index]-&gt;getAllActivations( greedy_connections[index] );
+        greedy_layers[index]-&gt;computeExpectation();
+        // accumulate negative stats
+        // no need to deep-copy because the values won't change before update
+        neg_down_val = layers[index]-&gt;sample;
+        neg_up_val = greedy_layers[index]-&gt;expectation;
+    }
+    
+    // Update hidden layer bias and weights
+
+    if( !fast_exact_is_equal( greedy_learning_rate, 0 ) )
+    {
+        layers[ index ]-&gt;update(reconstruction_activation_gradients);
+    
+        reconstruction_connections[ index ]-&gt;bpropUpdate( 
+            greedy_expectation,
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            reconstruction_activation_gradients);
+
+        greedy_layers[ index ]-&gt;bpropUpdate( 
+            greedy_activation,
+            greedy_expectation,
+            // reused
+            reconstruction_activation_gradients,
+            reconstruction_expectation_gradients);
+        
+        greedy_connections[ index ]-&gt;bpropUpdate( 
+            previous_input_representation,
+            greedy_activation,
+            reconstruction_expectation_gradients, //reused
+            reconstruction_activation_gradients);
+    }
+     
+
+    if( !fast_exact_is_equal( supervised_greedy_decrease_ct , 0 ) )
+        lr = supervised_greedy_learning_rate/(1 + supervised_greedy_decrease_ct 
+                               * this_stage); 
+    else
+        lr = supervised_greedy_learning_rate;
+    
+    layers[index]-&gt;setLearningRate( lr );
+    connections[index]-&gt;setLearningRate( lr );
+    layers[index+1]-&gt;setLearningRate( lr );
+    
+    layers[ index+1 ]-&gt;bpropUpdate( 
+        greedy_activation.subVec(0,layers[index+1]-&gt;size),
+        greedy_expectation.subVec(0,layers[index+1]-&gt;size),
+        activation_gradients[index+1], 
+        expectation_gradients[index+1]);
+    
+    connections[ index ]-&gt;bpropUpdate( 
+        previous_input_representation,
+        greedy_activation.subVec(0,layers[index+1]-&gt;size),
+        expectation_gradients[index],
+        activation_gradients[index+1]);
+
+    // RBM updates
+
+    if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
+    {
+        if( !fast_exact_is_equal( cd_decrease_ct , 0 ) )
+            lr = cd_learning_rate/(1 + cd_decrease_ct 
+                                       * this_stage); 
+        else
+            lr = cd_learning_rate;
+
+        layers[index]-&gt;setLearningRate( lr );
+        greedy_connections[index]-&gt;setLearningRate( lr );
+        greedy_layers[index]-&gt;setLearningRate( lr );
+
+        layers[index]-&gt;update( pos_down_val, neg_down_val );
+        greedy_connections[index]-&gt;update( pos_down_val, pos_up_val,
+                                    neg_down_val, neg_up_val );
+        greedy_layers[index]-&gt;update( pos_up_val, neg_up_val );
+    }
+}
+
+void StackedFocusedAutoassociatorsNet::fineTuningStep( 
+    const Vec&amp; input, const Vec&amp; target,
+    Vec&amp; train_costs, Vec similar_example, Vec dissimilar_example )
+{
+    train_set_representations_up_to_date = false;
+
+    // Get similar example representation
+    
+    computeRepresentation(similar_example, similar_example_representation, 
+                          n_layers-1);
+
+    // Get dissimilar example representation
+
+    computeRepresentation(dissimilar_example, dissimilar_example_representation, 
+                          n_layers-1);
+
+    // Get example representation
+
+    computeRepresentation(input, previous_input_representation, 
+                          n_layers-1);
+
+    // Compute supervised gradient
+
+    // Similar example contribution
+    substract(input_representation,similar_example_representation,
+              expectation_gradients[n_layers-1]);
+    expectation_gradients[n_layers-1] *= 4/layers[n_layers-1]-&gt;size;
+    
+    // Dissimilar example contribution
+    real dist = sqrt(powdistance(input_representation,
+                                 dissimilar_example_representation,
+                                 2));
+    
+    substract(input_representation,dissimilar_example_representation,
+              dissimilar_gradient_contribution);
+
+    dissimilar_gradient_contribution *= -5.54*
+        safeexp(-2.77*dist/layers[n_layers-1]-&gt;size)/dist;
+    
+    expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
+
+
+    for( int i=n_layers-1 ; i&gt;0 ; i-- )
+    {
+        layers[i]-&gt;bpropUpdate( activations[i],
+                                expectations[i],
+                                activation_gradients[i],
+                                expectation_gradients[i] );
+        
+        
+        connections[i-1]-&gt;bpropUpdate( expectations[i-1],
+                                       activations[i],
+                                       expectation_gradients[i-1],
+                                       activation_gradients[i] );
+    }        
+}
+
+void StackedFocusedAutoassociatorsNet::computeRepresentation(const Vec&amp; input,
+                                                             Vec&amp; representation,
+                                                             int layer) const
+{
+    if(layer == 0)
+    {
+        representation.resize(input.length());
+        representation &lt;&lt; input;
+        return;
+    }
+
+    expectations[0] &lt;&lt; input;
+    for( int i=0 ; i&lt;layer; i++ )
+    {
+        connections[i]-&gt;fprop( expectations[i], activations[i+1] );
+        layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
+    }
+    representation.resize(expectations[layer].length());
+    representation &lt;&lt; expectations[layer];
+}
+
+void StackedFocusedAutoassociatorsNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    updateTrainSetRepresentations();
+
+    computeRepresentation(input,input_representation, 
+                          max(currently_trained_layer,n_layers-1));
+
+    computeNearestNeighbors(train_set_representations_vmat,input_representation,
+                            test_nearest_neighbors_indices);
+
+    test_votes.clear();
+    for(int i=0; i&lt;test_nearest_neighbors_indices.length(); i++)
+        test_votes[train_set_targets[test_nearest_neighbors_indices[i]]]++;
+
+    output[0] = argmax(test_votes);
+
+}
+
+void StackedFocusedAutoassociatorsNet::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+
+    //Assumes that computeOutput has been called
+
+    costs.resize( getTestCostNames().length() );
+    costs.fill( MISSING_VALUE );
+
+    if( currently_trained_layer&lt;n_layers )
+    {
+        greedy_connections[currently_trained_layer-1]-&gt;fprop(
+            expectations[currently_trained_layer-1],
+            greedy_activation);
+        
+        greedy_layers[currently_trained_layer-1]-&gt;fprop(greedy_activation,
+                                    greedy_expectation);
+        
+        reconstruction_connections[ currently_trained_layer-1 ]-&gt;fprop( 
+            greedy_expectation,
+            reconstruction_activations);
+        layers[ currently_trained_layer-1 ]-&gt;fprop( 
+            reconstruction_activations,
+            layers[ currently_trained_layer-1 ]-&gt;expectation);
+        
+        layers[ currently_trained_layer-1 ]-&gt;activation &lt;&lt; 
+            reconstruction_activations;
+        layers[ currently_trained_layer-1 ]-&gt;expectation_is_up_to_date = true;
+        costs[ currently_trained_layer-1 ]  = 
+            layers[ currently_trained_layer-1 ]-&gt;fpropNLL(
+                expectations[currently_trained_layer-1]);
+    }
+
+    if( round(output[0]) == round(target[0]) )
+        costs[n_layers-1] = 0;
+    else
+        costs[n_layers-1] = 1;
+}
+
+//////////
+// test //
+//////////
+void StackedFocusedAutoassociatorsNet::updateTrainSetRepresentations() const
+{
+    if(!train_set_representations_up_to_date)
+    {
+        // Precompute training set examples' representation
+        int l = max(currently_trained_layer,n_layers-1);
+        Vec input( inputsize() );
+        Vec target( targetsize() );
+        Vec train_set_representation;
+        real weight;
+
+        train_set_representations.resize(train_set-&gt;length(), layers[l]-&gt;size);
+        train_set_targets.resize(train_set-&gt;length());
+        
+        for(int i=0; i&lt;train_set-&gt;length(); i++)
+        {
+            train_set-&gt;getExample(i,input,target,weight);
+            computeRepresentation(input,train_set_representation,l);
+            train_set_representations(i) &lt;&lt; train_set_representation;
+            train_set_targets[i] = round(target[0]);
+        }
+        train_set_representations_vmat = VMat(train_set_representations);
+
+        train_set_representations_up_to_date = true;
+    }
+}
+
+TVec&lt;string&gt; StackedFocusedAutoassociatorsNet::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    TVec&lt;string&gt; cost_names(0);
+
+    for( int i=0; i&lt;layers.size()-1; i++)
+        cost_names.push_back(&quot;reconstruction_error_&quot; + tostring(i+1));
+        
+    cost_names.append( &quot;class_error&quot; );
+
+    return cost_names;
+}
+
+TVec&lt;string&gt; StackedFocusedAutoassociatorsNet::getTrainCostNames() const
+{
+    return getTestCostNames() ;    
+}
+
+void StackedFocusedAutoassociatorsNet::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set,call_forget);
+    
+    train_set_representations_up_to_date = false;
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    real weight; // unused
+
+    // Separate classes
+    for(int k=0; k&lt;n_classes; k++)
+    {
+        class_datasets[k] = new ClassSubsetVMatrix();
+        class_datasets[k]-&gt;classes.resize(1);
+        class_datasets[k]-&gt;classes[0] = k;
+        class_datasets[k]-&gt;source = training_set;
+        class_datasets[k]-&gt;build();
+    }
+
+    // Find other classes proportions
+    other_classes_proportions.fill(0);
+    for(int k=0; k&lt;n_classes; k++)
+    {
+        real sum = 0;
+        for(int j=0; j&lt;n_classes; j++)
+        {
+            if(j==k) continue;
+            other_classes_proportions(k,j) = class_datasets[j]-&gt;length();
+            sum += class_datasets[j]-&gt;length();
+        }
+        other_classes_proportions(k) /= sum;
+    }
+
+    // Find training nearest neighbors
+    input.resize(training_set-&gt;inputsize());
+    target.resize(training_set-&gt;targetsize());
+    nearest_neighbors_indices.resize(training_set-&gt;length(), k_neighbors);
+    TVec&lt;int&gt; nearest_neighbors_indices_row;
+    for(int k=0; k&lt;n_classes; k++)
+    {
+        for(int i=0; i&lt;class_datasets[k]-&gt;length(); i++)
+        {
+            class_datasets[k]-&gt;getExample(i,input,target,weight);
+            nearest_neighbors_indices_row = nearest_neighbors_indices(
+                class_datasets[k]-&gt;indices[i]);
+            computeNearestNeighbors((VMatrix *)class_datasets[k],input,
+                                    nearest_neighbors_indices_row,
+                                    i);
+        }
+    }
+}
+
+
+//#####  Helper functions  ##################################################
+
+void StackedFocusedAutoassociatorsNet::setLearningRate( real the_learning_rate )
+{
+    for( int i=0 ; i&lt;n_layers-1 ; i++ )
+    {
+        layers[i]-&gt;setLearningRate( the_learning_rate );
+        connections[i]-&gt;setLearningRate( the_learning_rate );
+    }
+    layers[n_layers-1]-&gt;setLearningRate( the_learning_rate );
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h	2007-10-06 20:07:56 UTC (rev 8155)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h	2007-10-09 13:13:07 UTC (rev 8156)
@@ -0,0 +1,351 @@
+// -*- C++ -*-
+
+// StackedFocusedAutoassociatorsNet.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedFocusedAutoassociatorsNet.h */
+
+
+#ifndef StackedFocusedAutoassociatorsNet_INC
+#define StackedFocusedAutoassociatorsNet_INC
+
+#include &lt;plearn/vmat/ClassSubsetVMatrix.h&gt;
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn_learners/online/OnlineLearningModule.h&gt;
+#include &lt;plearn_learners/online/CostModule.h&gt;
+#include &lt;plearn_learners/online/NLLCostModule.h&gt;
+#include &lt;plearn_learners/online/RBMClassificationModule.h&gt;
+#include &lt;plearn_learners/online/RBMLayer.h&gt;
+#include &lt;plearn_learners/online/RBMMixedLayer.h&gt;
+#include &lt;plearn_learners/online/RBMConnection.h&gt;
+#include &lt;plearn/misc/PTimer.h&gt;
+
+namespace PLearn {
+
+/**
+ * Neural net, trained layer-wise in a greedy but focused fashion 
+ * using autoassociators/RBMs and a supervised non-parametric gradient.
+ * It is highly inspired by the StackedAutoassociators class, 
+ * and can use use the same RBMLayer and RBMConnection components.
+ */
+class StackedFocusedAutoassociatorsNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Contrastive divergence learning rate
+    real cd_learning_rate;
+    
+    //! Contrastive divergence decrease constant
+    real cd_decrease_ct;
+
+    //! The learning rate used during the autoassociator gradient descent training
+    real greedy_learning_rate;
+
+    //! The decrease constant of the learning rate used during the autoassociator
+    //! gradient descent training. When a hidden layer has finished its training,
+    //! the learning rate is reset to it's initial value.
+    real greedy_decrease_ct;
+
+    //! Supervised, non-parametric, greedy learning rate
+    real supervised_greedy_learning_rate;
+
+    //! Supervised, non-parametric, greedy decrease constant
+    real supervised_greedy_decrease_ct;
+
+    //! The learning rate used during the fine tuning gradient descent
+    real fine_tuning_learning_rate;
+
+    //! The decrease constant of the learning rate used during fine tuning
+    //! gradient descent
+    real fine_tuning_decrease_ct;
+
+    //! Number of examples to use during each phase of greedy pre-training.
+    //! The number of fine-tunig steps is defined by nstages.
+    TVec&lt;int&gt; training_schedule;
+
+    //! The layers of units in the network
+    TVec&lt; PP&lt;RBMLayer&gt; &gt; layers;
+
+    //! The weights of the connections between the layers
+    TVec&lt; PP&lt;RBMConnection&gt; &gt; connections;
+
+    //! The reconstruction weights of the autoassociators
+    TVec&lt; PP&lt;RBMConnection&gt; &gt; reconstruction_connections;
+
+    //! Additional units for greedy unsupervised learning
+    TVec&lt; PP&lt;RBMLayer&gt; &gt; unsupervised_layers;
+
+    //! Additional connections for greedy unsupervised learning
+    TVec&lt; PP&lt;RBMConnection&gt; &gt; unsupervised_connections;
+
+    //! Number of good nearest neighbors to attract and
+    //! bad nearest neighbors to repel.
+    int k_neighbors;
+
+    //! Number of classes
+    int n_classes;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Number of layers
+    int n_layers;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    StackedFocusedAutoassociatorsNet();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    /**
+     *  Precomputes the representations of the training set examples, 
+     *  to speed up nearest neighbors searches in that space.
+     */
+    virtual void updateTrainSetRepresentations() const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+    /**
+     *  Declares the training set.  Then calls build() and forget() if
+     *  necessary.  Also sets this learner's inputsize_ targetsize_ weightsize_
+     *  from those of the training_set.  Note: You shouldn't have to override
+     *  this in subclasses, except in maybe to forward the call to an
+     *  underlying learner.
+     */
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+
+    void greedyStep( const Vec&amp; input, const Vec&amp; target, int index, 
+                     Vec train_costs, int stage, Vec similar_example,
+                     Vec dissimilar_example);
+
+    void fineTuningStep( const Vec&amp; input, const Vec&amp; target,
+                         Vec&amp; train_costs, Vec similar_example, 
+                         Vec dissimilar_example );
+
+    void computeRepresentation( const Vec&amp; input, 
+                                Vec&amp; representation, int layer) const;
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(StackedFocusedAutoassociatorsNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    //! Stores the activations of the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec&lt;Vec&gt; activations;
+
+    //! Stores the expectations of the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec&lt;Vec&gt; expectations;
+
+    //! Stores the gradient of the cost wrt the activations of 
+    //! the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec&lt;Vec&gt; activation_gradients;
+
+    //! Stores the gradient of the cost wrt the expectations of 
+    //! the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec&lt;Vec&gt; expectation_gradients;
+
+    //! Stores the activation of the trained hidden layer during a greedy step
+    mutable Vec greedy_activation;
+
+    //! Stores the expectation of the trained hidden layer during a greedy step
+    mutable Vec greedy_expectation;
+
+    //! Stores the activation gradient of the trained 
+    //! hidden layer during a greedy step
+    mutable Vec greedy_activation_gradient;
+
+    //! Stores the expectation gradient of the trained 
+    //! hidden layer during a greedy step
+    mutable Vec greedy_expectation_gradient;
+
+    //! Reconstruction activations
+    mutable Vec reconstruction_activations;
+    
+    //! Reconstruction activation gradients
+    mutable Vec reconstruction_activation_gradients;
+
+    //! Reconstruction expectation gradients
+    mutable Vec reconstruction_expectation_gradients;
+
+    //! Layers used for greedy learning
+    TVec&lt; PP&lt;RBMLayer&gt; &gt; greedy_layers;
+
+    //! Connections used for greedy learning
+    TVec&lt; PP&lt;RBMConnection&gt; &gt; greedy_connections;
+
+    //! Similar example representation
+    Vec similar_example_representation;
+
+    //! Dissimilar example representation
+    Vec dissimilar_example_representation;
+
+    //! Example representation
+    mutable Vec input_representation;
+
+    //! Example representation at the previous layer, in a greedy step
+    Vec previous_input_representation;
+
+    //! Dissimilar gradient contribution
+    Vec dissimilar_gradient_contribution;
+
+    //! Positive down statistic
+    Vec pos_down_val;
+    //! Positive up statistic
+    Vec pos_up_val;
+    //! Negative down statistic
+    Vec neg_down_val;
+    //! Negative up statistic
+    Vec neg_up_val;
+
+    //! Datasets for each class
+    TVec&lt; PP&lt;ClassSubsetVMatrix&gt; &gt; class_datasets;
+
+    //! Proportions of examples from the other classes (columns), for each
+    //! class (rows)
+    Mat other_classes_proportions;
+
+    //! Nearest neighbors for each training example
+    TMat&lt;int&gt; nearest_neighbors_indices;
+
+    //! Nearest neighbors for each test example
+    mutable TVec&lt;int&gt; test_nearest_neighbors_indices;
+
+    //! Nearest neighbor votes for test example
+    TVec&lt;int&gt; test_votes;
+
+    //! Data set mapped to last hidden layer space
+    mutable Mat train_set_representations;
+    mutable VMat train_set_representations_vmat;
+    mutable TVec&lt;int&gt; train_set_targets;
+
+    //! Indication that train_set_representations is up to date
+    mutable bool train_set_representations_up_to_date;
+
+    //! Stages of the different greedy phases
+    TVec&lt;int&gt; greedy_stages;
+
+    //! Currently trained layer (1 means the first hidden layer,
+    //! n_layers means the output layer)
+    int currently_trained_layer;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_classification_cost();
+
+    void setLearningRate( real the_learning_rate );
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(StackedFocusedAutoassociatorsNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001603.html">[Plearn-commits] r8155 - trunk/python_modules/plearn/learners
</A></li>
	<LI>Next message: <A HREF="001605.html">[Plearn-commits] r8157 - in trunk/plearn: base vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1604">[ date ]</a>
              <a href="thread.html#1604">[ thread ]</a>
              <a href="subject.html#1604">[ subject ]</a>
              <a href="author.html#1604">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
