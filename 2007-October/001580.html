<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8132 - trunk/plearn_learners/generic/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-October/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8132%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200710031605.l93G5Y4r006645%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001579.html">
   <LINK REL="Next"  HREF="001581.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8132 - trunk/plearn_learners/generic/EXPERIMENTAL</H1>
    <B>manzagop at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8132%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200710031605.l93G5Y4r006645%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8132 - trunk/plearn_learners/generic/EXPERIMENTAL">manzagop at mail.berlios.de
       </A><BR>
    <I>Wed Oct  3 18:05:34 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001579.html">[Plearn-commits] r8131 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
        <LI>Next message: <A HREF="001581.html">[Plearn-commits] r8133 - trunk/scripts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1580">[ date ]</a>
              <a href="thread.html#1580">[ thread ]</a>
              <a href="subject.html#1580">[ subject ]</a>
              <a href="author.html#1580">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: manzagop
Date: 2007-10-03 18:05:33 +0200 (Wed, 03 Oct 2007)
New Revision: 8132

Added:
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
Log:
This class inherits from mNNet and implements Pascal V's idea for a stochastic gradient
based on a step size adapted in a way similar to rprop.


Added: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-03 16:03:41 UTC (rev 8131)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-03 16:05:33 UTC (rev 8132)
@@ -0,0 +1,269 @@
+// -*- C++ -*-
+
+// PvGradNNet.cc
+//
+// Copyright (C) 2007 PA M, Pascal V
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: P AM, Pascal V
+
+/*! \file PvGradNNet.cc */
+
+#include &quot;PvGradNNet.h&quot;
+#include &lt;plearn/math/pl_erf.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    PvGradNNet,
+    &quot;Multi-layer neural network for experimenting with Pascal V's gradient idea.&quot;,
+    &quot;See the twiki's SRPropGradient entry.\n&quot;
+    );
+
+PvGradNNet::PvGradNNet()
+    : mNNet(),
+      pv_initial_stepsize(1e-1),
+      pv_min_stepsize(1e-6),
+      pv_max_stepsize(50.0),
+      pv_acceleration(1.2),
+      pv_deceleration(0.5),
+      pv_min_samples(2),
+      pv_required_confidence(0.80),
+      pv_random_sample_step(false)
+
+{
+    random_gen = new PRandom();
+}
+
+void PvGradNNet::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;pv_initial_stepsize&quot;,
+                  &amp;PvGradNNet::pv_initial_stepsize,
+                  OptionBase::buildoption,
+                  &quot;Initial size of steps in parameter space&quot;);
+
+    declareOption(ol, &quot;pv_min_stepsize&quot;,
+                  &amp;PvGradNNet::pv_min_stepsize,
+                  OptionBase::buildoption,
+                  &quot;Minimal size of steps in parameter space&quot;);
+
+    declareOption(ol, &quot;pv_max_stepsize&quot;,
+                  &amp;PvGradNNet::pv_max_stepsize,
+                  OptionBase::buildoption,
+                  &quot;Maximal size of steps in parameter space&quot;);
+
+    declareOption(ol, &quot;pv_acceleration&quot;,
+                  &amp;PvGradNNet::pv_acceleration,
+                  OptionBase::buildoption,
+                  &quot;Coefficient by which to multiply the step sizes.&quot;);
+
+    declareOption(ol, &quot;pv_deceleration&quot;,
+                  &amp;PvGradNNet::pv_deceleration,
+                  OptionBase::buildoption,
+                  &quot;Coefficient by which to multiply the step sizes.&quot;);
+
+    declareOption(ol, &quot;pv_min_samples&quot;,
+                  &amp;PvGradNNet::pv_min_samples,
+                  OptionBase::buildoption,
+                  &quot;PV's minimum number of samples to estimate gradient sign.\n&quot;
+                  &quot;This should at least be 2.&quot;);
+
+    declareOption(ol, &quot;pv_required_confidence&quot;,
+                  &amp;PvGradNNet::pv_required_confidence,
+                  OptionBase::buildoption,
+                  &quot;Minimum required confidence (probability of being positive or negative) for taking a step.&quot;);
+
+    declareOption(ol, &quot;pv_random_sample_step&quot;,
+                  &amp;PvGradNNet::pv_random_sample_step,
+                  OptionBase::buildoption,
+                  &quot;If this is set to true, then we will randomly choose the step sign\n&quot;
+                  &quot;for each parameter based on the estimated probability of it being\n&quot;
+                  &quot;positive or negative.&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+// TODO - reloading an object will not work! layer_params will juste get lost.
+void PvGradNNet::build_()
+{
+    pv_gradstats = new VecStatsCollector();
+
+    int n = all_params.length();
+    pv_all_stepsigns.resize(n);
+    pv_all_stepsizes.resize(n);
+    //pv_all_nsamples.resize(n);    // *stat*
+
+    // Get some structure on the previous Vecs
+    pv_layer_stepsigns.resize(n_layers-1);
+    pv_layer_stepsizes.resize(n_layers-1);
+    //pv_layer_nsamples.resize(n_layers-1); // *stat*
+    for (int i=0,p=0;i&lt;n_layers-1;i++)  {
+        int np=layer_sizes[i+1]*(1+layer_sizes[i]);
+        pv_layer_stepsigns[i]=pv_all_stepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        pv_layer_stepsizes[i]=pv_all_stepsizes.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        //pv_layer_nsamples[i]=pv_all_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1); // *stat*
+        p+=np;
+    }
+
+}
+
+// ### Nothing to add here, simply calls build_
+void PvGradNNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void PvGradNNet::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(pv_gradstats, copies);
+    deepCopyField(pv_all_stepsigns, copies);
+    deepCopyField(pv_layer_stepsigns, copies);
+    deepCopyField(pv_all_stepsizes, copies);
+    deepCopyField(pv_layer_stepsizes, copies);
+    //deepCopyField(pv_all_nsamples, copies); // *stat*
+    //deepCopyField(pv_layer_nsamples, copies); // *stat*
+
+}
+
+void PvGradNNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    inherited::forget();
+
+    pv_gradstats-&gt;forget();
+    pv_all_stepsigns.fill(0);
+    pv_all_stepsizes.fill(pv_initial_stepsize);
+    //pv_all_nsamples.fill(0);
+}
+
+//! Performs the backprop update. Must be called after the fbpropNet.
+void PvGradNNet::bpropUpdateNet(int t)
+{
+    bpropNet(t);
+    pv_gradstats-&gt;update(all_params_gradient);
+
+    int np = all_params.length();
+    int ns;
+    real m, e, prob_pos, prob_neg;
+
+    for(int k=0; k&lt;np; k++) {
+        StatsCollector&amp; st = pv_gradstats-&gt;getStats(k);
+        ns = (int)st.nnonmissing();
+        if(ns&gt;pv_min_samples)   {
+            m = st.mean();
+            e = st.stderror();
+            // test to see if numerical problems
+            if( fabs(m) &lt; 1e-15 || e &lt; 1e-15 )  {
+                cout &lt;&lt; &quot;PvGradNNet::bpropUpdateNet() - small mean-error ratio.&quot; &lt;&lt; endl;
+                continue;
+            }
+
+            // TODO - for current treatment, not necessary to compute actual prob.
+            // Comparing the ratio would be sufficient.
+            prob_pos = gauss_01_cum(m/e);
+            prob_neg = 1.-prob_pos;
+
+            if(!pv_random_sample_step)  {
+    
+                // We adapt the stepsize before taking the step
+                // gradient is positive
+                if(prob_pos&gt;=pv_required_confidence)    {
+                    //pv_all_stepsizes[k] *= (pv_all_stepsigns[k]?pv_acceleration:pv_deceleration);
+                    if(pv_all_stepsigns[k]&gt;0)   {
+                        pv_all_stepsizes[k]*=pv_acceleration;
+                        if( pv_all_stepsizes[k] &gt; pv_max_stepsize )
+                            pv_all_stepsizes[k] = pv_max_stepsize;
+                    }
+                    else if(pv_all_stepsigns[k]&lt;0)  {
+                        pv_all_stepsizes[k]*=pv_deceleration;
+                        if( pv_all_stepsizes[k] &lt; pv_min_stepsize )
+                            pv_all_stepsizes[k] = pv_min_stepsize;
+                    }
+                    all_params[k] -= pv_all_stepsizes[k];
+                    pv_all_stepsigns[k] = 1;
+                    st.forget();
+                }
+                // gradient is negative
+                else if(prob_neg&gt;=pv_required_confidence)   {
+                    //pv_all_stepsizes[k] *= ((!pv_all_stepsigns[k])?pv_acceleration:pv_deceleration);
+                    if(pv_all_stepsigns[k]&lt;0)   {
+                        pv_all_stepsizes[k]*=pv_acceleration;
+                        if( pv_all_stepsizes[k] &gt; pv_max_stepsize )
+                            pv_all_stepsizes[k] = pv_max_stepsize;
+                    }
+                    else if(pv_all_stepsigns[k]&gt;0)  {
+                        pv_all_stepsizes[k]*=pv_deceleration;
+                        if( pv_all_stepsizes[k] &lt; pv_min_stepsize )
+                            pv_all_stepsizes[k] = pv_min_stepsize;
+                    }
+                    all_params[k] += pv_all_stepsizes[k];
+                    pv_all_stepsigns[k] = -1;
+                    st.forget();
+                }
+            }
+            /*else  // random sample update direction (sign)
+            {
+                bool ispos = (random_gen-&gt;binomial_sample(prob_pos)&gt;0);
+                if(ispos) // picked positive
+                    all_params[k] += pv_all_stepsizes[k];
+                else  // picked negative
+                    all_params[k] -= pv_all_stepsizes[k];
+                pv_all_stepsizes[k] *= (pv_all_stepsigns[k]==ispos)?pv_acceleration :pv_deceleration;
+                pv_all_stepsigns[k] = ispos;
+                st.forget();
+            }*/
+        }
+        //pv_all_nsamples[k] = ns; // *stat*
+    }
+
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-03 16:03:41 UTC (rev 8131)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-03 16:05:33 UTC (rev 8132)
@@ -0,0 +1,163 @@
+// -*- C++ -*-
+// PvGradNNet.h
+//
+// Copyright (C) 2007 PA M
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: PA M
+
+/*! \file PvGradNNet.h */
+
+
+#ifndef PvGradNNet_INC
+#define PvGradNNet_INC
+
+#include &lt;plearn_learners/generic/EXPERIMENTAL/mNNet.h&gt;
+
+namespace PLearn {
+
+/**
+ * Multi-layer neural network based on matrix-matrix multiplications.
+ */
+class PvGradNNet : public mNNet
+{
+    typedef mNNet inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Initial size of steps in parameter space
+    real pv_initial_stepsize;
+
+    //! Bounds for the step sizes
+    real pv_min_stepsize, pv_max_stepsize;
+
+    //! Coefficients by which to multiply the step sizes
+    real pv_acceleration, pv_deceleration;
+
+    //! PV's gradient minimum number of samples to estimate confidence
+    int pv_min_samples;
+
+    //! Minimum required confidence (probability of being positive or negative)
+    //! for taking a step.
+    real pv_required_confidence;
+
+    //! If this is set to true, then we will randomly choose the step sign for
+    // each parameter based on the estimated probability of it being positive
+    // or negative.
+    bool pv_random_sample_step;
+
+public:
+    //#####  Public Not Build Options  ########################################
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    PvGradNNet();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(PvGradNNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+    // ### Declare protected option fields (such as learned parameters) here
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+    virtual void bpropUpdateNet(const int t);
+//    virtual void bpropNet(const int t);
+    
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    //! accumulated statistics of gradients on each parameter.
+    PP&lt;VecStatsCollector&gt; pv_gradstats;
+
+    //! Temporary add-on. Allows an undetermined signed value (zero).
+    TVec&lt;int&gt; pv_all_stepsigns;
+    TVec&lt; TMat&lt;int&gt; &gt; pv_layer_stepsigns;
+
+    //! The step size (absolute value) to be taken for each parameter.
+    Vec pv_all_stepsizes;
+    TVec&lt;Mat&gt; pv_layer_stepsizes;
+
+    //! Holds the number of samples gathered for each weight
+    //! This is purely for outputing stats.
+    //TVec&lt;int&gt; pv_all_nsamples;
+    //TVec&lt; TMat&lt;int&gt; &gt; pv_layer_nsamples;
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PvGradNNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001579.html">[Plearn-commits] r8131 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
	<LI>Next message: <A HREF="001581.html">[Plearn-commits] r8133 - trunk/scripts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1580">[ date ]</a>
              <a href="thread.html#1580">[ thread ]</a>
              <a href="subject.html#1580">[ subject ]</a>
              <a href="author.html#1580">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
