<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8190 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-October/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8190%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200710171801.l9HI1VtI030320%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001637.html">
   <LINK REL="Next"  HREF="001639.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8190 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8190%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200710171801.l9HI1VtI030320%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8190 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Wed Oct 17 20:01:31 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001637.html">[Plearn-commits] r8189 - in trunk: python_modules/plearn/parallel	scripts
</A></li>
        <LI>Next message: <A HREF="001639.html">[Plearn-commits] r8191 - trunk/plearn/vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1638">[ date ]</a>
              <a href="thread.html#1638">[ thread ]</a>
              <a href="subject.html#1638">[ subject ]</a>
              <a href="author.html#1638">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-10-17 20:01:31 +0200 (Wed, 17 Oct 2007)
New Revision: 8190

Modified:
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
Log:
Fix some warnings and whitespaces.


Modified: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2007-10-16 16:16:23 UTC (rev 8189)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2007-10-17 18:01:31 UTC (rev 8190)
@@ -55,10 +55,10 @@
     &quot;Some only apply for binomial layers. \n&quot;);
 
 LayerCostModule::LayerCostModule():
+    cost_function(&quot;&quot;),
     histo_size(10),
     alpha(0.0),
     momentum(0.0),
-    cost_function(&quot;&quot;),
     cost_function_completename(&quot;&quot;)
 {
     output_size = 1;
@@ -77,18 +77,18 @@
                   OptionBase::buildoption,
         &quot;The cost function applied to the layer:\n&quot;
         &quot;- \&quot;pascal\&quot;:&quot;
-	                    &quot; Pascal Vincent's God given cost function.\n&quot;
+        &quot; Pascal Vincent's God given cost function.\n&quot;
         &quot;- \&quot;correlation\&quot;:&quot;
-	                    &quot; average of a function applied to the correlations between outputs.\n&quot;
+        &quot; average of a function applied to the correlations between outputs.\n&quot;
         &quot;- \&quot;kl_div\&quot;:&quot;
-	                    &quot; KL divergence between distrubution of outputs (sampled with x)\n&quot;
+        &quot; KL divergence between distrubution of outputs (sampled with x)\n&quot;
         &quot;- \&quot;kl_div_simple\&quot;:&quot;
-	                    &quot; simple version of kl_div where we count at least one sample per histogram's bin\n&quot;
+        &quot; simple version of kl_div where we count at least one sample per histogram's bin\n&quot;
         &quot;- \&quot;stochastic_cross_entropy\&quot; [default]:&quot;
-	                    &quot; average cross-entropy between pairs of binomial units\n&quot;
+        &quot; average cross-entropy between pairs of binomial units\n&quot;
         &quot;- \&quot;stochastic_kl_div\&quot;:&quot;
-	                    &quot; average KL divergence between pairs of binomial units\n&quot;
-                 );
+        &quot; average KL divergence between pairs of binomial units\n&quot;
+        );
 
     declareOption(ol, &quot;histo_size&quot;, &amp;LayerCostModule::histo_size,
                   OptionBase::buildoption,
@@ -144,9 +144,9 @@
     PLASSERT( histo_size &gt; 1 );
     PLASSERT( momentum &gt;= 0.0);
     PLASSERT( momentum &lt; 1);
-    
+
     norm_factor = 1./(real)(input_size*(input_size-1));
-    
+
     string im = lowerstring( cost_function );
     // choose HERE the *default* cost function
     if( im == &quot;&quot; )
@@ -160,7 +160,7 @@
     if( ( cost_function == &quot;stochastic_cross_entropy&quot;)
      || ( cost_function == &quot;stochastic_kl_div&quot;) )
         is_cost_function_stochastic = true;
-        
+
     // list HERE all *non stochastic* cost functions
     // and the specific initialization
     else if( ( cost_function == &quot;kl_div&quot;)
@@ -225,16 +225,16 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(inputs_histo, copies);
-    
+
     deepCopyField(inputs_expectation, copies);
     deepCopyField(inputs_stds, copies);
-    
+
     deepCopyField(inputs_correlations, copies);
     deepCopyField(inputs_cross_quadratic_mean, copies);
-    
+
     deepCopyField(inputs_expectation_trainMemory, copies);
     deepCopyField(inputs_cross_quadratic_mean_trainMemory, copies);
-    
+
     deepCopyField(ports, copies);
 }
 
@@ -267,7 +267,7 @@
 {
     int n_samples = inputs.length();
     costs.resize( n_samples, output_size );
-    
+
     if( !is_cost_function_stochastic )
     {
         costs.clear(); // costs(i,0) = 0
@@ -278,7 +278,7 @@
         //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
         //! between probabilities of outputs vectors for all units
         //! ************************************************************
-        //! 
+        //!
         //!      cost = - MEAN_{i,j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
         //!
         //!           = - MEAN_{i,j#i} SUM_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
@@ -300,7 +300,7 @@
         //! ************************************************************
 
             computeHisto(inputs);
-            
+
             costs(0,0) = computeKLdiv();
         }
         else if( cost_function == &quot;kl_div_simple&quot; )
@@ -315,7 +315,7 @@
         //! ************************************************************
 
             computeSafeHisto(inputs);
-            
+
             // Computing the KL divergence
             for (int i = 0; i &lt; input_size; i++)
                 for (int j = 0; j &lt; i; j++)
@@ -331,7 +331,7 @@
         //! a Pascal Vincent's god-given similarity measure
         //! between outputs vectors for all units
         //! ************************************************************
-        //! 
+        //!
         //!      cost = MEAN_{i,j#i} f( Ex[q{i}.q{j}] ) - alpha. MEAN_{i} f( Ex[q{i}] )
         //!
         //! where  q{i} = P(h{i}=1|x): output of the i^th units of the layer
@@ -340,7 +340,7 @@
         //! ************************************************************
 
             computePascalStatistics(inputs);
-                                    
+
             // Computing the cost
             for (int i = 0; i &lt; input_size; i++)
             {
@@ -370,7 +370,7 @@
         //! ************************************************************
 
             computeCorrelationStatistics(inputs);
-                        
+
             // Computing the cost
             for (int i = 0; i &lt; input_size; i++)
                 for (int j = 0; j &lt; i; j++)
@@ -379,10 +379,10 @@
             costs(0,0) *= norm_factor;
         }
 
-        
+
         return; // Do not fprop with the conventional stochastic fprop...
     }
-    
+
     for (int isample = 0; isample &lt; n_samples; isample++)
         fprop(inputs(isample), costs(isample,0));
 }
@@ -394,8 +394,8 @@
 
     cost = 0.0;
     real  qi, qj, comp_qi, comp_qj; // The outputs (units i,j)
-                                    // and some basic operations on it (e.g.: 1-qi, qi/(1-qi)) 
-                                      
+                                    // and some basic operations on it (e.g.: 1-qi, qi/(1-qi))
+
     if( cost_function == &quot;stochastic_cross_entropy&quot; )
     {
     //! ************************************************************
@@ -421,10 +421,10 @@
            {
                qj = input[j];
                comp_qj = 1.0 - qj;
-               
+
                // H(pi||pj) = H(pi) + D_{KL}(pi||pj)
                cost += qi*safeflog(qj) + comp_qi*safeflog(comp_qj);
-               
+
                // The symetric part (loop  j=i+1...size)
                cost += qj*safeflog(qi) + comp_qj*safeflog(comp_qi);
            }
@@ -432,7 +432,7 @@
         // Normalization w.r.t. number of units
         cost *= norm_factor;
     }
-    
+
     else if( cost_function == &quot;stochastic_kl_div&quot; )
     {
     //! ************************************************************
@@ -457,7 +457,7 @@
                comp_qi = REAL_MAX;
            else
                comp_qi = qi/(1.0 - qi);
-       
+
            for( int j = 0; j &lt; i; j++ )
            {
                qj = input[j];
@@ -465,13 +465,13 @@
                    comp_qj = REAL_MAX;
                else
                    comp_qj = qj/(1.0 - qj);
-               
+
                //     - D_{KL}(pi||pj) - D_{KL}(pj||pi)
                cost += (qj-qi)*safeflog(comp_qi/comp_qj);
            }
         }
         // Normalization w.r.t. number of units
-        cost *= norm_factor;   
+        cost *= norm_factor;
     }
 
     else
@@ -598,7 +598,7 @@
         else if( cost_function == &quot;kl_div&quot; )
         {
             computeHisto(*p_inputs);
-            
+
             for (int isample = 0; isample &lt; n_samples; isample++)
             {
 
@@ -607,7 +607,7 @@
                 for (int i = 0; i &lt; input_size; i++)
                 {
                     (*p_inputs_grad)(isample, i) = 0.0;
-                    
+
                     qi = (*p_inputs)(isample,i);
                     int index_i = histo_index(qi);
                     if( ( index_i == histo_size-1 ) ) // we do not care about this...
@@ -620,11 +620,11 @@
                         shift_i = -1;
                     // qi + dq(qi) ==&gt; | p_inputs_histo(i,index_i)   - one_count
                     //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
-                    
+
                     for (int j = 0; j &lt; i; j++)
                     {
                         (*p_inputs_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi);
-                        
+
                         qj = (*p_inputs)(isample,j);
                         int index_j = histo_index(qj);
                         if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
@@ -637,7 +637,7 @@
                             shift_j = -1;
                             // qj + dq(qj) ==&gt; | p_inputs_histo(j,index_j)   - one_count
                           //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-                        
+
                         (*p_inputs_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj);
                     }
                 }
@@ -648,8 +648,8 @@
                     (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0) * norm_factor;
                 }
             }
-            
-            
+
+
             // debug Check
             int i=0;
             real cost_before = computeKLdiv();
@@ -661,19 +661,19 @@
                   (*p_inputs)(isample,i) += dq(qi);
                   computeHisto(*p_inputs);
                   real cost_after = computeKLdiv();
-                  (*p_inputs)(isample,i) -= dq(qi);                  
+                  (*p_inputs)(isample,i) -= dq(qi);
                   cout &lt;&lt; &quot;\tglobal cost comparison:&quot; &lt;&lt; cost_after - cost_before;
                   cout &lt;&lt; &quot;  &lt;?&gt;  &quot; &lt;&lt; (*p_inputs_grad)(isample, i)*dq(qi) &lt;&lt; endl;
                 }
             }
-            
-            
+
+
         }
 
         else if( cost_function == &quot;kl_div_simple&quot; )
         {
             computeSafeHisto(*p_inputs);
-            
+
             for (int isample = 0; isample &lt; n_samples; isample++)
             {
 
@@ -682,7 +682,7 @@
                 for (int i = 0; i &lt; input_size; i++)
                 {
                     (*p_inputs_grad)(isample, i) = 0.0;
-                    
+
                     qi = (*p_inputs)(isample,i);
                     int index_i = histo_index(qi);
                     if( ( index_i == histo_size-1 ) ) // we do not care about this...
@@ -695,11 +695,11 @@
                         shift_i = -1;
                     // qi + dq(qi) ==&gt; | p_inputs_histo(i,index_i)   - one_count
                     //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
-                    
+
                     for (int j = 0; j &lt; i; j++)
                     {
                         (*p_inputs_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi);
-                        
+
                         qj = (*p_inputs)(isample,j);
                         int index_j = histo_index(qj);
                         if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
@@ -712,7 +712,7 @@
                             shift_j = -1;
                             // qj + dq(qj) ==&gt; | p_inputs_histo(j,index_j)   - one_count
                           //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-                        
+
                         (*p_inputs_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
                     }
                 }
@@ -728,7 +728,7 @@
         else if( cost_function == &quot;pascal&quot; )
         {
             computePascalStatistics(*p_inputs);
-            
+
             if( momentum &gt; 0.0 )
                 for (int isample = 0; isample &lt; n_samples; isample++)
                 {
@@ -780,7 +780,7 @@
         else if( cost_function == &quot;correlation&quot;)
         {
             computeCorrelationStatistics(*p_inputs);
-            
+
             if( momentum &gt; 0.0 )
                 PLERROR( &quot;not implemented yet&quot;);
             else
@@ -789,13 +789,13 @@
                     Vec dSTDi_dqi, dCROSSij_dqj;
                     dSTDi_dqi.resize( input_size );
                     dCROSSij_dqj.resize( input_size );
-                    
+
                     for (int i = 0; i &lt; input_size; i++)
                     {
                         qi = (*p_inputs)(isample, i);
 
                         //!  dCROSSij_dqj[i] = d[ E(QiQj)-E(Qi)E(Qj) ]/d[qj(t)]
-                        //!                  = ( qi(t) - E(Qi) ) / n_samples 
+                        //!                  = ( qi(t) - E(Qi) ) / n_samples
                         //!
                         //!  dSTDi_dqi[i] = d[ STD(Qi) ]/d[qi(t)]
                         //!               = d[ sqrt( E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
@@ -806,7 +806,7 @@
                         //!
                         dCROSSij_dqj[i] = ( qi - inputs_expectation[i] )*one_count;
                         dSTDi_dqi[i] = dCROSSij_dqj[i] / inputs_stds[i];
-                        
+
                         for (int j = 0; j &lt; i; j++)
                         {
                             qj = (*p_inputs)(isample,j);
@@ -815,13 +815,13 @@
                             real dfunc_dCorr = deriv_func_correlation( inputs_correlations(i,j) );
                             real correlation_num = ( inputs_cross_quadratic_mean(i,j)
                                                      - inputs_expectation[i]*inputs_expectation[j] );
-                                  
-                            (*p_inputs_grad)(isample, i) += dfunc_dCorr * ( 
+
+                            (*p_inputs_grad)(isample, i) += dfunc_dCorr * (
                                                                     correlation_denum * dCROSSij_dqj[j]
                                                                   - correlation_num * dSTDi_dqi[i] * inputs_stds[j]
                                                                     ) / (correlation_denum * correlation_denum);
 
-                            (*p_inputs_grad)(isample, j) += dfunc_dCorr * ( 
+                            (*p_inputs_grad)(isample, j) += dfunc_dCorr * (
                                                                     correlation_denum * dCROSSij_dqj[i]
                                                                   - correlation_num * dSTDi_dqi[j] * inputs_stds[i]
                                                                     ) / (correlation_denum * correlation_denum);
@@ -853,10 +853,10 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
-    
-    inputs_expectation.clear(); 
-    inputs_cross_quadratic_mean.clear(); 
 
+    inputs_expectation.clear();
+    inputs_cross_quadratic_mean.clear();
+
     for (int isample = 0; isample &lt; n_samples; isample++)
     {
         input = inputs(isample);
@@ -867,7 +867,7 @@
                  inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
         }
     }
-    
+
     for (int i = 0; i &lt; input_size; i++)
     {
         inputs_expectation[i] *= one_count;
@@ -912,9 +912,9 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
-    
+
     inputs_expectation.clear();
-    inputs_cross_quadratic_mean.clear(); 
+    inputs_cross_quadratic_mean.clear();
     inputs_correlations.clear();
 
     for (int isample = 0; isample &lt; n_samples; isample++)
@@ -928,7 +928,7 @@
                  inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
         }
     }
-    
+
     for (int i = 0; i &lt; input_size; i++)
     {
         //! Normalization
@@ -951,7 +951,7 @@
         }
     }
     //! Be careful: 'inputs_correlations' matrix is only computed
-    //!  on the triangle subpart 'i' &gt; 'j' 
+    //!  on the triangle subpart 'i' &gt; 'j'
     //!  ('i'/'j': first/second argument)
 
     if(  during_training )
@@ -985,7 +985,7 @@
     real grad_update = 0.0;
 
     real Ni_ki = inputs_histo(i,index_i);
-    real Ni_ki_shift1 = inputs_histo(i,index_i+1);            
+    real Ni_ki_shift1 = inputs_histo(i,index_i+1);
     real Nj_ki        = inputs_histo(j,index_i);
     real Nj_ki_shift1 = inputs_histo(j,index_i+1);
 
@@ -1009,7 +1009,7 @@
     {
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki, Nj_ki ) *over_dq;
-                                                               
+
         if( fast_exact_is_equal(Ni_ki, one_count) )
         {
             differ_count_j_after = Nj_ki;
@@ -1018,19 +1018,19 @@
         else
             // adding the term of the sum with its modified value
             grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki )
-	                   *over_dq;
+                           *over_dq;
 
         if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
         {
             // adding the term of the sum with its modified value
             grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after )
-	                  *(real)(1+n_differ_j_after)*over_dq ;
+                          *(real)(1+n_differ_j_after)*over_dq ;
 
             if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // &quot;cas ou on regroupe avec le dernier&quot;;
             {
                 // removing the term of the sum that will be modified
                 grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )
-		               *over_dq;                
+                               *over_dq;
             }
             else
             {
@@ -1048,15 +1048,17 @@
                 if( ki &lt; histo_size )
                 {
                     grad_update -= KLdivTerm( inputs_histo( i, ki ), differ_count_j_before )
-		                   *(real)(1+n_differ_j_before)*over_dq;
+                                   *(real)(1+n_differ_j_before)*over_dq;
 
-                    if( differ_count_j_before &gt; Nj_ki_shift1 )                
+                    if( differ_count_j_before &gt; Nj_ki_shift1 )
                         grad_update += KLdivTerm( inputs_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )
-			               *(real)(n_differ_j_before)*over_dq;
+                                       *(real)(n_differ_j_before)*over_dq;
                         // pb avec differ_count_j_after plus haut??? semble pas
                 }
                 else
-                    &quot;cas ou on regroupe avec le dernier (easy)&quot;;
+                {
+                    // cas ou on regroupe avec le dernier (easy)
+                }
             }
         }
         else
@@ -1081,14 +1083,16 @@
             if( kj &lt; histo_size )
             {
                 grad_update += KLdivTerm( differ_count_i_after, inputs_histo( j, kj ) )
-		               *(real)(1+n_differ_i_after)*over_dq;
+                               *(real)(1+n_differ_i_after)*over_dq;
 
                 if( differ_count_i_before &gt; 0 )
                     grad_update -= KLdivTerm( differ_count_i_before, inputs_histo( j, kj ) )
-		                   *(real)(1+n_differ_i_before)*over_dq;
+                                   *(real)(1+n_differ_i_before)*over_dq;
             }
             else
-                &quot;cas ou on regroupe avec le dernier&quot;;   
+            {
+                // cas ou on regroupe avec le dernier
+            }
         }
     }
     return grad_update*over_dq;
@@ -1099,25 +1103,25 @@
     //PLASSERT( over_dq &gt; 0.0 )
 
     real grad_update = 0.0;
-                    
+
     real Ni_ki = inputs_histo(i,index_i);
     PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
                                                   // if inputs_histo is up to date,
                                                   // the input(isample,i) has been counted
     real Ni_ki_shift1 = inputs_histo(i,index_i+1);
-                    
+
     real Nj_ki        = inputs_histo(j,index_i);
     real Nj_ki_shift1 = inputs_histo(j,index_i+1);
 
 
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki, Nj_ki );
-                                                               
+
         // adding the term of the sum with its modified value
         grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
 
         grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1 );
-        
+
         grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );
 
     return grad_update *over_dq;
@@ -1135,7 +1139,7 @@
             for (int i = 0; i &lt; input_size; i++)
                 for (int j = 0; j &lt; i; j++)
                 {
-                    // These variables are used in case one bin of 
+                    // These variables are used in case one bin of
                     // the histogram is empty for one unit
                     // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
                     // In such case, we ''differ'' the count for the next bin and so on.
@@ -1171,22 +1175,22 @@
                         }
                     }
                     if( differ_count_i &gt; 0.0 )
-                    {   
-                        &quot;cas ou on regroupe avec le dernier&quot;;   
+                    {
+                        // cas ou on regroupe avec le dernier
 //                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
 //                                  *(real)(1+last_n_differ) *HISTO_STEP;
 //                        cost += KLdivTerm(last_positive_Ni_k+differ_count_i,last_positive_Nj_k)
-//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP; 
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
                     }
-                     
+
                     else if ( differ_count_j &gt; 0.0 )
                     {
-                        &quot;cas ou on regroupe avec le dernier&quot;;
+                        // cas ou on regroupe avec le dernier
 //                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
 //                                 *(real)(1+last_n_differ) *HISTO_STEP;
 //                        cost += KLdivTerm(last_positive_Ni_k,last_positive_Nj_k+differ_count_j)
 //                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
-                    }    
+                    }
                 }
             // Normalization w.r.t. number of units
             return cost *norm_factor;
@@ -1197,8 +1201,8 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
-    
-    inputs_histo.clear(); 
+
+    inputs_histo.clear();
     for (int isample = 0; isample &lt; n_samples; isample++)
     {
         input = inputs(isample);
@@ -1214,7 +1218,7 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)(n_samples+histo_size);
     Vec input;
-    
+
     inputs_histo.fill(one_count);
 
     for (int isample = 0; isample &lt; n_samples; isample++)
@@ -1272,7 +1276,7 @@
 
     inputs_expectation.clear();
     inputs_stds.clear();
-    
+
     inputs_correlations.clear();
     inputs_cross_quadratic_mean.clear();
     if( momentum &gt; 0.0)

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-10-16 16:16:23 UTC (rev 8189)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-10-17 18:01:31 UTC (rev 8190)
@@ -57,11 +57,11 @@
     //#####  Public Build Options  ############################################
 
     string cost_function;
-    
+
     int histo_size;
-    
+
     real alpha;
-    
+
     real momentum;
 
     //#####  Public Learnt Options  ###########################################
@@ -75,13 +75,13 @@
     //! or (for some) 'pascal'
     Vec inputs_expectation;
     Vec inputs_stds;         //! only for 'correlation' cost function
-    
+
     Mat inputs_correlations; //! only for 'correlation' cost function
     Mat inputs_cross_quadratic_mean;
 
     //! The generic name of the cost function
     string cost_function_completename;
-    
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -93,7 +93,7 @@
     virtual void fprop(const Mat&amp; inputs, Mat&amp; costs);
     //! Overridden.
     virtual void fprop(const TVec&lt;Mat*&gt;&amp; ports_value);
-    
+
     //! backpropagate the derivative w.r.t. activation
     virtual void bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
                                 const TVec&lt;Mat*&gt;&amp; ports_gradient);
@@ -163,14 +163,14 @@
     //! Range of a histogram's bin ( HISTO_STEP = 1/histo_size )
     real HISTO_STEP;
     //! the weight of a sample within a batch (usually, 1/n_samples)
-    real one_count; 
+    real one_count;
 
     //! Variables for (non stochastic) Pascal's/correlation function
     //! -------------------------------------------------------------
-    //! Statistics on outputs (estimated empiricially on the data)    
+    //! Statistics on outputs (estimated empiricially on the data)
     Vec inputs_expectation_trainMemory;
     Mat inputs_cross_quadratic_mean_trainMemory;
-        
+
     //! Map from a port name to its index in the 'ports' vector.
     map&lt;string, int&gt; portname_to_index;
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001637.html">[Plearn-commits] r8189 - in trunk: python_modules/plearn/parallel	scripts
</A></li>
	<LI>Next message: <A HREF="001639.html">[Plearn-commits] r8191 - trunk/plearn/vmat
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1638">[ date ]</a>
              <a href="thread.html#1638">[ thread ]</a>
              <a href="subject.html#1638">[ subject ]</a>
              <a href="author.html#1638">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
