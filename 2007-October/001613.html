<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8165 - trunk/plearn_learners/generic/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-October/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8165%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200710101534.l9AFYUkG018962%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001612.html">
   <LINK REL="Next"  HREF="001614.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8165 - trunk/plearn_learners/generic/EXPERIMENTAL</H1>
    <B>manzagop at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8165%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200710101534.l9AFYUkG018962%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8165 - trunk/plearn_learners/generic/EXPERIMENTAL">manzagop at mail.berlios.de
       </A><BR>
    <I>Wed Oct 10 17:34:30 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001612.html">[Plearn-commits] r8164 - trunk/python_modules/plearn/parallel
</A></li>
        <LI>Next message: <A HREF="001614.html">[Plearn-commits] r8166 - in trunk: python_modules/plearn/parallel	scripts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1613">[ date ]</a>
              <a href="thread.html#1613">[ thread ]</a>
              <a href="subject.html#1613">[ subject ]</a>
              <a href="author.html#1613">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: manzagop
Date: 2007-10-10 17:34:30 +0200 (Wed, 10 Oct 2007)
New Revision: 8165

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
Log:
- Bug fix: n_updates was not properly computed.
- Added the neuronDiscountGrad (now strategy 3) to perform specific
discounting to neighbour weights (within a neuron).



Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-10 13:27:19 UTC (rev 8164)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-10 15:34:30 UTC (rev 8165)
@@ -61,6 +61,7 @@
       pv_random_sample_step(false),
       pv_self_discount(0.5),
       pv_other_discount(0.95),
+      pv_within_neuron_discount(0.95),
       n_updates(0)
 {
     random_gen = new PRandom();
@@ -128,6 +129,12 @@
                   &quot;Discount used to perform soft invalidation of other weights'\n&quot; 
                   &quot;statistics after a weight update.&quot;);
 
+    declareOption(ol, &quot;pv_within_neuron_discount&quot;,
+                  &amp;PvGradNNet::pv_within_neuron_discount,
+                  OptionBase::buildoption,
+                  &quot;Discount used to perform soft invalidation of other weights'\n&quot;
+                  &quot;(same neuron) statistics after a weight update.&quot;);
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -149,6 +156,7 @@
     pv_layer_stepsigns.resize(n_layers-1);
     pv_layer_stepsizes.resize(n_layers-1);
     int np;
+    int n_neurons=0;
     for (int i=0,p=0;i&lt;n_layers-1;i++)  {
         np=layer_sizes[i+1]*(1+layer_sizes[i]);
         pv_layer_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
@@ -157,10 +165,10 @@
         pv_layer_stepsigns[i]=pv_all_stepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         pv_layer_stepsizes[i]=pv_all_stepsizes.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         p+=np;
+        n_neurons+=layer_sizes[i+1];
     }
+    n_neuron_updates.resize(n_neurons);
 
-//    pv_gradstats = new VecStatsCollector();
-
 }
 
 // ### Nothing to add here, simply calls build_
@@ -185,7 +193,7 @@
     deepCopyField(pv_layer_stepsigns, copies);
     deepCopyField(pv_all_stepsizes, copies);
     deepCopyField(pv_layer_stepsizes, copies);
-
+    deepCopyField(n_neuron_updates, copies);
 //    deepCopyField(pv_gradstats, copies);
 }
 
@@ -204,7 +212,8 @@
 
     // used in the discountGrad() strategy
     n_updates = 0; 
-    
+    n_neuron_updates.fill(0);    
+
 //    pv_gradstats-&gt;forget();
 }
 
@@ -221,9 +230,12 @@
             discountGrad();
             break;
         case 3 :
+            neuronDiscountGrad();
+            break;
+        case 4 :
             globalSyncGrad();
             break;
-        case 4 :
+        case 5 :
             neuronSyncGrad();
             break;
         default :
@@ -373,9 +385,8 @@
 
             // TODO - for current treatment, not necessary to compute actual
             // prob. Comparing the ratio would be sufficient.
-/*            prob_pos = gauss_01_cum(m/e);
+            /*prob_pos = gauss_01_cum(m/e);
             prob_neg = 1.-prob_pos;
-
             if(prob_pos&gt;=pv_required_confidence)
                 stepsign = 1;
             else if(prob_neg&gt;=pv_required_confidence)
@@ -413,10 +424,103 @@
             pv_all_nsamples[k]*=sd;
             pv_all_sum[k]*=sd;
             pv_all_sumsquare[k]*=sd;
+            n_updates++;
+
         }
     }
 }
 
+//! Same as discountGrad but also performs discount based on within neuron 
+//! relationships
+void PvGradNNet::neuronDiscountGrad()
+{
+    real m, e;//, prob_pos, prob_neg;
+    int stepsign;
+
+    real ratio;
+    real limit_ratio = gauss_01_quantile(pv_required_confidence);
+
+    //
+    real discount = pow(pv_other_discount,n_updates);
+    real d;
+    n_updates = 0;
+    if( discount &lt; 0.001 )
+        PLWARNING(&quot;PvGradNNet::discountGrad() - discount &lt; 0.001 - that seems small...&quot;);
+    real sd = pv_self_discount / pv_other_discount; // trick: apply this self discount
+                                                    // and then discount
+                                                    // everyone the same
+    sd /= pv_within_neuron_discount;
+
+    // k is an index on all the parameters.
+    // kk is an index on all neurons.
+    for(int l=0,k=0,kk=0; l&lt;n_layers-1; l++)    {
+        for(int n=0; n&lt;layer_sizes[l+1]; n++,kk++)   {
+            d = discount * pow(pv_within_neuron_discount,n_neuron_updates[kk]);
+            n_neuron_updates[kk]=0;
+            for(int w=0; w&lt;1+layer_sizes[l]; w++,k++)   {
+
+                // Perform soft invalidation
+                pv_all_nsamples[k] *= d;
+                pv_all_sum[k] *= d;
+                pv_all_sumsquare[k] *= d;
+
+                // update stats
+                pv_all_nsamples[k]++;
+                pv_all_sum[k] += all_params_gradient[k];
+                pv_all_sumsquare[k] += all_params_gradient[k] * all_params_gradient[k];
+
+                if(pv_all_nsamples[k]&gt;pv_min_samples)   {
+                    m = pv_all_sum[k] / pv_all_nsamples[k];
+                    e = real((pv_all_sumsquare[k] - square(pv_all_sum[k])/pv_all_nsamples[k])/(pv_all_nsamples[k]-1));
+                    e = sqrt(e/pv_all_nsamples[k]);
+
+                    // test to see if numerical problems
+                    if( fabs(m) &lt; 1e-15 || e &lt; 1e-15 )  {
+                        cout &lt;&lt; &quot;PvGradNNet::bpropUpdateNet() - small mean-error ratio.&quot; &lt;&lt; endl;
+                        continue;
+                    }
+
+                    ratio=m/e;
+                    if(ratio&gt;=limit_ratio)
+                        stepsign = 1;
+                    else if(ratio&lt;=-limit_ratio)
+                        stepsign = -1;
+                    else
+                        continue;
+
+                    // consecutive steps of same sign, accelerate
+                    if( stepsign*pv_all_stepsigns[k]&gt;0 )  {
+                        pv_all_stepsizes[k]*=pv_acceleration;
+                        if( pv_all_stepsizes[k] &gt; pv_max_stepsize )
+                            pv_all_stepsizes[k] = pv_max_stepsize;
+                    // else if different signs decelerate
+                    }   else if( stepsign*pv_all_stepsigns[k]&lt;0 )   {
+                        pv_all_stepsizes[k]*=pv_deceleration;
+                        if( pv_all_stepsizes[k] &lt; pv_min_stepsize )
+                            pv_all_stepsizes[k] = pv_min_stepsize;
+                    // else (previous sign was undetermined
+                    }//   else    {
+                    //}
+                    // step
+                    if( stepsign &gt; 0 )
+                        all_params[k] -= pv_all_stepsizes[k];
+                    else
+                        all_params[k] += pv_all_stepsizes[k];
+                    pv_all_stepsigns[k] = stepsign;
+                    // soft invalidation of self
+                    pv_all_nsamples[k]*=sd;
+                    pv_all_sum[k]*=sd;
+                    pv_all_sumsquare[k]*=sd;
+                    n_updates++;
+                    n_neuron_updates[kk]++;
+                }
+            //////////////////////////////////
+            }
+        }
+    }
+
+}
+
 void PvGradNNet::globalSyncGrad()
 {
 }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-10 13:27:19 UTC (rev 8164)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-10 15:34:30 UTC (rev 8165)
@@ -78,7 +78,7 @@
 
     //! For the discounting strategy. Used to discount stats when there are
     //! updates
-    real pv_self_discount, pv_other_discount;
+    real pv_self_discount, pv_other_discount, pv_within_neuron_discount;
 
 public:
     //#####  Public Not Build Options  ########################################
@@ -122,6 +122,7 @@
 
     void pvGrad(); 
     void discountGrad();
+    void neuronDiscountGrad();
     void globalSyncGrad();
     void neuronSyncGrad();
     
@@ -156,6 +157,8 @@
     // int is enough?
     int n_updates;
 
+    TVec&lt;int&gt; n_neuron_updates;
+
     //! accumulated statistics of gradients on each parameter.
     //PP&lt;VecStatsCollector&gt; pv_gradstats;
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001612.html">[Plearn-commits] r8164 - trunk/python_modules/plearn/parallel
</A></li>
	<LI>Next message: <A HREF="001614.html">[Plearn-commits] r8166 - in trunk: python_modules/plearn/parallel	scripts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1613">[ date ]</a>
              <a href="thread.html#1613">[ thread ]</a>
              <a href="subject.html#1613">[ subject ]</a>
              <a href="author.html#1613">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
