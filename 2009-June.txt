From plearner at mail.berlios.de  Tue Jun  2 23:51:39 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 2 Jun 2009 23:51:39 +0200
Subject: [Plearn-commits] r10225 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200906022151.n52Lpd6P017145@sheep.berlios.de>

Author: plearner
Date: 2009-06-02 23:51:38 +0200 (Tue, 02 Jun 2009)
New Revision: 10225

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
Log:
minor fix to experimental learner

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2009-05-30 15:51:50 UTC (rev 10224)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2009-06-02 21:51:38 UTC (rev 10225)
@@ -103,8 +103,17 @@
 
     declareOption(ol, "reconstructed_layers", &DeepReconstructorNet::reconstructed_layers,
                   OptionBase::buildoption,
-                  "reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]");
+                  "reconstructed_layers[k] is the reconstruction of layer k from hidden_for_reconstruction[k]\n"
+                  "(which corresponds to a version of layer k+1. See further explanation of hidden_for_reconstruction.\n");
 
+    declareOption(ol, "hidden_for_reconstruction", &DeepReconstructorNet::hidden_for_reconstruction,
+                  OptionBase::buildoption,
+                  "reconstructed_layers[k] is reconstructed from hidden_for_reconstruction[k]\n"
+                  "which corresponds to a version of layer k+1.\n"
+                  "hidden_for_reconstruction[k] can however be different from layers[k+1] \n"
+                  "since e.g. layers[k+1] may be obtained by transforming a clean input, while \n"
+                  "hidden_for_reconstruction[k] may be obtained by transforming a corrupted input \n");
+
     declareOption(ol, "reconstruction_optimizers", &DeepReconstructorNet::reconstruction_optimizers,
                   OptionBase::buildoption,
                   "");
@@ -267,6 +276,48 @@
     Func f(input&target, fullcost);
     parameters = f->parameters;
     outmat.resize(n_rec_costs);
+
+
+    // older versions did not specify hidden_for_reconstruction
+    // if it's not there, let's try to infer it
+    if( (reconstructed_layers.length()!=0) && (hidden_for_reconstruction.length()==0) ) 
+    {
+        int n = reconstructed_layers.length();
+        for(int k=0; k<n; k++)
+        {
+            VarArray proppath = propagationPath(layers[k+1],reconstructed_layers[k]);
+            if(proppath.length()>0) // great, we found a path from layers[k+1] !
+                hidden_for_reconstruction.append(layers[k+1]);
+            else // ok this is getting much more difficult, let's try to guess
+            {
+                // let's consider the full path from sources to reconstructed_layers[k]
+                VarArray fullproppath = propagationPath(reconstructed_layers[k]);
+                // look for a variable with same type and dimension as layers[k+1]
+                int pos;
+                for(pos = fullproppath.length()-2; pos>=0; pos--)
+                {
+                    if( fullproppath[pos]->length()    == layers[k+1]->length() &&
+                        fullproppath[pos]->width()     == layers[k+1]->width() &&
+                        fullproppath[pos]->classname() == layers[k+1]->classname() )
+                        break; // found a matching one!
+                }
+                if(pos>=0) // found a match at pos, let's use it
+                {
+                    hidden_for_reconstruction.append(fullproppath[pos]);
+                    perr << "Found match for hidden_for_reconstruction " << k << endl;
+                    //displayVarGraph(propagationPath(hidden_for_reconstruction[k],reconstructed_layers[k])
+                    //                ,true, 333, "reconstr");        
+                }
+                else
+                {
+                    PLERROR("Unable to guess hidden_for_reconstruction variable. Unable to find match.");
+                }
+            }
+        }
+    }
+
+    if( reconstructed_layers.length() != hidden_for_reconstruction.length() )
+        PLERROR("reconstructed_layers and hidden_for_reconstruction should have the same number of elements.");
 }
 
 // ### Nothing to add here, simply calls build_
@@ -301,6 +352,7 @@
     deepCopyField(layers, copies);
     deepCopyField(reconstruction_costs, copies);
     deepCopyField(reconstructed_layers, copies);
+    deepCopyField(hidden_for_reconstruction, copies);
     deepCopyField(reconstruction_optimizers, copies);
     deepCopyField(reconstruction_optimizer, copies);
     varDeepCopyField(target, copies);
@@ -483,15 +535,34 @@
     return representations;
 }
 
+
 void DeepReconstructorNet::reconstructInputFromLayer(int layer)
 {
     for(int k=layer; k>0; k--)
+        layers[k-1]->matValue << reconstructOneLayer(k);
+    /*
+    for(int k=layer; k>0; k--)
     {
-        VarArray proppath = propagationPath(layers[k],reconstructed_layers[k-1]);
+        VarArray proppath = propagationPath(hidden_for_reconstruction[k-1],reconstructed_layers[k-1]);
+
+        perr << "RECONSTRUCTING reconstructed_layers["<<k-1
+             << "] from layers["<< k
+             << "] " << endl;
+        perr << "proppath:" << endl;
+        perr << proppath << endl;
+        perr << "proppath length: " << proppath.length() << endl;
+
+        //perr << ">>>> reconstructed layers before fprop:" << endl;
+        //perr << reconstructed_layers[k-1]->matValue << endl;
+
         proppath.fprop();
-        // perr << "Graph for reconstructing layer " << k-1 << " from layer " << k << endl;
-        //displayVarGraph(proppath,true, 333, "reconstr");
 
+        //perr << ">>>> reconstructed layers after fprop:" << endl;
+        //perr << reconstructed_layers[k-1]->matValue << endl;
+
+        perr << "Graph for reconstructing layer " << k-1 << " from layer " << k << endl;
+        displayVarGraph(proppath,true, 333, "reconstr");        
+
         //WARNING MEGA-HACK
         if (reconstructed_layers[k-1].width() == 2*layers[k-1].width())
         {
@@ -505,6 +576,7 @@
         else
             reconstructed_layers[k-1]->matValue >> layers[k-1]->matValue;
     }
+    */
 }
 
 TVec<Mat> DeepReconstructorNet::computeReconstructions(Mat input)
@@ -832,7 +904,8 @@
 
 Mat DeepReconstructorNet::reconstructOneLayer(int layer)
 {
-    VarArray proppath = propagationPath(layers[layer],reconstructed_layers[layer-1]);
+    layers[layer]->matValue >> hidden_for_reconstruction[layer-1]->matValue;
+    VarArray proppath = propagationPath(hidden_for_reconstruction[layer-1],reconstructed_layers[layer-1]);
     proppath.fprop();       
     return reconstructed_layers[layer-1]->matValue;
 }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2009-05-30 15:51:50 UTC (rev 10224)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2009-06-02 21:51:38 UTC (rev 10225)
@@ -88,6 +88,10 @@
     // reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]
     VarArray reconstructed_layers;
 
+    // hidden_for_reconstruction[k] is the hidden representation used to reconstruct reconstructed_layers[k] 
+    // i.e. it is the representation at layer k+1 but possibly obtained from a corrupted input (contrary to layers[k+1]).
+    VarArray hidden_for_reconstruction;
+
     // optimizers if we use different ones for each layer
     TVec< PP<Optimizer> > reconstruction_optimizers;
     



From plearner at mail.berlios.de  Tue Jun  2 23:55:31 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Tue, 2 Jun 2009 23:55:31 +0200
Subject: [Plearn-commits] r10226 - trunk/scripts/EXPERIMENTAL
Message-ID: <200906022155.n52LtVOf017321@sheep.berlios.de>

Author: plearner
Date: 2009-06-02 23:55:31 +0200 (Tue, 02 Jun 2009)
New Revision: 10226

Modified:
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
minor fixes to experimental script


Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-02 21:51:38 UTC (rev 10225)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-02 21:55:31 UTC (rev 10226)
@@ -28,6 +28,7 @@
     print "deepnetplot.py plotSingleMatrix x.psave "
     print "deepnetplot.py plotEachRow learner.psave chars.pmat"
     print "deepnetplot.py plotRepAndRec learner.psave chars.pmat"
+    print "deepnetplot.py interact learner.psave chars.pmat"
     print "deepnetplot.py help"
     print ""
     print "where learner.psave is a .psave of a DeepReconstructorNet object"
@@ -45,6 +46,11 @@
     print 'F : fproping form the last layer and for the next ones'
     print 'r : reconstructing this layer from the next one'
     print 'R : reconstructing this layer from the next one and also the previous ones'
+    print 'k : choose the value for k (will be prompted in console)'
+    print 'K : keep only the top k pixels, and set all others to zero'
+    print 'p : choose the value for p (will be prompted in console)'
+    print 'P : generate from a factorial bernoulli with parameter p'
+    print 'G : generate from current layer: sampling from fact bernoulli and reconstructing'
     print 'm : set the max of each group of the current hidden layer to 1 and the other elements of the group to 0'
     print 's : each group of the current layer is sampled (each group has to sum to 1.0)'
     print 'S : samples the current layer, then reconstruct the previous layer, thant sample this reconstructed layer, and continues until the input'
@@ -53,7 +59,7 @@
     print 'c : set the current pixel to 0.5'
     print 'v : set the current pixel to 0.75'
     print 'b : set the current pixel to 1.0'
-    print ' space-bar : print value and position of the current pixel'
+    print ' space-bar : print value and position of the current pixel (printed in console)'
     print 'i : print values of the current layer'
     print 'w : plot the weight matrices associated with the current pixel'
     print 'W : same as w but for all a group'
@@ -141,6 +147,10 @@
     
     def __init__(self, learner, vmat, image_width=28, char_indice=0):
         '''constructor'''
+
+        self.k = 10
+        self.p = 0.01
+        
         self.current = char_indice-1#-1 because it's the first time
         self.learner = learner
         self.vmat = vmat
@@ -151,7 +161,11 @@
         self.fig_rep = 1
         figure(self.fig_rec)
         figure(self.fig_rep)
-
+        figure(2)
+        figure(3)
+        figure(4)
+        figure(5)
+        
         #self.current_fig = None
         #self.current_axes = None
         self.current_hl = None#current hidden layer
@@ -237,6 +251,7 @@
         imagetmat = TMat([self.input])
         print 'computing reconstructions...'
         rec = learner.computeReconstructions(imagetmat)
+        # rec = []
         print '...done.'
 
         matrices = [rowToMatrix(self.input, self.image_width)]
@@ -287,10 +302,55 @@
                 hl2 = self.hidden_layers[i+1]
 
             hl = hl1
+
+            # set k -- k
+            if char == 'k':
+                print "Enter the new value for k (currently "+str(self.k)+") : ",
+                self.k = input()
                         
+            # keep K top values -- K
+            elif char == 'K':
+                print 'keeping top k='+str(self.k)+' values'
+
+                values = [v for v in hl.hidden_layer]
+                values.sort()
+                keepval = values[-self.k]
+                for pos in xrange(hl.hidden_layer.size):
+                    if hl.hidden_layer[pos]<keepval:
+                        hl.hidden_layer[pos] = 0
+
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()                
+                print '...done'
+
+            # set p -- p
+            elif char == 'p':
+                print "Enter the new value for p (currently "+str(self.p)+") : ",
+                self.p = input()
+                        
+            # sample from independent bernoulli with prob p
+            elif char == 'P':
+                hl.hidden_layer[:] = numpy.random.binomial(1,self.p, hl.hidden_layer.size)
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()                
+
+            # generate (starting from independent bernoulli)            
+            elif char == 'G':
+                print "Generating..."
+                hl.hidden_layer[:] = numpy.random.binomial(1,self.p, hl.hidden_layer.size)
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                k = i-1
+                while k>=0:
+                    self.__reconstructLayer(k)
+                    if k!=0:
+                        self.__sampleLayer(k, "bernoulli")
+                    self.rep_axes[k].imshow(self.hidden_layers[k].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                    k = k-1
+                draw()                
+
             # fprop -- f
 
-            if char == 'f':                
+            elif char == 'f':                
                 print 'fproping...'
                 #update                
                 self.learner.setMatValue(i-1, reshape(hl0.hidden_layer, (1,-1)))
@@ -892,12 +952,26 @@
     ### utils
     ###
 
-    def __sampleLayer(self, which_layer):
+    def __sampleLayer(self, which_layer, sampling_type="multinomial"):
         hl = self.hidden_layers[which_layer]
-        for n in arange(hl.hidden_layer.size/hl.groupsize):
-            multi = numpy.random.multinomial(1,hl.getRow(n))                    
-            hl.setRow(n,multi)
 
+        if sampling_type=="multinomial":
+            for n in arange(hl.hidden_layer.size/hl.groupsize):
+                multi = numpy.random.multinomial(1,hl.getRow(n))                    
+                hl.setRow(n,multi)
+        elif sampling_type=="bernoulli":
+            for pos in xrange(hl.hidden_layer.size):
+                val = hl.hidden_layer[pos]
+                if val<0.:
+                    val = 0.
+                elif val>1.:
+                    val = 1.
+                else:
+                    val = numpy.random.binomial(1,val)
+                hl.hidden_layer[pos] = val
+        else:
+            raise ValueError("Invalid sampling_type: "+sampling_type)
+            
     def __reconstructLayer(self, which_layer):
         hl = self.hidden_layers[which_layer+1]
         
@@ -939,9 +1013,32 @@
         connect("button_press_event",self.plotNext)
         
     
+def interact(learner,vmat):
 
+    helptext = \
+"""
+*** Commands: ***
+help          : prints this help screen
+ls            : list available variables
+print varname : print the matrix
+show varname  : graphical display of the matrix
+generate <depth> <prior> <n> 
+exit          : terminate and exit
+"""
+
+    command = "help"
+    while command!="exit":
+
+        if command=="ls":
+            pass
+        elif command=="show":
+            pass
+        elif command!="exit": # print help
+            print helptext            
+        command = raw_input('>>> ')
         
 
+
 ############
 ### main ###
 ############
@@ -1009,6 +1106,20 @@
             print 'This matrix does not exist !'
 
 
+elif task == 'interact':
+    psave = sys.argv[2]
+    datafname = sys.argv[3]
+    #test = sys.argv[5]
+    #print test
+   
+    #loading learner
+    learner = serv.load(psave)
+    
+    #taking an input
+    vmat = openVMat(datafname)
+
+    interact(learner,vmat)
+    
 elif task == 'plotRepAndRec':
     
     psave = sys.argv[2]



From plearner at mail.berlios.de  Wed Jun  3 00:59:46 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 3 Jun 2009 00:59:46 +0200
Subject: [Plearn-commits] r10227 - in trunk/python_modules/plearn: . table
Message-ID: <200906022259.n52Mxkxv016391@sheep.berlios.de>

Author: plearner
Date: 2009-06-03 00:59:44 +0200 (Wed, 03 Jun 2009)
New Revision: 10227

Added:
   trunk/python_modules/plearn/table/
   trunk/python_modules/plearn/table/__init__.py
   trunk/python_modules/plearn/table/date.py
   trunk/python_modules/plearn/table/pltable_commands.py
   trunk/python_modules/plearn/table/table.py
   trunk/python_modules/plearn/table/tablestat.py
   trunk/python_modules/plearn/table/viewtable.py
Log:
ApSTAT contribution of the table classes


Added: trunk/python_modules/plearn/table/__init__.py
===================================================================

Added: trunk/python_modules/plearn/table/date.py
===================================================================
--- trunk/python_modules/plearn/table/date.py	2009-06-02 21:55:31 UTC (rev 10226)
+++ trunk/python_modules/plearn/table/date.py	2009-06-02 22:59:44 UTC (rev 10227)
@@ -0,0 +1,208 @@
+"""
+Module plearn.table.date
+
+Copyright (C) 2005 ApSTAT Technologies Inc.
+
+This file was contributed by ApSTAT Technologies to the
+PLearn library under the following BSD-style license: 
+
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+"""
+
+# Author: Pascal Vincent
+
+import datetime, string
+from re import match
+
+monthnum = {
+'JAN' : 1,
+'FEB' : 2,
+'MAR' : 3,
+'APR' : 4,
+'MAY' : 5,
+'JUN' : 6,
+'JUL' : 7,
+'AUG' : 8,
+'SEP' : 9,
+'OCT' : 10,
+'NOV' : 11,
+'DEC' : 12 }
+
+def CYYMMDD_to_YYYYMMDD(cyymmdd):
+    return 19000000+int(cyymmdd)
+
+def YYYYMMDD_to_CYYMMDD(yyyymmdd):
+    return int(yyyymmdd)-19000000
+
+def date_to_CYYMMDD(date):
+    """Takes a python datetime.date and returns an int in CYYMMDD format"""
+    if date.year<1900:
+        raise ValueError('In date_to_CYYMMDD CYYMMDD format is only valid for years 1900 and above, not'+str(date))
+    return (date.year-1900)*10000 + date.month*100 + date.day
+
+def date_to_YYYYMMDD(date):
+    """Takes a python datetime.date and returns an int in YYYYMMDD format"""
+    return date.year*10000 + date.month*100 + date.day
+
+def CYYMMDD_to_date(cyymmdd):
+    cyymm,dd = divmod(cyymmdd,100)
+    cyy,mm = divmod(cyymm,100)
+    try: d = datetime.date(1900+cyy,mm,dd)
+    except ValueError:
+        raise ValueError('Invalid CYYMMDD date: ' + str(cyymmdd))
+    return d
+
+daysinmonth = [31,28,31,30,31,30,31,31,30,31,30,31]
+def float_sum(l):
+    s = 0
+    for x in l:
+        s += float(x or '0')
+    return s
+
+def CYYMMDD_to_ymd(cyymmdd):
+    y = int(cyymmdd/10000)
+    mmdd = cyymmdd - 10000*y
+    m = int(mmdd/100)
+    d = mmdd - 100*m
+    return (y,m,d)
+    
+def toyears(date):
+    sdate = str(date)
+#    print ':',sdate,':'
+    # Assumed format:  DDMMMYYYY
+    if match(r'\d{2}[a-zA-Z]{3}\d{4}',sdate): 
+        try:
+            d = float(sdate[0:2])
+            m = monthnum[sdate[2:5]]
+            y = float(sdate[5:])
+        except:
+            raise ValueError('1:Can\'t convert ' + str(date) + ' in toyears()')
+        
+    # Assumed format:  YYYYMMDD
+    elif match(r'\d{8}',sdate):  
+        try:
+            d = date%100
+            date = (date - d)/100
+            m = date%100
+            y = (date - m)/100
+        except:
+            raise ValueError('2:Can\'t convert ' + str(date) + ' in toyears()')
+
+    # Assumed format:  YYMMDD        
+    elif match(r'\d{6}',sdate):
+        try:
+            d = date%100
+            date = (date - d)/100
+            m = date%100
+            y = (date - m)/100 + 1900
+            if y < 4:
+                y = y + 100
+        except:
+            raise ValueError('3:Can\'t convert ' + str(date) + ' in toyears()')        
+
+    # Assumed format:  YYYY        
+    elif match(r'\d{4}',sdate):
+	try:
+	    y = float(date)
+	    m = 1
+	    d = 1
+	except:
+            raise ValueError('4:Can\'t convert ' + str(date) + ' in toyears()')
+	    
+    else:
+        raise ValueError('0:Can\'t convert ' + str(date) + ' in toyears()')
+    if y%4==0:
+        if m > 2:
+            return y + (float_sum(daysinmonth[:m-1])+1+d-1)/366.
+        else:
+            return y + (float_sum(daysinmonth[:m-1])+d-1)/366.
+    return y + (float_sum(daysinmonth[:m-1])+d-1)/365.
+
+def YYYYMMDD_to_date(yyyymmdd):
+    yyyymm,dd = divmod(yyyymmdd,100)
+    yyyy,mm = divmod(yyyymm,100)
+    return datetime.date(yyyy,mm,dd)
+    
+def dateint_to_date(dateint):
+    """Takes a dateint in YYYYMMDD or CYYMMDD format and returns a python datetime.date"""
+    if dateint>=10000000: # YYYYMMDD format (we suppose we won't see date before year 1000 !)
+        return YYYYMMDD_to_date(dateint)
+    else: # CYYMMDD format
+        return CYYMMDD_to_date(dateint)
+
+def datestring_to_date(datestring):
+    """Takes a datestring in various formats and returns a python datetime.date
+    Currently recognized formats are:
+    YYYYMMDD
+    CYYMMDD
+    YYYY-MM-DD
+    YYYY/MM/DD
+    27JAN2003
+    """
+    date = datestring.strip()
+    dateint = 0
+    try: dateint = int(datestring)
+    except: pass
+    if dateint:
+        return dateint_to_date(dateint)
+
+    # Format "2003/01/27" or "2003-01-27"
+    if len(date) == 10 and date[4] in '/-' and date[7] in '/-':
+        year = int(date[0:4])
+        month = int(date[5:7])
+        day = int(date[8:10])
+        return datetime.date(year,month,day)
+        
+    # Format "27JAN2003"
+    if len(date)==9 and date[2] in string.uppercase and date[3] in string.uppercase and date[4] in string.uppercase:
+        year = int(date[5:9])
+        day = int(date[0:2])
+        month = monthnum[date[2:5]]
+        return datetime.date(year,month,day)
+        
+    raise ValueError("Invalid datestring format: "+datestring)
+
+def datestring_to_CYYMMDD(datestring):
+    return date_to_CYYMMDD(datestring_to_date(datestring))
+
+def datestring_to_YYYYMMDD(datestring):
+    return date_to_YYYYMMDD(datestring_to_date(datestring))
+
+def todate(date):
+    """Accepts both string date formats and int date formats, and returns a python datetime.date"""
+    if isinstance(date,datetime.date):
+        return date
+    elif type(date)==str:
+        return datestring_to_date(date)
+    elif type(date)==int:
+        return dateint_to_date(date)
+        
+def daydiff(cyymmdd1, cyymmdd2):
+    return (CYYMMDD_to_date(cyymmdd1)-CYYMMDD_to_date(cyymmdd2)).days

Added: trunk/python_modules/plearn/table/pltable_commands.py
===================================================================
--- trunk/python_modules/plearn/table/pltable_commands.py	2009-06-02 21:55:31 UTC (rev 10226)
+++ trunk/python_modules/plearn/table/pltable_commands.py	2009-06-02 22:59:44 UTC (rev 10227)
@@ -0,0 +1,254 @@
+
+"""
+pltable_command.py
+
+Copyright (C) 2005 ApSTAT Technologies Inc.
+
+This file was contributed by ApSTAT Technologies to the
+PLearn library under the following BSD-style license: 
+
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+"""
+
+# Author: Pascal Vincent
+
+from plearn.table.viewtable import *
+
+def print_usage_and_exit():
+    print """
+    Usage: pytable [options] command [params...]
+
+    Options:
+           -i <indexfile>
+           --index <indexfile>
+             uses indexfile as an index for the first table
+
+    Commands:
+           pytable info <table_file>
+             prints length x width
+             
+           pytable fields <table_file>
+             prints all fieldnames in original order
+             
+           pytable sorted_fields <table_file>
+             prints all fieldnames sorted alphabetically
+             
+           pytable view <table_file>
+             view of table interactively (curses based)
+
+           pytable convert <src_table> <dest_file>
+             saves src_table (can be a .txt .pytable .ptab .ptabdir .pmat)
+             as dest_file which must be a .txt .ptab .ptabdir .pmat or .dmat
+             Note that if you save into a .pmat or .dmat, all values must either
+             be convertible to float, or be the empty string '' (which will
+             be output as missing values (NaN)). If you want a smarter
+             handling of strings, consider invoking 'pytable tovmat'
+             instead.
+
+           pytable tovmat <src_table> <dest_vmat> <stringmap_reference_dir>
+             saves src_table (can be a .txt .pytable .ptab .ptabdir .pmat)
+             as dest_vmat which must be a .pmat or .dmat
+             The stringmap_reference_dir is a directory which
+             will contain the mapping from non-numerical string
+             representations to numerical value. This mapping will be
+             automatically extended if yet unmapped representations are
+             encountered. The dest_vmat.metadata/FieldInfo will be created
+             as a symbolic link to stringmap_reference_dir/mappings.
+             Also stringmap_reference_dir/logdir/YYYY-MM-DD_HH:MM:SS/ will
+             contain stats.txt with the new type stat counts, and a .smap
+             for each field whose string mapping had to be extended
+             containing the extra mappings.
+
+           pytable scan <table_file>
+             will do a pass through the table, accessing every row
+
+           pytable diff <table_file1> <table_file2>
+             will report the differences between the 2 tables
+          
+           pytable countall <table.pytable> <fieldname>
+             will count all possible instances of the value of the given field
+             and print a list of all values with the associated count.
+
+           pytable sum <table.pytable>
+             will sum the values of each fields and print the result
+
+           pytable dump <table.pytable>
+             will dump the whole table as a tab-separated text table
+
+      table_file can be a tab-separated text table
+      or a python script ending in .pytable and defining a
+      result variable that is a Table.
+      """
+    sys.exit()
+
+    
+def main(argv):
+    
+    if len(argv)<3:
+        print_usage_and_exit()
+    
+    index_name = None
+    n= 1
+    if argv[n][:7]=='--index':
+        if len(argv[n]) > 7:
+            if argv[n][7] == '=':
+                index_name = argv[n][8:]
+                n+= 1
+            else:
+                print "invalid option syntax: " + argv[n]
+                print_usage_and_exit()
+    
+        else:
+            index_name = argv[n+1]
+            n+= 2
+    
+    if argv[n][:2]=='-i':
+        if len(argv[n]) > 2:
+            if argv[n][2] == '=':
+                index_name = argv[n][3:]
+                n+= 1
+            else:
+                print "invalid option syntax: " + argv[n]
+                print_usage_and_exit()
+        else:
+            index_name = argv[n+1]
+            n+= 2
+    
+    command = argv[n]
+    
+    cmd_args = argv[n+1:]
+    
+    table_name= cmd_args[0]
+    table= openTable(table_name)
+    
+    if index_name!=None:
+        index= IntVecFile(index_name)
+        table= SelectRows(table, index)
+    
+    
+    if command == 'info':
+        print str(table.length())+'x'+str(table.width())
+    elif command == 'fields':
+        for fieldname in table.fieldnames:
+            print fieldname
+    elif command == 'sorted_fields':
+        fieldnames = table.fieldnames[:]
+        fieldnames.sort()
+        for fieldname in fieldnames:
+            print fieldname
+        
+    elif command == 'view':
+        fname = table_name
+        table.set_title('FILE: '+fname)
+        viewtable(table)
+    
+    elif command == 'convert':
+        saveTable(table,cmd_args[1])
+    
+    elif command == 'tovmat':
+        saveTableAsVMAT(table,cmd_args[1],cmd_args[2])
+    
+    elif command == 'scan':
+        fname = table_name
+        i = 0
+        pbar = PBar('Scanning '+fname,len(table))
+        # try:
+        for row in table:
+            pbar.update(i)
+            i += 1
+        # except Exception, e:
+        #    print 'At row ',i
+        #    raise e
+        print 'Successfully scanned',i,'rows of',len(table)
+    
+    elif command == 'diff':
+        m1 = table
+        m2 = openTable(cmd_args[1])
+        if len(m1)!=len(m2):
+            print 'Lengths differ: ',len(m1),'!=',len(m2)
+        if m1.width() != m2.width():
+            print 'Widths differ: ',m1.width(),'!=',m2.width()
+        if m1.fieldnames != m2.fieldnames:
+            print 'Fieldnames differ: ',m1.fieldnames,'!=',m2.fieldnames
+        w = m1.width()
+        for i in xrange(min(len(m1),len(m2))):
+            r1 = m1[i]
+            r2 = m2[i]
+            for j in xrange(w):
+                if str(r1[j])!=str(r2[j]):
+                    print 'Difference at (',i,',',j,') : ', r1[j], '!=', r2[j]            
+    
+    elif command == 'countall':
+        fieldname = cmd_args[1]
+        fieldpos = table.fieldnum(fieldname)
+        counts = {}
+        pbar = PBar('Counting all possible values of field '+fieldname,len(table))
+        i = 0
+        for row in table:
+            val = row[fieldpos]
+            try:
+                counts[val] += 1
+            except KeyError:
+                counts[val] = 1
+            i += 1
+            pbar.update(i)
+        pbar.close()
+        print '## Value : Count'
+        for key,val in counts.items():
+            print ' ',key,'\t:',val
+    
+    elif command == 'sum':
+        w = table.width()
+        tot = [0.]*w
+        pbar = PBar('Computing sums',len(table))
+        i = 0
+        for row in table:
+            for j in xrange(w):
+                try:
+                    tot[j] += float(row[j])
+                except ValueError:
+                    pass
+            i += 1
+            pbar.update(i)
+        pbar.close()
+        print '## Sums:'
+        for j in xrange(w):
+            print '#',j,table.fieldnames[j],':',tot[j]
+    
+    elif command == 'dump':
+        print '\t'.join(table.fieldnames)
+        for i in xrange(len(table)):
+            print '\t'.join(map(str,table[i]))
+    
+    else:
+        print "invalid command: ", command
+        print_usage_and_exit()
+
+    table.close()
+    

Added: trunk/python_modules/plearn/table/table.py
===================================================================
--- trunk/python_modules/plearn/table/table.py	2009-06-02 21:55:31 UTC (rev 10226)
+++ trunk/python_modules/plearn/table/table.py	2009-06-02 22:59:44 UTC (rev 10227)
@@ -0,0 +1,2136 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
+"""
+table.py
+
+Copyright (C) 2005-2009 ApSTAT Technologies Inc.
+
+This file was contributed by ApSTAT Technologies to the
+PLearn library under the following BSD-style license: 
+
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+"""
+
+# Author: Pascal Vincent
+
+import os.path, string, struct, zlib, fpconst, pickle, csv, time, copy, subprocess
+
+from numpy.numarray import array, argsort, random_array
+
+from plearn.utilities.progress import PBar
+
+from random import seed, randint, uniform
+
+# Use of VMat through the Python-bridge
+try:
+    from plearn.pyext import DiskVMatrix, FileVMatrix
+except ImportError:
+    print "WARNING: Import of DiskVMatrix, FileVMatrix from plearn.pyext failed. Probably no plearn python extension compiled. Skipping this import." 
+
+def float_or_str(x):
+    try: return float(x)
+    except: return str(x)
+
+def float_or_zero(x):
+    try: return float(x)
+    except: return 0.
+
+def float_or_value(x, val):
+    try: return float(x)
+    except: return val
+
+def int_or_zero(x):
+    try: return int(x)
+    except: return 0
+
+def int_or_value(x, val):
+    try: return int(x)
+    except: return val
+
+class StructFile:
+    """File with fixed size packed binary records.
+    Records are packed using the struct module;
+    all records use the same format string.
+    """
+
+    def __init__(self, fname, struct_format, openmode='r', data_offset=0):
+        """data_offset is the pos. of beg. of data from beg. of file, in bytes"""
+        self.fname = fname
+        self.openmode = openmode
+        self.struct_format = struct_format
+        self.struct_size = struct.calcsize(struct_format)
+        self.closed = False
+        self.data_offset = data_offset
+        self.last_call_was_append = False
+        
+        if openmode=='r':
+            self._len = (os.path.getsize(fname)-data_offset)/self.struct_size
+            self.f = open(fname,'rb')
+        elif openmode=='r+':
+            self._len = (os.path.getsize(fname)-data_offset)/self.struct_size
+            self.f = open(fname,'rb+')
+        elif openmode=='w+':
+            self._len = 0
+            self.f = open(fname,'wb+')
+        else:
+            raise ValueError("Invalid value for openmode ("+`openmode`+" Must be one of 'r' 'r+' or 'w+'")
+            
+    def append(self, x):
+        if not self.last_call_was_append:
+            self.f.seek(0,2) # seek to end of index file
+            
+        if isinstance(x,list):
+            self.f.write(struct.pack(self.struct_format,*x))
+        else:
+            self.f.write(struct.pack(self.struct_format,x))
+        self._len += 1
+
+        self.last_call_was_append = True
+
+    def __getitem__(self,i):
+        self.last_call_was_append = False
+        
+        l = self.__len__()
+        if i<0: i+=l
+        if i<0 or i>=l:
+            raise IndexError('StructFile index out of range')
+
+        self.f.seek(self.data_offset + i*self.struct_size)
+        x = struct.unpack(self.struct_format,self.f.read(self.struct_size))
+
+        if len(x)==1:
+            return x[0]
+        else:
+            return x
+
+    def __setitem__(self, i, x):
+        self.last_call_was_append = False
+
+        l = self.__len__()
+        if i<0: i+=l
+        if i<0 or i>=l:
+            raise IndexError('StructFile index out of range')
+        self.f.seek(self.data_offset + i*self.struct_size)
+        try: # should work when x is a kind of list
+            self.f.write(struct.pack(self.struct_format,*x))
+        except TypeError: # should work when x is a single element
+            self.f.write(struct.pack(self.struct_format,x))
+        #self.flush() ##is it necessary?
+            
+
+    def flush(self):
+        self.last_call_was_append = False
+        self.f.flush()
+
+    def close(self):
+        self.last_call_was_append = False
+
+        if not self.closed:
+            self.f.close()
+            self.closed = True
+
+    # Note: it's better for your code to call close() explicitly rather than rely on this being called automatically
+    def __del__(self):
+        self.close()
+        
+    def __len__(self):
+        return int(self._len)
+
+
+class IntVecFile(StructFile): # read-only for now...
+    def __init__(self, fname, openmode='r', endianness='L'):
+        """endianness is ignored when reading"""
+
+        #self.fname = fname
+        #self.openmode = openmode
+
+        
+        if openmode[0]=='r':
+            self.f = open(fname,'rb')
+            self.readFileSignature()
+            self.f.close()
+        else:
+            self.endianness= endianness
+#         if openmode=='r':
+#             self.f = open(fname,'rb')
+#             self.readFileSignature()
+#         elif openmode=='r+':
+#             self.f = open(fname,'rb+')
+#             self.readFileSignature()
+#         elif openmode=='w+':
+#             self._len = 0
+#             self.f = open(fname,'wb+')
+#             self.endianness= endianness
+#             self.writeFileSignature()
+#         else:
+#             raise ValueError("Invalid value for openmode ("+`openmode`+" Must be one of 'r' 'r+' or 'w+'")
+
+        endcode = '<'
+        if self.endianness=='L': endcode = '<'
+        elif self.endianness=='B': endcode = '>'
+        else: raise ValueError("Invalid value endianness (" + self.endianness + ") must be 'L' or 'B'")
+        #self.struct_format = endcode+'l'
+        struct_format = endcode+'l'
+        #self.struct_size = struct.calcsize(self.struct_format)
+
+        #if openmode[0]=='r':
+        #    self._len = os.path.getsize(fname)/self.struct_size -2
+
+        StructFile.__init__(self, fname, struct_format, openmode, 8)
+        if openmode[0]=='w':
+            self.writeFileSignature()
+
+
+    def readFileSignature(self):
+        self.f.seek(0)
+        s = self.f.read(4)
+        if s != '\xde\xad\xbe\xef':
+            raise RuntimeError("Is this an IntVecFile?? ("+self.fname+")")
+        self.endianness= self.f.read(1)
+        if self.endianness not in ['L','B']:
+            raise RuntimeError("Invalid endianness ("+self.endianness+") for IntVecFile "+self.fname\
+                               +" (should be 'L' or 'B')")
+        
+    def writeFileSignature(self):
+        self.f.seek(0)
+        self.f.write('\xde\xad\xbe\xef') #dead beef
+        self.f.write(self.endianness)
+        self.f.write('\0\0')
+        self.f.write('\1') # version number
+
+#     def append(self, x):
+#         self.f.seek(0,2) # seek to end of index file
+#         if isinstance(x,list):
+#             self.f.write(struct.pack(self.struct_format,*x))
+#         else:
+#             self.f.write(struct.pack(self.struct_format,x))
+#         self._len += 1
+
+#     def __getitem__(self,i):
+#         l = self.__len__()
+#         if i<0: i+=l
+#         if i<0 or i>=l:
+#             raise IndexError('StructFile index out of range')
+#         self.f.seek((i+2)*self.struct_size)
+#         x = struct.unpack(self.struct_format,self.f.read(self.struct_size))
+#         if len(x)==1:
+#             return x[0]
+#         else:
+#             return x
+
+#     def __setitem__(self, i, x):
+#         l = self.__len__()
+#         if i<0: i+=l
+#         if i<0 or i>=l:
+#             raise IndexError('StructFile index out of range')
+#         self.f.seek(i*self.struct_size)
+#         try: # should work when x is a kind of list
+#             self.f.write(struct.pack(self.struct_format,*x))
+#         except TypeError: # should work when x is a single element
+#             self.f.write(struct.pack(self.struct_format,x))
+            
+
+#     def flush(self):
+#         self.f.flush()
+
+#     def close(self):
+#         self.f.close()
+        
+#     def __len__(self):
+#         return int(self._len)
+
+
+def build_row_index_file(txtfilename, idxfilename, struct_format = '<Q', offset= 0):
+    """Creates an index file giving the offset to each line (record) in a
+    text file.    
+    """
+    f = open(txtfilename)
+    index = StructFile(idxfilename,'<Q','w+')
+    while True:
+        pos = f.tell()
+        line = f.readline()
+        if len(line)==0:
+            break
+        if offset > 0:
+            offset-= 1
+        else:
+            index.append(pos)
+
+class FieldValues:
+    """DEPRECATED -- use ListWithFieldNames instead
+    FieldValues are objects that represent a row of data by associating
+    field names with their values as object attributes.
+    """
+    def __init__(self, fieldnames, values):
+        for name,val in zip(fieldnames, values):
+            setattr(self,name,val)
+
+class FieldIndex:
+    """FieldIndex objects have an attribute for each field in a table
+    indicating the index of that field.
+    """
+    def __init__(self, fieldnames):
+        for i in range(len(fieldnames)):
+            setattr(self,fieldnames[i],i)
+
+
+class ListWithFieldNames:
+    """This allows to have a view on a sequence
+    object with associated fieldnames, so that the elements of the sequence
+    can be accessed with either its numerical position, or its
+    fieldname."""
+    
+    def __init__(self, list_items, fieldnames=None, fieldpos=None):
+        """elemlist can be any sequence of elements
+
+        fieldnames should be a sequence of strings of the same length as
+        elemlist
+
+        fieldpos can optionally be passed (otherwise self.fieldpos will be
+        computed from the fieldnames): it corresponds to a dictionary,
+        mapping a fieldname to its position in the list.
+        """
+        self.list = list(list_items)
+        self.fieldnames = fieldnames
+        self.fieldpos = fieldpos
+        self.rownum = -1
+
+        # if autoappend is set to True, then accessing a field with [fieldname] will
+        # append it if fieldname is not already a fieldname of the list
+        # if autoappend is set to false, an error will be raised if accessing
+        self.autoappend = False
+        
+        # we suppose we may initially have shared fieldnames and fieldpos
+        # make_private_copy_of_fields will make a private copy of those, and
+        # will be called as soon as a we append or delete an item.
+        self.shared_fieldnames = True    
+
+        assert(fieldnames is None or len(fieldnames)==len(list_items))
+        if fieldnames and not fieldpos:
+            self.fieldpos = {}
+            pos = 0
+            for name in fieldnames:
+                self.fieldpos[name] = pos
+                pos += 1
+                
+    def __len__(self):
+        return len(self.list)
+
+    def __getitem__(self,key):
+        if isinstance(key,str):
+            key = self.fieldpos[key]
+        return self.list[key]
+
+    def __setitem__(self, key, value):
+        if isinstance(key,str):
+            try:
+                key = self.fieldpos[key]
+                self.list[key] = value
+            except KeyError:
+                if self.autoappend:
+                    self.append(value, key)
+                else:
+                    raise
+        else:
+            self.list[key] = value
+
+    def __delitem__(self,key):
+        if self.shared_fieldnames:
+            self.make_private_copy_of_fields()
+        if isinstance(key,str):
+            key = self.fieldpos[key]
+        del self.list[key]
+        if self.fieldnames:
+            fieldname = self.fieldnames[key]
+            del self.fieldnames[key]
+            del self.fieldpos[fieldname]
+
+    def __iter__(self):
+        return self.list.__iter__()
+
+    def __repr__(self):
+        return self.list.__repr__()
+
+    def __str__(self):
+        return self.list.__str__()
+    
+    def __add__(self, other):
+        return self.list + list(other)
+
+
+    def set_fieldnames(self,fieldnames):
+        self.fieldnames = fieldnames
+        self.fieldpos = {}
+        pos = 0
+        for name in fieldnames:
+            self.fieldpos[name] = pos
+            pos += 1
+
+    def make_private_copy_of_fields(self):
+        if self.fieldnames:
+            self.set_fieldnames(self.fieldnames[:])
+        self.shared_fieldnames = False
+
+    def append(self, val, name='?'):
+        if self.shared_fieldnames:
+            self.make_private_copy_of_fields()
+        self.list.append(val)
+        if self.fieldnames:
+            self.fieldpos[name] = len(self.fieldnames)
+            self.fieldnames.append(name)
+
+    def as_dict(self):
+        return dict(zip(self.fieldnames, self.list))
+
+
+class Table:
+    """Subclasses should call self.set_fieldnames(fieldnames) with an appropriate list of fieldnames
+    And they should implement:
+      getRow(self,i) (no need to perform bound checks for i)
+      __len__(self)
+      """
+
+    def set_fieldnames(self,fieldnames):
+        self.fieldnames = fieldnames
+        self.fieldpos = {}
+        pos = 0
+        for name in fieldnames:
+            self.fieldpos[name] = pos
+            pos += 1
+
+    def colname(self,i):
+        try:
+            return self.fieldnames[i]
+        except:
+            return str(i)
+
+    def get_column(self,c):
+        return [r[c] for r in self]
+
+    def set_rownames(self,rownames):
+        self.rownames = rownames
+
+    def rowname(self,i):
+        try:
+            return self.rownames[i]
+        except:
+            return str(i)
+
+    def set_title(self,title):
+        self.title_ = title
+
+    def title(self):
+        try: return self.title_
+        except: return ''            
+
+    def set_filepath(self,filepath):
+        self.filepath_ = filepath
+
+    def filepath(self):
+        try: return self.filepath_
+        except: return ''
+
+    def rename_fields(self, name_map, fieldname_must_exist = True):
+        names = self.fieldnames[:]
+        for oldname, newname in name_map.items():
+            try:
+                k = self.fieldpos[oldname]
+                names[k] = newname
+            except KeyError:
+                if fieldname_must_exist:
+                    raise ValueError('No field named '+oldname)
+        self.set_fieldnames(names)
+
+    def fieldnum(self,fieldname_or_num):
+        try:
+            return self.fieldpos[fieldname_or_num]
+        except:
+            pass
+        return int(fieldname_or_num)
+            
+    def __getitem__(self,i):
+        # print 'getitem',i
+        if isinstance(i,slice):
+            start, stop, step = i.start,i.stop,i.step
+            if step!=None:
+                raise IndexError('Extended slice with step not currently supported')
+            l = self.__len__()
+            if stop>l:
+                stop = l
+            return SelectRowRange(self,start,stop)
+        else:
+            l = self.__len__()
+            if i<0: i+=l
+            if i<0 or i>=l:
+                raise IndexError('TableFile index out of range ('+str(i)+'/'+str(l)+')')
+            row = ListWithFieldNames(self.getRow(i), self.fieldnames, self.fieldpos)
+            row.rownum = i
+            return row
+
+    def length(self):
+        return len(self)
+
+    def width(self):
+        return len(self.fieldnames)
+
+    def __concat__(self,other):
+        return VConcatTable([self,other])
+
+    def close(self):
+        pass
+
+##     def __getslice__(self,start,stop):
+##         print 'getsli
+##         l = self.__len__()
+##         if start<0: start+=l
+##         if stop<0: stop+=l
+##         if stop 
+##         if i<0 or i>=l:
+
+class StructTable(Table):
+    """Table implemented as a StructFile.
+    The struct format and the field names are saved in a secondary file
+    with the same name as the data file, plus a '.format' extension.
+    This file is executed when the table is openned for read ('r' or 'r+');
+    it is created from the supplied field names and format when the table
+    is created ('w+').
+    """
+    
+    def __init__(self, fname, openmode='r', fieldnames=None, struct_format=None):
+        self.set_fieldnames(fieldnames)
+        self.fname = fname
+        self.set_filepath(fname)
+        self.closed = False
+        if openmode=='r' or openmode=='r+':
+            self.load_format()
+        elif openmode=='w+':
+            self.set_format(fieldnames, struct_format)
+        else:
+            raise ValueError('Invalid openmode '+openmode)
+        
+        self.struct = StructFile(fname,self.struct_format,openmode)
+
+    def set_format(self, fieldnames, struct_format):
+            if not isinstance(fieldnames,list) or not isinstance(struct_format,str):
+                raise ValueError('You must specify the list of fieldnames and the struct_format string')
+            self.set_fieldnames(fieldnames)
+            self.struct_format = struct_format
+            self.save_format()
+            
+    def load_format(self):
+        vars = {}
+        execfile(self.fname+'.format',vars)
+        self.set_fieldnames(vars['fieldnames'])
+        self.struct_format = vars['struct_format']
+
+    def save_format(self):
+        f = open(self.fname+'.format','wb')
+        f.write('fieldnames = '+repr(self.fieldnames)+'\n\n')
+        f.write('struct_format = '+repr(self.struct_format)+'\n\n')
+        
+    def getRow(self,i):
+        return self.struct[i]
+
+    def __len__(self):
+        return len(self.struct)
+
+    def append(self, x):
+        self.struct.append(x)
+
+    def close(self):
+        if not self.closed:
+            self.struct.close()        
+            self.closed = True
+
+    # Note: it's better for your code to call close() explicitly rather than rely on this being called automatically
+    def __del__(self):
+        self.close()
+
+def compr_factor(m):
+    s = '\n'.join([ '\t'.join(r) for r in m ])
+    cs = zlib.compress(s)
+    return float(len(s))/len(cs)
+
+def compr_factor2(m):
+    w = m.width()
+    m = [ r for r in m ]
+    uncompr_len = 0
+    compr_len = 0
+    for c in range(w):
+        s = '\n'.join([r[c] for r in m])
+        uncompr_len += len(s)
+        compr_len += len(zlib.compress(s))
+    return float(uncompr_len)/compr_len
+
+def cf(m):
+    return compr_factor(m), compr_factor2(m)
+
+
+class CSVTable(Table):
+    """Read-only for now"""
+    
+    def __init__(self, datafname, openmode="r"):
+        if openmode!="r":
+            raise ValueError("Currently only openmode=='r' is supported")
+        
+        self.datafname = datafname
+        self.len = -1
+        self.f = open(datafname,'rb')
+        self.reader = csv.reader(self.f)
+        self.i = -1
+        self.row_i = self.reader.next()
+        self.set_fieldnames(self.row_i)
+        
+    def getRow(self,i):
+        if i<self.i:
+            self.f.close()
+            self.f = open(self.datafname,'rb')
+            self.reader = csv.reader(self.f)
+            self.i = -1
+            self.row_i = self.reader.next()
+        
+        while self.i<i:
+            self.i += 1
+            self.row_i = self.reader.next()
+
+        return self.row_i
+
+    def __len__(self):
+        if self.len<0:
+            f = open(self.datafname,'rb')
+            tmpreader = csv.reader(f)
+            self.len = -1
+            for line in tmpreader:                
+                self.len += 1
+            f.close()
+        return self.len
+
+
+class CompressedTableFile(Table):
+    """
+    Main file format:
+    header: PLTABLE <version> <nrows_per_chunk> \n
+    fieldnames_row
+    chunks of data
+    
+    Each chunk of data contains the zlib compressed representation of at most nrows_per_chunk \n separated rows.
+    
+    There is also an associated index file containing 8-byte integers.
+    Integer #i gives the byteindex of chunk #i.
+    The last integer gives the number of rows in the last chunk.
+    """
+    
+    def __init__(self, datafname, openmode='r', fieldnames=None, nrows_per_chunk=100):
+        self.openmode = openmode
+        indexfname = datafname+'.idx'
+        self.set_filepath(datafname)
+        self.closed = False
+        self.final_chunk_rows = []
+        
+        if openmode=='r':
+            self.dataf = open(datafname,'rb')
+            self.index = StructFile(indexfname,'!Q','r')
+            self.read_header_and_fieldnames()
+            self.cached_chunk_rows = []
+            self.cached_chunknum = -1
+        elif openmode=='a' or openmode=='r+':
+            self.dataf = open(datafname,'r+b')
+            self.index = StructFile(indexfname,'!Q','r+')
+            self.read_header_and_fieldnames()
+            if self.nrows_in_last_chunk==self.nrows_per_chunk or self.nrows_in_last_chunk==0:
+                self.final_chunk_rows = []
+            else:
+                self.final_chunk_rows = self.read_chunk_rows(self.last_chunknum())
+        elif openmode=='w' or openmode=='w+': 
+            self.nrows_per_chunk = nrows_per_chunk
+            self.set_fieldnames(fieldnames)
+            self.dataf = open(datafname,'w+b')
+            self.index = StructFile(indexfname,'!Q','w+')
+            self.write_header_and_fieldnames()
+            self.final_chunk_rows = []
+            self.nrows_in_last_chunk = self.nrows_per_chunk
+        else:
+            raise ValueError('Invalid openmode: '+openmode)
+
+    def read_header_and_fieldnames(self):
+        self.dataf.seek(0)
+        headerline = self.dataf.readline()
+        headcode, version, nrows = headerline.split()
+        if headcode!='PLTABLE' or version!='01':
+            raise TypeError('Invalid header or version'+headcode+' '+version)
+        self.nrows_per_chunk = int(nrows)
+        fieldnamesline = self.dataf.readline()
+        self.set_fieldnames(fieldnamesline.split())
+        self.nrows_in_last_chunk = int(self.index[-1])
+        self._len = self.last_chunknum()*self.nrows_per_chunk + self.nrows_in_last_chunk
+        self.dataf.seek(0,2)   # seek to end of file
+
+    def last_chunknum(self):
+        return len(self.index)-3
+
+    def write_header_and_fieldnames(self):
+        self.dataf.seek(0)
+        self.dataf.write('PLTABLE 01\t'+str(self.nrows_per_chunk)+'\n')
+        self.dataf.write('\t'.join(self.fieldnames)+'\n')
+        self.dataf.flush()
+        self._len = 0
+        self.index.append(self.dataf.tell())
+        self.index.append(0)
+        self.index.flush()
+
+    def compress(self, rows):
+        return zlib.compress('\n'.join(rows))
+        # return '\n'.join(rows)
+
+    def decompress(self, encodedstring):
+        chunk = zlib.decompress(encodedstring)
+        # chunk = encodedstring
+        rows = chunk.split('\n')
+        return rows
+    
+    def read_chunk_rows(self, chunknum):
+        if chunknum>self.last_chunknum():
+            raise IndexError('chunk out of range: '+str(chunknum))
+        startpos = self.index[chunknum]
+        endpos = self.index[chunknum+1]
+        self.dataf.seek(startpos)
+        encodedchunk = self.dataf.read(endpos-startpos)
+        rows = self.decompress(encodedchunk)
+        return rows
+
+    def get_chunk_rows(self, chunknum):
+        if chunknum!=self.cached_chunknum:
+            self.cached_chunk_rows = self.read_chunk_rows(chunknum)
+            self.cached_chunknum = chunknum
+        return self.cached_chunk_rows
+        
+    def getRow(self,i):
+        if self.openmode!='r':
+            raise IOError("Can only getRow if in 'r' openmode, not in "+repr(self.openmode)+" mode.")
+        chunknum,ii = divmod(i,self.nrows_per_chunk)
+        line = self.get_chunk_rows(chunknum)[int(ii)]
+        elements = line.split('\t')
+        return elements
+
+    def flush(self):
+        if self.openmode!='r' and self.final_chunk_rows:
+            encodedchunk = self.compress(self.final_chunk_rows)
+            # print 'Flushing with nrows_in_last_chunk = ',self.nrows_in_last_chunk,' and self.final_chunk_rows=',repr(self.final_chunk_rows)
+            if self.nrows_in_last_chunk<self.nrows_per_chunk: # some more room in last chunk
+                # print 'Rewriting last chunk'
+                self.dataf.seek(self.index[self.last_chunknum()]) # seek to beginning of existing last chunk
+                self.dataf.write(encodedchunk)
+                self.index[-2] = self.dataf.tell()
+                self.nrows_in_last_chunk = len(self.final_chunk_rows)
+                self.index[-1] = self.nrows_in_last_chunk
+            else: # append new chunk
+                # print 'Appending new chunk'
+                self.dataf.seek(0,2) # seek to end of file
+                self.dataf.write(encodedchunk)
+                self.index[-1] = self.dataf.tell()
+                self.nrows_in_last_chunk = len(self.final_chunk_rows)
+                self.index.append(self.nrows_in_last_chunk)
+            if len(self.final_chunk_rows)==self.nrows_per_chunk:
+                self.final_chunk_rows = []
+            self.dataf.flush()
+            self.index.flush()    
+
+    def close(self):
+        if not self.closed:
+            self.flush()
+            self.dataf.close()
+            self.index.close()
+            self.closed = True
+
+    def append(self, row):
+        if self.openmode not in ['w','a','w+','r+']:
+            raise IOError("Can only append if in 'w','w+','r+' or 'a' openmode, not in "+repr(self.openmode)+" mode.")
+        if len(row)!=self.width():
+            raise ValueError('length of row does not match table width')
+        self.final_chunk_rows.append('\t'.join([ str(elem).strip() for elem in row]))
+
+        if len(self.final_chunk_rows)==self.nrows_per_chunk:
+            self.flush()
+        self._len += 1
+
+    # Note: it's better for your code to call close() explicitly rather than rely on this being called automatically
+    def __del__(self):
+        self.close()
+
+    def __len__(self):
+        return int(self._len)
+
+
+        
+
+class MemoryTable(Table):
+    """Table saved in RAM.
+    This is a simple list of lists of fields; field names must be supplied to
+    the constructor.
+    """
+
+    def __init__(self, fieldnames, data=None):
+        self.set_fieldnames(fieldnames)
+        if data==None:
+            self.data = []
+        else:
+            self.data = data
+        
+    def append(self,elements):
+        if len(elements)!=len(self.fieldnames):
+            raise ValueError("elements (len="+str(len(elements))+") must be a vector of same length as fieldnames (len="+str(len(self.fieldnames))+")")
+        self.data.append(elements)
+    
+    def getRow(self,i):
+        return self.data[i][:]
+
+    def __len__(self):
+        return len(self.data)
+
+def memorize(table):
+    """Loads a table in memory: fetches all rows from table into a
+    MemoryTable and returns that object.
+    """
+    mtable = MemoryTable(table.fieldnames)
+    for row in table:
+        mtable.append(row)
+    return mtable
+
+
+class PMatTable(Table):
+    """Matrix of reals saved on disk in binary format.
+    The data file starts with a 64 bytes header that indicates:
+    - the number of rows in the table
+    - the number of fields in a row
+    - the data type (float or double)
+    - the endianness of the data
+    The rest is the data itself, in a fixed width format.
+    Other data is saved in a subdirectory with the same name as the
+    data file, plus a '.metadata' extension.  For example, field names
+    are saved in '<table_name>.metadata/fieldnames', one name per line.
+    """
+    def __init__(self, fname, openmode='r', fieldnames=None):
+        self.fname = fname
+        self.set_filepath(fname)
+        if openmode=='r':
+            self.f = open(fname,'rb')
+            self.read_and_parse_header()
+            self.set_fieldnames(self.determine_fieldnames())
+        else:
+            raise ValueError("Currently only supported openmode is 'r'."+repr(openmode)+" is not supported")
+
+    def read_and_parse_header(self):        
+            header = self.f.read(64)
+            mat_type, l, w, data_type, endianness = header.split()
+            if mat_type!='MATRIX':
+                raise ValueError('Invalid file header (should start with MATRIX)')
+            self.len = int(l)
+            self.w = int(w)
+            if endianness=='LITTLE_ENDIAN':              
+                struct_format = '<'
+            elif endianness=='BIG_ENDIAN':
+                struct_format = '>'
+            else:
+                raise ValueError('Invalid endianness in file header: '+endianness)
+
+            if data_type=='DOUBLE':
+                struct_format += 'd'*self.w
+            elif data_type=='FLOAT':
+                struct_format += 'f'*self.w
+                
+            self.struct_format = struct_format
+            self.struct_size = struct.calcsize(struct_format)
+
+    def determine_fieldnames(self):
+        fieldnames = []
+        fieldnamefile = os.path.join(self.fname+'.metadata','fieldnames')
+        if os.path.isfile(fieldnamefile):
+            for row in open(fieldnamefile):
+                row = row.split()
+                if len(row)>0:
+                    fieldnames.append(row[0])
+        else:
+            fieldnames = map(str,range(self.w))
+        return fieldnames
+
+    def getRow(self,i):
+        self.f.seek(64+i*self.struct_size)
+        x = struct.unpack(self.struct_format,self.f.read(self.struct_size))
+        return list(x)
+
+    def __len__(self):
+        return int(self.len)
+
+
+
+class TableFile(Table):
+    """Table saved as a tab separated text file (variable width records.)
+    An index giving the offset to each record is saved in a file with the same
+    name as the data file, plus a '.idx' extension; this index is a simple list
+    of 64 bit offsets in binary format.
+    The first line of the data file contains the field names, separated by
+    tabs; these must be supplied when the file is created ('w+').
+    """
+
+    def __init__(self, fname, openmode='r', fieldnames=None,
+                 tolerate_different_field_count=True, separator='\t'):
+        self.separator= separator
+        self.struct_format = '<Q'
+        self.fname = fname
+        self.set_filepath(fname)
+        self.closed = False
+        self.openmode = openmode
+        self.tolerate_different_field_count = tolerate_different_field_count
+        self.last_call_was_append = False
+       
+        indexfname = self.fname+'.idx'
+        if openmode=='r':
+            self.f = open(fname,'r')
+            fieldnames = self.f.readline().strip('\r\n').split(self.separator)
+            self.set_fieldnames(fieldnames)
+            if not os.path.isfile(indexfname):
+                build_row_index_file(fname,indexfname,self.struct_format)
+            self.index = StructFile(indexfname,self.struct_format,'r')
+        elif openmode=='r+':
+            self.f = open(fname,'r+')
+            fieldnames = self.f.readline().strip('\r\n').split(self.separator)
+            self.set_fieldnames(fieldnames)
+            if not os.path.isfile(indexfname):
+                build_row_index_file(fname,indexfname)
+            self.index = StructFile(indexfname,self.struct_format,'r+')
+        elif openmode=='w+':
+            self.f = open(fname,'w+')
+            fieldnames = fieldnames[:]
+            self.set_fieldnames(fieldnames)
+            self.index = StructFile(indexfname,self.struct_format,'w+')
+            self.append(fieldnames)
+        else:
+            raise ValueError("Invalid value for openmode ("+`openmode`+" Must be one of 'r' 'r+' or 'w+'")
+
+            
+    def append(self,elements):
+        if len(elements)!=len(self.fieldnames):
+            raise ValueError("elements must be a vector of same length as fieldnames")
+
+        self.index.append(self.f.tell())
+        
+        if not self.last_call_was_append:
+            self.f.seek(0,2) # seek to end of file
+
+        self.f.write(self.separator.join(map(str,elements))+'\r\n')
+        
+        self.last_call_was_append = True
+
+    def flush(self):
+        if self.openmode!='r':
+            self.f.flush()
+            self.index.flush()
+
+    def close(self):
+        if not self.closed:
+            self.flush()
+            self.f.close()
+            self.index.close()
+            self.closed = True
+
+    # Note: it's better for your code to call close() explicitly rather than rely on this being called automatically
+    def __del__(self):
+        self.close()
+
+    def getRow(self,i):
+        self.last_call_was_append = False
+        
+        self.f.seek(self.index[i+1])
+        line = self.f.readline().strip('\r\n')
+        elements = line.split(self.separator)
+        if len(elements)!=len(self.fieldnames):
+            if self.tolerate_different_field_count:
+                if len(elements)<len(self.fieldnames):
+                    elements += ['']*(len(self.fieldnames)-len(elements))
+                else: # len(elements)>len(self.fieldnames)
+                    elements = elements[0:len(self.fieldnames)]
+            else:
+                raise ValueError("At row "+str(i)+
+                                 ", read only "+str(len(elements))+
+                                 " while there are "+str(len(self.fieldnames))+
+                                 " fieldnames")
+        return map(string.strip, elements)
+
+    def __len__(self):
+        return len(self.index)-1
+
+
+class SortWithinGroup(Table):
+    """Creates a view from an existing table by grouping
+    records and then sorting records within a group.  Grouping is made
+    according to the supplied grouping fields: all consecutive records with
+    the same values for all those fields will be part of the same group.  The
+    group is then sorted according to the supplied sorting fields (ascending,
+    in the same order the fields are given.)
+    """
+    def __init__(self, table, grouping_fields, sorting_fields):
+        self.table = table
+        self.set_fieldnames(table.fieldnames)
+        self.grouping_fields = [self.fieldnum(f) for f in grouping_fields]
+        self.sorting_fields = [self.fieldnum(f) for f in sorting_fields]
+        self.cached_group_start = None
+        self.cached_group_end = None
+        self.cached_group = None
+    
+    def getRow(self,i):
+
+        if self.cached_group and i>=self.cached_group_start and i<self.cached_group_end:
+            return self.cached_group[i-self.cached_group_start]
+
+        row_i = self.table[i]
+        groupkeys = [ row_i[k] for k in self.grouping_fields ]
+
+        group = []
+        pos = i-1
+        while pos>=0:
+            row = self.table[pos]
+            if [ row[k] for k in self.grouping_fields ] != groupkeys:
+                break
+            group.append(row)
+            pos -= 1
+        
+##         if pos>=0:
+##             row = self.table[pos]
+##         while(pos>=0 and [ row[k] for k in self.grouping_fields ] == groupkeys):
+##             group.append(row)
+##             pos -= 1
+##             if pos>=0:
+##                 row = self.table[pos]
+
+        self.cached_group_start = pos+1
+        group.reverse()
+        group.append(row_i)
+
+        l = len(self.table)
+        pos = i+1
+        while pos<l:
+            row = self.table[pos]
+            if [ row[k] for k in self.grouping_fields ] != groupkeys:
+                break
+            group.append(row)
+            pos += 1
+            
+        
+##         row = self.table[pos]
+##         while(pos<l and [ row[k] for k in self.grouping_fields ] == groupkeys):
+##             group.append(row)
+##             pos += 1
+##             print 'getting ',pos,l
+##             if pos<l:
+##                 row = self.table[pos]
+
+        self.cached_group_end = pos
+
+        def cmpfunc(x,y):
+            xkeys = [ x[k] for k in self.sorting_fields ]
+            ykeys = [ y[k] for k in self.sorting_fields ]
+            return cmp(xkeys, ykeys)
+        
+        group.sort(cmpfunc)
+        self.cached_group = group
+
+        return list(self.cached_group[i-self.cached_group_start])
+
+    def __len__(self):
+        return len(self.table)
+
+
+class SelectRowRange(Table):
+    """Creates view of a table by extracting a range of rows from an
+    existing table.
+    """
+    def __init__(self, table, start, stop):
+        self.table = table
+        self.start = start
+        self.stop = stop
+        try: self.set_fieldnames(table.fieldnames)
+        except: pass
+
+    def getRow(self,i):
+        #return self.table[self.start+i]
+        return self.table.getRow(self.start+i)
+
+    def __len__(self):
+        return int(self.stop-self.start)
+
+
+class VConcatTable(Table):
+    """Creates a view by concatenating the rows of several
+    existing tables.  All tables must have the same fields.
+    """
+    def __init__(self, tables):
+        self.tables = tables
+        self.set_fieldnames(tables[0].fieldnames)
+        for t in tables:
+            if t.fieldnames != self.fieldnames:
+                print "got: ", t.fieldnames
+                print "s/b: ", self.fieldnames
+                raise RuntimeError('In VConcatTable fieldnames of the individual tables differ')
+
+    def getRow(self,i):
+        start = 0
+        stop = 0
+        for t in self.tables:
+            stop += len(t)
+            if i<stop:
+                #return t[i-start]
+                return t.getRow(i-start)
+            start = stop
+        raise IndexError('In VConcatTable: index out of range')
+
+    def __len__(self):
+        l = 0
+        for t in self.tables:
+            l += len(t)
+        return l
+
+class HConcatTable(Table):
+    """Creates a view by concatenating the fields of several
+    existing tables.  All tables must have the same number of rows.
+    """
+    def __init__(self, tables):
+        self.tables = [] 
+        fieldnames = []
+        l = len(tables[0])
+        for table in tables:
+            if len(table)!=l:
+                raise RuntimeError('In HConcatTable length of the individual tables differ')
+            self.tables.append(table)
+            fieldnames.extend(table.fieldnames)
+        self.set_fieldnames(fieldnames)
+
+    def getRow(self,i):
+        row = []
+        for table in self.tables:
+            #row.extend(table[i])
+            row.extend(table.getRow(i))
+        return row
+
+    def __len__(self):
+        return len(self.tables[0])
+        
+class SelectRows(Table):
+    """Creates a view by selecting specific rows from an
+    existing table, in the given order.
+    """
+    def __init__(self, table, indexes):
+        self.table = table
+        self.indexes = indexes
+        #///***///***///***
+        # ARGHHH!!! FIXME:
+        try: self.set_fieldnames(table.fieldnames)
+        except: pass
+
+    def getRow(self,i):
+        #return self.table[self.indexes[i]]
+        #///***///***///***
+        # TODO: make sure this is OK (not that ^)
+        return self.table.getRow(self.indexes[i])
+
+    def __len__(self):
+        return len(self.indexes)
+
+class UpsideDownTable(Table):
+    """Creates a view of a table with rows in reverse order"""
+    def __init__(self, table):
+        self.table = table
+        try: self.set_fieldnames(table.fieldnames)
+        except: pass
+
+    def getRow(self,i):
+        #return self.table[len(self.table)-1-i]
+        return self.table.getRow(len(self.table)-1-i)
+
+    def __len__(self):
+        return len(self.table)
+
+class AddFieldsTable(Table):
+    def __init__(self, table, extra_fields):
+        self.table= table
+        self.extra_fields= extra_fields
+        self.set_fieldnames(table.fieldnames + extra_fields)
+
+        
+    def getRow(self,i):
+        return self.table.getRow(i) + ['' for f in self.extra_fields]
+        
+    def __len__(self):
+        return len(self.table)
+    
+
+class SelectFields(Table):
+    """Returns a view of a table by selecting only some of the fields.
+    """
+
+    def __init__(self, table, selected_fields, newnames=[], must_exist=True):
+        """Returns a view of table with only the selected_fields (in that order).
+        selected_fields is a list of fieldnames or fieldpositions in the original table.
+        The fields can optionally be renamed by giving a non-empty list of newnames
+        (which must be the same length as the selected_fields list)"""
+        
+        if len(newnames)==0:
+            newnames = selected_fields
+        elif len(newnames)!= len(selected_fields):
+            raise ValueError('In SelectFields invalid specification of newnames len(newnames) is not 0 and is different from len(selected_fields)')
+
+        self.table = table
+        self.fieldnums = []
+        fieldnames = []
+        for field,newname in zip(selected_fields,newnames):
+            if isinstance(field,int):
+                self.fieldnums.append(field)
+                fieldnames.append(table.fieldnames[field])
+            else:
+                try: pos = table.fieldnames.index(field)
+                except ValueError: pos = -1
+                if pos>=0:
+                    self.fieldnums.append(pos)
+                    fieldnames.append(newname)
+                elif must_exist:
+                    raise ValueError('In SelectFields invalid field specification: '+repr(field)+'\n Fields are:'+repr(table.fieldnames))
+        self.set_fieldnames(fieldnames)
+
+    def getRow(self,i):
+        row = self.table[i]
+        return [ row[field] for field in self.fieldnums ]
+
+    def __len__(self):
+        return len(self.table)
+
+def DeleteFields(table, deleted_fields):
+    """Creates a SelectFields table by selecting all table fields _not_ listed
+    in deleted_fields.
+    """
+    selected_fields = [ field for field in table.fieldnames if field not in deleted_fields ]
+    return SelectFields(table,selected_fields)
+
+def GeneratedTable(tablefname, generating_func, list_of_dependencies=[]):
+    """Opens a table with dependencies, regenerating the table when necessary.
+    The last modification time of the table is compared against the last
+    modification time of each file listed in list_of_dependencies.  If some
+    dependency was modified after the table, generating_func is called
+    prior to openning the file.
+    """
+    try:
+        target_mtime = os.path.getmtime(tablefname)
+    except OSError:
+        target_mtime = 0
+
+    dep_mtime = 0
+    for fname in list_of_dependencies:
+        try:
+            mtime = os.path.getmtime(fname)
+        except OSError:
+            mtime = 0
+        if mtime>dep_mtime:
+            dep_mtime = mtime
+
+    if target_mtime==0 or dep_mtime>target_mtime:
+        generating_func()
+
+    return openTable(tablefname)
+
+#     def getRow(self,i):
+#         row = self.table.getRow
+#         t = FieldValues(self.table.fieldnames, self.table[i])
+#         t._rownum_ = i
+#         self.processingfunc(t)
+#         return [ getattr(t,fieldname) for fieldname in self.fieldnames ]
+    
+#     def __len__(self):
+#         return len(self.table)
+
+
+class ProcessFields(Table):
+    """DEPRECATED -- use ProcessFields2 instead
+    Creates a view by mapping a function on each row of an existing table.
+    processingfunc must take a FieldValues object and modify it in place.
+    newfieldnames is a list of names of fields to be added to the already
+    existing fields; these should be created by processingfunc.
+    """
+    def __init__(self, table, processingfunc, newfieldnames=[]):
+        self.table = table
+        self.processingfunc = processingfunc
+        fieldnames = table.fieldnames+newfieldnames
+        self.set_fieldnames(fieldnames)
+
+    def getRow(self,i):
+        t = FieldValues(self.table.fieldnames, self.table[i])
+        t._rownum_ = i
+        self.processingfunc(t)
+        return [ getattr(t,fieldname) for fieldname in self.fieldnames ]
+    
+    def __len__(self):
+        return len(self.table)
+
+class ProcessFields2(Table):
+    """Creates a view by applying a function to each row of an existing table.
+    processingfunc must work in place on a ListWithFieldNames object, to
+    which it can add fields if autoappend is True.
+    """
+    def __init__(self, table, processingfunc, autoappend=False):
+        self.table = table
+        self.processingfunc = processingfunc
+        self.autoappend = autoappend
+        row = table[0]
+        row.autoappend = self.autoappend
+        processingfunc(row)
+        self.set_fieldnames(row.fieldnames)
+
+    def getRow(self,i):
+        row = self.table[i]
+        row.autoappend = self.autoappend
+        self.processingfunc(row)
+        return list(row)
+    
+    def __len__(self):
+        return len(self.table)
+
+class AppendRownum(Table):
+    """Creates a view from an existing table by adding a field that contains
+    the row number.
+    """
+    def __init__(self, table, rownum_fieldname='rownum'):
+        self.table = table
+        fieldnames = table.fieldnames+[rownum_fieldname]
+        self.set_fieldnames(fieldnames)
+
+    def getRow(self,i):
+        #return self.table[i]+[i]
+        return list(self.table.getRow(i))+[i]
+    
+    def __len__(self):
+        return len(self.table)
+
+class AppendConstantFields(Table):
+    """Creates a view from an existing table by appending a list of fields with 
+    constant values.
+    """
+    def __init__(self, table, extra_fieldnames, extra_fieldvals):
+        self.table = table
+        self.set_fieldnames(table.fieldnames+extra_fieldnames)
+        self.extra_fieldvals = extra_fieldvals
+
+    def getRow(self,i):
+        #return self.table[i]+self.extra_fieldvals
+        return self.table.getRow(i)+self.extra_fieldvals
+    
+    def __len__(self):
+        return len(self.table)
+
+class StrippedFieldsTable(Table):
+    """Creates a view from an existing table by stripping all leading
+    and trailing whitespace from text fields.  If stripquotes is true, double
+    quotes and leading and trailing whitespace within those quotes will also
+    be stripped.
+    """
+    def __init__(self, table, stripquotes=True):
+        self.stripquotes = stripquotes
+        self.table = table
+        self.set_fieldnames(table.fieldnames)
+
+    def mystrip(self,val):
+        if isinstance(val,str):
+            val = val.strip()
+            if self.stripquotes and len(val)>=2 and val[0]=='"' and val[-1]=='"':
+                val = val[1:-1].strip()
+        return val
+            
+
+    def getRow(self,i):
+        #return map(self.mystrip, self.table[i])
+        return map(self.mystrip, self.table.getRow(i))
+    
+    def __len__(self):
+        return len(self.table)
+
+
+class CacheTable(Table):
+    """Creates a direct view of an existing table and caches the last
+    max_nrows_in_cache rows fetched in RAM.
+    """
+    def __init__(self, table, max_nrows_in_cache=200):
+        self.table = table
+        self.max_nrows_in_cache = 200
+        self.set_fieldnames(table.fieldnames)
+        self.cached_rownum = []
+        self.cached_rows = []
+
+    def getRow(self,i):
+
+        try:
+            k = self.cached_rownum.index(i)
+            return self.cached_rows[k]
+        except ValueError:
+            row = self.table.getRow(i)
+            if len(self.cached_rows)>=self.max_nrows_in_cache:
+                del self.cached_rows[0]
+                del self.cached_rownum[0]
+            self.cached_rows.append(row)
+            self.cached_rownum.append(i)
+            return row
+            
+    def clearCache(self):
+        self.cached_rownum = []
+        self.cached_rows = []
+    
+    def __len__(self):
+        return len(self.table)
+
+class ShuffleTable(SelectRows):
+    """Creates a view of an existing table with rows randomly permuted.
+    If no seed is given, this will always generate the same permutation.
+    """
+    def __init__(self, table, seed_x=123, seed_y=456):
+        random_array.seed(seed_x,seed_y)
+        indexes = random_array.permutation(len(table))
+        SelectRows.__init__(self, table, indexes)
+        
+
+class FilterTable(SelectRows):
+    """DEPRECATED -- use FilterTable2 instead
+    Creates a view of an existing table by selecting some of the rows
+    according to boolfunc.  An indexfile can also be supplied to save/restore
+    the indexes of rows selected by boolfunc.
+    boolfunc must be a predicate that takes a FieldValues parameter.
+    """
+    def __init__(self, table, boolfunc, indexfile=""):
+            
+        if os.path.isfile(indexfile):
+            indexes = StructFile(indexfile, '<L', 'r')
+            
+        else: # create indexes
+            if indexfile=='':
+                indexes = []
+            else:
+                indexes = StructFile(indexfile+'.tmp', '<L', 'w+')
+            l = len(table)
+            pbar = PBar('Filtering',l)
+            for i in xrange(l):
+                t = FieldValues(table.fieldnames, table[i])
+                if boolfunc(t):
+                    indexes.append(i)
+                pbar.update(i)
+            pbar.close()
+            if indexfile!='':
+                os.rename(indexfile+'.tmp',indexfile)
+
+        SelectRows.__init__(self, table, indexes)
+
+
+class FilterTable2(SelectRows):
+    """Creates a view of an existing table by selecting some of the rows
+    according to boolfunc.  An indexfile can also be supplied to save/restore
+    the indexes of rows selected by boolfunc.
+    boolfunc must be a predicate that takes a ListWithFieldNames parameter.
+    """
+    def __init__(self, table, boolfunc, indexfile=""):
+
+        struct_fmt= '<L'
+            
+        if os.path.isfile(indexfile):
+            indexes = StructFile(indexfile, struct_fmt, 'r')
+            
+        else: # create indexes
+            if indexfile=='':
+                indexes = []
+            else:
+                indexes = StructFile(indexfile+'.tmp', struct_fmt, 'w+')
+            l = len(table)
+            pbar = PBar('Filtering',l)
+            for i in xrange(l):
+                if boolfunc(table[i]):
+                    indexes.append(i)
+                pbar.update(i)
+            pbar.close()
+            if indexfile!='':
+                indexes.close()
+                os.rename(indexfile+'.tmp',indexfile)
+                indexes= StructFile(indexfile,struct_fmt)
+
+        SelectRows.__init__(self, table, indexes)
+
+class BootstrapTable(SelectRows):
+    """Creates a table by sampling (with replacement) 
+       the rows of an original table.
+       Unweighted version: tables (original and sample) are of same length.
+       Weighted version: total weight of sample table is made close to the original's
+    """
+    def __init__(self, table, manualseed="", weightfield="", indexfile="", verbose=1):
+
+        def binsearch(x,vec):
+            vec = [0] + vec
+            n = len(vec)
+            il = 0
+            ih = n-1
+            while ih - il > 1: 
+                im = int((ih+il)/2)
+                if x > vec[im]:
+                    il = im
+                else:
+                    ih = im
+            return il
+            
+        struct_fmt= '<L'
+            
+        if os.path.isfile(indexfile):
+            indexes = StructFile(indexfile, struct_fmt, 'r')
+            
+        else: # create indexes
+            if indexfile=='':
+                indexes = []
+            else:
+                indexes = StructFile(indexfile+'.tmp', struct_fmt, 'w+')
+            l = len(table)
+            if manualseed: seed(manualseed)
+            if not weightfield:
+                if verbose:
+                    pb = PBar('Sampling table',l) 
+                for i in range(l):
+                    if verbose:
+                        pb.update(i)
+                    indexes.append(randint(0,l-1))
+                if verbose:
+                    pb.close()
+            else:
+                cumw = l*[0]
+                totw = 0
+                for i in range(l):
+                    totw += table[i][weightfield]
+                    cumw[i] = totw
+                totw_sam = 0
+                if verbose:
+                    pb = PBar('Sampling table', int(totw))
+                while totw_sam < totw:
+                    if verbose:
+                        pb.update(int(totw_sam))
+                    cw = totw * uniform(0,1)
+                    i = binsearch(cw,cumw)
+                    indexes.append(i)
+                    wi = table[i][weightfield]
+                    totw_sam += wi
+                if wi < 2*(totw_sam - totw):
+                    indexes.pop()
+                if verbose:
+                    pb.close()
+            if indexfile!='':
+                indexes.close()
+                os.rename(indexfile+'.tmp',indexfile)
+                indexes= StructFile(indexfile,struct_fmt)
+
+        SelectRows.__init__(self, table, indexes)
+
+class SortTable(SelectRows):
+    """Creates a view of an existing table by sorting the rows in
+    ascending order of sortfields.  An index file name can be supplied
+    to save/restore the index so that it does not need to be recalculated.
+    """
+    def __init__(self, table, sortfields, indexfile="", reverse=False, verbose=1):
+            
+        if os.path.isfile(indexfile):
+            indexes = StructFile(indexfile, '<L', 'r')
+            
+        else: # create indexes
+            sortfields = [ isinstance(field,int) and field or table.fieldnames.index(field) for field in sortfields ]
+            keylist = []
+            l = len(table)
+            if verbose:
+                pbar = PBar('Reading fields to be sorted',l)
+            for i in xrange(l):
+                row = table[i]
+                keylist.append( [ row[field] for field in sortfields ]+[i] )
+                if verbose:
+                    pbar.update(i)
+            keylist.sort(reverse=reverse)
+            if indexfile=="":
+                indexes = [ row[-1] for row in keylist ]
+            else:
+                indexes = StructFile(indexfile+'.tmp', '<L', 'w+')
+                if verbose:
+                    pbar = PBar('Writing sorted index',l)
+                for row,i in zip(keylist,xrange(l)):
+                    indexes.append(row[-1])
+                    if verbose:
+                        pbar.update(i)
+                os.rename(indexfile+'.tmp',indexfile)
+
+        SelectRows.__init__(self, table, indexes)
+
+
+class EfficientSortTable(SelectRows):
+    """Creates a view of an existing table by sorting rows in ascending order
+    of a single numeric field.  Can be more memory efficient than SortTable.
+    """
+    def __init__(self, table, sortfield, sortfieldtype='d', indexfile=""):
+            
+        if os.path.isfile(indexfile):
+            indexes = StructFile(indexfile, '<L', 'r')
+            
+        else: # create indexes
+            if not isinstance(sortfield,int):
+                sortfield = table.fieldnames.index(sortfield)
+
+            l = len(table)
+            keys = array(type=sortfieldtype, shape=(l,) )
+            pbar = PBar('Reading fields to be sorted',l)
+            for i in xrange(l):
+                key = table[i][sortfield]
+                keys[i] = float(key)
+                pbar.update(i)
+
+            indexarray = argsort(keys)
+            if indexfile=="":
+                indexes = indexarray
+            else:
+                indexes = StructFile(indexfile+'.tmp', '<L', 'w+')
+                pbar = PBar('Writing sorted index',l)
+                for pos in indexarray:
+                    indexes.append(pos)
+                    pbar.update(i)
+                os.rename(indexfile+'.tmp',indexfile)
+
+        SelectRows.__init__(self, table, indexes)
+
+
+        
+
+def group_by(table, selected_fields):
+    """Creates a generator that acts like a list of sub tables, where the rows
+    are grouped by field values of selected_fields to make each sub table.
+    """
+    fieldnums = []
+    for field in selected_fields:
+        try:
+            if isinstance(field,int):
+                fieldnums.append(field)
+            else:
+                fieldnums.append(table.fieldnames.index(field))
+        except ValueError:
+            raise ValueError('In table.group_by invalid field specification: '+repr(field)+'\n Fields are:'+repr(table.fieldnames))
+
+    row = table[0]
+    current_values = [ row[field] for field in fieldnums ]
+    start = 0
+    # print len(table)
+    for stop in xrange(len(table)):
+        # print stop
+        row = table[stop]
+        values = [ row[field] for field in fieldnums ]
+        if values!=current_values:
+            yield table[start:stop]
+            current_values = values
+            start = stop
+    yield table[start:len(table)]
+
+
+def float_or_NaN_if_empty(x):
+    """Returns a float from another value, or NaN if the parameter is
+    the empty string.
+    """
+    if x=='':
+        return fpconst.NaN
+    else:
+        return float(x)
+
+
+class FieldTypeStats:
+
+    def __init__(self):
+        self.n = 0
+        self.n_missing_value = 0
+        self.n_numerical_value = 0
+        self.n_other_value = 0
+        self.n_numerical_type = 0
+        self.min_value = None
+        self.max_value = None
+                
+    def __repr__(self):
+        return 'FieldTypeStats(' + \
+        'n ='+ repr(self.n) + \
+        ', n_missing_value ='+ repr(self.n_missing_value) + \
+        ', n_numerical_value ='+ repr(self.n_numerical_value) + \
+        ', n_other_value ='+ repr(self.n_other_value) + \
+        ', n_numerical_type ='+ repr(self.n_numerical_type) + \
+        ', min_value ='+ repr(self.min_value) + \
+        ', max_value ='+ repr(self.max_value) + ')'       
+
+    def update(self, val):
+        self.n += 1
+        if isinstance(val, (int, long, float, bool)):
+            self.n_numerical_type += 1
+            
+        if val is None or val=="":
+            self.n_missing_value += 1
+        else:
+            try:
+                numval = float(val)
+                self.n_numerical_value +=1
+                if self.min_value is None:
+                    self.min_value = numval
+                    self.max_value = numval
+                else:
+                    self.min_value = min(numval, self.min_value)
+                    self.max_value = max(numval, self.max_value)
+            except ValueError:
+                self.n_other_value += 1
+
+    def smap_start_id(self):
+        if self.max_value is None:
+            return 1
+        else:
+            return int(self.max_value+1)
+        
+
+class StringMapFile:
+
+    def __init__(self, filepath, openmode='a', startid=1):
+        if openmode not in "rwa":
+            raise ValueError("openmode must be one of 'r','w','a'")
+        self.startid = startid
+        self.maxid = None
+        self.filepath = filepath
+        self.openmode = openmode
+        self.map = {}
+        if openmode=='r':
+            self.load_map()
+        elif openmode=='w':
+            self.f = open(filepath,'w')
+        elif openmode=='a':
+            if os.path.exists(filepath):
+                self.load_map()
+            self.f = open(filepath,'a')
+
+    def load_map(self):
+        f = open(self.filepath,'r')
+        for row in f:
+            pos = row.rfind(' ')
+            strval = row[1:pos-1]
+            numid = float(row[pos+1:])
+            self.map[strval] = numid
+            if self.maxid is None:
+                self.maxid = numid
+            else:
+                self.maxid = max(numid, self.maxid)
+        f.close()
+
+    def __getitem__(self,strval):
+        return self.map[str(strval)]
+
+    def __len__(self):
+        return len(self.map)
+
+    def append(self, strval, numid=None):
+        strval = str(strval) # make sure it's a string
+        if numid is None:
+            if self.maxid is None:
+                self.maxid = self.startid
+            else:
+                self.maxid += 1
+            numid = self.maxid
+        # self.f.seek(0,2)
+        print >> self.f, '"'+strval.replace('"','\\"')+'"', numid
+        self.map[strval] = numid
+        self.flush()
+        return numid
+            
+    def close(self):
+        self.f.close()
+        
+    def flush(self):
+        self.f.flush()
+
+def computeFieldTypeStats(table, show_progress=True):
+    w = table.width()
+    stats = []
+    for j in xrange(w):
+        stats.append(FieldTypeStats())
+    if show_progress:
+        pbar = PBar('Computing stats',len(table))
+    i = 0
+    for row in table:
+        for j in xrange(w):
+            stats[j].update(row[j])
+        if show_progress:
+            pbar.update(i)
+        i += 1
+    if show_progress:
+        pbar.close()
+
+    return stats
+
+def saveFieldTypeStats(stats, fieldnames, statsfname):
+    f = open(statsfname,'w')
+    for fieldname,stat in zip(fieldnames,stats):
+        print >> f, fieldname, ': ', stat
+    f.close()
+
+def saveTableAsVMAT(table, vmat_name, stringmap_reference_dir="", show_progress=True, overwrite=True):
+    ext = os.path.splitext(vmat_name)[1]
+    assert ext in ['.dmat', '.pmat']
+
+    ## What should we do if the VMat aleardy exists?
+    if os.path.exists(vmat_name):
+        ## Erase VMat
+        if overwrite:
+            vmat_metadata = vmat_name + '.metadata'
+            rm_cmd = 'rm -fr %s %s' %(vmat_name, vmat_metadata)
+            subprocess.Popen(rm_cmd, shell=True, bufsize=0, stdout=subprocess.PIPE).stdout.read()
+        else:
+            raise RuntimeError, 'File or directory path "%s" already exists.' %vmat_name
+
+    if isinstance(table, str):
+        table = openTable(table)
+    width = table.width()
+
+    if ext == '.pmat':
+        vmat = FileVMatrix(filename=vmat_name, width=width, length=0)
+    elif ext == '.dmat':
+        vmat = DiskVMatrix(dirname=vmat_name, writable=True, width=width, length=0)
+    print 'len(table) = ', len(table)
+    print 'table.fieldnames = ', table.fieldnames
+    print 'vmat_name = ', vmat_name
+    print 'vmat = ', vmat
+    vmat.declareFieldNames(table.fieldnames)
+    vmat.saveFieldInfos()
+
+    if stringmap_reference_dir=="":
+        if show_progress:
+            i = 0
+            pbar = PBar('Saving '+vmat_name, len(table))
+        for row in table:
+            vmat.appendRow( [ float_or_NaN_if_empty(e) for e in row ] )
+            if show_progress:
+                pbar.update(i)
+                i += 1
+        if show_progress:
+            pbar.close()
+
+    else: # we have a reference directory from which to retrieve/store stringmaps
+        refdir = os.path.join(stringmap_reference_dir,"mappings")
+        if not os.path.isdir(refdir):
+            os.makedirs(refdir)
+
+        logdir = os.path.join(stringmap_reference_dir,"logdir")
+        logdir = os.path.join(logdir, time.strftime('%Y-%m-%d_%H:%M:%S'))
+        if not os.path.isdir(logdir):
+            os.makedirs(logdir)
+        statsfname = os.path.join(logdir,'stats.txt')
+
+        # get and save field type stats
+        stats = computeFieldTypeStats(table, show_progress)
+        saveFieldTypeStats(stats, table.fieldnames, statsfname)
+        
+        stringmaps = {}
+        extramappings = {}
+
+        # now loop over the rows and save as pmat, completing the mapping if needed
+        if show_progress:
+            i = 0
+            pbar = PBar('Saving '+vmat_name, len(table))
+        for row in table:
+            numrow = []
+            for j in xrange(width):
+                val = row[j]
+                if val is None or val=="":
+                    numrow.append(fpconst.NaN)
+                else:
+                    try:
+                        numrow.append(float(val))
+                    except ValueError:
+                        strval = str(val)
+                        fieldname = table.fieldnames[j]
+                        try:
+                            smap = stringmaps[fieldname]
+                        except KeyError:                            
+                            smap = StringMapFile(os.path.join(refdir,fieldname+".smap"),"a",stats[j].smap_start_id())
+                            stringmaps[fieldname] = smap
+                        try:
+                            numval = smap[strval]
+                        except KeyError:
+                            numval = smap.append(strval)
+                            try:
+                                extramap = extramappings[fieldname]
+                            except KeyError:
+                                extramap = StringMapFile(os.path.join(logdir,fieldname+".smap"),"w")
+                                extramappings[fieldname] = extramap
+                            extramap.append(strval,numval)
+                        numrow.append(numval)
+                        
+            vmat.appendRow(numrow)
+            if show_progress:
+                pbar.update(i)
+                i += 1
+        if show_progress:
+            pbar.close()
+
+        # now close the maps
+        for map in stringmaps.values():
+            map.close()
+        for map in extramappings.values():
+            map.close()
+
+        # Create a symbolic link from vmat_name.metadata/FieldInfo -> refdir
+        if show_progress:
+            inmetadata = os.path.join(vmat_name+'.metadata', 'FieldInfo')
+            absrefdir = os.path.abspath(refdir)
+            print ">>> Creating symbolic link: ",inmetadata,'->',absrefdir
+        os.symlink(absrefdir, inmetadata)
+
+        if show_progress:
+            print ">>> If any, extra added mappings have been written in", logdir
+            print "    You should consult this directory to check if there's anything suspect."
+            print "    (Did you expect any new previously unseen strings?)"
+            print
+
+    ## Close vmat file
+    vmat.flush()
+    del(vmat)
+            
+def saveTable(table, fname, show_progress=True, stringmap_reference_dir=""):
+    """Saves a table to disk in the format specified by the extension of fname.
+    Can be one of:
+    - .pmat : PLearn matrix: table of reals in binary format
+    - .ptab : table compressed in chunks using zlib
+    - .ptabdir : TBD! do not use!
+    - .txt : tab separated text file
+    table can be a Table object or the name of the file containing the table.
+    When the extension is .pmat, stringmap_reference_dir could be used for string mapping.
+    """
+    if isinstance(table,str):
+        table = openTable(table)
+    if fname.endswith('.pmat') or fname.endswith('.dmat'):
+        saveTableAsVMAT(table, fname, stringmap_reference_dir, show_progress)
+    else:
+        if fname.endswith('.ptab'):
+            m = CompressedTableFile(fname, 'w', table.fieldnames)
+        elif fname.endswith('.ptabdir'):
+            m = TableDir(fname, 'w+', table.fieldnames)
+        elif fname.endswith('.txt'):
+            m = TableFile(fname, 'w+', table.fieldnames)
+        else:
+            raise ValueError('Invalid extension for destination file '+fname)
+
+        if show_progress:
+            pbar = PBar('Saving '+fname,len(table))
+        i = 0
+        for row in table:
+            m.append( [ str(e) for e in row ] )
+            if show_progress:
+                pbar.update(i)
+            i += 1
+        m.close()
+        if show_progress:
+            pbar.close()
+
+
+class VMatTable(Table):
+    """
+    Table that wraps a PLearn VMatrix
+    """
+    __buflen= 256
+    
+    def __init__(self, vmat, get_strings=False):
+        self.vmat= vmat
+        self.set_fieldnames(vmat.fieldNames())
+        self.buf= None
+        self.bufstart= -self.__buflen
+        self.get_strings= get_strings
+
+    def getRow(self, i):
+        if self.get_strings:
+            r= []
+            for j in range(self.width()):
+                r+= [self.vmat.getString(i,j)]
+            return r
+        if i < self.bufstart or i >= self.bufstart+self.__buflen:
+            buflen= min(self.__buflen, len(self)-i)
+            self.buf= self.vmat.subMat(i, 0, buflen, self.width()).getMat()
+            self.bufstart= i
+        return self.buf[i-self.bufstart]
+
+    def __len__(self):
+        return int(self.vmat.length)
+
+
+class H5Table(Table):
+    """
+    Table from an HDF5 array
+    """
+    def __init__(self, filename, arrayname, fieldnames=None, mode='r'):
+        import tables
+        if mode!='r': raise NotImplementedError
+        self.file= tables.openFile(filename)
+        self.array= self.file.getNode(arrayname)
+        if fieldnames:
+            self.set_fieldnames(fieldnames)
+        else:
+            self.set_fieldnames(['f'+str(i) for i in range(len(self.array[0]))])
+        
+    def getRow(self, i):
+        return self.array[i]
+
+    def __len__(self):
+        return len(self.array)
+
+class AMATTable(TableFile):
+    """
+    read-only for now
+    """
+    def __init__(self, fname, openmode):
+        self.struct_format = '<Q'
+        self.fname = fname
+        self.set_filepath(fname)
+        self.closed = False
+        self.openmode = openmode
+        self.tolerate_different_field_count = True
+        self.separator= None
+        indexfname = self.fname+'.idx'
+        if openmode=='r':
+            self.f = open(fname,'r')
+            l= self.f.readline().strip('\r\n').split(self.separator)
+            n= 0
+            while l[0] != '#:':
+                l= self.f.readline().strip('\r\n').split(self.separator)
+                n+= 1
+            fieldnames = l[1:]
+            fieldnames = [f for f in fieldnames if f != '']
+            self.set_fieldnames(fieldnames)
+
+            if not os.path.isfile(indexfname):
+                build_row_index_file(fname,indexfname,self.struct_format, n)
+            self.index = StructFile(indexfname,self.struct_format,'r')
+        else:
+            raise ValueError("Invalid value for openmode ("+`openmode`+" Must be 'r'")
+
+
+class JoinTable(Table):
+    def __init__(self, tables, key_fields, indexfname= None):
+        ntables= len(tables)
+        assert(ntables >= 2)
+        self.tables= tables #should already be sorted by key_fields!
+        self.key_fields= key_fields
+
+        # take all fieldnames from all files
+        fieldnames= []
+        nextid= 1 # to differentiate dup. field names
+        for t in tables:
+            for f in t.fieldnames:
+                while f in fieldnames: # add suffix if already there
+                    f+= '-'+str(nextid)
+                    nextid+= 1
+                fieldnames.append(f)
+        self.set_fieldnames(fieldnames)
+
+        idxstructformat= '!'+'Q'*ntables
+        if indexfname and os.path.isfile(indexfname):
+            self.index= StructFile(indexfname, idxstructformat, 'r')
+        else: # build index
+            rowidx= [0]*ntables
+            index= []
+            pbar= PBar('Building Join Index', len(tables[0]))
+            while rowidx[0] < len(tables[0]):
+                match= True
+                key= [tables[0][rowidx[0]][f] for f in key_fields]
+                for i in range(1,ntables):
+                    while rowidx[i] < len(tables[i]) and key > [tables[i][rowidx[i]][f] for f in key_fields]:
+                        rowidx[i]+= 1 #try to find a match
+                    if rowidx[i] == len(tables[i]) or key != [tables[i][rowidx[i]][f] for f in key_fields]:
+                        match= False
+                        break
+                if match: index.append(copy.copy(rowidx))
+                rowidx[0]+= 1
+                pbar.update(rowidx[0])
+            del pbar
+            self.index= index
+            if indexfname and not os.path.isfile(indexfname):
+                indexfile= StructFile(indexfname, idxstructformat, 'w+')
+                for i in index: indexfile.append(i)
+
+    def getRow(self, i):
+        rowidx= self.index[i]
+        row= []
+        for i,t in zip(rowidx, self.tables):
+            row+= t[i]
+        return row
+
+    def __len__(self):
+        return len(self.index)
+
+
+def openTable(tablespec,openmode='r'):
+    """Opens a file containing a representation of a Table object.
+    The format of the file is determined by its extension like for saveTable,
+    but the '.pytable' type is also supported; this is a file containing
+    Python code to create the table.
+    openmode is used only for CompressedTableFile,PMatTable,TableDir and
+    TableFile
+    """
+    
+    if isinstance(tablespec,str):
+        if tablespec.endswith('.pytable'):
+            olddir = os.getcwd()
+            try:
+                dirname, filename = os.path.split(os.path.abspath(tablespec))
+                os.chdir(dirname)
+                vars = {}
+                execfile(filename, vars)
+            finally:
+                os.chdir(olddir)
+            table = vars['result']
+            table.set_filepath(tablespec)
+            return table
+        elif tablespec.endswith('.ptab'):
+            return CompressedTableFile(tablespec,openmode)
+        elif tablespec.endswith('.csv'):
+            return CSVTable(tablespec, openmode)
+        elif tablespec.endswith('.pmat'):
+            return PMatTable(tablespec,openmode)
+        elif tablespec.endswith('.ptabdir'):
+            return TableDir(tablespec,openmode)
+        elif tablespec.endswith('.amat'):
+            return AMATTable(tablespec,openmode)
+        elif tablespec.endswith('.dmat') or tablespec.endswith('.vmat'):
+            from plearn import pyext
+            vm= pyext.AutoVMatrix(filename= tablespec)
+            return VMatTable(vm)
+        elif tablespec.endswith('.psave'):
+            from plearn import pyext
+            vm= pyext.loadObject(tablespec)
+            return VMatTable(vm)
+        else:
+            return TableFile(tablespec,openmode)
+    # otherwise, assume tablespec is already a Table
+    return tablespec
+
+# vim: filetype=python:expandtab:shiftwidth=4:tabstop=8:softtabstop=4 :

Added: trunk/python_modules/plearn/table/tablestat.py
===================================================================
--- trunk/python_modules/plearn/table/tablestat.py	2009-06-02 21:55:31 UTC (rev 10226)
+++ trunk/python_modules/plearn/table/tablestat.py	2009-06-02 22:59:44 UTC (rev 10227)
@@ -0,0 +1,720 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
+"""
+tablestat.py
+
+Copyright (C) 2005 ApSTAT Technologies Inc.
+
+This file was contributed by ApSTAT Technologies to the
+PLearn library under the following BSD-style license: 
+
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+"""
+
+# Author: Pascal Vincent
+
+import numpy.numarray as numarray
+import numpy
+import bisect
+import math
+from numpy.numarray import *
+from plearn.table.table import *
+
+def smartdiv(a,b):
+    try: return a/b
+    except: return 'Error'
+
+def smartpercent(a,b):
+    try: return str(a*100./b)+' %'
+    except: return 'Error'
+
+def num2str(num):
+    try:
+        if isinstance(num,float) and math.floor(num)==num:
+            return str(int(num))
+        else:
+            return str(num)
+    except OverflowError:
+        #raise OverflowError('in num2str for num '+repr(num))
+        return repr(num)
+
+## def interval_to_string(interval):
+##     if isinstance(interval,str):
+##         return interval
+##     elif isinstance(interval,tuple):
+##         include_low, low, high, include_high = interval
+##         if include_low: repr = '[ '
+##         else: repr = '] '
+##         repr += num2str(low)+', '+num2str(high)
+##         if include_high: repr += ' ]'
+##         else: repr += ' ['
+##         return repr
+##     return num2str(interval)
+
+
+class Interval:
+    """Defines an interval between values low and high where include_low and include_high
+    are booleans that indicate where the interval is closed or open.
+    If low==high this is to be understood as a point value"""    
+
+    def __init__(self, include_low, low, high, include_high):
+        if low>high:
+            raise ValueError('Interval must have low<=high')
+        self.include_low = include_low
+        self.low = low
+        self.high = high
+        self.include_high = include_high
+
+    def __str__(self):
+        if self.low==self.high:
+            return num2str(self.low)
+        else:
+            return repr(self)
+
+    def __repr__(self):
+        if self.include_low: res = '[ '
+        else: res = '] '
+        res += num2str(self.low)+', '+num2str(self.high)
+        if self.include_high: res += ' ]'
+        else: res += ' ['
+        return res
+
+    def __contains__(self, item):
+        if not isinstance(item,Interval): # assume item is a number 
+            if self.include_low:
+                res = item>=self.low
+            else:
+                res = item>self.low
+            if self.include_high:
+                res = res and (item<=self.high)
+            else:                
+                res = res and (item<self.high)
+        else: # item is an Interval
+            if self.include_low:
+                res = item.low>=self.low
+            else:
+                res = (item.low>self.low) or (item.low==self.low and not item.include_low)
+            if self.include_high:
+                res = res and (item.high<=self.high)
+            else:
+                res = res and (item.high<self.high or (item.high==self.high and not item.include_high))
+        return res
+
+    def __add__(self, other):
+        """union of self and other (result must be an interval, or exception is raised"""
+        if self.low<=other.low:
+            low = self.low
+            if self.low==other.low:
+                include_low = self.include_low or other.include_low
+            else:
+                include_low = self.include_low
+
+            if self.high<other.low or (self.high==other.low and not (self.include_high or other.include_low)):
+                raise ValueError('Union of these 2 intervals is not an interval')
+
+            if self.high>other.high:
+                high = self.high
+                include_high = self.include_high
+            elif other.high>self.high:
+                high = other.high
+                include_high = other.include_high
+            else: # self.high==other.high
+                high = self.high
+                include_high = self.include_high or other.include_high
+            return Interval(include_low, low, high, include_high)
+        else:
+            return other+self
+
+
+class AutoDomain:
+
+    def __init__(self, maxnstrings=25, maxnnumbers=25):
+
+        # maximum number of strings recorded in strings_index
+        # (actually strings_index might end up containing one or two extra
+        # special strings: the empty string '' and the '/OTHER/' string.)
+        self.maxnstrings = maxnstrings
+        self.maxnnumbers = maxnnumbers
+        
+        # number of allocated indexes (the index method will return indexes witihn 0..nindexes_-1
+        self.nindexes_ = 0
+
+        # maps non float convertible strings to their index
+        # It will include the empty string '' if it was encountered
+        # It will include the '/OTHER/' string if we exceed maxnstrings registered strings
+        self.strings_index = {}
+
+        # sorted list of registered numbers 
+        self.numbers = []
+        # numbers_index[k] is the integer index of float value numbers[k]
+        self.numbers_index = []
+        # less_than_index[k] is the integer index of float values <numbers[k] and >number[k-1]
+        # Note 1: less_than_index is one element longer than numbers (and numbers_index),
+        # so that its last element is the index for values greater than the last of numbers.
+        # (its first element is the index for values smaller than the first of numbers).
+        # Note 1: less_than_index is empty as long as numbers has not been filled (len < maxnnumbers)
+        # Then it is filled with -1, before recruiting indexes in it.
+        self.less_than_index = []
+
+        # min and max number values encountered (stays None if no numerical value is encountered).
+        self.min_val = None
+        self.max_val = None
+
+    def nindexes(self):
+        """Returns the total number of recorded indexes."""
+        return self.nindexes_
+
+    def index(self, x):
+        """Will return the integer index associated with value x. If x does
+        not map to an index already in the domain then the domain will
+        be extended to include x and a corresponding new index will be
+        associated to that extension and returned. """ 
+
+        v = None
+        
+        try: v = float(x)
+        except ValueError: pass
+
+        if v is None or (isinstance(x,str) 
+                         and len(x)>=2
+                         and x[0]=='0' and x[1]!='.'): # handle it as a string
+            s = str(x).strip()
+            id = self.strings_index.get(s)
+            #if not id: # unknown string
+            if id == None: # unknown string
+                
+                if len(self.strings_index)<self.maxnstrings:
+                    # we haven't reached maxnstrings so let's add this new one
+                    id = self.nindexes_
+                    self.strings_index[s] = id
+                    self.nindexes_ += 1
+                else: # we have reached maxnstrings
+                    if s=='': # it's the empty string, add it all the same
+                        id = self.nindexes_
+                        self.strings_index[s] = id
+                        self.nindexes_ += 1
+                    else: # look for '/OTHER/' 
+                        id = self.strings_index.get('/OTHER/')
+                        if not id: # no /OTHER/ : add it
+                            id = self.nindexes_
+                            self.strings_index['/OTHER/'] = id
+                            self.nindexes_ += 1                            
+
+        else: # handle it as a float v
+            pos = bisect.bisect_left(self.numbers, v)
+            if pos < len(self.numbers) and self.numbers[pos]==v:
+                # we have already registered that value
+                id = self.numbers_index[pos]
+            else: # this is not a registered value
+
+                if self.min_val is None:
+                    self.min_val = v
+                    self.max_val = v
+                else:
+                    self.min_val = min(self.min_val, v)
+                    self.max_val = max(self.max_val, v)
+
+                if len(self.numbers)<self.maxnnumbers: # self.numbers not full: insert the new value
+                    self.numbers.insert(pos,v)
+                    id = self.nindexes_
+                    self.numbers_index.insert(pos,id)
+                    self.nindexes_ += 1
+                    if len(self.numbers)==self.maxnnumbers: # we've filled self.numbers
+                        if self.min_val<0. and self.max_val>0.:
+                            # insert 0. if not already there
+                            pos = bisect.bisect_left(self.numbers, 0.)
+                            if self.numbers[pos]!=0.:
+                                self.numbers.insert(pos,0)
+                                self.numbers_index.insert(pos,self.nindexes_)
+                                self.nindexes_ += 1
+                        # fill the less_than_index with -1
+                        self.less_than_index = [-1]*(len(self.numbers)+1)                        
+                else: # self.numbers is full 
+                    id = self.less_than_index[pos]
+                    if id<0:
+                        id = self.nindexes_
+                        self.less_than_index[pos] = id
+                        self.nindexes_ += 1
+
+        return id
+
+    def string_domain(self):
+        """This will return an ordered list of (xstring, index) for the string part of the domain"""
+        keys = self.strings_index.keys()
+        keys.sort()
+        dom = [ (key,self.strings_index[key]) for key in keys ]
+        return dom
+        
+    def number_domain(self):
+        """This will return an ordered list of (interval, index) for the numerical part of the domain
+        interval is an instance of Interval 
+        """
+        dom = []
+        if self.min_val and self.less_than_index and self.less_than_index[0]>=0:
+            interval = Interval(True, self.min_val, self.numbers[0], False)
+            dom.append( (interval, self.less_than_index[0]) )
+        for k in xrange(len(self.numbers)):
+            interval = Interval(True, self.numbers[k], self.numbers[k], True)
+            dom.append((interval, self.numbers_index[k]))
+            if self.less_than_index and self.less_than_index[k+1]>=0:
+                if k<len(self.numbers)-1:
+                    interval = Interval(False, self.numbers[k], self.numbers[k+1], False)
+                    dom.append( (interval, self.less_than_index[k+1]) )
+                elif self.max_val:
+                    interval = Interval(False, self.numbers[k], self.max_val, True)
+                    dom.append( (interval, self.less_than_index[k+1]) )
+        return dom
+
+    def domain(self):
+        return self.string_domain()+self.number_domain()
+
+    def domain_descr_id(self):
+        do = self.domain()
+        descriptions = []
+        ids = []
+        for range,id in do:            
+            descriptions.append(str(range))
+            ids.append(id)
+        return descriptions,ids
+
+
+class AutoCube:
+
+    def __init__(self, nkeys, nvalues=1, typecode='f8'):
+        self.nvalues = nvalues
+        shape = [1]*nkeys
+        if nvalues>0:
+            shape.append(nvalues)
+        self.data = zeros(shape, typecode)
+
+    def enlarge(self,idx):
+        if isinstance(idx,int):
+            idx = [idx] # make it a list
+        oldshape = self.data.shape
+        newshape = list(oldshape)
+        for k in xrange(len(idx)):
+            newshape[k] = max(newshape[k], idx[k]+1)
+        newdata = zeros(newshape, numarray.typefrom(self.data))
+        slicespec = [ slice(0,dim) for dim in oldshape ]
+        newdata[slicespec] = self.data
+        self.data = newdata
+
+    def __getitem__(self,idx):
+        try:
+            return self.data[idx]
+        except IndexError:
+            self.enlarge(idx)
+        return self.data[idx]
+            
+    def __setitem__(self,idx,val):
+        try:
+            self.data[idx] = val
+        except IndexError:
+            self.enlarge(idx)
+        self.data[idx] = val
+        
+
+def combinations(lists):    
+    if not lists:
+        yield []
+    else:
+        for i in lists[0]:
+            for c in combinations(lists[1:]):
+                yield [i]+c
+
+def all_combinations(lists):
+    return [ comb for comb in combinations(lists) ]
+    
+
+def domain_union( range_ids_a, range_ids_b ):
+    ra,idsa = range_ids_a
+    rb,idsb = range_ids_b
+    
+
+
+
+class BaseTableStats:
+
+    def __init__(self, var_combinations, var_domains, valuenames=[], weightvar='', full_shuffle_stats=True):
+        """
+        var_combinations is a list of tuples of variable names
+        var domains maps variable names to AutoDomain
+        valuef is a function that takes a row and returns a tuple of values
+        valuenames is a list of variable names for the tuples returned by valuef
+        """
+        self.var_combinations = var_combinations
+        self.var_domains = var_domains
+        self.valuenames = valuenames
+        self.weightvar = weightvar
+        # self.valuef = valuef
+        self.cubes = {}
+        self.nsamples = 0
+        self.full_shuffle_stats = full_shuffle_stats
+        
+        for vars in var_combinations:
+            self.cubes[tuple(vars)] = AutoCube(len(vars), 1+len(self.valuenames))
+
+    def extended_value_names(self):
+        if self.weightvar:
+            return [self.weightvar]+[ self.weightvar+'*'+varname for varname in self.valuenames ]
+        else:
+            return ['_count_']+self.valuenames
+
+##         countvar = self.weightvar
+##         if not countvar:
+##             countvar = '_count_'
+##         valuenames = [countvar]+self.valuenames
+##        return valuenames        
+
+    def update(self, row):
+        # values = self.valuef(row)
+        values = array([1.]+[ float_or_zero(row[valuename]) for valuename in self.valuenames ])
+        if self.weightvar:
+            w = float_or_zero(row[self.weightvar])
+            values *= w
+        for vars,cube in self.cubes.items():
+            indexes = tuple([ self.var_domains[varname].index(row[varname]) for varname in vars ])
+            cube[indexes] += values
+        self.nsamples += 1
+        return self.nsamples
+
+
+    def stringDomain(self, varname):
+        s_domain = self.var_domains[varname].string_domain()
+        return [ (s, [id] ) for s,id in s_domain ]
+
+    def numberDomain(self, varname, condvar, summarize_min_prob = -1.):
+        """Will return an ordered list of (interval, indexes) for the numerical part of the domain
+        interval is an instance of Interval
+        indexes is a list of corresponding cube-indexes for variable varname in a cube
+        corresponding to varname, such that the union of the individual intervals associated with
+        those indexes gives interval (thus the slices corresponding to those indexes must be summed
+        to get the values corresponding to interval).
+        Consecutive intervals will be merged until the largest conditional probability within them
+        reaches summarize_min_prob
+        """
+        
+        num_domain = self.var_domains[varname].number_domain()
+        if not num_domain: # empty list
+            return num_domain
+
+        if summarize_min_prob<=0.:
+            return [ (interval, [id] ) for interval,id in num_domain ]
+        
+        try:
+            cube = self.cubes[(varname, condvar)]
+        except KeyError:
+            cube = self.cubes[(condvar, varname)]
+            cube = transpose(cube,(1,0,2))
+
+        num_ids = [ id for interval,id in num_domain ]
+        # we consider only the "counts" (value index 0)
+        counts = cube[:,:,0]
+        # sum the counts 
+        count_sums = abs(sum(counts, 0))+1e-6  # we add 1e-6 just to make sure we don't have zeros and divisions by zero
+        condprobs = counts/count_sums
+        # we keep only the numerical domain
+        condprobs = take(condprobs,num_ids)
+        l,w = condprobs.shape
+
+        summarized_domain = []
+        newinterval = None
+        newrow = zeros(w, numarray.typefrom(condprobs))
+        ids = []
+        for i in xrange(l):
+            row = condprobs[i]
+            interval, id = num_domain[i]
+            if not newinterval:
+                newinterval = interval
+            else:
+                newinterval = Interval(newinterval.include_low, newinterval.low, interval.high, interval.include_high)
+            ids.append(id)
+            newrow += row
+            if max(newrow)>=summarize_min_prob or i==l-1:
+                summarized_domain.append( (newinterval, ids) )
+                newinterval = None
+                newrow[:] = 0.
+                ids = []
+        
+        return summarized_domain
+
+    def trimmedNumberDomain(self, varname, condvar, summarize_remove_n, summarize_min_prob):
+        """Will return an ordered list of (interval, indexes) for the numerical part of the domain
+        interval is an instance of Interval
+        indexes is a list of corresponding cube-indexes for variable varname in a cube
+        corresponding to varname, such that the union of the individual intervals associated with
+        those indexes gives interval (thus the slices corresponding to those indexes must be summed
+        to get the values corresponding to interval).
+        summarize_remove_n is the number of small intervals to 'remove'
+        In addition, all intervals whose prob is less than summarize_min_prob will also be 'removed'
+        """
+
+        num_domain = self.var_domains[varname].number_domain()
+        summarized_domain = [ (interval, (id,) ) for interval,id in num_domain ]
+
+        if (summarize_remove_n<=0 and summarize_min_prob>=1.) or len(summarized_domain)==0:
+            return summarized_domain
+
+        try:
+            cube = self.cubes[(varname, condvar)]
+        except KeyError:
+            cube = self.cubes[(condvar, varname)]
+            cube = transpose(cube,(1,0,2))
+
+        num_ids = [ id for xrange,id in num_domain ] 
+        # we consider only the "counts" (value index 0)
+        counts = cube[:,:,0]
+        # sum the counts 
+        count_sums = abs(sum(counts, 0))+1e-6  # we add 1e-6 just to make sure we don't have zeros and divisions by zero
+        condprobs = counts/count_sums
+        # we keep only the numerical domain
+        condprobs = take(condprobs,num_ids)
+
+        l,w = condprobs.shape
+        nremoved = 0
+
+        while l>0:
+            maxprobs = array([ max(p) for p in condprobs ])
+            k = argmin(maxprobs)
+            minprob = maxprobs[k]
+            #print >>f, 'maxprobs: ',maxprobs
+            #print >>f, 'k: ',k
+            #print >>f, 'minprob: ',minprob
+            if nremoved>=summarize_remove_n or minprob>=summarize_min_prob:
+                break # exit while loop
+            if k==0:
+                k_a = 0
+                k_b = 1
+            elif k==l-1:
+                k_a = l-2
+                k_b = l-1
+            elif maxprobs[k-1]<=maxprobs[k+1]:
+                k_a = k-1
+                k_b = k
+            else:
+                k_a = k
+                k_b = k+1
+            interval_a, ids_a = summarized_domain[k_a]
+            interval_b, ids_b = summarized_domain[k_b]
+            try:
+                union_interval = interval_a + interval_b
+            except ValueError: # union of the 2 intervals is not an interval!
+                break
+            summarized_domain[k_a] = (union_interval, ids_a+ids_b)
+            del summarized_domain[k_b]
+            #newcondprobs = numarray.typefrom(zeros((l-1,w),condprobs))
+            newcondprobs = zeros((l-1,w),numarray.typefrom(condprobs))
+            newcondprobs[0:k_b] += condprobs[0:k_b]
+            newcondprobs[k_a:] += condprobs[k_b:]
+            condprobs = newcondprobs
+            l = l-1
+            nremoved += 1
+
+        return summarized_domain
+
+
+    def getSumsMatrixAndNames(self, condvari, condvarj, summarize_min_prob=1e10):
+        """ Returns the tuple (mat, rownames, colnames)"""
+        try:
+            cube = self.cubes[(condvari, condvarj)]
+        except KeyError:
+            cube = self.cubes[(condvarj, condvari)]
+            cube = transpose(cube,(1,0,2))
+
+        condi_domain = self.stringDomain(condvari)+self.numberDomain(condvari, condvarj, summarize_min_prob)
+        condi_descr = [ str(interval) for interval,ids in condi_domain ]
+
+        condj_descr, condj_id = self.var_domains[condvarj].domain_descr_id()
+
+        values = cube[:,:,:]
+        values = take(values,condj_id,axis=1)
+        l, w, d = values.shape
+        n = len(condi_domain)
+        #mat = numarray.typefrom(zeros((1+n,1+w,d),values))
+        mat = zeros((1+n,1+w,d),numarray.typefrom(values))
+        firstrow = sum(values, 0)
+        mat[0, 1:1+w, :] = firstrow
+
+        mat[0, 0, :]   = sum(firstrow)
+        for i in xrange(n):
+            interval,ids = condi_domain[i]
+            s = mat[1+i,0,:]
+            for id in ids:
+                row = values[id]                
+                mat[1+i, 1:, :] += row
+                s += sum(row)
+
+        condi_descr.insert(0,'/*/')
+        condj_descr.insert(0,'/*/')
+        return mat, condi_descr, condj_descr
+
+    def getCondTable(self, condvari, condvarj, sumvar='', divvar='', marginalize=0, summarize_min_prob=0., combinvarsmode=0):
+        mat, rownames, colnames = self.getSumsMatrixAndNames(condvari, condvarj, summarize_min_prob)
+        l,w,d = mat.shape
+        valuenames = self.extended_value_names()
+        #numarray.ufunc.Error.pushMode(dividebyzero="ignore", invalid="ignore")
+        if sumvar and not divvar:
+            m = mat[:,:,valuenames.index(sumvar)]
+            title = 'SUM['+sumvar+'] | ' + condvari +', '+condvarj
+        elif sumvar and divvar:
+            if combinvarsmode==0:
+                title = 'SUM['+sumvar+']/SUM['+divvar+'] | ' + condvari +', '+condvarj
+                m = mat[:,:,valuenames.index(sumvar)]/mat[:,:,valuenames.index(divvar)]
+            elif combinvarsmode==1:
+                title = 'SUM['+sumvar+']-0.7*SUM['+divvar+'] | ' + condvari +', '+condvarj
+                m = mat[:,:,valuenames.index(sumvar)]-0.7*mat[:,:,valuenames.index(divvar)]
+        elif divvar and not sumvar:
+            m = 1./mat[:,:,valuenames.index(divvar)]
+            title = '1/SUM['+divvar+'] | ' + condvari +', '+condvarj
+        else:
+            raise ValueError('Must specify at least one of sumvar or divvar')
+
+        if marginalize:
+            if marginalize==1:
+                m = m/m[0:1,:]
+            elif marginalize==2:
+                m = m/m[:,0:1]
+            elif marginalize==3:
+                m = m/m[0,0]
+            m *= 100.0
+            title = title + ' -> percents '+str(marginalize)
+
+        if summarize_min_prob>0.:
+            title = title + '   [prob>='+str(int(summarize_min_prob*10000)/100.0)+'%]'
+
+        #numarray.ufunc.Error.popMode()
+        table = MemoryTable(colnames, m)
+        table.set_rownames(rownames)
+        table.set_title(title)
+        return table
+
+
+##     def getCondProbTable(self, targetvar, condvar):
+##         try:
+##             cube = self.cubes[(targetvar, condvar)]
+##         except KeyError:
+##             cube = self.cubes[(condvar, targetvar)]
+##             cube = transpose(cube,(1,0,2))
+            
+##         cond_descr, cond_id = self.var_domains[condvar].domain_descr_id()
+##         targ_descr, targ_id = self.var_domains[targetvar].domain_descr_id()
+
+##         prob = cube[:,:,0]
+##         prob = take(take(prob,targ_id,axis=0),cond_id,axis=1)
+##         prob_sum = sum(prob,0)
+##         # prob = prob*100./sum(prob)
+
+##         table = MemoryTable(cond_descr)
+##         for pr in prob:
+##             row = [ smartpercent(p,ps) for p,ps in zip(pr,prob_sum) ]
+##             table.append(row)
+##         table.set_rownames(targ_descr)
+##         return table
+        
+##     def getCondValuesTable(self, condvar1, condvar2, strvaluef):
+##         try:
+##             cube = self.cubes[(condvar1, condvar2)]
+##         except KeyError:
+##             cube = self.cubes[(condvar2, condvar1)]
+##             cube = transpose(cube,(1,0,2))
+            
+##         cond1_descr, cond1_id = self.var_domains[condvar1].domain_descr_id()
+##         cond2_descr, cond2_id = self.var_domains[condvar2].domain_descr_id()
+
+##         sumt = take(take(cube,cond1_id,axis=0),cond2_id,axis=1)
+
+##         table = MemoryTable(cond2_descr)
+##         table.set_rownames(cond1_descr)
+##         for i in xrange(len(cond1_descr)):
+##             row = []
+##             for j in xrange(len(cond2_descr)):
+##                 row.append(strvaluef(sumt[i,j]))
+##             table.append(row)
+##         return table
+            
+##     def getCondSumsTable(self, condvari, condvarj):
+##         try:
+##             cube = self.cubes[(condvari, condvarj)]
+##         except KeyError:
+##             cube = self.cubes[(condvarj, condvari)]
+##             cube = transpose(cube,(1,0,2))
+            
+##         condi_descr, condi_id = self.var_domains[condvari].domain_descr_id()
+##         condj_descr, condj_id = self.var_domains[condvarj].domain_descr_id()
+
+##         sumt = cube[:,:,:]
+##         sumt = take(take(sumt,condi_id,axis=0),condj_id,axis=1)
+
+##         table = MemoryTable(['sum_var']+condj_descr)
+##         rownames = []
+##         valuenames = self.extended_value_names()
+##         for i in xrange(len(condi_descr)):
+##             condi_str = condi_descr[i]
+##             for k in xrange(len(valuenames)):
+##                 row = [ valuenames[k] ]
+##                 for j in xrange(len(condj_descr)):
+##                     row.append(sumt[i,j,k])
+##                 table.append(row)
+##                 rownames.append(condi_str)
+##                 condi_str = ''
+##         table.set_rownames(rownames)
+##         return table
+            
+##     def getCondRatioTable(self, condvari, condvarj, divide_by):
+##         """divide_by must be the valuename by which to divide"""        
+##         try:
+##             cube = self.cubes[(condvari, condvarj)]
+##         except KeyError:
+##             cube = self.cubes[(condvarj, condvari)]
+##             cube = transpose(cube,(1,0,2))
+            
+##         condi_descr, condi_id = self.var_domains[condvari].domain_descr_id()
+##         condj_descr, condj_id = self.var_domains[condvarj].domain_descr_id()
+
+##         sumt = cube[:,:,:]
+##         sumt = take(take(sumt,condi_id,axis=0),condj_id,axis=1)
+
+##         valuenames = self.extended_value_names()
+##         divideidx = valuenames.index(divide_by)
+##         table = MemoryTable(['sum_var']+condj_descr)
+##         rownames = []
+##         for i in xrange(len(condi_descr)):
+##             condi_str = condi_descr[i]
+##             for k in xrange(len(valuenames)):
+##                 row = [ valuenames[k] ]
+##                 for j in xrange(len(condj_descr)):
+##                     row.append(smartdiv(sumt[i,j,k],sumt[i,j,divideidx]))
+##                 table.append(row)
+##                 rownames.append(condi_str)
+##                 condi_str = ''
+##         table.set_rownames(rownames)
+##         return table
+            
+

Added: trunk/python_modules/plearn/table/viewtable.py
===================================================================
--- trunk/python_modules/plearn/table/viewtable.py	2009-06-02 21:55:31 UTC (rev 10226)
+++ trunk/python_modules/plearn/table/viewtable.py	2009-06-02 22:59:44 UTC (rev 10227)
@@ -0,0 +1,1625 @@
+"""
+viewtable.py
+
+Copyright (C) 2005-2009 ApSTAT Technologies Inc.
+
+This file was contributed by ApSTAT Technologies to the
+PLearn library under the following BSD-style license: 
+
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+"""
+
+# Author: Pascal Vincent
+
+import sys
+import curses
+import threading
+import cPickle as pickle
+import bisect
+import os
+import os.path
+import subprocess
+import traceback
+import locale
+from plearn.table.table import *
+from plearn.table.tablestat import *
+from plearn.table.date import *
+from plearn.plotting.numpy_utils import to_numpy_float_array, mapping_of_values_to_pos
+import matplotlib.colors
+
+def format_string_to_width(s, width):
+    if len(s)<width:
+        s += ' '*(width-len(s))
+    else:
+        s = s[0:width-1]+' '
+    return s
+
+def daydiff(cyymmdd1, cyymmdd2):
+    return (CYYMMDD_to_date(cyymmdd1)-CYYMMDD_to_date(cyymmdd2)).days
+
+def select_item(scr, items, title='',
+               instructions='Select one of the following by pressing the appropriate key'):
+
+    """scr is the curses screen or window,
+    items is a list of strings to be selected from.  This displays a selection
+    screen allowing the user to select one of the items.
+    The index of the selected item is returned"""
+
+    scr.erase()
+    scr.addstr(0,0,title,curses.A_REVERSE)
+    scr.addstr(2,1,instructions,curses.A_BOLD)
+
+    n = len(items)
+    keys = "123456789abcdefghijklmnop"
+    keys = keys[0:n]
+    keycodes = map(ord,keys)
+    for i in xrange(n):
+        scr.addstr(4+i,3,'['+keys[i]+'] '+items[i])
+    scr.move(4+n,3)
+    scr.refresh();
+    while True:
+        k = scr.getch();
+        try:
+            id = keycodes.index(k)
+            break
+        except ValueError:
+            pass
+    scr.erase()
+    return id
+
+class TableView:
+
+    def __init__(self, table, stdscr):
+        
+        self.stdscr = stdscr
+        self.transposed = False
+        self.dot_mode = False
+        self.margin_top = 2
+        self.margin_left = 0
+        self.margin_right = 0
+        self.margin_bottom = 2
+        self.rowname_width = 20
+        self.set_field_dim(20, 1)
+
+        self.monetary_format= False
+        self.orig_locale= locale.getlocale(locale.LC_NUMERIC)
+        self.full_shuffle_stats= False
+
+        self.reinit(table)
+        
+
+    def reinit(self, table):
+        self.i0 = 0
+        self.j0 = 0
+        self.current_i = 0
+        self.current_j = 0
+        self.setTable(table)
+
+        self.conditioning_field = ''
+        self.weight_field = ''
+        self.target_fields = []
+
+        self.search_field = 0
+        self.search_value = table[0][0]
+        self.search_expression = ''
+
+        self.stats = None
+        self.statsthread = None
+        self.stop_thread = False
+        self.statslock = threading.Lock()
+        self.shuffled_rowidx = None
+        self.statsview = None
+
+        self.sumvar = ''
+        self.divvar = ''
+        self.marginalize = 0
+        # combinvarsmode 0: numerator_sum_var/denominator_sum_var   1: numerator_sum_var-0.7*denominator_sum_var
+        self.combinvarsmode = 0
+        # a map containing for each variable name the summarize_remove_n setting
+        self.summarize_remove_n = {}
+        self.minprob = 0.
+
+        # used by sort and filter operations
+        self.selected_rows = None
+        self.selected_fields = []
+        
+        self.graph_fields = {}
+        
+        self.redraw()        
+
+
+    def writecolname(self,sj,text,attr=curses.A_NORMAL):
+        text = format_string_to_width(text, self.fieldwidth)
+        si = self.margin_top
+        sj = self.margin_left+self.rowname_width+sj*self.fieldwidth
+        self.stdscr.addstr(si,sj,text,attr)        
+        
+    def writerowname(self,si,text,attr=curses.A_NORMAL):
+        text = format_string_to_width(text, self.rowname_width)
+        si = self.margin_top+1+si*self.fieldheight
+        sj = self.margin_left
+        self.stdscr.addstr(si,sj,text,attr)                
+
+    def writetop(self,text,attr=curses.A_NORMAL):
+        text = format_string_to_width(text,curses.COLS-1)
+        self.stdscr.addstr(0,0,text,attr)
+
+    def writebottom(self,text,attr=curses.A_NORMAL):
+        self.stdscr.addstr(curses.LINES-2,0,'_'*curses.COLS)
+        text = format_string_to_width(text,curses.COLS-1)
+        self.stdscr.addstr(curses.LINES-1,0,text,attr)
+
+    def writefield(self,si,sj,text,attr=curses.A_NORMAL):
+        text = format_string_to_width(text, self.fieldwidth)
+        si = self.margin_top+1+si*self.fieldheight
+        sj = self.margin_left+self.rowname_width+sj*self.fieldwidth
+        self.stdscr.addstr(si,sj,text,attr)
+
+    def setTable(self,table):
+        self.table = table
+        self.selected_columns = range(self.table.width())
+        self.cache = {}
+        self.cache_capacity = 100
+        self.cache_min_i = 0
+        self.cache_max_i = 0
+        if self.current_i>=self.table.length():
+            self.current_i = 0
+            self.i0 = 0
+        if self.current_j>=self.table.width():
+            self.current_j = 0
+            self.j0 = 0
+        l,w = table.length(), table.width()
+        self.set_toprightinfo(str(l)+'x'+str(w))
+
+    def getFullRow(self,i):
+        if self.selected_rows is not None:
+            i = self.selected_rows[i]
+        # row = self.table.getRow(i)
+        row = self.table[i]
+        return row
+
+    def getRow(self,i):
+        if self.selected_rows is not None:
+            i = self.selected_rows[i]
+        try:
+            row = self.cache[i]
+        except KeyError:
+            self.statslock.acquire()
+            row = self.table.getRow(i)
+            self.statslock.release()
+            if len(self.cache)>=self.cache_capacity: # delete one row from the cache
+                if i==self.cache_max_i+1:
+                    del self.cache[self.cache_min_i]
+                    self.cache_min_i = min(self.cache)
+                elif i==self.cache_min_i-1:
+                    del self.cache[self.cache_max_i]
+                    self.cache_max_i = max(self.cache)
+                else:
+                    self.cache = {}
+                    self.cache_min_i = i
+                    self.cache_max_i = i
+                    
+            self.cache[i] = row
+            self.cache_min_i = min(i,self.cache_min_i)
+            self.cache_max_i = max(i, self.cache_max_i)
+
+        return [ row[col] for col in self.selected_columns ]
+
+    def getVal(self,i,j):
+        return self.getRow(i)[j]
+    
+##         if i>=self.row_cache_mini and i<=self.row_cache_maxi:
+##         try:
+##             return self.row_cache[i-]
+    
+
+    def set_field_dim(self, fieldwidth, fieldheight):
+        self.fieldwidth = max(2, min(fieldwidth, 100))
+        self.fieldheight = max(1, min(fieldheight, 50))
+        self.compute_ni_nj()
+
+    def compute_ni_nj(self):
+        if self.transposed:            
+            self.ni = (curses.COLS-(self.rowname_width+self.margin_left+self.margin_right))/self.fieldwidth
+            self.nj = (curses.LINES-(self.margin_top+self.margin_bottom+1))/self.fieldheight
+        else:            
+            self.ni = (curses.LINES-(self.margin_top+self.margin_bottom+1))/self.fieldheight
+            self.nj = (curses.COLS-(self.rowname_width+self.margin_left+self.margin_right))/self.fieldwidth
+
+    def display_help(self,c=0):
+#        self.stdscr.erase()
+        helptext = """
+*************************
+**  KEYS IN DATA VIEW  **
+*************************
+ arrow keys   : move in corresponding direction
+ page up/down : move up/down one screen
+ home/end     : move to first / last screen
+ t            : transpose view
+ > <          : increase or decrease width of display field
+ ) (          : sort rows in increasing or decreasing order of values of current field 
+ a            : sort fieldnames alphabetically
+ SPACE        : select/deselect field 
+ c            : select constant fields (fields having constant value in all rows)
+ ENTER        : keep only selected fields in display
+ k            : hide selected fields from display (or if none selected, hide current field)
+ l            : prompt for a line number and go to that line
+ .            : toggle displaying of ... for values that do not change
+ =            : search for a value of the current field
+ /            : search for row satisfying python expression (ex: float(AGE1)<float(AGE2) )
+ N            : search next
+ P            : search previous
+ !            : filter rows satisfying python expression (or current selected fields values)
+ o            : revert to the original table (all fields and rows in orignal order)
+ h            : display this help screen
+ *            : mark main conditioning field (Also used for subplot rows)
+ +            : mark conditional summing fields (Also used for subplot columns)
+ ~            : mark (optional) weight field. (Also used for figure number).
+ s            : choose among saved stats
+ x            : execute a shell command
+ TAB          : switch to stats view
+ F            : toggle full shuffle of records for stats [currently: %s]
+ f            : find a field by name
+ m            : toggle display of numeric fields in monetary format [currently: %s]
+ X            : select x field
+ Y            : select y field
+ M            : select marker field (discrete variable)
+ C            : select color field (discrete variable)
+ S            : select marker-size field
+ U            : select arrow x component field
+ V            : select arrow y component field
+ G            : graphical plot of selected fields
+ w            : write (save) current view as a pytable file (NEEDS TO BE FIXED)
+ q            : quit program
+
+*******************************
+** EXTRA KEYS IN STATS VIEW: **
+*******************************
+ 1/2       : cycle through numerator or denominator sum variable 
+ 3         : cycle through percentage views
+ 4         : toggle between numerator/denominator and numerator-0.7*denominator
+ - =       : decrease/increase number of numerical intervals by increasing min cond prob
+ n/p       : move to next/previous field for first conditioning var
+ SPACE     : update view 
+ TAB or q  : go back to data view
+
+(press any key to continue)
+""" % (self.full_shuffle_stats, self.monetary_format)
+        # if c:
+        #    helptext = 'Pressed '+str(c)+'\n'+helptext
+
+        self.display_fullscreen(helptext)
+            
+#         helpi = 1
+#         for line in helptext.split('\n'):
+#             self.safeaddstr(helpi,2,line)
+#             helpi += 1
+#         self.stdscr.refresh();
+#         self.stdscr.getch();
+#         self.redraw()
+        
+    def safeaddstr(self,i,j,line,attr=curses.A_NORMAL):
+        if i<curses.LINES-1 and j<curses.COLS:
+            self.stdscr.addstr(i,j,line,attr)
+        
+    def fieldname(self, j):
+        return self.table.fieldnames[self.selected_columns[j]]
+
+    def fieldnames(self):
+        return [ self.table.fieldnames[self.selected_columns[j]] for j in xrange(self.width()) ]
+
+    def length(self):
+        if self.selected_rows is not None:
+            return len(self.selected_rows)
+        else:
+            return self.table.length()
+
+    def width(self):
+        return len(self.selected_columns)
+
+    def set_graph_field(self, fieldchar):
+        fieldname = self.fieldname(self.current_j)
+        if fieldchar in self.graph_fields and self.graph_fields[fieldchar]==fieldname:
+            del self.graph_fields[fieldchar]
+        else:
+            self.graph_fields[fieldchar]=fieldname
+        self.redraw()
+        
+    def fieldname_repr_and_style(self, j):
+        fieldname = self.fieldname(j)
+        fnamerepr = fieldname
+        style = curses.A_BOLD
+        if fieldname in self.target_fields:
+            fnamerepr = ' + '+fnamerepr
+            style = curses.A_BOLD | curses.A_UNDERLINE
+        if self.conditioning_field and fieldname==self.conditioning_field:
+            fnamerepr = ' * '+fnamerepr
+            style = curses.A_BOLD | curses.A_UNDERLINE
+        if self.weight_field and fieldname==self.weight_field:
+            fnamerepr = ' ~ '+fnamerepr
+            style = curses.A_BOLD | curses.A_UNDERLINE
+
+        for fieldchar,gfieldname in self.graph_fields.items():
+            if fieldname==gfieldname:
+                fnamerepr = fieldchar+' '+fnamerepr            
+
+        pos = None
+        try:
+            pos = self.selected_fields.index(fieldname)
+        except ValueError:
+            pass
+
+        if pos != None:
+            fnamerepr = str(pos)+'. '+fnamerepr 
+            style = style | curses.A_REVERSE
+           
+        return fnamerepr, style
+
+    def redraw(self):
+        self.stdscr.erase()
+        l, w = self.length(),self.width()
+        table = self.table
+        if self.current_j>=self.width():
+            self.current_j = self.width()-1
+        if self.transposed:
+            self.draw_transposed()
+        else:
+            self.draw_normal()
+
+        # Write title
+        # self.stdscr.hline(0,0,curses.COLS-1,ord(' '))
+        title = self.table.title()
+        self.stdscr.addstr(0,0,title,curses.A_BOLD)
+        # Write dimensions
+        dimstr = self.get_toprightinfo()
+        self.stdscr.addstr(0,curses.COLS-len(dimstr)-1,dimstr,curses.A_BOLD)
+        
+        # write info line
+        self.stdscr.hline(curses.LINES-2,0,curses.COLS,ord('_'))
+        # self.stdscr.addstr(curses.LINES-2,0,'_'*curses.COLS)
+        current_val = self.getVal(self.current_i,self.current_j)
+        infoline = self.rowname(self.current_i)+' | '+self.fieldname(self.current_j)+' | '+str(current_val) + ' | ' + str(type(current_val))
+        self.writebottom(infoline)
+        self.stdscr.refresh();
+
+    def get_toprightinfo(self):
+        return self.toprightinfo
+
+    def set_toprightinfo(self, text):
+        self.toprightinfo = text
+
+    def rowname(self,i):
+        return self.table.rowname(i)
+
+    def format(self,x):
+        v = None
+        try: v = float(x)
+        except ValueError: pass
+        except TypeError: pass
+        if v == None or not self.monetary_format:
+            return str(x)
+        return locale.format('%.2f', v, True)
+
+
+    def draw_normal(self):
+        istart, jstart = self.i0, self.j0
+        istop = istart+self.ni
+        if istop>self.length():
+            istop = self.length()
+        jstop = jstart+self.nj
+        if jstop>self.width():
+            jstop = self.width()
+
+        # write fieldnames
+        for j in xrange(jstart,jstop):
+            fieldname, style = self.fieldname_repr_and_style(j)
+            self.writecolname(j-jstart,fieldname,style)
+
+        # write rowname and values of table row i
+        prev_row = None
+        for i in xrange(istart,istop):
+            # write name of row i            
+            rowname = self.rowname(i)
+            self.writerowname(i-istart,rowname,curses.A_BOLD)
+            row = self.getRow(i)
+            # write values of row i
+            for j in xrange(jstart,jstop):
+                val = row[j]
+                #strval = str(val)
+                strval = self.format(val)
+                if self.dot_mode and prev_row!=None and prev_row[j]==val:
+                    strval = '...'
+                style = curses.A_NORMAL
+                if j==self.current_j or i==self.current_i:
+                    style = curses.A_REVERSE
+                self.writefield(i-istart,j-jstart,strval,style)
+            prev_row = row
+
+    def draw_transposed(self):
+        istart, jstart = self.i0, self.j0
+        istop = istart+self.ni
+        if istop>self.length():
+            istop = self.length()
+        jstop = jstart+self.nj
+        if jstop>self.width():
+            jstop = self.width()
+
+        # write fieldnames
+        for j in xrange(jstart,jstop):
+            fieldname, style = self.fieldname_repr_and_style(j)
+            self.writerowname(j-jstart,fieldname,style)
+
+        # write rowname and values of table row i
+        prev_row = None
+        for i in xrange(istart,istop):
+            # write name of row i            
+            rowname = self.rowname(i)
+            self.writecolname(i-istart,rowname,curses.A_BOLD)
+            row = self.getRow(i)
+            # write values of row i
+            for j in xrange(jstart,jstop):
+                val = row[j]
+                #strval = str(val)
+                strval = self.format(val)
+                if self.dot_mode and prev_row!=None and prev_row[j]==val:
+                    strval = '...'
+                style = curses.A_NORMAL
+                if j==self.current_j or i==self.current_i:
+                    style = curses.A_REVERSE
+                self.writefield(j-jstart,i-istart,strval,style)
+            prev_row = row
+
+    def saveStats(self):
+        self.statslock.acquire()
+        fpath = self.statsFilePathFromSelectedVars()
+        dirpath = os.path.dirname(fpath)
+        if not os.path.isdir(dirpath):
+            os.makedirs(dirpath)
+        f = open(fpath+'.new','wb')
+        pickle.dump(self.stats, f, 0) # using protocol 0, because 1 and 2 have bug not allowing pickling of inf
+        f.close()
+        try: os.remove(fpath)
+        except OSError: pass
+        os.rename(fpath+'.new',fpath)
+        self.statslock.release()
+
+    def statsFilePathFromSelectedVars(self):
+        dirpath = self.table.filepath()+'.stats'
+        fname = 'CONDSTAT_'+self.conditioning_field+'___'+self.weight_field+'___'+'___'.join(self.target_fields)+'.pickle'
+        fpath = os.path.join(dirpath,fname)
+        return fpath
+
+    def loadStats(self, statsfpath):
+        self.clearStats()
+        self.selectVarsFromStatsFilePath(statsfpath)
+        f = open(statsfpath)
+        self.stats = pickle.load(f)
+        f.close()
+        self.sumvar = ''
+        self.divvar = ''
+        self.marginalize = 0
+        self.combinvarsmode = 0
+        
+    def loadMostRecentStats(self):
+        dirpath = self.table.filepath()+'.stats'
+        try:
+            fpathlist = [ os.path.join(dirpath,fname) for fname in os.listdir(dirpath) if fname.startswith('CONDSTAT_') and fname.endswith('.pickle') ]
+        except OSError:
+            fpathlist = []
+        if not fpathlist:
+            scr = self.stdscr
+            scr.erase()
+            self.safeaddstr(5,3,"You must choose at least the conditioning field with * before getting statistics",curses.A_BOLD)
+            scr.refresh();
+            scr.getch();
+            self.redraw()
+            return                
+        fpathtimes = [ os.stat(fpath).st_mtime for fpath in fpathlist ]
+        fpath = fpathlist[argmax(fpathtimes)]
+        self.loadStats(fpath)        
+
+    def chooseStats(self):
+        self.stopThread()
+        dirpath = self.table.filepath()+'.stats'
+        try:
+            fnamelist = [ fname for fname in os.listdir(dirpath) if fname.startswith('CONDSTAT_') and fname.endswith('.pickle') ]
+        except OSError:
+            fnamelist = []
+        if not fnamelist:
+            return
+        else:
+            headlen = len('CONDSTAT_')
+            taillen = len('.pickle')
+            def myrepr(fname):
+                tokens = fname[headlen:-taillen].split('___')
+                condfield = tokens[0]
+                wfield = tokens[1]
+                rest = tokens[2:]
+                if wfield:
+                    return '*'+condfield+'  ~'+wfield+'  +'+' +'.join(rest)
+                else:
+                    return '*'+condfield+'  +'+' +'.join(rest)
+            itemlist = map(myrepr, fnamelist)
+            itemnum = select_item(self.stdscr, itemlist, 'Choose a stats file to load')
+        statsfpath = os.path.join(dirpath,fnamelist[itemnum])
+        self.loadStats(statsfpath)        
+
+    def selectVarsFromStatsFilePath(self, fpath):
+        fname = os.path.basename(fpath)
+        basename,ext   = os.path.splitext(fname)
+
+        if ext!='.pickle':
+            raise ValueError('statfilepath should end with .pickle')
+        if not basename.startswith('CONDSTAT_'):
+            raise ValueError('statfilepath should start with CONDSTAT_')
+        fieldnames = basename[len('CONDSTAT_'):].split('___')
+        self.conditioning_field = fieldnames[0]
+        self.weight_field = fieldnames[1]
+        if fieldnames[2]:
+            self.target_fields = fieldnames[2:]
+        else:
+            self.target_fields = []
+            
+        # f = open('toto.log','w')
+        # print >>f, 'fpath: ',fpath 
+        # print >>f, 'fieldnames: ',fieldnames 
+        # print >>f, 'self.target_fields: ',self.target_fields
+        # f.close()
+
+    def initStats(self):
+        """Initializes the stats member according to selected variables
+        trying to load a start version from file if available"""
+
+        try:
+            self.loadStats(self.statsFilePathFromSelectedVars())
+        except IOError:
+            var_combinations = all_combinations([self.table.fieldnames, [self.conditioning_field]])
+            var_domains = {}
+            for varname in self.table.fieldnames:
+                var_domains[varname] = AutoDomain()
+            self.stats = BaseTableStats(var_combinations, var_domains, self.target_fields, self.weight_field,
+                                        self.full_shuffle_stats)
+            self.initShuffledIndex()
+            # update with row 0 so that stats are not empty
+            row = self.table[self.shuffled_rowidx[0]]
+            self.stats.update(row) 
+            self.startThread()
+            self.sumvar = ''
+            self.divvar = ''
+            self.marginalize = 0
+            self.combinvarsmode = 0
+
+    def clearStats(self):
+        self.stopThread()
+        self.stats = None
+
+    def initShuffledIndex(self):
+        min_len_for_partial_shuffle= 5000
+        if self.shuffled_rowidx is None:
+            self.stdscr.erase()
+            self.safeaddstr(5,3,"Building shuffled index. Please be patient...",curses.A_BOLD)
+            self.stdscr.refresh()
+            numpy.numarray.random_array.seed(58273,26739)
+            self.shuffled_rowidx = numpy.numarray.random_array.permutation(self.table.length()).astype(int)
+            if hasattr(self.stats, 'full_shuffle_stats'):
+                self.full_shuffle_stats= self.stats.full_shuffle_stats
+            else:
+                self.full_shuffle_stats= True
+            if not self.full_shuffle_stats and self.table.length() >= min_len_for_partial_shuffle:
+                x= self.shuffled_rowidx
+                #x.resize(min_len_for_partial_shuffle)
+                resize(x,[min_len_for_partial_shuffle])
+                xd = {}
+                for z in x:
+                    xd[z]=z
+                y = numpy.numarray.arange(self.table.length())
+                y= numpy.numarray.array([z for z in y if xd.get(z) == None])
+                self.shuffled_rowidx= numpy.numarray.concatenate((x,y)).astype(int)
+    
+    def startThread(self):
+        if not self.statsthread and self.stats.nsamples<self.table.length():
+            self.initShuffledIndex()
+            self.statsthread = threading.Thread(target=self.statsThreadRun)
+            self.statsthread.start()
+
+    def statsThreadRun(self):
+        l = self.table.length()
+        idx = self.shuffled_rowidx
+        self.statslock.acquire()
+        nsamples = self.stats.nsamples
+        self.statslock.release()
+
+        #f=open('xxxxx.nfg.tmp','w')
+        
+
+        while nsamples<l:            
+            self.statslock.acquire()
+            row = self.table[idx[nsamples]]
+            
+            #f.write(str([nsamples, idx[nsamples]]))
+
+            nsamples = self.stats.update(row)
+            stop_thread = self.stop_thread
+            self.statslock.release()
+
+            
+            if stop_thread:
+                break
+            if nsamples % 2000 == 0:
+                self.saveStats()
+        self.saveStats()
+
+        #f.close()
+            
+    def stopThread(self):
+        if self.statsthread:
+            self.statslock.acquire()
+            self.stop_thread = True
+            self.statslock.release()
+            self.statsthread.join()
+            self.statsthread = None
+            self.stop_thread = False
+
+    def transpose(self):
+        self.transposed = not self.transposed
+        self.compute_ni_nj()
+        self.i0 = max(0,self.current_i-self.ni//2)
+        self.j0 = max(0,self.current_j-self.nj//2)
+        self.redraw()
+
+    def sort_rows(self, j, reverse=False):
+        """sort in incresing order of field j"""
+        if self.selected_rows is None:
+            self.selected_rows = range(self.length())
+        if reverse: # hack to make sure we keep the partial order of previous sorts on other columns
+            self.selected_rows.reverse()
+        biglist = [ (float_or_str(self.getVal(pos,j)), pos) for pos in xrange(self.length()) ]
+        biglist.sort(reverse=reverse)
+        self.selected_rows = [ self.selected_rows[pos] for val,pos in biglist ]
+        self.redraw()
+
+    def previous_row(self):
+        if self.current_i>0:
+            self.current_i -= 1
+            if self.current_i<self.i0:
+                self.i0 -= 1
+
+    def next_row(self):
+        if self.current_i<self.length()-1:
+            self.current_i += 1
+            if self.current_i>=self.i0+self.ni:
+                self.i0 += 1
+
+    def previous_field(self):
+        if self.current_j>0:
+            self.current_j -= 1
+            if self.current_j<self.j0:
+                self.j0 -= 1
+
+    def next_field(self):
+        if self.current_j<self.width()-1:
+            self.current_j += 1
+            if self.current_j>=self.j0+self.nj:
+                self.j0 += 1
+
+    def left(self):
+        if self.transposed:
+            self.previous_row()
+        else:
+            self.previous_field()
+        self.redraw()
+
+    def right(self):
+        if self.transposed:
+            self.next_row()
+        else:
+            self.next_field()
+        self.redraw()
+
+    def up(self):
+        if self.transposed:
+            self.previous_field()
+        else:
+            self.previous_row()
+        self.redraw()
+
+    def down(self):
+        if self.transposed:
+            self.next_field()
+        else:
+            self.next_row()
+        self.redraw()
+
+    def pgdown(self):
+        if self.transposed:
+            self.j0 = min(self.j0+self.nj, max(0,self.width()-self.nj))
+            self.current_j = min(self.current_j+self.nj, self.width()-1)
+        else:
+            self.i0 = min(self.i0+self.ni, max(0,self.length()-self.ni))
+            self.current_i = min(self.current_i+self.ni, self.length()-1)
+        self.redraw()
+
+    def pgup(self):
+        if self.transposed:
+            self.j0 = max(self.j0-self.nj, 0)
+            self.current_j = max(self.current_j-self.nj, 0)
+        else:
+            self.i0 = max(self.i0-self.ni, 0)
+            self.current_i = max(self.current_i-self.ni, 0)
+        self.redraw()
+
+    def home(self):
+        if self.transposed:
+            self.j0 = 0
+            self.current_j = 0
+        else:
+            self.i0 = 0
+            self.current_i = 0
+        self.redraw()
+
+    def end(self):
+        if self.transposed:
+            self.current_j = self.width()-1
+            self.j0 = max(0, self.width()-self.nj)
+        else:
+            self.current_i = self.length()-1
+            self.i0 = max(0, self.length()-self.ni)
+        self.redraw()
+
+    def input(self,prompt):
+        si = curses.LINES-1
+        self.stdscr.addstr(si,0,' '*(curses.COLS-1))
+        self.stdscr.addstr(si,0,prompt, curses.A_BOLD)
+        curses.echo()
+        entry = self.stdscr.getstr(si,len(prompt))
+        curses.noecho()
+        return entry
+
+    def goto_line(self,i):
+        i = max(0, min(i, self.length()-1))
+        self.current_i = i
+        self.i0 = max(0,i-self.ni//2)
+        self.redraw()
+
+    def goto_col(self,j):
+        j = max(0, min(j, self.width()-1))
+        self.current_j = j
+        self.j0 = max(0,j-self.nj//2)
+        self.redraw()
+
+    def search_next(self):
+        orig_i = self.current_i
+        for i in xrange(self.current_i+1, self.length()):
+            if i%10000 == 0: #prorgess...
+                self.goto_line(i)
+            if self.search_expression=='':
+                val = self.getVal(i,self.search_field)
+                if str(val) == self.search_value:
+                    self.goto_line(i)
+                    return #break
+            else:
+                v = dict(zip(self.table.fieldnames, self.getFullRow(i)))
+                try:
+                    res = eval(self.search_expression, globals(), v)
+                except:
+                    self.goto_line(i)
+                    self.display_exception(sys.exc_info())
+                    return #break
+                else:
+                    if res:
+                        self.goto_line(i)
+                        return #break
+        self.goto_line(orig_i) # not found, reset cursor (was moved to show progress)
+
+    def select_constant_cols(self):
+        """Will select all columns for which the values do not change"""
+
+        constant_cols = self.selected_columns[:]
+
+        first_row = None
+        for i in xrange(self.length()):
+            if self.selected_rows is None:
+                orig_i = i
+            else:
+                orig_i = self.selected_rows[i]
+            row = self.table.getRow(orig_i)
+            if first_row is None:
+                first_row = row[:]
+            constant_cols = [ j for j in constant_cols if row[j]==first_row[j] ]
+        self.selected_fields = [ self.table.fieldnames[j] for j in constant_cols ]
+        self.redraw()
+
+    def filter(self):
+        if self.selected_fields == []:
+            filter_expression = self.input('Filter expression ex: float(AGE)>3 : ').strip()
+        else:
+            row = self.getFullRow(self.current_i)
+            filter_expression = ' and '.join([ 'locals()["'+fname+'"]=='+repr(row[fname]) for fname in self.selected_fields ])
+            #self.selected_columns = [ self.table.fieldnames.index(fname) for fname in self.selected_fields]
+
+        if filter_expression!='':
+            selection = []
+            for i in xrange(self.length()):
+                if self.selected_rows is None:
+                    orig_i = i
+                else:
+                    orig_i = self.selected_rows[i]
+                v = dict(zip(self.table.fieldnames, self.table.getRow(orig_i)))
+                try:
+                    keepit = eval(filter_expression, globals(), v)
+                except:
+                    self.goto_line(i)
+                    self.display_exception(sys.exc_info(), filter_expression)
+                    self.redraw()
+                    return
+                else:
+                    if keepit:
+                        selection.append(orig_i)
+                if i==self.current_i:
+                    self.current_i = len(selection)-1
+            self.selected_rows = selection
+            self.selected_fields = []
+        self.i0 = 0
+        #self.j0 = 0
+        self.current_i = 0
+        #self.current_j = 0
+        self.redraw()
+
+    def executeShellCommand(self, command):
+
+        filepathdir = ""
+        # First figure out filepathdir (if possible)
+        try:
+            filepath = self.getFullRow(self.current_i)["_filepath_"]
+            filepathdir = os.path.dirname(filepath)
+        except KeyError:
+            pass
+
+        # replace _DIRPATH_ in command by filepathdir
+        command = command.replace('_DIRPATH_',filepathdir)
+        
+        # os.system(command)
+        subprocess.Popen(command, shell=True)
+        
+    def chooseAndExecuteShellCommand(self):
+        
+        commands = [
+            ('s',"Launch terminal and shell in this matrix's directory",
+             """xterm -e sh -c "cd '_DIRPATH_'; pwd; ls; sh" """),
+            ('1',"View layer 1 unsup training costs",
+             """xterm -e sh -c "cd '_DIRPATH_'; myplearn vmat view training_costs_layer_1.pmat" """),
+            ('i',"deepnetplot.py plotRepAndRec learner.psave", 
+             """xterm -e sh -c "cd '_DIRPATH_'; pwd; deepnetplot.py plotRepAndRec learner.psave ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh" """),
+            ('w',"deepnetplot.py plotEachRow learner.psave", 
+             """xterm -e sh -c "cd '_DIRPATH_'; pwd; deepnetplot.py plotEachRow learner.psave ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh" """),
+            ('I',"deepnetplot.py plotRepAndRec final_learner.psave", 
+             """xterm -e sh -c "cd '_DIRPATH_'; pwd; deepnetplot.py plotRepAndRec final_learner.psave ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh" """),
+            ('W',"deepnetplot.py plotEachRow final_learner.psave", 
+             """xterm -e sh -c "cd '_DIRPATH_'; pwd; deepnetplot.py plotEachRow final_learner.psave ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh" """),
+            ]
+
+        menutxt = '\n'.join([ '['+commands[i][0]+'] '+commands[i][1] for i in range(len(commands)) ])+'\n'
+        k = self.display_fullscreen(menutxt)
+        for com in commands:
+            key, descr, command = com
+            if k==ord(key):
+                self.executeShellCommand(command)
+                break
+            
+        if( k==ord('\n') ):
+            return
+
+    def collect_fieldvecs(self, fieldnames):
+        """Returns a dictionary indexed by the names of fields,
+        containing vectors of values of those fields."""
+        orig_fieldnames = self.table.fieldnames
+        orig_pos = [ orig_fieldnames.index(fname) for fname in fieldnames ]
+        l = self.length()
+        n = len(fieldnames)
+        fieldvecs = [ [] for i in xrange(n) ]
+        for i in xrange(l):
+            row = self.getFullRow(i)
+            for j in xrange(n):
+                fieldvecs[j].append( row[orig_pos[j]] )
+        fieldvecdict = {}
+        for j in xrange(n):
+            fieldvecdict[fieldnames[j]] = numpy.array(fieldvecs[j])
+        return fieldvecdict
+
+    def take_part_of_fieldvecs(self, positions, fieldvecs):
+        result = {}
+        for name,fieldvec in fieldvecs.items():
+            result[name] = numpy.take(fieldvec, positions)
+        return result
+    
+    def graphical_scatter_plot(self):
+        """
+        (STILL UNDER DEVELOPMENT, SOME DEBUGGING LEFT TO DO)
+        This docstring is probably not fully accurate.
+        The function can potentially create several figures, one for each value of the 'weight field' (if specified).
+        For each figure, it can create several subplots, based on the values of the 'conditioning field' and selected target fields.
+        For each such subplot, it will use the following tagged fields:
+        X: coordinate
+        Y: coordinate
+        C: color
+        M: marker
+        S: size of marker
+
+        A color field can be specified for a categorical variable.
+        A marker field can be specified for a categorical variable (points will use different markers).
+        
+        If U and/or V are specified, an arrow plot is superposed (pylab.quiver) of given U,V relative tip coordinates
+        """
+
+        # First find out what fields are of interest
+        fieldnames = self.graph_fields.values()
+        if self.conditioning_field!='':
+            fieldnames.append(self.conditioning_field)
+        if self.weight_field!='':
+            fieldnames.append(self.weight_field)
+        fieldnames += self.target_fields
+
+        # collect the values of those fields in a dictionary indexed by fieldname
+        fieldvecs = self.collect_fieldvecs(fieldnames)
+
+        if self.weight_field!='':
+            val2pos = mapping_of_values_to_pos(fieldvecs[self.weight_field])
+            fignum = 0
+            for val, positions in val2pos.items():
+                fieldvecs_part = self.take_part_of_fieldvecs(positions, fieldvecs)
+                fignum += 1
+                pylab.figure(fignum)
+                self.scatter_plot_all_subplots(fieldvecs_part, title=self.weight_field+'='+str(val))
+        else:
+            pylab.figure(1)
+            self.scatter_plot_all_subplots(fieldvecs, title="")            
+
+        pylab.show()
+
+    def display_colors_legend(self, colorlabel, colorlist):
+        """colorlist is a list of pairs (value, coilrspec) for the legend"""
+        x = 0.92
+        y = 0.9
+        pylab.figtext(x,y,'['+colorlabel+']',color='k',weight='bold')
+        y -= 0.025
+        for val,color in colorlist:
+            pylab.figtext(x,y,str(val),color=color)
+            y -= 0.025
+
+    def display_markers_legend(self, markerlabel, markerdict):
+        legend_lines = []
+        legend_labels = markerdict.keys()
+        legend_labels.sort()
+        for label in legend_labels:
+            legend_lines.append(pylab.Line2D(range(2), range(2), linestyle=' ',
+                                             marker=markerdict[label], color='k'))
+        label_line = pylab.Line2D(range(2), range(2), linestyle=' ', marker=' ', color='k')
+        pylab.figlegend([label_line]+legend_lines,
+                        [markerlabel]+legend_labels,
+                        'lower right')        
+                
+    def scatter_plot_all_subplots(self, fieldvecs, title=''):
+        # TODO: fix this: pylab.suptitle(title)
+        rows_field = self.conditioning_field
+        if rows_field=='':
+            nrows = 1
+        else:
+            rows_val2pos = mapping_of_values_to_pos(fieldvecs[rows_field])
+            rows_vals = rows_val2pos.keys()
+            nrows = len(rows_vals)
+        
+        if len(self.target_fields)==0:
+            cols_field = ''
+            ncols = 1
+        else:
+            cols_field = self.target_fields[0]
+            cols_val2pos = mapping_of_values_to_pos(fieldvecs[cols_field])
+            cols_vals = cols_val2pos.keys()
+            ncols = len(cols_vals)
+
+        # For convenience, make a new dictionary indexes by keys ('X', 'Y','C') rather than by actual fieldnames
+        kvecs = {}
+        for key,fieldname in self.graph_fields.items():
+            kvecs[key] = fieldvecs[fieldname]
+
+        mymarkers= ['o','s','>','<','^','v','d','p','h','g','+','x']
+        mymarkerdict = {} # will map values of the 'M' field to marker symbols
+        self.colors = ['b','g','r','c','y','k','m']
+        self.C2idx = {} # will map values of the 'C' field to color index
+        self.cmap = matplotlib.colors.ListedColormap(self.colors)
+
+        # Numerize a number of fields
+        if 'X' in kvecs:
+            kvecs['X'] = to_numpy_float_array(kvecs['X'], missing_value = 0.)
+        if 'Y' in kvecs:
+            kvecs['Y'] = to_numpy_float_array(kvecs['Y'], missing_value = 0.)
+        if 'S' in kvecs:
+            kvecs['S'] = to_numpy_float_array(kvecs['S'], missing_value = 0.)
+        if 'U' in kvecs:
+            kvecs['U'] = to_numpy_float_array(kvecs['U'], missing_value = 0.)
+        if 'V' in kvecs:
+            kvecs['V'] = to_numpy_float_array(kvecs['V'], missing_value = 0.)
+        if 'C' in kvecs: # transform it to real values indexed in self.cmap
+            n = len(self.colors)
+            for val in kvecs['C']:
+                self.C2idx.setdefault(val, len(self.C2idx)%n)
+            # print >>sys.stderr, self.C2idx
+            kvecs['Cfloat'] = [ float(self.C2idx[val])/n for val in kvecs['C'] ]
+        if 'M' in kvecs: # transform it to marker strings
+            for val in kvecs['M']:
+                mymarkerdict.setdefault(val, mymarkers[len(mymarkerdict)%len(mymarkers)])
+            kvecs['M'] = [ mymarkerdict[val] for val in kvecs['M'] ]
+            
+        # make sure we have both X and Y
+        if 'X' not in kvecs or 'Y' not in kvecs:
+            if 'X' in kvecs:
+                kvecs['Y'] = zeros(len(kvecs['X']))
+            elif 'Y' in kvecs:
+                kvecs['X'] = zeros(len(kvecs['Y']))
+            else:
+                raise ValueError("Scatter plot requires you to specify at least one of X or Y fields")
+            
+        for i in range(nrows):             
+            newtitle = ''
+            if rows_field=='':
+                r_kvecs_part = kvecs
+            else:
+                newtitle += rows_field+'='+str(rows_vals[i])
+                r_kvecs_part = self.take_part_of_fieldvecs(rows_val2pos[rows_vals[i]], kvecs)        
+            
+            if cols_field=='':
+                pylab.subplot(nrows, ncols, 1+i)
+                pylab.title(newtitle)
+                self.scatter_plot_in_axes(r_kvecs_part)
+            else:
+                cols_val2pos = mapping_of_values_to_pos(r_kvecs_part[cols_field])                
+                for cval,j in zip(cols_vals, range(ncols)):
+                    if cval in cols_val2pos:
+                        newtitle += cols_field+'='+str(cols_vals[j])
+                        c_kvecs_part = self.take_part_of_fieldvecs(cols_val2pos[cval], r_kvecs_part)
+                        pylab.subplot(nrows, ncols, 1+i*ncols+j)
+                        pylab.title(newtitle)
+                        self.scatter_plot_in_axes(c_kvecs_part)
+
+
+        C2spec = [ (val, self.colors[self.C2idx[val]]) for val in self.C2idx ]
+        C2spec.sort()
+        self.display_colors_legend(self.graph_fields.get('C',''), C2spec)
+        self.display_markers_legend(self.graph_fields.get('M',''), mymarkerdict)
+
+        
+    def scatter_plot_in_axes(self, kvecs, default_color='b', default_marker='o'):
+
+        # make a useful all-zeros vector
+        n = len(kvecs['X'])
+        zer = zeros(n)
+
+        X = kvecs.get('X',None)
+        Y = kvecs.get('Y',None)
+        U = kvecs.get('U',None)
+        V = kvecs.get('V',None)
+
+        # plot arrows
+        if U is not None or V is not None:
+            if V is None:
+                V = zer
+            elif U is None:
+                U = zer
+            pylab.quiver(X,Y,U,V,width=0.002, color='gray')
+                
+
+#         if 'M' in kvecs and 'C' in kvecs:
+#             markers_v2pos = mapping_of_values_to_pos(kvecs['M'])
+#             i = 0
+#             for mval, mpositions in markers_v2pos.items():
+#                 kvecs_m = self.take_part_of_fieldvecs(mpositions, kvecs)
+#                 marker = markers[i%len(markers)]
+                
+#                 colors_v2pos = mapping_of_values_to_pos(kvecs_m['C'])
+#                 j = 0
+#                 for cval, cpositions in colors_v2pos.items():
+#                     kvecs_mc = self.take_part_of_fieldvecs(cpositions, kvecs_m)
+#                     color = colors[j%len(colors)]
+#                     line = pylab.plot(kvecs_mc['X'],
+#                                       kvecs_mc['Y'],
+#                                       # s = kvecsp.get('S',20),
+#                                       marker = marker,
+#                                       color = color,
+#                                       linestyle = 'None'
+#                                       )
+#                     if i==0:
+#                         colors_legend_lines.append(line)
+#                         colors_legend_labels.append(str(cval))
+#                     if j==0:
+#                         markers_legend_lines.append(line)
+#                         markers_legend_labels.append(str(mval))
+#                     j = j+1
+#                 i = i+1
+
+            
+        # split on marker?        
+        if 'M' in kvecs:
+            val2pos = mapping_of_values_to_pos(kvecs['M'])
+            for marker, positions in val2pos.items():
+                kvecsp = self.take_part_of_fieldvecs(positions, kvecs)
+                #print >>sys.stderr, "Len: ",len(kvecsp['X']),len(kvecsp['Cfloat'])
+                #print >>sys.stderr, marker, ": ",kvecsp.get('C',default_color)
+                line = pylab.scatter(kvecsp['X'],
+                                     kvecsp['Y'],
+                                     s = kvecsp.get('S',40),
+                                     marker = marker,
+                                     c = kvecsp.get('Cfloat',default_color),
+                                     cmap = self.cmap,
+                                     #norm = matplotlib.colors.NoNorm(vmin=0, vmax=1)
+                                     vmin = 0.,
+                                     vmax = 1.)
+                
+        else: # no markers, make a single scatter plot
+#             msg = " x="+repr(kvecs['X'])\
+#                   +"\r\n y="+repr(kvecs['Y'])\
+#                   +"\r\n s="+repr(kvecs.get('S',20))\
+#                   +"\r\n marker="+repr(default_marker)\
+#                   +"\r\n c="+repr(kvecs.get('Cfloat',default_color))\
+#                   +"\r\n";            
+#             sys.stderr.write(msg)                  
+            pylab.scatter(kvecs['X'],
+                          kvecs['Y'],
+                          s = kvecs.get('S',40),
+                          marker = default_marker,
+                          c = kvecs.get('Cfloat',default_color),
+                          cmap = self.cmap,
+                          #norm = matplotlib.colors.NoNorm(vmin=0, vmax=1)
+                          vmin = 0.,
+                          vmax = 1.)
+                
+        pylab.xlabel(self.graph_fields.get('X','no X'))
+        pylab.ylabel(self.graph_fields.get('Y','no Y'))
+
+
+        #if len(colors_legend_lines)!=0:
+        #    pylab.legend(colors_legend_lines, colors_legend_labels, loc=1)
+
+#         if len(markers_legend_lines)!=0:
+#             pylab.legend(markers_legend_lines, markers_legend_labels, loc=4)
+
+#         legend_lines = []
+#         legend_lines.append(pylab.Line2D(range(10), range(10), linestyle='-', marker='o', color='b'))
+#         legend_labels.append('aaaa')
+#         legend_lines.append(pylab.Line2D(range(10), range(10), linestyle='-', marker='x', color='g'))
+#         legend_labels.append('bbbb')
+#         pylab.legend(legend_lines, legend_labels)
+
+    def graphical_plot(self):
+        menutxt = """
+        ************************************************
+        ** Choose the kind of graphical plot you want **
+        ************************************************
+          NOTE: THIS FEATURE IS STILL UNDER DEVELOPMENT
+                AND NOT QUITE READY TO USE YET (Pascal)
+                
+          [1]: scatter plot (with optional horizontal and vertical arrows).
+               Can use X, Y, C (color), M (marker), S (size), U, V.
+               If present uses U,V for a horizontal and vertical arrow of different scales.
+          [2]: scatter plot (with optional single arrow).
+               Same as [1] but U,V are considered the relative coordinates
+               of the tip of a single arrow and will have the same scale. 
+        
+          Or press ENTER to cancel
+        """
+
+        if "pylab" not in globals():
+            global pylab
+            import matplotlib.pylab as pylab
+
+        self.graphical_scatter_plot()
+
+#         k = self.display_fullscreen(menutxt,"12\n")
+#         if( k==ord('\n') ):
+#             return
+    
+#         if( k==ord('1') ):           
+#         elif( k==ord('2') ):           
+#             self.graphical_scatter_plot()
+
+
+            
+    def display_exception(self,exc, expression=''):
+        ls= (["Error in search expression:\n"] +
+             [ expression +'\n'] +
+             traceback.format_exception(*exc) +
+             ["\n(press any key to continue.)"])
+        txt= ""
+        for l in ls:
+            txt= txt + l
+        self.display_fullscreen(txt)
+
+
+    def display_fullscreen(self,txt, check_key=""):
+        """Displays the given text, and waits for a valid key press.
+        The code of the pressed key is returned.
+        Any key is considered valid if check_key is the empty string
+        If check_key is a character string, then one of the
+        specified keys must be pressed."""
+        self.stdscr.erase()
+        i = 1
+        for line in txt.split('\n'):
+            self.safeaddstr(i,2,line)
+            i += 1
+        self.stdscr.refresh();
+        k = self.stdscr.getch();
+        if check_key!="":
+            allowed_codes = [ ord(c) for c in check_key ]
+            while k not in allowed_codes:
+                k = self.stdscr.getch();                            
+        self.redraw()
+        return k
+        
+    def search_previous(self):
+        for i in xrange(self.current_i-1, -1, -1):
+            if self.search_expression=='':
+                val = self.getVal(i,self.search_field)
+                if str(val) == self.search_value:
+                    self.goto_line(i)
+                    break
+            else:
+                v = dict(zip(self.table.fieldnames, self.getFullRow(i)))
+                if eval(self.search_expression, globals(), v):
+                    self.goto_line(i)
+                    break
+
+    def alphabetical_order(self):        
+        selected_fieldnames = [ self.table.fieldnames[col] for col in self.selected_columns ]
+        selected_fieldnames.sort()
+        self.selected_columns = [ self.table.fieldnames.index(fname) for fname in selected_fieldnames ]
+        self.redraw()
+
+    def revert_to_original(self):
+        self.selected_columns = range(self.table.width())
+        self.selected_rows = None
+        self.redraw()        
+
+    def hideSelectedFields(self):
+        if self.selected_fields != []:
+            self.selected_columns = [ k for k in self.selected_columns if self.table.fieldnames[k] not in self.selected_fields ]
+        else: # no selected fields: simply hide the current field           
+            del self.selected_columns[self.current_j]
+        self.selected_fields = []
+        self.redraw()
+
+    def find_field(self):
+        fldname= self.input('Field name to find: ')
+        good = False
+        for j in xrange(self.table.width()):
+            if fldname.lower() == self.fieldname(j).lower():
+                self.goto_col(j)
+                self.redraw()
+                good = True
+                break
+        if not good:
+            self.writebottom("Field '" + fldname + "' not found.")
+
+    def event_loop(self):
+        ret = None
+        while not ret:
+            k = self.stdscr.getch() 
+            ret = self.handle_key_press(k)
+        if self.statsthread:
+            self.stopThread()
+        return ret
+
+    def handle_key_press(self, c):
+        if c == ord('q') or c==27:
+            return c
+        elif c == ord('\t'):
+            self.viewStatsTable()
+        elif c == ord('.'):
+            self.dot_mode = not self.dot_mode
+            self.redraw()
+        elif c == ord('t'):
+            self.transpose()
+        elif c == curses.KEY_UP:
+            self.up()
+        elif c == curses.KEY_DOWN:
+            self.down()
+        elif c == curses.KEY_LEFT:
+            self.left()
+        elif c == curses.KEY_RIGHT:
+            self.right()      
+        elif c == curses.KEY_NPAGE:
+            self.pgdown()            
+        elif c == curses.KEY_PPAGE:
+            self.pgup()
+        elif c == curses.KEY_HOME:
+            self.home()
+        elif c == curses.KEY_END:
+            self.end()
+        elif c == ord('s'):
+            self.chooseStats()
+            self.redraw()
+        elif c == ord('l'):
+            try: self.goto_line(int(self.input('Goto line #')))
+            except: pass
+        elif c == ord('='):
+            self.search_field = self.current_j
+            self.search_value = self.input('Search '+self.fieldname(self.current_j)+ ' = ')
+            self.search_expression = ''
+            self.search_next()
+        elif c == ord('/'):
+            self.search_field = self.current_j
+            self.search_expression = self.input('Search expression ex: float(AGE)>3 : ').strip()
+            if self.search_expression!='':
+                self.search_next()
+        elif c == ord('!'):
+            self.filter()
+        elif c == ord('N'):
+            self.search_next()
+        elif c == ord('P'):
+            self.search_previous()
+        elif c == ord('n'):
+            self.next_field()
+            self.redraw()
+        elif c == ord('p'):
+            self.previous_field()
+            self.redraw()
+        elif c == ord('a'):
+            self.alphabetical_order()
+        elif c == ord('o'):
+            self.revert_to_original()
+        elif c == ord('c'):
+            self.select_constant_cols()
+        elif c == ord('k'):
+            self.hideSelectedFields()
+        elif c == ord('\n'):
+            self.hideFieldsNotSelected()
+        elif c == ord('h'):
+            self.display_help()
+        elif c == ord('>'):
+            self.set_field_dim(self.fieldwidth+1,self.fieldheight)
+            self.redraw()
+        elif c == ord('<'):
+            self.set_field_dim(self.fieldwidth-1,self.fieldheight)
+            self.redraw()
+        elif c == ord(')'):
+            self.sort_rows(self.current_j)
+        elif c == ord('('):
+            self.sort_rows(self.current_j, reverse=True)
+        elif c == ord('x'):
+            self.chooseAndExecuteShellCommand()
+        elif c == ord('*'):
+            self.clearStats()
+            fieldname = self.fieldname(self.current_j)
+            if self.conditioning_field == fieldname:
+                self.conditioning_field = ''
+            else:
+                self.conditioning_field = fieldname
+            self.redraw()
+        elif c == ord('~'):
+            self.clearStats()
+            fieldname = self.fieldname(self.current_j)
+            if self.weight_field == fieldname:
+                self.weight_field = ''
+            else:
+                self.weight_field = fieldname
+            self.redraw()
+        elif c == ord('+'):
+            self.clearStats()
+            fieldname = self.fieldname(self.current_j)
+            pos = bisect.bisect_left(self.target_fields, fieldname)
+            if pos<len(self.target_fields) and self.target_fields[pos]==fieldname:
+                del self.target_fields[pos]
+            else:
+                self.target_fields.insert(pos,fieldname)
+            self.redraw()
+        elif c == ord('f'):
+            self.find_field()
+        elif c == ord('m'):
+            self.monetary_format= not self.monetary_format
+            if self.monetary_format:
+                locale.setlocale(locale.LC_NUMERIC, ('en_US','utf-8'))
+            else:
+                locale.setlocale(locale.LC_NUMERIC, self.orig_locale)
+            self.redraw()
+        elif c == ord('F'):
+            self.full_shuffle_stats= not self.full_shuffle_stats
+
+        elif c == ord(' '):
+            self.selectField()
+            self.redraw()
+
+        elif chr(c) in 'XYCMSUVW':
+            self.set_graph_field(chr(c))
+
+        elif c == ord('G'):
+            self.graphical_plot()
+
+        elif c == ord('w'):
+            self.saveSubTable()        
+
+        else:
+            curses.flash()
+            self.display_help(c)
+
+    def selectField(self):
+        field = self.fieldname(self.current_j)
+        if field in self.selected_fields:
+            self.selected_fields.remove(field)
+        else:
+            self.selected_fields.append(field)
+
+    def hideFieldsNotSelected(self):
+        if self.selected_fields != []:
+            self.j0 = 0
+            self.current_j = 0
+            self.selected_columns = [ self.table.fieldnames.index(fname) for fname in self.selected_fields]
+            self.selected_fields= []
+        self.redraw()
+
+    def saveSubTable(self):
+        orig_table= self.table.filepath()
+        pytablecode= """
+from plearn.table.table import *
+result = SelectFields(openTable('%s'),%s)
+        """ % (orig_table, [ self.table.fieldnames[col] for col in self.selected_columns ])
+
+        new_tablename= self.input('File name for sub-table (.pytable): ').strip()
+        if new_tablename != '':
+            (base,ext)= os.path.splitext(new_tablename)
+            if ext != '.pytable':
+                new_tablename+= '.pytable'
+            if os.path.exists(new_tablename):
+                self.writebottom("File '" + new_tablename + "' already exists.")
+                return
+            self.stopThread()
+            f= open(new_tablename, 'w')
+            f.write(pytablecode)
+            f.close()
+            newtable = openTable(new_tablename)
+            newtable.set_title('FILE: '+new_tablename)
+            self.reinit(newtable)
+
+    def viewStatsTable(self):
+        if not self.stats:
+            if self.conditioning_field: # we have a conditioning field
+                self.initStats()
+            else: # no conditioning field, try to load the most recently updated stats
+                self.loadMostRecentStats()
+            if not self.stats:
+                return
+
+        while 1:
+            self.statslock.acquire()
+            ns = self.stats.nsamples
+            self.statslock.release()
+            if ns<1:
+                break
+            currentvar = self.fieldname(self.current_j)
+            condvar = self.conditioning_field
+            valuenames = self.stats.extended_value_names()
+            if not self.sumvar:
+                self.sumvar = valuenames[0]
+##             try:
+##                 rmn = self.summarize_remove_n[currentvar]
+##             except KeyError:
+##                 rmn = -1
+##                 self.summarize_remove_n[currentvar] = rmn
+                
+            self.statslock.acquire()
+            stattab = self.stats.getCondTable(currentvar, condvar,
+                                              self.sumvar, self.divvar,
+                                              self.marginalize, self.minprob, self.combinvarsmode)
+            self.statslock.release()
+
+            if not self.statsview:
+                self.statsview = StatsTableView(stattab, self.stdscr)
+            else:
+                self.statsview.setTable(stattab)
+            l = self.table.length()
+            progress = str(ns)+'/'+str(l)+'  [ '+str((ns*1000/l)*0.1)+'% ]'
+            self.statsview.set_toprightinfo(progress)
+            self.statsview.redraw()
+            k = self.statsview.event_loop()
+
+            if k==ord(' '):
+                self.startThread()
+            if k==ord('1'):
+                if not self.sumvar:
+                    self.sumvar = valuenames[0]
+                else:
+                    idx = valuenames.index(self.sumvar)+1
+                    if idx>=len(valuenames):
+                        idx = 0
+                    self.sumvar = valuenames[idx]
+            elif k==ord('2'):
+                if not self.divvar:
+                    self.divvar = valuenames[0]
+                else:
+                    idx = valuenames.index(self.divvar)+1
+                    if idx>=len(valuenames):
+                        self.divvar = ''
+                    else:
+                        self.divvar = valuenames[idx]
+            elif k==ord('3'):
+                self.marginalize = (self.marginalize+1)%4
+            elif k==ord('4'):
+                self.combinvarsmode = (self.combinvarsmode+1)%2
+            elif k==ord('-'):
+                # self.summarize_remove_n[currentvar] += 1
+                if self.minprob<=0.:
+                    self.minprob = 0.001
+                else:
+                    self.minprob *= 1.1
+            elif k==ord('='):
+                # rmn = self.summarize_remove_n[currentvar]                
+                # self.summarize_remove_n[currentvar] = max(rmn-1,0)
+                self.minprob /= 1.1               
+                if self.minprob<0.001:
+                    self.minprob = 0.
+            elif k==ord('p'):
+                self.previous_field()
+            elif k==ord('n'):
+                self.next_field()
+            elif k==ord('s'):
+                self.chooseStats()
+            elif k==ord('q') or k==ord('\t'):
+                break
+        self.redraw()
+        
+
+class StatsTableView(TableView):
+
+    #def __init__(self, table, stdscr):
+    #    TableView.__init__(self, table, stdscr)
+
+    def handle_key_press(self, k):
+        if k in map(ord,"\tpns 1234-="):
+            return k
+        elif k in map(ord,"~+*"):
+            pass # ignore this key
+        else:
+            return TableView.handle_key_press(self, k)
+
+    
+def curses_viewtable(stdscr, table):
+    view = TableView(table, stdscr)
+    view.redraw()
+    view.event_loop()
+
+def curses_showtable(stdscr, table):
+    view = TableView(table, stdscr)
+    view.redraw()
+
+def viewtable(table):
+    # table = CacheTable(table,100)
+    curses.wrapper(curses_viewtable,table)
+
+    
+
+if __name__ == "__main__":
+    viewtable(openTable(sys.argv[1]))
+    



From plearner at mail.berlios.de  Wed Jun  3 01:00:17 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 3 Jun 2009 01:00:17 +0200
Subject: [Plearn-commits] r10228 - in trunk/scripts: . DEPRECATED
Message-ID: <200906022300.n52N0HIC017039@sheep.berlios.de>

Author: plearner
Date: 2009-06-03 01:00:15 +0200 (Wed, 03 Jun 2009)
New Revision: 10228

Added:
   trunk/scripts/DEPRECATED/upackage.py
   trunk/scripts/plcollect
   trunk/scripts/pltable
Removed:
   trunk/scripts/upackage.py
Modified:
   trunk/scripts/cldispatch
Log:
ApSTAT contributions of a few scripts


Copied: trunk/scripts/DEPRECATED/upackage.py (from rev 10224, trunk/scripts/upackage.py)

Modified: trunk/scripts/cldispatch
===================================================================
--- trunk/scripts/cldispatch	2009-06-02 22:59:44 UTC (rev 10227)
+++ trunk/scripts/cldispatch	2009-06-02 23:00:15 UTC (rev 10228)
@@ -3,7 +3,37 @@
 # cldispatch: dispatch utility for parallel PLearn
 #
 # Copyright 2004, Apstat Technologies, inc.
-# All rights reserved.
+#
+# This file was contributed by ApSTAT Technologies to the
+# PLearn library under the following BSD-style license:
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
 
 from optparse import OptionParser
 from plearn.parallel.dispatch import *

Added: trunk/scripts/plcollect
===================================================================
--- trunk/scripts/plcollect	2009-06-02 22:59:44 UTC (rev 10227)
+++ trunk/scripts/plcollect	2009-06-02 23:00:15 UTC (rev 10228)
@@ -0,0 +1,154 @@
+#!/usr/bin/env python
+
+# plcollect.py
+# Copyright (C) 2008 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+
+import os
+import sys
+from os.path import join
+
+from plearn.table.table import *
+# from plearn.utilities.autoscript import autoscript
+
+def parse_filepath(filepath, separators=['__','/']):
+    """parses the filepath:
+      Splits it on all given separators then checks each part in turn,
+      If it contains an = then it is assumed to form a key=value pair
+      and the coresponding pair which will get appended to a pairs_list.
+      If not the part is appended to a summarized path string.
+      The call returns a tuple (pairs_list, summarized_path).
+      """
+    pairs_list = []
+    summarized_path = ""
+    n = len(filepath)
+    part_start = 0
+    i = 0
+    prev_sep = '' # previous found separator
+    added_ellipses = False    
+    while i<n:
+        foundsep = None
+        for sep in separators:
+            if filepath[i:min(i+len(sep),n)]==sep: # we have found a separator
+                foundsep = sep
+                break
+        if foundsep is None and i==n-1: # consider the end of string as a separator
+            i += 1
+            foundsep = ''
+            
+        if foundsep is None: # no separator: increment i
+            i += 1
+        else: # found a separator: take care of that part
+            part = filepath[part_start:i]                
+            pair = tuple(part.split('=',1))
+            if len(pair)==2: # this part is a key=value pair: append it to the list
+                pairs_list.append(pair)
+                if not added_ellipses:
+                    summarized_path += '...'
+                    added_ellipses = True
+            else: # this part is not a key=value pair, append it to the summarized_path
+                if added_ellipses:
+                    summarized_path += prev_sep
+                summarized_path += part+foundsep
+                added_ellipses = False
+            # move past that separator
+            i += len(foundsep)
+            part_start = i
+            prev_sep = foundsep
+    return pairs_list, summarized_path
+
+def plcollect(rootdir="expdir", resultfile_txt="exp_results.txt", searchedfilenames=[]):
+    """
+    Looks for all split_stats.pmat files it can find in rootdir and its subdirectories,
+    and comples them into a single resultfile_txt
+    """
+
+    if len(searchedfilenames) == 0: # use default
+        searchedfilenames = ["Strat0results.pmat"]
+
+    filenames = []
+    # first look for all matching files
+    for root, dirs, files in os.walk(rootdir):
+        for filename in files:
+            if filename in searchedfilenames:
+                filenames.append(os.path.join(root,filename))
+
+    filepath_fields = []
+    content_fields = []
+    for filename in filenames:
+        pairs, summarypath = parse_filepath(filename)
+        t = openTable(filename)
+        for key,val in pairs:
+            if key not in filepath_fields:
+                filepath_fields.append(key)
+        for fieldname in t.fieldnames:
+            if fieldname not in content_fields:
+                content_fields.append(fieldname)
+
+    fieldnames = filepath_fields+content_fields
+    result_table = TableFile(resultfile_txt, 'w+', ["_filepath_","_summarypath_"]+fieldnames)
+    print "fieldnames length:",len(fieldnames)
+
+    for filename in filenames:
+        try:
+            print "Processing ", filename
+            pairs, summarypath = parse_filepath(filename)
+            d = dict(pairs)
+            t = openTable(filename)
+            for trow in t:
+                d2 = d.copy()
+                d2.update(zip(t.fieldnames,trow))
+                resrow = [filename, summarypath]
+                for fieldname in fieldnames:
+                    if fieldname in d2:
+                        resrow.append(d2[fieldname])
+                    else:
+                        resrow.append("-")
+                result_table.append(resrow)
+        except struct.error:
+            print "Probably a buggy .pmat"
+
+
+if len(sys.argv)<3:
+    print "Usage: plcollect.py expdir results.txt list_of_matrix_filenames"
+    print """
+    Looks in expdir and its subdirectories for all matrix files whose name matches
+    one specified in the list_of_matrix_filenames  
+    and compiles their content into a single results.txt
+
+    list of matrix filenames if unspecified defaults to split_stats.pmat
+    """
+    sys.exit()
+
+
+plcollect(sys.argv[1], sys.argv[2], sys.argv[3:])
+
+


Property changes on: trunk/scripts/plcollect
___________________________________________________________________
Name: svn:executable
   + *

Added: trunk/scripts/pltable
===================================================================
--- trunk/scripts/pltable	2009-06-02 22:59:44 UTC (rev 10227)
+++ trunk/scripts/pltable	2009-06-02 23:00:15 UTC (rev 10228)
@@ -0,0 +1,43 @@
+#!/usr/bin/env python
+
+"""
+pltable
+Copyright (C) 2005 ApSTAT Technologies Inc.
+
+This file was contributed by ApSTAT Technologies to the
+PLearn library under the following BSD-style license: 
+
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+"""
+
+import sys
+import cgitb; cgitb.enable(format='PLearn')
+from plearn.table.pltable_commands import main
+
+main(sys.argv)


Property changes on: trunk/scripts/pltable
___________________________________________________________________
Name: svn:executable
   + *

Deleted: trunk/scripts/upackage.py
===================================================================
--- trunk/scripts/upackage.py	2009-06-02 22:59:44 UTC (rev 10227)
+++ trunk/scripts/upackage.py	2009-06-02 23:00:15 UTC (rev 10228)
@@ -1,380 +0,0 @@
-#!/usr/bin/env python2.3
-
-# upackage
-#
-# Copyright (C) 2004 ApSTAT Technologies Inc.
-#
-#  Redistribution and use in source and binary forms, with or without
-#  modification, are permitted provided that the following conditions are met:
-#
-#   1. Redistributions of source code must retain the above copyright
-#      notice, this list of conditions and the following disclaimer.
-#
-#   2. Redistributions in binary form must reproduce the above copyright
-#      notice, this list of conditions and the following disclaimer in the
-#      documentation and/or other materials provided with the distribution.
-#
-#   3. The name of the authors may not be used to endorse or promote
-#      products derived from this software without specific prior written
-#      permission.
-#
-#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-#  This file is part of the PLearn library. For more information on the PLearn
-#  library, go to the PLearn Web site at www.plearn.org
-
-# Author: Pascal Vincent
-__cvs_id__ = "$Id$"
-
-
-import sys
-import os
-import string
-import urllib
-
-upackagedir = os.environ.get('UPACKAGEDIR',os.path.join(os.environ.get(os.environ.get('HOME')),'.upackage'))
-
-def check_setup():
-    """Checks setup for upackage."""
-    if not os.path.isdir(upackagedir):
-        mkdir(upackagedir)
-
-    configfile = os.path.join(upackagedir,'config.py')
-    if not os.path.isfile(configfile):
-        f = open(configfile,'w')
-        f.write("""
-upackage_sources = [  
-'/home/pascal/mypackages',
-'upackdb://www.upackages.org/upackdb',
-'ftp://ftp.upackages.org/upackages'
-]
-""")
-        f.close()
-        print "Created "+configfile
-        print "You may edit it to configure the behavior of upackage or to add sources of upackages."
-
-    # make sure we have a upackages sub-directory
-    mkdir(os.path.join(upackagedir,'upackages'))
-
-    improper_environment = False
-    if upackagedir not in string.split(os.environ.get('PYTHONPATH'),':'):        
-        print 'You must add '+upackagedir+' to the PYTHONPATH environment variable.'
-        improper_environment = True
-
-    platform = 'intel-linux'
-    prefixpath = os.path.join(os.path.join(upackagedir,'local'),platform)
-    # make sure we have a local/platform sub-directory
-    mkdir(prefixpath)
-
-    libdir = os.path.join(prefixpath,'lib')
-    if libdir not in string.split(os.environ.get('LD_LIBRARY_PATH'),':'):
-        print 'You must add '+libdir+' to the LD_LIBRARY_PATH environment variable.'
-        improper_environment = True
-
-    if libdir not in string.split(os.environ.get('LIBRARY_PATH'),':'):
-        print 'You must add '+libdir+' to the LIBRARY_PATH environment variable.'
-        improper_environment = True
-
-    includedir = os.path.join(prefixpath,'include')
-    if includedir not in string.split(os.environ.get('CPATH'),':'):
-        print 'You must add '+includedir+' to the CPATH environment variable.'
-        improper_environment = True
-
-    execpath = string.split(os.environ.get('PATH'),':')
-    bindir = os.path.join(prefixpath,'bin')
-    if bindir not in execpath:
-        print 'You must add '+bindir+' to the PATH environment variable.'
-        improper_environment = True
-
-    if improper_environment:
-        print """\n
-Please edit your startup file (.cshrc or .profile)
-and define your environment variables correctly.
-Then start a new shell and rerun upackage."""
-        sys.exit()
-
-check_setup()
-
-# include config.py
-execfile(os.path.join(upackagedir,'config.py'))
-
-
-
-def ask_upackdb_for_package_url(upackdburl,packagename):    
-    raise Error('Could not contact upackage database '+upackdburl)
-    raise Error('Package '+packagename+' is not known by database '+upackdburl)
-
-def get_upack_version(upack_url):
-    """Attempts to open the given upack and reads its first line,
-    which should be like: upack_version = '2004.06.18'
-    If successful the obtained version string is returned.
-    Otherwise some exception is raised.
-    """
-    f = urllib.urlopen(url)    
-    firstline = f.readline()    
-    tokens = string.split(firstline, " =\t\"\'")
-    if len(tokens!=2) or tokens[0]!='upack_version':
-        raise Error('First line of upackage '+upack_url+" appears invalid. It should be ex: upack_version = '2004.03.29' instead of "+firstline)
-    return tokens[1]
-
-def get_package(packagename):
-    packagepath = os.path.join(os.path.join(upackagedir,'upackages'),packagename+'.py')
-
-    oldversion = ''
-    try: oldversion = get_upack_version(packagepath)
-    except: pass
-    
-    for source in upackage_sources:
-        if string_begins_with(source, 'upackdb:'):
-            url = ''
-            try: url = ask_upackdb_for_package_url(source,packagename)
-            except: pass
-
-            if url!='': # The upackage db query returned a url where to fetch the .upack file from
-                newversion = ''
-                try: newversion = get_upack_version(url)
-                except: pass
-                
-                if newversion!='': # found a .upack file of which we were able to read the version
-                    if oldversion='' or compare_versions(newversion,oldversion)>0:
-                        download(url, packagepath)
-                    break
-
-        elif string_begins_with(source, 'ftp:') or string_begins_with(source,'http:'):
-            url = source+'/'+packagename+'.upack'
-            newversion = ''
-            try: newversion = get_upack_version(url)
-            except: pass
-
-            if newversion!='': # found a .upack file of which we were able to read the version
-                if oldversion='' or compare_versions(newversion,oldversion)>0:
-                    download(url, packagepath)
-                break
-
-    exec 'import upackages.'+packagename
-    exec 'package = upackages.'+packagename
-    return package
-
-def install_single_package(packagename, version, prefixdir):
-    upackages.make_usr_structure(prefixdir)
-    package = get_package(packagename)
-    srcdir = prefixdir+'/src/'+packagename+'/'+version
-    upackages.chdir(srcdir)
-    package.install(version, prefixdir)
-
-def install(packagename, version='', prefixdir=''):
-    package = get_package(packagename)
-
-    installed_versions = package.get_installed_versions()
-    installable_versions = package.get_installable_versions()
-
-    if version=='': # Let the user choose a version 
-        if installed_versions!=[]:
-            print 'VERSIONS OF',packagename,'THAT APPEAR TO BE CURRENTLY INSTALLED ON YOUR SYSTEM:'
-            print '  '+string.join(installed_versions,'\n  ')
-        print 'VERSIONS OF',packagename,'THAT ARE INSTALLABLE AS UPACKAGES:'
-        print '  '+string.join(installable_versions,'\n  ')
-        version = string.strip(raw_input('VERSION TO INSTALL (leave blank to exit)? '))
-        print '\n\n'
-        if version=='':
-            sys.exit()
-
-    if version not in installable_versions:
-        print 'Version',version,'is not in the list of installable package versions.'
-        sys.exit()
-    # if version in installed_versions:
-    #    print packagename, version, 'appears to be already installed.'
-    #    sys.exit()
-
-    if prefixdir=='': # Let the user choose a prefix
-        prefixes = get_environment_prefixpath()
-        print """\n
-        CHOOSE THE PREFIX WHERE TO BUILD AND INSTALL THE PACKAGE
-        (make sure you have write permission for that directory, switch user and rerun if needed)
-        The following prefixes are listed in UPACKAGE_PREFIX_PATH:"""
-        for i in range(len(prefixes)):
-            print i+1,':',prefixes[i]
-        i = input('Which one to use? ')
-        print '\n\n'
-        prefixdir=prefixes[i-1]
-
-    dependencies = package.get_dependencies(version)
-    for (depname, minimum_version) in dependencies:
-        deppack = get_package(depname)
-        installed_versions = deppack.get_installed_versions()
-        found_ver = ''
-        for ver in installed_versions:
-            if upackages.version_equal_or_greater(ver, minimum_version):
-                found_ver = ver
-                break
-        if found_ver!='':
-            print 'Found dependency ', depname, found_ver, 'OK.'
-        else:
-            print 'MISSING DEPENDENCY: ', packagename, 'requires', depname, 'version >=',minimum_version
-            print '  Versions of '+depname+' installable with upackage are:'
-            print '    '+string.join(deppack.get_installable_versions(),'    \n')
-            print '  Please enter the version you wish upackage to install '
-            print '  (alternatively press return when you are finished installing it with your O.S. packaging system).'
-            iver = string.strip(raw_input('  Version of '+depname+' to install? '))
-            print '\n\n'
-            if iver!='':
-                install(depname, iver, prefixdir)
-
-    install_single_package(packagename, version, prefixdir)
-    print 'FINISHED INSTALLING', packagename, version
-
-
-def environment_help_and_exit():
-    print """
-    Please edit your shell startup file
-    (.cshrc or .profile or equivalent) and
-    properly define your environment variables there.
-    
-    The following environment variables must be
-    defined for upackage to work properly:
-
-    UPACKAGE_PREFIX_PATH should contain a colon-separated list of
-    base prefix directories in which you can install extra packages
-    for yourself or the group of people you work with.
-    c-shell ex: setenv UPACKAGE_PREFIX_PATH $HOME/usr:$GROUPHOME/usr 
-    Where $GROUPHOME is some common place where you or somebody else 
-    in your group can install stuff so that everybody in the group can use it.
-    Note that if you work on multiple architectures/OS, you will probably
-    want to set-up architecture-dependent paths
-    for ex: setenv UPACKAGE_PREFIX_PATH $HOME/usr/$ARCH:$GROUPHOME/usr/$ARCH
-    Where $ARCH is defined to depend on the os/architecture.
-
-    PATH should contain (among other things) all the prefixes in
-    UPACKAGE_PREFIX_PATH with /bin appended to them.
-
-    LD_LIBRARY_PATH should contain a colon-separated list
-    of directories in which dynamic libraries may be searched-for.
-    (/lib and /usr/lib may be omitted from the list, but the list will
-    typically contain /usr/local/lib). In particular, it should contain
-    all the prefixes in UPACKAGE_PREFIX_PATH with /lib appended to them.
-
-    LIBRARY_PATH is used to give gcc a list of directories where to locate libraries.
-    It will typically be the same as LD_LIBRARY_PATH
-
-    CPATH is used to give gcc a list of directories where to locate includes.
-    It should contain all the prefixes in UPACKAGE_PREFIX_PATH with
-    /include appended to them.
-    """
-    sys.exit()
-
-def get_environment_prefixpath():
-    """Checks if relevant environment variables are consistently defined.
-    And returns the content of environment variable UPACKAGE_PREFIX_PATH
-    as a list of strings (the paths of the prefix directories).
-    The variables that are checked are: UPACKAGE_PREFIX_PATH,
-    LD_LIBRARY_PATH, LIBRARY_PATH, CPATH
-    If one of those is missing or inconsistent with UPACKAGE_PREFIX_PATH,
-    then the function calls environment_help_and_exit()
-    """
-
-    upackage_prefix_path = os.environ.get('UPACKAGE_PREFIX_PATH')
-    if not upackage_prefix_path:
-        print 'Environment variable UPACKAGE_PREFIX_PATH not defined!!!\n'
-        environment_help_and_exit()
-    upackage_prefix_path = [ string.rstrip(d,'/') for d in string.split(upackage_prefix_path,':') if d!='' ]
-
-    ld_library_path = os.environ.get('LD_LIBRARY_PATH')
-    if not ld_library_path:
-        print 'Environment variable LD_LIBRARY_PATH not defined!!!\n'
-        environment_help_and_exit()
-    ld_library_path = [ string.rstrip(d,'/') for d in string.split(ld_library_path,':') if d!='' ]
-    for d in upackage_prefix_path:
-        if os.path.join(d,'lib') not in ld_library_path:
-            print os.path.join(d,'lib'), 'not in LD_LIBRARY_PATH' 
-            environment_help_and_exit()
-
-    library_path = os.environ.get('LIBRARY_PATH')
-    if not library_path:
-        print 'Environment variable LIBRARY_PATH not defined!!!\n'
-        environment_help_and_exit()
-    library_path = [ string.rstrip(d,'/') for d in string.split(library_path,':') if d!='' ]
-    for d in upackage_prefix_path:
-        if os.path.join(d,'lib') not in library_path:
-            print os.path.join(d,'lib'), 'not in LIBRARY_PATH' 
-            environment_help_and_exit()
-
-    cpath = os.environ.get('CPATH')
-    if not cpath:
-        print 'Environment variable CPATH not defined!!!\n'
-        environment_help_and_exit()
-    cpath = [ string.rstrip(d,'/') for d in string.split(cpath,':') if d!='' ]
-    for d in upackage_prefix_path:
-        if os.path.join(d,'include') not in cpath:
-            print os.path.join(d,'include'), 'not in CPATH' 
-            environment_help_and_exit()
-
-    return upackage_prefix_path
-    
-
-def help_and_exit():
-    print 'Usage:', sys.argv[0], ' force_install <packagename> <version> <prefixdir>'
-    print '   or:', sys.argv[0], ' install <packagename> [<version>] [<prefixdir>]'
-    print '   or:', sys.argv[0], ' list'
-    print '   or:', sys.argv[0], ' info <packagename>'    
-    print '   or:', sys.argv[0], ' dependencies <packagename> <packageversion>'    
-    sys.exit()
-
-        
-def run():
-    args = sys.argv[1:]
-
-    if len(args)==0:
-        help_and_exit()
-
-    command = args[0]
-
-    if command=='list':
-        print 'Available packages:'
-        print string.join(upackages.packages,'\n')
-
-    elif command=='force_install':
-        packagename = args[1]
-        version = args[2]
-        prefixdir = args[3]
-        install_single_package(packagename, version, prefixdir)
-
-    elif command=='install':
-        packagename = args[1]
-        version = ''
-        try: version = args[2]
-        except: pass
-        prefixdir=''
-        try: prefixdir = args[3]
-        except: pass
-        install(packagename, version, prefixdir)
-
-    elif command=='info':
-        packagename = args[1]
-        package = get_package(packagename)
-        print 'UPACKAGE',packagename
-        print package.description()
-        print 'Versions installable with upackage:', package.get_installable_versions() 
-        print 'Currently installed versions found on your system:', package.get_installed_versions()
-
-    elif command=='dependencies':
-        packagename = args[1]
-        version = args[2]
-        package = get_package(packagename)
-        dependencies = package.get_dependencies(version)
-        print 'UPACKAGE',packagename, version, ' installation depends on:'
-        for depname, depver in dependencies:
-            print '  ',depname,' version >=',depver
-        
-try:
-    run()
-except KeyboardInterrupt:
-    print '\nABORTED due to KeyboardInterrupt.'



From plearner at mail.berlios.de  Wed Jun  3 01:01:16 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 3 Jun 2009 01:01:16 +0200
Subject: [Plearn-commits] r10229 - trunk/scripts
Message-ID: <200906022301.n52N1Gck017831@sheep.berlios.de>

Author: plearner
Date: 2009-06-03 01:01:09 +0200 (Wed, 03 Jun 2009)
New Revision: 10229

Modified:
   trunk/scripts/plcollect
Log:
typo fix


Modified: trunk/scripts/plcollect
===================================================================
--- trunk/scripts/plcollect	2009-06-02 23:00:15 UTC (rev 10228)
+++ trunk/scripts/plcollect	2009-06-02 23:01:09 UTC (rev 10229)
@@ -1,6 +1,7 @@
 #!/usr/bin/env python
 
-# plcollect.py
+# plcollect
+#
 # Copyright (C) 2008 Pascal Vincent
 #
 #  Redistribution and use in source and binary forms, with or without
@@ -138,7 +139,7 @@
 
 
 if len(sys.argv)<3:
-    print "Usage: plcollect.py expdir results.txt list_of_matrix_filenames"
+    print "Usage: plcollect expdir results.txt list_of_matrix_filenames"
     print """
     Looks in expdir and its subdirectories for all matrix files whose name matches
     one specified in the list_of_matrix_filenames  



From plearner at mail.berlios.de  Wed Jun  3 01:05:38 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 3 Jun 2009 01:05:38 +0200
Subject: [Plearn-commits] r10230 - trunk/scripts
Message-ID: <200906022305.n52N5c74020157@sheep.berlios.de>

Author: plearner
Date: 2009-06-03 01:05:37 +0200 (Wed, 03 Jun 2009)
New Revision: 10230

Modified:
   trunk/scripts/plcollect
Log:
Minor update to plcollect


Modified: trunk/scripts/plcollect
===================================================================
--- trunk/scripts/plcollect	2009-06-02 23:01:09 UTC (rev 10229)
+++ trunk/scripts/plcollect	2009-06-02 23:05:37 UTC (rev 10230)
@@ -1,7 +1,6 @@
 #!/usr/bin/env python
 
 # plcollect
-#
 # Copyright (C) 2008 Pascal Vincent
 #
 #  Redistribution and use in source and binary forms, with or without
@@ -34,10 +33,11 @@
 
 
 import os
+import os.path
 import sys
-from os.path import join
+import fnmatch
 
-from plearn.table.table import *
+from apstat.data.table import *
 # from plearn.utilities.autoscript import autoscript
 
 def parse_filepath(filepath, separators=['__','/']):
@@ -86,25 +86,32 @@
             prev_sep = foundsep
     return pairs_list, summarized_path
 
-def plcollect(rootdir="expdir", resultfile_txt="exp_results.txt", searchedfilenames=[]):
+def plcollect(rootdir="expdir", resultfile_txt="exp_results.txt", searchedpatterns=[]):
     """
     Looks for all split_stats.pmat files it can find in rootdir and its subdirectories,
     and comples them into a single resultfile_txt
     """
 
-    if len(searchedfilenames) == 0: # use default
-        searchedfilenames = ["Strat0results.pmat"]
+    if len(searchedpatterns) == 0: # use default
+        searchedpatterns = ["*/Strat0results.pmat"]
 
-    filenames = []
+    filepaths = []
     # first look for all matching files
-    for root, dirs, files in os.walk(rootdir):
+    for dirpath, dirs, files in os.walk(rootdir):
         for filename in files:
-            if filename in searchedfilenames:
-                filenames.append(os.path.join(root,filename))
+            filepath = os.path.join(dirpath,filename)
+            match = False
+            for pattern in searchedpatterns:
+                #print 'trying to match '+filepath+' WITH '+pattern
+                match = match or fnmatch.fnmatch(filepath, pattern)               
+            if match:
+                filepaths.append(filepath)
+                #if filename in searchedpatterns:
+                #filepaths.append(os.path.join(dirpath,filename))
 
     filepath_fields = []
     content_fields = []
-    for filename in filenames:
+    for filename in filepaths:
         pairs, summarypath = parse_filepath(filename)
         t = openTable(filename)
         for key,val in pairs:
@@ -118,7 +125,7 @@
     result_table = TableFile(resultfile_txt, 'w+', ["_filepath_","_summarypath_"]+fieldnames)
     print "fieldnames length:",len(fieldnames)
 
-    for filename in filenames:
+    for filename in filepaths:
         try:
             print "Processing ", filename
             pairs, summarypath = parse_filepath(filename)
@@ -139,13 +146,17 @@
 
 
 if len(sys.argv)<3:
-    print "Usage: plcollect expdir results.txt list_of_matrix_filenames"
+    print "Usage: plcollect expdir results.txt matrix_filepath_pattern1 matrix_filepath_pattern2 ..."
     print """
     Looks in expdir and its subdirectories for all matrix files whose name matches
-    one specified in the list_of_matrix_filenames  
-    and compiles their content into a single results.txt
+    one of the specified patterns and compiles their content into a single results.txt table
 
-    list of matrix filenames if unspecified defaults to split_stats.pmat
+    Ex: plcollect exp results.txt '*/Strat?results.pmat'
+
+    Note how the quotes are important for the shell not to perform the wildcard expansion of the specified pattern.
+
+    You may then examine the produced results table with e.g.
+    pltable view results.txt
     """
     sys.exit()
 



From plearner at mail.berlios.de  Wed Jun  3 01:17:17 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 3 Jun 2009 01:17:17 +0200
Subject: [Plearn-commits] r10231 - trunk/plearn/var/EXPERIMENTAL
Message-ID: <200906022317.n52NHHvS002690@sheep.berlios.de>

Author: plearner
Date: 2009-06-03 01:17:15 +0200 (Wed, 03 Jun 2009)
New Revision: 10231

Added:
   trunk/plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.cc
   trunk/plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.h
   trunk/plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.cc
   trunk/plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.h
   trunk/plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.cc
   trunk/plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.h
   trunk/plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.cc
   trunk/plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.h
Log:
Some extra experimental variables


Added: trunk/plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.cc	2009-06-02 23:05:37 UTC (rev 10230)
+++ trunk/plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.cc	2009-06-02 23:17:15 UTC (rev 10231)
@@ -0,0 +1,226 @@
+// -*- C++ -*-
+
+// LinearCombinationOfScalarVariables.cc
+//
+// Copyright (C) 2009 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file LinearCombinationOfScalarVariables.cc */
+
+
+#include "LinearCombinationOfScalarVariables.h"
+
+namespace PLearn {
+using namespace std;
+
+/** LinearCombinationOfScalarVariables **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    LinearCombinationOfScalarVariables,
+    "Computes a linear combination of scalar variables",
+    "Given an varray of input variables, computes sum_k coefs[k]*varray[k]->value[0] \n"
+    "i.e. you may also include non-scalar variables, but only their first element will be used \n"
+    );
+
+LinearCombinationOfScalarVariables::LinearCombinationOfScalarVariables()
+    :constant_term(0),
+     appendinputs(false)
+{}
+
+// constructors from input variables.
+// NaryVariable constructor (inherited) takes a VarArray as argument.
+// You can either construct from a VarArray (if the number of parent Var is not
+// fixed, for instance), or construct a VarArray from Var by operator &:
+// input1 & input2 & input3. You can also do both, uncomment what you prefer.
+
+// LinearCombinationOfScalarVariables::LinearCombinationOfScalarVariables(const VarArray& vararray)
+// ### replace with actual parameters
+//  : inherited(vararray, this_variable_length, this_variable_width),
+//    parameter(default_value),
+//    ...
+// {
+//     // ### You may (or not) want to call build_() to finish building the
+//     // ### object
+// }
+
+// LinearCombinationOfScalarVariables::LinearCombinationOfScalarVariables(Var input1, Var input2,
+//                            Var input3, ...)
+// ### replace with actual parameters
+//  : inherited(input1 & input2 & input3 & ...,
+//              this_variable_length, this_variable_width),
+//    parameter(default_value),
+//    ...
+// {
+//     // ### You may (or not) want to call build_() to finish building the
+//     // ### object
+// }
+
+// constructor from input variable and parameters
+LinearCombinationOfScalarVariables::LinearCombinationOfScalarVariables(const VarArray& vararray,
+                                                                       Vec the_coefs)
+    : inherited(vararray, 1, appendinputs?(1+vararray.length()):1), 
+      coefs(the_coefs), constant_term(0), appendinputs(false)
+{
+    build_();
+}
+
+// constructor from input variable and parameters
+// LinearCombinationOfScalarVariables::LinearCombinationOfScalarVariables(Var input1, Var input2,
+//                            Var input3, ...,
+//                            param_type the_parameter, ...)
+// ### replace with actual parameters
+//  : inherited(input1 & input2 & input3 & ...,
+//              this_variable_length, this_variable_width),
+//    parameter(the_parameter),
+//    ...
+// {
+//     // ### You may (or not) want to call build_() to finish building the
+//     // ### object
+// }
+
+void LinearCombinationOfScalarVariables::recomputeSize(int& l, int& w) const
+{
+    l = 1;
+    if(!appendinputs)
+        w = 1;
+    else
+        w = 1+varray.length();
+}
+
+// ### computes value from varray values
+void LinearCombinationOfScalarVariables::fprop()
+{
+    real res = constant_term;
+    int n = varray.length();
+    if(appendinputs)
+    {
+        for(int k=0; k<n; k++)
+        {
+            real val = varray[k]->valuedata[0];
+            valuedata[1+k] = val;
+            res += coefs[k]*val;
+        }
+    }
+    else
+    {
+        for(int k=0; k<n; k++)
+            res += coefs[k]*varray[k]->valuedata[0];
+    }
+    valuedata[0] = res;
+}
+
+// ### computes varray gradients from gradient
+void LinearCombinationOfScalarVariables::bprop()
+{
+    real gr = gradientdata[0];
+    for(int k=0; k<varray.length(); k++)
+        varray[k]->gradientdata[0] += gr*coefs[k];
+}
+
+// ### You can implement these methods:
+// void LinearCombinationOfScalarVariables::bbprop() {}
+// void LinearCombinationOfScalarVariables::symbolicBprop() {}
+// void LinearCombinationOfScalarVariables::rfprop() {}
+
+
+// ### Nothing to add here, simply calls build_
+void LinearCombinationOfScalarVariables::build()
+{
+    inherited::build();
+    build_();
+}
+
+void LinearCombinationOfScalarVariables::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(coefs, copies);
+}
+
+void LinearCombinationOfScalarVariables::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "coefs", &LinearCombinationOfScalarVariables::coefs,
+                  OptionBase::buildoption,
+                  "The vector of coefficients for the linear combination (weights for the sum)");
+
+    declareOption(ol, "constant_term", &LinearCombinationOfScalarVariables::constant_term,
+                  OptionBase::buildoption,
+                  "A constant term whose valiue will be added to the linear combination");
+
+    declareOption(ol, "appendinputs", &LinearCombinationOfScalarVariables::appendinputs,
+                  OptionBase::buildoption,
+                  "If true, the values of the first element of all input variables \n"
+                  "(with which we compute the weighted sum) will be appended in the \n"
+                  "produced output vector (just after their weighted sum). \n"
+                  "Note: gradient received on those appended elements is ignored (i.e. not backpropagated).\n"
+                  "  these are only useful as extra information (a decomposition of the cost) for reporting purpose");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void LinearCombinationOfScalarVariables::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.h	2009-06-02 23:05:37 UTC (rev 10230)
+++ trunk/plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.h	2009-06-02 23:17:15 UTC (rev 10231)
@@ -0,0 +1,190 @@
+// -*- C++ -*-
+
+// LinearCombinationOfScalarVariables.h
+//
+// Copyright (C) 2009 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file LinearCombinationOfScalarVariables.h */
+
+
+#ifndef LinearCombinationOfScalarVariables_INC
+#define LinearCombinationOfScalarVariables_INC
+
+#include <plearn/var/NaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+/*! * LinearCombinationOfScalarVariables * */
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class LinearCombinationOfScalarVariables : public NaryVariable
+{
+    typedef NaryVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! The coeficients for the linear combination
+    Vec coefs;
+    real constant_term;
+    bool appendinputs;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor, usually does nothing
+    LinearCombinationOfScalarVariables();
+
+    //! Constructor initializing from input variables
+    // NaryVariable constructor (inherited) takes a VarArray as
+    // argument.  You can either construct from a VarArray (if the
+    // number of parent Var is not fixed, for instance), or construct
+    // a VarArray from Var by operator &: input1 & input2 &
+    // input3. You can also do both, uncomment what you prefer.
+
+    // ### Make sure the implementation in the .cc calls inherited constructor
+    // ### and initializes all fields with reasonable default values.
+    // LinearCombinationOfScalarVariables(const VarArray& vararray);
+    // LinearCombinationOfScalarVariables(Var input1, Var input2, Var input3, ...);
+
+    // ### If your class has parameters, you probably want a constructor that
+    // ### initializes them
+    LinearCombinationOfScalarVariables(const VarArray& vararray, Vec the_coefs);
+
+    // ### If your parent variables are a meaning and you want to be able to
+    // ### access them easily, you can add functions like:
+    // Var& First() { return varray[0]; }
+    // Var& Second() { return varray[1]; }
+    // ...
+
+    // Your other public member functions go here
+
+    //#####  PLearn::Variable methods #########################################
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+
+    // ### These ones are not always implemented
+    // virtual void bbprop();
+    // virtual void symbolicBprop();
+    // virtual void rfprop();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(LinearCombinationOfScalarVariables);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LinearCombinationOfScalarVariables);
+
+// ### Put here a convenient method for building your variable from other
+// ### existing ones, or a VarArray.
+// ### e.g., if your class is TotoVariable, with two parameters foo_type foo
+// ### and bar_type bar, you could write, depending on your constructor:
+// inline Var toto(Var v1, Var v2, Var v3,
+//                 foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v1, v2, v3, foo, bar); }
+// ### or:
+// inline Var toto(Var v1, Var v2, v3
+//                 foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v1 & v2 & v3, foo, bar); }
+// ### or:
+// inline Var toto(const VarArray& varray, foo_type foo=default_foo,
+//                 bar_type bar=default_bar)
+// { return new TotoVariable( varray, foo, bar); }
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.cc	2009-06-02 23:05:37 UTC (rev 10230)
+++ trunk/plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.cc	2009-06-02 23:17:15 UTC (rev 10231)
@@ -0,0 +1,213 @@
+// -*- C++ -*-
+
+// SaltPepperNoiseVariable.cc
+//
+// Copyright (C) 2009 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file SaltPepperNoiseVariable.cc */
+
+
+#include "SaltPepperNoiseVariable.h"
+
+namespace PLearn {
+using namespace std;
+
+/** SaltPepperNoiseVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    SaltPepperNoiseVariable,
+    "ONE LINE USER DESCRIPTION",
+    "MULTI LINE\nHELP FOR USERS"
+    );
+
+SaltPepperNoiseVariable::SaltPepperNoiseVariable()
+    :corruption_prob(0),
+     pepper_prob(0.5),
+     salt_value(0),
+     pepper_value(1)
+{
+    // ### You may (or not) want to call build_() to finish building the object
+    // ### (doing so assumes the parent classes' build_() have been called too
+    // ### in the parent classes' constructors, something that you must ensure)
+}
+
+// constructor from input variable and parameters
+// SaltPepperNoiseVariable::SaltPepperNoiseVariable(Variable* input, param_type the_parameter,...)
+// ### replace with actual parameters
+//  : inherited(input, this_variable_length, this_variable_width),
+//    parameter(the_parameter),
+//    ...
+//{
+//    // ### You may (or not) want to call build_() to finish building the
+//    // ### object
+//}
+
+void SaltPepperNoiseVariable::recomputeSize(int& l, int& w) const
+{
+    if (input) 
+    {
+        l = input.length();
+        w = input.width(); // the computed width
+    } 
+    else
+        l = w = 0;
+}
+
+// ### computes value from input's value
+void SaltPepperNoiseVariable::fprop()
+{
+    checkContiguity();
+
+    if(random_gen.isNull())
+        random_gen = PRandom::common(false);
+
+
+    int n = value.length();
+    corrupted.resize(n);
+
+    for(int i=0; i<n; i++)
+    {
+        if(random_gen->uniform_sample()<corruption_prob)
+        {
+            if(random_gen->uniform_sample()<pepper_prob)
+                valuedata[i] = pepper_value;
+            else
+                valuedata[i] = salt_value;
+
+            corrupted[i] = true;
+        }
+        else
+        {
+            valuedata[i] = input->valuedata[i];
+            corrupted[i] = false;
+        }
+    }
+}
+
+// ### computes input's gradient from gradient
+void SaltPepperNoiseVariable::bprop()
+{
+    int n = gradient.length();
+    for(int i=0; i<n; i++)
+    {
+        if(!corrupted[i])
+            input->gradientdata[i] += gradientdata[i];
+    }
+}
+
+// ### You can implement these methods:
+// void SaltPepperNoiseVariable::bbprop() {}
+// void SaltPepperNoiseVariable::symbolicBprop() {}
+// void SaltPepperNoiseVariable::rfprop() {}
+
+
+// ### Nothing to add here, simply calls build_
+void SaltPepperNoiseVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+void SaltPepperNoiseVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    deepCopyField(random_gen, copies);
+
+    // ### If you want to deepCopy a Var field:
+    // varDeepCopyField(somevariable, copies);
+}
+
+void SaltPepperNoiseVariable::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "corruption_prob", &SaltPepperNoiseVariable::corruption_prob,
+                  OptionBase::buildoption,
+                  "The probability of corrupting each input element");
+    declareOption(ol, "pepper_prob", &SaltPepperNoiseVariable::pepper_prob,
+                  OptionBase::buildoption,
+                  "If an element is to be corrupted, it will be set to pepper_value with probability pepper_prob and to salt_value with probability 1-pepper_prob.");
+    declareOption(ol, "salt_value", &SaltPepperNoiseVariable::salt_value,
+                  OptionBase::buildoption,
+                  "The value to which to set all inputs that have been elected to be corrupted to salt.");
+    declareOption(ol, "pepper_value", &SaltPepperNoiseVariable::pepper_value,
+                  OptionBase::buildoption,
+                  "The value to which to set all inputs that have been elected to be corrupted to pepper.");
+    declareOption(ol, "random_gen", &SaltPepperNoiseVariable::random_gen,
+                  OptionBase::buildoption,
+                  "Random number generator. If null, the PRandom::common(false) generator will be used.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void SaltPepperNoiseVariable::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.h	2009-06-02 23:05:37 UTC (rev 10230)
+++ trunk/plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.h	2009-06-02 23:17:15 UTC (rev 10231)
@@ -0,0 +1,166 @@
+// -*- C++ -*-
+
+// SaltPepperNoiseVariable.h
+//
+// Copyright (C) 2009 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file SaltPepperNoiseVariable.h */
+
+
+#ifndef SaltPepperNoiseVariable_INC
+#define SaltPepperNoiseVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+#include <plearn/math/PRandom.h>
+
+namespace PLearn {
+using namespace std;
+
+/*! * SaltPepperNoiseVariable * */
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class SaltPepperNoiseVariable : public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    PP<PRandom> random_gen;
+    real corruption_prob;
+    real pepper_prob;
+    real salt_value;
+    real pepper_value;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor, usually does nothing
+    SaltPepperNoiseVariable();
+
+    // ### If your class has parameters, you probably want a constructor that
+    // ### initializes them
+    // SaltPepperNoiseVariable(Variable* input, param_type the_parameter, ...);
+
+    // Your other public member functions go here
+
+    //#####  PLearn::Variable methods #########################################
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+
+    // ### These ones are not always implemented
+    // virtual void bbprop();
+    // virtual void symbolicBprop();
+    // virtual void rfprop();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(SaltPepperNoiseVariable);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+    
+    // will contain true where features have been corrupted 
+    TVec<bool> corrupted;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SaltPepperNoiseVariable);
+
+// ### Put here a convenient method for building your variable.
+// ### e.g., if your class is TotoVariable, with two parameters foo_type foo
+// ### and bar_type bar, you could write:
+// inline Var toto(Var v, foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v, foo, bar); }
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.cc	2009-06-02 23:05:37 UTC (rev 10230)
+++ trunk/plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.cc	2009-06-02 23:17:15 UTC (rev 10231)
@@ -0,0 +1,194 @@
+// -*- C++ -*-
+
+// SumEntropyOfBernoullis.cc
+//
+// Copyright (C) 2009 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file SumEntropyOfBernoullis.cc */
+
+
+#include "SumEntropyOfBernoullis.h"
+
+namespace PLearn {
+using namespace std;
+
+/** SumEntropyOfBernoullis **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    SumEntropyOfBernoullis,
+    "Computes the sum of the entropies of independent Bernoulli variables whose parameters are given as input.",
+    "i.e. if input is a matrix P, computes sum_ij - [P_ij log(P_ij) + (1-P_ij) log(1-P_ij)]"
+    );
+
+SumEntropyOfBernoullis::SumEntropyOfBernoullis()
+    : inherited(0, 1, 1)
+{
+}
+
+// constructor from input variable.
+SumEntropyOfBernoullis::SumEntropyOfBernoullis(Variable* input)
+    : inherited(input, 1, 1)
+{
+    build_();
+}
+
+// constructor from input variable and parameters
+// SumEntropyOfBernoullis::SumEntropyOfBernoullis(Variable* input, param_type the_parameter,...)
+// ### replace with actual parameters
+//  : inherited(input, this_variable_length, this_variable_width),
+//    parameter(the_parameter),
+//    ...
+//{
+//    // ### You may (or not) want to call build_() to finish building the
+//    // ### object
+//}
+
+void SumEntropyOfBernoullis::recomputeSize(int& l, int& w) const
+{
+    l = w = 1;
+}
+
+// ### computes value from input's value
+void SumEntropyOfBernoullis::fprop()
+{
+    double res = 0;
+    Mat P = input->matValue;
+    for(int i=0; i<P.length(); i++)
+    {
+        const real* P_i = P[i];
+        for(int j=0; j<P.width(); j++)
+        {
+            double P_ij = P_i[j];
+            if( (P_ij>1e-25) && ((1-P_ij)>1e-25) )
+                res -= P_ij*pl_log(P_ij) + (1-P_ij)*pl_log(1-P_ij);
+        }
+    }
+    valuedata[0] = (real)res;
+}
+
+// ### computes input's gradient from gradient
+void SumEntropyOfBernoullis::bprop()
+{
+    /*
+      -[p log(p) + (1-p) log(1-p)]' = -[log(p) + 1 - log(1-p) - 1 ]
+                                    = log(1-p) - log(p)
+    */
+    real gr = gradientdata[0];
+    Mat P = input->matValue;
+    Mat Pgr = input->matGradient;
+    for(int i=0; i<Pgr.length(); i++)
+    {
+        const real* P_i = P[i];
+        real* Pgr_i = Pgr[i];
+        for(int j=0; j<Pgr.width(); j++)
+        {
+            double P_ij = P_i[j];
+            double one_minus_P_ij = 1-P_ij;
+            if(P_ij<1e-25) 
+                P_ij = 1e-25;
+            if(one_minus_P_ij<1e-25)
+                one_minus_P_ij = 1e-25;
+            Pgr_i[j] += gr*( pl_log(one_minus_P_ij) - pl_log(P_ij) );
+        }
+    }
+    
+
+}
+
+// ### You can implement these methods:
+// void SumEntropyOfBernoullis::bbprop() {}
+// void SumEntropyOfBernoullis::symbolicBprop() {}
+// void SumEntropyOfBernoullis::rfprop() {}
+
+
+// ### Nothing to add here, simply calls build_
+void SumEntropyOfBernoullis::build()
+{
+    inherited::build();
+    build_();
+}
+
+void SumEntropyOfBernoullis::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void SumEntropyOfBernoullis::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, "myoption", &SumEntropyOfBernoullis::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+    // ...
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void SumEntropyOfBernoullis::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.h	2009-06-02 23:05:37 UTC (rev 10230)
+++ trunk/plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.h	2009-06-02 23:17:15 UTC (rev 10231)
@@ -0,0 +1,162 @@
+// -*- C++ -*-
+
+// SumEntropyOfBernoullis.h
+//
+// Copyright (C) 2009 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file SumEntropyOfBernoullis.h */
+
+
+#ifndef SumEntropyOfBernoullis_INC
+#define SumEntropyOfBernoullis_INC
+
+#include <plearn/var/UnaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+/*! * SumEntropyOfBernoullis * */
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class SumEntropyOfBernoullis : public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor, usually does nothing
+    SumEntropyOfBernoullis();
+
+    //! Constructor initializing from input variable
+    // ### Make sure the implementation in the .cc calls inherited constructor
+    // ### and initializes all fields with reasonable default values.
+    SumEntropyOfBernoullis(Variable* input);
+
+    // ### If your class has parameters, you probably want a constructor that
+    // ### initializes them
+    // SumEntropyOfBernoullis(Variable* input, param_type the_parameter, ...);
+
+    // Your other public member functions go here
+
+    //#####  PLearn::Variable methods #########################################
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+
+    // ### These ones are not always implemented
+    // virtual void bbprop();
+    // virtual void symbolicBprop();
+    // virtual void rfprop();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(SumEntropyOfBernoullis);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SumEntropyOfBernoullis);
+
+// ### Put here a convenient method for building your variable.
+// ### e.g., if your class is TotoVariable, with two parameters foo_type foo
+// ### and bar_type bar, you could write:
+// inline Var toto(Var v, foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v, foo, bar); }
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.cc	2009-06-02 23:05:37 UTC (rev 10230)
+++ trunk/plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.cc	2009-06-02 23:17:15 UTC (rev 10231)
@@ -0,0 +1,264 @@
+// -*- C++ -*-
+
+// SumVarianceOfLinearTransformedBernoullis.cc
+//
+// Copyright (C) 2009 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file SumVarianceOfLinearTransformedBernoullis.cc */
+
+
+#include "SumVarianceOfLinearTransformedBernoullis.h"
+
+namespace PLearn {
+using namespace std;
+
+/** SumVarianceOfLinearTransformedBernoullis **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    SumVarianceOfLinearTransformedBernoullis,
+    "Computes the sum of the variances of the components resulting from a linear transformation of independent bernoullis",
+    "Let H be a (l,m) matrix of independent bernoullis.\n"
+    "Let P be a (l,m) matrix containing their corresponding parameters (expectations or prob of 1) \n"
+    "Let W a (m,d) linear transformation matrix, \n"
+    "Define Y=HW, Y is a (l,d) matrix of random variables, with Y_ij = <H_i,W_.j> \n"
+    "i.e. Y_ij is a linear combination of bernoullis in row i of H whose parameters are in row i of P, \n"
+    "with the combination coefficients being in column j of W \n"
+    "We have Var[Y_ij] = sum_k Var[H_ik] (W_kj)^2 \n"
+    "                  = sum_k P_ik (1-P_ik) (W_kj)^2 \n"
+    "This binary variable, given input1 P and input2 W, computes:\n"
+    "   out = sum_ij Var[Y_ij] \n"
+    "       = sum_ijk P_ik (1-P_ik) (W_kj)^2 \n"
+    "       = sum_i {sum_k P_ik (1-P_ik) {sum_j (W_kj)^2}} \n"
+    "       = sum_i {sum_k P_ik (1-P_ik) ||W_k.||^2 } \n"
+    );
+
+SumVarianceOfLinearTransformedBernoullis::SumVarianceOfLinearTransformedBernoullis()
+    : inherited(0,0,1,1)
+{}
+
+SumVarianceOfLinearTransformedBernoullis::SumVarianceOfLinearTransformedBernoullis(Variable* input1, Variable* input2,
+                           bool call_build_)
+    : inherited(input1, input2, 1, 1, call_build_)
+{
+    if (call_build_)
+        build_();
+}
+
+// constructor from input variable and parameters
+// SumVarianceOfLinearTransformedBernoullis::SumVarianceOfLinearTransformedBernoullis(Variable* input1, Variable* input2,
+//                            param_type the_parameter, ...,
+//                            bool call_build_)
+// ### replace with actual parameters
+//  : inherited(input1, input2, this_variable_length, this_variable_width,
+//              call_build_),
+//    parameter(the_parameter),
+//    ...
+//{
+//    if (call_build_)
+//        build_();
+//}
+
+void SumVarianceOfLinearTransformedBernoullis::recomputeSize(int& l, int& w) const
+{
+    l = 1;
+    w = 1;
+}
+
+void SumVarianceOfLinearTransformedBernoullis::computeWsqnorm()
+{
+    Mat W = input2->matValue;
+    int m = W.length();
+    int d = W.width();
+
+    Wsqnorm.resize(m);
+    real* pWsqnorm = Wsqnorm.data();
+    for(int k=0; k<m; k++)
+    {
+        real* Wk = W[k];
+        real sqnorm = 0;
+        for(int j=0; j<d; j++)
+        {
+            real Wkj = Wk[j];
+            sqnorm += Wkj*Wkj;
+        }
+        pWsqnorm[k] = sqnorm;
+    }
+}
+
+// ### computes value from input1 and input2 values
+void SumVarianceOfLinearTransformedBernoullis::fprop()
+{
+    Mat P = input1->matValue;
+    int l = P.length();
+    int m = P.width();
+
+    if(m!=input2.length())
+        PLERROR("Incompatible sizes: width of P (input1) must equal length of W (input2)"); 
+
+    computeWsqnorm();
+    const real* pWsqnorm = Wsqnorm.data();
+
+    double out = 0;
+    for(int i=0; i<l; i++)
+    {
+        const real* P_i = P[i];
+        for(int k=0; k<m; k++)
+        {
+            real P_ik = P_i[k];
+            out += P_ik*(1-P_ik)*pWsqnorm[k];
+        }
+    }
+    value[0] = out;
+}
+
+// ### computes input1 and input2 gradients from gradient
+void SumVarianceOfLinearTransformedBernoullis::bprop()
+{
+    Mat P = input1->matValue;
+    Mat Pgrad = input1->matGradient;
+    Mat W = input2->matValue;
+    Mat Wgrad = input2->matGradient;
+    int l = P.length();
+    int m = W.length();
+    int d = W.width();
+
+    real gr = gradient[0];
+
+    computeWsqnorm();
+    const real* pWsqnorm = Wsqnorm.data();
+
+    Wsqnormgrad.resize(m);
+    Wsqnormgrad.fill(0);
+    real* pWsqnormgrad = Wsqnormgrad.data(); 
+
+    for(int i=0; i<l; i++)
+    {
+        const real* P_i = P[i];
+        real* Pgrad_i = Pgrad[i];
+        for(int k=0; k<m; k++)
+        {
+            real P_ik = P_i[k];
+            Pgrad_i[k] += gr*(1-2*P_ik)*pWsqnorm[k];
+            pWsqnormgrad[k] += P_ik*(1-P_ik);
+        }
+    }
+
+    for(int k=0; k<m; k++)
+    {
+        real coefk = 2*gr*pWsqnormgrad[k];
+        const real* W_k = W[k];
+        real* Wgrad_k = Wgrad[k];
+        for(int j=0; j<d; j++)
+            Wgrad_k[j] += coefk*W_k[j];
+    }
+    
+}
+
+// ### You can implement these methods:
+// void SumVarianceOfLinearTransformedBernoullis::bbprop() {}
+// void SumVarianceOfLinearTransformedBernoullis::symbolicBprop() {}
+// void SumVarianceOfLinearTransformedBernoullis::rfprop() {}
+
+
+// ### Nothing to add here, simply calls build_
+void SumVarianceOfLinearTransformedBernoullis::build()
+{
+    inherited::build();
+    build_();
+}
+
+void SumVarianceOfLinearTransformedBernoullis::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+    deepCopyField(Wsqnorm, copies);
+    deepCopyField(Wsqnormgrad, copies);
+
+    // ### If you want to deepCopy a Var field:
+    // varDeepCopyField(somevariable, copies);
+}
+
+void SumVarianceOfLinearTransformedBernoullis::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, "myoption", &SumVarianceOfLinearTransformedBernoullis::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+    // ...
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void SumVarianceOfLinearTransformedBernoullis::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.h	2009-06-02 23:05:37 UTC (rev 10230)
+++ trunk/plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.h	2009-06-02 23:17:15 UTC (rev 10231)
@@ -0,0 +1,169 @@
+// -*- C++ -*-
+
+// SumVarianceOfLinearTransformedBernoullis.h
+//
+// Copyright (C) 2009 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file SumVarianceOfLinearTransformedBernoullis.h */
+
+
+#ifndef SumVarianceOfLinearTransformedBernoullis_INC
+#define SumVarianceOfLinearTransformedBernoullis_INC
+
+#include <plearn/var/BinaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+/*! * SumVarianceOfLinearTransformedBernoullis * */
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class SumVarianceOfLinearTransformedBernoullis : public BinaryVariable
+{
+    typedef BinaryVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor.
+    SumVarianceOfLinearTransformedBernoullis();
+
+    //! Constructor initializing from two input variables.
+    // ### Make sure the implementation in the .cc calls inherited constructor
+    // ### and initializes all fields with reasonable default values.
+    SumVarianceOfLinearTransformedBernoullis(Variable* input1, Variable* input2, bool call_build_ = true);
+
+    // ### If your class has parameters, you probably want a constructor that
+    // ### initializes them
+    // SumVarianceOfLinearTransformedBernoullis(Variable* input1, Variable* input2,
+    //              param_type the_parameter, ..., bool call_build_ = true);
+
+    // Your other public member functions go here
+
+    //#####  PLearn::Variable methods #########################################
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+
+    // ### These ones are not always implemented
+    // virtual void bbprop();
+    // virtual void symbolicBprop();
+    // virtual void rfprop();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(SumVarianceOfLinearTransformedBernoullis);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+    void computeWsqnorm();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // temporary variables used in fprop and bprop
+    Vec Wsqnorm;
+    Vec Wsqnormgrad;
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SumVarianceOfLinearTransformedBernoullis);
+
+// ### Put here a convenient method for building your variable from two
+// ### existing ones.
+// ### e.g., if your class is TotoVariable, with two parameters foo_type foo
+// ### and bar_type bar, you could write:
+// inline Var toto(Var v1, Var v2,
+//                 foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v1, v2, foo, bar); }
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From plearner at mail.berlios.de  Wed Jun  3 01:24:01 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 3 Jun 2009 01:24:01 +0200
Subject: [Plearn-commits] r10232 - trunk/commands/EXPERIMENTAL
Message-ID: <200906022324.n52NO1PT011890@sheep.berlios.de>

Author: plearner
Date: 2009-06-03 01:24:00 +0200 (Wed, 03 Jun 2009)
New Revision: 10232

Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
Log:
experimental stuff

Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2009-06-02 23:17:15 UTC (rev 10231)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2009-06-02 23:24:00 UTC (rev 10232)
@@ -355,6 +355,11 @@
 // ***************************************************
 // ***   New EXPERIMENTAL stuff
 
+#include <plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.h>
+#include <plearn/var/EXPERIMENTAL/SumEntropyOfBernoullis.h>
+#include <plearn/var/EXPERIMENTAL/LinearCombinationOfScalarVariables.h>
+#include <plearn/var/EXPERIMENTAL/SaltPepperNoiseVariable.h>
+
 // includes Pascal's gradient hack
 #include <plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h>
 
@@ -373,12 +378,12 @@
 #include <plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.h>
 #include <plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.h>
 #include <plearn/var/EXPERIMENTAL/LogSoftSoftMaxVariable.h>
-#include <plearn/var/EXPERIMENTAL/SumVarianceOfLinearTransformedBernoullis.h>
 #include <plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h>
 #include <plearn/var/SourceVariable.h>
 #include <plearn/var/ConcatColumnsVariable.h>
 #include <plearn/var/ConcatRowsVariable.h>
 #include <plearn/var/ExpVariable.h>
+#include <plearn/var/LogVariable.h>
 #include <plearn/var/SigmoidVariable.h>
 #include <plearn/var/ProductTransposeVariable.h>
 #include <plearn/var/NegCrossEntropySigmoidVariable.h>



From plearner at mail.berlios.de  Wed Jun  3 22:04:33 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 3 Jun 2009 22:04:33 +0200
Subject: [Plearn-commits] r10233 - in trunk/python_modules/plearn: . data
Message-ID: <200906032004.n53K4XaK022802@sheep.berlios.de>

Author: plearner
Date: 2009-06-03 22:04:33 +0200 (Wed, 03 Jun 2009)
New Revision: 10233

Added:
   trunk/python_modules/plearn/data/
   trunk/python_modules/plearn/data/datasets.py
Log:
access to common dataset pyplearn specifications

Added: trunk/python_modules/plearn/data/datasets.py
===================================================================
--- trunk/python_modules/plearn/data/datasets.py	2009-06-02 23:24:00 UTC (rev 10232)
+++ trunk/python_modules/plearn/data/datasets.py	2009-06-03 20:04:33 UTC (rev 10233)
@@ -0,0 +1,140 @@
+import sys
+from plearn.pyplearn import *
+from plearn.var.Var import *
+
+datadir = "DBDIR:"
+
+# Format: [ inputsize, targetsize, nclasses, trainset, validset, testset ]
+
+
+def listDatasets():
+    """Returns the list of all classification tasks available with getDatasets(datasetname)"""
+    return all_data_sets.keys()
+
+def getDatasets(datasetname, ntrain=None, nvalid=None, ntest=None):
+    """Ex: getDataset("mnist_bg")
+    will return a tuple: inputsize, targetsize, nclasses, trainset, validset, testset
+    where the sets are VMatrix specifications.
+    If ntrain, nvalid, ntest are left unspecified (or None) then the full standard sets
+    will be used. If they are specified, then a subset with that many examples will be used.
+    """
+    inputsize, targetsize, nclasses, trainset, validset, testset = all_data_sets[datasetname]
+
+    if ntrain is not None:
+        trainset = pl.RowsSubVMatrix(source=trainset, startrow=0, length=ntrain)
+    if nvalid is not None:
+        validset = pl.RowsSubVMatrix(source=validset, startrow=0, length=nvalid)
+    if ntest is not None:
+        testset = pl.RowsSubVMatrix(source=testset, startrow=0, length=ntest)
+        
+    return inputsize, targetsize, nclasses, trainset, validset, testset
+
+
+mnist_small = [ 784, 1, 10,
+                pl.FileVMatrix(filename=datadir+"mnist/mnist_small/mnist_basic2_train.pmat",
+                               inputsize = 784,
+                               targetsize = 1,
+                               weightsize = 0),
+                pl.FileVMatrix(filename=datadir+"mnist/mnist_small/mnist_basic2_valid.pmat",
+                               inputsize = 784,
+                               targetsize = 1,
+                               weightsize = 0),
+                pl.FileVMatrix(filename=datadir+"mnist/mnist_small/mnist_basic2_test.pmat",
+                               inputsize = 784,
+                               targetsize = 1,
+                               weightsize = 0)
+                ]
+
+mnist_bg = [ 784, 1, 10, 
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_train.amat",
+                            inputsize=784,
+                            targetsize = 1,
+                            weightsize = 0),
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_valid.amat",
+                              inputsize=784,
+                              targetsize = 1,
+                              weightsize = 0),
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_big_test.amat",
+                              inputsize=784,
+                              targetsize = 1,
+                              weightsize = 0)
+             ]
+
+
+mnist_rot = [ 784, 1, 10,
+              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_train.amat",
+                              inputsize=784,
+                              targetsize = 1,
+                              weightsize = 0),
+              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_valid.amat",
+                              inputsize=784,
+                              targetsize = 1,
+                              weightsize = 0),
+              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_test.amat",
+                              inputsize=784,
+                              targetsize = 1,
+                              weightsize = 0)
+              ]
+
+
+# mnist_full
+
+mnist_all_examples = pl.FileVMatrix(filename=datadir+"mnist/mnist_all.pmat",
+                                    inputsize = 784,
+                                    targetsize = 1,
+                                    weightsize = 0)
+
+mnist_full = [ 784, 1, 10,
+               pl.RowsSubVMatrix(source=mnist_all_examples, startrow=0, length=50000),
+               pl.RowsSubVMatrix(source=mnist_all_examples, startrow=50000, length=10000),
+               pl.RowsSubVMatrix(source=mnist_all_examples, startrow=60000, length=10000) ]
+
+
+# mnist_tiny
+
+mnist_tiny = [ 784, 1, 10,
+               pl.RowsSubVMatrix(source=mnist_all_examples, startrow=0, length=100),
+               pl.RowsSubVMatrix(source=mnist_all_examples, startrow=100, length=30),
+               pl.RowsSubVMatrix(source=mnist_all_examples, startrow=130, length=30) ]
+
+# babyAI shape
+
+babyAIshape = [ 1024, 1, 3,
+                pl.AutoVMatrix(filename=datadir+"babyAI/curriculum/shapeset1_1cspo_2_3.10000.train.shape.vmat",
+                               inputsize=1024,
+                               targetsize = 1,
+                               weightsize = 0),
+                pl.AutoVMatrix(filename=datadir+"babyAI/curriculum/shapeset1_1cspo_2_3.5000.valid.shape.vmat",
+                               inputsize=1024,
+                               targetsize = 1,
+                               weightsize = 0),
+                pl.AutoVMatrix(filename=datadir+"babyAI/curriculum/shapeset1_1cspo_2_3.5000.test.shape.vmat",
+                               inputsize=1024,
+                               targetsize = 1,
+                               weightsize = 0)
+                ]
+
+
+# babyAIshape normalized
+
+babyAIshape_norm_all_examples = pl.AutoVMatrix(filename=datadir+"babyAI/curriculum/shapeset1_1cspo_2_3.20000.all.shape.vmat",
+                                      inputsize=1024,
+                                      targetsize = 1,
+                                      weightsize = 0)
+
+babyAIshape_norm = [ 1024, 1, 3,
+                     pl.RowsSubVMatrix(source=babyAIshape_norm_all_examples, startrow=0, length=10000),
+                     pl.RowsSubVMatrix(source=babyAIshape_norm_all_examples, startrow=10000, length=5000),
+                     pl.RowsSubVMatrix(source=babyAIshape_norm_all_examples, startrow=15000, length=5000) ]
+
+
+all_data_sets = {
+    "mnist_small" : mnist_small,
+    "mnist_bg" : mnist_bg,
+    "mnist_rot" : mnist_rot,
+    "mnist_full" : mnist_full,
+    "mnist_tiny" : mnist_tiny,
+    "babyAIshape" : babyAIshape,
+    "babyAIshape_norm" : babyAIshape_norm,    
+    }
+



From islaja at mail.berlios.de  Wed Jun  3 22:21:23 2009
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Wed, 3 Jun 2009 22:21:23 +0200
Subject: [Plearn-commits] r10234 - trunk/python_modules/plearn/data
Message-ID: <200906032021.n53KLNYo024984@sheep.berlios.de>

Author: islaja
Date: 2009-06-03 22:21:23 +0200 (Wed, 03 Jun 2009)
New Revision: 10234

Added:
   trunk/python_modules/plearn/data/__init__.py
Log:
added forgotten __init__.py file

Added: trunk/python_modules/plearn/data/__init__.py
===================================================================



From islaja at mail.berlios.de  Thu Jun  4 01:05:49 2009
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Thu, 4 Jun 2009 01:05:49 +0200
Subject: [Plearn-commits] r10235 - trunk/scripts
Message-ID: <200906032305.n53N5nYp007425@sheep.berlios.de>

Author: islaja
Date: 2009-06-04 01:05:49 +0200 (Thu, 04 Jun 2009)
New Revision: 10235

Added:
   trunk/scripts/append_except_firstrow
Modified:
   trunk/scripts/plcollect
Log:
minor fixes and a new script

Added: trunk/scripts/append_except_firstrow
===================================================================
--- trunk/scripts/append_except_firstrow	2009-06-03 20:21:23 UTC (rev 10234)
+++ trunk/scripts/append_except_firstrow	2009-06-03 23:05:49 UTC (rev 10235)
@@ -0,0 +1,22 @@
+#! /bin/sh
+
+if [ "$2" = "" ]; then
+  echo "Usage:"
+  echo "     $0 sourcefile.txt destfile.txt "
+  echo "If destfile does not exist, will copy sourcefile to it as is."
+  echo "If destfile exists will append all but the first rows of sourcefile to it"
+  exit 127
+fi
+source=$1
+dest=$2
+
+lockfile -60 "$dest.lock"
+
+if [ -f "$dest" ];then
+    \tail -n+2 "$source" >> "$dest"
+else
+    \cp  "$source" "$dest"
+fi
+# Release the lock
+\rm -f "$dest.lock"
+


Property changes on: trunk/scripts/append_except_firstrow
___________________________________________________________________
Name: svn:executable
   + *

Modified: trunk/scripts/plcollect
===================================================================
--- trunk/scripts/plcollect	2009-06-03 20:21:23 UTC (rev 10234)
+++ trunk/scripts/plcollect	2009-06-03 23:05:49 UTC (rev 10235)
@@ -37,7 +37,7 @@
 import sys
 import fnmatch
 
-from apstat.data.table import *
+from plearn.table.table import *
 # from plearn.utilities.autoscript import autoscript
 
 def parse_filepath(filepath, separators=['__','/']):
@@ -100,11 +100,13 @@
     for dirpath, dirs, files in os.walk(rootdir):
         for filename in files:
             filepath = os.path.join(dirpath,filename)
+            # print "Considering",filepath
             match = False
             for pattern in searchedpatterns:
                 #print 'trying to match '+filepath+' WITH '+pattern
-                match = match or fnmatch.fnmatch(filepath, pattern)               
+                match = match or filepath==pattern or fnmatch.fnmatch(filepath, pattern)               
             if match:
+                # print "**** MATCHED "+filepath
                 filepaths.append(filepath)
                 #if filename in searchedpatterns:
                 #filepaths.append(os.path.join(dirpath,filename))



From islaja at mail.berlios.de  Thu Jun  4 02:37:48 2009
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Thu, 4 Jun 2009 02:37:48 +0200
Subject: [Plearn-commits] r10236 - trunk/python_modules/plearn/data
Message-ID: <200906040037.n540bme2030098@sheep.berlios.de>

Author: islaja
Date: 2009-06-04 02:37:48 +0200 (Thu, 04 Jun 2009)
New Revision: 10236

Modified:
   trunk/python_modules/plearn/data/datasets.py
Log:
Added mnist_bg_rand and image_net dataset. Changed mnist_bg for mnist_bg_img.


Modified: trunk/python_modules/plearn/data/datasets.py
===================================================================
--- trunk/python_modules/plearn/data/datasets.py	2009-06-03 23:05:49 UTC (rev 10235)
+++ trunk/python_modules/plearn/data/datasets.py	2009-06-04 00:37:48 UTC (rev 10236)
@@ -12,7 +12,7 @@
     return all_data_sets.keys()
 
 def getDatasets(datasetname, ntrain=None, nvalid=None, ntest=None):
-    """Ex: getDataset("mnist_bg")
+    """Ex: getDataset("mnist_bg_img")
     will return a tuple: inputsize, targetsize, nclasses, trainset, validset, testset
     where the sets are VMatrix specifications.
     If ntrain, nvalid, ntest are left unspecified (or None) then the full standard sets
@@ -45,7 +45,7 @@
                                weightsize = 0)
                 ]
 
-mnist_bg = [ 784, 1, 10, 
+mnist_bg_img = [ 784, 1, 10, 
              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_train.amat",
                             inputsize=784,
                             targetsize = 1,
@@ -60,7 +60,22 @@
                               weightsize = 0)
              ]
 
+mnist_bg_rand = [ 784, 1, 10,
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_train.amat",
+                            inputsize=784,
+                            targetsize = 1,
+                            weightsize = 0),
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_valid.amat",
+			      inputsize=784,
+                              targetsize = 1,
+                              weightsize = 0),
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_big_test.amat",
+		  	      inputsize=784,
+                              targetsize = 1,
+                              weightsize = 0)
+             ]
 
+
 mnist_rot = [ 784, 1, 10,
               pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_train.amat",
                               inputsize=784,
@@ -97,6 +112,25 @@
                pl.RowsSubVMatrix(source=mnist_all_examples, startrow=100, length=30),
                pl.RowsSubVMatrix(source=mnist_all_examples, startrow=130, length=30) ]
 
+
+# image_net
+
+image_net = [ 1369, 1, 10,
+	      pl.AutoVMatrix(filename=datadir+"image_net/plearn/10_percent_sample/10_top_classes_10_percent_37X37_train.vmat",
+                              inputsize=1369,
+                              targetsize = 1,
+                              weightsize = 0),
+              pl.AutoVMatrix(filename=datadir+"image_net/plearn/10_percent_sample/10_top_classes_10_percent_37X37_valid.vmat",
+                              inputsize=1369,
+                              targetsize = 1,
+                              weightsize = 0),
+              pl.AutoVMatrix(filename=datadir+"image_net/plearn/10_percent_sample/10_top_classes_10_percent_37X37_test.vmat",
+                              inputsize=1369,
+                              targetsize = 1,
+                              weightsize = 0)
+	]
+
+
 # babyAI shape
 
 babyAIshape = [ 1024, 1, 3,
@@ -130,10 +164,12 @@
 
 all_data_sets = {
     "mnist_small" : mnist_small,
-    "mnist_bg" : mnist_bg,
+    "mnist_bg_img" : mnist_bg_img,
+    "mnist_bg_rand" : mnist_bg_rand,	
     "mnist_rot" : mnist_rot,
     "mnist_full" : mnist_full,
     "mnist_tiny" : mnist_tiny,
+    "image_net" : image_net,
     "babyAIshape" : babyAIshape,
     "babyAIshape_norm" : babyAIshape_norm,    
     }



From islaja at mail.berlios.de  Thu Jun  4 03:15:47 2009
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Thu, 4 Jun 2009 03:15:47 +0200
Subject: [Plearn-commits] r10237 - trunk/python_modules/plearn/data
Message-ID: <200906040115.n541Fli5002924@sheep.berlios.de>

Author: islaja
Date: 2009-06-04 03:15:47 +0200 (Thu, 04 Jun 2009)
New Revision: 10237

Modified:
   trunk/python_modules/plearn/data/datasets.py
Log:
Use of pmat instead of amat.


Modified: trunk/python_modules/plearn/data/datasets.py
===================================================================
--- trunk/python_modules/plearn/data/datasets.py	2009-06-04 00:37:48 UTC (rev 10236)
+++ trunk/python_modules/plearn/data/datasets.py	2009-06-04 01:15:47 UTC (rev 10237)
@@ -46,30 +46,30 @@
                 ]
 
 mnist_bg_img = [ 784, 1, 10, 
-             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_train.amat",
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_train.pmat",
                             inputsize=784,
                             targetsize = 1,
                             weightsize = 0),
-             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_valid.amat",
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_valid.pmat",
                               inputsize=784,
                               targetsize = 1,
                               weightsize = 0),
-             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_big_test.amat",
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_images/plearn/mnist_all_background_images_big_test.pmat",
                               inputsize=784,
                               targetsize = 1,
                               weightsize = 0)
              ]
 
 mnist_bg_rand = [ 784, 1, 10,
-             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_train.amat",
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_train.pmat",
                             inputsize=784,
                             targetsize = 1,
                             weightsize = 0),
-             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_valid.amat",
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_valid.pmat",
 			      inputsize=784,
                               targetsize = 1,
                               weightsize = 0),
-             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_big_test.amat",
+             pl.AutoVMatrix(filename=datadir+"icml07data/mnist_background_random/plearn/mnist_all_background_random_big_test.pmat",
 		  	      inputsize=784,
                               targetsize = 1,
                               weightsize = 0)
@@ -77,15 +77,15 @@
 
 
 mnist_rot = [ 784, 1, 10,
-              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_train.amat",
+              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_train.pmat",
                               inputsize=784,
                               targetsize = 1,
                               weightsize = 0),
-              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_valid.amat",
+              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_valid.pmat",
                               inputsize=784,
                               targetsize = 1,
                               weightsize = 0),
-              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_test.amat",
+              pl.AutoVMatrix(filename=datadir+"icml07data/mnist_rotations/plearn/mnist_all_rotation_normalized_float_test.pmat",
                               inputsize=784,
                               targetsize = 1,
                               weightsize = 0)



From plearner at mail.berlios.de  Thu Jun  4 04:49:49 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 4 Jun 2009 04:49:49 +0200
Subject: [Plearn-commits] r10238 - trunk/scripts
Message-ID: <200906040249.n542nnbI013655@sheep.berlios.de>

Author: plearner
Date: 2009-06-04 04:49:48 +0200 (Thu, 04 Jun 2009)
New Revision: 10238

Modified:
   trunk/scripts/plcollect
Log:
More efficient version of plcollect when exact literal paths are given


Modified: trunk/scripts/plcollect
===================================================================
--- trunk/scripts/plcollect	2009-06-04 01:15:47 UTC (rev 10237)
+++ trunk/scripts/plcollect	2009-06-04 02:49:48 UTC (rev 10238)
@@ -88,29 +88,39 @@
 
 def plcollect(rootdir="expdir", resultfile_txt="exp_results.txt", searchedpatterns=[]):
     """
-    Looks for all split_stats.pmat files it can find in rootdir and its subdirectories,
-    and comples them into a single resultfile_txt
+    Looks for matrix result matrix files in rootdir and its subdirectories,
+    and compiles them into a single resultfile_txt
+    searchedpatterns is a list of searhcedpattenrs that can be either:
+     - full exact literal paths including the rootdir
+     - or wildcard pattenrs such as e.g. */Strat?results.pmat
+    Note that contrary to the shell, wildcard * does not stop at / boundaries.
     """
 
     if len(searchedpatterns) == 0: # use default
         searchedpatterns = ["*/Strat0results.pmat"]
 
-    filepaths = []
-    # first look for all matching files
-    for dirpath, dirs, files in os.walk(rootdir):
-        for filename in files:
-            filepath = os.path.join(dirpath,filename)
-            # print "Considering",filepath
-            match = False
-            for pattern in searchedpatterns:
-                #print 'trying to match '+filepath+' WITH '+pattern
-                match = match or filepath==pattern or fnmatch.fnmatch(filepath, pattern)               
-            if match:
-                # print "**** MATCHED "+filepath
-                filepaths.append(filepath)
-                #if filename in searchedpatterns:
-                #filepaths.append(os.path.join(dirpath,filename))
+    # first look for files with the exact literal path specified
+    filepaths = [ pattern for pattern in searchedpatterns if os.path.exists(pattern) ]
 
+    # remove those found from searchedpatterns
+    searchedpatterns = [ pattern for pattern in searchedpatterns if pattern not in filepaths ]
+
+    if len(searchedpatterns)>0:
+        # now look recursively for all files matching the remaining patterns
+        for dirpath, dirs, files in os.walk(rootdir):
+            for filename in files:
+                filepath = os.path.join(dirpath,filename)
+                # print "Considering",filepath
+                match = False
+                for pattern in searchedpatterns:
+                    # print 'trying to match '+filepath+' WITH '+pattern
+                    match = match or filepath==pattern or fnmatch.fnmatch(filepath, pattern)               
+                if match:
+                    # print "**** MATCHED "+filepath
+                    filepaths.append(filepath)
+                    #if filename in searchedpatterns:
+                    #filepaths.append(os.path.join(dirpath,filename))
+
     filepath_fields = []
     content_fields = []
     for filename in filepaths:
@@ -150,15 +160,25 @@
 if len(sys.argv)<3:
     print "Usage: plcollect expdir results.txt matrix_filepath_pattern1 matrix_filepath_pattern2 ..."
     print """
+
+    Looks for matrix result files in rootdir and its subdirectories,
+    and comples them into a single results.txt file
+    specified patterns can be either:
+     - full exact literal paths including the rootdir
+     - or wildcard pattenrs such as e.g. '*/Strat?results.pmat' 
+    Note that contrary to the shell, wildcard * does not stop at / boundaries.
+
     Looks in expdir and its subdirectories for all matrix files whose name matches
     one of the specified patterns and compiles their content into a single results.txt table
 
     Ex: plcollect exp results.txt '*/Strat?results.pmat'
-
+    Or: plcollect exp results.txt '*/split_stats.pmat'
+    
+    If results.txt existed prior to the call it will be overwritten.
     Note how the quotes are important for the shell not to perform the wildcard expansion of the specified pattern.
 
     You may then examine the produced results table with e.g.
-    pltable view results.txt
+    pltable view splitresults.txt
     """
     sys.exit()
 



From plearner at mail.berlios.de  Thu Jun  4 06:26:46 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 4 Jun 2009 06:26:46 +0200
Subject: [Plearn-commits] r10239 - in trunk: python_modules/plearn/table
	scripts scripts/EXPERIMENTAL
Message-ID: <200906040426.n544Qk4m026638@sheep.berlios.de>

Author: plearner
Date: 2009-06-04 06:26:43 +0200 (Thu, 04 Jun 2009)
New Revision: 10239

Added:
   trunk/scripts/EXPERIMENTAL/expcleanup.py
Modified:
   trunk/python_modules/plearn/table/table.py
   trunk/scripts/plcollect
Log:
Now checking file modification time in .idx files fr .txt matrices.
Also new expcleanup.py script and minor update to plcollect


Modified: trunk/python_modules/plearn/table/table.py
===================================================================
--- trunk/python_modules/plearn/table/table.py	2009-06-04 02:49:48 UTC (rev 10238)
+++ trunk/python_modules/plearn/table/table.py	2009-06-04 04:26:43 UTC (rev 10239)
@@ -39,7 +39,7 @@
 
 # Author: Pascal Vincent
 
-import os.path, string, struct, zlib, fpconst, pickle, csv, time, copy, subprocess
+import os, os.path, string, struct, zlib, fpconst, pickle, csv, time, copy, subprocess
 
 from numpy.numarray import array, argsort, random_array
 
@@ -914,8 +914,13 @@
             self.f = open(fname,'r')
             fieldnames = self.f.readline().strip('\r\n').split(self.separator)
             self.set_fieldnames(fieldnames)
+
+            if os.path.isfile(indexfname) and os.path.getmtime(self.fname)>os.path.getmtime(indexfname):
+                os.remove(indexfname)
+                
             if not os.path.isfile(indexfname):
                 build_row_index_file(fname,indexfname,self.struct_format)
+
             self.index = StructFile(indexfname,self.struct_format,'r')
         elif openmode=='r+':
             self.f = open(fname,'r+')

Added: trunk/scripts/EXPERIMENTAL/expcleanup.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/expcleanup.py	2009-06-04 02:49:48 UTC (rev 10238)
+++ trunk/scripts/EXPERIMENTAL/expcleanup.py	2009-06-04 04:26:43 UTC (rev 10239)
@@ -0,0 +1,107 @@
+#!/usr/bin/env python
+
+# expcleanup.py 
+# Copyright (C) 2009 Pascal Vincent
+
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+#  1. Redistributions of source code must retain the above copyright
+#     notice, this list of conditions and the following disclaimer.
+# 
+#  2. Redistributions in binary form must reproduce the above copyright
+#     notice, this list of conditions and the following disclaimer in the
+#     documentation and/or other materials provided with the distribution.
+# 
+#  3. The name of the authors may not be used to endorse or promote
+#     products derived from this software without specific prior written
+#     permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+# OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+# NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+# TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+# 
+# This file is part of the PLearn library. For more information on the PLearn
+# library, go to the PLearn Web site at www.plearn.org
+
+
+import sys
+import os
+import os.path
+import shutil
+
+
+def expcleanup(rootdir):
+
+    # search for finished experiments
+    for dirpath, dirs, files in os.walk(rootdir):
+
+        # Now try detecting signature of various experiment types to call appropriate cleanup operation
+
+        # 1) newautoencoders experiment
+        if 'tmpstats.txt' in files and 'global_stats.pmat' in files and 'Split0' in dirs \
+            and os.path.exists(os.path.join(dirpath,'Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir/training_costs_layer_1.pmat')):
+            cleanup_newautoencoders_exp(dirpath)
+
+    print "All done."
+
+def cleanup_newautoencoders_exp(dirpath):
+    print
+    print "************************************************"
+    print "*** Cleaning up "+dirpath+"***"     
+
+    # move useful stuff out of srcdir and into destdir
+
+    srcdir = os.path.join(dirpath,'Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir')
+    destdir = os.path.join(dirpath,'Split0/LearnerExpdir')
+
+    useful_files = [
+        "learner.psave",
+        "training_costs_layer_1.pmat",
+        "training_costs_layer_1.pmat.metadata",
+        "training_costs_layer_2.pmat",
+        "training_costs_layer_2.pmat.metadata",
+        "training_costs_layer_3.pmat",
+        "training_costs_layer_3.pmat.metadata"
+        ]
+
+    # move those files to destdir
+    for filename in useful_files:
+        srcfile = os.path.join(srcdir,filename)
+        dest = os.path.join(destdir,filename)
+        if os.path.exists(srcfile):
+            print "* moving",srcfile,"->",dest
+            shutil.move(srcfile, dest)
+    
+    # remove the Split0/LearnerExpdir/Strat0 subdirectory
+    dirtoremove = os.path.join(dirpath,'Split0/LearnerExpdir/Strat0')
+    print '* removing ',dirtoremove
+    shutil.rmtree(dirtoremove, True)
+
+    print
+
+
+
+# main script:
+
+if len(sys.argv)!=2:
+    print """
+Usage: expcleanup.py dirname
+
+Will attempt to free some disk space in all experiments contained in dirname
+removing unnecessary subdirectories (possibly after having relocated useful files)
+    """
+    sys.exit()
+
+expcleanup(sys.argv[1])
+
+
+
+


Property changes on: trunk/scripts/EXPERIMENTAL/expcleanup.py
___________________________________________________________________
Name: svn:executable
   + *

Modified: trunk/scripts/plcollect
===================================================================
--- trunk/scripts/plcollect	2009-06-04 02:49:48 UTC (rev 10238)
+++ trunk/scripts/plcollect	2009-06-04 04:26:43 UTC (rev 10239)
@@ -1,7 +1,7 @@
 #!/usr/bin/env python
 
 # plcollect
-# Copyright (C) 2008 Pascal Vincent
+# Copyright (C) 2008,2009 Pascal Vincent
 #
 #  Redistribution and use in source and binary forms, with or without
 #  modification, are permitted provided that the following conditions are met:



From plearner at mail.berlios.de  Thu Jun  4 07:46:10 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 4 Jun 2009 07:46:10 +0200
Subject: [Plearn-commits] r10240 - trunk/scripts/EXPERIMENTAL
Message-ID: <200906040546.n545kAGO029366@sheep.berlios.de>

Author: plearner
Date: 2009-06-04 07:46:05 +0200 (Thu, 04 Jun 2009)
New Revision: 10240

Modified:
   trunk/scripts/EXPERIMENTAL/expcleanup.py
Log:
further cleanup of no longer needed experiment output


Modified: trunk/scripts/EXPERIMENTAL/expcleanup.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/expcleanup.py	2009-06-04 04:26:43 UTC (rev 10239)
+++ trunk/scripts/EXPERIMENTAL/expcleanup.py	2009-06-04 05:46:05 UTC (rev 10240)
@@ -40,6 +40,10 @@
 
 def expcleanup(rootdir):
 
+    # remove all no loner needed outmat?.pmat
+    os.system("""sh -c 'PATH=/bin:/usr/bin; find %s -name training_costs_layer_2.pmat -execdir rm -f outmat1.pmat \;' """ % rootdir)
+    os.system("""sh -c 'PATH=/bin:/usr/bin; find %s -name training_costs_layer_3.pmat -execdir rm -f outmat2.pmat \;' """ % rootdir)
+
     # search for finished experiments
     for dirpath, dirs, files in os.walk(rootdir):
 



From nouiz at mail.berlios.de  Thu Jun  4 16:28:28 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Jun 2009 16:28:28 +0200
Subject: [Plearn-commits] r10241 - trunk/python_modules/plearn/parallel
Message-ID: <200906041428.n54ESSZ9032524@sheep.berlios.de>

Author: nouiz
Date: 2009-06-04 16:28:27 +0200 (Thu, 04 Jun 2009)
New Revision: 10241

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
for condor, if we ask for N cores, we will accept N or more(was accepting only N cores)


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-06-04 05:46:05 UTC (rev 10240)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-06-04 14:28:27 UTC (rev 10241)
@@ -1416,7 +1416,7 @@
         else :
             self.req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
         if self.cpu>0:
-            self.req+='&&(target.CPUS=='+str(self.cpu)+')'
+            self.req+='&&(target.CPUS>='+str(self.cpu)+')'
 
         if self.os:
             self.req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',



From plearner at mail.berlios.de  Thu Jun  4 18:36:46 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 4 Jun 2009 18:36:46 +0200
Subject: [Plearn-commits] r10242 - trunk/scripts/EXPERIMENTAL
Message-ID: <200906041636.n54GakEl018057@sheep.berlios.de>

Author: plearner
Date: 2009-06-04 18:36:43 +0200 (Thu, 04 Jun 2009)
New Revision: 10242

Modified:
   trunk/scripts/EXPERIMENTAL/expcleanup.py
Log:
Commented out problematic part of expcleanup


Modified: trunk/scripts/EXPERIMENTAL/expcleanup.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/expcleanup.py	2009-06-04 14:28:27 UTC (rev 10241)
+++ trunk/scripts/EXPERIMENTAL/expcleanup.py	2009-06-04 16:36:43 UTC (rev 10242)
@@ -41,8 +41,12 @@
 def expcleanup(rootdir):
 
     # remove all no loner needed outmat?.pmat
-    os.system("""sh -c 'PATH=/bin:/usr/bin; find %s -name training_costs_layer_2.pmat -execdir rm -f outmat1.pmat \;' """ % rootdir)
-    os.system("""sh -c 'PATH=/bin:/usr/bin; find %s -name training_costs_layer_3.pmat -execdir rm -f outmat2.pmat \;' """ % rootdir)
+    # This is commented out: it's problematic since for now, the outmats are not closed before all
+    # pretraining is finiched. It is thus problematic to delete them while they're still open by the prg
+    # and apparently causes a bug while running.
+    #
+    # os.system("""sh -c 'PATH=/bin:/usr/bin; find %s -name training_costs_layer_2.pmat -execdir rm -f outmat1.pmat \;' """ % rootdir)
+    # os.system("""sh -c 'PATH=/bin:/usr/bin; find %s -name training_costs_layer_3.pmat -execdir rm -f outmat2.pmat \;' """ % rootdir)
 
     # search for finished experiments
     for dirpath, dirs, files in os.walk(rootdir):



From laulysta at mail.berlios.de  Fri Jun  5 00:52:01 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 5 Jun 2009 00:52:01 +0200
Subject: [Plearn-commits] r10243 - trunk/plearn_learners_experimental
Message-ID: <200906042252.n54Mq1mR011227@sheep.berlios.de>

Author: laulysta
Date: 2009-06-05 00:51:55 +0200 (Fri, 05 Jun 2009)
New Revision: 10243

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-06-04 16:36:43 UTC (rev 10242)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-06-04 22:51:55 UTC (rev 10243)
@@ -758,7 +758,7 @@
                       //  inject_zero_forcing_noise(encoded_seq, input_noise_prob);
 
                 // recurrent no noise phase
-                if(stage>=nb_stage_reconstruction){
+                if(stage >= nb_stage_reconstruction){
                     if(recurrent_lr!=0)
                     {
                         
@@ -769,24 +769,27 @@
                     }
                 }
 
-                if(stage<nb_stage_reconstruction || nb_stage_reconstruction == 0 ){
+                if(stage < nb_stage_reconstruction || nb_stage_reconstruction == 0 ){
+                
 
                     // greedy phase hidden
                     if(hidden_reconstruction_lr!=0){
+                        
                         setLearningRate( hidden_reconstruction_lr);
+                        
                         recurrentFprop(train_costs, train_n_items, true);
                         //recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
                         recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
                     }
-
+                
                     /*if(recurrent_lr!=0)
-                    {                 
-                        setLearningRate( recurrent_lr );                    
-                        recurrentFprop(train_costs, train_n_items);
-                        //recurrentUpdate(0,0,1, prediction_cost_weight,0, train_costs, train_n_items );
-                        recurrentUpdate(0,0,0, prediction_cost_weight,0, train_costs, train_n_items );
-                        
-                        }*/
+                      {                 
+                      setLearningRate( recurrent_lr );                    
+                      recurrentFprop(train_costs, train_n_items);
+                      //recurrentUpdate(0,0,1, prediction_cost_weight,0, train_costs, train_n_items );
+                      recurrentUpdate(0,0,0, prediction_cost_weight,0, train_costs, train_n_items );
+                      
+                      }*/
                     
                     // greedy phase input
                     if(input_reconstruction_lr!=0){
@@ -839,6 +842,12 @@
             //double totalCosts = 0;
             for(int i=0; i<train_costs.length(); i++)
             {
+                
+                if (train_costs[i] <= 0 || train_n_items[i] <= 0 ){
+                    train_costs[i] = 1;
+                    train_n_items[i] = 1; 
+                }
+                
                 if (i < target_layers_weights.length()){
                     if( !fast_exact_is_equal(target_layers_weights[i],0) ){
                         train_costs[i] /= train_n_items[i];
@@ -882,11 +891,50 @@
         splitRawMaskedSupervisedSequence(seq, doNoise);
     else if(encoding=="generic")
         encode_artificialData(seq);
-    else
+    else if(encoding=="note_octav_duration")
         encodeAndCreateSupervisedSequence(seq);
+    else if(encoding=="diffNote_duration")
+        encodeAndCreateSupervisedSequence2(seq);
 }
 
 // encodes sequ, then populates: input_list, targets_list, masks_list
+void DenoisingRecurrentNet::encodeAndCreateSupervisedSequence2(Mat seq) const
+{
+     if(use_target_layers_masks)
+        PLERROR("Bug: use_target_layers_masks is expected to be false (no masks) when in encodeAndCreateSupervisedSequence");
+
+    encodeSequence(seq, encoded_seq);
+    // now work with encoded_seq
+    Vec tempoTar;
+    int l = encoded_seq.length();
+    resize_lists(l-input_window_size);
+
+
+    int ntargets = target_layers.length();
+    targets_list.resize(ntargets);
+   
+    for(int tar=0; tar<ntargets; tar++)
+    {
+        int targsize = target_layers[tar]->size;
+    
+        targets_list[tar].resize(l-input_window_size, targsize);   
+    }  
+    int startTar;
+    for(int t=input_window_size; t<l; t++)
+    {
+
+        input_list[t-input_window_size] = encoded_seq.subMatRows(t-input_window_size,input_window_size).toVec();
+        startTar = 43;
+        for(int tar=0; tar<ntargets; tar++)
+        {
+            int targsize = target_layers[tar]->size;
+            targets_list[tar](t-input_window_size) << encoded_seq(t).subVec(startTar,targsize);
+            startTar += targsize;
+        }
+    }
+}
+
+// encodes sequ, then populates: input_list, targets_list, masks_list
 void DenoisingRecurrentNet::encodeAndCreateSupervisedSequence(Mat seq) const
 {
     if(use_target_layers_masks)
@@ -1421,7 +1469,106 @@
     productAcc(hidden_gradient, reconstruction_weights, input_reconstruction_activation_grad);
 }
 
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden2(Vec theInput, 
+                                                                      Vec hidden, 
+                                                                      Mat reconstruction_weights, 
+                                                                      Mat& acc_weights_gr, 
+                                                                      Vec& reconstruction_bias, 
+                                                                      Vec& reconstruction_bias2, 
+                                                                      Vec hidden_reconstruction_activation_grad, 
+                                                                      Vec& reconstruction_prob, 
+                                                                      Vec hidden_target, 
+                                                                      Vec hidden_gradient, 
+                                                                      double hidden_reconstruction_cost_weight, 
+                                                                      double lr)
+{
+    // set appropriate sizes
+    int fullhiddenlength = hidden_target.length();
+    Vec reconstruction_activation;
+    Vec reconstruction_activation2;
+    Vec reconstruction_prob2;
+    Vec hidden_act_no_bias;
+    Vec hidden_exp;
+    Vec dynamic_act_no_bias_contribution;
+    if(reconstruction_bias.length()==0)
+    {
+        reconstruction_bias.resize(fullhiddenlength);
+        reconstruction_bias.clear();
+    }
+    if(reconstruction_bias2.length()==0)
+    {
+        reconstruction_bias2.resize(fullhiddenlength);
+        reconstruction_bias2.clear();
+    }
+    reconstruction_prob2.resize(fullhiddenlength);
+    reconstruction_activation.resize(fullhiddenlength);
+    reconstruction_activation2.resize(fullhiddenlength);
+    reconstruction_prob.resize(fullhiddenlength);
 
+   
+    hidden_act_no_bias.resize(fullhiddenlength);
+    hidden_exp.resize(fullhiddenlength);
+    dynamic_act_no_bias_contribution.resize(fullhiddenlength);
+
+    
+
+    // predict (denoised) input_reconstruction 
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
+    //product(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice not tied
+    reconstruction_activation += reconstruction_bias;
+
+    for( int j=0 ; j<fullhiddenlength ; j++ )
+        reconstruction_prob[j] = fastsigmoid( reconstruction_activation[j] );
+
+
+
+     // predict (denoised) input_reconstruction 
+    transposeProduct(reconstruction_activation2, reconstruction_weights, reconstruction_prob); //dynamic matrice tied
+    reconstruction_activation2 += reconstruction_bias2;
+
+    for( int j=0 ; j<fullhiddenlength ; j++ )
+        reconstruction_prob2[j] = fastsigmoid( reconstruction_activation2[j] );
+
+
+    //hidden_layer->fprop(reconstruction_activation, reconstruction_prob);
+
+    /********************************************************************************/
+    hidden_reconstruction_activation_grad.resize(reconstruction_prob.size());
+    hidden_reconstruction_activation_grad << reconstruction_prob;
+    hidden_reconstruction_activation_grad -= hidden_target;
+    hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
+    
+
+    productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad); //dynamic matrice tied
+    //transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad); //dynamic matrice not tied
+    
+    //update bias
+    multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
+    // update weight
+    externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
+    //externalProductScaleAcc(acc_weights_gr, hidden_reconstruction_activation_grad, hidden, -lr); //dynamic matrice not tied
+                
+    //update bias2
+    //multiplyAcc(reconstruction_bias2, hidden_gradient, -lr);
+    /********************************************************************************/
+    // Vec hidden_reconstruction_activation_grad;
+    /*hidden_reconstruction_activation_grad.clear();
+    for(int k=0; k<reconstruction_prob.length(); k++){
+        //    hidden_reconstruction_activation_grad[k] = safelog(1-reconstruction_prob[k]) - safelog(reconstruction_prob[k]);
+        hidden_reconstruction_activation_grad[k] = - reconstruction_activation[k];
+        }*/
+
+    double result_cost = 0;
+    double neg_log_cost = 0; // neg log softmax
+    for(int k=0; k<reconstruction_prob.length(); k++){
+        //if(hidden_target[k]!=0)
+        neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]) + (1-hidden_target[k])*safelog(1-reconstruction_prob[k]);
+    }
+    result_cost = neg_log_cost;
+    
+    return result_cost;
+}
+
 double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec theInput, 
                                                                       Vec hidden, 
                                                                       Mat reconstruction_weights, 
@@ -1935,17 +2082,52 @@
             }
         }
     }
+    
+    
     //update matrice's connections
     for( int tar=0; tar<target_layers.length(); tar++)
     {
         multiplyAcc(targetWeights[tar], acc_target_connections_gr[tar], 1);
     }
     multiplyAcc(inputWeights, acc_input_connections_gr, 1);
+    
     if(dynamic_connections )
     {
         multiplyAcc(dynamicWeights, acc_dynamic_connections_gr, 1);
         //multiplyAcc(reconsWeights, acc_reconstruction_dynamic_connections_gr, 1);
     }
+    
+    
+
+
+     /* int r;
+    int modulo;
+    if(input_reconstruction_weight!=0)
+        modulo = 2;
+    else
+        modulo=3;
+    
+    r = rand() % modulo +1;
+   
+   
+    if(r==1)
+    {
+        multiplyAcc(inputWeights, acc_input_connections_gr, 1);
+    }
+    else if (r==2){
+        if(dynamic_connections )
+        {
+            multiplyAcc(dynamicWeights, acc_dynamic_connections_gr, 1);
+            //multiplyAcc(reconsWeights, acc_reconstruction_dynamic_connections_gr, 1);
+        }
+    }
+    else {
+        //update matrice's connections
+        for( int tar=0; tar<target_layers.length(); tar++)
+        {
+            multiplyAcc(targetWeights[tar], acc_target_connections_gr[tar], 1);
+        }
+        }*/
 }
 
 
@@ -2035,6 +2217,8 @@
         encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, false, 0);
     else if(encoding=="note_octav_duration")
         encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, false, 4);    
+    else if(encoding=="diffNote_duration")
+        encode_onehot_diffNote_duration(sequence, encoded_seq, false);
     else if(encoding=="raw_masked_supervised")
         PLERROR("raw_masked_supervised means already encoded! You shouldnt have landed here!!!");
     else if(encoding=="generic")
@@ -2101,6 +2285,39 @@
   bit at position note_nbits+octav_nbits+duration is on
  */
 
+void DenoisingRecurrentNet::encode_onehot_diffNote_duration(Mat sequence, Mat& encoded_sequence,
+                                                              bool use_silence,  int duration_nbits)
+{
+    int l = sequence.length();
+    //diff paussible -21 ... -1 0 1 ... 21
+    // index          0     20 21 22    43
+    int note_nbits = 43; //de -21 a 21
+
+    encoded_sequence.resize(l,note_nbits+duration_nbits);
+    encoded_sequence.clear();
+    
+    
+    for(int i=0; i<l; i++)
+    {
+        //int midi_number = int(sequence(i,0));
+
+        if(i==0) // silence
+        {
+            encoded_sequence(i,21) = 1;
+        }
+        else{
+            int diffNote = int(sequence(i,0))-int(sequence(i-1,0))+21;
+            encoded_sequence(i,diffNote) = 1;
+        }
+
+       
+        int duration_bit = getDurationBit(int(sequence(i,1)));
+        if(duration_bit<0 || duration_bit>=duration_nbits)
+            PLERROR("duration_bit out of valid range");
+        encoded_sequence(i,note_nbits+duration_bit) = 1;
+    }
+}
+
 void DenoisingRecurrentNet::encode_onehot_note_octav_duration(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows,
                                                               bool use_silence, int octav_nbits, int duration_nbits)
 {

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-06-04 16:36:43 UTC (rev 10242)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-06-04 22:51:55 UTC (rev 10243)
@@ -186,6 +186,9 @@
     //! (declared const because it needs to be called in test)
     void encodeSequence(Mat sequence, Mat& encoded_seq) const;
 
+    static void encode_onehot_diffNote_duration(Mat sequence, Mat& encoded_sequence,
+                                                  bool use_silence, int duration_nbits=20);
+
     static void encode_onehot_note_octav_duration(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows,
                                                   bool use_silence, int octav_nbits, int duration_nbits=20);
     
@@ -438,6 +441,8 @@
     //! encodes seq, then populates: inputslist, targets_list, masks_list
     void encodeAndCreateSupervisedSequence(Mat seq) const;
 
+    void encodeAndCreateSupervisedSequence2(Mat seq) const;
+
     //! For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
     void splitRawMaskedSupervisedSequence(Mat seq, bool doNoise) const;
 
@@ -501,6 +506,9 @@
     void updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Mat& acc_weights_gr, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
+    double fpropHiddenReconstructionFromLastHidden2(Vec theInput, Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec& reconstruction_bias2, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+                                                                          Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
+
     double fpropHiddenReconstructionFromLastHidden(Vec theInput, Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec& reconstruction_bias2, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
                                                                           Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
     



From islaja at mail.berlios.de  Fri Jun  5 02:38:59 2009
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Fri, 5 Jun 2009 02:38:59 +0200
Subject: [Plearn-commits] r10244 - in trunk/scripts: . EXPERIMENTAL
Message-ID: <200906050038.n550cxKE014074@sheep.berlios.de>

Author: islaja
Date: 2009-06-05 02:38:58 +0200 (Fri, 05 Jun 2009)
New Revision: 10244

Modified:
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
   trunk/scripts/plcollect
Log:
minor fixes


Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-04 22:51:55 UTC (rev 10243)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-05 00:38:58 UTC (rev 10244)
@@ -145,7 +145,7 @@
 class InteractiveRepRecPlotter:
     '''this class is used to plot representations and reconstructions of a DeepReconstructorNet'''
     
-    def __init__(self, learner, vmat, image_width=28, char_indice=0):
+    def __init__(self, learner, vmat, image_width=None, char_indice=0):
         '''constructor'''
 
         self.k = 10
@@ -155,6 +155,12 @@
         self.learner = learner
         self.vmat = vmat
         self.char = -1#the char we're looking for, -1 for anyone (it can be -1,0,1,2,3,4,5,6,7,8,9)
+
+        if image_width is None:
+            # guess image dimensions            
+            image_width,image_height = guess_image_dimensions(vmat.width-1)
+            print "guessed dimensions: ",vmat.width, image_width, image_height
+
         self.image_width = image_width
 
         self.fig_rec = 0
@@ -521,7 +527,7 @@
                     
                     #HACK !!!
                     print 'just hacked...'
-                    if i==1 and len(row) == 28*28*2:
+                    if i==1 and len(row) == self.image_width*self.image_width*2:
                         matricesToPlot.append(doubleSizedWeightVectorToImageMatrix(row))
                     #END OF HACK
                     else:
@@ -534,7 +540,7 @@
                     
                         #HACK !!!
                         print 'just hacked...'
-                        if i==1 and len(row) == 28*28*2:
+                        if i==1 and len(row) == self.image_width*self.image_width*2:
                             matricesToPlot.append(doubleSizedWeightVectorToImageMatrix(row))
                         #END OF HACK 
                         else:
@@ -554,7 +560,7 @@
                     
                     #HACK !!!
                     #print 'just hacked...'
-                    #if i==1 and len(row) == 28*28*2:
+                    #if i==1 and len(row) == self.image_width*self.image_width*2:
                     #    row = array(toMinusRow(row))
                     #END OF HACK
                                       
@@ -638,7 +644,7 @@
                     figure(3)
                     myioff()                    
                     clf()
-                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.05, self.from1568to784functions[self.from1568to784function], [], names, self.same_scale)
+                    plotLayer1(learner.getParameterValue(nameW), self.image_width, .1, n, hl.groupsize,.05, self.from1568to784functions[self.from1568to784function], [], names, self.same_scale)
                     myion()
                     draw()
 
@@ -655,7 +661,7 @@
                     figure(3)
                     myioff()
                     clf()
-                    plotLayer1(M, 28, .056,0,M.shape[0],.05, self.from1568to784functions[self.from1568to784function], [], names, self.same_scale)
+                    plotLayer1(M, self.image_width, .056,0,M.shape[0],.05, self.from1568to784functions[self.from1568to784function], [], names, self.same_scale)
                     myion()
                     draw()
 
@@ -695,7 +701,7 @@
                     figure(3)
                     myioff()                    
                     clf()
-                    plotLayer1(learner.getParameterValue(nameW), 28, .056, 0,0,.05, self.from1568to784functions[self.from1568to784function], indexes,names, self.same_scale)
+                    plotLayer1(learner.getParameterValue(nameW), self.image_width, .056, 0,0,.05, self.from1568to784functions[self.from1568to784function], indexes,names, self.same_scale)
                     myion()
                     draw()
                     
@@ -714,14 +720,14 @@
                         M[y] =  m[y]*w[x%hl.groupsize]
                     print M
                     
-                    plotLayer1(M, 28, .056,0,M.shape[0],.05,self.from1568to784functions[self.from1568to784function],[],names, self.same_scale)
+                    plotLayer1(M, self.image_width, .056,0,M.shape[0],.05,self.from1568to784functions[self.from1568to784function],[],names, self.same_scale)
                     myion()
                     draw()
 
                     figure(4)
                     myioff()
                     clf()                    
-                    plotLayer1(M, 28, .056,0,M.shape[0],.05,self.from1568to784functions[self.from1568to784function],[],names, self.same_scale)
+                    plotLayer1(M, self.image_width, .056,0,M.shape[0],.05,self.from1568to784functions[self.from1568to784function],[],names, self.same_scale)
                     myion()
                     draw()
 
@@ -730,7 +736,7 @@
                     figure(5)
                     myioff()
                     clf()
-                    plotLayer1(learner.getParameterValue(nameWr), 28, .056,0,0,.05,self.from1568to784functions[self.from1568to784function],indexes,names, self.same_scale)
+                    plotLayer1(learner.getParameterValue(nameWr), self.image_width, .056,0,0,.05,self.from1568to784functions[self.from1568to784function],indexes,names, self.same_scale)
                     myion()
                     draw()
                     
@@ -905,7 +911,7 @@
         print 'executing some matrix manipulations...'
         row = raw_rep[0][0]
         
-        image = rowToMatrix(row,28)
+        image = rowToMatrix(row,self.image_width)
         
         listDeMatrices = [image]        
         for el in raw_rep[1:]:
@@ -914,7 +920,7 @@
         listDeMatrices2 = [image]
         for el in rec:
             row = el[0]
-            listDeMatrices2.append(rowToMatrix(row,28))
+            listDeMatrices2.append(rowToMatrix(row,self.image_width))
         
     
         print 'plotting'
@@ -946,7 +952,7 @@
 
             file.write("\n\n\n---------- REC ------------------------------------------------------\n")            
             for i, mat in enumerate(rec):
-                appendMatrixToFile(file,  rowToMatrix(mat[0],28), 'rec of hidden layer ' + str(i+1))
+                appendMatrixToFile(file,  rowToMatrix(mat[0],self.image_width), 'rec of hidden layer ' + str(i+1))
 
     ###
     ### utils
@@ -979,7 +985,7 @@
         #reconstruct
         row = self.learner.reconstructOneLayer(which_layer+1)[0]
         #HACK
-        if len(row) == 28*28*2:
+        if len(row) == self.image_width*self.image_width*2:
             print 'hacking...'
             row = array(toMinusRow(row))
             #print the new layer
@@ -1043,7 +1049,7 @@
 ### main ###
 ############
 
-server_command = "myplearn server"
+server_command = "plearn_exp server"
 serv = launch_plearn_server(command = server_command)
 
 #print "Press Enter to continue"
@@ -1054,6 +1060,14 @@
 
 task = sys.argv[1]
 
+def guess_image_dimensions(n):
+    imgheight = int(math.sqrt(n))+1
+    while n%imgheight != 0:
+        imgheight = imgheight-1
+    imgwidth = n/imgheight
+    return imgwidth,imgheight
+
+
 def openVMat(vmatspec):
     if vmatspec.endswith(".amat") or vmatspec.endswith(".pmat"):
         vmat = serv.new('AutoVMatrix(specification ="'+vmatspec+'");')
@@ -1085,19 +1099,15 @@
         
         if matrixName in names:
 
-            #matrix = rand(500,28*28*2)
+            #matrix = rand(500,imgwidth*imgwidth*2)
             matrix = learner.getParameterValue(matrixName)
             print "shape: ",matrix.shape
 
             # guess image dimensions
-            n = matrix.shape[1]
-            imgheight = int(math.sqrt(n))+1
-            while n%imgheight != 0:
-                imgheight = imgheight-1
-            imgwidth = n/imgheight
+            imgwidth,imgheight = guess_image_dimensions(matrix.shape[1])
             
             showRowsAsImages(matrix, img_height=imgheight, img_width=imgwidth, nrows=10, ncols=20, figtitle=matrixName)
-            #plotter = EachRowPlotter(matrix, 28, .1, .01, doToRow)
+            #plotter = EachRowPlotter(matrix, imgwidth, .1, .01, doToRow)
             #plotter.plot()
             #show()          
             

Modified: trunk/scripts/plcollect
===================================================================
--- trunk/scripts/plcollect	2009-06-04 22:51:55 UTC (rev 10243)
+++ trunk/scripts/plcollect	2009-06-05 00:38:58 UTC (rev 10244)
@@ -124,15 +124,21 @@
     filepath_fields = []
     content_fields = []
     for filename in filepaths:
-        pairs, summarypath = parse_filepath(filename)
-        t = openTable(filename)
-        for key,val in pairs:
-            if key not in filepath_fields:
-                filepath_fields.append(key)
-        for fieldname in t.fieldnames:
-            if fieldname not in content_fields:
-                content_fields.append(fieldname)
-
+        try:
+            pairs, summarypath = parse_filepath(filename)
+            try:
+                t = openTable(filename)
+            except ValueError:
+                print >> sys.stderr, "Warning plcollect could not open",filename," (incomplete or corrupted header?)"
+                raise
+            for key,val in pairs:
+                if key not in filepath_fields:
+                    filepath_fields.append(key)
+            for fieldname in t.fieldnames:
+                if fieldname not in content_fields:
+                    content_fields.append(fieldname)
+        except ValueError:
+            pass
     fieldnames = filepath_fields+content_fields
     result_table = TableFile(resultfile_txt, 'w+', ["_filepath_","_summarypath_"]+fieldnames)
     print "fieldnames length:",len(fieldnames)
@@ -153,7 +159,7 @@
                     else:
                         resrow.append("-")
                 result_table.append(resrow)
-        except struct.error:
+        except (struct.error, ValueError):
             print "Probably a buggy .pmat"
 
 



From plearner at mail.berlios.de  Fri Jun  5 02:57:39 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 5 Jun 2009 02:57:39 +0200
Subject: [Plearn-commits] r10245 - trunk/scripts/EXPERIMENTAL
Message-ID: <200906050057.n550vdOc016422@sheep.berlios.de>

Author: plearner
Date: 2009-06-05 02:57:37 +0200 (Fri, 05 Jun 2009)
New Revision: 10245

Modified:
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
minor improvements


Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-05 00:38:58 UTC (rev 10244)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-05 00:57:37 UTC (rev 10245)
@@ -42,6 +42,7 @@
 def print_usage_repAndRec():
     print 'putting you mouse OVER a hidden layer and hitting these keyes does... things :'
     print
+    print '! : toggle between sampling types (multinomial or bernoulli)'
     print 'f : fproping from the last layer'
     print 'F : fproping form the last layer and for the next ones'
     print 'r : reconstructing this layer from the next one'
@@ -52,7 +53,7 @@
     print 'P : generate from a factorial bernoulli with parameter p'
     print 'G : generate from current layer: sampling from fact bernoulli and reconstructing'
     print 'm : set the max of each group of the current hidden layer to 1 and the other elements of the group to 0'
-    print 's : each group of the current layer is sampled (each group has to sum to 1.0)'
+    print 's : sample current layer (each group has to sum to 1.0)'
     print 'S : samples the current layer, then reconstruct the previous layer, thant sample this reconstructed layer, and continues until the input'
     print 'z : set the current pixel (the one the mouse is over) to 0.0'
     print 'x : set the current pixel to 0.25'
@@ -150,6 +151,7 @@
 
         self.k = 10
         self.p = 0.01
+        self.sampling_type = "bernoulli"
         
         self.current = char_indice-1#-1 because it's the first time
         self.learner = learner
@@ -309,8 +311,17 @@
 
             hl = hl1
 
+            # toggle between sampling types -- !
+            if char == '!':
+                if self.sampling_type=="multinomial":
+                    self.sampling_type = "bernoulli"
+                elif self.sampling_type=="bernoulli":
+                    self.sampling_type = "multinomial"
+                else:
+                    raise ValueError("invalid samplong type")
+
             # set k -- k
-            if char == 'k':
+            elif char == 'k':
                 print "Enter the new value for k (currently "+str(self.k)+") : ",
                 self.k = input()
                         
@@ -349,7 +360,7 @@
                 while k>=0:
                     self.__reconstructLayer(k)
                     if k!=0:
-                        self.__sampleLayer(k, "bernoulli")
+                        self.__sampleLayer(k)
                     self.rep_axes[k].imshow(self.hidden_layers[k].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                     k = k-1
                 draw()                
@@ -958,14 +969,16 @@
     ### utils
     ###
 
-    def __sampleLayer(self, which_layer, sampling_type="multinomial"):
+    def __sampleLayer(self, which_layer):
         hl = self.hidden_layers[which_layer]
 
-        if sampling_type=="multinomial":
+        if self.sampling_type=="multinomial":
             for n in arange(hl.hidden_layer.size/hl.groupsize):
-                multi = numpy.random.multinomial(1,hl.getRow(n))                    
+                probs = hl.getRow(n)
+                probs = probs/sum(probs)
+                multi = numpy.random.multinomial(1,probs)                    
                 hl.setRow(n,multi)
-        elif sampling_type=="bernoulli":
+        elif self.sampling_type=="bernoulli":
             for pos in xrange(hl.hidden_layer.size):
                 val = hl.hidden_layer[pos]
                 if val<0.:
@@ -976,7 +989,7 @@
                     val = numpy.random.binomial(1,val)
                 hl.hidden_layer[pos] = val
         else:
-            raise ValueError("Invalid sampling_type: "+sampling_type)
+            raise ValueError("Invalid sampling_type: "+self.sampling_type)
             
     def __reconstructLayer(self, which_layer):
         hl = self.hidden_layers[which_layer+1]



From tihocan at mail.berlios.de  Fri Jun  5 16:12:27 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 5 Jun 2009 16:12:27 +0200
Subject: [Plearn-commits] r10246 - trunk/plearn_learners/online
Message-ID: <200906051412.n55ECRBV004595@sheep.berlios.de>

Author: tihocan
Date: 2009-06-05 16:12:26 +0200 (Fri, 05 Jun 2009)
New Revision: 10246

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Declare a couple more remote methods

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2009-06-05 00:57:37 UTC (rev 10245)
+++ trunk/plearn_learners/online/RBMModule.cc	2009-06-05 14:12:26 UTC (rev 10246)
@@ -338,6 +338,17 @@
                    ArgDoc ("v_k", "Negative phase statistics on visible layer"),
                    ArgDoc ("h_k", "Negative phase statistics on hidden layer")
                   ));
+
+    declareMethod(rmm, "computePartitionFunction",
+        &RBMModule::computePartitionFunction,
+        (BodyDoc("Compute the log partition function (will be stored within "
+                 "the 'log_partition_function' field)")));
+
+    declareMethod(rmm, "computeLogLikelihoodOfVisible",
+        &RBMModule::computeLogLikelihoodOfVisible,
+        (BodyDoc("Compute log-likehood"),
+         ArgDoc("visible", "Matrix of visible inputs"),
+         RetDoc("A vector with the log-likelihood of each input")));
 }
 
 void RBMModule::CDUpdate(const Mat& v_0, const Mat& h_0,
@@ -630,6 +641,20 @@
 }
 
 ///////////////////////////////////
+// computeLogLikelihoodOfVisible //
+///////////////////////////////////
+Vec RBMModule::computeLogLikelihoodOfVisible(const Mat& visible)
+{
+    Mat energy;
+    computePartitionFunction();
+    computeFreeEnergyOfVisible(visible, energy, false);
+    negateElements(energy);
+    for (int i = 0; i < energy.length(); i++)
+        energy(i, 0) -= log_partition_function;
+    return energy.toVec();
+}
+
+///////////////////////////////////
 // computeAllHiddenProbabilities //
 ///////////////////////////////////
 void RBMModule::computeAllHiddenProbabilities(const Mat& visible,

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2009-06-05 00:57:37 UTC (rev 10245)
+++ trunk/plearn_learners/online/RBMModule.h	2009-06-05 14:12:26 UTC (rev 10246)
@@ -355,6 +355,11 @@
 
     void computePartitionFunction();
 
+    //! See remote documentation.
+    //! Note that this is really a convenience method only, to avoid having to
+    //! obtain this likelihood through the ports.
+    Vec computeLogLikelihoodOfVisible(const Mat& visible);
+
     //! Compute probabilities of all hidden configurations given some visible
     //! inputs. The 'p_hidden' matrix is filled such that the (i,j)-th element
     //! is P(hidden_configuration_i | visible_j).



From nouiz at mail.berlios.de  Tue Jun  9 19:57:16 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Jun 2009 19:57:16 +0200
Subject: [Plearn-commits] r10247 - trunk/plearn/vmat
Message-ID: <200906091757.n59HvGVr004454@sheep.berlios.de>

Author: nouiz
Date: 2009-06-09 19:57:15 +0200 (Tue, 09 Jun 2009)
New Revision: 10247

Modified:
   trunk/plearn/vmat/VMat_computeStats.cc
Log:
added a PLCHECK so that we don't segfault.


Modified: trunk/plearn/vmat/VMat_computeStats.cc
===================================================================
--- trunk/plearn/vmat/VMat_computeStats.cc	2009-06-05 14:12:26 UTC (rev 10246)
+++ trunk/plearn/vmat/VMat_computeStats.cc	2009-06-09 17:57:15 UTC (rev 10247)
@@ -53,6 +53,7 @@
 {
     int w = m.width();
     int l = m.length();
+    PLCHECK(w>=0);
     TVec<StatsCollector> stats(w, StatsCollector(maxnvalues));
     Vec v(w);
     PP<ProgressBar> pbar;



From nouiz at mail.berlios.de  Tue Jun  9 20:16:11 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Jun 2009 20:16:11 +0200
Subject: [Plearn-commits] r10248 - trunk/plearn_learners/unsupervised
Message-ID: <200906091816.n59IGBIe006585@sheep.berlios.de>

Author: nouiz
Date: 2009-06-09 20:16:11 +0200 (Tue, 09 Jun 2009)
New Revision: 10248

Modified:
   trunk/plearn_learners/unsupervised/UniformizeLearner.cc
Log:
let UniformizeLearner work with missing value.


Modified: trunk/plearn_learners/unsupervised/UniformizeLearner.cc
===================================================================
--- trunk/plearn_learners/unsupervised/UniformizeLearner.cc	2009-06-09 17:57:15 UTC (rev 10247)
+++ trunk/plearn_learners/unsupervised/UniformizeLearner.cc	2009-06-09 18:16:11 UTC (rev 10248)
@@ -59,7 +59,8 @@
 PLEARN_IMPLEMENT_OBJECT(UniformizeLearner, "Uniformizes selected input fields", 
                         "For each specified field, the full training set column will be read,\n"
                         "then sorted, and we'll store up to nquantiles and their mapping to [0,1] rank (as well as min and max)\n"
-                        "Uniformization maps to [0,1]. It is a piecewise linear interpolation between the remembered quantiles\n");
+                        "Uniformization maps to [0,1]. It is a piecewise linear interpolation between the remembered quantiles\n"
+                        "Work with missing value. We don't map them");
 
 void UniformizeLearner::declareOptions(OptionList& ol)
 {
@@ -397,8 +398,13 @@
     int n= outputsize();
     output.resize(n);
     int nk= which_fieldnums.size();
-    for(int k= 0; k < nk; ++k)
-        output[k]= mapToRank(input[which_fieldnums[k]], val_to_rank[k]);
+    for(int k= 0; k < nk; ++k){
+        real val=input[which_fieldnums[k]];
+        if(is_missing(val))
+            output[k] = MISSING_VALUE;
+        else
+            output[k] = mapToRank(val, val_to_rank[k]);
+    }
     for(int k= nk; k < n; ++k)
         output[k]= input[k-nk];
 



From plearner at mail.berlios.de  Mon Jun 15 18:05:39 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Mon, 15 Jun 2009 18:05:39 +0200
Subject: [Plearn-commits] r10249 - trunk/python_modules/plearn/table
Message-ID: <200906151605.n5FG5dAJ030399@sheep.berlios.de>

Author: plearner
Date: 2009-06-15 18:05:39 +0200 (Mon, 15 Jun 2009)
New Revision: 10249

Modified:
   trunk/python_modules/plearn/table/table.py
Log:
Merged in Rejean's changes 


Modified: trunk/python_modules/plearn/table/table.py
===================================================================
--- trunk/python_modules/plearn/table/table.py	2009-06-09 18:16:11 UTC (rev 10248)
+++ trunk/python_modules/plearn/table/table.py	2009-06-15 16:05:39 UTC (rev 10249)
@@ -1717,16 +1717,42 @@
             return int(self.max_value+1)
         
 
-class StringMapFile:
+class StringMap:
+    """Mapping from string to numeric values of a field."""
 
-    def __init__(self, filepath, openmode='a', startid=1):
-        if openmode not in "rwa":
-            raise ValueError("openmode must be one of 'r','w','a'")
+    def __init__(self, startid=1):
         self.startid = startid
         self.maxid = None
+        self.map = {}
+
+    def __getitem__(self,strval):
+        return self.map[str(strval)]
+
+    def __len__(self):
+        return len(self.map)
+
+    def append(self, strval, numid=None):
+        strval = str(strval) # make sure it's a string
+        if numid is None:
+            if self.maxid is None:
+                self.maxid = self.startid
+            else:
+                self.maxid += 1
+            numid = self.maxid
+        self.map[strval] = numid
+        return numid
+            
+class StringMapFile(StringMap):
+    """A StringMap saved on disk."""
+
+    def __init__(self, filepath, openmode='a', startid=1):
+        StringMap.__init__(self, startid)
         self.filepath = filepath
         self.openmode = openmode
-        self.map = {}
+
+        if openmode not in "rwa":
+            raise ValueError("openmode must be one of 'r','w','a'")
+
         if openmode=='r':
             self.load_map()
         elif openmode=='w':
@@ -1749,23 +1775,10 @@
                 self.maxid = max(numid, self.maxid)
         f.close()
 
-    def __getitem__(self,strval):
-        return self.map[str(strval)]
-
-    def __len__(self):
-        return len(self.map)
-
     def append(self, strval, numid=None):
-        strval = str(strval) # make sure it's a string
-        if numid is None:
-            if self.maxid is None:
-                self.maxid = self.startid
-            else:
-                self.maxid += 1
-            numid = self.maxid
+        numid = StringMap.append(self, strval, numid)
         # self.f.seek(0,2)
         print >> self.f, '"'+strval.replace('"','\\"')+'"', numid
-        self.map[strval] = numid
         self.flush()
         return numid
             



From saintmlx at mail.berlios.de  Fri Jun 19 22:53:35 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 19 Jun 2009 22:53:35 +0200
Subject: [Plearn-commits] r10250 - trunk/python_modules/plearn/table
Message-ID: <200906192053.n5JKrZhF011099@sheep.berlios.de>

Author: saintmlx
Date: 2009-06-19 22:53:34 +0200 (Fri, 19 Jun 2009)
New Revision: 10250

Modified:
   trunk/python_modules/plearn/table/table.py
Log:
optimizations for speed (should'n affect readability too much)

Modified: trunk/python_modules/plearn/table/table.py
===================================================================
--- trunk/python_modules/plearn/table/table.py	2009-06-15 16:05:39 UTC (rev 10249)
+++ trunk/python_modules/plearn/table/table.py	2009-06-19 20:53:34 UTC (rev 10250)
@@ -343,9 +343,11 @@
         return len(self.list)
 
     def __getitem__(self,key):
-        if isinstance(key,str):
-            key = self.fieldpos[key]
-        return self.list[key]
+        # try 'fast' case first, ask forgiveness if it doesn't work
+        try:
+            return self.list[key]
+        except TypeError:
+            return self.list[self.fieldpos[key]]
 
     def __setitem__(self, key, value):
         if isinstance(key,str):
@@ -394,7 +396,9 @@
 
     def make_private_copy_of_fields(self):
         if self.fieldnames:
-            self.set_fieldnames(self.fieldnames[:])
+            # copy names and pos as is instead of recalculating
+            self.fieldnames = self.fieldnames[:]
+            self.fieldpos = copy.copy(self.fieldpos)
         self.shared_fieldnames = False
 
     def append(self, val, name='?'):
@@ -1228,7 +1232,8 @@
 
     def getRow(self,i):
         row = self.table[i]
-        return [ row[field] for field in self.fieldnums ]
+        # map is faster than list comprehension (but uglier)
+        return map(row.__getitem__, self.fieldnums)
 
     def __len__(self):
         return len(self.table)



From nouiz at mail.berlios.de  Mon Jun 22 16:22:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 22 Jun 2009 16:22:12 +0200
Subject: [Plearn-commits] r10251 - trunk/plearn/vmat
Message-ID: <200906221422.n5MEMCwH030438@sheep.berlios.de>

Author: nouiz
Date: 2009-06-22 16:22:12 +0200 (Mon, 22 Jun 2009)
New Revision: 10251

Modified:
   trunk/plearn/vmat/CompactFileVMatrix.cc
   trunk/plearn/vmat/CompactFileVMatrix.h
Log:
register CompactFileVMatrix to have its extension cmat recognised as by getDataSet. This allow it to work for plearn vmat view and many other command.




Modified: trunk/plearn/vmat/CompactFileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.cc	2009-06-19 20:53:34 UTC (rev 10250)
+++ trunk/plearn/vmat/CompactFileVMatrix.cc	2009-06-22 14:22:12 UTC (rev 10251)
@@ -554,6 +554,12 @@
     PLERROR("In CompactFileVMatrix::put - not implemented.");
 }
 
+VMatrixExtensionRegistrar* CompactFileVMatrix::extension_registrar =
+    new VMatrixExtensionRegistrar(
+        "cmat",
+        &CompactFileVMatrix::instantiateFromPPath,
+        "A compact file matrix");
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/vmat/CompactFileVMatrix.h
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.h	2009-06-19 20:53:34 UTC (rev 10250)
+++ trunk/plearn/vmat/CompactFileVMatrix.h	2009-06-22 14:22:12 UTC (rev 10251)
@@ -39,6 +39,8 @@
 #define CompactFileVMatrix_INC
 
 #include "RowBufferedVMatrix.h"
+#include <plearn/db/getDataSet.h>
+#include <plearn/vmat/VMat.h>
 
 // While under development, we use this define to control
 // whether to use the NSPR 64 bit file access or the old std C FILE*
@@ -90,6 +92,7 @@
 
 private:
     typedef RowBufferedVMatrix inherited;
+    static VMatrixExtensionRegistrar* extension_registrar;
 
 protected:
     PPath filename_;
@@ -151,9 +154,13 @@
 
     virtual void build();
 
+    static VMat instantiateFromPPath(const PPath& filename)
+    {
+        return VMat(new CompactFileVMatrix(filename));
+    }
+
     PLEARN_DECLARE_OBJECT(CompactFileVMatrix);
 
-
     //! Transform a shallow copy into a deep copy.
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 



From plearner at mail.berlios.de  Wed Jun 24 18:41:03 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 24 Jun 2009 18:41:03 +0200
Subject: [Plearn-commits] r10252 - in trunk: commands/EXPERIMENTAL
	plearn/var/EXPERIMENTAL
Message-ID: <200906241641.n5OGf3je014297@sheep.berlios.de>

Author: plearner
Date: 2009-06-24 18:41:02 +0200 (Wed, 24 Jun 2009)
New Revision: 10252

Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/plearn/var/EXPERIMENTAL/MultiMaxVariable.cc
Log:
minor fix


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2009-06-22 14:22:12 UTC (rev 10251)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2009-06-24 16:41:02 UTC (rev 10252)
@@ -343,7 +343,7 @@
 // #include <plearn_learners/distributions/GaussianDistribution.h>
 // #include <plearn_learners/distributions/GaussMix.h>
 // #include <plearn_learners/distributions/RandomGaussMix.h>
-// #include <plearn_learners/distributions/ParzenWindow.h>
+#include <plearn_learners/distributions/ParzenWindow.h>
 // #include <plearn_learners/distributions/ManifoldParzen2.h>
 
 // Experimental

Modified: trunk/plearn/var/EXPERIMENTAL/MultiMaxVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/MultiMaxVariable.cc	2009-06-22 14:22:12 UTC (rev 10251)
+++ trunk/plearn/var/EXPERIMENTAL/MultiMaxVariable.cc	2009-06-24 16:41:02 UTC (rev 10252)
@@ -258,24 +258,27 @@
     // ### You should assume that the parent class' build_() has already been
     // ### called.
     
-    if (groupsizes.length() <= 0)
+    if (input.isNotNull() ) // otherwise postpone building until we have an input!
     {
-        if (groupsize <= 0)
-            PLERROR("Groupsize(s) not specified or invalid in MultiMaxVariable");    
-        if (input->width() % groupsize != 0)
-            PLERROR("Invalid groupsize in MultiMaxVariable");
+        if (groupsizes.length() <= 0)
+        {
+            if (groupsize <= 0)
+                PLERROR("Groupsize(s) not specified or invalid in MultiMaxVariable");    
+            if (input->width() % groupsize != 0)
+                PLERROR("Invalid groupsize in MultiMaxVariable");
 
-        TVec<int> vec(input->width()/groupsize, groupsize);
-        groupsizes = vec;
+            TVec<int> vec(input->width()/groupsize, groupsize);
+            groupsizes = vec;
+        }
+        else
+        {
+            int sum = 0;
+            for(int i=0; i<groupsizes.length(); i++)
+                sum += groupsizes[i];       
+            if(sum != input->width())
+                PLERROR("Invalid groupsizes in MultiMaxVariable");    
+        }
     }
-    else
-    {
-        int sum = 0;
-        for(int i=0; i<groupsizes.length(); i++)
-            sum += groupsizes[i];       
-        if(sum != input->width())
-            PLERROR("Invalid groupsizes in MultiMaxVariable");    
-    }
 }
 
 



From plearner at mail.berlios.de  Wed Jun 24 19:37:44 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 24 Jun 2009 19:37:44 +0200
Subject: [Plearn-commits] r10253 - in trunk: python_modules/plearn/table
	scripts/EXPERIMENTAL
Message-ID: <200906241737.n5OHbi17022615@sheep.berlios.de>

Author: plearner
Date: 2009-06-24 19:37:44 +0200 (Wed, 24 Jun 2009)
New Revision: 10253

Modified:
   trunk/python_modules/plearn/table/viewtable.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
Cosmetic changes


Modified: trunk/python_modules/plearn/table/viewtable.py
===================================================================
--- trunk/python_modules/plearn/table/viewtable.py	2009-06-24 16:41:02 UTC (rev 10252)
+++ trunk/python_modules/plearn/table/viewtable.py	2009-06-24 17:37:44 UTC (rev 10253)
@@ -903,6 +903,8 @@
 
         # replace _DIRPATH_ in command by filepathdir
         command = command.replace('_DIRPATH_',filepathdir)
+
+        # k = self.display_fullscreen(command)
         
         # os.system(command)
         subprocess.Popen(command, shell=True)
@@ -911,17 +913,17 @@
         
         commands = [
             ('s',"Launch terminal and shell in this matrix's directory",
-             """xterm -e sh -c "cd '_DIRPATH_'; pwd; ls; sh" """),
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; ls; sh' """),
             ('1',"View layer 1 unsup training costs",
-             """xterm -e sh -c "cd '_DIRPATH_'; myplearn vmat view training_costs_layer_1.pmat" """),
+             """xterm -e sh -c 'cd "_DIRPATH_"; myplearn vmat view `find . -name training_costs_layer_1.pmat`' """),
             ('i',"deepnetplot.py plotRepAndRec learner.psave", 
-             """xterm -e sh -c "cd '_DIRPATH_'; pwd; deepnetplot.py plotRepAndRec learner.psave ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh" """),
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; learnerfile=`find . -name learner.psave`; echo "found $learnerfile"; deepnetplot.py plotRepAndRec $learnerfile ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
             ('w',"deepnetplot.py plotEachRow learner.psave", 
-             """xterm -e sh -c "cd '_DIRPATH_'; pwd; deepnetplot.py plotEachRow learner.psave ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh" """),
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; learnerfile=`find . -name learner.psave`; echo "found $learnerfile"; deepnetplot.py plotEachRow $learnerfile ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
             ('I',"deepnetplot.py plotRepAndRec final_learner.psave", 
-             """xterm -e sh -c "cd '_DIRPATH_'; pwd; deepnetplot.py plotRepAndRec final_learner.psave ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh" """),
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; learnerfile=`find . -name final_learner.psave`; echo "found $learnerfile"; deepnetplot.py plotRepAndRec $learnerfile ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
             ('W',"deepnetplot.py plotEachRow final_learner.psave", 
-             """xterm -e sh -c "cd '_DIRPATH_'; pwd; deepnetplot.py plotEachRow final_learner.psave ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh" """),
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; learnerfile=`find . -name final_learner.psave`; echo "found $learnerfile"; deepnetplot.py plotEachRow $learnerfile ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
             ]
 
         menutxt = '\n'.join([ '['+commands[i][0]+'] '+commands[i][1] for i in range(len(commands)) ])+'\n'

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-24 16:41:02 UTC (rev 10252)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-24 17:37:44 UTC (rev 10253)
@@ -1062,7 +1062,7 @@
 ### main ###
 ############
 
-server_command = "plearn_exp server"
+server_command = "myplearn server"
 serv = launch_plearn_server(command = server_command)
 
 #print "Press Enter to continue"



From plearner at mail.berlios.de  Thu Jun 25 01:38:44 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 25 Jun 2009 01:38:44 +0200
Subject: [Plearn-commits] r10254 - in trunk: python_modules/plearn/io
	python_modules/plearn/plotting python_modules/plearn/table
	scripts/EXPERIMENTAL
Message-ID: <200906242338.n5ONcima031368@sheep.berlios.de>

Author: plearner
Date: 2009-06-25 01:38:43 +0200 (Thu, 25 Jun 2009)
New Revision: 10254

Modified:
   trunk/python_modules/plearn/io/server.py
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/table/viewtable.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
Further improvements for viewing filters


Modified: trunk/python_modules/plearn/io/server.py
===================================================================
--- trunk/python_modules/plearn/io/server.py	2009-06-24 17:37:44 UTC (rev 10253)
+++ trunk/python_modules/plearn/io/server.py	2009-06-24 23:38:43 UTC (rev 10254)
@@ -238,7 +238,7 @@
 
     def callLoadObject(self, objid, filepath):
         self.clearMaps()
-        self.logged_write('!L '+str(objid)+' '+filepath+'\n')
+        self.logged_write('!L '+str(objid)+' "'+filepath+'"\n')
         self.expectResults(0)
 
     def callDeleteObject(self, objid):

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2009-06-24 17:37:44 UTC (rev 10253)
+++ trunk/python_modules/plearn/plotting/netplot.py	2009-06-24 23:38:43 UTC (rev 10254)
@@ -469,6 +469,38 @@
             
     return truncMat
 
+
+def saveRowsAsImage(imgfile,
+                    matrix, img_width, img_height,
+                    nrows=10, ncols=20,
+                    figtitle="",
+                    luminance_scale_mode=0,
+                    colormap = cm.gray,
+                    show_colorbar = False,
+                    vmin = None,
+                    vmax = None,
+                    transpose_img = False):
+    """Saves filters contained in first few rows of matrix as an image file.
+    imgfile specifies the name of the image file,
+    img_width and img_height specify the sizes of the imagettes (not the size of the big saved image).
+    Other parameters are passed directly to plorRowsAsImages
+    """
+    clf()
+    endidx = min(nrows*ncols, len(matrix))        
+    plotRowsAsImages(matrix[0:endidx],
+                     img_height = img_height,
+                     img_width = img_width,
+                     nrows = nrows,
+                     ncols = ncols,
+                     figtitle = figtitle,
+                     luminance_scale_mode = luminance_scale_mode,
+                     show_colorbar = show_colorbar,
+                     disable_ticks = True,
+                     colormap = colormap,
+                     vmin = vmin,
+                     vmax = vmax,
+                     transpose_img = transpose_img)
+    savefig(imgfile)
       
 class showRowsAsImages:
 

Modified: trunk/python_modules/plearn/table/viewtable.py
===================================================================
--- trunk/python_modules/plearn/table/viewtable.py	2009-06-24 17:37:44 UTC (rev 10253)
+++ trunk/python_modules/plearn/table/viewtable.py	2009-06-24 23:38:43 UTC (rev 10254)
@@ -910,6 +910,18 @@
         subprocess.Popen(command, shell=True)
         
     def chooseAndExecuteShellCommand(self):
+
+        def set_filepath(filename, guessdir):
+            return """
+            filepath="%s/%s";
+            if [[ -r $filepath ]]
+              then echo "located $filepath";
+              else echo "inexistent $filepath";
+                filepath=`find . -name %s`;
+                echo "instead found $filepath";
+            fi
+            """ % (guessdir,filename,filename)
+
         
         commands = [
             ('s',"Launch terminal and shell in this matrix's directory",
@@ -917,13 +929,37 @@
             ('1',"View layer 1 unsup training costs",
              """xterm -e sh -c 'cd "_DIRPATH_"; myplearn vmat view `find . -name training_costs_layer_1.pmat`' """),
             ('i',"deepnetplot.py plotRepAndRec learner.psave", 
-             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; learnerfile=`find . -name learner.psave`; echo "found $learnerfile"; deepnetplot.py plotRepAndRec $learnerfile ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
-            ('w',"deepnetplot.py plotEachRow learner.psave", 
-             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; learnerfile=`find . -name learner.psave`; echo "found $learnerfile"; deepnetplot.py plotEachRow $learnerfile ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd;"""+
+             set_filepath("learner.psave","Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir")+
+             """deepnetplot.py plotRepAndRec $filepath ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
+            ('v',"deepnetplot.py plotEachRow learner.psave", 
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
+             set_filepath("learner.psave","Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir")+
+             """deepnetplot.py plotEachRow $filepath ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
+            ('w',"open learner_Layer1_W.png", 
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
+             set_filepath("learner_Layer1_W.png","Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir")+
+             """open $filepath' """),
+            ('r',"open learner_Layer1_Wr.png", 
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
+             set_filepath("learner_Layer1_Wr.png","Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir")+
+             """open $filepath' """),
             ('I',"deepnetplot.py plotRepAndRec final_learner.psave", 
-             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; learnerfile=`find . -name final_learner.psave`; echo "found $learnerfile"; deepnetplot.py plotRepAndRec $learnerfile ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
-            ('W',"deepnetplot.py plotEachRow final_learner.psave", 
-             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; learnerfile=`find . -name final_learner.psave`; echo "found $learnerfile"; deepnetplot.py plotEachRow $learnerfile ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
+             set_filepath("final_learner.psave","Split0/LearnerExpdir")+
+             """deepnetplot.py plotRepAndRec $filepath ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
+            ('V',"deepnetplot.py plotEachRow final_learner.psave", 
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
+             set_filepath("final_learner.psave","Split0/LearnerExpdir")+
+             """deepnetplot.py plotEachRow $filepath ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
+            ('W',"open final_learner_Layer1_W.png", 
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
+             set_filepath("final_learner_Layer1_W.png","Split0/LearnerExpdir")+
+             """open $filepath' """),
+            ('R',"open final_learner_Layer1_Wr.png", 
+             """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
+             set_filepath("final_learner_Layer1_Wr.png","Split0/LearnerExpdir")+
+             """open $filepath' """),
             ]
 
         menutxt = '\n'.join([ '['+commands[i][0]+'] '+commands[i][1] for i in range(len(commands)) ])+'\n'

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-24 17:37:44 UTC (rev 10253)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-24 23:38:43 UTC (rev 10254)
@@ -3,6 +3,10 @@
 import sys
 # import matplotlib.pyplot as plt
 
+import os
+import os.path
+import fnmatch
+
 from pylab import *
 from plearn.io.server import *
 from plearn.pyplearn import *
@@ -28,6 +32,7 @@
     print "deepnetplot.py plotSingleMatrix x.psave "
     print "deepnetplot.py plotEachRow learner.psave chars.pmat"
     print "deepnetplot.py plotRepAndRec learner.psave chars.pmat"
+    print "deepnetplot.py collectFilters dirname"
     print "deepnetplot.py interact learner.psave chars.pmat"
     print "deepnetplot.py help"
     print ""
@@ -1088,13 +1093,34 @@
         vmat = serv.load(vmatspec)
     return vmat
 
+def collectFilters(basedir):
+    for dirpath, dirs, files in os.walk(basedir):
+        for filename in files:
+            if filename.endswith(".psave") and "learner" in filename:
+                filepath = os.path.join(dirpath,filename)
+                print 
+                print "*** EXTRACTING FILTERS FROM "+filepath
+                learner = serv.load(filepath)
+                # matrices = learner.listParameter() 
+                names = learner.listParameterNames()
+                for varname in ["Layer1_W","Layer1_Wr"]:
+                    if varname in names:
+                        matrix = learner.getParameterValue(varname)
+                        imgwidth,imgheight = guess_image_dimensions(matrix.shape[1])
+                        imgfilename = filename[:-6]+"_"+varname+".png"
+                        print "   --> "+imgfilename
+                        # chose a non-GUI backend
+                        # matplotlib.use( 'Agg' )
+                        saveRowsAsImage(os.path.join(dirpath,imgfilename), matrix,
+                                        imgwidth, imgheight, 10, 20, figtitle=imgfilename)
+
 if task == 'plotEachRow':
 
     psave = sys.argv[2]
     
     learner = serv.load(psave)
         
-    matrices = learner.listParameter() 
+    # matrices = learner.listParameter() 
     names = learner.listParameterNames()
     
     doToRow = None
@@ -1128,7 +1154,11 @@
             print
             print 'This matrix does not exist !'
 
-
+elif task == 'collectFilters':
+    basedir = sys.argv[2]
+    collectFilters(basedir)
+    
+    
 elif task == 'interact':
     psave = sys.argv[2]
     datafname = sys.argv[3]



From plearner at mail.berlios.de  Thu Jun 25 02:13:56 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 25 Jun 2009 02:13:56 +0200
Subject: [Plearn-commits] r10255 - trunk/scripts/EXPERIMENTAL
Message-ID: <200906250013.n5P0DuNI001467@sheep.berlios.de>

Author: plearner
Date: 2009-06-25 02:13:55 +0200 (Thu, 25 Jun 2009)
New Revision: 10255

Added:
   trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py
Modified:
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
moved collect_filters to its own script in order to be able to use matplotlib with a non gui backend

Added: trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py	2009-06-24 23:38:43 UTC (rev 10254)
+++ trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py	2009-06-25 00:13:55 UTC (rev 10255)
@@ -0,0 +1,67 @@
+#!/usr/bin/env python
+
+import sys
+import os
+import os.path
+
+import matplotlib
+# chose a non-GUI backend
+matplotlib.use( 'Agg' )
+
+from plearn.io.server import *
+from plearn.plotting.netplot import *
+
+
+################
+### methods ###
+################
+
+def print_usage_and_exit():
+    print "Usage : deepnet_collect_filters.py dirname"
+    print "  will collect ad save .png files of filters found in any *learner*.psave in specified directory and subdirectories"
+    sys.exit()
+
+#print "Press Enter to continue"
+#raw_input()
+
+def guess_image_dimensions(n):
+    imgheight = int(math.sqrt(n))+1
+    while n%imgheight != 0:
+        imgheight = imgheight-1
+    imgwidth = n/imgheight
+    return imgwidth,imgheight
+
+
+def collectFilters(basedir):
+    for dirpath, dirs, files in os.walk(basedir):
+        for filename in files:
+            if filename.endswith(".psave") and "learner" in filename:
+                filepath = os.path.join(dirpath,filename)
+                print 
+                print "*** EXTRACTING FILTERS FROM "+filepath
+                learner = serv.load(filepath)
+                # matrices = learner.listParameter() 
+                names = learner.listParameterNames()
+                for varname in ["Layer1_W","Layer1_Wr"]:
+                    if varname in names:
+                        matrix = learner.getParameterValue(varname)
+                        imgwidth,imgheight = guess_image_dimensions(matrix.shape[1])
+                        imgfilename = filename[:-6]+"_"+varname+".png"
+                        print "   --> "+imgfilename
+                        saveRowsAsImage(os.path.join(dirpath,imgfilename), matrix,
+                                        imgwidth, imgheight, 10, 20, figtitle=imgfilename)
+
+
+############
+### main ###
+############
+
+if len(sys.argv)<2:
+    print_usage_and_exit()
+
+server_command = "myplearn server"
+serv = launch_plearn_server(command = server_command)
+
+basedir = sys.argv[1]
+collectFilters(basedir)
+    


Property changes on: trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py
___________________________________________________________________
Name: svn:executable
   + *

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-24 23:38:43 UTC (rev 10254)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2009-06-25 00:13:55 UTC (rev 10255)
@@ -32,7 +32,6 @@
     print "deepnetplot.py plotSingleMatrix x.psave "
     print "deepnetplot.py plotEachRow learner.psave chars.pmat"
     print "deepnetplot.py plotRepAndRec learner.psave chars.pmat"
-    print "deepnetplot.py collectFilters dirname"
     print "deepnetplot.py interact learner.psave chars.pmat"
     print "deepnetplot.py help"
     print ""
@@ -1093,27 +1092,6 @@
         vmat = serv.load(vmatspec)
     return vmat
 
-def collectFilters(basedir):
-    for dirpath, dirs, files in os.walk(basedir):
-        for filename in files:
-            if filename.endswith(".psave") and "learner" in filename:
-                filepath = os.path.join(dirpath,filename)
-                print 
-                print "*** EXTRACTING FILTERS FROM "+filepath
-                learner = serv.load(filepath)
-                # matrices = learner.listParameter() 
-                names = learner.listParameterNames()
-                for varname in ["Layer1_W","Layer1_Wr"]:
-                    if varname in names:
-                        matrix = learner.getParameterValue(varname)
-                        imgwidth,imgheight = guess_image_dimensions(matrix.shape[1])
-                        imgfilename = filename[:-6]+"_"+varname+".png"
-                        print "   --> "+imgfilename
-                        # chose a non-GUI backend
-                        # matplotlib.use( 'Agg' )
-                        saveRowsAsImage(os.path.join(dirpath,imgfilename), matrix,
-                                        imgwidth, imgheight, 10, 20, figtitle=imgfilename)
-
 if task == 'plotEachRow':
 
     psave = sys.argv[2]
@@ -1153,12 +1131,7 @@
         elif matrixName != 'exit':
             print
             print 'This matrix does not exist !'
-
-elif task == 'collectFilters':
-    basedir = sys.argv[2]
-    collectFilters(basedir)
     
-    
 elif task == 'interact':
     psave = sys.argv[2]
     datafname = sys.argv[3]



From plearner at mail.berlios.de  Thu Jun 25 02:20:52 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 25 Jun 2009 02:20:52 +0200
Subject: [Plearn-commits] r10256 - trunk/scripts/EXPERIMENTAL
Message-ID: <200906250020.n5P0KqjA003106@sheep.berlios.de>

Author: plearner
Date: 2009-06-25 02:20:51 +0200 (Thu, 25 Jun 2009)
New Revision: 10256

Modified:
   trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py
Log:
esthetic fix

Modified: trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py	2009-06-25 00:13:55 UTC (rev 10255)
+++ trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py	2009-06-25 00:20:51 UTC (rev 10256)
@@ -49,7 +49,7 @@
                         imgfilename = filename[:-6]+"_"+varname+".png"
                         print "   --> "+imgfilename
                         saveRowsAsImage(os.path.join(dirpath,imgfilename), matrix,
-                                        imgwidth, imgheight, 10, 20, figtitle=imgfilename)
+                                        imgwidth, imgheight, 10, 20)
 
 
 ############



From plearner at mail.berlios.de  Mon Jun 29 15:33:13 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Mon, 29 Jun 2009 15:33:13 +0200
Subject: [Plearn-commits] r10257 - in trunk: commands/EXPERIMENTAL
	plearn_learners/unsupervised/EXPERIMENTAL
	python_modules/plearn/table scripts/EXPERIMENTAL
Message-ID: <200906291333.n5TDXD7u025403@sheep.berlios.de>

Author: plearner
Date: 2009-06-29 15:33:10 +0200 (Mon, 29 Jun 2009)
New Revision: 10257

Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc
   trunk/python_modules/plearn/table/viewtable.py
   trunk/scripts/EXPERIMENTAL/dcaexperiment.py
   trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py
Log:
minor fixes to experimental stuff


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2009-06-25 00:20:51 UTC (rev 10256)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2009-06-29 13:33:10 UTC (rev 10257)
@@ -405,7 +405,7 @@
 #include <plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h>
 
 // Stuff used for DiverseComponentAnalysis
-// #include <plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h>
+#include <plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h>
 
 // Stuff used for DenoisingRecurrentNet
 #include <plearn_learners_experimental/DenoisingRecurrentNet.h>

Modified: trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc
===================================================================
--- trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc	2009-06-25 00:20:51 UTC (rev 10256)
+++ trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc	2009-06-29 13:33:10 UTC (rev 10257)
@@ -546,7 +546,7 @@
             int l = C.length();            
             inv_stddev_of_projections.resize(l);
             for(int i=0; i<l; i++)
-                inv_stddev_of_projections = 1.0/sqrt(C(i,i));
+                inv_stddev_of_projections.fill(1.0/sqrt(C(i,i)));
         }
 
         //... train for 1 stage, and update train_stats,

Modified: trunk/python_modules/plearn/table/viewtable.py
===================================================================
--- trunk/python_modules/plearn/table/viewtable.py	2009-06-25 00:20:51 UTC (rev 10256)
+++ trunk/python_modules/plearn/table/viewtable.py	2009-06-29 13:33:10 UTC (rev 10257)
@@ -936,14 +936,14 @@
              """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
              set_filepath("learner.psave","Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir")+
              """deepnetplot.py plotEachRow $filepath ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
-            ('w',"open learner_Layer1_W.png", 
+            ('w',"xv learner_Layer1_W.png", 
              """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
              set_filepath("learner_Layer1_W.png","Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir")+
-             """open $filepath' """),
-            ('r',"open learner_Layer1_Wr.png", 
+             """xv $filepath' """),
+            ('r',"xv learner_Layer1_Wr.png", 
              """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
              set_filepath("learner_Layer1_Wr.png","Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir")+
-             """open $filepath' """),
+             """xv $filepath' """),
             ('I',"deepnetplot.py plotRepAndRec final_learner.psave", 
              """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
              set_filepath("final_learner.psave","Split0/LearnerExpdir")+
@@ -952,14 +952,14 @@
              """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
              set_filepath("final_learner.psave","Split0/LearnerExpdir")+
              """deepnetplot.py plotEachRow $filepath ~/data/mnist/mnist_small/mnist_basic2_valid.pmat; sh' """),
-            ('W',"open final_learner_Layer1_W.png", 
+            ('W',"xv final_learner_Layer1_W.png", 
              """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
              set_filepath("final_learner_Layer1_W.png","Split0/LearnerExpdir")+
-             """open $filepath' """),
-            ('R',"open final_learner_Layer1_Wr.png", 
+             """xv $filepath' """),
+            ('R',"xv final_learner_Layer1_Wr.png", 
              """xterm -e sh -c 'cd "_DIRPATH_"; pwd; """+
              set_filepath("final_learner_Layer1_Wr.png","Split0/LearnerExpdir")+
-             """open $filepath' """),
+             """xv $filepath' """),
             ]
 
         menutxt = '\n'.join([ '['+commands[i][0]+'] '+commands[i][1] for i in range(len(commands)) ])+'\n'

Modified: trunk/scripts/EXPERIMENTAL/dcaexperiment.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2009-06-25 00:20:51 UTC (rev 10256)
+++ trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2009-06-29 13:33:10 UTC (rev 10257)
@@ -477,6 +477,9 @@
     OLDEXAMPLE: dcaexperiment.py  123:1    123 2    -2 cov     -1 square 1       1 square 1   0"
     OLDEXAMPLE: dcaexperiment.py 121:-2    123 4    -2 squaredist     0 exp 1       1 exp -1.6   0"
     OLDEXAMPLE: dcaexperiment.py /data/icml07data/mnist_basic/plearn/mnist_basic2_train.pmat    125 400    -2 squaredist     0 exp -1       1 exp -1   0
+
+    Ex:
+                 data_set=123:1 seed=1827 ncomponents=2 nonlinearity=none constrain_norm_type=-2 cov_transformation_type=cov diag_add=0. diag_weight=0. diag_nonlinearity=square diag_premul=1.0 offdiag_weight=1.0 offdiag_nonlinearity=exp offdiag_premul=1.0 lr=0.01 nsteps=1 optimizer_nsteps=1 force_zero_mean=False
     """
     autoscript(DCAExperiment,True,helptext=helptext)
 

Modified: trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py	2009-06-25 00:20:51 UTC (rev 10256)
+++ trunk/scripts/EXPERIMENTAL/deepnet_collect_filters.py	2009-06-29 13:33:10 UTC (rev 10257)
@@ -50,8 +50,8 @@
                         print "   --> "+imgfilename
                         saveRowsAsImage(os.path.join(dirpath,imgfilename), matrix,
                                         imgwidth, imgheight, 10, 20)
+                learner.delete()
 
-
 ############
 ### main ###
 ############



