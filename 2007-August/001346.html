<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7898 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-August/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7898%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200708020406.l724683b019769%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001345.html">
   <LINK REL="Next"  HREF="001347.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7898 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7898%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200708020406.l724683b019769%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7898 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Thu Aug  2 06:06:08 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001345.html">[Plearn-commits] r7897 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="001347.html">[Plearn-commits] r7899 - trunk/commands
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1346">[ date ]</a>
              <a href="thread.html#1346">[ thread ]</a>
              <a href="subject.html#1346">[ subject ]</a>
              <a href="author.html#1346">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2007-08-02 06:06:08 +0200 (Thu, 02 Aug 2007)
New Revision: 7898

Added:
   trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
   trunk/plearn_learners/online/RBMLocalMultinomialLayer.h
Log:
New subclass of RBMLayer


Added: trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2007-08-02 03:55:57 UTC (rev 7897)
+++ trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2007-08-02 04:06:08 UTC (rev 7898)
@@ -0,0 +1,858 @@
+// -*- C++ -*-
+
+// RBMLocalMultinomialLayer.cc
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Author: Pascal Lamblin
+
+/*! \file RBMPLayer.cc */
+
+
+
+#include &quot;RBMLocalMultinomialLayer.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &quot;RBMConnection.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+// Helper functions, like the ones using Vecs, but with Mats
+template &lt;class T&gt;
+void softmax(const TMat&lt;T&gt;&amp; x, const TMat&lt;T&gt;&amp; y)
+{
+    int l = x.length();
+    int w = x.width();
+    PLASSERT( y.length() == l );
+    PLASSERT( y.width() == w );
+
+    if (l*w&gt;0)
+    {
+        TMatElementIterator&lt;real&gt; xp = x.begin();
+        TMatElementIterator&lt;real&gt; yp = y.begin();
+        T maxx = max(x);
+        real s = 0;
+
+        for (int i=0; i&lt;l*w; i++, xp++, yp++)
+            s += ( (*yp) = safeexp((*xp) - maxx) );
+
+        if (s == 0)
+            PLERROR( &quot;Trying to divide by 0 in softmax&quot;);
+        s = 1.0 / s;
+
+        for (yp = y.begin(); yp != y.end(); yp++)
+            (*yp) *= s;
+    }
+}
+
+int multinomial_sample(const PP&lt;PRandom&gt;&amp; rg, const Mat&amp; distribution)
+{
+    real u = rg-&gt;uniform_sample();
+    TMatElementIterator&lt;real&gt; pi = distribution.begin();
+    real s = *pi;
+    int w = distribution.width();
+    int n = distribution.size();
+    int i = 0;
+
+    while (s&lt;u &amp;&amp; i&lt;n)
+    {
+        PLASSERT( *pi == distribution(i / w, i % w) );
+        i++;
+        pi++;
+        s += *pi;
+    }
+    if (i == n)
+        i = n - 1; // Improbable, but...
+    return i;
+}
+
+template&lt;class T&gt;
+void fill_one_hot(const TMat&lt;T&gt;&amp; mat, int hotpos, T coldvalue, T hotvalue)
+{
+    PLASSERT_MSG( mat.isNotEmpty(), &quot;Given mat must not be empty&quot; );
+    PLASSERT_MSG( hotpos &gt;= 0, &quot;hotpos out of mat range&quot; );
+    PLASSERT_MSG( mat.size() &gt; 1 || hotpos &lt;= 1, &quot;hotpos out of mat range&quot; );
+    PLASSERT_MSG( hotpos &lt; mat.size() || mat.size() == 1,
+                  &quot;hotpos out of mat range&quot; );
+
+    if (mat.size() == 1)
+        mat(0,0) = (hotpos == 0 ? coldvalue : hotvalue);
+    else
+    {
+        mat.fill(coldvalue);
+        int w = mat.width();
+        mat(hotpos / w, hotpos % w);
+    }
+}
+
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMLocalMultinomialLayer,
+    &quot;Layer in an RBM, consisting in one multinomial unit&quot;,
+    &quot;&quot;);
+
+RBMLocalMultinomialLayer::RBMLocalMultinomialLayer( real the_learning_rate ) :
+    inherited( the_learning_rate )
+{
+}
+
+void RBMLocalMultinomialLayer::generateSample()
+{
+    PLASSERT_MSG(random_gen,
+                 &quot;random_gen should be initialized before generating samples&quot;);
+
+    PLCHECK_MSG(expectation_is_up_to_date, &quot;Expectation should be computed &quot;
+            &quot;before calling generateSample()&quot;);
+
+    for (int l=0; l&lt;n_images; l++)
+    {
+        Mat expectation_image = expectation
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat sample_image = sample
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i&lt;images_length; i+=area_length)
+            for (int j=0; j&lt;images_width; j+=area_width)
+            {
+                Mat expectation_area =
+                    expectation_image.subMat(i, j, area_length, area_width);
+                Mat sample_area =
+                    sample_image.subMat(i, j, area_length, area_width);
+                int index = multinomial_sample(random_gen, expectation_area);
+                fill_one_hot(sample_area, index, real(0), real(1));
+            }
+    }
+}
+
+void RBMLocalMultinomialLayer::generateSamples()
+{
+    PLASSERT_MSG(random_gen,
+                 &quot;random_gen should be initialized before generating samples&quot;);
+
+    PLCHECK_MSG(expectations_are_up_to_date, &quot;Expectations should be computed &quot;
+                        &quot;before calling generateSamples()&quot;);
+
+    PLASSERT( samples.width() == size &amp;&amp; samples.length() == batch_size );
+
+    for (int k = 0; k &lt; batch_size; k++)
+        for (int l=0; l&lt;n_images; l++)
+        {
+            Mat expectation_image = expectations(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat sample_image = samples(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+            for (int i=0; i&lt;images_length; i+=area_length)
+                for (int j=0; j&lt;images_width; j+=area_width)
+                {
+                    Mat expectation_area =
+                        expectation_image.subMat(i, j, area_length, area_width);
+                    Mat sample_area =
+                        sample_image.subMat(i, j, area_length, area_width);
+                    int index = multinomial_sample(random_gen,
+                                                   expectation_area);
+                    fill_one_hot(sample_area, index, real(0), real(1));
+               }
+        }
+}
+
+void RBMLocalMultinomialLayer::computeExpectation()
+{
+    if( expectation_is_up_to_date )
+        return;
+
+    for (int l=0; l&lt;n_images; l++)
+    {
+        Mat activation_image = activation
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat expectation_image = expectation
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i&lt;images_length; i+=area_length)
+            for (int j=0; j&lt;images_width; j+=area_width)
+                softmax(
+                    activation_image.subMat(i, j, area_length, area_width),
+                    expectation_image.subMat(i, j, area_length, area_width)
+                    );
+    }
+    expectation_is_up_to_date = true;
+}
+
+void RBMLocalMultinomialLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    PLASSERT( expectations.width() == size
+              &amp;&amp; expectations.length() == batch_size );
+
+    for (int k = 0; k &lt; batch_size; k++)
+        for (int l=0; l&lt;n_images; l++)
+        {
+            Mat activation_image = activations(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat expectation_image = expectations(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+            for (int i=0; i&lt;images_length; i+=area_length)
+                for (int j=0; j&lt;images_width; j+=area_width)
+                    softmax(
+                        activation_image.subMat(i, j, area_length, area_width),
+                        expectation_image.subMat(i, j, area_length, area_width)
+                        );
+        }
+
+    expectations_are_up_to_date = true;
+}
+
+
+void RBMLocalMultinomialLayer::fprop( const Vec&amp; input, Vec&amp; output ) const
+{
+    PLASSERT( input.size() == input_size );
+    output.resize( output_size );
+
+    // inefficient
+    Vec input_plus_bias = input + bias;
+    for (int l=0; l&lt;n_images; l++)
+    {
+        Mat input_image = input_plus_bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat output_image = output
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i&lt;images_length; i+=area_length)
+            for (int j=0; j&lt;images_width; j+=area_width)
+                softmax(
+                    input_image.subMat(i, j, area_length, area_width),
+                    output_image.subMat(i, j, area_length, area_width)
+                    );
+    }
+}
+
+///////////
+// fprop //
+///////////
+void RBMLocalMultinomialLayer::fprop( const Vec&amp; input, const Vec&amp; rbm_bias,
+                                      Vec&amp; output ) const
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( rbm_bias.size() == input_size );
+    output.resize( output_size );
+
+    // inefficient
+    Vec input_plus_bias = input + rbm_bias;
+    for (int l=0; l&lt;n_images; l++)
+    {
+        Mat input_image = input_plus_bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat output_image = output
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i&lt;images_length; i+=area_length)
+            for (int j=0; j&lt;images_width; j+=area_width)
+                softmax(
+                    input_image.subMat(i, j, area_length, area_width),
+                    output_image.subMat(i, j, area_length, area_width)
+                    );
+    }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMLocalMultinomialLayer::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                           Vec&amp; input_gradient,
+                                           const Vec&amp; output_gradient,
+                                           bool accumulate)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      &quot;Cannot resize input_gradient AND accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    for (int l=0; l&lt;n_images; l++)
+    {
+        Mat output_image = output
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat input_grad_image = input_gradient
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat output_grad_image = output_gradient
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat bias_image = bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat bias_inc_image;
+        if (momentum != 0)
+            bias_inc_image = bias_inc
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+        for (int i=0; i&lt;images_length; i+=area_length)
+            for (int j=0; j&lt;images_width; j+=area_width)
+            {
+                Mat output_area = output_image
+                    .subMat(i, j, area_length, area_width);
+                Mat input_grad_area = input_grad_image
+                    .subMat(i, j, area_length, area_width);
+                Mat output_grad_area = output_grad_image
+                    .subMat(i, j, area_length, area_width);
+                Mat bias_area = bias_image
+                    .subMat(i, j, area_length, area_width);
+                Mat bias_inc_area;
+                if (momentum != 0)
+                    bias_inc_area = bias_inc_image
+                        .subMat(i, j, area_length, area_width);
+
+                real outga_dot_outa = dot(output_grad_area, output_area);
+
+                TMatElementIterator&lt;real&gt; pog = output_grad_area.begin();
+                TMatElementIterator&lt;real&gt; po = output_area.begin();
+                TMatElementIterator&lt;real&gt; pig = input_grad_area.begin();
+                TMatElementIterator&lt;real&gt; pb = bias_area.begin();
+
+                TMatElementIterator&lt;real&gt; pbi = bias_inc_area.begin();
+/*
+                TMatElementIterator&lt;real&gt; pbi;
+                if (momentum != 0)
+                    pbi = bias_inc_area.begin();
+*/
+                for (int m=0; m&lt;area_size; m++, pog++, po++, pig++, pb++)
+                {
+                    real inga_m = (*pog - outga_dot_outa) * (*po);
+                    *pig += inga_m;
+
+                    if (momentum == 0)
+                    {
+                        // update the bias: bias -= learning_rate * input_grad
+                        *pb -= learning_rate * (*pig);
+                    }
+                    else
+                    {
+                        // The update rule becomes:
+                        // bias_inc = momentum * bias_inc
+                        //            - learning_rate * input_grad
+                        *pbi = momentum * (*pbi) - learning_rate * (*pig);
+                        *pb += *pbi;
+                        pbi++;
+                    }
+                }
+            }
+    }
+}
+
+void RBMLocalMultinomialLayer::bpropUpdate(const Mat&amp; inputs,
+                                           const Mat&amp; outputs,
+                                           Mat&amp; input_gradients,
+                                           const Mat&amp; output_gradients,
+                                           bool accumulate)
+{
+    PLASSERT( inputs.width() == size );
+    PLASSERT( outputs.width() == size );
+    PLASSERT( output_gradients.width() == size );
+
+    int mbatch_size = inputs.length();
+    PLASSERT( outputs.length() == mbatch_size );
+    PLASSERT( output_gradients.length() == mbatch_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == size &amp;&amp;
+                input_gradients.length() == inputs.length(),
+                &quot;Cannot resize input_gradient and accumulate into it.&quot; );
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), size);
+        input_gradients.clear();
+    }
+
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    // TODO see if we can have a speed-up by reorganizing the different steps
+
+    // input_gradients[k][i] =
+    //   (output_gradients[k][i]-output_gradients[k].outputs[k]) outputs[k][i]
+    real mean_lr = learning_rate / mbatch_size;
+    for (int l=0; l&lt;n_images; l++)
+    {
+        Mat bias_image = bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat bias_inc_image;
+        if (momentum != 0)
+            bias_inc_image = bias_inc
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+        for( int k=0; k&lt;mbatch_size; k++ )
+        {
+            Mat output_image = outputs(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat input_grad_image = input_gradients(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat output_grad_image = output_gradients(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+            for (int i=0; i&lt;images_length; i+=area_length)
+                for (int j=0; j&lt;images_width; j+=area_width)
+                {
+                    Mat output_area = output_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat input_grad_area = input_grad_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat output_grad_area = output_grad_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat bias_area = bias_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat bias_inc_area;
+                    if (momentum != 0)
+                        bias_inc_area = bias_inc_image
+                            .subMat(i, j, area_length, area_width);
+
+                    real outga_dot_outa = dot(output_grad_area, output_area);
+
+                    TMatElementIterator&lt;real&gt; pog = output_grad_area.begin();
+                    TMatElementIterator&lt;real&gt; po = output_area.begin();
+                    TMatElementIterator&lt;real&gt; pig = input_grad_area.begin();
+                    TMatElementIterator&lt;real&gt; pb = bias_area.begin();
+
+                    if (momentum == 0)
+                    {
+                        for (int i=0; i&lt;area_size; i++, pog++, po++, pig++,
+                                                   pb++)
+                        {
+                            real inga_i = (*pog - outga_dot_outa) * (*po);
+                            *pig += inga_i;
+
+                            // update the bias:
+                            // bias -= learning_rate * input_grad
+                            *pb -= mean_lr * (*pig);
+                        }
+                    }
+                    else
+                        PLCHECK_MSG(false,
+                                    &quot;Momentum and mini-batch not implemented&quot;);
+                }
+        }
+    }
+}
+
+//! TODO: add &quot;accumulate&quot; here
+void RBMLocalMultinomialLayer::bpropUpdate(const Vec&amp; input,
+                                           const Vec&amp; rbm_bias,
+                                           const Vec&amp; output,
+                                           Vec&amp; input_gradient,
+                                           Vec&amp; rbm_bias_gradient,
+                                           const Vec&amp; output_gradient)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( rbm_bias.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+    input_gradient.resize( size );
+    rbm_bias_gradient.resize( size );
+
+    for (int l=0; l&lt;n_images; l++)
+    {
+        Mat output_image = output
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat input_grad_image = input_gradient
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat output_grad_image = output_gradient
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat rbm_bias_image = rbm_bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i&lt;images_length; i+=area_length)
+            for (int j=0; j&lt;images_width; j+=area_width)
+            {
+                Mat output_area = output_image
+                    .subMat(i, j, area_length, area_width);
+                Mat input_grad_area = input_grad_image
+                    .subMat(i, j, area_length, area_width);
+                Mat output_grad_area = output_grad_image
+                    .subMat(i, j, area_length, area_width);
+                Mat rbm_bias_area = rbm_bias_image
+                    .subMat(i, j, area_length, area_width);
+
+                real outga_dot_outa = dot(output_grad_area, output_area);
+
+                TMatElementIterator&lt;real&gt; pog = output_grad_area.begin();
+                TMatElementIterator&lt;real&gt; po = output_area.begin();
+                TMatElementIterator&lt;real&gt; pig = input_grad_area.begin();
+                TMatElementIterator&lt;real&gt; prb = rbm_bias_area.begin();
+
+                for (int m=0; m&lt;area_size; m++, pog++, po++, pig++, prb++)
+                {
+                    real inga_m = (*pog - outga_dot_outa) * (*po);
+                    *pig += inga_m;
+
+                    // update the bias: bias -= learning_rate * input_grad
+                    *prb -= learning_rate * (*pig);
+                }
+            }
+    }
+
+    rbm_bias_gradient &lt;&lt; input_gradient;
+}
+
+//////////////
+// fpropNLL //
+//////////////
+real RBMLocalMultinomialLayer::fpropNLL(const Vec&amp; target)
+{
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+
+    real nll = 0;
+    for (int l=0; l&lt;n_images; l++)
+    {
+        Mat target_image = target
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat expectation_image = expectation
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i&lt;images_length; i+=area_length)
+            for (int j=0; j&lt;images_width; j+= area_width)
+            {
+                Mat target_area = target_image
+                    .subMat(i, j, area_length, area_width);
+                Mat expectation_area = expectation_image
+                    .subMat(i, j, area_length, area_width);
+
+#ifdef BOUNDCHECK
+                if (!target_area.hasMissing())
+                {
+                    PLASSERT_MSG( min(target_area) &gt;= 0.,
+                                  &quot;Elements of \&quot;target_areal\&quot; should be&quot;
+                                  &quot; positive&quot; );
+                    // Ensure the distribution probabilities sum to 1. We relax a
+                    // bit the default tolerance as probabilities using
+                    // exponentials could suffer numerical imprecisions.
+                    if (!is_equal( sum(target_area), 1., 1., 1e-5, 1e-5 ))
+                        PLERROR(&quot;In RBMLocalMultinomialLayer::fpropNLL -&quot;
+                                &quot; Elements of \&quot;target_area\&quot; should sum to 1&quot;
+                                &quot; (found a sum = %f)&quot;,
+                                sum(target_area));
+                }
+#endif
+                TMatElementIterator&lt;real&gt; p_tgt = target_area.begin();
+                TMatElementIterator&lt;real&gt; p_exp = expectation_area.begin();
+                for (int m=0; m&lt;area_size; m++, p_tgt++, p_exp++)
+                {
+                    if (!fast_exact_is_equal(*p_tgt, 0))
+                        nll -= *p_tgt * pl_log(*p_exp);
+                }
+            }
+    }
+    return nll;
+}
+
+void RBMLocalMultinomialLayer::fpropNLL(const Mat&amp; targets, const Mat&amp; costs_column)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+
+    for (int k=0; k&lt;batch_size; k++) // loop over minibatch
+    {
+        real nll = 0;
+        for (int l=0; l&lt;n_images; l++)
+        {
+            Mat target_image = targets(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat expectation_image = expectations(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+            for (int i=0; i&lt;images_length; i+=area_length)
+                for (int j=0; j&lt;images_width; j+= area_width)
+                {
+                    Mat target_area = target_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat expectation_area = expectation_image
+                        .subMat(i, j, area_length, area_width);
+
+#ifdef BOUNDCHECK
+                    if (!target_area.hasMissing())
+                    {
+                        PLASSERT_MSG( min(target_area) &gt;= 0.,
+                                      &quot;Elements of \&quot;target_areal\&quot; should be&quot;
+                                      &quot; positive&quot; );
+                        // Ensure the distribution probabilities sum to 1. We relax a
+                        // bit the default tolerance as probabilities using
+                        // exponentials could suffer numerical imprecisions.
+                        if (!is_equal( sum(target_area), 1., 1., 1e-5, 1e-5 ))
+                            PLERROR(&quot;In RBMLocalMultinomialLayer::fpropNLL -&quot;
+                                    &quot; Elements of \&quot;target_area\&quot; should sum&quot;
+                                    &quot; to 1 (found a sum = %f) at row %d&quot;,
+                                    sum(target_area), k);
+                    }
+#endif
+                    TMatElementIterator&lt;real&gt; p_tgt = target_area.begin();
+                    TMatElementIterator&lt;real&gt; p_exp = expectation_area.begin();
+                    for (int m=0; m&lt;area_size; m++, p_tgt++, p_exp++)
+                    {
+                        if (!fast_exact_is_equal(*p_tgt, 0))
+                            nll -= *p_tgt * pl_log(*p_exp);
+                    }
+                }
+        }
+        costs_column(k, 0) = nll;
+    }
+}
+
+void RBMLocalMultinomialLayer::bpropNLL(const Vec&amp; target, real nll,
+                                        Vec&amp; bias_gradient)
+{
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+    bias_gradient.resize( size );
+
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
+}
+
+void RBMLocalMultinomialLayer::bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                                        Mat&amp; bias_gradients)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
+}
+
+void RBMLocalMultinomialLayer::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;n_images&quot;, &amp;RBMLocalMultinomialLayer::n_images,
+                  OptionBase::buildoption,
+                  &quot;Number of images in the layer.&quot;);
+
+    declareOption(ol, &quot;images_length&quot;,
+                  &amp;RBMLocalMultinomialLayer::images_length,
+                  OptionBase::buildoption,
+                  &quot;Length of the images.&quot;);
+
+    declareOption(ol, &quot;images_width&quot;,
+                  &amp;RBMLocalMultinomialLayer::images_width,
+                  OptionBase::buildoption,
+                  &quot;Width of the images.&quot;);
+
+    declareOption(ol, &quot;images_size&quot;,
+                  &amp;RBMLocalMultinomialLayer::images_size,
+                  OptionBase::learntoption,
+                  &quot;images_width &#215; images_length.&quot;);
+
+    declareOption(ol, &quot;area_length&quot;,
+                  &amp;RBMLocalMultinomialLayer::area_length,
+                  OptionBase::buildoption,
+                  &quot;Length of the areas over which the multinomial is set.&quot;);
+
+    declareOption(ol, &quot;area_width&quot;,
+                  &amp;RBMLocalMultinomialLayer::area_width,
+                  OptionBase::buildoption,
+                  &quot;Width of the areas over which the multinomial is set.&quot;);
+
+    declareOption(ol, &quot;area_size&quot;,
+                  &amp;RBMLocalMultinomialLayer::area_size,
+                  OptionBase::learntoption,
+                  &quot;area_width &#215; area_length.&quot;);
+
+/*
+    declareOption(ol, &quot;size&quot;, &amp;RBMLocalMultinomialLayer::size,
+                  OptionBase::buildoption,
+                  &quot;Number of units.&quot;);
+*/
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, &quot;size&quot;,
+                  &amp;RBMLocalMultinomialLayer::size,
+                  OptionBase::learntoption,
+                  &quot;n_images &#215; images_width &#215; images_length.&quot;);
+
+}
+
+void RBMLocalMultinomialLayer::build_()
+{
+    PLCHECK_MSG(images_length % area_length == 0,
+                &quot;\&quot;images_length\&quot; should be a multiple of \&quot;area_length\&quot;&quot;);
+    PLCHECK_MSG(images_width % area_width == 0,
+                &quot;\&quot;images_width\&quot; should be a multiple of \&quot;area_width\&quot;&quot;);
+
+    images_size = images_length * images_width;
+    area_size = area_length * area_width;
+    size = images_size * n_images;
+    n_areas = size / area_size;
+}
+
+void RBMLocalMultinomialLayer::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMLocalMultinomialLayer::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+real RBMLocalMultinomialLayer::energy(const Vec&amp; unit_values) const
+{
+    return -dot(unit_values, bias);
+}
+
+
+real RBMLocalMultinomialLayer::freeEnergyContribution(
+    const Vec&amp; activation_values) const
+{
+    PLASSERT( activation_values.size() == size );
+
+    // result = -\sum_{i=0}^{n_areas-1} log(\sum_{j=0}^{area_size-1} a_{ij})
+    real result = 0;
+    Mat activation_images = activation_values
+        .toMat(n_images*images_length, images_width);
+    for (int i=0; i&lt;n_areas; i++)
+    {
+        Mat activation_area = activation_images
+            .subMat((i/images_width)*area_length,
+                    (i*area_width) % images_width,
+                    area_length,
+                    area_width);
+
+        result -= pl_log(sum(activation_area));
+    }
+    return result;
+}
+
+int RBMLocalMultinomialLayer::getConfigurationCount()
+{
+    real approx_count = pow(real(area_size), n_areas);
+    int count = 1;
+    if (approx_count &gt; 1e30)
+        count = INFINITE_CONFIGURATIONS;
+    else
+        for (int i=0; i&lt;n_areas; i++)
+            count *= area_size;
+
+    return count;
+}
+
+void RBMLocalMultinomialLayer::getConfiguration(int conf_index, Vec&amp; output)
+{
+    PLASSERT( output.length() == size );
+    PLASSERT( conf_index &gt;= 0 &amp;&amp; conf_index &lt; getConfigurationCount() );
+
+    output.clear();
+    Mat output_images = output.toMat(n_images*images_length, images_width);
+    for (int i=0; i&lt;n_areas; i++)
+    {
+        int area_conf_index = conf_index % area_size;
+        conf_index /= area_size;
+
+        Mat output_area = output_images
+            .subMat((i/images_width)*area_length,
+                    (i*area_width) % images_width,
+                    area_length,
+                    area_width );
+
+        output_area(area_conf_index/area_width, area_conf_index%area_width)=1;
+    }
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMLocalMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLocalMultinomialLayer.h	2007-08-02 03:55:57 UTC (rev 7897)
+++ trunk/plearn_learners/online/RBMLocalMultinomialLayer.h	2007-08-02 04:06:08 UTC (rev 7898)
@@ -0,0 +1,198 @@
+// -*- C++ -*-
+
+// RBMLocalMultinomialLayer.h
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Author: Pascal Lamblin
+
+/*! \file RBMLocalMultinomialLayer.h */
+
+
+#ifndef RBMLocalMultinomialLayer_INC
+#define RBMLocalMultinomialLayer_INC
+
+#include &quot;RBMLayer.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Multiple multinomial units, each of them seeing an area of nearby pixels
+ *
+ */
+class RBMLocalMultinomialLayer: public RBMLayer
+{
+    typedef RBMLayer inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Number of images present at the same time in the input vector
+    int n_images;
+
+    //! Length of each of the images
+    int images_length;
+
+    //! Width of each of the images
+    int images_width;
+
+    //! Length of the areas to consider
+    int area_length;
+
+    //! Width of the areas to consider
+    int area_width;
+
+    //#####  Learnt Options  ##################################################
+    int images_size;
+    int area_size;
+    int n_areas;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMLocalMultinomialLayer( real the_learning_rate=0. );
+
+    //! Constructor from the number of units in the multinomial
+    RBMLocalMultinomialLayer( int the_size, real the_learning_rate=0. );
+
+
+    //! generate a sample, and update the sample field
+    virtual void generateSample();
+
+    //! batch version
+    virtual void generateSamples();
+
+    //! compute the expectation
+    virtual void computeExpectation();
+
+    //! batch version
+    virtual void computeExpectations();
+
+    //! forward propagation
+    virtual void fprop( const Vec&amp; input, Vec&amp; output ) const;
+
+    //! forward propagation with provided bias
+    virtual void fprop( const Vec&amp; input, const Vec&amp; rbm_bias,
+                        Vec&amp; output ) const;
+
+    //! back-propagates the output gradient to the input
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             Vec&amp; input_gradient, const Vec&amp; output_gradient,
+                             bool accumulate=false);
+
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                             Mat&amp; input_gradients,
+                             const Mat&amp; output_gradients,
+                             bool accumulate = false);
+
+    //! back-propagates the output gradient to the input and the bias
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; rbm_bias,
+                             const Vec&amp; output,
+                             Vec&amp; input_gradient, Vec&amp; rbm_bias_gradient,
+                             const Vec&amp; output_gradient) ;
+
+    //! Computes the negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec&amp; target);
+    virtual void fpropNLL(const Mat&amp; targets, const Mat&amp; costs_column);
+
+    //! Computes the gradient of the negative log-likelihood of target
+    //! with respect to the layer's bias, given the internal activations
+    virtual void bpropNLL(const Vec&amp; target, real nll, Vec&amp; bias_gradient);
+    virtual void bpropNLL(const Mat&amp; targets, const Mat&amp; costs_column,
+                          Mat&amp; bias_gradients);
+
+    // Compute -bias' unit_values
+    virtual real energy(const Vec&amp; unit_values) const;
+
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec&amp; unit_activations) const;
+
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec&amp; output);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMLocalMultinomialLayer);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMLocalMultinomialLayer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001345.html">[Plearn-commits] r7897 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="001347.html">[Plearn-commits] r7899 - trunk/commands
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1346">[ date ]</a>
              <a href="thread.html#1346">[ thread ]</a>
              <a href="subject.html#1346">[ subject ]</a>
              <a href="author.html#1346">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
