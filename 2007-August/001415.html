<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7967 -	trunk/plearn_learners/distributions/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-August/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7967%20-%0A%09trunk/plearn_learners/distributions/EXPERIMENTAL&In-Reply-To=%3C200708091529.l79FTAQF000491%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001414.html">
   <LINK REL="Next"  HREF="001416.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7967 -	trunk/plearn_learners/distributions/EXPERIMENTAL</H1>
    <B>lysiane at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7967%20-%0A%09trunk/plearn_learners/distributions/EXPERIMENTAL&In-Reply-To=%3C200708091529.l79FTAQF000491%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7967 -	trunk/plearn_learners/distributions/EXPERIMENTAL">lysiane at mail.berlios.de
       </A><BR>
    <I>Thu Aug  9 17:29:10 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001414.html">[Plearn-commits] r7966 - trunk/plearn/math
</A></li>
        <LI>Next message: <A HREF="001416.html">[Plearn-commits] r7968 - in branches: . numarray	numarray/python_modules/plearn/plide	numarray/python_modules/plearn/pyplearn
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1415">[ date ]</a>
              <a href="thread.html#1415">[ thread ]</a>
              <a href="subject.html#1415">[ subject ]</a>
              <a href="author.html#1415">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lysiane
Date: 2007-08-09 17:29:09 +0200 (Thu, 09 Aug 2007)
New Revision: 7967

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
Modifs in methods MStepTransformation, applyTransformationOn, purpose = more rapidity  
experiment : dir = exp2_tiny_10..., profiler file= gprof2.txt
             compilation: without special options (except profiler option)
             time :(real=1m9.334s, user=0m6.872s, sys=0m0.252s)          


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-09 12:18:20 UTC (rev 7966)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-09 15:29:09 UTC (rev 7967)
@@ -424,6 +424,9 @@
                    RetDoc(&quot;mdXd matrix, m = number of transformations \n&quot;
                           &quot;             d = dimensionality of the input space&quot;)));
     
+    declareMethod(rmm,&quot;buildLearnedParameters&quot;,
+                  &amp;TransformationLearner::buildLearnedParameters,
+                  (BodyDoc(&quot;builds the structures related to learned parameters&quot;)));
     declareMethod(rmm,
                   &quot;generatorBuild&quot;,
                   &amp;TransformationLearner::generatorBuild,
@@ -593,7 +596,7 @@
     neighborIdx=pickNeighborIdx();
     Vec neighbor;
     neighbor.resize(inputSpaceDim);
-    seeNeighbor(neighborIdx, neighbor);
+    seeTrainingPoint(neighborIdx, neighbor);
     generatePredictedFrom(neighbor, y);
 }
 
@@ -605,28 +608,8 @@
 }
 
 
-/*TVec&lt;string&gt; PDistribution::getTestCostNames() const
-{
-    TVec&lt;string&gt; nll_cost;
-    if (nll_cost.isEmpty())
-        nll_cost.append(&quot;NLL&quot;);
-    return nll_cost;
-    }*/
 
-///////////////////////
-// getTrainCostNames //
-///////////////////////
-TVec&lt;string&gt; TransformationLearner::getTrainCostNames() const
-{
-    return getTestCostNames();
-}
 
-
-
-
-
-
-
 /////////////////
 // log_density //
 /////////////////
@@ -1029,6 +1012,7 @@
 void TransformationLearner::setTransformsParameters(TVec&lt;Mat&gt; transforms_,
                                                     Mat biasSet_)
 {
+    
     PLASSERT(transforms_.length() == nbTransforms);
     
     int nbRows = inputSpaceDim*nbTransforms;
@@ -1349,7 +1333,7 @@
         transformIdx= reconstructionSet[candidateIdx].transformIdx;
         Vec neighbor;
         neighbor.resize(inputSpaceDim);
-        seeNeighbor(neighborIdx, neighbor);
+        seeTrainingPoint(neighborIdx, neighbor);
         Vec v = reconstructions(i);
         applyTransformationOn(transformIdx, neighbor, v);
         candidateIdx ++;
@@ -1368,7 +1352,7 @@
         neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
         Vec neighbor;
         neighbor.resize(inputSpaceDim);
-        seeNeighbor(neighborIdx, neighbor);
+        seeTrainingPoint(neighborIdx, neighbor);
         neighbors(i) &lt;&lt; neighbor;
         candidateIdx++;
     }
@@ -1405,23 +1389,13 @@
                                                        nbTargetReconstructions); 
 }
 
-// stores the &quot;targetIdx&quot;th point in the training set into the variable
-// &quot;target&quot;
-void TransformationLearner::seeTarget(const int targetIdx, Vec &amp; storage)const
-{
-    Vec v;
-    real w;
-    train_set-&gt;getExample(targetIdx,storage,v,w);
-    
-}
 
-// stores the &quot;neighborIdx&quot;th input in the training set into the variable
-// &quot;neighbor&quot; 
-void TransformationLearner::seeNeighbor(const int neighborIdx, Vec &amp; neighbor)const
+// stores the 'idx'th training data point into 'dst'
+void TransformationLearner::seeTrainingPoint(const int idx, Vec &amp; dst)const
 {
     Vec v;
     real w;
-    train_set-&gt;getExample(neighborIdx, neighbor,v,w);
+    train_set-&gt;getExample(idx, dst,v,w);
 }
 
 
@@ -1607,7 +1581,7 @@
 
     Vec target;
     target.resize(inputSpaceDim);
-    seeTarget(targetIdx,target);
+    seeTrainingPoint(targetIdx,target);
     return computeReconstructionWeight(target,
                                        neighborIdx,
                                        transformIdx);
@@ -1618,7 +1592,7 @@
 {
     Vec neighbor;
     neighbor.resize(inputSpaceDim);
-    seeNeighbor(neighborIdx, neighbor);
+    seeTrainingPoint(neighborIdx, neighbor);
     Vec predictedTarget ;
     predictedTarget.resize(inputSpaceDim);
     applyTransformationOn(transformIdx, neighbor, predictedTarget);
@@ -1634,14 +1608,16 @@
 {
     if(transformFamily==TRANSFORM_FAMILY_LINEAR){
         Mat m  = transforms[transformIdx];
-        transposeProduct(dst,m,src); 
+        //transposeProduct(dst,m,src); 
+        product(dst,m,src);
         if(withBias){
             dst += biasSet(transformIdx);
         }
     }
     else{ //transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT
         Mat m = transforms[transformIdx];
-        transposeProduct(dst,m,src);
+        //transposeProduct(dst,m,src);
+        product(dst,m,src);
         dst += src;
         if(withBias){
             dst += biasSet(transformIdx);
@@ -1776,7 +1752,7 @@
     //capture the target from his index in the training set
     Vec target;
     target.resize(inputSpaceDim);
-    seeTarget(targetIdx, target);
+    seeTrainingPoint(targetIdx, target);
     
     //for each potential neighbor,
     real dist;    
@@ -1785,7 +1761,7 @@
             //computes the distance to the target
             Vec neighbor;
             neighbor.resize(inputSpaceDim);
-            seeNeighbor(i, neighbor);
+            seeTrainingPoint(i, neighbor);
             dist = powdistance(target, neighbor); 
             //if the distance is among &quot;nbNeighbors&quot; smallest distances seen,
             //keep it until to see a closer neighbor. 
@@ -2032,8 +2008,7 @@
     if(biasPeriod &gt; 0 &amp;&amp; stage % biasPeriod == biasOffset)
         MStepBias();
     if(stage % transformsPeriod == transformsOffset)
-        MStepTransformations();
-    
+        MStepTransformations();    
 }
 
 //!maximization step  with respect to  transformation distribution
@@ -2080,6 +2055,11 @@
 
 //!maximization step with respect to transformation parameters
 //!(MAP version)
+/*-Notation: we will use the symbol _T to indicate the transposition operation
+  -To better understand how the algorithm  is working, 
+   see the NOTE (in comments) placed right after the method.
+   (The method is called often and has to be efficient. Some details
+   of the implantation might be a bit unclear for this reason.) */
 void TransformationLearner::MStepTransformations()
 {
     
@@ -2098,10 +2078,10 @@
         //catch the target and neighbor points from the training set
         Vec target;
         target.resize(inputSpaceDim);
-        seeTarget(reconstructionSet[idx].targetIdx, target);
+        seeTrainingPoint(reconstructionSet[idx].targetIdx, target);
         Vec neighbor;
         neighbor.resize(inputSpaceDim);
-        seeNeighbor(reconstructionSet[idx].neighborIdx, neighbor);
+        seeTrainingPoint(reconstructionSet[idx].neighborIdx, neighbor);
         
         int t = reconstructionSet[idx].transformIdx;
         
@@ -2113,17 +2093,64 @@
         if(withBias){
             v = v - biasSet(t);
         }
+        //at the end, we want that matrix C[t] represents
+        //the matrix ( (NeighborPart(t)_T)W(NeighborPart(t)) + lambdaI ) transposed. 
         externalProductScaleAcc(C[t], neighbor, neighbor, p);
         
-        externalProductScaleAcc(B[t], neighbor, v,p); 
+        //at the end, that matrix B[t] represents
+        //the matrix (NeighborPart(t)_T)W(TargetPart(t)) transposed.
+        //externalProductScaleAcc(B[t], neighbor, v,p);
+        externalProductScaleAcc(B[t],v,neighbor,p);
     }
+    
+    TVec&lt;int&gt; pivots(inputSpaceDim);
     for(int t=0; t&lt;nbTransforms; t++){
         addToDiagonal(C[t],lambda);
-        transforms[t] &lt;&lt; solveLinearSystem(C[t], B[t]);  
+        //transforms[t] &lt;&lt; solveLinearSystem(C[t], B[t]);  
+        lapackSolveLinearSystem(C[t],B[t],pivots);
+        transforms[t] &lt;&lt; B[t];
+        
     }  
 }
+/*NOTE : MStepTransformations() 
+ -Notation: we will use the symbol _T to indicate the transposition operation
+ -The algorithm consist in solving a linear system :
+      for each t, we want to find
+                 transforms(t)=X ,
+                 with X such that : E(t)X_T=D(t)
+                 Here, 
+                 E(t) = (NeighborPart(t)_T)W(NeighborPart(t)) + lambda(I)
+                 D(t) = (NeighborPart(t)_T)W(TargetPart(t))
+ -We will compute E(t)_T = C(t) , and D(t)_T =B(t)
+  in the algorithm. It is necessary to compute directly those transposed 
+  versions of E(t) and D(t) to solve the linear system with efficiency. 
+ -once the computations of C(t) and B(t) are done,
+  we use a method from plapack package to solve our linear system 
+    lapackSolveLinearSystem(A_T,B_T, pivots):
+    Here is a copy of the description of the method: 
+ -------------------------------------------------------------------------------------------------------
+   Solves AX = B
+  This is a simple wrapper over the lapack routine. It expects At and Bt (transposes of A and B) as input, 
+  as well as storage for resulting pivots vector of ints of same length as A.
+  The call overwrites Bt, putting the transposed solution Xt in there,
+  and At is also overwritten to contain the factors L and U from the factorization A = P*L*U; 
+  (the unit diagonal elements of L  are  not stored).
+  The lapack status is returned:
+  = 0:  successful exit
+  &lt; 0:  if INFO = -i, the i-th argument had an illegal value
+  &gt; 0:  if INFO = i, U(i,i) is  exactly  zero.   The factorization has been completed, 
+  but the factor U is exactly singular, so the solution could not be computed.
+--------------------------------------------------------------------------------------------------------
+-Like you can see, we have to transmit the transposed versions of matrices
+ E(t) and D(t) to the procedure, that is, matrices C(t) and B(t)
+ -The matrix transforms(t)=X will be stored in B(t) at the end of the algorithm
+*/
+
  
 
+
+
+
 //TODO
 //!maximization step with respect to transformation bias
 //!(MAP version)
@@ -2143,9 +2170,9 @@
         weight = reconstructionSet[idx].weight;
         proba = PROBA_weight(weight);
         target.resize(inputSpaceDim);
-        seeTarget(reconstructionSet[idx].targetIdx,target);
+        seeTrainingPoint(reconstructionSet[idx].targetIdx,target);
         neighbor.resize(inputSpaceDim);
-        seeNeighbor(reconstructionSet[idx].neighborIdx, neighbor);
+        seeTrainingPoint(reconstructionSet[idx].neighborIdx, neighbor);
         reconstruction.resize(inputSpaceDim);
         applyTransformationOn(transformIdx,neighbor, reconstruction);
         newBiasSet(transformIdx) += proba*(target - reconstruction);
@@ -2189,10 +2216,10 @@
 real TransformationLearner::reconstructionEuclideanDistance(int candidateIdx){
     Vec target;
     target.resize(inputSpaceDim);
-    seeTarget(reconstructionSet[candidateIdx].targetIdx, target);
+    seeTrainingPoint(reconstructionSet[candidateIdx].targetIdx, target);
     Vec neighbor;
     neighbor.resize(inputSpaceDim);
-    seeNeighbor(reconstructionSet[candidateIdx].neighborIdx,
+    seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx,
                 neighbor);
     Vec reconstruction;
     reconstruction.resize(inputSpaceDim);

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-09 12:18:20 UTC (rev 7966)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-09 15:29:09 UTC (rev 7967)
@@ -306,7 +306,7 @@
     //#####  PDistribution Member Functions  ##################################
  
 
-    virtual TVec&lt;string&gt; getTrainCostNames() const;
+    // virtual TVec&lt;string&gt; getTrainCostNames() const;
     
     
     //! Return log of probability density log(p(y | x)).
@@ -654,14 +654,12 @@
     //! target (into the variable &quot;targetReconstructionSet&quot; )
     void seeTargetReconstructionSet(int targetIdx,
                                     TVec&lt;ReconstructionCandidate&gt; &amp; targetReconstructionSet)const ;
-    // stores the &quot;targetIdx&quot;th point in the training set into the variable
-    // &quot;target&quot;
-    void seeTarget(const int targetIdx, Vec &amp; target) const ;
-    // stores the &quot;neighborIdx&quot;th input in the training set into the variable
-    // &quot;neighbor&quot; 
-    void seeNeighbor(const int neighborIdx, Vec &amp; neighbor)const;
+ 
 
+    //! stores the &quot;idx&quot;th training data point into the variable 'dst'
+    void seeTrainingPoint(const int idx, Vec &amp; dst) const ;
 
+
     //!GENERATE GAMMA RANDOM VARIABLES
     
     //!source of the algorithm: <A HREF="http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf">http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf</A>


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001414.html">[Plearn-commits] r7966 - trunk/plearn/math
</A></li>
	<LI>Next message: <A HREF="001416.html">[Plearn-commits] r7968 - in branches: . numarray	numarray/python_modules/plearn/plide	numarray/python_modules/plearn/pyplearn
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1415">[ date ]</a>
              <a href="thread.html#1415">[ thread ]</a>
              <a href="subject.html#1415">[ subject ]</a>
              <a href="author.html#1415">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
