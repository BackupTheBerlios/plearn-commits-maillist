<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7924 - in trunk: python_modules/plearn/plotting	python_modules/plearn/var scripts/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-August/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7924%20-%20in%20trunk%3A%20python_modules/plearn/plotting%0A%09python_modules/plearn/var%20scripts/EXPERIMENTAL&In-Reply-To=%3C200708032149.l73Ln2Cs010741%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001371.html">
   <LINK REL="Next"  HREF="001373.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7924 - in trunk: python_modules/plearn/plotting	python_modules/plearn/var scripts/EXPERIMENTAL</H1>
    <B>simonl at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7924%20-%20in%20trunk%3A%20python_modules/plearn/plotting%0A%09python_modules/plearn/var%20scripts/EXPERIMENTAL&In-Reply-To=%3C200708032149.l73Ln2Cs010741%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7924 - in trunk: python_modules/plearn/plotting	python_modules/plearn/var scripts/EXPERIMENTAL">simonl at mail.berlios.de
       </A><BR>
    <I>Fri Aug  3 23:49:02 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001371.html">[Plearn-commits] r7923 - trunk/plearn_learners/regressors
</A></li>
        <LI>Next message: <A HREF="001373.html">[Plearn-commits] r7925 - trunk/plearn/var/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1372">[ date ]</a>
              <a href="thread.html#1372">[ thread ]</a>
              <a href="subject.html#1372">[ subject ]</a>
              <a href="author.html#1372">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: simonl
Date: 2007-08-03 23:49:00 +0200 (Fri, 03 Aug 2007)
New Revision: 7924

Modified:
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/var/Var.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
some functions added in Var.py
some help in deepnetplot.py


Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-08-03 19:30:22 UTC (rev 7923)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-08-03 21:49:00 UTC (rev 7924)
@@ -6,7 +6,7 @@
 ### constants ###
 #################
 
-defaultColorMap = cm.jet
+defaultColorMap = cm.gray#cm.jet
 defaultInterpolation = 'nearest'
 
 ########################
@@ -25,12 +25,14 @@
     M = matrix
     mi = M[0][0]
     ma = M[0][0]
-    for row in M:
-        for el in row:
-            if el &gt; ma:
-                ma = el
-            if el &lt; mi:
-                mi = el
+
+    for i in arange(M.shape[0]):
+        for j in arange(M.shape[1]):
+            if M[i,j] &gt; ma:
+                ma = M[i,j]
+            if M[i,j] &lt; mi:
+                mi = M[i,j]
+    
     return mi,ma
 
 def customColorBar(min, max, (x,y,width,height) = (0.9,0.1,0.1,0.8), nb_ticks = 50., color_map = defaultColorMap):
@@ -91,7 +93,7 @@
             imshow(rowToMatrix(toPlusRow(row),width), interpolation=&quot;nearest&quot;, cmap = colorMap)
             setPlotParams(names[i%2] + &quot;_&quot; + str(i) + &quot;_&quot; + str(j), False, True)
 
-def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows = None):
+def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows = None, index_to_plot = [], names = []):
 
     
     #some calculations for plotting
@@ -121,9 +123,14 @@
     
     x,y = sbi,sbi
     toReturn = -1
-    
-    for i in arange(start,length):
-    
+
+    if index_to_plot == []:
+        index_to_plot = arange(start,start+length)
+    print index_to_plot
+
+    j=0
+    for i in index_to_plot:
+           
         #normal
         row = M[i]
         if apply_to_rows != None:
@@ -134,8 +141,11 @@
         
         axes((x, y, plotWidth, plotHeight))
         imshow(rowToMatrix(row, width), interpolation=&quot;nearest&quot;, cmap = colorMap, vmin = mi, vmax = ma)
-        setPlotParams('row_' + str(i), False, True)
-
+        if names == []:
+            setPlotParams('row_' + str(i), False, True)
+        else:
+            setPlotParams(names[j],False,True)
+        j+=1
         
 
         x = x + plotWidth + sbi
@@ -154,7 +164,7 @@
 
 
 
-def plotMatrices(matrices, names = None, ticks = False, same_color_bar = False, space_between_matrices = 5):
+def plotMatrices(matrices, names = None, ticks = False, same_color_bar = False, space_between_matrices = 5, horizontal=True):
     '''plot matrices from left to right
     TODO : same_color_bar does nothing !!
     '''

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-08-03 19:30:22 UTC (rev 7923)
+++ trunk/python_modules/plearn/var/Var.py	2007-08-03 21:49:00 UTC (rev 7924)
@@ -37,17 +37,34 @@
     
 class Var:
 
-    def __init__(self, l, w=1, random_type=&quot;none&quot;, random_a=0., random_b=1., random_clear_first_row=False, varname=&quot;&quot;):
+    def __init__(self, l, w=1, random_type=&quot;none&quot;, random_a=0., random_b=1., random_clear_first_row=False, varname=&quot;&quot;, min_value = None, max_value = None):
         if isinstance(l, Var):
             self.v = l.v
         elif isinstance(l,int):
-            self.v = pl.SourceVariable(build_length=l,
-                                       build_width=w,
-                                       random_type=random_type,
-                                       random_a=random_a,
-                                       random_b=random_b,
-                                       random_clear_first_row=random_clear_first_row,
-                                       varname=varname)
+            if min_value == None and max_value == None:
+                self.v = pl.SourceVariable(build_length=l,
+                                           build_width=w,
+                                           random_type=random_type,
+                                           random_a=random_a,
+                                           random_b=random_b,
+                                           random_clear_first_row=random_clear_first_row,
+                                           varname=varname)
+            elif min_value == None:
+                pass
+            elif max_value == None:
+                pass
+            else:
+                self.v = pl.SourceVariable(build_length=l,
+                                           build_width=w,
+                                           random_type=random_type,
+                                           random_a=random_a,
+                                           random_b=random_b,
+                                           random_clear_first_row=random_clear_first_row,
+                                           varname=varname,
+                                           min_value = min_value,
+                                           max_value = max_value)
+                
+                
         else: # assume parameter l is a pl. plvar
             self.v = l
 
@@ -127,6 +144,9 @@
     def multiLogSoftMax(self, igs):
         return self.multiMax(igs, 'L')
 
+    def multiSample(self, gs):
+        return Var(pl.MultiSampleVariable(input=self.v, groupsize=gs))
+
     def transposeDoubleProduct(self, W, M):
         return Var(pl.TransposedDoubleProductVariable(varray=[self.v, W, M]))
 
@@ -180,16 +200,23 @@
     cost = reconstr_activation.negCrossEntropySigmoid(input)
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, constrain_mask=False, basename=&quot;&quot;):
+def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, constrain_mask=False, basename=&quot;&quot;, positive=False):
     &quot;&quot;&quot;iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output&quot;&quot;&quot;
+
     ra = 1./max(iw,ow)
     sqra = sqrt(ra)
-    M = Var(ow/ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_M&quot;)
+
+    if positive:
+        W = Var(ogs, iw, &quot;uniform&quot;, 0, sqra, False, varname=basename+&quot;_W&quot;, min_value=0, max_value=1e100)
+        M = Var(ow/ogs, iw, &quot;uniform&quot;, 0, sqra, False, varname=basename+&quot;_M&quot;, min_value=0, max_value=1e100)
+    else:
+        W = Var(ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_W&quot;)
+        M = Var(ow/ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_M&quot;)
+       
     if constrain_mask:
         M = M.sigmoid()
-    W = Var(ogs, iw, &quot;uniform&quot;, -sqra, sqra, False, varname=basename+&quot;_W&quot;)
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
         hidden = input.doubleProduct(W,M).add(b).multiSoftMax(ogs)
@@ -242,32 +269,51 @@
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=&quot;&quot;):
-    ra = 1./max(iw,ow)
-    W = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_W')
+def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=&quot;&quot;, positive=False, stochastic_sample=False):
+    
+    sup = 1./max(iw,ow)
+    if positive:
+        W = Var(ow, iw, &quot;uniform&quot;, 0, sup, varname=basename+'_W', min_value=0, max_value=1e100)
+    else:
+        W = Var(ow, iw, &quot;uniform&quot;, -sup, sup, varname=basename+'_W')
+
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
         hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)
+        if stochastic_sample:
+            hidden = hidden.multiSample(ogs)
         br = Var(1,iw,&quot;fill&quot;,0, varname=basename+'_br')
         log_reconstructed = hidden.matrixProduct(W).add(br).multiLogSoftMax(igs)
     else:        
         hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
+        if stochastic_sample:
+            hidden = hidden.multiSample(ogs)
         log_reconstructed = hidden.matrixProduct(W).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=&quot;&quot;):
+def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=&quot;&quot;, positive=False, stochastic_sample=False):
     ra = 1./max(iw,ow)
-    W = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_W')
-    Wr = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_Wr')
+
+    if positive:
+        W = Var(ow, iw, &quot;uniform&quot;, 0, ra, varname=basename+'_W',min_value=0, max_value=1e100)
+        Wr = Var(ow, iw, &quot;uniform&quot;, 0, ra, varname=basename+'_Wr', min_value=0, max_value=1e100)
+    else :            
+        W = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_W')
+        Wr = Var(ow, iw, &quot;uniform&quot;, -ra, ra, varname=basename+'_Wr')
+        
     if add_bias:
         b = Var(1,ow,&quot;fill&quot;,0, varname=basename+'_b')
         hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)
+        if stochastic_sample:
+            hidden = hidden.multiSample(ogs)
         br = Var(1,iw,&quot;fill&quot;,0, varname=basename+'_br')
         log_reconstructed = hidden.matrixProduct(Wr).add(br).multiLogSoftMax(igs)
     else:
         hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
+        if stochastic_sample:
+            hidden = hidden.multiSample(ogs)
         log_reconstructed = hidden.matrixProduct(Wr).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-08-03 19:30:22 UTC (rev 7923)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-08-03 21:49:00 UTC (rev 7924)
@@ -6,7 +6,8 @@
 from plearn.pyplearn import *
 from plearn.plotting.netplot import *
 from numarray import *
-from numarray.random_array import *
+# from numpy.numarray import *
+#from numarray.random_array import *
 import numpy.random
 
 
@@ -15,13 +16,49 @@
 ################
 
 def print_usage_and_exit():
-    print &quot;Usage: drnPlot &lt;task&gt; &lt;file&gt; [&lt;other arguments&gt;]&quot;,    
-    &quot;drnPlot plotSingleMatrix x.psave &quot;,
-    &quot;drnPlot plotEachRow learner.psave chars.pmat&quot;
-    &quot;drnPlot plotRepAndRec learner.psave chars.pmat&quot;
+    print &quot;Usage :&quot;  
+    print &quot;deepnetplot.py plotSingleMatrix x.psave &quot;
+    print &quot;deepnetplot.py plotEachRow learner.psave chars.pmat&quot;
+    print &quot;deepnetplot.py plotRepAndRec learner.psave chars.pmat&quot;
+    print &quot;deepnetplot.py help&quot;
+    print &quot;&quot;
+    print &quot;where learner.psave is a .psave of a DeepReconstructorNet object&quot;
+    print &quot;and chars.pmat are the training examples&quot;
+    print &quot;&quot;
+    print &quot;exemple:&quot;
+    print &quot;&quot;
+    print &quot;deepnetplot.py plotRepAndRec multimax_tied_small_gs1\=4_ng1\=225/Strat0/Trials0/Split0/final_learner.psave /cluster/pauli/data/mnist/mnist_small/mnist_basic2_train.pmat&quot;
     sys.exit()
 
+def print_usage_repAndRec():
+    print 'putting you mouse OVER a hidden layer an hitting these keyes does... things :'
+    print
+    print 'f : fproping from the last layer'
+    print 'F : fproping form the last layer and for the next ones'
+    print 'r : reconstructing this layer from the next one'
+    print 'R : reconstructing this layer from the next one and also the previous ones'
+    print 'm : set the max of each group of the current hidden layer to 1 and the other elements of the group to 0'
+    print 's : each group of the current layer is sampled (each group has to sum to 1.0)'
+    print 'S : samples the current layer, then reconstruct the previous layer, thant sample this reconstructed layer, and continues until the input'
+    print 'z : set the current pixel (the one the mouse is over) to 0.0'
+    print 'x : set the current pixel to 0.25'
+    print 'c : set the current pixel to 0.5'
+    print 'v : set the current pixel to 0.75'
+    print 'b : set the current pixel to 1.0'
+    print ' space-bar : print value and position of the current pixel'
+    print 'i : print values of the current layer'
+    print 'w : plot the weight matrices associated with the current pixel'
+    print 'W : same as w but for all a group'
+    print 'C : same as w but for the hidden unit that has the highest value in each group'
+    print 'o : set the current hidden layer to its original state'
+    print 'O : same as o but for every layer'
+    print 'right arrow : prints the next character in the dataset and its corresponding hidden layers and reconstructions'
+    print 'left arrow :same but for previous character'
+    print '0,1,2,3...9 : after having pressed a digit, right and left arrows will only find this digit'
+    print '. : cancel the effec of pressing a digit'
+    print 'Control + clic : plots the clicked hidden layer in a new figure'
 
+
 def appendMatrixToFile(file, matrix, matrix_name=&quot;&quot;):
     file.write(&quot;\n\n&quot; + matrix_name + ' ('+ str(len(matrix)) + 'x' + str(len(matrix[0])) + ')\n\n')
     for i, row in enumerate(matrix):
@@ -32,11 +69,13 @@
         
 
 class HiddenLayer:
+    '''represents a hidden layer which elements are divised in groups
+       and we have some methods to be able to plot it'''
     
     def __init__(self,hidden_Layer, groupsize):
         self.hidden_layer = hidden_Layer
         self.groupsize = groupsize
-
+        
         self.max_height = 150
 
         #if it's too tall, we do this little tweak
@@ -56,7 +95,7 @@
     def matrixToLayer(self, x, y):
         gs = self.groupsize
         x,y = self.correctXY(x,y)
-        return x + gs*self.nbgroups*y
+        return x + gs*self.nbgroups*y        
 
     def correctXY(self,x,y):
         return int(x + .4), int(y + .3)        
@@ -77,10 +116,12 @@
         for i in arange(n*self.groupsize,(n+1)*self.groupsize):
            self.hidden_layer[i] = row[c]
            c+=1
+    def getNGroups(self):
+        return self.hidden_layer.size()/self.groupsize
 
 
-
 class InteractiveRepRecPlotter:
+    '''this class is used to plot representations and reconstructions of a DeepReconstructorNet'''
     
     def __init__(self, learner, vmat, image_width=28, char_indice=0):
         '''constructor'''
@@ -105,7 +146,6 @@
 
         self.plotNext()#starting with a plot...
         self.__linkEvents()
-               
 
 
     def size(self):
@@ -122,7 +162,7 @@
         self.input = row[:-1]
 
     def __getNextChar(self):
-        '''get next input row from the vmat'''        
+        '''get next input row from the vmat'''
         while True:
             self.current+=1
             raw_input = vmat.getRow(self.current)
@@ -132,7 +172,7 @@
         self.__rowToClassInput(raw_input)
 
     def __getPrevChar(self):
-        '''get next input row from the vmat'''        
+        '''get next input row from the vmat'''
         while True:
             self.current-=1
             raw_input = vmat.getRow(self.current)
@@ -209,7 +249,7 @@
 
 
     def __repCommands(self,event):
-
+        
         #met a jour self.current_hl
         self.__findCurrentLayer(event)
         
@@ -258,35 +298,27 @@
             # reconstruction -- r
                 
             elif char == 'r':
-                print 'reconstructing...'
-                self.learner.setMatValue(i+1, reshape(hl2.hidden_layer, (1,-1)))
-                #reconstruct
-                row = self.learner.reconstructOneLayer(i+1)[0]
-                #HACK
-                if len(row) == 28*28*2:
-                    print 'hacking...'
-                    row = array(toMinusRow(row))
-                #print the new layer
-                hl1.hidden_layer = row
-                self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)                 
-                draw()
-                print '...done'
+                if i&lt;self.size()-2:
+                    print 'reconstructing...'
+                    self.__reconstructLayer(i)
+                    self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)                 
+                    draw()
+                    print '...done'
+                else:
+                    print 'invalid layer for reconstruction'
 
             # big-reconstruction -- R
 
             elif char == 'R':
-                print 'big-reconstructing...'
-                for k in arange(i+1, 0, -1):
-                    self.learner.setMatValue(k, reshape(self.hidden_layers[k].hidden_layer, (1,-1)))
-                    row = self.learner.reconstructOneLayer(k)[0]
-                    #HACK
-                    if len(row) == 28*28*2:
-                        print 'hacking...'
-                        row = array(toMinusRow(row))
-                    self.hidden_layers[k-1].hidden_layer = row
-                    self.rep_axes[k-1].imshow(self.hidden_layers[k-1].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                if i&lt;self.size()-2:
+                    print 'big-reconstructing...'
+                    for k in arange(i, -1, -1):
+                        self.__reconstructLayer(k)
+                        self.rep_axes[k].imshow(self.hidden_layers[k].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                     draw()
-                print '...done'                
+                    print '...done'
+                else:
+                    print 'invalid layer for reconstruction'
 
             # max -- m
 
@@ -321,13 +353,22 @@
 
             elif char == 's':
                 print 'sampling...'
-                for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
-                    print 'sum', hl.getRow(n).sum()
-                    multi = numpy.random.multinomial(1,hl.getRow(n))                    
-                    hl.setRow(n,multi)                            
+                self.__sampleLayer(i)                
                 self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                 draw()
                 print '...done'
+
+            # sampling + reconstructin -- S
+
+            elif char == 'S':
+                print 'sampling and reconstruction...'
+                for n in arange(i,0,-1):
+                    self.__sampleLayer(n)
+                    self.__reconstructLayer(n-1)
+                for n in arange(i+1):
+                    self.rep_axes[n].imshow(self.hidden_layers[n].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                print '...done'
                 
             # set pixel -- z,x,c,v,b
 
@@ -355,8 +396,13 @@
                 x,y = event.xdata, event.ydata
                 print
                 print 'Position', hl.matrixToLayer(x,y), '(', x, y, ')'
-                print 'Value', hl.getElement(x,y)                
+                print 'Value', hl.getElement(x,y)
 
+            # more infos -- 'i'
+
+            elif char == 'i':
+                print hl.getMatrix()                
+
             # plot W and M -- w
 
             elif char == 'w':
@@ -375,7 +421,8 @@
                 namesToPlot = []
 
                 x,y = hl.correctXY(event.xdata,event.ydata)
-                n = hl.matrixToLayer(x, y)
+                #n = hl.matrixToLayer(x, y)
+                n = hl.matrixToLayer(event.xdata, event.ydata)               
                 
                 
                 if nameW in listNames and nameM not in listNames:
@@ -405,14 +452,15 @@
                         namesToPlot.append(nameWr)
 
                     figure(3)
+                    
                     clf()
                     plotMatrices(matricesToPlot, namesToPlot)
                     draw()
 
                 if nameW in listNames and nameM in listNames:
 
-                    rowW = learner.getParameterRow(nameW,x)
-                    rowM = learner.getParameterRow(nameM,y)
+                    rowW = learner.getParameterRow(nameW,n%hl.groupsize)
+                    rowM = learner.getParameterRow(nameM,int(n/hl.groupsize))
                     
                     #HACK !!!
                     #print 'just hacked...'
@@ -420,16 +468,21 @@
                     #    row = array(toMinusRow(row))
                     #END OF HACK
                                       
-                    rowW = reshape(rowW, (-1,self.hidden_layers[i-1].groupsize))
-                    rowM = reshape(rowM,(-1,self.hidden_layers[i-1].groupsize))
+                    matW1 = reshape(toMinusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
+                    matM1 = reshape(toMinusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
+                    matW2 = reshape(toPlusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
+                    matM2 = reshape(toPlusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
 
                     #TODO: rajouter les deux cas  : juste un Wr et lautre : Mr ET Wr
+                    produit = matW1*matM1
+                    produit2 = matW2*matM2
 
-                    produit = rowW*rowM                   
-
+                    
                     figure(3)
+                    ioff()
                     clf()                  
-                    plotMatrices([rowW,rowM,produit], [nameW,nameM, 'term-to-term product'])
+                    plotMatrices([matW1,matM1,matW2,matM2,produit,produit2], [nameW + &quot;-&quot;,nameM+&quot;-&quot;, nameW + &quot;+&quot;,nameM+&quot;+&quot;,'term-to-term product (-)', 'term-to-term product (+)'])
+                    ion()
                     draw()
 
                 #BIAS
@@ -464,8 +517,168 @@
                     clf()
                     plotMatrices(matricesToPlot, namesToPlot)
                     draw()
+            
+            #like 'w' but for an entire row -- 'W'
+            elif char == 'W':
+
+                prefixe = 'Layer' + str(i)
+                nameW = prefixe + '_W'
+                nameWr = nameW + 'r'
+                nameM = prefixe + '_M'
+                nameMr = nameM + 'r'
+                nameB = prefixe + '_b'
+                nameBr = nameB + 'r'
+
+                listNames = learner.listParameterNames()
+
+                matricesToPlot = []
+                namesToPlot = []                
+
+                print 'x,y=', event.xdata, event.ydata
+                n = hl.matrixToLayer(event.xdata, event.ydata)
+                print 'n =',n
+            
                 
+                #                 if nameW in listNames and nameM not in listNames:
+                
+                #                     row_list = []
+                #                     for j in arange(n,n+hl.groupsize):
+                #                         row_list.append(learner.getParameterRow(nameW,j))
+                #                     print row_list
+                    
+                #                     #HACK !!!
+                #                     for row in row_list:
+                #                         if len(row) == 28*28*2:
+                #                             row = array(toMinusRow(list(row)))
+                #                             print 'just hacked...'
+                #                     #END OF HACK
+                
+                #                     for j,row in enumerate(row_list):
+                #                         matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                #                         namesToPlot.append(nameW + '_' + str(n+j))
+                
+                #                     if nameWr in listNames:
+                
+                #                         row = learner.getParameterRow(nameWr,n)
+                
+                #                         #HACK !!!
+                #                         print 'just hacked...'
+                #                         if i==1 and len(row) == 28*28*2:
+                #                             row = array(toMinusRow(list(row)))
+                #                         #END OF HACK 
+                
+                #                         matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                #                         namesToPlot.append(nameWr)
 
+                if nameW in listNames and nameM not in listNames:                
+
+                    n = n - n%hl.groupsize
+                    print 'plotting weigths from',n,'to',n+hl.groupsize-1
+                    print learner.getParameterValue(nameW).shape
+                   
+                    figure(3)
+                    ioff()                    
+                    clf()
+                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.01, toMinusRow)
+                    ion()
+                    draw()
+                    
+                #if nameM in listNames and nameM in listNames:
+                    
+                   # index_w = n%hl.groupsize
+                   # index_m = int(n/hl.groupsize)#
+
+#                    figure(3)
+#                    ioff()                    
+#                    clf()
+#                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.01, toMinusRow)
+#                    ion()
+#                    draw()
+#                    
+#                    figure(4)
+#                    ioff()
+#                    clf()
+#                    plotLayer1(learner.getParameterValue(nameM), 28, .1, n, hl.groupsize,.01, toMinusRow)
+#                    ion()
+#                    draw()
+                    
+                #  if nameW in listNames and nameM in listNames:
+                
+                #                     rowW = learner.getParameterRow(nameW,x)
+                #                     rowM = learner.getParameterRow(nameM,y)
+                
+                #                     #HACK !!!
+                #                     #print 'just hacked...'
+                #                     #if i==1 and len(row) == 28*28*2:
+                #                     #    row = array(toMinusRow(row))
+                #                     #END OF HACK
+                
+                #                     rowW = reshape(rowW, (-1,self.hidden_layers[i-1].groupsize))
+                #                     rowM = reshape(rowM,(-1,self.hidden_layers[i-1].groupsize))
+                
+                #                     #TODO: rajouter les deux cas  : juste un Wr et lautre : Mr ET Wr
+                
+                #                     produit = rowW*rowM                   
+                
+                #                     figure(3)
+                
+                #                     clf()                  
+                #                     plotMatrices([rowW,rowM,produit], [nameW,nameM, 'term-to-term product'])
+                #                     draw()
+
+
+            # like 'w' on the max of each row -- 'C'
+
+            elif char == 'C':
+
+                prefixe = 'Layer' + str(i)
+                nameW = prefixe + '_W'
+                nameWr = nameW + 'r'
+                nameM = prefixe + '_M'
+                nameMr = nameM + 'r'
+                nameB = prefixe + '_b'
+                nameBr = nameB + 'r'
+                
+                listNames = learner.listParameterNames()
+                
+                matricesToPlot = []
+                namesToPlot = []
+                
+                print event.xdata,event.ydata
+                
+                indexes = []
+                names = []
+                for j in arange(hl.getNGroups()):
+                    row = hl.getRow(j)
+                    index = 0
+                    for k,el in enumerate(row):
+                        if el &gt; row[index]:
+                            index = k
+
+                    indexes.append(j*hl.groupsize + index)
+                    names.append(str(row[index])[0:8])
+                
+                
+                if nameW in listNames:# and nameM not in listNames:                
+                   
+                    figure(3)
+                    ioff()                    
+                    clf()
+                    plotLayer1(learner.getParameterValue(nameW), 28, .056, 0,0,.01, toMinusRow, indexes,names)
+                    ion()
+                    draw()
+                    
+                if nameM in listNames:
+                    
+                    figure(4)
+                    ioff()
+                    clf()
+                    plotLayer1(learner.getParameterValue(nameM), 28, .05,0,0,.01,toMinusRow,indexes,names)
+                    ion()
+                    draw()
+                    
+                    
+
             # back to Original -- o
             
             elif char == 'o':
@@ -474,6 +687,9 @@
                 self.rep_axes[i].imshow(self.hidden_layers[i].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                 draw()
                 print '...done'
+
+            else :
+                print_usage_repAndRec()
                 
        
 
@@ -485,28 +701,36 @@
                 self.rep_axes[k].imshow(self.hidden_layers[k].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                 draw()
             print '...done'
-        
 
 
+
     def __clicked(self, event):
             
         self.__findCurrentLayer(event)
         
-        print 'current hidden layer is now', self.current_hl
+        if event.key == 'control':
+            print 'control-clic performed'
 
-        if event.key == 'control' and self.current_hl != -1:
+            data = self.hidden_layers[self.current_hl].getMatrix()
+            x,y,s,c = [],[],[],[]
+            for i in arange(data.shape[0]):
+                for j in arange(data.shape[1]):
+                    x.append(j)
+                    y.append(data.shape[1]-i)
+                    s.append(data[i,j]*25.)
+                    if data[i,j]&lt;0 :
+                        c.append(1)
+                    else:
+                        c.append(-1)
+            f = figure(5)
             
-            hidden_layer = self.hidden_layers[self.current_hl]
-            
-            n = hidden_layer.matrixToLayer(x,y)
-           # print 'n', n
-            hidden_layer.hidden_layer[n] = 1 - hidden_layer.hidden_layer[n]
-            axes.imshow(hidden_layer.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)        
-            draw()
+            clf()
+            scatter(x,y,s=s, c=c, marker='s',cmap = cm.gray, norm = Normalize())
+            f.gca().set_axis_bgcolor(&quot;0.75&quot;)
+            colorbar()
+            draw()            
+        
 
-        if event.button == 3:
-            print 'Layer', self.current_hl, ', position (',x,y,'), value', xself.hidden_layers[self.current_hl].getElement(x,y)
-
     def __findCurrentLayer(self, event):
 
         fig = event.canvas.figure.number
@@ -516,6 +740,7 @@
         if axes != None:
             figure(fig)
             
+            
             #we find to which hidden layer corresponds our axes
             for i,a in enumerate(self.rep_axes):                
                 if a == axes:
@@ -525,11 +750,13 @@
     def __linkEvents(self):
 
         figure(self.fig_rep)
+        
         connect('key_press_event', self.__changeChar)
-        #connect('button_press_event', self.__clicked)
+        connect('button_press_event', self.__clicked)
         connect('key_press_event', self.__repCommands)
 
         figure(self.fig_rec)
+        
         connect('key_press_event', self.__changeChar)        
         #connect('button_press_event', self.__clicked)        
         
@@ -538,23 +765,27 @@
     ###
 
     def __plotReconstructions(self):
-        print 'plotting reconstructions...'
+        print 'plotting reconstructions...'        
         figure(self.fig_rec)
+        ioff()
         clf()
         plotMatrices(self.reconstructions)
+        ion()
         draw()
         print '...done.'
 
     def __plotRepresentations(self):
         print 'plotting representations...'
+        ioff()
         figure(self.fig_rep)
         clf()
         temp = []
         for x in self.hidden_layers:
             temp.append( x.getMatrix() )        
+        #draw()
+        self.rep_axes = plotMatrices(temp)        
         draw()
-
-        self.rep_axes = plotMatrices(temp)
+        ion()
         print '...done.'
 
     def __computeAndPlot(self):        
@@ -639,9 +870,32 @@
             file.write(&quot;\n\n\n---------- REC ------------------------------------------------------\n&quot;)            
             for i, mat in enumerate(rec):
                 appendMatrixToFile(file,  rowToMatrix(mat[0],28), 'rec of hidden layer ' + str(i+1))
-            
 
+    ###
+    ### utils
+    ###
 
+    def __sampleLayer(self, which_layer):
+        hl = self.hidden_layers[which_layer]
+        for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
+            multi = numpy.random.multinomial(1,hl.getRow(n))                    
+            hl.setRow(n,multi)
+
+    def __reconstructLayer(self, which_layer):
+        hl = self.hidden_layers[which_layer+1]
+        
+        self.learner.setMatValue(which_layer+1, reshape(hl.hidden_layer, (1,-1)))
+        #reconstruct
+        row = self.learner.reconstructOneLayer(which_layer+1)[0]
+        #HACK
+        if len(row) == 28*28*2:
+            print 'hacking...'
+            row = array(toMinusRow(row))
+            #print the new layer
+        self.hidden_layers[which_layer].hidden_layer = row
+    
+
+
 class EachRowPlotter:
     
     def __init__(self, matrix, width = 28, plot_width = .1, space_between_images = .01, do_to_rows = None, i=0, nofig=0):
@@ -703,7 +957,7 @@
     names = learner.listParameterNames()
     
     #doToRow = None
-    doToRow = toPlusRow
+    doToRow = toMinusRow
     #doToRow = toMinusRow
 
     matrixName = ''
@@ -780,10 +1034,18 @@
 
 elif task == 'test':
     #jutilise cet endroit pour faire des tests
-
     pass
+elif task == 'help':
+    print 'plotRepAndRec:'
+    print
+    print_usage_repAndRec()
 
 
 
 else:
     print_usage_and_exit()
+
+
+
+
+    


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001371.html">[Plearn-commits] r7923 - trunk/plearn_learners/regressors
</A></li>
	<LI>Next message: <A HREF="001373.html">[Plearn-commits] r7925 - trunk/plearn/var/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1372">[ date ]</a>
              <a href="thread.html#1372">[ thread ]</a>
              <a href="subject.html#1372">[ subject ]</a>
              <a href="author.html#1372">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
