<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r7917 - trunk/python_modules/plearn/learners/autolr
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-August/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7917%20-%20trunk/python_modules/plearn/learners/autolr&In-Reply-To=%3C200708031454.l73EsmQ1000826%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001364.html">
   <LINK REL="Next"  HREF="001366.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r7917 - trunk/python_modules/plearn/learners/autolr</H1>
    <B>nouiz at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r7917%20-%20trunk/python_modules/plearn/learners/autolr&In-Reply-To=%3C200708031454.l73EsmQ1000826%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r7917 - trunk/python_modules/plearn/learners/autolr">nouiz at mail.berlios.de
       </A><BR>
    <I>Fri Aug  3 16:54:48 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="001364.html">[Plearn-commits] r7916 - trunk/plearn/io
</A></li>
        <LI>Next message: <A HREF="001366.html">[Plearn-commits] r7918 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1365">[ date ]</a>
              <a href="thread.html#1365">[ thread ]</a>
              <a href="subject.html#1365">[ subject ]</a>
              <a href="author.html#1365">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: nouiz
Date: 2007-08-03 16:54:47 +0200 (Fri, 03 Aug 2007)
New Revision: 7917

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
-Better removing of active candidate\n-Better printing\n-Corrected bug

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-02 23:04:02 UTC (rev 7916)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-03 14:54:47 UTC (rev 7917)
@@ -388,8 +388,12 @@
     if plearn.bridgemode.useserver:
         servers[0][1] = 1 # learner
 
-    # although 1 is probably too small
-    min_epochs_to_delete = max(1, min_epochs_to_delete)
+    train_costnames = learner.getTrainCostNames()
+    if selected_costnames is not None:
+        # Filter out unwanted costnames
+        train_costnames = [ name for name in train_costnames
+                            if name in selected_costnames ]
+    n_train_costs = ifthenelse(get_train_costs,len(train_costnames),0)
 
     def error_curve(active,start_t,current_t):
         delta_t = current_t+1-start_t
@@ -398,29 +402,25 @@
         # cost number 'cost' (index in the selected_costnames list),
         # in testset 'test'.
         # And testset 0 is the one used for selection.
-        return all_results[active][start_t:current_t+1,2+cost_to_select_best]
+        return all_results[active][start_t:current_t+1,
+                                   2+cost_to_select_best+n_train_costs]
 
     def error_curve_dominates(c1,c2,t):
         &quot;&quot;&quot;curve1 has a lower last error than curve2, but will curve2
         eventually cross curve1? if yes return False o/w return True&quot;&quot;&quot;
 
-        start_t = max(all_start[c1],all_start[c2])
-        curve1 = error_curve(c1,start_t,t)
-        curve2 = error_curve(c2,start_t,t)
-        if curve1.shape[0]-1&lt;min_epochs_to_delete or curve1[-1]&gt;=curve2[-1]:
-            return False
-        slope1=curve1[-1]-curve1[-2]
-        slope2=curve2[-1]-curve2[-2]
-        if  slope1 &gt;= slope2:
-            return False
-
         # check to see if c2 is alone with its learning rate (or nearby);
         # if yes keep it
         alone=True
         c2lr=all_lr[c2]
         c2err=all_last_err[c2]
+        start_t = max(all_start[c1],all_start[c2])
+        curve1 = error_curve(c1,start_t,t)#[valid_error]
+        curve2 = error_curve(c2,start_t,t)
+        if curve2.shape[0]-1&lt;min_epochs_to_delete:
+            return False
         for a in actives:
-            if all_lr[a]==c2lr and all_last_err[a]&lt;=c2err:
+            if a!=c2 and all_lr[a]==c2lr and all_last_err[a]&lt;=c2err:
                 # throw it away if worse than other actives of same lr
                 return True
 
@@ -428,24 +428,29 @@
             # and greater lr
             if a!=c2 and all_lr[a]&gt;c2lr and abs(log(all_lr[a]/c2lr))&lt;keep_lr*log(lr_steps):
                 alone=False
+
+        if curve1[-1]&gt;=curve2[-1]:
+            return False
+        slope1=curve1[-1]-curve1[-2]
+        slope2=curve2[-1]-curve2[-2]
+        if  slope1 &gt;= slope2 or all_last_err[c1] &gt;= c2err:
+            return False
+
         c1lr=all_lr[c1]
         if alone and c2lr&gt;c1lr: # and slope2&lt;0:
             # keep if alone and a larger learning rate and improving
             return False
         return True
 
-    train_costnames = learner.getTrainCostNames()
+    # although 1 is probably too small
+    min_epochs_to_delete = max(1, min_epochs_to_delete)
+
     test_costnames = getTestCostNames(learner)
+    n_tests = len(testsets)
+    #n_costs = len(cost_indices)
     if selected_costnames is not None:
-        # Filter out unwanted costnames
-        train_costnames = [ name for name in train_costnames
-                            if name in selected_costnames ]
         test_costnames = [ name for name in test_costnames
                            if name in selected_costnames ]
-
-    n_tests = len(testsets)
-    #n_costs = len(cost_indices)
-    n_train_costs = ifthenelse(get_train_costs,len(train_costnames),0)
     n_test_costs = len(test_costnames)
 
     if schedules:
@@ -488,8 +493,9 @@
     for t in range(n_train):
         stage=stages[t]
         if logfile:
-            print &gt;&gt;logfile, &quot;At stage &quot;, stage
-        print &quot;actives now: &quot;,actives, &quot; with lr=&quot;, array(all_lr)[actives]
+            print &gt;&gt;logfile, &quot;After &quot;, stage, &quot;stage&quot;
+        print &quot;After&quot;,stage,&quot;stages, actives now: &quot;,actives, &quot; with lr=&quot;, array(all_lr)[actives]
+        print &quot;current best actives:&quot;,best_active,&quot;best_error:&quot;,all_last_err[best_active],&quot;lr:&quot;,all_lr[best_active]
         print &gt;&gt;logfile, &quot;actives now: &quot;,actives, &quot; with lr=&quot;, array(all_lr)[actives]
 
         threads = []
@@ -591,7 +597,7 @@
         if previous_best_err &gt;= best_err:
             previous_best_err = best_err
             if logfile:
-                print &gt;&gt;logfile,&quot;BEST to now is candidate &quot;,best_active,&quot; with err=&quot;,best_err
+                print &gt;&gt;logfile,&quot;BEST to now is candidate &quot;,best_active,&quot; with err=&quot;,best_err,&quot;and lr=&quot;,all_lr[best_active]
                 print &gt;&gt;logfile, &quot;stage\tl.rate\t&quot;,
                 if get_train_costs:
                     for costname in train_costnames:
@@ -609,7 +615,7 @@
         else:
             if logfile:
                 print &gt;&gt;logfile, &quot;THE BEST ACTIVE HAS GOTTEN WORSE!!!!&quot;
-        if s%(epoch*nskip)==0 and s&lt;nstages:
+        if learner.stage%(epoch*nskip)==0 and learner.stage&lt;nstages:
             best_active = actives[argmin(array(all_last_err)[actives])]
             # remove candidates that are worse and have higher slope
             best_last = all_last_err[best_active]
@@ -638,7 +644,7 @@
                 all_last_err.append(best_last)
                 all_start.append(t)
                 if logfile:
-                    print &gt;&gt;logfile,&quot;CREATE candidate &quot;, new_a, &quot; from &quot;,best_active,&quot;at epoch &quot;,s,&quot; with lr=&quot;,all_lr[new_a]
+                    print &gt;&gt;logfile,&quot;CREATE candidate &quot;, new_a, &quot; from &quot;,best_active,&quot;at stage &quot;,learner.stage,&quot; with lr=&quot;,all_lr[new_a]
                     logfile.flush()
     if return_best_model:
         final_model = best_candidate


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001364.html">[Plearn-commits] r7916 - trunk/plearn/io
</A></li>
	<LI>Next message: <A HREF="001366.html">[Plearn-commits] r7918 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1365">[ date ]</a>
              <a href="thread.html#1365">[ thread ]</a>
              <a href="subject.html#1365">[ subject ]</a>
              <a href="author.html#1365">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
