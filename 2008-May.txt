From saintmlx at mail.berlios.de  Thu May  1 20:03:56 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 1 May 2008 20:03:56 +0200
Subject: [Plearn-commits] r8922 - trunk/plearn_learners/classifiers
Message-ID: <200805011803.m41I3uj9019126@sheep.berlios.de>

Author: saintmlx
Date: 2008-05-01 20:03:51 +0200 (Thu, 01 May 2008)
New Revision: 8922

Modified:
   trunk/plearn_learners/classifiers/KNNClassifier.cc
Log:
- fixed error msg (varargs w/ wrong type & size!)



Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-04-30 15:53:15 UTC (rev 8921)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-05-01 18:03:51 UTC (rev 8922)
@@ -246,7 +246,7 @@
         int nn_class = int(*output_data++);
         if (nn_class < 0 || nn_class >= nclasses)
             PLERROR("KNNClassifier::computeOutput: expected the class to be between 0 "
-                    "and %d but found %f", nclasses-1, nn_class);
+                    "and %d but found %d", nclasses-1, nn_class);
         w *= *output_data++;                     //!< account for training weight
         PLASSERT( w >= 0.0 );
         class_weights[nn_class] += w;



From saintmlx at mail.berlios.de  Thu May  1 20:04:51 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 1 May 2008 20:04:51 +0200
Subject: [Plearn-commits] r8923 - trunk/plearn/vmat
Message-ID: <200805011804.m41I4ptn019194@sheep.berlios.de>

Author: saintmlx
Date: 2008-05-01 20:04:50 +0200 (Thu, 01 May 2008)
New Revision: 8923

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
- declare new remote methods



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-05-01 18:03:51 UTC (rev 8922)
+++ trunk/plearn/vmat/VMatrix.cc	2008-05-01 18:04:50 UTC (rev 8923)
@@ -284,6 +284,24 @@
          ArgDoc ("weightsize", "weightsize"),
          ArgDoc ("extrasize", "extrasize")));
 
+    declareMethod(
+        rmm, "addStringMapping", static_cast<void (VMatrix::*)(int, string, real)>(&VMatrix::addStringMapping),
+        (BodyDoc("Add or replace a string mapping for a column\n"),
+         ArgDoc ("col", "column number"),
+         ArgDoc ("str", "string value"),
+         ArgDoc ("val", "numeric value")));
+
+    declareMethod(
+        rmm, "getStringToRealMapping", &VMatrix::getStringToRealMapping,
+        (BodyDoc("Get the string->real mapping for a given column.\n"),
+         ArgDoc ("col", "column number"),
+         RetDoc ("map of string->real")));
+
+    declareMethod(
+        rmm, "getRealToStringMapping", &VMatrix::getRealToStringMapping,
+        (BodyDoc("Get the real->string mapping for a given column.\n"),
+         ArgDoc ("col", "column number"),
+         RetDoc ("map of real->string")));
 }
 
 



From nouiz at mail.berlios.de  Thu May  1 21:29:46 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 1 May 2008 21:29:46 +0200
Subject: [Plearn-commits] r8924 - trunk/python_modules/plearn/parallel
Message-ID: <200805011929.m41JTkG3026749@sheep.berlios.de>

Author: nouiz
Date: 2008-05-01 21:29:46 +0200 (Thu, 01 May 2008)
New Revision: 8924

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
put full path to binary


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-01 18:04:50 UTC (rev 8923)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-01 19:29:46 UTC (rev 8924)
@@ -872,7 +872,7 @@
                 if source_file:
                     launch_dat.write('source ' + source_file + '\n')
                 launch_dat.write(dedent('''\
-                    echo "Executing on " `hostname` 1>&2
+                    echo "Executing on " `/bin/hostname` 1>&2
                     echo "HOSTNAME: ${HOSTNAME}" 1>&2
                     echo "PATH: $PATH" 1>&2
                     echo "PYTHONPATH: $PYTHONPATH" 1>&2
@@ -891,7 +891,7 @@
                 if source_file:
                     launch_dat.write('source ' + source_file + '\n')
                 launch_dat.write(dedent('''\
-                    echo "Executing on " `hostname`
+                    echo "Executing on " `/bin/hostname`
                     echo "HOSTNAME: ${HOSTNAME}"
                     echo "PATH: $PATH"
                     echo "PYTHONPATH: $PYTHONPATH"



From chapados at mail.berlios.de  Thu May  1 23:57:15 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 1 May 2008 23:57:15 +0200
Subject: [Plearn-commits] r8925 - trunk/plearn/vmat
Message-ID: <200805012157.m41LvFr9016364@sheep.berlios.de>

Author: chapados
Date: 2008-05-01 23:57:14 +0200 (Thu, 01 May 2008)
New Revision: 8925

Modified:
   trunk/plearn/vmat/VMatrix.h
Log:
Brought back TMat_maths include -- too much code fail to compile.  Please advise before implementing global changes such as this one.  People's time on this planet is limited.

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-05-01 19:29:46 UTC (rev 8924)
+++ trunk/plearn/vmat/VMatrix.h	2008-05-01 21:57:14 UTC (rev 8925)
@@ -50,6 +50,7 @@
 #include <plearn/math/StatsCollector.h>
 #include "VMField.h"
 #include <plearn/dict/Dictionary.h>
+#include <plearn/math/TMat_maths.h>
 #include <plearn/io/PPath.h>
 
 #include <map>



From chapados at mail.berlios.de  Fri May  2 00:20:29 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 2 May 2008 00:20:29 +0200
Subject: [Plearn-commits] r8926 - trunk/plearn/vmat
Message-ID: <200805012220.m41MKT15019226@sheep.berlios.de>

Author: chapados
Date: 2008-05-02 00:20:28 +0200 (Fri, 02 May 2008)
New Revision: 8926

Modified:
   trunk/plearn/vmat/VMatrix.h
Log:
Brought back nouiz changes

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-05-01 21:57:14 UTC (rev 8925)
+++ trunk/plearn/vmat/VMatrix.h	2008-05-01 22:20:28 UTC (rev 8926)
@@ -50,7 +50,6 @@
 #include <plearn/math/StatsCollector.h>
 #include "VMField.h"
 #include <plearn/dict/Dictionary.h>
-#include <plearn/math/TMat_maths.h>
 #include <plearn/io/PPath.h>
 
 #include <map>



From larocheh at mail.berlios.de  Fri May  2 22:52:17 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 2 May 2008 22:52:17 +0200
Subject: [Plearn-commits] r8927 - trunk/plearn_learners/online
Message-ID: <200805022052.m42KqH4Y012298@sheep.berlios.de>

Author: larocheh
Date: 2008-05-02 22:52:16 +0200 (Fri, 02 May 2008)
New Revision: 8927

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
- Corrected bug with partial_costs stats
- Added an option to only mask input layer when doing unsupervised fine-tuning



Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-05-01 22:20:28 UTC (rev 8926)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-05-02 20:52:16 UTC (rev 8927)
@@ -32,7 +32,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Pascal Lamblin
+// Authors: Hugo Larochelle
 
 /*! \file StackedAutoassociatorsNet.cc */
 
@@ -66,6 +66,7 @@
     unsupervised_nstages( 0 ),
     unsupervised_fine_tuning_learning_rate( 0. ),
     unsupervised_fine_tuning_decrease_ct( 0. ),
+    mask_input_layer_only_in_unsupervised_fine_tuning( false ),
     n_layers( 0 ),
     unsupervised_stage( 0 ),
     currently_trained_layer( 0 )
@@ -231,6 +232,12 @@
                   "The decrease constant of the learning rate used during\n"
                   "unsupervised fine tuning gradient descent.\n");
 
+    declareOption(ol, "mask_input_layer_only_in_unsupervised_fine_tuning", 
+                  &StackedAutoassociatorsNet::mask_input_layer_only_in_unsupervised_fine_tuning,
+                  OptionBase::buildoption,
+                  "Indication that only the input layer should be masked\n"
+                  "during unsupervised fine-tuning.\n");
+
     declareOption(ol, "greedy_stages", 
                   &StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -597,7 +604,7 @@
                         "partial_costs[%i] should have an input_size of %d.\n", 
                         i,layers[i+1]->size);
             if(i==0)
-                partial_costs_positions[i] = n_layers;
+                partial_costs_positions[i] = n_layers-1;
             else
                 partial_costs_positions[i] = partial_costs_positions[i-1]
                     + partial_costs[i-1]->name().length();
@@ -1086,7 +1093,7 @@
                                              expectation_gradients[ index + 1 ]
                                              );
 
-        train_costs.subVec(partial_costs_positions[index],
+        train_costs.subVec(partial_costs_positions[index]+1,
                            partial_cost_value.length()) << partial_cost_value;
 
         if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
@@ -1311,8 +1318,9 @@
         for( int i=0 ; i<n_layers-1; i++ )
         {
             masked_autoassociator_expectations[i] << expectations[i];
-            for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
-                masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
+            if( !(mask_input_layer_only_in_unsupervised_fine_tuning && i > 0) )
+                for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
+                    masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
             
             connections[i]->fprop( masked_autoassociator_expectations[i], 
                                    activations[i+1] );
@@ -1524,7 +1532,7 @@
                     expectation_gradients[ i + 1 ]
                     );
                 
-                train_costs.subVec(partial_costs_positions[i],
+                train_costs.subVec(partial_costs_positions[i]+1,
                                    partial_cost_value.length()) 
                     << partial_cost_value;
                 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-05-01 22:20:28 UTC (rev 8926)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-05-02 20:52:16 UTC (rev 8927)
@@ -32,7 +32,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Pascal Lamblin
+// Authors: Hugo Larochelle
 
 /*! \file StackedAutoassociatorsNet.h */
 
@@ -56,10 +56,6 @@
  * Neural net, trained layer-wise in a greedy fashion using autoassociators.
  * It is highly inspired by the DeepBeliefNet class, and can use use the
  * same RBMLayer and RBMConnection components.
- *
- * TODO: - code globally online version (can't use hyperlearner, 
- *         because of copies in earlystopping oracle and testing after change...)
- *       - make sure fpropNLL only uses the expectation field in RBMLayer objects
  */
 class StackedAutoassociatorsNet : public PLearner
 {
@@ -160,6 +156,10 @@
     //! unsupervised fine tuning gradient descent
     real unsupervised_fine_tuning_decrease_ct;
 
+    //! Indication that only the input layer should be masked 
+    //! during unsupervised fine-tuning
+    bool mask_input_layer_only_in_unsupervised_fine_tuning;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers



From larocheh at mail.berlios.de  Fri May  2 23:26:47 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 2 May 2008 23:26:47 +0200
Subject: [Plearn-commits] r8928 - trunk/plearn_learners_experimental
Message-ID: <200805022126.m42LQl8q014240@sheep.berlios.de>

Author: larocheh
Date: 2008-05-02 23:26:47 +0200 (Fri, 02 May 2008)
New Revision: 8928

Modified:
   trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.cc
   trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.h
Log:
Added fraction_of_masked_inputs...


Modified: trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.cc
===================================================================
--- trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.cc	2008-05-02 20:52:16 UTC (rev 8927)
+++ trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.cc	2008-05-02 21:26:47 UTC (rev 8928)
@@ -66,6 +66,7 @@
     n_classes( -1 ),
     output_weights_l1_penalty_factor(0),
     output_weights_l2_penalty_factor(0),
+    fraction_of_masked_inputs( 0 ),
     n_layers( 0 ),
     currently_trained_layer( 0 )
 {
@@ -155,6 +156,23 @@
                   OptionBase::buildoption,
                   "Number of classes.");
 
+    declareOption(ol, "output_weights_l1_penalty_factor", 
+                  &TopDownAsymetricDeepNetwork::output_weights_l1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Output weights l1_penalty_factor.\n");
+
+    declareOption(ol, "output_weights_l2_penalty_factor", 
+                  &TopDownAsymetricDeepNetwork::output_weights_l2_penalty_factor,
+                  OptionBase::buildoption,
+                  "Output weights l2_penalty_factor.\n");
+
+    declareOption(ol, "fraction_of_masked_inputs", 
+                  &TopDownAsymetricDeepNetwork::fraction_of_masked_inputs,
+                  OptionBase::buildoption,
+                  "Fraction of the autoassociators' random input components "
+                  "that are\n"
+                  "masked, i.e. unsused to reconstruct the input.\n");
+
     declareOption(ol, "greedy_stages", 
                   &TopDownAsymetricDeepNetwork::greedy_stages,
                   OptionBase::learntoption,
@@ -326,6 +344,10 @@
                 "top_down_layers[0] should have a size of %d.\n",
                 inputsize_);
     
+    if( fraction_of_masked_inputs < 0 )
+        PLERROR("TopDownAsymetricDeepNetwork::build_()"
+                " - \n"
+                "fraction_of_masked_inputs should be > or equal to 0.\n");
 
     activations.resize( n_layers );
     expectations.resize( n_layers );
@@ -452,6 +474,8 @@
     deepCopyField(reconstruction_activation_gradients, copies);
     deepCopyField(reconstruction_expectation_gradients, copies);
     deepCopyField(input_representation, copies);
+    deepCopyField(masked_autoassociator_input, copies);
+    deepCopyField(autoassociator_input_indices, copies);
     deepCopyField(pos_down_val, copies);
     deepCopyField(pos_up_val, copies);
     deepCopyField(neg_down_val, copies);
@@ -547,6 +571,13 @@
         pos_up_val.resize(layers[i+1]->size);
         neg_down_val.resize(layers[i]->size);
         neg_up_val.resize(layers[i+1]->size);
+        if( fraction_of_masked_inputs > 0 )
+        {
+            masked_autoassociator_input.resize(layers[i]->size);
+            autoassociator_input_indices.resize(layers[i]->size);
+            for( int j=0 ; j < autoassociator_input_indices.length() ; j++ )
+                autoassociator_input_indices[j] = j;
+        }
 
         for( ; *this_stage<end_stage ; (*this_stage)++ )
         {
@@ -636,13 +667,24 @@
         else
             lr = greedy_learning_rate;
 
+        if( fraction_of_masked_inputs > 0 )
+            random_gen->shuffleElements(autoassociator_input_indices);
+
         top_down_layers[index]->setLearningRate( lr );
         connections[index]->setLearningRate( lr );
         reconstruction_connections[index]->setLearningRate( lr );
         layers[index+1]->setLearningRate( lr );
 
-        connections[index]->fprop(input_representation,
-                              activations[index+1]);
+        if( fraction_of_masked_inputs > 0 )
+        {
+            masked_autoassociator_input << input_representation;
+            for( int j=0 ; j < round(fraction_of_masked_inputs*layers[index]->size) ; j++)
+                masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0; 
+            connections[index]->fprop( masked_autoassociator_input, activations[index+1]);
+        }
+        else
+            connections[index]->fprop(input_representation,
+                                      activations[index+1]);
         layers[index+1]->fprop(activations[index+1], expectations[index+1]);
 
         reconstruction_connections[ index ]->fprop( expectations[index+1],
@@ -711,11 +753,18 @@
             reconstruction_activation_gradients,
             reconstruction_expectation_gradients);
         
-        connections[ index ]->bpropUpdate( 
-            input_representation,
-            activations[index+1],
-            reconstruction_expectation_gradients, //reused
-            reconstruction_activation_gradients);
+        if( fraction_of_masked_inputs > 0 )
+            connections[ index ]->bpropUpdate( 
+                masked_autoassociator_input,
+                activations[index+1],
+                reconstruction_expectation_gradients, //reused
+                reconstruction_activation_gradients);
+        else
+            connections[ index ]->bpropUpdate( 
+                input_representation,
+                activations[index+1],
+                reconstruction_expectation_gradients, //reused
+                reconstruction_activation_gradients);
     }
      
 

Modified: trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.h
===================================================================
--- trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.h	2008-05-02 20:52:16 UTC (rev 8927)
+++ trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.h	2008-05-02 21:26:47 UTC (rev 8928)
@@ -117,6 +117,10 @@
     //! Output weights l2_penalty_factor
     real output_weights_l2_penalty_factor;
 
+    //! Fraction of the autoassociators' random input components that are
+    //! masked, i.e. unsused to reconstruct the input.
+    real fraction_of_masked_inputs;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
@@ -215,6 +219,12 @@
     //! Example representation
     mutable Vec input_representation;
 
+    //! Perturbed input for current layer
+    mutable Vec masked_autoassociator_input;
+
+    //! Indices of input components for current layer
+    mutable TVec<int> autoassociator_input_indices;
+
     //! Positive down statistic
     Vec pos_down_val;
     //! Positive up statistic



From larocheh at mail.berlios.de  Fri May  2 23:31:56 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 2 May 2008 23:31:56 +0200
Subject: [Plearn-commits] r8929 - trunk/plearn_learners_experimental
Message-ID: <200805022131.m42LVuQY014636@sheep.berlios.de>

Author: larocheh
Date: 2008-05-02 23:31:55 +0200 (Fri, 02 May 2008)
New Revision: 8929

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added training CPU time cost


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-02 21:26:47 UTC (rev 8928)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-02 21:31:55 UTC (rev 8929)
@@ -64,6 +64,13 @@
     n_classes( -1 ),
     compute_input_space_nll( false ),
     pseudolikelihood_context_size ( 0 ),
+    nll_cost_index( -1 ),
+    class_cost_index( -1 ),
+    training_cpu_time_cost_index ( -1 ),
+    cumulative_training_time_cost_index ( -1 ),
+    //cumulative_testing_time_cost_index ( -1 ),
+    cumulative_training_time( 0 ),
+    //cumulative_testing_time( 0 ),
     log_Z( MISSING_VALUE ),
     Z_is_up_to_date( false )
 {
@@ -128,6 +135,18 @@
                   OptionBase::buildoption,
                   "The connection weights between the input and hidden layer.\n");
 
+    declareOption(ol, "cumulative_training_time", 
+                  &PseudolikelihoodRBM::cumulative_training_time,
+                  //OptionBase::learntoption | OptionBase::nosave,
+                  OptionBase::learntoption,
+                  "Cumulative training time since age=0, in seconds.\n");
+
+//    declareOption(ol, "cumulative_testing_time", 
+//                  &PseudolikelihoodRBM::cumulative_testing_time,
+//                  //OptionBase::learntoption | OptionBase::nosave,
+//                  OptionBase::learntoption,
+//                  "Cumulative testing time since age=0, in seconds.\n");
+
     declareOption(ol, "log_Z", &PseudolikelihoodRBM::log_Z,
                   OptionBase::learntoption,
                   "Normalisation constant (on log scale).\n");
@@ -174,6 +193,9 @@
 
         build_layers_and_connections();
         build_costs();
+
+        // Activate the profiler
+        Profiler::activate();
     }
 }
 
@@ -199,6 +221,18 @@
         current_index++;
     }
 
+    cost_names.append("cpu_time");
+    cost_names.append("cumulative_train_time");
+    cost_names.append("cumulative_test_time");
+
+    training_cpu_time_cost_index = current_index;
+    current_index++;
+    cumulative_training_time_cost_index = current_index;
+    current_index++;
+    //cumulative_testing_time_cost_index = current_index;
+    //current_index++;
+
+
     PLASSERT( current_index == cost_names.length() );
 }
 
@@ -330,6 +364,9 @@
     input_layer->forget();
     hidden_layer->forget();
     connection->forget();
+
+    cumulative_training_time = 0;
+    //cumulative_testing_time = 0;
     Z_is_up_to_date = false;
 }
 
@@ -373,6 +410,10 @@
                               + classname(),
                               nstages - stage );
 
+    // Start the actual time counting
+    Profiler::reset("training");
+    Profiler::start("training");
+
     for( ; stage<nstages ; stage++ )
     {
         Z_is_up_to_date = false;
@@ -811,6 +852,18 @@
         
     }
     
+    Profiler::end("training");
+    const Profiler::Stats& stats = Profiler::getStats("training");
+    real ticksPerSec = Profiler::ticksPerSecond();
+    real cpu_time = (stats.user_duration+stats.system_duration)/ticksPerSec;
+    cumulative_training_time += cpu_time;
+
+    train_costs.fill(MISSING_VALUE);
+    train_costs[training_cpu_time_cost_index] = cpu_time;
+    train_costs[cumulative_training_time_cost_index] = cumulative_training_time;
+    train_stats->update( train_costs );
+
+
     train_stats->finalize();
 }
 
@@ -856,12 +909,14 @@
     }
     else
     {        
-        compute_Z();
-        connection->setAsDownInput( input );
-        hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
-        costs[nll_cost_index] = hidden_layer->freeEnergyContribution(
-            hidden_layer->activation) + log_Z;
-
+        if( compute_input_space_nll )
+        {
+            compute_Z();
+            connection->setAsDownInput( input );
+            hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
+            costs[nll_cost_index] = hidden_layer->freeEnergyContribution(
+                hidden_layer->activation) + log_Z;
+        }
     }
 }
 

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-02 21:26:47 UTC (rev 8928)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-02 21:31:55 UTC (rev 8929)
@@ -239,6 +239,15 @@
     //! Keeps the index of the class_error cost in train_costs
     int class_cost_index;
 
+    //! CPU time costs indices
+    int training_cpu_time_cost_index;
+    int cumulative_training_time_cost_index;
+    //real cumulative_testing_time_cost_index;
+
+    //! Cumulative CPU time costs
+    real cumulative_training_time;
+    //real cumulative_testing_time;
+    
     //! Normalisation constant (on log scale)
     mutable real log_Z;
 



From tihocan at mail.berlios.de  Mon May  5 16:43:23 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 5 May 2008 16:43:23 +0200
Subject: [Plearn-commits] r8930 - trunk/plearn_learners/hyper
Message-ID: <200805051443.m45EhNBQ021168@sheep.berlios.de>

Author: tihocan
Date: 2008-05-05 16:43:23 +0200 (Mon, 05 May 2008)
New Revision: 8930

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
   trunk/plearn_learners/hyper/HyperOptimize.h
Log:
Added missing deepCopyField statements, also made the inherited typedef private

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-05-02 21:31:55 UTC (rev 8929)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-05-05 14:43:23 UTC (rev 8930)
@@ -577,14 +577,20 @@
 }
 */
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void HyperOptimize::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(resultsmat, copies);
-    deepCopyField(oracle, copies);
+    deepCopyField(resultsmat,   copies);
+    deepCopyField(best_results, copies);
+    deepCopyField(best_learner, copies);
+    deepCopyField(option_vals,  copies);
+    deepCopyField(oracle,       copies);
     deepCopyField(sub_strategy, copies);
-    deepCopyField(splitter, copies);
+    deepCopyField(splitter,     copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/hyper/HyperOptimize.h
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.h	2008-05-02 21:31:55 UTC (rev 8929)
+++ trunk/plearn_learners/hyper/HyperOptimize.h	2008-05-05 14:43:23 UTC (rev 8930)
@@ -105,6 +105,8 @@
  */
 class HyperOptimize: public HyperCommand
 {
+    typedef HyperCommand inherited;
+
 protected:
     //! Store the results computed for each trial
     VMat resultsmat;
@@ -117,10 +119,8 @@
 
 public:
 
-    typedef HyperCommand inherited;
     PLEARN_DECLARE_OBJECT(HyperOptimize);
 
-
     // ************************
     // * public build options *
     // ************************
@@ -154,12 +154,10 @@
 
 private:
     //! This does the actual building.
-    // (Please implement in .cc)
     void build_();
 
 protected:
     //! Declares this class' options
-    // (Please implement in .cc)
     static void declareOptions(OptionList& ol);
 
     void getResultsMat();



From tihocan at mail.berlios.de  Mon May  5 17:09:22 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 5 May 2008 17:09:22 +0200
Subject: [Plearn-commits] r8931 - trunk/plearn_learners/hyper
Message-ID: <200805051509.m45F9MHm025486@sheep.berlios.de>

Author: tihocan
Date: 2008-05-05 17:09:21 +0200 (Mon, 05 May 2008)
New Revision: 8931

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
   trunk/plearn_learners/hyper/HyperOptimize.h
Log:
Using PTimer instead of Profiler to make sure there is no overflow of the time elapsed for long durations

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-05-05 14:43:23 UTC (rev 8930)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-05-05 15:09:21 UTC (rev 8931)
@@ -110,6 +110,7 @@
 HyperOptimize::HyperOptimize()
     : best_objective(REAL_MAX),
       trialnum(0),
+      auto_save_timer(new PTimer()),
       which_cost_pos(-1),
       which_cost(),
       min_n_trials(0),
@@ -122,7 +123,9 @@
       auto_save_diff_time(3*60*60)
 { }
 
-
+////////////////////
+// declareOptions //
+////////////////////
 void HyperOptimize::declareOptions(OptionList& ol)
 {
     declareOption(
@@ -177,8 +180,9 @@
     declareOption(
         ol, "auto_save_diff_time", &HyperOptimize::auto_save_diff_time,
         OptionBase::buildoption,
-        "HyperOptimize::auto_save_diff_time is the mininum amount of time before the\n"
-        " first save point and between two save point in second. Default 3h.");
+        "HyperOptimize::auto_save_diff_time is the mininum amount of time\n"
+        "(in seconds) before the first save point, then between two\n"
+        "consecutive save points.");
 
     declareOption(
         ol, "auto_save_test", &HyperOptimize::auto_save_test, OptionBase::buildoption,
@@ -212,28 +216,19 @@
     declareOption(ol, "option_vals", &HyperOptimize::option_vals,
                   OptionBase::learntoption,"The option value to try." );
 
-//     declareOption(ol, "auto_save_timer", &HyperOptimize::auto_save_timer,
-//                   OptionBase::learntoption|OptionBase::nosave,
-//                   "The last time a save was done." );
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void HyperOptimize::build_()
-{
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a "reloaded" object: i.e. from the complete set of all serialised options.
-    // ###  - Updating or "re-building" of an object after a few "tuning" options have been modified.
-    // ### You should assume that the parent class' build_() has already been called.
+{}
 
-    auto_save_timer.activate();
-}
-
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void HyperOptimize::build()
 {
     inherited::build();
@@ -371,7 +366,7 @@
     Vec results;
     while(option_vals)
     {
-        auto_save_timer.start("auto_save");
+        auto_save_timer->startTimer("auto_save");
 
         if(verbosity>0)
             perr << "In HyperOptimize::optimize() - We optimize with "
@@ -429,15 +424,14 @@
         }
         ++trialnum;
 
-        auto_save_timer.end("auto_save");
-        if(auto_save>0){
-            if(trialnum%auto_save!=0 && option_vals)
-                continue;
-            int s=auto_save_timer.getStats("auto_save").wall_duration;
-            s/=auto_save_timer.ticksPerSecond();
-            if(s>auto_save_diff_time|| ! option_vals){
+        auto_save_timer->stopTimer("auto_save");
+        if (auto_save > 0 &&
+                (trialnum % auto_save == 0) || option_vals.isEmpty())
+        {
+            int s = int(auto_save_timer->getTimer("auto_save"));
+            if(s > auto_save_diff_time || option_vals.isEmpty()) {
                 hlearner->auto_save();
-                auto_save_timer.reset("auto_save");
+                auto_save_timer->resetTimer("auto_save");
                 if(auto_save_test>0 && trialnum%auto_save_test==0)
                     PLERROR("In HyperOptimize::optimize() - auto_save_test is true,"
                             " exiting");
@@ -584,13 +578,14 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(resultsmat,   copies);
-    deepCopyField(best_results, copies);
-    deepCopyField(best_learner, copies);
-    deepCopyField(option_vals,  copies);
-    deepCopyField(oracle,       copies);
-    deepCopyField(sub_strategy, copies);
-    deepCopyField(splitter,     copies);
+    deepCopyField(resultsmat,       copies);
+    deepCopyField(best_results,     copies);
+    deepCopyField(best_learner,     copies);
+    deepCopyField(option_vals,      copies);
+    deepCopyField(auto_save_timer,  copies);
+    deepCopyField(oracle,           copies);
+    deepCopyField(sub_strategy,     copies);
+    deepCopyField(splitter,         copies);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/hyper/HyperOptimize.h
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.h	2008-05-05 14:43:23 UTC (rev 8930)
+++ trunk/plearn_learners/hyper/HyperOptimize.h	2008-05-05 15:09:21 UTC (rev 8931)
@@ -45,12 +45,12 @@
 #define HyperOptimize_INC
 
 #include "HyperCommand.h"
+#include "OptionsOracle.h"
+#include <plearn/misc/PTimer.h>
 #include <plearn/vmat/Splitter.h>
-#include "OptionsOracle.h"
 
 namespace PLearn {
 using namespace std;
-class Profiler;
 /**
  *  Carry out an hyper-parameter optimization according to an Oracle
  *
@@ -115,7 +115,7 @@
     PP<PLearner> best_learner;
     int trialnum;
     TVec<string> option_vals;
-    Profiler auto_save_timer;
+    PP<PTimer> auto_save_timer;
 
 public:
 



From tihocan at mail.berlios.de  Mon May  5 17:24:08 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 5 May 2008 17:24:08 +0200
Subject: [Plearn-commits] r8932 - trunk/plearn_learners/hyper
Message-ID: <200805051524.m45FO8NR027141@sheep.berlios.de>

Author: tihocan
Date: 2008-05-05 17:24:08 +0200 (Mon, 05 May 2008)
New Revision: 8932

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
Log:
Added a comment and fixed the conditions for a warning to be displayed

Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-05-05 15:09:21 UTC (rev 8931)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-05-05 15:24:08 UTC (rev 8932)
@@ -323,6 +323,9 @@
 }
 
 
+///////////////
+// auto_save //
+///////////////
 void HyperLearner::auto_save()
 {
     if(expdir.isEmpty())
@@ -338,8 +341,15 @@
     mvforce(tmp,f);
 }
 
+///////////////
+// auto_load //
+///////////////
 void HyperLearner::auto_load()
 {
+    // Reload the saved HyperLearner. Note that becase the boolean 'reloading'
+    // is static, it means we can currently reload only one single
+    // HyperLearner at a time. It may be interesting in the future to change
+    // this reload mechanism to let us reload multiple (chained) HyperLearners.
     if(expdir.isEmpty()){
         if(verbosity>1)
             PLWARNING("In HyperLearner::auto_load() - no expdir. Can't reload.");
@@ -355,9 +365,8 @@
         reloading = false;
         reloaded = true;
     }
-    else if(isf && verbosity>1)
+    else if(!isf && verbosity>1)
         PLWARNING("In HyperLearner::auto_load() - no file to reload.");
-
 }
 
 } // end of namespace PLearn



From tihocan at mail.berlios.de  Mon May  5 17:30:49 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 5 May 2008 17:30:49 +0200
Subject: [Plearn-commits] r8933 - trunk/python_modules/plearn/pymake
Message-ID: <200805051530.m45FUnMP027833@sheep.berlios.de>

Author: tihocan
Date: 2008-05-05 17:30:49 +0200 (Mon, 05 May 2008)
New Revision: 8933

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
PYMAKE_OPTION renamed into PYMAKE_OPTIONS since people are likely to put more options

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-05-05 15:24:08 UTC (rev 8932)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-05-05 15:30:49 UTC (rev 8933)
@@ -182,9 +182,9 @@
   'opt_boundcheck': '-Wall -g -DBOUNDCHECK'  }
 
 
-The environnement variable PYMAKE_OPTION is prepended to the command line 
-option. So you can define your default option their is they won't conflict
-with the one you add on the command line.
+The environment variable PYMAKE_OPTIONS is prepended to the command line 
+options. You can define your default options there if they do not conflict
+with the ones from the command line.
 """
 
 
@@ -2536,7 +2536,7 @@
     linkname = ''
     link_target_override = None
 
-    env_options=os.getenv('PYMAKE_OPTION')
+    env_options=os.getenv('PYMAKE_OPTIONS')
     option_to_parse=[]
     if env_options:
         option_to_parse=env_options.split()



From tihocan at mail.berlios.de  Mon May  5 17:46:27 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 5 May 2008 17:46:27 +0200
Subject: [Plearn-commits] r8934 - trunk/plearn_learners/hyper
Message-ID: <200805051546.m45FkREB030807@sheep.berlios.de>

Author: tihocan
Date: 2008-05-05 17:46:26 +0200 (Mon, 05 May 2008)
New Revision: 8934

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
Fixed bug just introduced in r8931, due to badly placed parenthesis

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-05-05 15:30:49 UTC (rev 8933)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-05-05 15:46:26 UTC (rev 8934)
@@ -426,7 +426,7 @@
 
         auto_save_timer->stopTimer("auto_save");
         if (auto_save > 0 &&
-                (trialnum % auto_save == 0) || option_vals.isEmpty())
+                (trialnum % auto_save == 0 || option_vals.isEmpty()))
         {
             int s = int(auto_save_timer->getTimer("auto_save"));
             if(s > auto_save_diff_time || option_vals.isEmpty()) {



From louradou at mail.berlios.de  Mon May  5 19:29:20 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 5 May 2008 19:29:20 +0200
Subject: [Plearn-commits] r8935 - trunk/plearn_learners/online
Message-ID: <200805051729.m45HTK0X030084@sheep.berlios.de>

Author: louradou
Date: 2008-05-05 19:29:20 +0200 (Mon, 05 May 2008)
New Revision: 8935

Modified:
   trunk/plearn_learners/online/ModuleLearner.cc
Log:
mbatch_size is now updated each time train() is called.
The previous behaviour was to "learn" it only when stage=0,
but we may want to change the batch_sioze externally during
the training.



Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2008-05-05 15:46:26 UTC (rev 8934)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2008-05-05 17:29:20 UTC (rev 8935)
@@ -301,17 +301,16 @@
         else PLERROR("ModuleLearner::reset_seed_upon_train should be >=-1");
     }
     OnlineLearningModule::during_training=true;
-    if (stage == 0) {
-        // Perform training set-dependent initialization here.
-        if (batch_size == 0)
-            mbatch_size = train_set->length();
-        else
-            mbatch_size = batch_size;
-        if (train_set->weightsize() >= 1 && !store_weights)
-            PLWARNING("In ModuleLearner::train - The training set contains "
-                    "weights, but the network is not using them");
-    }
 
+    // Perform training set-dependent initialization here.
+    if (batch_size == 0)
+        mbatch_size = train_set->length();
+    else
+        mbatch_size = batch_size;
+    if (train_set->weightsize() >= 1 && !store_weights)
+        PLWARNING("In ModuleLearner::train - The training set contains "
+                  "weights, but the network is not using them");
+
     Mat inputs, targets;
     Vec weights;
     PP<ProgressBar> pb = NULL;



From nouiz at mail.berlios.de  Mon May  5 20:03:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 5 May 2008 20:03:06 +0200
Subject: [Plearn-commits] r8936 - trunk/scripts
Message-ID: <200805051803.m45I36wj000203@sheep.berlios.de>

Author: nouiz
Date: 2008-05-05 20:03:06 +0200 (Mon, 05 May 2008)
New Revision: 8936

Modified:
   trunk/scripts/dbidispatch
Log:
bugfix and when the SERVER param is not set, we consider it as a desktop machine


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-05 17:29:20 UTC (rev 8935)
+++ trunk/scripts/dbidispatch	2008-05-05 18:03:06 UTC (rev 8936)
@@ -198,10 +198,10 @@
         else:
             param='req'
         dbi_param.setdefault(param,'True')
-        if argv.find('no_')!=-1:
-            dbi_param[param]+='&&(SERVER==True)'
+        if argv.find('no_')==-1:
+            dbi_param[param]+='&&(SERVER=?=True)'
         else:
-            dbi_param[param]+='&&(SERVER==False)'
+            dbi_param[param]+='&&(SERVER=?=False || SERVER =?= UNDEFINED )'
     elif argv[0:1] == '-':
 	print "Unknow option (%s)"%argv
 	print ShortHelp



From lamblin at mail.berlios.de  Mon May  5 20:50:49 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 5 May 2008 20:50:49 +0200
Subject: [Plearn-commits] r8937 - trunk/scripts
Message-ID: <200805051850.m45IonS9003524@sheep.berlios.de>

Author: lamblin
Date: 2008-05-05 20:50:47 +0200 (Mon, 05 May 2008)
New Revision: 8937

Modified:
   trunk/scripts/dbidispatch
Log:
Some whitespace fixes


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-05 18:03:06 UTC (rev 8936)
+++ trunk/scripts/dbidispatch	2008-05-05 18:50:47 UTC (rev 8937)
@@ -62,7 +62,7 @@
 
   dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
      or
-  dbidispatch '--req=Machine=="computer.example.com"' 
+  dbidispatch '--req=Machine=="computer.example.com"'
 
   The '--server'(--no_server) option add the requirement that the executing host must be a server dedicated to computing. This is equivalent to: dbidispatch '--req=SERVER==True'(SERVER==False)
   The '--prefserver' option will tell that you prefer to execute on server first. This is equivalent to 'rank=SERVER=?=True' in the submit file.
@@ -70,7 +70,7 @@
   The '--machine=full_host_name' option add the requirement that the executing host is full_host_name
      dbidispatch --machine=computer.example.com
         witch is equivalent to
-     dbidispatch '--req=Machine=="computer.example.com"' 
+     dbidispatch '--req=Machine=="computer.example.com"'
   The '--machines=regexp' option add the requirement that the executing host name must be match the regexp
      dbidispatch '--machines=computer00*'
         witch is equivalent to
@@ -203,8 +203,8 @@
         else:
             dbi_param[param]+='&&(SERVER=?=False || SERVER =?= UNDEFINED )'
     elif argv[0:1] == '-':
-	print "Unknow option (%s)"%argv
-	print ShortHelp
+        print "Unknow option (%s)"%argv
+        print ShortHelp
         sys.exit(1)
     else:
         break
@@ -284,13 +284,13 @@
     commands=[]
     for line in FD.readlines():
         line = line.rstrip()
-	sp = line.split(" ")
+        sp = line.split(" ")
         commands+=generate_commands(sp)
     FD.close
 else:
     commands=generate_commands(command_argv)
 
-if FILE == "":    
+if FILE == "":
     t = [x for x in sys.argv[1:] if not x[:2]=="--"]
     t[0]=os.path.split(t[0])[1]
     tmp="_".join(t)
@@ -337,7 +337,7 @@
     if "test" in dbi_param:
         print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
     else:
-        SCRIPT.write("jobs.clean()") 
+        SCRIPT.write("jobs.clean()")
     SCRIPT.close()
     os.system("chmod +x %s"%(ScriptName));
 



From nouiz at mail.berlios.de  Mon May  5 21:42:41 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 5 May 2008 21:42:41 +0200
Subject: [Plearn-commits] r8938 - trunk/scripts
Message-ID: <200805051942.m45JgfHs008362@sheep.berlios.de>

Author: nouiz
Date: 2008-05-05 21:42:40 +0200 (Mon, 05 May 2008)
New Revision: 8938

Removed:
   trunk/scripts/dbi.test/
Log:
removed directory that is not needed anymore




From louradou at mail.berlios.de  Mon May  5 21:47:59 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 5 May 2008 21:47:59 +0200
Subject: [Plearn-commits] r8939 - trunk/python_modules/plearn/learners
Message-ID: <200805051947.m45Jlx8p008796@sheep.berlios.de>

Author: louradou
Date: 2008-05-05 21:47:59 +0200 (Mon, 05 May 2008)
New Revision: 8939

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
real forget() function. semiforget() (Previous forget())
forgets everything except the previous best hyperparameters
that are kept as a starting point.



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-05-05 19:42:40 UTC (rev 8938)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-05-05 19:47:59 UTC (rev 8939)
@@ -101,7 +101,7 @@
         so that we can assume new best parameters are
         close to previous ones.
     """
-    def forget(self):
+    def semiforget(self):
         self.trials_param_list  = []
         self.trials_cost_list = []
         self.stats_are_uptodate = False
@@ -110,6 +110,9 @@
         #       a new search (when data changed a bit) to
         #       a good candidate
 
+    def forget(self):
+        self.semiforget()
+        self.best_param = None
 
     def set_input_stats( self, inputsize, input_avgstd ):
         self.stats_are_uptodate = True
@@ -804,9 +807,9 @@
 
         self.outputs_type = 'votes'
 
-    def forget(self):
+    def semiforget(self):
         for expert in self.all_experts:
-             expert.forget()
+             expert.semiforget()
         self.valid_stats     = None
         self.test_stats      = None
         self.train_stats     = None
@@ -820,6 +823,12 @@
         #       a new search (when data changed a bit) to
         #       a good candidate
 
+    def forget(self):
+        self.semiforget()
+        for expert in self.all_experts:
+             expert.forget()
+        self.best_param = None
+
     def train_inputspec(self, dataspec):
         assert type(dataspec) == dict
         if self.trainset_key not in dataspec:



From larocheh at mail.berlios.de  Mon May  5 23:36:51 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 5 May 2008 23:36:51 +0200
Subject: [Plearn-commits] r8940 - trunk/plearn_learners_experimental
Message-ID: <200805052136.m45Lap3L020210@sheep.berlios.de>

Author: larocheh
Date: 2008-05-05 23:36:50 +0200 (Mon, 05 May 2008)
New Revision: 8940

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
Log:
Corrected some bugs...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-05 19:47:59 UTC (rev 8939)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-05 21:36:50 UTC (rev 8940)
@@ -106,6 +106,7 @@
     declareOption(ol, "n_classes", &PseudolikelihoodRBM::n_classes,
                   OptionBase::buildoption,
                   "Number of classes in the training set (for supervised learning).\n"
+                  "If < 2, unsupervised learning will be performed.\n"
                   );
 
     declareOption(ol, "compute_input_space_nll", 
@@ -223,7 +224,7 @@
 
     cost_names.append("cpu_time");
     cost_names.append("cumulative_train_time");
-    cost_names.append("cumulative_test_time");
+    //cost_names.append("cumulative_test_time");
 
     training_cpu_time_cost_index = current_index;
     current_index++;
@@ -261,6 +262,7 @@
                 connection->up_size, connection->down_size,
                 hidden_layer->size, input_layer->size);
 
+    input_gradient.resize( input_layer->size );
     hidden_activation_pos_i.resize( hidden_layer->size );
     hidden_activation_neg_i.resize( hidden_layer->size );
     hidden_activation_gradient.resize( hidden_layer->size );
@@ -422,7 +424,7 @@
         if( pb )
             pb->update( stage - init_stage + 1 );
 
-        if( targetsize() == 1 )
+        if( n_classes > 1 )
         {
             target_one_hot.clear();
             if( !is_missing(target[0]) )
@@ -510,7 +512,7 @@
                         else
                             pseudolikelihood += softplus( 
                                 num_pos_act - num_neg_act ) 
-                                - input_i * (num_pos_act - num_neg_act);;
+                                - input_i * (num_pos_act - num_neg_act);
                         input_gradient[i] = input_probs_i - input_i;
 
                         hidden_layer->freeEnergyContributionGradient(
@@ -551,6 +553,10 @@
                     // N.B.: train costs contains pseudolikelihood
                     //       or pseudoNLL, not NLL
                     train_costs[nll_cost_index] = pseudolikelihood;
+
+//                    cout << "input_gradient: " << input_gradient << endl;
+//                    cout << "hidden_activation_gradient" << hidden_activation_gradient << endl;
+
                 }
                 else
                 {
@@ -593,28 +599,29 @@
                     real* gnums_data;
                     real* cp_data;
                     real* a = hidden_layer->activation.data();
-                    real* w, *gw, *gi, *ac, *gac;
+                    real* w, *gw, *gi, *ac, *bi, *gac;
                     int* context_i;
-                    int m = connection->weights.mod();
+                    int m;
                     int conf_index;
-                    real input_i, input_j, bi, Zi, log_Zi;
+                    real input_i, input_j,  log_Zi;
                     real pseudolikelihood = 0;
 
                     input_gradient.clear();
                     hidden_activation_gradient.clear();
                     connection_gradient.clear();
                     gi = input_gradient.data();
+                    bi = input_layer->bias.data();
                     for( int i=0; i<input_layer->size ; i++ )
                     {
                         nums_data = nums_act.data();
                         cp_data = context_probs.data();
                         input_i = input[i];
-                        bi = input_layer->bias[i];
 
+                        m = connection->weights.mod();
                         // input_i = 1
                         for( int k=0; k<n_conf; k++)
                         {
-                            *nums_data = bi;
+                            *nums_data = bi[i];
                             *cp_data = input_i;
                             conf_index = k;
                             ac = hidden_activations_context[k];
@@ -626,21 +633,22 @@
                             context_i = context_indices_per_i[i];
                             for( int l=0; l<pseudolikelihood_context_size; l++ )
                             {
+                                input_j = input[*context_i];
                                 w = &(connection->weights(0,*context_i));
-                                input_j = input[*context_i];
-                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                if( conf_index & 1)
                                 {
-                                    if( conf_index & 1)
-                                    {
+                                    *cp_data *= input_j;
+                                    *nums_data += bi[*context_i];
+                                    for( int j=0; j<hidden_layer->size; j++,w+=m )
                                         ac[j] -=  *w * ( input_j - 1 );
-                                        *cp_data *= input_j;
-                                    }
-                                    else
-                                    {
+                                }
+                                else
+                                {
+                                    *cp_data *= (1-input_j);
+                                    for( int j=0; j<hidden_layer->size; j++,w+=m )
                                         ac[j] -=  *w * input_j;
-                                        *cp_data *= (1-input_j);
-                                    }
                                 }
+
                                 conf_index >>= 1;
                                 context_i++;
                             }
@@ -667,19 +675,20 @@
                             {
                                 w = &(connection->weights(0,*context_i));
                                 input_j = input[*context_i];
-                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                if( conf_index & 1)
                                 {
-                                    if( conf_index & 1)
-                                    {
+                                    *cp_data *= input_j;
+                                    *nums_data += bi[*context_i];
+                                    for( int j=0; j<hidden_layer->size; j++,w+=m )
                                         ac[j] -=  *w * ( input_j - 1 );
-                                        *cp_data *= input_j;
-                                    }
-                                    else
-                                    {
+                                }
+                                else
+                                {
+                                    *cp_data *= (1-input_j);
+                                    for( int j=0; j<hidden_layer->size; j++,w+=m )
                                         ac[j] -=  *w * input_j;
-                                        *cp_data *= (1-input_j);
-                                    }
                                 }
+
                                 conf_index >>= 1;
                                 context_i++;
                             }
@@ -691,9 +700,10 @@
                     
 
                         // Gradient computation
-                        exp( nums_act, nums);
-                        Zi = sum(nums);
-                        log_Zi = pl_log(Zi);
+                        //exp( nums_act, nums);
+                        //Zi = sum(nums);
+                        //log_Zi = pl_log(Zi);
+                        log_Zi = logadd(nums_act);
 
                         nums_data = nums_act.data();
                         gnums_data = gnums_act.data();
@@ -701,12 +711,13 @@
 
                         // Compute input_prob gradient
 
+                        m = connection_gradient.mod();
                         // input_i = 1                    
                         for( int k=0; k<n_conf; k++)
                         {
                             pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
-                            *gnums_data = *nums_data/Zi - *cp_data;
-                            *gi += *gnums_data;
+                            *gnums_data = (safeexp(*nums_data - log_Zi) - *cp_data);
+                            gi[i] += *gnums_data;
                         
                             hidden_layer->freeEnergyContributionGradient(
                                 hidden_activations_context(k),
@@ -718,7 +729,7 @@
                         
                             gac = hidden_activations_context_k_gradient.data();
                             gw = &(connection_gradient(0,i));
-                            for( int j=0; j<hidden_layer->size; j++,w+=m )
+                            for( int j=0; j<hidden_layer->size; j++,gw+=m )
                                 *gw -= gac[j] * ( input_i - 1 );
 
                             context_i = context_indices_per_i[i];
@@ -726,11 +737,15 @@
                             {
                                 gw = &(connection_gradient(0,*context_i));
                                 input_j = input[*context_i];
-                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                if( conf_index & 1)
                                 {
-                                    if( conf_index & 1)
+                                    gi[*context_i] += *gnums_data;
+                                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
                                         *gw -= gac[j] * ( input_j - 1 );
-                                    else
+                                }
+                                else
+                                {
+                                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
                                         *gw -= gac[j] * input_j;
                                 }
                                 conf_index >>= 1;
@@ -746,8 +761,7 @@
                         for( int k=0; k<n_conf; k++)
                         {
                             pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
-                            *gnums_data = *nums_data/Zi - *cp_data;
-                            *gi += *gnums_data;
+                            *gnums_data = (safeexp(*nums_data - log_Zi) - *cp_data);
                         
                             hidden_layer->freeEnergyContributionGradient(
                                 hidden_activations_context(n_conf + k),
@@ -759,7 +773,7 @@
                         
                             gac = hidden_activations_context_k_gradient.data();
                             gw = &(connection_gradient(0,i));
-                            for( int j=0; j<hidden_layer->size; j++,w+=m )
+                            for( int j=0; j<hidden_layer->size; j++,gw+=m )
                                 *gw -= gac[j] *input_i;
 
                             context_i = context_indices_per_i[i];
@@ -767,13 +781,18 @@
                             {
                                 gw = &(connection_gradient(0,*context_i));
                                 input_j = input[*context_i];
-                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                if( conf_index & 1)
                                 {
-                                    if( conf_index & 1)
+                                    gi[*context_i] += *gnums_data;
+                                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
                                         *gw -= gac[j] * ( input_j - 1 );
-                                    else
+                                }
+                                else
+                                {
+                                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
                                         *gw -= gac[j] * input_j;
                                 }
+
                                 conf_index >>= 1;
                                 context_i++;
                             }
@@ -782,9 +801,11 @@
                             gnums_data++;
                             cp_data++;
                         }
-                        gi++;
                     }
 
+//                    cout << "input_gradient: " << input_gradient << endl;
+//                    cout << "hidden_activation_gradient" << hidden_activation_gradient << endl;
+
                     externalProductAcc( connection_gradient, hidden_activation_gradient,
                                         input );
 
@@ -863,7 +884,28 @@
     train_costs[cumulative_training_time_cost_index] = cumulative_training_time;
     train_stats->update( train_costs );
 
-
+//    // Sums to 1 test
+//    compute_Z();
+//    conf.resize( input_layer->size );
+//    Vec output,costs;
+//    output.resize(outputsize());
+//    costs.resize(getTestCostNames().length());
+//    target.resize( targetsize() );
+//    real sums = 0;
+//    int input_n_conf = input_layer->getConfigurationCount();
+//    for(int i=0; i<input_n_conf; i++)
+//    {
+//        input_layer->getConfiguration(i,conf);
+//        computeOutput(conf,output);
+//        computeCostsFromOutputs( conf, output, target, costs );
+//        //if( i==0 )
+//        //    sums = -costs[nll_cost_index];
+//        //else
+//        //    sums = logadd( sums, -costs[nll_cost_index] );
+//        sums += safeexp( -costs[nll_cost_index] );
+//    }        
+//    cout << "sums: " << //safeexp(sums) << endl;
+//        sums << endl;
     train_stats->finalize();
 }
 
@@ -874,7 +916,6 @@
 void PseudolikelihoodRBM::computeOutput(const Vec& input, Vec& output) const
 {
     // Compute the output from the input.
-    output.resize(0);
     if( n_classes > 1 )
     {
         // Get output probabilities
@@ -915,7 +956,7 @@
             connection->setAsDownInput( input );
             hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
             costs[nll_cost_index] = hidden_layer->freeEnergyContribution(
-                hidden_layer->activation) + log_Z;
+                hidden_layer->activation) - dot(input,input_layer->bias) + log_Z;
         }
     }
 }
@@ -969,10 +1010,11 @@
             hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
             if( i == 0 )
                 log_Z = -hidden_layer->freeEnergyContribution(
-                    hidden_layer->activation);
+                    hidden_layer->activation) + dot(conf,input_layer->bias);
             else
                 log_Z = logadd(-hidden_layer->freeEnergyContribution(
-                                   hidden_layer->activation),
+                                   hidden_layer->activation) 
+                               + dot(conf,input_layer->bias),
                                log_Z);
         }
     }
@@ -986,10 +1028,11 @@
             input_layer->getAllActivations( (RBMMatrixConnection *) connection );
             if( i == 0 )
                 log_Z = -input_layer->freeEnergyContribution(
-                    input_layer->activation);
+                    input_layer->activation) + dot(conf,hidden_layer->bias);
             else
                 log_Z = logadd(-input_layer->freeEnergyContribution(
-                                   hidden_layer->activation),
+                                   input_layer->activation)
+                               + dot(conf,hidden_layer->bias),
                                log_Z);
         }        
     }



From louradou at mail.berlios.de  Tue May  6 16:26:48 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 6 May 2008 16:26:48 +0200
Subject: [Plearn-commits] r8941 - trunk/python_modules/plearn/learners
Message-ID: <200805061426.m46EQmZE003163@sheep.berlios.de>

Author: louradou
Date: 2008-05-06 16:26:48 +0200 (Tue, 06 May 2008)
New Revision: 8941

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
fixed a bug with max_ntrials



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-05-05 21:36:50 UTC (rev 8940)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-05-06 14:26:48 UTC (rev 8941)
@@ -1539,7 +1539,7 @@
         The train set is mandatory, but valid and/or test sets can be missing.
         cf. train_inputspec(), valid_inputspec(), and test_inputspec().
     """
-    def run(self, dataspec):
+    def run(self, dataspec, param = None, L0 = None):
         assert self.testlevel >= 0
         assert self.max_ntrials > 0
         trainset = self.train_inputspec(dataspec)
@@ -1552,15 +1552,20 @@
         expert = self.get_expert( self.kernel_type )
         expert.verbosity = self.verbosity
         
-        L0=len(expert.trials_param_list)
+        if L0 == None:
+            L0=len(expert.trials_param_list)
+        local_retrain_until_local_optimum_is_found = True
 
         # HyperParamOracle__kernel.best_param is None just at the __init__
-        if expert.best_param  == None:
-            param_to_try = expert.choose_first_param()
+        if param == None or len(param) == 0:
+            if expert.best_param  == None:
+                param_to_try = expert.choose_first_param()
+            else:
+                param_to_try = expert.choose_new_param()
         else:
-            param_to_try = expert.choose_new_param()
+            param_to_try = [param]
+            local_retrain_until_local_optimum_is_found = False
         
-        local_retrain_until_local_optimum_is_found = True
         for param in param_to_try:
 
             if self.max_ntrials == 1: # No hyper-optimization (debug)
@@ -1597,7 +1602,7 @@
         and local_retrain_until_local_optimum_is_found
         and expert.should_be_tuned_again()
         and ( self.max_cost == None or expert.best_cost <= self.max_cost ) ):
-           return self.run( dataspec )
+           return self.run( dataspec, None, L0 )
 
         if self.testlevel == 0:
              return self.retrain_and_writeresults(dataspec)



From saintmlx at mail.berlios.de  Tue May  6 16:58:41 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 6 May 2008 16:58:41 +0200
Subject: [Plearn-commits] r8942 - trunk/plearn/python
Message-ID: <200805061458.m46Ewfrv006423@sheep.berlios.de>

Author: saintmlx
Date: 2008-05-06 16:58:40 +0200 (Tue, 06 May 2008)
New Revision: 8942

Modified:
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
- allow conversion of numpy array to TVec<T> or TMat<T> for T != real
- new python wrapper for VMats: includes __len__, __getitem__, etc.
 


Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2008-05-06 14:26:48 UTC (rev 8941)
+++ trunk/plearn/python/PythonExtension.cc	2008-05-06 14:58:40 UTC (rev 8942)
@@ -38,6 +38,7 @@
 #include <plearn/base/TypeFactory.h>
 #include <plearn/base/PMemPool.h>
 #include <plearn/base/stringutils.h>
+#include <plearn/vmat/VMatrix.h>
 
 namespace PLearn {
 
@@ -248,6 +249,13 @@
         if(!tit->second.constructor)
             continue; //skip abstract classes
 
+        string actual_wrapper_name= wrapper_name;
+        Object* temp_obj= tit->second.constructor();
+        // use different wrapper for VMats (w/ len, getitem, etc.)
+        if(dynamic_cast<VMatrix*>(temp_obj))
+            actual_wrapper_name= "WrappedPLearnVMat";
+        delete temp_obj;
+
         // create new python type deriving from WrappedPLearnObject
         string classname= tit->first;
         string pyclassname= classname;
@@ -255,7 +263,7 @@
         search_replace(pyclassname, "<", "_");
         search_replace(pyclassname, ">", "_");
         string derivcode= string("\nclass ")
-            + pyclassname + "(" + wrapper_name + "):\n"
+            + pyclassname + "(" + actual_wrapper_name + "):\n"
             "  \"\"\" ... \"\"\"\n"
             "  def __new__(cls,*args,**kwargs):\n"
             "    #print '** "+pyclassname+".__new__',kwargs\n"
@@ -267,6 +275,7 @@
         PyObject* res= PyRun_String(derivcode.c_str(), 
                                     Py_file_input, 
                                     PyModule_GetDict(module), PyModule_GetDict(module));
+
         Py_XDECREF(res);
         env= PythonObjectWrapper(
             PyModule_GetDict(module), 
@@ -276,7 +285,7 @@
             PLERROR("in injectPLearnClasses : "
                     "Cannot create new python class deriving from "
                     "%s (%s).", 
-                    wrapper_name.c_str(),
+                    actual_wrapper_name.c_str(),
                     classname.c_str());
 
         //set option names

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-05-06 14:26:48 UTC (rev 8941)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-05-06 14:58:40 UTC (rev 8942)
@@ -306,6 +306,41 @@
     return static_cast<VarArray>(ConvertFromPyObject<TVec<Var> >::convert(pyobj, print_traceback));
 }
 
+template<> int numpyType<bool>()               { return NPY_BOOL; }
+template<> int numpyType<signed char>()        { return NPY_BYTE; }
+template<> int numpyType<unsigned char>()      { return NPY_UBYTE; }
+template<> int numpyType<signed short>()       { return NPY_SHORT; }
+template<> int numpyType<unsigned short>()     { return NPY_USHORT; }
+template<> int numpyType<signed int>()         { return NPY_INT; }
+template<> int numpyType<unsigned int>()       { return NPY_UINT; }
+template<> int numpyType<signed long>()        { return NPY_LONG; }
+template<> int numpyType<unsigned long>()      { return NPY_ULONG; }
+template<> int numpyType<signed long long>()   { return NPY_LONGLONG; }
+template<> int numpyType<unsigned long long>() { return NPY_ULONGLONG; }
+template<> int numpyType<float>()              { return NPY_FLOAT; }
+template<> int numpyType<double>()             { return NPY_DOUBLE; }
+template<> int numpyType<long double>()        { return NPY_LONGDOUBLE; }
+
+PyObject* convertArrayCheck(PyObject* pyobj, int numpy_type, int ndim, bool print_traceback)
+{
+    PythonGlobalInterpreterLock gil;         // For thread-safety
+    static PythonEmbedder embedder;
+    PythonObjectWrapper::initializePython();
+
+    if(!PyArray_Check(pyobj)) return 0; //not an array
+
+    PyObject* pyarr0= PyArray_CheckFromAny(pyobj, NULL,
+                                           ndim, ndim, NPY_CARRAY_RO, Py_None);
+    PyObject* pyarr= 
+        PyArray_CastToType(reinterpret_cast<PyArrayObject*>(pyarr0),
+                           PyArray_DescrFromType(numpy_type), 0);
+    Py_XDECREF(pyarr0);
+    if(!pyarr)
+        PLPythonConversionError("convertArrayCheck", pyobj,
+                                print_traceback);
+    return pyarr;
+}
+
 //#####  Constructors+Destructors  ############################################
 
 PythonObjectWrapper::PythonObjectWrapper(OwnershipMode o,

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2008-05-06 14:26:48 UTC (rev 8941)
+++ trunk/plearn/python/PythonObjectWrapper.h	2008-05-06 14:58:40 UTC (rev 8942)
@@ -452,7 +452,33 @@
 };
 
 
+/*****
+ * Equivalence of types C++ -> numpy
+ */
+template <typename T>
+int numpyType()
+{
+    PLERROR("in numpyType: no numpy equivalent to C++ type ",
+            TypeTraits<T*>::name().c_str());
+    return -1; //shut up compiler
+}
 
+template<> int numpyType<bool>();
+template<> int numpyType<signed char>();
+template<> int numpyType<unsigned char>();
+template<> int numpyType<signed short>();
+template<> int numpyType<unsigned short>();
+template<> int numpyType<signed int>();
+template<> int numpyType<unsigned int>();
+template<> int numpyType<signed long>();
+template<> int numpyType<unsigned long>();
+template<> int numpyType<signed long long>();
+template<> int numpyType<unsigned long long>();
+template<> int numpyType<float>();
+template<> int numpyType<double>();
+template<> int numpyType<long double>();
+
+
 //! Used to convert integer values to python, using PyInt if possible
 template <class I>
 PyObject* integerToPyObject(const I& x)
@@ -971,6 +997,13 @@
     return p;
 }
 
+/*****
+ * convert numpy array to the right array type; return 0 if pyobj is not an array
+ * pyobj is the python object to check/convert, numpy_type is an int representing a
+ * numpy type, ndim is the number of dimensions that the array should have (1=vec, 2=mat, 0=any)
+ */
+PyObject* convertArrayCheck(PyObject* pyobj, int numpy_type, int ndim, bool print_traceback);
+
 template <class T>
 TVec<T> ConvertFromPyObject< TVec<T> >::convert(PyObject* pyobj,
                                                 bool print_traceback)
@@ -998,6 +1031,14 @@
         }
         return v;
     }
+    else if (PyObject* pyarr= convertArrayCheck(pyobj, numpyType<T>(), 1, print_traceback))
+    {
+        int sz= PyArray_DIM(pyarr,0);
+        TVec<T> v(sz);
+        v.copyFrom(static_cast<T*>(PyArray_DATA(pyarr)), sz);
+        Py_XDECREF(pyarr);
+        return v;
+    }
     else
         PLPythonConversionError("ConvertFromPyObject< TVec<T> >", pyobj,
                                 print_traceback);
@@ -1042,6 +1083,15 @@
         }
         return v;
     }
+    else if (PyObject* pyarr= convertArrayCheck(pyobj, numpyType<T>(), 2, print_traceback))
+    {
+        TMat<T> m;
+        m.resize(PyArray_DIM(pyarr,0), PyArray_DIM(pyarr,1));
+        m.toVec().copyFrom(static_cast<T*>(PyArray_DATA(pyarr)),
+                           PyArray_DIM(pyarr,0) * PyArray_DIM(pyarr,1));
+        Py_XDECREF(pyarr);
+        return m;
+    }
     else
         PLPythonConversionError("ConvertFromPyObject< TMat<T> >", pyobj,
                                 print_traceback);



From saintmlx at mail.berlios.de  Tue May  6 16:59:59 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 6 May 2008 16:59:59 +0200
Subject: [Plearn-commits] r8943 - in trunk/python_modules/plearn: pybridge
	pyext
Message-ID: <200805061459.m46ExxuW006562@sheep.berlios.de>

Author: saintmlx
Date: 2008-05-06 16:59:58 +0200 (Tue, 06 May 2008)
New Revision: 8943

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
   trunk/python_modules/plearn/pyext/__init__.py
Log:
- wrapper for VMats



Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-05-06 14:58:40 UTC (rev 8942)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-05-06 14:59:58 UTC (rev 8943)
@@ -144,8 +144,6 @@
 from numpy.numarray import *
 
 class WrappedPLearnVMat(WrappedPLearnObject):
-    def __init__(self, cptr):
-        WrappedPLearnObject.__init__(self, cptr)
 
     def __len__(self):
         return self.length

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2008-05-06 14:58:40 UTC (rev 8942)
+++ trunk/python_modules/plearn/pyext/__init__.py	2008-05-06 14:59:58 UTC (rev 8943)
@@ -86,8 +86,8 @@
         sys.exit()
     loggingControl(verb, logs)
 
-pl.AutoVMatrix()
-AutoVMatrix.__len__ = lambda self: self.length
+#pl.AutoVMatrix()
+#AutoVMatrix.__len__ = lambda self: self.length
 
 if __name__ == "__main__":
     class A(plargs):



From saintmlx at mail.berlios.de  Tue May  6 17:02:41 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 6 May 2008 17:02:41 +0200
Subject: [Plearn-commits] r8944 - trunk/python_modules/plearn/learners
Message-ID: <200805061502.m46F2f8U006922@sheep.berlios.de>

Author: saintmlx
Date: 2008-05-06 17:02:41 +0200 (Tue, 06 May 2008)
New Revision: 8944

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
- avoid div by 0; return nan instead



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-05-06 14:59:58 UTC (rev 8943)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-05-06 15:02:41 UTC (rev 8944)
@@ -1351,13 +1351,18 @@
             print "SVM::test() called on ",len(targets)," samples"
             print "class priors: ", class_priors
 
-        cm_weights = [ 1./class_priors[t] for t in range(nclasses) ]
+        cm_weights = [ (class_priors.get(t,0.)!=0. and 1./class_priors.get(t,0.)) or float('nan') for t in range(nclasses) ]
         if 'norm_ce' in self.costnames:
             if self.errorcosts == None:        
                 errorcosts = zeros((nclasses,nclasses))
                 for classe in range(nclasses):
                     for prediction in range(classe)+range(classe+1,nclasses):
-                        errorcosts[classe,prediction] = 1./ ( nclasses * class_priors[classe] )
+                        cp= class_priors.get(classe,0.)
+                        if cp != 0.:
+                            errorcosts[classe,prediction] = 1./ ( nclasses * cp )
+                        else:
+                            errorcosts[classe,prediction] = float('nan')
+                            
             else:
                 errorcosts = self.errorcosts
 



From tihocan at mail.berlios.de  Tue May  6 17:24:16 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 6 May 2008 17:24:16 +0200
Subject: [Plearn-commits] r8945 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200805061524.m46FOGrb008803@sheep.berlios.de>

Author: tihocan
Date: 2008-05-06 17:24:14 +0200 (Tue, 06 May 2008)
New Revision: 8945

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
- Using tcsh instead of csh (I believe that's what most people not using bash are using at LISA)
- Added ability to have a CONDOR_HOME environment variable to set the HOME variable for jobs run with condor


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-06 15:02:41 UTC (rev 8944)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-06 15:24:14 UTC (rev 8945)
@@ -802,6 +802,7 @@
             req = req+'&&('+self.req+')'
 
         source_file=os.getenv("CONDOR_LOCAL_SOURCE")
+        condor_home = os.getenv('CONDOR_HOME')
         if source_file and source_file.endswith(".cshrc"):
             launch_file = os.path.join(self.log_dir, 'launch.csh')
         else:
@@ -864,11 +865,12 @@
             self.temp_files.append(launch_file)
             launch_dat = open(launch_file,'w')
             if source_file and not source_file.endswith(".cshrc"):
-
                 launch_dat.write(dedent('''\
                 #!/bin/sh
                 PROGRAM=$1
                 shift\n'''))
+                if condor_home:
+                    launch_dat.write('export HOME=%s\n' % condor_home)
                 if source_file:
                     launch_dat.write('source ' + source_file + '\n')
                 launch_dat.write(dedent('''\
@@ -886,8 +888,10 @@
                     ${PROGRAM} "$@"'''))
             else:
                 launch_dat.write(dedent('''\
-                    #!/bin/csh
+                    #! /bin/tcsh
                     \n'''))
+                if condor_home:
+                    launch_dat.write('setenv HOME %s\n' % condor_home)
                 if source_file:
                     launch_dat.write('source ' + source_file + '\n')
                 launch_dat.write(dedent('''\
@@ -903,7 +907,9 @@
                     #/usr/bin/python -V
                     #echo ${PROGRAM} $@
                     #${PROGRAM} "$@"
-                    $argv'''))
+                    echo "Running command: $argv"
+                    $argv
+                    '''))
             launch_dat.close()
             os.chmod(launch_file, 0755)
 

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-06 15:02:41 UTC (rev 8944)
+++ trunk/scripts/dbidispatch	2008-05-06 15:24:14 UTC (rev 8945)
@@ -77,6 +77,8 @@
      dbidispatch '--req=regexp("computer0*", target.Machine)'
   The '--nice'('--no_nice') option set the nice_user option to condor. If nice, the job(s) will have the lowest possible priority.
   The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
+  If the CONDOR_HOME environment variable is set, then the HOME variable will
+     be set to this value for jobs submitted to condor.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.



From louradou at mail.berlios.de  Tue May  6 17:39:24 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 6 May 2008 17:39:24 +0200
Subject: [Plearn-commits] r8946 - trunk/python_modules/plearn/learners
Message-ID: <200805061539.m46FdOGi009920@sheep.berlios.de>

Author: louradou
Date: 2008-05-06 17:39:21 +0200 (Tue, 06 May 2008)
New Revision: 8946

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
fixed a bug with the forget



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-05-06 15:24:14 UTC (rev 8945)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-05-06 15:39:21 UTC (rev 8946)
@@ -102,9 +102,12 @@
         close to previous ones.
     """
     def semiforget(self):
+        if self.verbosity > 3:
+            print "  (forget called)"
         self.trials_param_list  = []
         self.trials_cost_list = []
         self.stats_are_uptodate = False
+        self.best_cost = None
         # Note: when we forget, we keep the value of
         #       'self.best_param'. This allow to initialize
         #       a new search (when data changed a bit) to



From tihocan at mail.berlios.de  Tue May  6 18:37:35 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 6 May 2008 18:37:35 +0200
Subject: [Plearn-commits] r8947 - trunk/python_modules/plearn/parallel
Message-ID: <200805061637.m46GbZHw028211@sheep.berlios.de>

Author: tihocan
Date: 2008-05-06 18:37:34 +0200 (Tue, 06 May 2008)
New Revision: 8947

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Can now handle executables in PATH rather than only those in the current directory

Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-06 15:39:21 UTC (rev 8946)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-06 16:37:34 UTC (rev 8947)
@@ -1,12 +1,13 @@
 #! /usr/bin/env python
-import sys
 import os
 import getopt
 import re
+import shutil
 import string
+import subprocess
+import sys
 import time
 import traceback
-import shutil
 from subprocess import Popen,PIPE,STDOUT
 from utils import *
 from configobj import ConfigObj
@@ -691,21 +692,32 @@
         # create the information about the tasks
         id=len(self.tasks)+1
         for command in commands:
-            pos = string.find(command,' ')
-            if pos>=0:
-                c = command[0:pos]
-                c2 = command[pos:]
-            else:
-                c=command
-                c2=""
+	    c_split = command.split()
+            # c = program name, c2 = arguments
+	    c = c_split[0]
+	    if len(c_split) > 1:
+	    	c2 = ' ' + ' '.join(c_split[1:])
+	    else:
+		c2 = ''
 
             # We use the absolute path so that we don't have corner case as with ./
             shellcommand=False
-            autorized_shell_command=[ "touch", "echo"]
-            if c in autorized_shell_command:
+            authorized_shell_commands=[ "touch", "echo"]
+            if c in authorized_shell_commands:
                 shellcommand=True
             elif not self.files:
-                c = os.path.normpath(os.path.join(os.getcwd(), c))
+		# Transform path to get an absolute path.
+                c_abs = os.path.abspath(c)
+		if os.path.isfile(c_abs):
+		    # The file is in the current directory (easy case).
+		    c = c_abs
+		elif not os.path.isabs(c):
+		    # We need to find where the file could be... easiest way to
+                    # do it is ask the 'which' shell command.
+	 	    which_out = subprocess.Popen('which %s' % c, shell = True, stdout = PIPE).stdout.readlines()
+		    if len(which_out) == 1:
+			c = which_out[0].strip()
+
             command = "".join([c,c2])
 
                 # We will execute the command on the specified architecture
@@ -756,7 +768,7 @@
             if shellcommand:
                 pass
             elif not os.path.exists(c):
-                raise Exception("The command '"+c+"' does not exist! You must provide the full path to the executable")
+                raise Exception("The command '"+c+"' does not exist!")
             elif not os.access(c, os.X_OK):
                 raise Exception("The command '"+c+"' does not have execution permission!")
 



From tihocan at mail.berlios.de  Tue May  6 18:38:14 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 6 May 2008 18:38:14 +0200
Subject: [Plearn-commits] r8948 - trunk/scripts
Message-ID: <200805061638.m46GcETF028642@sheep.berlios.de>

Author: tihocan
Date: 2008-05-06 18:38:13 +0200 (Tue, 06 May 2008)
New Revision: 8948

Modified:
   trunk/scripts/dbidispatch
Log:
Minor typo fix

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-06 16:37:34 UTC (rev 8947)
+++ trunk/scripts/dbidispatch	2008-05-06 16:38:13 UTC (rev 8948)
@@ -245,7 +245,7 @@
 for i in dbi_param:
     if i not in valid_dbi_param:
         print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end"
-print "With the command to be expended:"," ".join(command_argv),"\n\n"
+print "With the command to be expanded:"," ".join(command_argv),"\n\n"
 
 def generate_combination(repl):
     if repl == []:



From louradou at mail.berlios.de  Tue May  6 21:41:22 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 6 May 2008 21:41:22 +0200
Subject: [Plearn-commits] r8949 - trunk/python_modules/plearn/learners
Message-ID: <200805061941.m46JfMvR022418@sheep.berlios.de>

Author: louradou
Date: 2008-05-06 21:41:21 +0200 (Tue, 06 May 2008)
New Revision: 8949

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
fixed a problem when a class is not present in the test set.



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-05-06 16:38:13 UTC (rev 8948)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-05-06 19:41:21 UTC (rev 8949)
@@ -8,8 +8,8 @@
 from numpy.numarray import *
 from math import *
 import random
+import fpconst
 
-
 class SVMHyperParamOracle__kernel(object):
     """ An oracle that gives values of hyperparameters      
         to train a SVM on vectors, given a popular kernel.
@@ -1354,7 +1354,7 @@
             print "SVM::test() called on ",len(targets)," samples"
             print "class priors: ", class_priors
 
-        cm_weights = [ (class_priors.get(t,0.)!=0. and 1./class_priors.get(t,0.)) or float('nan') for t in range(nclasses) ]
+        cm_weights = [ (class_priors.get(t,0.)!=0. and 1./class_priors.get(t,0.)) or fpconst.NaN for t in range(nclasses) ]
         if 'norm_ce' in self.costnames:
             if self.errorcosts == None:        
                 errorcosts = zeros((nclasses,nclasses))
@@ -1364,7 +1364,7 @@
                         if cp != 0.:
                             errorcosts[classe,prediction] = 1./ ( nclasses * cp )
                         else:
-                            errorcosts[classe,prediction] = float('nan')
+                            errorcosts[classe,prediction] = fpconst.NaN
                             
             else:
                 errorcosts = self.errorcosts
@@ -1383,6 +1383,8 @@
                     c2 = int(cn[4])
                     if c1 == truth and c2 == prediction:
                         statVec.append(cm_weights[truth])
+                    elif fpconst.isNaN(cm_weights[c1]):
+                        statVec.append(fpconst.NaN)
                     else:
                         statVec.append(0.)
                 elif cn == 'norm_ce':
@@ -1392,7 +1394,7 @@
                 else:
                     raise ValueError, "computation of cost %s not implemented in SVM::test()" % cn
             teststats.update(statVec,1.)
-        
+
         return teststats #, outputs, costs
 
 



From saintmlx at mail.berlios.de  Wed May  7 16:46:32 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 7 May 2008 16:46:32 +0200
Subject: [Plearn-commits] r8950 - in trunk: plearn/python
	python_modules/plearn/pybridge
Message-ID: <200805071446.m47EkW9V024328@sheep.berlios.de>

Author: saintmlx
Date: 2008-05-07 16:46:31 +0200 (Wed, 07 May 2008)
New Revision: 8950

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- added support for RealRange in PLearn<->Python bridge



Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-05-06 19:41:21 UTC (rev 8949)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-05-07 14:46:31 UTC (rev 8950)
@@ -56,6 +56,7 @@
 #include <plearn/base/RemoteTrampoline.h>
 #include <plearn/base/HelpSystem.h>
 #include <plearn/var/VarArray.h>
+#include <plearn/base/RealMapping.h> // for RealRange
 
 namespace PLearn {
 using namespace std;
@@ -306,6 +307,40 @@
     return static_cast<VarArray>(ConvertFromPyObject<TVec<Var> >::convert(pyobj, print_traceback));
 }
 
+RealRange ConvertFromPyObject<RealRange>::convert(PyObject* pyobj, bool print_traceback)
+{
+    PLASSERT(pyobj);
+    PyObject* py_leftbracket= PyObject_GetAttrString(pyobj, "leftbracket");
+    if(!py_leftbracket)
+        PLPythonConversionError("ConvertFromPyObject<RealRange>: "
+                                "not a RealRange (no 'leftbracket' attr.)",
+                                pyobj, print_traceback);
+    string leftbracket= ConvertFromPyObject<string>::convert(py_leftbracket, print_traceback);
+    Py_DECREF(py_leftbracket);
+    PyObject* py_low= PyObject_GetAttrString(pyobj, "low");
+    if(!py_low) 
+        PLPythonConversionError("ConvertFromPyObject<RealRange>: "
+                                "not a RealRange (no 'low' attr.)",
+                                pyobj, print_traceback);
+    real low= ConvertFromPyObject<real>::convert(py_low, print_traceback);
+    Py_DECREF(py_low);
+    PyObject* py_high= PyObject_GetAttrString(pyobj, "high");
+    if(!py_high) 
+        PLPythonConversionError("ConvertFromPyObject<RealRange>: "
+                                "not a RealRange (no 'high' attr.)",
+                                pyobj, print_traceback);
+    real high= ConvertFromPyObject<real>::convert(py_high, print_traceback);
+    Py_DECREF(py_high);
+    PyObject* py_rightbracket= PyObject_GetAttrString(pyobj, "rightbracket");
+    if(!py_rightbracket) 
+        PLPythonConversionError("ConvertFromPyObject<RealRange>: "
+                                "not a RealRange (no 'rightbracket' attr.)",
+                                pyobj, print_traceback);
+    string rightbracket= ConvertFromPyObject<string>::convert(py_rightbracket, print_traceback);
+    Py_DECREF(py_rightbracket);
+    return RealRange(leftbracket[0], low, high, rightbracket[0]);
+}
+
 template<> int numpyType<bool>()               { return NPY_BOOL; }
 template<> int numpyType<signed char>()        { return NPY_BYTE; }
 template<> int numpyType<unsigned char>()      { return NPY_UBYTE; }
@@ -770,6 +805,31 @@
     return ConvertToPyObject<TVec<Var> >::newPyObject(var);
 }
 
+PyObject* ConvertToPyObject<RealRange>::newPyObject(const RealRange& rr)
+{
+    string pycode= "\nfrom plearn.pybridge.wrapped_plearn_object import RealRange\n";
+    pycode+= string("\nresult= RealRange(leftbracket='") + rr.leftbracket + "', "
+        + "low= " + tostring(rr.low) + ", high= " + tostring(rr.high) + ", "
+        + "rightbracket= '" + rr.rightbracket + "')\n";
+    PyObject* env= PyDict_New();
+    if(0 != PyDict_SetItemString(env, "__builtins__", PyEval_GetBuiltins()))
+        PLERROR("in ConvertToPyObject<RealRange>::newPyObject : "
+                "cannot insert builtins in env.");
+    PyObject* res= PyRun_String(pycode.c_str(), Py_file_input, env, env);
+    if(!res)
+    {
+        Py_DECREF(env);
+        if(PyErr_Occurred()) PyErr_Print();
+        PLERROR("in ConvertToPyObject<RealRange>::newPyObject : "
+                "cannot convert to a RealRange.");
+    }
+    Py_DECREF(res);
+    PyObject* py_rr= PythonObjectWrapper(env).as<std::map<string, PyObject*> >()["result"];
+    Py_INCREF(py_rr);
+    Py_DECREF(env);
+    return py_rr;
+}
+
 PStream& operator>>(PStream& in, PythonObjectWrapper& v)
 {
     PLERROR("operator>>(PStream&, PythonObjectWrapper&) : "

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2008-05-06 19:41:21 UTC (rev 8949)
+++ trunk/plearn/python/PythonObjectWrapper.h	2008-05-07 14:46:31 UTC (rev 8950)
@@ -83,6 +83,7 @@
 class Object;
 class VMatrix;
 class VarArray;
+class RealRange;
 
 //! Used for error reporting.  If 'print_traceback' is true, a full
 //! Python traceback is printed to stderr.  Otherwise, raise PLERROR.
@@ -451,6 +452,11 @@
     static VarArray convert(PyObject*, bool print_traceback);
 };
 
+template <>
+struct ConvertFromPyObject<RealRange>
+{
+    static RealRange convert(PyObject*, bool print_traceback);
+};
 
 /*****
  * Equivalence of types C++ -> numpy
@@ -691,6 +697,9 @@
 template <> struct ConvertToPyObject<VarArray>
 { static PyObject* newPyObject(const VarArray&); };
 
+template <> struct ConvertToPyObject<RealRange>
+{ static PyObject* newPyObject(const RealRange&); };
+
 struct PLPyClass
 {
     // holds info about a PLearn class

Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-05-06 19:41:21 UTC (rev 8949)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-05-07 14:46:31 UTC (rev 8950)
@@ -167,3 +167,12 @@
             raise NotImplementedError, 'slice step != 1'
         raise TypeError, "key should be an int or a slice"
     
+class RealRange:
+    """
+    To support PLearn<->Python conversion of RealRange (which is not a PLearn Object)
+    """
+    def __init__(self, leftbracket, low, high, rightbracket):
+        self.leftbracket = leftbracket
+        self.low = low
+        self.high = high
+        self.rightbracket = rightbracket



From tihocan at mail.berlios.de  Thu May  8 17:44:35 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 May 2008 17:44:35 +0200
Subject: [Plearn-commits] r8951 - trunk/plearn/vmat
Message-ID: <200805081544.m48FiZf1026407@sheep.berlios.de>

Author: tihocan
Date: 2008-05-08 17:44:34 +0200 (Thu, 08 May 2008)
New Revision: 8951

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
Log:
Added explicit error message when trying to replicate samples from a class that actually does not exist

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-05-07 14:46:31 UTC (rev 8950)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-05-08 15:44:34 UTC (rev 8951)
@@ -150,9 +150,14 @@
             bag_sizes[bag_start_idx]++;
     }
     int max_n = -1;
-    for (int c = 0; c < class_indices.length(); c++)
+    for (int c = 0; c < class_indices.length(); c++) {
         if (class_indices[c].length() > max_n)
             max_n = class_indices[c].length();
+        if (class_indices[c].isEmpty())
+            PLERROR("In ReplicateSamplesVMatrix::build_ - Cannot replicate "
+                    "samples for class %d since there are zero samples from "
+                    "this class", c);
+    }
     for (int c = 0; c < class_indices.length(); c++) {
         int n_replicated = max_n - class_indices[c].length();
         for (int i = 0; i < n_replicated; i++) {



From tihocan at mail.berlios.de  Thu May  8 17:45:50 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 May 2008 17:45:50 +0200
Subject: [Plearn-commits] r8952 - in trunk: commands plearn/vmat
Message-ID: <200805081545.m48Fjogj026565@sheep.berlios.de>

Author: tihocan
Date: 2008-05-08 17:45:50 +0200 (Thu, 08 May 2008)
New Revision: 8952

Added:
   trunk/plearn/vmat/PutSubVMatrix.cc
   trunk/plearn/vmat/PutSubVMatrix.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
New VMat to put one VMat into another one

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-05-08 15:44:34 UTC (rev 8951)
+++ trunk/commands/plearn_noblas_inc.h	2008-05-08 15:45:50 UTC (rev 8952)
@@ -336,6 +336,7 @@
 #include <plearn/vmat/ProcessDatasetVMatrix.h>
 #include <plearn/vmat/ProcessingVMatrix.h>
 #include <plearn/vmat/ProcessSymbolicSequenceVMatrix.h>
+#include <plearn/vmat/PutSubVMatrix.h>
 #include <plearn/vmat/RandomSamplesVMatrix.h>
 #include <plearn/vmat/RandomSamplesFromVMatrix.h>
 #include <plearn/vmat/RankedVMatrix.h>

Added: trunk/plearn/vmat/PutSubVMatrix.cc
===================================================================
--- trunk/plearn/vmat/PutSubVMatrix.cc	2008-05-08 15:44:34 UTC (rev 8951)
+++ trunk/plearn/vmat/PutSubVMatrix.cc	2008-05-08 15:45:50 UTC (rev 8952)
@@ -0,0 +1,153 @@
+// -*- C++ -*-
+
+// PutSubVMatrix.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file PutSubVMatrix.cc */
+
+
+#include "PutSubVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    PutSubVMatrix,
+    "Replace a sub-matrix of its source by another VMat.",
+    "Keep in mind that field names and string mappings of the imputed submat\n"
+    "will be ignored."
+);
+
+//////////////////
+// PutSubVMatrix //
+//////////////////
+PutSubVMatrix::PutSubVMatrix():
+    istart(0),
+    jstart(0)
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+void PutSubVMatrix::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "submat", &PutSubVMatrix::submat,
+                  OptionBase::buildoption,
+        "The data matrix to put in the source.");
+
+    declareOption(ol, "istart", &PutSubVMatrix::istart,
+                  OptionBase::buildoption,
+        "Row index where 'submat' will be put.");
+
+    declareOption(ol, "jstart", &PutSubVMatrix::jstart,
+                  OptionBase::buildoption,
+        "Column index where 'submat' will be put.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void PutSubVMatrix::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void PutSubVMatrix::build_()
+{
+    if (!source)
+        return;
+
+    if (submat) {
+        updateMtime(submat);
+        PLCHECK( istart + submat.length() <= source->length() );
+        PLCHECK( jstart + submat.width() <= source->width() );
+        subrow.resize(submat->width());
+    }
+
+    setMetaInfoFromSource();
+}
+
+///////////////
+// getNewRow //
+///////////////
+void PutSubVMatrix::getNewRow(int i, const Vec& v) const
+{
+    source->getRow(i, v);
+    if (submat && submat->length() > 0 && subrow.length() > 0) {
+        if (i >= istart && i < istart + submat->length()) {
+            submat->getRow(i - istart, subrow);
+            for (int j = 0; j < subrow.length(); j++)
+                v[jstart + j] = subrow[j];
+        }
+    }
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void PutSubVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("PutSubVMatrix::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/PutSubVMatrix.h
===================================================================
--- trunk/plearn/vmat/PutSubVMatrix.h	2008-05-08 15:44:34 UTC (rev 8951)
+++ trunk/plearn/vmat/PutSubVMatrix.h	2008-05-08 15:45:50 UTC (rev 8952)
@@ -0,0 +1,138 @@
+// -*- C++ -*-
+
+// PutSubVMatrix.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file PutSubVMatrix.h */
+
+
+#ifndef PutSubVMatrix_INC
+#define PutSubVMatrix_INC
+
+#include <plearn/vmat/SourceVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class PutSubVMatrix : public SourceVMatrix
+{
+    typedef SourceVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int istart;
+    int jstart;
+    VMat submat;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    PutSubVMatrix();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(PutSubVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    Vec subrow;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+    //! Fill the vector 'v' with the content of the i-th row.
+    //! v is assumed to be the right size.
+    //! ### This function must be overridden in your class
+    virtual void getNewRow(int i, const Vec& v) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PutSubVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Thu May  8 18:54:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 8 May 2008 18:54:04 +0200
Subject: [Plearn-commits] r8953 - trunk/plearn/io
Message-ID: <200805081654.m48Gs4s4000169@sheep.berlios.de>

Author: nouiz
Date: 2008-05-08 18:54:02 +0200 (Thu, 08 May 2008)
New Revision: 8953

Modified:
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/fileutils.h
Log:
added isemptyFile(filename)


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-05-08 15:45:50 UTC (rev 8952)
+++ trunk/plearn/io/fileutils.cc	2008-05-08 16:54:02 UTC (rev 8953)
@@ -147,6 +147,19 @@
         return fi.type == PR_FILE_FILE;
 }
 
+////////////
+// isfile //
+////////////
+bool isemptyFile(const PPath& path)
+{
+    PRFileInfo64 fi;
+
+    if (PR_GetFileInfo64(path.absolute().c_str(), &fi) != PR_SUCCESS)
+        return false;
+    else
+        return (fi.type == PR_FILE_FILE) && (fi.size == 0);
+}
+
 ///////////
 // mtime //
 ///////////

Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2008-05-08 15:45:50 UTC (rev 8952)
+++ trunk/plearn/io/fileutils.h	2008-05-08 16:54:02 UTC (rev 8953)
@@ -75,6 +75,9 @@
 //! Returns true if the given path is an existing regular file (or a symbolic link pointing to a file).
 bool isfile(const PPath& path);
 
+//! Returns true if the given path is an existing regular file (or a symbolic link pointing to a file) and the size of the file is 0.
+bool isemptyFile(const PPath& path);
+
 //! Returns the time of last modification of file (or 0 if file does not exist).
 time_t mtime(const PPath& path);
 



From nouiz at mail.berlios.de  Thu May  8 18:58:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 8 May 2008 18:58:55 +0200
Subject: [Plearn-commits] r8954 - trunk/plearn/vmat
Message-ID: <200805081658.m48GwtaE006573@sheep.berlios.de>

Author: nouiz
Date: 2008-05-08 18:58:44 +0200 (Thu, 08 May 2008)
New Revision: 8954

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/plearn/vmat/TextFilesVMatrix.h
Log:
Added the option partion_match.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-05-08 16:54:02 UTC (rev 8953)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-05-08 16:58:44 UTC (rev 8954)
@@ -42,6 +42,7 @@
 #include <plearn/base/ProgressBar.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/io/load_and_save.h>
+#include <plearn/io/fileutils.h>
 
 namespace PLearn {
 using namespace std;
@@ -56,7 +57,8 @@
     auto_build_map(false),
     auto_extend_map(true),
     build_vmatrix_stringmap(false),
-    reorder_fieldspec_from_headers(false)
+    reorder_fieldspec_from_headers(false),
+    partial_match(false)
 {}
 
 PLEARN_IMPLEMENT_OBJECT(
@@ -192,37 +194,77 @@
 {
     width_ = 0;
     TVec<string> fnames;
-    if(reorder_fieldspec_from_headers)
+    TVec<string> fnames_header;//field names take in the header of source file
+    if(reorder_fieldspec_from_headers || partial_match)
     {
         //read the fieldnames from the files.
-        TVec<string> fn;
         for(int i=0; i<txtfiles.size(); i++)
         {
             FILE* f = txtfiles[i];
             fseek(f,0,SEEK_SET);
             if(!fgets(buf, sizeof(buf), f))
                 PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth() - "
-                        "Couldn't read the fiedls names from file '%s'",
+                        "Couldn't read the fields names from file '%s'",
                         txtfilenames[i].c_str());
             fseek(f,0,SEEK_SET);
 
             TVec<string> fields = splitIntoFields(buf);
             fields.append(removeblanks(fields.pop()));
 
-            fn.append(fields);
+            fnames_header.append(fields);
         }
-        if(fn.size()!=fieldspec.size())
+    }
+    if(partial_match)
+    {
+        TVec< pair<string, string> > new_fieldspec;
+        PLCHECK_MSG(reorder_fieldspec_from_headers,
+                    "In TextFilesVMatrix::setColumnNamesAndWidth - "
+                    "when partial_match is true, reorder_fieldspec_from_headers"
+                    " must be true.");
+        for(int i=0;i<fieldspec.size();i++)
         {
+            bool expended = false;
+            string fname=fieldspec[i].first;
+            if(fname[fname.size()-1]!='*')
+            {
+                new_fieldspec.append(fieldspec[i]);
+                continue;
+            }
+            fname.resize(fname.size()-1);//remove the last caracter (*)
+            for(int j=0;j<fnames_header.size();j++)
+            {
+                if(string_begins_with(fnames_header[j],fname))
+                {
+                    pair<string,string> n=make_pair(fnames_header[j],
+                                                    fieldspec[i].second);
+//                    perr<<"expanding "<<fieldspec[i] << " to " << n <<endl;
+                    
+                    new_fieldspec.append(n);
+                    expended = true;
+                }
+            }
+            if(!expended)
+                PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth - "
+                        "Don't have find any partial match to %s",
+                        fieldspec[i].first.c_str());
+        }
+        fieldspec = new_fieldspec;
+    }
+
+    if(reorder_fieldspec_from_headers)
+    {
+        if(fnames_header.size()!=fieldspec.size())
+        {
             PLWARNING("In TextFilesVMatrix::setColumnNamesAndWidth() - "
                     "We read %d field names from the header but have %d"
-                    "fieldspec",fn.size(),fieldspec.size());
+                    "fieldspec",fnames_header.size(),fieldspec.size());
         }
 
         //check that all field names from the header have a spec
         TVec<string> not_used_fn;
-        for(int i=0;i<fn.size();i++)
+        for(int i=0;i<fnames_header.size();i++)
         {
-            string name=fn[i];
+            string name=fnames_header[i];
             int j=0;
             for(;j<fieldspec.size();j++)
                 if(fieldspec[j].first==name)
@@ -236,10 +278,10 @@
         {
             string name=fieldspec[i].first;
             int j=0;
-            for(;j<fn.size();j++)
-                if(fn[j]==name)
+            for(;j<fnames_header.size();j++)
+                if(fnames_header[j]==name)
                     break;
-            if(j>=fn.size())
+            if(j>=fnames_header.size())
                 not_used_fs.append(name);
         }
         if(not_used_fs.size()!=0)
@@ -255,10 +297,10 @@
     
 
         //the new order for fieldspecs
-        TVec< pair<string, string> > fs(fn.size());
-        for(int i=0;i<fn.size();i++)
+        TVec< pair<string, string> > fs(fnames_header.size());
+        for(int i=0;i<fnames_header.size();i++)
         {
-            string name=fn[i];
+            string name=fnames_header[i];
             int j=0;
             for(;j<fieldspec.size();j++)
                 if(fieldspec[j].first==name)
@@ -328,11 +370,13 @@
     setColumnNamesAndWidth();
 
     // open the index file
-    if(!isUpToDate(idxfname))
+    if(!isUpToDate(idxfname) || isemptyFile(idxfname))
         buildIdx(); // (re)build it first!
     idxfile = fopen(idxfname.c_str(),"rb");
     if(fgetc(idxfile) != byte_order())
-        PLERROR("In TextFilesVMatrix::build_ - Wrong endianness. Remove the index file for it to be automatically rebuilt");
+        PLERROR("In TextFilesVMatrix::build_ - Wrong endianness."
+                " Remove the index file %s for it to be automatically rebuilt",
+                idxfname.c_str());
     fread(&length_, 4, 1, idxfile);
 
     // Initialize some sizes
@@ -841,6 +885,14 @@
                   "If true, will reorder the fieldspec in the order given "
                   "by the field names taken from txtfilenames.");
 
+    declareOption(ol, "partial_match", 
+                  &TextFilesVMatrix::partial_match,
+                  OptionBase::buildoption,
+                  "If true, will repeatedly expand all fieldspec name ending "
+                  "with * to the full name from header."
+                  "The expansion is equivalent to the regex 'field_spec_name*'."
+                  "The option reorder_fieldspec_from_headers must be true");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -886,7 +938,7 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
     idxfile=0;
     txtfiles.resize(0);
-    //should be already build.
+    //the map should be already build.
     auto_build_map=false;
     build();
 }

Modified: trunk/plearn/vmat/TextFilesVMatrix.h
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.h	2008-05-08 16:54:02 UTC (rev 8953)
+++ trunk/plearn/vmat/TextFilesVMatrix.h	2008-05-08 16:58:44 UTC (rev 8954)
@@ -127,6 +127,7 @@
     bool build_vmatrix_stringmap;
 
     bool reorder_fieldspec_from_headers;
+    bool partial_match;
 
     // ****************
     // * Constructors *
@@ -149,8 +150,8 @@
     void getFileAndPos(int i, unsigned char& fileno, int& pos) const;
     void buildIdx();
     static void readAndCheckOptionName(PStream& in, const string& optionname);
+    void closeCurrentFile();
 
-
 protected:
     //! Declares this class' options
     // (Please implement in .cc)



From nouiz at mail.berlios.de  Thu May  8 20:44:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 8 May 2008 20:44:58 +0200
Subject: [Plearn-commits] r8955 - trunk/plearn/vmat
Message-ID: <200805081844.m48Iiwtj000551@sheep.berlios.de>

Author: nouiz
Date: 2008-05-08 20:44:58 +0200 (Thu, 08 May 2008)
New Revision: 8955

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
Print all the warning for fields not in source that we add with missing value in one warning.


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-05-08 16:58:44 UTC (rev 8954)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-05-08 18:44:58 UTC (rev 8955)
@@ -199,6 +199,7 @@
             // Find out the indices from the fields.
             indices.resize(0);
             if (!fields_partial_match) {
+                TVec<string> missing_field;
                 for (int i = 0; i < fields.length(); i++) {
                     string the_field = fields[i];
                     int the_index = source->fieldIndex(the_field);  // string only
@@ -232,12 +233,14 @@
                     } else
                         indices.append(the_index);
                     if(extend_with_missing && the_index == -1)
-                        PLWARNING("In SelectColumnsVMatrix::build_() - We are"
-                                  " extending the source matrix with the"
-                                  " columns '%s' with missing value",
-                                  the_field.c_str());
-                    
+                        missing_field.append(the_field);
                 }
+                if(missing_field.size()>0){
+                    PLWARNING("In SelectColumnsVMatrix::build_() - We are"
+                              " added %d columns to the source matrix with missing value"
+                              " columns names: %s",missing_field.size(),
+                              tostring(missing_field).c_str());
+                }
             } else {
                 // We need to check whether or not we should add each field.
                 TVec<string> source_fields = source->fieldNames();



From tihocan at mail.berlios.de  Thu May  8 21:19:04 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 May 2008 21:19:04 +0200
Subject: [Plearn-commits] r8956 - trunk/plearn/var
Message-ID: <200805081919.m48JJ43I003198@sheep.berlios.de>

Author: tihocan
Date: 2008-05-08 21:19:03 +0200 (Thu, 08 May 2008)
New Revision: 8956

Modified:
   trunk/plearn/var/ConcatColumnsVariable.cc
   trunk/plearn/var/ConcatColumnsVariable.h
   trunk/plearn/var/DivVariable.cc
   trunk/plearn/var/DivVariable.h
   trunk/plearn/var/PlusConstantVariable.cc
   trunk/plearn/var/PlusConstantVariable.h
   trunk/plearn/var/PlusVariable.h
   trunk/plearn/var/RowSumSquareVariable.cc
   trunk/plearn/var/RowSumSquareVariable.h
   trunk/plearn/var/SoftplusVariable.cc
   trunk/plearn/var/SoftplusVariable.h
   trunk/plearn/var/SquareVariable.cc
   trunk/plearn/var/SquareVariable.h
Log:
Various small fixes to make these Variables closer to the usual Object standard.


Modified: trunk/plearn/var/ConcatColumnsVariable.cc
===================================================================
--- trunk/plearn/var/ConcatColumnsVariable.cc	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/ConcatColumnsVariable.cc	2008-05-08 19:19:03 UTC (rev 8956)
@@ -49,25 +49,37 @@
 
 /** ConcatColumnsVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(ConcatColumnsVariable,
-                        "Concatenation of the columns of several variables",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        ConcatColumnsVariable,
+        "Concatenation of the columns of several variables",
+        ""
+);
 
-ConcatColumnsVariable::ConcatColumnsVariable(const VarArray& vararray)
-    : inherited(vararray.nonNull(), vararray.maxLength(), vararray.sumOfWidths())
+///////////////////////////
+// ConcatColumnsVariable //
+///////////////////////////
+ConcatColumnsVariable::ConcatColumnsVariable(const VarArray& vararray,
+                                             bool call_build_):
+    inherited(vararray.nonNull(), vararray.maxLength(), vararray.sumOfWidths(),
+              call_build_)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-void
-ConcatColumnsVariable::build()
+///////////
+// build //
+///////////
+void ConcatColumnsVariable::build()
 {
     inherited::build();
     build_();
 }
 
-void
-ConcatColumnsVariable::build_()
+////////////
+// build_ //
+////////////
+void ConcatColumnsVariable::build_()
 {
     if (varray->length()) {
         int l = varray[0]->length();

Modified: trunk/plearn/var/ConcatColumnsVariable.h
===================================================================
--- trunk/plearn/var/ConcatColumnsVariable.h	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/ConcatColumnsVariable.h	2008-05-08 19:19:03 UTC (rev 8956)
@@ -55,11 +55,13 @@
     typedef NaryVariable inherited;
 
 public:
-    //!  default constructor for persistence
+
+    //! Default constructor.
     ConcatColumnsVariable() {}
-    //!  all the variables must have the same number of rows
-    ConcatColumnsVariable(const VarArray& vararray);
 
+    //! Convenience constructor.
+    ConcatColumnsVariable(const VarArray& vararray, bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(ConcatColumnsVariable);
 
     virtual void build();
@@ -69,7 +71,8 @@
     virtual void bprop();
     virtual void symbolicBprop();
 
-protected:
+private:
+
     void build_();
 };
 

Modified: trunk/plearn/var/DivVariable.cc
===================================================================
--- trunk/plearn/var/DivVariable.cc	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/DivVariable.cc	2008-05-08 19:19:03 UTC (rev 8956)
@@ -50,17 +50,25 @@
 
 /** DivVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(DivVariable,
-                        "Divide 2 matrix vars of same size elementwise",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        DivVariable,
+        "Divide 2 matrices of same size elementwise",
+        ""
+);
 
-DivVariable::DivVariable(Variable* input1, Variable* input2)
-    : BinaryVariable(input1, input2, input1->length(), input1->width())
+/////////////////
+// DivVariable //
+/////////////////
+DivVariable::DivVariable(Variable* input1, Variable* input2, bool call_build_):
+    inherited(input1, input2, input1->length(), input1->width(), call_build_)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-
+///////////////////
+// recomputeSize //
+///////////////////
 void DivVariable::recomputeSize(int& l, int& w) const
 {
     if (input1) {
@@ -70,15 +78,19 @@
         l = w = 0;
 }
 
-void
-DivVariable::build()
+///////////
+// build //
+///////////
+void DivVariable::build()
 {
     inherited::build();
     build_();
 }
 
-void
-DivVariable::build_()
+////////////
+// build_ //
+////////////
+void DivVariable::build_()
 {
     if (input1 && input2)
         if(input1->length() != input2->length() || input1->width() != input2->width())

Modified: trunk/plearn/var/DivVariable.h
===================================================================
--- trunk/plearn/var/DivVariable.h	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/DivVariable.h	2008-05-08 19:19:03 UTC (rev 8956)
@@ -49,18 +49,18 @@
 using namespace std;
 
 
-/*! * Div... * */
-
-//!  divides 2 matrix vars of same size elementwise
 class DivVariable: public BinaryVariable
 {
     typedef BinaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+
+    //! Default constructor.
     DivVariable() {}
-    DivVariable(Variable* input1, Variable* input2);
 
+    //! Convenience constructor.
+    DivVariable(Variable* input1, Variable* input2, bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(DivVariable);
 
     virtual void build();
@@ -71,7 +71,8 @@
     virtual void symbolicBprop();
     virtual void rfprop();
 
-protected:
+private:
+
     void build_();
 };
 

Modified: trunk/plearn/var/PlusConstantVariable.cc
===================================================================
--- trunk/plearn/var/PlusConstantVariable.cc	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/PlusConstantVariable.cc	2008-05-08 19:19:03 UTC (rev 8956)
@@ -48,20 +48,52 @@
 
 /** PlusConstantVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(PlusConstantVariable,
-                        "Adds a scalar constant to a matrix var",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        PlusConstantVariable,
+        "Adds a scalar constant to a matrix Variable.",
+        ""
+);
 
-PlusConstantVariable::PlusConstantVariable(Variable* input, real c)
-    : inherited(input, input->length(), input->width()), cst(c) 
+//////////////////////////
+// PlusConstantVariable //
+//////////////////////////
+PlusConstantVariable::PlusConstantVariable():
+    cst(0)
 {}
 
+PlusConstantVariable::PlusConstantVariable(Variable* input, real c,
+                                           bool call_build_):
+    inherited(input, input->length(), input->width(), call_build_),
+    cst(c)
+{
+    if (call_build_)
+        build_();
+}
 
-void
-PlusConstantVariable::declareOptions(OptionList &ol)
+///////////
+// build //
+///////////
+void PlusConstantVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void PlusConstantVariable::build_() {
+    // Nothing to do here.
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void PlusConstantVariable::declareOptions(OptionList &ol)
 {
-    declareOption(ol, "cst", &PlusConstantVariable::cst, OptionBase::buildoption, "");
     inherited::declareOptions(ol);
+    declareOption(ol, "cst", &PlusConstantVariable::cst,
+                  OptionBase::buildoption,
+        "The constant to be added.");
 }
 
 void PlusConstantVariable::recomputeSize(int& l, int& w) const

Modified: trunk/plearn/var/PlusConstantVariable.h
===================================================================
--- trunk/plearn/var/PlusConstantVariable.h	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/PlusConstantVariable.h	2008-05-08 19:19:03 UTC (rev 8956)
@@ -59,17 +59,20 @@
     typedef UnaryVariable inherited;
 
 public:
-//protected:
-    real cst; //!<  The constant
 
+    real cst;
+
 public:
-    //!  Default constructor for persistence
-    PlusConstantVariable() : cst() {}
-    PlusConstantVariable(Variable* input, real c);
+    //! Default constructor.
+    PlusConstantVariable();
 
+    //! Convenience constructor.
+    PlusConstantVariable(Variable* input, real c, bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(PlusConstantVariable);
-    static void declareOptions(OptionList &ol);
 
+    virtual void build();
+
     virtual string info() const
     { return string("PlusConstant (+ ")+tostring(cst)+")"; }
 
@@ -78,6 +81,14 @@
     virtual void bprop();
     virtual void symbolicBprop();
     virtual void rfprop();
+
+protected:
+
+    static void declareOptions(OptionList &ol);
+
+private:
+
+    void build_();
 };
 
 DECLARE_OBJECT_PTR(PlusConstantVariable);

Modified: trunk/plearn/var/PlusVariable.h
===================================================================
--- trunk/plearn/var/PlusVariable.h	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/PlusVariable.h	2008-05-08 19:19:03 UTC (rev 8956)
@@ -70,7 +70,8 @@
     virtual void bbprop();
     virtual void symbolicBprop();
 
-protected:
+private:
+
     void build_();
 };
 

Modified: trunk/plearn/var/RowSumSquareVariable.cc
===================================================================
--- trunk/plearn/var/RowSumSquareVariable.cc	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/RowSumSquareVariable.cc	2008-05-08 19:19:03 UTC (rev 8956)
@@ -52,10 +52,34 @@
     ""
 );
 
-RowSumSquareVariable::RowSumSquareVariable(Variable* input)
-    : inherited(input, input->length(), 1)
-{}
+//////////////////////////
+// RowSumSquareVariable //
+//////////////////////////
+RowSumSquareVariable::RowSumSquareVariable(Variable* input, bool call_build_):
+    inherited(input, input->length(), 1, call_build_)
+{
+    if (call_build_)
+        build_();
+}
 
+///////////
+// build //
+///////////
+void RowSumSquareVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void RowSumSquareVariable::build_() {
+    // Nothing to do here.
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
 void RowSumSquareVariable::recomputeSize(int& l, int& w) const
 {
     if (input)

Modified: trunk/plearn/var/RowSumSquareVariable.h
===================================================================
--- trunk/plearn/var/RowSumSquareVariable.h	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/RowSumSquareVariable.h	2008-05-08 19:19:03 UTC (rev 8956)
@@ -52,16 +52,24 @@
     typedef UnaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+    //! Default constructor.
     RowSumSquareVariable() {}
-    RowSumSquareVariable(Variable* input);
 
+    //! Convenience constructor.
+    RowSumSquareVariable(Variable* input, bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(RowSumSquareVariable);
 
+    virtual void build();
+
     virtual void recomputeSize(int& l, int& w) const;
     virtual void fprop();
     virtual void bprop();
 
+private:
+
+    void build_();
+
 };
 
 DECLARE_OBJECT_PTR(RowSumSquareVariable);

Modified: trunk/plearn/var/SoftplusVariable.cc
===================================================================
--- trunk/plearn/var/SoftplusVariable.cc	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/SoftplusVariable.cc	2008-05-08 19:19:03 UTC (rev 8956)
@@ -51,14 +51,40 @@
 
 /** SoftplusVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(SoftplusVariable,
-                        "This is the primitive of a sigmoid: log(1+exp(x))",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        SoftplusVariable,
+       "This is the primitive of a sigmoid: log(1+exp(x)).",
+       ""
+);
 
-SoftplusVariable::SoftplusVariable(Variable* input) 
-    : inherited(input, input->length(), input->width())
-{}
+//////////////////////
+// SoftplusVariable //
+//////////////////////
+SoftplusVariable::SoftplusVariable(Variable* input, bool call_build_):
+    inherited(input, input->length(), input->width(), call_build_)
+{
+    if (call_build_)
+        build_();
+}
 
+///////////
+// build //
+///////////
+void SoftplusVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void SoftplusVariable::build_() {
+    // Nothing to do here.
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
 void SoftplusVariable::recomputeSize(int& l, int& w) const
 {
     if (input) {

Modified: trunk/plearn/var/SoftplusVariable.h
===================================================================
--- trunk/plearn/var/SoftplusVariable.h	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/SoftplusVariable.h	2008-05-08 19:19:03 UTC (rev 8956)
@@ -48,23 +48,30 @@
 namespace PLearn {
 using namespace std;
 
-
-//!  This is the primitive of a sigmoid: log(1+exp(x))
 class SoftplusVariable: public UnaryVariable
 {
     typedef UnaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+
+    //! Default constructor.
     SoftplusVariable() {}
-    SoftplusVariable(Variable* input);
 
+    //! Convenience constructor.
+    SoftplusVariable(Variable* input, bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(SoftplusVariable);
 
+    virtual void build();
+
     virtual void recomputeSize(int& l, int& w) const;
     virtual void fprop();
     virtual void bprop();
     virtual void symbolicBprop();
+
+private:
+
+    void build_();
 };
 
 DECLARE_OBJECT_PTR(SoftplusVariable);

Modified: trunk/plearn/var/SquareVariable.cc
===================================================================
--- trunk/plearn/var/SquareVariable.cc	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/SquareVariable.cc	2008-05-08 19:19:03 UTC (rev 8956)
@@ -50,14 +50,40 @@
 
 /** SquareVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(SquareVariable,
-                        "ONE LINE DESCR",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        SquareVariable,
+        "Element-wise square of the input matrix."
+        ""
+);
 
-SquareVariable::SquareVariable(Variable* input)
-    : inherited(input, input->length(), input->width())
-{}
+////////////////////
+// SquareVariable //
+////////////////////
+SquareVariable::SquareVariable(Variable* input, bool call_build_):
+    inherited(input, input->length(), input->width(), call_build_)
+{
+    if (call_build_)
+        build_();
+}
 
+///////////
+// build //
+///////////
+void SquareVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void SquareVariable::build_() {
+    // Nothing to do here.
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
 void SquareVariable::recomputeSize(int& l, int& w) const
 {
     if (input) {

Modified: trunk/plearn/var/SquareVariable.h
===================================================================
--- trunk/plearn/var/SquareVariable.h	2008-05-08 18:44:58 UTC (rev 8955)
+++ trunk/plearn/var/SquareVariable.h	2008-05-08 19:19:03 UTC (rev 8956)
@@ -54,12 +54,15 @@
     typedef UnaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+    //! Default constructor.
     SquareVariable() {}
-    SquareVariable(Variable* input);
 
+    SquareVariable(Variable* input, bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(SquareVariable);
 
+    virtual void build();
+
     virtual void recomputeSize(int& l, int& w) const;
     virtual void fprop();
     virtual void bprop();
@@ -67,6 +70,11 @@
     virtual void bbprop();
     virtual void symbolicBprop();
     virtual void rfprop();
+
+private:
+
+    void build_();
+
 };
 
 DECLARE_OBJECT_PTR(SquareVariable);



From tihocan at mail.berlios.de  Thu May  8 21:19:31 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 May 2008 21:19:31 +0200
Subject: [Plearn-commits] r8957 - trunk/plearn/var
Message-ID: <200805081919.m48JJVdI003230@sheep.berlios.de>

Author: tihocan
Date: 2008-05-08 21:19:31 +0200 (Thu, 08 May 2008)
New Revision: 8957

Modified:
   trunk/plearn/var/ProductVariable.cc
Log:
Minor cosmetic change in help

Modified: trunk/plearn/var/ProductVariable.cc
===================================================================
--- trunk/plearn/var/ProductVariable.cc	2008-05-08 19:19:03 UTC (rev 8956)
+++ trunk/plearn/var/ProductVariable.cc	2008-05-08 19:19:31 UTC (rev 8957)
@@ -53,9 +53,10 @@
 
 // Matrix product
 
-PLEARN_IMPLEMENT_OBJECT(ProductVariable,
-                        "Matrix product",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        ProductVariable,
+       "Matrix product.",
+       "");
 
 ProductVariable::ProductVariable(Variable* m1, Variable* m2)
     : inherited(m1, m2, m1->length(), m2->width())



From tihocan at mail.berlios.de  Thu May  8 21:20:26 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 May 2008 21:20:26 +0200
Subject: [Plearn-commits] r8958 - trunk/plearn/var
Message-ID: <200805081920.m48JKQCC003392@sheep.berlios.de>

Author: tihocan
Date: 2008-05-08 21:20:25 +0200 (Thu, 08 May 2008)
New Revision: 8958

Modified:
   trunk/plearn/var/SquareVariable.cc
Log:
Ooops, fixed compilation issue

Modified: trunk/plearn/var/SquareVariable.cc
===================================================================
--- trunk/plearn/var/SquareVariable.cc	2008-05-08 19:19:31 UTC (rev 8957)
+++ trunk/plearn/var/SquareVariable.cc	2008-05-08 19:20:25 UTC (rev 8958)
@@ -52,7 +52,7 @@
 
 PLEARN_IMPLEMENT_OBJECT(
         SquareVariable,
-        "Element-wise square of the input matrix."
+        "Element-wise square of the input matrix.",
         ""
 );
 



From tihocan at mail.berlios.de  Thu May  8 21:47:48 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 May 2008 21:47:48 +0200
Subject: [Plearn-commits] r8959 -
	trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results
Message-ID: <200805081947.m48JlmgU005689@sheep.berlios.de>

Author: tihocan
Date: 2008-05-08 21:47:48 +0200 (Thu, 08 May 2008)
New Revision: 8959

Modified:
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log
Log:
Fixed test PL_GaussMix_Generate

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log	2008-05-08 19:20:25 UTC (rev 8958)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log	2008-05-08 19:47:48 UTC (rev 8959)
@@ -1,3 +1,3 @@
- WARNING: In SelectColumnsVMatrix::build_() - We are extending the source matrix with the columns '2' with missing value
- WARNING: In SelectColumnsVMatrix::build_() - We are extending the source matrix with the columns '2' with missing value
- WARNING: In SelectColumnsVMatrix::build_() - We are extending the source matrix with the columns '2' with missing value
+ WARNING: In SelectColumnsVMatrix::build_() - We are added 1 columns to the source matrix with missing value columns names: 2 
+ WARNING: In SelectColumnsVMatrix::build_() - We are added 1 columns to the source matrix with missing value columns names: 2 
+ WARNING: In SelectColumnsVMatrix::build_() - We are added 1 columns to the source matrix with missing value columns names: 2 



From tihocan at mail.berlios.de  Thu May  8 22:31:33 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 May 2008 22:31:33 +0200
Subject: [Plearn-commits] r8960 - trunk/plearn_learners/generic
Message-ID: <200805082031.m48KVXic009298@sheep.berlios.de>

Author: tihocan
Date: 2008-05-08 22:31:32 +0200 (Thu, 08 May 2008)
New Revision: 8960

Modified:
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/generic/NNet.h
Log:
Added new 'ratio' transfer function for hidden units

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-05-08 19:47:48 UTC (rev 8959)
+++ trunk/plearn_learners/generic/NNet.cc	2008-05-08 20:31:32 UTC (rev 8960)
@@ -45,6 +45,7 @@
 #include <plearn/var/ClassificationLossVariable.h>
 #include <plearn/var/ConcatColumnsVariable.h>
 #include <plearn/var/CrossEntropyVariable.h>
+#include <plearn/var/DivVariable.h>
 #include <plearn/var/ExpVariable.h>
 #include <plearn/var/LiftOutputVariable.h>
 #include <plearn/var/LogSoftmaxVariable.h>
@@ -56,11 +57,15 @@
 #include <plearn/var/NegCrossEntropySigmoidVariable.h>
 #include <plearn/var/NegLogPoissonVariable.h>
 #include <plearn/var/OneHotSquaredLoss.h>
+#include <plearn/var/PlusConstantVariable.h>
+#include <plearn/var/PlusVariable.h>
 #include <plearn/var/ProductVariable.h>
-// #include "RBFLayerVariable.h" //TODO Put it back when the file exists.
+#include <plearn/var/RowSumSquareVariable.h>
 #include <plearn/var/SigmoidVariable.h>
 #include <plearn/var/SoftmaxVariable.h>
 #include <plearn/var/SoftplusVariable.h>
+#include <plearn/var/SquareVariable.h>
+#include <plearn/var/SquareRootVariable.h>
 #include <plearn/var/SumVariable.h>
 #include <plearn/var/SumAbsVariable.h>
 #include <plearn/var/SumOfVariable.h>
@@ -124,7 +129,8 @@
 transpose_first_hidden_layer(false),
 n_non_params_in_first_hidden_layer(0),
 batch_size(1),
-initialization_method("uniform_linear")
+initialization_method("uniform_linear"),
+ratio_rank(0)
 {
     // Use the generic PLearner random number generator.
     random_gen = new PRandom();
@@ -262,7 +268,9 @@
         "  - \"softmax\" \n"
         "  - \"log_softmax\" \n"
         "  - \"hard_slope\" \n"
-        "  - \"symm_hard_slope\" \n");
+        "  - \"symm_hard_slope\" \n"
+        "  - \"ratio\": e/(1+e) with e=sqrt(x'V'Vx + softplus(a)^2)\n"
+        "               with a=b+W'x and V a matrix of rank 'ratio_rank'");
 
     declareOption(
         ol, "cost_funcs", &NNet::cost_funcs, OptionBase::buildoption, 
@@ -373,7 +381,14 @@
         ol, "max_bag_size", &NNet::max_bag_size, OptionBase::buildoption,
         "Maximum number of samples in a bag (used with 'operate_on_bags').",
         OptionBase::advanced_level);
+    
+    declareOption(
+        ol, "ratio_rank", &NNet::ratio_rank, OptionBase::buildoption,
+        "Rank of matrix V when using the 'ratio' hidden transfer function.\n"
+        "Use -1 for full rank, and 0 to have no quadratic term.",
+        OptionBase::advanced_level);
 
+
     // Learnt options.
     
     declareOption(
@@ -758,7 +773,15 @@
     {
         w1 = Var(1 + the_input->width(), nhidden, "w1");      
         params.append(w1);
-        hidden_layer = hiddenLayer(output, w1);
+        if (hidden_transfer_func == "ratio") {
+            v1.resize(ratio_rank != 0 ? nhidden : 0);
+            for (int i = 0; i < v1.length(); i++) {
+                int rank = ratio_rank < 0 ? the_input->width() : ratio_rank;
+                v1[i] = Var(the_input->width(), rank, "v1[" + tostring(i) + "]");
+                params.append(v1[i]);
+            }
+        }
+        hidden_layer = hiddenLayer(output, w1, "default", &v1);
         output = hidden_layer;
         // TODO BEWARE! This is not the same 'hidden_layer' as before.
     }
@@ -769,7 +792,15 @@
         PLASSERT( !first_hidden_layer_is_output );
         w2 = Var(1 + output.width(), nhidden2, "w2");
         params.append(w2);
-        output = hiddenLayer(output, w2);
+        if (hidden_transfer_func == "ratio") {
+            v2.resize(ratio_rank != 0 ? nhidden2 : 0);
+            for (int i = 0; i < v2.length(); i++) {
+                int rank = ratio_rank < 0 ? output->width() : ratio_rank;
+                v2[i] = Var(output->width(), rank, "v2[" + tostring(i) + "]");
+                params.append(v2[i]);
+            }
+        }
+        output = hiddenLayer(output, w2, "default", &v2);
     }
 
     if (nhidden2>0 && nhidden==0 && !first_hidden_layer)
@@ -1039,7 +1070,8 @@
 /////////////////
 // hiddenLayer //
 /////////////////
-Var NNet::hiddenLayer(const Var& input, const Var& weights, string transfer_func) {
+Var NNet::hiddenLayer(const Var& input, const Var& weights, string transfer_func,
+                      VarArray* ratio_quad_weights) {
     Var hidden = affine_transform(input, weights, true);
     hidden->setName("hidden_layer_activations");
     Var result;
@@ -1063,6 +1095,28 @@
         result = unary_hard_slope(hidden,0,1);
     else if(transfer_func=="symm_hard_slope")
         result = unary_hard_slope(hidden,-1,1);
+    else if (transfer_func == "ratio") {
+        PLASSERT( ratio_quad_weights );
+        Var softp = new SoftplusVariable(hidden);
+        Var before_ratio = softp;
+        if (ratio_rank != 0) {
+            // Compute quadratic term for each hidden neuron.
+            VarArray quad_terms(ratio_quad_weights->length());
+            for (int i = 0; i < ratio_quad_weights->length(); i++) {
+                Var X_V = product(input, (*ratio_quad_weights)[i]);
+                quad_terms[i] = new RowSumSquareVariable(X_V);
+            }
+            // Concatenate quadratic terms into a single Var.
+            Var quad = new ConcatColumnsVariable(quad_terms);
+            // Add the softplus term.
+            Var softp_square = new SquareVariable(softp);
+            Var total = new PlusVariable(quad, softp_square);
+            // Take the square root and perform the ratio.
+            before_ratio = new SquareRootVariable(total);
+        }
+        result = new DivVariable(before_ratio,
+                                 new PlusConstantVariable(before_ratio, 1.0));
+    }
     else
         PLERROR("In NNet::hiddenLayer - Unknown value for transfer_func: %s",transfer_func.c_str());
     return result;
@@ -1077,14 +1131,19 @@
         random_gen->manual_seed(seed_);
 
     if (nhidden>0) {
-        if (!first_hidden_layer)
+        if (!first_hidden_layer) {
             fillWeights(w1, true);
+            for (int i = 0; i < v1.length(); i++)
+                fillWeights(v1[i], true);
+        }
         if (direct_in_to_out)
             fillWeights(wdirect, false);
     }
 
     if(nhidden2>0) {
         fillWeights(w2, true);
+        for (int i = 0; i < v2.length(); i++)
+            fillWeights(v2[i], true);
     }
 
     if (fixed_output_weights) {
@@ -1147,6 +1206,8 @@
     varDeepCopyField(sampleweight, copies);
     varDeepCopyField(w1, copies);
     varDeepCopyField(w2, copies);
+    deepCopyField(v1, copies);
+    deepCopyField(v2, copies);
     varDeepCopyField(wout, copies);
     varDeepCopyField(outbias, copies);
     varDeepCopyField(wdirect, copies);

Modified: trunk/plearn_learners/generic/NNet.h
===================================================================
--- trunk/plearn_learners/generic/NNet.h	2008-05-08 19:47:48 UTC (rev 8959)
+++ trunk/plearn_learners/generic/NNet.h	2008-05-08 20:31:32 UTC (rev 8960)
@@ -100,6 +100,10 @@
     Var wdirect; // bias and weights for direct in-to-out connection
     Var wrec; // input reconstruction weights (optional), from hidden layer to predicted input
 
+    //! Matrices used for the quadratic term in the 'ratio' transfer function.
+    //! There is one matrix for each hidden neuron.
+    VarArray v1, v2;
+
     // first hidden layer
     Var hidden_layer;
 
@@ -185,6 +189,7 @@
     // 0 means the whole training set (default: 1)
 
     string initialization_method;
+    int ratio_rank;
 
 
 private:
@@ -239,7 +244,8 @@
     //! Return a variable that is the hidden layer corresponding to given
     //! input and weights. If the 'default' transfer_func is used, we use the
     //! hidden_transfer_func option.
-    Var hiddenLayer(const Var& input, const Var& weights, string transfer_func = "default");
+    Var hiddenLayer(const Var& input, const Var& weights, string transfer_func = "default",
+                    VarArray* ratio_quad_weights = NULL);
 
     //! Build the output of the neural network, from the given input.
     //! The hidden layer is also made available in the 'hidden_layer' parameter.



From nouiz at mail.berlios.de  Fri May  9 16:33:36 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 May 2008 16:33:36 +0200
Subject: [Plearn-commits] r8961 - trunk/plearn/vmat
Message-ID: <200805091433.m49EXavY005010@sheep.berlios.de>

Author: nouiz
Date: 2008-05-09 16:33:35 +0200 (Fri, 09 May 2008)
New Revision: 8961

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
removed include not needed anymore


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-05-08 20:31:32 UTC (rev 8960)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-05-09 14:33:35 UTC (rev 8961)
@@ -39,7 +39,6 @@
 
 #include "GaussianizeVMatrix.h"
 #include <plearn/math/pl_erf.h>
-#include <plearn/vmat/VMat_computeStats.h>
 
 namespace PLearn {
 using namespace std;



From nouiz at mail.berlios.de  Fri May  9 17:03:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 May 2008 17:03:19 +0200
Subject: [Plearn-commits] r8962 - trunk/plearn/vmat
Message-ID: <200805091503.m49F3JQQ007323@sheep.berlios.de>

Author: nouiz
Date: 2008-05-09 17:03:18 +0200 (Fri, 09 May 2008)
New Revision: 8962

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
if the mapping file is modified, we should update the mtime of this object. This happen if the mapping file is modified manually


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-05-09 14:33:35 UTC (rev 8961)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-05-09 15:03:18 UTC (rev 8962)
@@ -420,6 +420,7 @@
     {
         string fname = getMapFilePath(k);
         if (isfile(fname)) {
+            updateMtime(fname);
             vector<string> all_lines = getNonBlankLines(loadFileAsString(fname));
             for (size_t i = 0; i < all_lines.size(); i++) {
                 string map_line = all_lines[i];



From nouiz at mail.berlios.de  Fri May  9 17:07:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 May 2008 17:07:15 +0200
Subject: [Plearn-commits] r8963 - trunk/plearn/vmat
Message-ID: <200805091507.m49F7FFc008004@sheep.berlios.de>

Author: nouiz
Date: 2008-05-09 17:07:13 +0200 (Fri, 09 May 2008)
New Revision: 8963

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
optimisation:
-if max_constant_threshold is 1, we only need to know if their is more then 1 different value.
-save precomputed stats


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-05-09 15:03:18 UTC (rev 8962)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-05-09 15:07:13 UTC (rev 8963)
@@ -189,7 +189,7 @@
 ///////////
 void VariableDeletionVMatrix::build()
 {
-    inherited::build();
+//    inherited::build();
     build_();
 }
 
@@ -249,7 +249,14 @@
                                           the_train_source->width());
     TVec<StatsCollector> stats;
     if(min_non_missing_threshold > 0 || max_constant_threshold > 0){
-        stats = PLearn::computeStats(the_train_source, -1, false);
+        int maxnvalues = -1;
+        if(is_equal(max_constant_threshold,1))
+//We don't need all the value, if (min==max && non_missing_value>0) it is constant value.
+            maxnvalues = 0;
+        stats = the_train_source->
+            getPrecomputedStatsFromFile("stats_variableDeletionVMatrix_"+
+                                        tostring(maxnvalues)+".psave",
+                                        maxnvalues, true);
         PLASSERT( stats.length() == source->width() );
     }
 
@@ -267,9 +274,18 @@
             indices.append(i);
     // Then remove columns that are too constant.
     TVec<int> final_indices;
-    if (max_constant_threshold > 0){
+    if (is_equal(max_constant_threshold,1)){
         for (int k = 0; k < indices.length(); k++) {
             int i = indices[k];
+            StatsCollector stat = stats[i];
+            if(!(stat.min()==stat.max() && stat.nnonmissing()>0))
+                final_indices.append(i);
+        }
+        indices.resize(final_indices.length());
+        indices << final_indices; 
+    }else if (max_constant_threshold > 0){
+        for (int k = 0; k < indices.length(); k++) {
+            int i = indices[k];
             int max_constant_absolute =
                 int(round(max_constant_threshold * stats[i].nnonmissing()));
             map<real, StatsCollectorCounts>* counts = stats[i].getCounts();



From tihocan at mail.berlios.de  Fri May  9 17:48:09 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 9 May 2008 17:48:09 +0200
Subject: [Plearn-commits] r8964 - trunk/plearn/var
Message-ID: <200805091548.m49Fm9v4011517@sheep.berlios.de>

Author: tihocan
Date: 2008-05-09 17:48:08 +0200 (Fri, 09 May 2008)
New Revision: 8964

Added:
   trunk/plearn/var/PlusManyVariable.cc
   trunk/plearn/var/PlusManyVariable.h
Log:
New Var to compute the sum of more than 2 Vars

Added: trunk/plearn/var/PlusManyVariable.cc
===================================================================
--- trunk/plearn/var/PlusManyVariable.cc	2008-05-09 15:07:13 UTC (rev 8963)
+++ trunk/plearn/var/PlusManyVariable.cc	2008-05-09 15:48:08 UTC (rev 8964)
@@ -0,0 +1,175 @@
+// -*- C++ -*-
+
+// PlusManyVariable.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file PlusManyVariable.cc */
+
+
+#include "PlusManyVariable.h"
+
+namespace PLearn {
+using namespace std;
+
+/** PlusManyVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    PlusManyVariable,
+    "Sum of an arbitrary number of variables (that must have same sizes).",
+    ""
+);
+
+//////////////////////
+// PlusManyVariable //
+//////////////////////
+PlusManyVariable::PlusManyVariable()
+{}
+
+PlusManyVariable::PlusManyVariable(const VarArray& the_array,
+                                   bool call_build_):
+    inherited(the_array, the_array[0]->length(), the_array[0]->width(),
+              call_build_)
+{
+    if (call_build_)
+        build_();
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
+void PlusManyVariable::recomputeSize(int& l, int& w) const
+{
+    if (varray.length() > 0) {
+        l = varray[0]->length();
+        w = varray[0]->width();
+    } else
+        l = w = 0;
+}
+
+///////////
+// fprop //
+///////////
+void PlusManyVariable::fprop()
+{
+    fillValue(0);
+    int n = nelems();
+    for (int i = 0; i < varray.length(); i++) {
+        real* vi = varray[i]->valuedata;
+        for (int k=0; k<n; k++)
+            valuedata[k] += vi[k];
+    }
+}
+
+///////////
+// bprop //
+///////////
+void PlusManyVariable::bprop()
+{
+    int n = nelems();
+    for (int i = 0; i < varray.length(); i++) {
+        real* vi = varray[i]->gradientdata;
+        for (int k=0; k<n; k++)
+            vi[k] += gradientdata[k];
+    }
+}
+
+///////////
+// build //
+///////////
+void PlusManyVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void PlusManyVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### If you want to deepCopy a Var field:
+    // varDeepCopyField(somevariable, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("PlusManyVariable::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void PlusManyVariable::declareOptions(OptionList& ol)
+{
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void PlusManyVariable::build_()
+{
+    // Check sizes are coherent.
+    if (varray.length() >= 2) {
+        int l = varray[0]->length();
+        int w = varray[1]->width();
+        for (int i = 1; i < varray.length(); i++)
+            if (varray[i]->length() != l || varray[i]->width() != w)
+                PLERROR("In PlusManyVariable::build_ - All source variables "
+                        "must have the same size");
+    }
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/PlusManyVariable.h
===================================================================
--- trunk/plearn/var/PlusManyVariable.h	2008-05-09 15:07:13 UTC (rev 8963)
+++ trunk/plearn/var/PlusManyVariable.h	2008-05-09 15:48:08 UTC (rev 8964)
@@ -0,0 +1,156 @@
+// -*- C++ -*-
+
+// PlusManyVariable.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file PlusManyVariable.h */
+
+
+#ifndef PlusManyVariable_INC
+#define PlusManyVariable_INC
+
+#include <plearn/var/NaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+/*! * PlusManyVariable * */
+
+class PlusManyVariable : public NaryVariable
+{
+    typedef NaryVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor.
+    PlusManyVariable();
+
+    //! Convenience constructor.
+    PlusManyVariable(const VarArray& the_array, bool call_build_ = true);
+
+    // Your other public member functions go here
+
+    //#####  PLearn::Variable methods #########################################
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+
+    // ### These ones are not always implemented
+    // virtual void bbprop();
+    // virtual void symbolicBprop();
+    // virtual void rfprop();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(PlusManyVariable);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PlusManyVariable);
+
+// ### Put here a convenient method for building your variable from other
+// ### existing ones, or a VarArray.
+// ### e.g., if your class is TotoVariable, with two parameters foo_type foo
+// ### and bar_type bar, you could write, depending on your constructor:
+// inline Var toto(Var v1, Var v2, Var v3,
+//                 foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v1, v2, v3, foo, bar); }
+// ### or:
+// inline Var toto(Var v1, Var v2, v3
+//                 foo_type foo=default_foo, bar_type bar=default_bar)
+// { return new TotoVariable(v1 & v2 & v3, foo, bar); }
+// ### or:
+// inline Var toto(const VarArray& varray, foo_type foo=default_foo,
+//                 bar_type bar=default_bar)
+// { return new TotoVariable( varray, foo, bar); }
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Fri May  9 17:49:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 May 2008 17:49:06 +0200
Subject: [Plearn-commits] r8965 - in trunk: plearn/vmat
	plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results
Message-ID: <200805091549.m49Fn6F5011537@sheep.berlios.de>

Author: nouiz
Date: 2008-05-09 17:49:05 +0200 (Fri, 09 May 2008)
New Revision: 8965

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.h
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log
Log:
added option SelectColumnsVMatrix::warn_non_selected_field. When true will show the field not selected.
Also, modified a warning to be more clear and regenerated the test where is matter


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-05-09 15:48:08 UTC (rev 8964)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-05-09 15:49:05 UTC (rev 8965)
@@ -59,7 +59,8 @@
 SelectColumnsVMatrix::SelectColumnsVMatrix()
     : extend_with_missing(false),
       fields_partial_match(false),
-      inverse_fields_selection(false)
+      inverse_fields_selection(false),
+      warn_non_selected_field(false)
 {}
 
 SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source,
@@ -70,7 +71,8 @@
     extend_with_missing(the_extend_with_missing),
     fields(the_fields),
     fields_partial_match(false),
-    inverse_fields_selection(false)
+    inverse_fields_selection(false),
+    warn_non_selected_field(false)
 {
     if (call_build_)
         build_();
@@ -83,7 +85,8 @@
     extend_with_missing(false),
     indices(the_indices),
     fields_partial_match(false),
-    inverse_fields_selection(false)
+    inverse_fields_selection(false),
+    warn_non_selected_field(false)
 {
     if (call_build_)
         build_();
@@ -92,7 +95,8 @@
 SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source, Vec the_indices)
     : extend_with_missing(false),
       fields_partial_match(false),
-      inverse_fields_selection(false)
+      inverse_fields_selection(false),
+      warn_non_selected_field(false)
 {
     source = the_source;
     indices.resize(the_indices.length());
@@ -164,6 +168,12 @@
         "If set to true, the selection is reversed. This provides an easy\n"
         "way to specify columns we do not want.");
 
+    declareOption(ol, "warn_non_selected_field",
+                  &SelectColumnsVMatrix::warn_non_selected_field,
+                  OptionBase::buildoption,
+                  "If set to true, we generate a PLWARNING with all the field"
+                  " that is not selected.");
+
     inherited::declareOptions(ol);
 }
 
@@ -237,8 +247,8 @@
                 }
                 if(missing_field.size()>0){
                     PLWARNING("In SelectColumnsVMatrix::build_() - We are"
-                              " added %d columns to the source matrix with missing value"
-                              " columns names: %s",missing_field.size(),
+                              " adding %d columns to the source matrix with missing values."
+                              " The columns names are: %s",missing_field.size(),
                               tostring(missing_field).c_str());
                 }
             } else {
@@ -271,6 +281,25 @@
         } else
             sel_indices = indices;
 
+        if(warn_non_selected_field){
+            TVec<string> v;
+            for(int i=0;i<source.width();i++)
+            {
+                bool found=false;
+                for(int j=0;j<indices.size();j++)
+                    if(indices[j]==i){
+                        found=true;
+                        break;
+                    }
+                if(!found)
+                    v.append(source.fieldName(i));
+            }
+            if(v.size()>0)
+                PLWARNING("In SelectColumnsVMatrix::build_() - Their is %d "
+                          " columns in the source matrix that we don't "
+                          "select: %s",v.size(),tostring(v).c_str());
+        }
+
         // Copy matrix dimensions
         width_ = sel_indices.length();
         length_ = source->length();

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-05-09 15:48:08 UTC (rev 8964)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-05-09 15:49:05 UTC (rev 8965)
@@ -72,7 +72,7 @@
     TVec<string> fields;
     bool fields_partial_match;
     bool inverse_fields_selection;
-
+    bool warn_non_selected_field;
 public:
 
     //! Default constructor.

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log	2008-05-09 15:48:08 UTC (rev 8964)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussMix_Generate/expected_results/RUN.log	2008-05-09 15:49:05 UTC (rev 8965)
@@ -1,3 +1,3 @@
- WARNING: In SelectColumnsVMatrix::build_() - We are added 1 columns to the source matrix with missing value columns names: 2 
- WARNING: In SelectColumnsVMatrix::build_() - We are added 1 columns to the source matrix with missing value columns names: 2 
- WARNING: In SelectColumnsVMatrix::build_() - We are added 1 columns to the source matrix with missing value columns names: 2 
+ WARNING: In SelectColumnsVMatrix::build_() - We are adding 1 columns to the source matrix with missing values. The columns names are: 2 
+ WARNING: In SelectColumnsVMatrix::build_() - We are adding 1 columns to the source matrix with missing values. The columns names are: 2 
+ WARNING: In SelectColumnsVMatrix::build_() - We are adding 1 columns to the source matrix with missing values. The columns names are: 2 



From nouiz at mail.berlios.de  Fri May  9 17:50:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 May 2008 17:50:02 +0200
Subject: [Plearn-commits] r8966 - trunk/plearn/vmat
Message-ID: <200805091550.m49Fo228011572@sheep.berlios.de>

Author: nouiz
Date: 2008-05-09 17:50:01 +0200 (Fri, 09 May 2008)
New Revision: 8966

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
hide variable that are learned each time the object is build. So we don't need to save them


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-05-09 15:49:05 UTC (rev 8965)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-05-09 15:50:01 UTC (rev 8966)
@@ -175,6 +175,14 @@
                   " that is not selected.");
 
     inherited::declareOptions(ol);
+
+    redeclareOption(ol, "length", &SelectColumnsVMatrix::length_,
+                    OptionBase::nosave,
+                    "Taked from source");
+
+    redeclareOption(ol, "width", &SelectColumnsVMatrix::width_,
+                    OptionBase::nosave,
+                    "Computed from parameter");
 }
 
 /////////////////////////////////



From tihocan at mail.berlios.de  Fri May  9 17:51:08 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 9 May 2008 17:51:08 +0200
Subject: [Plearn-commits] r8967 - trunk/plearn_learners/generic
Message-ID: <200805091551.m49Fp7sS011657@sheep.berlios.de>

Author: tihocan
Date: 2008-05-09 17:51:07 +0200 (Fri, 09 May 2008)
New Revision: 8967

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
Optimization for hidden_transfer_func=ratio

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-05-09 15:50:01 UTC (rev 8966)
+++ trunk/plearn_learners/generic/NNet.cc	2008-05-09 15:51:07 UTC (rev 8967)
@@ -59,6 +59,7 @@
 #include <plearn/var/OneHotSquaredLoss.h>
 #include <plearn/var/PlusConstantVariable.h>
 #include <plearn/var/PlusVariable.h>
+#include <plearn/var/PlusManyVariable.h>
 #include <plearn/var/ProductVariable.h>
 #include <plearn/var/RowSumSquareVariable.h>
 #include <plearn/var/SigmoidVariable.h>
@@ -774,10 +775,11 @@
         w1 = Var(1 + the_input->width(), nhidden, "w1");      
         params.append(w1);
         if (hidden_transfer_func == "ratio") {
-            v1.resize(ratio_rank != 0 ? nhidden : 0);
+            v1.resize(ratio_rank > 0 ? ratio_rank
+                                     : ratio_rank == -1 ? the_input->width()
+                                                        : 0);
             for (int i = 0; i < v1.length(); i++) {
-                int rank = ratio_rank < 0 ? the_input->width() : ratio_rank;
-                v1[i] = Var(the_input->width(), rank, "v1[" + tostring(i) + "]");
+                v1[i] = Var(the_input->width(), nhidden, "v1[" + tostring(i) + "]");
                 params.append(v1[i]);
             }
         }
@@ -793,10 +795,11 @@
         w2 = Var(1 + output.width(), nhidden2, "w2");
         params.append(w2);
         if (hidden_transfer_func == "ratio") {
-            v2.resize(ratio_rank != 0 ? nhidden2 : 0);
+            v2.resize(ratio_rank > 0 ? ratio_rank
+                                     : ratio_rank == -1 ? output->width()
+                                                        : 0);
             for (int i = 0; i < v2.length(); i++) {
-                int rank = ratio_rank < 0 ? output->width() : ratio_rank;
-                v2[i] = Var(output->width(), rank, "v2[" + tostring(i) + "]");
+                v2[i] = Var(output->width(), nhidden2, "v2[" + tostring(i) + "]");
                 params.append(v2[i]);
             }
         }
@@ -1100,20 +1103,20 @@
         Var softp = new SoftplusVariable(hidden);
         Var before_ratio = softp;
         if (ratio_rank != 0) {
-            // Compute quadratic term for each hidden neuron.
+            // Compute quadratic term.
             VarArray quad_terms(ratio_quad_weights->length());
             for (int i = 0; i < ratio_quad_weights->length(); i++) {
-                Var X_V = product(input, (*ratio_quad_weights)[i]);
-                quad_terms[i] = new RowSumSquareVariable(X_V);
+                quad_terms[i] = new SquareVariable(
+                        new ProductVariable(input, (*ratio_quad_weights)[i]));
             }
-            // Concatenate quadratic terms into a single Var.
-            Var quad = new ConcatColumnsVariable(quad_terms);
+            Var sum_quad_terms = new PlusManyVariable(quad_terms);
             // Add the softplus term.
             Var softp_square = new SquareVariable(softp);
-            Var total = new PlusVariable(quad, softp_square);
-            // Take the square root and perform the ratio.
+            Var total = new PlusVariable(sum_quad_terms, softp_square);
+            // Take the square root.
             before_ratio = new SquareRootVariable(total);
         }
+        // Perform ratio.
         result = new DivVariable(before_ratio,
                                  new PlusConstantVariable(before_ratio, 1.0));
     }



From tihocan at mail.berlios.de  Fri May  9 19:40:12 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 9 May 2008 19:40:12 +0200
Subject: [Plearn-commits] r8968 - trunk/plearn/var
Message-ID: <200805091740.m49HeCDS019508@sheep.berlios.de>

Author: tihocan
Date: 2008-05-09 19:40:11 +0200 (Fri, 09 May 2008)
New Revision: 8968

Modified:
   trunk/plearn/var/PlusManyVariable.cc
   trunk/plearn/var/PlusManyVariable.h
Log:
Implemented deep copy

Modified: trunk/plearn/var/PlusManyVariable.cc
===================================================================
--- trunk/plearn/var/PlusManyVariable.cc	2008-05-09 15:51:07 UTC (rev 8967)
+++ trunk/plearn/var/PlusManyVariable.cc	2008-05-09 17:40:11 UTC (rev 8968)
@@ -119,18 +119,6 @@
 void PlusManyVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### If you want to deepCopy a Var field:
-    // varDeepCopyField(somevariable, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("PlusManyVariable::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
 ////////////////////

Modified: trunk/plearn/var/PlusManyVariable.h
===================================================================
--- trunk/plearn/var/PlusManyVariable.h	2008-05-09 15:51:07 UTC (rev 8967)
+++ trunk/plearn/var/PlusManyVariable.h	2008-05-09 17:40:11 UTC (rev 8968)
@@ -69,28 +69,19 @@
     // Your other public member functions go here
 
     //#####  PLearn::Variable methods #########################################
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void recomputeSize(int& l, int& w) const;
     virtual void fprop();
     virtual void bprop();
 
-    // ### These ones are not always implemented
-    // virtual void bbprop();
-    // virtual void symbolicBprop();
-    // virtual void rfprop();
-
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
     PLEARN_DECLARE_OBJECT(PlusManyVariable);
 
     // Simply calls inherited::build() then build_()
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 protected:
@@ -103,14 +94,12 @@
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.
-    // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList& ol);
 
 private:
     //#####  Private Member Functions  ########################################
 
     //! This does the actual building.
-    // (PLEASE IMPLEMENT IN .cc)
     void build_();
 
 private:



From nouiz at mail.berlios.de  Fri May  9 20:21:56 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 May 2008 20:21:56 +0200
Subject: [Plearn-commits] r8969 - in trunk/plearn: io vmat
Message-ID: <200805091821.m49ILu2h023948@sheep.berlios.de>

Author: nouiz
Date: 2008-05-09 20:21:55 +0200 (Fri, 09 May 2008)
New Revision: 8969

Modified:
   trunk/plearn/io/PyPLearnScript.cc
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/fileutils.h
   trunk/plearn/vmat/VMatrix.cc
Log:
Added the preprocessing macro ${MTIME}. This will put the latest mtime between the current file and the one included by this file(recursivly) BEFORE the ${MTIME}


Modified: trunk/plearn/io/PyPLearnScript.cc
===================================================================
--- trunk/plearn/io/PyPLearnScript.cc	2008-05-09 17:40:11 UTC (rev 8968)
+++ trunk/plearn/io/PyPLearnScript.cc	2008-05-09 18:21:55 UTC (rev 8969)
@@ -118,7 +118,7 @@
         // list of variables used when runing the PLearn preprocessor on the
         // output of the .pyplearn file to handle $DATE, etc.
         // in PLearn strings.
-        addFileAndDateVariables(scriptfile, vars);
+        addFileAndDateVariables(scriptfile, vars, 0);
 
         // Also add these variables (DATE, etc.) to the pyplearn driver
         // script so they show up as pyplearn arguments too.

Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-05-09 17:40:11 UTC (rev 8968)
+++ trunk/plearn/io/fileutils.cc	2008-05-09 18:21:55 UTC (rev 8969)
@@ -610,7 +610,7 @@
 /////////////////////////////
 // addFileAndDateVariables //
 /////////////////////////////
-void addFileAndDateVariables(const PPath& filepath, map<string, string>& variables)
+void addFileAndDateVariables(const PPath& filepath, map<string, string>& variables, const time_t& latest)
 {
     // Define new local variables
     variables["HOME"]        = PPath::getenv("HOME");
@@ -635,6 +635,7 @@
     variables["DATE"] = time_buffer;
     strftime(time_buffer,SIZE,"%H%M%S",broken_down_time);
     variables["TIME"] = time_buffer;
+    variables["MTIME"] = tostring(latest);
 }
 
 /////////////////////////////
@@ -674,7 +675,7 @@
     latest=max(latest,mtime(file.absolute()));
 
     // Add the new file and date variables
-    addFileAndDateVariables(file, variables);
+    addFileAndDateVariables(file, variables, latest);
 
     // Perform actual parsing and macro processing...
     PStream in = openFile(file, PStream::plearn_ascii, "r");
@@ -969,8 +970,12 @@
                         raw_includefilepath = removeblanks(raw_includefilepath);
                         PPath p = PPath(raw_includefilepath);
                         // Read file with appropriate variable definitions.
+                        time_t new_latest = 0;
                         text += readFileAndMacroProcess
-                            (p, variables, latest);
+                            (p, variables, new_latest);
+                        latest=max(latest,new_latest);
+                        string s=tostring(latest);
+                        variables["MTIME"]=s;
                     }
                     break;
 

Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2008-05-09 17:40:11 UTC (rev 8968)
+++ trunk/plearn/io/fileutils.h	2008-05-09 18:21:55 UTC (rev 8969)
@@ -215,7 +215,9 @@
   DIRPATH, FILENAME, FILEBASE, FILEEXT, DATE, TIME and DATETIME and
   adds them to the map of variables passed as an argument.
 */
-void addFileAndDateVariables(const PPath& filepath, map<string, string>& variables);
+void addFileAndDateVariables(const PPath& filepath,
+                             map<string, string>& variables,
+                             const time_t& latest);
     
 /*! Same as readAndMacroProcess, but takes a filename instead of a string.
   The following variables are automatically set from the filepath: FILEPATH DIRPATH FILENAME FILEBASE FILEEXT 

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-05-09 17:40:11 UTC (rev 8968)
+++ trunk/plearn/vmat/VMatrix.cc	2008-05-09 18:21:55 UTC (rev 8969)
@@ -151,7 +151,13 @@
         "automatically be set to name_of_vmat_file.metadata/ \n"
         "And if it is the source inside another VMatrix that sets its \n"
         "metadatadir, it will often be set from that surrounding vmat's metadata.\n");
-    
+
+    declareOption(
+        ol, "mtime", &VMatrix::mtime_, 
+        OptionBase::buildoption|OptionBase::nosave,
+        "DO NOT play with this if you don't know the implementation!\n"
+        "The modification time of this VMatrix. Defaults to 0(unknow)");
+
     inherited::declareOptions(ol);
 }
 



From tihocan at mail.berlios.de  Fri May  9 20:51:12 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 9 May 2008 20:51:12 +0200
Subject: [Plearn-commits] r8970 - trunk/plearn/vmat
Message-ID: <200805091851.m49IpCvh029741@sheep.berlios.de>

Author: tihocan
Date: 2008-05-09 20:51:12 +0200 (Fri, 09 May 2008)
New Revision: 8970

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
Fixed a few typos

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-05-09 18:21:55 UTC (rev 8969)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-05-09 18:51:12 UTC (rev 8970)
@@ -171,14 +171,14 @@
     declareOption(ol, "warn_non_selected_field",
                   &SelectColumnsVMatrix::warn_non_selected_field,
                   OptionBase::buildoption,
-                  "If set to true, we generate a PLWARNING with all the field"
-                  " that is not selected.");
+                  "If set to true, we generate a PLWARNING with all fields "
+                  "that are not selected.");
 
     inherited::declareOptions(ol);
 
     redeclareOption(ol, "length", &SelectColumnsVMatrix::length_,
                     OptionBase::nosave,
-                    "Taked from source");
+                    "Taken from source");
 
     redeclareOption(ol, "width", &SelectColumnsVMatrix::width_,
                     OptionBase::nosave,
@@ -303,8 +303,8 @@
                     v.append(source.fieldName(i));
             }
             if(v.size()>0)
-                PLWARNING("In SelectColumnsVMatrix::build_() - Their is %d "
-                          " columns in the source matrix that we don't "
+                PLWARNING("In SelectColumnsVMatrix::build_() - There are %d"
+                          " columns in the source matrix that we do not "
                           "select: %s",v.size(),tostring(v).c_str());
         }
 



From tihocan at mail.berlios.de  Fri May  9 20:57:59 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 9 May 2008 20:57:59 +0200
Subject: [Plearn-commits] r8971 -
	trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass
Message-ID: <200805091857.m49Ivxdg030329@sheep.berlios.de>

Author: tihocan
Date: 2008-05-09 20:57:59 +0200 (Fri, 09 May 2008)
New Revision: 8971

Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/
Log:
Ignoring PSAVEDIFF


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass
___________________________________________________________________
Name: svn:ignore
   - .plearn
run_results

   + .plearn
run_results
PSAVEDIFF




From nouiz at mail.berlios.de  Fri May  9 21:10:38 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 May 2008 21:10:38 +0200
Subject: [Plearn-commits] r8972 - trunk/plearn/vmat
Message-ID: <200805091910.m49JAcaC031777@sheep.berlios.de>

Author: nouiz
Date: 2008-05-09 21:10:35 +0200 (Fri, 09 May 2008)
New Revision: 8972

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
commented change that break test.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-05-09 18:57:59 UTC (rev 8971)
+++ trunk/plearn/vmat/VMatrix.cc	2008-05-09 19:10:35 UTC (rev 8972)
@@ -152,11 +152,11 @@
         "And if it is the source inside another VMatrix that sets its \n"
         "metadatadir, it will often be set from that surrounding vmat's metadata.\n");
 
-    declareOption(
-        ol, "mtime", &VMatrix::mtime_, 
-        OptionBase::buildoption|OptionBase::nosave,
-        "DO NOT play with this if you don't know the implementation!\n"
-        "The modification time of this VMatrix. Defaults to 0(unknow)");
+//     declareOption(
+//         ol, "mtime", &VMatrix::mtime_, 
+//         OptionBase::buildoption|OptionBase::nosave,
+//         "DO NOT play with this if you don't know the implementation!\n"
+//         "The modification time of this VMatrix. Defaults to 0(unknow)");
 
     inherited::declareOptions(ol);
 }



From nouiz at mail.berlios.de  Fri May  9 21:42:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 May 2008 21:42:39 +0200
Subject: [Plearn-commits] r8973 - trunk/plearn/vmat
Message-ID: <200805091942.m49JgddK001545@sheep.berlios.de>

Author: nouiz
Date: 2008-05-09 21:42:39 +0200 (Fri, 09 May 2008)
New Revision: 8973

Modified:
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
Log:
added support for instruction with fieldname that have this regex 'fieldname*' in MissingInstructionVMatrix


Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-05-09 19:10:35 UTC (rev 8972)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-05-09 19:42:39 UTC (rev 8973)
@@ -111,7 +111,9 @@
     declareOption(ol, "missing_instructions", &MissingInstructionVMatrix::missing_instructions,
                   OptionBase::buildoption,
                   "The variable missing regeneration instructions in the form of pairs field : instruction.\n"
-                  "Supported instructions are skip, as_is, zero_is_missing, 2436935_is_missing(01JAN1960 in julian day), present.");
+                  "Supported instructions are skip, as_is, zero_is_missing, 2436935_is_missing(01JAN1960 in julian day), present.\n"
+                  "If the instruction fieldname end with '*' we will extend it to all the source matrix fieldname that match the regex.\n"
+                  "No other regex are supported");
     declareOption(ol, "default_instruction",
                   &MissingInstructionVMatrix::default_instruction,
                   OptionBase::buildoption,
@@ -156,11 +158,23 @@
     for (int ins_col = 0; ins_col < missing_instructions.size(); ins_col++)
     {
         int source_col = 0;
+        pair<string, string> mis_ins=missing_instructions[ins_col];
         for (source_col = 0; source_col < source->width(); source_col++)
         {
-            if (missing_instructions[ins_col].first == source_names[source_col]) break;
+            if (mis_ins.first == source_names[source_col]) break;
         }
-        if (source_col >= source->width()) 
+        if (source_col >= source->width() && 
+            mis_ins.first[mis_ins.first.size()-1]=='*') {
+            string sub_name=mis_ins.first.substr(0,mis_ins.first.size()-1);
+            for (source_col = 0; source_col < source->width(); source_col++)
+            {
+                if (string_begins_with(source_names[source_col],sub_name)){
+                    missing_instructions.append(
+                        make_pair(source_names[source_col],mis_ins.second));
+                } 
+            }
+            continue;
+        } else if (source_col >= source->width()) 
         {
             PLWARNING("In MissingInstructionVMatrix::build_() - missing_instructions '%d': no field with this name: '%s'" 
                     ,ins_col,(missing_instructions[ins_col].first).c_str());



From dumitruerhan at mail.berlios.de  Fri May  9 22:10:17 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Fri, 9 May 2008 22:10:17 +0200
Subject: [Plearn-commits] r8974 - trunk/plearn/ker
Message-ID: <200805092010.m49KAHB2004323@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-09 22:10:16 +0200 (Fri, 09 May 2008)
New Revision: 8974

Added:
   trunk/plearn/ker/BetaKernel.cc
   trunk/plearn/ker/BetaKernel.h
Log:
kernel for inputs coming from distributions that have a [0;1]^D support

Added: trunk/plearn/ker/BetaKernel.cc
===================================================================
--- trunk/plearn/ker/BetaKernel.cc	2008-05-09 19:42:39 UTC (rev 8973)
+++ trunk/plearn/ker/BetaKernel.cc	2008-05-09 20:10:16 UTC (rev 8974)
@@ -0,0 +1,214 @@
+// -*- C++ -*-
+
+// BetaKernel.cc
+//
+// Copyright (C) 2008 Dumitru Erhan
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dumitru Erhan
+
+/*! \file BetaKernel.cc */
+
+
+#include "BetaKernel.h"
+#include <plearn/math/distr_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    BetaKernel,
+    "Several implementatins of the Beta kernel, for distributions that have support in the [0;1] range",
+    "Useful for performing Parzen Windows-style density estimation. Need to specify the type of the kernel\n"
+     "- simple: the basic Beta kernel \n"
+     "- alternative: the alternative, faster converging version \n"
+     "output_type should be set to either log_density or density"  );
+
+//////////////////
+// BetaKernel //
+//////////////////
+BetaKernel::BetaKernel()
+    : width(1.),
+     kernel_type("simple"),
+     output_type("log_density")
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void BetaKernel::declareOptions(OptionList& ol)
+{
+
+    declareOption(ol, "width", &BetaKernel::width,
+                   OptionBase::buildoption,
+                  "The (positive, real-valued) smoothing parameter of the kernel (note that this does not quite correspond to the variance). If you use the Beta Kernel to do density estimation, this should go towards zero as the number of samples goes to infinity");
+ 
+    declareOption(ol, "kernel_type", &BetaKernel::kernel_type,
+                   OptionBase::buildoption,
+                  "A string containing the type of Beta kernel. The \"simple\" kernel has a particularly simple mathematical form and is easily shown to integrate to 1 (thus is a density). The \"alternative\" kernel is slightly more complicated, but is better suited for those distributions that have a lot of mass near the boundaries (0 or 1). It will converge faster, but asymptotically both kernels are boundary bias free. Also, the \"alternative\" kernel is not yet shown by us to integrate to 1, thus we don't know for sure whether it's a valid density. Default is simple");
+    
+    declareOption(ol, "output_type", &BetaKernel::output_type,
+                  OptionBase::buildoption,
+                  "A string specifying whether we want log densities as outputs (\"log_density\"; default) or just densities (\"density\")");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void BetaKernel::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void BetaKernel::build_()
+{
+}
+
+//////////////
+// evaluate //
+//////////////
+real BetaKernel::evaluate(const Vec& x1, const Vec& x2) const {
+#ifdef BOUNDCHECK
+    if(x1.length()!=x2.length())
+        PLERROR("In BetaKernel::evaluate x1 and x2 must have the same length");
+#endif
+    int l = x1.length();
+    real* px1 = x1.data();
+    real* px2 = x2.data();
+    real kvalue = 0.;
+
+    //kernel_type = lowerstring(kernel_type);
+
+    // check http://www.sta.nus.edu.sg/documents/publication_chen2.pdf for an
+    // explanation of the estimators ("Beta kernel estimators for
+    // density functions" is the paper title)
+    if (kernel_type=="simple")
+        for(int i=0; i<l; i++)
+        {
+            real a = px1[i] / width + 1.0;
+            real b = (1.0 - px1[i]) / width + 1.0;
+            real val = log_beta_density(px2[i],a,b);
+            kvalue += val;
+        } 
+    else if (kernel_type=="alternative")
+        for(int i=0; i<l; i++)
+        {
+            real x = px1[i];
+            real a, b;
+
+            if (x<0 || x >1 || px2[i] < 0 || px2[i] > 1)
+                 PLERROR("In BetaKernel::evaluate x1 and x2 must contain values in the (closed) interval [0;1]");
+            
+            real p_xb = 2*pow(width,2.) + 2.5 - sqrt(4*pow(width,4.) + 6*pow(width,2.) + 2.25 - x*x - x / width);
+            real y = 1-x;
+            real p_1xb = 2*pow(width,2.) + 2.5 - sqrt(4*pow(width,4.)+ 6*pow(width,2.) + 2.25 - y*y - y / width);
+
+            if ((x >= 2*width) && (x <= 1 - 2*width)) {
+                a = x / width; 
+                b = (1 - x) / width;
+            }
+            else if ((x >= 0) & (x <= 2*width)) {
+                a = p_xb;
+                b = (1 - x) / width;   
+            }
+            else {
+                a = x / width;
+                b = p_1xb;
+            }
+                
+            real val = log_beta_density(px2[i],a,b);
+            kvalue += val;
+        } 
+    else
+        PLERROR("In BetaKernel::evaluate kernel_type must be either \"simple\" or \"alternative\"");
+   
+    real retval;
+ 
+    if (output_type=="log_density")
+        retval = kvalue;
+    else if (output_type=="density")
+        retval = exp(kvalue);
+    else
+        PLERROR("In BetaKernel::evaluate output_type must be either \"log_density\" or \"density\"");
+
+    return retval;
+}
+
+/* ### This method will very often be overridden.
+//////////////////
+// evaluate_i_j //
+//////////////////
+real BetaKernel::evaluate_i_j(int i, int j) const {
+// ### Evaluate the kernel on a pair of training points.
+}
+*/
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void BetaKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+/* ### This method will be overridden if computations need to be done,
+   ### or to forward the call to another object.
+   ### In this case, be careful that it may be called BEFORE the build_()
+   ### method has been called, if the 'specify_dataset' option is used.
+////////////////////////////
+// setDataForKernelMatrix //
+////////////////////////////
+void BetaKernel::setDataForKernelMatrix(VMat the_data) {
+}
+*/
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/BetaKernel.h
===================================================================
--- trunk/plearn/ker/BetaKernel.h	2008-05-09 19:42:39 UTC (rev 8973)
+++ trunk/plearn/ker/BetaKernel.h	2008-05-09 20:10:16 UTC (rev 8974)
@@ -0,0 +1,162 @@
+// -*- C++ -*-
+
+// BetaKernel.h
+//
+// Copyright (C) 2008 Dumitru Erhan
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dumitru Erhan
+
+/*! \file BetaKernel.h */
+
+
+#ifndef BetaKernel_INC
+#define BetaKernel_INC
+
+#include <plearn/ker/Kernel.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class BetaKernel : public Kernel
+{
+    typedef Kernel inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! The "width" of the kernel
+    real width; 
+    //! Kernel type (simple or alternative)
+    string kernel_type;
+   //! log_density or density
+    string output_type;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    BetaKernel();
+
+
+    //#####  Kernel Member Functions  #########################################
+
+    //! Compute K(x1,x2).
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual real evaluate_i_j(int i, int j) const;
+    // virtual real evaluate_i_x(int i, const Vec& x, real squared_norm_of_x=-1) const;
+    // virtual real evaluate_x_i(const Vec& x, int i, real squared_norm_of_x=-1) const;
+    // virtual real evaluate_i_x_again(int i, const Vec& x, real squared_norm_of_x=-1, bool first_time = false) const;
+    // virtual real evaluate_x_i_again(const Vec& x, int i, real squared_norm_of_x=-1, bool first_time = false) const;
+    // virtual void computeGramMatrix(Mat K) const;
+    // virtual void setDataForKernelMatrix(VMat the_data);
+    // virtual void addDataForKernelMatrix(const Vec& newRow);
+    // virtual void setParameters(Vec paramvec);
+    // virtual Vec getParameters() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(BetaKernel);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(BetaKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From dumitruerhan at mail.berlios.de  Fri May  9 22:12:44 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Fri, 9 May 2008 22:12:44 +0200
Subject: [Plearn-commits] r8975 - trunk/plearn_learners/distributions
Message-ID: <200805092012.m49KCi1B004522@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-09 22:12:43 +0200 (Fri, 09 May 2008)
New Revision: 8975

Added:
   trunk/plearn_learners/distributions/KernelDensityEstimator.cc
   trunk/plearn_learners/distributions/KernelDensityEstimator.h
Log:
Generic kernel density estimator that takes any kerkel as parameter. Works with the Gaussian kernel, too (gives rise to a Parzen Windows estimator). No weights supported yet

Added: trunk/plearn_learners/distributions/KernelDensityEstimator.cc
===================================================================
--- trunk/plearn_learners/distributions/KernelDensityEstimator.cc	2008-05-09 20:10:16 UTC (rev 8974)
+++ trunk/plearn_learners/distributions/KernelDensityEstimator.cc	2008-05-09 20:12:43 UTC (rev 8975)
@@ -0,0 +1,201 @@
+// -*- C++ -*-
+
+// KernelDensityEstimator.cc
+//
+// Copyright (C) 2008 Dumitru Erhan
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dumitru Erhan
+
+/*! \file KernelDensityEstimator.cc */
+
+
+#include "KernelDensityEstimator.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    KernelDensityEstimator,
+    "Performs kernel density estimation ('Parzen Windows') with ANY given kernel",
+    "Does not take into account the input weights!"
+    );
+
+//////////////////
+// KernelDensityEstimator //
+//////////////////
+KernelDensityEstimator::KernelDensityEstimator()
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void KernelDensityEstimator::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "kernel", &KernelDensityEstimator::kernel,
+                   OptionBase::buildoption,
+                   "The kernel used at each point in the training set");
+
+    // Now call the parent class' declareOptions().
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void KernelDensityEstimator::build()
+{
+    // ### Nothing to add here, simply calls build_().
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void KernelDensityEstimator::build_()
+{
+}
+
+/////////
+// cdf //
+/////////
+real KernelDensityEstimator::cdf(const Vec& y) const
+{
+    PLERROR("cdf not implemented for KernelDensityEstimator"); return 0;
+}
+
+/////////////////
+// expectation //
+/////////////////
+void KernelDensityEstimator::expectation(Vec& mu) const
+{
+    PLERROR("expectation not implemented for KernelDensityEstimator");
+}
+
+// ### Remove this method if your distribution does not implement it.
+////////////
+// forget //
+////////////
+void KernelDensityEstimator::forget()
+{
+    inherited::forget();
+}
+
+//////////////
+// generate //
+//////////////
+void KernelDensityEstimator::generate(Vec& y) const
+{
+    PLERROR("generate not implemented for KernelDensityEstimator");
+}
+
+// ### Default version of inputsize returns learner->inputsize()
+// ### If this is not appropriate, you should uncomment this and define
+// ### it properly here:
+// int KernelDensityEstimator::inputsize() const {}
+
+/////////////////
+// log_density //
+/////////////////
+real KernelDensityEstimator::log_density(const Vec& y) const
+{
+    int numTrain = train_set.length();
+    Vec input, target;
+    real weight;
+    real logprob = -INFINITY;
+
+    for(int i=0; i<numTrain; i++) {
+        train_set->getExample(i,input,target,weight);
+        logprob = logadd(logprob,kernel->evaluate(input,y)); 
+    }
+    
+    logprob -= pl_log(numTrain);
+
+    return logprob;
+
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void KernelDensityEstimator::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+////////////////////
+// resetGenerator //
+////////////////////
+void KernelDensityEstimator::resetGenerator(long g_seed)
+{
+    inherited::resetGenerator(g_seed);
+}
+
+/////////////////
+// survival_fn //
+/////////////////
+real KernelDensityEstimator::survival_fn(const Vec& y) const
+{
+    PLERROR("survival_fn not implemented for KernelDensityEstimator"); return 0;
+}
+
+// ### Remove this method, if your distribution does not implement it.
+///////////
+// train //
+///////////
+void KernelDensityEstimator::train()
+{
+    // PLERROR("train method not implemented for KernelDensityEstimator");
+}
+
+//////////////
+// variance //
+//////////////
+void KernelDensityEstimator::variance(Mat& covar) const
+{
+    PLERROR("variance not implemented for KernelDensityEstimator");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/distributions/KernelDensityEstimator.h
===================================================================
--- trunk/plearn_learners/distributions/KernelDensityEstimator.h	2008-05-09 20:10:16 UTC (rev 8974)
+++ trunk/plearn_learners/distributions/KernelDensityEstimator.h	2008-05-09 20:12:43 UTC (rev 8975)
@@ -0,0 +1,191 @@
+// -*- C++ -*-
+
+// KernelDensityEstimator.h
+//
+// Copyright (C) 2008 Dumitru Erhan
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Dumitru Erhan
+
+/*! \file KernelDensityEstimator.h */
+
+
+#ifndef KernelDensityEstimator_INC
+#define KernelDensityEstimator_INC
+
+#include <plearn_learners/distributions/UnconditionalDistribution.h>
+#include <plearn/ker/Kernel.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class KernelDensityEstimator : public UnconditionalDistribution
+{
+    typedef UnconditionalDistribution inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    KernelDensityEstimator();
+
+
+    //! Kernel type
+    PP<Kernel> kernel;
+
+    //#####  UnconditionalDistribution Member Functions  ######################
+
+    //! Return log of probability density log(p(y)).
+    virtual real log_density(const Vec& x) const;
+
+    //! Return survival function: P(Y>y).
+    virtual real survival_fn(const Vec& y) const;
+
+    //! Return cdf: P(Y<y).
+    virtual real cdf(const Vec& y) const;
+
+    //! Return E[Y].
+    virtual void expectation(Vec& mu) const;
+
+    //! Return Var[Y].
+    virtual void variance(Mat& cov) const;
+
+    //! Return a pseudo-random sample generated from the distribution.
+    virtual void generate(Vec& y) const;
+
+    //! Reset the random number generator used by generate() using the
+    //! given seed.
+    virtual void resetGenerator(long g_seed);
+
+    // ### These methods may be overridden for efficiency purpose:
+    /*
+    //! Return probability density p(y)
+    virtual real density(const Vec& y) const;
+    */
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    // ### Default version of inputsize returns learner->inputsize()
+    // ### If this is not appropriate, you should uncomment this and define
+    // ### it properly in the .cc
+    // virtual int inputsize() const;
+
+    //! (Re-)initializes the PDistribution in its fresh state (that state may
+    //! depend on the 'seed' option) and sets 'stage' back to 0 (this is the
+    //! stage of a fresh learner!).
+    // ### You may remove this method if your distribution does not
+    // ### implement it.
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage == nstages, updating the train_stats collector with training
+    //! costs measured on-line in the process.
+    // ### You may remove this method if your distribution does not
+    // ### implement it.
+    virtual void train();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(KernelDensityEstimator);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(KernelDensityEstimator);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From dumitruerhan at mail.berlios.de  Fri May  9 22:15:58 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Fri, 9 May 2008 22:15:58 +0200
Subject: [Plearn-commits] r8976 - trunk/commands
Message-ID: <200805092015.m49KFwql004928@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-09 22:15:58 +0200 (Fri, 09 May 2008)
New Revision: 8976

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Inclusion of BetaKernel and KernelDensityEstimator

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-05-09 20:12:43 UTC (rev 8975)
+++ trunk/commands/plearn_noblas_inc.h	2008-05-09 20:15:58 UTC (rev 8976)
@@ -106,6 +106,7 @@
  * Kernel *
  **********/
 #include <plearn/ker/AdditiveNormalizationKernel.h>
+#include <plearn/ker/BetaKernel.h>
 #include <plearn/ker/DistanceKernel.h>
 #include <plearn/ker/DotProductKernel.h>
 #include <plearn/ker/EpanechnikovKernel.h>
@@ -190,6 +191,7 @@
 #include <plearn_learners/distributions/MixtureDistribution.h>
 #include <plearn_learners/distributions/SpiralDistribution.h>
 #include <plearn_learners/distributions/UniformDistribution.h>
+#include <plearn_learners/distributions/KernelDensityEstimator.h>
 
 // Nearest-Neighbors
 #include <plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h>



From dumitruerhan at mail.berlios.de  Fri May  9 22:22:05 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Fri, 9 May 2008 22:22:05 +0200
Subject: [Plearn-commits] r8977 - trunk/plearn_learners/distributions
Message-ID: <200805092022.m49KM50w005978@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-09 22:22:04 +0200 (Fri, 09 May 2008)
New Revision: 8977

Modified:
   trunk/plearn_learners/distributions/KernelDensityEstimator.cc
Log:
fixed log bug

Modified: trunk/plearn_learners/distributions/KernelDensityEstimator.cc
===================================================================
--- trunk/plearn_learners/distributions/KernelDensityEstimator.cc	2008-05-09 20:15:58 UTC (rev 8976)
+++ trunk/plearn_learners/distributions/KernelDensityEstimator.cc	2008-05-09 20:22:04 UTC (rev 8977)
@@ -138,7 +138,7 @@
         logprob = logadd(logprob,kernel->evaluate(input,y)); 
     }
     
-    logprob -= pl_log(numTrain);
+    logprob -= pl_log(real(numTrain));
 
     return logprob;
 



From tihocan at mail.berlios.de  Sat May 10 21:30:08 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sat, 10 May 2008 21:30:08 +0200
Subject: [Plearn-commits] r8978 - trunk/python_modules/plearn/learners
Message-ID: <200805101930.m4AJU8Ie004518@sheep.berlios.de>

Author: tihocan
Date: 2008-05-10 21:30:04 +0200 (Sat, 10 May 2008)
New Revision: 8978

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
Added new option to return outputs in a SVM test method

Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-05-09 20:22:04 UTC (rev 8977)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-05-10 19:30:04 UTC (rev 8978)
@@ -4,9 +4,9 @@
 
 from plearn.pyext import *
 
-
 from numpy.numarray import *
 from math import *
+import numpy
 import random
 import fpconst
 
@@ -1320,11 +1320,14 @@
         return outputs, targets
 
     """ Return the costs obtained by a libSVM model
-        on a given dataset
+        on a given dataset.
+        If 'return_outputs' is set to True, also returns a numpy array
+        containing outputs.
     """
     def test( self,
               testset,
-              teststats = None
+              teststats = None,
+              return_outputs = False
              ):
         nclasses = self.nclasses
         costnames = self.costnames
@@ -1395,7 +1398,13 @@
                     raise ValueError, "computation of cost %s not implemented in SVM::test()" % cn
             teststats.update(statVec,1.)
 
-        return teststats #, outputs, costs
+        if return_outputs:
+            numpy_out = numpy.zeros((len(outputs), len(outputs[0])))
+            for i, out_i in enumerate(outputs):
+                numpy_out[i,:] = out_i
+            return teststats, numpy_out
+        else:
+            return teststats #, outputs, costs
 
 
     def valid( self,



From larocheh at mail.berlios.de  Sun May 11 15:00:46 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sun, 11 May 2008 15:00:46 +0200
Subject: [Plearn-commits] r8979 - trunk/plearn_learners_experimental
Message-ID: <200805111300.m4BD0kr9022732@sheep.berlios.de>

Author: larocheh
Date: 2008-05-11 15:00:44 +0200 (Sun, 11 May 2008)
New Revision: 8979

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added PCD, MF-CD and (denoising) autoencoder learning.


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-10 19:30:04 UTC (rev 8978)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-11 13:00:44 UTC (rev 8979)
@@ -61,6 +61,10 @@
     cd_learning_rate( 0. ),
     cd_decrease_ct( 0. ),
     cd_n_gibbs( 1 ),
+    persistent_cd_weight( 0. ),
+    use_mean_field_cd( false ),
+    denoising_learning_rate( 0. ),
+    denoising_decrease_ct( 0. ),
     n_classes( -1 ),
     compute_input_space_nll( false ),
     pseudolikelihood_context_size ( 0 ),
@@ -72,7 +76,8 @@
     cumulative_training_time( 0 ),
     //cumulative_testing_time( 0 ),
     log_Z( MISSING_VALUE ),
-    Z_is_up_to_date( false )
+    Z_is_up_to_date( false ),
+    persistent_gibbs_chain_is_started( false )
 {
     random_gen = new PRandom();
 }
@@ -103,6 +108,34 @@
                   OptionBase::buildoption,
                   "Number of negative phase gibbs sampling steps.\n");
 
+    declareOption(ol, "persistent_cd_weight", 
+                  &PseudolikelihoodRBM::persistent_cd_weight,
+                  OptionBase::buildoption,
+                  "Weight of Persistent Contrastive Divergence, i.e. "
+                  "weight of the prolonged gibbs chain.\n");
+
+    declareOption(ol, "use_mean_field_cd", &PseudolikelihoodRBM::use_mean_field_cd,
+                  OptionBase::buildoption,
+                  "Indication that a mean-field version of Contrastive "
+                  "Divergence (MF-CD) should be used.\n");
+
+    declareOption(ol, "denoising_learning_rate", 
+                  &PseudolikelihoodRBM::denoising_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used for denoising autoencoder learning.\n");
+
+    declareOption(ol, "denoising_decrease_ct", 
+                  &PseudolikelihoodRBM::denoising_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the denoising autoencoder "
+                  "learning rate.\n");
+
+    declareOption(ol, "fraction_of_masked_inputs", 
+                  &PseudolikelihoodRBM::fraction_of_masked_inputs,
+                  OptionBase::buildoption,
+                  "Fraction of input components set to 0 for denoising "
+                  "autoencoder learning.\n");
+
     declareOption(ol, "n_classes", &PseudolikelihoodRBM::n_classes,
                   OptionBase::buildoption,
                   "Number of classes in the training set (for supervised learning).\n"
@@ -156,6 +189,12 @@
                   OptionBase::learntoption,
                   "Indication that the normalisation constant Z is up to date.\n");
 
+    declareOption(ol, "persistent_gibbs_chain_is_started", 
+                  &PseudolikelihoodRBM::persistent_gibbs_chain_is_started,
+                  OptionBase::learntoption,
+                  "Indication that the prolonged gibbs chain for "
+                  "Persistent Consistent Divergence is started.\n");
+
 //    declareOption(ol, "target_weights_L1_penalty_factor", 
 //                  &PseudolikelihoodRBM::target_weights_L1_penalty_factor,
 //                  OptionBase::buildoption,
@@ -270,6 +309,38 @@
     hidden_activation_neg_i_gradient.resize( hidden_layer->size );
     connection_gradient.resize( connection->up_size, connection->down_size );
 
+    // Generalized pseudolikelihood option
+    context_indices.resize( input_layer->size - 1);
+    if( pseudolikelihood_context_size > 0 )
+    {
+        context_indices_per_i.resize( input_layer->size, 
+                                      pseudolikelihood_context_size );
+
+        int n_conf = ipow(2, pseudolikelihood_context_size);
+        nums_act.resize( 2 * n_conf );
+        gnums_act.resize( 2 * n_conf );
+        context_probs.resize( 2 * n_conf );
+        hidden_activations_context.resize( 2*n_conf, hidden_layer->size );
+        hidden_activations_context_k_gradient.resize( hidden_layer->size );
+    }
+
+    // CD option
+    pos_hidden.resize( hidden_layer->size );
+    pers_cd_input.resize( input_layer->size );
+    pers_cd_hidden.resize( hidden_layer->size );
+
+    // Denoising autoencoder options
+    transpose_connection = new RBMMatrixTransposeConnection;
+    transpose_connection->rbm_matrix_connection = connection;
+    transpose_connection->build();
+    reconstruction_activation_gradient.resize(input_layer->size);
+    hidden_layer_expectation_gradient.resize(hidden_layer->size);
+    hidden_layer_activation_gradient.resize(hidden_layer->size);
+    masked_autoencoder_input.resize(input_layer->size);
+    autoencoder_input_indices.resize(input_layer->size);
+    for(int i=0; i<input_layer->size; i++)
+        autoencoder_input_indices[i] = i;
+
     if( inputsize_ >= 0 )
         PLASSERT( input_layer->size == inputsize() );
 
@@ -320,6 +391,7 @@
     deepCopyField(hidden_layer, copies);
     deepCopyField(connection, copies);
     deepCopyField(cost_names, copies);
+    deepCopyField(transpose_connection, copies);
 
     deepCopyField(target_one_hot, copies);
     deepCopyField(input_gradient, copies);
@@ -345,6 +417,13 @@
     deepCopyField(pos_hidden, copies);
     deepCopyField(neg_input, copies);
     deepCopyField(neg_hidden, copies);
+    deepCopyField(reconstruction_activation_gradient, copies);
+    deepCopyField(hidden_layer_expectation_gradient, copies);
+    deepCopyField(hidden_layer_activation_gradient, copies);
+    deepCopyField(masked_autoencoder_input, copies);
+    deepCopyField(autoencoder_input_indices, copies);
+    deepCopyField(pers_cd_input, copies);
+    deepCopyField(pers_cd_hidden, copies);
 }
 
 
@@ -370,6 +449,8 @@
     cumulative_training_time = 0;
     //cumulative_testing_time = 0;
     Z_is_up_to_date = false;
+
+    persistent_gibbs_chain_is_started = false;
 }
 
 ///////////
@@ -561,9 +642,6 @@
                 else
                 {
                     // Generate contexts
-                    context_indices.resize( input_layer->size - 1);
-                    context_indices_per_i.resize( input_layer->size, 
-                                                  pseudolikelihood_context_size );
                     for( int i=0; i<context_indices.length(); i++)
                         context_indices[i] = i;
                     int tmp,k;
@@ -590,11 +668,11 @@
                         (RBMMatrixConnection *) connection );
 
                     int n_conf = ipow(2, pseudolikelihood_context_size);
-                    nums_act.resize( 2 * n_conf );
-                    gnums_act.resize( 2 * n_conf );
-                    context_probs.resize( 2 * n_conf );
-                    hidden_activations_context.resize( 2*n_conf, hidden_layer->size );
-                    hidden_activations_context_k_gradient.resize( hidden_layer->size );
+                    //nums_act.resize( 2 * n_conf );
+                    //gnums_act.resize( 2 * n_conf );
+                    //context_probs.resize( 2 * n_conf );
+                    //hidden_activations_context.resize( 2*n_conf, hidden_layer->size );
+                    //hidden_activations_context_k_gradient.resize( hidden_layer->size );
                     real* nums_data;
                     real* gnums_data;
                     real* cp_data;
@@ -828,46 +906,187 @@
             // CD learning
             if( !fast_is_equal(cd_learning_rate, 0.) )
             {
-                if( cd_decrease_ct != 0 )
-                    lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
-                else
-                    lr = cd_learning_rate;
 
-                setLearningRate(lr);
+                if( !fast_is_equal(persistent_cd_weight, 1.) )
+                {
+                    if( cd_decrease_ct != 0 )
+                        lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                    else
+                        lr = cd_learning_rate;
+                    
+                    lr *= (1-persistent_cd_weight);
 
-                // Positive phase
-                pos_input = input;
-                connection->setAsDownInput( input );
-                hidden_layer->getAllActivations( 
-                    (RBMMatrixConnection*) connection );
-                hidden_layer->computeExpectation();
-                pos_hidden.resize( hidden_layer->size );
-                pos_hidden << hidden_layer->expectation;
+                    setLearningRate(lr);
 
-                // Negative phase
-                for(int i=0; i<cd_n_gibbs; i++)
-                {
-                    hidden_layer->generateSample();
-                    connection->setAsUpInput( hidden_layer->sample );
-                    input_layer->getAllActivations( 
-                        (RBMMatrixConnection*) connection );
-                    input_layer->computeExpectation();
-                    input_layer->generateSample();
-                    connection->setAsDownInput( input_layer->sample );
+                    // Positive phase
+                    pos_input = input;
+                    connection->setAsDownInput( input );
                     hidden_layer->getAllActivations( 
                         (RBMMatrixConnection*) connection );
                     hidden_layer->computeExpectation();
+                    //pos_hidden.resize( hidden_layer->size );
+                    pos_hidden << hidden_layer->expectation;
+                    
+                    // Negative phase
+                    for(int i=0; i<cd_n_gibbs; i++)
+                    {
+                        if( use_mean_field_cd )
+                        {
+                            connection->setAsUpInput( hidden_layer->expectation );
+                        }
+                        else
+                        {
+                            hidden_layer->generateSample();
+                            connection->setAsUpInput( hidden_layer->sample );
+                        }
+                        input_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        input_layer->computeExpectation();
+                        if( use_mean_field_cd )
+                        {
+                            connection->setAsDownInput( input_layer->expectation );
+                        }
+                        else
+                        {
+                            input_layer->generateSample();
+                            connection->setAsDownInput( input_layer->sample );
+                        }
+                        hidden_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        hidden_layer->computeExpectation();
+                    }
+                    
+                    if( use_mean_field_cd )
+                        neg_input = input_layer->expectation;
+                    else
+                        neg_input = input_layer->sample;
+                    neg_hidden = hidden_layer->expectation;
+                    
+                    input_layer->update(pos_input,neg_input);
+                    hidden_layer->update(pos_hidden,neg_hidden);
+                    connection->update(pos_input,pos_hidden,
+                                       neg_input,neg_hidden);
                 }
+
+                if( !fast_is_equal(persistent_cd_weight, 0.) )
+                {
+                    if( use_mean_field_cd )
+                        PLERROR("In PseudolikelihoodRBM::train(): Persistent "
+                            "Contrastive Divergence was not implemented for "
+                            "MF-CD");
+
+                    if( cd_decrease_ct != 0 )
+                        lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                    else
+                        lr = cd_learning_rate;
+                    
+                    lr *= persistent_cd_weight;
+
+                    if( !persistent_gibbs_chain_is_started )
+                    {  
+                        // Start gibbs chain
+                        connection->setAsDownInput( input );
+                        hidden_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        hidden_layer->computeExpectation();
+                        hidden_layer->generateSample();
+                        pers_cd_hidden << hidden_layer->sample;
+                        persistent_gibbs_chain_is_started = true;
+                    }
+
+                    if( fast_is_equal(persistent_cd_weight, 1.) )
+                    {
+                        // Hidden positive sample was not computed previously
+                        connection->setAsDownInput( input );
+                        hidden_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        hidden_layer->computeExpectation();
+                        pos_hidden << hidden_layer->expectation;
+                    }
+
+                    // Prolonged Gibbs chain
+                    for(int i=0; i<cd_n_gibbs; i++)
+                    {
+                        connection->setAsUpInput( pers_cd_hidden );
+                        input_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        input_layer->computeExpectation();
+                        input_layer->generateSample();
+                        connection->setAsDownInput( input_layer->sample );
+                        hidden_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        hidden_layer->computeExpectation();
+                        hidden_layer->generateSample();
+                    }
+
+                    pers_cd_input << input_layer->sample;
+                    pers_cd_hidden << hidden_layer->sample;
+
+                    input_layer->update(input, pers_cd_input);
+                    hidden_layer->update(pos_hidden,hidden_layer->expectation);
+                    connection->update(input,pos_hidden,
+                                       pers_cd_input,hidden_layer->expectation);
+                }
+            }
+        
+            if( !fast_is_equal(denoising_learning_rate, 0.) )
+            {
+                if( denoising_decrease_ct != 0 )
+                    lr = denoising_learning_rate / 
+                        (1.0 + stage * denoising_decrease_ct );
+                else
+                    lr = denoising_learning_rate;
+
+                setLearningRate(lr);
+                // I'm here
+                if( fraction_of_masked_inputs > 0 )
+                    random_gen->shuffleElements(autoencoder_input_indices);
                 
-                neg_input = input_layer->sample;
-                neg_hidden = hidden_layer->expectation;
+                masked_autoencoder_input << input;
+                if( fraction_of_masked_inputs > 0 )
+                {
+                    for( int j=0 ; 
+                         j < round(fraction_of_masked_inputs*input_layer->size) ; 
+                         j++)
+                        masked_autoencoder_input[ autoencoder_input_indices[j] ] = 0; 
+                }
 
-                input_layer->update(pos_input,neg_input);
-                hidden_layer->update(pos_hidden,neg_hidden);
-                connection->update(pos_input,pos_hidden,
-                                   neg_input,neg_hidden);
+                // Somehow, doesn't compile without the fancy casts...
+                ((RBMMatrixConnection *)connection)->RBMConnection::fprop( masked_autoencoder_input, 
+                                   hidden_layer->activation );
+
+                hidden_layer->fprop( hidden_layer->activation,
+                                     hidden_layer->expectation );
+                
+                transpose_connection->fprop( hidden_layer->expectation,
+                                             input_layer->activation );
+                input_layer->fprop( input_layer->activation,
+                                    input_layer->expectation );
+                input_layer->setExpectation( input_layer->expectation );
+
+                real cost = input_layer->fpropNLL(input);
+                
+                input_layer->bpropNLL(input, cost, 
+                                      reconstruction_activation_gradient);
+                input_layer->update( reconstruction_activation_gradient );
+
+                transpose_connection->bpropUpdate( 
+                    hidden_layer->expectation,
+                    input_layer->activation,
+                    hidden_layer_expectation_gradient,
+                    reconstruction_activation_gradient );
+
+                hidden_layer->bpropUpdate( hidden_layer->activation,
+                                           hidden_layer->expectation,
+                                           hidden_layer_activation_gradient,
+                                           hidden_layer_expectation_gradient );
+                
+                connection->bpropUpdate( masked_autoencoder_input, 
+                                         hidden_layer->activation,
+                                         reconstruction_activation_gradient, // is not used afterwards...
+                                         hidden_layer_activation_gradient );
             }
-            
+
         }
         train_stats->update( train_costs );
         
@@ -884,28 +1103,28 @@
     train_costs[cumulative_training_time_cost_index] = cumulative_training_time;
     train_stats->update( train_costs );
 
-//    // Sums to 1 test
-//    compute_Z();
-//    conf.resize( input_layer->size );
-//    Vec output,costs;
-//    output.resize(outputsize());
-//    costs.resize(getTestCostNames().length());
-//    target.resize( targetsize() );
-//    real sums = 0;
-//    int input_n_conf = input_layer->getConfigurationCount();
-//    for(int i=0; i<input_n_conf; i++)
-//    {
-//        input_layer->getConfiguration(i,conf);
-//        computeOutput(conf,output);
-//        computeCostsFromOutputs( conf, output, target, costs );
-//        //if( i==0 )
-//        //    sums = -costs[nll_cost_index];
-//        //else
-//        //    sums = logadd( sums, -costs[nll_cost_index] );
-//        sums += safeexp( -costs[nll_cost_index] );
-//    }        
-//    cout << "sums: " << //safeexp(sums) << endl;
-//        sums << endl;
+    // Sums to 1 test
+    //compute_Z();
+    //conf.resize( input_layer->size );
+    //Vec output,costs;
+    //output.resize(outputsize());
+    //costs.resize(getTestCostNames().length());
+    //target.resize( targetsize() );
+    //real sums = 0;
+    //int input_n_conf = input_layer->getConfigurationCount();
+    //for(int i=0; i<input_n_conf; i++)
+    //{
+    //    input_layer->getConfiguration(i,conf);
+    //    computeOutput(conf,output);
+    //    computeCostsFromOutputs( conf, output, target, costs );
+    //    //if( i==0 )
+    //    //    sums = -costs[nll_cost_index];
+    //    //else
+    //    //    sums = logadd( sums, -costs[nll_cost_index] );
+    //    sums += safeexp( -costs[nll_cost_index] );
+    //}        
+    //cout << "sums: " << //safeexp(sums) << endl;
+    //    sums << endl;
     train_stats->finalize();
 }
 

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-10 19:30:04 UTC (rev 8978)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-11 13:00:44 UTC (rev 8979)
@@ -45,6 +45,7 @@
 #include <plearn_learners/online/CrossEntropyCostModule.h>
 #include <plearn_learners/online/NLLCostModule.h>
 #include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMMatrixTransposeConnection.h>
 #include <plearn_learners/online/RBMMultitaskClassificationModule.h>
 #include <plearn_learners/online/RBMLayer.h>
 #include <plearn_learners/online/RBMMixedLayer.h>
@@ -80,6 +81,23 @@
     //! Number of negative phase gibbs sampling steps
     int cd_n_gibbs;
 
+    //! Weight of Persistent Contrastive Divergence, i.e. weight of the
+    //! prolonged gibbs chain
+    real persistent_cd_weight;
+
+    //! Indication that a mean-field version of Contrastive Divergence
+    //! (MF-CD) should be used.
+    bool use_mean_field_cd;
+
+    //! The learning rate used for denoising autoencoder learning
+    real denoising_learning_rate;
+
+    //! The decrease constant of the denoising autoencoder learning rate
+    real denoising_decrease_ct;
+
+    //! Fraction of input components set to 0 for denoising autoencoder learning
+    real fraction_of_masked_inputs;
+
     //! Number of classes in the training set (for supervised learning)
     int n_classes;
 
@@ -111,6 +129,8 @@
     //! The computed cost names
     TVec<string> cost_names;
 
+    PP<RBMMatrixTransposeConnection> transpose_connection;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -232,6 +252,13 @@
     mutable Vec pos_hidden;
     mutable Vec neg_input;
     mutable Vec neg_hidden;
+    mutable Vec reconstruction_activation_gradient;
+    mutable Vec hidden_layer_expectation_gradient;
+    mutable Vec hidden_layer_activation_gradient;
+    mutable Vec masked_autoencoder_input;
+    mutable TVec<int> autoencoder_input_indices;
+    mutable Vec pers_cd_input;
+    mutable Vec pers_cd_hidden;
 
     //! Keeps the index of the NLL cost in train_costs
     int nll_cost_index;
@@ -251,9 +278,13 @@
     //! Normalisation constant (on log scale)
     mutable real log_Z;
 
-    // Indication that the normalisation constant Z is up to date
+    //! Indication that the normalisation constant Z is up to date
     mutable bool Z_is_up_to_date;
 
+    //! Indication that the prolonged gibbs chain for 
+    //! Persistent Consistent Divergence is started
+    mutable bool persistent_gibbs_chain_is_started;
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From larocheh at mail.berlios.de  Sun May 11 15:23:43 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sun, 11 May 2008 15:23:43 +0200
Subject: [Plearn-commits] r8980 - trunk/plearn_learners/online
Message-ID: <200805111323.m4BDNhbW023826@sheep.berlios.de>

Author: larocheh
Date: 2008-05-11 15:23:43 +0200 (Sun, 11 May 2008)
New Revision: 8980

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMWoodsLayer.cc
Log:
Removed the call to addBiasDecay in bpropNLL as well as in bpropUpdate with rbm_bias given as a parameter.


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-05-11 13:00:44 UTC (rev 8979)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-05-11 13:23:43 UTC (rev 8980)
@@ -325,7 +325,6 @@
     }
 
     rbm_bias_gradient << input_gradient;
-    addBiasDecay(rbm_bias_gradient);
 }
 
 real RBMBinomialLayer::fpropNLL(const Vec& target)
@@ -417,7 +416,6 @@
 
     // bias_gradient = expectation - target
     substract(expectation, target, bias_gradient);
-    addBiasDecay(bias_gradient);
 }
 
 void RBMBinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -433,8 +431,6 @@
 
     // bias_gradients = expectations - targets
     substract(expectations, targets, bias_gradients);
-
-    addBiasDecay(bias_gradients);
 }
 
 void RBMBinomialLayer::declareOptions(OptionList& ol)

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-05-11 13:00:44 UTC (rev 8979)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-05-11 13:23:43 UTC (rev 8980)
@@ -784,7 +784,6 @@
 
     // bias_gradient = expectation - target
     substract(expectation, target, bias_gradient);
-    addBiasDecay(bias_gradient);
 }
 
 void RBMGaussianLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -800,7 +799,6 @@
 
     // bias_gradients = expectations - targets
     substract(expectations, targets, bias_gradients);
-    addBiasDecay(bias_gradients);
 }
 
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-05-11 13:00:44 UTC (rev 8979)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-05-11 13:23:43 UTC (rev 8980)
@@ -295,7 +295,6 @@
         ing[i] = (outg[i] - outg_dot_out) * out[i];
 
     rbm_bias_gradient << input_gradient;
-    addBiasDecay(rbm_bias_gradient);
 }
 
 //////////////
@@ -385,7 +384,6 @@
 
     // bias_gradient = expectation - target
     substract(expectation, target, bias_gradient);
-    addBiasDecay(bias_gradient);
 }
 
 void RBMMultinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -401,7 +399,6 @@
 
     // bias_gradients = expectations - targets
     substract(expectations, targets, bias_gradients);
-    addBiasDecay(bias_gradients);
 }
 
 void RBMMultinomialLayer::declareOptions(OptionList& ol)

Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-05-11 13:00:44 UTC (rev 8979)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-05-11 13:23:43 UTC (rev 8980)
@@ -839,7 +839,6 @@
     }
 
     rbm_bias_gradient << input_gradient;
-    addBiasDecay(rbm_bias_gradient);
 }
 
 real RBMWoodsLayer::fpropNLL(const Vec& target)
@@ -936,7 +935,6 @@
 
     // bias_gradient = expectation - target
     substract(expectation, target, bias_gradient);
-    addBiasDecay(bias_gradient);
 }
 
 void RBMWoodsLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -954,7 +952,6 @@
     // bias_gradients = expectations - targets
     substract(expectations, targets, bias_gradients);
 
-    addBiasDecay(bias_gradients);
 }
 
 void RBMWoodsLayer::declareOptions(OptionList& ol)



From larocheh at mail.berlios.de  Mon May 12 15:20:31 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 12 May 2008 15:20:31 +0200
Subject: [Plearn-commits] r8981 - trunk/plearn_learners_experimental
Message-ID: <200805121320.m4CDKVGO005719@sheep.berlios.de>

Author: larocheh
Date: 2008-05-12 15:20:31 +0200 (Mon, 12 May 2008)
New Revision: 8981

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added some other options...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-11 13:23:43 UTC (rev 8980)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-12 13:20:31 UTC (rev 8981)
@@ -62,6 +62,7 @@
     cd_decrease_ct( 0. ),
     cd_n_gibbs( 1 ),
     persistent_cd_weight( 0. ),
+    n_gibbs_chains( 1 ),
     use_mean_field_cd( false ),
     denoising_learning_rate( 0. ),
     denoising_decrease_ct( 0. ),
@@ -76,8 +77,7 @@
     cumulative_training_time( 0 ),
     //cumulative_testing_time( 0 ),
     log_Z( MISSING_VALUE ),
-    Z_is_up_to_date( false ),
-    persistent_gibbs_chain_is_started( false )
+    Z_is_up_to_date( false )
 {
     random_gen = new PRandom();
 }
@@ -114,6 +114,12 @@
                   "Weight of Persistent Contrastive Divergence, i.e. "
                   "weight of the prolonged gibbs chain.\n");
 
+    declareOption(ol, "n_gibbs_chains", 
+                  &PseudolikelihoodRBM::n_gibbs_chains,
+                  OptionBase::buildoption,
+                  "Number of gibbs chains maintained in parallel for "
+                  "Persistent Contrastive Divergence.\n");
+
     declareOption(ol, "use_mean_field_cd", &PseudolikelihoodRBM::use_mean_field_cd,
                   OptionBase::buildoption,
                   "Indication that a mean-field version of Contrastive "
@@ -193,7 +199,7 @@
                   &PseudolikelihoodRBM::persistent_gibbs_chain_is_started,
                   OptionBase::learntoption,
                   "Indication that the prolonged gibbs chain for "
-                  "Persistent Consistent Divergence is started.\n");
+                  "Persistent Consistent Divergence is started, for each chain.\n");
 
 //    declareOption(ol, "target_weights_L1_penalty_factor", 
 //                  &PseudolikelihoodRBM::target_weights_L1_penalty_factor,
@@ -326,8 +332,18 @@
 
     // CD option
     pos_hidden.resize( hidden_layer->size );
-    pers_cd_input.resize( input_layer->size );
-    pers_cd_hidden.resize( hidden_layer->size );
+    pers_cd_input.resize( n_gibbs_chains );
+    pers_cd_hidden.resize( n_gibbs_chains );
+    for( int i=0; i<n_gibbs_chains; i++ )
+    {
+        pers_cd_input[i].resize( input_layer->size );
+        pers_cd_hidden[i].resize( hidden_layer->size );
+    }
+    if( persistent_gibbs_chain_is_started.length() != n_gibbs_chains )
+    {
+        persistent_gibbs_chain_is_started.resize( n_gibbs_chains );
+        persistent_gibbs_chain_is_started.fill( false );
+    }
 
     // Denoising autoencoder options
     transpose_connection = new RBMMatrixTransposeConnection;
@@ -424,6 +440,7 @@
     deepCopyField(autoencoder_input_indices, copies);
     deepCopyField(pers_cd_input, copies);
     deepCopyField(pers_cd_hidden, copies);
+    deepCopyField(persistent_gibbs_chain_is_started, copies);
 }
 
 
@@ -450,7 +467,7 @@
     //cumulative_testing_time = 0;
     Z_is_up_to_date = false;
 
-    persistent_gibbs_chain_is_started = false;
+    persistent_gibbs_chain_is_started.fill( false );
 }
 
 ///////////
@@ -982,7 +999,11 @@
                     
                     lr *= persistent_cd_weight;
 
-                    if( !persistent_gibbs_chain_is_started )
+                    setLearningRate(lr);
+
+                    int chain_i = stage % n_gibbs_chains;
+
+                    if( !persistent_gibbs_chain_is_started[chain_i] )
                     {  
                         // Start gibbs chain
                         connection->setAsDownInput( input );
@@ -990,8 +1011,8 @@
                             (RBMMatrixConnection*) connection );
                         hidden_layer->computeExpectation();
                         hidden_layer->generateSample();
-                        pers_cd_hidden << hidden_layer->sample;
-                        persistent_gibbs_chain_is_started = true;
+                        pers_cd_hidden[chain_i] << hidden_layer->sample;
+                        persistent_gibbs_chain_is_started[chain_i] = true;
                     }
 
                     if( fast_is_equal(persistent_cd_weight, 1.) )
@@ -1004,10 +1025,11 @@
                         pos_hidden << hidden_layer->expectation;
                     }
 
+                    hidden_layer->sample << pers_cd_hidden[chain_i];
                     // Prolonged Gibbs chain
                     for(int i=0; i<cd_n_gibbs; i++)
                     {
-                        connection->setAsUpInput( pers_cd_hidden );
+                        connection->setAsUpInput( hidden_layer->sample );
                         input_layer->getAllActivations( 
                             (RBMMatrixConnection*) connection );
                         input_layer->computeExpectation();
@@ -1019,13 +1041,14 @@
                         hidden_layer->generateSample();
                     }
 
-                    pers_cd_input << input_layer->sample;
-                    pers_cd_hidden << hidden_layer->sample;
+                    pers_cd_input[chain_i] << input_layer->sample;
+                    pers_cd_hidden[chain_i] << hidden_layer->sample;
 
-                    input_layer->update(input, pers_cd_input);
+                    input_layer->update(input, pers_cd_input[chain_i]);
                     hidden_layer->update(pos_hidden,hidden_layer->expectation);
                     connection->update(input,pos_hidden,
-                                       pers_cd_input,hidden_layer->expectation);
+                                       pers_cd_input[chain_i],
+                                       hidden_layer->expectation);
                 }
             }
         

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-11 13:23:43 UTC (rev 8980)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-12 13:20:31 UTC (rev 8981)
@@ -85,6 +85,10 @@
     //! prolonged gibbs chain
     real persistent_cd_weight;
 
+    //! Number of gibbs chains maintained in parallel for 
+    //! Persistent Contrastive Divergence
+    int n_gibbs_chains;
+
     //! Indication that a mean-field version of Contrastive Divergence
     //! (MF-CD) should be used.
     bool use_mean_field_cd;
@@ -257,8 +261,8 @@
     mutable Vec hidden_layer_activation_gradient;
     mutable Vec masked_autoencoder_input;
     mutable TVec<int> autoencoder_input_indices;
-    mutable Vec pers_cd_input;
-    mutable Vec pers_cd_hidden;
+    mutable TVec<Vec> pers_cd_input;
+    mutable TVec<Vec> pers_cd_hidden;
 
     //! Keeps the index of the NLL cost in train_costs
     int nll_cost_index;
@@ -282,8 +286,8 @@
     mutable bool Z_is_up_to_date;
 
     //! Indication that the prolonged gibbs chain for 
-    //! Persistent Consistent Divergence is started
-    mutable bool persistent_gibbs_chain_is_started;
+    //! Persistent Consistent Divergence is started, for each chain
+    mutable TVec<bool> persistent_gibbs_chain_is_started;
 
 protected:
     //#####  Protected Member Functions  ######################################



From larocheh at mail.berlios.de  Mon May 12 15:48:24 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 12 May 2008 15:48:24 +0200
Subject: [Plearn-commits] r8982 - trunk/plearn_learners/online
Message-ID: <200805121348.m4CDmOxZ009107@sheep.berlios.de>

Author: larocheh
Date: 2008-05-12 15:48:23 +0200 (Mon, 12 May 2008)
New Revision: 8982

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Debugged and added unsupervised fine-tuning for correlation connections



Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-05-12 13:20:31 UTC (rev 8981)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-05-12 13:48:23 UTC (rev 8982)
@@ -110,7 +110,8 @@
                   OptionBase::buildoption,
                   " L1 penalty weight on the hidden layers, to encourage "
                   "sparsity during\n"
-                  "the greedy unsupervised phases.\n");
+                  "the greedy unsupervised phases.\n"
+                  );
 
     declareOption(ol, "l1_neuron_decay_center", 
                   &StackedAutoassociatorsNet::l1_neuron_decay_center,
@@ -312,10 +313,10 @@
                     " - \n"
                     "cannot use online setting with reconstruct_hidden=true.\n");
 
-        if( unsupervised_nstages > 0 && correlation_connections.length() != 0 )
-            PLERROR("StackedAutoassociatorsNet::build_()"
-                    " - \n"
-                    "cannot use unsupervised fine-tuning with correlation connections.\n");
+//        if( unsupervised_nstages > 0 && correlation_connections.length() != 0 )
+//            PLERROR("StackedAutoassociatorsNet::build_()"
+//                    " - \n"
+//                    "cannot use unsupervised fine-tuning with correlation connections.\n");
 
         if( fraction_of_masked_inputs < 0 )
             PLERROR("StackedAutoassociatorsNet::build_()"
@@ -868,10 +869,10 @@
         /***** unsupervised fine-tuning by gradient descent *****/
         if( unsupervised_stage < unsupervised_nstages )
         {
-            if( unsupervised_nstages > 0 && correlation_connections.length() != 0 )
-                PLERROR("StackedAutoassociatorsNet::train()"
-                        " - \n"
-                        "cannot use unsupervised fine-tuning with correlation connections.\n");
+//            if( unsupervised_nstages > 0 && correlation_connections.length() != 0 )
+//                PLERROR("StackedAutoassociatorsNet::train()"
+//                        " - \n"
+//                        "cannot use unsupervised fine-tuning with correlation connections.\n");
             
             MODULE_LOG << "Unsupervised fine-tuning all parameters, ";
             MODULE_LOG << "by gradient descent" << endl;
@@ -1226,18 +1227,19 @@
 
     }
 
+    
     if(!fast_exact_is_equal(l1_neuron_decay,0))
     {
         // Compute L1 penalty gradient on neurons
         real* hid = expectations[ index + 1 ].data();
         real* grad = reconstruction_expectation_gradients.data();
         int len = expectations[ index + 1 ].length();
-        for(int i=0; i<len; i++)
+        for(int l=0; l<len; l++)
         {
             if(*hid > l1_neuron_decay_center)
+                *grad += l1_neuron_decay;
+            else if(*hid < l1_neuron_decay_center)
                 *grad -= l1_neuron_decay;
-            else if(*hid < l1_neuron_decay_center)
-                *grad += l1_neuron_decay;
             hid++;
             grad++;
         }
@@ -1310,32 +1312,72 @@
     // fprop
     expectations[0] << input;
 
-    if( fraction_of_masked_inputs > 0 )
+    if(correlation_connections.length() != 0)
     {
-        for( int i=0; i<autoassociator_expectation_indices.length(); i++ )
-            random_gen->shuffleElements(autoassociator_expectation_indices[i]);
-        
-        for( int i=0 ; i<n_layers-1; i++ )
+        if( fraction_of_masked_inputs > 0 )
         {
-            masked_autoassociator_expectations[i] << expectations[i];
-            if( !(mask_input_layer_only_in_unsupervised_fine_tuning && i > 0) )
-                for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
-                    masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
+            for( int i=0; i<autoassociator_expectation_indices.length(); i++ )
+                random_gen->shuffleElements(autoassociator_expectation_indices[i]);
             
-            connections[i]->fprop( masked_autoassociator_expectations[i], 
-                                   activations[i+1] );
-            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+            for( int i=0 ; i<n_layers-1; i++ )
+            {
+                masked_autoassociator_expectations[i] << expectations[i];
+                if( !(mask_input_layer_only_in_unsupervised_fine_tuning && i > 0) )
+                    for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
+                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
+                
+                connections[i]->fprop( masked_autoassociator_expectations[i], 
+                                       correlation_activations[i] );
+                layers[i+1]->fprop( correlation_activations[i],
+                                    correlation_expectations[i] );
+                correlation_connections[i]->fprop( correlation_expectations[i], 
+                                                   activations[i+1] );
+                correlation_layers[i]->fprop( activations[i+1], 
+                                              expectations[i+1] );
+            }
         }
+        else
+        {
+            for( int i=0 ; i<n_layers-1; i++ )
+            {
+                connections[i]->fprop( expectations[i], correlation_activations[i]);
+                layers[i+1]->fprop( correlation_activations[i],
+                                    correlation_expectations[i] );
+                correlation_connections[i]->fprop( correlation_expectations[i], 
+                                                   activations[i+1] );
+                correlation_layers[i]->fprop( activations[i+1], 
+                                              expectations[i+1] );
+            }
+        }
     }
     else
     {
-        for( int i=0 ; i<n_layers-1; i++ )
+        if( fraction_of_masked_inputs > 0 )
         {
-            connections[i]->fprop( expectations[i], activations[i+1] );
-            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+            for( int i=0; i<autoassociator_expectation_indices.length(); i++ )
+                random_gen->shuffleElements(autoassociator_expectation_indices[i]);
+            
+            for( int i=0 ; i<n_layers-1; i++ )
+            {
+                masked_autoassociator_expectations[i] << expectations[i];
+                if( !(mask_input_layer_only_in_unsupervised_fine_tuning && i > 0) )
+                    for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
+                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
+                
+                connections[i]->fprop( masked_autoassociator_expectations[i], 
+                                       activations[i+1] );
+                layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+            }
         }
+        else
+        {
+            for( int i=0 ; i<n_layers-1; i++ )
+            {
+                connections[i]->fprop( expectations[i], activations[i+1] );
+                layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+            }
+        }
     }
-
     fine_tuning_reconstruction_expectations[ n_layers-1 ] << 
         expectations[ n_layers-1 ];
 
@@ -1377,18 +1419,73 @@
     
     for( int i=n_layers-2 ; i>=0; i-- )
     {
-        layers[i+1]->bpropUpdate(
-            activations[i+1],expectations[i+1],
-            activation_gradients[i+1],expectation_gradients[i+1]);
-        if( fraction_of_masked_inputs > 0 )
-            connections[i]->bpropUpdate( 
-                masked_autoassociator_expectations[i], activations[i+1],
-                expectation_gradients[i], activation_gradients[i+1] );
+
+        if(!fast_exact_is_equal(l1_neuron_decay,0))
+        {
+            // Compute L1 penalty gradient on neurons
+            real* hid = expectations[ i + 1 ].data();
+            real* grad = expectation_gradients[ i + 1 ].data();
+            int len = expectations[ i + 1 ].length();
+            for(int l=0; l<len; l++)
+            {
+                if(*hid > l1_neuron_decay_center)
+                    *grad += l1_neuron_decay;
+                else if(*hid < l1_neuron_decay_center)
+                    *grad -= l1_neuron_decay;
+                hid++;
+                grad++;
+            }
+        }
+
+        if(correlation_connections.length() != 0)
+        {
+            correlation_layers[ i ]->bpropUpdate(
+                activations[ i + 1 ],
+                expectations[ i + 1 ],
+                activation_gradients[ i + 1 ],
+                expectation_gradients[ i + 1 ]
+                );
+            
+            correlation_connections[ i ]->bpropUpdate( 
+                correlation_expectations[ i ],
+                activations[ i + 1 ],
+                correlation_expectation_gradients[ i ], 
+                activation_gradients[ i + 1 ] );
+            
+            layers[ i + 1 ]->bpropUpdate( 
+                correlation_activations[ i ],
+                correlation_expectations[ i ],
+                correlation_activation_gradients [ i ],
+                correlation_expectation_gradients [ i ]);    
+            
+            if( fraction_of_masked_inputs > 0 )
+                connections[ i ]->bpropUpdate( 
+                    masked_autoassociator_expectations[ i ],
+                    correlation_activations[ i ],
+                    expectation_gradients[i], 
+                    correlation_activation_gradients [ i ]);
+            else
+                connections[ i ]->bpropUpdate( 
+                    expectations[ i ],
+                    correlation_activations[ i ],
+                    expectation_gradients[i],
+                    correlation_activation_gradients [ i ]);
+        }
         else
-            connections[i]->bpropUpdate( 
-                expectations[i], activations[i+1],
-                expectation_gradients[i], activation_gradients[i+1] );
+        {
 
+            layers[i+1]->bpropUpdate(
+                activations[i+1],expectations[i+1],
+                activation_gradients[i+1],expectation_gradients[i+1]);
+            if( fraction_of_masked_inputs > 0 )
+                connections[i]->bpropUpdate( 
+                    masked_autoassociator_expectations[i], activations[i+1],
+                    expectation_gradients[i], activation_gradients[i+1] );
+            else
+                connections[i]->bpropUpdate( 
+                    expectations[i], activations[i+1],
+                    expectation_gradients[i], activation_gradients[i+1] );
+        }
     }
 }
 
@@ -1635,9 +1732,9 @@
             for(int j=0; j<len; j++)
             {
                 if(*hid > l1_neuron_decay_center)
+                    *grad += l1_neuron_decay;
+                else if(*hid < l1_neuron_decay_center)
                     *grad -= l1_neuron_decay;
-                else if(*hid < l1_neuron_decay_center)
-                    *grad += l1_neuron_decay;
                 hid++;
                 grad++;
             }



From nouiz at mail.berlios.de  Mon May 12 16:08:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 12 May 2008 16:08:11 +0200
Subject: [Plearn-commits] r8983 - trunk/plearn/vmat
Message-ID: <200805121408.m4CE8Btr010846@sheep.berlios.de>

Author: nouiz
Date: 2008-05-12 16:08:10 +0200 (Mon, 12 May 2008)
New Revision: 8983

Modified:
   trunk/plearn/vmat/FilteredVMatrix.cc
Log:
Hide not used option. Those option was already overrided by the code.


Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2008-05-12 13:48:23 UTC (rev 8982)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2008-05-12 14:08:10 UTC (rev 8983)
@@ -230,6 +230,30 @@
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
+
+    declareOption(ol, "length", &FilteredVMatrix::length_,
+                  OptionBase::nosave,
+                  "The number of example. Computed each time from source.");
+
+    redeclareOption(ol, "inputsize", &FilteredVMatrix::inputsize_,
+                    OptionBase::nosave,
+                    "Taken from source in  FilteredVMatrix.");
+    
+    redeclareOption(ol, "targetsize", &FilteredVMatrix::targetsize_,
+                    OptionBase::nosave,
+                    "Taken from source in FilteredVMatrix.");
+    
+    redeclareOption(ol, "weightsize", &FilteredVMatrix::weightsize_,
+                    OptionBase::nosave,
+                    "Taken from source in FilteredVMatrix.");
+    
+    redeclareOption(ol, "extrasize", &FilteredVMatrix::extrasize_,
+                    OptionBase::nosave,
+                    "Taken from source in FilteredVMatrix.");
+
+    redeclareOption(ol, "width", &FilteredVMatrix::width_,
+                    OptionBase::nosave,
+                    "Taken from source in FilteredVMatrix.");
 }
 
 ////////////



From larocheh at mail.berlios.de  Mon May 12 16:47:49 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 12 May 2008 16:47:49 +0200
Subject: [Plearn-commits] r8984 - trunk/plearn_learners/online
Message-ID: <200805121447.m4CElnKE016578@sheep.berlios.de>

Author: larocheh
Date: 2008-05-12 16:47:48 +0200 (Mon, 12 May 2008)
New Revision: 8984

Added:
   trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc
   trunk/plearn_learners/online/RBMDiagonalMatrixConnection.h
Log:
Diagonal matrix connection...


Added: trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc	2008-05-12 14:08:10 UTC (rev 8983)
+++ trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc	2008-05-12 14:47:48 UTC (rev 8984)
@@ -0,0 +1,658 @@
+// -*- C++ -*-
+
+// RBMDiagonalMatrixConnection.cc
+//
+// Copyright (C) 2006 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMDiagonalMatrixConnection.cc */
+
+
+
+#include "RBMDiagonalMatrixConnection.h"
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMDiagonalMatrixConnection,
+    "Stores and learns the parameters between two linear layers of an RBM",
+    "");
+
+RBMDiagonalMatrixConnection::RBMDiagonalMatrixConnection( real the_learning_rate ) :
+    inherited(the_learning_rate)
+{
+}
+
+void RBMDiagonalMatrixConnection::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "weights_diag", &RBMDiagonalMatrixConnection::weights_diag,
+                  OptionBase::learntoption,
+                  "Vector containing the diagonal of the weight matrix.\n");
+
+    declareOption(ol, "L1_penalty_factor", 
+                  &RBMDiagonalMatrixConnection::L1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L1 regularization term, i.e.\n"
+                  "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| "
+                  "during training.\n");
+
+    declareOption(ol, "L2_penalty_factor", 
+                  &RBMDiagonalMatrixConnection::L2_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L2 regularization term, i.e.\n"
+                  "minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 "
+                  "during training.\n");
+
+    declareOption(ol, "L2_decrease_constant", 
+                  &RBMDiagonalMatrixConnection::L2_decrease_constant,
+                  OptionBase::buildoption,
+        "Parameter of the L2 penalty decrease (see L2_decrease_type).",
+        OptionBase::advanced_level);
+
+    declareOption(ol, "L2_shift", 
+                  &RBMDiagonalMatrixConnection::L2_shift,
+                  OptionBase::buildoption,
+        "Parameter of the L2 penalty decrease (see L2_decrease_type).",
+        OptionBase::advanced_level);
+
+    declareOption(ol, "L2_decrease_type", 
+                  &RBMDiagonalMatrixConnection::L2_decrease_type,
+                  OptionBase::buildoption,
+        "The kind of L2 decrease that is being applied. The decrease\n"
+        "consists in scaling the L2 penalty by a factor that depends on the\n"
+        "number 't' of times this penalty has been used to modify the\n"
+        "weights of the connection. It can be one of:\n"
+        " - 'one_over_t': 1 / (1 + t * L2_decrease_constant)\n"
+        " - 'sigmoid_like': sigmoid((L2_shift - t) * L2_decrease_constant)",
+        OptionBase::advanced_level);
+
+    declareOption(ol, "L2_n_updates", 
+                  &RBMDiagonalMatrixConnection::L2_n_updates,
+                  OptionBase::learntoption,
+        "Number of times that weights have been changed by the L2 penalty\n"
+        "update rule.");
+
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMDiagonalMatrixConnection::build_()
+{
+    if( up_size <= 0 || down_size <= 0 )
+        return;
+
+    if( up_size != down_size )
+        PLERROR("In RBMDiagonalMatrixConnection::build_(): up_size should be "
+            "equal to down_size");
+
+    bool needs_forget = false; // do we need to reinitialize the parameters?
+
+    if( weights_diag.length() != up_size )
+    {
+        weights_diag.resize( up_size );
+        needs_forget = true;
+    }
+
+    weights_pos_stats.resize( up_size );
+    weights_neg_stats.resize( up_size );
+
+    if( momentum != 0. )
+        weights_inc.resize( up_size );
+
+    if( needs_forget )
+        forget();
+
+    clearStats();
+}
+
+void RBMDiagonalMatrixConnection::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMDiagonalMatrixConnection::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(weights_diag, copies);
+    deepCopyField(weights_pos_stats, copies);
+    deepCopyField(weights_neg_stats, copies);
+    deepCopyField(weights_inc, copies);
+}
+
+void RBMDiagonalMatrixConnection::accumulatePosStats( const Vec& down_values,
+                                              const Vec& up_values )
+{
+    real* wps = weights_pos_stats.data();
+    real* uv = up_values.data();
+    real* dv = down_values.data();
+    for( int i=0; i<up_size; i++ )
+        wps[i] += uv[i]*dv[i];
+
+    pos_count++;
+}
+
+void RBMDiagonalMatrixConnection::accumulatePosStats( const Mat& down_values,
+                                              const Mat& up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+
+    real* wps;
+    real* uv;
+    real* dv;
+    for( int t=0; t<mbs; t++ )
+    {
+        wps = weights_pos_stats.data();
+        uv = up_values[t];
+        dv = down_values[t];
+        for( int i=0; i<up_size; i++ )
+            wps[i] += uv[i]*dv[i];
+    }
+    pos_count+=mbs;
+}
+
+////////////////////////
+// accumulateNegStats //
+////////////////////////
+void RBMDiagonalMatrixConnection::accumulateNegStats( const Vec& down_values,
+                                              const Vec& up_values )
+{
+    real* wns = weights_neg_stats.data();
+    real* uv = up_values.data();
+    real* dv = down_values.data();
+    for( int i=0; i<up_size; i++ )
+        wns[i] += uv[i]*dv[i];
+    neg_count++;
+}
+
+void RBMDiagonalMatrixConnection::accumulateNegStats( const Mat& down_values,
+                                              const Mat& up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+
+    real* wns;
+    real* uv;
+    real* dv;
+    for( int t=0; t<mbs; t++ )
+    {
+        wns = weights_neg_stats.data();
+        uv = up_values[t];
+        dv = down_values[t];
+        for( int i=0; i<up_size; i++ )
+            wns[i] += uv[i]*dv[i];
+    }
+    neg_count+=mbs;
+}
+
+////////////
+// update //
+////////////
+void RBMDiagonalMatrixConnection::update()
+{
+    // updates parameters
+    //weights += learning_rate * (weights_pos_stats/pos_count
+    //                              - weights_neg_stats/neg_count)
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = -learning_rate / neg_count;
+
+    int l = weights_diag.length();
+
+    real* w_i = weights_diag.data();
+    real* wps_i = weights_pos_stats.data();
+    real* wns_i = weights_neg_stats.data();
+
+    if( momentum == 0. )
+    {
+        // no need to use weights_inc
+        for( int i=0 ; i<l ; i++ )
+            w_i[i] += pos_factor * wps_i[i] + neg_factor * wns_i[i];
+    }
+    else
+    {
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l );
+
+        // The update rule becomes:
+        // weights_inc = momentum * weights_inc
+        //               - learning_rate * (weights_pos_stats/pos_count
+        //                                  - weights_neg_stats/neg_count);
+        // weights += weights_inc;
+        real* winc_i = weights_inc.data();
+        for( int i=0 ; i<l ; i++ )
+        {
+            winc_i[i] = momentum * winc_i[i]
+                + pos_factor * wps_i[i] + neg_factor * wns_i[i];
+            w_i[i] += winc_i[i];
+        }
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+
+    clearStats();
+}
+
+// Instead of using the statistics, we assume we have only one markov chain
+// runned and we update the parameters from the first 4 values of the chain
+void RBMDiagonalMatrixConnection::update( const Vec& pos_down_values, // v_0
+                                  const Vec& pos_up_values,   // h_0
+                                  const Vec& neg_down_values, // v_1
+                                  const Vec& neg_up_values )  // h_1
+{
+    int l = weights_diag.length();
+    PLASSERT( pos_up_values.length() == l );
+    PLASSERT( neg_up_values.length() == l );
+    PLASSERT( pos_down_values.length() == l );
+    PLASSERT( neg_down_values.length() == l );
+
+    real* w_i = weights_diag.data();
+    real* pdv = pos_down_values.data();
+    real* puv = pos_up_values.data();
+    real* ndv = neg_down_values.data();
+    real* nuv = neg_up_values.data();
+
+    if( momentum == 0. )
+    {
+        for( int i=0 ; i<l ; i++)
+            w_i[i] += learning_rate * (puv[i] * pdv[i] - nuv[i] * ndv[i]);
+    }
+    else
+    {
+        // ensure that weights_inc has the right size
+        weights_inc.resize( l );
+
+        real* winc_i = weights_inc.data();
+        for( int i=0 ; i<l ; i++ )
+        {
+            winc_i[i] = momentum * winc_i[i]
+                + learning_rate * (puv[i] * pdv[i] - nuv[i] * ndv[i]);
+            w_i[i] += winc_i[i];
+        }
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+void RBMDiagonalMatrixConnection::update( const Mat& pos_down_values, // v_0
+                                  const Mat& pos_up_values,   // h_0
+                                  const Mat& neg_down_values, // v_1
+                                  const Mat& neg_up_values )  // h_1
+{
+    // weights += learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // or:
+    // weights[i][j] += learning_rate * (h_0[i] v_0[j] - h_1[i] v_1[j]);
+
+    int l = weights_diag.length();
+
+    PLASSERT( pos_up_values.width() == l );
+    PLASSERT( neg_up_values.width() == l );
+    PLASSERT( pos_down_values.width() == l );
+    PLASSERT( neg_down_values.width() == l );
+
+    real* w_i = weights_diag.data();
+    real* pdv;
+    real* puv;
+    real* ndv;
+    real* nuv;
+
+    if( momentum == 0. )
+    {
+        // We use the average gradient over a mini-batch.
+        real avg_lr = learning_rate / pos_down_values.length();
+
+        for( int t=0; t<pos_up_values.length(); t++ )
+        {
+            pdv = pos_down_values[t];
+            puv = pos_up_values[t];
+            ndv = neg_down_values[t];
+            nuv = neg_up_values[t];
+            for( int i=0 ; i<l ; i++)
+                w_i[i] += avg_lr * (puv[i] * pdv[i] - nuv[i] * ndv[i]);
+        }
+    }
+    else
+    {
+        PLERROR("RBMDiagonalMatrixConnection::update minibatch with momentum - Not implemented");
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+////////////////
+// clearStats //
+////////////////
+void RBMDiagonalMatrixConnection::clearStats()
+{
+    weights_pos_stats.clear();
+    weights_neg_stats.clear();
+
+    pos_count = 0;
+    neg_count = 0;
+}
+
+////////////////////
+// computeProduct //
+////////////////////
+void RBMDiagonalMatrixConnection::computeProduct( int start, int length,
+                                          const Vec& activations,
+                                          bool accumulate ) const
+{
+    PLASSERT( activations.length() == length );
+    PLASSERT( start+length <= up_size );
+    real* act = activations.data();
+    real* w = weights_diag.data();
+    real* iv = input_vec.data();
+    if( accumulate )
+        for( int i=0; i<length; i++ )
+            act[i] += w[i+start] * iv[i+start];
+    else
+        for( int i=0; i<length; i++ )
+            act[i] = w[i+start] * iv[i+start];
+}
+
+/////////////////////
+// computeProducts //
+/////////////////////
+void RBMDiagonalMatrixConnection::computeProducts(int start, int length,
+                                          Mat& activations,
+                                          bool accumulate ) const
+{
+    PLASSERT( activations.width() == length );
+    activations.resize(inputs_mat.length(), length);
+    real* act;
+    real* w = weights_diag.data();
+    real* iv;
+    if( accumulate )
+        for( int t=0; t<inputs_mat.length(); t++ )
+        {
+            act = activations[t];
+            iv = inputs_mat[t];
+            for( int i=0; i<length; i++ )
+                act[i] += w[i+start] * iv[i+start];
+        }
+    else
+        for( int t=0; t<inputs_mat.length(); t++ )
+        {
+            act = activations[t];
+            iv = inputs_mat[t];
+            for( int i=0; i<length; i++ )
+                act[i] = w[i+start] * iv[i+start];
+        }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMDiagonalMatrixConnection::bpropUpdate(const Vec& input, const Vec& output,
+                                      Vec& input_gradient,
+                                      const Vec& output_gradient,
+                                      bool accumulate)
+{
+    PLASSERT( input.size() == down_size );
+    PLASSERT( output.size() == up_size );
+    PLASSERT( output_gradient.size() == up_size );
+
+    real* w = weights_diag.data();
+    real* in = input.data();
+    real* ing = input_gradient.data();
+    real* outg = output_gradient.data();
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+
+        for( int i=0; i<down_size; i++ )
+        {
+            ing[i] += outg[i]*w[i];
+            w[i] -= learning_rate * in[i] * outg[i];
+        }
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+        for( int i=0; i<down_size; i++ )
+        {
+            ing[i] = outg[i]*w[i];
+            w[i] -= learning_rate * in[i] * outg[i];
+        }
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+void RBMDiagonalMatrixConnection::bpropUpdate(const Mat& inputs, const Mat& outputs,
+                                      Mat& input_gradients,
+                                      const Mat& output_gradients,
+                                      bool accumulate)
+{
+    PLASSERT( inputs.width() == down_size );
+    PLASSERT( outputs.width() == up_size );
+    PLASSERT( output_gradients.width() == up_size );
+
+    int mbatch = inputs.length();
+
+    real* w = weights_diag.data();
+    real* in;
+    real* ing;
+    real* outg;
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == down_size &&
+                      input_gradients.length() == inputs.length(),
+                      "Cannot resize input_gradients and accumulate into it" );
+
+        for( int t=0; t<mbatch; t++ )
+        {
+            ing = input_gradients[t];
+            outg = output_gradients[t];
+            for( int i=0; i<down_size; i++ )
+                ing[i] += outg[i]*w[i];
+        }
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), down_size);
+        for( int t=0; t<mbatch; t++ )
+        {
+            ing = input_gradients[t];
+            outg = output_gradients[t];
+            for( int i=0; i<down_size; i++ )
+                ing[i] = outg[i]*w[i];
+        }
+    }
+
+    real avg_lr = learning_rate / mbatch;
+    for( int t=0; t<mbatch; t++ )
+    {
+        in = inputs[t];
+        outg = output_gradients[t];
+        for( int i=0; i<down_size; i++ )
+            w[i] -= avg_lr * in[i] * outg[i];
+    }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+////////////////////////
+// applyWeightPenalty //
+////////////////////////
+void RBMDiagonalMatrixConnection::applyWeightPenalty()
+{
+    // Apply penalty (decay) on weights.
+    real delta_L1 = learning_rate * L1_penalty_factor;
+    real delta_L2 = learning_rate * L2_penalty_factor;
+    if (L2_decrease_type == "one_over_t")
+        delta_L2 /= (1 + L2_decrease_constant * L2_n_updates);
+    else if (L2_decrease_type == "sigmoid_like")
+        delta_L2 *= sigmoid((L2_shift - L2_n_updates) * L2_decrease_constant);
+    else
+        PLERROR("In RBMDiagonalMatrixConnection::applyWeightPenalty - Invalid value "
+                "for L2_decrease_type: %s", L2_decrease_type.c_str());
+    real* w_ = weights_diag.data();
+    for( int i=0; i<down_size; i++ )
+    {
+        if( delta_L2 != 0. )
+            w_[i] *= (1 - delta_L2);
+        
+        if( delta_L1 != 0. )
+        {
+            if( w_[i] > delta_L1 )
+                w_[i] -= delta_L1;
+            else if( w_[i] < -delta_L1 )
+                w_[i] += delta_L1;
+            else
+                w_[i] = 0.;
+        }
+    }
+    
+    if (delta_L2 > 0)
+        L2_n_updates++;
+}
+
+//////////////////////
+// addWeightPenalty //
+//////////////////////
+void RBMDiagonalMatrixConnection::addWeightPenalty(Vec weights_diag, Vec weight_diag_gradients)
+{
+    // Add penalty (decay) gradient.
+    real delta_L1 = L1_penalty_factor;
+    real delta_L2 = L2_penalty_factor;
+    PLASSERT_MSG( is_equal(L2_decrease_constant, 0) && is_equal(L2_shift, 100),
+                  "L2 decrease not implemented in this method" );
+    real* w_ = weights_diag.data();
+    real* gw_ = weight_diag_gradients.data();
+    for( int i=0; i<down_size; i++ )
+    {
+        if( delta_L2 != 0. )
+            gw_[i] += delta_L2*w_[i];
+        
+        if( delta_L1 != 0. )
+        {
+            if( w_[i] > 0 )
+                gw_[i] += delta_L1;
+            else if( w_[i] < 0 )
+                gw_[i] -= delta_L1;
+        }
+    }
+}
+
+////////////
+// forget //
+////////////
+// Reset the parameters to the state they would be BEFORE starting training.
+// Note that this method is necessarily called from build().
+void RBMDiagonalMatrixConnection::forget()
+{
+    clearStats();
+    if( initialization_method == "zero" )
+        weights_diag.clear();
+    else
+    {
+        if( !random_gen )
+        {
+            PLWARNING( "RBMDiagonalMatrixConnection: cannot forget() without"
+                       " random_gen" );
+            return;
+        }
+
+        //random_gen->manual_seed(1827);
+
+        real d = 1. / max( down_size, up_size );
+        if( initialization_method == "uniform_sqrt" )
+            d = sqrt( d );
+
+        random_gen->fill_random_uniform( weights_diag, -d, d );
+    }
+    L2_n_updates = 0;
+}
+
+
+/* THIS METHOD IS OPTIONAL
+//! reset the parameters to the state they would be BEFORE starting training.
+//! Note that this method is necessarily called from build().
+//! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT DO
+//! ANYTHING.
+void RBMDiagonalMatrixConnection::finalize()
+{
+}
+*/
+
+//! return the number of parameters
+int RBMDiagonalMatrixConnection::nParameters() const
+{
+    return weights_diag.size();
+}
+
+//! Make the parameters data be sub-vectors of the given global_parameters.
+//! The argument should have size >= nParameters. The result is a Vec
+//! that starts just after this object's parameters end, i.e.
+//!    result = global_parameters.subVec(nParameters(),global_parameters.size()-nParameters());
+//! This allows to easily chain calls of this method on multiple RBMParameters.
+Vec RBMDiagonalMatrixConnection::makeParametersPointHere(const Vec& global_parameters)
+{
+    int n=weights_diag.size();
+    int m = global_parameters.size();
+    if (m<n)
+        PLERROR("RBMDiagonalMatrixConnection::makeParametersPointHere: argument has length %d, should be longer than nParameters()=%d",m,n);
+    real* p = global_parameters.data();
+    weights_diag.makeSharedValue(p,n);
+
+    return global_parameters.subVec(n,m-n);
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMDiagonalMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMDiagonalMatrixConnection.h	2008-05-12 14:08:10 UTC (rev 8983)
+++ trunk/plearn_learners/online/RBMDiagonalMatrixConnection.h	2008-05-12 14:47:48 UTC (rev 8984)
@@ -0,0 +1,237 @@
+// -*- C++ -*-
+
+// RBMDiagonalMatrixConnection.h
+//
+// Copyright (C) 2006 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMDiagonalMatrixConnection.h */
+
+
+#ifndef RBMDiagonalMatrixConnection_INC
+#define RBMDiagonalMatrixConnection_INC
+
+#include "RBMConnection.h"
+
+namespace PLearn {
+using namespace std;
+
+
+/**
+ * Stores and learns the parameters between two linear layers of an RBM.
+ *
+ */
+class RBMDiagonalMatrixConnection: public RBMConnection
+{
+    typedef RBMConnection inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Vector containing the diagonal of the weight matrix
+    Vec weights_diag;
+
+    //! Optional (default=0) factor of L1 regularization term
+    real L1_penalty_factor;
+
+    //! Optional (default=0) factor of L2 regularization term
+    real L2_penalty_factor;
+
+    real L2_decrease_constant;
+    real L2_shift;
+    string L2_decrease_type;
+    int L2_n_updates;
+
+  //#####  Not Options  #####################################################
+
+
+    //! Accumulates positive contribution to the weights' gradient
+    Vec weights_pos_stats;
+
+    //! Accumulates negative contribution to the weights' gradient
+    Vec weights_neg_stats;
+
+    //! Used if momentum != 0.
+    Vec weights_inc;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMDiagonalMatrixConnection( real the_learning_rate=0 );
+
+    // Your other public member functions go here
+
+    //! Accumulates positive phase statistics to *_pos_stats
+    virtual void accumulatePosStats( const Vec& down_values,
+                                     const Vec& up_values );
+
+    virtual void accumulatePosStats( const Mat& down_values,
+                                     const Mat& up_values );
+
+    //! Accumulates negative phase statistics to *_neg_stats
+    virtual void accumulateNegStats( const Vec& down_values,
+                                     const Vec& up_values );
+
+    virtual void accumulateNegStats( const Mat& down_values,
+                                     const Mat& up_values );
+
+    //! Updates parameters according to contrastive divergence gradient
+    virtual void update();
+
+    //! Updates parameters according to contrastive divergence gradient,
+    //! not using the statistics but the explicit values passed
+    virtual void update( const Vec& pos_down_values,
+                         const Vec& pos_up_values,
+                         const Vec& neg_down_values,
+                         const Vec& neg_up_values );
+
+    //! Not implemented.
+    virtual void update( const Mat& pos_down_values,
+                         const Mat& pos_up_values,
+                         const Mat& neg_down_values,
+                         const Mat& neg_up_values);
+
+    //! Clear all information accumulated during stats
+    virtual void clearStats();
+
+    //! Computes the vectors of activation of "length" units,
+    //! starting from "start", and stores (or add) them into "activations".
+    //! "start" indexes an up unit if "going_up", else a down unit.
+    virtual void computeProduct( int start, int length,
+                                 const Vec& activations,
+                                 bool accumulate=false ) const;
+
+    //! Same as 'computeProduct' but for mini-batches.
+    virtual void computeProducts(int start, int length,
+                                 Mat& activations,
+                                 bool accumulate=false ) const;
+
+    //! Adapt based on the output gradient: this method should only
+    //! be called just after a corresponding fprop; it should be
+    //! called with the same arguments as fprop for the first two arguments
+    //! (and output should not have been modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+    //! JUST CALLS
+    //!     bpropUpdate(input, output, input_gradient, output_gradient)
+    //! AND IGNORES INPUT GRADIENT.
+    // virtual void bpropUpdate(const Vec& input, const Vec& output,
+    //                          const Vec& output_gradient);
+
+    //! this version allows to obtain the input gradient as well
+    //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient,
+                             bool accumulate=false);
+
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+
+    //! Applies penalty (decay) on weights
+    virtual void applyWeightPenalty();
+
+    //! Adds penalty (decay) gradient
+    virtual void addWeightPenalty(Vec weights_diag, Vec weights_diag_gradients);
+
+    //! reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+    //! optionally perform some processing after training, or after a
+    //! series of fprop/bpropUpdate calls to prepare the model for truly
+    //! out-of-sample operation.  THE DEFAULT IMPLEMENTATION PROVIDED IN
+    //! THE SUPER-CLASS DOES NOT DO ANYTHING.
+    // virtual void finalize();
+
+    //! return the number of parameters
+    virtual int nParameters() const;
+
+    //! Make the parameters data be sub-vectors of the given global_parameters.
+    //! The argument should have size >= nParameters. The result is a Vec
+    //! that starts just after this object's parameters end, i.e.
+    //!    result = global_parameters.subVec(nParameters(),global_parameters.size()-nParameters());
+    //! This allows to easily chain calls of this method on multiple RBMParameters.
+    virtual Vec makeParametersPointHere(const Vec& global_parameters);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMDiagonalMatrixConnection);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Member Functions  ######################################
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMDiagonalMatrixConnection);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Mon May 12 17:20:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 12 May 2008 17:20:15 +0200
Subject: [Plearn-commits] r8985 - trunk/plearn/vmat
Message-ID: <200805121520.m4CFKFL9022131@sheep.berlios.de>

Author: nouiz
Date: 2008-05-12 17:20:15 +0200 (Mon, 12 May 2008)
New Revision: 8985

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
hide option that are overrided.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-05-12 14:47:48 UTC (rev 8984)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-05-12 15:20:15 UTC (rev 8985)
@@ -92,6 +92,32 @@
                 "The vector of coded instruction for each variables.");
 
   inherited::declareOptions(ol);
+
+  declareOption(ol, "length", &MeanMedianModeImputationVMatrix::length_,
+		OptionBase::nosave,
+		"The number of example. Computed each time from source.");
+  
+  redeclareOption(ol, "inputsize", &MeanMedianModeImputationVMatrix::inputsize_,
+		  OptionBase::nosave,
+		  "Taken from source in  MeanMedianModeImputationVMatrix.");
+  
+  redeclareOption(ol, "targetsize",
+		  &MeanMedianModeImputationVMatrix::targetsize_,
+		  OptionBase::nosave,
+		  "Taken from source in MeanMedianModeImputationVMatrix.");
+  
+  redeclareOption(ol, "weightsize",
+		  &MeanMedianModeImputationVMatrix::weightsize_,
+		  OptionBase::nosave,
+		  "Taken from source in MeanMedianModeImputationVMatrix.");
+  
+  redeclareOption(ol, "extrasize", &MeanMedianModeImputationVMatrix::extrasize_,
+		  OptionBase::nosave,
+		  "Taken from source in MeanMedianModeImputationVMatrix.");
+  
+  redeclareOption(ol, "width", &MeanMedianModeImputationVMatrix::width_,
+		  OptionBase::nosave,
+		  "Taken from source in MeanMedianModeImputationVMatrix.");
 }
 
 void MeanMedianModeImputationVMatrix::build()



From lamblin at mail.berlios.de  Tue May 13 00:32:34 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 13 May 2008 00:32:34 +0200
Subject: [Plearn-commits] r8986 - trunk/plearn_learners/online
Message-ID: <200805122232.m4CMWYow013482@sheep.berlios.de>

Author: lamblin
Date: 2008-05-13 00:32:32 +0200 (Tue, 13 May 2008)
New Revision: 8986

Modified:
   trunk/plearn_learners/online/ClassErrorCostModule.cc
   trunk/plearn_learners/online/ClassErrorCostModule.h
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.h
   trunk/plearn_learners/online/CostModule.cc
   trunk/plearn_learners/online/CostModule.h
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
   trunk/plearn_learners/online/CrossEntropyCostModule.h
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
   trunk/plearn_learners/online/ModulesLearner.cc
   trunk/plearn_learners/online/NLLCostModule.cc
   trunk/plearn_learners/online/NLLCostModule.h
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/ProcessInputCostModule.cc
   trunk/plearn_learners/online/ProcessInputCostModule.h
   trunk/plearn_learners/online/SquaredErrorCostModule.cc
   trunk/plearn_learners/online/SquaredErrorCostModule.h
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
- changed the name of method CostModule::name() to CostModule::costNames()
  to avoid clash with OnlineLearningModule::name
- made it reflect the given name of the module (or containing module) if any
- several sub_costs for each sub_cost of a CombiningCostsModule are now allowed
- export setLearningRate to use it from python


Modified: trunk/plearn_learners/online/ClassErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/ClassErrorCostModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -166,9 +166,12 @@
 //////////
 // name //
 //////////
-TVec<string> ClassErrorCostModule::name()
+TVec<string> ClassErrorCostModule::costNames()
 {
-    return TVec<string>(1, "class_error");
+    if (name == "" || name == classname())
+        return TVec<string>(1, "class_error");
+    else
+        return TVec<string>(1, name + ".class_error");
 }
 
 //////////////////////

Modified: trunk/plearn_learners/online/ClassErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.h	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/ClassErrorCostModule.h	2008-05-12 22:32:32 UTC (rev 8986)
@@ -113,7 +113,7 @@
     virtual bool bpropDoesNothing();
 
     //! Indicates the name of the computed costs
-    virtual TVec<string> name();
+    virtual TVec<string> costNames();
 
 
     //#####  PLearn::Object Protocol  #########################################

Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -95,10 +95,9 @@
 ////////////
 void CombiningCostsModule::build_()
 {
-    
     n_sub_costs = sub_costs.length();
     if( n_sub_costs == 0 )
-    {   
+    {
         //PLWARNING("In CombiningCostsModule::build_ - sub_costs is empty (length 0)");
         return;
     }
@@ -118,7 +117,7 @@
 
     if(sub_costs.length() == 0)
         PLERROR( "CombiningCostsModule::build_(): sub_costs.length()\n"
-                 "should be > 0.\n");                 
+                 "should be > 0.\n");
 
     input_size = sub_costs[0]->input_size;
     target_size = sub_costs[0]->target_size;
@@ -138,11 +137,13 @@
     }
 
     sub_costs_values.resize( n_sub_costs );
-    output_size = n_sub_costs+1;
+    output_size = 1;
+    for (int i=0; i<n_sub_costs; i++)
+        output_size += sub_costs[i]->output_size;
 
     // If we have a random_gen and some sub_costs do not, share it with them
     if( random_gen )
-        for( int i=0; i<sub_costs.length(); i++ )
+        for( int i=0; i<n_sub_costs; i++ )
         {
             if( !(sub_costs[i]->random_gen) )
             {
@@ -189,11 +190,17 @@
     PLASSERT( target.size() == target_size );
     cost.resize( output_size );
 
+    int cost_index = 1;
     for( int i=0 ; i<n_sub_costs ; i++ )
-        sub_costs[i]->fprop( input, target, sub_costs_values[i] );
+    {
+        Vec sub_costs_val_i_all = cost.subVec(cost_index,
+                                              sub_costs[i]->output_size);
+        sub_costs[i]->fprop(input, target, sub_costs_val_i_all);
+        sub_costs_values[i] = cost[cost_index];
+        cost_index += sub_costs[i]->output_size;
+    }
 
     cost[0] = dot( cost_weights, sub_costs_values );
-    cost.subVec( 1, n_sub_costs ) << sub_costs_values;
 }
 
 void CombiningCostsModule::fprop(const Mat& inputs, const Mat& targets,
@@ -202,22 +209,22 @@
     PLASSERT( inputs.width() == input_size );
     PLASSERT( targets.width() == target_size );
     costs.resize(inputs.length(), output_size);
-
-    Mat final_cost = costs.column(0);
-    final_cost.fill(0);
-    Mat other_costs = costs.subMatColumns(1, n_sub_costs);
     sub_costs_mbatch_values.resize(n_sub_costs, inputs.length());
-    for( int i=0 ; i<n_sub_costs ; i++ ) {
-        Vec sub_costs_i = sub_costs_mbatch_values(i);
-        sub_costs[i]->fprop(inputs, targets, sub_costs_i);
-        Mat first_sub_cost = sub_costs_i.toMat(sub_costs_i.length(), 1);
 
-        // final_cost += weight_i * cost_i
-        multiplyAcc(final_cost, first_sub_cost, cost_weights[i]);
+    int cost_index = 1;
+    for (int i=0; i<n_sub_costs; i++)
+    {
+        Mat sub_costs_val_i_all =
+            costs.subMatColumns(cost_index, sub_costs[i]->output_size);
+        sub_costs[i]->fprop(inputs, targets, sub_costs_val_i_all);
+        sub_costs_mbatch_values(i) << costs.column(cost_index);
+        cost_index += sub_costs[i]->output_size;
+    }
 
-        // Fill the rest of the costs matrix.
-        other_costs.column(i) << first_sub_cost;
-    }
+    // final_cost = \sum weight_i * cost_i
+    Mat final_cost = costs.column(0);
+    Mat m_cost_weights = cost_weights.toMat(n_sub_costs, 1);
+    transposeProduct(final_cost, sub_costs_mbatch_values, m_cost_weights);
 }
 
 ////////////////////
@@ -480,12 +487,16 @@
 }
 
 //! Indicates the name of the computed costs
-TVec<string> CombiningCostsModule::name()
+TVec<string> CombiningCostsModule::costNames()
 {
     TVec<string> names(1, "combined_cost");
     for( int i=0 ; i<n_sub_costs ; i++ )
-        names.append( sub_costs[i]->name() );
+        names.append( sub_costs[i]->costNames() );
 
+    if (name != "" && name != classname())
+        for (int j=0; j<names.length(); j++)
+            names[j] = name + "." + names[j];
+
     return names;
 }
 

Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2008-05-12 22:32:32 UTC (rev 8986)
@@ -124,7 +124,7 @@
     virtual bool bpropDoesNothing();
 
     //! Indicates the name of the computed costs
-    virtual TVec<string> name();
+    virtual TVec<string> costNames();
 
     //#####  PLearn::Object Protocol  #########################################
 
@@ -157,6 +157,10 @@
     //! Stores the output values of the sub_costs
     mutable Vec sub_costs_values;
 
+    //! Stores all the costs of one sub_cost
+    mutable Vec sub_cost_all;
+    mutable Mat sub_cost_all_m;
+
     //! Stores mini-batch outputs values of sub costs.
     mutable Mat sub_costs_mbatch_values;
 

Modified: trunk/plearn_learners/online/CostModule.cc
===================================================================
--- trunk/plearn_learners/online/CostModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/CostModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -392,7 +392,7 @@
 //////////
 // name //
 //////////
-TVec<string> CostModule::name()
+TVec<string> CostModule::costNames()
 {
     return TVec<string>();
 }

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/CostModule.h	2008-05-12 22:32:32 UTC (rev 8986)
@@ -138,7 +138,7 @@
     virtual void forget();
 
     //! Indicates the name of the computed costs
-    virtual TVec<string> name();
+    virtual TVec<string> costNames();
 
     //! Overridden so that the default ports being returned are "prediction",
     //! "target" and "cost".

Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -187,9 +187,12 @@
                 "not implemented for class '%s'", classname().c_str());
 }
 
-TVec<string> CrossEntropyCostModule::name()
+TVec<string> CrossEntropyCostModule::costNames()
 {
-    return TVec<string>(1, "CrossEntropy");
+    if (name == "" || name == classname())
+        return TVec<string>(1, "CrossEntropy");
+    else
+        return TVec<string>(1, name + ".CrossEntropy");
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/CrossEntropyCostModule.h
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.h	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.h	2008-05-12 22:32:32 UTC (rev 8986)
@@ -76,7 +76,7 @@
     virtual void setLearningRate(real dynamic_learning_rate) {}
 
     //! Indicates the name of the computed costs
-    virtual TVec<string> name();
+    virtual TVec<string> costNames();
 
     //#####  PLearn::Object Protocol  #########################################
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -430,7 +430,7 @@
     {
         build_final_cost();
 
-        TVec<string> final_names = final_cost->name();
+        TVec<string> final_names = final_cost->costNames();
         int n_final_costs = final_names.length();
 
         for( int i=0; i<n_final_costs; i++ )
@@ -452,7 +452,7 @@
         for( int i=0; i<n_partial_costs; i++ )
             if( partial_costs[i] )
             {
-                TVec<string> names = partial_costs[i]->name();
+                TVec<string> names = partial_costs[i]->costNames();
                 int n_partial_costs_i = names.length();
                 for( int j=0; j<n_partial_costs_i; j++ )
                     cost_names.append("partial"+tostring(i)+"."+names[j]);

Modified: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -1583,9 +1583,9 @@
 //////////
 // name //
 //////////
-TVec<string> LayerCostModule::name()
+TVec<string> LayerCostModule::costNames()
 {
-    return TVec<string>(1, OnlineLearningModule::name);
+    return TVec<string>(1, name);
 }
 
 /////////////////

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/LayerCostModule.h	2008-05-12 22:32:32 UTC (rev 8986)
@@ -162,7 +162,7 @@
     virtual void setLearningRate(real dynamic_learning_rate) {}
 
     //! Indicates the name of the computed costs
-    virtual TVec<string> name();
+    virtual TVec<string> costNames();
 
     //#####  PLearn::Object Protocol  #########################################
 

Modified: trunk/plearn_learners/online/ModulesLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModulesLearner.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/ModulesLearner.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -295,14 +295,14 @@
 TVec<string> ModulesLearner::getTestCostNames() const
 {
     // Return the names of the costs computed by computeCostsFromOutputs
-    return cost->name();
+    return cost->costNames();
 }
 
 TVec<string> ModulesLearner::getTrainCostNames() const
 {
     // Return the names of the objective costs that the train method computes
     // and for which it updates the VecStatsCollector train_stats
-    return cost->name();
+    return cost->costNames();
 }
 
 

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -320,9 +320,12 @@
     bpropUpdate( input, target, cost, input_gradient, accumulate );
 }
 
-TVec<string> NLLCostModule::name()
+TVec<string> NLLCostModule::costNames()
 {
-    return TVec<string>(1, "NLL");
+    if (name == "" || name == classname())
+        return TVec<string>(1, "NLL");
+    else
+        return TVec<string>(1, name + ".NLL");
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/NLLCostModule.h
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.h	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/NLLCostModule.h	2008-05-12 22:32:32 UTC (rev 8986)
@@ -107,7 +107,7 @@
     virtual void setLearningRate(real dynamic_learning_rate) {}
 
     //! Indicates the name of the computed costs
-    virtual TVec<string> name();
+    virtual TVec<string> costNames();
 
     //#####  PLearn::Object Protocol  #########################################
 

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -378,6 +378,13 @@
                  "by their port name), including those in the gradients argument\n"
                  "and those named in the additional_input_gradiaents argument.\n")));
 
+    declareMethod(
+        rmm, "setLearningRate", &OnlineLearningModule::setLearningRate,
+        (BodyDoc("Allows to change the learning rate or equivalent parameter"),
+         ArgDoc ("dynamic_learning_rate",
+                 "The value we want for the learning rate")
+        ));
+
 }
 
 map<string,Mat> OnlineLearningModule::namedFprop(map<string,Mat>& inputs, TVec<string> wanted_outputs)

Modified: trunk/plearn_learners/online/ProcessInputCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/ProcessInputCostModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -302,10 +302,19 @@
 //////////
 // name //
 //////////
-TVec<string> ProcessInputCostModule::name()
+TVec<string> ProcessInputCostModule::costNames()
 {
-    // ### Usually, the name of the class without the trailing "CostModule"
-    return cost_module->name();
+    if (name == "" || name == classname())
+        return cost_module->costNames();
+    else
+    {
+        int n_costs = cost_module->costNames().length();
+        TVec<string> cost_names(n_costs);
+        for (int i=0; i<n_costs; i++)
+            cost_names[i] = name + "." + cost_module->costNames()[i];
+
+        return cost_names;
+    }
 }
 
 //////////////

Modified: trunk/plearn_learners/online/ProcessInputCostModule.h
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.h	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/ProcessInputCostModule.h	2008-05-12 22:32:32 UTC (rev 8986)
@@ -146,7 +146,7 @@
     virtual void setLearningRate(real dynamic_learning_rate);
 
     //! Indicates the name of the computed costs
-    virtual TVec<string> name();
+    virtual TVec<string> costNames();
 
 
     //#####  PLearn::Object Protocol  #########################################

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -202,9 +202,12 @@
     bpropUpdate( input, target, cost, input_gradient, accumulate );
 }
 
-TVec<string> SquaredErrorCostModule::name()
+TVec<string> SquaredErrorCostModule::costNames()
 {
-    return TVec<string>(1, "mse");
+    if (name == "" || name == classname())
+        return TVec<string>(1, "mse");
+    else
+        return TVec<string>(1, name + ".mse");
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/SquaredErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/SquaredErrorCostModule.h	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/SquaredErrorCostModule.h	2008-05-12 22:32:32 UTC (rev 8986)
@@ -94,7 +94,7 @@
     }
 
     //! Indicates the name of the computed costs
-    virtual TVec<string> name();
+    virtual TVec<string> costNames();
 
     //#####  PLearn::Object Protocol  #########################################
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-05-12 15:20:15 UTC (rev 8985)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-05-12 22:32:32 UTC (rev 8986)
@@ -608,7 +608,7 @@
                 partial_costs_positions[i] = n_layers-1;
             else
                 partial_costs_positions[i] = partial_costs_positions[i-1]
-                    + partial_costs[i-1]->name().length();
+                    + partial_costs[i-1]->costNames().length();
 
             if( !(partial_costs[i]->random_gen) )
             {
@@ -2027,13 +2027,13 @@
     
     for( int i=0 ; i<partial_costs.size() ; i++ )
     {
-        TVec<string> names = partial_costs[i]->name();
+        TVec<string> names = partial_costs[i]->costNames();
         for(int j=0; j<names.length(); j++)
             cost_names.push_back("partial" + tostring(i) + "." + 
                 names[j]);
     }
 
-    cost_names.append( final_cost->name() );
+    cost_names.append( final_cost->costNames() );
 
     return cost_names;
 }
@@ -2049,13 +2049,13 @@
     
     for( int i=0 ; i<partial_costs.size() ; i++ )
     {
-        TVec<string> names = partial_costs[i]->name();
+        TVec<string> names = partial_costs[i]->costNames();
         for(int j=0; j<names.length(); j++)
             cost_names.push_back("partial" + tostring(i) + "." + 
                 names[j]);
     }
 
-    cost_names.append( final_cost->name() );
+    cost_names.append( final_cost->costNames() );
 
     return cost_names;
 }



From nouiz at mail.berlios.de  Tue May 13 15:18:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 13 May 2008 15:18:52 +0200
Subject: [Plearn-commits] r8987 - trunk/plearn/vmat
Message-ID: <200805131318.m4DDIqku032064@sheep.berlios.de>

Author: nouiz
Date: 2008-05-13 15:18:51 +0200 (Tue, 13 May 2008)
New Revision: 8987

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
spec can take the form "fieldname* : instrution". This instruction will be used for all fieldname in the source that match the regex


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-05-12 22:32:32 UTC (rev 8986)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-05-13 13:18:51 UTC (rev 8987)
@@ -249,17 +249,46 @@
     variable_imputation_instruction.resize(train_width);
     variable_imputation_instruction.clear();
     TVec<string> nofields;
+    
+    //We sho
+    TVec<pair<string,string> > save_imputation_spec = imputation_spec;
+    imputation_spec = save_imputation_spec.copy();
+
     for (int spec_col = 0; spec_col < imputation_spec.size(); spec_col++)
     {
         int train_col;
+	string fname = imputation_spec[spec_col].first;
         for (train_col = 0; train_col < train_width; train_col++)
         {
-            if (imputation_spec[spec_col].first == train_field_names[train_col]) break;
+	  if (fname == train_field_names[train_col]) break;
         }
-        if (train_col >= train_width){
-	  nofields.append((imputation_spec[spec_col].first).c_str());
+        if (train_col >= train_width && fname[fname.size()-1]!='*'){
+	  nofields.append(fname.c_str());
 	  continue;
 	}
+	else
+	{
+	  bool expended = false;
+	  fname.resize(fname.size()-1);//remove the last caracter (*)
+	  for(train_col = 0; train_col < train_width; train_col++)
+          {
+	    if(string_begins_with(train_field_names[train_col],fname))
+	    {
+	      pair<string,string> n=make_pair(train_field_names[train_col],
+					      imputation_spec[spec_col].second);
+//                    perr<<"expanding "<<fieldspec[i] << " to " << n <<endl;
+	      
+	      imputation_spec.append(n);
+	      expended = true;
+	    }
+	  }
+	  if(!expended)
+	    PLERROR("In MeanMedianModeImputationVMatrix::build_() - "
+		    "Don't have find any partial match to %s",
+		    imputation_spec[spec_col].first.c_str());
+	  continue;
+	}
+	
         if (imputation_spec[spec_col].second == "mean") variable_imputation_instruction[train_col] = 1;
         else if (imputation_spec[spec_col].second == "median") variable_imputation_instruction[train_col] = 2;
         else if (imputation_spec[spec_col].second == "mode") variable_imputation_instruction[train_col] = 3;
@@ -268,6 +297,8 @@
 	  PLERROR("In MeanMedianModeImputationVMatrix: unsupported imputation instruction: %s : %s",
 		     (imputation_spec[spec_col].first).c_str(), (imputation_spec[spec_col].second).c_str());
     }
+    imputation_spec = save_imputation_spec;
+
     if(nofields.length()>0)
       PLERROR("In MeanMedianModeImputationVMatrix::build_() Their is %d fields in the imputation_spec that are not in train set: %s",nofields.length(),
 	      tostring(nofields).c_str());



From nouiz at mail.berlios.de  Tue May 13 15:45:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 13 May 2008 15:45:02 +0200
Subject: [Plearn-commits] r8988 - trunk/plearn/vmat
Message-ID: <200805131345.m4DDj2eN002872@sheep.berlios.de>

Author: nouiz
Date: 2008-05-13 15:45:02 +0200 (Tue, 13 May 2008)
New Revision: 8988

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
bugfix last commit


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-05-13 13:18:51 UTC (rev 8987)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-05-13 13:45:02 UTC (rev 8988)
@@ -262,11 +262,12 @@
         {
 	  if (fname == train_field_names[train_col]) break;
         }
-        if (train_col >= train_width && fname[fname.size()-1]!='*'){
+	char last_char = fname[fname.size()-1];
+        if (train_col >= train_width && last_char!='*'){
 	  nofields.append(fname.c_str());
 	  continue;
 	}
-	else
+	else if(train_col >= train_width && last_char=='*')
 	{
 	  bool expended = false;
 	  fname.resize(fname.size()-1);//remove the last caracter (*)
@@ -276,7 +277,7 @@
 	    {
 	      pair<string,string> n=make_pair(train_field_names[train_col],
 					      imputation_spec[spec_col].second);
-//                    perr<<"expanding "<<fieldspec[i] << " to " << n <<endl;
+//                    perr<<"expanding "<<train_field_names[train_col] << " to " << n <<endl;
 	      
 	      imputation_spec.append(n);
 	      expended = true;



From nouiz at mail.berlios.de  Tue May 13 18:26:38 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 13 May 2008 18:26:38 +0200
Subject: [Plearn-commits] r8989 - trunk/python_modules/plearn/pyplearn
Message-ID: <200805131626.m4DGQc5Z020794@sheep.berlios.de>

Author: nouiz
Date: 2008-05-13 18:26:37 +0200 (Tue, 13 May 2008)
New Revision: 8989

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
bugfix for the test. Following the new defautl arg MTIME in plearn script, we need to modif also the plargs as default arg have some special treatment.


Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2008-05-13 13:45:02 UTC (rev 8988)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2008-05-13 16:26:37 UTC (rev 8989)
@@ -502,7 +502,7 @@
     buildClassContext = staticmethod(buildClassContext)
 
     def closeClassContext(context):
-        exceptions = [ 'FILEBASE', 'FILEPATH', 'TIME', 'DATETIME',
+        exceptions = [ 'FILEBASE', 'FILEPATH', 'TIME', 'DATETIME', 'MTIME',
                        'FILEEXT', 'DIRPATH', 'DATE', 'HOME', 'FILENAME' ]
 
         del context.plopt_binders



From ducharme at mail.berlios.de  Tue May 13 22:47:25 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 13 May 2008 22:47:25 +0200
Subject: [Plearn-commits] r8990 - trunk/plearn_learners/regressors
Message-ID: <200805132047.m4DKlP4v019729@sheep.berlios.de>

Author: ducharme
Date: 2008-05-13 22:47:24 +0200 (Tue, 13 May 2008)
New Revision: 8990

Modified:
   trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
   trunk/plearn_learners/regressors/BasisSelectionRegressor.h
Log:
Meilleure utilisation potentielle des ressources en multi-threading
en faisaint un preload d'une fraction des donnees.


Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-05-13 16:26:37 UTC (rev 8989)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-05-13 20:47:24 UTC (rev 8990)
@@ -45,7 +45,6 @@
 #include <plearn/math/RealFunctionProduct.h>
 #include <plearn/math/RealValueIndicatorFunction.h>
 #include <plearn/math/RealRangeIndicatorFunction.h>
-// #include <plearn/math/TruncatedRealFunction.h>
 #include <plearn/vmat/MemoryVMatrix.h>
 #include <plearn/math/random.h>
 #include <plearn/vmat/RealFunctionsProcessedVMatrix.h>
@@ -80,6 +79,7 @@
       normalize_features(false),
       precompute_features(true),
       n_threads(0),
+      thread_subtrain_length(0),
       residue_sum(0),
       residue_sum_sq(0),
       weights_sum(0)
@@ -88,21 +88,6 @@
 
 void BasisSelectionRegressor::declareOptions(OptionList& ol)
 {
-    // ### Declare all of this object's options here.
-    // ### For the "flags" of each option, you should typically specify
-    // ### one of OptionBase::buildoption, OptionBase::learntoption or
-    // ### OptionBase::tuningoption. If you don't provide one of these three,
-    // ### this option will be ignored when loading values from a script.
-    // ### You can also combine flags, for example with OptionBase::nosave:
-    // ### (OptionBase::buildoption | OptionBase::nosave)
-
-    // ### ex:
-    // declareOption(ol, "myoption", &BasisSelectionRegressor::myoption,
-    //               OptionBase::buildoption,
-    //               "Help text describing this option");
-    // ...
-
-
     //#####  Public Build Options  ############################################
 
     declareOption(ol, "consider_constant_function", &BasisSelectionRegressor::consider_constant_function,
@@ -211,6 +196,10 @@
                   "The number of threads to use when computing residue scores.\n"
                   "NOTE: MOST OF PLEARN IS NOT THREAD-SAFE; THIS CODE ASSUMES THAT SOME PARTS ARE, BUT THESE MAY CHANGE.");
 
+    declareOption(ol, "thread_subtrain_length", &BasisSelectionRegressor::thread_subtrain_length,
+                  OptionBase::buildoption,
+                  "Preload thread_subtrain_length data when using multi-threading.");
+
     //#####  Public Learnt Options  ############################################
 
     declareOption(ol, "selected_functions", &BasisSelectionRegressor::selected_functions,
@@ -239,21 +228,9 @@
 
 void BasisSelectionRegressor::build_()
 {
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation.
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a "reloaded" object: i.e. from the complete set of
-    // ###    all serialised options.
-    // ###  - Updating or "re-building" of an object after a few "tuning"
-    // ###    options have been modified.
-    // ### You should assume that the parent class' build_() has already been
-    // ### called.
 }
 
 
-
-
 void BasisSelectionRegressor::setExperimentDirectory(const PPath& the_expdir)
 { 
     inherited::setExperimentDirectory(the_expdir);
@@ -271,15 +248,6 @@
 
 void BasisSelectionRegressor::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    //PLERROR("BasisSelectionRegressor::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
-
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(explicit_functions, copies);
@@ -302,7 +270,6 @@
     deepCopyField(input, copies);
     deepCopyField(targ, copies);
     deepCopyField(featurevec, copies);
-
 }
 
 
@@ -521,12 +488,14 @@
     {
         int nselected = selected_functions.length();
         for(int k=0; k<nselected; k++)
+        {
             for(int j=candidate_start; j<ncandidates; j++)
             {
                 RealFunc f = new RealFunctionProduct(selected_functions[k],simple_candidate_functions[j]);
                 f->setInfo("("+selected_functions[k]->getInfo()+"*"+simple_candidate_functions[j]->getInfo()+")");
                 interaction_candidate_functions.append(f);
             }
+        }
     }
 
     // explicit interaction variables / functions
@@ -536,12 +505,14 @@
                                               explicit_interaction_functions);
 
     for(int k= 0; k < explicit_interaction_functions.length(); ++k)
+    {
         for(int j= candidate_start; j < ncandidates; ++j)
         {
             RealFunc f = new RealFunctionProduct(explicit_interaction_functions[k],simple_candidate_functions[j]);
             f->setInfo("("+explicit_interaction_functions[k]->getInfo()+"*"+simple_candidate_functions[j]->getInfo()+")");
             interaction_candidate_functions.append(f);
         }
+    }
 
     
     if(max_interaction_terms < 0)
@@ -568,8 +539,7 @@
     real E_y = 0;
     real E_yy = 0;
 
-    computeWeightedAveragesWithResidue(candidate_functions,   
-                                       wsum, E_x, E_xx, E_y, E_yy, E_xy);
+    computeWeightedAveragesWithResidue(candidate_functions, wsum, E_x, E_xx, E_y, E_yy, E_xy);
     
     Vec scores = (E_xy-E_y*E_x)/sqrt(E_xx-square(E_x));
 
@@ -601,34 +571,13 @@
 
     if(verbosity>=10)
         perr << endl;
-
-    /*
-    computeWeightedCorrelationsWithY(candidate_functions, residue,  
-                                     wsum,
-                                     E_x, V_x,
-                                     E_y, V_y,
-                                     E_xy, V_xy,
-                                     covar, correl);
-
-    for(int j=0; j<n_candidates; j++)
-    {
-        real abs_correl = fabs(correl[j]);
-        if(abs_correl>best_score)
-        {
-            best_candidate_index = j;
-            best_score = abs_correl;
-        }
-    }
-    */
 }
 
 
-
-
 //function-object for a thread
 struct BasisSelectionRegressor::thread_wawr
 {
-    int tid, nt;
+    int thread_id, n_threads;
     const TVec<RealFunc>& functions;
     real& wsum;
     Vec& E_x;
@@ -641,8 +590,9 @@
     const VMat& train_set;  
     boost::mutex& pb_mx;
     PP<ProgressBar> pb;
+    int thread_subtrain_length;
 
-    thread_wawr(int tid_, int nt_,
+    thread_wawr(int thread_id_, int n_threads_,
                 const TVec<RealFunc>& functions_,  
                 real& wsum_,
                 Vec& E_x_, Vec& E_xx_,
@@ -650,9 +600,10 @@
                 Vec& E_xy_, const Vec& Y_, boost::mutex& ts_mx_,
                 const VMat& train_set_,
                 boost::mutex& pb_mx_,
-                PP<ProgressBar> pb_)
-        : tid(tid_), 
-          nt(nt_),
+                PP<ProgressBar> pb_,
+                int thread_subtrain_length_)
+        : thread_id(thread_id_), 
+          n_threads(n_threads_),
           functions(functions_),
           wsum(wsum_),
           E_x(E_x_),
@@ -664,7 +615,8 @@
           ts_mx(ts_mx_),
           train_set(train_set_),
           pb_mx(pb_mx_),
-          pb(pb_)
+          pb(pb_),
+          thread_subtrain_length(thread_subtrain_length_)
     {}
 
     void operator()()
@@ -673,7 +625,7 @@
         real w;
         Vec candidate_features;
         int n_candidates = functions.length();
-        int l= train_set->length();
+        int train_len = train_set->length();
      
         E_x.resize(n_candidates);
         E_x.fill(0.);
@@ -684,16 +636,49 @@
         E_xy.resize(n_candidates);
         E_xy.fill(0.);
         wsum = 0.;
+
+        // Used when thread_subtrain_length > 1
+        Mat all_inputs;
+        Vec all_w;
+        int input_size = train_set->inputsize();
+        if (thread_subtrain_length > 1)
+        {
+            // pre-allocate memory
+            all_inputs.resize(thread_subtrain_length, input_size);
+            all_w.resize(thread_subtrain_length);
+        }
    
-        for(int i=tid; i<l; i+= nt)
+        for(int i=thread_id; i<train_len; i+= n_threads)
         {
-            real y = Y[i];
+            if (thread_subtrain_length > 1)
             {
+                int j = (i-thread_id)/n_threads;
+                int j_mod = j % thread_subtrain_length;
+                if (j_mod == 0)  // on doit faire le plein de donnees
+                {
+                    all_inputs.resize(0, input_size);
+                    all_w.resize(0);
+
+                    boost::mutex::scoped_lock lock(ts_mx);
+                    int max_train = min(train_len, i + thread_subtrain_length*n_threads);
+                    for (int ii=i; ii<max_train; ii+= n_threads)
+                    {
+                        train_set->getExample(ii, input, targ, w);
+                        all_inputs.appendRow(input);
+                        all_w.append(w);
+                    }
+                }
+                input = all_inputs(j_mod);
+                w = all_w[j_mod];
+            }
+            else
+            {
                 boost::mutex::scoped_lock lock(ts_mx);
                 train_set->getExample(i, input, targ, w);
             }
             evaluate_functions(functions, input, candidate_features);
             wsum += w;
+            real y = Y[i];
             real wy = w*y;
             E_y  += wy;
             E_yy  += wy*y;
@@ -716,14 +701,12 @@
 };
 
 
-
 void BasisSelectionRegressor::computeWeightedAveragesWithResidue(const TVec<RealFunc>& functions,  
                                                                  real& wsum,
                                                                  Vec& E_x, Vec& E_xx,
                                                                  real& E_y, real& E_yy,
                                                                  Vec& E_xy) const
 {
-
     const Vec& Y = residue;
     int n_candidates = functions.length();
     E_x.resize(n_candidates);
@@ -763,7 +746,7 @@
                                     E_xs[i], E_xxs[i],
                                     E_ys[i], E_yys[i],
                                     E_xys[i], Y, ts_mx, train_set,
-                                    pb_mx, pb);
+                                    pb_mx, pb, thread_subtrain_length);
             threads[i]= new boost::thread(*tws[i]);
         }
         for(int i= 0; i < n_threads; ++i)
@@ -783,6 +766,7 @@
         }
     }
     else // single-thread version
+    {
         for(int i=0; i<l; i++)
         {
             real y = Y[i];
@@ -803,6 +787,7 @@
             if(pb)
                 pb->update(i);
         }
+    }
 
     // Finalize computation
     real inv_wsum = 1.0/wsum;
@@ -972,11 +957,13 @@
     {
         features.resize(l,nf+(weighted?2:1), max(1,int(0.25*l*nf)), true); // enlarge width while preserving content
         if(weighted)
+        {
             for(int i=0; i<l; i++) // append target and weight columns to features matrix
             {
                 features(i,nf) = targets[i];
                 features(i,nf+1) = weights[i];
             }
+        }
         else // no weights
             features.lastColumn() << targets; // append target column to features matrix
     
@@ -1057,12 +1044,6 @@
             if(verbosity>=2)
                 perr << "\n\n*** Stage " << stage << " : no more candidate functions. *****" << endl;
         }
-        // clear statistics of previous epoch
-        // train_stats->forget();
-        // Vec train_costs(1);
-        // train_costs[0] = residue_sum_sq/weights_sum;
-        // train_stats->update(train_costs, weights_sum);
-        // train_stats->finalize(); // finalize statistics for this epoch
         ++stage;
     }
 }
@@ -1089,22 +1070,6 @@
         weights[i] = w;
         weights_sum += w;
     }
-
-    /*
-    bias = residue_sum/weights_sum;
-
-    // Now sutract the bias
-    residue_sum = 0.;
-    residue_sum_sq = 0.;    
-    for(int i=0; i<l; i++)
-    {
-        real w = weights[i];
-        real resval = residue[i]-bias;
-        residue[i] = resval;
-        residue_sum += w*resval;
-        residue_sum_sq += w*square(resval);        
-    }
-    */
 }
 
 void BasisSelectionRegressor::recomputeFeatures()
@@ -1133,7 +1098,6 @@
     // perr << "recomp_residue: { ";
     for(int i=0; i<l; i++)
     {
-        // train_set->getExample(i, input, targ, w);
         real t = targets[i];
         real w = weights[i];
         if(precompute_features)
@@ -1193,7 +1157,6 @@
 
 TVec<string> BasisSelectionRegressor::getTestCostNames() const
 {
-    //return getTrainCostNames();
     return TVec<string>(1,string("mse"));
 }
 
@@ -1206,9 +1169,6 @@
 
 TVec<string> BasisSelectionRegressor::getTrainCostNames() const
 {
-//     TVec<string> costnames(1);
-//     costnames[0] = "mse";
-//     return costnames;
     return learner->getTrainCostNames();
 }
 

Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2008-05-13 16:26:37 UTC (rev 8989)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2008-05-13 20:47:24 UTC (rev 8990)
@@ -84,6 +84,7 @@
     PP<PLearner> learner;
     bool precompute_features;
     int n_threads;
+    int thread_subtrain_length;
 
     //#####  Public Learnt Options  ############################################
     TVec<RealFunc> selected_functions;
@@ -106,13 +107,11 @@
 
     //! Returns the size of this learner's output, (which typically
     //! may depend on its inputsize(), targetsize() and set options).
-    // (PLEASE IMPLEMENT IN .cc)
     virtual int outputsize() const;
 
     //! (Re-)initializes the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!).
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void forget();
 
     //! The role of the train method is to bring the learner up to
@@ -121,17 +120,14 @@
     virtual void train();
 
     //! Computes the output from the input.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
     //! Computes the costs from already computed output.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
                                          const Vec& target, Vec& costs) const;
 
     //! Returns the names of the costs computed by computeCostsFromOutpus (and
     //! thus the test method).
-    // (PLEASE IMPLEMENT IN .cc)
     virtual TVec<std::string> getTestCostNames() const;
 
 
@@ -140,7 +136,6 @@
 
     //! Returns the names of the objective costs that the train method computes
     //! and  for which it updates the VecStatsCollector train_stats.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual TVec<std::string> getTrainCostNames() const;
 
 
@@ -171,18 +166,11 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
     virtual void setExperimentDirectory(const PPath& the_expdir);
 
 protected:
-    //#####  Protected Options  ###############################################
-
-    // ### Declare protected option fields (such as learned parameters) here
-    // ...
-
-protected:
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.



From dumitruerhan at mail.berlios.de  Wed May 14 00:17:36 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Wed, 14 May 2008 00:17:36 +0200
Subject: [Plearn-commits] r8991 - trunk/plearn/ker
Message-ID: <200805132217.m4DMHav4003452@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-14 00:17:36 +0200 (Wed, 14 May 2008)
New Revision: 8991

Modified:
   trunk/plearn/ker/GaussianDensityKernel.cc
Log:
compared to 2003, log now takes two arguments :)

Modified: trunk/plearn/ker/GaussianDensityKernel.cc
===================================================================
--- trunk/plearn/ker/GaussianDensityKernel.cc	2008-05-13 20:47:24 UTC (rev 8990)
+++ trunk/plearn/ker/GaussianDensityKernel.cc	2008-05-13 22:17:36 UTC (rev 8991)
@@ -51,7 +51,7 @@
                         "");
 
 real GaussianDensityKernel::evaluate(const Vec& x1, const Vec& x2) const
-{ return exp(-real(0.5)*powdistance(x1, x2, real(2.0))/(sigma*sigma) - x1.length()*(0.5*Log2Pi + log(sigma))); }
+{ return exp(-real(0.5)*powdistance(x1, x2, real(2.0))/(sigma*sigma) - x1.length()*(0.5*Log2Pi + log(exp(1.0),sigma))); }
 
 void GaussianDensityKernel::declareOptions(OptionList& ol)
 {



From dumitruerhan at mail.berlios.de  Wed May 14 00:20:52 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Wed, 14 May 2008 00:20:52 +0200
Subject: [Plearn-commits] r8992 - trunk/commands
Message-ID: <200805132220.m4DMKqax003760@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-14 00:20:49 +0200 (Wed, 14 May 2008)
New Revision: 8992

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
included GaussianDensityKernel.h (to be used in conjuction with KernelDensityEstimator for doing Parzen Windows)

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-05-13 22:17:36 UTC (rev 8991)
+++ trunk/commands/plearn_noblas_inc.h	2008-05-13 22:20:49 UTC (rev 8992)
@@ -111,6 +111,7 @@
 #include <plearn/ker/DotProductKernel.h>
 #include <plearn/ker/EpanechnikovKernel.h>
 #include <plearn/ker/GaussianKernel.h>
+#include <plearn/ker/GaussianDensityKernel.h>
 #include <plearn/ker/GeodesicDistanceKernel.h>
 #include <plearn/ker/IIDNoiseKernel.h>
 #include <plearn/ker/LinearARDKernel.h>



From dumitruerhan at mail.berlios.de  Wed May 14 05:14:10 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Wed, 14 May 2008 05:14:10 +0200
Subject: [Plearn-commits] r8993 - trunk/plearn/ker
Message-ID: <200805140314.m4E3EAAr012196@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-14 05:14:10 +0200 (Wed, 14 May 2008)
New Revision: 8993

Modified:
   trunk/plearn/ker/BetaKernel.cc
Log:
small improvements, no bugfix

Modified: trunk/plearn/ker/BetaKernel.cc
===================================================================
--- trunk/plearn/ker/BetaKernel.cc	2008-05-13 22:20:49 UTC (rev 8992)
+++ trunk/plearn/ker/BetaKernel.cc	2008-05-14 03:14:10 UTC (rev 8993)
@@ -49,7 +49,7 @@
     "Useful for performing Parzen Windows-style density estimation. Need to specify the type of the kernel\n"
      "- simple: the basic Beta kernel \n"
      "- alternative: the alternative, faster converging version \n"
-     "output_type should be set to either log_density or density"  );
+     "output_type should be set to either log_value or normal"  );
 
 //////////////////
 // BetaKernel //
@@ -57,7 +57,7 @@
 BetaKernel::BetaKernel()
     : width(1.),
      kernel_type("simple"),
-     output_type("log_density")
+     output_type("normal")
 {
 }
 
@@ -77,7 +77,7 @@
     
     declareOption(ol, "output_type", &BetaKernel::output_type,
                   OptionBase::buildoption,
-                  "A string specifying whether we want log densities as outputs (\"log_density\"; default) or just densities (\"density\")");
+                  "A string specifying whether we want log densities as outputs (\"log_value\" ) or just densities (\"normal\"; default)");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -158,14 +158,14 @@
     else
         PLERROR("In BetaKernel::evaluate kernel_type must be either \"simple\" or \"alternative\"");
    
-    real retval;
+    real retval = 0.0;
  
-    if (output_type=="log_density")
+    if (output_type=="log_value")
         retval = kvalue;
-    else if (output_type=="density")
+    else if (output_type=="normal")
         retval = exp(kvalue);
     else
-        PLERROR("In BetaKernel::evaluate output_type must be either \"log_density\" or \"density\"");
+        PLERROR("In BetaKernel::evaluate output_type must be either \"log_value\" or \"normal\"");
 
     return retval;
 }



From dumitruerhan at mail.berlios.de  Wed May 14 05:14:58 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Wed, 14 May 2008 05:14:58 +0200
Subject: [Plearn-commits] r8994 - trunk/plearn_learners/distributions
Message-ID: <200805140314.m4E3Ewwj012275@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-14 05:14:58 +0200 (Wed, 14 May 2008)
New Revision: 8994

Modified:
   trunk/plearn_learners/distributions/KernelDensityEstimator.h
Log:
added option kernel_output_type option

Modified: trunk/plearn_learners/distributions/KernelDensityEstimator.h
===================================================================
--- trunk/plearn_learners/distributions/KernelDensityEstimator.h	2008-05-14 03:14:10 UTC (rev 8993)
+++ trunk/plearn_learners/distributions/KernelDensityEstimator.h	2008-05-14 03:14:58 UTC (rev 8994)
@@ -77,6 +77,9 @@
     //! Kernel type
     PP<Kernel> kernel;
 
+    //! Kernel output type
+    string kernel_output_type;
+
     //#####  UnconditionalDistribution Member Functions  ######################
 
     //! Return log of probability density log(p(y)).



From dumitruerhan at mail.berlios.de  Wed May 14 05:15:37 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Wed, 14 May 2008 05:15:37 +0200
Subject: [Plearn-commits] r8995 - trunk/plearn_learners/distributions
Message-ID: <200805140315.m4E3Fbuq012349@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-14 05:15:36 +0200 (Wed, 14 May 2008)
New Revision: 8995

Modified:
   trunk/plearn_learners/distributions/KernelDensityEstimator.cc
Log:
added option kernel_output_type and handling thereof

Modified: trunk/plearn_learners/distributions/KernelDensityEstimator.cc
===================================================================
--- trunk/plearn_learners/distributions/KernelDensityEstimator.cc	2008-05-14 03:14:58 UTC (rev 8994)
+++ trunk/plearn_learners/distributions/KernelDensityEstimator.cc	2008-05-14 03:15:36 UTC (rev 8995)
@@ -51,7 +51,8 @@
 //////////////////
 // KernelDensityEstimator //
 //////////////////
-KernelDensityEstimator::KernelDensityEstimator()
+KernelDensityEstimator::KernelDensityEstimator() 
+    : kernel_output_type("normal")
 {
 }
 
@@ -64,6 +65,10 @@
                    OptionBase::buildoption,
                    "The kernel used at each point in the training set");
 
+    declareOption(ol, "kernel_output_type", &KernelDensityEstimator::kernel_output_type,
+                    OptionBase::buildoption,
+                   "Specifies whether the output of our kernel is logarithmic in the distance (\"log\") or normal (anything else)");
+    
     // Now call the parent class' declareOptions().
     inherited::declareOptions(ol);
 }
@@ -131,16 +136,33 @@
     int numTrain = train_set.length();
     Vec input, target;
     real weight;
-    real logprob = -INFINITY;
+    real result = 0.0;
 
-    for(int i=0; i<numTrain; i++) {
-        train_set->getExample(i,input,target,weight);
-        logprob = logadd(logprob,kernel->evaluate(input,y)); 
+    // It can happen that for efficiency/numerical reasons, your kernel outputs
+    // the logarithm of the actual value of the kernel at (input_i,y).
+    if (kernel_output_type=="log") {
+        real logprob = -INFINITY;
+        for(int i=0; i<numTrain; i++) {
+            train_set->getExample(i,input,target,weight);
+            logprob = logadd(logprob,kernel->evaluate(input,y));
+        }
+        logprob -= pl_log(real(numTrain));
+        result = logprob;
     }
-    
-    logprob -= pl_log(real(numTrain));
+    // Otherwise, it's just a log(\sum_i{k(input_i,y)} / numTrain)
+    else if (kernel_output_type=="normal") {
+        real sprob = 0.0;
+        for(int i=0; i<numTrain; i++) {
+            train_set->getExample(i,input,target,weight);
+            sprob += kernel->evaluate(input,y);
+        }
+        sprob /= real(numTrain);
+        result = pl_log(sprob);
+    }
+    else
+        PLERROR("In KernelDensityEstimator::log_density kernel_output_type must be either \"log\" or \"normal\"");
 
-    return logprob;
+    return result;
 
 }
 



From larocheh at mail.berlios.de  Wed May 14 15:12:52 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 14 May 2008 15:12:52 +0200
Subject: [Plearn-commits] r8996 - trunk/plearn_learners_experimental
Message-ID: <200805141312.m4EDCqd3023504@sheep.berlios.de>

Author: larocheh
Date: 2008-05-14 15:12:51 +0200 (Wed, 14 May 2008)
New Revision: 8996

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
Log:
Unimportant stuff...



Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-14 03:15:36 UTC (rev 8995)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-14 13:12:51 UTC (rev 8996)
@@ -598,9 +598,18 @@
                             hidden_activation_pos_i);
                         num_neg_act -= hidden_layer->freeEnergyContribution(
                             hidden_activation_neg_i);
-                        num_pos = safeexp(num_pos_act);
-                        num_neg = safeexp(num_neg_act);
-                        input_probs_i = num_pos / (num_pos + num_neg);
+                        //num_pos = safeexp(num_pos_act);
+                        //num_neg = safeexp(num_neg_act);
+                        //input_probs_i = num_pos / (num_pos + num_neg);
+                        if( input_layer->use_fast_approximations )
+                            input_probs_i = fastsigmoid(
+                                num_pos_act - num_neg_act);
+                        else
+                        {
+                            num_pos = safeexp(num_pos_act);
+                            num_neg = safeexp(num_neg_act);
+                            input_probs_i = num_pos / (num_pos + num_neg);
+                        }
 
                         // Compute input_prob gradient
                         if( input_layer->use_fast_approximations )
@@ -1140,14 +1149,14 @@
     //    input_layer->getConfiguration(i,conf);
     //    computeOutput(conf,output);
     //    computeCostsFromOutputs( conf, output, target, costs );
-    //    //if( i==0 )
-    //    //    sums = -costs[nll_cost_index];
-    //    //else
-    //    //    sums = logadd( sums, -costs[nll_cost_index] );
-    //    sums += safeexp( -costs[nll_cost_index] );
+    //    if( i==0 )
+    //        sums = -costs[nll_cost_index];
+    //    else
+    //        sums = logadd( sums, -costs[nll_cost_index] );
+    //    //sums += safeexp( -costs[nll_cost_index] );
     //}        
-    //cout << "sums: " << //safeexp(sums) << endl;
-    //    sums << endl;
+    //cout << "sums: " << safeexp(sums) << endl;
+    //    //sums << endl;
     train_stats->finalize();
 }
 



From laulysta at mail.berlios.de  Wed May 14 20:18:22 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 14 May 2008 20:18:22 +0200
Subject: [Plearn-commits] r8997 - trunk/plearn_learners_experimental
Message-ID: <200805141818.m4EIIMYX011221@sheep.berlios.de>

Author: laulysta
Date: 2008-05-14 20:18:21 +0200 (Wed, 14 May 2008)
New Revision: 8997

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
pour HUGO


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-14 13:12:51 UTC (rev 8996)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-14 18:18:21 UTC (rev 8997)
@@ -43,6 +43,23 @@
 #include "DynamicallyLinkedRBMsModel.h"
 #include "plearn/math/plapack.h"
 
+// Options to have:
+//
+// - input_layer
+// - input_connection
+// - target_layers
+// - target_connections
+// - target_layers_weights
+// - mask_size
+// - end_of_sequence_symbol
+// - input reconstruction weight
+//
+// Problems to have in mind:
+// 
+// - have a proper normalization of costs
+// - output one cost per target + weighted sum of all costs
+// - make sure gradient descent is proper (change some vectors into matrices, etc.)
+
 namespace PLearn {
 using namespace std;
 
@@ -59,6 +76,8 @@
     fine_tuning_learning_rate( 0.01 ),
     recurrent_net_learning_rate( 0.01),
     untie_weights( false ),
+    taillePart( 0 ),
+    isRegression( 0 ),
     rbm_nstages( 0 ),
     dynamic_nstages( 0 ),
     fine_tuning_nstages( 0 ),
@@ -99,6 +118,14 @@
                   OptionBase::buildoption,
                   "Indication to untie weights in recurrent net");
 
+    declareOption(ol, "taillePart", &DynamicallyLinkedRBMsModel::taillePart,
+                  OptionBase::buildoption,
+                  "Indication the size of the partition");
+
+    declareOption(ol, "isRegression", &DynamicallyLinkedRBMsModel::isRegression,
+                  OptionBase::buildoption,
+                  "Indication if the model is used for regression");
+
     declareOption(ol, "rbm_nstages", &DynamicallyLinkedRBMsModel::rbm_nstages,
                   OptionBase::buildoption,
                   "Number of epochs for rbm phase");
@@ -115,10 +142,14 @@
                   OptionBase::buildoption,
                   "Number of epochs for the recurrent phase");
 
-    declareOption(ol, "visible_layer", &DynamicallyLinkedRBMsModel::visible_layer,
+    declareOption(ol, "input_layer", &DynamicallyLinkedRBMsModel::input_layer,
                   OptionBase::buildoption,
-                  "The visible layer of the RBMs");
+                  "The input layer of the model");
 
+    declareOption(ol, "target_layer", &DynamicallyLinkedRBMsModel::target_layer,
+                  OptionBase::buildoption,
+                  "The target layer of the model");
+
     declareOption(ol, "hidden_layer", &DynamicallyLinkedRBMsModel::hidden_layer,
                   OptionBase::buildoption,
                   "The hidden layer of the RBMs. Its size must be set, and will\n"
@@ -175,10 +206,10 @@
 
     if(train_set)
     {
-        visible_size = 0;
-        symbol_sizes.resize(0);
+        input_size = 0;
+        input_symbol_sizes.resize(0);
         PP<Dictionary> dict;        
-        for(int i=0; i<train_set->inputsize(); i++)
+        for(int i=0; i<input_layer->size; i++)
         {
             dict = train_set->getDictionary(i);
             if(dict)
@@ -186,41 +217,61 @@
                 if( dict->size() == 0 )
                     PLERROR("DynamicallyLinkedRBMsModel::build_(): dictionary "
                         "of field %d is empty", i);
-                symbol_sizes.push_back(dict->size());
+                input_symbol_sizes.push_back(dict->size());
                 // Adjust size to include one-hot vector
-                visible_size += dict->size();
+                input_size += dict->size();
             }
             else
             {
-                symbol_sizes.push_back(-1);
-                visible_size++;
+                input_symbol_sizes.push_back(-1);
+                input_size++;
             }
         }
 
+        //target_size.fill(-1);
+        //target_symbol_sizes.resize(0);
+        target_size = 0;
+        for( int tar=0; tar<target_layers.length(); tar++){
+            target_layers_size[tar] = 0;
+            target_symbol_sizes.resize(0,0);
+            for(int i=0; i<target_layer[tar]->size; i++)
+            {
+                dict = train_set->getDictionary(i);
+                if(dict)
+                {
+                    if( dict->size() == 0 )
+                        PLERROR("DynamicallyLinkedRBMsModel::build_(): dictionary "
+                                "of field %d is empty", i);
+                    target_symbol_sizes.resize(tar+1,i+1);
+                    target_symbol_sizes(tar,i) = dict->size();
+                    // Adjust size to include one-hot vector
+                    target_layers_size[tar] += dict->size();
+                    target_size += dict->size();
+                }
+                else
+                {
+
+                    target_symbol_sizes.resize(tar+1,i+1);
+                    target_symbol_sizes(tar,i) = -1;
+                    target_layers_size[tar]++;
+                    target_size++;
+                }
+            }
+        }
         // Set and verify sizes
         if(hidden_layer->size <= 0)
             PLERROR("DynamicallyLinkedRBMsModel::build_(): hidden_layer->size "
                 "must be > 0");
 
-        //cond_bias.resize(hidden_layer->size);
-
-        pos_down_values.resize(visible_size);
-        pos_up_values.resize(hidden_layer->size);
-        hidden_layer_target.resize(hidden_layer->size);
-        hidden_layer_sample.resize(hidden_layer->size);
-        visible_layer_sample.resize(visible_size);
-        previous_hidden_layer.resize(hidden_layer->size);
-        previous_hidden_layer_activation.resize(hidden_layer->size);
+        
         previous_visible_layer.resize(visible_size);
 
         visi_bias_gradient.resize(visible_size);
-        hidden_temporal_gradient.resize(hidden_layer->size);
 
         visible_layer->size = visible_size;
 
         connections->down_size = visible_size;
         connections->up_size = hidden_layer->size;
-        connections_idem = connections;
 
         dynamic_connections->input_size = hidden_layer->size;
         dynamic_connections->output_size = hidden_layer->size;
@@ -244,7 +295,19 @@
         connections_transpose = new RBMMatrixTransposeConnection(connections);
         connections_idem_t = connections_transpose;
     }
+    if(hidden_layer->size>0){
+        previous_hidden_layer.resize(hidden_layer->size);
 
+        pos_up_values.resize(hidden_layer->size);
+        hidden_layer_target.resize(hidden_layer->size);
+        hidden_layer_sample.resize(hidden_layer->size);
+        previous_hidden_layer.resize(hidden_layer->size);
+        previous_hidden_layer_activation.resize(hidden_layer->size);
+        hidden_temporal_gradient.resize(hidden_layer->size);
+    }
+    if(connections)
+        connections_idem = connections;
+
 }
 
 // ### Nothing to add here, simply calls build_
@@ -280,6 +343,7 @@
     deepCopyField( hidden_gradient2 , copies);
     deepCopyField( hidden_temporal_gradient , copies);
     deepCopyField( previous_input , copies);
+    deepCopyField( previous_target , copies);
     deepCopyField( previous_hidden_layer , copies);
     deepCopyField( previous_hidden_layer_activation , copies);
     deepCopyField( previous_visible_layer , copies);
@@ -297,6 +361,7 @@
     deepCopyField( input_prediction_list , copies);
     deepCopyField( input_prediction_activations_list , copies);
     deepCopyField( input_list , copies);
+    deepCopyField( target_list , copies);
     deepCopyField( nll_list , copies);
     deepCopyField( input_expectation , copies);
 
@@ -403,230 +468,26 @@
     /***** RBM training phase *****/
     if(stage < rbm_nstages)
     {
-        MODULE_LOG << "Training connection weights between RBMs" << endl;
-
-        int init_stage = stage;
-        int end_stage = min( rbm_nstages, nstages );
-
-        MODULE_LOG << "  stage = " << stage << endl;
-        MODULE_LOG << "  end_stage = " << end_stage << endl;
-        MODULE_LOG << "  rbm_learning_rate = " << rbm_learning_rate << endl;
-
-        if( report_progress && stage < end_stage )
-            pb = new ProgressBar( "RBM training phase of "+classname(),
-                                  end_stage - init_stage );
-
-        visible_layer->setLearningRate( rbm_learning_rate );
-        connections->setLearningRate( rbm_learning_rate );
-        hidden_layer->setLearningRate( rbm_learning_rate );
-        real mean_cost = 0;
-        while(stage < end_stage)
-        {
-            for(int sample=0 ; sample<train_set->length() ; sample++ )
-            {
-                train_set->getExample(sample, input, target, weight);
-
-               
-                if(train_set->getString(sample,0) == "<oov>")
-                    continue;
-
-                clamp_visible_units(input);
-
-                mean_cost += rbm_update();
-
-            }
-
-            if( pb )
-                pb->update( stage + 1 - init_stage);
-            if(verbosity>0)
-                cout << "mean cost at stage " << stage << 
-                    " = " << mean_cost/train_set->length() << endl;
-            mean_cost = 0;
-            stage++;
-        }    
-        if( pb )
-        {
-            delete pb;
-            pb = 0;
-        }
-
-        //cout << "RBM training phase" << endl;
     }
 
 
-
-
     /***** dynamic phase training  *****/
 
     if(stage < rbm_nstages +  dynamic_nstages)
     {
-        MODULE_LOG << "Training dynamic connections between RBMs' hidden layers" << endl;
+    }  
 
-        int init_stage = stage;
-        int end_stage = min( rbm_nstages + dynamic_nstages, nstages );
 
-        MODULE_LOG << "  stage = " << stage << endl;
-        MODULE_LOG << "  end_stage = " << end_stage << endl;
-        MODULE_LOG << "  dynamic_learning_rate = " << dynamic_learning_rate << endl;
-        MODULE_LOG << "  visible_dynamic_learning_rate = " << visible_dynamic_learning_rate << endl;
-
-
-        if( report_progress && stage < end_stage )
-            pb = new ProgressBar( "Dynamic training phase of "+classname(),
-                                  end_stage - init_stage );
-
-        previous_hidden_layer.resize(hidden_layer->size);
-        previous_visible_layer.resize(visible_layer->size);
-        dynamic_connections->setLearningRate( dynamic_learning_rate );
-        visible_connections->setLearningRate( visible_dynamic_learning_rate );
-        real mean_cost = 0;
-        while(stage < end_stage)
-        {
-            for(int sample=0 ; sample<train_set->length() ; sample++ )
-            {
-                if(sample > 0)
-                {
-                    previous_hidden_layer << hidden_layer_sample;
-                    previous_visible_layer << visible_layer_sample;
-                    // ** or **
-                    // hidden_layer->generateSample();
-                    // previous_hidden_layer << hidden_layer->sample;
-                }
-                else
-                {
-                    previous_hidden_layer.clear();
-                    previous_visible_layer.clear();
-                }
-
-                train_set->getExample(sample, input, target, weight);
-
-                if(train_set->getString(sample,0) == "<oov>")
-                {
-                    hidden_layer_sample.clear();
-                    visible_layer_sample.clear();
-                    continue;
-                }
-            
-                clamp_visible_units(input);
-                
-                mean_cost += dynamic_connections_update();
-                                
-            }
-            if( pb )
-                pb->update( stage + 1 - init_stage);
-
-            if(verbosity>0)
-                cout << "mean cost at stage " << stage << 
-                    " = " << mean_cost/train_set->length() << endl;
-            mean_cost = 0;
-            stage++;
-        }    
-        if( pb )
-        {
-            delete pb;
-            pb = 0;
-        }
-
-        //Make a copy of the dynamique phase
-        //CopiesMap map;
-        //dynamic_connections_copy = dynamic_connections->deepCopy(map);
-
-        //cout << "dynamic phase training" << endl;
-    }
-
-
-    //cout << dynamic_connections_copy->weights << endl;
-
-
-    //alpha = Vec(hidden_layer->size,1);
-
-
-
     /***** fine-tuning *****/
     if( stage >= nstages )
         return;
 
     if(stage < rbm_nstages +  dynamic_nstages + fine_tuning_nstages )
     {
-        MODULE_LOG << "Training the whole model" << endl;
-
-        int init_stage = stage;
-        //int end_stage = max(0,nstages-(rbm_nstages + dynamic_nstages));
-        //int end_stage = nstages;
-        int end_stage = min( rbm_nstages + dynamic_nstages + fine_tuning_nstages, nstages );
-
-        MODULE_LOG << "  stage = " << stage << endl;
-        MODULE_LOG << "  end_stage = " << end_stage << endl;
-        MODULE_LOG << "  fine_tuning_learning_rate = " << fine_tuning_learning_rate << endl;
-
-        if( report_progress && stage < end_stage )
-            pb = new ProgressBar( "Fine-tuning training phase of "+classname(),
-                                  end_stage - init_stage );
-
-        previous_hidden_layer.resize(hidden_layer->size);
-        dynamic_connections->setLearningRate( fine_tuning_learning_rate );
-        visible_layer->setLearningRate( fine_tuning_learning_rate );
-        connections->setLearningRate( fine_tuning_learning_rate );
-
-        real mean_cost = 0;
-
-
-        while(stage < end_stage)
-        {
-            for(int sample=0 ; sample<train_set->length() ; sample++ )
-            {
-                if(sample > 0)
-                {
-                    previous_hidden_layer << hidden_layer_sample;
-
-                    // ** or **
-                    // hidden_layer->generateSample();
-                    // previous_hidden_layer << hidden_layer->sample;
-                }
-                else
-                    previous_hidden_layer.clear();
-
-
-                train_set->getExample(sample, input, target, weight);
-
-                
-
-                if(train_set->getString(sample,0) == "<oov>")
-                {
-                    hidden_layer_sample.clear();
-                    continue;
-                }
-
-                
-
-                clamp_visible_units(input);
-                
-                mean_cost += fine_tuning_update();     
-                //cout << "lalalalalalallalalalalalalala4" << endl;
-            }
-
-            if( pb )
-                pb->update( stage + 1 - init_stage);
-
-            if(verbosity>0)
-                cout << "mean cost at stage " << stage << 
-                    " = " << mean_cost/train_set->length() << endl;
-            mean_cost = 0;
-            stage++;
-        }    
-        if( pb )
-        {
-            delete pb;
-            pb = 0;
-        }
-
     }
 
-    //cout << dynamic_connections->weights - dynamic_connections_copy->weights<< endl;    
-    
 
-
- /***** Recurrent phase *****/
+    /***** Recurrent phase *****/
     if( stage >= nstages )
         return;
 
@@ -660,21 +521,25 @@
         real mean_cost = 0;
         int ith_sample_in_sequence = 0;
         int nb_oov = 0;
-
+        
+        RBMMixedLayer* p_visible_layer = dynamic_cast<RBMMixedLayer*>((RBMLayer*)visible_layer);
+        target_layer = p_visible_layer->sub_layers[3];
+        //target_layer = (PLearn::PP<PLearn::RBMMixedLayer>)visible_layer;
+        //test_layer = target_layer.sub_layers(1);
         while(stage < end_stage)
         {
             if(untie_weights && 
                stage == rbm_nstages + dynamic_nstages + fine_tuning_nstages)
             {
                 
-                //CopiesMap map;
-                //dynamic_connections_copy = dynamic_connections->deepCopy(map);
+                CopiesMap map;
+                dynamic_connections_copy = dynamic_connections->deepCopy(map);
 
                 //CopiesMap map2;
                 //connections_transpose_copy = connections_transpose->deepCopy(map2);
                 //connections_transpose = connections_transpose_copy;  
 /*
-                TMat<real> U,V;//////////*********************crap James
+                TMat<real> U,V;//////////crap James
                 TVec<real> S;
                 U.resize(hidden_layer->size,hidden_layer->size);
                 V.resize(hidden_layer->size,hidden_layer->size);
@@ -702,32 +567,17 @@
                 input_prediction_activations_list.resize(
                     ith_sample_in_sequence+1,visible_layer->size);
                 input_list.resize(ith_sample_in_sequence+1,visible_layer->size);
+                target_list.resize(ith_sample_in_sequence+1,target_layer->size);
                 nll_list.resize(ith_sample_in_sequence+1);
+                
+               
 
+                //if(train_set->getString(sample,0) == "<oov>")
+                if(train_set->get(sample,0) == 8)
+                {
 
-                if(train_set->getString(sample,0) == "<oov>")
-                {/*
-                    nb_oov++;
-                    hidden_list.resize(ith_sample_in_sequence,hidden_layer->size);
-                    hidden_activations_list.resize(ith_sample_in_sequence,hidden_layer->size);
-                    hidden2_list.resize(ith_sample_in_sequence,hidden_layer->size);
-                    hidden2_activations_list.resize(ith_sample_in_sequence,hidden_layer->size);
-                    input_prediction_list.resize(
-                        ith_sample_in_sequence,visible_layer->size);
-                    input_prediction_activations_list.resize(
-                        ith_sample_in_sequence,visible_layer->size);
-                    input_list.resize(ith_sample_in_sequence,visible_layer->size);
-                    nll_list.resize(ith_sample_in_sequence);
-                    recurrent_update();
-                    ith_sample_in_sequence = 0;
-                    hidden_list.clear();
-                    hidden2_list.clear();
-                    input_prediction_list.clear();
-                    input_list.clear();
-                    nll_list.clear();
-                    continue;
-                 */
                     input_list(ith_sample_in_sequence) << previous_input;
+                    target_list(ith_sample_in_sequence) << previous_target;
                     connections->setAsDownInput( previous_input );
                     hidden_layer->getAllActivations( connections_idem );
                     hidden_layer->computeExpectation();
@@ -753,39 +603,30 @@
                     hidden2_list.clear();
                     input_prediction_list.clear();
                     input_list.clear();
+                    target_list.clear();
                     nll_list.clear();
                     continue;
                 }
-/*
-                hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                hidden_activations_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                hidden2_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                hidden2_activations_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                input_prediction_list.resize(
-                    ith_sample_in_sequence+1,visible_layer->size);
-                input_prediction_activations_list.resize(
-                    ith_sample_in_sequence+1,visible_layer->size);
-                input_list.resize(ith_sample_in_sequence+1,visible_layer->size);
-                nll_list.resize(ith_sample_in_sequence+1);
-*/
+
          
-                clamp_visible_units(input);
 
+
+                if(isRegression)
+                    visible_layer->setExpectation(input);
+                else
+                    clamp_visible_units(input);
+                
+
                 if(ith_sample_in_sequence > 0)
                 {
                    
                     input_list(ith_sample_in_sequence) << previous_input;
+                    target_list(ith_sample_in_sequence) << previous_target;
                     //h*_{t-1}
                     //////////////////////////////////
                     dynamic_connections->fprop(previous_hidden_layer, cond_bias);
                     hidden_layer->setAllBias(cond_bias); //**************************
 
-//                    if (visible_connections_option){
-//                        //v*_{t-1} VISIBLE DYNAMIC CONNECTION
-//                        //////////////////////////////////
-//                        visible_connections->fprop(previous_input, visi_cond_bias);
-//                        visible_layer->getAllBias(visi_cond_bias); //**************************
-//                    }
 
                     //up phase
                     connections->setAsDownInput( previous_input );
@@ -803,18 +644,19 @@
                         dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
                     else
                         dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-                    //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+                    
                     hidden_layer->expectation_is_not_up_to_date();
                     hidden_layer->computeExpectation();//h_{t}
-                    ///////////
+                    
 
-                    previous_input << visible_layer->expectation;//v_{t-1}
+                    
              
                 }
                 else
                 {
                     
                     input_list(ith_sample_in_sequence).fill(-1);
+                    target_list(ith_sample_in_sequence).fill(-1);
  
                     previous_hidden_layer.clear();//h_{t-1}
                     //previous_hidden_layer.fill(0.5);//**************************crap James
@@ -827,39 +669,35 @@
                         dynamic_connections->fprop(
                             previous_hidden_layer,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
 
-                    //dynamic_connections_copy->fprop(previous_hidden_layer,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+                    
                     hidden_layer->expectation_is_not_up_to_date();
                     hidden_layer->computeExpectation();//h_{t}
 
                     previous_input.resize(visible_layer->size);
-                    previous_input << visible_layer->expectation;
+                    previous_target.resize(target_layer->size);
  
 
-//                    if (visible_connections_option){
-//
-//                        /////////VISIBLE DYNAMIC CONNECTION
-//                        previous_visible_layer.clear();//v_{t-1}
-//                        visible_connections->fprop(previous_visible_layer,visible_layer->activation);//conection entre v_{t-1} et v_{t}
-//                
-//                        visible_layer->expectation_is_not_up_to_date();
-//                        visible_layer->computeExpectation();//v_{t}
-//                
-//                        visi_bias_tempo.resize(visible_layer->bias.length());
-//                        visi_bias_tempo << visible_layer->bias;
-//                    }
             
                 }
 
-                // cout << "trililililililili" << endl;
+               
+                previous_input << visible_layer->expectation;//v_{t-1}
+                previous_target << target_layer->expectation;
+                
+               
 
-                connections_transpose->setAsDownInput( hidden_layer->expectation );
-                visible_layer->getAllActivations( connections_idem_t );
+                //connections_transpose->setAsDownInput( hidden_layer->expectation );
+                //visible_layer->getAllActivations( connections_idem_t );
 
-                //connections->setAsUpInput( hidden_layer->expectation );
-                //visible_layer->getAllActivations( connections_idem );
-
+                connections->setAsUpInput( hidden_layer->expectation );
+                visible_layer->getAllActivations( connections_idem );
                 visible_layer->computeExpectation();
- 
+
+                if(isRegression){
+                    partition(previous_input.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
+                    partition(previous_input.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
+                }
+
                 // Copies for backprop
                 hidden_list(ith_sample_in_sequence) << previous_hidden_layer;
                 hidden_activations_list(ith_sample_in_sequence) 
@@ -872,17 +710,17 @@
                 input_prediction_activations_list(ith_sample_in_sequence) << 
                     visible_layer->activation;
 
+                
  
-                nll_list[ith_sample_in_sequence] = 
-                    visible_layer->fpropNLL(previous_input) / inputsize() ;
+                //nll_list[ith_sample_in_sequence] = visible_layer->fpropNLL(previous_input); // / inputsize() ;
+                // real sum_mask = sums(mask);
+                nll_list[ith_sample_in_sequence] = target_layer->fpropNLL(previous_target); // / sum_mask;
+                
+
                 mean_cost += nll_list[ith_sample_in_sequence];
                 ith_sample_in_sequence++;
-                /////////VISIBLE DYNAMIC CONNECTION
-//                if (visible_connections_option){
-//                    visible_layer->getAllBias(visi_bias_tempo); 
-//                }
-
                
+               
             }
             if( pb )
                 pb->update( stage + 1 - init_stage);
@@ -905,7 +743,18 @@
     train_stats->finalize();
 }
 
+void DynamicallyLinkedRBMsModel::partition(TVec<double> part, TVec<double> periode, TVec<double> vel ) const
+{
+    for(int i = 0; i<part->size();i++){
+        periode[i] = part[i]*periode[i];
+        vel[i] = part[i]*vel[i];
 
+    }
+
+
+
+}
+
 void DynamicallyLinkedRBMsModel::clamp_visible_units(const Vec& input) const
 {
     int it = 0;
@@ -917,7 +766,6 @@
         // If input is a real ...
         if(ss < 0) 
         {
-            //cout << "yoyoyoyoyo" << endl;
             input_expectation[it++] = input[i];
         }
         else // ... or a symbol
@@ -931,320 +779,46 @@
     visible_layer->setExpectation(input_expectation);
 }
 
-real DynamicallyLinkedRBMsModel::rbm_update()
-{
 
-    //###### Positive phase #####################
 
-    //up phase
-    connections->setAsDownInput( visible_layer->expectation );
-    
-    hidden_layer->getAllActivations( connections_idem );
-
-    hidden_layer->computeExpectation();
-
-
-    //save the stats for the positive phase
-    pos_down_values << visible_layer->expectation;
-    pos_up_values << hidden_layer->expectation;
-
-
-
-    //down phase
-    hidden_layer->generateSample();
-    
-    connections->setAsUpInput( hidden_layer->sample );
-
-    visible_layer->getAllActivations( connections_idem );
-
-    visible_layer->computeExpectation();
-    
-    visible_layer->generateSample();
-
-
-
-
-    //############ Negative phase  ##################
-
-    connections->setAsDownInput( visible_layer->sample );
-    
-    hidden_layer->getAllActivations( connections_idem );
-
-    hidden_layer->computeExpectation();
-
-    hidden_layer->generateSample();
-
-    //############ CD update #########################
-
-    visible_layer->update( 
-        pos_down_values, visible_layer->sample ); // ... of visible_layer bias ...
-
-    hidden_layer->update( 
-        pos_up_values, hidden_layer->expectation );// ... of hidden_layer bias ...
-    
-    connections->update( 
-        pos_down_values, pos_up_values, visible_layer->sample
-        , hidden_layer->expectation  ); // ... of connections between layers.
-    
-    // Compute reconstruction error
-    
-    connections->setAsUpInput( pos_up_values );
-
-    visible_layer->getAllActivations( connections_idem );
-
-    
-    visible_layer->computeExpectation();
-    
-    
-    return visible_layer->fpropNLL(pos_down_values);
-   
-}
-
-real DynamicallyLinkedRBMsModel::dynamic_connections_update()
-{
-    
-
-    // Obtain target hidden_layer h_t
-    connections->setAsDownInput(visible_layer->expectation);
-    hidden_layer->getAllActivations(connections_idem);
-    hidden_layer->computeExpectation();
-    hidden_layer_target << hidden_layer->expectation;
-    hidden_layer->generateSample();
-    hidden_layer_sample << hidden_layer->sample;
-    
-    // Use "previous_hidden_layer" field and "dynamic_connections" module 
-    // to set bias of "hidden_layer"
-    dynamic_connections->fprop(previous_hidden_layer,hidden_layer->activation);
-    hidden_layer->expectation_is_not_up_to_date();
-    hidden_layer->computeExpectation();
-    
-    
-    
-    // Ask "hidden_layer" for maximum likelyhood gradient on bias
-    real nll = hidden_layer->fpropNLL(hidden_layer_target);
-    hidden_layer->bpropNLL(hidden_layer_target, nll, bias_gradient);
-    
-    
-    dynamic_connections->bpropUpdate(previous_hidden_layer,
-                                     hidden_layer->activation,
-                                     input_gradient, bias_gradient);
-    
-    
-    
-    
-    
-    
-    
-    
-    //////////////////// VISIBLE DYNAMIC CONNECTION
-    /*  if (visible_connections_option){
-        visible_layer_sample << visible_layer->expectation;
-        
-        // Use "previous_visible_layer" field and "visible_connections" module 
-        // to set bias of "visible_layer"
-        
-        visible_connections->fprop(previous_visible_layer,visible_layer->activation);
-        visible_layer->expectation_is_not_up_to_date();
-        visible_layer->computeExpectation();
-        
-        
-        
-        // Ask "visible_layer" for maximum likelyhood gradient on bias
-        real nll_visi = visible_layer->fpropNLL(visible_layer_sample);
-        visible_layer->bpropNLL(visible_layer_sample, nll_visi, visi_bias_gradient);
-        
-        
-        visible_connections->bpropUpdate(previous_visible_layer,
-                                         visible_layer->activation,
-                                         input_gradient, visi_bias_gradient);
-                                         }*/
-
-    return nll;
-}
-
-real DynamicallyLinkedRBMsModel::fine_tuning_update()
-{
-
- 
-
-     
-
-
-
-    cond_bias.resize(hidden_layer->size);
-   
-    dynamic_connections->fprop(previous_hidden_layer, cond_bias);
-
-
-  
-
-    hidden_layer->setAllBias(cond_bias);
-    
-
-
-
- //###### Positive phase #####################
-
-    //up phase
-    connections->setAsDownInput( visible_layer->expectation );
-    hidden_layer->getAllActivations( connections_idem );
-    hidden_layer->computeExpectation();
-
-
-    //save the stats for the positive phase
-    pos_down_values << visible_layer->expectation;
-    pos_up_values << hidden_layer->expectation;
-
-
-
-    //down phase
-    hidden_layer->generateSample();
-    hidden_layer_sample << hidden_layer->sample;
-    connections->setAsUpInput( hidden_layer->sample );
-
-    visible_layer->getAllActivations( connections_idem );
-
-    visible_layer->computeExpectation();
-    
-    visible_layer->generateSample();
-
-
-
-
-    //############ Negative phase  ##################
-
-    connections->setAsDownInput( visible_layer->sample );
-    
-    hidden_layer->getAllActivations( connections_idem );
-
-    hidden_layer->computeExpectation();
-
-    hidden_layer->generateSample();
-
-    
-
-
-    //cout <<  hidden_layer->bias << endl;
-     //############ CD update #########################
-
-    visible_layer->update( 
-        pos_down_values, visible_layer->sample ); // ... of visible_layer bias ...
-
-    
-
-
-// cout << pos_up_values << endl;
-// cout << "blabla" << endl;
-// cout << hidden_layer->sample << endl;
-    bias_gradient.resize( hidden_layer-> size );
-
-
-
-
-
-
-    hidden_layer->bpropCD(pos_up_values, hidden_layer->expectation, bias_gradient);
-
-
-
-    dynamic_connections->bpropUpdate(previous_hidden_layer,
-                                     cond_bias,
-                                     input_gradient, bias_gradient);
-
-
-    // hidden_layer->update( 
-    //    pos_up_values, hidden_layer->sample );// ... of hidden_layer bias ...
-    
-    connections->update( 
-        pos_down_values, pos_up_values, visible_layer->sample
-        , hidden_layer->expectation ); // ... of connections between layers.
-
-    //cout << ((PLearn::PP<PLearn::GradNNetLayerModule>)dynamic_connections)->weights << endl;
-
-
-
-
-
-
-
-
-    // Use "previous_hidden_layer" field and "dynamic_connections" module 
-    // to set bias of "hidden_layer"
-
-    // Positive phase
-
-    // Negative phase
-
-    // CD update ...
-
-    // .. ask "hidden_layer" for CD gradient on bias and ...
-
-    // ... bpropUpdate through dynamic_connections 
-    // to update conditional bias (*)
-    
-    // ... of visible_layer bias ...
-
-    // ... of connections between layers.
-
-    // NB.: it is important not to update the hidden_layer's bias
-    //      using usual CD update. Conditional bias was updated
-    //      before at (*)
-
-
-
-
-
-    // Compute reconstruction error
-
-    //cond_bias.clear();
-    //dynamic_connections->fprop(previous_hidden_layer, cond_bias);
-    //hidden_layer->getAllBias(cond_bias);
-
-    
-
-
-
-    connections->setAsUpInput( pos_up_values );
-
-    visible_layer->getAllActivations( connections_idem );
-
-    visible_layer->computeExpectation();
-    
-    return visible_layer->fpropNLL(pos_down_values);
-
-    
-
-
-}
-
-
 void DynamicallyLinkedRBMsModel::recurrent_update()
 {
     // Notes: 
     //    - not all lists are useful (some *_activations_* are not)
-    int segment = hidden_list.length()/2;
-    int seg =0;
-    for(int k=hidden_list.length()-3; k>=-segment; k-=segment){ 
-        seg = k;
-        if(seg < 0)
-            seg = 0;
+    //int segment = hidden_list.length()/2;
+    //int seg =0;
+    //for(int k=hidden_list.length()-3; k>=-segment; k-=segment){ 
+    //  seg = k;
+    //  if(seg < 0)
+    //      seg = 0;
         //cout << "segment: " << seg << endl;
         hidden_temporal_gradient.clear();
-        //for(int i=hidden_list.length()-2; i>=0; i--){  
-        for(int i=hidden_list.length()-2; i>=seg; i--){     
+        for(int i=hidden_list.length()-2; i>=0; i--){  
+        // for(int i=hidden_list.length()-2; i>=seg; i--){     
             
             //visible_layer->expectation << input_prediction_list(i);
             //visible_layer->activation << ?????;
             visible_layer->setExpectation(input_prediction_list(i));
             
+            //visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient, (taillePart*3)+14);
+
+            //      hidden_gradient.clear();
+            //HUGO: for( int tar=0; tar<target_layers.length(); tar++)
+            //      {
+            //           target_layers[tar]->bpropNLL(targets_list[tar](i+1),nll_list(i,tar),target_bias_gradient[tar]);
+            //           target_bias_gradient[tar] *= target_layers_weights[tar];
+            //           target_layers[tar]->update(target_bias_gradient[tar]);
+            //           target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_activations_list[tar](i),
+            //                                                hidden_gradient, target_bias_gradient[tar],true);
+            //      }
+
             visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient);
             
             visible_layer->update(visi_bias_gradient);
             
+            //visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient, (taillePart*3)+14);
             
-            connections_transpose->bpropUpdate(
-                hidden2_list(i),input_prediction_activations_list(i),
-                hidden_gradient, visi_bias_gradient);
+            connections_transpose->bpropUpdate(hidden2_list(i),input_prediction_activations_list(i),hidden_gradient, visi_bias_gradient);
             
             
             //hidden_layer->setExpectation(hidden_list(i+1));//////////////////////////////
@@ -1301,7 +875,7 @@
                 // Could learn initial value for h_{-1}
             }
         }
-    }
+    
 }
 
 
@@ -1358,7 +932,8 @@
        
 
 
-        if(testset->getString(i,0) == "<oov>")
+        //if(testset->getString(i,0) == "<oov>")
+        if(testset->get(i,0) == 8)
         {
             begin = 0;
             nb_oov++;
@@ -1369,11 +944,11 @@
 
 
 
-        clamp_visible_units(input);
+        //clamp_visible_units(input);
+        visible_layer->setExpectation(input);
 
 
 
-
         if(begin > 0)
         {
 
@@ -1386,7 +961,7 @@
                 //v*_{t-1} VISIBLE DYNAMIC CONNECTION
                 //////////////////////////////////
                 visible_connections->fprop(previous_input, visi_cond_bias);
-                visible_layer->setAllBias(visi_cond_bias); //**************************
+                visible_layer->setAllBias(visi_cond_bias); 
                 }*/
 
             //up phase
@@ -1458,20 +1033,20 @@
 
 
      
-        connections_transpose->setAsDownInput( hidden_layer->expectation );
-        visible_layer->getAllActivations( connections_idem_t );
+        //connections_transpose->setAsDownInput( hidden_layer->expectation );
+        //visible_layer->getAllActivations( connections_idem_t );
 
-        //connections->setAsUpInput( hidden_layer->expectation );
-        //visible_layer->getAllActivations( connections_idem );
-
-
-       
+        connections->setAsUpInput( hidden_layer->expectation );
+        visible_layer->getAllActivations( connections_idem );
         visible_layer->computeExpectation();
 
-       
 
-        costs[0] = visible_layer->fpropNLL(previous_input) / inputsize() ;
+        partition(previous_input.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
+        partition(previous_input.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
 
+
+        //costs[0] = visible_layer->fpropNLL(previous_input,(taillePart*3)+14) ;
+        costs[0] = visible_layer->fpropNLL(previous_input) ;
        
         hidden_layer->setAllBias(bias_tempo); 
 
@@ -1514,6 +1089,137 @@
     return getTestCostNames();
 }
 
+void DynamicallyLinkedRBMsModel::gen()
+{
+    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data = new AutoVMatrix();
+    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data->defineSizes(21,0,0);
+    //data->inputsize = 21;
+    //data->targetsize = 0;
+    //data->weightsize = 0;
+    data->build();
+
+    
+    int len = data->length();
+    Vec score;
+    Vec target;
+    real weight;
+    Vec bias_tempo;
+    Vec visi_bias_tempo;
+   
+   
+    
+    previous_hidden_layer.resize(hidden_layer->size);
+    connections_idem = connections;
+
+    for (int ith_sample = 0; ith_sample < len ; ith_sample++ ){
+        
+        data->getExample(ith_sample, score, target, weight);
+        //score << data(ith_sample);
+        input_prediction_list.resize(
+            ith_sample+1,visible_layer->size);
+        if(ith_sample > 0)
+        {
+            
+            //input_list(ith_sample_in_sequence) << previous_input;
+            //h*_{t-1}
+            //////////////////////////////////
+            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+            hidden_layer->setAllBias(cond_bias); //**************************
+            
+            
+            
+            //up phase
+            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
+            hidden_layer->getAllActivations( connections_idem );
+            hidden_layer->computeExpectation();
+            //////////////////////////////////
+            
+            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
+            //previous_hidden_layer_activation << hidden_layer->activation;
+            
+            
+            //h*_{t}
+            ////////////
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            else
+                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            hidden_layer->expectation_is_not_up_to_date();
+            hidden_layer->computeExpectation();//h_{t}
+            ///////////
+            
+            //previous_input << visible_layer->expectation;//v_{t-1}
+            
+        }
+        else
+        {
+            
+            previous_hidden_layer.clear();//h_{t-1}
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->fprop( previous_hidden_layer ,
+                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            else
+                dynamic_connections->fprop(previous_hidden_layer,
+                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            
+            hidden_layer->expectation_is_not_up_to_date();
+            hidden_layer->computeExpectation();//h_{t}
+            //previous_input.resize(data->inputsize);
+            //previous_input << data(ith_sample);
+            
+        }
+        
+        //connections_transpose->setAsDownInput( hidden_layer->expectation );
+        //visible_layer->getAllActivations( connections_idem_t );
+        
+        connections->setAsUpInput( hidden_layer->expectation );
+        visible_layer->getAllActivations( connections_idem );
+        
+        visible_layer->computeExpectation();
+        //visible_layer->generateSample();
+        partition(score.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
+        partition(score.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
+
+
+        visible_layer->activation.subVec(0,14+taillePart) << score;
+        visible_layer->expectation.subVec(0,14+taillePart) << score;
+
+        input_prediction_list(ith_sample) << visible_layer->expectation;
+        
+    }
+    
+    //Vec tempo;
+    TVec<real> tempo;
+    tempo.resize(visible_layer->size);
+    ofstream myfile;
+    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/test.txt");
+    
+    for (int i = 0; i < len ; i++ ){
+        tempo << input_prediction_list(i);
+        
+        //cout << tempo[2] << endl;
+       
+        for (int j = 0; j < tempo.length() ; j++ ){
+            
+            
+                
+                
+               myfile << tempo[j] << " ";
+               
+
+               
+           
+        }
+        myfile << "\n";
+    }
+     
+
+     myfile.close();
+
+}
 void DynamicallyLinkedRBMsModel::generate(int nbNotes)
 {
     
@@ -1593,10 +1299,10 @@
     TVec<int> tempo;
     tempo.resize(visible_layer->size);
     int theNote;
-    int nbNoteVisiLayer = input_prediction_list(1).length()/13;
+    //int nbNoteVisiLayer = input_prediction_list(1).length()/13;
     ofstream myfile;
     int theLayer;
-    myfile.open ("/u/laulysta/recherche_maitrise/projet_GenerationDeMusique/data/generate/test.txt");
+    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_musicGeneration/data/generate/test.txt");
     
     for (int i = 0; i < nbNotes ; i++ ){
         tempo << input_prediction_list(i);

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-14 13:12:51 UTC (rev 8996)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-14 18:18:21 UTC (rev 8997)
@@ -46,8 +46,10 @@
 #include <plearn_learners/online/OnlineLearningModule.h>
 #include <plearn_learners/online/RBMClassificationModule.h>
 #include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
 #include <plearn_learners/online/RBMConnection.h>
 #include <plearn_learners/online/RBMMatrixConnection.h>
+#include <plearn/vmat/AutoVMatrix.h>
 #include <plearn_learners/online/RBMMatrixTransposeConnection.h>
 
 #include <plearn_learners/online/GradNNetLayerModule.h>
@@ -81,7 +83,13 @@
 
     //! Indication to untie weights in recurrent net
     bool untie_weights;
+    
+    //! Indicate the size of the partition
+    int taillePart;
 
+    //! Indicate if the model is used for regression
+    int isRegression;
+
     // TODO The weight decay used during the gradient descent 
     //real grad_weight_decay;
 
@@ -100,9 +108,15 @@
     //! Option for the visible dynamic connection 
     int visible_connections_option;
 
+    //! The target layer of the RBMs
+    PP<RBMLayer> target_layer;
+
     //! The visible layer of the RBMs
-    PP<RBMLayer> visible_layer;
+    TVec<RBMLayer> input_layer;
 
+     //! The visible layer of the RBMs
+    PP<RBMLayer> test_layer;
+
     //! The hidden layer of the RBMs
     PP<RBMLayer> hidden_layer;
 
@@ -130,12 +144,21 @@
 
     //#####  Public Learnt Options  ###########################################
 
-    //! Size of the visible layer
-    int visible_size;
+    //! Size of the input layer
+    int input_size;
 
+    //! Size of the target layer
+    int target_size;
+
+    //! Size of each target layers
+    TVec<int> target_layers_size;
+
     //! Number of symbols for each symbolic field of train_set
-    TVec<int> symbol_sizes;
-
+    TVec<int> input_symbol_sizes;
+    
+    //! Number of symbols for each symbolic field of train_set
+    Mat target_symbol_sizes;
+    
     //#####  Not Options  #####################################################
 
 
@@ -177,10 +200,16 @@
     //! Generate music in a folder
     void generate(int nbNotes);
 
+    //! Generate a part of the data in a folder
+    void gen();
+
     //! Returns the names of the objective costs that the train method computes
     //! and  for which it updates the VecStatsCollector train_stats.
     virtual TVec<std::string> getTrainCostNames() const;
 
+    //! Use the partition
+    void partition(TVec<double> part, TVec<double> periode, TVec<double> vel ) const;
+    
     //! Clamps the visible units based on an input vector
     void clamp_visible_units(const Vec& input) const;
 
@@ -243,6 +272,10 @@
 protected:
     //#####  Not Options  #####################################################
 
+
+    //! Store external data;
+    AutoVMatrix*  data;
+   
     //! Stores conditional bias
     mutable Vec cond_bias;
 
@@ -271,8 +304,11 @@
     //! Stores hidden gradient of dynamic connections coming from time t+1
     mutable Vec hidden_temporal_gradient;
     
-    //! Stores previous hidden layer value
+    //! Stores previous input layer value
     mutable Vec previous_input;
+
+    //! Stores previous target layer value
+    mutable Vec previous_target;
     
     //! Stores previous hidden layer value
     mutable Vec previous_hidden_layer;
@@ -315,6 +351,9 @@
     //! List of inputs values
     Mat input_list;
 
+    //! List of inputs values
+    Mat target_list;
+
     //! List of the nll of the input samples in a sequence
     Vec nll_list;
 



From nouiz at mail.berlios.de  Wed May 14 20:49:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 May 2008 20:49:23 +0200
Subject: [Plearn-commits] r8998 - trunk/plearn/vmat
Message-ID: <200805141849.m4EInNHI014450@sheep.berlios.de>

Author: nouiz
Date: 2008-05-14 20:49:22 +0200 (Wed, 14 May 2008)
New Revision: 8998

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
implemented a new instruction: err. This instruction make it an error to have a missing value in a field


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-05-14 18:18:21 UTC (rev 8997)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-05-14 18:49:22 UTC (rev 8998)
@@ -77,7 +77,13 @@
                 "If greater or equal to 1, the integer portion is interpreted as the number of samples to use.");
       
   declareOption(ol, "imputation_spec", &MeanMedianModeImputationVMatrix::imputation_spec, OptionBase::buildoption, 
-                "Pairs of instruction of the form field_name : mean | median | mode.");
+                "Pairs of instruction of the form field_name : mean | median | mode | none | err.\n"
+		" -mean  : put the mean of the field if one value is missing\n"
+       		" -median: put the median of the field if one value is missing\n"
+		" -mode  : put the mode(most frequent value) of the field if one value is missing\n"
+		" -none  : let the missing value in this field\n"
+		" -err   : make it an error to have a missing value in this field"
+		);
 
   declareOption(ol, "variable_mean", &MeanMedianModeImputationVMatrix::variable_mean, OptionBase::learntoption, 
                 "The vector of variable means observed from the train set.");
@@ -146,6 +152,9 @@
   else if (variable_imputation_instruction[j] == 2) return variable_median[j];
   else if (variable_imputation_instruction[j] == 3) return variable_mode[j];
   else if (variable_imputation_instruction[j] == 4) return variable_value;
+  else if (variable_imputation_instruction[j] == 5)
+    return MISSING_VALUE;//PLERROR("");//PLERROR("In MeanMedianModeImputationVMatrix::get(%d,%d) - the value is"
+      //	   " missing and have a instruction err!",i,j);
   else
     PLERROR("In MeanMedianModeImputationVMatrix::get(%d,%d) - "
 	    "unknow variable_imputation_instruction value of %d",i,j,
@@ -167,6 +176,9 @@
 	v[source_col] = variable_mode[source_col + j];
       else if (variable_imputation_instruction[source_col + j] == 4)
 	;//do nothing
+      else if (variable_imputation_instruction[source_col + j] == 5)
+	return PLERROR("In MeanMedianModeImputationVMatrix::getSubRow(%d,%d) - the value is"
+		       " missing and have a instruction err!",i,j);
       else
 	PLERROR("In MeanMedianModeImputationVMatrix::getSubRow(%d,%d, Vec) - "
 		"unknow variable_imputation_instruction value of %d",i,j,
@@ -187,6 +199,9 @@
 	v[source_col] = variable_mode[source_col]; 
       else if (variable_imputation_instruction[source_col] == 4)
 	;//do nothing
+      else if (variable_imputation_instruction[source_col] == 5)
+	return PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d) - the value is"
+		       " missing and have a instruction err!",i);
       else
 	PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d, Vec) - "
 		"unknow variable_imputation_instruction value of %d",i,
@@ -204,6 +219,9 @@
       else if (variable_imputation_instruction[i] == 3) v[source_row] = variable_mode[i];
       else if (variable_imputation_instruction[i] == 4)
 	;//do nothing
+      else if (variable_imputation_instruction[i] == 5)
+	return PLERROR("In MeanMedianModeImputationVMatrix::getColumn(%d) - the value is"
+		       " missing and have a instruction err!",i);
       else
 	PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d, Vec) - "
 		"unknow variable_imputation_instruction value of %d",i,
@@ -294,6 +312,7 @@
         else if (imputation_spec[spec_col].second == "median") variable_imputation_instruction[train_col] = 2;
         else if (imputation_spec[spec_col].second == "mode") variable_imputation_instruction[train_col] = 3;
         else if (imputation_spec[spec_col].second == "none") variable_imputation_instruction[train_col] = 4;
+        else if (imputation_spec[spec_col].second == "err") variable_imputation_instruction[train_col] = 5;
         else
 	  PLERROR("In MeanMedianModeImputationVMatrix: unsupported imputation instruction: %s : %s",
 		     (imputation_spec[spec_col].first).c_str(), (imputation_spec[spec_col].second).c_str());



From larocheh at mail.berlios.de  Wed May 14 22:08:50 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 14 May 2008 22:08:50 +0200
Subject: [Plearn-commits] r8999 - trunk/plearn_learners_experimental
Message-ID: <200805142008.m4EK8opD021554@sheep.berlios.de>

Author: larocheh
Date: 2008-05-14 22:08:50 +0200 (Wed, 14 May 2008)
New Revision: 8999

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
On code comme des malades!!!


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-14 18:49:22 UTC (rev 8998)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-14 20:08:50 UTC (rev 8999)
@@ -32,7 +32,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Pascal Lamblin
+// Authors: Stanislas Lauly
 
 /*! \file DynamicallyLinkedRBMsModel.cc */
 
@@ -206,10 +206,16 @@
 
     if(train_set)
     {
-        input_size = 0;
+
+        PLASSERT( target_layers_weights.length() == target_layers.length() );
+
+
+        // Parsing symbols in input
+        input_layer_size = 0;
         input_symbol_sizes.resize(0);
-        PP<Dictionary> dict;        
-        for(int i=0; i<input_layer->size; i++)
+        PP<Dictionary> dict;
+        int inputsize_without_masks = inputsize() - ( use_target_layers_masks ? targetsize() : 0 );
+        for(int i=0; i<inputsize_without_masks; i++)
         {
             dict = train_set->getDictionary(i);
             if(dict)
@@ -219,95 +225,120 @@
                         "of field %d is empty", i);
                 input_symbol_sizes.push_back(dict->size());
                 // Adjust size to include one-hot vector
-                input_size += dict->size();
+                input_layer_size += dict->size();
             }
             else
             {
                 input_symbol_sizes.push_back(-1);
-                input_size++;
+                input_layer_size++;
             }
         }
 
-        //target_size.fill(-1);
-        //target_symbol_sizes.resize(0);
-        target_size = 0;
-        for( int tar=0; tar<target_layers.length(); tar++){
-            target_layers_size[tar] = 0;
-            target_symbol_sizes.resize(0,0);
-            for(int i=0; i<target_layer[tar]->size; i++)
+        if( input_layer->size != input_layer_size )
+            PLERROR("DynamicallyLinkedRBMsModel::build_(): input_layer->size %d "
+                    "should be %d", input_layer->size, input_layer_size);
+
+        // Parsing symbols in target
+        int tar_layer = 0;
+        target_symbol_sizes.resize(target_layers.length());
+        target_layers_size.clear();
+        for( int tar=0; tar<targetsize(); tar++)
+        {
+            if( tar_layer > target_layers.length() )
+                PLERROR("DynamicallyLinkedRBMsModel::build_(): target layers "
+                        "does not cover all targets.");            
+
+            target_symbol_sizes[tar_layer].resize(0);
+            dict = train_set->getDictionary(tar+inputsize());
+            if(dict)
             {
-                dict = train_set->getDictionary(i);
-                if(dict)
-                {
-                    if( dict->size() == 0 )
-                        PLERROR("DynamicallyLinkedRBMsModel::build_(): dictionary "
-                                "of field %d is empty", i);
-                    target_symbol_sizes.resize(tar+1,i+1);
-                    target_symbol_sizes(tar,i) = dict->size();
-                    // Adjust size to include one-hot vector
-                    target_layers_size[tar] += dict->size();
-                    target_size += dict->size();
-                }
-                else
-                {
+                if( use_target_layers_masks )
+                    PLERROR("DynamicallyLinkedRBMsModel::build_(): masks for "
+                            "symbolic targets is not implemented.");
+                if( dict->size() == 0 )
+                    PLERROR("DynamicallyLinkedRBMsModel::build_(): dictionary "
+                            "of field %d is empty", i);
 
-                    target_symbol_sizes.resize(tar+1,i+1);
-                    target_symbol_sizes(tar,i) = -1;
-                    target_layers_size[tar]++;
-                    target_size++;
-                }
+                target_symbol_sizes[tar_layer].push_back(dict->size());
+                // Adjust size to include one-hot vector
+                target_layers_size[tar_layer] += dict->size();
             }
+            else
+            {
+                target_symbol_sizes[tar_layer].push_back(-1);
+                target_layers_size[tar_layer] ++
+            }
+
+            if( target_layers[tar_layer]->size == target_layers_size[tar_layer] )
+                tar_layer++;
         }
-        // Set and verify sizes
-        if(hidden_layer->size <= 0)
-            PLERROR("DynamicallyLinkedRBMsModel::build_(): hidden_layer->size "
-                "must be > 0");
 
-        
-        previous_visible_layer.resize(visible_size);
+        if( tar_layer != target_layers.length() )
+            PLERROR("DynamicallyLinkedRBMsModel::build_(): target layers "
+                    "does not cover all targets.");
 
-        visi_bias_gradient.resize(visible_size);
+        if( !input_layer->random_gen )
+        {
+            input_layer->random_gen = random_gen;
+            input_layer->forget();
+        }
 
-        visible_layer->size = visible_size;
+        if( !hidden_layer->random_gen )
+        {
+            hidden_layer->random_gen = random_gen;
+            hidden_layer->forget();
+        }
 
-        connections->down_size = visible_size;
-        connections->up_size = hidden_layer->size;
+        input_connections->down_size = input_layer->size;
+        input_connections->up_size = hidden_layer->size;
+        if( !input_connections->random_gen )
+        {
+            input_connections->random_gen = random_gen;
+            input_connections->forget();
+        }
+        input_connections->build();
 
-        dynamic_connections->input_size = hidden_layer->size;
-        dynamic_connections->output_size = hidden_layer->size;
+        if( hidden_layer2 )
+        {
+            if( !hidden_layer2->random_gen )
+            {
+                hidden_layer2->random_gen = random_gen;
+                hidden_layer2->forget();
+            }
 
-        visible_connections->input_size = visible_size;
-        visible_connections->output_size = visible_size;
+            hidden_connections->down_size = hidden_layer->size;
+            hidden_connections->up_size = hidden_layer2->size;
+            if( !hidden_connections->random_gen )
+            {
+                hidden_connections->random_gen = random_gen;
+                hidden_connections->forget();
+            }
+            hidden_connections->build();
+        }
 
-        // Set random generators
-        visible_layer->random_gen = random_gen;
-        hidden_layer->random_gen = random_gen;
-        connections->random_gen = random_gen;
-        dynamic_connections->random_gen = random_gen;
-        visible_connections->random_gen = random_gen;
+        for( int tar_layer = 0; tar_layer < target_layers.length(); tar_layer++ )
+        {
+            if( !target_layers[tar_layer]->random_gen )
+            {
+                target_layers[tar_layer]->random_gen = random_gen;
+                target_layers[tar_layer]->forget();
+            }
 
-        // Build components
-        visible_layer->build();
-        hidden_layer->build();
-        connections->build();
-        dynamic_connections->build();
-        visible_connections->build();
-        connections_transpose = new RBMMatrixTransposeConnection(connections);
-        connections_idem_t = connections_transpose;
-    }
-    if(hidden_layer->size>0){
-        previous_hidden_layer.resize(hidden_layer->size);
+            if( hidden_layer2 )
+                target_connections[tar_layer]->down_size = hidden_layer2->size;
+            else
+                target_connections[tar_layer]->down_size = hidden_layer->size;
 
-        pos_up_values.resize(hidden_layer->size);
-        hidden_layer_target.resize(hidden_layer->size);
-        hidden_layer_sample.resize(hidden_layer->size);
-        previous_hidden_layer.resize(hidden_layer->size);
-        previous_hidden_layer_activation.resize(hidden_layer->size);
-        hidden_temporal_gradient.resize(hidden_layer->size);
+            target_connections[tar_layer]->up_size = target_layers[tar_layer]->size;
+            if( !target_connections[tar_layer]->random_gen )
+            {
+                target_connections[tar_layer]->random_gen = random_gen;
+                target_connections[tar_layer]->forget();
+            }
+            target_connections[tar_layer]->build();
+        }
+
     }
-    if(connections)
-        connections_idem = connections;
-
 }
 
 // ### Nothing to add here, simply calls build_

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-14 18:49:22 UTC (rev 8998)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-14 20:08:50 UTC (rev 8999)
@@ -33,7 +33,7 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
-// Authors: Pascal Lamblin
+// Authors: Stanislas Lauly
 
 /*! \file DynamicallyLinkedRBMsModel.h */
 
@@ -66,90 +66,57 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! The learning rate used during RBM contrastive divergence learning phase
-    real rbm_learning_rate;
+    ////! The learning rate used during RBM contrastive divergence learning phase
+    //real rbm_learning_rate;
 
-    //! The learning rate used during the dynamic links learning phase
-    real dynamic_learning_rate;
-
-    //! The learning rate used during the dynamic links learning phase for the visible_layer
-    real visible_dynamic_learning_rate;
-
-    //! The learning rate used during the fine tuning phase
-    real fine_tuning_learning_rate;
-
     //! The learning rate used during the recurrent phase
     real recurrent_net_learning_rate;
 
-    //! Indication to untie weights in recurrent net
-    bool untie_weights;
-    
-    //! Indicate the size of the partition
-    int taillePart;
+    ////! Number of epochs for rbm phase
+    //int rbm_nstages;
 
-    //! Indicate if the model is used for regression
-    int isRegression;
+    //! The target layers of the RBMs
+    TVec< PP<RBMLayer> > target_layers;
 
-    // TODO The weight decay used during the gradient descent 
-    //real grad_weight_decay;
+    //! The training weights of each target layers
+    Vec target_layers_weights;
+    
+    //! Indication that a mask indicating which target to predict
+    //! is present in the input part of the VMatrix dataset.
+    bool use_target_layers_masks;
 
-    //! Number of epochs for rbm phase
-    int rbm_nstages;
+    //! Value of the first input component for end-of-sequence delimiter
+    real end_of_sequence_symbol;
 
-    //! Number of epochs for dynamic phase
-    int dynamic_nstages;
+    //! The weight of an additional input reconstruction error
+    real input_reconstruction_weight;
 
-    //! Number of epochs for fine tuning phase
-    int fine_tuning_nstages;
-
-    //! Number of epochs for the recurrent phase
-    int recurrent_nstages;
-
-    //! Option for the visible dynamic connection 
-    int visible_connections_option;
-
-    //! The target layer of the RBMs
-    PP<RBMLayer> target_layer;
-
     //! The visible layer of the RBMs
     TVec<RBMLayer> input_layer;
 
-     //! The visible layer of the RBMs
-    PP<RBMLayer> test_layer;
-
     //! The hidden layer of the RBMs
     PP<RBMLayer> hidden_layer;
 
-    //! OnlineLearningModule corresponding to dynamic links
-    //! between RBMs' hidden layers
-    PP<GradNNetLayerModule> dynamic_connections;
+    //! The second hidden layer of the RBMs (optional) 
+    PP<RBMLayer> hidden_layer2;
 
-    //! Copy OnlineLearningModule corresponding to dynamic links
-    //! between RBMs' hidden layers
-    PP<GradNNetLayerModule> dynamic_connections_copy;
+    //! The RBMConnection between the first hidden layers, through time
+    PP<RBMConnection> dynamic_connections;
 
-    //! OnlineLearningModule corresponding to dynamic links
-    //! between RBMs' visible layers
-    PP<GradNNetLayerModule> visible_connections;
+    //! The RBMConnection between the first and second hidden layers (optional)
+    PP<RBMConnection> hidden_connections;
 
-    //! The weights of the connections between the RBM visible and hidden layers
-    PP<RBMMatrixConnection> connections;
-    PP<RBMConnection> connections_idem;
-    PP<RBMConnection> connections_idem_t;
+    //! Connection from input_layer to hidden_layer
+    PP<RBMConnection> input_connections;
 
-    //! The weights of the connections between the RBM hidden and input layers.
-    //! It is the transpose "connections".
-    PP<RBMMatrixTransposeConnection> connections_transpose;
-    PP<RBMMatrixTransposeConnection> connections_transpose_copy;
+    //! Connection from input_layer to hidden_layer
+    TVec< PP<RBMConnection> > target_connections;
 
     //#####  Public Learnt Options  ###########################################
 
     //! Size of the input layer
-    int input_size;
+    int input_layer_size;
 
-    //! Size of the target layer
-    int target_size;
-
     //! Size of each target layers
     TVec<int> target_layers_size;
 
@@ -157,7 +124,7 @@
     TVec<int> input_symbol_sizes;
     
     //! Number of symbols for each symbolic field of train_set
-    Mat target_symbol_sizes;
+    TVec< TVec<int> > target_symbol_sizes;
     
     //#####  Not Options  #####################################################
 
@@ -213,26 +180,7 @@
     //! Clamps the visible units based on an input vector
     void clamp_visible_units(const Vec& input) const;
 
-    //! Updates the RBM parameters in the rbm training phase,
-    //! after the visible units have been clamped.
-    //! Outputs the negative log-likelihood of the visible training example
-    //! given the down phase expectation of the visible unit.
-    real rbm_update();
-
-    //! Updates the dynamic connections in the dynamic training
-    //! phase, after the visible units have been clamped
-    //! Outputs the negative log-likelihood of the hidden representation
-    //! of training example t given a sample from the hidden representation
-    //! of training example t-1.
-    real dynamic_connections_update();
-
     //! Updates both the RBM parameters and the 
-    //! dynamic connections in the fine tuning phase,
-    //! after the visible units have been clamped
-    real fine_tuning_update();
-
-
-    //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,
     //! after the visible units have been clamped
     void recurrent_update();



From larocheh at mail.berlios.de  Wed May 14 22:34:04 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 14 May 2008 22:34:04 +0200
Subject: [Plearn-commits] r9000 - trunk/plearn_learners_experimental
Message-ID: <200805142034.m4EKY4bs023435@sheep.berlios.de>

Author: larocheh
Date: 2008-05-14 22:34:04 +0200 (Wed, 14 May 2008)
New Revision: 9000

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
..


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-14 20:08:50 UTC (rev 8999)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-14 20:34:04 UTC (rev 9000)
@@ -59,6 +59,12 @@
 // - have a proper normalization of costs
 // - output one cost per target + weighted sum of all costs
 // - make sure gradient descent is proper (change some vectors into matrices, etc.)
+// - make sure end_of_sequence_symbol is used appropriately
+// - make sure declareOption includes everything, including saved variable
+// - implement deepcopy appropriately
+// - recurrent_nstages doesn't exist anymore
+// - verify use of mask is proper
+// - verify code works with and without hidden_layer2
 
 namespace PLearn {
 using namespace std;
@@ -70,114 +76,99 @@
     );
 
 DynamicallyLinkedRBMsModel::DynamicallyLinkedRBMsModel() :
-    rbm_learning_rate( 0.01 ),
-    dynamic_learning_rate( 0.01 ),
-    visible_dynamic_learning_rate( 0.01 ),
-    fine_tuning_learning_rate( 0.01 ),
+    //rbm_learning_rate( 0.01 ),
     recurrent_net_learning_rate( 0.01),
-    untie_weights( false ),
-    taillePart( 0 ),
-    isRegression( 0 ),
-    rbm_nstages( 0 ),
-    dynamic_nstages( 0 ),
-    fine_tuning_nstages( 0 ),
-    recurrent_nstages(0),
-    visible_connections_option(0),
-    visible_size( -1 )
+    use_target_layers_masks( false ),
+    end_of_sequence_symbol( -1000 ),
+    input_reconstruction_weight( 0. ),
+    input_layer_size( -1 )    
+    //rbm_nstages( 0 ),
 {
     random_gen = new PRandom();
 }
 
 void DynamicallyLinkedRBMsModel::declareOptions(OptionList& ol)
 {
-    declareOption(ol, "rbm_learning_rate", &DynamicallyLinkedRBMsModel::rbm_learning_rate,
-                  OptionBase::buildoption,
-                  "The learning rate used during RBM contrastive "
-                  "divergence learning phase");
+//    declareOption(ol, "rbm_learning_rate", &DynamicallyLinkedRBMsModel::rbm_learning_rate,
+//                  OptionBase::buildoption,
+//                  "The learning rate used during RBM contrastive "
+//                  "divergence learning phase.\n");
 
-    declareOption(ol, "dynamic_learning_rate", &DynamicallyLinkedRBMsModel::dynamic_learning_rate,
+    declareOption(ol, "recurrent_net_learning_rate", 
+                  &DynamicallyLinkedRBMsModel::recurrent_net_learning_rate,
                   OptionBase::buildoption,
-                  "The learning rate used during the dynamic links "
-                  "learning phase");
+                  "The learning rate used during the recurrent phase.\n");
 
-    declareOption(ol, "visible_dynamic_learning_rate", &DynamicallyLinkedRBMsModel::visible_dynamic_learning_rate,
-                  OptionBase::buildoption,
-                  "The learning rate used during the visible dynamic links "
-                  "learning phase");
+//    declareOption(ol, "rbm_nstages", &DynamicallyLinkedRBMsModel::rbm_nstages,
+//                  OptionBase::buildoption,
+//                  "Number of epochs for rbm phase.\n");
 
-    declareOption(ol, "fine_tuning_learning_rate", &DynamicallyLinkedRBMsModel::fine_tuning_learning_rate,
-                  OptionBase::buildoption,
-                  "The learning rate used during the fine tuning "
-                  "phase");
 
-    declareOption(ol, "recurrent_net_learning_rate", &DynamicallyLinkedRBMsModel::recurrent_net_learning_rate,
+    declareOption(ol, "input_layer", &DynamicallyLinkedRBMsModel::input_layer,
                   OptionBase::buildoption,
-                  "The learning rate used during the recurrent phase");
+                  "The input layer of the model.\n");
 
-    declareOption(ol, "untie_weights", &DynamicallyLinkedRBMsModel::untie_weights,
+    declareOption(ol, "target_layers", &DynamicallyLinkedRBMsModel::target_layers,
                   OptionBase::buildoption,
-                  "Indication to untie weights in recurrent net");
+                  "The target layers of the model.\n");
 
-    declareOption(ol, "taillePart", &DynamicallyLinkedRBMsModel::taillePart,
+    declareOption(ol, "hidden_layer", &DynamicallyLinkedRBMsModel::hidden_layer,
                   OptionBase::buildoption,
-                  "Indication the size of the partition");
+                  "The hidden layer of the model.\n");
 
-    declareOption(ol, "isRegression", &DynamicallyLinkedRBMsModel::isRegression,
+    declareOption(ol, "hidden_layer2", &DynamicallyLinkedRBMsModel::hidden_layer2,
                   OptionBase::buildoption,
-                  "Indication if the model is used for regression");
+                  "The second hidden layer of the model (optional).\n");
 
-    declareOption(ol, "rbm_nstages", &DynamicallyLinkedRBMsModel::rbm_nstages,
+    declareOption(ol, "dynamic_connections", 
+                  &DynamicallyLinkedRBMsModel::dynamic_connections,
                   OptionBase::buildoption,
-                  "Number of epochs for rbm phase");
+                  "The RBMConnection between the first hidden layers, "
+                  "through time.\n");
 
-    declareOption(ol, "dynamic_nstages", &DynamicallyLinkedRBMsModel::dynamic_nstages,
+    declareOption(ol, "hidden_connections", 
+                  &DynamicallyLinkedRBMsModel::hidden_connections,
                   OptionBase::buildoption,
-                  "Number of epochs for dynamic phase");
+                  "The RBMConnection between the first and second "
+                  "hidden layers (optional).\n");
 
-    declareOption(ol, "fine_tuning_nstages", &DynamicallyLinkedRBMsModel::fine_tuning_nstages,
+    declareOption(ol, "input_connections", 
+                  &DynamicallyLinkedRBMsModel::input_connections,
                   OptionBase::buildoption,
-                  "Number of epochs for fine tuning phase");
+                  "The RBMConnection from input_layer to hidden_layer.\n");
 
-    declareOption(ol, "recurrent_nstages", &DynamicallyLinkedRBMsModel::recurrent_nstages,
+    declareOption(ol, "target_connections", 
+                  &DynamicallyLinkedRBMsModel::target_connections,
                   OptionBase::buildoption,
-                  "Number of epochs for the recurrent phase");
+                  "The RBMConnection from input_layer to hidden_layer.\n");
 
-    declareOption(ol, "input_layer", &DynamicallyLinkedRBMsModel::input_layer,
+    /*
+    declareOption(ol, "", 
+                  &DynamicallyLinkedRBMsModel::,
                   OptionBase::buildoption,
-                  "The input layer of the model");
+                  "");
+    */
 
-    declareOption(ol, "target_layer", &DynamicallyLinkedRBMsModel::target_layer,
-                  OptionBase::buildoption,
-                  "The target layer of the model");
 
-    declareOption(ol, "hidden_layer", &DynamicallyLinkedRBMsModel::hidden_layer,
-                  OptionBase::buildoption,
-                  "The hidden layer of the RBMs. Its size must be set, and will\n"
-                  "correspond to the RBMs hidden layer size.");
+    declareOption(ol, "input_layer_size", 
+                  &DynamicallyLinkedRBMsModel::input_layer_size,
+                  OptionBase::learntoption,
+                  "Size of the input layer.\n");
 
-    declareOption(ol, "connections", &DynamicallyLinkedRBMsModel::connections,
-                  OptionBase::buildoption,
-                  "The weights of the connections between the RBM "
-                  "visible and hidden layers");
+    declareOption(ol, "target_layers_size", 
+                  &DynamicallyLinkedRBMsModel::target_layers_size,
+                  OptionBase::learntoption,
+                  "Size of each target layers.\n");
 
-    declareOption(ol, "dynamic_connections", &DynamicallyLinkedRBMsModel::dynamic_connections,
-                  OptionBase::buildoption,
-                  "OnlineLearningModule corresponding to dynamic links "
-                  "between RBMs' hidden layers");
+    declareOption(ol, "input_symbol_sizes", 
+                  &DynamicallyLinkedRBMsModel::input_symbol_sizes,
+                  OptionBase::learntoption,
+                  "Number of symbols for each symbolic field of train_set.\n");
 
-    declareOption(ol, "visible_connections", &DynamicallyLinkedRBMsModel::visible_connections,
-                  OptionBase::buildoption,
-                  "OnlineLearningModule corresponding to dynamic links "
-                  "between RBMs' visible layers");
-
-    declareOption(ol, "visible_connections_option", &DynamicallyLinkedRBMsModel::visible_connections_option,
-                  OptionBase::buildoption,
-                  "Option for the onlineLearningModule corresponding to dynamic links "
-                  "between RBMs' visible layers");
-
-    declareOption(ol, "dynamic_connections_copy", &DynamicallyLinkedRBMsModel::dynamic_connections_copy,
+    declareOption(ol, "target_symbol_sizes", 
+                  &DynamicallyLinkedRBMsModel::target_symbol_sizes,
                   OptionBase::learntoption,
-                  "Independent copy of dynamic connections");
+                  "Number of symbols for each symbolic field of train_set.\n");
 
     /*
     declareOption(ol, "", &DynamicallyLinkedRBMsModel::,
@@ -206,15 +197,14 @@
 
     if(train_set)
     {
-
         PLASSERT( target_layers_weights.length() == target_layers.length() );
 
-
         // Parsing symbols in input
         input_layer_size = 0;
         input_symbol_sizes.resize(0);
         PP<Dictionary> dict;
-        int inputsize_without_masks = inputsize() - ( use_target_layers_masks ? targetsize() : 0 );
+        int inputsize_without_masks = inputsize() 
+            - ( use_target_layers_masks ? targetsize() : 0 );
         for(int i=0; i<inputsize_without_masks; i++)
         {
             dict = train_set->getDictionary(i);
@@ -277,6 +267,8 @@
             PLERROR("DynamicallyLinkedRBMsModel::build_(): target layers "
                     "does not cover all targets.");
 
+
+        // Building weights and layers
         if( !input_layer->random_gen )
         {
             input_layer->random_gen = random_gen;
@@ -298,6 +290,16 @@
         }
         input_connections->build();
 
+
+        dynamic_connections->down_size = hidden_layer->size;
+        dynamic_connections->up_size = hidden_layer->size;
+        if( !dynamic_connections->random_gen )
+        {
+            dynamic_connections->random_gen = random_gen;
+            dynamic_connections->forget();
+        }
+        dynamic_connections->build();
+
         if( hidden_layer2 )
         {
             if( !hidden_layer2->random_gen )
@@ -398,14 +400,16 @@
 
     // deepCopyField(, copies);
 
-    //PLERROR("DynamicallyLinkedRBMsModel::makeDeepCopyFromShallowCopy(): "
-    //"not implemented yet");
+    PLERROR("DynamicallyLinkedRBMsModel::makeDeepCopyFromShallowCopy(): "
+    "not implemented yet");
 }
 
 
 int DynamicallyLinkedRBMsModel::outputsize() const
 {
-    int out_size = 1; // Not really...
+    int out_size = 0;
+    for( int i=0; i<target_layers.length(); i++ )
+        out_size += target_layers[i]->size;
     return out_size;
 }
 
@@ -422,13 +426,23 @@
       - stage = 0
     */
     inherited::forget();
-    
-    visible_layer->forget();
+
+    input_layer->forget();
     hidden_layer->forget();
-    connections->forget();
-    dynamic_connections->forget();
-    visible_connections->forget();
+    input_connections->forget();
+    dynamic_connections-forget();
+    if( hidden_layer2 )
+    {
+        hidden_layer2->forget();
+        hidden_connections->forget();
+    }
 
+    for( int i=0; i<target_layers.length(); i++ )
+    {
+        target_layers[i]->forget();
+        target_connections[i]->forget();
+    }
+
     stage = 0;
 }
 

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-14 20:08:50 UTC (rev 8999)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-14 20:34:04 UTC (rev 9000)
@@ -75,9 +75,6 @@
     ////! Number of epochs for rbm phase
     //int rbm_nstages;
 
-    //! The target layers of the RBMs
-    TVec< PP<RBMLayer> > target_layers;
-
     //! The training weights of each target layers
     Vec target_layers_weights;
     
@@ -91,13 +88,16 @@
     //! The weight of an additional input reconstruction error
     real input_reconstruction_weight;
 
-    //! The visible layer of the RBMs
+    //! The input layer of the model
     TVec<RBMLayer> input_layer;
 
-    //! The hidden layer of the RBMs
+    //! The target layers of the model
+    TVec< PP<RBMLayer> > target_layers;
+
+    //! The hidden layer of the model
     PP<RBMLayer> hidden_layer;
 
-    //! The second hidden layer of the RBMs (optional) 
+    //! The second hidden layer of the model (optional) 
     PP<RBMLayer> hidden_layer2;
 
     //! The RBMConnection between the first hidden layers, through time
@@ -106,10 +106,10 @@
     //! The RBMConnection between the first and second hidden layers (optional)
     PP<RBMConnection> hidden_connections;
 
-    //! Connection from input_layer to hidden_layer
+    //! The RBMConnection from input_layer to hidden_layer
     PP<RBMConnection> input_connections;
 
-    //! Connection from input_layer to hidden_layer
+    //! The RBMConnection from input_layer to hidden_layer
     TVec< PP<RBMConnection> > target_connections;
 
     //#####  Public Learnt Options  ###########################################



From lamblin at mail.berlios.de  Wed May 14 23:00:51 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 14 May 2008 23:00:51 +0200
Subject: [Plearn-commits] r9001 - trunk/plearn_learners/online
Message-ID: <200805142100.m4EL0pJ6025532@sheep.berlios.de>

Author: lamblin
Date: 2008-05-14 23:00:51 +0200 (Wed, 14 May 2008)
New Revision: 9001

Modified:
   trunk/plearn_learners/online/CombiningCostsModule.h
Log:
Forgotten change.


Modified: trunk/plearn_learners/online/CombiningCostsModule.h
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.h	2008-05-14 20:34:04 UTC (rev 9000)
+++ trunk/plearn_learners/online/CombiningCostsModule.h	2008-05-14 21:00:51 UTC (rev 9001)
@@ -157,10 +157,6 @@
     //! Stores the output values of the sub_costs
     mutable Vec sub_costs_values;
 
-    //! Stores all the costs of one sub_cost
-    mutable Vec sub_cost_all;
-    mutable Mat sub_cost_all_m;
-
     //! Stores mini-batch outputs values of sub costs.
     mutable Mat sub_costs_mbatch_values;
 



From lamblin at mail.berlios.de  Wed May 14 23:01:50 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 14 May 2008 23:01:50 +0200
Subject: [Plearn-commits] r9002 - trunk/python_modules/plearn/pymake
Message-ID: <200805142101.m4EL1ogr025610@sheep.berlios.de>

Author: lamblin
Date: 2008-05-14 23:01:49 +0200 (Wed, 14 May 2008)
New Revision: 9002

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
First draft of option allowing to dispatch compile jobs with DBI.
The goal is to parallelize compilation on mammouth.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-05-14 21:00:51 UTC (rev 9001)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-05-14 21:01:49 UTC (rev 9002)
@@ -1045,6 +1045,28 @@
             if verbose >= 2:
                 print '[ %d FILES TO RETRY, BUT NO MORE TRIES AVAILABLE ]' % len(ccfiles_to_retry)
 
+
+def dbi_parallel_compile(files_to_compile, dbi_mode):
+
+    #FIXME: Prepare something?
+    commands = []
+
+    for ccfile in files_to_compile:
+        ccfile.make_objs_dir() # make sure the OBJS dir exists, otherwise the command will fail
+        # Remove .o file if it exists
+        try:
+            os.remove(ccfile.corresponding_ofile)
+        except OSError:
+            pass
+
+        #Command to execute: "'cd " + ccfile.filedir + "; " + self.compile_command(nice_value) + "; echo $?'"
+        commands.append("'cd " + ccfile.filedir + "; " + self.compile_command(nice_value) + "; echo $?'")
+
+    from plearn.parallel.dbi import DBI
+    jobs = DBI(commands, dbi_mode)
+    jobs.run()
+    jobs.wait()
+
 def win32_parallel_compile(files_to_compile):
     """files_to_compile is a list of FileInfo of .cc files"""
 
@@ -1981,7 +2003,7 @@
 
         return compileroptions
 
-    def compile_command(self, nice_value):
+    def compile_command(self, nice_value=0):
         """returns the command line necessary to compile this .cc file"""
 
         compiler = default_compiler
@@ -2633,6 +2655,20 @@
         sys.exit(100)
 
     ##  Options that will not affect the final compiled file
+    # Use the DBI interface instead of compiling locally or directly using SSH
+    dbi_mode = None
+    for option in optionargs:
+        if option.startswith('dbi'):
+            optionargs.remove(option)
+            #FIXME: is default behaviour if no dbi mode is specified?
+            if len(option) > 3 and option[3]=='=':
+                dbi_mode = option[4:]
+            else:
+                print dedent('''\
+                        Syntax of \'dbi\' option is \'-dbi=<dbi_mode>\',
+                        where <dbi_mode> is one of (...)''')
+                #FIXME: is there a way to get a list of supported modes?
+
     # force recompilation of everything?
     if 'force' in optionargs:
         force_recompilation = 1
@@ -2873,6 +2909,9 @@
 
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())
+            else if dbi_mode is not None:
+                #TODO: use ofiles here?
+                dbi_parallel_compile(ccfiles_to_compile.keys(), dbi_mode)
             else:
                 ofiles_to_copy = get_ofiles_to_copy(executables_to_link.keys())
                 ofiles_to_copy = [x for x in ofiles_to_copy if x not in [y.corresponding_ofile for y in ccfiles_to_compile.keys()]]



From lamblin at mail.berlios.de  Wed May 14 23:23:21 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 14 May 2008 23:23:21 +0200
Subject: [Plearn-commits] r9003 - trunk/python_modules/plearn/pymake
Message-ID: <200805142123.m4ELNL9Y028193@sheep.berlios.de>

Author: lamblin
Date: 2008-05-14 23:23:21 +0200 (Wed, 14 May 2008)
New Revision: 9003

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Oops.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-05-14 21:01:49 UTC (rev 9002)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-05-14 21:23:21 UTC (rev 9003)
@@ -1059,8 +1059,8 @@
         except OSError:
             pass
 
-        #Command to execute: "'cd " + ccfile.filedir + "; " + self.compile_command(nice_value) + "; echo $?'"
-        commands.append("'cd " + ccfile.filedir + "; " + self.compile_command(nice_value) + "; echo $?'")
+        #Command to execute: "'cd " + ccfile.filedir + "; " + ccfile.compile_command() + "; echo $?'"
+        commands.append("'cd " + ccfile.filedir + "; " + ccfile.compile_command() + "; echo $?'")
 
     from plearn.parallel.dbi import DBI
     jobs = DBI(commands, dbi_mode)
@@ -2909,7 +2909,7 @@
 
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())
-            else if dbi_mode is not None:
+            elif dbi_mode is not None:
                 #TODO: use ofiles here?
                 dbi_parallel_compile(ccfiles_to_compile.keys(), dbi_mode)
             else:



From nouiz at mail.berlios.de  Thu May 15 02:15:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 15 May 2008 02:15:39 +0200
Subject: [Plearn-commits] r9004 - trunk/plearn/vmat
Message-ID: <200805150015.m4F0Fdvp009408@sheep.berlios.de>

Author: nouiz
Date: 2008-05-15 02:15:36 +0200 (Thu, 15 May 2008)
New Revision: 9004

Modified:
   trunk/plearn/vmat/FilteredVMatrix.cc
Log:
bugfix


Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2008-05-14 21:23:21 UTC (rev 9003)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2008-05-15 00:15:36 UTC (rev 9004)
@@ -138,7 +138,7 @@
         rm(idxfname);       // force remove it
         indexes.open(idxfname.absolute(), true);
         for (int i = 0; i < mem_indices.length(); i++)
-            indexes.append(i);
+            indexes.append(mem_indices[i]);
         indexes.close();
         indexes.open(idxfname.absolute());
         mem_indices = TVec<int>();  // Free memory.



From larocheh at mail.berlios.de  Thu May 15 02:49:20 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 15 May 2008 02:49:20 +0200
Subject: [Plearn-commits] r9005 - trunk/plearn_learners_experimental
Message-ID: <200805150049.m4F0nK7g013918@sheep.berlios.de>

Author: larocheh
Date: 2008-05-15 02:49:19 +0200 (Thu, 15 May 2008)
New Revision: 9005

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
getting closer!


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-15 00:15:36 UTC (rev 9004)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-15 00:49:19 UTC (rev 9005)
@@ -57,7 +57,7 @@
 // Problems to have in mind:
 // 
 // - have a proper normalization of costs
-// - output one cost per target + weighted sum of all costs
+// - output one cost per target 
 // - make sure gradient descent is proper (change some vectors into matrices, etc.)
 // - make sure end_of_sequence_symbol is used appropriately
 // - make sure declareOption includes everything, including saved variable
@@ -65,6 +65,7 @@
 // - recurrent_nstages doesn't exist anymore
 // - verify use of mask is proper
 // - verify code works with and without hidden_layer2
+// - do proper resize of recurrent internal variables
 
 namespace PLearn {
 using namespace std;
@@ -79,9 +80,7 @@
     //rbm_learning_rate( 0.01 ),
     recurrent_net_learning_rate( 0.01),
     use_target_layers_masks( false ),
-    end_of_sequence_symbol( -1000 ),
-    input_reconstruction_weight( 0. ),
-    input_layer_size( -1 )    
+    end_of_sequence_symbol( -1000 )
     //rbm_nstages( 0 ),
 {
     random_gen = new PRandom();
@@ -104,6 +103,23 @@
 //                  "Number of epochs for rbm phase.\n");
 
 
+    declareOption(ol, "target_layers_weights", 
+                  &DynamicallyLinkedRBMsModel::target_layers_weights,
+                  OptionBase::buildoption,
+                  "The training weights of each target layers.\n");
+
+    declareOption(ol, "use_target_layers_masks", 
+                  &DynamicallyLinkedRBMsModel::use_target_layers_masks,
+                  OptionBase::buildoption,
+                  "Indication that a mask indicating which target to predict\n"
+                  "is present in the input part of the VMatrix dataset.\n");
+
+    declareOption(ol, "end_of_sequence_symbol", 
+                  &DynamicallyLinkedRBMsModel::end_of_sequence_symbol,
+                  OptionBase::buildoption,
+                  "Value of the first input component for end-of-sequence "
+                  "delimiter.\n");
+
     declareOption(ol, "input_layer", &DynamicallyLinkedRBMsModel::input_layer,
                   OptionBase::buildoption,
                   "The input layer of the model.\n");
@@ -150,16 +166,12 @@
     */
 
 
-    declareOption(ol, "input_layer_size", 
-                  &DynamicallyLinkedRBMsModel::input_layer_size,
+    declareOption(ol, "target_layers_n_of_target_elements", 
+                  &DynamicallyLinkedRBMsModel::target_layers_n_of_target_elements,
                   OptionBase::learntoption,
-                  "Size of the input layer.\n");
+                  "Number of elements in the target part of a VMatrix associated\n"
+                  "to each target layer.\n");
 
-    declareOption(ol, "target_layers_size", 
-                  &DynamicallyLinkedRBMsModel::target_layers_size,
-                  OptionBase::learntoption,
-                  "Size of each target layers.\n");
-
     declareOption(ol, "input_symbol_sizes", 
                   &DynamicallyLinkedRBMsModel::input_symbol_sizes,
                   OptionBase::learntoption,
@@ -200,7 +212,7 @@
         PLASSERT( target_layers_weights.length() == target_layers.length() );
 
         // Parsing symbols in input
-        input_layer_size = 0;
+        int input_layer_size = 0;
         input_symbol_sizes.resize(0);
         PP<Dictionary> dict;
         int inputsize_without_masks = inputsize() 
@@ -230,8 +242,9 @@
 
         // Parsing symbols in target
         int tar_layer = 0;
+        int tar_layer_size = 0;
         target_symbol_sizes.resize(target_layers.length());
-        target_layers_size.clear();
+        target_layers_n_of_target_elements.clear();
         for( int tar=0; tar<targetsize(); tar++)
         {
             if( tar_layer > target_layers.length() )
@@ -250,17 +263,21 @@
                             "of field %d is empty", i);
 
                 target_symbol_sizes[tar_layer].push_back(dict->size());
-                // Adjust size to include one-hot vector
-                target_layers_size[tar_layer] += dict->size();
+                target_layers_n_of_target_elements[tar_layer]++;
+                tar_layer_size += dict->size();
             }
             else
             {
                 target_symbol_sizes[tar_layer].push_back(-1);
-                target_layers_size[tar_layer] ++
+                target_layers_n_of_target_elements[tar_layer]++;
+                tar_layer_size++;
             }
 
-            if( target_layers[tar_layer]->size == target_layers_size[tar_layer] )
+            if( target_layers[tar_layer]->size == tar_layer_size )
+            {
                 tar_layer++;
+                tar_layer_size = 0;
+            }
         }
 
         if( tar_layer != target_layers.length() )
@@ -415,16 +432,6 @@
 
 void DynamicallyLinkedRBMsModel::forget()
 {
-    //! (Re-)initialize the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!)
-    /*!
-      A typical forget() method should do the following:
-      - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
-      - initialize the learner's parameters, using this random generator
-      - stage = 0
-    */
     inherited::forget();
 
     input_layer->forget();
@@ -450,53 +457,12 @@
 {
     MODULE_LOG << "train() called " << endl;
 
-    // The role of the train method is to bring the learner up to
-    // stage==nstages, updating train_stats with training costs measured
-    // on-line in the process.
-
-    /* TYPICAL CODE:
-
-    static Vec input;  // static so we don't reallocate memory each time...
-    static Vec target; // (but be careful that static means shared!)
-    input.resize(inputsize());    // the train_set's inputsize()
-    target.resize(targetsize());  // the train_set's targetsize()
-    real weight;
-
-    // This generic PLearner method does a number of standard stuff useful for
-    // (almost) any learner, and return 'false' if no training should take
-    // place. See PLearner.h for more details.
-    if (!initTrain())
-        return;
-
-    while(stage<nstages)
-    {
-        // clear statistics of previous epoch
-        train_stats->forget();
-
-        //... train for 1 stage, and update train_stats,
-        // using train_set->getExample(input, target, weight)
-        // and train_stats->update(train_costs)
-
-        ++stage;
-        train_stats->finalize(); // finalize statistics for this epoch
-    }
-    */
-
-    if(fine_tuning_nstages >= 0){   
-        nstages = rbm_nstages + dynamic_nstages + fine_tuning_nstages;
-    }
-    if(recurrent_nstages >= 0){   
-        nstages = rbm_nstages + dynamic_nstages + fine_tuning_nstages + recurrent_nstages;
-    }
-
-    if(visible_size < 0)
-        PLERROR("DynamicallyLinkedRBMsModel::train(): \n"
-                "build() must be called before train()");                
-
     Vec input( inputsize() );
-    Vec target( targetsize() );// Unused
+    Vec target( targetsize() );
     real weight = 0; // Unused
     Vec train_costs( getTrainCostNames().length() );
+    train_costs.clear();
+    TVec<int> train_n_items( getTrainCostNames().length() );
 
     if( !initTrain() )
     {
@@ -511,32 +477,16 @@
 
 
     /***** RBM training phase *****/
-    if(stage < rbm_nstages)
-    {
-    }
+//    if(rbm_stage < rbm_nstages)
+//    {
+//    }
 
 
-    /***** dynamic phase training  *****/
-
-    if(stage < rbm_nstages +  dynamic_nstages)
-    {
-    }  
-
-
-    /***** fine-tuning *****/
-    if( stage >= nstages )
-        return;
-
-    if(stage < rbm_nstages +  dynamic_nstages + fine_tuning_nstages )
-    {
-    }
-
-
     /***** Recurrent phase *****/
     if( stage >= nstages )
         return;
 
-    if(stage >= rbm_nstages +  dynamic_nstages + fine_tuning_nstages)
+    if( stage < nstages )
     {        
 
         MODULE_LOG << "Training the whole model" << endl;
@@ -553,36 +503,14 @@
             pb = new ProgressBar( "Recurrent training phase of "+classname(),
                                   end_stage - init_stage );
 
-        previous_hidden_layer.resize(hidden_layer->size);
-        dynamic_connections->setLearningRate( recurrent_net_learning_rate );
-        visible_layer->setLearningRate( recurrent_net_learning_rate );
-        connections->setLearningRate( recurrent_net_learning_rate );
-        connections_transpose->setLearningRate( recurrent_net_learning_rate );
-        if(dynamic_connections_copy)
-            dynamic_connections_copy->setLearningRate( recurrent_net_learning_rate );
-        if(connections_transpose_copy)
-            connections_transpose_copy->setLearningRate( recurrent_net_learning_rate );
+        setLearningRate( recurrent_net_learning_rate );
 
-        real mean_cost = 0;
         int ith_sample_in_sequence = 0;
-        int nb_oov = 0;
-        
-        RBMMixedLayer* p_visible_layer = dynamic_cast<RBMMixedLayer*>((RBMLayer*)visible_layer);
-        target_layer = p_visible_layer->sub_layers[3];
-        //target_layer = (PLearn::PP<PLearn::RBMMixedLayer>)visible_layer;
-        //test_layer = target_layer.sub_layers(1);
+        int inputsize_without_masks = inputsize() 
+            - ( use_target_layers_masks ? targetsize() : 0 );
+        int sum_target_elements = 0;
         while(stage < end_stage)
         {
-            if(untie_weights && 
-               stage == rbm_nstages + dynamic_nstages + fine_tuning_nstages)
-            {
-                
-                CopiesMap map;
-                dynamic_connections_copy = dynamic_connections->deepCopy(map);
-
-                //CopiesMap map2;
-                //connections_transpose_copy = connections_transpose->deepCopy(map2);
-                //connections_transpose = connections_transpose_copy;  
 /*
                 TMat<real> U,V;//////////crap James
                 TVec<real> S;
@@ -595,174 +523,193 @@
                 S.fill(-0.5);
                 productScaleAcc(dynamic_connections->bias,dynamic_connections->weights,S,1,0);
 */
-            }
-            
+            train_costs.clear();
+            train_n_items.clear();
             for(int sample=0 ; sample<train_set->length() ; sample++ )
             {
-
                 train_set->getExample(sample, input, target, weight);
-                
 
-                hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                hidden_activations_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                hidden2_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                hidden2_activations_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                input_prediction_list.resize(
-                    ith_sample_in_sequence+1,visible_layer->size);
-                input_prediction_activations_list.resize(
-                    ith_sample_in_sequence+1,visible_layer->size);
-                input_list.resize(ith_sample_in_sequence+1,visible_layer->size);
-                target_list.resize(ith_sample_in_sequence+1,target_layer->size);
-                nll_list.resize(ith_sample_in_sequence+1);
-                
-               
-
-                //if(train_set->getString(sample,0) == "<oov>")
-                if(train_set->get(sample,0) == 8)
+                if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
                 {
-
-                    input_list(ith_sample_in_sequence) << previous_input;
-                    target_list(ith_sample_in_sequence) << previous_target;
-                    connections->setAsDownInput( previous_input );
-                    hidden_layer->getAllActivations( connections_idem );
-                    hidden_layer->computeExpectation();
-                    previous_hidden_layer << hidden_layer->expectation;
-                    previous_hidden_layer_activation << hidden_layer->activation;
-                    hidden_list(ith_sample_in_sequence) << previous_hidden_layer;
-                    hidden_activations_list(ith_sample_in_sequence) 
-                        << previous_hidden_layer_activation;
-                    hidden2_list(ith_sample_in_sequence) << hidden_layer->expectation;
-                    hidden2_activations_list(ith_sample_in_sequence) << 
-                        hidden_layer->activation;
-                    input_prediction_list(ith_sample_in_sequence) << 
-                        visible_layer->expectation;
-                    input_prediction_activations_list(ith_sample_in_sequence) << 
-                        visible_layer->activation;
-                    //cout << "hidden_expectation crap james :" <<  hidden_list << endl;
                     //update
-                    nb_oov++;
                     recurrent_update();
                     
                     ith_sample_in_sequence = 0;
                     hidden_list.clear();
+                    hidden_activations_list.clear();
                     hidden2_list.clear();
-                    input_prediction_list.clear();
+                    hidden2_activations_list.clear();
+                    target_prediction_list.clear();
+                    target_prediction_activations_list.clear();
                     input_list.clear();
-                    target_list.clear();
+                    targets_list.clear();
                     nll_list.clear();
+                    masks_list.clear();
                     continue;
                 }
 
-         
+                // Resize internal variables
+                hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
+                hidden_activations_list.resize(ith_sample_in_sequence+1,
+                                               hidden_layer->size);
+                if( hidden_layer2 )
+                {
+                    hidden2_list.resize(ith_sample_in_sequence+1,
+                                        hidden_layer2->size);
+                    hidden2_activations_list.resize(ith_sample_in_sequence+1,
+                                                    hidden_layer2->size);
+                }
+                 
+                input_list.resize(ith_sample_in_sequence+1,input_layer->size);
 
+                targets_list.resize( target_layers.length() );
+                target_prediction_list.resize( target_layers.length() );
+                target_prediction_activations_list.resize( target_layers.length() );
+                for( int tar=0; tar < target_layers.length(); tar++ )
+                {
+                    targets_list[tar].resize( ith_sample_in_sequence+1,
+                                              target_layers[tar]->size );
+                    target_prediction_list[tar].resize(
+                        ith_sample_in_sequence+1, target_layers[tar]->size);
+                    target_prediction_activations_list[tar].resize(
+                        ith_sample_in_sequence+1, target_layers[tar]->size);
 
-                if(isRegression)
-                    visible_layer->setExpectation(input);
-                else
-                    clamp_visible_units(input);
-                
-
-                if(ith_sample_in_sequence > 0)
+                }
+                nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
+                if( use_target_layers_masks )
                 {
-                   
-                    input_list(ith_sample_in_sequence) << previous_input;
-                    target_list(ith_sample_in_sequence) << previous_target;
-                    //h*_{t-1}
-                    //////////////////////////////////
-                    dynamic_connections->fprop(previous_hidden_layer, cond_bias);
-                    hidden_layer->setAllBias(cond_bias); //**************************
+                    masks_list.resize( target_layers.length() );
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                        masks_list[tar].resize( ith_sample_in_sequence+1,
+                                                target_layers[tar]->size );
+                }
 
+                // Forward propagation
 
-                    //up phase
-                    connections->setAsDownInput( previous_input );
-                    hidden_layer->getAllActivations( connections_idem );
-                    hidden_layer->computeExpectation();
-                    //////////////////////////////////
+                // Fetch right representation for input
+                clamp_units(input.subVec(0,inputsize_without_masks),
+                            input_layer,
+                            input_symbol_sizes);                
+                input_list(ith_sample_in_sequence) << input_layer->expectation;
 
-                    previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
-                    previous_hidden_layer_activation << hidden_layer->activation;
-            
-                    
-                    //h*_{t}
-                    ////////////
-                    if(dynamic_connections_copy)
-                        dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+                // Fetch right representation for target
+                sum_target_elements = 0;
+                for( int tar=0; tar < target_layers.length(); tar++ )
+                {
+                    if( use_target_layers_masks )
+                        clamp_units(target.subVec(
+                                        sum_target_elements,
+                                        target_layers_n_of_target_elements[tar]),
+                                    target_layers[tar],
+                                    target_symbol_sizes[tar],
+                                    input.subVec(
+                                        inputsize_without_masks 
+                                        + sum_target_elements, 
+                                        target_layers_n_of_target_elements[tar]),
+                                    masks_list[tar](ith_sample_in_sequence)
+                            );
                     else
-                        dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-                    
-                    hidden_layer->expectation_is_not_up_to_date();
-                    hidden_layer->computeExpectation();//h_{t}
-                    
+                        clamp_units(target.subVec(
+                                        sum_target_elements,
+                                        target_layers_n_of_target_elements[tar]),
+                                    target_layers[tar],
+                                    target_symbol_sizes[tar]);
+                    sum_target_elements += target_layers_n_of_target_elements[tar];
+                    target_list[tar](ith_sample_in_sequence) << 
+                        target_layers[tar]->expectation;
+                }
+                
+                input_connections->fprop( input_list(ith_sample_in_sequence), 
+                                          hidden_activations_list(ith_sample_in_sequence));
+                
+                if( ith_sample_in_sequence > 0 )
+                {
+                    dynamic_connections->fprop( 
+                        hidden_list(ith_sample_in_sequence-1),
+                        dynamic_activation_contribution );
 
-                    
-             
+                    hidden_activations_list(ith_sample_in_sequence) += 
+                        dynamic_actvation_contribution;
                 }
-                else
+                 
+                hidden_layer->fprop( hidden_activations_list(ith_sample_in_sequence) 
+                                     hidden_list(ith_sample_in_sequence) );
+                 
+                if( hidden_layer2 )
                 {
-                    
-                    input_list(ith_sample_in_sequence).fill(-1);
-                    target_list(ith_sample_in_sequence).fill(-1);
- 
-                    previous_hidden_layer.clear();//h_{t-1}
-                    //previous_hidden_layer.fill(0.5);//**************************crap James
-                    previous_hidden_layer_activation.clear();//h_{t-1}
+                    hidden_connections->fprop( 
+                        hidden_list(ith_sample_in_sequence),
+                        hidden2_activations_list(ith_sample_in_sequence));
 
-                    if(dynamic_connections_copy)
-                        dynamic_connections_copy->fprop( previous_hidden_layer ,
-                                                         hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-                    else
-                        dynamic_connections->fprop(
-                            previous_hidden_layer,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+                    hidden_layer2->fprop( 
+                        hidden2_activations_list(ith_sample_in_sequence) 
+                        hidden2_list(ith_sample_in_sequence) 
+                        );
 
-                    
-                    hidden_layer->expectation_is_not_up_to_date();
-                    hidden_layer->computeExpectation();//h_{t}
-
-                    previous_input.resize(visible_layer->size);
-                    previous_target.resize(target_layer->size);
- 
-
-            
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                    {
+                        targets_connections[tar]->fprop(
+                            hidden2_list(ith_sample_in_sequence),
+                            target_prediction_activations_list[tar](
+                                ith_sample_in_sequence)
+                            );
+                        target_layers[tar]->fprop(
+                            target_prediction_activations_list[tar](
+                                ith_sample_in_sequence),
+                            target_prediction_list[tar](
+                                ith_sample_in_sequence) );
+                        if( use_target_layers_masks )
+                            target_prediction_list[tar]( ith_sample_in_sequence) *= 
+                                masks_list[tar](ith_sample_in_sequence);
+                    }
                 }
+                else
+                {
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                    {
+                        targets_connections[tar]->fprop(
+                            hidden_list(ith_sample_in_sequence),
+                            target_prediction_activations_list[tar](
+                                ith_sample_in_sequence)
+                            );
+                        target_layers[tar]->fprop(
+                            target_prediction_activations_list[tar](
+                                ith_sample_in_sequence),
+                            target_prediction_list[tar](
+                                ith_sample_in_sequence) );
+                        if( use_target_layers_masks )
+                            target_prediction_list[tar]( ith_sample_in_sequence) *= 
+                                masks_list[tar](ith_sample_in_sequence);
+                    }
+                }
 
-               
-                previous_input << visible_layer->expectation;//v_{t-1}
-                previous_target << target_layer->expectation;
-                
-               
+                sum_target_elements = 0;
+                for( int tar=0; tar < target_layers.length(); tar++ )
+                {
+                    target_layers[tar]->activation << 
+                        target_prediction_activations_list[tar](
+                            ith_sample_in_sequence);
+                    target_layers[tar]->setExpectation(
+                        target_prediction_list[tar](
+                            ith_sample_in_sequence));
+                    nll_list(ith_sample_in_sequence,tar) = 
+                        target_layer->fpropNLL( 
+                            targets_list[tar](ith_sample_in_sequence) ); 
+                    train_costs[tar] += nll_list(ith_sample_in_sequence,tar);
 
-                //connections_transpose->setAsDownInput( hidden_layer->expectation );
-                //visible_layer->getAllActivations( connections_idem_t );
-
-                connections->setAsUpInput( hidden_layer->expectation );
-                visible_layer->getAllActivations( connections_idem );
-                visible_layer->computeExpectation();
-
-                if(isRegression){
-                    partition(previous_input.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
-                    partition(previous_input.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
+                    // Normalize by the number of things to predict
+                    if( use_target_layers_masks )
+                    {
+                        train_n_items[tar] += sum(
+                            input.subVec( inputsize_without_masks 
+                                          + sum_target_elements, 
+                                          target_layers_n_of_target_elements[tar]) );
+                        sum_target_elements += 
+                            target_layers_n_of_target_elements[tar];
+                    }
+                    else
+                        train_n_itmes[tar]++;
                 }
-
-                // Copies for backprop
-                hidden_list(ith_sample_in_sequence) << previous_hidden_layer;
-                hidden_activations_list(ith_sample_in_sequence) 
-                    << previous_hidden_layer_activation;
-                hidden2_list(ith_sample_in_sequence) << hidden_layer->expectation;
-                hidden2_activations_list(ith_sample_in_sequence) << 
-                    hidden_layer->activation;
-                input_prediction_list(ith_sample_in_sequence) << 
-                    visible_layer->expectation;
-                input_prediction_activations_list(ith_sample_in_sequence) << 
-                    visible_layer->activation;
-
-                
- 
-                //nll_list[ith_sample_in_sequence] = visible_layer->fpropNLL(previous_input); // / inputsize() ;
-                // real sum_mask = sums(mask);
-                nll_list[ith_sample_in_sequence] = target_layer->fpropNLL(previous_target); // / sum_mask;
-                
-
-                mean_cost += nll_list[ith_sample_in_sequence];
                 ith_sample_in_sequence++;
                
                
@@ -770,11 +717,14 @@
             if( pb )
                 pb->update( stage + 1 - init_stage);
             
+            for(int i=0; i<train_costs.length(); i++)
+                train_costs[i] /= train_n_items[i];
+
             if(verbosity>0)
-                cout << "mean cost at stage " << stage << 
-                    " = " << mean_cost/train_set->length() << endl;
-            mean_cost = 0;
+                cout << "mean costs at stage " << stage << 
+                    " = " << train_costs << endl;
             stage++;
+            train_stats->update(train_costs);
         }    
         if( pb )
         {
@@ -788,43 +738,82 @@
     train_stats->finalize();
 }
 
-void DynamicallyLinkedRBMsModel::partition(TVec<double> part, TVec<double> periode, TVec<double> vel ) const
+}
+
+void DynamicallyLinkedRBMsModel::clamp_units(const Vec& layer_vector,
+                                             PP<RBMLayer> layer,
+                                             TVec<int> symbol_sizes) const
 {
-    for(int i = 0; i<part->size();i++){
-        periode[i] = part[i]*periode[i];
-        vel[i] = part[i]*vel[i];
-
+    int it = 0;
+    int ss = -1;
+    for(int i=0; i<layer_vector.length(); i++)
+    {
+        ss = symbol_sizes[i];
+        // If input is a real ...
+        if(ss < 0) 
+        {
+            layer->expectation[it++] = layer_vector[i];
+        }
+        else // ... or a symbol
+        {
+            // Convert to one-hot vector
+            layer->expectation.subVec(it,ss).clear();
+            layer->expectation[it+round(input[i])] = 1;
+            it += ss;
+        }
     }
-
-
-
+    layer->setExpectation( layer->expectation );
 }
 
-void DynamicallyLinkedRBMsModel::clamp_visible_units(const Vec& input) const
+void DynamicallyLinkedRBMsModel::clamp_units(const Vec& layer_vector,
+                                             PP<RBMLayer> layer,
+                                             TVec<int> symbol_sizes,
+                                             const Vec& original_mask,
+                                             Vec& formated_mask) const
 {
     int it = 0;
     int ss = -1;
-    input_expectation.resize(visible_layer->size);
-    for(int i=0; i<inputsize_; i++)
+    formated_mask.resize( layer->size );
+    PLASSER( original_mask.length() == layer_vector.length() );
+    for(int i=0; i<layer_vector.length(); i++)
     {
         ss = symbol_sizes[i];
         // If input is a real ...
         if(ss < 0) 
         {
-            input_expectation[it++] = input[i];
+            formated_mask[it] = original_mask[i];
+            layer->expectation[it++] = layer_vector[i];
         }
         else // ... or a symbol
         {
             // Convert to one-hot vector
-            input_expectation.subVec(it,ss).clear();
-            input_expectation[it+(int)input[i]] = 1;
+            layer->expectation.subVec(it,ss).clear();
+            formated_mask.subVec(it,ss).fill(original_mask[i]);
+            layer->expectation[it+round(input[i])] = 1;
             it += ss;
         }
     }
-    visible_layer->setExpectation(input_expectation);
+    layer->setExpectation( layer->expectation );
 }
 
+void DynamicallyLinkedRBMsModel::setLearningRate( real the_learning_rate )
+{
+    input_layer->setLearningRate( the_learning_rate );
+    hidden_layer->setLearningRate( the_learning_rate );
+    input_connections->setLearningRate( the_learning_rate );
+    dynamic_connections-forget();
+    if( hidden_layer2 )
+    {
+        hidden_layer2->setLearningRate( the_learning_rate );
+        hidden_connections->setLearningRate( the_learning_rate );
+    }
 
+    for( int i=0; i<target_layers.length(); i++ )
+    {
+        target_layers[i]->setLearningRate( the_learning_rate );
+        target_connections[i]->setLearningRate( the_learning_rate );
+    }
+}
 
 void DynamicallyLinkedRBMsModel::recurrent_update()
 {
@@ -944,18 +933,18 @@
 
 void DynamicallyLinkedRBMsModel::test(VMat testset, PP<VecStatsCollector> test_stats,
                   VMat testoutputs, VMat testcosts)const
-{
-   
+{ 
 
     int len = testset.length();
     Vec input;
     Vec target;
-    Vec bias_tempo;
-    Vec visi_bias_tempo;
     real weight;
 
     Vec output(outputsize());
     Vec costs(nTestCosts());
+    costs.clear();
+    TVec<int> n_items(nTestCosts());
+    n_items.clear();
 
     PP<ProgressBar> pb;
     if (report_progress) 
@@ -967,165 +956,235 @@
         test_stats->update(costs);
     }
 
-
-    int begin = 0;
-    int nb_oov = 0;
     for (int i = 0; i < len; i++)
     {
         testset.getExample(i, input, target, weight);
-      
-       
 
+        if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
+        {                    
+            ith_sample_in_sequence = 0;
+            hidden_list.clear();
+            hidden_activations_list.clear();
+            hidden2_list.clear();
+            hidden2_activations_list.clear();
+            target_prediction_list.clear();
+            target_prediction_activations_list.clear();
+            input_list.clear();
+            targets_list.clear();
+            nll_list.clear();
+            masks_list.clear();
+            continue;
+        }
 
-        //if(testset->getString(i,0) == "<oov>")
-        if(testset->get(i,0) == 8)
+        // Resize internal variables
+        hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
+        hidden_activations_list.resize(ith_sample_in_sequence+1,
+                                       hidden_layer->size);
+        if( hidden_layer2 )
         {
-            begin = 0;
-            nb_oov++;
-            continue;
+            hidden2_list.resize(ith_sample_in_sequence+1,
+                                hidden_layer2->size);
+            hidden2_activations_list.resize(ith_sample_in_sequence+1,
+                                            hidden_layer2->size);
         }
         
-
-
-
-
-        //clamp_visible_units(input);
-        visible_layer->setExpectation(input);
-
-
-
-        if(begin > 0)
+        input_list.resize(ith_sample_in_sequence+1,input_layer->size);
+        
+        targets_list.resize( target_layers.length() );
+        target_prediction_list.resize( target_layers.length() );
+        target_prediction_activations_list.resize( target_layers.length() );
+        for( int tar=0; tar < target_layers.length(); tar++ )
         {
-
-            //h*_{t-1}
-            //////////////////////////////////
-            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
-            hidden_layer->setAllBias(cond_bias); //**************************
-
-            /* if (visible_connections_option){
-                //v*_{t-1} VISIBLE DYNAMIC CONNECTION
-                //////////////////////////////////
-                visible_connections->fprop(previous_input, visi_cond_bias);
-                visible_layer->setAllBias(visi_cond_bias); 
-                }*/
-
-            //up phase
-            connections->setAsDownInput( previous_input );
-            hidden_layer->getAllActivations( connections_idem );
-            hidden_layer->computeExpectation();
-            //////////////////////////////////
-
-            previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
+            targets_list[tar].resize( ith_sample_in_sequence+1,
+                                      target_layers[tar]->size );
+            target_prediction_list[tar].resize(
+                ith_sample_in_sequence+1, target_layers[tar]->size);
+            target_prediction_activations_list[tar].resize(
+                ith_sample_in_sequence+1, target_layers[tar]->size);
             
-
-            //h*_{t}
-            ////////////
-            if(dynamic_connections_copy)
-                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            else                
-                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            hidden_layer->expectation_is_not_up_to_date();
-            hidden_layer->computeExpectation();//h_{t}
-            ///////////
-
-            previous_input << visible_layer->expectation;//v_{t-1}
-            
         }
-        else
+        nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
+        if( use_target_layers_masks )
         {
-            previous_hidden_layer.clear();//h_{t-1}
-            if(dynamic_connections_copy)
-                dynamic_connections_copy->fprop( previous_hidden_layer ,
-                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            masks_list.resize( target_layers.length() );
+            for( int tar=0; tar < target_layers.length(); tar++ )
+                masks_list[tar].resize( ith_sample_in_sequence+1,
+                                        target_layers[tar]->size );
+        }
+        
+        // Forward propagation
+        
+        // Fetch right representation for input
+        clamp_units(input.subVec(0,inputsize_without_masks),
+                    input_layer,
+                    input_symbol_sizes);                
+        input_list(ith_sample_in_sequence) << input_layer->expectation;
+        
+        // Fetch right representation for target
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( use_target_layers_masks )
+                clamp_units(target.subVec(
+                                sum_target_elements,
+                                target_layers_n_of_target_elements[tar]),
+                            target_layers[tar],
+                            target_symbol_sizes[tar],
+                            input.subVec(
+                                inputsize_without_masks 
+                                + sum_target_elements, 
+                                target_layers_n_of_target_elements[tar]),
+                            masks_list[tar](ith_sample_in_sequence)
+                    );
             else
-                dynamic_connections->fprop(previous_hidden_layer,
-                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-
-            //dynamic_connections_copy->fprop(previous_hidden_layer,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            hidden_layer->expectation_is_not_up_to_date();
-            hidden_layer->computeExpectation();//h_{t}
-
-            previous_input.resize(visible_layer->size);
-            previous_input << visible_layer->expectation;
-
-            bias_tempo.resize(hidden_layer->bias.length());
-            bias_tempo << hidden_layer->bias;
-
-
-            /*  if (visible_connections_option){
-
-                /////////VISIBLE DYNAMIC CONNECTION
-                previous_visible_layer.clear();//v_{t-1}
-                visible_connections->fprop(previous_visible_layer,visible_layer->activation);//conection entre v_{t-1} et v_{t}
-                
-                visible_layer->expectation_is_not_up_to_date();
-                visible_layer->computeExpectation();//v_{t}
-                
-                visi_bias_tempo.resize(visible_layer->bias.length());
-                visi_bias_tempo << visible_layer->bias;
-                }*/
+                clamp_units(target.subVec(
+                                sum_target_elements,
+                                target_layers_n_of_target_elements[tar]),
+                            target_layers[tar],
+                            target_symbol_sizes[tar]);
+            sum_target_elements += target_layers_n_of_target_elements[tar];
+            target_list[tar](ith_sample_in_sequence) << 
+                target_layers[tar]->expectation;
+        }
+        
+        input_connections->fprop( input_list(ith_sample_in_sequence), 
+                                  hidden_activations_list(ith_sample_in_sequence));
+        
+        if( ith_sample_in_sequence > 0 )
+        {
+            dynamic_connections->fprop( 
+                hidden_list(ith_sample_in_sequence-1),
+                dynamic_activation_contribution );
             
-            begin++;
+            hidden_activations_list(ith_sample_in_sequence) += 
+                dynamic_actvation_contribution;
         }
-
-
-       
-
-
-
-
-
-
-     
-        //connections_transpose->setAsDownInput( hidden_layer->expectation );
-        //visible_layer->getAllActivations( connections_idem_t );
-
-        connections->setAsUpInput( hidden_layer->expectation );
-        visible_layer->getAllActivations( connections_idem );
-        visible_layer->computeExpectation();
-
-
-        partition(previous_input.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
-        partition(previous_input.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
-
-
-        //costs[0] = visible_layer->fpropNLL(previous_input,(taillePart*3)+14) ;
-        costs[0] = visible_layer->fpropNLL(previous_input) ;
-       
-        hidden_layer->setAllBias(bias_tempo); 
-
-        /////////VISIBLE DYNAMIC CONNECTION
-        /* if (visible_connections_option){
-            visible_layer->setAllBias(visi_bias_tempo); 
-            }*/
-
-        // costs[0] = 0; //nll/nb_de_temps_par_mesure
-
+        
+        hidden_layer->fprop( hidden_activations_list(ith_sample_in_sequence) 
+                             hidden_list(ith_sample_in_sequence) );
+        
+        if( hidden_layer2 )
+        {
+            hidden_connections->fprop( 
+                hidden_list(ith_sample_in_sequence),
+                hidden2_activations_list(ith_sample_in_sequence));
+            
+            hidden_layer2->fprop( 
+                hidden2_activations_list(ith_sample_in_sequence) 
+                hidden2_list(ith_sample_in_sequence) 
+                );
+            
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                targets_connections[tar]->fprop(
+                    hidden2_list(ith_sample_in_sequence),
+                    target_prediction_activations_list[tar](
+                        ith_sample_in_sequence)
+                    );
+                target_layers[tar]->fprop(
+                    target_prediction_activations_list[tar](
+                        ith_sample_in_sequence),
+                    target_prediction_list[tar](
+                        ith_sample_in_sequence) );
+                if( use_target_layers_masks )
+                    target_prediction_list[tar]( ith_sample_in_sequence) *= 
+                        masks_list[tar](ith_sample_in_sequence);
+            }
+        }
+        else
+        {
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                targets_connections[tar]->fprop(
+                    hidden_list(ith_sample_in_sequence),
+                    target_prediction_activations_list[tar](
+                        ith_sample_in_sequence)
+                    );
+                target_layers[tar]->fprop(
+                    target_prediction_activations_list[tar](
+                        ith_sample_in_sequence),
+                    target_prediction_list[tar](
+                        ith_sample_in_sequence) );
+                if( use_target_layers_masks )
+                    target_prediction_list[tar]( ith_sample_in_sequence) *= 
+                        masks_list[tar](ith_sample_in_sequence);
+            }
+        }
+   
         if (testoutputs)
+        {
+            int sum_target_layers_size = 0;
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                output.subVec(sum_target_layers_size,target_layers[tar]->size)
+                    << target_prediction_list[tar]( ith_sample_in_sequence);
+                sum_target_layers_size += target_layers[tar]->size;
+            }
             testoutputs->putOrAppendRow(i, output);
+        }
+     
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            target_layers[tar]->activation << 
+                target_prediction_activations_list[tar](
+                    ith_sample_in_sequence);
+            target_layers[tar]->setExpectation(
+                target_prediction_list[tar](
+                    ith_sample_in_sequence));
+            nll_list(ith_sample_in_sequence,tar) = 
+                target_layer->fpropNLL( 
+                    targets_list[tar](ith_sample_in_sequence) ); 
+            costs[tar] += nll_list(ith_sample_in_sequence,tar);
+            
+            // Normalize by the number of things to predict
+            if( use_target_layers_masks )
+            {
+                n_items[tar] += sum(
+                    input.subVec( inputsize_without_masks 
+                                  + sum_target_elements, 
+                                  target_layers_n_of_target_elements[tar]) );
+                sum_target_elements += 
+                    target_layers_n_of_target_elements[tar];
+            }
+            else
+                n_itmes[tar]++;
+        }
+        ith_sample_in_sequence++;
 
-        if (testcosts)
-            testcosts->putOrAppendRow(i, costs);
-
-        if (test_stats)
-            test_stats->update(costs, weight);
-
         if (report_progress)
             pb->update(i);
+
+        for(int i=0; i<costs.length(); i++)
+            costs[i] /= n_items[i];
     }
 
-    //costs[0] = costs[0]/(len - nb_oov) ;
-
-    //cout << "Probabilite moyenne : " << costs[0] << endl;
-
+    if (testcosts)
+        testcosts->putOrAppendRow(i, costs);
+    
+    if (test_stats)
+        test_stats->update(costs, weight);
+    
+    ith_sample_in_sequence = 0;
+    hidden_list.clear();
+    hidden_activations_list.clear();
+    hidden2_list.clear();
+    hidden2_activations_list.clear();
+    target_prediction_list.clear();
+    target_prediction_activations_list.clear();
+    input_list.clear();
+    targets_list.clear();
+    nll_list.clear();
+    masks_list.clear();
+   
 }
 
 
 TVec<string> DynamicallyLinkedRBMsModel::getTestCostNames() const
 {
-    TVec<string> cost_names;
-    cost_names.append( "NLL" );
+    TVec<string> cost_names(0);
+    for( int i=0; i<target_layers.length(); i++ )
+        cost_names.append("target" + tostring(i) + ".NLL");
     return cost_names;
 }
 

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-15 00:15:36 UTC (rev 9004)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-15 00:49:19 UTC (rev 9005)
@@ -85,9 +85,6 @@
     //! Value of the first input component for end-of-sequence delimiter
     real end_of_sequence_symbol;
 
-    //! The weight of an additional input reconstruction error
-    real input_reconstruction_weight;
-
     //! The input layer of the model
     TVec<RBMLayer> input_layer;
 
@@ -114,18 +111,18 @@
 
     //#####  Public Learnt Options  ###########################################
 
-    //! Size of the input layer
-    int input_layer_size;
+    //! Number of elements in the target part of a VMatrix associated
+    //! to each target layer
+    TVec<int> target_layers_n_of_target_elements;
 
-    //! Size of each target layers
-    TVec<int> target_layers_size;
-
     //! Number of symbols for each symbolic field of train_set
     TVec<int> input_symbol_sizes;
     
     //! Number of symbols for each symbolic field of train_set
     TVec< TVec<int> > target_symbol_sizes;
+
     
+    
     //#####  Not Options  #####################################################
 
 
@@ -152,6 +149,9 @@
     //! measured on-line in the process.
     virtual void train();
 
+    //! Sets the learning of all layers and connections
+    void setLearningRate( real the_learning_rate );
+
     //! Computes the output from the input.
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
@@ -177,15 +177,21 @@
     //! Use the partition
     void partition(TVec<double> part, TVec<double> periode, TVec<double> vel ) const;
     
-    //! Clamps the visible units based on an input vector
-    void clamp_visible_units(const Vec& input) const;
+    //! Clamps the layer units based on a layer vector
+    void clamp_units(const Vec& layer_vector, PP<RBMLayer> layer,
+                     TVec<int> symbol_sizes) const;
 
+    //! Clamps the layer units based on a layer vector
+    //! and provides the associated mask in the correct format.
+    void clamp_units(const Vec& layer_vector, PP<RBMLayer> layer,
+                     TVec<int> symbol_sizes, const Vec& original_mask,
+                     Vec& formated_mask) const;
+    
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,
     //! after the visible units have been clamped
     void recurrent_update();
 
-
     virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;
 
@@ -224,12 +230,6 @@
     //! Store external data;
     AutoVMatrix*  data;
    
-    //! Stores conditional bias
-    mutable Vec cond_bias;
-
-    //! Stores visible conditional bias
-    mutable Vec visi_cond_bias;
-
     //! Stores bias gradient
     mutable Vec bias_gradient;
 
@@ -256,7 +256,7 @@
     mutable Vec previous_input;
 
     //! Stores previous target layer value
-    mutable Vec previous_target;
+    mutable TVec< Vec > previous_targets;
     
     //! Stores previous hidden layer value
     mutable Vec previous_hidden_layer;
@@ -285,30 +285,32 @@
     mutable Vec alpha;
 
     //! List of hidden layers values
-    Mat hidden_list;
-    Mat hidden_activations_list;
+    mutable Mat hidden_list;
+    mutable Mat hidden_activations_list;
 
     //! List of second hidden layers values
-    Mat hidden2_list;
-    Mat hidden2_activations_list;
+    mutable Mat hidden2_list;
+    mutable Mat hidden2_activations_list;
 
-    //! List of input prediction values
-    Mat input_prediction_list;
-    Mat input_prediction_activations_list;
+    //! List of target prediction values
+    mutable TVec< Mat > target_prediction_list;
+    mutable TVec< Mat > target_prediction_activations_list;
 
     //! List of inputs values
-    Mat input_list;
+    mutable Mat input_list;
 
     //! List of inputs values
-    Mat target_list;
+    mutable TVec< Mat > targets_list;
 
     //! List of the nll of the input samples in a sequence
-    Vec nll_list;
+    mutable Mat nll_list;
 
-    //! Temporary variable to clamp visible units (i.e. set the expectation
-    //! field of visible_layer)
-    mutable Vec input_expectation;
+    //! List of all targets' masks
+    mutable TVec< Mat > masks_list;
 
+    //! Contribution of dynamic weights to hidden layer activation
+    mutable Vec dynamic_activation_contribution;
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From laulysta at mail.berlios.de  Thu May 15 02:51:25 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Thu, 15 May 2008 02:51:25 +0200
Subject: [Plearn-commits] r9006 - trunk/plearn_learners_experimental
Message-ID: <200805150051.m4F0pPRi014127@sheep.berlios.de>

Author: laulysta
Date: 2008-05-15 02:51:25 +0200 (Thu, 15 May 2008)
New Revision: 9006

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
Log:
recurent update


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-15 00:49:19 UTC (rev 9005)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-15 00:51:25 UTC (rev 9006)
@@ -817,6 +817,81 @@
 
 void DynamicallyLinkedRBMsModel::recurrent_update()
 {
+   
+        hidden_temporal_gradient.clear();
+        for(int i=hidden_list.length()-2; i>=0; i--){   
+
+            hidden_gradient.clear();
+            if(use_target_layers_masks)
+            {
+                for( int tar=0; tar<target_layers.length(); tar++)
+                {
+                    target_layer[tar]->setExpectation(targets_prediction_list[tar](i));
+                    target_layers[tar]->bpropNLL(targets_list[tar](i+1),nll_list(i,tar),target_bias_gradient[tar]);
+                    target_bias_gradient[tar] *= target_layers_weights[tar];
+                    target_bias_gradient[tar] *= mask_list[tar](i);
+                    target_layers[tar]->update(target_bias_gradient[tar]);
+                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_activations_list[tar](i),
+                                                         hidden_gradient, target_bias_gradient[tar],true);
+                }
+            }
+            else
+            {
+                for( int tar=0; tar<target_layers.length(); tar++)
+                {
+                    target_layer[tar]->setExpectation(targets_prediction_list[tar](i));
+                    target_layers[tar]->bpropNLL(targets_list[tar](i+1),nll_list(i,tar),target_bias_gradient[tar]);
+                    target_bias_gradient[tar] *= target_layers_weights[tar];
+                    target_layers[tar]->update(target_bias_gradient[tar]);
+                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_activations_list[tar](i),
+                                                         hidden_gradient, target_bias_gradient[tar],true); 
+                }
+            }
+
+            if (hidden_layer2)
+            {
+                hidden_layer2->bpropUpdate(
+                    hidden2_activations_list(i), hidden2_list(i),
+                    bias_gradient, hidden_gradient);
+                
+                hidden_connections->bpropUpdate(
+                    hidden_list(i),
+                    hidden2_activations_list(i), 
+                    hidden_gradient, bias_gradient);
+            }
+            
+            if(i!=0)
+            {   
+                hidden_gradient += hidden_temporal_gradient;
+                
+                hidden_layer->bpropUpdate(
+                    hidden_activations_list(i), hidden_list(i),
+                    hidden_temporal_gradient, hidden_gradient);
+                
+                dynamic_connections->bpropUpdate(
+                    hidden_list(i-1),
+                    hidden_activations_list(i), // Here, it should be cond_bias, but doesn't matter
+                    hidden_gradient, hidden_temporal_gradient);
+                
+                hidden_temporal_gradient << hidden_gradient;
+                
+                connections->bpropUpdate(
+                    input_list(i),
+                    hidden_activations_list(i), 
+                    visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+                
+            }
+            else
+            {
+                // Could learn initial value for h_{-1}
+            }
+        }
+    
+}
+
+
+void DynamicallyLinkedRBMsModel::recurrent_update()
+{
     // Notes: 
     //    - not all lists are useful (some *_activations_* are not)
     //int segment = hidden_list.length()/2;
@@ -828,42 +903,43 @@
         //cout << "segment: " << seg << endl;
         hidden_temporal_gradient.clear();
         for(int i=hidden_list.length()-2; i>=0; i--){  
-        // for(int i=hidden_list.length()-2; i>=seg; i--){     
+            // for(int i=hidden_list.length()-2; i>=seg; i--){     
             
             //visible_layer->expectation << input_prediction_list(i);
             //visible_layer->activation << ?????;
-            visible_layer->setExpectation(input_prediction_list(i));
             
-            //visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient, (taillePart*3)+14);
-
-            //      hidden_gradient.clear();
-            //HUGO: for( int tar=0; tar<target_layers.length(); tar++)
-            //      {
-            //           target_layers[tar]->bpropNLL(targets_list[tar](i+1),nll_list(i,tar),target_bias_gradient[tar]);
-            //           target_bias_gradient[tar] *= target_layers_weights[tar];
-            //           target_layers[tar]->update(target_bias_gradient[tar]);
-            //           target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_activations_list[tar](i),
-            //                                                hidden_gradient, target_bias_gradient[tar],true);
-            //      }
-
-            visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient);
             
-            visible_layer->update(visi_bias_gradient);
             
-            //visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient, (taillePart*3)+14);
             
-            connections_transpose->bpropUpdate(hidden2_list(i),input_prediction_activations_list(i),hidden_gradient, visi_bias_gradient);
             
+
+            hidden_gradient.clear();
+            for( int tar=0; tar<target_layers.length(); tar++)
+            {
+                target_layer[tar]->setExpectation(input_prediction_list[tar](i));
+                target_layers[tar]->bpropNLL(targets_list[tar](i+1),nll_list(i,tar),target_bias_gradient[tar]);
+                target_bias_gradient[tar] *= target_layers_weights[tar];
+                target_layers[tar]->update(target_bias_gradient[tar]);
+                target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_activations_list[tar](i),
+                                                     hidden_gradient, target_bias_gradient[tar],true);
+            }
+            //visible_layer->setExpectation(input_prediction_list(i));
+            //visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient);
+            //visible_layer->update(visi_bias_gradient);
+            //connections_transpose->bpropUpdate(hidden2_list(i),input_prediction_activations_list(i),hidden_gradient, visi_bias_gradient);
             
+            
             //hidden_layer->setExpectation(hidden_list(i+1));//////////////////////////////
             //hidden_layer->bpropNLL(hidden2_list(i),nll_list[i], hidden_gradient2);////////////////////////////////
             
-            
-            
-            hidden_layer->bpropUpdate(
+           
+            hidden_layer2->bpropUpdate(
                 hidden2_activations_list(i), hidden2_list(i),
                 bias_gradient, hidden_gradient);
             
+
+
+
             //hidden_layer->update(hidden_gradient2);/////////////////////////////////////
             
             
@@ -913,9 +989,6 @@
 }
 
 
-
-
-
 void DynamicallyLinkedRBMsModel::computeOutput(const Vec& input, Vec& output) const
 {
     PLERROR("DynamicallyLinkedRBMsModel::computeOutput(): this is a dynamic, "



From larocheh at mail.berlios.de  Thu May 15 03:30:24 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 15 May 2008 03:30:24 +0200
Subject: [Plearn-commits] r9007 - trunk/plearn_learners_experimental
Message-ID: <200805150130.m4F1UO5Y016354@sheep.berlios.de>

Author: larocheh
Date: 2008-05-15 03:30:22 +0200 (Thu, 15 May 2008)
New Revision: 9007

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
GETTING CLOSER!!!!!


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-15 00:51:25 UTC (rev 9006)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-15 01:30:22 UTC (rev 9007)
@@ -61,12 +61,15 @@
 // - make sure gradient descent is proper (change some vectors into matrices, etc.)
 // - make sure end_of_sequence_symbol is used appropriately
 // - make sure declareOption includes everything, including saved variable
-// - implement deepcopy appropriately
-// - recurrent_nstages doesn't exist anymore
 // - verify use of mask is proper
-// - verify code works with and without hidden_layer2
 // - do proper resize of recurrent internal variables
+// - implement deepcopy appropriately
+// - corriger bug avec activation (faut additionner les biais!!!)
 
+// - commiter mse
+// - add dynamic_activations_list and use it in recurrent_update
+// - verify code works with and without hidden_layer2
+
 namespace PLearn {
 using namespace std;
 
@@ -372,53 +375,39 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(visible_layer, copies);
-    deepCopyField( hidden_layer , copies);
+    deepCopyField( input_layer, copies);
+    deepCopyField( target_layers , copies);
+    deepCopyField( hidden_layer, copies);
+    deepCopyField( hidden_layer2 , copies);
     deepCopyField( dynamic_connections , copies);
-    deepCopyField( dynamic_connections_copy , copies);
-    deepCopyField( visible_connections , copies);
-    deepCopyField( connections , copies);
-    deepCopyField( connections_idem , copies);
-    deepCopyField( connections_idem_t , copies);
-    deepCopyField( connections_transpose, copies);
-    deepCopyField( connections_transpose_copy, copies);
-    deepCopyField( symbol_sizes , copies);
-    deepCopyField( cond_bias , copies);
-    deepCopyField( visi_cond_bias , copies);
+    deepCopyField( hidden_connections , copies);
+    deepCopyField( input_connections , copies);
+    deepCopyField( target_connections , copies);
+    deepCopyField( target_layers_n_of_target_elements, copies);
+    deepCopyField( input_symbol_sizes, copies);
+    deepCopyField( target_symbol_sizes, copies);
+    
+
     deepCopyField( bias_gradient , copies);
-    deepCopyField( visi_bias_gradient , copies);
-    deepCopyField( hidden_layer_target , copies);
-    deepCopyField( input_gradient , copies);
     deepCopyField( hidden_gradient , copies);
-    deepCopyField( hidden_gradient2 , copies);
     deepCopyField( hidden_temporal_gradient , copies);
-    deepCopyField( previous_input , copies);
-    deepCopyField( previous_target , copies);
-    deepCopyField( previous_hidden_layer , copies);
-    deepCopyField( previous_hidden_layer_activation , copies);
-    deepCopyField( previous_visible_layer , copies);
-    deepCopyField( hidden_layer_sample , copies);
-    deepCopyField( hidden_layer_expectation , copies);
-    deepCopyField( visible_layer_sample , copies);
-    deepCopyField( visible_layer_input , copies);
-    deepCopyField( pos_down_values , copies);
-    deepCopyField( pos_up_values , copies);
-    deepCopyField( alpha , copies);
     deepCopyField( hidden_list , copies);
-    deepCopyField( hidden_activations_list , copies);
+    deepCopyField( hidden_act_no_bias_list , copies);
     deepCopyField( hidden2_list , copies);
-    deepCopyField( hidden2_activations_list , copies);
-    deepCopyField( input_prediction_list , copies);
-    deepCopyField( input_prediction_activations_list , copies);
+    deepCopyField( hidden2_act_no_bias_list , copies);
+    deepCopyField( target_prediction_list , copies);
+    deepCopyField( target_prediction_act_no_bias_list , copies);
     deepCopyField( input_list , copies);
-    deepCopyField( target_list , copies);
+    deepCopyField( targets_list , copies);
     deepCopyField( nll_list , copies);
-    deepCopyField( input_expectation , copies);
+    deepCopyField( masks_list , copies);
+    deepCopyField( dynamic_activation_contribution, copies);
 
+
     // deepCopyField(, copies);
 
-    PLERROR("DynamicallyLinkedRBMsModel::makeDeepCopyFromShallowCopy(): "
-    "not implemented yet");
+    //PLERROR("DynamicallyLinkedRBMsModel::makeDeepCopyFromShallowCopy(): "
+    //"not implemented yet");
 }
 
 
@@ -536,11 +525,11 @@
                     
                     ith_sample_in_sequence = 0;
                     hidden_list.clear();
-                    hidden_activations_list.clear();
+                    hidden_act_no_bias_list.clear();
                     hidden2_list.clear();
-                    hidden2_activations_list.clear();
+                    hidden2_act_no_bias_list.clear();
                     target_prediction_list.clear();
-                    target_prediction_activations_list.clear();
+                    target_prediction_act_no_bias_list.clear();
                     input_list.clear();
                     targets_list.clear();
                     nll_list.clear();
@@ -550,13 +539,13 @@
 
                 // Resize internal variables
                 hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                hidden_activations_list.resize(ith_sample_in_sequence+1,
+                hidden_act_no_bias_list.resize(ith_sample_in_sequence+1,
                                                hidden_layer->size);
                 if( hidden_layer2 )
                 {
                     hidden2_list.resize(ith_sample_in_sequence+1,
                                         hidden_layer2->size);
-                    hidden2_activations_list.resize(ith_sample_in_sequence+1,
+                    hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1,
                                                     hidden_layer2->size);
                 }
                  
@@ -564,14 +553,14 @@
 
                 targets_list.resize( target_layers.length() );
                 target_prediction_list.resize( target_layers.length() );
-                target_prediction_activations_list.resize( target_layers.length() );
+                target_prediction_act_no_bias_list.resize( target_layers.length() );
                 for( int tar=0; tar < target_layers.length(); tar++ )
                 {
                     targets_list[tar].resize( ith_sample_in_sequence+1,
                                               target_layers[tar]->size );
                     target_prediction_list[tar].resize(
                         ith_sample_in_sequence+1, target_layers[tar]->size);
-                    target_prediction_activations_list[tar].resize(
+                    target_prediction_act_no_bias_list[tar].resize(
                         ith_sample_in_sequence+1, target_layers[tar]->size);
 
                 }
@@ -615,46 +604,46 @@
                                     target_layers[tar],
                                     target_symbol_sizes[tar]);
                     sum_target_elements += target_layers_n_of_target_elements[tar];
-                    target_list[tar](ith_sample_in_sequence) << 
+                    targets_list[tar](ith_sample_in_sequence) << 
                         target_layers[tar]->expectation;
                 }
                 
                 input_connections->fprop( input_list(ith_sample_in_sequence), 
-                                          hidden_activations_list(ith_sample_in_sequence));
+                                          hidden_act_no_bias_list(ith_sample_in_sequence));
                 
                 if( ith_sample_in_sequence > 0 )
                 {
                     dynamic_connections->fprop( 
                         hidden_list(ith_sample_in_sequence-1),
-                        dynamic_activation_contribution );
+                        dynamic_act_no_bias_contribution );
 
-                    hidden_activations_list(ith_sample_in_sequence) += 
+                    hidden_act_no_bias_list(ith_sample_in_sequence) += 
                         dynamic_actvation_contribution;
                 }
                  
-                hidden_layer->fprop( hidden_activations_list(ith_sample_in_sequence) 
+                hidden_layer->fprop( hidden_act_no_bias_list(ith_sample_in_sequence) 
                                      hidden_list(ith_sample_in_sequence) );
                  
                 if( hidden_layer2 )
                 {
                     hidden_connections->fprop( 
                         hidden_list(ith_sample_in_sequence),
-                        hidden2_activations_list(ith_sample_in_sequence));
+                        hidden2_act_no_bias_list(ith_sample_in_sequence));
 
                     hidden_layer2->fprop( 
-                        hidden2_activations_list(ith_sample_in_sequence) 
+                        hidden2_act_no_bias_list(ith_sample_in_sequence) 
                         hidden2_list(ith_sample_in_sequence) 
                         );
 
                     for( int tar=0; tar < target_layers.length(); tar++ )
                     {
-                        targets_connections[tar]->fprop(
+                        target_connections[tar]->fprop(
                             hidden2_list(ith_sample_in_sequence),
-                            target_prediction_activations_list[tar](
+                            target_prediction_act_no_bias_list[tar](
                                 ith_sample_in_sequence)
                             );
                         target_layers[tar]->fprop(
-                            target_prediction_activations_list[tar](
+                            target_prediction_act_no_bias_list[tar](
                                 ith_sample_in_sequence),
                             target_prediction_list[tar](
                                 ith_sample_in_sequence) );
@@ -667,13 +656,13 @@
                 {
                     for( int tar=0; tar < target_layers.length(); tar++ )
                     {
-                        targets_connections[tar]->fprop(
+                        target_connections[tar]->fprop(
                             hidden_list(ith_sample_in_sequence),
-                            target_prediction_activations_list[tar](
+                            target_prediction_act_no_bias_list[tar](
                                 ith_sample_in_sequence)
                             );
                         target_layers[tar]->fprop(
-                            target_prediction_activations_list[tar](
+                            target_prediction_act_no_bias_list[tar](
                                 ith_sample_in_sequence),
                             target_prediction_list[tar](
                                 ith_sample_in_sequence) );
@@ -687,8 +676,9 @@
                 for( int tar=0; tar < target_layers.length(); tar++ )
                 {
                     target_layers[tar]->activation << 
-                        target_prediction_activations_list[tar](
+                        target_prediction_act_no_bias_list[tar](
                             ith_sample_in_sequence);
+                    target_layers[tar]->activation += target_layers[tar]->bias;
                     target_layers[tar]->setExpectation(
                         target_prediction_list[tar](
                             ith_sample_in_sequence));
@@ -817,46 +807,50 @@
 
 void DynamicallyLinkedRBMsModel::recurrent_update()
 {
-   
+    
         hidden_temporal_gradient.clear();
-        for(int i=hidden_list.length()-2; i>=0; i--){   
+        for(int i=hidden_list.length()-1; i>=0; i--){   
 
             hidden_gradient.clear();
             if(use_target_layers_masks)
             {
                 for( int tar=0; tar<target_layers.length(); tar++)
                 {
+                    target_layer[tar]->activation << targets_prediction_act_no_bias_list[tar](i);
+                    target_layer[tar]->activation += target_layer[tar]->bias;
                     target_layer[tar]->setExpectation(targets_prediction_list[tar](i));
-                    target_layers[tar]->bpropNLL(targets_list[tar](i+1),nll_list(i,tar),target_bias_gradient[tar]);
-                    target_bias_gradient[tar] *= target_layers_weights[tar];
-                    target_bias_gradient[tar] *= mask_list[tar](i);
-                    target_layers[tar]->update(target_bias_gradient[tar]);
-                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_activations_list[tar](i),
-                                                         hidden_gradient, target_bias_gradient[tar],true);
+                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    bias_gradient *= target_layers_weights[tar];
+                    bias_gradient *= mask_list[tar](i);
+                    target_layers[tar]->update(bias_gradient);
+                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                         hidden_gradient, bias_gradient,true);
                 }
             }
             else
             {
                 for( int tar=0; tar<target_layers.length(); tar++)
                 {
+                    target_layer[tar]->activation << targets_prediction_act_no_bias_list[tar](i);
+                    target_layer[tar]->activation += target_layer[tar]->bias;
                     target_layer[tar]->setExpectation(targets_prediction_list[tar](i));
-                    target_layers[tar]->bpropNLL(targets_list[tar](i+1),nll_list(i,tar),target_bias_gradient[tar]);
-                    target_bias_gradient[tar] *= target_layers_weights[tar];
-                    target_layers[tar]->update(target_bias_gradient[tar]);
-                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_activations_list[tar](i),
-                                                         hidden_gradient, target_bias_gradient[tar],true); 
+                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    bias_gradient *= target_layers_weights[tar];
+                    target_layers[tar]->update(bias_gradient);
+                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                         hidden_gradient, bias_gradient,true); 
                 }
             }
 
             if (hidden_layer2)
             {
                 hidden_layer2->bpropUpdate(
-                    hidden2_activations_list(i), hidden2_list(i),
+                    hidden2_act_no_bias_list(i), hidden2_list(i),
                     bias_gradient, hidden_gradient);
                 
                 hidden_connections->bpropUpdate(
                     hidden_list(i),
-                    hidden2_activations_list(i), 
+                    hidden2_act_no_bias_list(i), 
                     hidden_gradient, bias_gradient);
             }
             
@@ -865,130 +859,38 @@
                 hidden_gradient += hidden_temporal_gradient;
                 
                 hidden_layer->bpropUpdate(
-                    hidden_activations_list(i), hidden_list(i),
+                    hidden_act_no_bias_list(i), hidden_list(i),
                     hidden_temporal_gradient, hidden_gradient);
                 
                 dynamic_connections->bpropUpdate(
                     hidden_list(i-1),
-                    hidden_activations_list(i), // Here, it should be cond_bias, but doesn't matter
+                    hidden_act_no_bias_list(i), // Here, it should be cond_bias, but doesn't matter
                     hidden_gradient, hidden_temporal_gradient);
                 
                 hidden_temporal_gradient << hidden_gradient;
                 
                 connections->bpropUpdate(
                     input_list(i),
-                    hidden_activations_list(i), 
+                    hidden_act_no_bias_list(i), 
                     visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
                 
             }
             else
             {
-                // Could learn initial value for h_{-1}
-            }
-        }
-    
-}
-
-
-void DynamicallyLinkedRBMsModel::recurrent_update()
-{
-    // Notes: 
-    //    - not all lists are useful (some *_activations_* are not)
-    //int segment = hidden_list.length()/2;
-    //int seg =0;
-    //for(int k=hidden_list.length()-3; k>=-segment; k-=segment){ 
-    //  seg = k;
-    //  if(seg < 0)
-    //      seg = 0;
-        //cout << "segment: " << seg << endl;
-        hidden_temporal_gradient.clear();
-        for(int i=hidden_list.length()-2; i>=0; i--){  
-            // for(int i=hidden_list.length()-2; i>=seg; i--){     
-            
-            //visible_layer->expectation << input_prediction_list(i);
-            //visible_layer->activation << ?????;
-            
-            
-            
-            
-            
-
-            hidden_gradient.clear();
-            for( int tar=0; tar<target_layers.length(); tar++)
-            {
-                target_layer[tar]->setExpectation(input_prediction_list[tar](i));
-                target_layers[tar]->bpropNLL(targets_list[tar](i+1),nll_list(i,tar),target_bias_gradient[tar]);
-                target_bias_gradient[tar] *= target_layers_weights[tar];
-                target_layers[tar]->update(target_bias_gradient[tar]);
-                target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_activations_list[tar](i),
-                                                     hidden_gradient, target_bias_gradient[tar],true);
-            }
-            //visible_layer->setExpectation(input_prediction_list(i));
-            //visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient);
-            //visible_layer->update(visi_bias_gradient);
-            //connections_transpose->bpropUpdate(hidden2_list(i),input_prediction_activations_list(i),hidden_gradient, visi_bias_gradient);
-            
-            
-            //hidden_layer->setExpectation(hidden_list(i+1));//////////////////////////////
-            //hidden_layer->bpropNLL(hidden2_list(i),nll_list[i], hidden_gradient2);////////////////////////////////
-            
-           
-            hidden_layer2->bpropUpdate(
-                hidden2_activations_list(i), hidden2_list(i),
-                bias_gradient, hidden_gradient);
-            
-
-
-
-            //hidden_layer->update(hidden_gradient2);/////////////////////////////////////
-            
-            
-            //bias_gradient += hidden_gradient2;///////////////////////////////////
-            
-            
-            
-            if(dynamic_connections_copy)
-                dynamic_connections_copy->bpropUpdate(
-                    hidden_list(i),
-                    hidden2_activations_list(i), 
-                    hidden_gradient, bias_gradient);        
-            else
-                dynamic_connections->bpropUpdate(
-                    hidden_list(i),
-                    hidden2_activations_list(i), 
-                    hidden_gradient, bias_gradient);
-            
-            if(i!=0)
-            {
-                
-                hidden_gradient += hidden_temporal_gradient;
-                
                 hidden_layer->bpropUpdate(
-                    hidden_activations_list(i), hidden_list(i),
-                    hidden_temporal_gradient, hidden_gradient);
-                
-                dynamic_connections->bpropUpdate(
-                    hidden_list(i-1),
-                    hidden_activations_list(i), // Here, it should be cond_bias, but doesn't matter
-                    hidden_gradient, hidden_temporal_gradient);
-                
-                hidden_temporal_gradient << hidden_gradient;
-                
+                    hidden_act_no_bias_list(i), hidden_list(i),
+                    hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
                 connections->bpropUpdate(
                     input_list(i),
-                    hidden_activations_list(i), 
+                    hidden_act_no_bias_list(i), 
                     visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
-                
-            }
-            else
-            {
+
                 // Could learn initial value for h_{-1}
             }
         }
     
 }
 
-
 void DynamicallyLinkedRBMsModel::computeOutput(const Vec& input, Vec& output) const
 {
     PLERROR("DynamicallyLinkedRBMsModel::computeOutput(): this is a dynamic, "
@@ -1037,11 +939,11 @@
         {                    
             ith_sample_in_sequence = 0;
             hidden_list.clear();
-            hidden_activations_list.clear();
+            hidden_act_no_bias_list.clear();
             hidden2_list.clear();
-            hidden2_activations_list.clear();
+            hidden2_act_no_bias_list.clear();
             target_prediction_list.clear();
-            target_prediction_activations_list.clear();
+            target_prediction_act_no_bias_list.clear();
             input_list.clear();
             targets_list.clear();
             nll_list.clear();
@@ -1051,13 +953,13 @@
 
         // Resize internal variables
         hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-        hidden_activations_list.resize(ith_sample_in_sequence+1,
+        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1,
                                        hidden_layer->size);
         if( hidden_layer2 )
         {
             hidden2_list.resize(ith_sample_in_sequence+1,
                                 hidden_layer2->size);
-            hidden2_activations_list.resize(ith_sample_in_sequence+1,
+            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1,
                                             hidden_layer2->size);
         }
         
@@ -1065,14 +967,14 @@
         
         targets_list.resize( target_layers.length() );
         target_prediction_list.resize( target_layers.length() );
-        target_prediction_activations_list.resize( target_layers.length() );
+        target_prediction_act_no_bias_list.resize( target_layers.length() );
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
             targets_list[tar].resize( ith_sample_in_sequence+1,
                                       target_layers[tar]->size );
             target_prediction_list[tar].resize(
                 ith_sample_in_sequence+1, target_layers[tar]->size);
-            target_prediction_activations_list[tar].resize(
+            target_prediction_act_no_bias_list[tar].resize(
                 ith_sample_in_sequence+1, target_layers[tar]->size);
             
         }
@@ -1116,46 +1018,46 @@
                             target_layers[tar],
                             target_symbol_sizes[tar]);
             sum_target_elements += target_layers_n_of_target_elements[tar];
-            target_list[tar](ith_sample_in_sequence) << 
+            targets_list[tar](ith_sample_in_sequence) << 
                 target_layers[tar]->expectation;
         }
         
         input_connections->fprop( input_list(ith_sample_in_sequence), 
-                                  hidden_activations_list(ith_sample_in_sequence));
+                                  hidden_act_no_bias_list(ith_sample_in_sequence));
         
         if( ith_sample_in_sequence > 0 )
         {
             dynamic_connections->fprop( 
                 hidden_list(ith_sample_in_sequence-1),
-                dynamic_activation_contribution );
+                dynamic_act_no_bias_contribution );
             
-            hidden_activations_list(ith_sample_in_sequence) += 
+            hidden_act_no_bias_list(ith_sample_in_sequence) += 
                 dynamic_actvation_contribution;
         }
         
-        hidden_layer->fprop( hidden_activations_list(ith_sample_in_sequence) 
+        hidden_layer->fprop( hidden_act_no_bias_list(ith_sample_in_sequence) 
                              hidden_list(ith_sample_in_sequence) );
         
         if( hidden_layer2 )
         {
             hidden_connections->fprop( 
                 hidden_list(ith_sample_in_sequence),
-                hidden2_activations_list(ith_sample_in_sequence));
+                hidden2_act_no_bias_list(ith_sample_in_sequence));
             
             hidden_layer2->fprop( 
-                hidden2_activations_list(ith_sample_in_sequence) 
+                hidden2_act_no_bias_list(ith_sample_in_sequence) 
                 hidden2_list(ith_sample_in_sequence) 
                 );
             
             for( int tar=0; tar < target_layers.length(); tar++ )
             {
-                targets_connections[tar]->fprop(
+                target_connections[tar]->fprop(
                     hidden2_list(ith_sample_in_sequence),
-                    target_prediction_activations_list[tar](
+                    target_prediction_act_no_bias_list[tar](
                         ith_sample_in_sequence)
                     );
                 target_layers[tar]->fprop(
-                    target_prediction_activations_list[tar](
+                    target_prediction_act_no_bias_list[tar](
                         ith_sample_in_sequence),
                     target_prediction_list[tar](
                         ith_sample_in_sequence) );
@@ -1168,13 +1070,13 @@
         {
             for( int tar=0; tar < target_layers.length(); tar++ )
             {
-                targets_connections[tar]->fprop(
+                target_connections[tar]->fprop(
                     hidden_list(ith_sample_in_sequence),
-                    target_prediction_activations_list[tar](
+                    target_prediction_act_no_bias_list[tar](
                         ith_sample_in_sequence)
                     );
                 target_layers[tar]->fprop(
-                    target_prediction_activations_list[tar](
+                    target_prediction_act_no_bias_list[tar](
                         ith_sample_in_sequence),
                     target_prediction_list[tar](
                         ith_sample_in_sequence) );
@@ -1200,8 +1102,9 @@
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
             target_layers[tar]->activation << 
-                target_prediction_activations_list[tar](
+                target_prediction_act_no_bias_list[tar](
                     ith_sample_in_sequence);
+            target_layers[tar]->activation += target_layers[tar]->bias;
             target_layers[tar]->setExpectation(
                 target_prediction_list[tar](
                     ith_sample_in_sequence));
@@ -1240,11 +1143,11 @@
     
     ith_sample_in_sequence = 0;
     hidden_list.clear();
-    hidden_activations_list.clear();
+    hidden_act_no_bias_list.clear();
     hidden2_list.clear();
-    hidden2_activations_list.clear();
+    hidden2_act_no_bias_list.clear();
     target_prediction_list.clear();
-    target_prediction_activations_list.clear();
+    target_prediction_act_no_bias_list.clear();
     input_list.clear();
     targets_list.clear();
     nll_list.clear();
@@ -1266,257 +1169,257 @@
     return getTestCostNames();
 }
 
-void DynamicallyLinkedRBMsModel::gen()
-{
-    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
-    data = new AutoVMatrix();
-    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
-    data->defineSizes(21,0,0);
-    //data->inputsize = 21;
-    //data->targetsize = 0;
-    //data->weightsize = 0;
-    data->build();
+//void DynamicallyLinkedRBMsModel::gen()
+//{
+//    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+//    data = new AutoVMatrix();
+//    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+//    data->defineSizes(21,0,0);
+//    //data->inputsize = 21;
+//    //data->targetsize = 0;
+//    //data->weightsize = 0;
+//    data->build();
+//
+//    
+//    int len = data->length();
+//    Vec score;
+//    Vec target;
+//    real weight;
+//    Vec bias_tempo;
+//    Vec visi_bias_tempo;
+//   
+//   
+//    
+//    previous_hidden_layer.resize(hidden_layer->size);
+//    connections_idem = connections;
+//
+//    for (int ith_sample = 0; ith_sample < len ; ith_sample++ ){
+//        
+//        data->getExample(ith_sample, score, target, weight);
+//        //score << data(ith_sample);
+//        input_prediction_list.resize(
+//            ith_sample+1,visible_layer->size);
+//        if(ith_sample > 0)
+//        {
+//            
+//            //input_list(ith_sample_in_sequence) << previous_input;
+//            //h*_{t-1}
+//            //////////////////////////////////
+//            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+//            hidden_layer->setAllBias(cond_bias); //**************************
+//            
+//            
+//            
+//            //up phase
+//            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
+//            hidden_layer->getAllActivations( connections_idem );
+//            hidden_layer->computeExpectation();
+//            //////////////////////////////////
+//            
+//            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
+//            //previous_hidden_layer_act_no_bias << hidden_layer->activation;
+//            
+//            
+//            //h*_{t}
+//            ////////////
+//            if(dynamic_connections_copy)
+//                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            else
+//                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            hidden_layer->expectation_is_not_up_to_date();
+//            hidden_layer->computeExpectation();//h_{t}
+//            ///////////
+//            
+//            //previous_input << visible_layer->expectation;//v_{t-1}
+//            
+//        }
+//        else
+//        {
+//            
+//            previous_hidden_layer.clear();//h_{t-1}
+//            if(dynamic_connections_copy)
+//                dynamic_connections_copy->fprop( previous_hidden_layer ,
+//                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            else
+//                dynamic_connections->fprop(previous_hidden_layer,
+//                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            
+//            hidden_layer->expectation_is_not_up_to_date();
+//            hidden_layer->computeExpectation();//h_{t}
+//            //previous_input.resize(data->inputsize);
+//            //previous_input << data(ith_sample);
+//            
+//        }
+//        
+//        //connections_transpose->setAsDownInput( hidden_layer->expectation );
+//        //visible_layer->getAllActivations( connections_idem_t );
+//        
+//        connections->setAsUpInput( hidden_layer->expectation );
+//        visible_layer->getAllActivations( connections_idem );
+//        
+//        visible_layer->computeExpectation();
+//        //visible_layer->generateSample();
+//        partition(score.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
+//        partition(score.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
+//
+//
+//        visible_layer->activation.subVec(0,14+taillePart) << score;
+//        visible_layer->expectation.subVec(0,14+taillePart) << score;
+//
+//        input_prediction_list(ith_sample) << visible_layer->expectation;
+//        
+//    }
+//    
+//    //Vec tempo;
+//    TVec<real> tempo;
+//    tempo.resize(visible_layer->size);
+//    ofstream myfile;
+//    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/test.txt");
+//    
+//    for (int i = 0; i < len ; i++ ){
+//        tempo << input_prediction_list(i);
+//        
+//        //cout << tempo[2] << endl;
+//       
+//        for (int j = 0; j < tempo.length() ; j++ ){
+//            
+//            
+//                
+//                
+//               myfile << tempo[j] << " ";
+//               
+//
+//               
+//           
+//        }
+//        myfile << "\n";
+//    }
+//     
+//
+//     myfile.close();
+//
+//}
+//void DynamicallyLinkedRBMsModel::generate(int nbNotes)
+//{
+//    
+//    previous_hidden_layer.resize(hidden_layer->size);
+//    connections_idem = connections;
+//
+//    for (int ith_sample = 0; ith_sample < nbNotes ; ith_sample++ ){
+//        
+//        input_prediction_list.resize(
+//            ith_sample+1,visible_layer->size);
+//        if(ith_sample > 0)
+//        {
+//            
+//            //input_list(ith_sample_in_sequence) << previous_input;
+//            //h*_{t-1}
+//            //////////////////////////////////
+//            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+//            hidden_layer->setAllBias(cond_bias); //**************************
+//            
+//            
+//            
+//            //up phase
+//            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
+//            hidden_layer->getAllActivations( connections_idem );
+//            hidden_layer->computeExpectation();
+//            //////////////////////////////////
+//            
+//            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
+//            //previous_hidden_layer_act_no_bias << hidden_layer->activation;
+//            
+//            
+//            //h*_{t}
+//            ////////////
+//            if(dynamic_connections_copy)
+//                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            else
+//                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            hidden_layer->expectation_is_not_up_to_date();
+//            hidden_layer->computeExpectation();//h_{t}
+//            ///////////
+//            
+//            //previous_input << visible_layer->expectation;//v_{t-1}
+//            
+//        }
+//        else
+//        {
+//            
+//            previous_hidden_layer.clear();//h_{t-1}
+//            if(dynamic_connections_copy)
+//                dynamic_connections_copy->fprop( previous_hidden_layer ,
+//                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            else
+//                dynamic_connections->fprop(previous_hidden_layer,
+//                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+//            
+//            hidden_layer->expectation_is_not_up_to_date();
+//            hidden_layer->computeExpectation();//h_{t}
+//            
+//            
+//        }
+//        
+//        //connections_transpose->setAsDownInput( hidden_layer->expectation );
+//        //visible_layer->getAllActivations( connections_idem_t );
+//        
+//        connections->setAsUpInput( hidden_layer->expectation );
+//        visible_layer->getAllActivations( connections_idem );
+//        
+//        visible_layer->computeExpectation();
+//        visible_layer->generateSample();
+//        
+//        input_prediction_list(ith_sample) << visible_layer->sample;
+//        
+//    }
+//    
+//    //Vec tempo;
+//    TVec<int> tempo;
+//    tempo.resize(visible_layer->size);
+//    int theNote;
+//    //int nbNoteVisiLayer = input_prediction_list(1).length()/13;
+//    ofstream myfile;
+//    int theLayer;
+//    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_musicGeneration/data/generate/test.txt");
+//    
+//    for (int i = 0; i < nbNotes ; i++ ){
+//        tempo << input_prediction_list(i);
+//        
+//        //cout << tempo[2] << endl;
+//       
+//        for (int j = 0; j < tempo.length() ; j++ ){
+//            
+//            if (tempo[j] == 1){
+//                theLayer = (j/13);
+//                
+//                theNote = j - (13*theLayer);
+//               
+//
+//                if (theNote<=11){
+//                    //print theNote
+//                    //cout << theNote+50 << " ";
+//                    myfile << theNote << " ";
+//                }
+//                else{
+//                    //print #
+//                    //cout << "# ";
+//                    myfile << "# ";
+//                    
+//                }
+//     
+//            }
+//           
+//        }
+//        myfile << "\n";
+//    }
+//     myfile << "<oov> <oov> \n";
+//
+//     myfile.close();
+//
+//}
 
-    
-    int len = data->length();
-    Vec score;
-    Vec target;
-    real weight;
-    Vec bias_tempo;
-    Vec visi_bias_tempo;
-   
-   
-    
-    previous_hidden_layer.resize(hidden_layer->size);
-    connections_idem = connections;
-
-    for (int ith_sample = 0; ith_sample < len ; ith_sample++ ){
-        
-        data->getExample(ith_sample, score, target, weight);
-        //score << data(ith_sample);
-        input_prediction_list.resize(
-            ith_sample+1,visible_layer->size);
-        if(ith_sample > 0)
-        {
-            
-            //input_list(ith_sample_in_sequence) << previous_input;
-            //h*_{t-1}
-            //////////////////////////////////
-            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
-            hidden_layer->setAllBias(cond_bias); //**************************
-            
-            
-            
-            //up phase
-            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
-            hidden_layer->getAllActivations( connections_idem );
-            hidden_layer->computeExpectation();
-            //////////////////////////////////
-            
-            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
-            //previous_hidden_layer_activation << hidden_layer->activation;
-            
-            
-            //h*_{t}
-            ////////////
-            if(dynamic_connections_copy)
-                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            else
-                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            hidden_layer->expectation_is_not_up_to_date();
-            hidden_layer->computeExpectation();//h_{t}
-            ///////////
-            
-            //previous_input << visible_layer->expectation;//v_{t-1}
-            
-        }
-        else
-        {
-            
-            previous_hidden_layer.clear();//h_{t-1}
-            if(dynamic_connections_copy)
-                dynamic_connections_copy->fprop( previous_hidden_layer ,
-                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            else
-                dynamic_connections->fprop(previous_hidden_layer,
-                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            
-            hidden_layer->expectation_is_not_up_to_date();
-            hidden_layer->computeExpectation();//h_{t}
-            //previous_input.resize(data->inputsize);
-            //previous_input << data(ith_sample);
-            
-        }
-        
-        //connections_transpose->setAsDownInput( hidden_layer->expectation );
-        //visible_layer->getAllActivations( connections_idem_t );
-        
-        connections->setAsUpInput( hidden_layer->expectation );
-        visible_layer->getAllActivations( connections_idem );
-        
-        visible_layer->computeExpectation();
-        //visible_layer->generateSample();
-        partition(score.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
-        partition(score.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
-
-
-        visible_layer->activation.subVec(0,14+taillePart) << score;
-        visible_layer->expectation.subVec(0,14+taillePart) << score;
-
-        input_prediction_list(ith_sample) << visible_layer->expectation;
-        
-    }
-    
-    //Vec tempo;
-    TVec<real> tempo;
-    tempo.resize(visible_layer->size);
-    ofstream myfile;
-    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/test.txt");
-    
-    for (int i = 0; i < len ; i++ ){
-        tempo << input_prediction_list(i);
-        
-        //cout << tempo[2] << endl;
-       
-        for (int j = 0; j < tempo.length() ; j++ ){
-            
-            
-                
-                
-               myfile << tempo[j] << " ";
-               
-
-               
-           
-        }
-        myfile << "\n";
-    }
-     
-
-     myfile.close();
-
-}
-void DynamicallyLinkedRBMsModel::generate(int nbNotes)
-{
-    
-    previous_hidden_layer.resize(hidden_layer->size);
-    connections_idem = connections;
-
-    for (int ith_sample = 0; ith_sample < nbNotes ; ith_sample++ ){
-        
-        input_prediction_list.resize(
-            ith_sample+1,visible_layer->size);
-        if(ith_sample > 0)
-        {
-            
-            //input_list(ith_sample_in_sequence) << previous_input;
-            //h*_{t-1}
-            //////////////////////////////////
-            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
-            hidden_layer->setAllBias(cond_bias); //**************************
-            
-            
-            
-            //up phase
-            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
-            hidden_layer->getAllActivations( connections_idem );
-            hidden_layer->computeExpectation();
-            //////////////////////////////////
-            
-            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
-            //previous_hidden_layer_activation << hidden_layer->activation;
-            
-            
-            //h*_{t}
-            ////////////
-            if(dynamic_connections_copy)
-                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            else
-                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            hidden_layer->expectation_is_not_up_to_date();
-            hidden_layer->computeExpectation();//h_{t}
-            ///////////
-            
-            //previous_input << visible_layer->expectation;//v_{t-1}
-            
-        }
-        else
-        {
-            
-            previous_hidden_layer.clear();//h_{t-1}
-            if(dynamic_connections_copy)
-                dynamic_connections_copy->fprop( previous_hidden_layer ,
-                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            else
-                dynamic_connections->fprop(previous_hidden_layer,
-                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-            
-            hidden_layer->expectation_is_not_up_to_date();
-            hidden_layer->computeExpectation();//h_{t}
-            
-            
-        }
-        
-        //connections_transpose->setAsDownInput( hidden_layer->expectation );
-        //visible_layer->getAllActivations( connections_idem_t );
-        
-        connections->setAsUpInput( hidden_layer->expectation );
-        visible_layer->getAllActivations( connections_idem );
-        
-        visible_layer->computeExpectation();
-        visible_layer->generateSample();
-        
-        input_prediction_list(ith_sample) << visible_layer->sample;
-        
-    }
-    
-    //Vec tempo;
-    TVec<int> tempo;
-    tempo.resize(visible_layer->size);
-    int theNote;
-    //int nbNoteVisiLayer = input_prediction_list(1).length()/13;
-    ofstream myfile;
-    int theLayer;
-    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_musicGeneration/data/generate/test.txt");
-    
-    for (int i = 0; i < nbNotes ; i++ ){
-        tempo << input_prediction_list(i);
-        
-        //cout << tempo[2] << endl;
-       
-        for (int j = 0; j < tempo.length() ; j++ ){
-            
-            if (tempo[j] == 1){
-                theLayer = (j/13);
-                
-                theNote = j - (13*theLayer);
-               
-
-                if (theNote<=11){
-                    //print theNote
-                    //cout << theNote+50 << " ";
-                    myfile << theNote << " ";
-                }
-                else{
-                    //print #
-                    //cout << "# ";
-                    myfile << "# ";
-                    
-                }
-     
-            }
-           
-        }
-        myfile << "\n";
-    }
-     myfile << "<oov> <oov> \n";
-
-     myfile.close();
-
-}
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-15 00:51:25 UTC (rev 9006)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-15 01:30:22 UTC (rev 9007)
@@ -164,12 +164,12 @@
     virtual TVec<std::string> getTestCostNames() const;
 
 
-    //! Generate music in a folder
-    void generate(int nbNotes);
+//    //! Generate music in a folder
+//    void generate(int nbNotes);
+//
+//    //! Generate a part of the data in a folder
+//    void gen();
 
-    //! Generate a part of the data in a folder
-    void gen();
-
     //! Returns the names of the objective costs that the train method computes
     //! and  for which it updates the VecStatsCollector train_stats.
     virtual TVec<std::string> getTrainCostNames() const;
@@ -232,69 +232,27 @@
    
     //! Stores bias gradient
     mutable Vec bias_gradient;
-
+    
      //! Stores bias gradient
     mutable Vec visi_bias_gradient;
 
-    //! Stores hidden layer target in dynamic learning phase
-    mutable Vec hidden_layer_target;
-
-    //! Stores input gradient of dynamic connections
-    mutable Vec input_gradient;
-    
     //! Stores hidden gradient of dynamic connections
     mutable Vec hidden_gradient;
     
-    //! Stores hidden gradient of dynamic connections
-    mutable Vec hidden_gradient2;
-
-    
     //! Stores hidden gradient of dynamic connections coming from time t+1
     mutable Vec hidden_temporal_gradient;
-    
-    //! Stores previous input layer value
-    mutable Vec previous_input;
-
-    //! Stores previous target layer value
-    mutable TVec< Vec > previous_targets;
-    
-    //! Stores previous hidden layer value
-    mutable Vec previous_hidden_layer;
-    mutable Vec previous_hidden_layer_activation;
-
-    //! Stores previous visible layer value
-    mutable Vec previous_visible_layer;
-
-    //! Stores a sample from the hidden layer
-    mutable Vec hidden_layer_sample;
-
-     //! Stores a expectation from the hidden layer
-    mutable Vec hidden_layer_expectation;
-
-    //! Stores a sample from the visible layer
-    mutable Vec visible_layer_sample;
-
-    //! Stores a input from the visible layer
-    mutable Vec visible_layer_input;
-
-    //! Store a copy of the positive phase values
-    mutable Vec pos_down_values;
-    mutable Vec pos_up_values;
-
-    //! Parameter of the dynamic connection
-    mutable Vec alpha;
-
+        
     //! List of hidden layers values
     mutable Mat hidden_list;
-    mutable Mat hidden_activations_list;
+    mutable Mat hidden_act_no_bias_list;
 
     //! List of second hidden layers values
     mutable Mat hidden2_list;
-    mutable Mat hidden2_activations_list;
+    mutable Mat hidden2_act_no_bias_list;
 
     //! List of target prediction values
     mutable TVec< Mat > target_prediction_list;
-    mutable TVec< Mat > target_prediction_activations_list;
+    mutable TVec< Mat > target_prediction_act_no_bias_list;
 
     //! List of inputs values
     mutable Mat input_list;



From nouiz at mail.berlios.de  Thu May 15 17:18:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 15 May 2008 17:18:58 +0200
Subject: [Plearn-commits] r9008 - trunk/plearn/vmat
Message-ID: <200805151518.m4FFIwgr013214@sheep.berlios.de>

Author: nouiz
Date: 2008-05-15 17:18:58 +0200 (Thu, 15 May 2008)
New Revision: 9008

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
in VariableDeletionVMatrix, if the threashold for missing value is >0, we will always delete columns with all missing value


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-05-15 01:30:22 UTC (rev 9007)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-05-15 15:18:58 UTC (rev 9008)
@@ -99,7 +99,8 @@
                   &VariableDeletionVMatrix::min_non_missing_threshold,
                   OptionBase::buildoption,
         "Minimum proportion of non-missing values for a variable to be kept\n"
-        "(a 1 means only variables with no missing values are kept).");
+        "(a 1 means only variables with no missing values are kept).\n"
+        "if >0, we will always remove columns with all missing value.");
 
     declareOption(ol, "max_constant_threshold",
                   &VariableDeletionVMatrix::max_constant_threshold,
@@ -267,7 +268,8 @@
         int min_non_missing =
             int(round(min_non_missing_threshold * the_train_source->length()));
         for (int i = 0; i < is; i++)
-            if (stats[i].nnonmissing() >= min_non_missing)
+            if (stats[i].nnonmissing() >= min_non_missing 
+                && stats[i].nnonmissing() > 0)
                 indices.append(i);
     } else
         for (int i = 0; i < is; i++)



From nouiz at mail.berlios.de  Thu May 15 20:39:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 15 May 2008 20:39:07 +0200
Subject: [Plearn-commits] r9009 - trunk/scripts
Message-ID: <200805151839.m4FId7p4027559@sheep.berlios.de>

Author: nouiz
Date: 2008-05-15 20:39:07 +0200 (Thu, 15 May 2008)
New Revision: 9009

Modified:
   trunk/scripts/dbidispatch
Log:
Allow fringant1


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-15 15:18:58 UTC (rev 9008)
+++ trunk/scripts/dbidispatch	2008-05-15 18:39:07 UTC (rev 9009)
@@ -232,7 +232,7 @@
 from socket import gethostname
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     p = os.path.abspath(os.path.curdir)
-    if p.startswith("/home/fringant2/") or p.startswith("/cluster") or dbi_param.get('files'):
+    if any([p.startswith(x) for x in ["/home/fringant1/","/home/fringant2/","/cluster/"]]) or dbi_param.get('files'):
         pass
     else:
         raise Exception("You must be in a subfolder of /home/fringant2/")



From larocheh at mail.berlios.de  Thu May 15 23:15:20 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 15 May 2008 23:15:20 +0200
Subject: [Plearn-commits] r9010 - trunk/plearn_learners_experimental
Message-ID: <200805152115.m4FLFKlt008527@sheep.berlios.de>

Author: larocheh
Date: 2008-05-15 23:15:19 +0200 (Thu, 15 May 2008)
New Revision: 9010

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added an option to adapt the pseudolikelihood context selection 


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-15 18:39:07 UTC (rev 9009)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-15 21:15:19 UTC (rev 9010)
@@ -41,6 +41,7 @@
 #include "PseudolikelihoodRBM.h"
 #include <plearn_learners/online/RBMLayer.h>
 #include <plearn/io/pl_log.h>
+#include <plearn/math/TMat_sort.h>
 
 #define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
 
@@ -69,6 +70,8 @@
     n_classes( -1 ),
     compute_input_space_nll( false ),
     pseudolikelihood_context_size ( 0 ),
+    pseudolikelihood_context_type( "uniform_random" ),
+    k_most_correlated( -1 ),
     nll_cost_index( -1 ),
     class_cost_index( -1 ),
     training_cpu_time_cost_index ( -1 ),
@@ -163,6 +166,26 @@
                   "(default = 0, which corresponds to standard pseudolikelihood).\n"
                   );
 
+    declareOption(ol, "pseudolikelihood_context_type", 
+                  &PseudolikelihoodRBM::pseudolikelihood_context_type,
+                  OptionBase::buildoption,
+                  "Type of context for generalized pseudolikelihood:\n"
+                  "\"uniform_random\": context elements are picked uniformly randomly\n"
+                  "\n"
+                  "- \"most_correlated\": the most correlated (positively or negatively\n"
+                  "                     elemenst with the current input element are picked\n"
+                  "\n"
+                  "- \"most_correlated_uniform_random\": context elements are picked uniformly\n"
+                  "                                    among the k_most_correlated other input\n"
+                  "                                    elements, for each current input\n"
+                  );
+
+    declareOption(ol, "k_most_correlated", 
+                  &PseudolikelihoodRBM::k_most_correlated,
+                  OptionBase::buildoption,
+                  "Number of most correlated input elements over which to sample.\n"
+                  );
+
     declareOption(ol, "input_layer", &PseudolikelihoodRBM::input_layer,
                   OptionBase::buildoption,
                   "The binomial input layer of the RBM.\n");
@@ -237,6 +260,18 @@
             PLERROR("In PseudolikelihoodRBM::build_(): "
                     "pseudolikelihood_context_size should be >= 0.");
 
+        if( pseudolikelihood_context_type != "uniform_random" &&
+            pseudolikelihood_context_type != "most_correlated" &&
+            pseudolikelihood_context_type != "most_correlated_uniform_random" )
+            PLERROR("In PseudolikelihoodRBM::build_(): "
+                    "pseudolikelihood_context_type is not valid.");
+
+        if( pseudolikelihood_context_type == "most_correlated"
+            && pseudolikelihood_context_size <= 0 )
+            PLERROR("In PseudolikelihoodRBM::train(): "
+                    "pseudolikelihood_context_size should be > 0 "
+                    "for \"most_correlated\" context type");        
+
         build_layers_and_connections();
         build_costs();
 
@@ -422,6 +457,8 @@
     deepCopyField(connection_gradient, copies);
     deepCopyField(context_indices, copies);
     deepCopyField(context_indices_per_i, copies);
+    deepCopyField(correlations_per_i, copies);
+    deepCopyField(context_most_correlated, copies);
     deepCopyField(hidden_activations_context, copies);
     deepCopyField(hidden_activations_context_k_gradient, copies);
     deepCopyField(nums, copies);
@@ -468,6 +505,7 @@
     Z_is_up_to_date = false;
 
     persistent_gibbs_chain_is_started.fill( false );
+    correlations_per_i.resize(0,0);
 }
 
 ///////////
@@ -667,28 +705,211 @@
                 }
                 else
                 {
-                    // Generate contexts
-                    for( int i=0; i<context_indices.length(); i++)
-                        context_indices[i] = i;
-                    int tmp,k;
-                    int n = input_layer->size-1;
-                    int* c = context_indices.data();
-                    int* ci;
-                    for( int i=0; i<context_indices_per_i.length(); i++)
+                    if( ( pseudolikelihood_context_type == "most_correlated" ||
+                          pseudolikelihood_context_type == "most_correlated_uniform_random" )
+                        && correlations_per_i.length() == 0 )
                     {
-                        ci = context_indices_per_i[i];
-                        for (int j = 0; j < context_indices_per_i.width(); j++) {
-                            k = j + random_gen->uniform_multinomial_sample(n - j);
-                            tmp = c[j];
-                            c[j] = c[k];
-                            c[k] = tmp;
-                            if( c[j] >= i )
-                                ci[j] = c[j]+1;
-                            else
-                                ci[j] = c[j];
+                        Vec corr_input(inputsize());
+                        Vec corr_target(targetsize());
+                        real corr_weight;
+                        Vec mean(inputsize());
+                        mean.clear();
+                        for(int t=0; t<train_set->length(); t++)
+                        {
+                            train_set->getExample(t,corr_input,corr_target,
+                                                  corr_weight);
+                            mean += corr_input;
                         }
+                        mean /= train_set->length();
+                        
+                        correlations_per_i.resize(inputsize(),inputsize());
+                        correlations_per_i.clear();
+                        Mat cov(inputsize(), inputsize());
+                        cov.clear();
+                        for(int t=0; t<train_set->length(); t++)
+                        {
+                            train_set->getExample(t,corr_input,corr_target,
+                                                  corr_weight);
+                            corr_input -= mean;
+                            externalProductAcc(cov,
+                                               corr_input,corr_input);
+                        }
+                        //correlations_per_i /= train_set->length();
+
+                        for( int i=0; i<inputsize(); i++ )
+                            for( int j=0; j<inputsize(); j++)
+                            {
+                                correlations_per_i(i,j) = 
+                                    abs(cov(i,j)) 
+                                    / sqrt(cov(i,i)*cov(j,j));
+                            }
+
+                        if( pseudolikelihood_context_type == "most_correlated")
+                        {
+                            if( pseudolikelihood_context_size <= 0 )
+                                PLERROR("In PseudolikelihoodRBM::train(): "
+                                    "pseudolikelihood_context_size should be > 0 "
+                                    "for \"most_correlated\" context type");
+                            real current_min;
+                            int current_min_position;
+                            real* corr;
+                            int* context;
+                            Vec context_corr(pseudolikelihood_context_size);
+                            context_indices_per_i.resize(
+                                inputsize(),
+                                pseudolikelihood_context_size);
+
+                            // HUGO: this is quite inefficient for big 
+                            // pseudolikelihood_context_sizes, should use a heap
+                            for( int i=0; i<inputsize(); i++ )
+                            {
+                                current_min = REAL_MAX;
+                                current_min_position = -1;
+                                corr = correlations_per_i[i];
+                                context = context_indices_per_i[i];
+                                for( int j=0; j<inputsize(); j++ )
+                                {
+                                    if( i == j )
+                                        continue;
+
+                                    // Filling first pseudolikelihood_context_size elements
+                                    if( j - (j>i?1:0) < pseudolikelihood_context_size )
+                                    {
+                                        context[j - (j>i?1:0)] = j;
+                                        context_corr[j - (j>i?1:0)] = corr[j];
+                                        if( current_min > corr[j] )
+                                        {
+                                            current_min = corr[j];
+                                            current_min_position = j - (j>i?1:0);
+                                        }
+                                        continue;
+                                    }
+
+                                    if( corr[j] > current_min )
+                                    {
+                                        context[current_min_position] = j;
+                                        context_corr[current_min_position] = corr[j];
+                                        current_min = 
+                                            min( context_corr, 
+                                                 current_min_position );
+                                    }
+                                }
+                            }
+                        }
+                        
+                        if( pseudolikelihood_context_type == 
+                            "most_correlated_uniform_random" )
+                        {
+                            if( k_most_correlated < 
+                                pseudolikelihood_context_size )
+                                PLERROR("In PseudolikelihoodRBM::train(): "
+                                        "k_most_correlated should be "
+                                        ">= pseudolikelihood_context_size");
+
+                            if( k_most_correlated > inputsize() - 1 )
+                                PLERROR("In PseudolikelihoodRBM::train(): "
+                                        "k_most_correlated should be "
+                                        "< inputsize()");
+
+                            real current_min;
+                            int current_min_position;
+                            real* corr;
+                            int* context;
+                            Vec context_corr( k_most_correlated );
+                            context_most_correlated.resize( inputsize() );
+
+                            // HUGO: this is quite inefficient for big 
+                            // pseudolikelihood_context_sizes, should use a heap
+                            for( int i=0; i<inputsize(); i++ )
+                            {
+                                context_most_correlated[i].resize( 
+                                    k_most_correlated );
+                                current_min = REAL_MAX;
+                                current_min_position = -1;
+                                corr = correlations_per_i[i];
+                                context = context_most_correlated[i].data();
+                                for( int j=0; j<inputsize(); j++ )
+                                {
+                                    if( i == j )
+                                        continue;
+
+                                    // Filling first k_most_correlated elements
+                                    if( j - (j>i?1:0) <  k_most_correlated )
+                                    {
+                                        context[j - (j>i?1:0)] = j;
+                                        context_corr[j - (j>i?1:0)] = corr[j];
+                                        if( current_min > corr[j] )
+                                        {
+                                            current_min = corr[j];
+                                            current_min_position = j - (j>i?1:0);
+                                        }
+                                        continue;
+                                    }
+
+                                    if( corr[j] > current_min )
+                                    {
+                                        context[current_min_position] = j;
+                                        context_corr[current_min_position] = corr[j];
+                                        current_min = 
+                                            min( context_corr, 
+                                                 current_min_position );
+                                    }
+                                }
+                            }
+                        }                        
                     }
 
+                    if( pseudolikelihood_context_type == "uniform_random" ||
+                        pseudolikelihood_context_type == "most_correlated_uniform_random" )
+                    {
+                        // Generate contexts
+                        if( pseudolikelihood_context_type == "uniform_random" )
+                            for( int i=0; i<context_indices.length(); i++)
+                                context_indices[i] = i;
+                        int tmp,k;
+                        int* c;
+                        int n;
+                        if( pseudolikelihood_context_type == "uniform_random" )
+                        {
+                            c = context_indices.data();
+                            n = input_layer->size-1;
+                        }
+                        int* ci;
+                        for( int i=0; i<context_indices_per_i.length(); i++)
+                        {
+                            if( pseudolikelihood_context_type == 
+                                "most_correlated_uniform_random" )
+                            {
+                                c = context_most_correlated[i].data();
+                                n = context_most_correlated[i].length();
+                            }
+
+                            ci = context_indices_per_i[i];
+                            for (int j = 0; j < context_indices_per_i.width(); j++) 
+                            {
+                                k = j + 
+                                    random_gen->uniform_multinomial_sample(n - j);
+                                
+                                tmp = c[j];
+                                c[j] = c[k];
+                                c[k] = tmp;
+
+                                if( pseudolikelihood_context_type 
+                                    == "uniform_random" )
+                                {
+                                    if( c[j] >= i )
+                                        ci[j] = c[j]+1;
+                                    else
+                                        ci[j] = c[j];
+                                }
+
+                                if( pseudolikelihood_context_type == 
+                                    "most_correlated_uniform_random" )
+                                    ci[j] = c[j];
+                            }
+                        }
+                    }
+
                     connection->setAsDownInput( input );
                     hidden_layer->getAllActivations( 
                         (RBMMatrixConnection *) connection );

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-15 18:39:07 UTC (rev 9009)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-15 21:15:19 UTC (rev 9010)
@@ -104,7 +104,7 @@
 
     //! Number of classes in the training set (for supervised learning)
     int n_classes;
-
+    
     //! Indication that the input space NLL should be computed
     //! during test
     bool compute_input_space_nll;
@@ -114,6 +114,20 @@
     //! (default = 0, which corresponds to standard pseudolikelihood)
     int pseudolikelihood_context_size;
 
+    //! Type of context for generalized pseudolikelihood:
+    //! - "uniform_random": context elements are picked uniformly randomly
+    //! 
+    //! - "most_correlated": the most correlated (positively or negatively
+    //!                      elemenst with the current input element are picked
+    //!
+    //! - "most_correlated_uniform_random": context elements are picked uniformly
+    //!                                     among the k_most_correlated other input
+    //!                                     elements, for each current input
+    string pseudolikelihood_context_type;
+
+    //! Number of most correlated input elements over which to sample
+    real k_most_correlated;
+
     //! The binomial input layer of the RBM
     PP<RBMBinomialLayer> input_layer;
 
@@ -245,6 +259,8 @@
     mutable Mat connection_gradient;
     mutable TVec<int> context_indices;
     mutable TMat<int> context_indices_per_i;
+    mutable Mat correlations_per_i;
+    mutable TVec< TVec< int > > context_most_correlated;
     mutable Mat hidden_activations_context;
     mutable Vec hidden_activations_context_k_gradient;
     mutable Vec nums;



From larocheh at mail.berlios.de  Thu May 15 23:24:24 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 15 May 2008 23:24:24 +0200
Subject: [Plearn-commits] r9011 - trunk/plearn_learners_experimental
Message-ID: <200805152124.m4FLOO15009012@sheep.berlios.de>

Author: larocheh
Date: 2008-05-15 23:24:23 +0200 (Thu, 15 May 2008)
New Revision: 9011

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Change the type of k_most_correlated to int


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-15 21:15:19 UTC (rev 9010)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-15 21:24:23 UTC (rev 9011)
@@ -126,7 +126,7 @@
     string pseudolikelihood_context_type;
 
     //! Number of most correlated input elements over which to sample
-    real k_most_correlated;
+    int k_most_correlated;
 
     //! The binomial input layer of the RBM
     PP<RBMBinomialLayer> input_layer;



From nouiz at mail.berlios.de  Fri May 16 18:56:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 May 2008 18:56:35 +0200
Subject: [Plearn-commits] r9012 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200805161656.m4GGuZKO013264@sheep.berlios.de>

Author: nouiz
Date: 2008-05-16 18:56:33 +0200 (Fri, 16 May 2008)
New Revision: 9012

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
Added the --env option for condor


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-15 21:24:23 UTC (rev 9011)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-16 16:56:33 UTC (rev 9012)
@@ -675,6 +675,7 @@
         self.file_redirect_stdout = False
         self.file_redirect_stderr = False
         self.redirect_stderr_to_stdout = False
+        self.env = ''
 
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
@@ -836,9 +837,11 @@
         if self.files: #ON_EXIT_OR_EVICT
             condor_dat.write( dedent('''\
                 when_to_transfer_output = ON_EXIT
-                should_transfer_files = Yes
-                transfer_input_files = %s
+                should_transfer_files   = Yes
+                transfer_input_files    = %s
                 '''%(self.files+','+launch_file+','+self.tasks[0].commands[0].split()[0]))) # no directory
+        if self.env:
+            condor_dat.write('environment    = '+self.env+'\n')
         if self.raw:
             condor_dat.write( self.raw+'\n')
         if self.rank:

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-15 21:24:23 UTC (rev 9011)
+++ trunk/scripts/dbidispatch	2008-05-16 16:56:33 UTC (rev 9012)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--testdbi] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] [--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--raw=CONDOR_EXPRESSION] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--testdbi] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] [--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--env=VAR=VALUE][--raw=CONDOR_EXPRESSION] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -76,6 +76,7 @@
         witch is equivalent to
      dbidispatch '--req=regexp("computer0*", target.Machine)'
   The '--nice'('--no_nice') option set the nice_user option to condor. If nice, the job(s) will have the lowest possible priority.
+  The '--env=VAR=VALUE' option will set in the environment of the executing jobs the variable VAR with value VALUE. To pass many variable you can 1) use one --env option and separ the value by ';'(don't forget to quote) or 2) you can pass many time the --env parameter.
   The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
   If the CONDOR_HOME environment variable is set, then the HOME variable will
      be set to this value for jobs submitted to condor.
@@ -174,7 +175,7 @@
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
         dbi_param[argv[5:]]=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
-                                "--req", "--files", "--raw", "--rank"]:
+                                "--req", "--files", "--raw", "--rank", "--env"]:
         param=argv.split('=')[0][2:]
         if param in ["req", "files", "rank"]:
             #param that we happend to if defined more then one time
@@ -183,6 +184,9 @@
         elif param == "raw":
             dbi_param.setdefault(param,'')
             dbi_param[param]+='\n'+argv.split('=',1)[1]
+        elif param=="env":
+            dbi_param.setdefault(param,"")
+            dbi_param[param]+=";"+argv.split('=',1)[1]
         else:
             #otherwise we erase the old value
             dbi_param[param]=argv.split('=',1)[1]



From laulysta at mail.berlios.de  Fri May 16 19:13:02 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 16 May 2008 19:13:02 +0200
Subject: [Plearn-commits] r9013 - trunk/plearn_learners_experimental
Message-ID: <200805161713.m4GHD24V001126@sheep.berlios.de>

Author: laulysta
Date: 2008-05-16 19:12:52 +0200 (Fri, 16 May 2008)
New Revision: 9013

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
avec moin de bug


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 16:56:33 UTC (rev 9012)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 17:12:52 UTC (rev 9013)
@@ -79,6 +79,7 @@
     ""
     );
 
+
 DynamicallyLinkedRBMsModel::DynamicallyLinkedRBMsModel() :
     //rbm_learning_rate( 0.01 ),
     recurrent_net_learning_rate( 0.01),
@@ -263,7 +264,7 @@
                             "symbolic targets is not implemented.");
                 if( dict->size() == 0 )
                     PLERROR("DynamicallyLinkedRBMsModel::build_(): dictionary "
-                            "of field %d is empty", i);
+                            "of field %d is empty", tar);
 
                 target_symbol_sizes[tar_layer].push_back(dict->size());
                 target_layers_n_of_target_elements[tar_layer]++;
@@ -426,7 +427,7 @@
     input_layer->forget();
     hidden_layer->forget();
     input_connections->forget();
-    dynamic_connections-forget();
+    dynamic_connections->forget();
     if( hidden_layer2 )
     {
         hidden_layer2->forget();
@@ -586,6 +587,7 @@
                 for( int tar=0; tar < target_layers.length(); tar++ )
                 {
                     if( use_target_layers_masks )
+                    {
                         clamp_units(target.subVec(
                                         sum_target_elements,
                                         target_layers_n_of_target_elements[tar]),
@@ -597,15 +599,18 @@
                                         target_layers_n_of_target_elements[tar]),
                                     masks_list[tar](ith_sample_in_sequence)
                             );
+                    }
                     else
+                    {
                         clamp_units(target.subVec(
                                         sum_target_elements,
                                         target_layers_n_of_target_elements[tar]),
                                     target_layers[tar],
                                     target_symbol_sizes[tar]);
-                    sum_target_elements += target_layers_n_of_target_elements[tar];
-                    targets_list[tar](ith_sample_in_sequence) << 
-                        target_layers[tar]->expectation;
+                        sum_target_elements += target_layers_n_of_target_elements[tar];
+                        targets_list[tar](ith_sample_in_sequence) << 
+                            target_layers[tar]->expectation;
+                    }
                 }
                 
                 input_connections->fprop( input_list(ith_sample_in_sequence), 
@@ -621,7 +626,7 @@
                         dynamic_actvation_contribution;
                 }
                  
-                hidden_layer->fprop( hidden_act_no_bias_list(ith_sample_in_sequence) 
+                hidden_layer->fprop( hidden_act_no_bias_list(ith_sample_in_sequence), 
                                      hidden_list(ith_sample_in_sequence) );
                  
                 if( hidden_layer2 )
@@ -755,11 +760,11 @@
     layer->setExpectation( layer->expectation );
 }
 
-void DynamicallyLinkedRBMsModel::clamp_units(const Vec& layer_vector,
+void DynamicallyLinkedRBMsModel::clamp_units(const Vec layer_vector,
                                              PP<RBMLayer> layer,
                                              TVec<int> symbol_sizes,
-                                             const Vec& original_mask,
-                                             Vec& formated_mask) const
+                                             const Vec original_mask,
+                                             Vec formated_mask) const
 {
     int it = 0;
     int ss = -1;

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-16 16:56:33 UTC (rev 9012)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-16 17:12:52 UTC (rev 9013)
@@ -86,7 +86,7 @@
     real end_of_sequence_symbol;
 
     //! The input layer of the model
-    TVec<RBMLayer> input_layer;
+    PP<RBMLayer> input_layer;
 
     //! The target layers of the model
     TVec< PP<RBMLayer> > target_layers;
@@ -163,6 +163,7 @@
     //! thus the test method).
     virtual TVec<std::string> getTestCostNames() const;
 
+    
 
 //    //! Generate music in a folder
 //    void generate(int nbNotes);
@@ -183,9 +184,9 @@
 
     //! Clamps the layer units based on a layer vector
     //! and provides the associated mask in the correct format.
-    void clamp_units(const Vec& layer_vector, PP<RBMLayer> layer,
-                     TVec<int> symbol_sizes, const Vec& original_mask,
-                     Vec& formated_mask) const;
+    void clamp_units(const Vec layer_vector, PP<RBMLayer>& layer,
+                     TVec<int>& symbol_sizes, const Vec original_mask,
+                     Vec formated_mask) const;
     
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,



From nouiz at mail.berlios.de  Fri May 16 19:25:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 May 2008 19:25:24 +0200
Subject: [Plearn-commits] r9014 - trunk/scripts
Message-ID: <200805161725.m4GHPO7m032679@sheep.berlios.de>

Author: nouiz
Date: 2008-05-16 19:25:23 +0200 (Fri, 16 May 2008)
New Revision: 9014

Modified:
   trunk/scripts/dbidispatch
Log:
better help message and bugfix for the --test option


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-16 17:12:52 UTC (rev 9013)
+++ trunk/scripts/dbidispatch	2008-05-16 17:25:23 UTC (rev 9014)
@@ -3,7 +3,17 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--testdbi] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] [--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--env=VAR=VALUE][--raw=CONDOR_EXPRESSION] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] <back-end parameter> {--file=FILEPATH | <command-template>}
+
+<back-end parameter>:
+    bqtools, cluster option  :[--duree=X]
+    bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
+    all except condor options:[--[*no_]nb_proc=N]
+    cluster, condor options  : [--32|--64|--3264]
+    condor option            : [--req="CONDOR_REQUIREMENT"] [--[*no_]nice] [--[*no_]getenv] [*--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--env=VAR=VALUE[;VAR2=VALUE2]][--raw=CONDOR_EXPRESSION]
+    cluster option           : [--mem=X] [--os=X] [*--[no_]cwait]  [--[*no_]force] [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
+An * after '[' signals the default value.
+'''
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -12,20 +22,19 @@
   The -h, --help print the long help(this)
   The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
   The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
-  The '--test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
+  The '--[no_]test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
   The '--testdbi' set only dbi in test mode. Not dbidispatch
-  The '--no_test' option cancel the '--test' option.
   The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.
 
 dbidispatch --test --file=tests
 
 
 bqtools, cluster, local and ssh options:
-  --nb_proc=nb_proc, specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly.
-    --local=X is the same as --local --nb_proc=X
-    --cluster=X is the same as --cluster --nb_proc=X
-    --bqtools=X is the same as --bqtools --nb_proc=X
-    --ssh=X is the same as --ssh --nb_proc=X
+  --nb_proc=nb_proc, specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly. (work for condor, bqtools, cluster with --cwait)
+    --local=N is the same as --local --nb_proc=N
+    --cluster=N is the same as --cluster --nb_proc=N
+    --bqtools=N is the same as --bqtools --nb_proc=N
+    --ssh=N is the same as --ssh --nb_proc=N
 
 bqtools and cluster option:
   The '--duree' option specifies the maximum duration of the jobs. The syntax depends on where the job is dispatched. For the cluster syntax, see 'cluster --help'. For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 13 minutes and 15 seconds.
@@ -48,8 +57,7 @@
   The '--3264', '--32' or '--64' specify which type of cpu the node must have to execute the commands.
 
 cluster only options:
-  The '--cwait' is transfered to cluster. This must be enabled if there is not nb_proc available nodes. Otherwise when there are no nodes available, the launch of that command fails.
-  The '--no_cwait' means the --cwait option is not given to the cluster command, as in the default.
+  The '--[no_]cwait' is transfered to cluster. This must be enabled if there is not nb_proc available nodes. Otherwise when there are no nodes available, the launch of that command fails.
   The '--mem=X' speficify the number of meg the program need to execute.
   The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
   The '--force' option is passed to cluster
@@ -57,15 +65,15 @@
   The '--cpu=nb_cpu_per_node' option is passed to cluster
 
 condor only options:
-  The '--getenv'('--no_getenv') option is forwarded to condor. If True, the current environnement variable will be forwarded to the execution node.
+  The '--[no_]getenv' option is forwarded to condor. If True, the current environnement variable will be forwarded to the execution node.
   The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:
 
   dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
      or
   dbidispatch '--req=Machine=="computer.example.com"'
 
-  The '--server'(--no_server) option add the requirement that the executing host must be a server dedicated to computing. This is equivalent to: dbidispatch '--req=SERVER==True'(SERVER==False)
-  The '--prefserver' option will tell that you prefer to execute on server first. This is equivalent to 'rank=SERVER=?=True' in the submit file.
+  The '--[no_]server' option add the requirement that the executing host must be a server dedicated to computing. This is equivalent to: dbidispatch '--req=SERVER==True'(SERVER==False)
+  The '--[no_]prefserver' option will tell that you prefer to execute on server first. This is equivalent to 'rank=SERVER=?=True' in the submit file.
   The '--rank=STRING' option add rank=STRING in the submit file.
   The '--machine=full_host_name' option add the requirement that the executing host is full_host_name
      dbidispatch --machine=computer.example.com
@@ -75,7 +83,7 @@
      dbidispatch '--machines=computer00*'
         witch is equivalent to
      dbidispatch '--req=regexp("computer0*", target.Machine)'
-  The '--nice'('--no_nice') option set the nice_user option to condor. If nice, the job(s) will have the lowest possible priority.
+  The '--[no_]nice' option set the nice_user option to condor. If nice, the job(s) will have the lowest possible priority.
   The '--env=VAR=VALUE' option will set in the environment of the executing jobs the variable VAR with value VALUE. To pass many variable you can 1) use one --env option and separ the value by ';'(don't forget to quote) or 2) you can pass many time the --env parameter.
   The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
   If the CONDOR_HOME environment variable is set, then the HOME variable will
@@ -112,7 +120,7 @@
 
 In the file of the option --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
 
-The '--no_clean_up' set the DBI option clean_up to false
+The '--[*no_]clean_up' set the DBI option clean_up to true(false)
 
 The environnement variable DBIDISPATCH_DEFAULT_OPTION can contain default option that you always want to pass to dbidispatch. You can override them on the command line.
 """%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}
@@ -165,13 +173,21 @@
         if len(argv)>7:
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
-    elif argv in  ["--force", "--interruptible", "--long", "--test",
-                   "--getenv", "--cwait", "--nice"]:
+    elif argv in  ["--force", "--interruptible", "--long", 
+                   "--getenv", "--cwait", "--clean_up" ,"--nice"]:
         dbi_param[argv[2:]]=True
         testmode=True
     elif argv=="--testdbi":
         dbi_param["test"]=True
-    elif argv in ["--no_force", "--no_interruptible", "--no_long", "--no_test",
+    elif argv=="--no_testdbi":
+        dbi_param["test"]=False
+    elif argv=="--test":
+        dbi_param[argv[2:]]=True
+        testmode=True
+    elif argv=="--no_test":
+        dbi_param[argv[2:]]=True
+        testmode=False
+    elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
         dbi_param[argv[5:]]=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
@@ -227,7 +243,8 @@
     valid_dbi_param +=["cwait","force","nb_proc","arch","interruptible",
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
-    valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "raw"]
+    valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
+                       "raw"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long","nb_proc","duree"]
 elif launch_cmd=="Local":



From nouiz at mail.berlios.de  Fri May 16 19:31:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 May 2008 19:31:11 +0200
Subject: [Plearn-commits] r9015 - trunk/plearn/vmat
Message-ID: <200805161731.m4GHVBWV000184@sheep.berlios.de>

Author: nouiz
Date: 2008-05-16 19:31:10 +0200 (Fri, 16 May 2008)
New Revision: 9015

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
correctly implemented the option mtime in VMatrix


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-05-16 17:25:23 UTC (rev 9014)
+++ trunk/plearn/vmat/VMatrix.cc	2008-05-16 17:31:10 UTC (rev 9015)
@@ -76,6 +76,7 @@
 VMatrix::VMatrix(bool call_build_):
     inherited   (call_build_),
     mtime_      (0),
+    mtime_update(0),
     length_     (-1),
     width_      (-1),
     inputsize_  (-1),
@@ -92,6 +93,7 @@
 VMatrix::VMatrix(int the_length, int the_width, bool call_build_):
     inherited                       (call_build_),
     mtime_                          (0),
+    mtime_update                    (0),
     length_                         (the_length),
     width_                          (the_width),
     inputsize_                      (-1),
@@ -152,11 +154,11 @@
         "And if it is the source inside another VMatrix that sets its \n"
         "metadatadir, it will often be set from that surrounding vmat's metadata.\n");
 
-//     declareOption(
-//         ol, "mtime", &VMatrix::mtime_, 
-//         OptionBase::buildoption|OptionBase::nosave,
-//         "DO NOT play with this if you don't know the implementation!\n"
-//         "The modification time of this VMatrix. Defaults to 0(unknow)");
+    declareOption(
+        ol, "mtime", &VMatrix::mtime_update, 
+        OptionBase::buildoption|OptionBase::nosave,
+        "DO NOT play with this if you don't know the implementation!\n"
+        "This add a dependency mtime to the gived value.");
 
     inherited::declareOptions(ol);
 }
@@ -476,6 +478,8 @@
 {
     if(!metadatadir.isEmpty())
         setMetaDataDir(metadatadir); // make sure we perform all necessary operations
+    if(mtime_update!=0)
+        updateMtime(mtime_update);
 }
 
 ///////////

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-05-16 17:25:23 UTC (rev 9014)
+++ trunk/plearn/vmat/VMatrix.h	2008-05-16 17:31:10 UTC (rev 9015)
@@ -89,6 +89,7 @@
     mutable Vec dotrow_1;
     mutable Vec dotrow_2;
     time_t mtime_;          ///< Time of "last modification" of data files.
+    time_t mtime_update;    
 
 protected:
 



From larocheh at mail.berlios.de  Fri May 16 19:41:04 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 16 May 2008 19:41:04 +0200
Subject: [Plearn-commits] r9016 - trunk/plearn_learners_experimental
Message-ID: <200805161741.m4GHf4pJ000651@sheep.berlios.de>

Author: larocheh
Date: 2008-05-16 19:41:04 +0200 (Fri, 16 May 2008)
New Revision: 9016

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
Je t'aime Fred (oui, oui, je sais que tu lis les commits ;-) )


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 17:31:10 UTC (rev 9015)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 17:41:04 UTC (rev 9016)
@@ -525,16 +525,16 @@
                     recurrent_update();
                     
                     ith_sample_in_sequence = 0;
-                    hidden_list.clear();
-                    hidden_act_no_bias_list.clear();
-                    hidden2_list.clear();
-                    hidden2_act_no_bias_list.clear();
-                    target_prediction_list.clear();
-                    target_prediction_act_no_bias_list.clear();
-                    input_list.clear();
-                    targets_list.clear();
-                    nll_list.clear();
-                    masks_list.clear();
+                    hidden_list.resize(0);
+                    hidden_act_no_bias_list.resize(0);
+                    hidden2_list.resize(0);
+                    hidden2_act_no_bias_list.resize(0);
+                    target_prediction_list.resize(0);
+                    target_prediction_act_no_bias_list.resize(0);
+                    input_list.resize(0);
+                    targets_list.resize(0);
+                    nll_list.resize(0);
+                    masks_list.resize(0);
                     continue;
                 }
 
@@ -599,6 +599,7 @@
                                         target_layers_n_of_target_elements[tar]),
                                     masks_list[tar](ith_sample_in_sequence)
                             );
+
                     }
                     else
                     {
@@ -607,10 +608,10 @@
                                         target_layers_n_of_target_elements[tar]),
                                     target_layers[tar],
                                     target_symbol_sizes[tar]);
-                        sum_target_elements += target_layers_n_of_target_elements[tar];
-                        targets_list[tar](ith_sample_in_sequence) << 
-                            target_layers[tar]->expectation;
                     }
+                    sum_target_elements += target_layers_n_of_target_elements[tar];
+                    targets_list[tar](ith_sample_in_sequence) << 
+                        target_layers[tar]->expectation;
                 }
                 
                 input_connections->fprop( input_list(ith_sample_in_sequence), 
@@ -768,8 +769,8 @@
 {
     int it = 0;
     int ss = -1;
-    formated_mask.resize( layer->size );
-    PLASSER( original_mask.length() == layer_vector.length() );
+    PLASSERT( original_mask.length() == layer_vector.length() );
+    PLASSERT( layer->size == formated_mask.length() );
     for(int i=0; i<layer_vector.length(); i++)
     {
         ss = symbol_sizes[i];

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-16 17:31:10 UTC (rev 9015)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-16 17:41:04 UTC (rev 9016)
@@ -244,28 +244,28 @@
     mutable Vec hidden_temporal_gradient;
         
     //! List of hidden layers values
-    mutable Mat hidden_list;
-    mutable Mat hidden_act_no_bias_list;
+    mutable TVec< Vec > hidden_list;
+    mutable TVec< Vec > hidden_act_no_bias_list;
 
     //! List of second hidden layers values
-    mutable Mat hidden2_list;
-    mutable Mat hidden2_act_no_bias_list;
+    mutable TVec< Vec > hidden2_list;
+    mutable TVec< Vec > hidden2_act_no_bias_list;
 
     //! List of target prediction values
-    mutable TVec< Mat > target_prediction_list;
-    mutable TVec< Mat > target_prediction_act_no_bias_list;
+    mutable TVec< TVec< Vec > > target_prediction_list;
+    mutable TVec< TVec< Vec > > target_prediction_act_no_bias_list;
 
     //! List of inputs values
-    mutable Mat input_list;
+    mutable TVec< Vec > input_list;
 
     //! List of inputs values
-    mutable TVec< Mat > targets_list;
+    mutable TVec< TVec< Vec > > targets_list;
 
     //! List of the nll of the input samples in a sequence
-    mutable Mat nll_list;
+    mutable TVec< Vec > nll_list;
 
     //! List of all targets' masks
-    mutable TVec< Mat > masks_list;
+    mutable TVec< TVec< Vec > > masks_list;
 
     //! Contribution of dynamic weights to hidden layer activation
     mutable Vec dynamic_activation_contribution;



From nouiz at mail.berlios.de  Fri May 16 19:57:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 May 2008 19:57:35 +0200
Subject: [Plearn-commits] r9017 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200805161757.m4GHvZ51001624@sheep.berlios.de>

Author: nouiz
Date: 2008-05-16 19:57:35 +0200 (Fri, 16 May 2008)
New Revision: 9017

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
implemented the --os option for condor


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-16 17:41:04 UTC (rev 9016)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-16 17:57:35 UTC (rev 9017)
@@ -676,6 +676,7 @@
         self.file_redirect_stderr = False
         self.redirect_stderr_to_stdout = False
         self.env = ''
+        self.os = ''
 
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
@@ -805,15 +806,15 @@
         self.temp_files.append(condor_file)
         condor_dat = open( condor_file, 'w' )
 
-        req=""
+        if self.req:
+            req = req+'&&('+self.req+')'
         if self.targetcondorplatform == 'BOTH':
             req="((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
         else :
             req="(Arch == \"%s\")"%(self.targetcondorplatform)
+        if self.os:
+            req+='&&(OpSyS == "'+self.os+'")'
 
-        if self.req != "":
-            req = req+'&&('+self.req+')'
-
         source_file=os.getenv("CONDOR_LOCAL_SOURCE")
         condor_home = os.getenv('CONDOR_HOME')
         if source_file and source_file.endswith(".cshrc"):

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-16 17:41:04 UTC (rev 9016)
+++ trunk/scripts/dbidispatch	2008-05-16 17:57:35 UTC (rev 9017)
@@ -9,9 +9,9 @@
     bqtools, cluster option  :[--duree=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
     all except condor options:[--[*no_]nb_proc=N]
-    cluster, condor options  : [--32|--64|--3264]
+    cluster, condor options  : [--32|--64|--3264] [--os=X]
     condor option            : [--req="CONDOR_REQUIREMENT"] [--[*no_]nice] [--[*no_]getenv] [*--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--env=VAR=VALUE[;VAR2=VALUE2]][--raw=CONDOR_EXPRESSION]
-    cluster option           : [--mem=X] [--os=X] [*--[no_]cwait]  [--[*no_]force] [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
+    cluster option           : [--mem=X] [*--[no_]cwait]  [--[*no_]force] [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
 An * after '[' signals the default value.
 '''
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
@@ -244,7 +244,7 @@
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
-                       "raw"]
+                       "raw", "os"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long","nb_proc","duree"]
 elif launch_cmd=="Local":



From larocheh at mail.berlios.de  Fri May 16 20:19:42 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 16 May 2008 20:19:42 +0200
Subject: [Plearn-commits] r9018 - trunk/plearn_learners_experimental
Message-ID: <200805161819.m4GIJgeb002974@sheep.berlios.de>

Author: larocheh
Date: 2008-05-16 20:19:42 +0200 (Fri, 16 May 2008)
New Revision: 9018

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
Debugging!


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 17:57:35 UTC (rev 9017)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 18:19:42 UTC (rev 9018)
@@ -402,7 +402,7 @@
     deepCopyField( targets_list , copies);
     deepCopyField( nll_list , copies);
     deepCopyField( masks_list , copies);
-    deepCopyField( dynamic_activation_contribution, copies);
+    deepCopyField( dynamic_act_no_bias_contribution, copies);
 
 
     // deepCopyField(, copies);
@@ -533,36 +533,35 @@
                     target_prediction_act_no_bias_list.resize(0);
                     input_list.resize(0);
                     targets_list.resize(0);
-                    nll_list.resize(0);
+                    nll_list.resize(0,0);
                     masks_list.resize(0);
                     continue;
                 }
 
                 // Resize internal variables
-                hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-                hidden_act_no_bias_list.resize(ith_sample_in_sequence+1,
-                                               hidden_layer->size);
+                hidden_list.resize(ith_sample_in_sequence+1);
+                hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
                 if( hidden_layer2 )
                 {
-                    hidden2_list.resize(ith_sample_in_sequence+1,
-                                        hidden_layer2->size);
-                    hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1,
-                                                    hidden_layer2->size);
+                    hidden2_list.resize(ith_sample_in_sequence+1);
+                    hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
                 }
                  
-                input_list.resize(ith_sample_in_sequence+1,input_layer->size);
+                input_list.resize(ith_sample_in_sequence+1);
+                input_list[ith_sample_in_sequence].resize(input_layer->size);
 
                 targets_list.resize( target_layers.length() );
                 target_prediction_list.resize( target_layers.length() );
                 target_prediction_act_no_bias_list.resize( target_layers.length() );
                 for( int tar=0; tar < target_layers.length(); tar++ )
                 {
-                    targets_list[tar].resize( ith_sample_in_sequence+1,
-                                              target_layers[tar]->size );
+                    targets_list[tar].resize( ith_sample_in_sequence+1);
+                    targets_list[tar][ith_sample_in_sequence].resize( 
+                        target_layers[tar]->size);
                     target_prediction_list[tar].resize(
-                        ith_sample_in_sequence+1, target_layers[tar]->size);
+                        ith_sample_in_sequence+1);
                     target_prediction_act_no_bias_list[tar].resize(
-                        ith_sample_in_sequence+1, target_layers[tar]->size);
+                        ith_sample_in_sequence+1);
 
                 }
                 nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
@@ -570,8 +569,7 @@
                 {
                     masks_list.resize( target_layers.length() );
                     for( int tar=0; tar < target_layers.length(); tar++ )
-                        masks_list[tar].resize( ith_sample_in_sequence+1,
-                                                target_layers[tar]->size );
+                        masks_list[tar].resize( ith_sample_in_sequence+1 );
                 }
 
                 // Forward propagation
@@ -580,7 +578,7 @@
                 clamp_units(input.subVec(0,inputsize_without_masks),
                             input_layer,
                             input_symbol_sizes);                
-                input_list(ith_sample_in_sequence) << input_layer->expectation;
+                input_list[ith_sample_in_sequence] << input_layer->expectation;
 
                 // Fetch right representation for target
                 sum_target_elements = 0;
@@ -597,7 +595,7 @@
                                         inputsize_without_masks 
                                         + sum_target_elements, 
                                         target_layers_n_of_target_elements[tar]),
-                                    masks_list[tar](ith_sample_in_sequence)
+                                    masks_list[tar][ith_sample_in_sequence]
                             );
 
                     }
@@ -610,52 +608,52 @@
                                     target_symbol_sizes[tar]);
                     }
                     sum_target_elements += target_layers_n_of_target_elements[tar];
-                    targets_list[tar](ith_sample_in_sequence) << 
+                    targets_list[tar][ith_sample_in_sequence] << 
                         target_layers[tar]->expectation;
                 }
                 
-                input_connections->fprop( input_list(ith_sample_in_sequence), 
-                                          hidden_act_no_bias_list(ith_sample_in_sequence));
+                input_connections->fprop( input_list[ith_sample_in_sequence], 
+                                          hidden_act_no_bias_list[ith_sample_in_sequence]);
                 
                 if( ith_sample_in_sequence > 0 )
                 {
                     dynamic_connections->fprop( 
-                        hidden_list(ith_sample_in_sequence-1),
+                        hidden_list[ith_sample_in_sequence-1],
                         dynamic_act_no_bias_contribution );
 
-                    hidden_act_no_bias_list(ith_sample_in_sequence) += 
-                        dynamic_actvation_contribution;
+                    hidden_act_no_bias_list[ith_sample_in_sequence] += 
+                        dynamic_act_no_bias_contribution;
                 }
                  
-                hidden_layer->fprop( hidden_act_no_bias_list(ith_sample_in_sequence), 
-                                     hidden_list(ith_sample_in_sequence) );
+                hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
+                                     hidden_list[ith_sample_in_sequence] );
                  
                 if( hidden_layer2 )
                 {
                     hidden_connections->fprop( 
-                        hidden_list(ith_sample_in_sequence),
-                        hidden2_act_no_bias_list(ith_sample_in_sequence));
+                        hidden_list[ith_sample_in_sequence],
+                        hidden2_act_no_bias_list[ith_sample_in_sequence]);
 
                     hidden_layer2->fprop( 
-                        hidden2_act_no_bias_list(ith_sample_in_sequence) 
-                        hidden2_list(ith_sample_in_sequence) 
+                        hidden2_act_no_bias_list[ith_sample_in_sequence],
+                        hidden2_list[ith_sample_in_sequence] 
                         );
 
                     for( int tar=0; tar < target_layers.length(); tar++ )
                     {
                         target_connections[tar]->fprop(
-                            hidden2_list(ith_sample_in_sequence),
-                            target_prediction_act_no_bias_list[tar](
-                                ith_sample_in_sequence)
+                            hidden2_list[ith_sample_in_sequence],
+                            target_prediction_act_no_bias_list[tar][
+                                ith_sample_in_sequence]
                             );
                         target_layers[tar]->fprop(
-                            target_prediction_act_no_bias_list[tar](
-                                ith_sample_in_sequence),
-                            target_prediction_list[tar](
-                                ith_sample_in_sequence) );
+                            target_prediction_act_no_bias_list[tar][
+                                ith_sample_in_sequence],
+                            target_prediction_list[tar][
+                                ith_sample_in_sequence] );
                         if( use_target_layers_masks )
-                            target_prediction_list[tar]( ith_sample_in_sequence) *= 
-                                masks_list[tar](ith_sample_in_sequence);
+                            target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                                masks_list[tar][ith_sample_in_sequence];
                     }
                 }
                 else
@@ -663,18 +661,18 @@
                     for( int tar=0; tar < target_layers.length(); tar++ )
                     {
                         target_connections[tar]->fprop(
-                            hidden_list(ith_sample_in_sequence),
-                            target_prediction_act_no_bias_list[tar](
-                                ith_sample_in_sequence)
+                            hidden_list[ith_sample_in_sequence],
+                            target_prediction_act_no_bias_list[tar][
+                                ith_sample_in_sequence]
                             );
                         target_layers[tar]->fprop(
-                            target_prediction_act_no_bias_list[tar](
-                                ith_sample_in_sequence),
-                            target_prediction_list[tar](
-                                ith_sample_in_sequence) );
+                            target_prediction_act_no_bias_list[tar][
+                                ith_sample_in_sequence],
+                            target_prediction_list[tar][
+                                ith_sample_in_sequence] );
                         if( use_target_layers_masks )
-                            target_prediction_list[tar]( ith_sample_in_sequence) *= 
-                                masks_list[tar](ith_sample_in_sequence);
+                            target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                                masks_list[tar][ith_sample_in_sequence];
                     }
                 }
 
@@ -682,15 +680,15 @@
                 for( int tar=0; tar < target_layers.length(); tar++ )
                 {
                     target_layers[tar]->activation << 
-                        target_prediction_act_no_bias_list[tar](
-                            ith_sample_in_sequence);
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence];
                     target_layers[tar]->activation += target_layers[tar]->bias;
                     target_layers[tar]->setExpectation(
-                        target_prediction_list[tar](
-                            ith_sample_in_sequence));
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence]);
                     nll_list(ith_sample_in_sequence,tar) = 
-                        target_layer->fpropNLL( 
-                            targets_list[tar](ith_sample_in_sequence) ); 
+                        target_layers[tar]->fpropNLL( 
+                            targets_list[tar][ith_sample_in_sequence] ); 
                     train_costs[tar] += nll_list(ith_sample_in_sequence,tar);
 
                     // Normalize by the number of things to predict
@@ -704,7 +702,7 @@
                             target_layers_n_of_target_elements[tar];
                     }
                     else
-                        train_n_itmes[tar]++;
+                        train_n_items[tar]++;
                 }
                 ith_sample_in_sequence++;
                
@@ -734,8 +732,6 @@
     train_stats->finalize();
 }
 
-}
-
 void DynamicallyLinkedRBMsModel::clamp_units(const Vec& layer_vector,
                                              PP<RBMLayer> layer,
                                              TVec<int> symbol_sizes) const
@@ -936,76 +932,76 @@
         costs.fill(-1);
         test_stats->update(costs);
     }
-
+    
+    int ith_sample_in_sequence = 0;
     for (int i = 0; i < len; i++)
     {
         testset.getExample(i, input, target, weight);
 
         if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
-        {                    
+        {
             ith_sample_in_sequence = 0;
-            hidden_list.clear();
-            hidden_act_no_bias_list.clear();
-            hidden2_list.clear();
-            hidden2_act_no_bias_list.clear();
-            target_prediction_list.clear();
-            target_prediction_act_no_bias_list.clear();
-            input_list.clear();
-            targets_list.clear();
-            nll_list.clear();
-            masks_list.clear();
+            hidden_list.resize(0);
+            hidden_act_no_bias_list.resize(0);
+            hidden2_list.resize(0);
+            hidden2_act_no_bias_list.resize(0);
+            target_prediction_list.resize(0);
+            target_prediction_act_no_bias_list.resize(0);
+            input_list.resize(0);
+            targets_list.resize(0);
+            nll_list.resize(0,0);
+            masks_list.resize(0);
             continue;
         }
 
         // Resize internal variables
-        hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
-        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1,
-                                       hidden_layer->size);
+        hidden_list.resize(ith_sample_in_sequence+1);
+        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
         if( hidden_layer2 )
         {
-            hidden2_list.resize(ith_sample_in_sequence+1,
-                                hidden_layer2->size);
-            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1,
-                                            hidden_layer2->size);
+            hidden2_list.resize(ith_sample_in_sequence+1);
+            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
         }
-        
-        input_list.resize(ith_sample_in_sequence+1,input_layer->size);
-        
+                 
+        input_list.resize(ith_sample_in_sequence+1);
+        input_list[ith_sample_in_sequence].resize(input_layer->size);
+
         targets_list.resize( target_layers.length() );
         target_prediction_list.resize( target_layers.length() );
         target_prediction_act_no_bias_list.resize( target_layers.length() );
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
-            targets_list[tar].resize( ith_sample_in_sequence+1,
-                                      target_layers[tar]->size );
+            targets_list[tar].resize( ith_sample_in_sequence+1);
+            targets_list[tar][ith_sample_in_sequence].resize( 
+                target_layers[tar]->size);
             target_prediction_list[tar].resize(
-                ith_sample_in_sequence+1, target_layers[tar]->size);
+                ith_sample_in_sequence+1);
             target_prediction_act_no_bias_list[tar].resize(
-                ith_sample_in_sequence+1, target_layers[tar]->size);
-            
+                ith_sample_in_sequence+1);
+
         }
         nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
         if( use_target_layers_masks )
         {
             masks_list.resize( target_layers.length() );
             for( int tar=0; tar < target_layers.length(); tar++ )
-                masks_list[tar].resize( ith_sample_in_sequence+1,
-                                        target_layers[tar]->size );
+                masks_list[tar].resize( ith_sample_in_sequence+1 );
         }
-        
+
         // Forward propagation
-        
+
         // Fetch right representation for input
         clamp_units(input.subVec(0,inputsize_without_masks),
                     input_layer,
                     input_symbol_sizes);                
-        input_list(ith_sample_in_sequence) << input_layer->expectation;
-        
+        input_list[ith_sample_in_sequence] << input_layer->expectation;
+
         // Fetch right representation for target
         sum_target_elements = 0;
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
             if( use_target_layers_masks )
+            {
                 clamp_units(target.subVec(
                                 sum_target_elements,
                                 target_layers_n_of_target_elements[tar]),
@@ -1015,61 +1011,65 @@
                                 inputsize_without_masks 
                                 + sum_target_elements, 
                                 target_layers_n_of_target_elements[tar]),
-                            masks_list[tar](ith_sample_in_sequence)
+                            masks_list[tar][ith_sample_in_sequence]
                     );
+
+            }
             else
+            {
                 clamp_units(target.subVec(
                                 sum_target_elements,
                                 target_layers_n_of_target_elements[tar]),
                             target_layers[tar],
                             target_symbol_sizes[tar]);
+            }
             sum_target_elements += target_layers_n_of_target_elements[tar];
-            targets_list[tar](ith_sample_in_sequence) << 
+            targets_list[tar][ith_sample_in_sequence] << 
                 target_layers[tar]->expectation;
         }
-        
-        input_connections->fprop( input_list(ith_sample_in_sequence), 
-                                  hidden_act_no_bias_list(ith_sample_in_sequence));
-        
+                
+        input_connections->fprop( input_list[ith_sample_in_sequence], 
+                                  hidden_act_no_bias_list[ith_sample_in_sequence]);
+                
         if( ith_sample_in_sequence > 0 )
         {
             dynamic_connections->fprop( 
-                hidden_list(ith_sample_in_sequence-1),
+                hidden_list[ith_sample_in_sequence-1],
                 dynamic_act_no_bias_contribution );
-            
-            hidden_act_no_bias_list(ith_sample_in_sequence) += 
-                dynamic_actvation_contribution;
+
+            hidden_act_no_bias_list[ith_sample_in_sequence] += 
+                dynamic_act_no_bias_contribution;
         }
-        
-        hidden_layer->fprop( hidden_act_no_bias_list(ith_sample_in_sequence) 
-                             hidden_list(ith_sample_in_sequence) );
-        
+                 
+        hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
+                             hidden_list[ith_sample_in_sequence] );
+                 
         if( hidden_layer2 )
         {
             hidden_connections->fprop( 
-                hidden_list(ith_sample_in_sequence),
-                hidden2_act_no_bias_list(ith_sample_in_sequence));
-            
+                hidden_list[ith_sample_in_sequence],
+                hidden2_act_no_bias_list[ith_sample_in_sequence]);
+
             hidden_layer2->fprop( 
-                hidden2_act_no_bias_list(ith_sample_in_sequence) 
-                hidden2_list(ith_sample_in_sequence) 
+                hidden2_act_no_bias_list[ith_sample_in_sequence],
+                hidden2_list[ith_sample_in_sequence] 
                 );
-            
+
             for( int tar=0; tar < target_layers.length(); tar++ )
             {
                 target_connections[tar]->fprop(
-                    hidden2_list(ith_sample_in_sequence),
-                    target_prediction_act_no_bias_list[tar](
-                        ith_sample_in_sequence)
+                    hidden2_list[ith_sample_in_sequence],
+                    target_prediction_act_no_bias_list[tar][
+                        ith_sample_in_sequence]
                     );
                 target_layers[tar]->fprop(
-                    target_prediction_act_no_bias_list[tar](
-                        ith_sample_in_sequence),
-                    target_prediction_list[tar](
-                        ith_sample_in_sequence) );
+                    target_prediction_act_no_bias_list[tar][
+                        ith_sample_in_sequence],
+                    target_prediction_list[tar][
+                        ith_sample_in_sequence] );
                 if( use_target_layers_masks )
-                    target_prediction_list[tar]( ith_sample_in_sequence) *= 
-                        masks_list[tar](ith_sample_in_sequence);
+                    target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                        masks_list[tar][ith_sample_in_sequence];
             }
         }
         else
@@ -1077,48 +1077,48 @@
             for( int tar=0; tar < target_layers.length(); tar++ )
             {
                 target_connections[tar]->fprop(
-                    hidden_list(ith_sample_in_sequence),
-                    target_prediction_act_no_bias_list[tar](
-                        ith_sample_in_sequence)
+                    hidden_list[ith_sample_in_sequence],
+                    target_prediction_act_no_bias_list[tar][
+                        ith_sample_in_sequence]
                     );
                 target_layers[tar]->fprop(
-                    target_prediction_act_no_bias_list[tar](
-                        ith_sample_in_sequence),
-                    target_prediction_list[tar](
-                        ith_sample_in_sequence) );
+                    target_prediction_act_no_bias_list[tar][
+                        ith_sample_in_sequence],
+                    target_prediction_list[tar][
+                        ith_sample_in_sequence] );
                 if( use_target_layers_masks )
-                    target_prediction_list[tar]( ith_sample_in_sequence) *= 
-                        masks_list[tar](ith_sample_in_sequence);
+                    target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                        masks_list[tar][ith_sample_in_sequence];
             }
         }
-   
+
         if (testoutputs)
         {
             int sum_target_layers_size = 0;
             for( int tar=0; tar < target_layers.length(); tar++ )
             {
                 output.subVec(sum_target_layers_size,target_layers[tar]->size)
-                    << target_prediction_list[tar]( ith_sample_in_sequence);
+                    << target_prediction_list[tar][ ith_sample_in_sequence ];
                 sum_target_layers_size += target_layers[tar]->size;
             }
             testoutputs->putOrAppendRow(i, output);
         }
-     
+
         sum_target_elements = 0;
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
             target_layers[tar]->activation << 
-                target_prediction_act_no_bias_list[tar](
-                    ith_sample_in_sequence);
+                target_prediction_act_no_bias_list[tar][
+                    ith_sample_in_sequence];
             target_layers[tar]->activation += target_layers[tar]->bias;
             target_layers[tar]->setExpectation(
-                target_prediction_list[tar](
-                    ith_sample_in_sequence));
+                target_prediction_list[tar][
+                    ith_sample_in_sequence]);
             nll_list(ith_sample_in_sequence,tar) = 
-                target_layer->fpropNLL( 
-                    targets_list[tar](ith_sample_in_sequence) ); 
+                target_layers[tar]->fpropNLL( 
+                    targets_list[tar][ith_sample_in_sequence] ); 
             costs[tar] += nll_list(ith_sample_in_sequence,tar);
-            
+
             // Normalize by the number of things to predict
             if( use_target_layers_masks )
             {
@@ -1130,17 +1130,18 @@
                     target_layers_n_of_target_elements[tar];
             }
             else
-                n_itmes[tar]++;
+                n_items[tar]++;
         }
         ith_sample_in_sequence++;
 
         if (report_progress)
             pb->update(i);
 
-        for(int i=0; i<costs.length(); i++)
-            costs[i] /= n_items[i];
     }
 
+    for(int i=0; i<costs.length(); i++)
+        costs[i] /= n_items[i];
+
     if (testcosts)
         testcosts->putOrAppendRow(i, costs);
     
@@ -1148,17 +1149,16 @@
         test_stats->update(costs, weight);
     
     ith_sample_in_sequence = 0;
-    hidden_list.clear();
-    hidden_act_no_bias_list.clear();
-    hidden2_list.clear();
-    hidden2_act_no_bias_list.clear();
-    target_prediction_list.clear();
-    target_prediction_act_no_bias_list.clear();
-    input_list.clear();
-    targets_list.clear();
-    nll_list.clear();
-    masks_list.clear();
-   
+    hidden_list.resize(0);
+    hidden_act_no_bias_list.resize(0);
+    hidden2_list.resize(0);
+    hidden2_act_no_bias_list.resize(0);
+    target_prediction_list.resize(0);
+    target_prediction_act_no_bias_list.resize(0);
+    input_list.resize(0);
+    targets_list.resize(0);
+    nll_list.resize(0,0);
+    masks_list.resize(0);   
 }
 
 

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-16 17:57:35 UTC (rev 9017)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-16 18:19:42 UTC (rev 9018)
@@ -262,13 +262,13 @@
     mutable TVec< TVec< Vec > > targets_list;
 
     //! List of the nll of the input samples in a sequence
-    mutable TVec< Vec > nll_list;
+    mutable Mat nll_list;
 
     //! List of all targets' masks
     mutable TVec< TVec< Vec > > masks_list;
 
     //! Contribution of dynamic weights to hidden layer activation
-    mutable Vec dynamic_activation_contribution;
+    mutable Vec dynamic_act_no_bias_contribution;
 
 protected:
     //#####  Protected Member Functions  ######################################



From laulysta at mail.berlios.de  Fri May 16 20:28:42 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 16 May 2008 20:28:42 +0200
Subject: [Plearn-commits] r9019 - trunk/plearn_learners_experimental
Message-ID: <200805161828.m4GISgiL003822@sheep.berlios.de>

Author: laulysta
Date: 2008-05-16 20:28:41 +0200 (Fri, 16 May 2008)
New Revision: 9019

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
Log:
cool


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 18:19:42 UTC (rev 9018)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 18:28:41 UTC (rev 9019)
@@ -732,6 +732,8 @@
     train_stats->finalize();
 }
 
+
+
 void DynamicallyLinkedRBMsModel::clamp_units(const Vec& layer_vector,
                                              PP<RBMLayer> layer,
                                              TVec<int> symbol_sizes) const
@@ -750,7 +752,7 @@
         {
             // Convert to one-hot vector
             layer->expectation.subVec(it,ss).clear();
-            layer->expectation[it+round(input[i])] = 1;
+            layer->expectation[it+round(layer_vector[i])] = 1;
             it += ss;
         }
     }



From nouiz at mail.berlios.de  Fri May 16 20:42:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 May 2008 20:42:24 +0200
Subject: [Plearn-commits] r9020 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200805161842.m4GIgOBK005008@sheep.berlios.de>

Author: nouiz
Date: 2008-05-16 20:42:24 +0200 (Fri, 16 May 2008)
New Revision: 9020

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
implemented the --mem=N option for condor. If not present will set the size of the application


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-16 18:28:41 UTC (rev 9019)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-16 18:42:24 UTC (rev 9020)
@@ -667,6 +667,9 @@
     def __init__( self, commands, **args ):
         self.getenv = False
         self.nice = False
+        # in Meg for initialization for consistency with cluster
+        # then in kilo as that is what is needed by condor
+        self.mem = 0
         self.req = ''
         self.raw = ''
         self.rank = ''
@@ -679,6 +682,7 @@
         self.os = ''
 
         DBIBase.__init__(self, commands, **args)
+        self.mem=int(self.mem)*1024
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
 
@@ -821,7 +825,12 @@
             launch_file = os.path.join(self.log_dir, 'launch.csh')
         else:
             launch_file = os.path.join(self.log_dir, 'launch.sh')
-            
+
+        if self.mem<=0:
+            try:
+                self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size
+            except:
+                pass
         condor_dat.write( dedent('''\
                 executable     = %s
                 universe       = vanilla
@@ -835,6 +844,10 @@
                        self.log_dir,
                        self.log_dir,
                        self.log_dir,str(self.getenv),str(self.nice))))
+        if self.mem>0:
+            #condor need value in Kb
+            condor_dat.write('ImageSize      = %d\n'%(self.mem))
+
         if self.files: #ON_EXIT_OR_EVICT
             condor_dat.write( dedent('''\
                 when_to_transfer_output = ON_EXIT

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-16 18:28:41 UTC (rev 9019)
+++ trunk/scripts/dbidispatch	2008-05-16 18:42:24 UTC (rev 9020)
@@ -9,9 +9,9 @@
     bqtools, cluster option  :[--duree=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
     all except condor options:[--[*no_]nb_proc=N]
-    cluster, condor options  : [--32|--64|--3264] [--os=X]
+    cluster, condor options  : [--32|--64|--3264] [--os=X] [--mem=N]
     condor option            : [--req="CONDOR_REQUIREMENT"] [--[*no_]nice] [--[*no_]getenv] [*--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--env=VAR=VALUE[;VAR2=VALUE2]][--raw=CONDOR_EXPRESSION]
-    cluster option           : [--mem=X] [*--[no_]cwait]  [--[*no_]force] [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
+    cluster option           : [*--[no_]cwait]  [--[*no_]force] [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
 An * after '[' signals the default value.
 '''
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
@@ -55,11 +55,11 @@
 
 cluster and condor options:
   The '--3264', '--32' or '--64' specify which type of cpu the node must have to execute the commands.
+  The '--mem=X' speficify the number of meg the program need to execute.
+  The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
 
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. This must be enabled if there is not nb_proc available nodes. Otherwise when there are no nodes available, the launch of that command fails.
-  The '--mem=X' speficify the number of meg the program need to execute.
-  The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
   The '--force' option is passed to cluster
   The '--interruptible' option is passed to cluster
   The '--cpu=nb_cpu_per_node' option is passed to cluster



From larocheh at mail.berlios.de  Fri May 16 20:53:37 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 16 May 2008 20:53:37 +0200
Subject: [Plearn-commits] r9021 - trunk/plearn_learners_experimental
Message-ID: <200805161853.m4GIrbOV005923@sheep.berlios.de>

Author: larocheh
Date: 2008-05-16 20:53:32 +0200 (Fri, 16 May 2008)
New Revision: 9021

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
Now compiles!!!


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 18:42:24 UTC (rev 9020)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 18:53:32 UTC (rev 9021)
@@ -734,7 +734,7 @@
 
 
 
-void DynamicallyLinkedRBMsModel::clamp_units(const Vec& layer_vector,
+void DynamicallyLinkedRBMsModel::clamp_units(const Vec layer_vector,
                                              PP<RBMLayer> layer,
                                              TVec<int> symbol_sizes) const
 {
@@ -752,7 +752,7 @@
         {
             // Convert to one-hot vector
             layer->expectation.subVec(it,ss).clear();
-            layer->expectation[it+round(layer_vector[i])] = 1;
+            layer->expectation[it+(int)layer_vector[i]] = 1;
             it += ss;
         }
     }
@@ -763,12 +763,12 @@
                                              PP<RBMLayer> layer,
                                              TVec<int> symbol_sizes,
                                              const Vec original_mask,
-                                             Vec formated_mask) const
+                                             Vec& formated_mask) const
 {
     int it = 0;
     int ss = -1;
     PLASSERT( original_mask.length() == layer_vector.length() );
-    PLASSERT( layer->size == formated_mask.length() );
+    formated_mask.resize(layer->size);
     for(int i=0; i<layer_vector.length(); i++)
     {
         ss = symbol_sizes[i];
@@ -783,7 +783,7 @@
             // Convert to one-hot vector
             layer->expectation.subVec(it,ss).clear();
             formated_mask.subVec(it,ss).fill(original_mask[i]);
-            layer->expectation[it+round(input[i])] = 1;
+            layer->expectation[it+(int)layer_vector[i]] = 1;
             it += ss;
         }
     }
@@ -795,7 +795,7 @@
     input_layer->setLearningRate( the_learning_rate );
     hidden_layer->setLearningRate( the_learning_rate );
     input_connections->setLearningRate( the_learning_rate );
-    dynamic_connections-forget();
+    dynamic_connections->forget();
     if( hidden_layer2 )
     {
         hidden_layer2->setLearningRate( the_learning_rate );
@@ -820,14 +820,14 @@
             {
                 for( int tar=0; tar<target_layers.length(); tar++)
                 {
-                    target_layer[tar]->activation << targets_prediction_act_no_bias_list[tar](i);
-                    target_layer[tar]->activation += target_layer[tar]->bias;
-                    target_layer[tar]->setExpectation(targets_prediction_list[tar](i));
-                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
+                    target_layers[tar]->activation += target_layers[tar]->bias;
+                    target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
+                    target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
                     bias_gradient *= target_layers_weights[tar];
-                    bias_gradient *= mask_list[tar](i);
+                    bias_gradient *= masks_list[tar][i];
                     target_layers[tar]->update(bias_gradient);
-                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                    target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
                                                          hidden_gradient, bias_gradient,true);
                 }
             }
@@ -835,13 +835,13 @@
             {
                 for( int tar=0; tar<target_layers.length(); tar++)
                 {
-                    target_layer[tar]->activation << targets_prediction_act_no_bias_list[tar](i);
-                    target_layer[tar]->activation += target_layer[tar]->bias;
-                    target_layer[tar]->setExpectation(targets_prediction_list[tar](i));
-                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
+                    target_layers[tar]->activation += target_layers[tar]->bias;
+                    target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
+                    target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
                     bias_gradient *= target_layers_weights[tar];
                     target_layers[tar]->update(bias_gradient);
-                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                    target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
                                                          hidden_gradient, bias_gradient,true); 
                 }
             }
@@ -849,12 +849,12 @@
             if (hidden_layer2)
             {
                 hidden_layer2->bpropUpdate(
-                    hidden2_act_no_bias_list(i), hidden2_list(i),
+                    hidden2_act_no_bias_list[i], hidden2_list[i],
                     bias_gradient, hidden_gradient);
                 
                 hidden_connections->bpropUpdate(
-                    hidden_list(i),
-                    hidden2_act_no_bias_list(i), 
+                    hidden_list[i],
+                    hidden2_act_no_bias_list[i], 
                     hidden_gradient, bias_gradient);
             }
             
@@ -863,30 +863,30 @@
                 hidden_gradient += hidden_temporal_gradient;
                 
                 hidden_layer->bpropUpdate(
-                    hidden_act_no_bias_list(i), hidden_list(i),
+                    hidden_act_no_bias_list[i], hidden_list[i],
                     hidden_temporal_gradient, hidden_gradient);
                 
                 dynamic_connections->bpropUpdate(
-                    hidden_list(i-1),
-                    hidden_act_no_bias_list(i), // Here, it should be cond_bias, but doesn't matter
+                    hidden_list[i-1],
+                    hidden_act_no_bias_list[i], // Here, it should be cond_bias, but doesn't matter
                     hidden_gradient, hidden_temporal_gradient);
                 
                 hidden_temporal_gradient << hidden_gradient;
                 
-                connections->bpropUpdate(
-                    input_list(i),
-                    hidden_act_no_bias_list(i), 
+                input_connections->bpropUpdate(
+                    input_list[i],
+                    hidden_act_no_bias_list[i], 
                     visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
                 
             }
             else
             {
                 hidden_layer->bpropUpdate(
-                    hidden_act_no_bias_list(i), hidden_list(i),
+                    hidden_act_no_bias_list[i], hidden_list[i],
                     hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
-                connections->bpropUpdate(
-                    input_list(i),
-                    hidden_act_no_bias_list(i), 
+                input_connections->bpropUpdate(
+                    input_list[i],
+                    hidden_act_no_bias_list[i], 
                     visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
 
                 // Could learn initial value for h_{-1}
@@ -936,6 +936,9 @@
     }
     
     int ith_sample_in_sequence = 0;
+    int inputsize_without_masks = inputsize() 
+        - ( use_target_layers_masks ? targetsize() : 0 );
+    int sum_target_elements = 0;
     for (int i = 0; i < len; i++)
     {
         testset.getExample(i, input, target, weight);
@@ -1145,7 +1148,7 @@
         costs[i] /= n_items[i];
 
     if (testcosts)
-        testcosts->putOrAppendRow(i, costs);
+        testcosts->putOrAppendRow(0, costs);
     
     if (test_stats)
         test_stats->update(costs, weight);

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-16 18:42:24 UTC (rev 9020)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-16 18:53:32 UTC (rev 9021)
@@ -179,14 +179,14 @@
     void partition(TVec<double> part, TVec<double> periode, TVec<double> vel ) const;
     
     //! Clamps the layer units based on a layer vector
-    void clamp_units(const Vec& layer_vector, PP<RBMLayer> layer,
+    void clamp_units(const Vec layer_vector, PP<RBMLayer> layer,
                      TVec<int> symbol_sizes) const;
 
     //! Clamps the layer units based on a layer vector
     //! and provides the associated mask in the correct format.
-    void clamp_units(const Vec layer_vector, PP<RBMLayer>& layer,
-                     TVec<int>& symbol_sizes, const Vec original_mask,
-                     Vec formated_mask) const;
+    void clamp_units(const Vec layer_vector, PP<RBMLayer> layer,
+                     TVec<int> symbol_sizes, const Vec original_mask,
+                     Vec &formated_mask) const;
     
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,



From larocheh at mail.berlios.de  Fri May 16 22:05:11 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 16 May 2008 22:05:11 +0200
Subject: [Plearn-commits] r9022 - trunk/plearn_learners_experimental
Message-ID: <200805162005.m4GK5BW9011166@sheep.berlios.de>

Author: larocheh
Date: 2008-05-16 22:05:10 +0200 (Fri, 16 May 2008)
New Revision: 9022

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added an option to reconstruct only the masked inputs


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-16 18:53:32 UTC (rev 9021)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-16 20:05:10 UTC (rev 9022)
@@ -67,6 +67,8 @@
     use_mean_field_cd( false ),
     denoising_learning_rate( 0. ),
     denoising_decrease_ct( 0. ),
+    fraction_of_masked_inputs( 0. ),
+    only_reconstruct_masked_inputs( false ),
     n_classes( -1 ),
     compute_input_space_nll( false ),
     pseudolikelihood_context_size ( 0 ),
@@ -145,6 +147,11 @@
                   "Fraction of input components set to 0 for denoising "
                   "autoencoder learning.\n");
 
+    declareOption(ol, "only_reconstruct_masked_inputs", 
+                  &PseudolikelihoodRBM::only_reconstruct_masked_inputs,
+                  OptionBase::buildoption,
+                  "Indication that only the masked inputs should be reconstructed.\n");
+
     declareOption(ol, "n_classes", &PseudolikelihoodRBM::n_classes,
                   OptionBase::buildoption,
                   "Number of classes in the training set (for supervised learning).\n"
@@ -268,7 +275,7 @@
 
         if( pseudolikelihood_context_type == "most_correlated"
             && pseudolikelihood_context_size <= 0 )
-            PLERROR("In PseudolikelihoodRBM::train(): "
+            PLERROR("In PseudolikelihoodRBM::build_(): "
                     "pseudolikelihood_context_size should be > 0 "
                     "for \"most_correlated\" context type");        
 
@@ -1321,6 +1328,15 @@
                 
                 input_layer->bpropNLL(input, cost, 
                                       reconstruction_activation_gradient);
+                if( only_reconstruct_masked_inputs && 
+                    fraction_of_masked_inputs > 0 )
+                {
+                    for( int j=round(fraction_of_masked_inputs*input_layer->size) ; 
+                         j < input_layer->size ; 
+                         j++)
+                        reconstruction_activation_gradient[ 
+                            autoencoder_input_indices[j] ] = 0; 
+                }
                 input_layer->update( reconstruction_activation_gradient );
 
                 transpose_connection->bpropUpdate( 

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-16 18:53:32 UTC (rev 9021)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-05-16 20:05:10 UTC (rev 9022)
@@ -102,6 +102,9 @@
     //! Fraction of input components set to 0 for denoising autoencoder learning
     real fraction_of_masked_inputs;
 
+    //! Indication that only the masked inputs should be reconstructed
+    bool only_reconstruct_masked_inputs;
+
     //! Number of classes in the training set (for supervised learning)
     int n_classes;
     



From lamblin at mail.berlios.de  Fri May 16 22:21:28 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 16 May 2008 22:21:28 +0200
Subject: [Plearn-commits] r9023 - trunk/python_modules/plearn/pymake
Message-ID: <200805162021.m4GKLSfY012479@sheep.berlios.de>

Author: lamblin
Date: 2008-05-16 22:21:27 +0200 (Fri, 16 May 2008)
New Revision: 9023

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Seems to work, at least on mammouth


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-05-16 20:05:10 UTC (rev 9022)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-05-16 20:21:27 UTC (rev 9023)
@@ -74,7 +74,6 @@
     print
     print 'Options from pymake:'
     print """
-    
 Options specifying the type of compiled file to produce:
   -dll: create a dll instead of an executable file.
         It probably works ONLY on Windows with MinGW installed.
@@ -94,6 +93,9 @@
   -static: produce a statically linked executable.
 
 Options that will not affect the final compiled file:
+  -dbi=<dbi_mode>: use the DBI interface instead of launching compilation
+                   directly on remote hosts.
+                   <dbi_mode> can be one of Bqtools, Condor, or Cluster.
   -force: force recompilation of all necessary files,
           even if they are up to date.
   -link: force relinking of the target, even if it is up to date.
@@ -1047,7 +1049,6 @@
 
 
 def dbi_parallel_compile(files_to_compile, dbi_mode):
-
     #FIXME: Prepare something?
     commands = []
 
@@ -1064,8 +1065,8 @@
 
     from plearn.parallel.dbi import DBI
     jobs = DBI(commands, dbi_mode)
+    jobs.micro = 10 # Compile 10 files on each node
     jobs.run()
-    jobs.wait()
 
 def win32_parallel_compile(files_to_compile):
     """files_to_compile is a list of FileInfo of .cc files"""



From laulysta at mail.berlios.de  Fri May 16 23:12:26 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 16 May 2008 23:12:26 +0200
Subject: [Plearn-commits] r9024 - trunk/plearn_learners/online
Message-ID: <200805162112.m4GLCQ5c015102@sheep.berlios.de>

Author: laulysta
Date: 2008-05-16 23:12:25 +0200 (Fri, 16 May 2008)
New Revision: 9024

Modified:
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
Log:


Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-05-16 20:21:27 UTC (rev 9023)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-05-16 21:12:25 UTC (rev 9024)
@@ -53,7 +53,9 @@
     real the_learning_rate ) :
     inherited(the_learning_rate), 
     rbm_matrix_connection(the_rbm_matrix_connection)
-{}
+{
+    build();
+}
 
 void RBMMatrixTransposeConnection::declareOptions(OptionList& ol)
 {



From laulysta at mail.berlios.de  Fri May 16 23:13:21 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 16 May 2008 23:13:21 +0200
Subject: [Plearn-commits] r9025 - trunk/plearn_learners_experimental
Message-ID: <200805162113.m4GLDLw3015146@sheep.berlios.de>

Author: laulysta
Date: 2008-05-16 23:13:21 +0200 (Fri, 16 May 2008)
New Revision: 9025

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
Log:
bug free


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 21:12:25 UTC (rev 9024)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 21:13:21 UTC (rev 9025)
@@ -248,14 +248,18 @@
         int tar_layer = 0;
         int tar_layer_size = 0;
         target_symbol_sizes.resize(target_layers.length());
+        for( int tar_layer=0; tar_layer<target_layers.length(); 
+             tar_layer++ )
+            target_symbol_sizes[tar_layer].resize(0);
+        target_layers_n_of_target_elements.resize( targetsize() );
         target_layers_n_of_target_elements.clear();
+
         for( int tar=0; tar<targetsize(); tar++)
         {
             if( tar_layer > target_layers.length() )
                 PLERROR("DynamicallyLinkedRBMsModel::build_(): target layers "
                         "does not cover all targets.");            
 
-            target_symbol_sizes[tar_layer].resize(0);
             dict = train_set->getDictionary(tar+inputsize());
             if(dict)
             {
@@ -452,7 +456,7 @@
     real weight = 0; // Unused
     Vec train_costs( getTrainCostNames().length() );
     train_costs.clear();
-    TVec<int> train_n_items( getTrainCostNames().length() );
+    Vec train_n_items( getTrainCostNames().length() );
 
     if( !initTrain() )
     {
@@ -811,10 +815,14 @@
 
 void DynamicallyLinkedRBMsModel::recurrent_update()
 {
-    
+        hidden_temporal_gradient.resize(hidden_layer->size);
         hidden_temporal_gradient.clear();
         for(int i=hidden_list.length()-1; i>=0; i--){   
 
+            if( hidden_layer2 )
+                hidden_gradient.resize(hidden_layer2->size);
+            else
+                hidden_gradient.resize(hidden_layer->size);
             hidden_gradient.clear();
             if(use_target_layers_masks)
             {
@@ -827,8 +835,12 @@
                     bias_gradient *= target_layers_weights[tar];
                     bias_gradient *= masks_list[tar][i];
                     target_layers[tar]->update(bias_gradient);
-                    target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                         hidden_gradient, bias_gradient,true);
+                    if( hidden_layer2 )
+                        target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                             hidden_gradient, bias_gradient,true);
+                    else
+                        target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                             hidden_gradient, bias_gradient,true);
                 }
             }
             else
@@ -841,8 +853,13 @@
                     target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
                     bias_gradient *= target_layers_weights[tar];
                     target_layers[tar]->update(bias_gradient);
-                    target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                         hidden_gradient, bias_gradient,true); 
+                    if( hidden_layer2 )
+                        target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                             hidden_gradient, bias_gradient,true); 
+                    else
+                        target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                             hidden_gradient, bias_gradient,true); 
+
                 }
             }
 
@@ -922,7 +939,7 @@
     Vec output(outputsize());
     Vec costs(nTestCosts());
     costs.clear();
-    TVec<int> n_items(nTestCosts());
+    Vec n_items(nTestCosts());
     n_items.clear();
 
     PP<ProgressBar> pb;



From larocheh at mail.berlios.de  Mon May 19 04:29:16 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 19 May 2008 04:29:16 +0200
Subject: [Plearn-commits] r9026 - trunk/plearn_learners_experimental
Message-ID: <200805190229.m4J2TGqV018129@sheep.berlios.de>

Author: larocheh
Date: 2008-05-19 04:29:15 +0200 (Mon, 19 May 2008)
New Revision: 9026

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
Log:
Debugging...


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-16 21:13:21 UTC (rev 9025)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-19 02:29:15 UTC (rev 9026)
@@ -799,7 +799,7 @@
     input_layer->setLearningRate( the_learning_rate );
     hidden_layer->setLearningRate( the_learning_rate );
     input_connections->setLearningRate( the_learning_rate );
-    dynamic_connections->forget();
+    dynamic_connections->setLearningRate( the_learning_rate );
     if( hidden_layer2 )
     {
         hidden_layer2->setLearningRate( the_learning_rate );
@@ -906,7 +906,6 @@
                     hidden_act_no_bias_list[i], 
                     visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
 
-                // Could learn initial value for h_{-1}
             }
         }
     
@@ -973,6 +972,13 @@
             targets_list.resize(0);
             nll_list.resize(0,0);
             masks_list.resize(0);
+
+            if (testoutputs)
+            {
+                output.fill(end_of_sequence_symbol);
+                testoutputs->putOrAppendRow(i, output);
+            }
+
             continue;
         }
 



From larocheh at mail.berlios.de  Tue May 20 04:20:01 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 20 May 2008 04:20:01 +0200
Subject: [Plearn-commits] r9027 - trunk/plearn_learners_experimental
Message-ID: <200805200220.m4K2K1Q3019119@sheep.berlios.de>

Author: larocheh
Date: 2008-05-20 04:20:00 +0200 (Tue, 20 May 2008)
New Revision: 9027

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
Log:
Added cumulative training cost as test cost


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-19 02:29:15 UTC (rev 9026)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-05-20 02:20:00 UTC (rev 9027)
@@ -1331,7 +1331,7 @@
                 if( only_reconstruct_masked_inputs && 
                     fraction_of_masked_inputs > 0 )
                 {
-                    for( int j=round(fraction_of_masked_inputs*input_layer->size) ; 
+                    for( int j=(int)round(fraction_of_masked_inputs*input_layer->size) ; 
                          j < input_layer->size ; 
                          j++)
                         reconstruction_activation_gradient[ 
@@ -1447,6 +1447,7 @@
                 hidden_layer->activation) - dot(input,input_layer->bias) + log_Z;
         }
     }
+    costs[cumulative_training_time_cost_index] = cumulative_training_time;
 }
 
 TVec<string> PseudolikelihoodRBM::getTestCostNames() const



From larocheh at mail.berlios.de  Tue May 20 08:49:50 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 20 May 2008 08:49:50 +0200
Subject: [Plearn-commits] r9028 - trunk/plearn_learners/online
Message-ID: <200805200649.m4K6noaI021854@sheep.berlios.de>

Author: larocheh
Date: 2008-05-20 08:49:49 +0200 (Tue, 20 May 2008)
New Revision: 9028

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
Log:
Added an option to have samples be in {-1,1} instead of {0,1}. This means fprop uses tanh() instead of sigmoid()...


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-05-20 02:20:00 UTC (rev 9027)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-05-20 06:49:49 UTC (rev 9028)
@@ -51,12 +51,14 @@
     "");
 
 RBMBinomialLayer::RBMBinomialLayer( real the_learning_rate ) :
-    inherited( the_learning_rate )
+    inherited( the_learning_rate ),
+    use_signed_samples( false )
 {
 }
 
 RBMBinomialLayer::RBMBinomialLayer( int the_size, real the_learning_rate ) :
-    inherited( the_learning_rate )
+    inherited( the_learning_rate ),
+    use_signed_samples( false )
 {
     size = the_size;
     activation.resize( the_size );
@@ -80,8 +82,12 @@
 
     //random_gen->manual_seed(1827);
 
-    for( int i=0 ; i<size ; i++ )
-        sample[i] = random_gen->binomial_sample( expectation[i] );
+    if( use_signed_samples )
+        for( int i=0 ; i<size ; i++ )
+            sample[i] = 2*random_gen->binomial_sample( (expectation[i]+1)/2 )-1;
+    else
+        for( int i=0 ; i<size ; i++ )
+            sample[i] = random_gen->binomial_sample( expectation[i] );        
 }
 
 /////////////////////
@@ -99,10 +105,17 @@
 
     //random_gen->manual_seed(1827);
 
-    for (int k = 0; k < batch_size; k++) {
-        for (int i=0 ; i<size ; i++)
-            samples(k, i) = random_gen->binomial_sample( expectations(k, i) );
-    }
+    if( use_signed_samples )
+        for (int k = 0; k < batch_size; k++) {
+            for (int i=0 ; i<size ; i++)
+                samples(k, i) = 2*random_gen->binomial_sample( (expectations(k, i)+1)/2 )-1;
+        }
+    else
+        for (int k = 0; k < batch_size; k++) {
+            for (int i=0 ; i<size ; i++)
+                samples(k, i) = random_gen->binomial_sample( expectations(k, i) );
+        }
+        
 }
 
 ////////////////////////
@@ -113,12 +126,20 @@
     if( expectation_is_up_to_date )
         return;
 
-    if (use_fast_approximations)
-        for( int i=0 ; i<size ; i++ )
-            expectation[i] = fastsigmoid( activation[i] );
+    if( use_signed_samples )
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                expectation[i] = fasttanh( activation[i] );
+        else
+            for( int i=0 ; i<size ; i++ )
+                expectation[i] = tanh( activation[i] );
     else
-        for( int i=0 ; i<size ; i++ )
-            expectation[i] = sigmoid( activation[i] );
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                expectation[i] = fastsigmoid( activation[i] );
+        else
+            for( int i=0 ; i<size ; i++ )
+                expectation[i] = sigmoid( activation[i] );
 
     expectation_is_up_to_date = true;
 }
@@ -133,14 +154,24 @@
 
     PLASSERT( expectations.width() == size
               && expectations.length() == batch_size );
-    if (use_fast_approximations)
-        for (int k = 0; k < batch_size; k++)
-            for (int i = 0 ; i < size ; i++)
-                expectations(k, i) = fastsigmoid(activations(k, i));
+    if( use_signed_samples )
+        if (use_fast_approximations)
+            for (int k = 0; k < batch_size; k++)
+                for (int i = 0 ; i < size ; i++)
+                    expectations(k, i) = fasttanh(activations(k, i));
+        else
+            for (int k = 0; k < batch_size; k++)
+                for (int i = 0 ; i < size ; i++)
+                    expectations(k, i) = tanh(activations(k, i));
     else
-        for (int k = 0; k < batch_size; k++)
-            for (int i = 0 ; i < size ; i++)
-                expectations(k, i) = sigmoid(activations(k, i));
+        if (use_fast_approximations)
+            for (int k = 0; k < batch_size; k++)
+                for (int i = 0 ; i < size ; i++)
+                    expectations(k, i) = fastsigmoid(activations(k, i));
+        else
+            for (int k = 0; k < batch_size; k++)
+                for (int i = 0 ; i < size ; i++)
+                    expectations(k, i) = sigmoid(activations(k, i));
 
     expectations_are_up_to_date = true;
 }
@@ -153,12 +184,21 @@
     PLASSERT( input.size() == input_size );
     output.resize( output_size );
 
-    if (use_fast_approximations)
-        for( int i=0 ; i<size ; i++ )
-            output[i] = fastsigmoid( input[i] + bias[i] );
+    if( use_signed_samples )
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                output[i] = fasttanh( input[i] + bias[i] );
+        else
+            for( int i=0 ; i<size ; i++ )
+                output[i] = tanh( input[i] + bias[i] );
     else
-        for( int i=0 ; i<size ; i++ )
-            output[i] = sigmoid( input[i] + bias[i] );
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                output[i] = fastsigmoid( input[i] + bias[i] );
+        else
+            for( int i=0 ; i<size ; i++ )
+                output[i] = sigmoid( input[i] + bias[i] );
+
 }
 
 void RBMBinomialLayer::fprop( const Mat& inputs, Mat& outputs ) const
@@ -167,14 +207,25 @@
     PLASSERT( inputs.width() == size );
     outputs.resize( mbatch_size, size );
 
-    if (use_fast_approximations)
-        for( int k = 0; k < mbatch_size; k++ )
-            for( int i = 0; i < size; i++ )
-                outputs(k,i) = fastsigmoid( inputs(k,i) + bias[i] );
+    if( use_signed_samples )
+        if (use_fast_approximations)
+            for( int k = 0; k < mbatch_size; k++ )
+                for( int i = 0; i < size; i++ )
+                    outputs(k,i) = fasttanh( inputs(k,i) + bias[i] );
+        else
+            for( int k = 0; k < mbatch_size; k++ )
+                for( int i = 0; i < size; i++ )
+                    outputs(k,i) = tanh( inputs(k,i) + bias[i] );
     else
-        for( int k = 0; k < mbatch_size; k++ )
-            for( int i = 0; i < size; i++ )
-                outputs(k,i) = sigmoid( inputs(k,i) + bias[i] );
+        if (use_fast_approximations)
+            for( int k = 0; k < mbatch_size; k++ )
+                for( int i = 0; i < size; i++ )
+                    outputs(k,i) = fastsigmoid( inputs(k,i) + bias[i] );
+        else
+            for( int k = 0; k < mbatch_size; k++ )
+                for( int i = 0; i < size; i++ )
+                    outputs(k,i) = sigmoid( inputs(k,i) + bias[i] );
+
 }
 
 void RBMBinomialLayer::fprop( const Vec& input, const Vec& rbm_bias,
@@ -184,12 +235,20 @@
     PLASSERT( rbm_bias.size() == input_size );
     output.resize( output_size );
 
-    if (use_fast_approximations)
-        for( int i=0 ; i<size ; i++ )
-            output[i] = fastsigmoid( input[i] + rbm_bias[i]);
+    if( use_signed_samples )
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                output[i] = fasttanh( input[i] + rbm_bias[i]);
+        else
+            for( int i=0 ; i<size ; i++ )
+                output[i] =tanh( input[i] + rbm_bias[i]);
     else
-        for( int i=0 ; i<size ; i++ )
-            output[i] = sigmoid( input[i] + rbm_bias[i]);
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                output[i] = fastsigmoid( input[i] + rbm_bias[i]);
+        else
+            for( int i=0 ; i<size ; i++ )
+                output[i] = sigmoid( input[i] + rbm_bias[i]);
 }
 
 /////////////////
@@ -218,27 +277,54 @@
     if( momentum != 0. )
         bias_inc.resize( size );
 
-    for( int i=0 ; i<size ; i++ )
+    if( use_signed_samples )
     {
-        real output_i = output[i];
-        real in_grad_i = output_i * (1-output_i) * output_gradient[i];
-        input_gradient[i] += in_grad_i;
-
-        if( momentum == 0. )
+        for( int i=0 ; i<size ; i++ )
         {
-            // update the bias: bias -= learning_rate * input_gradient
-            bias[i] -= learning_rate * in_grad_i;
+            real output_i = output[i];
+            real in_grad_i;
+            in_grad_i = (1 -  output_i * output_i) * output_gradient[i];
+            input_gradient[i] += in_grad_i;
+            
+            if( momentum == 0. )
+            {
+                // update the bias: bias -= learning_rate * input_gradient
+                bias[i] -= learning_rate * in_grad_i;
+            }
+            else
+            {
+                // The update rule becomes:
+                // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                // bias += bias_inc
+                bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+                bias[i] += bias_inc[i];
+            }
         }
-        else
+    }
+    else
+    {
+        for( int i=0 ; i<size ; i++ )
         {
-            // The update rule becomes:
-            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-            // bias += bias_inc
-            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
-            bias[i] += bias_inc[i];
+            real output_i = output[i];
+            real in_grad_i;
+            in_grad_i = output_i * (1-output_i) * output_gradient[i];
+            input_gradient[i] += in_grad_i;
+            
+            if( momentum == 0. )
+            {
+                // update the bias: bias -= learning_rate * input_gradient
+                bias[i] -= learning_rate * in_grad_i;
+            }
+            else
+            {
+                // The update rule becomes:
+                // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                // bias += bias_inc
+                bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+                bias[i] += bias_inc[i];
+            }
         }
     }
-
     applyBiasDecay();
 }
 
@@ -275,32 +361,64 @@
     // We use the average gradient over the mini-batch.
     real avg_lr = learning_rate / inputs.length();
 
-    for (int j = 0; j < mbatch_size; j++)
+    if( use_signed_samples )
     {
-        for( int i=0 ; i<size ; i++ )
+        for (int j = 0; j < mbatch_size; j++)
         {
-            real output_i = outputs(j, i);
-            real in_grad_i = output_i * (1-output_i) * output_gradients(j, i);
-            input_gradients(j, i) += in_grad_i;
-
-            if( momentum == 0. )
+            for( int i=0 ; i<size ; i++ )
             {
-                // update the bias: bias -= learning_rate * input_gradient
-                bias[i] -= avg_lr * in_grad_i;
+                real output_i = outputs(j, i);
+                real in_grad_i;
+                in_grad_i = (1 - output_i * output_i) * output_gradients(j, i);
+                input_gradients(j, i) += in_grad_i;
+                
+                if( momentum == 0. )
+                {
+                    // update the bias: bias -= learning_rate * input_gradient
+                    bias[i] -= avg_lr * in_grad_i;
+                }
+                else
+                {
+                    PLERROR("In RBMBinomialLayer:bpropUpdate - Not implemented for "
+                            "momentum with mini-batches");
+                    // The update rule becomes:
+                    // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                    // bias += bias_inc
+                    bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+                    bias[i] += bias_inc[i];
+                }
             }
-            else
+        }
+    }
+    else
+    {
+        for (int j = 0; j < mbatch_size; j++)
+        {
+            for( int i=0 ; i<size ; i++ )
             {
-                PLERROR("In RBMBinomialLayer:bpropUpdate - Not implemented for "
-                        "momentum with mini-batches");
-                // The update rule becomes:
-                // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-                // bias += bias_inc
-                bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
-                bias[i] += bias_inc[i];
+                real output_i = outputs(j, i);
+                real in_grad_i;
+                in_grad_i = output_i * (1-output_i) * output_gradients(j, i);
+                input_gradients(j, i) += in_grad_i;
+                
+                if( momentum == 0. )
+                {
+                    // update the bias: bias -= learning_rate * input_gradient
+                    bias[i] -= avg_lr * in_grad_i;
+                }
+                else
+                {
+                    PLERROR("In RBMBinomialLayer:bpropUpdate - Not implemented for "
+                            "momentum with mini-batches");
+                    // The update rule becomes:
+                    // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                    // bias += bias_inc
+                    bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+                    bias[i] += bias_inc[i];
+                }
             }
         }
     }
-
     applyBiasDecay();
 }
 
@@ -318,11 +436,23 @@
     input_gradient.resize( size );
     rbm_bias_gradient.resize( size );
 
-    for( int i=0 ; i<size ; i++ )
+    if( use_signed_samples )
     {
-        real output_i = output[i];
-        input_gradient[i] = output_i * (1-output_i) * output_gradient[i];
+        for( int i=0 ; i<size ; i++ )
+        {
+            real output_i = output[i];
+            
+            input_gradient[i] = ( 1 - output_i * output_i ) * output_gradient[i];
+        }
     }
+    else
+    {
+        for( int i=0 ; i<size ; i++ )
+        {
+            real output_i = output[i];
+            input_gradient[i] = output_i * (1-output_i) * output_gradient[i];
+        }   
+    }
 
     rbm_bias_gradient << input_gradient;
 }
@@ -330,29 +460,57 @@
 real RBMBinomialLayer::fpropNLL(const Vec& target)
 {
     PLASSERT( target.size() == input_size );
-
+    //I'm here!!!
     real ret = 0;
     real target_i, activation_i;
-    if(use_fast_approximations){
-        for( int i=0 ; i<size ; i++ )
-        {
-            target_i = target[i];
-            activation_i = activation[i];
-            ret += tabulated_softplus(activation_i) - target_i * activation_i;
-            // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
-            // but it is numerically unstable, so use instead the following identity:
-            //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
-            //     = act + softplus(-act) - target*act 
-            //     = softplus(act) - target*act
+    if( use_signed_samples )
+    {
+        if(use_fast_approximations){
+            for( int i=0 ; i<size ; i++ )
+            {
+                target_i = (target[i]+1)/2;
+                activation_i = 2*activation[i];
+                
+                ret += tabulated_softplus(activation_i) - target_i * activation_i;
+                // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+                // but it is numerically unstable, so use instead the following identity:
+                //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+                //     = act + softplus(-act) - target*act 
+                //     = softplus(act) - target*act
+            }
+        } else {
+            for( int i=0 ; i<size ; i++ )
+            {
+                target_i = (target[i]+1)/2;
+                activation_i = 2*activation[i];                
+                ret += softplus(activation_i) - target_i * activation_i;
+            }
         }
-    } else {
-        for( int i=0 ; i<size ; i++ )
-        {
-            target_i = target[i];
-            activation_i = activation[i];
-            ret += softplus(activation_i) - target_i * activation_i;
+    }
+    else
+    {
+        if(use_fast_approximations){
+            for( int i=0 ; i<size ; i++ )
+            {
+                target_i = target[i];
+                activation_i = activation[i];
+                ret += tabulated_softplus(activation_i) - target_i * activation_i;
+                // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+                // but it is numerically unstable, so use instead the following identity:
+                //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+                //     = act + softplus(-act) - target*act 
+                //     = softplus(act) - target*act
+            }
+        } else {
+            for( int i=0 ; i<size ; i++ )
+            {
+                target_i = target[i];
+                activation_i = activation[i];
+                ret += softplus(activation_i) - target_i * activation_i;
+            }
         }
     }
+
     return ret;
 }
 
@@ -365,45 +523,49 @@
     PLASSERT( costs_column.width() == 1 );
     PLASSERT( costs_column.length() == batch_size );
 
-    for (int k=0;k<batch_size;k++) // loop over minibatch
+    if( use_signed_samples )
     {
-        real nll = 0;
-        real* activation = activations[k];
-        real* target = targets[k];
-        if(use_fast_approximations){
-            for( int i=0 ; i<size ; i++ ) // loop over outputs
-            {
-                if(!fast_exact_is_equal(target[i],0.0))
-                    // nll -= target[i] * pl_log(expectations[i]); 
-                    // but it is numerically unstable, so use instead
-                    // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                    nll += target[i] * tabulated_softplus(-activation[i]);
-                if(!fast_exact_is_equal(target[i],1.0))
-                    // nll -= (1-target[i]) * pl_log(1-output[i]);
-                    // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
-                    //                         = log(1/(1+exp(x)))
-                    //                         = -log(1+exp(x))
-                    //                         = -softplus(x)
-                    nll += (1-target[i]) * tabulated_softplus(activation[i]);
+        for (int k=0;k<batch_size;k++) // loop over minibatch
+        {
+            real nll = 0;
+            real* activation = activations[k];
+            real* target = targets[k];
+            if(use_fast_approximations){
+                for( int i=0 ; i<size ; i++ ) // loop over outputs
+                {
+                    nll += tabulated_softplus(2*activation[i])
+                        - (target[i]+1) * activation[i] ;
+                }
+            } else {
+                for( int i=0 ; i<size ; i++ ) // loop over outputs
+                {
+                    nll += softplus(2*activation[i]) - (target[i]+1)*activation[i] ;
+                }
             }
-        } else {
-            for( int i=0 ; i<size ; i++ ) // loop over outputs
-            {
-                if(!fast_exact_is_equal(target[i],0.0))
-                    // nll -= target[i] * pl_log(expectations[i]); 
-                    // but it is numerically unstable, so use instead
-                    // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
-                    nll += target[i] * softplus(-activation[i]);
-                if(!fast_exact_is_equal(target[i],1.0))
-                    // nll -= (1-target[i]) * pl_log(1-output[i]);
-                    // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
-                    //                         = log(1/(1+exp(x)))
-                    //                         = -log(1+exp(x))
-                    //                         = -softplus(x)
-                    nll += (1-target[i]) * softplus(activation[i]);
+            costs_column(k,0) = nll;
+        }
+    }
+    else
+    {
+        for (int k=0;k<batch_size;k++) // loop over minibatch
+        {
+            real nll = 0;
+            real* activation = activations[k];
+            real* target = targets[k];
+            if(use_fast_approximations){
+                for( int i=0 ; i<size ; i++ ) // loop over outputs
+                {
+                    nll += tabulated_softplus(activation[i])
+                        -target[i] * activation[i] ;
+                }
+            } else {
+                for( int i=0 ; i<size ; i++ ) // loop over outputs
+                {
+                    nll += softplus(activation[i]) - target[i] * activation[i] ;
+                }
             }
+            costs_column(k,0) = nll;
         }
-        costs_column(k,0) = nll;
     }
 }
 
@@ -435,11 +597,11 @@
 
 void RBMBinomialLayer::declareOptions(OptionList& ol)
 {
-/*
-    declareOption(ol, "size", &RBMBinomialLayer::size,
+
+    declareOption(ol, "use_signed_samples", &RBMBinomialLayer::use_signed_samples,
                   OptionBase::buildoption,
-                  "Number of units.");
-*/
+                  "Indication that samples should be in {-1,1}, not {0,1}.\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -473,13 +635,26 @@
     // result = -\sum_{i=0}^{size-1} softplus(a_i)
     real result = 0;
     real* a = unit_activations.data();
-    for (int i=0; i<size; i++)
+    if( use_signed_samples )
     {
-        if (use_fast_approximations)
-            result -= tabulated_softplus(a[i]);
-        else
-            result -= softplus(a[i]);
+        for (int i=0; i<size; i++)
+        {
+            if (use_fast_approximations)
+                result -= tabulated_softplus(2*a[i]) - a[i];
+            else
+                result -= softplus(2*a[i]) - a[i];
+        }
     }
+    else
+    {
+        for (int i=0; i<size; i++)
+        {
+            if (use_fast_approximations)
+                result -= tabulated_softplus(a[i]);
+            else
+                result -= softplus(a[i]);
+        }
+    }
     return result;
 }
 
@@ -493,15 +668,30 @@
     if( !accumulate ) unit_activations_gradient.clear();
     real* a = unit_activations.data();
     real* ga = unit_activations_gradient.data();
-    for (int i=0; i<size; i++)
+    if( use_signed_samples )
     {
-        if (use_fast_approximations)
-            ga[i] -= output_gradient *
-                fastsigmoid( a[i] );
-        else
-            ga[i] -= output_gradient *
-                sigmoid( a[i] );
+        for (int i=0; i<size; i++)
+        {
+            if (use_fast_approximations)
+                ga[i] -= output_gradient *
+                    ( fasttanh( a[i] ) );
+            else
+                ga[i] -= output_gradient *
+                    ( tanh( a[i] ) );
+        }
     }
+    else
+    {
+        for (int i=0; i<size; i++)
+        {
+            if (use_fast_approximations)
+                ga[i] -= output_gradient *
+                    fastsigmoid( a[i] );
+            else
+                ga[i] -= output_gradient *
+                    sigmoid( a[i] );
+        }
+    }
 }
 
 int RBMBinomialLayer::getConfigurationCount()
@@ -513,11 +703,21 @@
 {
     PLASSERT( output.length() == size );
     PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
-
-    for ( int i = 0; i < size; ++i ) {
-        output[i] = conf_index & 1;
-        conf_index >>= 1;
+    
+    if( use_signed_samples )
+    {
+        for ( int i = 0; i < size; ++i ) {
+            output[i] = 2 * (conf_index & 1) - 1;
+            conf_index >>= 1;
+        }        
     }
+    else
+    {
+        for ( int i = 0; i < size; ++i ) {
+            output[i] = conf_index & 1;
+            conf_index >>= 1;
+        }
+    }
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2008-05-20 02:20:00 UTC (rev 9027)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2008-05-20 06:49:49 UTC (rev 9028)
@@ -56,8 +56,9 @@
 public:
     //#####  Public Build Options  ############################################
 
+    // Indication that samples should be in {-1,1}, not {0,1}
+    bool use_signed_samples;
 
-
 public:
     //#####  Public Member Functions  #########################################
 



From nouiz at mail.berlios.de  Tue May 20 18:09:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 May 2008 18:09:04 +0200
Subject: [Plearn-commits] r9029 - trunk/python_modules/plearn/parallel
Message-ID: <200805201609.m4KG94ot000986@sheep.berlios.de>

Author: nouiz
Date: 2008-05-20 18:09:03 +0200 (Tue, 20 May 2008)
New Revision: 9029

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
bugfix


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-20 06:49:49 UTC (rev 9028)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-20 16:09:03 UTC (rev 9029)
@@ -811,11 +811,13 @@
         condor_dat = open( condor_file, 'w' )
 
         if self.req:
-            req = req+'&&('+self.req+')'
+            req = self.req
+        else:
+            req = "True"
         if self.targetcondorplatform == 'BOTH':
-            req="((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
+            req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
         else :
-            req="(Arch == \"%s\")"%(self.targetcondorplatform)
+            req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
         if self.os:
             req+='&&(OpSyS == "'+self.os+'")'
 



From nouiz at mail.berlios.de  Tue May 20 19:39:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 May 2008 19:39:29 +0200
Subject: [Plearn-commits] r9030 - in trunk: plearn_learners/meta
	python_modules/plearn/learners
Message-ID: <200805201739.m4KHdTOZ032582@sheep.berlios.de>

Author: nouiz
Date: 2008-05-20 19:39:28 +0200 (Tue, 20 May 2008)
New Revision: 9030

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
Modified to allow reuse the old train_set When reloading an old version to continue the computeation. This allow less memory usage.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-05-20 16:09:03 UTC (rev 9029)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-05-20 17:39:28 UTC (rev 9030)
@@ -217,6 +217,12 @@
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
+
+    declareOption(ol, "train_set",
+                  &AdaBoost::train_set,
+                  OptionBase::learntoption||OptionBase::nosave,
+                  "The training set, so we can reload it.\n");
+
 }
 
 void AdaBoost::build_()

Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-05-20 16:09:03 UTC (rev 9029)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-05-20 17:39:28 UTC (rev 9030)
@@ -55,6 +55,8 @@
         t2=time.time()
         self.train_time+=t2-t1
         self.train_stats=VecStatsCollector()
+        print self.learner1.getTrainStatsCollector().length()
+        print self.learner2.getTrainStatsCollector().length()
         self.train_stats.append(self.learner1.getTrainStatsCollector(),
                                 "sublearner1.",[])
         self.train_stats.append(self.learner2.getTrainStatsCollector(),
@@ -224,7 +226,7 @@
         self.learner2.save(file2,encoding)
     
     def load_old_learner(self,filepath=None,trainSet1=None,trainSet2=None,stage1=-1,stage2=-1):
-        assert(trainSet1 and trainSet2)
+        # if their is no trainSet1 and trainSet2, we suppose that their is one saved in the learner
         if not filepath:
             assert(not self.learner1.expdir.endswith("/learner1") or not self.learner2.expdir.endswith("/learner2"))
             path=self.learner1.expdir[:-9]
@@ -271,6 +273,7 @@
             self.learner1.setTrainingSet(trainSet1,False)
         if trainSet2:
             self.learner2.setTrainingSet(trainSet2,False)
+
         self.learner1.setTrainStatsCollector(VecStatsCollector())
         self.learner2.setTrainStatsCollector(VecStatsCollector())
 



From nouiz at mail.berlios.de  Tue May 20 19:47:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 May 2008 19:47:40 +0200
Subject: [Plearn-commits] r9031 - trunk/python_modules/plearn/learners
Message-ID: <200805201747.m4KHleLC000481@sheep.berlios.de>

Author: nouiz
Date: 2008-05-20 19:47:40 +0200 (Tue, 20 May 2008)
New Revision: 9031

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
removed print


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-05-20 17:39:28 UTC (rev 9030)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-05-20 17:47:40 UTC (rev 9031)
@@ -55,8 +55,6 @@
         t2=time.time()
         self.train_time+=t2-t1
         self.train_stats=VecStatsCollector()
-        print self.learner1.getTrainStatsCollector().length()
-        print self.learner2.getTrainStatsCollector().length()
         self.train_stats.append(self.learner1.getTrainStatsCollector(),
                                 "sublearner1.",[])
         self.train_stats.append(self.learner2.getTrainStatsCollector(),



From nouiz at mail.berlios.de  Tue May 20 21:28:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 May 2008 21:28:47 +0200
Subject: [Plearn-commits] r9032 - trunk/plearn_learners/cgi
Message-ID: <200805201928.m4KJSl2A009571@sheep.berlios.de>

Author: nouiz
Date: 2008-05-20 21:28:46 +0200 (Tue, 20 May 2008)
New Revision: 9032

Modified:
   trunk/plearn_learners/cgi/SecondIterationWrapper.cc
Log:
forward the forget call


Modified: trunk/plearn_learners/cgi/SecondIterationWrapper.cc
===================================================================
--- trunk/plearn_learners/cgi/SecondIterationWrapper.cc	2008-05-20 17:47:40 UTC (rev 9031)
+++ trunk/plearn_learners/cgi/SecondIterationWrapper.cc	2008-05-20 19:28:46 UTC (rev 9032)
@@ -314,6 +314,7 @@
 
 void SecondIterationWrapper::forget()
 {
+    base_regressor->forget();
 }
 
 int SecondIterationWrapper::outputsize() const



From nouiz at mail.berlios.de  Tue May 20 21:30:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 May 2008 21:30:19 +0200
Subject: [Plearn-commits] r9033 - trunk/plearn_learners/cgi
Message-ID: <200805201930.m4KJUJPK009797@sheep.berlios.de>

Author: nouiz
Date: 2008-05-20 21:30:19 +0200 (Tue, 20 May 2008)
New Revision: 9033

Modified:
   trunk/plearn_learners/cgi/SecondIterationWrapper.cc
   trunk/plearn_learners/cgi/SecondIterationWrapper.h
Log:
forget is correctly implemented by parent class


Modified: trunk/plearn_learners/cgi/SecondIterationWrapper.cc
===================================================================
--- trunk/plearn_learners/cgi/SecondIterationWrapper.cc	2008-05-20 19:28:46 UTC (rev 9032)
+++ trunk/plearn_learners/cgi/SecondIterationWrapper.cc	2008-05-20 19:30:19 UTC (rev 9033)
@@ -312,11 +312,6 @@
     return (search_table(min, 1) + search_table(max, 1)) / 2.0;
 }
 
-void SecondIterationWrapper::forget()
-{
-    base_regressor->forget();
-}
-
 int SecondIterationWrapper::outputsize() const
 {
     return base_regressor?base_regressor->outputsize():-1;

Modified: trunk/plearn_learners/cgi/SecondIterationWrapper.h
===================================================================
--- trunk/plearn_learners/cgi/SecondIterationWrapper.h	2008-05-20 19:28:46 UTC (rev 9032)
+++ trunk/plearn_learners/cgi/SecondIterationWrapper.h	2008-05-20 19:30:19 UTC (rev 9033)
@@ -100,7 +100,6 @@
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
     virtual void         train();
-    virtual void         forget();
     virtual int          outputsize() const;
     virtual TVec<string> getTrainCostNames() const;
     virtual TVec<string> getTestCostNames() const;



From tihocan at mail.berlios.de  Tue May 20 21:50:19 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 20 May 2008 21:50:19 +0200
Subject: [Plearn-commits] r9034 - in trunk/plearn/vmat/test: . .pytest
	.pytest/PL_FilteredVMatrix_Indices
	.pytest/PL_FilteredVMatrix_Indices/expected_results
	.pytest/PL_FilteredVMatrix_Mem
	.pytest/PL_FilteredVMatrix_Mem/expected_results
	.pytest/PL_FilteredVMatrix_Mem/expected_results/filteredvmatrix_fileindex.pymat.metadata
Message-ID: <200805201950.m4KJoJR5011051@sheep.berlios.de>

Author: tihocan
Date: 2008-05-20 21:50:18 +0200 (Tue, 20 May 2008)
New Revision: 9034

Added:
   trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Indices/
   trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Indices/expected_results/
   trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Indices/expected_results/RUN.log
   trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem/
   trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem/expected_results/
   trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem/expected_results/RUN.log
   trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem/expected_results/filteredvmatrix_fileindex.pymat.metadata/
   trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem/expected_results/filteredvmatrix_fileindex.pymat.metadata/filtered.idx
   trunk/plearn/vmat/test/filteredvmatrix_fileindex.pymat
   trunk/plearn/vmat/test/filteredvmatrix_memindex.pymat
Modified:
   trunk/plearn/vmat/test/pytest.config
Log:
Added test of FilteredVMatrix to make sure I don't break it again


Property changes on: trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Indices
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Indices/expected_results/RUN.log
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Indices/expected_results/RUN.log	2008-05-20 19:30:19 UTC (rev 9033)
+++ trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Indices/expected_results/RUN.log	2008-05-20 19:50:18 UTC (rev 9034)
@@ -0,0 +1,3 @@
+1 0 0 0 2 
+1 1 0 1 2 
+1 1 1 2 3 


Property changes on: trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem/expected_results/RUN.log
===================================================================

Added: trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem/expected_results/filteredvmatrix_fileindex.pymat.metadata/filtered.idx
===================================================================
(Binary files differ)


Property changes on: trunk/plearn/vmat/test/.pytest/PL_FilteredVMatrix_Mem/expected_results/filteredvmatrix_fileindex.pymat.metadata/filtered.idx
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn/vmat/test/filteredvmatrix_fileindex.pymat
===================================================================
--- trunk/plearn/vmat/test/filteredvmatrix_fileindex.pymat	2008-05-20 19:30:19 UTC (rev 9033)
+++ trunk/plearn/vmat/test/filteredvmatrix_fileindex.pymat	2008-05-20 19:50:18 UTC (rev 9034)
@@ -0,0 +1,10 @@
+# FilteredVMatrix using a file to store filtered indices.
+
+from plearn.pyplearn import pl
+
+def main():
+    return pl.FilteredVMatrix(
+            source = pl.AutoVMatrix(
+                filename = 'PLEARNDIR:examples/data/test_suite/toy_multi_class_bags.amat'),
+            prg = '@bag_info 2 >=')
+

Added: trunk/plearn/vmat/test/filteredvmatrix_memindex.pymat
===================================================================
--- trunk/plearn/vmat/test/filteredvmatrix_memindex.pymat	2008-05-20 19:30:19 UTC (rev 9033)
+++ trunk/plearn/vmat/test/filteredvmatrix_memindex.pymat	2008-05-20 19:50:18 UTC (rev 9034)
@@ -0,0 +1,11 @@
+# FilteredVMatrix using memory to store filtered indices.
+
+from plearn.pyplearn import pl
+
+def main():
+    return pl.ForwardVMatrix(
+            vm = pl.FilteredVMatrix(
+                source = pl.AutoVMatrix(
+                    filename = 'PLEARNDIR:examples/data/test_suite/toy_multi_class_bags.amat'),
+                prg = '@bag_info 2 >='))
+

Modified: trunk/plearn/vmat/test/pytest.config
===================================================================
--- trunk/plearn/vmat/test/pytest.config	2008-05-20 19:30:19 UTC (rev 9033)
+++ trunk/plearn/vmat/test/pytest.config	2008-05-20 19:50:18 UTC (rev 9034)
@@ -228,3 +228,42 @@
     runtime = None,
     difftime = None
     )
+
+Test(
+    name = "PL_FilteredVMatrix_Mem",
+    description = "Ensure that indices filtered by the memory and file methods are consistent with each other",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "vmat diff filteredvmatrix_fileindex.pymat filteredvmatrix_memindex.pymat",
+    resources = [
+        "filteredvmatrix_fileindex.pymat",
+        "filteredvmatrix_memindex.pymat"
+        ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+
+Test(
+    name = "PL_FilteredVMatrix_Indices",
+    description = "Test that indices are properly filtered",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "vmat cat filteredvmatrix_memindex.pymat",
+    resources = [ "filteredvmatrix_memindex.pymat" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+
+



From nouiz at mail.berlios.de  Tue May 20 21:51:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 May 2008 21:51:55 +0200
Subject: [Plearn-commits] r9035 - trunk/plearn_learners/cgi
Message-ID: <200805201951.m4KJptjL011220@sheep.berlios.de>

Author: nouiz
Date: 2008-05-20 21:51:54 +0200 (Tue, 20 May 2008)
New Revision: 9035

Modified:
   trunk/plearn_learners/cgi/SecondIterationWrapper.cc
   trunk/plearn_learners/cgi/SecondIterationWrapper.h
Log:
reverted last commit, as SecondIterationWrapper don't heritate from EmbendedLearner as I thaught


Modified: trunk/plearn_learners/cgi/SecondIterationWrapper.cc
===================================================================
--- trunk/plearn_learners/cgi/SecondIterationWrapper.cc	2008-05-20 19:50:18 UTC (rev 9034)
+++ trunk/plearn_learners/cgi/SecondIterationWrapper.cc	2008-05-20 19:51:54 UTC (rev 9035)
@@ -312,6 +312,12 @@
     return (search_table(min, 1) + search_table(max, 1)) / 2.0;
 }
 
+void SecondIterationWrapper::forget()
+{
+    PLASSERT( base_regressor );
+    base_regressor->forget();
+}
+
 int SecondIterationWrapper::outputsize() const
 {
     return base_regressor?base_regressor->outputsize():-1;

Modified: trunk/plearn_learners/cgi/SecondIterationWrapper.h
===================================================================
--- trunk/plearn_learners/cgi/SecondIterationWrapper.h	2008-05-20 19:50:18 UTC (rev 9034)
+++ trunk/plearn_learners/cgi/SecondIterationWrapper.h	2008-05-20 19:51:54 UTC (rev 9035)
@@ -100,6 +100,7 @@
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
     virtual void         train();
+    virtual void         forget();
     virtual int          outputsize() const;
     virtual TVec<string> getTrainCostNames() const;
     virtual TVec<string> getTestCostNames() const;



From nouiz at mail.berlios.de  Tue May 20 21:54:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 May 2008 21:54:20 +0200
Subject: [Plearn-commits] r9036 - trunk/python_modules/plearn/pymake
Message-ID: <200805201954.m4KJsK0h011299@sheep.berlios.de>

Author: nouiz
Date: 2008-05-20 21:54:20 +0200 (Tue, 20 May 2008)
New Revision: 9036

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
bugfix: remove the local_ofiles options as many times it appears


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-05-20 19:51:54 UTC (rev 9035)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-05-20 19:54:20 UTC (rev 9036)
@@ -1205,6 +1205,7 @@
     configpath = get_config_path(target)
     execfile( configpath, globals() )
     options = getOptions(options_choices, optionargs)
+
     sourcedirs = unique(sourcedirs)
 
     if isccfile(target) or type=='include':
@@ -2691,7 +2692,7 @@
 
     if 'local_ofiles' in optionargs:
         local_ofiles = 1
-        optionargs.remove('local_ofiles')
+        while 'local_ofiles' in optionargs: optionargs.remove('local_ofiles')
     else:
         local_ofiles = 0
     for option in optionargs:



From nouiz at mail.berlios.de  Wed May 21 17:02:56 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 May 2008 17:02:56 +0200
Subject: [Plearn-commits] r9037 - trunk/plearn/math
Message-ID: <200805211502.m4LF2ucZ004932@sheep.berlios.de>

Author: nouiz
Date: 2008-05-21 17:02:55 +0200 (Wed, 21 May 2008)
New Revision: 9037

Modified:
   trunk/plearn/math/StatsCollector.cc
Log:
print the % of non missing value in the cdf


Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2008-05-20 19:54:20 UTC (rev 9036)
+++ trunk/plearn/math/StatsCollector.cc	2008-05-21 15:02:55 UTC (rev 9037)
@@ -1052,7 +1052,9 @@
             out << "value: " << it->first 
                 << "  #equal:" << it->second.n
                 << "  #less:" << it->second.nbelow
-                << "  avg_of_less:" << it->second.sum/it->second.nbelow << endl;
+                << "  avg_of_less:" << it->second.sum/it->second.nbelow
+                << "  % of non missing:"<< (real(it->second.n)/nnonmissing())
+                << endl;
         }
         out << "\n# samples: " << n() << "\n";
         out << "# missing: " << nmissing() << "\n";



From nouiz at mail.berlios.de  Wed May 21 19:44:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 May 2008 19:44:26 +0200
Subject: [Plearn-commits] r9038 - trunk/plearn/io
Message-ID: <200805211744.m4LHiQwW012602@sheep.berlios.de>

Author: nouiz
Date: 2008-05-21 19:44:26 +0200 (Wed, 21 May 2008)
New Revision: 9038

Modified:
   trunk/plearn/io/fileutils.cc
Log:
added error message.


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-05-21 15:02:55 UTC (rev 9037)
+++ trunk/plearn/io/fileutils.cc	2008-05-21 17:44:26 UTC (rev 9038)
@@ -679,7 +679,16 @@
 
     // Perform actual parsing and macro processing...
     PStream in = openFile(file, PStream::plearn_ascii, "r");
-    string text = readAndMacroProcess(in, variables, latest);
+    string text;
+    try
+    { 
+        text = readAndMacroProcess(in, variables, latest);
+    }
+    catch(const PLearnError& e)
+    {
+        PLERROR("while parsing file %s we got an error: \n%s",
+                filepath.c_str(),e.message().c_str());
+    }
 
     // Restore previous variables
     if (added)



From larocheh at mail.berlios.de  Wed May 21 21:14:54 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 21 May 2008 21:14:54 +0200
Subject: [Plearn-commits] r9039 - trunk/plearn_learners_experimental
Message-ID: <200805211914.m4LJEsKT020624@sheep.berlios.de>

Author: larocheh
Date: 2008-05-21 21:14:53 +0200 (Wed, 21 May 2008)
New Revision: 9039

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
- added some PLASSERT
- made dynamic_connections optional (makes it easier to test whether they are useful)
- now computations of targets with 0 weight are not made



Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-21 17:44:26 UTC (rev 9038)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-05-21 19:14:53 UTC (rev 9039)
@@ -43,33 +43,24 @@
 #include "DynamicallyLinkedRBMsModel.h"
 #include "plearn/math/plapack.h"
 
-// Options to have:
-//
-// - input_layer
-// - input_connection
-// - target_layers
-// - target_connections
-// - target_layers_weights
-// - mask_size
-// - end_of_sequence_symbol
-// - input reconstruction weight
-//
-// Problems to have in mind:
-// 
-// - have a proper normalization of costs
-// - output one cost per target 
-// - make sure gradient descent is proper (change some vectors into matrices, etc.)
-// - make sure end_of_sequence_symbol is used appropriately
-// - make sure declareOption includes everything, including saved variable
-// - verify use of mask is proper
-// - do proper resize of recurrent internal variables
-// - implement deepcopy appropriately
-// - corriger bug avec activation (faut additionner les biais!!!)
-
 // - commiter mse
+// - ajouter denoising recurrent net. Deux possibilit?s:
+//   1) on ajoute du bruit ? l'input, et on reconstruit les targets avec des poids
+//      possiblement diff?rents
+//     * option denoising_target_layers_weights (c'est l? qu'on met l'input)
+//     * version de clamp_units qui ajoute le bruit
+//   2) on reconstruit l'input directement (sans 2e couche cach?e)
+//     * toujours clamp_units qui ajoute le bruit
+//     * une option qui dit quelle partie de l'input reconstruire et du code 
+//       pour bloquer le gradient qui ne doit pas passer (pas tr?s propre, 
+//       mais bon...)
+//     * une option donnant les connections de reconstruction
+//     * du code pour entra?ner s?par?ment les hidden_connections (si pr?sentes)
+// - pourrait avoir le gradient du denoising recurrent net en m?me temps que
+//   celui du "fine-tuning"
 // - add dynamic_activations_list and use it in recurrent_update
-// - verify code works with and without hidden_layer2
 
+
 namespace PLearn {
 using namespace std;
 
@@ -144,7 +135,7 @@
                   &DynamicallyLinkedRBMsModel::dynamic_connections,
                   OptionBase::buildoption,
                   "The RBMConnection between the first hidden layers, "
-                  "through time.\n");
+                  "through time (optional).\n");
 
     declareOption(ol, "hidden_connections", 
                   &DynamicallyLinkedRBMsModel::hidden_connections,
@@ -214,6 +205,11 @@
     if(train_set)
     {
         PLASSERT( target_layers_weights.length() == target_layers.length() );
+        PLASSERT( target_connections.length() == target_layers.length() );
+        PLASSERT( target_layers.length() > 0 );
+        PLASSERT( input_layer );
+        PLASSERT( hidden_layer );
+        PLASSERT( input_connections );
 
         // Parsing symbols in input
         int input_layer_size = 0;
@@ -316,14 +312,17 @@
         input_connections->build();
 
 
-        dynamic_connections->down_size = hidden_layer->size;
-        dynamic_connections->up_size = hidden_layer->size;
-        if( !dynamic_connections->random_gen )
+        if( dynamic_connections )
         {
-            dynamic_connections->random_gen = random_gen;
-            dynamic_connections->forget();
+            dynamic_connections->down_size = hidden_layer->size;
+            dynamic_connections->up_size = hidden_layer->size;
+            if( !dynamic_connections->random_gen )
+            {
+                dynamic_connections->random_gen = random_gen;
+                dynamic_connections->forget();
+            }
+            dynamic_connections->build();
         }
-        dynamic_connections->build();
 
         if( hidden_layer2 )
         {
@@ -333,6 +332,8 @@
                 hidden_layer2->forget();
             }
 
+            PLASSERT( hidden_connections );
+
             hidden_connections->down_size = hidden_layer->size;
             hidden_connections->up_size = hidden_layer2->size;
             if( !hidden_connections->random_gen )
@@ -345,6 +346,9 @@
 
         for( int tar_layer = 0; tar_layer < target_layers.length(); tar_layer++ )
         {
+            PLASSERT( target_layers[tar_layer] );
+            PLASSERT( target_connections[tar_layer] );
+
             if( !target_layers[tar_layer]->random_gen )
             {
                 target_layers[tar_layer]->random_gen = random_gen;
@@ -431,7 +435,8 @@
     input_layer->forget();
     hidden_layer->forget();
     input_connections->forget();
-    dynamic_connections->forget();
+    if( dynamic_connections )
+        dynamic_connections->forget();
     if( hidden_layer2 )
     {
         hidden_layer2->forget();
@@ -559,21 +564,24 @@
                 target_prediction_act_no_bias_list.resize( target_layers.length() );
                 for( int tar=0; tar < target_layers.length(); tar++ )
                 {
-                    targets_list[tar].resize( ith_sample_in_sequence+1);
-                    targets_list[tar][ith_sample_in_sequence].resize( 
-                        target_layers[tar]->size);
-                    target_prediction_list[tar].resize(
-                        ith_sample_in_sequence+1);
-                    target_prediction_act_no_bias_list[tar].resize(
-                        ith_sample_in_sequence+1);
-
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {                        
+                        targets_list[tar].resize( ith_sample_in_sequence+1);
+                        targets_list[tar][ith_sample_in_sequence].resize( 
+                            target_layers[tar]->size);
+                        target_prediction_list[tar].resize(
+                            ith_sample_in_sequence+1);
+                        target_prediction_act_no_bias_list[tar].resize(
+                            ith_sample_in_sequence+1);
+                    }
                 }
                 nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
                 if( use_target_layers_masks )
                 {
                     masks_list.resize( target_layers.length() );
                     for( int tar=0; tar < target_layers.length(); tar++ )
-                        masks_list[tar].resize( ith_sample_in_sequence+1 );
+                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                            masks_list[tar].resize( ith_sample_in_sequence+1 );
                 }
 
                 // Forward propagation
@@ -588,38 +596,41 @@
                 sum_target_elements = 0;
                 for( int tar=0; tar < target_layers.length(); tar++ )
                 {
-                    if( use_target_layers_masks )
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
                     {
-                        clamp_units(target.subVec(
-                                        sum_target_elements,
-                                        target_layers_n_of_target_elements[tar]),
-                                    target_layers[tar],
-                                    target_symbol_sizes[tar],
-                                    input.subVec(
-                                        inputsize_without_masks 
-                                        + sum_target_elements, 
-                                        target_layers_n_of_target_elements[tar]),
-                                    masks_list[tar][ith_sample_in_sequence]
-                            );
-
+                        if( use_target_layers_masks )
+                        {
+                            clamp_units(target.subVec(
+                                            sum_target_elements,
+                                            target_layers_n_of_target_elements[tar]),
+                                        target_layers[tar],
+                                        target_symbol_sizes[tar],
+                                        input.subVec(
+                                            inputsize_without_masks 
+                                            + sum_target_elements, 
+                                            target_layers_n_of_target_elements[tar]),
+                                        masks_list[tar][ith_sample_in_sequence]
+                                );
+                            
+                        }
+                        else
+                        {
+                            clamp_units(target.subVec(
+                                            sum_target_elements,
+                                            target_layers_n_of_target_elements[tar]),
+                                        target_layers[tar],
+                                        target_symbol_sizes[tar]);
+                        }
+                        targets_list[tar][ith_sample_in_sequence] << 
+                            target_layers[tar]->expectation;
                     }
-                    else
-                    {
-                        clamp_units(target.subVec(
-                                        sum_target_elements,
-                                        target_layers_n_of_target_elements[tar]),
-                                    target_layers[tar],
-                                    target_symbol_sizes[tar]);
-                    }
                     sum_target_elements += target_layers_n_of_target_elements[tar];
-                    targets_list[tar][ith_sample_in_sequence] << 
-                        target_layers[tar]->expectation;
                 }
                 
                 input_connections->fprop( input_list[ith_sample_in_sequence], 
                                           hidden_act_no_bias_list[ith_sample_in_sequence]);
                 
-                if( ith_sample_in_sequence > 0 )
+                if( ith_sample_in_sequence > 0 && dynamic_connections )
                 {
                     dynamic_connections->fprop( 
                         hidden_list[ith_sample_in_sequence-1],
@@ -645,78 +656,92 @@
 
                     for( int tar=0; tar < target_layers.length(); tar++ )
                     {
-                        target_connections[tar]->fprop(
-                            hidden2_list[ith_sample_in_sequence],
-                            target_prediction_act_no_bias_list[tar][
-                                ith_sample_in_sequence]
-                            );
-                        target_layers[tar]->fprop(
-                            target_prediction_act_no_bias_list[tar][
-                                ith_sample_in_sequence],
-                            target_prediction_list[tar][
-                                ith_sample_in_sequence] );
-                        if( use_target_layers_masks )
-                            target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                                masks_list[tar][ith_sample_in_sequence];
+                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                        {
+                            target_connections[tar]->fprop(
+                                hidden2_list[ith_sample_in_sequence],
+                                target_prediction_act_no_bias_list[tar][
+                                    ith_sample_in_sequence]
+                                );
+                            target_layers[tar]->fprop(
+                                target_prediction_act_no_bias_list[tar][
+                                    ith_sample_in_sequence],
+                                target_prediction_list[tar][
+                                    ith_sample_in_sequence] );
+                            if( use_target_layers_masks )
+                                target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                                    masks_list[tar][ith_sample_in_sequence];
+                        }
                     }
                 }
                 else
                 {
                     for( int tar=0; tar < target_layers.length(); tar++ )
                     {
-                        target_connections[tar]->fprop(
-                            hidden_list[ith_sample_in_sequence],
-                            target_prediction_act_no_bias_list[tar][
-                                ith_sample_in_sequence]
-                            );
-                        target_layers[tar]->fprop(
-                            target_prediction_act_no_bias_list[tar][
-                                ith_sample_in_sequence],
-                            target_prediction_list[tar][
-                                ith_sample_in_sequence] );
-                        if( use_target_layers_masks )
-                            target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                                masks_list[tar][ith_sample_in_sequence];
+                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                        {
+                            target_connections[tar]->fprop(
+                                hidden_list[ith_sample_in_sequence],
+                                target_prediction_act_no_bias_list[tar][
+                                    ith_sample_in_sequence]
+                                );
+                            target_layers[tar]->fprop(
+                                target_prediction_act_no_bias_list[tar][
+                                    ith_sample_in_sequence],
+                                target_prediction_list[tar][
+                                    ith_sample_in_sequence] );
+                            if( use_target_layers_masks )
+                                target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                                    masks_list[tar][ith_sample_in_sequence];
+                        }
                     }
                 }
 
                 sum_target_elements = 0;
                 for( int tar=0; tar < target_layers.length(); tar++ )
                 {
-                    target_layers[tar]->activation << 
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence];
-                    target_layers[tar]->activation += target_layers[tar]->bias;
-                    target_layers[tar]->setExpectation(
-                        target_prediction_list[tar][
-                            ith_sample_in_sequence]);
-                    nll_list(ith_sample_in_sequence,tar) = 
-                        target_layers[tar]->fpropNLL( 
-                            targets_list[tar][ith_sample_in_sequence] ); 
-                    train_costs[tar] += nll_list(ith_sample_in_sequence,tar);
-
-                    // Normalize by the number of things to predict
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {
+                        target_layers[tar]->activation << 
+                            target_prediction_act_no_bias_list[tar][
+                                ith_sample_in_sequence];
+                        target_layers[tar]->activation += target_layers[tar]->bias;
+                        target_layers[tar]->setExpectation(
+                            target_prediction_list[tar][
+                                ith_sample_in_sequence]);
+                        nll_list(ith_sample_in_sequence,tar) = 
+                            target_layers[tar]->fpropNLL( 
+                                targets_list[tar][ith_sample_in_sequence] ); 
+                        train_costs[tar] += nll_list(ith_sample_in_sequence,tar);
+                        
+                        // Normalize by the number of things to predict
+                        if( use_target_layers_masks )
+                        {
+                            train_n_items[tar] += sum(
+                                input.subVec( inputsize_without_masks 
+                                              + sum_target_elements, 
+                                              target_layers_n_of_target_elements[tar]) );
+                        }
+                        else
+                            train_n_items[tar]++;
+                    }
                     if( use_target_layers_masks )
-                    {
-                        train_n_items[tar] += sum(
-                            input.subVec( inputsize_without_masks 
-                                          + sum_target_elements, 
-                                          target_layers_n_of_target_elements[tar]) );
                         sum_target_elements += 
                             target_layers_n_of_target_elements[tar];
-                    }
-                    else
-                        train_n_items[tar]++;
+                    
                 }
                 ith_sample_in_sequence++;
-               
-               
             }
             if( pb )
                 pb->update( stage + 1 - init_stage);
             
             for(int i=0; i<train_costs.length(); i++)
-                train_costs[i] /= train_n_items[i];
+            {
+                if( !fast_exact_is_equal(target_layers_weights[i],0) )
+                    train_costs[i] /= train_n_items[i];
+                else
+                    train_costs[i] = MISSING_VALUE;
+            }
 
             if(verbosity>0)
                 cout << "mean costs at stage " << stage << 
@@ -799,7 +824,8 @@
     input_layer->setLearningRate( the_learning_rate );
     hidden_layer->setLearningRate( the_learning_rate );
     input_connections->setLearningRate( the_learning_rate );
-    dynamic_connections->setLearningRate( the_learning_rate );
+    if( dynamic_connections )
+        dynamic_connections->setLearningRate( the_learning_rate );
     if( hidden_layer2 )
     {
         hidden_layer2->setLearningRate( the_learning_rate );
@@ -828,38 +854,44 @@
             {
                 for( int tar=0; tar<target_layers.length(); tar++)
                 {
-                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
-                    target_layers[tar]->activation += target_layers[tar]->bias;
-                    target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
-                    target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
-                    bias_gradient *= target_layers_weights[tar];
-                    bias_gradient *= masks_list[tar][i];
-                    target_layers[tar]->update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true);
-                    else
-                        target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true);
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {
+                        target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
+                        target_layers[tar]->activation += target_layers[tar]->bias;
+                        target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
+                        target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
+                        bias_gradient *= target_layers_weights[tar];
+                        bias_gradient *= masks_list[tar][i];
+                        target_layers[tar]->update(bias_gradient);
+                        if( hidden_layer2 )
+                            target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                                 hidden_gradient, bias_gradient,true);
+                        else
+                            target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                                 hidden_gradient, bias_gradient,true);
+                    }
                 }
             }
             else
             {
                 for( int tar=0; tar<target_layers.length(); tar++)
                 {
-                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
-                    target_layers[tar]->activation += target_layers[tar]->bias;
-                    target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
-                    target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
-                    bias_gradient *= target_layers_weights[tar];
-                    target_layers[tar]->update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true); 
-                    else
-                        target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true); 
-
+                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    {
+                        target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
+                        target_layers[tar]->activation += target_layers[tar]->bias;
+                        target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
+                        target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
+                        bias_gradient *= target_layers_weights[tar];
+                        target_layers[tar]->update(bias_gradient);
+                        if( hidden_layer2 )
+                            target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                                 hidden_gradient, bias_gradient,true); 
+                        else
+                            target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
+                                                                 hidden_gradient, bias_gradient,true); 
+                        
+                    }
                 }
             }
 
@@ -875,7 +907,7 @@
                     hidden_gradient, bias_gradient);
             }
             
-            if(i!=0)
+            if(i!=0 && dynamic_connections )
             {   
                 hidden_gradient += hidden_temporal_gradient;
                 
@@ -936,6 +968,7 @@
     real weight;
 
     Vec output(outputsize());
+    output.clear();
     Vec costs(nTestCosts());
     costs.clear();
     Vec n_items(nTestCosts());
@@ -999,21 +1032,24 @@
         target_prediction_act_no_bias_list.resize( target_layers.length() );
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
-            targets_list[tar].resize( ith_sample_in_sequence+1);
-            targets_list[tar][ith_sample_in_sequence].resize( 
-                target_layers[tar]->size);
-            target_prediction_list[tar].resize(
-                ith_sample_in_sequence+1);
-            target_prediction_act_no_bias_list[tar].resize(
-                ith_sample_in_sequence+1);
-
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                targets_list[tar].resize( ith_sample_in_sequence+1);
+                targets_list[tar][ith_sample_in_sequence].resize( 
+                    target_layers[tar]->size);
+                target_prediction_list[tar].resize(
+                    ith_sample_in_sequence+1);
+                target_prediction_act_no_bias_list[tar].resize(
+                    ith_sample_in_sequence+1);
+            }
         }
         nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
         if( use_target_layers_masks )
         {
             masks_list.resize( target_layers.length() );
             for( int tar=0; tar < target_layers.length(); tar++ )
-                masks_list[tar].resize( ith_sample_in_sequence+1 );
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    masks_list[tar].resize( ith_sample_in_sequence+1 );
         }
 
         // Forward propagation
@@ -1028,38 +1064,41 @@
         sum_target_elements = 0;
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
-            if( use_target_layers_masks )
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
             {
-                clamp_units(target.subVec(
-                                sum_target_elements,
-                                target_layers_n_of_target_elements[tar]),
-                            target_layers[tar],
-                            target_symbol_sizes[tar],
-                            input.subVec(
-                                inputsize_without_masks 
-                                + sum_target_elements, 
-                                target_layers_n_of_target_elements[tar]),
-                            masks_list[tar][ith_sample_in_sequence]
-                    );
-
+                if( use_target_layers_masks )
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar],
+                                input.subVec(
+                                    inputsize_without_masks 
+                                    + sum_target_elements, 
+                                    target_layers_n_of_target_elements[tar]),
+                                masks_list[tar][ith_sample_in_sequence]
+                        );
+                    
+                }
+                else
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar]);
+                }
+                targets_list[tar][ith_sample_in_sequence] << 
+                    target_layers[tar]->expectation;
             }
-            else
-            {
-                clamp_units(target.subVec(
-                                sum_target_elements,
-                                target_layers_n_of_target_elements[tar]),
-                            target_layers[tar],
-                            target_symbol_sizes[tar]);
-            }
             sum_target_elements += target_layers_n_of_target_elements[tar];
-            targets_list[tar][ith_sample_in_sequence] << 
-                target_layers[tar]->expectation;
         }
                 
         input_connections->fprop( input_list[ith_sample_in_sequence], 
                                   hidden_act_no_bias_list[ith_sample_in_sequence]);
                 
-        if( ith_sample_in_sequence > 0 )
+        if( ith_sample_in_sequence > 0 && dynamic_connections )
         {
             dynamic_connections->fprop( 
                 hidden_list[ith_sample_in_sequence-1],
@@ -1085,38 +1124,44 @@
 
             for( int tar=0; tar < target_layers.length(); tar++ )
             {
-                target_connections[tar]->fprop(
-                    hidden2_list[ith_sample_in_sequence],
-                    target_prediction_act_no_bias_list[tar][
-                        ith_sample_in_sequence]
-                    );
-                target_layers[tar]->fprop(
-                    target_prediction_act_no_bias_list[tar][
-                        ith_sample_in_sequence],
-                    target_prediction_list[tar][
-                        ith_sample_in_sequence] );
-                if( use_target_layers_masks )
-                    target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                        masks_list[tar][ith_sample_in_sequence];
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_connections[tar]->fprop(
+                        hidden2_list[ith_sample_in_sequence],
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence]
+                        );
+                    target_layers[tar]->fprop(
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence],
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence] );
+                    if( use_target_layers_masks )
+                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                            masks_list[tar][ith_sample_in_sequence];
+                }
             }
         }
         else
         {
             for( int tar=0; tar < target_layers.length(); tar++ )
             {
-                target_connections[tar]->fprop(
-                    hidden_list[ith_sample_in_sequence],
-                    target_prediction_act_no_bias_list[tar][
-                        ith_sample_in_sequence]
-                    );
-                target_layers[tar]->fprop(
-                    target_prediction_act_no_bias_list[tar][
-                        ith_sample_in_sequence],
-                    target_prediction_list[tar][
-                        ith_sample_in_sequence] );
-                if( use_target_layers_masks )
-                    target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                        masks_list[tar][ith_sample_in_sequence];
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_connections[tar]->fprop(
+                        hidden_list[ith_sample_in_sequence],
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence]
+                        );
+                    target_layers[tar]->fprop(
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence],
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence] );
+                    if( use_target_layers_masks )
+                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                            masks_list[tar][ith_sample_in_sequence];
+                }
             }
         }
 
@@ -1125,8 +1170,11 @@
             int sum_target_layers_size = 0;
             for( int tar=0; tar < target_layers.length(); tar++ )
             {
-                output.subVec(sum_target_layers_size,target_layers[tar]->size)
-                    << target_prediction_list[tar][ ith_sample_in_sequence ];
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    output.subVec(sum_target_layers_size,target_layers[tar]->size)
+                        << target_prediction_list[tar][ ith_sample_in_sequence ];
+                }
                 sum_target_layers_size += target_layers[tar]->size;
             }
             testoutputs->putOrAppendRow(i, output);
@@ -1135,30 +1183,34 @@
         sum_target_elements = 0;
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
-            target_layers[tar]->activation << 
-                target_prediction_act_no_bias_list[tar][
-                    ith_sample_in_sequence];
-            target_layers[tar]->activation += target_layers[tar]->bias;
-            target_layers[tar]->setExpectation(
-                target_prediction_list[tar][
-                    ith_sample_in_sequence]);
-            nll_list(ith_sample_in_sequence,tar) = 
-                target_layers[tar]->fpropNLL( 
-                    targets_list[tar][ith_sample_in_sequence] ); 
-            costs[tar] += nll_list(ith_sample_in_sequence,tar);
-
-            // Normalize by the number of things to predict
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                target_layers[tar]->activation << 
+                    target_prediction_act_no_bias_list[tar][
+                        ith_sample_in_sequence];
+                target_layers[tar]->activation += target_layers[tar]->bias;
+                target_layers[tar]->setExpectation(
+                    target_prediction_list[tar][
+                        ith_sample_in_sequence]);
+                nll_list(ith_sample_in_sequence,tar) = 
+                    target_layers[tar]->fpropNLL( 
+                        targets_list[tar][ith_sample_in_sequence] ); 
+                costs[tar] += nll_list(ith_sample_in_sequence,tar);
+                
+                // Normalize by the number of things to predict
+                if( use_target_layers_masks )
+                {
+                    n_items[tar] += sum(
+                        input.subVec( inputsize_without_masks 
+                                      + sum_target_elements, 
+                                      target_layers_n_of_target_elements[tar]) );
+                }
+                else
+                    n_items[tar]++;
+            }
             if( use_target_layers_masks )
-            {
-                n_items[tar] += sum(
-                    input.subVec( inputsize_without_masks 
-                                  + sum_target_elements, 
-                                  target_layers_n_of_target_elements[tar]) );
                 sum_target_elements += 
                     target_layers_n_of_target_elements[tar];
-            }
-            else
-                n_items[tar]++;
         }
         ith_sample_in_sequence++;
 
@@ -1168,8 +1220,12 @@
     }
 
     for(int i=0; i<costs.length(); i++)
-        costs[i] /= n_items[i];
-
+    {
+        if( !fast_exact_is_equal(target_layers_weights[i],0) )
+            costs[i] /= n_items[i];
+        else
+            costs[i] = MISSING_VALUE;
+    }
     if (testcosts)
         testcosts->putOrAppendRow(0, costs);
     

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-21 17:44:26 UTC (rev 9038)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-05-21 19:14:53 UTC (rev 9039)
@@ -1,6 +1,5 @@
 // -*- C++ -*-
 
-
 // DynamicallyLinkedRBMsModel.h
 //
 // Copyright (C) 2006 Stanislas Lauly



From lamblin at mail.berlios.de  Thu May 22 03:52:25 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 22 May 2008 03:52:25 +0200
Subject: [Plearn-commits] r9040 - trunk/plearn/ker
Message-ID: <200805220152.m4M1qPF7012671@sheep.berlios.de>

Author: lamblin
Date: 2008-05-22 03:52:24 +0200 (Thu, 22 May 2008)
New Revision: 9040

Modified:
   trunk/plearn/ker/BetaKernel.cc
Log:
Fix compilation in -float


Modified: trunk/plearn/ker/BetaKernel.cc
===================================================================
--- trunk/plearn/ker/BetaKernel.cc	2008-05-21 19:14:53 UTC (rev 9039)
+++ trunk/plearn/ker/BetaKernel.cc	2008-05-22 01:52:24 UTC (rev 9040)
@@ -125,7 +125,7 @@
             real b = (1.0 - px1[i]) / width + 1.0;
             real val = log_beta_density(px2[i],a,b);
             kvalue += val;
-        } 
+        }
     else if (kernel_type=="alternative")
         for(int i=0; i<l; i++)
         {
@@ -134,32 +134,32 @@
 
             if (x<0 || x >1 || px2[i] < 0 || px2[i] > 1)
                  PLERROR("In BetaKernel::evaluate x1 and x2 must contain values in the (closed) interval [0;1]");
-            
-            real p_xb = 2*pow(width,2.) + 2.5 - sqrt(4*pow(width,4.) + 6*pow(width,2.) + 2.25 - x*x - x / width);
+
+            real p_xb = 2*pow(width,real(2)) + 2.5 - sqrt(4*pow(width,real(4)) + 6*pow(width,real(2)) + 2.25 - x*x - x / width);
             real y = 1-x;
-            real p_1xb = 2*pow(width,2.) + 2.5 - sqrt(4*pow(width,4.)+ 6*pow(width,2.) + 2.25 - y*y - y / width);
+            real p_1xb = 2*pow(width,real(2)) + 2.5 - sqrt(4*pow(width,real(4))+ 6*pow(width,real(2)) + 2.25 - y*y - y / width);
 
             if ((x >= 2*width) && (x <= 1 - 2*width)) {
-                a = x / width; 
+                a = x / width;
                 b = (1 - x) / width;
             }
             else if ((x >= 0) & (x <= 2*width)) {
                 a = p_xb;
-                b = (1 - x) / width;   
+                b = (1 - x) / width;
             }
             else {
                 a = x / width;
                 b = p_1xb;
             }
-                
+
             real val = log_beta_density(px2[i],a,b);
             kvalue += val;
-        } 
+        }
     else
         PLERROR("In BetaKernel::evaluate kernel_type must be either \"simple\" or \"alternative\"");
-   
+
     real retval = 0.0;
- 
+
     if (output_type=="log_value")
         retval = kvalue;
     else if (output_type=="normal")



From lamblin at mail.berlios.de  Thu May 22 03:55:22 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 22 May 2008 03:55:22 +0200
Subject: [Plearn-commits] r9041 - in trunk/python_modules/plearn: parallel
	pymake
Message-ID: <200805220155.m4M1tMUX012877@sheep.berlios.de>

Author: lamblin
Date: 2008-05-22 03:55:22 +0200 (Thu, 22 May 2008)
New Revision: 9041

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Fix some problems with pymake -dbi=Condor


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-22 01:52:24 UTC (rev 9040)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-22 01:55:22 UTC (rev 9041)
@@ -32,6 +32,7 @@
 STATUS_WAITING = 2
 STATUS_INIT = 3
 
+
 #original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
 class LockedIterator:
     def __init__( self, iterator ):
@@ -662,6 +663,14 @@
     def wait(self):
         print "[DBI] WARNING cannot wait until all jobs are done for bqtools, use bqwatch or bqstatus"
 
+
+# Transfor a string so that it is treated by Condor as a single argument
+def condor_escape_argument(argstring):
+    # Double every single quote and double quote character,
+    # surround the result by a pair of single quotes,
+    # then surrount everything by a pair of double quotes
+    return "\"'" + argstring.replace("'", "''").replace('"','""') + "'\""
+
 class DBICondor(DBIBase):
 
     def __init__( self, commands, **args ):
@@ -698,31 +707,38 @@
         # create the information about the tasks
         id=len(self.tasks)+1
         for command in commands:
-	    c_split = command.split()
+            c_split = command.split()
             # c = program name, c2 = arguments
-	    c = c_split[0]
-	    if len(c_split) > 1:
-	    	c2 = ' ' + ' '.join(c_split[1:])
-	    else:
-		c2 = ''
+            c = c_split[0]
+            if len(c_split) > 1:
+                c2 = ' ' + ' '.join(c_split[1:])
+            else:
+                c2 = ''
 
             # We use the absolute path so that we don't have corner case as with ./
             shellcommand=False
-            authorized_shell_commands=[ "touch", "echo"]
-            if c in authorized_shell_commands:
+            # Maybe the command is not in the form: executable_name args,
+            # but starts with a character that is interpreted by the shell
+            # in a special way. I.e., a sequence of commands, like:
+            # 'prog1; prog2 arg1 arg2' (with the quotes).
+            # The command might also be a shell built-in command.
+            # Feel free to complete this list
+            shell_special_chars = ["'", '"', ' ', '$', '`', '(', ';']
+            authorized_shell_commands=[ "touch", "echo", "cd" ]
+            if c[0] in shell_special_chars or c in authorized_shell_commands:
                 shellcommand=True
             elif not self.files:
-		# Transform path to get an absolute path.
+                # Transform path to get an absolute path.
                 c_abs = os.path.abspath(c)
-		if os.path.isfile(c_abs):
-		    # The file is in the current directory (easy case).
-		    c = c_abs
-		elif not os.path.isabs(c):
-		    # We need to find where the file could be... easiest way to
+                if os.path.isfile(c_abs):
+                    # The file is in the current directory (easy case).
+                    c = c_abs
+                elif not os.path.isabs(c):
+                    # We need to find where the file could be... easiest way to
                     # do it is ask the 'which' shell command.
-	 	    which_out = subprocess.Popen('which %s' % c, shell = True, stdout = PIPE).stdout.readlines()
-		    if len(which_out) == 1:
-			c = which_out[0].strip()
+                    which_out = subprocess.Popen('which %s' % c, shell = True, stdout = PIPE).stdout.readlines()
+                    if len(which_out) == 1:
+                        c = which_out[0].strip()
 
             command = "".join([c,c2])
 
@@ -869,7 +885,8 @@
                 condor_dat.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")
         else:
             for task in self.tasks:
-                condor_dat.write("arguments      = %s \nqueue\n" %(' ; '.join(task.commands)))
+                argstring = condor_escape_argument(' ; '.join(task.commands))
+                condor_dat.write("arguments      = %s \nqueue\n" % argstring)
         condor_dat.close()
 
         dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
@@ -883,13 +900,13 @@
                 if mtimed>mtimel:
                     print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your needs!'
                     overwrite_launch_file=True
-        
+
         if self.copy_local_source_file:
             source_file_dest = os.path.join(self.log_dir,
                                             os.path.basename(source_file))
             shutil.copy( source_file, source_file_dest)
             self.temp_files.append(source_file_dest)
-            os.chmod(source_file_dest, 0755)   
+            os.chmod(source_file_dest, 0755)
             source_file=source_file_dest
 
         if not os.path.exists(launch_file) or overwrite_launch_file:
@@ -897,9 +914,8 @@
             launch_dat = open(launch_file,'w')
             if source_file and not source_file.endswith(".cshrc"):
                 launch_dat.write(dedent('''\
-                #!/bin/sh
-                PROGRAM=$1
-                shift\n'''))
+                    #!/bin/sh
+                    '''))
                 if condor_home:
                     launch_dat.write('export HOME=%s\n' % condor_home)
                 if source_file:
@@ -915,8 +931,9 @@
                     #python -V 1>&2
                     #echo -n /usr/bin/python version: 1>&2
                     #/usr/bin/python -V 1>&2
-                    echo ${PROGRAM} $@ 1>&2
-                    ${PROGRAM} "$@"'''))
+                    echo "Running command: sh -c \\"$@\\"" 1>&2
+                    sh -c "$@"
+                    '''))
             else:
                 launch_dat.write(dedent('''\
                     #! /bin/tcsh

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-05-22 01:52:24 UTC (rev 9040)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-05-22 01:55:22 UTC (rev 9041)
@@ -1060,8 +1060,8 @@
         except OSError:
             pass
 
-        #Command to execute: "'cd " + ccfile.filedir + "; " + ccfile.compile_command() + "; echo $?'"
-        commands.append("'cd " + ccfile.filedir + "; " + ccfile.compile_command() + "; echo $?'")
+        # Maybe the "echo $?" part isn't useful
+        commands.append("cd " + ccfile.filedir + "; " + ccfile.compile_command() + "; echo $?")
 
     from plearn.parallel.dbi import DBI
     jobs = DBI(commands, dbi_mode)



From nouiz at mail.berlios.de  Thu May 22 17:42:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 22 May 2008 17:42:16 +0200
Subject: [Plearn-commits] r9042 - in trunk: plearn/base plearn/io
	plearn_learners/hyper
Message-ID: <200805221542.m4MFgGON032572@sheep.berlios.de>

Author: nouiz
Date: 2008-05-22 17:42:15 +0200 (Thu, 22 May 2008)
New Revision: 9042

Modified:
   trunk/plearn/base/TypeTraits.h
   trunk/plearn/io/PStream.h
   trunk/plearn_learners/hyper/StepwiseSelectionOracle.cc
Log:
implemented the serialization of priority_queue.

Implmented the declareOption that need it.


Modified: trunk/plearn/base/TypeTraits.h
===================================================================
--- trunk/plearn/base/TypeTraits.h	2008-05-22 01:55:22 UTC (rev 9041)
+++ trunk/plearn/base/TypeTraits.h	2008-05-22 15:42:15 UTC (rev 9042)
@@ -49,6 +49,7 @@
 #include <vector>
 #include <list>
 #include <map>
+#include <queue>
 #include <set>
 #include <nspr/prlong.h>
 #include <plearn/base/pl_stdint.h>
@@ -343,6 +344,20 @@
     { return 0xFF; }
 };
 
+template<class T>
+class TypeTraits< std::priority_queue<T> >
+{
+public:
+    static inline string name()
+    { return string("priority_queue< ") + TypeTraits<T>::name() + " >"; }
+
+    static inline unsigned char little_endian_typecode()
+    { return 0xFF; }
+
+    static inline unsigned char big_endian_typecode()
+    { return 0xFF; }
+};
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2008-05-22 01:55:22 UTC (rev 9041)
+++ trunk/plearn/io/PStream.h	2008-05-22 15:42:15 UTC (rev 9042)
@@ -1523,6 +1523,56 @@
 operator<<(PStream &out, const set<T> &v)
 { writeSet(out, v); return out; }
 
+
+
+// Serialization of priority_queue types
+
+template<class PriorityQueueT>
+void writePriorityQueue(PStream& out, const PriorityQueueT& pq)
+{
+    PriorityQueueT pq2(pq);
+    out.put('[');
+    while(!pq2.empty())
+    {
+        typename PriorityQueueT::value_type val;
+        val = pq2.top();
+        out << val;
+        pq2.pop();
+        if (!pq2.empty())
+            out.write(", ");
+    }
+    out.put(']');
+}
+
+template<class PriorityQueueT>
+void readPriorityQueue(PStream& in, PriorityQueueT& pq)
+{
+    PLCHECK(pq.empty());
+    in.skipBlanksAndCommentsAndSeparators();
+    int c = in.get();
+    if(c!='[')
+        PLERROR("In readPriorityQueue(Pstream& in, PriorityQueueT& pq) expected '[' but read %c",c);
+    in.skipBlanksAndCommentsAndSeparators();
+    c = in.peek(); // do we have a ']' ?
+    while(c!=']')
+    {
+        typename PriorityQueueT::value_type val;
+        in >> val;
+        pq.push(val);
+        in.skipBlanksAndCommentsAndSeparators();
+        c = in.peek(); // do we have a ']' ?
+    }
+    in.get(); // eat the ']'
+}
+
+template <class T> inline PStream &
+operator>>(PStream &in, priority_queue<T> &v)
+{ readPriorityQueue(in, v); return in; }
+
+template <class T> inline PStream &
+operator<<(PStream &out, const priority_queue<T> &v)
+{ writePriorityQueue(out, v); return out; }
+
 /// @deprecated Use openFile instead.
 class PIFStream: public PStream
 {

Modified: trunk/plearn_learners/hyper/StepwiseSelectionOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/StepwiseSelectionOracle.cc	2008-05-22 01:55:22 UTC (rev 9041)
+++ trunk/plearn_learners/hyper/StepwiseSelectionOracle.cc	2008-05-22 15:42:15 UTC (rev 9042)
@@ -86,17 +86,11 @@
                   "Contains the remaining combinations to generate for the"
                   " current variable.");
 
-    //Should be saved, but PLearn don't save priority_queue now.
-    //TODO: implement in PStream.h 
-    //void writePriorityQueue(PStream& out, const PriorityQueueTemplate& s)
-    //void readPriorityQueue(PStream& in, const PriorityQueueTemplate& s)
-    //operator>>(PStream &in, priority_queue<T> &v)
-    //operator>>(PStream &in, priority_queue<T> &v)
-//     declareOption(ol, "combination_performance",
-//                   &StepwiseSelectionOracle:: combination_performance,
-//                   OptionBase::learntoption,
-//                   "This remembers the performance of each combination tried in the"
-//                   " current search set.");
+    declareOption(ol, "combination_performance",
+                  &StepwiseSelectionOracle:: combination_performance,
+                  OptionBase::learntoption,
+                  "This remembers the performance of each combination tried in the"
+                  " current search set.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);



From nouiz at mail.berlios.de  Fri May 23 16:18:25 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 May 2008 16:18:25 +0200
Subject: [Plearn-commits] r9043 - trunk/python_modules/plearn/parallel
Message-ID: <200805231418.m4NEIPYP018039@sheep.berlios.de>

Author: nouiz
Date: 2008-05-23 16:18:24 +0200 (Fri, 23 May 2008)
New Revision: 9043

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
typo, the max time on mammouth is 120h and not 12h for the normal queue


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-22 15:42:15 UTC (rev 9042)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-23 14:18:24 UTC (rev 9043)
@@ -540,7 +540,7 @@
         self.micro = 1
         self.queue = "qwork at ms"
         self.long = False
-        self.duree = "12:00:00"
+        self.duree = "120:00:00"
         self.mem = None
         DBIBase.__init__(self, commands, **args)
 



From dumitruerhan at mail.berlios.de  Fri May 23 17:08:36 2008
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Fri, 23 May 2008 17:08:36 +0200
Subject: [Plearn-commits] r9044 - trunk/plearn/ker
Message-ID: <200805231508.m4NF8avQ023146@sheep.berlios.de>

Author: dumitruerhan
Date: 2008-05-23 17:08:36 +0200 (Fri, 23 May 2008)
New Revision: 9044

Modified:
   trunk/plearn/ker/BetaKernel.cc
Log:
several efficiency improvements

Modified: trunk/plearn/ker/BetaKernel.cc
===================================================================
--- trunk/plearn/ker/BetaKernel.cc	2008-05-23 14:18:24 UTC (rev 9043)
+++ trunk/plearn/ker/BetaKernel.cc	2008-05-23 15:08:36 UTC (rev 9044)
@@ -132,24 +132,20 @@
             real x = px1[i];
             real a, b;
 
-            if (x<0 || x >1 || px2[i] < 0 || px2[i] > 1)
-                 PLERROR("In BetaKernel::evaluate x1 and x2 must contain values in the (closed) interval [0;1]");
+            PLASSERT_MSG(x<0 || x >1 || px2[i] < 0 || px2[i] > 1, "In BetaKernel::evaluate x1 and x2 must contain values in the (closed) interval [0;1]");
 
-            real p_xb = 2*pow(width,real(2)) + 2.5 - sqrt(4*pow(width,real(4)) + 6*pow(width,real(2)) + 2.25 - x*x - x / width);
-            real y = 1-x;
-            real p_1xb = 2*pow(width,real(2)) + 2.5 - sqrt(4*pow(width,real(4))+ 6*pow(width,real(2)) + 2.25 - y*y - y / width);
-
             if ((x >= 2*width) && (x <= 1 - 2*width)) {
                 a = x / width;
                 b = (1 - x) / width;
             }
             else if ((x >= 0) & (x <= 2*width)) {
-                a = p_xb;
+                a = 2*pow(width,real(2)) + 2.5 - sqrt(4*pow(width,real(4)) + 6*pow(width,real(2)) + 2.25 - x*x - x / width);
                 b = (1 - x) / width;
             }
             else {
                 a = x / width;
-                b = p_1xb;
+                real y = 1-x;
+                b = 2*pow(width,real(2)) + 2.5 - sqrt(4*pow(width,real(4))+ 6*pow(width,real(2)) + 2.25 - y*y - y / width);
             }
 
             real val = log_beta_density(px2[i],a,b);



From nouiz at mail.berlios.de  Fri May 23 18:05:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 May 2008 18:05:26 +0200
Subject: [Plearn-commits] r9045 - trunk/plearn/base
Message-ID: <200805231605.m4NG5QZU030302@sheep.berlios.de>

Author: nouiz
Date: 2008-05-23 18:05:26 +0200 (Fri, 23 May 2008)
New Revision: 9045

Modified:
   trunk/plearn/base/CopiesMap.h
Log:
Added basic type to don't copy


Modified: trunk/plearn/base/CopiesMap.h
===================================================================
--- trunk/plearn/base/CopiesMap.h	2008-05-23 15:08:36 UTC (rev 9044)
+++ trunk/plearn/base/CopiesMap.h	2008-05-23 16:05:26 UTC (rev 9045)
@@ -109,8 +109,17 @@
 NODEEPCOPY(const float)
 NODEEPCOPY(int)
 NODEEPCOPY(const int)
+NODEEPCOPY(unsigned int)
+NODEEPCOPY(const unsigned int)
+NODEEPCOPY(short)
+NODEEPCOPY(const short)
+NODEEPCOPY(unsigned short)
+NODEEPCOPY(const unsigned short)
+NODEEPCOPY(char)
+NODEEPCOPY(const char)
+NODEEPCOPY(unsigned char)
+NODEEPCOPY(const unsigned char)
 NODEEPCOPY(const string)
-NODEEPCOPY(unsigned int)
 NODEEPCOPY(clock_t)
 NODEEPCOPY(bool)
 NODEEPCOPY(map_string_float)



From nouiz at mail.berlios.de  Fri May 23 20:23:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 May 2008 20:23:14 +0200
Subject: [Plearn-commits] r9046 - trunk/python_modules/plearn/pymake
Message-ID: <200805231823.m4NINEWt003456@sheep.berlios.de>

Author: nouiz
Date: 2008-05-23 20:23:13 +0200 (Fri, 23 May 2008)
New Revision: 9046

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
pytest with -local_ofiles was not making the needed directory


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-05-23 16:05:26 UTC (rev 9045)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-05-23 18:23:13 UTC (rev 9046)
@@ -2328,6 +2328,7 @@
         new_corresponding_output = self.corresponding_output+".new"
         if local_ofiles:
             self.corresponding_output = local_filepath(new_corresponding_output)
+            mkdirs_public(os.path.dirname(self.corresponding_output))
         else:
             self.corresponding_output = new_corresponding_output
         try:



From laulysta at mail.berlios.de  Fri May 23 21:06:19 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 23 May 2008 21:06:19 +0200
Subject: [Plearn-commits] r9047 - trunk/plearn_learners/online
Message-ID: <200805231906.m4NJ6Jai016335@sheep.berlios.de>

Author: laulysta
Date: 2008-05-23 21:06:19 +0200 (Fri, 23 May 2008)
New Revision: 9047

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
Log:
good gradiant for the MSE



Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-05-23 18:23:13 UTC (rev 9046)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-05-23 19:06:19 UTC (rev 9047)
@@ -56,6 +56,7 @@
     share_quad_coeff( false ),
     size_quad_coeff( 0 ),
     fixed_std_deviation( -1 ),
+    compute_mse_instead_of_nll( false ),
     sigma_is_up_to_date( false )
 {
 }
@@ -66,6 +67,7 @@
     share_quad_coeff( false ),
     size_quad_coeff( 0 ),
     fixed_std_deviation( -1 ),
+    compute_mse_instead_of_nll( false ),
     quad_coeff( the_size, 1. ), // or 1./M_SQRT2 ?
     quad_coeff_pos_stats( the_size ),
     quad_coeff_neg_stats( the_size ),
@@ -86,6 +88,7 @@
     inherited( the_learning_rate ),
     min_quad_coeff( 0. ),
     fixed_std_deviation( -1 ),
+    compute_mse_instead_of_nll( false ),
     quad_coeff_pos_stats( the_size ),
     quad_coeff_neg_stats( the_size ),
     sigma_is_up_to_date( false )
@@ -347,6 +350,10 @@
                   OptionBase::learntoption,
                   "Quadratic coefficients of the units.");
 
+    declareOption(ol, "sigma", &RBMGaussianLayer::sigma,
+                  OptionBase::learntoption,
+                  "comments...");
+
     declareOption(ol, "share_quad_coeff", &RBMGaussianLayer::share_quad_coeff,
                   OptionBase::buildoption,
                   "Should all the units share the same quadratic coefficients?\n"
@@ -361,7 +368,13 @@
                   "appropriate value.\n"
                   "If < 0, then this option is ignored.\n");
 
+    declareOption(ol, "compute_mse_instead_of_nll", &RBMGaussianLayer::compute_mse_instead_of_nll,
+                  OptionBase::buildoption,
+                  "Indication that fpropNLL should compute the MSE instead of the NLL..\n"
+                  "bpropNLL will also give the appropriate gradient. Might want to\n"
+                  "set fixed_std_deviation to 1 in this case.\n");
 
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -710,25 +723,37 @@
     computeStdDeviation();
 
     real ret = 0;
-    if(share_quad_coeff)
+    if( compute_mse_instead_of_nll )
+    {
+        real r;
         for( int i=0 ; i<size ; i++ )
         {
-            real r = (target[i] - expectation[i]) * quad_coeff[0];
-            ret += r * r + pl_log(sigma[0]);
+            r = (target[i] - expectation[i]);
+            ret += r * r;
         }
+    } 
     else
-        for( int i=0 ; i<size ; i++ )
-        {
-            // ret += (target[i]-expectation[i])^2/(2 sigma[i]^2)
-            //      + log(sqrt(2*Pi) * sigma[i])
-            real r = (target[i] - expectation[i]) * quad_coeff[i];
-            ret += r * r + pl_log(sigma[i]);
-        }
-    ret += 0.5*size*Log2Pi;
+    {
+        if(share_quad_coeff)
+            for( int i=0 ; i<size ; i++ )
+            {
+                real r = (target[i] - expectation[i]) * quad_coeff[0];
+                ret += r * r + pl_log(sigma[0]);
+            }
+        else
+            for( int i=0 ; i<size ; i++ )
+            {
+                // ret += (target[i]-expectation[i])^2/(2 sigma[i]^2)
+                //      + log(sqrt(2*Pi) * sigma[i])
+                real r = (target[i] - expectation[i]) * quad_coeff[i];
+                ret += r * r + pl_log(sigma[i]);
+                
+            }
+        ret += 0.5*size*Log2Pi;
+    }
     return ret;
 }
 
-
 void RBMGaussianLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
 {
 
@@ -743,38 +768,57 @@
     real nll;
     real *expectation, *target;
 
-    if(share_quad_coeff)
+    if( compute_mse_instead_of_nll )
+    {
         for (int k=0;k<batch_size;k++) // loop over minibatch
         {
             nll = 0;
             expectation = expectations[k];
             target = targets[k];
+            real r;
             for( register int i=0 ; i<size ; i++ ) // loop over outputs
             {
-                real r = (target[i] - expectation[i]) * quad_coeff[0];
-                nll += r * r + pl_log(sigma[0]);
+                r = (target[i] - expectation[i]);
+                nll += r * r;
             }
-            nll += 0.5*size*Log2Pi;
             costs_column(k,0) = nll;
         }
+    }
     else
-        for (int k=0;k<batch_size;k++) // loop over minibatch
-        {
-            nll = 0;
-            expectation = expectations[k];
-            target = targets[k];
-            for( register int i=0 ; i<size ; i++ ) // loop over outputs
+    {
+        if(share_quad_coeff)
+            for (int k=0;k<batch_size;k++) // loop over minibatch
             {
-                // nll += (target[i]-expectation[i])^2/(2 sigma[i]^2)
-                //      + log(sqrt(2*Pi) * sigma[i])
-                real r = (target[i] - expectation[i]) * quad_coeff[i];
-                nll += r * r + pl_log(sigma[i]);
+                nll = 0;
+                expectation = expectations[k];
+                target = targets[k];
+                for( register int i=0 ; i<size ; i++ ) // loop over outputs
+                {
+                    real r = (target[i] - expectation[i]) * quad_coeff[0];
+                    nll += r * r + pl_log(sigma[0]);
+                }
+                nll += 0.5*size*Log2Pi;
+                costs_column(k,0) = nll;
             }
-            nll += 0.5*size*Log2Pi;
-            costs_column(k,0) = nll;
-        }
+        else
+            for (int k=0;k<batch_size;k++) // loop over minibatch
+            {
+                nll = 0;
+                expectation = expectations[k];
+                target = targets[k];
+                for( register int i=0 ; i<size ; i++ ) // loop over outputs
+                {
+                    // nll += (target[i]-expectation[i])^2/(2 sigma[i]^2)
+                    //      + log(sqrt(2*Pi) * sigma[i])
+                    real r = (target[i] - expectation[i]) * quad_coeff[i];
+                    nll += r * r + pl_log(sigma[i]);
+                }
+                nll += 0.5*size*Log2Pi;
+                costs_column(k,0) = nll;
+            }
+    }
 }
-
+    
 void RBMGaussianLayer::bpropNLL(const Vec& target, real nll, Vec& bias_gradient)
 {
     computeExpectation();
@@ -784,6 +828,11 @@
 
     // bias_gradient = expectation - target
     substract(expectation, target, bias_gradient);
+
+    if( compute_mse_instead_of_nll )
+        bias_gradient *= 2;
+    addBiasDecay(bias_gradient);
+
 }
 
 void RBMGaussianLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -799,6 +848,11 @@
 
     // bias_gradients = expectations - targets
     substract(expectations, targets, bias_gradients);
+
+    if( compute_mse_instead_of_nll )
+        bias_gradients *= 2;
+    addBiasDecay(bias_gradients);
+
 }
 
 

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2008-05-23 18:23:13 UTC (rev 9046)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2008-05-23 19:06:19 UTC (rev 9047)
@@ -69,6 +69,11 @@
     //! If < 0, then this option is ignored.
     real fixed_std_deviation;
 
+    //! Indication that fpropNLL should compute the MSE instead of the NLL.
+    //! bpropNLL will also give the appropriate gradient. Might want to
+    //! set fixed_std_deviation to 1 in this case.
+    bool compute_mse_instead_of_nll;
+
 public:
     //#####  Public Member Functions  #########################################
 



From nouiz at mail.berlios.de  Fri May 23 21:24:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 May 2008 21:24:57 +0200
Subject: [Plearn-commits] r9048 - trunk/plearn_learners/regressors
Message-ID: <200805231924.m4NJOvxe018427@sheep.berlios.de>

Author: nouiz
Date: 2008-05-23 21:24:57 +0200 (Fri, 23 May 2008)
New Revision: 9048

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
Added macro RTR_type with value int. If you use a not too big trainset, you can set it to uint16_t to save memory space.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-05-23 19:06:19 UTC (rev 9047)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-05-23 19:24:57 UTC (rev 9048)
@@ -42,6 +42,7 @@
 #include "RegressionTreeNode.h"
 #include "RegressionTreeRegisters.h"
 #include "RegressionTreeLeave.h"
+#include "RegressionTreeRegisters.h"
 
 namespace PLearn {
 using namespace std;
@@ -217,16 +218,16 @@
 {
     if(leave->length<=1)
         return;
-    TVec<int> candidate(0,train_set->length());//list of candidate row to split
-    TVec<int> registered_row;
+    TVec<RTR_type> candidate(0, leave->length);//list of candidate row to split
+    TVec<RTR_type> registered_row(0, leave->length);
     tmp_vec.resize(2);
     Vec left_error(3);
     Vec right_error(3);
     Vec missing_error(3);
     missing_error.clear();
-
+    int inputsize = train_set->inputsize();
     int leave_id = leave->id;
-    for (int col = 0; col < train_set->inputsize(); col++)
+    for (int col = 0; col < inputsize; col++)
     {
         missing_leave->initStats();
         left_leave->initStats();
@@ -255,7 +256,9 @@
             int next_row = candidate.pop();
             left_leave->removeRow(row, tmp_vec, left_error);
             right_leave->addRow(row, tmp_vec, right_error);
-            compareSplit(col, train_set->get(next_row, col), train_set->get(row, col), left_error, right_error, missing_error);
+            compareSplit(col, train_set->get(next_row, col),
+                         train_set->get(row, col), left_error,
+                         right_error, missing_error);
             row = next_row;
         }
     }
@@ -288,7 +291,7 @@
     missing_leave->initStats();
     left_leave->initStats();
     right_leave->initStats();
-    TVec<int>registered_row;
+    TVec<RTR_type>registered_row;
     train_set->getAllRegisteredRow(leave->id,split_col,registered_row);
 
     for (int row_index = 0;row_index<registered_row.size();row_index++)

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-05-23 19:06:19 UTC (rev 9047)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-05-23 19:24:57 UTC (rev 9048)
@@ -121,6 +121,10 @@
 
 void RegressionTreeRegisters::initRegisters(VMat the_train_set)
 {   
+    //check that we can put all the examples of the train_set
+    //with respect to the size of RTR_type who limit the capacity
+    PLCHECK(the_train_set.length()<pow((real)2,(real)(sizeof(RTR_type)*8)));
+
     if(the_train_set==source && tsource)
         //we set the existing source file
         return;
@@ -178,13 +182,13 @@
     setWeight(i,value);
 }
 
-int RegressionTreeRegisters::getNextId()
+RTR_type RegressionTreeRegisters::getNextId()
 {
     next_id += 1;
     return next_id;
 }
 
-void RegressionTreeRegisters::getAllRegisteredRow(int leave_id, int col, TVec<int> &reg)
+void RegressionTreeRegisters::getAllRegisteredRow(int leave_id, int col, TVec<RTR_type> &reg)
 {
     for(int i=0;i<length();i++)
     {

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-05-23 19:06:19 UTC (rev 9047)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-05-23 19:24:57 UTC (rev 9048)
@@ -47,6 +47,11 @@
 #include <plearn/math/TMat.h>
 #include <plearn/vmat/VMat.h>
 
+//!used to limit the memory used by limiting the length of the dataset.
+//!work with unsigned int, uint16_t, but fail with uint8_t???
+//!always use unsigned type! Otherwise you need to modif RegressionTreeRegisters.cc too
+#define RTR_type uint32_t 
+
 namespace PLearn {
 using namespace std;
 
@@ -70,8 +75,9 @@
 
     int       next_id;
     //TMat<int> sorted_row;
-    TMat<int> tsorted_row;
-    TVec<int> leave_register;
+
+    TMat<RTR_type> tsorted_row;
+    TVec<RTR_type> leave_register;
     VMat tsource;
  
 public:
@@ -91,8 +97,8 @@
     real         getTarget(int row);
     real         getWeight(int row);
     void         setWeight(int row,real val);
-    int          getNextId();
-    void         getAllRegisteredRow(int leave_id, int col, TVec<int> &reg);
+    RTR_type     getNextId();
+    void         getAllRegisteredRow(int leave_id, int col, TVec<RTR_type> &reg);
     void         sortRows();
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);



From nouiz at mail.berlios.de  Fri May 23 21:29:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 May 2008 21:29:54 +0200
Subject: [Plearn-commits] r9049 - trunk/plearn_learners/regressors
Message-ID: <200805231929.m4NJTs66019077@sheep.berlios.de>

Author: nouiz
Date: 2008-05-23 21:29:53 +0200 (Fri, 23 May 2008)
New Revision: 9049

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
implemented new version of lookForBestSplit() that find the best by row before globally. This is not slower and allow to get stats by row.
Also will be easier to make parallel version


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-05-23 19:24:57 UTC (rev 9048)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-05-23 19:29:53 UTC (rev 9049)
@@ -42,7 +42,6 @@
 #include "RegressionTreeNode.h"
 #include "RegressionTreeRegisters.h"
 #include "RegressionTreeLeave.h"
-#include "RegressionTreeRegisters.h"
 
 namespace PLearn {
 using namespace std;
@@ -213,7 +212,8 @@
 
     leave->getOutputAndError(leave_output,leave_error);
 }
-
+#define BY_ROW
+//#define RCMP
 void RegressionTreeNode::lookForBestSplit()
 {
     if(leave->length<=1)
@@ -226,6 +226,14 @@
     Vec missing_error(3);
     missing_error.clear();
     int inputsize = train_set->inputsize();
+#ifdef RCMP
+    Vec row_split_err(inputsize);
+    Vec row_split_value(inputsize);
+    Vec row_split_balance(inputsize);
+    row_split_err.clear();
+    row_split_value.clear();
+    row_split_balance.clear();
+#endif
     int leave_id = leave->id;
     for (int col = 0; col < inputsize; col++)
     {
@@ -235,6 +243,7 @@
         registered_row.resize(0);
         train_set->getAllRegisteredRow(leave_id,col,registered_row);
         PLASSERT(registered_row.size()>0);
+        PLASSERT(candidate.size()==0);
         for(int row_idx = 0;row_idx<registered_row.size();row_idx++)
         {
             int row=registered_row[row_idx];
@@ -247,6 +256,7 @@
         }
         missing_leave->getOutputAndError(tmp_vec, missing_error);
 
+#ifndef BY_ROW
         //in case of missing value
         if(candidate.size()==0)
             continue;
@@ -261,9 +271,89 @@
                          right_error, missing_error);
             row = next_row;
         }
+#else
+        tuple<real,real,int> ret=bestSplitInRow(col, candidate, left_error,
+                                                right_error, missing_error,
+                                                right_leave, left_leave,
+                                                train_set);
+#ifdef RCMP
+        row_split_err[col] = get<0>(ret);
+        row_split_value[col] = get<1>(ret);
+        row_split_balance[col] = get<2>(ret);
+#endif
+        if (fast_is_more(get<0>(ret), after_split_error)) continue;
+        else if (fast_is_equal(get<0>(ret), after_split_error) &&
+                 fast_is_more(get<2>(ret), split_balance)) continue;
+
+        split_col = col;
+        after_split_error = get<0>(ret);
+        split_feature_value = get<1>(ret);
+        split_balance = get<2>(ret);
+#endif
     }
+#ifdef RCMP
+    pout<<"error after split: "<<after_split_error<<endl;
+    pout<<"split value: "<<split_feature_value<<endl;
+    pout<<"split_col: "<<split_col<<" "<<train_set->fieldName(split_col)<<endl;
+    pout<<"col 27 ("<<train_set->fieldName(27)<<") split error "
+        <<row_split_err[27]<<endl;
+    pout<<"col 27 ("<<train_set->fieldName(27)<<") split value "
+        <<row_split_value[27]<<endl;
+#endif
 }
+tuple<real,real,int>RegressionTreeNode::bestSplitInRow(
+    int col,
+    TVec<RTR_type>& candidates,
+    Vec left_error,
+    Vec right_error,
+    const Vec missing_error,
+    PP<RegressionTreeLeave> right_leave,
+    PP<RegressionTreeLeave> left_leave,
+    PP<RegressionTreeRegisters> train_set
+    )
+{
+    int best_balance=INT_MAX;
+    real best_feature_value = REAL_MAX;
+    real best_split_error = REAL_MAX;
+    //in case of only missing value
+    if(candidates.size()==0)
+        return make_tuple(best_feature_value, best_split_error, best_balance);
+    int row = candidates.pop();
+//    real row_value = train_set->get(row, col);
+    Vec tmp(3);
 
+    while (candidates.size()>0)
+    {
+        int next_row = candidates.pop();
+        left_leave->removeRow(row, tmp, left_error);
+        right_leave->addRow(row, tmp, right_error);
+
+        {//compare split
+        real next_feature=train_set->get(next_row, col);
+        real row_feature=train_set->get(row, col);
+        PLASSERT(next_feature<=row_feature);
+//        PLASSERT(row_value == row_feature);
+        row = next_row;
+//        row_value = next_feature;
+
+        if (next_feature >= row_feature) continue;
+        real work_error = missing_error[0] + missing_error[1] + left_error[0]
+            + left_error[1] + right_error[0] + right_error[1];
+        int work_balance = abs(left_leave->getLength() -
+                               right_leave->getLength());
+        if (fast_is_more(work_error,best_split_error)) continue;
+        else if (fast_is_equal(work_error,best_split_error) &&
+                 fast_is_more(work_balance,best_balance)) continue;
+
+        best_feature_value = 0.5 * (row_feature + next_feature);
+        best_split_error = work_error;
+        best_balance = work_balance;
+        }
+
+    }
+    return make_tuple(best_split_error, best_feature_value, best_balance);
+}
+
 void RegressionTreeNode::compareSplit(int col, real left_leave_last_feature, real right_leave_first_feature,
                                       Vec left_error, Vec right_error, Vec missing_error)
 {

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-05-23 19:24:57 UTC (rev 9048)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-05-23 19:29:53 UTC (rev 9049)
@@ -45,6 +45,7 @@
 #include <plearn/base/Object.h>
 #include <plearn/base/PP.h>
 #include <plearn/math/TVec.h>
+#include <boost/tuple/tuple.hpp>
 
 namespace PLearn {
 using namespace std;
@@ -117,6 +118,11 @@
 private:
     void         build_();
     void         verbose(string msg, int level); 
+    static tuple<real,real,int> bestSplitInRow(int col, TVec<RTR_type>& candidates,
+        Vec left_error, Vec right_error, const Vec missing_error,
+        PP<RegressionTreeLeave> right_leave, PP<RegressionTreeLeave> left_leave,
+        PP<RegressionTreeRegisters> train_set                                
+        );
 };
 
 DECLARE_OBJECT_PTR(RegressionTreeNode);



From nouiz at mail.berlios.de  Fri May 23 21:33:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 May 2008 21:33:09 +0200
Subject: [Plearn-commits] r9050 - trunk/plearn_learners/regressors
Message-ID: <200805231933.m4NJX9Bl020160@sheep.berlios.de>

Author: nouiz
Date: 2008-05-23 21:33:08 +0200 (Fri, 23 May 2008)
New Revision: 9050

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
added missing include in last commit


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-05-23 19:29:53 UTC (rev 9049)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-05-23 19:33:08 UTC (rev 9050)
@@ -46,6 +46,7 @@
 #include <plearn/base/PP.h>
 #include <plearn/math/TVec.h>
 #include <boost/tuple/tuple.hpp>
+#include "RegressionTreeRegisters.h"
 
 namespace PLearn {
 using namespace std;



From nouiz at mail.berlios.de  Fri May 23 22:17:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 May 2008 22:17:00 +0200
Subject: [Plearn-commits] r9051 - in trunk: plearn/base
	plearn_learners/regressors
Message-ID: <200805232017.m4NKH0fN024324@sheep.berlios.de>

Author: nouiz
Date: 2008-05-23 22:16:59 +0200 (Fri, 23 May 2008)
New Revision: 9051

Modified:
   trunk/plearn/base/Option.h
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
Log:
First implementation of declareStaticOption.

This is made with Pascal Vincent is should be considered bugged. I needed it to save memory and reload old saved data. But I don't need all functionality, so not all is tested. Use at your own risk.


Modified: trunk/plearn/base/Option.h
===================================================================
--- trunk/plearn/base/Option.h	2008-05-23 19:33:08 UTC (rev 9050)
+++ trunk/plearn/base/Option.h	2008-05-23 20:16:59 UTC (rev 9051)
@@ -179,7 +179,116 @@
     }
 };
 
+//! Template class for static option definitions
+//! This is not thread safe while loading or saving!
+//! If you have some data in memory then load some other,
+//!   the static value will be overwrited!
+//! This will be saved and loaded for each instance,
+//!   but will override the station version each time
+template<class OptionType>
+class StaticOption : public OptionBase
+{
+    typedef OptionBase inherited;
+  
+protected:
+    OptionType* ptr;
 
+public:
+
+    //! Most of these parameters only serve to provide the user 
+    //! with an informative help text.
+    StaticOption(const string& optionname, OptionType* member_ptr, 
+           flag_t flags, const string& optiontype, const string& defaultval,
+           const string& description, const OptionLevel& level)
+        : inherited(optionname, flags, optiontype, defaultval, description, level),
+          ptr(member_ptr)
+    { }
+
+    virtual void read(Object* o, PStream& in) const
+    { in >> *ptr; }
+
+    virtual void write(const Object* o, PStream& out) const
+    { out << *ptr; }
+
+    virtual Object* getAsObject(Object* o) const
+    { return toObjectPtr(*ptr); }
+
+    virtual const Object* getAsObject(const Object* o) const
+    { return toObjectPtr(*ptr); }
+
+    virtual Object *getIndexedObject(Object *o, int i) const
+    { return toIndexedObjectPtr(*ptr, i); };
+
+    virtual const Object* getIndexedObject(const Object *o, int i) const
+    { return toIndexedObjectPtr(*ptr, i); };
+
+    virtual void* getAsVoidPtr(Object* o) const
+    { return ptr; }
+    
+    virtual const void* getAsVoidPtr(const Object* o) const
+    { return ptr; }
+
+
+    //!@todo check that this is correct
+#ifdef PL_PYTHON_VERSION 
+    virtual PythonObjectWrapper getAsPythonObject(Object* o) const 
+    { 
+        return PythonObjectWrapper(*(OptionType*)getAsVoidPtr(o)); 
+    }
+
+    virtual PythonObjectWrapper getAsPythonObject(const Object* o) const 
+    { 
+        return PythonObjectWrapper(*(OptionType*)getAsVoidPtr(o)); 
+    }
+
+    virtual void setFromPythonObject(Object* o, const PythonObjectWrapper& v) const
+    {
+        *ptr=
+            ConvertFromPyObject<OptionType>::convert(v.getPyObject(), true);
+    }
+
+#endif //def PL_PYTHON_VERSION 
+
+    virtual string optionHolderClassName(const Object* o) const
+    { return "static"; }
+
+    virtual int diff(const string& refer, const string& other,
+                     PLearnDiff* diffs) const
+    {
+        /*
+        pout << "Calling Option<" << TypeTraits<ObjectType>::name()
+            << "," << TypeTraits<OptionType>::name() << ">::diff" << endl;
+        */
+        // return PLearn::diff(refer, other, this, diffs);       
+        //return DiffTemplate<ObjectType, OptionType>::diff(refer, other,
+        //                                                  this, diffs);
+        // @todo: quick hack, fix this (maybe? Xavier?)
+        return 0;
+    }
+
+    //! Implementation of isAccessibleAsObject() relies on caching since
+    //! the first call may need to default-construct an object; relatively slow.
+    virtual bool isAccessibleAsObject() const
+    {
+        static bool accessible = isConvertibleToObjectPtr(OptionType());
+        return accessible;
+    }
+
+    virtual int indexableSize(const Object* o) const
+    {
+        return indexableObjectSize(*ptr);
+    }
+
+//!@todo check that this is correct that we don't have it
+    //! Accessor to the member pointer wrapped by the option
+    /*
+    OptionType ObjectType::* getPtr() const
+    {
+        return ptr;
+    }
+    */
+};
+
 //#####  TVec-Specific Option  ################################################
 
 // This is a special version of Option designed for TVec<T>.
@@ -242,6 +351,7 @@
                                                     defaultval, description, level));
 }
 
+
 // Overload for simple pointers
 template <class ObjectType, class OptionType>
 inline void declareOption(OptionList& ol,
@@ -257,6 +367,24 @@
                                                       defaultval, description, level));
 }
 
+// Overload for pointer to static member
+template <class OptionType>
+inline void declareStaticOption(OptionList& ol,                      //!< list to which this option should be appended 
+                          const string& optionname,            //!< the name of this option
+                          OptionType* ptr,                     //!< &YourClass::your_static_field
+                          OptionBase::flag_t flags,            //!< see the flags in OptionBase
+                          const string& description,           //!< a description of the option
+                          const OptionBase::OptionLevel level= OptionBase::default_level, //!< Option level (see OptionBase)
+                          const string& defaultval="")         //!< default value for this option, as set by the default constructor
+{
+    perr<<"declareStaticOption() is not considered debuged, use at you own risk"<<endl;
+    ol.push_back(new StaticOption<OptionType>(optionname, ptr, flags, 
+                                                    TypeTraits<OptionType>::name(), 
+                                                    defaultval, description, level));
+}
+
+
+
 // Overload for TVec<T>
 template <class ObjectType, class VecElementType>
 inline void declareOption(OptionList& ol,

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-05-23 19:33:08 UTC (rev 9050)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-05-23 20:16:59 UTC (rev 9051)
@@ -51,11 +51,12 @@
                         "of the samples in the leave.\n"
     );
 
+int RegressionTreeLeave::verbosity = 0;
+
 RegressionTreeLeave::RegressionTreeLeave():
     id(-1),
     missing_leave(false),
     loss_function_weight(0),
-    verbosity(0),
     length(0),
     weights_sum(0),
     targets_sum(0),
@@ -78,7 +79,7 @@
                   "The indicator that it is a leave with missing values for the split feature\n");
     declareOption(ol, "loss_function_weight", &RegressionTreeLeave::loss_function_weight, OptionBase::buildoption,
                   "The hyper parameter to balance the error and the confidence factor\n");
-    declareOption(ol, "verbosity", &RegressionTreeLeave::verbosity, OptionBase::buildoption,
+    declareStaticOption(ol, "verbosity", &RegressionTreeLeave::verbosity, OptionBase::buildoption,
                   "The desired level of verbosity\n");
     declareOption(ol, "train_set", &RegressionTreeLeave::train_set, OptionBase::buildoption,
                   "The train set with the sorted row index matrix and the leave id vector\n");

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-05-23 19:33:08 UTC (rev 9050)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-05-23 20:16:59 UTC (rev 9051)
@@ -65,7 +65,7 @@
     int  id;
     bool  missing_leave;
     real loss_function_weight;
-    int  verbosity;
+    static int  verbosity;
     PP<RegressionTreeRegisters> train_set;
  
 /*



From tihocan at mail.berlios.de  Fri May 23 22:57:33 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 23 May 2008 22:57:33 +0200
Subject: [Plearn-commits] r9052 - trunk/plearn/vmat
Message-ID: <200805232057.m4NKvXW4028472@sheep.berlios.de>

Author: tihocan
Date: 2008-05-23 22:57:32 +0200 (Fri, 23 May 2008)
New Revision: 9052

Modified:
   trunk/plearn/vmat/ToBagSplitter.cc
   trunk/plearn/vmat/ToBagSplitter.h
Log:
Can now provide the target to the underlying splitter

Modified: trunk/plearn/vmat/ToBagSplitter.cc
===================================================================
--- trunk/plearn/vmat/ToBagSplitter.cc	2008-05-23 20:16:59 UTC (rev 9051)
+++ trunk/plearn/vmat/ToBagSplitter.cc	2008-05-23 20:57:32 UTC (rev 9052)
@@ -54,6 +54,7 @@
 ///////////////////
 ToBagSplitter::ToBagSplitter():
     expected_size_of_bag(10),
+    provide_target(false),
     remove_bag(false)
 {}
 
@@ -81,6 +82,12 @@
                   &ToBagSplitter::remove_bag, OptionBase::buildoption,
         "If true, then the bag column will be removed from the data splits.");
 
+    declareOption(ol, "provide_target",
+                  &ToBagSplitter::provide_target, OptionBase::buildoption,
+        "If true, then the target (without the bag info) of a bag will be\n"
+        "provided to the underlying splitter. This target is obtained from\n"
+        "the first sample in each bag.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -132,7 +139,31 @@
         }
         // Resize to the minimum size needed.
         bags_store.resize(num_bag, max_ninstances + 1, 0, true);
+        int bags_store_is = max_ninstances + 1;
+        int bags_store_ts = 0;
+        if (provide_target) {
+            if (dataset->targetsize() <= 1)
+                PLWARNING("In ToBagSplitter::build_ - 'provide_target' is true,"
+                        " but the dataset does not seem to have any target "
+                        "besides the bag information: no target provided to "
+                        "the underlying splitter");
+            else {
+                bags_store_ts = dataset->targetsize() - 1;
+                bags_store.resize(bags_store.length(),
+                                  bags_store.width() + bags_store_ts,
+                                  0, true);
+                Vec input, target;
+                real weight;
+                for (int i = 0; i < bags_store.length(); i++) {
+                    dataset->getExample(int(round(bags_store(i, 1))),
+                                        input, target, weight);
+                    bags_store(i).subVec(bags_store_is, bags_store_ts) <<
+                        target.subVec(0, bags_store_ts);
+                }
+            }
+        }
         bags_index = VMat(bags_store);
+        bags_index->defineSizes(bags_store_is, bags_store_ts, 0);
         //bags_index->savePMAT("HOME:tmp/bid.pmat");
         // Provide this index to the sub_splitter.
         sub_splitter->setDataSet(bags_index);

Modified: trunk/plearn/vmat/ToBagSplitter.h
===================================================================
--- trunk/plearn/vmat/ToBagSplitter.h	2008-05-23 20:16:59 UTC (rev 9051)
+++ trunk/plearn/vmat/ToBagSplitter.h	2008-05-23 20:57:32 UTC (rev 9052)
@@ -62,9 +62,6 @@
     // * protected options *
     // *********************
 
-    // ### declare protected option fields (such as learnt parameters) here
-    // ...
-
     // Fields below are not options.
 
     //! Used to store the list of all bags, with the indices of the corresponding
@@ -78,6 +75,7 @@
     // ************************
 
     int expected_size_of_bag;
+    bool provide_target;
     bool remove_bag;
     PP<Splitter> sub_splitter;
 



From plearner at mail.berlios.de  Sun May 25 02:12:17 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sun, 25 May 2008 02:12:17 +0200
Subject: [Plearn-commits] r9053 - trunk/python_modules/plearn/plotting
Message-ID: <200805250012.m4P0CHXL020720@sheep.berlios.de>

Author: plearner
Date: 2008-05-25 02:12:16 +0200 (Sun, 25 May 2008)
New Revision: 9053

Modified:
   trunk/python_modules/plearn/plotting/netplot.py
Log:
Added non-interactive plotRowsAsImages
and interactive showRowsAsImages



Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2008-05-23 20:57:32 UTC (rev 9052)
+++ trunk/python_modules/plearn/plotting/netplot.py	2008-05-25 00:12:16 UTC (rev 9053)
@@ -52,7 +52,7 @@
     return "%.2e" % float
 
 
-def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.02, apply_to_rows = None, index_to_plot = [], names = [], same_scale = False):
+def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.02, apply_to_rows = None, index_to_plot = [], names = [], same_scale = False, colormap = defaultColorMap):
 
     
     #some calculations for plotting
@@ -68,8 +68,6 @@
     plotHeight = mWidth/width/width*plotWidth
     cbw = .01 # color bar width
 
-    colorMap = defaultColorMap
-
     if same_scale:
         mi,ma = findMinMax(M)
         print 'min', mi
@@ -100,9 +98,9 @@
         
         axes((x, y, plotWidth, plotHeight))
         if same_scale:
-            imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap, vmin = mi, vmax = ma)
+            imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colormap, vmin = mi, vmax = ma)
         else:
-            imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap)#, vmin = mi, vmax = ma)
+            imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colormap)#, vmin = mi, vmax = ma)
             
         if names == []:
             setPlotParams('row_' + str(i), 1-same_scale, True)
@@ -125,8 +123,61 @@
     return toReturn
 
 
+def plotRowsAsImages(X, figtitle="", nrows=10, ncols=20, img_width=None, show_colorbar=False, disable_ticks=False, colormap = cm.gray, luminance_scale_mode = 0, vmin=None, vmax=None):
+    """
+    If provided, vmin and vmax will be used for luminance scale (see imshow)
+    If not povided, they will be set depending on luminance_scale_mode:
+       0: vmin and vmax are left None, i.e. luminance
+          will be scaled independently for each image
+       1: vmin and vmax will be set to min,max of X
+       2: vmin and vmax will be set to +-min of X
+          or +-max of X (whichever is bigger).
+    """
+    #some calculations for plotting
 
+    img_size = len(X[0])
+    if img_width is None:
+        img_width = math.sqrt(img_size)
+    img_height = img_size/img_width
 
+    if vmin is None and luminance_scale_mode!=0:
+        print 'luminanca_scale_mode = ',luminance_scale_mode
+        vmin = X.min()
+        vmax = X.max()
+        print 'filter value range: ',vmin,',',vmax
+        if luminance_scale_mode==2:
+            vmax = max(abs(vmin),abs(vmax))
+            vmin = -vmax
+        print 'used luminance scale: ',vmin,',',vmax            
+
+    #THE plotting
+
+    subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9,
+                    wspace=0.01, hspace=0.01)
+    
+    for i in range(min(len(X),nrows*ncols)):
+        row = X[i]
+        subplot(nrows,ncols,i+1)
+        img = reshape(row,(img_height,img_width))
+        imshow(img, interpolation="nearest", cmap = colormap, vmin = vmin, vmax = vmax)
+            
+        # if show_colorbar and vmin is None:
+        #    colorbar()
+        if disable_ticks:
+            xticks([],[])
+            yticks([],[])
+
+    if figtitle!="":
+        figtext(0.5, 0.95, figtitle,
+                horizontalalignment='center',
+                verticalalignment='bottom')
+
+    if show_colorbar and vmin is not None:
+        cbw = .01 # color bar width
+        customColorBar(vmin,vmax,color_map=colormap)
+
+
+
 def plotMatrices(matrices, names = None, ticks = False, same_color_bar = False, space_between_matrices = 5, horizontal=True):
     '''plot matrices from left to right
     TODO : same_color_bar does nothing !!
@@ -372,5 +423,117 @@
     return truncMat
 
       
+class showRowsAsImages:
+
+    def __init__(self, X, figtitle="",
+                 nrows = 10, ncols = 20,
+                 startidx = 0,
+                 img_width=None,
+                 luminance_scale_mode=0,
+                 colormaps = [cm.gray, cm.jet],
+                 vmin = None, vmax = None):
+
+        self.X = X
+        self.figtitle = figtitle
+        self.nrows = nrows
+        self.ncols = ncols
+        self.startidx = startidx
+        self.img_width = img_width
+
+        # appearance control
+        self.luminance_scale_mode = luminance_scale_mode
+        self.interpolation = 'nearest'
+        self.colormaps = colormaps
+        self.cmapchoice = 0
+        self.show_colorbar = True
+        self.disable_ticks = True
+        self.vmin = vmin
+        self.vmax = vmax
+
+        # plot it
+        self.draw()      
+        connect('key_press_event', self.keyPressed)
+        # connect('button_press_event', self.__clicked)
+
+        # start interactive loop
+        show()
+
+    def draw(self):
+        print "Start plotting..."
+        clf()
+        endidx = min(self.startidx+self.nrows*self.ncols, len(self.X))
+        title = self.figtitle+" ("+str(self.startidx)+" ... "+str(endidx-1)+")"
+        plotRowsAsImages(self.X[self.startidx : endidx],
+                         figtitle = title,
+                         nrows = self.nrows,
+                         ncols = self.ncols,
+                         img_width=self.img_width,
+                         luminance_scale_mode = self.luminance_scale_mode,
+                         show_colorbar = self.show_colorbar,
+                         disable_ticks = self.disable_ticks,
+                         colormap = self.colormaps[self.cmapchoice],
+                         vmin = self.vmin,
+                         vmax = self.vmax
+                         )
         
-    
+        print "Plotted,"
+        draw()
+        print "Drawn."
+                   
+
+    def plotNext(self):
+        self.startidx += self.nrows*self.ncols
+        if self.startidx >= len(self.X):
+            self.startidx = 0
+        self.draw()
+
+    def plotPrev(self):
+        if self.startidx>0:
+            self.startidx -= self.nrows*self.ncols
+        else:
+            self.startidx = len(self.X)-self.nrows*self.ncols
+        if self.startidx<0:
+            self.startidx = 0            
+        self.draw()
+
+    def keyPressed(self, event):
+        char = event.key
+        print 'Pressed',char
+        if char == 'c':
+            self.changeColorMap()
+        elif char == 'right':
+            self.plotNext()
+        elif char == 'left':
+            self.plotPrev()
+        elif char == 'b':
+            self.show_colorbar = not self.show_colorbar
+            self.draw()
+        elif char == 't':
+            self.disable_ticks = not self.disable_ticks
+            self.draw()
+        elif char == 's':
+            self.luminance_scale_mode = (self.luminance_scale_mode+1)%3
+            self.draw()
+        elif char == '':
+            pass
+        else:
+            print """
+            *******************************************************
+            * KEYS
+            *  right : show next filters
+            *  left  : show previous filters
+            *  c     : change colormap
+            *  s     : cycle through luminance scale mode
+            *          0 independent luminance scaling for each
+            *          1 min-max luminance scaling across display
+            *          2 +-min or +- max (largest range)
+            *  b     : toggle showing colorbar 
+            *  t     : toggle showing ticks
+            *
+            * Close window to stop.
+            *******************************************************
+            """
+
+    def changeColorMap(self):
+        self.cmapchoice = (self.cmapchoice+1)%len(self.colormaps)
+        self.draw()



From tihocan at mail.berlios.de  Mon May 26 16:11:28 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 26 May 2008 16:11:28 +0200
Subject: [Plearn-commits] r9054 - in
	trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results:
	expdir-tester/Split0 expdir-tester2 expdir-tester2/Split0
Message-ID: <200805261411.m4QEBSRA007493@sheep.berlios.de>

Author: tihocan
Date: 2008-05-26 16:11:27 +0200 (Mon, 26 May 2008)
New Revision: 9054

Modified:
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat
Log:
Fixed test PL_ModuleLearner_TwoRBMs

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave	2008-05-25 00:12:16 UTC (rev 9053)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester/Split0/final_learner.psave	2008-05-26 14:11:27 UTC (rev 9054)
@@ -2,14 +2,17 @@
 module = *2 ->NetworkModule(
 modules = 8 [ *3 ->RBMModule(
 visible_layer = *4 ->RBMBinomialLayer(
+use_signed_samples = 0 ;
 size = 5 ;
 learning_rate = 0.00100000000000000002 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 5 [ -0.000346543794653077165 -0.00181343646441971262 -0.00256344846238087606 0.00123072211653035054 0.0105796540684799031 ] ;
+bias = 5 [ -0.000346543794653076677 -0.00181343646441971349 -0.0025634484623808752 0.00123072211653034989 0.0105796540684799031 ] ;
 input_size = 5 ;
 output_size = 5 ;
 name = "RBMBinomialLayer" ;
@@ -19,43 +22,54 @@
 random_gen = *5 ->PRandom(
 seed = 1827 ;
 fixed_seed = 0  )
- )
 ;
+verbosity = 1  )
+;
 hidden_layer = *6 ->RBMBinomialLayer(
+use_signed_samples = 0 ;
 size = 10 ;
 learning_rate = 0.00100000000000000002 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 10 [ -0.00106440430474535582 0.000369301683813727641 0.000513350554420346637 -0.000709235433622423567 -0.00081825368190638749 -0.000936944443114255785 5.21038279847706566e-05 -0.00060723211325813993 -0.00012022306301326933 0.000849342609018322535 ] ;
+bias = 10 [ -0.00106440430474535603 0.000369301683813728617 0.000513350554420345662 -0.000709235433622423892 -0.000818253681906388358 -0.000936944443114256218 5.21038279847711987e-05 -0.000607232113258140364 -0.000120223063013269628 0.000849342609018324379 ] ;
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMBinomialLayer" ;
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 connection = *7 ->RBMMatrixConnection(
 weights = 10  5  [ 
--0.25033029216361119 	0.196297332269796926 	0.291863216743886378 	-0.271177564771000135 	-0.256443283847534731 	
-0.000750454165465756448 	-0.153051576826741781 	-0.0251140371746953056 	0.0951948005469039904 	0.101690890357688787 	
-0.264014387667070805 	0.296640484199731047 	-0.0733552745166331926 	0.234544519515334449 	0.20878469797271762 	
--0.119055970902863212 	-0.246763413010821359 	0.140945917211564492 	0.0199301834550586615 	-0.272554977700818657 	
-0.0704522821956235917 	0.0459836760937902683 	0.112858621879697182 	-0.289343817002414683 	-0.226235693833594709 	
--0.0364476443667945474 	0.145989242583078827 	0.234450892747573192 	-0.0821300666227130632 	-0.25953723478216989 	
-0.0719990749894757337 	0.0767519526074037367 	-0.255160105938427029 	0.0929396172618830452 	-0.0337076755676481918 	
-0.0743735670898155177 	-0.22811485856468966 	0.0918907401537037843 	0.180020360495644433 	-0.258920436145750887 	
-0.0447145136911352367 	0.112516270735609575 	0.170636125028530872 	-0.308567573762140279 	0.0539703490405033862 	
--0.229320061680709814 	-0.015335473409631175 	-0.178325969033065856 	-0.158212845296357901 	0.300210567187072896 	
+-0.25033029216361119 	0.196297332269796954 	0.291863216743886433 	-0.271177564771000079 	-0.256443283847534731 	
+0.000750454165465730644 	-0.153051576826741781 	-0.0251140371746952848 	0.0951948005469039904 	0.101690890357688773 	
+0.26401438766707086 	0.296640484199731103 	-0.0733552745166331788 	0.234544519515334449 	0.208784697972717592 	
+-0.119055970902863212 	-0.246763413010821414 	0.14094591721156452 	0.0199301834550586338 	-0.272554977700818712 	
+0.0704522821956235917 	0.0459836760937902475 	0.112858621879697155 	-0.289343817002414683 	-0.226235693833594737 	
+-0.0364476443667945613 	0.145989242583078827 	0.23445089274757322 	-0.0821300666227130494 	-0.25953723478216989 	
+0.0719990749894757615 	0.0767519526074036812 	-0.255160105938427084 	0.0929396172618830313 	-0.0337076755676482126 	
+0.0743735670898155177 	-0.22811485856468966 	0.0918907401537038121 	0.180020360495644377 	-0.258920436145750887 	
+0.0447145136911352367 	0.112516270735609575 	0.170636125028530816 	-0.308567573762140168 	0.0539703490405033931 	
+-0.229320061680709814 	-0.0153354734096311784 	-0.178325969033065856 	-0.158212845296357874 	0.300210567187072896 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 5 ;
 up_size = 10 ;
 learning_rate = 0.00100000000000000002 ;
@@ -67,12 +81,16 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 reconstruction_connection = *0 ;
+stochastic_reconstruction = 0 ;
 grad_learning_rate = 0.0100000000000000002 ;
 cd_learning_rate = 0.00100000000000000002 ;
+tied_connection_weights = 0 ;
 compute_contrastive_divergence = 0 ;
+deterministic_reconstruction_in_cd = 0 ;
 standard_cd_grad = 1 ;
 standard_cd_bias_grad = 1 ;
 standard_cd_weights_grad = 1 ;
@@ -90,150 +108,165 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *8 ->RBMModule(
 visible_layer = *9 ->RBMBinomialLayer(
+use_signed_samples = 0 ;
 size = 10 ;
 learning_rate = 0.00100000000000000002 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 10 [ -0.00497296088693641256 -0.00960776823318316812 0.00300914972033978307 -0.010843134408859461 -0.0184786313956434023 -0.00746522161667526832 0.000908085308330407095 -0.00808384343338955022 -0.000599485986346771807 0.000143083459286945185 ] ;
+bias = 10 [ -0.0049729608869364117 -0.00960776823318316812 0.00300914972033978437 -0.010843134408859461 -0.0184786313956434058 -0.00746522161667526919 0.000908085308330407095 -0.00808384343338955196 -0.00059948598634676953 0.000143083459286945402 ] ;
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMBinomialLayer" ;
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 hidden_layer = *10 ->RBMBinomialLayer(
+use_signed_samples = 0 ;
 size = 100 ;
 learning_rate = 0.00100000000000000002 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ 0.000597156499075335366 -0.000557823606602718511 -0.000238253337450569153 -0.000235879342121554105 -8.69444764492487197e-05 -0.00022858411903278514 -0.000730159306076523691 -6.1310862865765039e-05 -0.000338108827126749683 -0.000106820660282853225 -0.000240598232693696955 0.000178999751807968119 0.000147132194605513656 -0.000441800759242361532 -0.000802922511599219986 1.2260192346137149e-05 8.16029189217909819e-07 -0.000883220981615029919 -0.000311479120957375928 -1.5260112041313645e-05 -0.000757912731700950931 -6.9395901164534822e-05 -0.000636567710498314146 -5.75710033384960597e-05 -0.0010289751399738911 -0.000961161353018534308 0.000141822862868206463 -0.000403791916958990045 1.02762718962510986e-05 0.000341415377768926841 8.14495862766646253e-05 0.000278342135779378137 0.000494169695295036747 -0.000265473688031311785 0.000154029955903179039 -4.41655417330052039e-05 0.000628093550740835673 -0.00037777589507353884 -0.000296093156850220189 -0.000160696743342817!
 687 -0.000102863595841186077 0.000173761070511071596 -0.000245392814311622891 -0.000107863608452741852 7.33017429951104049e-05 -0.000308803079148418631 -0.000339285603320606949 0.000367751727385619656 0.000184930165812790398 -0.000346475970163319857 -4.0110584105177597e-05 -0.000585039447767461869 -0.000291850089263570893 -0.000299463125227201956 -0.000417868516658002247 0.000480876115391925055 4.64757193019625954e-05 -0.000586106684639309304 0.000257814955044925449 -0.000259232385338512429 -0.000648200046445872573 -0.00026824023145179815 -0.00029401570728265498 0.000353914946215985271 -0.00051662926462639632 0.000329196416397329022 -0.000264261909695937512 -0.000414322501555562227 -0.000574572372497456573 -0.000307518676965848022 -0.000277021766343235902 -0.000685439464607044279 0.000318077902563635047 -1.406624648959265e-05 0.000362849808789448549 6.2624018284797791e-05 0.000728078154506861574 0.00022795360397397457 -5.87026037444542326e-05 -0.000458977441061176196 -0.000!
 160753719307193829 -0.000441783646407283015 0.0007941407743608!
 73529 -0
.000122821327054280883 -0.000799745805908814002 0.000178776017105758064 -0.000329561939590614268 -0.000110755124289750007 0.000417785418861984646 -0.000317521079520178173 -0.000280339867216013899 -0.000381508040821792148 0.000144435280777569891 -0.00075784843857775808 -0.000557582505652309021 -0.000367294182775605195 -0.000128098503839851695 0.000331263477375859752 0.000277292872525089561 -0.000105039647617090483 ] ;
+bias = 100 [ 0.00059715649907533645 -0.000557823606602719162 -0.000238253337450568828 -0.000235879342121553726 -8.69444764492498311e-05 -0.000228584119032785817 -0.0007301593060765238 -6.13108628657647273e-05 -0.000338108827126748707 -0.000106820660282853116 -0.000240598232693696684 0.000178999751807967469 0.000147132194605513656 -0.000441800759242360827 -0.000802922511599219336 1.22601923461366052e-05 8.16029189217585193e-07 -0.000883220981615030244 -0.000311479120957376199 -1.52601120413135637e-05 -0.000757912731700950497 -6.93959011645352828e-05 -0.000636567710498314797 -5.75710033384960597e-05 -0.00102897513997389153 -0.000961161353018534633 0.000141822862868206734 -0.000403791916958990533 1.02762718962517508e-05 0.000341415377768928088 8.14495862766652216e-05 0.000278342135779377595 0.000494169695295034795 -0.000265473688031311677 0.000154029955903178822 -4.41655417330055292e-05 0.000628093550740835022 -0.000377775895073538624 -0.000296093156850219864 -0.00016069674334!
 2817687 -0.00010286359584118586 0.000173761070511071759 -0.000245392814311622511 -0.000107863608452740889 7.33017429951095376e-05 -0.000308803079148418686 -0.000339285603320607979 0.000367751727385620198 0.000184930165812790506 -0.000346475970163319694 -4.01105841051775089e-05 -0.000585039447767461869 -0.000291850089263570513 -0.000299463125227202227 -0.000417868516658002464 0.000480876115391925488 4.64757193019624937e-05 -0.000586106684639309088 0.000257814955044925558 -0.000259232385338513242 -0.000648200046445872682 -0.000268240231451798204 -0.000294015707282654817 0.000353914946215984079 -0.000516629264626396211 0.000329196416397328697 -0.000264261909695937675 -0.000414322501555563095 -0.000574572372497457223 -0.000307518676965847968 -0.00027702176634323639 -0.00068543946460704417 0.000318077902563635698 -1.40662464895917691e-05 0.000362849808789448983 6.26240182847976826e-05 0.000728078154506861574 0.000227953603973973595 -5.87026037444540632e-05 -0.0004589774410611769!
 54 -0.000160753719307193938 -0.000441783646407283449 0.0007941!
 40774360
873312 -0.000122821327054280531 -0.000799745805908813677 0.000178776017105758498 -0.000329561939590613401 -0.00011075512428975044 0.000417785418861985405 -0.000317521079520178282 -0.000280339867216014929 -0.000381508040821791335 0.000144435280777569376 -0.000757848438577757538 -0.000557582505652309346 -0.000367294182775605303 -0.000128098503839851668 0.000331263477375859589 0.000277292872525090536 -0.000105039647617090023 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMBinomialLayer" ;
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 connection = *11 ->RBMMatrixConnection(
 weights = 100  10  [ 
--0.0978471800215257037 	-0.0902053404604457182 	0.0869754571825244738 	0.04089118804206434 	-0.101368086228391774 	-0.0465450149174224462 	-0.0747286765670649306 	0.0781538958897185221 	-0.0910596663974124754 	-0.027060705005204036 	
--0.0252235703285304709 	0.0354286612789488087 	0.0865639653081493043 	0.0614650337319190138 	0.0504116863076464333 	0.0453555903578909195 	-0.0713761496668450651 	-0.00510598328215907966 	-0.0401508455479649556 	-0.0381547596057954103 	
--0.012201564296345337 	0.0420692804838235443 	-0.0589592119877042353 	0.0271329090054837352 	-0.0246631886592046899 	-0.0536774953404823632 	-0.0373242762756519977 	0.0839985306538834353 	-0.075511151135593188 	-0.0137996039460813787 	
-0.00368161158062422014 	-0.0501473051925635516 	0.0673034534554419889 	0.090145308971782856 	0.000450396820622549448 	0.0129954022032453178 	-0.051842324345002036 	0.0186563341973783187 	-0.0120690257360572071 	-0.0285276541865736519 	
--0.0129379800125673291 	-0.103980705380140676 	-0.0539147279523639888 	0.0479517446087257757 	0.002094493937602701 	-0.046696977104081952 	-0.0802431914099717286 	0.0735409085950746444 	0.0620187184289074656 	-0.0632002931674442298 	
--0.0716343255372051541 	0.0590042959001069733 	0.0072827631311536559 	-0.0834759753498713064 	0.0867722030742086037 	0.0385226901828749912 	-0.0652884327719597674 	-0.0824271906213171618 	0.0309700920772651468 	-0.0129277639998376884 	
-0.0318868724906055737 	0.0650593210786601789 	-0.0245714586022686143 	0.0433299122052953969 	0.0201364126839004114 	0.0425738749199970068 	-0.0526247915666996774 	0.0675331131708467175 	-0.0477044071959432589 	-0.025598622557758275 	
--0.0640216405987112103 	-0.0299635163224174494 	0.0952592499384477542 	0.0668103062730507208 	0.0166086112657421181 	0.0664500679329425165 	0.0227828956923666094 	-0.0893090662942366553 	0.0829269017431499555 	0.06856507275119339 	
--0.0550278698599023092 	0.0770251765830581442 	0.0500563329594138628 	0.0550146273205624689 	-0.0516753580715226812 	0.0537945041440263463 	-0.0693359883892173123 	0.0729218325706957604 	0.0799929000213076835 	0.0960063223401683014 	
--0.0707554591003550087 	0.0261111657093878249 	0.0941372243888994598 	0.0631654536171545877 	0.0170878181536895449 	-0.0229053753818019984 	-0.0250195065968273275 	-0.0434434186710700304 	-0.0244383203596413717 	-0.0307728326241952094 	
--0.0282726977772407816 	0.0692922496347341127 	0.0220729968096950724 	-0.0973701172734391524 	0.0479943686715152487 	-0.0518425289509872947 	-0.0658329478901666376 	0.0904534376062660767 	-0.0967483311631070902 	0.0274779032809275119 	
--0.0749180771549134289 	-0.0556314468690856934 	0.0546280338170591212 	-0.015759322709810647 	0.0708181780326413574 	-0.0517166526253960424 	0.0557141215974588169 	-0.0797537802331814011 	-0.00352613999712846592 	0.0599144738690658776 	
--0.0528670085289950284 	-0.038762170620892461 	0.0293976134619478198 	0.0433346791942989459 	-0.0382805807924307892 	0.0116297052156124018 	0.0375198279873637033 	-0.00532168376490164442 	0.0736722032536451937 	-0.00607366033620099048 	
-0.0391095640230744584 	0.0712773667374083791 	-0.058777370369054463 	0.0312712307707591133 	0.067242349443067187 	-0.0377084412744321251 	0.0971308441427504443 	-0.103652307129281654 	0.0209696696634045464 	0.00253509251293177742 	
--0.0171588364027313658 	0.0907601551232998655 	-0.0981836322692552915 	0.0457181217897632761 	0.0694491906621885757 	-0.100096807598169274 	0.0288349067131420563 	0.0939563190155330713 	0.0580861665086693096 	-0.0668678678101209123 	
-0.0412420631610641505 	-0.0510501257610846207 	0.014572665601196402 	-0.016143936728842706 	0.02900140314584242 	-0.0332714076777847964 	-0.0851973071178984964 	-0.0336245111848018155 	0.0868618018288649374 	0.0498116353297165179 	
-0.0608481024886332392 	-0.0569116799989186073 	0.068400372248770297 	-0.0571831431037030449 	0.0872489540589142798 	-0.0230842286627061591 	0.0455652187818045007 	-0.0755394545215909846 	-0.073978778720638308 	-0.00589058283666940574 	
-0.0587743374572413171 	0.00789329314258944113 	-0.0893465210301869345 	0.015147978699547579 	0.0641597853200572971 	0.0876528070286558963 	0.0481404816619878523 	0.0714061221447997718 	0.085947483446818268 	-0.0319564492764216473 	
-0.00821089371880088723 	-0.0555051105100034595 	-0.0280933946238387765 	-0.0453126509210930103 	0.0843524901379098524 	0.0458737090022064212 	0.0176972451932225529 	-0.0136543532179953086 	0.0964521208966484661 	0.0684559826177911362 	
--0.0280958870094338217 	-0.0605968595383250125 	-0.0536939830278911484 	0.0185050456732452039 	0.0297323388736373574 	0.0229160670400564009 	0.0570508108183647791 	-0.0630618711032856771 	-0.0793649365440222132 	0.0461738074819058181 	
-0.0728351420887712597 	0.0666410235313574378 	0.0494792264267038748 	0.0496763266619888941 	0.0646676527061976619 	0.0793576424332215064 	0.0114131708236924114 	-0.0581806821111141154 	0.0504859418107055993 	-0.00911237013743403448 	
-0.0967952710302167896 	-0.0718511758900344244 	-0.0610162385239595706 	-0.0728244178184092733 	0.0449532172677859013 	0.0378468997457577574 	-0.0921341442982845926 	-0.055664063262185752 	0.0563301655537746179 	0.0786459667210254038 	
-0.0164745996068444006 	0.0924700937753852592 	-0.0540731976644756376 	0.0430464584705524192 	0.0755343311208682283 	-0.0499327949708687907 	-0.0724181596807987188 	-0.0582060784221022642 	-0.0158077489368196773 	0.0326834633026971549 	
--0.0017875468964886228 	-0.0869974968769170326 	0.0295976076183521418 	0.0441334710836344332 	-0.000192463262426864287 	-0.0642391132227323886 	-0.0916698367580487811 	0.0932836870941053803 	-0.0107793497686112395 	-0.0728094587693954576 	
-0.0587479219193328406 	0.0821680843214413342 	-0.0152641196778351643 	0.0155875877114616222 	0.0740472414247421301 	0.0854385475675609879 	-0.038305799066720117 	0.0583463179284819458 	0.0801144536650854194 	0.0564738144694165167 	
-0.0940899965797011434 	0.0307091441747545831 	0.0900328705251852435 	0.0785367782152500832 	0.0881328378019818753 	0.0481656782083180884 	0.0626514637218753279 	0.0370131535481861423 	0.0539419100316362587 	-0.08554070466044425 	
-0.0267770875099327746 	-0.0527028069259232596 	-0.0875121894418438712 	-0.102763886213666097 	0.0115735260855198765 	0.0662990428625958583 	-0.0252398580773533915 	-0.0507800685040700056 	0.000888250710291267577 	-0.0614608956463818801 	
--0.0258964815038111966 	0.0188390051509506745 	0.00227796022337114394 	0.0657967815509993559 	0.0466204263681099795 	0.0243726024934155967 	-0.00878145291935440893 	-0.059414483397086576 	-0.0931162159601290201 	-0.0336980703159352746 	
--0.0827055227273208599 	0.0818624414826298469 	-0.0100739312020822012 	-0.0859335293119874305 	0.00327143050697012352 	0.0226369583876831447 	0.0146765130992764989 	-0.0121176597533669203 	0.0967442086187228073 	0.0697890856835364504 	
--0.00745826725616853883 	-0.0964640904546764932 	-0.0795215372867596054 	0.0352150068892425339 	-0.0638309265380326007 	0.0155327281291228799 	-0.00305800644031185799 	-0.0380918216866338449 	0.0272470536843010362 	-0.0419965098332424625 	
--0.0411439402374458724 	-0.0728629894578508991 	0.0552819486421851358 	0.0211570654093570822 	0.0649121319561195009 	-0.0145505857125498 	0.0699884843315146804 	-0.0982756424787766397 	-0.063944643334474735 	0.000620209474960204668 	
-0.00416525307999549444 	0.0166465191526073905 	0.0464601937490181777 	0.00159073011554853932 	-0.0810419547018469133 	0.0587695853496177853 	-0.0752120721188494629 	-0.0650844598149195319 	0.0641881737373511813 	0.0203239899258255211 	
--0.0834353432236865228 	-0.0955822808254698508 	0.0348501126493723015 	-0.0550142504178344066 	-0.0201505792185051914 	0.0409778259330861455 	0.0151601782900929634 	-0.0167503488448184912 	-0.0780623677050969189 	0.0131996537271319124 	
--0.0311875626213257012 	0.00964766922438716965 	0.0208310649693951716 	-0.030166128094916457 	0.0245147953760221637 	0.0800177054946719785 	0.0195435731221534278 	0.0216283567782093039 	-0.0180527829305334592 	0.0929732723292680113 	
-0.0494884033174168639 	-0.054358742361751422 	0.0640360253134918123 	0.0700181231003522836 	-0.0618263622355974471 	0.0602747299634640507 	0.0562135558304485278 	-0.0603367217069216283 	-0.0108509209337982193 	-0.0764004339137397126 	
-0.0848612067087960997 	0.0329109132943089591 	-0.0136977181999342005 	-0.0836780609100953587 	0.0167781460043432348 	-0.0836734220703956033 	-0.0375079134292391966 	0.0358231923109561187 	-0.0535012528723355779 	-0.02581413891750442 	
-0.093872581974977623 	-0.100288405586654278 	-0.0667369635404056472 	-0.0757432695708722681 	-0.0914286777768788944 	0.00506720166870514282 	-0.0569430514126728557 	-0.0219288156913358609 	0.0876542703845367838 	-0.0145189574390498823 	
--0.0818798343322028055 	-0.00366424918696527362 	-0.0944773533676381977 	-0.0261456960698514818 	0.0626537530347011096 	-0.0189298493990359799 	-0.0667215622413877274 	0.0740343450983735191 	-0.0680679552072742111 	0.0830434652894488923 	
-0.075616084261812877 	0.0209163601145687973 	0.00200392697592247353 	-0.0814615918998283783 	0.0194881244677043743 	0.00265130477851547794 	-0.0719831298173050638 	0.0889834087611306446 	0.0031138416969896149 	0.0224144322520954016 	
--0.0896849819534228931 	-0.0507681076651540775 	-0.0289779219096500985 	0.0627111423993589739 	-0.00388153114169776346 	0.050949102919937185 	0.0355886318638906976 	0.0312050420039175977 	-0.0614487171521126557 	-0.0112073945688570542 	
-0.0424085111970063794 	0.0693315307404584608 	-0.0846862732797641765 	-0.095393870981865761 	0.0376548271927241024 	-0.0944778011564852693 	-0.0882780307280190552 	-0.00897148407766692156 	-0.0139322356736678553 	0.0350624563590257315 	
--0.0454355257807062193 	0.0407335288575404603 	0.0180117722199369633 	0.0771493812157636322 	-0.0675758387109726688 	0.0153502240693769044 	0.0738811919590802307 	-0.0956471899756761029 	-0.00114179025482402099 	-0.083127948494937931 	
--0.0334213644483165387 	0.058862185098133811 	0.0769761653587529709 	-0.0848236867748237977 	0.0881689584635699936 	0.0517082924243271319 	0.0686564098239453718 	-0.0589395097959882833 	-0.0957627504743620295 	-0.000411925827903007378 	
-0.0847763359119753507 	-0.0763375138329060315 	0.0577498920380896397 	-0.0402810002700549064 	0.0648664324744550047 	-0.0209504144505155525 	0.0880108710566264996 	0.0105080587462940585 	0.008022103259592012 	0.0992584693355362535 	
-0.0555958892386492418 	-0.0036287394435019593 	0.0143069433727433942 	0.0152863546044747795 	-0.103839765995600436 	0.0624549146255257079 	-0.0731947351204865204 	0.0533670424959666531 	-0.0234650870602571907 	0.0411367046365152672 	
--0.0784377634731182327 	0.043013821241959814 	0.0694041032397587909 	0.00752475691259612504 	0.0642290136577166187 	-0.0907926066714660751 	0.0631589696110833737 	0.0675612503274558018 	0.0271515196662819938 	-0.0680822117213741168 	
-0.0776204240847777893 	0.00551169289000760353 	0.0568811442911565032 	-0.0266963208569938049 	0.0216901896270737318 	0.0753191394404137821 	0.0237798723518230296 	0.00724602707662163215 	0.0679537365137067723 	-0.0376416963609675775 	
-0.0290078560004861583 	0.00526448632767929046 	-0.00998920760775910514 	0.0761213374238426804 	-0.0899273903063939339 	-0.0924918258563098422 	-0.01722691103449785 	-0.0662764783305807709 	0.0656496418263388115 	0.0423025198272805672 	
--0.0428145482360620735 	-0.0504746153192209557 	-0.0783926234610002326 	0.0890118049976672265 	-0.085156531974798208 	-0.0169602694851252872 	-0.0893771321465304802 	0.00203897817814427873 	0.0227081666325220609 	-0.0827621401647491428 	
--0.0202939332038845968 	0.0499651065821050816 	0.022370442965727922 	0.0904591859661133257 	-0.00400335804406958765 	-0.0531147537264334418 	-0.0239984404112206819 	0.023937289048371696 	0.0181681047757719302 	-0.083538633665291781 	
-0.00491340188589175272 	0.0177981967637759995 	-0.0655557033661144095 	0.0138100424275088545 	-0.0685934613762886336 	0.0067489785737811208 	0.0529034439817044205 	0.0615799327121482054 	0.0934024260239453841 	-0.0943407425561808011 	
-0.0768151585785116359 	0.0597422989017513673 	0.0837595244610199324 	0.00872565094808127467 	0.0445126667769655329 	0.0316961713127733163 	0.0887695112490788363 	0.032737584807786449 	-0.00510488375715354228 	-0.0070663215006711794 	
--0.0978956823384336677 	-0.00200908659061244327 	0.0672434044670669057 	0.0899697203124039391 	0.0881844596800903796 	-0.103233207082043713 	-0.052931062714118432 	-0.0413267167620032655 	-0.00972650187846000464 	-0.00671074612941753579 	
--0.0106292336415997925 	-0.0346021147296487752 	-0.0869695336119137991 	-0.0343722972888396566 	0.0648971622010802579 	-0.0607500675535119355 	-0.0357395000343674676 	0.0720958341470771813 	0.00704618764407910578 	-0.0340237311570772138 	
-0.0690819359301237668 	-0.063247316946823659 	0.0779931352755664081 	-0.0153871641512064901 	0.0446621165647628215 	0.0416411351295535928 	-0.0898780644202687579 	0.0942492828334858812 	0.0544579672023831063 	-0.0442268507056601118 	
--0.075344362525791797 	-0.0206650076247724343 	0.100501537574655547 	-0.0651641498538071684 	-0.0875231576506865783 	0.0935193382443201687 	0.0750709168043961761 	0.04720028002728794 	-0.0969269081918623654 	-0.0377364874448853707 	
-0.0524635402955774058 	-0.020256222914507991 	-0.0425486874800179993 	-0.0213338359419469603 	-0.0519005485415260231 	0.0374293880400871221 	0.00548471073340413805 	0.0254655463435365555 	0.0252572388840250385 	0.0917372757377646897 	
--0.0489883465507234758 	0.0808549616726040171 	0.0175690392216576834 	0.0905441074731389295 	0.0597978198485654075 	-0.051931566427641053 	-0.0220680567163850788 	-0.0184359263509704351 	-0.0777786933795720781 	-0.0528490428893146763 	
-0.0197866657554006016 	-0.0623268710318765629 	0.0512216021139901834 	0.00481367540814497692 	-0.0524808786895438056 	-0.0293432470985781277 	-0.0469914955742517237 	0.0422838757133955079 	0.0715508130881882254 	-0.0502512994820883199 	
-0.047162830758310452 	0.0329843168639450332 	0.0839644723490537287 	0.0505133876477681654 	0.0113144966169118639 	-0.0336779467177079767 	0.0400864726177653508 	-0.00875443578863203953 	0.00141773973231703785 	-0.0739639332619120848 	
-0.0724060653433289264 	0.0249891318163628189 	-0.0779804093548025351 	-0.00418599605605514836 	0.0621294941531449138 	-0.0559658941182699793 	-0.0732465016145991549 	0.0784311003474822688 	0.0767134927729501981 	0.0126710224321379319 	
-0.0583069380662354989 	-0.0293102779377430317 	0.0645766828723413205 	-0.0297037758640440359 	0.00373395119793487303 	0.0635226154708643936 	-0.0246390714515497718 	0.0857587717904586821 	-0.0071112261976321537 	-0.0126705462308569199 	
--0.0638861182897556973 	0.0606482406746605071 	0.0475696350655952713 	0.0733154834629952262 	-0.0469453033611485562 	0.0790303515559192815 	0.0883217850098304696 	0.0327253475504564739 	0.0586621296998571656 	-0.0880996216489109557 	
--0.0611465984403298801 	-0.00625200849821254921 	0.0215485781279635726 	-0.0714409142747882558 	-0.0240918699485527833 	-0.0471072137986393372 	-0.0658120049173296084 	0.0253410948031073252 	0.0298882899553423141 	0.0525663923173221095 	
--0.0446255036375375058 	0.0298737678039323225 	-0.0294200842196646398 	0.0715801808220764696 	0.0266416466882419163 	0.0456747000463598937 	0.021156654567761779 	0.00729003319023803845 	-0.08648148970374471 	0.0976206317447985694 	
-0.0325006861144892037 	0.0948047441966916454 	0.0552968812959793374 	-0.0448840539946116918 	-0.0874241780518338757 	-0.0529760302504636918 	-0.0318827349764679649 	-0.00278714771528817268 	-0.0858725369518935633 	0.0163996717366173411 	
--0.0565635907236660407 	0.0393659838055386499 	-0.0233180830360347723 	0.072863158273558673 	0.0467477920730156662 	-0.0476420517489369796 	0.0292247241584120479 	-0.0840660705971008754 	-0.0622357183100257505 	-0.0657602182571084826 	
--0.0675117869112619867 	0.0579327661780450823 	0.0272521470153735366 	0.0238255598644825164 	0.00818234658452314696 	0.0476342449350507582 	-0.07600264331210696 	0.0395888239670870098 	0.0511301106927895543 	-0.0331474220748810972 	
-0.00665646873795682197 	0.0375958396472406142 	-0.0848427208591413656 	0.0587497797189621765 	-0.0245756817811631431 	0.0854398162685425372 	0.0377263946322403468 	0.0692954913285604296 	-0.0216598497790273826 	0.0361372816773090424 	
-0.0331965773376135392 	0.0574177242029850141 	0.0675521383088981131 	-0.092874323009645432 	0.0646108581813402 	-0.03158427038835733 	-0.0333754792947546855 	0.0510736519378347034 	-0.0221540054924109185 	-0.0582445210225887994 	
-0.00831261159168455131 	0.043949936149094343 	0.0807988898334478511 	0.0639048827841863926 	-0.0700620858635795374 	0.0425263342773682593 	-0.0798596085912762765 	0.0904361374548426494 	0.0673149697901879313 	-0.0495078811992989906 	
-0.0419252774569496209 	-0.00274780174071750883 	-0.0156616940455278075 	0.0119951715798352794 	0.0699234306356514013 	0.0492385818970535807 	0.0737547335980926339 	0.0622351858607390895 	-0.0456057665774118534 	0.0512802770624419674 	
-0.096390229619075285 	-0.0779849853422766304 	-0.0860802167023280163 	-0.0715296967936188827 	-0.0225309237815867411 	-0.00363272948207470905 	0.0420066209404783564 	-0.0352401961709145825 	-0.0656094816363315542 	0.00473140422529562746 	
-0.0833825990666294814 	-0.00624769795217346418 	0.0261331971787710504 	-0.0991511552554600467 	0.0257142706855030267 	0.0323690044508465405 	0.0474230023001297671 	-0.0137130565796517081 	-0.0232590910323398278 	0.0754590374261445013 	
--0.066105648662068639 	0.00577369683238790517 	-0.00256740781634764471 	0.0333594230424996696 	-0.0677189469204543815 	0.0248398724313597108 	0.0702538799750030418 	-0.0915703777218647846 	0.0234901769983958085 	0.0327162617841887937 	
-0.0583985241336935634 	0.0820107470728685672 	-0.00955928345111235062 	-0.0993580599362161032 	0.0112385800919470868 	-0.0926553902029668419 	0.0505787953213084895 	-0.00376689350291678059 	-0.0465189557234865769 	0.0520450241696136656 	
-0.0837502091232682905 	-0.0841884966418415814 	0.0494121729707651947 	-0.083936133206873681 	-0.057966470336787583 	-0.0238596826816774993 	0.0074211822514484195 	-0.06134676063250738 	-0.0109940538886606209 	0.0983730670734794393 	
-0.0529123183531036573 	-0.0432345001520824043 	0.00781517721136753586 	-0.0118531469690165252 	-0.0948478666683585103 	0.028792792703982082 	0.0542174633404027967 	0.0759149147840643967 	0.0800598366226032976 	-0.0472517348772329912 	
--0.0239333137589194597 	-0.0990300630786514896 	-0.032659862078664309 	0.0186467634580876618 	0.0609526977463653793 	-0.00216736699074640852 	-0.00121879808021418306 	-0.0469006585582069785 	-0.0832894957266879232 	-0.0146878193159981473 	
-0.0599722138658623227 	0.020659008771052919 	-0.0323822445617458557 	0.0905535021272905055 	-0.0309009341657960138 	0.0752865968092552157 	0.0354442313417307475 	-0.00519450309841556178 	-0.00626713774979259942 	-0.0892717640885118957 	
--0.0150246354325077117 	0.0592881779616925772 	0.0967698546253035613 	-0.0733209827353182436 	0.0431741209534788201 	-0.0264526659172714743 	0.0831733342366334505 	0.0509486301014937248 	-0.020779836100568494 	0.0745160348521420601 	
--0.0402585896396290208 	0.0894841536363436274 	-0.0944303698064100777 	-0.0287095310195410146 	0.0556318572909870884 	-0.0875172338170900205 	0.0816467544554470431 	0.0595895079693248494 	0.0432700294858368237 	-0.0507418226081152654 	
--0.0809319774806011444 	-0.0924418585648622143 	-0.0100643304933994756 	-0.0903205327462608537 	-0.029226336637553349 	-0.0707156936603000053 	-0.025055311486261115 	-0.0309185316225945514 	-0.0568428656173292218 	-0.0805315476908849553 	
-0.00625181634523529813 	-0.049085613016863601 	-0.0171859723498964945 	0.0276611138545139815 	0.0323232909201209007 	-0.00874766115791745895 	0.0650206249906532391 	-0.0252155741309334318 	-0.0106590562263953643 	0.0771124162221107218 	
-0.000754516090355915889 	0.0921314113864970113 	-0.0366355947914679334 	0.0032200353476968804 	0.0678823463794506893 	0.0845419605618411285 	-0.0863804653901767833 	-0.0150447948971687197 	-0.0185310063059124404 	-0.00267969226841359648 	
-0.0534687822377956853 	0.00883277851451328734 	-0.080587161326575299 	0.0562053915657517791 	-0.0592228707012416797 	-0.0745477080543376497 	-0.0184071952324021577 	-0.0693042091926673737 	-0.0557136555986918192 	0.0856277545351637331 	
--0.0901180419774364333 	0.0196822318357597545 	0.0549339855542598995 	-0.0407378702141069429 	0.084912674586448314 	0.036434032152185214 	-0.0388734174225119564 	-0.00932332780646582399 	0.066842251031516503 	-0.0937655525763287506 	
-0.0418094611472034847 	-0.0034865495405729109 	-0.0194268913471666443 	0.0596401053066652684 	0.0114756524502698503 	-0.0487885484250375548 	0.0809407284520493181 	-0.0626085378556807542 	-0.0304553560075297089 	0.0551435526152317598 	
--0.0709662261717747822 	-0.0495529079667091088 	-0.0521124899547957612 	-0.0623922623241493848 	-0.0551999586897789149 	0.0129194853780452319 	0.00901508776135405247 	0.0415840756926816602 	-0.0857215515425445024 	-0.0296371271449199745 	
-0.0774465411392550457 	0.0191608557913295272 	0.0449317265383520487 	-0.0317803820965146741 	0.0556669639242832401 	0.0869109445508808587 	0.0393351692325231983 	-0.0997391157559899683 	0.0163021708533605328 	-0.0448930060002757925 	
--0.018418132561986135 	-0.0394433166496583887 	-0.0218883833798195933 	-0.00254786229958634935 	0.0436960887846094825 	0.0948916596651195182 	0.0101859201705475338 	-0.0411800986788466786 	0.0933280365166843651 	0.0228310993419338498 	
-0.0172904373863724641 	0.0553657391310228633 	0.0389401209215691282 	0.0387697874927849517 	-0.015063296509071962 	-0.0177175456163194134 	-0.0126091300850640024 	0.0871464199720079491 	-0.0585451638022550791 	0.0290960679291529939 	
-0.00810595506075861491 	-0.00876555606222915486 	-0.0734014035199618275 	-0.0124936078550607604 	-0.0439246418069537473 	0.0562730522301329861 	-0.0195028412347448936 	-0.0657665036585061979 	-0.0471461632622273766 	0.00106772839415366001 	
--0.0471256240253403111 	0.0674603157880786786 	0.0472513376814104047 	0.0713426610452947141 	0.0613530236129621667 	0.0686458077983143206 	0.0863777940649222004 	0.017018328124105378 	-0.0548410041778817636 	-0.0461669862492033178 	
--0.00522413158780944513 	0.063453200293711931 	-0.0621297409088511082 	-0.0712824688434225834 	0.0459522273945540544 	0.0690492122492832178 	0.0768613542128165211 	0.079767484699452551 	0.0102854984391889098 	0.0918132944267897738 	
--0.00393427891828779241 	0.0425840829275945451 	0.0491208377697215023 	-0.103318937985919818 	0.0674843008411907253 	0.0493452973535978209 	-0.0781833223247483111 	0.0431451359384204536 	-0.0440880980351303006 	-0.0869082782702664119 	
--0.00837582867377460218 	0.00878233975268849151 	0.0739575362477558618 	0.0345049475001776851 	0.000531406817804182881 	0.0867710584392183265 	0.0544403128258240457 	-0.0758545580659029939 	0.03247325983917599 	0.076979697548940379 	
-0.00426838174104775085 	0.00558167442142068308 	0.0395484715143735385 	-0.0101225888321720883 	-0.0446245005720766058 	-0.0292792912149089229 	-0.0100682685054897039 	-0.0563179675782023367 	0.0133393130862614635 	-0.000614661970216939953 	
--0.0784082845832225622 	0.0646942741948428046 	0.041722169649281271 	0.00777852843327421842 	-0.0866909991926182177 	0.0512377842593555871 	0.0988431344334173151 	-0.0415543275238469539 	0.0296994591925104565 	-0.0735547351960807172 	
--0.0149061239641139071 	0.00328688974623620508 	0.0544199374629077692 	-0.0591943112961044737 	0.0715185650527177252 	-0.0373254372535212278 	0.0264238647628533899 	-0.0148004898905053327 	0.0568208495103211184 	-0.0413109107758464489 	
+-0.0978471800215257315 	-0.0902053404604457182 	0.0869754571825244599 	0.0408911880420643192 	-0.101368086228391774 	-0.0465450149174224462 	-0.0747286765670649444 	0.0781538958897185221 	-0.0910596663974124754 	-0.027060705005204036 	
+-0.0252235703285304778 	0.0354286612789488087 	0.0865639653081493043 	0.0614650337319190138 	0.0504116863076464333 	0.0453555903578909056 	-0.0713761496668450651 	-0.00510598328215907879 	-0.0401508455479649418 	-0.0381547596057954103 	
+-0.0122015642963453387 	0.0420692804838235443 	-0.0589592119877042353 	0.0271329090054837352 	-0.0246631886592047003 	-0.0536774953404823563 	-0.0373242762756519977 	0.0839985306538834353 	-0.075511151135593188 	-0.0137996039460813822 	
+0.00368161158062421494 	-0.0501473051925635516 	0.0673034534554419889 	0.090145308971782856 	0.000450396820622554436 	0.0129954022032453109 	-0.051842324345002036 	0.0186563341973783187 	-0.012069025736057214 	-0.0285276541865736623 	
+-0.0129379800125673326 	-0.103980705380140676 	-0.0539147279523639888 	0.0479517446087257895 	0.0020944939376027023 	-0.046696977104081952 	-0.0802431914099717286 	0.0735409085950746583 	0.0620187184289074656 	-0.063200293167444202 	
+-0.0716343255372051541 	0.0590042959001069733 	0.00728276313115366111 	-0.0834759753498713203 	0.0867722030742085759 	0.0385226901828749774 	-0.0652884327719597951 	-0.0824271906213171618 	0.0309700920772651364 	-0.0129277639998376815 	
+0.0318868724906055806 	0.0650593210786602066 	-0.0245714586022686109 	0.0433299122052953969 	0.020136412683900394 	0.0425738749199970137 	-0.0526247915666996774 	0.0675331131708467175 	-0.0477044071959432589 	-0.0255986225577582716 	
+-0.0640216405987112103 	-0.0299635163224174529 	0.0952592499384477542 	0.0668103062730507485 	0.0166086112657421146 	0.0664500679329425165 	0.022782895692366599 	-0.0893090662942366414 	0.0829269017431499555 	0.06856507275119339 	
+-0.0550278698599023092 	0.0770251765830581303 	0.0500563329594138559 	0.0550146273205624758 	-0.0516753580715226882 	0.0537945041440263672 	-0.0693359883892173123 	0.0729218325706957604 	0.0799929000213076835 	0.0960063223401683014 	
+-0.0707554591003550087 	0.0261111657093878111 	0.0941372243888994598 	0.0631654536171545739 	0.0170878181536895518 	-0.0229053753818019915 	-0.0250195065968273345 	-0.0434434186710700165 	-0.0244383203596413683 	-0.0307728326241952198 	
+-0.0282726977772407954 	0.0692922496347341127 	0.0220729968096950689 	-0.0973701172734391524 	0.0479943686715152626 	-0.0518425289509872947 	-0.0658329478901666515 	0.0904534376062660905 	-0.0967483311631070902 	0.0274779032809275223 	
+-0.0749180771549134289 	-0.0556314468690856934 	0.0546280338170591212 	-0.0157593227098106435 	0.0708181780326413574 	-0.0517166526253960424 	0.0557141215974588239 	-0.079753780233181415 	-0.00352613999712845898 	0.0599144738690658776 	
+-0.0528670085289950284 	-0.0387621706208924749 	0.0293976134619478267 	0.0433346791942989459 	-0.0382805807924307892 	0.0116297052156123931 	0.0375198279873637033 	-0.00532168376490164356 	0.0736722032536451937 	-0.00607366033620099308 	
+0.0391095640230744446 	0.0712773667374084208 	-0.0587773703690544561 	0.0312712307707591064 	0.0672423494430672147 	-0.0377084412744321321 	0.0971308441427504443 	-0.10365230712928164 	0.0209696696634045394 	0.00253509251293178089 	
+-0.0171588364027313728 	0.0907601551232998655 	-0.0981836322692552915 	0.0457181217897632899 	0.0694491906621885619 	-0.100096807598169274 	0.0288349067131420493 	0.0939563190155330713 	0.0580861665086693235 	-0.0668678678101209123 	
+0.0412420631610641436 	-0.0510501257610846207 	0.0145726656011963933 	-0.0161439367288427095 	0.02900140314584242 	-0.0332714076777847895 	-0.0851973071178984964 	-0.0336245111848018155 	0.0868618018288649374 	0.0498116353297164971 	
+0.060848102488633253 	-0.0569116799989186073 	0.0684003722487702692 	-0.0571831431037030449 	0.087248954058914252 	-0.0230842286627061591 	0.0455652187818044868 	-0.0755394545215909569 	-0.073978778720638308 	-0.00589058283666940834 	
+0.0587743374572413171 	0.00789329314258943419 	-0.0893465210301869345 	0.015147978699547579 	0.0641597853200572832 	0.0876528070286558963 	0.0481404816619878592 	0.0714061221447997718 	0.085947483446818268 	-0.0319564492764216612 	
+0.00821089371880087508 	-0.0555051105100034387 	-0.0280933946238387765 	-0.0453126509210930103 	0.0843524901379098524 	0.0458737090022064142 	0.0176972451932225459 	-0.0136543532179953051 	0.0964521208966484939 	0.0684559826177911501 	
+-0.0280958870094338252 	-0.0605968595383250125 	-0.0536939830278911484 	0.0185050456732452004 	0.0297323388736373435 	0.022916067040056394 	0.057050810818364793 	-0.0630618711032856771 	-0.0793649365440221993 	0.0461738074819058181 	
+0.0728351420887712597 	0.0666410235313574378 	0.0494792264267038678 	0.0496763266619888871 	0.0646676527061976619 	0.0793576424332214925 	0.0114131708236924166 	-0.0581806821111141292 	0.0504859418107055855 	-0.00911237013743402581 	
+0.0967952710302167896 	-0.0718511758900344383 	-0.0610162385239595706 	-0.0728244178184092733 	0.0449532172677859013 	0.0378468997457577574 	-0.0921341442982845926 	-0.055664063262185752 	0.0563301655537746179 	0.0786459667210254038 	
+0.0164745996068443937 	0.0924700937753852592 	-0.0540731976644756376 	0.0430464584705524053 	0.0755343311208682283 	-0.0499327949708687907 	-0.0724181596807987188 	-0.0582060784221022642 	-0.0158077489368196704 	0.0326834633026971619 	
+-0.00178754689648861608 	-0.0869974968769170326 	0.0295976076183521487 	0.0441334710836344471 	-0.000192463262426869572 	-0.0642391132227323886 	-0.0916698367580487811 	0.0932836870941053803 	-0.0107793497686112395 	-0.0728094587693954715 	
+0.0587479219193328614 	0.0821680843214413342 	-0.0152641196778351626 	0.0155875877114616153 	0.0740472414247421162 	0.0854385475675610156 	-0.038305799066720117 	0.0583463179284819458 	0.0801144536650854194 	0.0564738144694165306 	
+0.0940899965797011573 	0.0307091441747545553 	0.0900328705251852018 	0.0785367782152500832 	0.0881328378019818615 	0.0481656782083180884 	0.0626514637218753279 	0.0370131535481861562 	0.0539419100316362518 	-0.08554070466044425 	
+0.0267770875099327572 	-0.0527028069259232596 	-0.0875121894418438712 	-0.102763886213666097 	0.0115735260855198765 	0.0662990428625958583 	-0.0252398580773533915 	-0.0507800685040700056 	0.00088825071029126801 	-0.0614608956463818731 	
+-0.0258964815038111966 	0.018839005150950678 	0.00227796022337115132 	0.0657967815509993559 	0.0466204263681099934 	0.0243726024934155898 	-0.00878145291935440546 	-0.0594144833970865899 	-0.0931162159601290479 	-0.0336980703159352746 	
+-0.0827055227273208599 	0.0818624414826298191 	-0.0100739312020821977 	-0.0859335293119874166 	0.00327143050697012569 	0.0226369583876831447 	0.0146765130992764919 	-0.0121176597533669151 	0.0967442086187227934 	0.0697890856835364504 	
+-0.00745826725616854057 	-0.0964640904546764932 	-0.0795215372867596193 	0.03521500688924252 	-0.0638309265380326007 	0.0155327281291228799 	-0.00305800644031185582 	-0.0380918216866338519 	0.0272470536843010293 	-0.0419965098332424625 	
+-0.0411439402374458724 	-0.0728629894578508991 	0.0552819486421851428 	0.0211570654093570822 	0.0649121319561195148 	-0.0145505857125498017 	0.0699884843315146804 	-0.0982756424787766397 	-0.063944643334474735 	0.000620209474960204668 	
+0.00416525307999549704 	0.016646519152607387 	0.0464601937490181707 	0.00159073011554853889 	-0.0810419547018469133 	0.0587695853496177853 	-0.075212072118849449 	-0.0650844598149195597 	0.0641881737373511951 	0.0203239899258255211 	
+-0.0834353432236865228 	-0.0955822808254698508 	0.0348501126493722876 	-0.0550142504178344066 	-0.020150579218505188 	0.0409778259330861455 	0.0151601782900929634 	-0.0167503488448185051 	-0.0780623677050969189 	0.0131996537271319176 	
+-0.0311875626213257012 	0.00964766922438717138 	0.0208310649693951681 	-0.0301661280949164536 	0.0245147953760221603 	0.0800177054946719785 	0.0195435731221534313 	0.0216283567782093004 	-0.0180527829305334522 	0.0929732723292679974 	
+0.0494884033174168569 	-0.0543587423617514151 	0.0640360253134917984 	0.0700181231003522836 	-0.0618263622355974471 	0.0602747299634640646 	0.0562135558304485278 	-0.0603367217069216213 	-0.0108509209337982228 	-0.0764004339137397404 	
+0.0848612067087961136 	0.0329109132943089661 	-0.0136977181999341953 	-0.0836780609100953449 	0.0167781460043432418 	-0.0836734220703956033 	-0.0375079134292391897 	0.0358231923109561187 	-0.0535012528723355849 	-0.025814138917504427 	
+0.093872581974977623 	-0.100288405586654292 	-0.0667369635404056472 	-0.0757432695708722681 	-0.0914286777768788944 	0.00506720166870514195 	-0.0569430514126728626 	-0.0219288156913358609 	0.0876542703845367838 	-0.0145189574390498736 	
+-0.0818798343322028055 	-0.00366424918696527449 	-0.0944773533676381977 	-0.0261456960698514818 	0.0626537530347011096 	-0.0189298493990359938 	-0.0667215622413877274 	0.0740343450983735329 	-0.068067955207274225 	0.0830434652894488923 	
+0.0756160842618128631 	0.0209163601145688077 	0.0020039269759224718 	-0.0814615918998283783 	0.0194881244677043673 	0.00265130477851548141 	-0.0719831298173050638 	0.0889834087611306446 	0.0031138416969896201 	0.0224144322520954016 	
+-0.0896849819534229209 	-0.0507681076651540775 	-0.0289779219096501055 	0.0627111423993589739 	-0.0038815311416977652 	0.050949102919937185 	0.0355886318638906976 	0.0312050420039175977 	-0.0614487171521126557 	-0.0112073945688570577 	
+0.0424085111970063655 	0.0693315307404584608 	-0.0846862732797641904 	-0.0953938709818657471 	0.0376548271927240955 	-0.0944778011564852832 	-0.0882780307280190552 	-0.0089714840776669233 	-0.0139322356736678518 	0.0350624563590257246 	
+-0.0454355257807062193 	0.0407335288575404394 	0.0180117722199369598 	0.0771493812157636322 	-0.0675758387109726688 	0.0153502240693769113 	0.0738811919590802169 	-0.0956471899756761029 	-0.00114179025482402424 	-0.083127948494937931 	
+-0.0334213644483165387 	0.058862185098133811 	0.0769761653587529709 	-0.0848236867748237977 	0.0881689584635699936 	0.0517082924243271388 	0.0686564098239453996 	-0.0589395097959882763 	-0.0957627504743620156 	-0.000411925827903004884 	
+0.0847763359119753229 	-0.0763375138329060593 	0.0577498920380896258 	-0.0402810002700548994 	0.0648664324744550047 	-0.0209504144505155455 	0.0880108710566264857 	0.0105080587462940585 	0.00802210325959201546 	0.0992584693355362396 	
+0.0555958892386492418 	-0.00362873944350195453 	0.0143069433727433994 	0.0152863546044747865 	-0.103839765995600464 	0.0624549146255256801 	-0.0731947351204865204 	0.0533670424959666531 	-0.0234650870602571802 	0.0411367046365152672 	
+-0.0784377634731182327 	0.043013821241959814 	0.0694041032397587632 	0.0075247569125961233 	0.0642290136577166049 	-0.0907926066714660751 	0.0631589696110833876 	0.0675612503274558157 	0.0271515196662819869 	-0.0680822117213741307 	
+0.0776204240847777616 	0.00551169289000760267 	0.0568811442911565032 	-0.0266963208569938049 	0.0216901896270737284 	0.0753191394404137682 	0.0237798723518230226 	0.00724602707662162608 	0.0679537365137067723 	-0.0376416963609675706 	
+0.0290078560004861721 	0.00526448632767929654 	-0.0099892076077590982 	0.0761213374238426804 	-0.0899273903063939339 	-0.0924918258563098422 	-0.01722691103449785 	-0.066276478330580757 	0.0656496418263388115 	0.0423025198272805811 	
+-0.0428145482360620735 	-0.0504746153192209557 	-0.0783926234610002326 	0.0890118049976672265 	-0.0851565319747982358 	-0.0169602694851252872 	-0.0893771321465304802 	0.00203897817814427612 	0.0227081666325220609 	-0.0827621401647491428 	
+-0.0202939332038846038 	0.0499651065821050816 	0.022370442965727922 	0.0904591859661133257 	-0.00400335804406959112 	-0.0531147537264334418 	-0.0239984404112206819 	0.023937289048371696 	0.0181681047757719302 	-0.083538633665291781 	
+0.00491340188589174925 	0.01779819676377601 	-0.0655557033661144095 	0.0138100424275088458 	-0.0685934613762886058 	0.00674897857378112254 	0.0529034439817044344 	0.0615799327121481777 	0.0934024260239453702 	-0.0943407425561808011 	
+0.0768151585785116497 	0.0597422989017513742 	0.0837595244610199186 	0.00872565094808127467 	0.0445126667769655399 	0.0316961713127733163 	0.0887695112490788085 	0.0327375848077864351 	-0.00510488375715354315 	-0.00706632150067118374 	
+-0.0978956823384336677 	-0.00200908659061244501 	0.0672434044670669195 	0.0899697203124039252 	0.0881844596800903796 	-0.103233207082043713 	-0.052931062714118432 	-0.0413267167620032586 	-0.0097265018784599977 	-0.00671074612941752885 	
+-0.0106292336415997959 	-0.0346021147296487683 	-0.0869695336119137991 	-0.0343722972888396497 	0.0648971622010802579 	-0.0607500675535119355 	-0.0357395000343674746 	0.0720958341470771535 	0.00704618764407910318 	-0.0340237311570772069 	
+0.0690819359301237668 	-0.063247316946823659 	0.077993135275566422 	-0.0153871641512064936 	0.0446621165647628285 	0.0416411351295535789 	-0.0898780644202687579 	0.0942492828334858812 	0.0544579672023831202 	-0.0442268507056601118 	
+-0.075344362525791797 	-0.0206650076247724378 	0.100501537574655561 	-0.0651641498538071684 	-0.0875231576506865783 	0.0935193382443201826 	0.0750709168043962177 	0.0472002800272879469 	-0.0969269081918623515 	-0.0377364874448853707 	
+0.0524635402955774197 	-0.020256222914507991 	-0.0425486874800179993 	-0.0213338359419469534 	-0.0519005485415260231 	0.0374293880400871151 	0.00548471073340413978 	0.0254655463435365347 	0.0252572388840250246 	0.0917372757377646619 	
+-0.0489883465507234758 	0.0808549616726040171 	0.0175690392216576868 	0.0905441074731389711 	0.0597978198485653936 	-0.051931566427641053 	-0.0220680567163850753 	-0.0184359263509704351 	-0.0777786933795720642 	-0.0528490428893146763 	
+0.0197866657554005947 	-0.0623268710318765629 	0.0512216021139901695 	0.00481367540814497084 	-0.0524808786895438056 	-0.0293432470985781173 	-0.0469914955742517237 	0.0422838757133955079 	0.0715508130881882115 	-0.0502512994820883199 	
+0.0471628307583104658 	0.0329843168639450263 	0.0839644723490537287 	0.0505133876477681792 	0.0113144966169118535 	-0.0336779467177079836 	0.0400864726177653508 	-0.008754435788632043 	0.00141773973231703872 	-0.0739639332619120848 	
+0.0724060653433289542 	0.0249891318163628258 	-0.0779804093548025351 	-0.00418599605605514749 	0.0621294941531448999 	-0.0559658941182699793 	-0.0732465016145991549 	0.0784311003474822688 	0.0767134927729501981 	0.0126710224321379319 	
+0.0583069380662354989 	-0.0293102779377430282 	0.0645766828723413067 	-0.0297037758640440359 	0.00373395119793487737 	0.0635226154708643798 	-0.0246390714515497718 	0.0857587717904586821 	-0.00711122619763215977 	-0.0126705462308569078 	
+-0.0638861182897556973 	0.0606482406746605071 	0.0475696350655952713 	0.0733154834629952262 	-0.0469453033611485562 	0.0790303515559192815 	0.0883217850098304696 	0.03272534755045646 	0.0586621296998571726 	-0.0880996216489109557 	
+-0.0611465984403298801 	-0.00625200849821254401 	0.0215485781279635621 	-0.0714409142747882558 	-0.0240918699485527764 	-0.0471072137986393372 	-0.0658120049173295946 	0.0253410948031073287 	0.0298882899553422933 	0.0525663923173221165 	
+-0.0446255036375374919 	0.029873767803932319 	-0.0294200842196646432 	0.0715801808220764418 	0.0266416466882419163 	0.0456747000463599007 	0.0211566545677617721 	0.00729003319023804192 	-0.0864814897037447239 	0.0976206317447985694 	
+0.0325006861144892037 	0.0948047441966916593 	0.0552968812959793374 	-0.0448840539946116987 	-0.0874241780518338757 	-0.0529760302504636918 	-0.0318827349764679857 	-0.00278714771528817094 	-0.0858725369518935633 	0.0163996717366173342 	
+-0.0565635907236660337 	0.0393659838055386499 	-0.0233180830360347688 	0.0728631582735586592 	0.0467477920730156662 	-0.0476420517489369796 	0.029224724158412041 	-0.0840660705971008754 	-0.0622357183100257644 	-0.0657602182571084687 	
+-0.0675117869112620006 	0.05793276617804511 	0.0272521470153735193 	0.023825559864482506 	0.00818234658452314523 	0.0476342449350507582 	-0.0760026433121069739 	0.039588823967086996 	0.0511301106927895682 	-0.033147422074881111 	
+0.0066564687379568185 	0.0375958396472406003 	-0.0848427208591413656 	0.0587497797189621904 	-0.024575681781163157 	0.0854398162685425372 	0.0377263946322403329 	0.0692954913285604157 	-0.0216598497790273826 	0.0361372816773090424 	
+0.0331965773376135392 	0.0574177242029850002 	0.0675521383088981409 	-0.092874323009645432 	0.0646108581813402277 	-0.031584270388357337 	-0.0333754792947546994 	0.0510736519378347173 	-0.0221540054924109255 	-0.0582445210225887994 	
+0.00831261159168454784 	0.043949936149094336 	0.0807988898334478511 	0.0639048827841863648 	-0.0700620858635795513 	0.0425263342773682662 	-0.0798596085912762765 	0.0904361374548426494 	0.0673149697901879313 	-0.0495078811992989906 	
+0.0419252774569496209 	-0.00274780174071750363 	-0.015661694045527811 	0.0119951715798352759 	0.0699234306356513735 	0.0492385818970535946 	0.0737547335980926755 	0.0622351858607390895 	-0.0456057665774118534 	0.0512802770624419813 	
+0.0963902296190753266 	-0.0779849853422766304 	-0.0860802167023280163 	-0.0715296967936188827 	-0.022530923781586748 	-0.00363272948207471122 	0.0420066209404783286 	-0.0352401961709145617 	-0.0656094816363315542 	0.00473140422529563093 	
+0.0833825990666294814 	-0.00624769795217345898 	0.0261331971787710469 	-0.0991511552554600467 	0.0257142706855030163 	0.0323690044508465405 	0.0474230023001297671 	-0.0137130565796517116 	-0.0232590910323398278 	0.0754590374261445013 	
+-0.066105648662068639 	0.00577369683238789996 	-0.00256740781634763647 	0.0333594230424996696 	-0.0677189469204543815 	0.0248398724313597039 	0.0702538799750030279 	-0.0915703777218647846 	0.023490176998395805 	0.0327162617841887937 	
+0.0583985241336935634 	0.0820107470728685672 	-0.00955928345111234368 	-0.0993580599362161032 	0.0112385800919470834 	-0.0926553902029668419 	0.0505787953213084895 	-0.0037668935029167832 	-0.0465189557234865769 	0.0520450241696136656 	
+0.0837502091232682905 	-0.0841884966418415676 	0.0494121729707651947 	-0.083936133206873681 	-0.057966470336787583 	-0.0238596826816774993 	0.00742118225144842384 	-0.06134676063250738 	-0.0109940538886606244 	0.0983730670734794532 	
+0.0529123183531036573 	-0.0432345001520824043 	0.00781517721136753239 	-0.0118531469690165304 	-0.0948478666683585242 	0.0287927927039820786 	0.0542174633404027898 	0.075914914784064369 	0.0800598366226032976 	-0.0472517348772329981 	
+-0.0239333137589194528 	-0.0990300630786515174 	-0.032659862078664316 	0.0186467634580876618 	0.0609526977463653655 	-0.00216736699074641243 	-0.00121879808021418761 	-0.0469006585582069785 	-0.0832894957266879232 	-0.0146878193159981525 	
+0.0599722138658623227 	0.020659008771052912 	-0.0323822445617458418 	0.0905535021272904916 	-0.0309009341657960138 	0.0752865968092552157 	0.0354442313417307267 	-0.00519450309841556438 	-0.00626713774979260289 	-0.0892717640885118957 	
+-0.0150246354325076995 	0.0592881779616925494 	0.0967698546253035613 	-0.0733209827353182575 	0.0431741209534788131 	-0.0264526659172714743 	0.0831733342366334505 	0.0509486301014937248 	-0.020779836100568487 	0.0745160348521420601 	
+-0.0402585896396290277 	0.089484153636343669 	-0.0944303698064100777 	-0.0287095310195410076 	0.0556318572909871023 	-0.0875172338170900344 	0.0816467544554470293 	0.0595895079693248633 	0.0432700294858368237 	-0.0507418226081152654 	
+-0.0809319774806011305 	-0.0924418585648622143 	-0.0100643304933994669 	-0.0903205327462608537 	-0.029226336637553349 	-0.0707156936602999914 	-0.0250553114862611184 	-0.0309185316225945583 	-0.0568428656173292218 	-0.0805315476908849553 	
+0.00625181634523529553 	-0.049085613016863601 	-0.0171859723498964945 	0.0276611138545139884 	0.0323232909201209007 	-0.00874766115791745895 	0.0650206249906532391 	-0.0252155741309334248 	-0.0106590562263953591 	0.0771124162221107495 	
+0.000754516090355918058 	0.0921314113864970252 	-0.0366355947914679334 	0.00322003534769688734 	0.0678823463794506754 	0.0845419605618411285 	-0.0863804653901767833 	-0.0150447948971687215 	-0.0185310063059124265 	-0.00267969226841359865 	
+0.0534687822377956853 	0.00883277851451328214 	-0.080587161326575299 	0.056205391565751793 	-0.0592228707012416797 	-0.0745477080543376636 	-0.0184071952324021577 	-0.0693042091926673737 	-0.0557136555986918053 	0.0856277545351637331 	
+-0.0901180419774364333 	0.0196822318357597545 	0.0549339855542598995 	-0.0407378702141069429 	0.084912674586448314 	0.036434032152185214 	-0.0388734174225119564 	-0.00932332780646582052 	0.0668422510315165308 	-0.0937655525763287506 	
+0.0418094611472034847 	-0.00348654954057290569 	-0.0194268913471666374 	0.0596401053066652753 	0.0114756524502698503 	-0.0487885484250375548 	0.0809407284520493181 	-0.0626085378556807542 	-0.0304553560075297124 	0.0551435526152317598 	
+-0.0709662261717747822 	-0.0495529079667091088 	-0.0521124899547957612 	-0.0623922623241493779 	-0.0551999586897789149 	0.0129194853780452319 	0.00901508776135405594 	0.0415840756926816602 	-0.0857215515425445024 	-0.029637127144919985 	
+0.0774465411392550596 	0.0191608557913295306 	0.0449317265383520556 	-0.031780382096514681 	0.0556669639242832401 	0.0869109445508808448 	0.0393351692325231983 	-0.0997391157559899544 	0.0163021708533605328 	-0.0448930060002757925 	
+-0.018418132561986128 	-0.0394433166496583887 	-0.0218883833798195863 	-0.00254786229958634284 	0.0436960887846094825 	0.094891659665119546 	0.0101859201705475268 	-0.0411800986788466716 	0.0933280365166843928 	0.0228310993419338568 	
+0.0172904373863724571 	0.0553657391310228564 	0.0389401209215691074 	0.0387697874927849517 	-0.0150632965090719533 	-0.0177175456163194134 	-0.0126091300850639954 	0.0871464199720079213 	-0.058545163802255086 	0.0290960679291529939 	
+0.00810595506075860971 	-0.00876555606222916006 	-0.0734014035199618137 	-0.0124936078550607656 	-0.0439246418069537473 	0.0562730522301329791 	-0.0195028412347448901 	-0.0657665036585061979 	-0.0471461632622273766 	0.00106772839415365979 	
+-0.0471256240253403111 	0.0674603157880786369 	0.0472513376814103839 	0.0713426610452947141 	0.0613530236129621667 	0.0686458077983143206 	0.0863777940649222004 	0.0170183281241053745 	-0.0548410041778817706 	-0.0461669862492033248 	
+-0.00522413158780944253 	0.063453200293711931 	-0.0621297409088510874 	-0.0712824688434225834 	0.0459522273945540613 	0.0690492122492832178 	0.0768613542128165073 	0.079767484699452551 	0.0102854984391889202 	0.0918132944267898016 	
+-0.00393427891828779675 	0.0425840829275945451 	0.0491208377697215023 	-0.103318937985919804 	0.0674843008411907253 	0.0493452973535978279 	-0.0781833223247483111 	0.0431451359384204397 	-0.0440880980351303076 	-0.0869082782702664119 	
+-0.00837582867377460738 	0.00878233975268849845 	0.073957536247755834 	0.0345049475001776712 	0.000531406817804186459 	0.0867710584392183265 	0.0544403128258240457 	-0.0758545580659029939 	0.03247325983917599 	0.076979697548940379 	
+0.00426838174104775692 	0.00558167442142068135 	0.0395484715143735385 	-0.0101225888321720796 	-0.0446245005720766058 	-0.0292792912149089229 	-0.0100682685054897091 	-0.0563179675782023367 	0.0133393130862614565 	-0.000614661970216945483 	
+-0.0784082845832225622 	0.0646942741948427769 	0.0417221696492812641 	0.00777852843327421148 	-0.0866909991926182177 	0.0512377842593555871 	0.0988431344334173151 	-0.0415543275238469609 	0.0296994591925104703 	-0.0735547351960807172 	
+-0.0149061239641139105 	0.00328688974623620074 	0.0544199374629077831 	-0.0591943112961044737 	0.0715185650527177114 	-0.0373254372535212417 	0.0264238647628533899 	-0.0148004898905053292 	0.0568208495103211045 	-0.0413109107758464419 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 10 ;
 up_size = 100 ;
 learning_rate = 0.00100000000000000002 ;
@@ -245,12 +278,16 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 reconstruction_connection = *0 ;
+stochastic_reconstruction = 0 ;
 grad_learning_rate = 0.0100000000000000002 ;
 cd_learning_rate = 0.00100000000000000002 ;
+tied_connection_weights = 0 ;
 compute_contrastive_divergence = 0 ;
+deterministic_reconstruction_in_cd = 0 ;
 standard_cd_grad = 1 ;
 standard_cd_bias_grad = 1 ;
 standard_cd_weights_grad = 1 ;
@@ -268,7 +305,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *12 ->GradNNetLayerModule(
 start_learning_rate = 0.0100000000000000002 ;
 decrease_constant = 0 ;
@@ -281,25 +319,27 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
--0.00352105587679368229 	-0.00357354810371110932 	0.00890488617222215395 	0.00335092598517475757 	0.00410129629474930389 	0.00328131721187711195 	-0.00724348529870416386 	-0.00790721803139651618 	-0.00377540548143182357 	-0.00987397526955148021 	-0.000846853248181811424 	-0.0097106451184314057 	-0.00101484208149652715 	0.00439779577157847074 	0.0040341871863944518 	-0.00870366598845686255 	0.00204737765091421355 	-0.00726639929732078582 	0.00525372337084230659 	-0.00392427964261965641 	-0.0024053662587198133 	0.000170991325917960158 	0.00795705864310706709 	0.0057548868705265847 	0.000255604408781480821 	-0.00514063290295806597 	0.00745414562319659472 	0.00807584802275822591 	0.00366958268156301805 	0.00663127608167605499 	-0.00656721375058644946 	-0.0108239158127894797 	6.34527694779678632e-05 	-0.000906459224718117137 	0.00559722672401867169 	0.00734982591691369855 	-0.00859877079397596 	0.0024783492817755195 	0.00305145382606909375 	-0.00100694414771244781 	-0.0023635426!
 470743166 	-0.000543119707448018138 	-0.00380241953577049874 	0.00370967997352433451 	-0.00741465398244088233 	-0.0089191885763690807 	0.00910377752883012939 	-0.000563199178933415732 	0.00593549611870217413 	0.000384692560666607876 	-0.00213171801523757802 	-0.00132158145799427256 	0.0067312346743975222 	0.00259376405440213918 	-0.00522260806019301822 	0.00397489335352790244 	0.0044919261057543361 	0.00895539647173829167 	0.00710003389824242093 	-0.00850049898750221584 	0.00939741521443935093 	-0.00794720677179726505 	-0.00255343141509417918 	-0.0011832382855273371 	0.00486996100081734089 	-0.000304198523377983954 	-0.00482704882737465535 	-0.00465940526140109709 	-0.00305599630463488812 	-0.0059014634132350946 	-0.00215085851611780026 	-0.00734117331981761715 	0.0020957707352232029 	-0.00949115454420371499 	-0.00370571986074030423 	-0.00288355319652020816 	0.000515109721517998482 	-0.00422227694717533433 	0.00753861566472265737 	0.00728340993210358379 	0.00583098839794525!
 883 	-0.000258446958121499469 	-0.00278610266114094575 	-0.003!
 68208205
091915359 	-0.00294943868297097359 	-0.00114658589511941553 	-0.00398703532093420184 	-0.00756696972411996602 	-0.00419248545804548536 	0.00627166440409154379 	-0.0023563781922079155 	0.00324798868809928962 	0.00494263337252684887 	9.60063237721263532e-05 	-0.00370622637761735358 	-0.00809324784107792045 	-0.0103797355880524203 	0.00181847719525322351 	0.00827524248897480372 	-0.00242511316360745962 	
--0.00437708103139477535 	-0.00548987924211610454 	0.00646463169150594385 	0.000123613519211731237 	0.00230778956386396483 	0.00921965495384904389 	-0.00484395950404199315 	-0.00779730314687136512 	-0.00658615678737665706 	0.00235165075497618123 	-0.000606367754411816996 	0.00726297605939323857 	0.00665125792627973417 	0.00195460742027110971 	0.00565128025330919118 	0.00478173338708695952 	0.00215938338236002945 	-0.00935163994280847378 	0.00942022519651833194 	-0.00280452399388654962 	-0.00134299742148642071 	0.00676613916812528193 	-0.00680304728553140971 	0.00401499798961649484 	-0.0066158024725573748 	0.00556636539263337046 	0.00127041245548731785 	-0.00744752746662424251 	0.00736464953346151543 	-0.00986023989542055175 	-0.00882510648467334756 	0.0105994155981084578 	-0.00691613166819990237 	-0.00169028934898742336 	0.00185468213447432433 	0.00714341555153076221 	0.00212497489664858 	0.00381511953349902757 	-0.00582969124620332305 	0.00859658762424336809 	0.000231972102!
 049148179 	-0.00155040779586361173 	0.00963538700523923486 	-0.00519844330495995016 	0.00286806265491520128 	0.00102509550339589372 	-0.000731412422932949374 	-0.000370685529277404123 	0.00747187701627593636 	0.00970211388378461673 	0.00820946536952153749 	0.00214465576714964077 	0.00410952710820394838 	-0.00310860501769873003 	0.00110350289343739906 	0.005332569396474824 	-0.00459929100709582754 	0.00971496558542602091 	-0.00642497769404262124 	-0.00481832655821851693 	0.00683872182086346127 	0.0036237165237355401 	0.00221826983625268573 	0.00797566398971597847 	0.0008498074926189432 	-0.00302820331198164075 	-0.00679498815215908575 	0.00443734147505092311 	0.00234929012719161875 	-0.000523064881939133653 	-0.00365869893574876119 	-0.00975524124156581032 	0.00702787917859446897 	0.000323945923856628964 	-0.000890226184852111629 	0.00571402266512871602 	0.00413712091097342277 	0.00847838068445512343 	-0.00170791992384406593 	-0.00470763449597788423 	-0.00501592958172093414 !
 	0.00908371175007952752 	0.0084174563952020779 	-0.00955733838!
 26095693
2 	0.000347685069990730201 	0.000671148701010459417 	-0.00478402901640877318 	-0.0037550711131381408 	-0.00220396198535779833 	-0.00484939295497268785 	-0.0021847036731101944 	-0.00961153445140557755 	0.00144569228144339934 	0.00362336056288088942 	0.00322258930267744831 	0.00126655914724157505 	0.0072174239698360931 	0.00591933924189433879 	0.00875398176335348902 	-0.00528574677026058277 	
+-0.00352105587679368316 	-0.00357354810371111366 	0.00890488617222214875 	0.00335092598517475714 	0.00410129629474929435 	0.00328131721187710631 	-0.00724348529870416039 	-0.00790721803139652658 	-0.00377540548143181316 	-0.00987397526955148541 	-0.000846853248181816628 	-0.00971064511843141263 	-0.00101484208149653192 	0.00439779577157846987 	0.00403418718639445093 	-0.00870366598845685735 	0.00204737765091421269 	-0.00726639929732078929 	0.00525372337084229965 	-0.00392427964261965988 	-0.0024053662587198198 	0.000170991325917964061 	0.00795705864310706536 	0.00575488687052657516 	0.000255604408781481634 	-0.00514063290295806597 	0.00745414562319659732 	0.00807584802275823459 	0.00366958268156301936 	0.00663127608167605933 	-0.00656721375058646074 	-0.0108239158127894745 	6.34527694779698961e-05 	-0.000906459224718122883 	0.00559722672401865868 	0.00734982591691369681 	-0.00859877079397596174 	0.00247834928177551777 	0.00305145382606909201 	-0.00100694414771245193 	-0.002!
 36354264707431833 	-0.000543119707448018246 	-0.00380241953577050047 	0.00370967997352433321 	-0.00741465398244088233 	-0.00891918857636908417 	0.00910377752883012939 	-0.000563199178933414431 	0.0059354961187021776 	0.000384692560666613514 	-0.00213171801523758019 	-0.00132158145799427624 	0.00673123467439752481 	0.00259376405440213918 	-0.00522260806019301475 	0.0039748933535278981 	0.00449192610575433783 	0.00895539647173829861 	0.00710003389824242007 	-0.00850049898750221064 	0.00939741521443935614 	-0.00794720677179726678 	-0.00255343141509417875 	-0.0011832382855273384 	0.00486996100081734003 	-0.000304198523377986231 	-0.00482704882737465188 	-0.00465940526140110143 	-0.00305599630463488942 	-0.00590146341323509634 	-0.00215085851611779766 	-0.00734117331981761542 	0.00209577073522320551 	-0.00949115454420371846 	-0.00370571986074030597 	-0.00288355319652020686 	0.000515109721517996422 	-0.00422227694717534387 	0.00753861566472265737 	0.00728340993210358379 	0.005830!
 98839794525536 	-0.000258446958121501421 	-0.00278610266114094!
 054 	-0.
00368208205091915359 	-0.00294943868297097532 	-0.0011465858951194177 	-0.00398703532093420965 	-0.00756696972411995388 	-0.00419248545804548622 	0.00627166440409154379 	-0.0023563781922079129 	0.00324798868809928571 	0.00494263337252684627 	9.60063237721293212e-05 	-0.00370622637761734838 	-0.00809324784107791524 	-0.0103797355880524151 	0.0018184771952532233 	0.00827524248897480025 	-0.00242511316360746309 	
+-0.00437708103139477448 	-0.00548987924211610974 	0.00646463169150594732 	0.000123613519211735249 	0.00230778956386396396 	0.00921965495384903695 	-0.00484395950404198881 	-0.00779730314687137292 	-0.00658615678737665446 	0.00235165075497618297 	-0.000606367754411819056 	0.00726297605939322643 	0.00665125792627973764 	0.00195460742027110841 	0.00565128025330919465 	0.00478173338708696126 	0.00215938338236003292 	-0.00935163994280847552 	0.00942022519651832847 	-0.00280452399388655222 	-0.00134299742148642136 	0.00676613916812529061 	-0.00680304728553140971 	0.0040149979896164905 	-0.00661580247255737913 	0.00556636539263336179 	0.00127041245548731329 	-0.00744752746662424251 	0.00736464953346151197 	-0.00986023989542054828 	-0.0088251064846733545 	0.0105994155981084491 	-0.0069161316681998963 	-0.00169028934898742683 	0.00185468213447432129 	0.00714341555153076742 	0.0021249748966485774 	0.00381511953349902323 	-0.00582969124620332999 	0.00859658762424336462 	0.000231972102!
 049149154 	-0.00155040779586361303 	0.00963538700523922619 	-0.0051984433049599519 	0.00286806265491520041 	0.00102509550339588548 	-0.000731412422932950133 	-0.000370685529277404936 	0.00747187701627593983 	0.00970211388378461152 	0.00820946536952153229 	0.00214465576714963644 	0.00410952710820395098 	-0.00310860501769872656 	0.00110350289343739906 	0.00533256939647482834 	-0.00459929100709582928 	0.00971496558542601918 	-0.00642497769404262644 	-0.00481832655821852387 	0.00683872182086346387 	0.00362371652373554183 	0.00221826983625268356 	0.00797566398971596806 	0.000849807492618949271 	-0.00302820331198163945 	-0.00679498815215907794 	0.00443734147505092137 	0.00234929012719162222 	-0.00052306488193913322 	-0.00365869893574876379 	-0.00975524124156580859 	0.00702787917859447418 	0.000323945923856627555 	-0.000890226184852114665 	0.00571402266512871862 	0.0041371209109734219 	0.0084783806844551321 	-0.00170791992384406615 	-0.00470763449597788944 	-0.00501592958172093328!
  	0.00908371175007952232 	0.0084174563952020779 	-0.0095573383!
 82609558
91 	0.000347685069990732369 	0.0006711487010104592 	-0.00478402901640876798 	-0.0037550711131381421 	-0.00220396198535779964 	-0.00484939295497269218 	-0.00218470367311019223 	-0.00961153445140558796 	0.00144569228144339544 	0.00362336056288087814 	0.00322258930267744831 	0.00126655914724157202 	0.00721742396983609483 	0.00591933924189434052 	0.00875398176335347687 	-0.00528574677026058624 	
 ]
 ;
-bias = 2 [ -0.000596536845831870197 0.000596536845831888304 ] ;
+bias = 2 [ -0.000596536845831876703 0.000596536845831889171 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "affine_net" ;
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *13 ->SoftmaxModule(
 input_size = 2 ;
 name = "softmax" ;
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *14 ->NLLCostModule(
 target_size = 1 ;
 input_size = 2 ;
@@ -308,7 +348,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *15 ->ClassErrorCostModule(
 target_size = 1 ;
 input_size = 2 ;
@@ -317,7 +358,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *16 ->ArgmaxModule(
 input_size = -1 ;
 output_size = 1 ;
@@ -325,7 +367,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *17 ->SquaredErrorCostModule(
 target_size = 1 ;
 input_size = 1 ;
@@ -334,7 +377,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ] ;
 connections = 7 [ *18 ->NetworkConnection(
 source = "rbm_1.hidden.state" ;
@@ -371,7 +415,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 batch_size = 11 ;
 reset_seed_upon_train = 0 ;
@@ -380,7 +425,9 @@
 target_ports = 3 [ "nll.target" "class_error.target" "mse.target" ] ;
 weight_ports = []
 ;
+operate_on_bags = 0 ;
 mbatch_size = 11 ;
+random_gen = *5  ;
 seed = 1827 ;
 stage = 1001 ;
 n_examples = 200 ;

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave	2008-05-25 00:12:16 UTC (rev 9053)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/Split0/final_learner.psave	2008-05-26 14:11:27 UTC (rev 9054)
@@ -3,16 +3,21 @@
 modules = 8 [ *3 ->RBMModule(
 visible_layer = *4 ->RBMGaussianLayer(
 min_quad_coeff = 0 ;
-quad_coeff = 5 [ 1.04126452354610177 1.04051544992275513 1.00843700350871535 1.02648808921970036 0.997683771588443125 ] ;
+quad_coeff = 5 [ 1.04126452354610177 1.04051544992275513 1.00843700350871512 1.02648808921970036 0.997683771588443347 ] ;
+sigma = 5 [ 0.679146393769089585 0.680486183903941666 0.701880667884824283 0.689417966909771529 0.708727019431275584 ] ;
 share_quad_coeff = 0 ;
+fixed_std_deviation = -1 ;
+compute_mse_instead_of_nll = 0 ;
 size = 5 ;
 learning_rate = 0.00100000000000000002 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 5 [ 0.0434941669190519672 0.0364142277196536426 0.034241237123591145 0.0501800016691351519 0.0578342822039583232 ] ;
+bias = 5 [ 0.0434941669190519742 0.0364142277196536426 0.0342412371235911311 0.0501800016691351519 0.0578342822039583093 ] ;
 input_size = 5 ;
 output_size = 5 ;
 name = "RBMGaussianLayer" ;
@@ -22,43 +27,54 @@
 random_gen = *5 ->PRandom(
 seed = 1827 ;
 fixed_seed = 0  )
- )
 ;
+verbosity = 1  )
+;
 hidden_layer = *6 ->RBMBinomialLayer(
+use_signed_samples = 0 ;
 size = 10 ;
 learning_rate = 0.00100000000000000002 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 10 [ -0.00489387387961851255 0.00160433220966122499 0.0111635940286596343 -0.00544183202598008068 -0.00415357938715449522 -0.00122289797479547581 0.000539999407976528311 -0.00139379294340556182 0.000468276194375018523 -0.00130073698323172155 ] ;
+bias = 10 [ -0.00489387387961851428 0.00160433220966122586 0.0111635940286596361 -0.00544183202598008155 -0.00415357938715449695 -0.00122289797479547581 0.000539999407976528203 -0.00139379294340556291 0.000468276194375016788 -0.0013007369832317196 ] ;
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMBinomialLayer" ;
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 connection = *7 ->RBMMatrixConnection(
 weights = 10  5  [ 
--0.227792003702524865 	0.213078987007091047 	0.307229903614284316 	-0.245421671196374314 	-0.231661885947266138 	
-0.0227634339277156816 	-0.132765758861420169 	-0.00598562280103339533 	0.119192935630912156 	0.12501964543711705 	
-0.289494620724608964 	0.319651563456747057 	-0.0485688859221443323 	0.262655021633441677 	0.235511746120987631 	
--0.0987995665763502784 	-0.22852210103816209 	0.156140865592929973 	0.0423180829303292858 	-0.24883553839637651 	
-0.0910683050232134889 	0.063544779331590695 	0.129116509035014204 	-0.264071188535917767 	-0.201761414116804844 	
--0.0136575598826177112 	0.16486296546771656 	0.251952474852185526 	-0.0560741977509775641 	-0.233431504053665889 	
-0.0933686583109359231 	0.0955230499882306211 	-0.235117347498481205 	0.116660054365897131 	-0.0100077233218127306 	
-0.0952679704615028017 	-0.207883287442698711 	0.109514671318512447 	0.203153840670997871 	-0.233793901172238527 	
-0.067369071125721755 	0.131496759666859669 	0.188701779988374646 	-0.281404822764224849 	0.078351339709537407 	
--0.207605280356845362 	0.00197533522814370959 	-0.160360718055488488 	-0.134550885771956424 	0.320699614475532413 	
+-0.22779200370252492 	0.213078987007091047 	0.307229903614284428 	-0.245421671196374314 	-0.231661885947266111 	
+0.0227634339277156539 	-0.132765758861420169 	-0.00598562280103338146 	0.119192935630912156 	0.125019645437117022 	
+0.289494620724609131 	0.319651563456747112 	-0.0485688859221443323 	0.262655021633441677 	0.235511746120987603 	
+-0.0987995665763502784 	-0.228522101038162118 	0.156140865592929973 	0.042318082930329258 	-0.24883553839637651 	
+0.0910683050232134889 	0.0635447793315906534 	0.129116509035014204 	-0.264071188535917822 	-0.201761414116804871 	
+-0.0136575598826177147 	0.164862965467716532 	0.251952474852185471 	-0.0560741977509775641 	-0.233431504053665889 	
+0.0933686583109359647 	0.0955230499882305933 	-0.235117347498481233 	0.116660054365897103 	-0.0100077233218127531 	
+0.0952679704615028156 	-0.207883287442698683 	0.10951467131851246 	0.203153840670997843 	-0.233793901172238583 	
+0.0673690711257217412 	0.131496759666859669 	0.188701779988374591 	-0.281404822764224904 	0.078351339709537407 	
+-0.207605280356845362 	0.00197533522814369788 	-0.160360718055488488 	-0.134550885771956424 	0.320699614475532413 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 5 ;
 up_size = 10 ;
 learning_rate = 0.00100000000000000002 ;
@@ -70,12 +86,16 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 reconstruction_connection = *0 ;
+stochastic_reconstruction = 0 ;
 grad_learning_rate = 0.0100000000000000002 ;
 cd_learning_rate = 0.00100000000000000002 ;
+tied_connection_weights = 0 ;
 compute_contrastive_divergence = 0 ;
+deterministic_reconstruction_in_cd = 0 ;
 standard_cd_grad = 1 ;
 standard_cd_bias_grad = 1 ;
 standard_cd_weights_grad = 1 ;
@@ -93,153 +113,170 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *8 ->RBMModule(
 visible_layer = *9 ->RBMGaussianLayer(
 min_quad_coeff = 0 ;
-quad_coeff = 10 [ 1.05316323046459637 1.06757569854843326 1.03994099141493934 1.07443793317521119 1.10081520919294573 1.07882255745024347 1.05605868349609522 1.05405803109891871 1.05915337093685702 1.06366958190106264 ] ;
+quad_coeff = 10 [ 1.05316323046459637 1.06757569854843326 1.03994099141493934 1.07443793317521119 1.10081520919294595 1.07882255745024347 1.056058683496095 1.05405803109891871 1.05915337093685702 1.06366958190106242 ] ;
+sigma = 10 [ 0.671805308063093909 0.663442590047932934 0.68032093955026629 0.658497878794087055 0.643389166676638036 0.656916919799033994 0.669770401593847042 0.671113429233069936 0.667630007526228031 0.665896851943575396 ] ;
 share_quad_coeff = 0 ;
+fixed_std_deviation = -1 ;
+compute_mse_instead_of_nll = 0 ;
 size = 10 ;
 learning_rate = 0.00100000000000000002 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 10 [ 0.0200271393396775077 0.0154680798098186349 0.0258585400742424379 0.0140981283564797152 -0.00134668739887318974 0.0148545929121032746 0.0257687050941674337 0.0193589672997187615 0.0263077974042512375 0.0282150856406524382 ] ;
+bias = 10 [ 0.0200271393396775008 0.0154680798098186453 0.0258585400742424379 0.014098128356479717 -0.00134668739887318758 0.0148545929121032677 0.0257687050941674198 0.0193589672997187719 0.0263077974042512375 0.0282150856406524347 ] ;
 input_size = 10 ;
 output_size = 10 ;
 name = "RBMGaussianLayer" ;
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 hidden_layer = *10 ->RBMBinomialLayer(
+use_signed_samples = 0 ;
 size = 100 ;
 learning_rate = 0.00100000000000000002 ;
 momentum = 0 ;
+bias_decay_type = "none" ;
+bias_decay_parameter = 0 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 100 [ -0.000920452577715708052 0.000245701349212379579 -0.000513077974386876761 0.000410352819676094147 -0.000742123590533037424 -0.000675782301077305003 0.000379734490765858514 0.00172813952100234289 0.00220798838919422473 3.99980255300325208e-05 -0.000413952498886994145 0.000157356220871785504 0.000945096442174088555 0.000644914269874834623 0.000283696372582155994 0.000423717536541108576 -9.15917405183856227e-05 0.00145722505219732083 0.00111973030386828621 -0.000430089794572077386 0.00180342228352120396 3.76830315064562149e-05 -0.000277803164695551166 -0.000680519488708591696 0.00218594087690755258 0.00230148729430260709 -0.00132872648160434312 -0.000631343935544002107 0.00109563535685577493 -0.000782682985352260111 -0.000283404115289804788 0.000697331069274859214 -0.000741063872868963662 0.00129202952159453914 0.00070394782503980461 -0.000463997372728362399 -0.000313527430361479006 -0.000862388653056470186 0.000649854775102858147 -0.00022810098612181592 -0.000946!
 814695892115449 0.000105766666845392263 0.000269216623610254546 0.0019636946951001235 0.000904082800554930352 0.000647769756047155516 0.00165906233071529195 0.000637019062785513287 -0.00138369558315339829 0.000162091282283951968 0.00066056979294287744 0.00228451936284440344 -0.000513154038857693362 -0.00090647098336691745 0.000947944692690520329 0.00043419375277817284 0.0012706135131809672 -0.000516609340150207927 0.000419785807383476746 0.000982417941495361026 0.000541407992745716622 0.00117241581446960418 0.00162007915223743811 -6.75347534531582026e-05 0.000733180109199808535 0.00017396165751435428 -0.00102286550012955341 0.000444409454669872491 0.00119017907337350256 0.000148655712771739775 0.00148825598296720152 0.00148321181373795886 -0.000666771763512361113 0.0012626007802255773 0.000589837172197661469 0.000520785344685928951 0.000693809052826012428 0.00146489812392977428 -0.00129024726941498989 0.000635727603563731214 0.00191371662548164975 0.000147950302635302335 -0!
 .00241805910905778859 0.000866529300444627399 -1.5075127391659!
 7944e-05
 -0.000221866166384207525 -0.000263900397151751987 0.000814278940789008639 -0.00129677226049522811 0.000833049390240212851 0.000905089293650155821 0.00111404133163897265 -0.00076339797952878867 0.00112138332929521446 0.00175315541221823627 -0.000634608866670686111 0.00198662592302062922 0.000198270783730450828 0.000785476440785266264 0.000407673491056323477 ] ;
+bias = 100 [ -0.000920452577715708269 0.00024570134921238153 -0.000513077974386876652 0.000410352819676095015 -0.000742123590533038074 -0.00067578230107730522 0.000379734490765857538 0.00172813952100234333 0.00220798838919422386 3.99980255300330764e-05 -0.000413952498886994307 0.00015735622087178629 0.000945096442174088555 0.000644914269874833539 0.000283696372582155235 0.000423717536541108684 -9.15917405183858937e-05 0.00145722505219732126 0.00111973030386828664 -0.000430089794572077874 0.00180342228352120527 3.76830315064561336e-05 -0.000277803164695551274 -0.000680519488708591263 0.00218594087690755171 0.00230148729430261012 -0.00132872648160434442 -0.000631343935544002107 0.00109563535685577537 -0.000782682985352260003 -0.000283404115289805384 0.000697331069274859322 -0.000741063872868963662 0.00129202952159453936 0.000703947825039804393 -0.000463997372728361694 -0.000313527430361479006 -0.000862388653056470078 0.000649854775102859448 -0.000228100986121816787 -0.0009468!
 14695892116533 0.000105766666845392697 0.00026921662361025563 0.0019636946951001235 0.000904082800554929919 0.00064776975604715595 0.00165906233071529195 0.000637019062785513721 -0.00138369558315340068 0.000162091282283952673 0.000660569792942878307 0.00228451936284440084 -0.000513154038857693037 -0.000906470983366916908 0.000947944692690519896 0.000434193752778173274 0.00127061351318096612 -0.000516609340150208252 0.000419785807383477288 0.000982417941495361893 0.000541407992745716188 0.00117241581446960287 0.00162007915223743811 -6.75347534531583788e-05 0.00073318010919980886 0.000173961657514353575 -0.00102286550012955319 0.000444409454669872925 0.00119017907337350235 0.000148655712771741076 0.00148825598296720325 0.0014832118137379593 -0.000666771763512361763 0.00126260078022557773 0.000589837172197660276 0.000520785344685928625 0.000693809052826011995 0.00146489812392977341 -0.00129024726941499054 0.000635727603563731865 0.00191371662548165018 0.000147950302635301901 -!
 0.00241805910905778902 0.000866529300444628266 -1.507512739165!
 98097e-0
5 -0.000221866166384207525 -0.000263900397151752041 0.000814278940789009073 -0.00129677226049522789 0.000833049390240214152 0.000905089293650155387 0.00111404133163897395 -0.000763397979528787694 0.00112138332929521598 0.00175315541221823541 -0.000634608866670685894 0.00198662592302063052 0.000198270783730452346 0.000785476440785267999 0.000407673491056324778 ] ;
 input_size = 100 ;
 output_size = 100 ;
 name = "RBMBinomialLayer" ;
 use_fast_approximations = 0 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 connection = *11 ->RBMMatrixConnection(
 weights = 100  10  [ 
--0.0861417325165266257 	-0.0786638063827643924 	0.0966714479698655149 	0.0516780664389202213 	-0.0935759860853147829 	-0.0364467826231854516 	-0.0632230535763845664 	0.0899660456888490834 	-0.0785036383028760859 	-0.014121450415743135 	
--0.0118011407174486611 	0.0485753097209529819 	0.0984039313044046265 	0.0742977643447037817 	0.0592505794545960945 	0.0568950489975254547 	-0.0577722069161301852 	0.00955998174413244008 	-0.0256568308342410671 	-0.0230714865657609472 	
-6.86409515634804434e-05 	0.0539688960990592076 	-0.0474098608737475308 	0.0390513213728072989 	-0.0163310080854958024 	-0.0424903746844489533 	-0.0250371765781936574 	0.0969578142480475441 	-0.0618559765714472437 	1.0038411417479738e-05 	
-0.016710236954564639 	-0.0368186488099817161 	0.0789652499668531543 	0.102548747765204756 	0.00938864343331914479 	0.0244532752127019255 	-0.0386115567242019397 	0.0328043149676868623 	0.00190350816000103471 	-0.0137535446178924624 	
--0.000954765156340244567 	-0.0913976646034908025 	-0.0427049009465165427 	0.0595171970706733153 	0.00985895543397233981 	-0.0359627788803006804 	-0.067808688020551261 	0.0860914763124255022 	0.0743649106463285248 	-0.0492203084620364828 	
--0.0590837552483936615 	0.0709852711401236652 	0.0184607515186512583 	-0.0707698079308057976 	0.0942957407422609778 	0.0491742987756700334 	-0.0525511842831454801 	-0.0684996368201336442 	0.0440761316090169386 	0.00102870713363901025 	
-0.0451452796278654783 	0.078206554857198296 	-0.0119609284414650737 	0.0563893684865491851 	0.0293357018721735058 	0.0542868400177525245 	-0.0390224449260943296 	0.0819483695956100405 	-0.0330015865550528018 	-0.0105219668517385918 	
--0.0499894242243375184 	-0.015894074857273361 	0.107471610197175299 	0.0802425806743711639 	0.0263286127686550957 	0.0784702958989455246 	0.0364745409049370209 	-0.0737217182490697287 	0.0972820759963946152 	0.0835359815717947873 	
--0.0406043806064697171 	0.0911117414574526413 	0.0630438906076174199 	0.0689966241156475674 	-0.0409675703179725043 	0.0664546808627409225 	-0.0546834086539292955 	0.0882598205419448389 	0.0948128898564022105 	0.111091202043442056 	
--0.0577282240834442947 	0.0385927859031357928 	0.105268405747193386 	0.0753678978141263112 	0.0255934525326895236 	-0.011463510068290373 	-0.0122885429212349091 	-0.0292666128593720987 	-0.0105928704586070194 	-0.0163378778543190988 	
--0.0157063878647499891 	0.081256864396226744 	0.0334458860507079458 	-0.084509224140674008 	0.0560778025405891181 	-0.0404645054342887833 	-0.0530869295061847449 	0.103741105865636632 	-0.0828094369476290448 	0.0411698534625893897 	
--0.0621140454422243193 	-0.0429971206212671972 	0.0657430869007141067 	-0.00325691458029098158 	0.0788406018437792649 	-0.0403026041408256416 	0.0678723377877541029 	-0.0656720797189949967 	0.00997771535482402938 	0.073642271430935477 	
--0.0396962958142890107 	-0.0254810603405014845 	0.0411679579951165636 	0.0559713605412997736 	-0.0290663479480464323 	0.0231695939106714086 	0.0502866704491430963 	0.00889143825357812528 	0.0872213430530378725 	0.0084964686974578485 	
-0.0522217454229906702 	0.0841518958659751987 	-0.0461661076913289642 	0.0443513476295392325 	0.0761312908644860842 	-0.02549649345602506 	0.10982836038155859 	-0.0884739628090181335 	0.0353683117101483632 	0.0173923230628976883 	
--0.00374796660707523354 	0.103780497970321003 	-0.0852256331389483202 	0.0587023816493198666 	0.0783518567127009236 	-0.0874692209144963262 	0.0420216669865208156 	0.108197354322956782 	0.0722225678151461464 	-0.0515708969259319749 	
-0.0537420830108407341 	-0.0380244804292810823 	0.026164430920372829 	-0.00329877393710177689 	0.0374834397951652673 	-0.0217635549371616617 	-0.0719117383255620107 	-0.0195808461344031369 	0.0998755627764081472 	0.0637633694993499228 	
-0.0731786080715679521 	-0.0442273706555351767 	0.079611369006817867 	-0.0444354898402060175 	0.0951557701522115357 	-0.0118920966779942437 	0.0578001151286287249 	-0.0613560732694219957 	-0.0600623613275997059 	0.00825944018930453196 	
-0.0727545087991228051 	0.022501010892436852 	-0.0754437600551990523 	0.0293142845246605541 	0.0740018326844740504 	0.0999558764868055533 	0.0622457379065934555 	0.0867616411224109707 	0.100815237584435891 	-0.015878874761582961 	
-0.0216471235775783574 	-0.0414865047469387335 	-0.0153837885406207867 	-0.0314777842825800583 	0.0933348231789881688 	0.0577040457278816324 	0.0312849998979567306 	0.00124736923410225268 	0.110387969295368094 	0.0831866957618924846 	
--0.0158676964480599543 	-0.0483277394966532062 	-0.0423398434183346467 	0.0304304408489798027 	0.0375953333665299261 	0.0334747761769227381 	0.0687454537092801604 	-0.0495298047499326682 	-0.0657249394513980678 	0.059697367150593178 	
-0.0870486817088142739 	0.0810302056306821533 	0.062823121054105327 	0.0639294557777338041 	0.0747826981844236238 	0.0919973549081927466 	0.0258849172438556235 	-0.0418067047009972326 	0.0658379882835492952 	0.00700521966772083005 	
-0.108773222389732596 	-0.0589819425792947188 	-0.0492586517648492123 	-0.0598701836531344428 	0.0530194755731719092 	0.0486197628755582686 	-0.0790363649123128481 	-0.0418281084210015974 	0.0692595970449976922 	0.0922223344679270612 	
-0.0291780755943490296 	0.104675562718049522 	-0.0420272337621786299 	0.0556023915279756295 	0.0837845408729784163 	-0.0381816439907214089 	-0.0592873049958303963 	-0.0438537476898653023 	-0.00187167168714608796 	0.0468441481761720097 	
-0.010309211388964826 	-0.0744753547431004498 	0.0405072525339581074 	0.0557743386919779677 	0.00771808818288482329 	-0.0533071950011594631 	-0.079142590228909987 	0.105930392230867074 	0.00207570760029600516 	-0.0587360642017585241 	
-0.0733965862472779201 	0.0970373361984766258 	-0.00109849263293838352 	0.0305287505369259728 	0.0845862567563747808 	0.0984948638090170775 	-0.0230498018508680375 	0.0745952559226813611 	0.0956719710427825221 	0.072576700989362572 	
-0.108912322688067401 	0.0460630728303972198 	0.103924077225970704 	0.0931835847016890401 	0.0987757598439102102 	0.0615682600340273778 	0.0774781476388523016 	0.0536684385955828805 	0.0698955075194553771 	-0.0683256484083163279 	
-0.037970379286684057 	-0.0410890660465148549 	-0.0767246133195958191 	-0.090941035139824003 	0.0185846877983473352 	0.0757633836419891599 	-0.0136823190686553454 	-0.0381813239556530112 	0.013141196531555337 	-0.0480829558159745277 	
--0.0133240826496765757 	0.0310445835666900878 	0.013671194979685131 	0.0777258034048227825 	0.0546127244544156731 	0.0352094466291683963 	0.00355070753493351085 	-0.0454728981109072186 	-0.0790728300959578079 	-0.0194063201819028759 	
--0.0692634210448788423 	0.0947305025740419332 	0.00211369978218783937 	-0.0722712119269828951 	0.0124626768245360876 	0.0344385224095069831 	0.0278795414320396964 	0.00247073153068135161 	0.110467512958603797 	0.0839991815740364495 	
-0.00398281352443292502 	-0.0845493716388155125 	-0.0687451330406574079 	0.0463477871671479982 	-0.0561305982601386921 	0.0254556612634104333 	0.00837847739756166464 	-0.0255468361799351638 	0.0394466075054662538 	-0.0286202228478918685 	
--0.0286392430116645287 	-0.0603679364214899927 	0.0662258580202890329 	0.03319494200456162 	0.0727065123806992231 	-0.0036286893406804392 	0.0817950952518442659 	-0.0843198201388197788 	-0.0502615632725388825 	0.0145770024197204619 	
-0.0167402906901718031 	0.0292495317914311763 	0.0577628136799800893 	0.014210206634362019 	-0.0719394045562366247 	0.0697212331777804784 	-0.0621354616761680689 	-0.0509255733917263476 	0.0773937768913134788 	0.0343228431426546923 	
--0.0715707139718044227 	-0.0837175467046543387 	0.0450978515797024401 	-0.0433621067706970384 	-0.0126450137818251915 	0.0507834202411756586 	0.0265242726550822851 	-0.00413175557298824016 	-0.0651858469772916144 	0.026243031821955129 	
--0.017471650561774299 	0.0233032621970668506 	0.0333745827459559358 	-0.0163940019396641712 	0.034025271075211902 	0.0917831558468536618 	0.0330526481935554742 	0.0365281333098671501 	-0.00326566027688106374 	0.107551399248148188 	
-0.0621623926449725098 	-0.0411948172956044523 	0.075549634703823712 	0.0823213108380658337 	-0.0526455340069498789 	0.0713145677979969278 	0.0686465958903964973 	-0.0459533969321020141 	0.00308279456446657009 	-0.0615049390081123873 	
-0.0966774367645016619 	0.0447787833120154161 	-0.00240170793913392199 	-0.0711624629889161531 	0.0247741104500982384 	-0.0723894107585482532 	-0.02518444035136682 	0.0490272970660198745 	-0.0401208012571635034 	-0.0120644347924088544 	
-0.104896523558917057 	-0.088247860080853946 	-0.0559507943313718378 	-0.0638668641776518747 	-0.0834804606626358137 	0.0151166208062483327 	-0.0450394086847113462 	-0.00940439760158509955 	0.0993974174228176854 	-0.00141123609896087526 	
--0.0694705351948282923 	0.00836844618983835845 	-0.082892074690401607 	-0.0139454301750882581 	0.0702778345482266975 	-0.00810862368765842556 	-0.0542774235312317652 	0.0868698888360365656 	-0.0546222244360724252 	0.0962580555048138803 	
-0.088450340284502077 	0.0340920983084579479 	0.0142716655817638326 	-0.0678617045144634945 	0.0285207807267017638 	0.0144018957684261314 	-0.0583579374292024014 	0.103049707452223777 	0.0171435455939946586 	0.0368784764809119009 	
--0.0768768524549947041 	-0.0380837480406824441 	-0.0174156691185993334 	0.0746250803938452029 	0.00447983531724382555 	0.0616268975946653377 	0.0477100770065501806 	0.0446087823828205249 	-0.0476142831444484466 	0.00296633456802302678 	
-0.0539661493513540272 	0.080582051994979767 	-0.0734843818411942407 	-0.0831010940349215521 	0.0451120231654724702 	-0.0834462882886870122 	-0.0760003488700317364 	0.00398497120614737209 	-0.00114786528677741201 	0.0480962968356828252 	
--0.0328699639787280729 	0.0528157274592290921 	0.0291982272370953505 	0.0889000913905268336 	-0.058853216410345334 	0.0263002932630747692 	0.0856943415853353013 	-0.0816029069654683609 	0.0124065706959511739 	-0.0686830257735963345 	
--0.0201810103197048001 	0.0715424384109942246 	0.0886837224332765739 	-0.0714180202611850168 	0.0966127183945824503 	0.0630595050065478752 	0.0812818543367808261 	-0.0441297748579887392 	-0.0809887969486290799 	0.0141899334600852651 	
-0.0984104041724991191 	-0.0618961766005818592 	0.0705361246551845927 	-0.0260645621322587867 	0.0745990092946819811 	-0.00824612130850121798 	0.10153297434354909 	0.0258748887472621154 	0.022930298184031056 	0.11413739464306194 	
-0.0682468308991390216 	0.00935485276277766946 	0.02616049981290108 	0.0280691948412451278 	-0.0942483238529438355 	0.0736216775414032243 	-0.059944088366491767 	0.0672354339298348591 	-0.00950623923805744842 	0.055243903706101799 	
--0.0647203348909599213 	0.0562174520542019635 	0.0814442678077735988 	0.0206419449310587325 	0.0731732493967917758 	-0.0782596107034229105 	0.0761348876621017329 	0.081988665459976337 	0.0414087385509907119 	-0.0528294048973853153 	
-0.0913119163181111415 	0.0197729192916254425 	0.0697170163802186549 	-0.0125694301862307543 	0.0315630033509623209 	0.087456153811527465 	0.0377483602147873007 	0.0227062430791546699 	0.0825911761127729044 	-0.0218927707463625366 	
-0.0412407958738151836 	0.0175136901900071022 	0.00133792076556665893 	0.0880686142672262029 	-0.0808887210533200118 	-0.0807901445512391458 	-0.00481692937095364732 	-0.0524418046259498005 	0.0786356200848076642 	0.0559111915627236999 	
--0.0315027394460976468 	-0.0391007973715141177 	-0.0679413975484128158 	0.0995195871613830524 	-0.0776688188655555861 	-0.00712006588752233608 	-0.0777911813284563047 	0.0140438328799954142 	0.0345978502243173766 	-0.0694457428834264895 	
--0.00726763680335373082 	0.0626607101086971985 	0.0341710974120858074 	0.102718487628999322 	0.00487404164813985165 	-0.0412502466380579996 	-0.0110361818774208642 	0.038010030764682165 	0.0320005157425173187 	-0.0685833582522396845 	
-0.0177226364538976763 	0.0307849915215467167 	-0.0532981292231850534 	0.0264555447889565989 	-0.0593282429474002518 	0.0182389077520079025 	0.0654998964232903214 	0.0753679752185550528 	0.10679371523845245 	-0.0793821108424969529 	
-0.0912604634202468651 	0.0743478794195307768 	0.097220239319326357 	0.0233354719393765765 	0.055085846104544707 	0.0448648089526751045 	0.102967402263711275 	0.0489633798340027501 	0.0108101971229397128 	0.00914972251735276902 	
--0.0849809687268331809 	0.0103323293428373396 	0.0782606426675547667 	0.101827352843738464 	0.0959969554412966886 	-0.0915578694860124942 	-0.0402509761414983555 	-0.0274663769567490591 	0.00373489115083617965 	0.00741892197366237748 	
-0.00145183272208730695 	-0.0223501542720259266 	-0.07542270502961472 	-0.0222342025127398903 	0.0723968845762090474 	-0.0497782199743885684 	-0.0234495722596958986 	0.0848813022167153458 	0.019943564265201677 	-0.0201503621131671265 	
-0.0824305825874949993 	-0.0490107101764307648 	0.0903297359352691759 	-0.00173321969858411312 	0.0539108063986477315 	0.0534741071723026512 	-0.0756492235374460475 	0.108750788168692672 	0.0685291777136947389 	-0.0288132941942501643 	
--0.0625880573972465298 	-0.00816163087686855304 	0.111405356815965445 	-0.0526378793987037283 	-0.0786012704988595379 	0.103969529045432982 	0.0869553935788356241 	0.0606432982002185136 	-0.0829028454556515609 	-0.0236224752570268794 	
-0.06533351234519226 	-0.00692765580719425806 	-0.0301493079389002321 	-0.00804374270597679915 	-0.0423309488853819069 	0.0490271871647760329 	0.0186128378780059806 	0.039737222381360876 	0.0392496115688754782 	0.105815769667155196 	
--0.0360309654593565301 	0.0930131687470760471 	0.0291689127854928357 	0.102586807805481642 	0.0680255079999032702 	-0.0403031830725272822 	-0.00941038612265636398 	-0.00434338082798208479 	-0.0635262511249817946 	-0.0382379076388928221 	
-0.0322038567854431718 	-0.0493921379132445937 	0.0624464956070978558 	0.0171818272514066249 	-0.0437045328382055578 	-0.0180821456846998081 	-0.0341899619374273142 	0.0557210303917616279 	0.0844468450044989771 	-0.0359460161067151132 	
-0.0605110867530842811 	0.046354834151279424 	0.0960814898793639349 	0.0635832203032224985 	0.0207423732778661205 	-0.021370534598248718 	0.0532747561660496735 	0.00620639203468931373 	0.0159574703625993547 	-0.0585339464681958119 	
-0.0854098863273274195 	0.0383749256477820336 	-0.0651336122856546879 	0.00920700994449101529 	0.0710695971931226828 	-0.0436666128536733547 	-0.0594047118799946663 	0.0926846146363629747 	0.0905231559434967192 	0.027458892943277375 	
-0.0716787706281246384 	-0.0153600100611413631 	0.0769642814573454903 	-0.0160163313956490812 	0.0132956568408133472 	0.0752725399462989275 	-0.0108770644375806146 	0.100322111686597923 	0.00743522065119585698 	0.0024934191500020934 	
--0.0496875782208254027 	0.0745264291523592415 	0.0602964221677511555 	0.0866675334889287635 	-0.0367131105137037464 	0.0911437873522130959 	0.101717666780812219 	0.0479493466383217673 	0.0734592813113824628 	-0.072113114808994086 	
--0.048889762507005316 	0.00576665220695898612 	0.0323976248819665621 	-0.0590574377889105939 	-0.0159328548762790977 	-0.0361092095674989816 	-0.0533663657161536908 	0.0383742036302115799 	0.0426182679150549359 	0.0657470150822094812 	
--0.0310641866261481547 	0.0430511510380869322 	-0.0168711037294389903 	0.0845310130118581138 	0.0359149574417389206 	0.0574322911566908026 	0.0342836772440739332 	0.0219694738084665261 	-0.0714463693933729271 	0.112012574006262786 	
-0.0446070865595669663 	0.106324549132881682 	0.0662143505861573084 	-0.032528146893984991 	-0.0785988569875457482 	-0.0416858433603130227 	-0.0196350214180688633 	0.0107285756133524077 	-0.0721250591782636868 	0.0298617383887609909 	
--0.0442817643996714777 	0.0509814916614788755 	-0.0122512129523514625 	0.0842959321956655572 	0.0543198932839219217 	-0.0367292623995177717 	0.040919558965889731 	-0.0704755587188997251 	-0.0487365131569632501 	-0.0516895887108212715 	
--0.0540600238664465244 	0.0710029311522224205 	0.0392951512761265387 	0.0368060118481603124 	0.0172171366715894239 	0.0591507848146754708 	-0.0623579837344967886 	0.0538977289035742144 	0.0650159873279133327 	-0.0182209574434030047 	
-0.0202877169313203445 	0.0512503510157464939 	-0.0716152055631475715 	0.0720692571330929649 	-0.0146697715604106223 	0.0972726511001290178 	0.0511389445166192877 	0.0840021391470703732 	-0.00667526833338390104 	0.0511809241151031796 	
-0.0460841657647156067 	0.0701593466453695885 	0.0792930989248262774 	-0.0794914376738072165 	0.0731105960930893833 	-0.0198214222819848677 	-0.0201756113906326577 	0.0652202272548568635 	-0.00808872697198904228 	-0.0434649107228186962 	
-0.0220469976060823913 	0.057760123006089098 	0.0931977532934198521 	0.0772346664469984595 	-0.0598475777233471035 	0.0546670321583845389 	-0.0656482723600904788 	0.105148703985770878 	0.0816324277971470502 	-0.0340082982135949188 	
-0.0559073669022559297 	0.0115499190459667292 	-0.00227637075738397953 	0.0260368330527698498 	0.0797145175003680928 	0.0616751385907746835 	0.0874564372453314753 	0.0775559584151787468 	-0.0300766064678407476 	0.0666863867569315594 	
-0.107547133094365602 	-0.0661597175618631322 	-0.0750312873175343203 	-0.0596465291139990014 	-0.0148864131111699789 	0.00654354685106274189 	0.0532968302730689875 	-0.0224559145729239969 	-0.0527039145134178752 	0.0178677635211209572 	
-0.0963923033747754177 	0.00717344875208557504 	0.038453036435484339 	-0.0852305729109014837 	0.0349974754938469079 	0.0441776047503394248 	0.0605526140911736982 	0.00112664200986311658 	-0.0087323645914181066 	0.0898344769753848943 	
--0.0534115895805138616 	0.0181249695940979248 	0.00878363003094958204 	0.0455466177148464782 	-0.0588178514773917208 	0.0358653651595036357 	0.082242281840827422 	-0.0774890541600476573 	0.0369472008605820496 	0.0465351635632989744 	
-0.0708749948360742621 	0.0941472365254581306 	0.00226708912281538104 	-0.0861419439496607375 	0.019994648121601552 	-0.0806222482550226216 	0.062960522319972137 	0.0103389001022006783 	-0.0324336760051576259 	0.0658303459011729958 	
-0.0955803387142432764 	-0.0717037539477126395 	0.0603093016230788898 	-0.0712723826989075659 	-0.0493296803767544825 	-0.0128637489358975829 	0.0195715846647744121 	-0.047704226654149251 	0.00212767938104396715 	0.111425073998696722 	
-0.0658839890321762378 	-0.0295628146900774602 	0.0200661450461363106 	0.00131485756584521466 	-0.0849866013677955529 	0.0404686078881808867 	0.0671671998381296737 	0.0900259681327555267 	0.0937799245423497629 	-0.0322888665769429373 	
--0.012193640755859993 	-0.0870625365848600175 	-0.0219153724537396524 	0.0300269070805505404 	0.0679963006559635985 	0.00794668553911313802 	0.0103240085384890962 	-0.0340401721806524588 	-0.0702681992831467039 	-0.00124906146825614142 	
-0.0730247717053444517 	0.0339815257289185976 	-0.019848855447414427 	0.103214962249267753 	-0.0214835899446404119 	0.0867021686659173174 	0.0484463412604088511 	0.00943655047359577039 	0.00817973246608490884 	-0.0738347947439276647 	
--0.00089600701321111947 	0.0730742423097295035 	0.109410823441072003 	-0.0589267259135850333 	0.053123321772475543 	-0.0135355590753920615 	0.096744074162400745 	0.0663206959641660238 	-0.00549436838177646061 	0.0894934520821886043 	
--0.0271782661674656693 	0.102020433000738431 	-0.0819332552441457557 	-0.0157483850508686185 	0.0642100696527682091 	-0.0753445699775463773 	0.0941361076333349367 	0.0735604700520547383 	0.0571258886864093804 	-0.0359806897455684713 	
--0.0705536054527853007 	-0.0822073384032595555 	-0.00112150171006669837 	-0.0800304259132479656 	-0.0232782089529210978 	-0.0617095838930734122 	-0.0149387116402193675 	-0.0199394783436769515 	-0.0456782844612378669 	-0.0684025863257921724 	
-0.0193421802849989069 	-0.0357321956878843133 	-0.00496072883196821152 	0.0406104609691799387 	0.0413136602383304824 	0.00306079672871352633 	0.0777421250184955753 	-0.0107018642013677195 	0.00358113616845277078 	0.0913720102624297897 	
-0.0139232091273168331 	0.104943154693448962 	-0.0242118497989684961 	0.0163840269683069827 	0.0765070145006150754 	0.0958150456763250669 	-0.0727019189383650705 	-0.000419746250886005902 	-0.00416409232701833373 	0.0121238470826693732 	
-0.0651230092492420531 	0.0204138782025374797 	-0.0693453796121564769 	0.0677844774416461737 	-0.0508879293537423255 	-0.0634646977857465655 	-0.00656173557114147783 	-0.0559936636352839148 	-0.0424628758847008098 	0.0985469407427353034 	
--0.0769886117618060889 	0.032501820267599818 	0.0663893309678701693 	-0.0278876471174986269 	0.0929045402890513461 	0.0474998247528790329 	-0.0257978670799245066 	0.00476154287973440914 	0.0801698203420577182 	-0.0788382104339286627 	
-0.0547014778198773449 	0.00946833203311331188 	-0.00726462752600475457 	0.0723254716201774117 	0.0205615977056121738 	-0.0367530694409559516 	0.0934301107854380308 	-0.0479505446674512031 	-0.0160957470058741732 	0.0694328484699304055 	
--0.0595876310745595203 	-0.0383477302239165624 	-0.041792377416207567 	-0.0511609147197889594 	-0.0479087444457946612 	0.0225203087610199339 	0.0199693634725238364 	0.0534311524927292766 	-0.0731815681391047718 	-0.016781129205447963 	
-0.0905792518376024591 	0.0326154103319607688 	0.0572041638604988273 	-0.018205789586669973 	0.0646721485482818076 	0.0983918662842096087 	0.0525943899156008782 	-0.0843735568830600025 	0.0307371132750456598 	-0.0295980138681536178 	
--0.00504787451744056342 	-0.0256729736995633048 	-0.00942989152080439541 	0.0108273404656922673 	0.052692591304093539 	0.106240139148546284 	0.0235946303473424275 	-0.0263567496424630988 	0.107147791099797343 	0.0376929710309186969 	
-0.0308158526794357242 	0.0686749627074960955 	0.0513940667149026809 	0.0520384058598451771 	-0.00530110123048805823 	-0.00537524554129623772 	0.000916187426836804347 	0.101704735597045612 	-0.0435904190560880214 	0.0439280282078316278 	
-0.0197027542880891977 	0.00282764625979796663 	-0.0624133168796937379 	-0.000838184694977099696 	-0.0361146913489463275 	0.0662115725869421717 	-0.00779921545030481857 	-0.0527288120519812734 	-0.0341886177796775978 	0.0143417420339569616 	
--0.0328006992308578008 	0.0813174594280948321 	0.060185677611119523 	0.0848767764431867522 	0.0710419417995348801 	0.0809102300225778281 	0.0998703827619521756 	0.0325526540674225956 	-0.0392016732023402273 	-0.0302138858715109253 	
-0.00889465585724647304 	0.077477696001067492 	-0.048517506641452858 	-0.0567270884410531334 	0.0559349667575460979 	0.0814817504417899435 	0.0906402150722598332 	0.0950341870525590049 	0.0255652886676162247 	0.106900329777593742 	
-0.0086226725446124329 	0.0549555002798989348 	0.060457014530452928 	-0.0903770014132604521 	0.0753477814899700254 	0.0600692564159891404 	-0.065193230359046131 	0.0567607535996484738 	-0.030403393263260501 	-0.0723456062223043483 	
-0.0056503701531967921 	0.022848910592854519 	0.0865681102397283242 	0.0483345487266812826 	0.010626938123042785 	0.0989198164244114858 	0.0681344486795087551 	-0.0600214201563913016 	0.0474228192206973403 	0.0920626419468070084 	
-0.0165113020903557041 	0.0177311826089620629 	0.0505398087175398941 	0.00211791774586567866 	-0.0361001501220852769 	-0.0181649815856086451 	0.00221247923582702658 	-0.0425889963615622771 	0.0265007825372027619 	0.0131443579397779817 	
--0.0653153889137650656 	0.0771867556581843917 	0.053232169674471766 	0.0203190656293838964 	-0.0773921069536342293 	0.0624205572191128474 	0.110997722997582149 	-0.0272642040811898433 	0.0435103308168569708 	-0.0588413621392204211 	
--0.00187058312404764357 	0.0163102571290564546 	0.0661237839240759739 	-0.0459968476517004579 	0.0799619050368084311 	-0.0255381660134464256 	0.0393167535168238089 	-0.000428726455830320471 	0.0704347503703292022 	-0.0265693858624537725 	
+-0.0861417325165266257 	-0.0786638063827643924 	0.0966714479698655149 	0.0516780664389202143 	-0.0935759860853147829 	-0.0364467826231854586 	-0.0632230535763845941 	0.0899660456888490834 	-0.0785036383028760859 	-0.014121450415743142 	
+-0.0118011407174486629 	0.0485753097209529888 	0.0984039313044046265 	0.0742977643447037817 	0.0592505794545960945 	0.0568950489975254409 	-0.0577722069161301852 	0.00955998174413243834 	-0.0256568308342410706 	-0.0230714865657609507 	
+6.8640951563480091e-05 	0.0539688960990592076 	-0.0474098608737475308 	0.0390513213728072989 	-0.0163310080854958059 	-0.0424903746844489533 	-0.0250371765781936644 	0.0969578142480475302 	-0.0618559765714472437 	1.00384114174715709e-05 	
+0.016710236954564639 	-0.036818648809981723 	0.0789652499668531266 	0.102548747765204756 	0.0093886434333191552 	0.0244532752127019255 	-0.0386115567242019536 	0.0328043149676868623 	0.00190350816000102495 	-0.0137535446178924728 	
+-0.000954765156340244676 	-0.0913976646034908025 	-0.0427049009465165497 	0.0595171970706733292 	0.00985895543397234675 	-0.0359627788803006873 	-0.067808688020551261 	0.08609147631242553 	0.0743649106463285248 	-0.0492203084620364689 	
+-0.0590837552483936546 	0.0709852711401236791 	0.0184607515186512583 	-0.0707698079308057837 	0.0942957407422609639 	0.0491742987756700126 	-0.0525511842831454593 	-0.0684996368201336719 	0.044076131609016897 	0.00102870713363901849 	
+0.0451452796278654922 	0.0782065548571983238 	-0.0119609284414650702 	0.0563893684865491782 	0.0293357018721734815 	0.0542868400177525176 	-0.0390224449260943296 	0.0819483695956100683 	-0.0330015865550527948 	-0.0105219668517386039 	
+-0.0499894242243375184 	-0.015894074857273361 	0.107471610197175313 	0.0802425806743711778 	0.0263286127686550991 	0.0784702958989455385 	0.036474540904937007 	-0.0737217182490697287 	0.0972820759963946013 	0.0835359815717948012 	
+-0.0406043806064697102 	0.0911117414574526135 	0.063043890607617406 	0.0689966241156475535 	-0.0409675703179725112 	0.0664546808627409502 	-0.0546834086539293093 	0.0882598205419448806 	0.0948128898564022105 	0.111091202043442042 	
+-0.0577282240834442947 	0.0385927859031357789 	0.1052684057471934 	0.0753678978141263112 	0.025593452532689541 	-0.0114635100682903696 	-0.0122885429212349247 	-0.0292666128593720987 	-0.0105928704586070316 	-0.0163378778543191126 	
+-0.015706387864749996 	0.0812568643962267578 	0.0334458860507079458 	-0.0845092241406739803 	0.0560778025405891181 	-0.0404645054342887694 	-0.053086929506184731 	0.103741105865636646 	-0.0828094369476290726 	0.0411698534625894036 	
+-0.0621140454422243332 	-0.0429971206212671972 	0.0657430869007141205 	-0.00325691458029097854 	0.0788406018437792511 	-0.0403026041408256486 	0.0678723377877541029 	-0.0656720797189949967 	0.00997771535482403285 	0.073642271430935477 	
+-0.0396962958142890107 	-0.0254810603405014914 	0.0411679579951165706 	0.0559713605412997736 	-0.0290663479480464254 	0.0231695939106714016 	0.0502866704491431102 	0.00889143825357812181 	0.0872213430530378864 	0.00849646869745784503 	
+0.0522217454229906702 	0.0841518958659752125 	-0.0461661076913289642 	0.0443513476295392256 	0.0761312908644860842 	-0.02549649345602506 	0.10982836038155859 	-0.0884739628090181335 	0.0353683117101483632 	0.0173923230628977021 	
+-0.00374796660707524352 	0.103780497970321017 	-0.0852256331389483202 	0.0587023816493198736 	0.0783518567127009097 	-0.0874692209144963123 	0.0420216669865208156 	0.108197354322956768 	0.0722225678151461881 	-0.0515708969259319888 	
+0.0537420830108407271 	-0.0380244804292810754 	0.0261644309203728116 	-0.00329877393710177559 	0.0374834397951652604 	-0.0217635549371616478 	-0.0719117383255620107 	-0.0195808461344031369 	0.0998755627764081472 	0.0637633694993499228 	
+0.073178608071567966 	-0.0442273706555351906 	0.0796113690068178392 	-0.0444354898402060106 	0.0951557701522115357 	-0.0118920966779942402 	0.0578001151286287179 	-0.0613560732694219957 	-0.060062361327599692 	0.00825944018930452502 	
+0.072754508799122819 	0.0225010108924368554 	-0.0754437600551990384 	0.0293142845246605575 	0.0740018326844740504 	0.0999558764868055394 	0.0622457379065934555 	0.0867616411224109707 	0.100815237584435891 	-0.015878874761582961 	
+0.021647123577578354 	-0.0414865047469387266 	-0.0153837885406207867 	-0.0314777842825800652 	0.0933348231789881549 	0.0577040457278816046 	0.0312849998979567445 	0.0012473692341022581 	0.110387969295368107 	0.0831866957618924985 	
+-0.0158676964480599612 	-0.0483277394966532062 	-0.0423398434183346398 	0.0304304408489798062 	0.0375953333665299053 	0.0334747761769227381 	0.0687454537092801743 	-0.0495298047499326682 	-0.0657249394513980678 	0.0596973671505931849 	
+0.0870486817088142739 	0.0810302056306821394 	0.062823121054105327 	0.0639294557777338318 	0.0747826981844236238 	0.0919973549081927189 	0.0258849172438556201 	-0.0418067047009972395 	0.0658379882835492952 	0.00700521966772083698 	
+0.108773222389732582 	-0.0589819425792947605 	-0.0492586517648492192 	-0.0598701836531344636 	0.05301947557317193 	0.0486197628755582478 	-0.0790363649123128481 	-0.0418281084210015905 	0.0692595970449976922 	0.092222334467927089 	
+0.0291780755943490192 	0.104675562718049522 	-0.0420272337621786299 	0.0556023915279756087 	0.0837845408729784163 	-0.038181643990721395 	-0.0592873049958304033 	-0.0438537476898652884 	-0.00187167168714608145 	0.0468441481761720166 	
+0.0103092113889648382 	-0.0744753547431004637 	0.0405072525339581352 	0.0557743386919779816 	0.00771808818288481635 	-0.05330719500115947 	-0.079142590228909987 	0.105930392230867074 	0.00207570760029600777 	-0.0587360642017585102 	
+0.073396586247277934 	0.0970373361984766397 	-0.00109849263293838265 	0.0305287505369259693 	0.0845862567563747669 	0.0984948638090170775 	-0.0230498018508680375 	0.0745952559226813611 	0.0956719710427825082 	0.072576700989362572 	
+0.108912322688067414 	0.0460630728303972128 	0.103924077225970704 	0.0931835847016890262 	0.0987757598439102102 	0.0615682600340273639 	0.0774781476388522877 	0.0536684385955828874 	0.0698955075194553632 	-0.0683256484083163279 	
+0.0379703792866840431 	-0.0410890660465148549 	-0.0767246133195958052 	-0.0909410351398239752 	0.0185846877983473456 	0.075763383641989146 	-0.0136823190686553436 	-0.0381813239556530112 	0.013141196531555337 	-0.0480829558159745346 	
+-0.0133240826496765757 	0.0310445835666900878 	0.0136711949796851414 	0.0777258034048227686 	0.054612724454415687 	0.0352094466291683755 	0.00355070753493350565 	-0.0454728981109072186 	-0.0790728300959578217 	-0.0194063201819028724 	
+-0.0692634210448788423 	0.0947305025740419193 	0.00211369978218784371 	-0.0722712119269828951 	0.0124626768245360911 	0.0344385224095069761 	0.0278795414320396964 	0.00247073153068135638 	0.110467512958603784 	0.0839991815740364495 	
+0.00398281352443292241 	-0.0845493716388154987 	-0.0687451330406574218 	0.0463477871671479844 	-0.0561305982601386921 	0.0254556612634104333 	0.00837847739756166637 	-0.0255468361799351673 	0.0394466075054662468 	-0.028620222847891879 	
+-0.0286392430116645426 	-0.0603679364214899927 	0.066225858020289019 	0.033194942004561627 	0.0727065123806992369 	-0.0036286893406804366 	0.0817950952518442659 	-0.0843198201388197927 	-0.0502615632725388756 	0.0145770024197204619 	
+0.01674029069017181 	0.0292495317914311798 	0.0577628136799800754 	0.0142102066343620208 	-0.0719394045562365969 	0.0697212331777804784 	-0.0621354616761680689 	-0.0509255733917263476 	0.0773937768913135066 	0.0343228431426546923 	
+-0.0715707139718044227 	-0.0837175467046543526 	0.0450978515797024263 	-0.0433621067706970384 	-0.0126450137818251897 	0.0507834202411756655 	0.0265242726550822712 	-0.00413175557298824624 	-0.0651858469772916005 	0.0262430318219551255 	
+-0.017471650561774299 	0.0233032621970668437 	0.0333745827459559219 	-0.0163940019396641747 	0.034025271075211895 	0.0917831558468536479 	0.0330526481935554811 	0.0365281333098671293 	-0.0032656602768810607 	0.107551399248148161 	
+0.0621623926449725028 	-0.0411948172956044523 	0.0755496347038236843 	0.0823213108380658337 	-0.0526455340069498789 	0.0713145677979969417 	0.0686465958903964835 	-0.0459533969321020141 	0.00308279456446656618 	-0.0615049390081124012 	
+0.096677436764501648 	0.044778783312015423 	-0.00240170793913392243 	-0.071162462988916167 	0.0247741104500982488 	-0.0723894107585482671 	-0.0251844403513668165 	0.0490272970660198745 	-0.0401208012571635103 	-0.012064434792408851 	
+0.104896523558917085 	-0.0882478600808539737 	-0.0559507943313718517 	-0.0638668641776518886 	-0.0834804606626358275 	0.015116620806248331 	-0.0450394086847113601 	-0.00940439760158509955 	0.0993974174228176854 	-0.0014112360989608655 	
+-0.0694705351948282923 	0.00836844618983835671 	-0.0828920746904015932 	-0.0139454301750882546 	0.0702778345482266975 	-0.0081086236876584325 	-0.0542774235312317652 	0.0868698888360365656 	-0.0546222244360724599 	0.096258055504813908 	
+0.0884503402845020353 	0.0340920983084579479 	0.0142716655817638309 	-0.0678617045144634806 	0.0285207807267017568 	0.0144018957684261366 	-0.0583579374292024083 	0.10304970745222379 	0.0171435455939946621 	0.0368784764809119009 	
+-0.0768768524549947041 	-0.0380837480406824302 	-0.0174156691185993334 	0.0746250803938452029 	0.00447983531724382468 	0.0616268975946653377 	0.0477100770065501736 	0.0446087823828205179 	-0.0476142831444484466 	0.0029663345680230207 	
+0.0539661493513540133 	0.080582051994979767 	-0.0734843818411942545 	-0.0831010940349215521 	0.0451120231654724702 	-0.0834462882886869844 	-0.0760003488700317364 	0.00398497120614736775 	-0.00114786528677740746 	0.0480962968356828252 	
+-0.0328699639787280659 	0.052815727459229099 	0.0291982272370953436 	0.0889000913905268336 	-0.058853216410345327 	0.0263002932630747623 	0.0856943415853353152 	-0.0816029069654683747 	0.0124065706959511704 	-0.0686830257735963345 	
+-0.0201810103197048035 	0.0715424384109942524 	0.0886837224332765739 	-0.071418020261184989 	0.0966127183945824364 	0.0630595050065478752 	0.0812818543367808538 	-0.0441297748579887392 	-0.080988796948629066 	0.0141899334600852668 	
+0.0984104041724991052 	-0.0618961766005818592 	0.0705361246551845789 	-0.0260645621322587832 	0.0745990092946819811 	-0.00824612130850120757 	0.101532974343549076 	0.0258748887472621085 	0.022930298184031063 	0.114137394643061926 	
+0.0682468308991390354 	0.0093548527627776816 	0.026160499812901087 	0.0280691948412451278 	-0.0942483238529438216 	0.0736216775414031965 	-0.059944088366491767 	0.0672354339298348591 	-0.00950623923805744148 	0.055243903706101799 	
+-0.0647203348909599074 	0.0562174520542019704 	0.0814442678077735988 	0.0206419449310587325 	0.0731732493967917619 	-0.0782596107034229105 	0.0761348876621017467 	0.0819886654599763509 	0.0414087385509907049 	-0.0528294048973853153 	
+0.0913119163181111138 	0.0197729192916254425 	0.069717016380218641 	-0.012569430186230756 	0.0315630033509623209 	0.0874561538115274512 	0.0377483602147872938 	0.0227062430791546629 	0.0825911761127729044 	-0.0218927707463625366 	
+0.0412407958738151836 	0.0175136901900071057 	0.00133792076556665785 	0.088068614267226189 	-0.0808887210533200118 	-0.0807901445512391736 	-0.00481692937095365079 	-0.0524418046259497936 	0.0786356200848076503 	0.0559111915627237069 	
+-0.0315027394460976468 	-0.0391007973715141177 	-0.0679413975484128158 	0.0995195871613830524 	-0.0776688188655555861 	-0.00712006588752233608 	-0.0777911813284563047 	0.014043832879995409 	0.0345978502243173766 	-0.0694457428834264895 	
+-0.00726763680335373169 	0.0626607101086971846 	0.0341710974120858074 	0.102718487628999308 	0.00487404164813985078 	-0.0412502466380579996 	-0.0110361818774208694 	0.038010030764682165 	0.0320005157425173187 	-0.0685833582522396984 	
+0.0177226364538976659 	0.0307849915215467201 	-0.0532981292231850465 	0.0264555447889565989 	-0.0593282429474002448 	0.018238907752007906 	0.0654998964232903491 	0.0753679752185550528 	0.10679371523845245 	-0.0793821108424969529 	
+0.0912604634202468651 	0.0743478794195307768 	0.0972202393193263431 	0.02333547193937658 	0.0550858461045447001 	0.0448648089526751254 	0.102967402263711275 	0.0489633798340027362 	0.0108101971229397076 	0.00914972251735276382 	
+-0.0849809687268331948 	0.0103323293428373465 	0.0782606426675547806 	0.101827352843738464 	0.0959969554412966747 	-0.0915578694860125081 	-0.0402509761414983624 	-0.0274663769567490453 	0.00373489115083618399 	0.00741892197366238269 	
+0.00145183272208730522 	-0.0223501542720259232 	-0.07542270502961472 	-0.0222342025127398868 	0.0723968845762090613 	-0.0497782199743885684 	-0.0234495722596959298 	0.0848813022167153458 	0.0199435642652016736 	-0.0201503621131671196 	
+0.0824305825874950132 	-0.0490107101764307648 	0.0903297359352691759 	-0.00173321969858411464 	0.0539108063986477454 	0.0534741071723026373 	-0.0756492235374460475 	0.108750788168692672 	0.068529177713694725 	-0.0288132941942501712 	
+-0.0625880573972465298 	-0.00816163087686854784 	0.111405356815965459 	-0.0526378793987037283 	-0.078601270498859524 	0.103969529045432982 	0.0869553935788356658 	0.0606432982002185483 	-0.0829028454556515609 	-0.0236224752570268863 	
+0.06533351234519226 	-0.00692765580719425893 	-0.0301493079389002251 	-0.00804374270597679221 	-0.0423309488853819069 	0.0490271871647760191 	0.0186128378780059875 	0.0397372223813608622 	0.0392496115688754713 	0.105815769667155168 	
+-0.0360309654593565301 	0.0930131687470760332 	0.0291689127854928426 	0.10258680780548167 	0.0680255079999032425 	-0.0403031830725272822 	-0.00941038612265636572 	-0.00434338082798208132 	-0.0635262511249817807 	-0.0382379076388928221 	
+0.0322038567854431648 	-0.0493921379132446006 	0.0624464956070978419 	0.0171818272514066249 	-0.0437045328382055717 	-0.0180821456846998047 	-0.0341899619374273211 	0.0557210303917616279 	0.0844468450044989632 	-0.0359460161067151132 	
+0.060511086753084295 	0.0463548341512794171 	0.096081489879363921 	0.0635832203032224985 	0.0207423732778661205 	-0.0213705345982487145 	0.0532747561660496735 	0.006206392034689312 	0.0159574703625993547 	-0.058533946468195798 	
+0.0854098863273274195 	0.0383749256477820336 	-0.065133612285654674 	0.00920700994449101875 	0.071069597193122655 	-0.0436666128536733686 	-0.0594047118799946802 	0.0926846146363629886 	0.0905231559434967192 	0.027458892943277375 	
+0.0716787706281246384 	-0.0153600100611413666 	0.0769642814573454487 	-0.0160163313956490881 	0.0132956568408133524 	0.0752725399462988859 	-0.0108770644375806077 	0.100322111686597937 	0.00743522065119585091 	0.00249341915000210078 	
+-0.0496875782208253819 	0.0745264291523592554 	0.0602964221677511555 	0.0866675334889287635 	-0.0367131105137037533 	0.0911437873522130959 	0.101717666780812177 	0.0479493466383217673 	0.0734592813113824628 	-0.0721131148089940999 	
+-0.0488897625070053229 	0.00576665220695899306 	0.0323976248819665621 	-0.059057437788910587 	-0.0159328548762790873 	-0.0361092095674989885 	-0.053366365716153677 	0.0383742036302115869 	0.0426182679150549151 	0.0657470150822094673 	
+-0.0310641866261481547 	0.0430511510380869461 	-0.0168711037294389833 	0.0845310130118580999 	0.0359149574417389206 	0.0574322911566908165 	0.0342836772440739124 	0.0219694738084665331 	-0.0714463693933729271 	0.112012574006262799 	
+0.0446070865595669663 	0.106324549132881682 	0.0662143505861572945 	-0.032528146893984998 	-0.0785988569875457344 	-0.0416858433603130296 	-0.0196350214180688667 	0.0107285756133524111 	-0.0721250591782636868 	0.0298617383887609875 	
+-0.0442817643996714846 	0.0509814916614788685 	-0.0122512129523514642 	0.0842959321956655433 	0.0543198932839219217 	-0.0367292623995177578 	0.0409195589658897241 	-0.0704755587188997251 	-0.048736513156963257 	-0.0516895887108212715 	
+-0.0540600238664465382 	0.0710029311522224205 	0.0392951512761265248 	0.0368060118481603193 	0.0172171366715894343 	0.0591507848146754708 	-0.0623579837344968163 	0.0538977289035742074 	0.0650159873279133327 	-0.0182209574434030186 	
+0.0202877169313203411 	0.0512503510157464939 	-0.0716152055631475715 	0.0720692571330929649 	-0.0146697715604106275 	0.0972726511001289762 	0.0511389445166192877 	0.0840021391470703732 	-0.0066752683333838941 	0.0511809241151031658 	
+0.0460841657647155997 	0.0701593466453695747 	0.0792930989248263052 	-0.0794914376738072304 	0.0731105960930893972 	-0.0198214222819848712 	-0.0201756113906326681 	0.0652202272548568773 	-0.00808872697198904402 	-0.0434649107228186893 	
+0.0220469976060823913 	0.0577601230060890841 	0.0931977532934198521 	0.0772346664469984456 	-0.0598475777233471104 	0.0546670321583845528 	-0.0656482723600904788 	0.105148703985770878 	0.0816324277971470502 	-0.034008298213594905 	
+0.0559073669022559505 	0.0115499190459667361 	-0.00227637075738397953 	0.0260368330527698359 	0.0797145175003680651 	0.0616751385907746905 	0.0874564372453314892 	0.0775559584151787468 	-0.0300766064678407267 	0.0666863867569315871 	
+0.107547133094365602 	-0.06615971756186316 	-0.0750312873175343065 	-0.0596465291139990014 	-0.0148864131111699893 	0.00654354685106273929 	0.0532968302730689736 	-0.0224559145729239934 	-0.0527039145134178821 	0.0178677635211209572 	
+0.0963923033747754177 	0.00717344875208558285 	0.038453036435484346 	-0.0852305729109014559 	0.034997475493846901 	0.0441776047503394317 	0.0605526140911737121 	0.00112664200986311073 	-0.0087323645914181066 	0.0898344769753848943 	
+-0.0534115895805138685 	0.0181249695940979144 	0.00878363003094958898 	0.0455466177148464713 	-0.0588178514773917208 	0.0358653651595036427 	0.0822422818408273942 	-0.0774890541600476573 	0.0369472008605820357 	0.0465351635632989744 	
+0.0708749948360742482 	0.0941472365254581445 	0.00226708912281539015 	-0.0861419439496607375 	0.019994648121601559 	-0.0806222482550226077 	0.0629605223199721231 	0.0103389001022006766 	-0.0324336760051576259 	0.0658303459011729958 	
+0.0955803387142432764 	-0.0717037539477126534 	0.0603093016230788898 	-0.0712723826989075937 	-0.0493296803767544825 	-0.0128637489358975829 	0.019571584664774419 	-0.047704226654149251 	0.00212767938104397062 	0.111425073998696736 	
+0.0658839890321762239 	-0.0295628146900774498 	0.0200661450461363071 	0.00131485756584520793 	-0.0849866013677955529 	0.0404686078881808797 	0.0671671998381296737 	0.0900259681327555128 	0.0937799245423497768 	-0.0322888665769429373 	
+-0.0121936407558599861 	-0.0870625365848600175 	-0.0219153724537396698 	0.0300269070805505404 	0.0679963006559635846 	0.00794668553911313802 	0.0103240085384890892 	-0.0340401721806524588 	-0.0702681992831467039 	-0.00124906146825614402 	
+0.0730247717053444517 	0.0339815257289185907 	-0.0198488554474144201 	0.103214962249267767 	-0.0214835899446404049 	0.0867021686659173174 	0.0484463412604088373 	0.00943655047359576518 	0.00817973246608490537 	-0.0738347947439276786 	
+-0.000896007013211109712 	0.0730742423097294758 	0.109410823441072003 	-0.058926725913585061 	0.0531233217724755361 	-0.0135355590753920736 	0.0967440741624007172 	0.0663206959641660238 	-0.00549436838177645107 	0.0894934520821886043 	
+-0.0271782661674656624 	0.102020433000738431 	-0.0819332552441457418 	-0.0157483850508686046 	0.064210069652768223 	-0.0753445699775463495 	0.0941361076333349228 	0.0735604700520547661 	0.0571258886864093873 	-0.0359806897455684782 	
+-0.0705536054527852591 	-0.0822073384032595555 	-0.00112150171006669078 	-0.0800304259132479517 	-0.0232782089529210909 	-0.0617095838930734122 	-0.0149387116402193657 	-0.019939478343676955 	-0.0456782844612378808 	-0.0684025863257921724 	
+0.0193421802849989069 	-0.0357321956878843064 	-0.00496072883196820719 	0.0406104609691799456 	0.0413136602383304824 	0.0030607967287135285 	0.0777421250184955892 	-0.0107018642013677126 	0.00358113616845277642 	0.0913720102624298036 	
+0.0139232091273168383 	0.104943154693448948 	-0.024211849798968503 	0.0163840269683069896 	0.0765070145006150615 	0.0958150456763250669 	-0.0727019189383650705 	-0.000419746250886005143 	-0.00416409232701833026 	0.0121238470826693749 	
+0.0651230092492420531 	0.0204138782025374832 	-0.0693453796121564769 	0.0677844774416462015 	-0.0508879293537423186 	-0.0634646977857465378 	-0.00656173557114148043 	-0.0559936636352839287 	-0.0424628758847008098 	0.0985469407427352895 	
+-0.0769886117618060889 	0.032501820267599818 	0.0663893309678701693 	-0.0278876471174986304 	0.09290454028905136 	0.047499824752879026 	-0.0257978670799245136 	0.00476154287973441261 	0.0801698203420577182 	-0.0788382104339286766 	
+0.0547014778198773449 	0.00946833203311331709 	-0.00726462752600475284 	0.0723254716201774117 	0.0205615977056121668 	-0.0367530694409559724 	0.0934301107854380308 	-0.0479505446674512101 	-0.0160957470058741732 	0.0694328484699303916 	
+-0.0595876310745595203 	-0.0383477302239165554 	-0.041792377416207567 	-0.0511609147197889733 	-0.0479087444457946612 	0.0225203087610199373 	0.0199693634725238434 	0.0534311524927292766 	-0.0731815681391047718 	-0.0167811292054479699 	
+0.0905792518376025008 	0.0326154103319607688 	0.0572041638604988342 	-0.018205789586669973 	0.0646721485482818076 	0.0983918662842096087 	0.0525943899156008782 	-0.0843735568830600302 	0.0307371132750456633 	-0.0295980138681536109 	
+-0.00504787451744055388 	-0.0256729736995633048 	-0.00942989152080438674 	0.0108273404656922777 	0.052692591304093539 	0.106240139148546311 	0.0235946303473424206 	-0.0263567496424631023 	0.107147791099797343 	0.0376929710309186969 	
+0.0308158526794357172 	0.0686749627074960817 	0.051394066714902667 	0.0520384058598451563 	-0.00530110123048805216 	-0.00537524554129623511 	0.000916187426836807383 	0.101704735597045612 	-0.0435904190560880353 	0.0439280282078316278 	
+0.0197027542880891943 	0.00282764625979796793 	-0.0624133168796937102 	-0.000838184694977097311 	-0.0361146913489463206 	0.0662115725869421717 	-0.00779921545030481337 	-0.0527288120519812664 	-0.0341886177796776117 	0.0143417420339569616 	
+-0.0328006992308578008 	0.081317459428094846 	0.0601856776111195022 	0.0848767764431867661 	0.0710419417995348801 	0.0809102300225778281 	0.0998703827619521617 	0.0325526540674225887 	-0.0392016732023402412 	-0.0302138858715109392 	
+0.00889465585724648171 	0.077477696001067492 	-0.0485175066414528372 	-0.0567270884410531404 	0.0559349667575460771 	0.0814817504417899435 	0.0906402150722598332 	0.0950341870525589771 	0.0255652886676162247 	0.106900329777593742 	
+0.00862267254461242597 	0.0549555002798989348 	0.0604570145304529349 	-0.0903770014132604521 	0.0753477814899700116 	0.0600692564159891404 	-0.0651932303590461171 	0.0567607535996484669 	-0.0304033932632605079 	-0.0723456062223043483 	
+0.00565037015319678603 	0.0228489105928545225 	0.0865681102397283381 	0.0483345487266812757 	0.0106269381230428006 	0.0989198164244115136 	0.0681344486795087273 	-0.0600214201563913155 	0.0474228192206973403 	0.0920626419468070084 	
+0.0165113020903557145 	0.0177311826089620733 	0.0505398087175398941 	0.00211791774586568646 	-0.036100150122085263 	-0.0181649815856086451 	0.0022124792358270153 	-0.0425889963615622771 	0.0265007825372027585 	0.0131443579397779835 	
+-0.0653153889137650934 	0.0771867556581843917 	0.0532321696744717313 	0.020319065629383893 	-0.0773921069536342432 	0.0624205572191128336 	0.110997722997582135 	-0.0272642040811898398 	0.0435103308168569639 	-0.0588413621392204281 	
+-0.00187058312404764335 	0.0163102571290564581 	0.0661237839240759878 	-0.0459968476517004579 	0.0799619050368084033 	-0.0255381660134464394 	0.0393167535168238019 	-0.000428726455830321664 	0.0704347503703292022 	-0.0265693858624537968 	
 ]
 ;
 gibbs_ma_schedule = []
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
+L1_penalty_factor = 0 ;
+L2_penalty_factor = 0 ;
+L2_decrease_constant = 0 ;
+L2_shift = 100 ;
+L2_decrease_type = "one_over_t" ;
+L2_n_updates = 0 ;
 down_size = 10 ;
 up_size = 100 ;
 learning_rate = 0.00100000000000000002 ;
@@ -251,12 +288,16 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 reconstruction_connection = *0 ;
+stochastic_reconstruction = 0 ;
 grad_learning_rate = 0.0100000000000000002 ;
 cd_learning_rate = 0.00100000000000000002 ;
+tied_connection_weights = 0 ;
 compute_contrastive_divergence = 0 ;
+deterministic_reconstruction_in_cd = 0 ;
 standard_cd_grad = 1 ;
 standard_cd_bias_grad = 1 ;
 standard_cd_weights_grad = 1 ;
@@ -274,7 +315,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *12 ->GradNNetLayerModule(
 start_learning_rate = 0.0100000000000000002 ;
 decrease_constant = 0 ;
@@ -287,25 +329,27 @@
 L1_penalty_factor = 0 ;
 L2_penalty_factor = 0 ;
 weights = 2  100  [ 
--0.00328975223759890838 	-0.00346826422071847646 	0.00907581277511097101 	0.00347136775228049088 	0.0042871187908744314 	0.00343990328209323746 	-0.00714654471289515817 	-0.00784111630398876686 	-0.00372907859368766569 	-0.00973366008345864786 	-0.000688643740409981982 	-0.00956428308641964382 	-0.000897958259188700041 	0.00449082326757663242 	0.0041337330985273214 	-0.0085686570969960036 	0.00218937131657726642 	-0.00723381293808018216 	0.0053319848721854789 	-0.00375811070933532903 	-0.00238323519353948309 	0.000316195355204442862 	0.00808973600787776456 	0.00593774438444158621 	0.000253541476231059718 	-0.00515489325378380193 	0.00766421459345257173 	0.00822820508513122384 	0.00377213064554823407 	0.0068385909867962457 	-0.00641018616097005674 	-0.0106864673431768097 	0.000268021179583320559 	-0.000829993432607160496 	0.00571896098541352284 	0.00752085871272469789 	-0.00839566831792106123 	0.00265450762386824175 	0.00316020877807904679 	-0.000855220153014026431 	-0.00216!
 972224943933921 	-0.000391017313711675466 	-0.00369284201163942337 	0.00376187812233940398 	-0.00729078327222553498 	-0.00881831736733464273 	0.00915501757283917507 	-0.00040865373133016226 	0.00616774748917324998 	0.000512820143645118789 	-0.00200929522244623886 	-0.00131110755932426068 	0.00688874424070522705 	0.00277139637559157539 	-0.00513924785431744014 	0.00412479945516167935 	0.00459517600763888635 	0.00909708604687939695 	0.00724915463143394664 	-0.00841163282039994464 	0.00949616967726132563 	-0.00786567154882882556 	-0.00249471971964055489 	-0.00100552161343312255 	0.00496276761900886834 	-0.000137242292421134546 	-0.00464854749180680302 	-0.00454994604220504157 	-0.00298497966000130189 	-0.00578020370355490816 	-0.00207397345344491335 	-0.007298667005837642 	0.00229286690879886923 	-0.00940271360625089155 	-0.00356147796776346481 	-0.00275095748254701586 	0.000675238932615726604 	-0.00412192037523308099 	0.00773887852981238692 	0.00737895208119485405 	0.00588435!
 370720583484 	-0.000137416867987901782 	-0.0024894531471409070!
 8 	-0.00
357732556298006417 	-0.00284391451132938976 	-0.000963057691869287791 	-0.00385410792476033803 	-0.00745729185583782813 	-0.00396037228324378019 	0.00635389739462362095 	-0.00226757216711522295 	0.00333486553739848054 	0.00513589831121753163 	0.000145886258715480888 	-0.00366597445275700399 	-0.00794414559264285634 	-0.0103289536804432973 	0.00197890813580487834 	0.00840111384503721033 	-0.00230698117710193733 	
--0.00460838467058952757 	-0.0055951631251087517 	0.00629370508861714674 	3.17175210599941552e-06 	0.00212196706773883125 	0.00906106888363287545 	-0.00494090008985100925 	-0.00786340487427916908 	-0.00663248367512082968 	0.00221133556888333457 	-0.000764577262183648932 	0.00711661402738147843 	0.00653437410397188657 	0.00186157992427295866 	0.00555173434117631811 	0.00464672449562609276 	0.00201738971669697094 	-0.00938422630204910173 	0.00934196369517516917 	-0.00297069292717086703 	-0.00136512848666674701 	0.0066209351388387969 	-0.00693572465030210545 	0.00383214047570147771 	-0.00661373954000695971 	0.00558062574345910209 	0.00106034348523132544 	-0.00759988452899723263 	0.00726210156947633412 	-0.0100675548005407529 	-0.00898213407428978192 	0.0104619671284957843 	-0.00712070007830524748 	-0.00176675514109837914 	0.00173294787307944564 	0.00697238275571979323 	0.0019218724205936654 	0.00363896119140629144 	-0.0059384461982132878 	0.00844486362954492351 	3.8151704414179!
 9211e-05 	-0.00170251018959995028 	0.00952580948110814604 	-0.00525064145377502397 	0.00274419194469983918 	0.000924224294361454123 	-0.000782652466941984645 	-0.000525230976880659275 	0.00723962564580487699 	0.00957398630080609145 	0.00808704257673019053 	0.00213418186847960309 	0.00395201754189624006 	-0.00328623733888816104 	0.00102014268756182316 	0.00518266329484105837 	-0.00470254090898037606 	0.0095732760102849191 	-0.00657409842723416082 	-0.00490719272532080981 	0.00673996735804150305 	0.0035421813007671275 	0.00215955814079904712 	0.00779794731762177996 	0.000757000874427418355 	-0.00319515954293849748 	-0.00697348948772691726 	0.00432788225585485978 	0.0022782734825580477 	-0.000644324591619325733 	-0.00373558399842165764 	-0.00979774755554577506 	0.00683078300501879614 	0.000235504985903817014 	-0.00103446807782896352 	0.005581426951155548 	0.00397699169987568608 	0.00837802411251285968 	-0.00190818278893380025 	-0.00480317664506915362 	-0.00506929489098148847 	!
 0.00896268165994594523 	0.00812080688120202232 	-0.00966209487!
 05486674
2 	0.000242160898349140031 	0.00048762049776032853 	-0.00491695641258263656 	-0.00386474898142027783 	-0.00243607516015952041 	-0.00493162594550476414 	-0.00227350969820288651 	-0.00969841130070480013 	0.00125242734275270943 	0.00357348062793753788 	0.00318233737781711649 	0.00111745689880651897 	0.00716664206222697615 	0.00575890830134267572 	0.0086281104072910824 	-0.00540387875676610549 	
+-0.00328975223759890838 	-0.00346826422071847386 	0.00907581277511096927 	0.00347136775228049522 	0.0042871187908744314 	0.00343990328209323876 	-0.00714654471289517205 	-0.00784111630398875818 	-0.00372907859368767089 	-0.00973366008345865306 	-0.000688643740409980031 	-0.00956428308641964382 	-0.000897958259188707739 	0.00449082326757663762 	0.00413373309852732053 	-0.00856865709699599493 	0.00218937131657726686 	-0.00723381293808018216 	0.00533198487218548237 	-0.00375811070933532556 	-0.00238323519353948353 	0.000316195355204444001 	0.0080897360078777663 	0.00593774438444158188 	0.000253541476231059827 	-0.00515489325378380627 	0.00766421459345257086 	0.00822820508513121343 	0.0037721306455482345 	0.00683859098679624484 	-0.00641018616097006541 	-0.0106864673431768097 	0.000268021179583323649 	-0.000829993432607162339 	0.00571896098541351677 	0.00752085871272470743 	-0.00839566831792105603 	0.00265450762386824262 	0.00316020877807904549 	-0.000855220153014026431 	-0.002!
 16972224943933834 	-0.000391017313711678556 	-0.00369284201163941816 	0.00376187812233940572 	-0.00729078327222553758 	-0.00881831736733465141 	0.00915501757283916293 	-0.000408653731330169904 	0.00616774748917325259 	0.000512820143645118355 	-0.0020092952224462419 	-0.00131110755932426501 	0.00688874424070522184 	0.00277139637559157452 	-0.0051392478543174254 	0.00412479945516167935 	0.00459517600763888635 	0.00909708604687940216 	0.00724915463143395097 	-0.00841163282039994291 	0.00949616967726132043 	-0.00786567154882882036 	-0.00249471971964054968 	-0.00100552161343312645 	0.00496276761900886921 	-0.00013724229242113395 	-0.00464854749180679868 	-0.0045499460422050433 	-0.00298497966000130623 	-0.00578020370355489602 	-0.00207397345344491118 	-0.00729866700583764027 	0.00229286690879886837 	-0.00940271360625089155 	-0.00356147796776346004 	-0.00275095748254702236 	0.000675238932615725086 	-0.00412192037523307752 	0.00773887852981239559 	0.00737895208119485058 	0.0058843!
 5370720583311 	-0.000137416867987903734 	-0.002489453147140912!
 71 	-0.0
0357732556298005896 	-0.00284391451132938455 	-0.000963057691869292345 	-0.00385410792476033543 	-0.00745729185583782466 	-0.00396037228324377846 	0.00635389739462362269 	-0.00226757216711522599 	0.00333486553739848184 	0.00513589831121753077 	0.000145886258715480265 	-0.00366597445275700312 	-0.00794414559264286502 	-0.0103289536804433008 	0.00197890813580488094 	0.00840111384503720166 	-0.00230698117710193343 	
+-0.00460838467058953018 	-0.0055951631251087517 	0.00629370508861714327 	3.17175210600269481e-06 	0.00212196706773883255 	0.00906106888363286851 	-0.00494090008985101446 	-0.00786340487427916734 	-0.00663248367512083055 	0.00221133556888334151 	-0.000764577262183650992 	0.00711661402738146803 	0.0065343741039718831 	0.00186157992427296299 	0.00555173434117632678 	0.00464672449562609103 	0.00201738971669697398 	-0.00938422630204909305 	0.00934196369517516917 	-0.00297069292717086703 	-0.00136512848666674918 	0.00662093513883879516 	-0.00693572465030210111 	0.00383214047570148552 	-0.00661373954000695537 	0.00558062574345910816 	0.00106034348523132479 	-0.00759988452899722743 	0.00726210156947632891 	-0.0100675548005407459 	-0.00898213407428978886 	0.0104619671284957895 	-0.00712070007830524661 	-0.00176675514109838261 	0.00173294787307945409 	0.00697238275571979323 	0.00192187242059366713 	0.00363896119140629014 	-0.00593844619821328953 	0.00844486362954492698 	3.81517044141!
 789724e-05 	-0.0017025101895999518 	0.00952580948110815298 	-0.00525064145377502744 	0.00274419194469983788 	0.000924224294361456833 	-0.000782652466941982693 	-0.000525230976880658625 	0.00723962564580487179 	0.00957398630080610359 	0.00808704257673020267 	0.00213418186847960873 	0.00395201754189624267 	-0.0032862373388881619 	0.00102014268756182012 	0.00518266329484107051 	-0.00470254090898037779 	0.00957327601028492084 	-0.00657409842723416342 	-0.00490719272532080981 	0.00673996735804151086 	0.00354218130076713184 	0.00215955814079904192 	0.00779794731762178516 	0.000757000874427418355 	-0.00319515954293849184 	-0.006973489487726919 	0.0043278822558548511 	0.00227827348255805204 	-0.000644324591619327034 	-0.00373558399842165547 	-0.00979774755554577506 	0.00683078300501879961 	0.000235504985903814033 	-0.00103446807782896309 	0.00558142695115555407 	0.00397699169987569129 	0.00837802411251285274 	-0.0019081827889338009 	-0.00480317664506916056 	-0.00506929489098149454 !
 	0.0089626816599459383 	0.00812080688120203099 	-0.00966209487!
 05486604
8 	0.000242160898349140844 	0.000487620497760336011 	-0.00491695641258263656 	-0.00386474898142028477 	-0.00243607516015951998 	-0.00493162594550475893 	-0.00227350969820288434 	-0.00969841130070481748 	0.00125242734275270422 	0.00357348062793753181 	0.00318233737781711389 	0.00111745689880651724 	0.00716664206222697962 	0.00575890830134267919 	0.0086281104072910824 	-0.00540387875676610549 	
 ]
 ;
-bias = 2 [ -0.000388256263229395493 0.000388256263229405359 ] ;
+bias = 2 [ -0.000388256263229393704 0.000388256263229400642 ] ;
 input_size = 100 ;
 output_size = 2 ;
 name = "affine_net" ;
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *13 ->SoftmaxModule(
 input_size = 2 ;
 name = "softmax" ;
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *14 ->NLLCostModule(
 target_size = 1 ;
 input_size = 2 ;
@@ -314,7 +358,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *15 ->ClassErrorCostModule(
 target_size = 1 ;
 input_size = 2 ;
@@ -323,7 +368,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *16 ->ArgmaxModule(
 input_size = -1 ;
 output_size = 1 ;
@@ -331,7 +377,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 *17 ->SquaredErrorCostModule(
 target_size = 1 ;
 input_size = 1 ;
@@ -340,7 +387,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ] ;
 connections = 7 [ *18 ->NetworkConnection(
 source = "rbm_1.hidden.state" ;
@@ -377,7 +425,8 @@
 use_fast_approximations = 1 ;
 estimate_simpler_diag_hessian = 0 ;
 expdir = "" ;
-random_gen = *5   )
+random_gen = *5  ;
+verbosity = 1  )
 ;
 batch_size = 11 ;
 reset_seed_upon_train = 0 ;
@@ -386,7 +435,9 @@
 target_ports = 3 [ "nll.target" "class_error.target" "mse.target" ] ;
 weight_ports = []
 ;
+operate_on_bags = 0 ;
 mbatch_size = 11 ;
+random_gen = *5  ;
 seed = 1827 ;
 stage = 1001 ;
 n_examples = 200 ;

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_TwoRBMs/expected_results/expdir-tester2/split_stats.pmat
===================================================================
(Binary files differ)



From tihocan at mail.berlios.de  Mon May 26 16:13:18 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 26 May 2008 16:13:18 +0200
Subject: [Plearn-commits] r9055 - trunk/plearn/ker
Message-ID: <200805261413.m4QEDI0F008020@sheep.berlios.de>

Author: tihocan
Date: 2008-05-26 16:13:18 +0200 (Mon, 26 May 2008)
New Revision: 9055

Modified:
   trunk/plearn/ker/GaussianDensityKernel.cc
Log:
Used pl_log instead of log

Modified: trunk/plearn/ker/GaussianDensityKernel.cc
===================================================================
--- trunk/plearn/ker/GaussianDensityKernel.cc	2008-05-26 14:11:27 UTC (rev 9054)
+++ trunk/plearn/ker/GaussianDensityKernel.cc	2008-05-26 14:13:18 UTC (rev 9055)
@@ -51,7 +51,7 @@
                         "");
 
 real GaussianDensityKernel::evaluate(const Vec& x1, const Vec& x2) const
-{ return exp(-real(0.5)*powdistance(x1, x2, real(2.0))/(sigma*sigma) - x1.length()*(0.5*Log2Pi + log(exp(1.0),sigma))); }
+{ return exp(-real(0.5)*powdistance(x1, x2, real(2.0))/(sigma*sigma) - x1.length()*(0.5*Log2Pi + pl_log(sigma))); }
 
 void GaussianDensityKernel::declareOptions(OptionList& ol)
 {



From tihocan at mail.berlios.de  Mon May 26 16:14:01 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 26 May 2008 16:14:01 +0200
Subject: [Plearn-commits] r9056 - trunk/plearn_learners/online
Message-ID: <200805261414.m4QEE1Kj008242@sheep.berlios.de>

Author: tihocan
Date: 2008-05-26 16:14:00 +0200 (Mon, 26 May 2008)
New Revision: 9056

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
Log:
Minor fixes to help text

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-05-26 14:13:18 UTC (rev 9055)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-05-26 14:14:00 UTC (rev 9056)
@@ -340,6 +340,9 @@
     inherited::forget();
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void RBMGaussianLayer::declareOptions(OptionList& ol)
 {
     declareOption(ol, "min_quad_coeff", &RBMGaussianLayer::min_quad_coeff,
@@ -352,7 +355,7 @@
 
     declareOption(ol, "sigma", &RBMGaussianLayer::sigma,
                   OptionBase::learntoption,
-                  "comments...");
+                  "Standard deviations.");
 
     declareOption(ol, "share_quad_coeff", &RBMGaussianLayer::share_quad_coeff,
                   OptionBase::buildoption,
@@ -366,7 +369,7 @@
                   "if it should not be learned.\n"
                   "This will fix the value of the quad coeffs to the "
                   "appropriate value.\n"
-                  "If < 0, then this option is ignored.\n");
+                  "If <= 0, then this option is ignored.\n");
 
     declareOption(ol, "compute_mse_instead_of_nll", &RBMGaussianLayer::compute_mse_instead_of_nll,
                   OptionBase::buildoption,
@@ -379,6 +382,9 @@
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void RBMGaussianLayer::build_()
 {
     bool needs_forget = false;
@@ -414,6 +420,9 @@
     clearStats();
 }
 
+///////////
+// build //
+///////////
 void RBMGaussianLayer::build()
 {
     inherited::build();

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2008-05-26 14:13:18 UTC (rev 9055)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2008-05-26 14:14:00 UTC (rev 9056)
@@ -64,9 +64,6 @@
     //! or 1 when share_quad_coeff is True
     int size_quad_coeff;
 
-    //! Value for the usually learned standard deviation, if it should not be learned.
-    //! This will fix the value of the quad coeffs to the appropriate value.
-    //! If < 0, then this option is ignored.
     real fixed_std_deviation;
 
     //! Indication that fpropNLL should compute the MSE instead of the NLL.



From tihocan at mail.berlios.de  Mon May 26 17:15:45 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 26 May 2008 17:15:45 +0200
Subject: [Plearn-commits] r9057 - trunk/plearn_learners/online
Message-ID: <200805261515.m4QFFj6F018989@sheep.berlios.de>

Author: tihocan
Date: 2008-05-26 17:15:45 +0200 (Mon, 26 May 2008)
New Revision: 9057

Modified:
   trunk/plearn_learners/online/RBMConnection.cc
   trunk/plearn_learners/online/RBMConnection.h
Log:
Added ability to call build_ in constructor

Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2008-05-26 14:14:00 UTC (rev 9056)
+++ trunk/plearn_learners/online/RBMConnection.cc	2008-05-26 15:15:45 UTC (rev 9057)
@@ -54,7 +54,8 @@
 ///////////////////
 // RBMConnection //
 ///////////////////
-RBMConnection::RBMConnection( real the_learning_rate ) :
+RBMConnection::RBMConnection(real the_learning_rate, bool call_build_):
+    inherited("", call_build_),
     learning_rate(the_learning_rate),
     momentum(0.),
     down_size(-1),
@@ -63,6 +64,8 @@
     pos_count(0),
     neg_count(0)
 {
+    if (call_build_)
+        build_();
 }
 
 ////////////////////

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2008-05-26 14:14:00 UTC (rev 9056)
+++ trunk/plearn_learners/online/RBMConnection.h	2008-05-26 15:15:45 UTC (rev 9057)
@@ -85,7 +85,7 @@
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
-    RBMConnection( real the_learning_rate=0. );
+    RBMConnection(real the_learning_rate = 0, bool call_build_ = false);
 
     // Your other public member functions go here
 



From tihocan at mail.berlios.de  Mon May 26 17:16:15 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 26 May 2008 17:16:15 +0200
Subject: [Plearn-commits] r9058 - trunk/plearn_learners/online
Message-ID: <200805261516.m4QFGFER019023@sheep.berlios.de>

Author: tihocan
Date: 2008-05-26 17:16:14 +0200 (Mon, 26 May 2008)
New Revision: 9058

Modified:
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
Log:
Implemented proper building in constructor

Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-05-26 15:15:45 UTC (rev 9057)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-05-26 15:16:14 UTC (rev 9058)
@@ -50,11 +50,13 @@
 
 RBMMatrixTransposeConnection::RBMMatrixTransposeConnection( 
     PP<RBMMatrixConnection> the_rbm_matrix_connection,
-    real the_learning_rate ) :
-    inherited(the_learning_rate), 
+    real the_learning_rate,
+    bool call_build_) :
+    inherited(the_learning_rate, call_build_),
     rbm_matrix_connection(the_rbm_matrix_connection)
 {
-    build();
+    if (call_build_)
+        build_();
 }
 
 void RBMMatrixTransposeConnection::declareOptions(OptionList& ol)

Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-05-26 15:15:45 UTC (rev 9057)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-05-26 15:16:14 UTC (rev 9058)
@@ -79,10 +79,11 @@
 public:
     //#####  Public Member Functions  #########################################
 
-    //! Default constructor
+    //! Default constructor.
     RBMMatrixTransposeConnection( 
         PP<RBMMatrixConnection> the_rbm_matrix_connection = 0,
-        real the_learning_rate=0 );
+        real the_learning_rate = 0,
+        bool call_build_ = false);
 
     // Your other public member functions go here
 



From tihocan at mail.berlios.de  Mon May 26 17:16:39 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 26 May 2008 17:16:39 +0200
Subject: [Plearn-commits] r9059 - trunk/plearn/base
Message-ID: <200805261516.m4QFGddV019060@sheep.berlios.de>

Author: tihocan
Date: 2008-05-26 17:16:39 +0200 (Mon, 26 May 2008)
New Revision: 9059

Modified:
   trunk/plearn/base/Option.h
Log:
Removed warning output that was causing too many tests to fail

Modified: trunk/plearn/base/Option.h
===================================================================
--- trunk/plearn/base/Option.h	2008-05-26 15:16:14 UTC (rev 9058)
+++ trunk/plearn/base/Option.h	2008-05-26 15:16:39 UTC (rev 9059)
@@ -367,7 +367,10 @@
                                                       defaultval, description, level));
 }
 
-// Overload for pointer to static member
+//! Overload for pointer to static member.
+//! Note that the code to declare static options has not been thoroughly tested
+//! and thus may contain some bugs (especially with the Python interface). Use
+//! at your own risk!
 template <class OptionType>
 inline void declareStaticOption(OptionList& ol,                      //!< list to which this option should be appended 
                           const string& optionname,            //!< the name of this option
@@ -377,7 +380,6 @@
                           const OptionBase::OptionLevel level= OptionBase::default_level, //!< Option level (see OptionBase)
                           const string& defaultval="")         //!< default value for this option, as set by the default constructor
 {
-    perr<<"declareStaticOption() is not considered debuged, use at you own risk"<<endl;
     ol.push_back(new StaticOption<OptionType>(optionname, ptr, flags, 
                                                     TypeTraits<OptionType>::name(), 
                                                     defaultval, description, level));



From nouiz at mail.berlios.de  Mon May 26 18:38:12 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 26 May 2008 18:38:12 +0200
Subject: [Plearn-commits] r9060 - trunk/python_modules/plearn/parallel
Message-ID: <200805261638.m4QGcCT3004733@sheep.berlios.de>

Author: nouiz
Date: 2008-05-26 18:38:09 +0200 (Mon, 26 May 2008)
New Revision: 9060

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
corrected comment and the filesize parameter for condor(condor need kb size)


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-05-26 15:16:39 UTC (rev 9059)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-05-26 16:38:09 UTC (rev 9060)
@@ -846,7 +846,7 @@
 
         if self.mem<=0:
             try:
-                self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size
+                self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size/1024
             except:
                 pass
         condor_dat.write( dedent('''\
@@ -1385,7 +1385,7 @@
     try:
         jobs = eval('DBI'+launch_system+'(commands,**args)')
     except NameError:
-        print 'The launch system ',launch_system, ' does not exists. Available systems are: Cluster, Ssh, bqtools and Condor'
+        print 'The launch system ',launch_system, ' does not exists. Available systems are: Cluster, Ssh, Bqtools and Condor'
         traceback.print_exc()
         sys.exit(1)
     return jobs



From louradou at mail.berlios.de  Mon May 26 20:37:47 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 26 May 2008 20:37:47 +0200
Subject: [Plearn-commits] r9061 - trunk/plearn/vmat
Message-ID: <200805261837.m4QIbl3w023861@sheep.berlios.de>

Author: louradou
Date: 2008-05-26 20:37:46 +0200 (Mon, 26 May 2008)
New Revision: 9061

Modified:
   trunk/plearn/vmat/ClassSubsetVMatrix.cc
   trunk/plearn/vmat/ClassSubsetVMatrix.h
   trunk/plearn/vmat/KFoldSplitter.cc
   trunk/plearn/vmat/KFoldSplitter.h
Log:
- added an option to keep the same class frequencies
  in the splits of a KFoldSplitter



Modified: trunk/plearn/vmat/ClassSubsetVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ClassSubsetVMatrix.cc	2008-05-26 16:38:09 UTC (rev 9060)
+++ trunk/plearn/vmat/ClassSubsetVMatrix.cc	2008-05-26 18:37:46 UTC (rev 9061)
@@ -3,6 +3,7 @@
 // ClassSubsetVMatrix.cc
 //
 // Copyright (C) 2004 Olivier Delalleau
+// Copyright (C) 2008 Jerome Louradour
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -57,6 +58,22 @@
   // build_();
 }
 
+ClassSubsetVMatrix::ClassSubsetVMatrix(VMat the_source, TVec<int> the_classes)
+  : redistribute_classes(0), one_vs_minus_one_classification(0)
+{
+  source = the_source;
+  CopiesMap copies;
+  classes = the_classes.deepCopy(copies);
+}
+
+ClassSubsetVMatrix::ClassSubsetVMatrix(VMat the_source, int the_class)
+  : redistribute_classes(0), one_vs_minus_one_classification(0)
+{
+  source = the_source;
+  classes = TVec<int>(1, the_class);
+}
+
+
 PLEARN_IMPLEMENT_OBJECT(ClassSubsetVMatrix,
     "A VMatrix that keeps examples for a subset of the classes (target).",
     ""
@@ -114,9 +131,8 @@
       if(classes.find((int)target[0]) != -1)
         indices.push_back(i);
     }
-
-    if(indices.length() == 0)
-      PLERROR("In ClassSubsetVMatrix::build_(): no examples kept");
+    //if(indices.length() == 0)
+    //  PLERROR("In ClassSubsetVMatrix::build_(): no examples kept");
     inherited::build();
     if(one_vs_minus_one_classification && classes.length()!=2)
       PLERROR("In ClassSubsetVMatrix::build_(): no examples kept");

Modified: trunk/plearn/vmat/ClassSubsetVMatrix.h
===================================================================
--- trunk/plearn/vmat/ClassSubsetVMatrix.h	2008-05-26 16:38:09 UTC (rev 9060)
+++ trunk/plearn/vmat/ClassSubsetVMatrix.h	2008-05-26 18:37:46 UTC (rev 9061)
@@ -88,6 +88,8 @@
 
   //! Default constructor.
   ClassSubsetVMatrix();
+  ClassSubsetVMatrix(VMat the_source, TVec<int> the_classes);
+  ClassSubsetVMatrix(VMat the_source, int the_class);
 
   // ******************
   // * Object methods *

Modified: trunk/plearn/vmat/KFoldSplitter.cc
===================================================================
--- trunk/plearn/vmat/KFoldSplitter.cc	2008-05-26 16:38:09 UTC (rev 9060)
+++ trunk/plearn/vmat/KFoldSplitter.cc	2008-05-26 18:37:46 UTC (rev 9061)
@@ -5,6 +5,7 @@
 // Copyright (C) 1998 Pascal Vincent
 // Copyright (C) 1999,2000 Pascal Vincent, Yoshua Bengio and University of Montreal
 // Copyright (C) 2002 Frederic Morin
+// Copyright (C) 2008 Jerome Louradour
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -44,6 +45,7 @@
 #include "VMat_basic_stats.h"
 #include <plearn/vmat/ConcatRowsVMatrix.h>
 #include <plearn/vmat/SubVMatrix.h>
+#include <plearn/vmat/ClassSubsetVMatrix.h>
 
 namespace PLearn {
 using namespace std;
@@ -52,6 +54,7 @@
     : append_non_constant_test(false),
       append_train(false),
       include_test_in_train(false),
+      balance_classes(false),
       K(k)
 {
     // Default cross-validation range is the whole dataset.
@@ -87,6 +90,11 @@
     declareOption(ol, "cross_range", &KFoldSplitter::cross_range, OptionBase::buildoption,
                   "The range on which cross-validation is applied (similar to the FractionSplitter ranges).");
 
+    declareOption(ol, "balance_classes", &KFoldSplitter::balance_classes, OptionBase::buildoption,
+                  "Should we balance classes inside the splits to obtain the same class frequencies."
+                  "Note: For this option to work, you have to label your classes from 0 to (n_classes-1),"
+                  "and all classes must be present in the source VMat.");
+
     inherited::declareOptions(ol);
 }
 
@@ -149,6 +157,8 @@
     TVec<VMat> split_(2);
     VMat non_constant_test;
     if (do_partial_cross) {
+        if (balance_classes)
+            PLERROR("balance_classes not implemented with partial_cross");
         VMat sub_data = new SubVMatrix(dataset, i_start, 0, n_cross_data, dataset->width());
         split(sub_data, test_fraction, split_[0], split_[1], k, true);
         non_constant_test = split_[1];
@@ -160,8 +170,33 @@
             VMat constant_test = new SubVMatrix(dataset, i_end, 0, n_data - i_end, dataset->width());
             split_[1] = new ConcatRowsVMatrix(split_[1], constant_test);
         }
-    } else {
-        split(dataset, test_fraction, split_[0], split_[1], k, true);
+    }
+    else {
+        if (balance_classes)
+        {
+            PLASSERT( dataset->targetsize() > 0 );
+            TVec<VMat> tmp_split_(2);
+            int i_class=0;
+            if ( test_fraction > 1.0)
+                test_fraction /= n_cross_data;
+            while (true) { // break point below
+                VMat dataset_class = new ClassSubsetVMatrix(dataset,  i_class );
+                dataset_class->build();
+                int length = dataset_class->length();
+                if (length == 0 ) break;
+                split(dataset_class, test_fraction, tmp_split_[0], tmp_split_[1], k, true);
+                if (i_class == 0) {
+                    CopiesMap copies;
+                    split_ = tmp_split_.deepCopy(copies); //PLearn::deepCopy(visible_layer);
+                } else {
+                    split_[0] = new ConcatRowsVMatrix(split_[0], tmp_split_[0]);
+                    split_[1] = new ConcatRowsVMatrix(split_[1], tmp_split_[1]);
+                }
+                i_class++;
+            }
+        }
+        else
+            split(dataset, test_fraction, split_[0], split_[1], k, true);
         non_constant_test = split_[1];
     }
     if (include_test_in_train)

Modified: trunk/plearn/vmat/KFoldSplitter.h
===================================================================
--- trunk/plearn/vmat/KFoldSplitter.h	2008-05-26 16:38:09 UTC (rev 9060)
+++ trunk/plearn/vmat/KFoldSplitter.h	2008-05-26 18:37:46 UTC (rev 9061)
@@ -68,6 +68,7 @@
     bool append_train;
     pair<real, real> cross_range;
     bool include_test_in_train;
+    bool balance_classes;
     int K;
 
     // ****************



From ducharme at mail.berlios.de  Mon May 26 21:22:01 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Mon, 26 May 2008 21:22:01 +0200
Subject: [Plearn-commits] r9062 - trunk/plearn/vmat
Message-ID: <200805261922.m4QJM14b028124@sheep.berlios.de>

Author: ducharme
Date: 2008-05-26 21:21:57 +0200 (Mon, 26 May 2008)
New Revision: 9062

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Copie du StringMapping dans saveDMAT


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-05-26 18:37:46 UTC (rev 9061)
+++ trunk/plearn/vmat/VMatrix.cc	2008-05-26 19:21:57 UTC (rev 9062)
@@ -1978,6 +1978,7 @@
         pb(i);
     }
     vm.saveFieldInfos();
+    vm.saveAllStringMappings();
 }
 
 //////////////



From tihocan at mail.berlios.de  Tue May 27 15:55:20 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 27 May 2008 15:55:20 +0200
Subject: [Plearn-commits] r9063 - trunk/plearn/vmat
Message-ID: <200805271355.m4RDtK9g007665@sheep.berlios.de>

Author: tihocan
Date: 2008-05-27 15:55:19 +0200 (Tue, 27 May 2008)
New Revision: 9063

Modified:
   trunk/plearn/vmat/SelectRowsVMatrix.cc
Log:
Fixed indentation

Modified: trunk/plearn/vmat/SelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-05-26 19:21:57 UTC (rev 9062)
+++ trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-05-27 13:55:19 UTC (rev 9063)
@@ -47,9 +47,9 @@
 /** SelectRowsVMatrix **/
 
 PLEARN_IMPLEMENT_OBJECT(SelectRowsVMatrix,
-                        "VMat class that selects samples from a source matrix according to given vector of indices.",
-                        ""
-    );
+    "Selects samples from its source according to given vector of indices.",
+    ""
+);
 
 SelectRowsVMatrix::SelectRowsVMatrix()
     : obtained_inputsize_from_source(false),



From tihocan at mail.berlios.de  Tue May 27 15:56:48 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 27 May 2008 15:56:48 +0200
Subject: [Plearn-commits] r9064 - trunk/plearn/vmat
Message-ID: <200805271356.m4RDumUW007738@sheep.berlios.de>

Author: tihocan
Date: 2008-05-27 15:56:47 +0200 (Tue, 27 May 2008)
New Revision: 9064

Modified:
   trunk/plearn/vmat/ClassSubsetVMatrix.cc
   trunk/plearn/vmat/ClassSubsetVMatrix.h
Log:
Made the new convenience constructors build object by default


Modified: trunk/plearn/vmat/ClassSubsetVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ClassSubsetVMatrix.cc	2008-05-27 13:55:19 UTC (rev 9063)
+++ trunk/plearn/vmat/ClassSubsetVMatrix.cc	2008-05-27 13:56:47 UTC (rev 9064)
@@ -47,38 +47,46 @@
 namespace PLearn {
 using namespace std;
 
-////////////////////////////
+PLEARN_IMPLEMENT_OBJECT(ClassSubsetVMatrix,
+    "A VMatrix that keeps examples for a subset of the classes (target).",
+    ""
+);
+
+////////////////////////
 // ClassSubsetVMatrix //
-////////////////////////////
-ClassSubsetVMatrix::ClassSubsetVMatrix()
-  : redistribute_classes(0), one_vs_minus_one_classification(0)
-{
-  // ...
-  // ### You may or may not want to call build_() to finish building the object
-  // build_();
-}
+////////////////////////
+ClassSubsetVMatrix::ClassSubsetVMatrix():
+    redistribute_classes(false),
+    one_vs_minus_one_classification(false)
+{}
 
-ClassSubsetVMatrix::ClassSubsetVMatrix(VMat the_source, TVec<int> the_classes)
-  : redistribute_classes(0), one_vs_minus_one_classification(0)
+ClassSubsetVMatrix::ClassSubsetVMatrix(VMat the_source,
+                                       const TVec<int>& the_classes,
+                                       bool call_build_):
+    // There is no need to explicitely call the parent's constructor since
+    // the whole build process will be done in build_() if necessary.
+    classes(the_classes.copy()),
+    redistribute_classes(false),
+    one_vs_minus_one_classification(false)
 {
   source = the_source;
-  CopiesMap copies;
-  classes = the_classes.deepCopy(copies);
+  if (call_build_)
+      build_();
 }
 
-ClassSubsetVMatrix::ClassSubsetVMatrix(VMat the_source, int the_class)
-  : redistribute_classes(0), one_vs_minus_one_classification(0)
+ClassSubsetVMatrix::ClassSubsetVMatrix(VMat the_source, int the_class,
+                                       bool call_build_):
+    // There is no need to explicitely call the parent's constructor since
+    // the whole build process will be done in build_() if necessary.
+    classes(TVec<int>(1, the_class)),
+    redistribute_classes(false),
+    one_vs_minus_one_classification(false)
 {
-  source = the_source;
-  classes = TVec<int>(1, the_class);
+    source = the_source;
+    if (call_build_)
+        build_();
 }
 
-
-PLEARN_IMPLEMENT_OBJECT(ClassSubsetVMatrix,
-    "A VMatrix that keeps examples for a subset of the classes (target).",
-    ""
-);
-
 ////////////////////
 // declareOptions //
 ////////////////////
@@ -110,7 +118,6 @@
 ///////////
 void ClassSubsetVMatrix::build()
 {
-  // ### Nothing to add here, simply calls build_
   inherited::build();
   build_();
 }

Modified: trunk/plearn/vmat/ClassSubsetVMatrix.h
===================================================================
--- trunk/plearn/vmat/ClassSubsetVMatrix.h	2008-05-27 13:55:19 UTC (rev 9063)
+++ trunk/plearn/vmat/ClassSubsetVMatrix.h	2008-05-27 13:56:47 UTC (rev 9064)
@@ -88,9 +88,16 @@
 
   //! Default constructor.
   ClassSubsetVMatrix();
-  ClassSubsetVMatrix(VMat the_source, TVec<int> the_classes);
-  ClassSubsetVMatrix(VMat the_source, int the_class);
 
+  //! Convenience constructor.
+  //! Note that the vector 'the_classes' is copied and thus may be modified
+  //! afterwards.
+  ClassSubsetVMatrix(VMat the_source, const TVec<int>& the_classes,
+                     bool call_build_ = true);
+
+  //! Convenience constructor.
+  ClassSubsetVMatrix(VMat the_source, int the_class, bool call_build_ = true);
+
   // ******************
   // * Object methods *
   // ******************



From tihocan at mail.berlios.de  Tue May 27 15:57:12 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 27 May 2008 15:57:12 +0200
Subject: [Plearn-commits] r9065 - trunk/plearn/vmat
Message-ID: <200805271357.m4RDvCu4007779@sheep.berlios.de>

Author: tihocan
Date: 2008-05-27 15:57:11 +0200 (Tue, 27 May 2008)
New Revision: 9065

Modified:
   trunk/plearn/vmat/KFoldSplitter.cc
Log:
No need to call build explicitely when using this convenience constructor

Modified: trunk/plearn/vmat/KFoldSplitter.cc
===================================================================
--- trunk/plearn/vmat/KFoldSplitter.cc	2008-05-27 13:56:47 UTC (rev 9064)
+++ trunk/plearn/vmat/KFoldSplitter.cc	2008-05-27 13:57:11 UTC (rev 9065)
@@ -180,8 +180,7 @@
             if ( test_fraction > 1.0)
                 test_fraction /= n_cross_data;
             while (true) { // break point below
-                VMat dataset_class = new ClassSubsetVMatrix(dataset,  i_class );
-                dataset_class->build();
+                VMat dataset_class = new ClassSubsetVMatrix(dataset, i_class);
                 int length = dataset_class->length();
                 if (length == 0 ) break;
                 split(dataset_class, test_fraction, tmp_split_[0], tmp_split_[1], k, true);



From tihocan at mail.berlios.de  Tue May 27 16:48:01 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 27 May 2008 16:48:01 +0200
Subject: [Plearn-commits] r9066 - trunk/plearn/vmat
Message-ID: <200805271448.m4REm1oh014120@sheep.berlios.de>

Author: tihocan
Date: 2008-05-27 16:48:01 +0200 (Tue, 27 May 2008)
New Revision: 9066

Modified:
   trunk/plearn/vmat/KFoldSplitter.cc
Log:
Added some explanations and safety checks for the 'balance_classes' option

Modified: trunk/plearn/vmat/KFoldSplitter.cc
===================================================================
--- trunk/plearn/vmat/KFoldSplitter.cc	2008-05-27 13:57:11 UTC (rev 9065)
+++ trunk/plearn/vmat/KFoldSplitter.cc	2008-05-27 14:48:01 UTC (rev 9066)
@@ -50,6 +50,9 @@
 namespace PLearn {
 using namespace std;
 
+///////////////////
+// KFoldSplitter //
+///////////////////
 KFoldSplitter::KFoldSplitter(int k)
     : append_non_constant_test(false),
       append_train(false),
@@ -73,6 +76,9 @@
                         "after this range will be added to the test set.\n"
     );
 
+////////////////////
+// declareOptions //
+////////////////////
 void KFoldSplitter::declareOptions(OptionList& ol)
 {
     declareOption(ol, "K", &KFoldSplitter::K, OptionBase::buildoption,
@@ -81,34 +87,58 @@
     declareOption(ol, "append_train", &KFoldSplitter::append_train, OptionBase::buildoption,
                   "If set to 1, the trainset will be appended after in the returned sets.");
 
-    declareOption(ol, "append_non_constant_test", &KFoldSplitter::append_non_constant_test, OptionBase::buildoption,
-                  "If set to 1, the non-constant part of the test set will be appended in the returned sets.");
+    declareOption(ol, "append_non_constant_test",
+                  &KFoldSplitter::append_non_constant_test,
+                  OptionBase::buildoption,
+        "If true, the non-constant part of the test set will be appended\n"
+        "in the returned sets. This mostly makes sense when 'cross_range'\n"
+        "is not (0:1).",
+        OptionBase::advanced_level);
 
     declareOption(ol, "include_test_in_train", &KFoldSplitter::include_test_in_train, OptionBase::buildoption,
                   "If set to 1, the test set will be included in the train set.");
 
-    declareOption(ol, "cross_range", &KFoldSplitter::cross_range, OptionBase::buildoption,
-                  "The range on which cross-validation is applied (similar to the FractionSplitter ranges).");
+    declareOption(ol, "cross_range", &KFoldSplitter::cross_range,
+                  OptionBase::buildoption,
+        "The range on which cross-validation is applied (similar to the\n"
+        "FractionSplitter ranges).",
+        OptionBase::advanced_level);
 
-    declareOption(ol, "balance_classes", &KFoldSplitter::balance_classes, OptionBase::buildoption,
-                  "Should we balance classes inside the splits to obtain the same class frequencies."
-                  "Note: For this option to work, you have to label your classes from 0 to (n_classes-1),"
-                  "and all classes must be present in the source VMat.");
+    declareOption(ol, "balance_classes", &KFoldSplitter::balance_classes,
+                  OptionBase::buildoption,
+        "Should we balance classes inside the splits to obtain the same\n"
+        "class frequencies. This corresponds to concatenating the results\n"
+        "of a K-Fold performed on the subsets of examples from each class.\n"
+        "Note that it currently does not support leave-one-out, and that\n"
+        "you might obtain strange results if K > number of samples in one\n"
+        "class.\n"
+        "Note also that for this option to work, you have to label your\n"
+        "classes from 0 to (n_classes-1), and all classes must be present\n"
+        "in the source VMat.");
 
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void KFoldSplitter::build_()
 {
     PLASSERT( K > 0 || K == -1 );
 }
 
+///////////
+// build //
+///////////
 void KFoldSplitter::build()
 {
     inherited::build();
     build_();
 }
 
+/////////////
+// nsplits //
+/////////////
 int KFoldSplitter::nsplits() const
 {
     return K > 0 ? K
@@ -116,6 +146,9 @@
                            : -1;
 }
 
+///////////////////
+// nSetsPerSplit //
+///////////////////
 int KFoldSplitter::nSetsPerSplit() const
 {
     int nsets = 2;
@@ -126,6 +159,9 @@
     return nsets;
 }
 
+//////////////
+// getSplit //
+//////////////
 TVec<VMat> KFoldSplitter::getSplit(int k)
 {
     if (k >= nsplits())
@@ -176,17 +212,23 @@
         {
             PLASSERT( dataset->targetsize() > 0 );
             TVec<VMat> tmp_split_(2);
-            int i_class=0;
-            if ( test_fraction > 1.0)
+            int i_class = 0;
+            if (test_fraction > 1.0)
                 test_fraction /= n_cross_data;
+            else
+                PLERROR("In KFoldSplitter::getSplit - Leave-one-out not implemented");
             while (true) { // break point below
                 VMat dataset_class = new ClassSubsetVMatrix(dataset, i_class);
                 int length = dataset_class->length();
                 if (length == 0 ) break;
+                if (length < K)
+                    PLWARNING("In KFoldSplitter::getSplit - There are less "
+                            "samples from class %d (N = %d) than splits "
+                            "(K = %d): you may get weird results",
+                            i_class, length, K);
                 split(dataset_class, test_fraction, tmp_split_[0], tmp_split_[1], k, true);
                 if (i_class == 0) {
-                    CopiesMap copies;
-                    split_ = tmp_split_.deepCopy(copies); //PLearn::deepCopy(visible_layer);
+                    split_ = tmp_split_.copy();
                 } else {
                     split_[0] = new ConcatRowsVMatrix(split_[0], tmp_split_[0]);
                     split_[1] = new ConcatRowsVMatrix(split_[1], tmp_split_[1]);
@@ -202,8 +244,10 @@
         split_[0] = new ConcatRowsVMatrix(split_[0], split_[1]);
     if (append_train)
         split_.append(split_[0]);
-    if (append_non_constant_test)
+    if (append_non_constant_test) {
+        PLCHECK_MSG(!balance_classes, "Not implemented");
         split_.append(non_constant_test);
+    }
     return split_;
 }
 



From ducharme at mail.berlios.de  Tue May 27 22:21:23 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 27 May 2008 22:21:23 +0200
Subject: [Plearn-commits] r9067 - trunk/plearn/vmat
Message-ID: <200805272021.m4RKLN2v007160@sheep.berlios.de>

Author: ducharme
Date: 2008-05-27 22:21:22 +0200 (Tue, 27 May 2008)
New Revision: 9067

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
La methode 'declareField' est maintenant declare "remote".


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-05-27 14:48:01 UTC (rev 9066)
+++ trunk/plearn/vmat/VMatrix.cc	2008-05-27 20:21:22 UTC (rev 9067)
@@ -201,6 +201,13 @@
          RetDoc ("The content of this VMatrix as a Mat")));
 
     declareMethod(
+        rmm, "declareField", &VMatrix::declareField,
+        (BodyDoc("Declares the field infos for a given column (index).\n"),
+         ArgDoc ("fieldindex", "The column index.\n"),
+         ArgDoc ("fieldname", "The field name of this column.\n"),
+         ArgDoc ("fieldtype", "The field type of this column.\n")));
+
+    declareMethod(
         rmm, "declareFieldNames", &VMatrix::declareFieldNames,
         (BodyDoc("Declares the field names.\n"),
          ArgDoc ("fnames", "TVec of field names.\n")));



From ducharme at mail.berlios.de  Tue May 27 22:24:52 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 27 May 2008 22:24:52 +0200
Subject: [Plearn-commits] r9068 - trunk/python_modules/plearn/pybridge
Message-ID: <200805272024.m4RKOqtQ007278@sheep.berlios.de>

Author: ducharme
Date: 2008-05-27 22:24:52 +0200 (Tue, 27 May 2008)
New Revision: 9068

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
Definition des classes 'VMField' et 'FieldType' (mapping de leur pendant C++).


Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-05-27 20:21:22 UTC (rev 9067)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-05-27 20:24:52 UTC (rev 9068)
@@ -176,3 +176,24 @@
         self.low = low
         self.high = high
         self.rightbracket = rightbracket
+
+class VMField:
+    """
+    To support PLearn<->Python conversion of VMField (which is not a PLearn Object)
+    """
+    def __init__(self, name, fieldtype):
+        self.name = name
+        self.fieldtype = fieldtype
+
+class FieldType:
+    """
+    C-type enum implementation for PLearn::VMField::FieldType
+    """
+    UnknownType    = 0
+    Continuous     = 1
+    DiscrGeneral   = 2
+    DiscrMonotonic = 3
+    DiscrFloat     = 4
+    Date           = 5
+
+# vim: filetype=python:expandtab:shiftwidth=4:tabstop=8:softtabstop=4 :



From ducharme at mail.berlios.de  Tue May 27 22:28:54 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 27 May 2008 22:28:54 +0200
Subject: [Plearn-commits] r9069 - trunk/plearn/python
Message-ID: <200805272028.m4RKSsi6007383@sheep.berlios.de>

Author: ducharme
Date: 2008-05-27 22:28:54 +0200 (Tue, 27 May 2008)
New Revision: 9069

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Implementation des classes 'ConvertToPyObject' et 'ConvertFromPyObject' pour VMField.
Pour que tout fonctionne, il ne reste plus qu'a ecrire/definir les operateurs '<<' et '>>' de VMField.


Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-05-27 20:24:52 UTC (rev 9068)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-05-27 20:28:54 UTC (rev 9069)
@@ -57,6 +57,7 @@
 #include <plearn/base/HelpSystem.h>
 #include <plearn/var/VarArray.h>
 #include <plearn/base/RealMapping.h> // for RealRange
+#include <plearn/vmat/VMField.h>
 
 namespace PLearn {
 using namespace std;
@@ -341,6 +342,26 @@
     return RealRange(leftbracket[0], low, high, rightbracket[0]);
 }
 
+VMField ConvertFromPyObject<VMField>::convert(PyObject* pyobj, bool print_traceback)
+{
+    PLASSERT(pyobj);
+    PyObject* py_name = PyObject_GetAttrString(pyobj, "name");
+    if (!py_name)
+        PLPythonConversionError("ConvertFromPyObject<VMField>: not a VMField (no 'name' attr.)",
+                                pyobj, print_traceback);
+    string name = ConvertFromPyObject<string>::convert(py_name, print_traceback);
+    Py_DECREF(py_name);
+
+    PyObject* py_fieldtype = PyObject_GetAttrString(pyobj, "fieldtype");
+    if(!py_fieldtype) 
+        PLPythonConversionError("ConvertFromPyObject<VMField>: not a VMField (no 'fieldtype' attr.)",
+                                pyobj, print_traceback);
+    VMField::FieldType fieldtype = (VMField::FieldType)ConvertFromPyObject<int>::convert(py_fieldtype, print_traceback);
+    Py_DECREF(py_fieldtype);
+
+    return VMField(name, fieldtype);
+}
+
 template<> int numpyType<bool>()               { return NPY_BOOL; }
 template<> int numpyType<signed char>()        { return NPY_BYTE; }
 template<> int numpyType<unsigned char>()      { return NPY_UBYTE; }
@@ -830,6 +851,28 @@
     return py_rr;
 }
 
+PyObject* ConvertToPyObject<VMField>::newPyObject(const VMField& vmf)
+{
+    string pycode = "\nfrom plearn.pybridge.wrapped_plearn_object import VMField\n";
+    pycode += string("\nresult = VMField(name='") + vmf.name + "', " + "fieldtype=" + tostring(vmf.fieldtype) + ")\n";
+
+    PyObject* env = PyDict_New();
+    if (0 != PyDict_SetItemString(env, "__builtins__", PyEval_GetBuiltins()))
+        PLERROR("In ConvertToPyObject<VMField>::newPyObject : cannot insert builtins in env.");
+    PyObject* res = PyRun_String(pycode.c_str(), Py_file_input, env, env);
+    if (!res)
+    {
+        Py_DECREF(env);
+        if (PyErr_Occurred()) PyErr_Print();
+        PLERROR("In ConvertToPyObject<VMField>::newPyObject : cannot convert to a VMField.");
+    }
+    Py_DECREF(res);
+    PyObject* py_vmf = PythonObjectWrapper(env).as<std::map<string, PyObject*> >()["result"];
+    Py_INCREF(py_vmf);
+    Py_DECREF(env);
+    return py_vmf;
+}
+
 PStream& operator>>(PStream& in, PythonObjectWrapper& v)
 {
     PLERROR("operator>>(PStream&, PythonObjectWrapper&) : "

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2008-05-27 20:24:52 UTC (rev 9068)
+++ trunk/plearn/python/PythonObjectWrapper.h	2008-05-27 20:28:54 UTC (rev 9069)
@@ -84,6 +84,7 @@
 class VMatrix;
 class VarArray;
 class RealRange;
+class VMField;
 
 //! Used for error reporting.  If 'print_traceback' is true, a full
 //! Python traceback is printed to stderr.  Otherwise, raise PLERROR.
@@ -458,6 +459,12 @@
     static RealRange convert(PyObject*, bool print_traceback);
 };
 
+template <>
+struct ConvertFromPyObject<VMField>
+{
+    static VMField convert(PyObject*, bool print_traceback);
+};
+
 /*****
  * Equivalence of types C++ -> numpy
  */
@@ -700,6 +707,9 @@
 template <> struct ConvertToPyObject<RealRange>
 { static PyObject* newPyObject(const RealRange&); };
 
+template <> struct ConvertToPyObject<VMField>
+{ static PyObject* newPyObject(const VMField&); };
+
 struct PLPyClass
 {
     // holds info about a PLearn class



From nouiz at mail.berlios.de  Wed May 28 21:12:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 28 May 2008 21:12:01 +0200
Subject: [Plearn-commits] r9070 - trunk/scripts
Message-ID: <200805281912.m4SJC1Ik002921@sheep.berlios.de>

Author: nouiz
Date: 2008-05-28 21:12:01 +0200 (Wed, 28 May 2008)
New Revision: 9070

Modified:
   trunk/scripts/dbidispatch
Log:
bugfix, we are in test mode only if --test is set


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-27 20:28:54 UTC (rev 9069)
+++ trunk/scripts/dbidispatch	2008-05-28 19:12:01 UTC (rev 9070)
@@ -176,7 +176,6 @@
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice"]:
         dbi_param[argv[2:]]=True
-        testmode=True
     elif argv=="--testdbi":
         dbi_param["test"]=True
     elif argv=="--no_testdbi":



From nouiz at mail.berlios.de  Wed May 28 21:58:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 28 May 2008 21:58:01 +0200
Subject: [Plearn-commits] r9071 - trunk/scripts
Message-ID: <200805281958.m4SJw1Cf007876@sheep.berlios.de>

Author: nouiz
Date: 2008-05-28 21:58:01 +0200 (Wed, 28 May 2008)
New Revision: 9071

Modified:
   trunk/scripts/dbidispatch
Log:
typo


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-05-28 19:12:01 UTC (rev 9070)
+++ trunk/scripts/dbidispatch	2008-05-28 19:58:01 UTC (rev 9071)
@@ -136,7 +136,7 @@
 if search_file('condor_submit',PATH):
     launch_cmd = 'Condor'
 elif search_file('bqsubmit',PATH):
-    launch_cmd = 'bqtools'
+    launch_cmd = 'Bqtools'
 elif search_file('cluster',PATH):
     launch_cmd = 'Cluster'
 else:



From nouiz at mail.berlios.de  Fri May 30 15:44:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 30 May 2008 15:44:01 +0200
Subject: [Plearn-commits] r9072 - trunk/plearn_learners/regressors
Message-ID: <200805301344.m4UDi1Le001177@sheep.berlios.de>

Author: nouiz
Date: 2008-05-30 15:44:00 +0200 (Fri, 30 May 2008)
New Revision: 9072

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
made one deprecated option static


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-05-28 19:58:01 UTC (rev 9071)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-05-30 13:44:00 UTC (rev 9072)
@@ -53,7 +53,7 @@
                         "It may be an expanded node pointing to 3 children nodes: a leave for missing values on the splitting attribute,\n"
                         "a left leave for samples with values below the value of the splitting attribute, and a right leave for the others,\n"
     );
-
+int RegressionTreeNode::dummy_int = 0;
 RegressionTreeNode::RegressionTreeNode():
     missing_is_valid(0),
     loss_function_weight(1),
@@ -61,8 +61,7 @@
     split_col(-1),
     split_balance(INT_MAX),
     split_feature_value(REAL_MAX),
-    after_split_error(REAL_MAX),
-    dummy_int(0)
+    after_split_error(REAL_MAX)
 {
     build();
 }
@@ -130,22 +129,22 @@
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The mising leave output vector\n");
 
-    declareOption(ol, "right_leave_id", &RegressionTreeNode::dummy_int,
+    declareStaticOption(ol, "right_leave_id", &RegressionTreeNode::dummy_int,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The id of the right leave\n");     
-    declareOption(ol, "left_leave_id", &RegressionTreeNode::dummy_int,
+    declareStaticOption(ol, "left_leave_id", &RegressionTreeNode::dummy_int,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The id of the left leave\n");
-    declareOption(ol, "missing_leave_id", &RegressionTreeNode::dummy_int,
+    declareStaticOption(ol, "missing_leave_id", &RegressionTreeNode::dummy_int,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The id of the missing leave\n");
-    declareOption(ol, "leave_id", &RegressionTreeNode::dummy_int,
+    declareStaticOption(ol, "leave_id", &RegressionTreeNode::dummy_int,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The id of the leave\n");
-    declareOption(ol, "length", &RegressionTreeNode::dummy_int,
+    declareStaticOption(ol, "length", &RegressionTreeNode::dummy_int,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The length of the train set\n");
-    declareOption(ol, "inputsize", &RegressionTreeNode::dummy_int,
+    declareStaticOption(ol, "inputsize", &RegressionTreeNode::dummy_int,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The inputsize of the train set\n");
 

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-05-28 19:58:01 UTC (rev 9071)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-05-30 13:44:00 UTC (rev 9072)
@@ -88,7 +88,7 @@
     PP<RegressionTreeNode> right_node;
     PP<RegressionTreeLeave> right_leave;
     
-    int dummy_int;
+    static int dummy_int;
     Vec tmp_vec;
 public:  
     RegressionTreeNode();



From plearner at mail.berlios.de  Fri May 30 20:25:54 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 30 May 2008 20:25:54 +0200
Subject: [Plearn-commits] r9073 - trunk/python_modules/plearn/vmat
Message-ID: <200805301825.m4UIPseU020279@sheep.berlios.de>

Author: plearner
Date: 2008-05-30 20:25:54 +0200 (Fri, 30 May 2008)
New Revision: 9073

Modified:
   trunk/python_modules/plearn/vmat/PMat.py
Log:
Fixed bug: don't call flush when opened in read mode.


Modified: trunk/python_modules/plearn/vmat/PMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/PMat.py	2008-05-30 13:44:00 UTC (rev 9072)
+++ trunk/python_modules/plearn/vmat/PMat.py	2008-05-30 18:25:54 UTC (rev 9073)
@@ -233,6 +233,7 @@
         self.inputsize = inputsize
         self.targetsize = targetsize
         self.weightsize = weightsize
+        self.openmode = openmode
         if openmode=='r':
             self.f = open(fname,'rb')
             self.read_and_parse_header()
@@ -405,7 +406,8 @@
         self.write_header() # update length in header
 
     def flush(self):
-        self.f.flush()
+        if self.openmode!='r':
+            self.f.flush()
 
     def close(self):
         if hasattr(self, 'f'):



From plearner at mail.berlios.de  Fri May 30 20:26:55 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 30 May 2008 20:26:55 +0200
Subject: [Plearn-commits] r9074 - trunk/python_modules/plearn/plotting
Message-ID: <200805301826.m4UIQtZk020338@sheep.berlios.de>

Author: plearner
Date: 2008-05-30 20:26:54 +0200 (Fri, 30 May 2008)
New Revision: 9074

Modified:
   trunk/python_modules/plearn/plotting/netplot.py
Log:
Added generic visualisation of matrix rows as images. 


Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2008-05-30 18:25:54 UTC (rev 9073)
+++ trunk/python_modules/plearn/plotting/netplot.py	2008-05-30 18:26:54 UTC (rev 9074)
@@ -123,7 +123,9 @@
     return toReturn
 
 
-def plotRowsAsImages(X, figtitle="", nrows=10, ncols=20, img_width=None, show_colorbar=False, disable_ticks=False, colormap = cm.gray, luminance_scale_mode = 0, vmin=None, vmax=None):
+def plotRowsAsImages(X, figtitle="", nrows=10, ncols=20, img_width=None,
+                     show_colorbar=False, disable_ticks=True, colormap = cm.gray,
+                     luminance_scale_mode = 0, vmin=None, vmax=None):
     """
     If provided, vmin and vmax will be used for luminance scale (see imshow)
     If not povided, they will be set depending on luminance_scale_mode:
@@ -134,7 +136,6 @@
           or +-max of X (whichever is bigger).
     """
     #some calculations for plotting
-
     img_size = len(X[0])
     if img_width is None:
         img_width = math.sqrt(img_size)
@@ -158,6 +159,7 @@
     for i in range(min(len(X),nrows*ncols)):
         row = X[i]
         subplot(nrows,ncols,i+1)
+        # print "Reshaping: ",row.shape,"->",img_height,'x',img_width
         img = reshape(row,(img_height,img_width))
         imshow(img, interpolation="nearest", cmap = colormap, vmin = vmin, vmax = vmax)
             
@@ -176,6 +178,7 @@
         cbw = .01 # color bar width
         customColorBar(vmin,vmax,color_map=colormap)
 
+                   
 
 
 def plotMatrices(matrices, names = None, ticks = False, same_color_bar = False, space_between_matrices = 5, horizontal=True):
@@ -461,7 +464,7 @@
     def draw(self):
         print "Start plotting..."
         clf()
-        endidx = min(self.startidx+self.nrows*self.ncols, len(self.X))
+        endidx = min(self.startidx+self.nrows*self.ncols, len(self.X))        
         title = self.figtitle+" ("+str(self.startidx)+" ... "+str(endidx-1)+")"
         plotRowsAsImages(self.X[self.startidx : endidx],
                          figtitle = title,
@@ -475,11 +478,10 @@
                          vmin = self.vmin,
                          vmax = self.vmax
                          )
-        
         print "Plotted,"
         draw()
         print "Drawn."
-                   
+        
 
     def plotNext(self):
         self.startidx += self.nrows*self.ncols



From plearner at mail.berlios.de  Fri May 30 20:28:20 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 30 May 2008 20:28:20 +0200
Subject: [Plearn-commits] r9075 - trunk/scripts
Message-ID: <200805301828.m4UISK9t020418@sheep.berlios.de>

Author: plearner
Date: 2008-05-30 20:28:20 +0200 (Fri, 30 May 2008)
New Revision: 9075

Added:
   trunk/scripts/show_rows_as_images.py
Log:
Script to visualize rows of a .pmat as images (interactive but slow...)


Added: trunk/scripts/show_rows_as_images.py
===================================================================
--- trunk/scripts/show_rows_as_images.py	2008-05-30 18:26:54 UTC (rev 9074)
+++ trunk/scripts/show_rows_as_images.py	2008-05-30 18:28:20 UTC (rev 9075)
@@ -0,0 +1,30 @@
+#!/usr/bin/env python
+
+import sys
+from plearn.vmat.PMat import load_pmat_as_array
+from plearn.plotting.netplot import showRowsAsImages
+
+####################
+### main program ###
+
+if __name__ == "__main__":
+
+    try:
+        datapmatfile, imgheight, imgwidth = sys.argv[1:]
+        imgheight = int(imgheight)
+        imgwidth = int(imgwidth)
+    except:
+        print "Usage: "+sys.argv[0]+" <datafile.pmat> <imgheight> <imgwidth>"
+        print """
+        Will load a pmat in memory and consider the beginning of each row a imgheight x imgwidth imagette.
+        It will interactively display those.
+        """
+        raise
+    # sys.exit()
+
+    data = load_pmat_as_array(datapmatfile)
+    inputs = data[:,0:imgheight*imgwidth]
+    showRowsAsImages(inputs, figtitle=datapmatfile, img_width=imgwidth)
+
+
+


Property changes on: trunk/scripts/show_rows_as_images.py
___________________________________________________________________
Name: svn:executable
   + *



From plearner at mail.berlios.de  Fri May 30 20:56:45 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 30 May 2008 20:56:45 +0200
Subject: [Plearn-commits] r9076 - trunk/scripts
Message-ID: <200805301856.m4UIuj9H022329@sheep.berlios.de>

Author: plearner
Date: 2008-05-30 20:56:45 +0200 (Fri, 30 May 2008)
New Revision: 9076

Modified:
   trunk/scripts/show_rows_as_images.py
Log:
Added parameters to control the grid nrows and ncols


Modified: trunk/scripts/show_rows_as_images.py
===================================================================
--- trunk/scripts/show_rows_as_images.py	2008-05-30 18:28:20 UTC (rev 9075)
+++ trunk/scripts/show_rows_as_images.py	2008-05-30 18:56:45 UTC (rev 9076)
@@ -10,21 +10,24 @@
 if __name__ == "__main__":
 
     try:
-        datapmatfile, imgheight, imgwidth = sys.argv[1:]
+        datapmatfile, imgheight, imgwidth, nrows, ncols = sys.argv[1:]
         imgheight = int(imgheight)
         imgwidth = int(imgwidth)
+        nrows = int(nrows)
+        ncols = int(ncols)
     except:
-        print "Usage: "+sys.argv[0]+" <datafile.pmat> <imgheight> <imgwidth>"
+        print "Usage: "+sys.argv[0]+" <datafile.pmat> <imgheight> <imgwidth> <nrows> <ncols>"
         print """
         Will load a pmat in memory and consider the beginning of each row a imgheight x imgwidth imagette.
-        It will interactively display those.
+        These will be interactively displayed in a nrows x ncols grid of imagettes.
         """
+        print "Ex: "+sys.argv[0]+" /home/fringant2/lisa/data/faces/olivetti/faces.pmat 64 64  5 7"
         raise
     # sys.exit()
 
     data = load_pmat_as_array(datapmatfile)
     inputs = data[:,0:imgheight*imgwidth]
-    showRowsAsImages(inputs, figtitle=datapmatfile, img_width=imgwidth)
+    showRowsAsImages(inputs, figtitle=datapmatfile, nrows=nrows, ncols=ncols, img_width=imgwidth)
 
 
 



From plearner at mail.berlios.de  Fri May 30 21:46:53 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 30 May 2008 21:46:53 +0200
Subject: [Plearn-commits] r9077 - in trunk/scripts: . DEPRECATED
Message-ID: <200805301946.m4UJkrRB027716@sheep.berlios.de>

Author: plearner
Date: 2008-05-30 21:46:52 +0200 (Fri, 30 May 2008)
New Revision: 9077

Added:
   trunk/scripts/DEPRECATED/
   trunk/scripts/DEPRECATED/oldpyplot
Removed:
   trunk/scripts/oldpyplot
Log:
Started moving things in DEPRECATED


Copied: trunk/scripts/DEPRECATED/oldpyplot (from rev 9042, trunk/scripts/oldpyplot)

Deleted: trunk/scripts/oldpyplot
===================================================================
--- trunk/scripts/oldpyplot	2008-05-30 18:56:45 UTC (rev 9076)
+++ trunk/scripts/oldpyplot	2008-05-30 19:46:52 UTC (rev 9077)
@@ -1,161 +0,0 @@
-#!/usr/bin/env python
-
-# pyplot
-# Copyright (C) 2005 Pascal Vincent
-#
-#  Redistribution and use in source and binary forms, with or without
-#  modification, are permitted provided that the following conditions are met:
-#
-#   1. Redistributions of source code must retain the above copyright
-#      notice, this list of conditions and the following disclaimer.
-#
-#   2. Redistributions in binary form must reproduce the above copyright
-#      notice, this list of conditions and the following disclaimer in the
-#      documentation and/or other materials provided with the distribution.
-#
-#   3. The name of the authors may not be used to endorse or promote
-#      products derived from this software without specific prior written
-#      permission.
-#
-#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-#  This file is part of the PLearn library. For more information on the PLearn
-#  library, go to the PLearn Web site at www.plearn.org
-
-
-# Author: Pascal Vincent
-
-import os
-import sys
-import time
-from plearn.plotting import *
-
-plearncommand = 'plearn'
-
-CLASS_POINT_STYLES = ['bo','ro','go','mo','co','yo']
-
-def print_usage_and_exit():
-    print """
-    Usage: pyplot <task> <dataspec> [<learner>]
-      dataspec can be a .amat .pmat .vmat .pymat, actually anything understood by plearn vmat convert
-      task can be plot_2d 2d_density 2d_classif 2d_mapping 1d_regression
-      """
-    sys.exit()
-
-def run_plearn(subcommand_and_args):
-    commandstr = plearncommand+' '+subcommand_and_args
-    print commandstr
-    os.system(commandstr)
-
-
-if len(sys.argv)<3:
-    print_usage_and_exit()
-
-def load_data(dataspec):
-    datapmat = 'pyplot_data.pmat'
-    run_plearn('vmat convert '+dataspec+' '+datapmat)
-    data = load_pmat_as_array(datapmat)
-    return data
-
-def train_learner(learnerspec, dataspec, trained_learner_file):
-    run_plearn('learner train '+learnerspec+' '+dataspec+' '+trained_learner_file)
-
-def learner_compute_outputs_on_2d_grid(learner, trainset, nx=50, ny=50):
-    run_plearn('learner cg '+learner+' gridoutputs.pmat '+trainset+' '+str(nx)+' '+str(ny))
-    outputs = load_pmat_as_array('gridoutputs.pmat')
-    return outputs
-
-def learner_compute_outputs_on_1d_grid(learner, trainset, nx=1000):
-    run_plearn('learner cg '+learner+' gridoutputs.pmat '+trainset+' '+str(nx))
-    outputs = load_pmat_as_array('gridoutputs.pmat')
-    return outputs
-
-def learner_compute_outputs(learner, dataspec):
-    # learner compute_outputs <trained_learner.psave> <test_inputs.vmat> <outputs.pmat> 
-    run_plearn('learner compute_outputs '+learner+' '+dataspec+' outputs.pmat')
-    outputs = load_pmat_as_array('outputs.pmat')
-    return outputs
-
-task = sys.argv[1]
-dataspec = sys.argv[2]
-learnerspec = ''
-try: learnerspec = sys.argv[3]
-except: pass
-
-if task == 'plot_2d':
-    data = load_data(dataspec)
-    print 'Data loaded'
-    if data.shape[1]>=3:
-        plot_2d_class_points(data[:,0:3], CLASS_POINT_STYLES)
-    else:
-        plot_2d_points(data[:,0:2],'o')
-    show()
-
-elif task == '2d_density':
-    nx = 200
-    ny = 200
-    data = load_data(dataspec)
-    data = data[:,0:2]
-    # plot_2d_points(data,'o')
-    # show()
-    train_learner(learnerspec,dataspec,'trained_learner.psave')
-    xymagnitude = learner_compute_outputs_on_2d_grid('trained_learner.psave', dataspec, nx, ny)
-    transform_magnitude_into_covered_percentage(xymagnitude)
-    print 'xymagnitude.shape = ',xymagnitude.shape
-    imshow_xymagnitude(xymagnitude)
-    plot_2d_points(data,'ko')
-    show()
-    surfplot_xymagnitude(xymagnitude)
-    raw_input()
-
-elif task == '2d_classif':
-    nx = 200
-    ny = 200
-    data = load_data(dataspec)
-    train_learner(learnerspec,dataspec,'trained_learner.psave')
-    print 'computing outputs...'
-    xyscores = learner_compute_outputs_on_2d_grid('trained_learner.psave', dataspec, nx, ny)
-    print 'scores:'
-    print xyscores
-    xy_winner_margin = xyscores_to_winner_and_margin(xyscores)
-    # print xy_winner_margin
-    xyrgb = xy_winner_magnitude_to_xyrgb(xy_winner_margin)
-    # print array(xyrgb)
-    imshow_xyrgb(xyrgb)
-    # print 'Plotting points of ',dataspec, data
-    plot_2d_class_points(data,CLASS_POINT_STYLES)
-    show()
-
-elif task == '2d_mapping':
-    data = load_data(dataspec)    
-    train_learner(learnerspec,dataspec,'trained_learner.psave')
-    plot_2d_points(data[:,0:2],'ro')
-    outputs = learner_compute_outputs('trained_learner.psave', dataspec)
-    plot_2d_points(outputs[:,0:2],'bo')
-    show()
-    
-
-elif task == '1d_regression':
-    nx = 100
-    data = load_data(dataspec)
-    train_learner(learnerspec,dataspec,'trained_learner.psave')
-    print 'computing outputs...'
-    scores = learner_compute_outputs_on_1d_grid('trained_learner.psave', dataspec, nx)
-    print scores.shape
-    # print scores
-    plot_2d_points(scores,'b-')
-    plot_2d_points(data,'ro')
-    show()
-    
-else:
-    print_usage_and_exit()
-    



From plearner at mail.berlios.de  Sat May 31 00:32:37 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 31 May 2008 00:32:37 +0200
Subject: [Plearn-commits] r9078 - trunk/plearn/var/EXPERIMENTAL
Message-ID: <200805302232.m4UMWbie017661@sheep.berlios.de>

Author: plearner
Date: 2008-05-31 00:32:35 +0200 (Sat, 31 May 2008)
New Revision: 9078

Added:
   trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.cc
   trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h
   trunk/plearn/var/EXPERIMENTAL/Cov2CorrVariable.cc
   trunk/plearn/var/EXPERIMENTAL/Cov2CorrVariable.h
   trunk/plearn/var/EXPERIMENTAL/DiagVariable.cc
   trunk/plearn/var/EXPERIMENTAL/DiagVariable.h
   trunk/plearn/var/EXPERIMENTAL/NonDiagVariable.cc
   trunk/plearn/var/EXPERIMENTAL/NonDiagVariable.h
   trunk/plearn/var/EXPERIMENTAL/TraceVariable.cc
   trunk/plearn/var/EXPERIMENTAL/TraceVariable.h
Log:
Added a few useful vars


Added: trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.cc	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.cc	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,151 @@
+// -*- C++ -*-
+
+// ConstrainedSourceVariable.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file ConstrainedSourceVariable.cc */
+
+
+#include "ConstrainedSourceVariable.h"
+#include<plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+/** ConstrainedSourceVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    ConstrainedSourceVariable,
+    "SourceVariable that after each update, modifies values as needed to satisfy simple constraints",
+    "The currently supported constraint is rows having norm 1.\n"
+    "i.e. after each update rows are divided by their norm.\n");
+
+
+void ConstrainedSourceVariable::satisfyConstraints()
+{
+    switch(constraint_mode)
+    {
+    case 0:
+        for(int i=0; i<matValue.length(); i++)
+            normalize(matValue(i), 2);
+        break;
+    default:
+        PLERROR("Invalid constraint_mode %d",constraint_mode);
+    }
+}
+
+
+void ConstrainedSourceVariable::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "constraint_mode", &ConstrainedSourceVariable::constraint_mode, OptionBase::buildoption,
+        "The constraint_mode: \n"
+        "0: divide each row by its L2 norm after each update");
+    inherited::declareOptions(ol);
+}
+
+
+void ConstrainedSourceVariable::build_()
+{ }
+
+void ConstrainedSourceVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+void ConstrainedSourceVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+
+//#####  update*  #############################################################
+
+bool ConstrainedSourceVariable::update(real step_size, Vec direction_vec, real coeff, real b)
+{
+    bool ret = inherited::update(step_size, direction_vec, coeff, b);
+    satisfyConstraints();
+    return ret;
+}
+
+bool ConstrainedSourceVariable::update(Vec step_sizes, Vec direction_vec, real coeff, real b)
+{
+    bool ret = inherited::update(step_sizes, direction_vec, coeff, b);
+    satisfyConstraints();
+    return ret;
+}
+
+bool ConstrainedSourceVariable::update(real step_size, bool clear)
+{
+    bool ret = inherited::update(step_size, clear);
+    satisfyConstraints();
+    return ret;
+}
+
+bool ConstrainedSourceVariable::update(Vec new_value)
+{
+    bool ret = inherited::update(new_value);
+    satisfyConstraints();
+    return ret;
+}
+
+void ConstrainedSourceVariable::updateAndClear()
+{
+    inherited::updateAndClear();
+    satisfyConstraints();
+}
+
+void ConstrainedSourceVariable::updateWithWeightDecay(real step_size, real weight_decay,
+                                                 bool L1, bool clear)
+{
+    inherited::updateWithWeightDecay(step_size, weight_decay, L1, clear);
+    satisfyConstraints();
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,141 @@
+// -*- C++ -*-
+
+// ConstrainedSourceVariable.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file ConstrainedSourceVariable.h */
+
+
+#ifndef ConstrainedSourceVariable_INC
+#define ConstrainedSourceVariable_INC
+
+#include <plearn/var/SourceVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * SourceVariable that after each update, modifies values as needed to satisfy simple constraints.
+ *
+ * The currently supported constraint is rows having norm 1.
+ * i.e. after each update rows are divided by their norm.
+ */
+class ConstrainedSourceVariable : public SourceVariable
+{
+    typedef SourceVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int constraint_mode;
+
+public:
+    //!  Default constructor for persistence
+    ConstrainedSourceVariable()
+        :SourceVariable(), 
+         constraint_mode(0)
+    {}
+
+    ConstrainedSourceVariable(int thelength, int thewidth, int the_constraint_mode=0, bool call_build_ = true)
+        :SourceVariable(thelength, thewidth, call_build_), 
+         constraint_mode(the_constraint_mode)
+    {}
+
+    ConstrainedSourceVariable(const Vec& v, bool vertical=true, int the_constraint_mode=0, bool call_build_ = true)
+        :SourceVariable(v, vertical, call_build_), 
+         constraint_mode(the_constraint_mode)
+    {}
+
+    ConstrainedSourceVariable(const Mat& m, int the_constraint_mode=0, bool call_build_ = true)
+        :SourceVariable(m, call_build_), 
+         constraint_mode(the_constraint_mode)
+    {}
+
+    // The method that will be called after each update to satisfy the constraints.
+    virtual void satisfyConstraints();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(ConstrainedSourceVariable);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    
+    //#####  'Variable' Interface  ############################################
+
+
+    // All the 'update' methods are overridden to be followed by a call to satisfyConstraints(),
+    // whose purpose is to "correct" the values after an update so that they fulfill the constraints
+    virtual bool update(real step_size, Vec direction_vec, real coeff = 1.0, real b = 0.0);
+    virtual bool update(Vec step_sizes, Vec direction_vec, real coeff = 1.0, real b = 0.0);
+    virtual bool update(real step_size, bool clear=false);
+    virtual bool update(Vec new_value);
+    virtual void updateAndClear();
+    virtual void updateWithWeightDecay(real step_size, real weight_decay,
+                                       bool L1, bool clear=true);
+    
+    
+protected:
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //! This does the actual building.
+    void build_();
+};
+
+
+DECLARE_OBJECT_PTR(ConstrainedSourceVariable);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/Cov2CorrVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/Cov2CorrVariable.cc	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/Cov2CorrVariable.cc	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,232 @@
+// -*- C++ -*-
+
+// Cov2CorrVariable.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file Cov2CorrVariable.cc */
+
+#include "Cov2CorrVariable.h"
+//#include "Var_operators.h"
+//#include "Var_utils.h"
+
+namespace PLearn {
+using namespace std;
+
+
+/** Cov2CorrVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+        Cov2CorrVariable,
+        "Convert a covariance matrix to a correlation matrix",
+        "i.e. it divides off-diagonal term A_ij by sqrt(A_ii A_jj + epsilon).\n"
+        "Diagonal terms are set according to option diaognal_choice\n"
+);
+
+////////////////////
+// Cov2CorrVariable //
+////////////////////
+
+Cov2CorrVariable::Cov2CorrVariable():
+    diagonal_choice(1),
+    epsilon(0.)
+{}
+
+Cov2CorrVariable::Cov2CorrVariable(Variable* input, int diagonal_choice_,
+                                   double epsilon_, bool call_build_):
+    inherited(input, input->length(), input->width(), call_build_),
+    diagonal_choice(diagonal_choice_),
+    epsilon(epsilon_)
+{
+    if (call_build_)
+        build_();
+}
+
+void Cov2CorrVariable::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "diagonal_choice", &Cov2CorrVariable::diagonal_choice, OptionBase::buildoption,
+        "Controls how to fill the diagonal.\n"
+        "  0: fill it with 0\n"
+        "  1: fill it with 1\n"
+        "  2: keep the diagonal of the input (i.e. the original variances)\n");
+    declareOption(
+        ol, "epsilon", &Cov2CorrVariable::epsilon, OptionBase::buildoption,
+        "value to add to product of variances before taking their sqrt.\n");
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void Cov2CorrVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void Cov2CorrVariable::build_() {
+    // Nothing to do here.
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
+void Cov2CorrVariable::recomputeSize(int& l, int& w) const
+{
+    if (input) {
+        l = input->length();
+        w = input->width();
+    } else
+        l = w = 0;
+}
+
+
+void Cov2CorrVariable::fprop()
+{
+    Mat C = input->matValue;
+    int l = C.length();
+    int w = C.width();
+    for(int i=0; i<l; i++)
+        for(int j=0; j<w; j++)
+        {
+            if(i!=j)
+                matValue(i,j) = C(i,j)/sqrt(C(i,i)*C(j,j)+epsilon);
+            else // diagonal element
+            {
+                double diagval = 0;
+                switch(diagonal_choice)
+                {
+                case 0:
+                    diagval = 0;
+                    break;
+                case 1:
+                    diagval = 1;
+                    break;
+                case 2:
+                    diagval = C(i,j);
+                    break;
+                default:
+                    PLERROR("Invalid diagonal_choice option");
+                }
+                matValue(i,j) = diagval;
+            }
+        }
+}
+
+/*
+  Let C the covariance matrix and D the correlation matrix.
+  D_ij = C_ij (C_ii C_jj + epsilon)^(-1/2)
+
+  dL/dC_ij = \sum_i'j' dL/dD_i'j' dD_i'j'/dC_ij
+ 
+  case i!=j:
+     dL/dC_ij = dL/dD_ij dD_ij/dC_ij
+              = dL/dD_ij (C_ii C_jj + epsilon)^(-1/2)
+              = dL/dD_ij D_ij/C_ij
+
+  case i==j
+
+    D_ik = C_ik (C_ii C_kk + epsilon)^(-1/2)
+    dD_ik/dC_ii = C_ik (-1/2) C_kk (C_ii C_kk + epsilon)^(-3/2)
+                = -1/2 C_kk D_ik / (C_ii C_kk + epsilon)
+    D_ki = C_ki (C_kk C_ii + epsilon)^(-1/2)
+    dD_ki/dC_ii = C_ki (-1/2) C_kk (C_kk C_ii + epsilon)^(-3/2)
+                = -1/2 C_kk D_ki / (C_ii C_kk + epsilon)
+
+     dL/dC_ii = \sum_{k!=i}{   dL/dD_ik dD_ik/dC_ii 
+                             + dL/dD_ki dD_ki/dC_ii }
+              = \sum_{k!=i}{   dL/dD_ik [ -1/2 C_kk D_ik/(C_ii C_kk + epsilon) ]
+                             + dL/dD_ki [ -1/2 C_kk D_ki/(C_ii C_kk + epsilon) ]
+
+*/
+
+void Cov2CorrVariable::bprop()
+{
+    Mat C = input->matValue;
+    Mat Cg = input->matGradient;
+    Mat D = matValue;
+    Mat Dg = matGradient;
+    int l = C.length();
+    int w = C.width();
+    for(int i=0; i<l; i++)
+        for(int j=0; j<w; j++)
+        {
+            if(i!=j)
+            {
+                Cg(i,j) += Dg(i,j)*D(i,j)/C(i,j);
+                double h = -0.5*Dg(i,j)*D(i,j)/(C(i,i)*C(j,j)+epsilon);
+                Cg(i,i) += C(j,j)*h;
+                Cg(j,j) += C(i,i)*h;
+            }
+            else if(diagonal_choice==2)
+                Cg(i,i) += Dg(i,i);
+        }
+}
+
+
+void Cov2CorrVariable::bbprop()
+{
+    PLERROR("bbprop not implemented for this variable");
+}
+
+
+void Cov2CorrVariable::symbolicBprop()
+{
+    PLERROR("symbolic bprop not yet implemented for this variable");
+}
+
+
+void Cov2CorrVariable::rfprop()
+{
+    PLERROR("rfprop no yet implemented for this vairable");
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/Cov2CorrVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/Cov2CorrVariable.h	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/Cov2CorrVariable.h	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,102 @@
+// -*- C++ -*-
+
+// Cov2CorrVariable.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file Cov2CorrVariable.h */
+
+#ifndef Cov2CorrVariable_INC
+#define Cov2CorrVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+
+class Cov2CorrVariable: public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+    int diagonal_choice;
+    double epsilon;
+
+    //! Default constructor.
+    Cov2CorrVariable();
+
+    Cov2CorrVariable(Variable* input, int diagonal_choice_=1,
+                     double epsilon_=0, bool call_build_ = true);
+
+    PLEARN_DECLARE_OBJECT(Cov2CorrVariable);
+
+    static void declareOptions(OptionList& ol);
+
+    virtual void build();
+
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+    //!  here don't approximate, do d2C/dx^2 = 4 x^2 d2C/dy^2 + 2 dC/dy 
+    virtual void bbprop();
+    virtual void symbolicBprop();
+    virtual void rfprop();
+
+private:
+
+    void build_();
+
+};
+
+DECLARE_OBJECT_PTR(Cov2CorrVariable);
+
+inline Var cov2corr(Var v, int diagonal_choice=1, double epsilon=0.)
+{ return new Cov2CorrVariable(v,diagonal_choice,epsilon); }
+
+} // end of namespace PLearn
+
+#endif 
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/DiagVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/DiagVariable.cc	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/DiagVariable.cc	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,148 @@
+// -*- C++ -*-
+
+// DiagVariable.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file DiagVariable.cc */
+
+#include "DiagVariable.h"
+// #include "Var_operators.h"
+//#include "Var_utils.h"
+
+namespace PLearn {
+using namespace std;
+
+
+/** DiagVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+        DiagVariable,
+        "Extracts the diagonal from a matrix source variable as a column vector.",
+        "Resulting variable is a column vector."
+);
+
+////////////////////
+// DiagVariable //
+////////////////////
+
+DiagVariable::DiagVariable()
+{}
+
+DiagVariable::DiagVariable(Variable* input, bool call_build_):
+    inherited(input, min(input->length(), input->width()), 1, call_build_)
+{
+    if (call_build_)
+        build_();
+}
+
+///////////
+// build //
+///////////
+void DiagVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void DiagVariable::build_() {
+    // Nothing to do here.
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
+void DiagVariable::recomputeSize(int& l, int& w) const
+{
+    if (input) 
+    {
+        l = min(input->length(), input->width());
+        w = 1;
+    } 
+    else
+        l = w = 0;
+}
+
+
+void DiagVariable::fprop()
+{
+    int l = value.length();
+    Mat V = input->matValue;
+    for(int i=0; i<l; i++)
+        value[i] = V(i,i);
+}
+
+void DiagVariable::bprop()
+{
+    int l = gradient.length();
+    Mat G = input->matGradient;
+    for(int i=0; i<l; i++)
+        G(i,i) += gradient[i];
+}
+
+
+void DiagVariable::bbprop()
+{
+    PLERROR("bbprop not implemented for this variable");
+}
+
+
+void DiagVariable::symbolicBprop()
+{
+    PLERROR("symbolic bprop not yet implemented for this variable");
+}
+
+
+void DiagVariable::rfprop()
+{
+    PLERROR("rfprop no yet implemented for this vairable");
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/DiagVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/DiagVariable.h	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/DiagVariable.h	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,97 @@
+// -*- C++ -*-
+
+// DiagVariable.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file DiagVariable.h */
+
+#ifndef DiagVariable_INC
+#define DiagVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+
+class DiagVariable: public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+
+    //! Default constructor.
+    DiagVariable();
+
+    DiagVariable(Variable* input, bool call_build_ = true);
+
+    PLEARN_DECLARE_OBJECT(DiagVariable);
+
+    virtual void build();
+
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+    //!  here don't approximate, do d2C/dx^2 = 4 x^2 d2C/dy^2 + 2 dC/dy 
+    virtual void bbprop();
+    virtual void symbolicBprop();
+    virtual void rfprop();
+
+private:
+
+    void build_();
+
+};
+
+DECLARE_OBJECT_PTR(DiagVariable);
+
+inline Var diag(Var v)
+{ return new DiagVariable(v); }
+
+} // end of namespace PLearn
+
+#endif 
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/NonDiagVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/NonDiagVariable.cc	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/NonDiagVariable.cc	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,156 @@
+// -*- C++ -*-
+
+// DiagVariable.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file NonDiagVariable.cc */
+
+#include "NonDiagVariable.h"
+// #include "Var_operators.h"
+//#include "Var_utils.h"
+
+namespace PLearn {
+using namespace std;
+
+
+/** NonDiagVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+        NonDiagVariable,
+        "Extracts all non-diagonal elements from a matrix source variable as a column vector.",
+        "Resulting variable is a column vector."
+);
+
+////////////////////
+// NonDiagVariable //
+////////////////////
+
+NonDiagVariable::NonDiagVariable()
+{}
+
+NonDiagVariable::NonDiagVariable(Variable* input, bool call_build_):
+    inherited(input, input->length()*input->width()-min(input->length(), input->width()), 1, call_build_)
+{
+    if (call_build_)
+        build_();
+}
+
+///////////
+// build //
+///////////
+void NonDiagVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void NonDiagVariable::build_() {
+    // Nothing to do here.
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
+void NonDiagVariable::recomputeSize(int& l, int& w) const
+{
+    if (input) 
+    {
+        l = input->length()*input->width()-min(input->length(), input->width());
+        w = 1;
+    } 
+    else
+        l = w = 0;
+}
+
+
+void NonDiagVariable::fprop()
+{
+    Mat V = input->matValue;
+    int l = V.length();
+    int w = V.width();
+    int k=0;
+    for(int i=0; i<l; i++)
+        for(int j=0; j<w; j++)
+            if(i!=j)
+                value[k++] = V(i,j);
+}
+
+void NonDiagVariable::bprop()
+{
+    Mat G = input->matGradient;
+    int l = G.length();
+    int w = G.width();
+    int k=0;
+    for(int i=0; i<l; i++)
+        for(int j=0; j<w; j++)
+            if(i!=j)
+                G(i,j) += gradient[k++];
+}
+
+
+void NonDiagVariable::bbprop()
+{
+    PLERROR("bbprop not implemented for this variable");
+}
+
+
+void NonDiagVariable::symbolicBprop()
+{
+    PLERROR("symbolic bprop not yet implemented for this variable");
+}
+
+
+void NonDiagVariable::rfprop()
+{
+    PLERROR("rfprop no yet implemented for this vairable");
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/NonDiagVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/NonDiagVariable.h	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/NonDiagVariable.h	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,97 @@
+// -*- C++ -*-
+
+// NonDiagVariable.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file NonDiagVariable.h */
+
+#ifndef NonDiagVariable_INC
+#define NonDiagVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+
+class NonDiagVariable: public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+
+    //! Default constructor.
+    NonDiagVariable();
+
+    NonDiagVariable(Variable* input, bool call_build_ = true);
+
+    PLEARN_DECLARE_OBJECT(NonDiagVariable);
+
+    virtual void build();
+
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+    //!  here don't approximate, do d2C/dx^2 = 4 x^2 d2C/dy^2 + 2 dC/dy 
+    virtual void bbprop();
+    virtual void symbolicBprop();
+    virtual void rfprop();
+
+private:
+
+    void build_();
+
+};
+
+DECLARE_OBJECT_PTR(NonDiagVariable);
+
+inline Var nondiag(Var v)
+{ return new NonDiagVariable(v); }
+
+} // end of namespace PLearn
+
+#endif 
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/TraceVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/TraceVariable.cc	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/TraceVariable.cc	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,145 @@
+// -*- C++ -*-
+
+// TraceVariable.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file TraceVariable.cc */
+
+#include "TraceVariable.h"
+//#include "Var_operators.h"
+//#include "Var_utils.h"
+
+namespace PLearn {
+using namespace std;
+
+
+/** TraceVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+        TraceVariable,
+        "Computes the trace of the input variable (i.e. the sum of its diagonal).",
+        "Resulting variable is a scalar (1x1 matrix)"
+);
+
+////////////////////
+// TraceVariable //
+////////////////////
+
+TraceVariable::TraceVariable()
+{}
+
+TraceVariable::TraceVariable(Variable* input, bool call_build_):
+    inherited(input, 1, 1, call_build_)
+{
+    if (call_build_)
+        build_();
+}
+
+///////////
+// build //
+///////////
+void TraceVariable::build() {
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void TraceVariable::build_() {
+    // Nothing to do here.
+}
+
+///////////////////
+// recomputeSize //
+///////////////////
+void TraceVariable::recomputeSize(int& l, int& w) const
+{
+    l = w = 1;
+}
+
+
+void TraceVariable::fprop()
+{
+    Mat V = input->matValue;
+    int l = min(V.length(), V.width());
+    double result = 0;
+    for(int i=0; i<l; i++)
+        result += V(i,i);
+    matValue(0,0) = result;
+}
+
+void TraceVariable::bprop()
+{
+    double g = matGradient(0,0);
+    Mat G = input->gradientValue;
+    int l = min(G.length(), G.width());
+    for(int i=0; i<l; i++)
+        G(i,i) += g;
+}
+
+
+void TraceVariable::bbprop()
+{
+    PLERROR("bbprop not implemented for this variable");
+}
+
+
+void TraceVariable::symbolicBprop()
+{
+    PLERROR("symbolic bprop not yet implemented for this variable");
+}
+
+
+void TraceVariable::rfprop()
+{
+    PLERROR("rfprop no yet implemented for this vairable");
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/TraceVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/TraceVariable.h	2008-05-30 19:46:52 UTC (rev 9077)
+++ trunk/plearn/var/EXPERIMENTAL/TraceVariable.h	2008-05-30 22:32:35 UTC (rev 9078)
@@ -0,0 +1,97 @@
+// -*- C++ -*-
+
+// TraceVariable.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file TraceVariable.h */
+
+#ifndef TraceVariable_INC
+#define TraceVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+
+class TraceVariable: public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+
+    //! Default constructor.
+    TraceVariable();
+
+    TraceVariable(Variable* input, bool call_build_ = true);
+
+    PLEARN_DECLARE_OBJECT(TraceVariable);
+
+    virtual void build();
+
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+    //!  here don't approximate, do d2C/dx^2 = 4 x^2 d2C/dy^2 + 2 dC/dy 
+    virtual void bbprop();
+    virtual void symbolicBprop();
+    virtual void rfprop();
+
+private:
+
+    void build_();
+
+};
+
+DECLARE_OBJECT_PTR(TraceVariable);
+
+inline Var trace(Var v)
+{ return new TraceVariable(v); }
+
+} // end of namespace PLearn
+
+#endif 
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From plearner at mail.berlios.de  Sat May 31 23:21:42 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 31 May 2008 23:21:42 +0200
Subject: [Plearn-commits] r9079 - trunk/python_modules/plearn/plotting
Message-ID: <200805312121.m4VLLgbi013017@sheep.berlios.de>

Author: plearner
Date: 2008-05-31 23:21:42 +0200 (Sat, 31 May 2008)
New Revision: 9079

Added:
   trunk/python_modules/plearn/plotting/matplotlib_utils.py
   trunk/python_modules/plearn/plotting/mayavi_utils.py
   trunk/python_modules/plearn/plotting/numpy_utils.py
Modified:
   trunk/python_modules/plearn/plotting/__init__.py
   trunk/python_modules/plearn/plotting/netplot.py
Log:
Reorganized the plotting package 
__init__ no longer contains anything
For the matplotlib related stuff, you should now import from
plearn.plotting.matplotlib_utils 
instead of  plearn.plotting



Modified: trunk/python_modules/plearn/plotting/__init__.py
===================================================================
--- trunk/python_modules/plearn/plotting/__init__.py	2008-05-30 22:32:35 UTC (rev 9078)
+++ trunk/python_modules/plearn/plotting/__init__.py	2008-05-31 21:21:42 UTC (rev 9079)
@@ -34,257 +34,3 @@
 
 # Author: Pascal Vincent
 
-# from array import *
-import numpy.numarray as numarray
-import string
-import matplotlib
-# matplotlib.interactive(True)
-#matplotlib.use('TkAgg')
-#matplotlib.use('GTK')
-  
-from pylab import *
-from numpy.numarray import *
-from mayavi.tools import imv
-
-from plearn.vmat.PMat import *
-
-
-threshold = 0
-
-def margin(scorevec):
-    if len(scorevec)==1:
-        return abs(scorevec[0]-threshold)
-    else:
-        sscores = sort(scorevec)    
-        return sscores[-1]-sscores[-2]
-
-def winner(scorevec):
-    if len(scorevec)==1:
-        if scorevec[0]>threshold:
-            return 1
-        else:
-            return 0
-    else:
-        return argmax(scorevec)
-
-def xyscores_to_winner_and_magnitude(xyscores):
-    return array([ (v[0], v[1], winner(v[2:]),max(v[2:])) for v in xyscores ])
-
-def xyscores_to_winner_and_margin(xyscores):
-    return array([ (v[0], v[1], winner(v[2:]),margin(v[2:])) for v in xyscores ])
-
-def regular_xyval_to_2d_grid_values(xyval):
-    """Returns (grid_values, x0, y0, deltax, deltay)"""
-    xyval = numarray.array(xyval)
-    n = len(xyval)
-    x = xyval[:,0]
-    y = xyval[:,1]
-    values = xyval[:,2:].copy()
-    # print "type(values)",type(values)
-    valsize = numarray.size(values,1)
-    x0 = x[0]
-    y0 = y[0]
-
-    k = 1
-    if x[1]==x0:
-        deltay = y[1]-y[0]
-        while x[k]==x0:
-            k = k+1
-        deltax = x[k]-x0
-        ny = k
-        nx = n // ny
-        # print 'A) nx,ny:',nx,ny
-        values.shape = (nx,ny,valsize)
-        # print "A type(values)",type(values)
-        values = numarray.transpose(values,(1,0,2))
-        # print "B type(values)",type(values)
-    elif y[1]==y0:
-        deltax = x[1]-x[0]
-        while y[k]==y0:
-            k = k+1
-        deltay = y[k]-y0
-        nx = k
-        ny = n // nx
-        # print 'B) nx,ny:',nx,ny
-        values.shape = (ny,nx,valsize)
-        # print "C type(values)",type(values)
-        values = numarray.transpose(values,(1,0,2))
-        # print "D type(values)",type(values)
-    else:
-        raise ValueError("Strange: x[1]!=x0 and y[1]!=y0 this doesn't look like a regular grid...")
-
-    print 'In regular_xyval_to_2d_grid_values: ', type(xyval), type(values)
-    return values, x0, y0, deltax, deltay
-
-
-def divide_by_mean_magnitude(xymagnitude):
-    mag = xymagnitude[:,2]
-    meanval = mag.mean()
-    mag *= 1./meanval
-    return meanval
-
-def divide_by_max_magnitude(xymagnitude):
-    mag = xymagnitude[:,2]
-    maxval = mag.max()
-    mag *= 1./maxval
-    return maxval
-
-def transform_magnitude_into_covered_percentage(xymagnitude):
-    magnitudes = []
-    l = len(xymagnitude)
-    for i in xrange(l):
-        row = xymagnitude[i]
-        magnitudes.append([row[-1],i])
-    magnitudes.sort()
-    # magnitude.reverse()
-    cum = 0
-    for row in magnitudes:
-        mag, i = row
-        cum += mag
-        row[0] = cum
-    for mag,i in magnitudes:
-        xymagnitude[i][-1] = mag/cum
-    return cum
-        
-def imshow_xymagnitude(regular_xymagnitude, interpolation='nearest', cmap = cm.jet):
-    grid_values, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xymagnitude)
-    # print 'In imshow_xymagnitude: ', type(regular_xymagnitude), type(grid_values)
-    imshow_2d_grid_values(grid_values, x0, y0, deltax, deltay, interpolation, cm.jet)
-    
-def imshow_xyrgb(regular_xyrgb, interpolation='nearest'):
-    grid_rgb, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xyrgb)
-    # print 'grid_rgb shape=',grid_rgb.shape
-    imshow_2d_grid_rgb(grid_rgb, x0, y0, deltax, deltay, interpolation, cm.jet)
-
-def classcolor(winner,margin=0):
-    colors = { 0: [0.5, 0.5, 1.0],
-               1: [1.0, 0.5, 0.5],
-               2: [0.5, 1.0, 0.5],
-             }
-    return colors[winner]
-    
-def xy_winner_magnitude_to_xyrgb(xy_winner_margin):
-    res = []
-    for x,y,w,m in xy_winner_margin:
-        res.append([x,y]+classcolor(w,m))
-    return res
-
-def xymagnitude_to_x_y_grid(regular_xymagnitude):
-    gridvalues, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xymagnitude)
-    nx = numarray.size(gridvalues,0)
-    ny = numarray.size(gridvalues,1)
-    gridvalues = numarray.reshape(gridvalues,(nx,ny))
-    x = numarray.arange(x0,x0+nx*deltax-1e-6,deltax)
-    y = numarray.arange(y0,y0+ny*deltay-1e-6,deltay)
-    # print "x = ",x
-    # print "y = ",y
-    # print "z = ",gridvalues
-    # print "type(x) = ",type(x)
-    # print "type(y) = ",type(y)
-    # print "type(z) = ",type(gridvalues)
-    # imv.view(gridvalues)
-    return x, y, gridvalues
-    
-def surfplot_xymagnitude(regular_xymagnitude):
-    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
-    imv.surf(x, y, gridvalues)
-
-def contour_xymagnitude(regular_xymagnitude):
-    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
-    clabel(contour(x, y, gridvalues))
-    
-def contourf_xymagnitude(regular_xymagnitude):
-    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
-    contourf(x, y, gridvalues)
-    colorbar()
-
-def imshow_2d_grid_rgb(gridrgb, x0, y0, deltax, deltay, interpolation='nearest', cmap = cm.jet):
-    nx = numarray.size(gridrgb,0)
-    ny = numarray.size(gridrgb,1)
-    extent = (x0-.5*deltax, x0+nx*deltax, y0-.5*deltay, y0+ny*deltay)
-    # gridrgb = numarray.reshape(gridrgb,(nx,ny))
-    # print 'SHAPE:',gridrgb.shape
-    # print 'gridrgb:', gridrgb
-    imshow(gridrgb, cmap=cmap, origin='lower', extent=extent, interpolation=interpolation)
-    colorbar()
-
-
-def imshow_2d_grid_values(gridvalues, x0, y0, deltax, deltay, interpolation='nearest', cmap = cm.jet):
-    nx = numarray.size(gridvalues,0)
-    ny = numarray.size(gridvalues,1)
-    extent = (x0-.5*deltax, x0+nx*deltax, y0-.5*deltay, y0+ny*deltay)
-    print 'gridval type', type(gridvalues)
-    gridvalues = numarray.reshape(gridvalues,(nx,ny))
-    # print 'SHAPE:',gridvalues.shape
-    # print 'gridvalues:', gridvalues
-    imshow(gridvalues, cmap=cmap, origin='lower', extent=extent, interpolation=interpolation)
-    colorbar()
-
-def plot_2d_points(pointlist, style='bo'):
-    x, y = zip(*pointlist)
-    plot(x, y, style)
-
-def plot_2d_class_points(pointlist, styles):
-    classnum = 0
-    for style in styles:
-        points_c = [ [x,y] for x,y,c in pointlist if c==classnum]
-        if len(points_c)==0:
-            break
-        # print 'points',classnum,':',points_c
-        plot_2d_points(points_c, style)
-        classnum += 1
-
-
-# def generate_2D_color_plot(x_y_color):
-
-# def plot_2D_decision_surface(training_points
-
-def main():
-    print "Still under development. Do not use!!!"
-    extent = (1, 25, -5, 25)
-
-    x = arange(7)
-    y = arange(5)
-    X, Y = meshgrid(x,y)
-    Z = rand( len(x), len(y))
-    # pcolor_classic(X, Y, transpose(Z))
-    #show()
-    #print 'pcolor'
-
-    for interpol in ['bicubic',
-                     'bilinear',
-                     'blackman100',
-                     'blackman256',
-                     'blackman64',
-                     'nearest',
-                     'sinc144',
-                     'sinc256',
-                     'sinc64',
-                     'spline16',
-                     'spline36']:
-
-        raw_input()
-        print interpol
-        clf()
-        imshow(Z, cmap=cm.jet, origin='upper', extent=extent, interpolation=interpol)
-        markers = [(15.9, 14.5), (16.8, 15), (20,20)]
-        x,y = zip(*markers)
-        plot(x, y, 'o')
-        plot(rand(20)*25, rand(20)*25, 'o') 
-        show()
-        # draw()
-
-    show()
-
-
-if __name__ == "__main__":
-    main()
-
-
-
-#t = arange(0.0, 5.2, 0.2)
-
-# red dashes, blue squares and green triangles
-#plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
-#show()
-

Added: trunk/python_modules/plearn/plotting/matplotlib_utils.py
===================================================================
--- trunk/python_modules/plearn/plotting/matplotlib_utils.py	2008-05-30 22:32:35 UTC (rev 9078)
+++ trunk/python_modules/plearn/plotting/matplotlib_utils.py	2008-05-31 21:21:42 UTC (rev 9079)
@@ -0,0 +1,155 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
+# matplotlib_utils.py
+# Copyright (C) 2005 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+
+# Author: Pascal Vincent
+
+# from array import *
+#from numpy.numarray import *
+
+from plearn.plotting.numpy_utils import *
+import matplotlib
+# matplotlib.interactive(True)
+#matplotlib.use('TkAgg')
+#matplotlib.use('GTK')
+from pylab import *
+
+
+def imshow_xymagnitude(regular_xymagnitude, interpolation='nearest', cmap = cm.jet):
+    grid_values, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xymagnitude)
+    # print 'In imshow_xymagnitude: ', type(regular_xymagnitude), type(grid_values)
+    imshow_2d_grid_values(grid_values, x0, y0, deltax, deltay, interpolation, cm.jet)
+    
+def imshow_xyrgb(regular_xyrgb, interpolation='nearest'):
+    grid_rgb, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xyrgb)
+    # print 'grid_rgb shape=',grid_rgb.shape
+    imshow_2d_grid_rgb(grid_rgb, x0, y0, deltax, deltay, interpolation, cm.jet)
+    
+def contour_xymagnitude(regular_xymagnitude):
+    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
+    clabel(contour(x, y, gridvalues))
+    
+def contourf_xymagnitude(regular_xymagnitude):
+    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
+    contourf(x, y, gridvalues)
+    colorbar()
+
+def imshow_2d_grid_rgb(gridrgb, x0, y0, deltax, deltay, interpolation='nearest', cmap = cm.jet):
+    nx = numarray.size(gridrgb,0)
+    ny = numarray.size(gridrgb,1)
+    extent = (x0-.5*deltax, x0+nx*deltax, y0-.5*deltay, y0+ny*deltay)
+    # gridrgb = numarray.reshape(gridrgb,(nx,ny))
+    # print 'SHAPE:',gridrgb.shape
+    # print 'gridrgb:', gridrgb
+    imshow(gridrgb, cmap=cmap, origin='lower', extent=extent, interpolation=interpolation)
+    colorbar()
+
+def imshow_2d_grid_values(gridvalues, x0, y0, deltax, deltay, interpolation='nearest', cmap = cm.jet):
+    nx = numarray.size(gridvalues,0)
+    ny = numarray.size(gridvalues,1)
+    extent = (x0-.5*deltax, x0+nx*deltax, y0-.5*deltay, y0+ny*deltay)
+    print 'gridval type', type(gridvalues)
+    gridvalues = numarray.reshape(gridvalues,(nx,ny))
+    # print 'SHAPE:',gridvalues.shape
+    # print 'gridvalues:', gridvalues
+    imshow(gridvalues, cmap=cmap, origin='lower', extent=extent, interpolation=interpolation)
+    colorbar()
+
+def plot_2d_points(pointlist, style='bo'):
+    x, y = zip(*pointlist)
+    plot(x, y, style)
+
+def plot_2d_class_points(pointlist, styles):
+    classnum = 0
+    for style in styles:
+        points_c = [ [x,y] for x,y,c in pointlist if c==classnum]
+        if len(points_c)==0:
+            break
+        # print 'points',classnum,':',points_c
+        plot_2d_points(points_c, style)
+        classnum += 1
+
+
+# def generate_2D_color_plot(x_y_color):
+
+# def plot_2D_decision_surface(training_points
+
+def main():
+    print "Still under development. Do not use!!!"
+    extent = (1, 25, -5, 25)
+
+    x = arange(7)
+    y = arange(5)
+    X, Y = meshgrid(x,y)
+    Z = rand( len(x), len(y))
+    # pcolor_classic(X, Y, transpose(Z))
+    #show()
+    #print 'pcolor'
+
+    for interpol in ['bicubic',
+                     'bilinear',
+                     'blackman100',
+                     'blackman256',
+                     'blackman64',
+                     'nearest',
+                     'sinc144',
+                     'sinc256',
+                     'sinc64',
+                     'spline16',
+                     'spline36']:
+
+        raw_input()
+        print interpol
+        clf()
+        imshow(Z, cmap=cm.jet, origin='upper', extent=extent, interpolation=interpol)
+        markers = [(15.9, 14.5), (16.8, 15), (20,20)]
+        x,y = zip(*markers)
+        plot(x, y, 'o')
+        plot(rand(20)*25, rand(20)*25, 'o') 
+        show()
+        # draw()
+
+    show()
+
+
+if __name__ == "__main__":
+    main()
+
+
+
+#t = arange(0.0, 5.2, 0.2)
+
+# red dashes, blue squares and green triangles
+#plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
+#show()
+

Added: trunk/python_modules/plearn/plotting/mayavi_utils.py
===================================================================
--- trunk/python_modules/plearn/plotting/mayavi_utils.py	2008-05-30 22:32:35 UTC (rev 9078)
+++ trunk/python_modules/plearn/plotting/mayavi_utils.py	2008-05-31 21:21:42 UTC (rev 9079)
@@ -0,0 +1,56 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
+# mayavi_utils.py
+# Copyright (C) 2005 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+
+# Author: Pascal Vincent
+
+from plearn.plotting.numpy_utils import xymagnitude_to_x_y_grid
+
+try:
+    import mayavi.tools.imv
+    surf = mayavi.tools.imv.surf
+except:
+    print "Failed to properly import mayavi.tools.imv.surf"
+    print "Defining a surf that does nothing"
+    def surf(*args, **kargs):
+        pass
+    
+def surfplot_xymagnitude(regular_xymagnitude):
+    x,y,gridvalues = xymagnitude_to_x_y_grid(regular_xymagnitude)
+    surf(x, y, gridvalues)
+
+
+if __name__ == "__main__":
+    pass
+
+

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2008-05-30 22:32:35 UTC (rev 9078)
+++ trunk/python_modules/plearn/plotting/netplot.py	2008-05-31 21:21:42 UTC (rev 9079)
@@ -1,5 +1,7 @@
 from pylab import *
-from numpy.numarray import *
+from math import *
+from numpy import *
+#from numpy.numarray import *
 
 
 #################
@@ -123,10 +125,17 @@
     return toReturn
 
 
-def plotRowsAsImages(X, figtitle="", nrows=10, ncols=20, img_width=None,
+def plotRowsAsImages(X, 
+                     img_height, img_width,
+                     nrows=10, ncols=20,
+                     figtitle="",
                      show_colorbar=False, disable_ticks=True, colormap = cm.gray,
-                     luminance_scale_mode = 0, vmin=None, vmax=None):
+                     luminance_scale_mode = 0, vmin=None, vmax=None,
+                     transpose_img=False):
     """
+    Will plot rows of X in a nrows x ncols grid.
+    The first img_height x img_width elements of each roe are interpreted as
+    greyscale values of a img_height x img_width image.
     If provided, vmin and vmax will be used for luminance scale (see imshow)
     If not povided, they will be set depending on luminance_scale_mode:
        0: vmin and vmax are left None, i.e. luminance
@@ -135,16 +144,15 @@
        2: vmin and vmax will be set to +-min of X
           or +-max of X (whichever is bigger).
     """
-    #some calculations for plotting
-    img_size = len(X[0])
-    if img_width is None:
-        img_width = math.sqrt(img_size)
-    img_height = img_size/img_width
 
+    inputs = array(X[0:min(len(X),nrows*ncols)])
+    if len(inputs[0])>img_height*img_width:
+        inputs = inputs[:,0:(img_height*img_width)]
+        
     if vmin is None and luminance_scale_mode!=0:
         print 'luminanca_scale_mode = ',luminance_scale_mode
-        vmin = X.min()
-        vmax = X.max()
+        vmin = inputs.min()
+        vmax = inputs.max()
         print 'filter value range: ',vmin,',',vmax
         if luminance_scale_mode==2:
             vmax = max(abs(vmin),abs(vmax))
@@ -156,11 +164,13 @@
     subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9,
                     wspace=0.01, hspace=0.01)
     
-    for i in range(min(len(X),nrows*ncols)):
-        row = X[i]
+    for i in range(min(len(inputs),nrows*ncols)):
+        row = inputs[i]
         subplot(nrows,ncols,i+1)
         # print "Reshaping: ",row.shape,"->",img_height,'x',img_width
         img = reshape(row,(img_height,img_width))
+        if transpose_img:
+            img = transpose(img)        
         imshow(img, interpolation="nearest", cmap = colormap, vmin = vmin, vmax = vmax)
             
         # if show_colorbar and vmin is None:
@@ -428,20 +438,24 @@
       
 class showRowsAsImages:
 
-    def __init__(self, X, figtitle="",
+    def __init__(self, X, 
+                 img_height,
+                 img_width,
                  nrows = 10, ncols = 20,
                  startidx = 0,
-                 img_width=None,
+                 figtitle="",
                  luminance_scale_mode=0,
                  colormaps = [cm.gray, cm.jet],
-                 vmin = None, vmax = None):
+                 vmin = None, vmax = None,
+                 transpose_img=False):
 
         self.X = X
-        self.figtitle = figtitle
+        self.img_height = img_height
+        self.img_width = img_width
         self.nrows = nrows
         self.ncols = ncols
+        self.figtitle = figtitle
         self.startidx = startidx
-        self.img_width = img_width
 
         # appearance control
         self.luminance_scale_mode = luminance_scale_mode
@@ -452,6 +466,7 @@
         self.disable_ticks = True
         self.vmin = vmin
         self.vmax = vmax
+        self.transpose_img = transpose_img
 
         # plot it
         self.draw()      
@@ -467,16 +482,18 @@
         endidx = min(self.startidx+self.nrows*self.ncols, len(self.X))        
         title = self.figtitle+" ("+str(self.startidx)+" ... "+str(endidx-1)+")"
         plotRowsAsImages(self.X[self.startidx : endidx],
-                         figtitle = title,
+                         img_height = self.img_height,
+                         img_width = self.img_width,
                          nrows = self.nrows,
                          ncols = self.ncols,
-                         img_width=self.img_width,
+                         figtitle = title,
                          luminance_scale_mode = self.luminance_scale_mode,
                          show_colorbar = self.show_colorbar,
                          disable_ticks = self.disable_ticks,
                          colormap = self.colormaps[self.cmapchoice],
                          vmin = self.vmin,
-                         vmax = self.vmax
+                         vmax = self.vmax,
+                         transpose_img = self.transpose_img
                          )
         print "Plotted,"
         draw()
@@ -511,6 +528,9 @@
             self.show_colorbar = not self.show_colorbar
             self.draw()
         elif char == 't':
+            self.transpose_img = not self.transpose_img
+            self.draw()
+        elif char == 'i':
             self.disable_ticks = not self.disable_ticks
             self.draw()
         elif char == 's':
@@ -524,13 +544,14 @@
             * KEYS
             *  right : show next filters
             *  left  : show previous filters
+            *  t     : tranpose images
             *  c     : change colormap
             *  s     : cycle through luminance scale mode
             *          0 independent luminance scaling for each
             *          1 min-max luminance scaling across display
             *          2 +-min or +- max (largest range)
             *  b     : toggle showing colorbar 
-            *  t     : toggle showing ticks
+            *  i     : toggle showing ticks
             *
             * Close window to stop.
             *******************************************************

Added: trunk/python_modules/plearn/plotting/numpy_utils.py
===================================================================
--- trunk/python_modules/plearn/plotting/numpy_utils.py	2008-05-30 22:32:35 UTC (rev 9078)
+++ trunk/python_modules/plearn/plotting/numpy_utils.py	2008-05-31 21:21:42 UTC (rev 9079)
@@ -0,0 +1,174 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
+# matplotlib_utils.py
+# Copyright (C) 2005 Pascal Vincent
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+
+# Author: Pascal Vincent
+
+# from array import *
+import numpy.numarray as numarray
+from numpy.numarray import *
+
+threshold = 0
+
+def margin(scorevec):
+    if len(scorevec)==1:
+        return abs(scorevec[0]-threshold)
+    else:
+        sscores = sort(scorevec)    
+        return sscores[-1]-sscores[-2]
+
+def winner(scorevec):
+    if len(scorevec)==1:
+        if scorevec[0]>threshold:
+            return 1
+        else:
+            return 0
+    else:
+        return argmax(scorevec)
+
+def xyscores_to_winner_and_magnitude(xyscores):
+    return array([ (v[0], v[1], winner(v[2:]),max(v[2:])) for v in xyscores ])
+
+def xyscores_to_winner_and_margin(xyscores):
+    return array([ (v[0], v[1], winner(v[2:]),margin(v[2:])) for v in xyscores ])
+
+def regular_xyval_to_2d_grid_values(xyval):
+    """Returns (grid_values, x0, y0, deltax, deltay)"""
+    xyval = numarray.array(xyval)
+    n = len(xyval)
+    x = xyval[:,0]
+    y = xyval[:,1]
+    values = xyval[:,2:].copy()
+    # print "type(values)",type(values)
+    valsize = numarray.size(values,1)
+    x0 = x[0]
+    y0 = y[0]
+
+    k = 1
+    if x[1]==x0:
+        deltay = y[1]-y[0]
+        while x[k]==x0:
+            k = k+1
+        deltax = x[k]-x0
+        ny = k
+        nx = n // ny
+        # print 'A) nx,ny:',nx,ny
+        values.shape = (nx,ny,valsize)
+        # print "A type(values)",type(values)
+        values = numarray.transpose(values,(1,0,2))
+        # print "B type(values)",type(values)
+    elif y[1]==y0:
+        deltax = x[1]-x[0]
+        while y[k]==y0:
+            k = k+1
+        deltay = y[k]-y0
+        nx = k
+        ny = n // nx
+        # print 'B) nx,ny:',nx,ny
+        values.shape = (ny,nx,valsize)
+        # print "C type(values)",type(values)
+        values = numarray.transpose(values,(1,0,2))
+        # print "D type(values)",type(values)
+    else:
+        raise ValueError("Strange: x[1]!=x0 and y[1]!=y0 this doesn't look like a regular grid...")
+
+    print 'In regular_xyval_to_2d_grid_values: ', type(xyval), type(values)
+    return values, x0, y0, deltax, deltay
+
+
+def divide_by_mean_magnitude(xymagnitude):
+    mag = xymagnitude[:,2]
+    meanval = mag.mean()
+    mag *= 1./meanval
+    return meanval
+
+def divide_by_max_magnitude(xymagnitude):
+    mag = xymagnitude[:,2]
+    maxval = mag.max()
+    mag *= 1./maxval
+    return maxval
+
+def transform_magnitude_into_covered_percentage(xymagnitude):
+    magnitudes = []
+    l = len(xymagnitude)
+    for i in xrange(l):
+        row = xymagnitude[i]
+        magnitudes.append([row[-1],i])
+    magnitudes.sort()
+    # magnitude.reverse()
+    cum = 0
+    for row in magnitudes:
+        mag, i = row
+        cum += mag
+        row[0] = cum
+    for mag,i in magnitudes:
+        xymagnitude[i][-1] = mag/cum
+    return cum
+        
+def classcolor(winner,margin=0):
+    colors = { 0: [0.5, 0.5, 1.0],
+               1: [1.0, 0.5, 0.5],
+               2: [0.5, 1.0, 0.5],
+             }
+    return colors[winner]
+    
+def xy_winner_magnitude_to_xyrgb(xy_winner_margin):
+    res = []
+    for x,y,w,m in xy_winner_margin:
+        res.append([x,y]+classcolor(w,m))
+    return res
+
+def xymagnitude_to_x_y_grid(regular_xymagnitude):
+    gridvalues, x0, y0, deltax, deltay = regular_xyval_to_2d_grid_values(regular_xymagnitude)
+    nx = numarray.size(gridvalues,0)
+    ny = numarray.size(gridvalues,1)
+    gridvalues = numarray.reshape(gridvalues,(nx,ny))
+    x = numarray.arange(x0,x0+nx*deltax-1e-6,deltax)
+    y = numarray.arange(y0,y0+ny*deltay-1e-6,deltay)
+    # print "x = ",x
+    # print "y = ",y
+    # print "z = ",gridvalues
+    # print "type(x) = ",type(x)
+    # print "type(y) = ",type(y)
+    # print "type(z) = ",type(gridvalues)
+    # imv.view(gridvalues)
+    return x, y, gridvalues
+    
+
+def main():
+    pass
+
+if __name__ == "__main__":
+    main()
+
+



From plearner at mail.berlios.de  Sat May 31 23:22:58 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 31 May 2008 23:22:58 +0200
Subject: [Plearn-commits] r9080 - in trunk/scripts: . EXPERIMENTAL
Message-ID: <200805312122.m4VLMwQT013164@sheep.berlios.de>

Author: plearner
Date: 2008-05-31 23:22:57 +0200 (Sat, 31 May 2008)
New Revision: 9080

Added:
   trunk/scripts/EXPERIMENTAL/dcaexperiment.py
   trunk/scripts/EXPERIMENTAL/linearfilters.py
Modified:
   trunk/scripts/EXPERIMENTAL/iTraining.py
   trunk/scripts/EXPERIMENTAL/itest.py
   trunk/scripts/EXPERIMENTAL/itest2.py
   trunk/scripts/pyplot
   trunk/scripts/show_rows_as_images.py
Log:
Minor improvements/fixed to plotting scripts.
And added currently experimental plotting scripts.



Added: trunk/scripts/EXPERIMENTAL/dcaexperiment.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2008-05-31 21:21:42 UTC (rev 9079)
+++ trunk/scripts/EXPERIMENTAL/dcaexperiment.py	2008-05-31 21:22:57 UTC (rev 9080)
@@ -0,0 +1,442 @@
+#!/usr/bin/env python
+
+import os ,sys, time, matplotlib, math, copy
+from matplotlib.pylab import *
+from matplotlib.colors import *
+from plearn.plotting.netplot import plotRowsAsImages, showRowsAsImages
+from plearn.vmat.PMat import *
+
+import numpy
+from numpy import *
+#from numpy.linalg import *
+import numpy.numarray
+#from plearn.vmat.PMat import *
+
+# from numpy.numarray import *
+# from numarray import *
+#from numarray.linear_algebra import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+
+#from plearn.plotting import *
+#from copy import *
+
+#from plearn.plotting.netplot import showRowsAsImages
+
+server_command = 'plearn_exp server'
+serv = launch_plearn_server(command = server_command)
+
+class DCAExperiment:
+
+    def __init__(self,
+                 X,
+                 seed=1827,
+                 ncomponents=2,
+                 constrain_norm_type=-1,
+                 cov_transformation_type="cov",
+                 diag_weight = -1.0,
+                 diag_nonlinearity="square", 
+                 diag_premul = 1.0,
+                 offdiag_weight=1.0,
+                 offdiag_nonlinearity="square", 
+                 offdiag_premul = 1.0,
+                 lr=0.01, nsteps=1, optimizer_nsteps=1,
+                 epsilon=1e-4, nu=0,
+                 img_height=None,
+                 img_width=None):
+
+        self.nsteps = nsteps
+
+        #some calculations for plotting
+        if img_height is None:
+            img_size = len(X[0])
+            img_width = math.sqrt(img_size)
+            img_height = img_size/img_width
+        self.img_height = img_height
+        self.img_width = img_width
+
+        if isinstance(X, numpy.ndarray):
+            self.d = len(X[0])
+            self.X = X            
+            vmat = pl.MemoryVMatrix(data = X, inputsize=len(X[0]), targetsize=0, weightsize=0)
+        else:
+            self.d = 100000
+            vmat = X
+
+        if self.d==2:
+            C = cov(X, rowvar=0, bias=1)
+            ei, eiv = eig(C)
+            self.principal_components = eiv.T
+
+        dcaspec = pl.DiverseComponentAnalysis(
+            seed = seed,
+            ncomponents = ncomponents,
+            constrain_norm_type=constrain_norm_type,
+            cov_transformation_type=cov_transformation_type,
+            diag_weight = diag_weight,
+            diag_nonlinearity = diag_nonlinearity, 
+            diag_premul = diag_premul,
+            offdiag_weight = offdiag_weight,
+            offdiag_nonlinearity = offdiag_nonlinearity, 
+            offdiag_premul = offdiag_premul,
+            optimizer = pl.GradientOptimizer(start_learning_rate = lr,
+                                             decrease_constant = 0,
+                                             nstages = optimizer_nsteps),
+            epsilon = epsilon,
+            nu = nu
+            )
+
+        self.dca = serv.new(dcaspec)
+        print "Setting training set..."
+        self.dca.setTrainingSet(vmat,1)
+
+        W = self.dca.getVarValue('W')
+        # print "W[0]=",W[0]
+        print "Squared norm of first row of W: ",sum(W[0]*W[0])
+        
+        self.dca.nstages = 1
+        print "Training to stage 1"
+        self.dca.train()
+        print "Training to stage 1 DONE."
+        print "Squared norm of first row of W: ",sum(W[0]*W[0])
+        self.training_curve = []
+
+        self.datafig = 2
+        self.traincurvefig = 1
+        self.filterfig = 3
+        self.Wfig = 4
+        self.Cytfig = 5
+        self.Cxfig = 6
+        self.Cythist = 7
+
+        self.draw()
+        
+        figure(self.traincurvefig)
+        connect('key_press_event', self.keyPressed)
+
+        if self.d==2:
+            figure(self.datafig)
+            connect('key_press_event', self.keyPressed)
+            # connect('button_press_event', self.__clicked)
+
+        figure(self.filterfig)
+        connect('key_press_event', self.keyPressed)
+        #figure(self.Wfig)
+        figure(self.Cytfig)
+        #figure(self.Cxfig)
+        #figure(self.Cythist)
+
+        # start interactive loop
+        show()
+
+
+    def draw(self):
+        figure(self.traincurvefig)
+        clf()
+        # print 'stages:',arange(2,2+len(self.training_curve))
+        # print 'losses:',array(self.training_curve)
+        plot(arange(2,2+len(self.training_curve)),array(self.training_curve),'g-')
+        title("Training curve")
+        xlabel("stage")
+        ylabel("optimized cost L")
+        draw()
+
+        if(self.d==2):
+            figure(self.datafig)
+            clf()
+            axis('equal')
+            plot(self.X[:,0], self.X[:,1], 'b.')
+            mu = self.dca.mu
+            W = self.dca.getVarValue('W')
+            for direction in self.principal_components:
+                arrow( mu[0], mu[1], direction[0], direction[1],
+                       linewidth=2, edgecolor='black', facecolor='black')
+            i = 0
+            for w in W:
+                arrow( mu[0], mu[1], w[0], w[1],
+                       linewidth=2, edgecolor='red', facecolor='red')
+                text(mu[0]+w[0], mu[1]+w[1], str(i))
+                i = i+1
+            title("Data and learnt projection directions")
+            draw()
+        
+    def printState(self): 
+        print "----------------------------------"
+        print "stage",self.dca.nstages
+        print "L =",self.dca.getVarValue('L')[0,0]
+        if self.d<=2:
+            print "Cyt = "
+            print self.dca.getVarValue('Cyt')
+            print "gradient Cyt = "
+            print self.dca.getVarGradient('Cyt')
+            print "W = "
+            print self.dca.getVarValue('W')
+            print "gradient W = "
+            print self.dca.getVarGradient('W')
+       
+    def trainN(self, n=1):
+        lr = self.dca.getOption("optimizer.start_learning_rate")
+        print "-------------------------------"
+        print "TRAINING for",n,"steps at lr=",lr
+        while n>0:
+            self.dca.nstages = self.dca.nstages+1
+            self.dca.train()
+            loss = self.dca.getVarValue('L')[0,0]
+            self.training_curve.append(loss)
+            n = n-1
+        self.printState()
+        self.draw()
+
+    def changeSteps(self):
+        print "********************************"
+        print "new nsteps (",self.nsteps,")?",
+        self.nsteps = input()
+        lr = self.dca.getOption("optimizer.start_learning_rate")
+        print "new lr (",lr,")?",
+        lr = input()
+        self.dca.changeOptions({"optimizer.start_learning_rate":lr})
+        self.dca.changeOptions({"optimizer.learning_rate":lr})
+        self.draw()
+
+    def showFilters(self):
+        W = self.dca.getVarValue('W')
+
+        #figure(self.Wfig)
+        #clf()
+        #title('W')
+        #imshow(W, interpolation="nearest", cmap = cm.jet)
+        #colorbar()
+        #draw()
+
+        figure(self.Cytfig)
+        Cyt = self.dca.getVarValue('Cyt')
+        l = len(Cyt)
+        diagvals = []
+        offdiagvals = []
+        for i in xrange(l):            
+            for j in xrange(i):
+                offdiagvals.append(Cyt[i,j])
+            diagvals.append(Cyt[i,i])
+
+        clf()
+        subplot(1,3,1)
+        title('Cyt')
+        imshow(Cyt, interpolation="nearest", cmap = cm.jet)
+        colorbar()
+
+        subplot(1,3,2)
+        title('Cyt diagonal values histogram')
+        hist(diagvals)
+        
+        subplot(1,3,3)
+        title('Cyt off-diagonal values histogram')
+        hist(offdiagvals)
+
+        draw()
+        
+        #figure(self.Cxfig)
+        #clf()
+        #title('Cx')
+        #imshow(self.dca.getVarValue('Cx'), interpolation="nearest", cmap = cm.jet)
+        #colorbar()
+        #draw()
+
+        figure(self.filterfig)
+        clf()
+        plotRowsAsImages(W, 
+                         img_height = self.img_height,
+                         img_width = self.img_width,
+                         nrows=5, ncols=10,
+                         figtitle="filters",
+                         show_colorbar=False, disable_ticks=True, colormap = cm.gray,
+                         luminance_scale_mode = 0, vmin=None, vmax=None)        
+        draw()
+
+    def save(self):
+        print "*** SAVE TO FILE ***"
+        print "normalize output (0 or 1)? ",
+        normalize = int(raw_input())
+        self.dca.changeOptions({'normalize':normalize})        
+        print "filename (.psave)? ",
+        filename = raw_input()
+        self.dca.save(filename,'plearn_ascii')
+        print "*** SAVING DONE ***"
+
+    def keyPressed(self, event):
+        char = event.key
+        print 'Pressed',char
+        if char == ' ':
+            self.trainN(self.nsteps)
+        elif char == '1':
+            self.trainN(10*self.nsteps)
+        elif char == '2':
+            self.trainN(20*self.nsteps)
+        elif char == '3':
+            self.trainN(30*self.nsteps)
+        elif char == '4':
+            self.trainN(40*self.nsteps)
+        elif char == '5':
+            self.trainN(50*self.nsteps)
+        elif char == '6':
+            self.trainN(60*self.nsteps)
+        elif char == '7':
+            self.trainN(70*self.nsteps)
+        elif char == '8':
+            self.trainN(80*self.nsteps)
+        elif char == '9':
+            self.trainN(90*self.nsteps)
+        elif char == 'o':
+            self.changeSteps()
+        elif char == 'w':
+            self.showFilters()
+        elif char == 's':
+            self.save()
+        elif char == '':
+            pass
+        else:
+            print """
+            *******************************************************
+            * KEYS
+            *   spacebar: does nsteps training steps
+            *   '1'     : does 10*nsteps training steps
+            *   '2'     : does 20*nsteps training steps
+            *   ....           ...
+            *   '9'     : does 90*nsteps training steps
+            *   'o'     : to change optimizaiton nsteps and lr
+            *   'f'     : interactive show filters
+            *   's'     : save learner to file
+            * Close window to stop.
+            *******************************************************
+            """
+
+
+def generate2dNormalAtAngle(npoints=300, angle=None, stddev1=1, stddev2=0.1, mean=[0,0]):    
+    if angle is None:
+        angle = random.uniform(0., 2*math.pi)
+    v1 = array([stddev1*math.cos(angle), stddev1*math.sin(angle)])
+    v2 = array([stddev2*math.sin(angle), stddev2*math.cos(angle)])
+    V = vstack([v1,v2])
+    cov = 0.5*dot(V.T,V)
+    X = random.multivariate_normal(mean,cov,npoints)
+    return X
+    
+def generate2dNormal(npoints=200):
+    # rnd = random.RandomState(seed)
+    mean = random.normal(0, 1, 2)
+    #mean = array([0,0])
+    cov = random.uniform(-0.2,0.2,(2,2))
+    X = random.multivariate_normal(mean,cov,npoints)
+    #X = random.normal(0,1,(200,2))
+    #W = random.uniform(-1,1,(2,2))
+    #b = random.uniform(-0.5, 0.5, (2,1))
+    #X = dot(X,W)+b.T
+    return X
+
+def loadInputs(datapmatfile, inputsize=None):
+    data = load_pmat_as_array(datapmatfile)
+    if inputsize is None:
+        inputsize = len(data[0])-1
+    X = data[:,0:inputsize]
+    return X
+
+def getDataSet(dataset):
+    """dataset is a string that specifies either a .pmat or is of the form data_seed:ngaussians"""
+    data_seed = None
+    try:
+        data_seed, ngaussians = dataset.split(':')
+        data_seed = int(data_seed)
+        ngaussians = int(ngaussians)
+    except ValueError:
+        pass
+
+    if data_seed is None:
+        X = PMat(dataset)
+        print "** flushing"
+        X.flush()
+        print "** flushing DONE"
+        # X = loadInputs(dataset)
+    else:
+        random.seed(data_seed)
+        if ngaussians==0:
+            X = random.multivariate_normal(array([0,0]),eye(2),1000)
+        elif ngaussians>0:
+            pointgroups = [ generate2dNormal(200) for i in range(ngaussians) ]
+            X = vstack(pointgroups)
+        else: # ngaussians<0
+            ngaussians = -ngaussians
+            pointgroups = [ generate2dNormalAtAngle(200) for i in range(ngaussians) ]
+            X = vstack(pointgroups)
+            
+    return X
+    
+
+
+####################
+### main program ###
+
+if __name__ == "__main__":
+
+    try:
+        dataset, learner_seed, ncomponents, constrain_norm_type, cov_transformation_type, diag_weight, diag_nonlinearity, diag_premul, offdiag_weight, offdiag_nonlinearity, offdiag_premul = sys.argv[1:]        
+
+        learner_seed = int(learner_seed)
+        ncomponents = int(ncomponents)
+        constrain_norm_type = float(constrain_norm_type)
+        diag_weight = float(diag_weight)
+        diag_premul = float(diag_premul)
+        offdiag_weight = float(offdiag_weight)
+        offdiag_premul = float(offdiag_premul)
+        
+
+
+
+    except:
+        print "Usage: "+sys.argv[0]+" dataset learner_seed ncomponents constrain_norm_type cov_transformation_type diag_weight diag_nonlinearity diag_premul offdiag_weight offdiag_nonlinearity offdiag_premul"
+        print "  dataset can be either a .pmat or data_seed:ngaussians"
+        print """  constrain_norm_type controls how to constrain the norms of rows of W:
+        -1:constrained source;
+        -2:explicit normalization;
+        >0:ordinary weight decay"""
+        print """  cov_transformation_type controls the kind of transformation to apply to covariance matrix
+        cov: no transformation (keep covariance)
+        corr: transform into correlations, but keeping variances on the diagonal.
+        squaredist: do a 'squared distance kernel' kind of transformation."""
+        print "Ex: "+sys.argv[0]+" 123:1    123 2    -1 cov     -1 square 1       1 square 1"
+        print "Ex: "+sys.argv[0]+" 121:-2    123 4    -1 squaredist     0 exp 1       1 exp -1.6"
+        raise
+    # sys.exit()
+
+    print "Getting data"
+    X = getDataSet(dataset)
+    print "Data OK."
+
+    DCAExperiment(X,
+                  seed=learner_seed, 
+                  ncomponents=ncomponents,
+                  constrain_norm_type=constrain_norm_type,
+                  cov_transformation_type=cov_transformation_type,
+                  diag_weight = diag_weight,
+                  diag_nonlinearity = diag_nonlinearity, 
+                  diag_premul = diag_premul,
+                  offdiag_weight = offdiag_weight,
+                  offdiag_nonlinearity = offdiag_nonlinearity, 
+                  offdiag_premul = offdiag_premul,
+                  lr=0.01, nsteps=1, optimizer_nsteps=10)
+    
+
+#     try:
+#         datapmatfile, inputsize, filtertype, param = sys.argv[1:]
+#         inputsize = int(inputsize)
+#         param = float(param)
+#     except:
+#         print "Usage: "+sys.argv[0]+" <datafile.pmat> <inputsize> <filtertype> <param>"
+#         print """
+#         For filtertype 'PCA', param is the noise added to the diagonal of the covariance matrix
+#         For filtertype 'denoising', param is the destruction proportion.
+#         """
+#         raise
+#     # sys.exit()
+
+
+


Property changes on: trunk/scripts/EXPERIMENTAL/dcaexperiment.py
___________________________________________________________________
Name: svn:executable
   + *

Modified: trunk/scripts/EXPERIMENTAL/iTraining.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/iTraining.py	2008-05-31 21:21:42 UTC (rev 9079)
+++ trunk/scripts/EXPERIMENTAL/iTraining.py	2008-05-31 21:22:57 UTC (rev 9080)
@@ -8,7 +8,7 @@
 
 from plearn.io.server import *
 from plearn.pyplearn import *
-from plearn.plotting import *
+from plearn.plotting.matplotlib_utils import *
 from numpy import *
 from pickle import *
 from copy import *

Modified: trunk/scripts/EXPERIMENTAL/itest.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/itest.py	2008-05-31 21:21:42 UTC (rev 9079)
+++ trunk/scripts/EXPERIMENTAL/itest.py	2008-05-31 21:22:57 UTC (rev 9080)
@@ -6,10 +6,7 @@
 # from numarray import *
 # from numarray.linear_algebra import *
 
-from plearn.io.server import *
-from plearn.pyplearn import *
-from plearn.plotting import *
-from copy import *
+from plearn.plotting.matplotlib_utils import *
 
 from iTraining import *
 from generators import *
@@ -24,14 +21,11 @@
 #iTraining2.py
 #to control interactively the training process
 
-import os, sys, time, matplotlib, math, numpy ,copy
-
 from matplotlib.pylab import plot,show,draw,close,text,scatter,colorbar
 from matplotlib.colors import *
 
 from plearn.io.server import *
 from plearn.pyplearn import *
-from plearn.plotting import *
 from numpy import *
 from pickle import *
 from copy import *

Modified: trunk/scripts/EXPERIMENTAL/itest2.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/itest2.py	2008-05-31 21:21:42 UTC (rev 9079)
+++ trunk/scripts/EXPERIMENTAL/itest2.py	2008-05-31 21:22:57 UTC (rev 9080)
@@ -8,7 +8,7 @@
 
 from plearn.io.server import *
 from plearn.pyplearn import *
-from plearn.plotting import *
+from plearn.plotting.matplotlib_utils import *
 from copy import *
 
 from iTraining import *

Added: trunk/scripts/EXPERIMENTAL/linearfilters.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/linearfilters.py	2008-05-31 21:21:42 UTC (rev 9079)
+++ trunk/scripts/EXPERIMENTAL/linearfilters.py	2008-05-31 21:22:57 UTC (rev 9080)
@@ -0,0 +1,76 @@
+#!/usr/bin/env python
+
+import sys
+from numpy import *
+from numpy.linalg import *
+from plearn.vmat.PMat import load_pmat_as_array
+from plearn.plotting.netplot import showRowsAsImages
+
+#server_command = 'plearn_exp server'
+#serv = launch_plearn_server(command = server_command)
+
+def computeAndShowFilters(datapmatfile, img_height, img_width, filtertype, lambd, nu):
+    data = load_pmat_as_array(datapmatfile)
+    inputs = data[:,0:img_height*img_width]
+    C = cov(inputs, rowvar=0, bias=1)
+    if(filtertype=="PCA"):
+        filters = computePCAFilters(C, lambd, nu)
+    elif(filtertype=="denoising"):
+        filters = computeDenoisingFilters(C, lambd, nu)
+    else:
+        raise ValueError("Invalid filtertype "+filtertype)    
+    showRowsAsImages(filters, img_height, img_width, figtitle="Filters")
+
+def mycov(inputs, centered=True):
+    if centered:
+        C = cov(inputs, rowvar=0, bias=1)
+    else:
+        C = dot(inputs.T, inputs)
+        C *= 1.0/len(inputs)        
+    return C
+
+def computePCAFilters(C, lambd=1e-6, nu=0):
+    C = C+diag(len(C)*[lambd])
+    Cd = C.diagonal()
+    C2 = C*(1.0-nu)
+    # copy back intial diagonal
+    for i in range(len(Cd)):
+        C2[i,i] = Cd[i]
+    eigvals, eigvecs = eig(C2)
+    return real(eigvecs.T)
+
+def computeDenoisingFilters(C, lambd=1e-6, nu=0.10):
+    C = C+diag(len(C)*[lambd])
+    Cd = C.diagonal()
+    C2 = C*(1.0-nu)
+    # copy back intial diagonal
+    for i in range(len(Cd)):
+        C2[i,i] = Cd[i]
+    WW = dot(inv(C2),C)
+    return WW
+
+
+####################
+### main program ###
+
+if __name__ == "__main__":
+
+    try:
+        datapmatfile, img_height, img_width, filtertype, lambd, nu = sys.argv[1:]
+        img_height = int(img_height)
+        img_width = int(img_width)
+        lambd = float(lambd)
+        nu = float(nu)
+    except:
+        print "Usage: "+sys.argv[0]+" <datafile.pmat> <img_height> <img_width> <filtertype> <lambda> <nu>"
+        print """
+        Input is considered to be the first img_height x img_width columns.
+        Covariance matrix will get lambda*I added to its diagonal, and its off-diagonal terms multiplied by (1-nu).
+        Filtertype can be 'PCA' or 'denoising'        
+        """
+        raise
+    # sys.exit()
+
+    computeAndShowFilters(datapmatfile, img_height, img_width, filtertype, lambd, nu)
+
+


Property changes on: trunk/scripts/EXPERIMENTAL/linearfilters.py
___________________________________________________________________
Name: svn:executable
   + *

Modified: trunk/scripts/pyplot
===================================================================
--- trunk/scripts/pyplot	2008-05-31 21:21:42 UTC (rev 9079)
+++ trunk/scripts/pyplot	2008-05-31 21:22:57 UTC (rev 9080)
@@ -48,7 +48,8 @@
     pass
 
 from plearn.io.server import *
-from plearn.plotting import *
+from plearn.plotting.matplotlib_utils import *
+from plearn.plotting.mayavi_utils import surfplot_xymagnitude
 
 server_command = 'plearn server'
 serv = launch_plearn_server(command = server_command)

Modified: trunk/scripts/show_rows_as_images.py
===================================================================
--- trunk/scripts/show_rows_as_images.py	2008-05-31 21:21:42 UTC (rev 9079)
+++ trunk/scripts/show_rows_as_images.py	2008-05-31 21:22:57 UTC (rev 9080)
@@ -1,7 +1,7 @@
 #!/usr/bin/env python
 
 import sys
-from plearn.vmat.PMat import load_pmat_as_array
+from plearn.vmat.PMat import PMat
 from plearn.plotting.netplot import showRowsAsImages
 
 ####################
@@ -22,12 +22,12 @@
         These will be interactively displayed in a nrows x ncols grid of imagettes.
         """
         print "Ex: "+sys.argv[0]+" /home/fringant2/lisa/data/faces/olivetti/faces.pmat 64 64  5 7"
+        print "Ex: "+sys.argv[0]+" /home/fringant2/lisa/data/icml07data/mnist_basic/plearn/mnist_basic2_train.pmat 28 28 5 7"
         raise
     # sys.exit()
 
-    data = load_pmat_as_array(datapmatfile)
-    inputs = data[:,0:imgheight*imgwidth]
-    showRowsAsImages(inputs, figtitle=datapmatfile, nrows=nrows, ncols=ncols, img_width=imgwidth)
+    data = PMat(datapmatfile)
+    showRowsAsImages(data, img_height=imgheight, img_width=imgwidth, nrows=nrows, ncols=ncols, figtitle=datapmatfile)
 
 
 



